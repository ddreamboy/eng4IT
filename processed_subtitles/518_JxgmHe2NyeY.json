{
  "text": "so today's session what all things we\nare basically going to discuss so first\nof all we going to discuss about\ndifferent types of machine learning\nalgorithm like how many different types\nof machine learning\nalgor understand the purpose of taking\nthis session is to clear the interviews\nokay clear the interviews once you go\nfor a data science interviews and all\nthe main purpose is to clear the\ninterviews I've seen people who knew\nmachine learning algorithms in a proper\nway okay they were definitely able to\nclear it because they just explain the\nalgorithms in a better way to the\nrecruiter so that they got hired first\nof all is the introduction to machine\nlearning here I'm just specifically\ngoing to talk about AI versus ml versus\nDL versus data sign then the second\nthing that we are going to talk about\nover here is the difference between\nsupervised MS\nand unsupervised ml the third thing that\nwe are probably going to discuss about\nis something called as linear regression\nso we are going to clearly understand\nthe maths and geometric intuition the\nnext thing that we are probably going to\ndiscuss about is R square and adjusted R\nsquare the fifth topic that we are going\nto discuss about is Ridge and lasso\nregression the first topic that we are\ngoing to discuss about is AI versus ml\nversus DL versus data science so this is\nthe first topic that we are probably\ngoing to discuss if you really want to\nunderstand the difference between AI\nversus ml versus DL versus data science\nwe will go in this specific format so\njust imagine the entire universe so this\nentire universe I will probably call it\nas an AI now specifically when I say AI\nthis basically means AI artificial\nintelligence whatever role you are in\nyou are as a machine learning developer\nyou working as a deep learning developer\nVision developer or a data scientist or\nan AI engineer at the end of the day you\nare actually creating AI application so\nif I really want to Define what is this\nartificial intelligence you can just say\nthat it is a process wherein we create\nsome kind of applications in which it\nwill be able to do its task without any\nhuman intervention so that basically\nmeans a person need not monitor this AI\napplication automatically it'll be able\nto make decisions it will be able to\nperform its task and it will be able to\ndo many things so this is what an AI\napplication is some of the examples that\nI would definitely like to consider so\nthe first example that I would like to\nconsider AI application AI module\nNetflix has an AI module suppose if you\nsee a kind of action movie for some time\nthen the kind of AI work or AI work that\nis basically implemented over here is\nsomething called as recommendation\nso here through this application what\nhappens is that when you're continuously\nseeing the action movies then\nautomatically the AI module that is\npresent inside Netflix will make sure\nthat it gives us recommendation on\naction movies second if I take an\nexample of comedy movie If I\ncontinuously see comedy movie then also\nit'll give us the recommendation of the\ncomedy movie so this through this what\nhappens is that it understands your\nbehavior and it is being able to do its\ntask without asking you anything the\nsecond example that I would like to take\nup in is\namazon.in now amazon.in again if you buy\nan\niPhone then it may recommend you a\nheadphones so this kind of\nrecommendation is also a part of AI\nmodule that is integrated with the\namazon.in website the ads that you see\nprobably when you opening my channel\nthrough which I get paid a little bit\nfrom my from a from the hard work that I\ndo in YouTube right so through that ads\nhow that is recommended to you uh that\nis also an AI engine that is included in\nthe YouTube channel itself which really\nplays it is a business-driven goal\nunderstand it is a business driven\nthings that we basically do with the\nhelp of AI one more example that I would\nlike to give you is if I consider it\nself-driving cars so here you'll be able\nto see self-driving cars if you take an\nexample of Tesla so self-driving cars\nwhat happens based on the road it is\nable ble to drive it automatically who\nis doing that there is an AI application\nintegrated with the car itself right so\nif I consider all these things these all\nare AI application at the end of the day\nwhatever role you do you are going to\ncreate an AI application this is the\ncommon mistake what people do you know\nlike our CEO sudhansu Kumar he has\nwritten in his profile that he's an AI\nengineer that basically means his goal\nis to create an AI application so\nprobably in a product based companies\nyou'll be seeing this kind of roles\ncalled as AI engineer now let's go to\nthe next role which is called as machine\nlearning so where does machine learning\ncomes into existence so if I try to\ncreate this machine learning is a subset\nof AI and what is the role of machine\nlearning it provides stats\ntools\nto analyze the data visualize the data\nand apart from that to do\npredictions I'm\nforecasting so you will be seeing a lot\nof machine learning algorithms so\ninternally those machine learning\nalgorithm the equation that we are\nbasically using it is basically using it\nis having a kind of stats tool stat\ntechniques because whenever we work with\ndata statistics is definitely very much\nimportant so this exactly is called as\nmachine learning so it is a subset of AI\nthis is very much important to\nunderstand ml is a subset of AI so here\nyou can see that it is a part of this\nnow let's go to the next one which is\ncalled called as deep learning deep\nlearning is again a subset of ml now\nlet's consider why deep learning came\ninto existence because in 1950s 60s\nscientists thought that can we make\nmachine learn like how we human being\nlearn so for that particular purpose\ndeep learning came into existence here\nthe plan is to basically mimic human\nbrain so when I say mimicking human\nbrain that basically means we are trying\nto mimic the human brain to implement\nsomething to learn something so for this\nyou use something called as\nmulti-layered neural networks so this is\nwhat deep learning is it is a subset of\nmachine learning its main aim is to\nmimic human brain so they actually\ncreate multi-layer neural network and\nthis multi-layered neural network will\nbasically help you to train the machines\nor applications whatever we are trying\nto create and deep learning has really\nreally done an amazing work with the\nhelp of deep learning we are able to\nsolve such a complex complex complex use\ncases that we will be probably\ndiscussing as we go ahead now if I come\nto data science see this is the thing\nguys if you want to say yourself as a\ndata scientist tomorrow you given a\nbusiness use case and situation comes\nthat you probably have to solve that use\ncase with the help of machine learning\nalgorithms or deep learning algorithms\nagain the final goal is to create an AI\napplication right you cannot say that I\nam a data scientist and I'll just work\nin machine learning I or I'll work in\ndeep learning or I may I don't know how\nto analyze the data no you cannot do\nthat when I was working in Panasonic I\ngot various different kind of task\nsometime I was told to use W powerbi to\nvisualize analyze the data sometime I\nwas given a machine learning project\nsometime I was given a deep learning\nproject so as a data scientist if I\nconsider where does data scientist fall\ninto this it will be a part of\neverything so if I talk about machine\nlearning and deep learning with respect\nto any kind of problem statement that we\nsolve the majority of the business use\ncases will be falling in two sections\none is supervised machine learning one\nis unsupervised machine learning so most\nof the problems that you are basically\nsolving this is with respect to this two\nproblem statement two different types of\nmachine learning algorithms that is\nsupervised machine learning and deep\nlearning if I talk about supervised\nmachine learning two major problem\nstatements that you are basically\nsolving here also one is regression\nproblem\nand the other one is something called as\nclassification problem and in the case\nof unsupervised machine learning problem\nstatement you are basically solving two\ndifferent types of problem one is\nclustering and one is dimensionality\nreduction and there is also one more\ntype which is called as reinforcement\nlearning reinforcement learning I can I\nI will definitely talk about this not\nright now right now we are just focusing\non all these things now understand what\nhappens in supervised machine learning\nlet's consider consider a data set so\nhere I have a data set which says this\nis my age and this is my weight suppose\nI have these two specific features let's\nsay that I have values like 24 62 25 63\n21 72\n257 uh 62 and many more data over here\nlet's say that my task is to basically\ntake this particular data and create a\nmodel wherein so suppose my task is that\nI need to create a model whenever it\ntakes the New Age first of all we train\nthis model with this data and whenever\nwe take age a new age it should be able\nto give us the output of weight this\nparticular model is also called as\nhypothesis okay I'll discuss about this\ntoday when I we discussing about linear\nregression now what are the important\ncomponents whenever we have this kind of\nproblem statement first of all you need\nto understand there are two important\nthings one is independent features and\nthe other one is something called as\ndependent features now let's go ahead\nand discuss what is independent feature\nindependent feature basically means in\nthis particular case since the input\nthat I'm basically training in all those\nfeatures becomes an independent feature\nnow in this particular case my age is\nindependent feature and whatever I'm\nactually predicting so when I say\npredicting I know this is my output okay\nthis is the what I have to basically\nmake my model uh give this as a an\noutput so in this particular casee my\ndependent feature becomes weight why we\nspecifically say a dependent feature\nbecause this is completely dependent on\nthis value whenever this is increasing\nor decreasing this value is basically\ngetting changed so that is the reason\nwhy we basically say this has\nindependent and dependent feature\nwhenever we are solving a problem right\nin the case of supervised machine\nlearning remember they will be one\ndependent feature and there can be any\nnumber of independent features now let's\ngo ahead and let's discuss about\nregression and classification what is\nthe difference between them now let\nlet's go ahead and let's discuss about\ntwo things one\nis let's say I want a regression problem\nstatement suppose I take the same\nexample as age and weight so I have\nvalues like as discussed 24 72 23\n71 uh 24 or 25\n71.5 okay so this kind of data I have\nsee this is my output variable which is\nmy dependent feature now in this\nparticular dependent feature now\nwhenever I'm trying to find out the\noutput and in this particular output you\nhave a continuous variable when you have\na continuous variable then this becomes\na regression problem statement now one\nexample I would like to give suppose\nthis is my data set right this is my age\nthis is my weight suppose I am\npopulating this particular data set with\nthe help of scatter plot then in order\nto basically solve this problem what\nwe'll do suppose if I take an example of\nlinear regression I will try to draw a\nstraight line and this particular line\nis my equation which is called as yal mx\n+ C and with the help of this particular\nequation I will try to find out the\npredicted points so this will be my\npredicted point this will be my\npredicted point this this any new points\nthat I see over here will basically be\nmy predicted point with respect to Y so\nin this way we basically solve a\nregression problem statement so this is\nvery much important to understand let's\ngo to the always understand in a\nregression problem statement your output\nwill be a continuous variable the second\none is basically a classification\nproblem now in classification problem\nsuppose I have a data set let's say that\nnumber of hours study number of study\nhours number of play\nhours so this is my independent feature\nlet's say a number of sleeping hours and\nfinally I have my output which will will\nbe pass or fail so in this I have all\nthis as my independent features and this\nis my dependent feature so I will be\nhaving some values like this and here\neither you'll be pass or fail or pass or\nfail now whenever you have in your\noutput fixed number of categories then\nthat becomes a classification problem\nsuppose it just has two outputs then it\nbecomes a binary classification if you\nhave more than two different categories\nat that time it becomes a multiclass\nclassification so this is the difference\nbetween regression problem statement and\nthe classification problem statement now\nlet's go ahead and let's discuss about\nsomething called as unsupervised machine\nlearning now in unsupervised machine\nlearning which is my second main topic\nover here I'm just going to write\nunsupervised machine learning now what\nexactly is unsupervised machine learning\nhere whenever I talk about there are two\nmain problem statement that we solve one\nis clustering\none is dimensionality reduction let's\ntake one example of a specific data set\nover here let's say that my data set is\nsomething called as salary and age now\nin this scenario we don't have any\noutput variable no output variable no\ndependent variable then what kind of\nassumptions that we can take out from\nthis particular data set suppose I have\nsalary and age as my values so in this\nparticular case I would like to do\nsomething called as clustering now why\nclustering is used just understand let's\nsay I am going to do something called as\ncustomer segmentation now what does this\ncustomer segmentation do clustering\nbasically means that based on this data\nI will try to find out similar groups\ngroups of people suppose this is my one\ngroup this is my another group this is\nmy third group let's say that I was able\nto create this many groups this many\ngroups are clusters I'll say cluster 1 2\nthree each and every cluster will be\nspecifying some information this cluster\nMay specify that this person uh he was\nvery young but he was able to get some\namazing salary this person it may some\nspecify that these people are basically\nhaving more age and they are getting\ngood salary these people are like middle\nclass background where with respect to\nthe age the salary is not that much\nincreasing so here what we are doing we\nare doing clustering we are grouping\nthem together main thing is grouping\nthis word is very much important now why\ndo we use this suppose my company\nlaunches is a product and I want to just\nTarget this particular product to rich\npeople let's say product one is for rich\npeople product two is for middle class\npeople so if I make this kind of\nclusters I will be able to Target my ads\nonly to this kind of people let's say\nthat this is the rich people this is the\nmiddle class people I will be able to\nTarget this particular ads or this\nparticular product or send this\nparticular things to those specific\ngroup of people by that that is\nbasically called as ad marketing and\nthis uses something called as customer\nsegmentation a very important example\nand based on this customer segmentation\nwe can later apply any regression or\nclassification kind of problem statement\nnow coming to the second one after\nclustering which is called as\ndimensionality reduction now in\ndimensionality reduction what we are\nfocusing on suppose if we have th000\nfeatures can we reduce this features to\nlower Dimensions let's say that I want\nto convert this\nuh th000 feature to 100 features lower\nDimension so can we do that yes it is\npossible with the help of dimensionality\ndeduction algorithm there are some\nalgorithms like PCA so I'll also try to\ncover this as we go ahead understand\nclustering is not a classification\nproblem clustering is a grouping\nalgorithm there is no output feature no\ndependent variable in clustering sorry\nin unsupervised ml so yes I will also\ntry to cover up LDA we'll cover up PCA\nand all as we go ahead so with respect\nto supervised and unsupervised so first\nthing that we are going to cover is\nsomething called as linear regression\nthe second algorithm that we will try to\ncover after linear regression is\nsomething called as Ridge and lasso\nthird that we are going to cover is\nsomething called as logistic regression\nthe fourth that we are basically going\nto cover is something called as decision\ntree decision tree includes both\nclassification and regression four fifth\nthat we are going to cover is something\ncalled as adab boost sixth that we are\ngoing to cover is something called as\nrandom Forest seventh that we are going\nto cover is something called as gradient\nboosting eighth that we are going to\ncover is something called as XG boost N9\nthat we are going to cover is something\ncalled as n bias then when we go to the\nunsupervised machine learning algorithm\nthe first algorithm that we are going to\ndo is something called as K means K\nmeans algorithm then we also have DV\nscan then we are also going to do higher\nC clustering there is also something\ncalled as K nearest neighbor clustering\nfifth we'll try to see about PCA then\nLDA so different different things we\nwill try to cover up yes svm I have\nmissed here I'm going to include svm KNN\nwill also get covered so I have that in\nmy list probably I may miss one or two\nbut we are going to cover everything so\nlet's start our first algorithm linear\nregression so let's go ahead and discuss\nabout linear regression linear\nregression problem statement is very\nsimple guys so suppose I have let's say\nI have two features one is my X feature\nand one is my y feature let's say that X\nis nothing but age and Y is nothing but\nweight so based on these two features I\nhave some data points that has been\npresent over here so in linear\nregression what we try to do is that we\ntry to create a model with the help of\nthis training data set so this will be\nmy training data set what I'm actually\ngoing to do is that I'm going to\nbasically train a model and this model\nis nothing but a kind of hypothesis\ntesting or it is just kind of hypothesis\nwhich takes the new age and gives the\noutput of the weights and then with the\nhelp of performance metrics we try to\nverify whether this model is performing\nwell or not now in short what we are\ngoing to do in linear regression is that\nwe'll try to find out a best fit line\nwhich will actually help us to do the\nprediction that basically means if I get\nmy new age over here then what should be\nmy output with respect to Y okay so with\nrespect to this what should be my output\nover here in this particular case\nwhenever we are drawing a diagram like\nthis I can basically say that Y is a\nlinear function of X so this is what we\nare going to do now understand how we\nare going to create this best fit line\nthis is very much important whenever we\nsay linear regression it basically means\nthat we are going to create a linear\nline over there you may be thinking sir\nwhy to create linear line why not\nnonlinear line that I'll discuss about\nit as we go ahead see other other\nalgorithms so to begin with let's\nconsider this line that you see over\nhere right this line equation can be\ngiven by multiple equations someone some\npeople people write yal mx + C some\npeople write uh H some people write yal\nbeta 0 + beta 1 into X some people write\nH Theta of xal to Theta 0 + Theta 1 into\nX many many equations are there for this\nthis straight line this straight line\nmany many equations are there with\nrespect to many many different kind of\nnotations but the first algorithm that I\nhave probably learned of linear\nregression is from Andrew Ng definitely\nI would like to give him the entire\ncredits and based on his notation\nwhatever he has explained I'll try to\nexplain you over here so the credits for\nthis algorithm specifically goes to\nAndrew NG so let's consider this one\nover here in order to create this\nstraight line I will basically use a\nequation which is called as H Theta so\nthis is the equation of a straight line\nif I know the equation of the straight\nline whatever I can write I can write\nmany things yal mx + C yal beta 0 + beta\n1 * X and then I can also write one more\nthat is H Theta of xal theta 0 + Theta 1\ninto X of I here also you can basically\nsay x of I here also you can say x of I\nnow let's go ahead and let's take this\nequation for now let's take this\nequation of now so I'm I'm going to take\nout this equation and just write one\nequation through which I have also\nstudied but I will definitely be adding\nsome points which probably Andrew and\ncould not mention mention in his video\nbut I'll try my level best obviously he\nis the best I cannot even compare myself\nto him so Theta 0 + Theta 1 into X now\nlet's understand what is Theta 0 Theta 1\nas I said that let's say I have a\nproblem statement over here let's say I\nthis is my X and this is my y this is my\ndata points now what I'm doing I'm\ntrying to create a best fit line like\nthis now what is this best fit line what\nis uh when I say this best fit line is\nbasically given by this equation what\ndoes Theta 0 basically indicate Theta 0\nover here is something called as\nintercept now what exactly is intercept\nintercept basically means that when your\nX is zero then H Theta of X is equal to\nTheta 0 so in this particular case\nintercept basically indicates that at\nwhat point you are meeting the Y AIS so\nthis particular point is basically\nyour intercept when your X is equal to 0\nat that point of time you'll be seeing\nthat this line is intersecting the y-\nAIS whatever value this will be that is\nyour intercept now the second thing is\nabout your Theta 1 what is Theta 1 this\nis nothing but slope or coefficient now\nwhat does this basically indicate this\nindicates let let's say that this is the\nunit one unit in the x-axis and probably\nwith respect to this I can find one\npoint over here one point over here and\nif I try to draw this over here to here\nthis is the unit movement in y so what\ndoes it basically say slope with the\nunit movement in one one unit movement\ntowards the x-axis what is the unit\nmovement in y- axis that is basically\nslope or coefficient Theta 0 and Theta 1\ntwo things and X of I is definitely your\ndata points now our main aim is to\ncreate a best fit line in such a way\nthat I I'll just try to show it to you\nwhat is our main aim let's let's\nunderstand what is the aim of a linear\nregression so if I take an example of\nlinear regression I need to find out the\nbest fit line in such a way that the\ndistance\nbetween this data points that I have and\nthe predicted points should be very very\nless suppose I'm creating a best fit\nline okay I'm creating a best fit line\nso with respect to this data points\ninitially was this right but my\npredicted point is this point in this\nparticular case my predicted point is\nthis point so and if I do do the\nsummation of all these points those\ndistance should be minimal then only\nI'll be able to say that this is the\nbest fit line so I I cannot definitely\nsay that this is exactly the best fit\nline or not how will I say when I try to\ncalculate the difference between this\npoint and the predicted Point these are\nmy predicted point right if I try to\ncalculate the distance between them then\nI will basically have a aim to it should\nbe minimal if I do the summation of all\nthe distance it should be minimal\nso for that what I can do is that see\nyou may be also thinking Krish why not\njust do one thing okay suppose if these\nare my data points why not just play and\ncreate multiple lines and try to compare\nwhat we can do is that we can compare\nmultiple we can create multiple lines\nright like this and then whoever is\ngiving the best minimal point I will go\nand select that but how many iteration\nyou will do how you will come to know\nthat okay this line is the best line so\nfor that specific purpose we should\nstart at one point and we should lead\ntowards finding the best fit line start\nat one point and then we should go\ntowards finding the best fit line so for\nthis particular purpose what we do is\nthat we create a something called as uh\ncost function I have already shown you\nwhat is my hypothesis function my best\nfit line equation is basically given as\nH Theta of x equal to Theta 0 + Theta 1\n* X this is my hypothesis right now\ncoming to the cost function which is\nsuper super important why this it is\nsuper important because cost function\nbasically what what is cost function\nover here I told right right this\ndistance when I do the\nsummation this distance that I when I'm\ndoing the summation it should be minimal\nso if I really want to find out this\nparticular distance I will be using one\nmore equation how can I use a distance\nformula between the predicted and the\nreal point I will just say that H Theta\nof x - y so when I say h Theta of x - Y\nwhat does this basically mean this is my\nreal point and this is my predicted\nPoint predicted point is basically given\nby H Theta of X and what I'm going to do\nI'm going to basically do the squaring\nbecause I may get a negative value so\nbecause of that I really want to do the\nsquaring part Now understand one thing I\nneed to also do the\nsummation I = 1 to compl complete M\nlet's say that I'm taking the number of\ndata points over here as M because I\nneed to calculate the distance between\nall the points right with respect to the\npredicted and the predict with respect\nto the real\npoints so after this I also need to\ndivide by 1X 2m the reason why I'm\ndividing by first of all let me show you\nwhy we are dividing by 1 by m 1 by m\nwill give us the average of all the\nvalues that we have the specific reason\nwhy we are dividing by 1 by 2 do is for\nthe derivation purpose it helps us to\nmake our equation very much simpler so\nthat later on when I am updating the\nweights when I say weights I'm basically\nupdating Theta 0 and Theta 1 Theta 0 and\nTheta 1 at that point of time you'll be\nable to see that this particular value\nwhen we probably do the derivative it\nwill help us to do it again I'm going to\nrepeat it I'm going to write it down for\nyou first of\nall now in order to find find out the\nbest fit line I need to keep on changing\nTheta 0 and Theta 1 unless and until I\nget the best fit line unless and until I\ndon't get the best fit line I need to\nkeep on updating Theta 0 and Theta 1 now\nif I need to keep on updating Theta 0\nand Theta 1 I probably require a cost\nfunction okay what this cost function\nwill do I'll just tell you so cost\nfunction over here I will specify as J\nof theta 0 comma Theta 1 is equal to now\nwhat is cost fun function over here what\nthis distance I told right this distance\nbetween the H Theta of X and Y if I do\nthe summation of all these things it\nneeds to be minimal it needs to be less\nbecause with respect to an X point this\nis my y point\nright similarly with respect to this x\npoint this is my y point so what I'm\nactually going to do I'm going to use a\ncost function now in this cost function\nmy main aim is\nto basically write H Theta of x - y s\nthis will be with respect to I I I why I\nam saying I because this will be moving\nfrom I equal to 1 to all the points that\nis m m is basically all the points over\nhere now apart from this what I actually\ngoing to do I'm going to divide by 1X 2\nm I'll tell you why I'm specifically\ndividing by 1X 2 m first of all by\ndividing by m I will be getting an\naverage\noutput average cost function because\nhere I'm iterating M the reason why I'm\ndividing by two because it will help us\nin derivation why let's say that I have\nx² if I try to find out derivative of x²\nwith respect to X then what will I get I\nwill basically get 2x right that is what\nis the formula what is the derivation of\nX of n it is nothing but n x of n\nminus1 so that is the reason why I'm\nactually making it 1 by two so that when\ntwo comes over here this two and two\nwill get cancelled so I hope everybody's\nable to understand so this is my cost\nfunction Now understand what is this\ncalled as this entire equation is\nbasically called as squared error\nfunction yes mathematical Simplicity\nbasically means because when we are\nupdating Theta 0 and Theta 1 we\nbasically find out derivation in the\ncost function so that is the reason why\nwe are specifically doing it squaring\noff is basically done because so that we\ndon't get any negative values here\nsquared error function now let's go\ntowards the what we need to solve this\nis my cost function okay so I need to\nminimize minimize this particular value\nthat is 1x 2 m summation of I = 1 2 m\nand then this will basically be H Theta\nof X of I minus y of I whole Square we\nneed to minimize this by adjusting\nparameter Theta 0 and Theta 1\nthis entirely is what this is nothing\nbut J of theta 0 comma Theta 1 and we\nreally need to minimize this so this is\nour task okay this is our task now let's\ngo ahead and let's try to compare with\ntwo different thing one is the\nhypothesis testing and one is with\nrespect to the cost\nfunction okay let's take an\nexample so right now my equation of\nthe\nhypothesis is nothing but H Theta of x\nequal to Theta 0 + Theta 1 *\nX if Theta 0 is 0 then what does this\nbasically indicate can I say that it\nbasically the line the line the best fit\nline passes through the origin and this\nis nothing but s Theta of xal to Theta\n1 multiplied by X can I say like this\nobviously I can definitely say like this\nright so my equation will be like this\nso for right now let's consider that\nyour Theta 0 is equal to 0 so this is\nwhat it is we have done till here we\nhave minimized we have written the\nequation everything yes so it is passing\nthrough the origin and this is what is\nthe equation I'm actually getting now\nlet's take one example and let's try to\nsolve this if I if I have H Theta of X\nso this is my new hypothesis considering\nthat my intercept is passing through the\nregion so with respect to this let's say\nthat I will create one line over here\nlet's say this is\nmy this is my data points like X1 y1 I\nhave 1 2 3 I have 1 2 3 now let's\nconsider that if I have T I have data\npoints like what I have data points like\nlet's say I have three data points 1\ncomma 1 2A 2 3 comma 3 so 1A 1 is\nnothing but this is my data point 2A 2\nis nothing but this is my data point and\n3 comma 3 is this is my data point so\nthese are my data points from the data\nset that I\nhave so 2 comma 2 is this point and 3\ncomma 3 is basically this point let's\nconsider that these are my points that I\nhave these are my data points now if I\nconsider Theta 1 as 1 where do you think\nthe straight line will pass through\nwhere do you think the straight line\nwill pass the straight line will\ndefinitely pass like this right my\nstraight line will definitely pass\nthrough all the points this same point\nbecomes a prediction point also right\nsame point let's consider that this is\nalso getting pass through this it passes\nthrough all the points when Theta 1 is\nequal to 1 Theta 1 is nothing but slope\nwhen slope is equal to 1 in this\nscenario it passes through all the\npoints now go ahead and calculate your J\nof theta so what will the form of J of\ntheta 1 become because Theta 0 is 0 okay\nwe can basically write 1 by 2 m\nsummation of I = 1 2 three how many\npoints are there three right and here I\nhave J of H of theta of X1\nsorry X of theta of x i - y i\ns right now let's go ahead and compute\nnow in this particular scenario what\nwill happen 1X 2 m\nthen what is what is this point minus y\nof I see h of X is also 1 y of I is also\none both the point are 1 so this will\nbecome 1 - 1 whole S Plus because we are\ndoing summation the next point is also\nfalling in 2A 2 so this will become 2 -\n2 s + 3 - 3 S so in total this will\nbecome zero so when your J of theta when\nTheta 1 is 1 Theta 1 is 1 so J of theta\n1 is how much it is\nZ right so what is this J of theta 1 it\nis the cost function so let me draw the\ncost function graph over here let's say\nthat this is my Theta and this is\nmy so here I have 0.5 here I have 1 here\nI have 1.5 so this is my Theta here I\nhave two then I have 2.5 okay then\nsimilarly I have 0. five then I have 1\n1.5 2 2.5 this is my J of theta 1 so\nright now what is my Theta 1 my Theta 1\nis 1 at this particular Point what did I\nget J of theta 1 is nothing but zero so\nthis will be my first point this will be\nmy first point guys I have discussed why\nwhy the value will be 1X 2m basically to\nmake the calculation simpler we are\ndividing by 1X 2 m is basically used to\naverage aage is the sumission that we\nare actually doing over here now let's\ngo ahead and let's take the second\nscenario in the second scenario let's\nconsider my Theta 1 let's say that my\nTheta 1 over here is now 0.5 if my Theta\n1 is 0.5 then tell me what are the\npoints that I will get for x equal to\n1.5 * 1 so it will come as 0.5 over\nhere right then similarly when X is\nequal to 2.5 * 2 is nothing but 1 over\nhere and then similarly when uh for x\nequal to\n35 multiplied by 3 see we are\nmultiplying here right5 multi by 3 is\n1.5 so the next point will come over\nhere now when I create my best fit line\nwhat will happen so here is my next best\nfit line which I will probably create by\ngreen\ncolor okay so this is my second one\nwhich is green color here definitely\nslope is decreasing so if I go ahead and\ncalculate my J of theta let's see what\nI'll get so J of theta\n1 is nothing but 1X 2\nm again same equation summation of I = 1\n2 3 H Theta of X of\ni - y of\ni² so what we have for over here we have\nnothing but 1X 2 m now let's do the\nsummation what is this point this point\nis nothing but the predicted point and\nthis point is the real point right so in\nthis particular scenario the first point\nthat I will get is nothing but. 5 - 1\nwhole s how I'm getting. 5 - 1 whole\nSquare this is 1 this is the real Point\n1 this is the predicted Point .5 so here\nI'm getting. 5 - 1 whole Square the\nsecond point will be 1 - 2 whole s right\n2 so 1 - 2 whole\ns and then I will finally get 1.5 - 3\nwhole s so finally if I do this\ncalculation how much I'm actually\ngetting 1X 2 * 3 which is 6 here I'm\ngetting\n.25 5 Square here I'm getting 1 here I'm\ngetting 1.5 whole Square so my final\noutput will be which I have already\ncalculated it is nothing but point it\nwill be approximately equal to. 58 so 58\nnow with Theta as this is nothing but\nTheta Theta 1 as\n.5 right that is what Theta 1 as .5 we\nare able to get. 58 so Theta 1 is .5\nover here and. 58 will be coming\nsomewhere here right so this is my next\npoint which will be again in green color\nnow let's go ahead and calculate the\nthird condition now in third condition\nwhat I'm actually going to write I'm\ngoing to basically say Theta 1 as 0 at\nthat point of time just go and assume\nwhat is 0 multiplied by X it will\nobviously be zero so I will be getting\nthree points and my next line will be in\nthis line that is the\nx-axis and this is basically all my\npoints now if I go ahead and calculate\nthis what is J of theta 1\nnow what is J of theta 1 now in this\nparticular case when my Theta 1 is equal\n= to 0 1X 2 m now this part you'll be\nable to see this is 0 - 1 0 - 2 0 -\n3 okay so it will become 0 - 1 s 0 - 2 s\nand 0 - 3\nS okay so this will become 1X 6\n* 1 + 4 + 9 which will not be it will be\nnothing but 2.3 which is approximately\nequal to\n2.3 then what will happen with respect\nto Theta 1 as 0 we are getting 2.3 so if\nI draw this it is nothing but with\nrespect to zero I'm getting 2.\n2\n2.3 this is my point so similarly when\nyou start constructing with Theta 1 is\nequal 2 I may get some point over here\nso here when I join this points\ntogether you will be seeing that I will\nbe getting this kind of\ncurve okay and this curve is something\ncalled as gradient\ndescent and this gradient descent will\nplay a very very important role in\nmaking sure that in making sure that you\nget the right Theta 1 value or light\nslope value now which is the most\nsuitable point the most suitable point\nis to come over here because this is\nthis this point is basically called AS\nGlobal\nMinima because see out of all these\nthree lines which is the best fit line\nthis is the best fit line right this is\nthe best fit line when I had this best\nfit line my point that came over here\nwas here itself this was my point that\ncame over here right and I want to\nbasically come to this region because\nthis is my Global\nMinima when I basically am over here the\ndistance between the predicted and the\nreal point is very very less right so\nthis specific point is basically called\nAS Global minimum but still I did not\ndiscuss Krish you have assumed Theta 1\nis 1 Theta 1 is .5 Theta 1 is 0 here\nalso you're assuming many things right\nand then you probably calculating and\nyou're creating this gradient descent\nbut the thing should be that probably\nyou come to one point over here and then\nyou reach towards this so for that\nspecific reason how do you do that how\ndo I first of all come to a point and\nthen move towards This Global Minima so\nfor that specific case we will be using\none convergence algorithm because if I\ncome to one specific point after that I\njust need to keep on updating Theta 1\ninstead of using different different\nTheta 1 value so for this we use\nsomething called as convergence\nalgorithm so here the convergence\nalgorithm basically says\nrepeat until\nconvergence that basically means I'm in\na while loop let's say and here I'm\nbasically going to update my Theta value\nwhich will be given by this notation\nwhich is continuous updation where I'll\nsay Theta J minus I'll talk about this\nAlpha don't worry and then it will be\nderivative of theta\nJ with respect to this J of theta\n0 and Theta 1 so this should happen that\nbasically means after we reach to a\nspecific point of theta after performing\nthis particular operation we should be\nable to come to the global Minima and\nthis this specific thing that you are\nable to see is called as\nderivative this is called as derivative\nderivative basically means I'm trying to\nfind out the slope\nderivative which I can also say it as\nslope this equation will definitely work\nguys trust me this will definitely work\nwhy it will work I'll just draw it show\nit to you let's say that this is my cost\nfunction let's say that I've got this\ngradient\ndescent and let's say that my first\npoint is somewhere here but I have to\nreach somewhere here right now when I\nreach this this is my Theta 1 and this\nis my J of theta 1 suppose I reach at\nthis specific point and I will also have\nanother gradient descent which looks\nlike this let's say that in the initial\ntime I reach the point over here how we\nwill be coming to this minimal Global\nMinima by using this equation I'll talk\nabout Alpha also don't worry now this is\nalso my Theta 1 this is also my J of\ntheta 1 now let's say suppose I came to\nthis particular point right after coming\nto this particular point I will\nbasically apply this derivative on this\nJ of theta 1 okay now when I find out a\nderivative that basically means we are\ntrying to find out the slope and in\norder to find the slope we just create a\nstraight line like\nthis which will look like this I'll just\ntry to\ncreate so I'll try to create a slope\nlike this this\nslope so if you try to find out with\nrespect to this this is a positive slope\nhow do we indicate it because understand\nthe right hand side of the line of this\nis pointing on the top wordss Direction\nthis is the best easy way to find out\nwhether it is a positive slope or\nnegative slope now in this particular\ncase this is a positive slope now when I\nget a positive slope that basically\nmeans I will update my weights or Theta\n1 as Theta 1 let's say I'm writing it\nover here so I will just apply this\nconvergence algorithm see Theta\n1 colon Theta 1 minus this learning rate\nwhich is called as Alpha this is my my\nlearning rate I'll talk about learning\nrate don't worry then this derivative\nvalue in this particular case since I'm\nhaving a positive slope I will be\ngetting a positive value let's say that\nfor this Theta value I got this slope\ninitially now I need to come to this\nlocation so for that I have to reduce\nTheta 1 so that I come to this main\npoint now here you can see that I am I\nsubtracting Theta 1 with something which\nis a positive number\nright this is a positive number so\ndefinitely I know that after some n\nnumber of iteration I will be able to\ncome to the global Minima similarly if I\ntake the right hand side and if I try to\ndraw the slope in this particular case\nmy slope will be\nnegative so similarly I can write the\nequation as Theta\n1 = to Theta 1 minus learning rate\nmultiplied by a negative number so minus\ninto minus will be positive right\nsuppose initially my 1 was\nhere my Theta 1 was here now I'll keep\non updating the weight to come to this\nGlobal Minima so minus into minus is\npositive so I will basically get Theta 1\n+\nAlpha by a positive number because minus\ninto minus is plus so this will\ndefinitely work so that we will be able\nto come over here to the global Minima\nwhether it is a positive slope or a\nnegative slope now what is this learning\nlearning rate now learning rate based on\nthis learning rate suppose I want to\ncome from this point to the global\nMinima by what speed I should be coming\nwhat speed if my learning rate value is\nbigger what speed I may be coming\nsuppose if I say usually we select\nlearning rate as 01 if I select a small\nnumber then it'll start taking small\nsmall steps to move towards the optimal\nMinima but if I take a alpha value a\nhuge value if it is a huge huge value\nthen what will happen this uh this\nupdation of the Theta 1 will keep on\njumping here and there and the situation\nwill be that it will never meet it will\nnever reach the global Minima so it is a\nvery very good decision to take a alpha\nsmall value it should also not be a very\nvery small value if it becomes a very\nvery small value then what will happen\nvery tiny steps it will take forever to\nreach the global Minima that basically\nmeans my model will keep on training\nitself so definitely this Al is going to\nwork now let me talk about one\nscenario one scenario will be that what\nif my my cost function has a local\nMinima what if I have a local Minima\nbecause here if I\ncome here if I come this is a local\nMinima suppose one of my points come\nover here and finally I'm reaching over\nhere what will happen in this particular\ncase because in this case you'll be\nseeing that what will be my equation my\nequation will be simply Theta 1\nTheta 1 minus Alpha in this point in\nthis local Minima slope will be zero so\nin this particular case my Theta 1 will\nbe equal to Theta 1 now you may be\nthinking what is if this is the scenario\nthen we will be stuck in local Minima\nthis is called as local\nMinima but usually with respect to the\ngradient descent and the equation that\nwe are using here we do not get stuck in\nlocal Minima because our gradient\ndescent in this particular scenar iio\nwill always look like this but yes in\ndeep learning when we are learning about\ngrade in descent and a Ann at that point\nof time we have lot of local Minima and\nbecause of that we have different\ndifferent G decent algorithm like RMS\nprop we have Adam optimizers which will\nsolve that specific problem so this one\npoint also I wanted to mention because\ntomorrow if someone asks you as an\ninterview question that what if in your\nuh do you see any local Minima in linear\nregression you can just that the cost\nfunction that we use will definitely not\ngive us local Minima but if in deep\nlearning techniques with that we are\ntrying to use like Ann we have different\ndifferent kind of optimizers which will\nsolve that particular problem so that is\nthe answer you basically have to give\nnow let me go ahead and write with\nrespect to the gradient descent\nalgorithm so here again I'm going to\nwrite the gradient descent algorithm so\nthis will be my gradient descent\nalgorithm and remember guys gradient\ndescent is an amazing algorithm and you\nyou will definitely be using it so\nplease make sure that you know this\nperfectly now some questions are that\nwhen will convergence stop convergence\nwill stop when we come to near this area\nwhere my uh J of theta will be very very\nless now in gradient descent algorithm I\nwill again repeat it so what did I say I\nsaid\nrepeat until convergence I told you\nright here we have written this\nalgorithm\nand now let's take it for Theta 0 and\nTheta 1 so here I will write Theta 0\nJ equal to Theta\nJ minus learning rate of derivative of\ntheta\nJ J of theta 0 and Theta 1 so this is my\nrepeat until convergence now we really\nneed to find out what we'll try to\nequate we'll try to first of all find\nout what is this\nnow if I really want to find out\nderivative\nof derivative of derivative of theta J\nwith respect to J of theta 0 and Theta 1\nso how do I write this I can definitely\nwrite this in a easy way okay so this\nwill be derivative of theta J and\nremember J will be 0 and 1 right because\nwe need to find out for 0 Theta 0 and\nTheta 1 so this will be 1 by 2 m what is\nwhat is J of theta 0a Theta 1 obviously\nmy cost function so I will write\nsummation of IAL 1 to M and here I will\nbasically write J of theta of X of I\nminus y of I whole squar so if my J is\nequal to Z so what will happen for this\nso here I can specifically say that\nderivative of derivative of theta 0 J of\ntheta 0a 1\nnow it's simple here what I will be\ndoing is that I will be simply applying\nderivative function see guys what is\nthis derivative let's consider this is\nsomething like this 1X 2 m x² so if I\ntry to find out the derivative this will\nbe 2x 2 MX so 2 and 2 will get cancel so\nsimilarly I'll have 1 by m and here I\nwill specifically be writing summation\nof I = 1 2 m h Theta of x X of I which\nwill be my\nx - y of i² so this will be my\nderivative with respect to Theta 0 this\nis what I got now the second thing will\nbe that when J is equal to 1 derivative\nof derivative of theta 1 J of theta 0\ncomma Theta\n1 in this particular case I will be\nhaving 1 by m summation of I = 1 to M\nthen again see in this particular case\nTheta of 1 is there right Theta of 1\nbasically means what if I try to replace\nthis let's say that I'm trying to\nreplace this H Theta of X with something\nelse what is s Theta of X I know that\nright it is Theta 0 + Theta 1 * X so\nTheta 0 + Theta 1 * X so after this if\nI'm trying to find out the derivative\nwith respect to Theta 0 this will\nobviously become I will be able to get\nthis much right now with respect to the\nsecond derivative what I will be writing\nI will again be writing H thet of X of i\n- y of i s\nmultiplied X of I so this Square also\nwent off understand this H Theta of X is\nwhat see they H Theta of X is nothing\nbut Theta 0 + Theta 1 * X so if I'm\ntrying to find out derivative with\nrespect to Theta 0 nothing will be going\nto come okay Theta 1 of X will become a\nconstant in this particular case in this\ncase because Theta 1 of X is there so if\nI try to find out derivative of theta 1\ninto X only I'll be getting X Y Square\nwill not be there it's easy right X squ\nmeans 2x this is the derivative of x\nsquare right so that square went and 1X\n2 1 2 by two got cancelled so this will\nbe now my convergence algorithm so here\nwe have discussed about linear\nregression oh sorry I have to remove\nSquare here also so let me write it\nagain okay repeat until conver con let\nme write it down again repeat until\nconvergence finally your two updates\nwill be happening one is Theta 0 so here\nit will be Theta 0\nminus Alpha that is my learning rate 1\nby m summation of IAL 1 to M and this\nwill basically be H Theta of X of I\nminus y of\nI and similarly if I want to update\nTheta 1 it will be - alpha 1 by m\nsummation of I = 1 to m h Theta of X of\nI oh my God y of I uh multiplied by X of\nI Alpha is your learning rate guys Alpha\nis nothing but it is learning rate here\nwe have to initialize some value like\n0.1 see what is s Theta of X Theta 0 +\nTheta 1 into X right if I do derivative\nof theta 1 into x what is derivative of\ntheta 1 with Theta 1 x it is nothing but\nX so this x will come over here now\nlet's discuss about two important thing\none is R square and adjusted R square\nnow similarly what will happen you will\nhave lot of convex functions now see if\nI talk about uh like if you have\nmultiple features like X1 X2 X3 x4 at\nthat point of time you will be having a\n3D curve curve which looks like this\ngradient\ndecent which will be something like this\ngradient it's just like coming down a\nmountain now let's discuss about two\nperformance metrics which is important\nin this particular case one is R\nsquare and adjusted R square\nwe usually use this performance metrix\nto verify how our model is and how good\nour model is with respect to linear\nregression so R square is basically\ngiven R square is a performance Matrix\nto check how good the specific model is\nso here we basically have a formula\nwhich is like 1 minus sum of residual\ndivided by sum of total now this is the\nformula of R squ now what is this sum of\nresidual I can basically write like this\nsummation of y i Min - y i hat whole\nSquare this Yi hat is nothing but H\nTheta of X just consider in this way\ndivided by summation of Y of i - y mean\ny mean y s to formula this is the\nformula I'll try to explain you what\nthis formula definitely says okay so\nfirst thing first let's consider that\nthis is my this is my problem statement\nthat I'm trying to solve suppose these\nare my data points and if I try to\ncreate the best fit\nline This Yi hat Yi hat basically means\nthis specific point we are trying to\nfind out the difference between this\nthings difference between these things\nlet's say that these are my points I'm\ntrying to find out a difference between\nthis predicted this is my predicted the\npoint in green color are my predicted\npoints which I have denoted as y i hat\nand always understand this is what Su\nsum of residual is sum of residual is\nnothing but difference between this\npoint to this point this point to this\npoint this point to this point this\npoint to this point and I doing the all\nthe summation of those now the next\npoint which is very much important here\nis my X and Y what is this y IUS y y bar\nY Bar is nothing but mean mean of Y if I\ncalculate the mean of Y then I will\nprobably get a line which looks like\nthis I'll get a line something like this\nand then I will probably try to\ncalculate the distance between each and\nevery point and this specific point with\nrespect to the distance between this\npoint and this point the denominator\nwill definitely be high right this value\nobviously this value will be higher than\nthis value right the reason why it will\nbe higher because the mean of this\nparticular value distance will obviously\nbe higher so this 1 minus high this will\nbe a low value and this will be a high\nvalue when I try to divide Low by\nHigh Low by high then obviously this\nentire number will become a small number\nwhen this is a small number 1 minus\nsmall number will be a big number so\nthis basically shows that our R square\nhas fitted properly right it has\nbasically got a very good R square now\ntell me can I get this entire R square a\nnegative number let's say that in this\nparticular case I got 90% can I get this\nR square as negative number there will\nbe situation guys what if I create a\nbest fit line which looks like\nthis if I create this best fit line\nwhich looks like this then this value\nwill be quite High it is only possible\nwhen this value will be higher\nthan higher than this\nvalue okay but in the usual scenario it\nwill not happen because obviously we'll\ntry to fit a line which will be at least\ngood it's not just like pulling one line\nsomewhere we don't want to create a best\nfit line which is worse than this right\nworse than this so in this particular\nscenario you'll be saying that in R\nsquare now here you'll be able to see\none one amazing feature about R square\nis that let's say let's say one scenario\nsuppose I have features like let's say\nthat my feature is something like uh\nlet's say I have a price of a house okay\nso suppose this is my bedrooms how many\nbedrooms I have and this is basically\nthe price of the house now if I if I\nprobably solve this Pro problem I'll\ndefinitely get an R square value let's\nsay the R square value is 85% let's say\nthat my R square is 85% now what if if I\nadd one more feature the one more\nfeature basically says that okay if I\nadd\nlocation location of the house will be\ndefinitely correlated with price so\nthere is a definite chance that the R\nsquare value will increase let's say\nthat R square will become 90% if I\nprobably have this two specific feature\nand obviously it is basically increasing\nthe R square because this is also\ncorrelated to price\nand let me change the example see first\ncase I got by R square as 85% let's say\nnow as soon as I added location I got\n90% now let's say that I added one more\nfeature which gender is going to stay\ngender like male or female is going to\nstay you know that gender is no way\ncorrelated to price but even though I\nadd one feature there is a scenario that\nmy R square will still increase and it\nmay become\n91% even though my feature is not that\nimportant even gender is not that\nimportant the R square formula Works in\nsuch a way that if I keep on adding\nfeatures and that are not nowhere\ncorrelated this is obviously nowhere\ncorrelated this is not correlated with\nprice then also what it does is that it\nis basically increasing my r² so this\nspecific thing should not happen whether\na male will stay or female will stay\nthat does not matter at all still when\nyou do the calculation the R square will\nstill increase so in order to not impact\nthe model because see now right now with\nthis particular model where I have got\n90% now as soon as I see R square as 91%\nbecause it is considering this\nparticular gender so this model will be\npicked right because it is performing\nwell and is giving you a better R square\nvalue but this should not happen because\nthat is not at all corelated this model\nshould have been picked so in order to\nprevent this situation what we do we\nbasically Ally use something called as\nadjusted R square now what is this\nadjusted R square and how it will work\nI'll also show it to you very very nice\nconcept of adjusted R square so adjusted\nR square R square\nadjusted is given by the\nformula is given by the Formula 1 - 1 -\nr² * N - 1 where n is the total number\nof samples n minus P minus 1 this p p is\nnothing but number of features\nor predictors we'll also say or\npredictors suppose initially my number\nof predictors were in this particular\nscenario in this scenario where I saw\nthis my number of predictors was two and\nin this particular case my number of\npredictor was three now if my predictor\nis 2 I got the r squ as 90% so in this\nparticular scenario what all the\ncalculation will happen okay all the\ncalculation will happen and let's say\nthat my R square adjusted it'll be\nlittle bit less it'll be little bit less\nlet's say it8 is 6% let's say that my R\nsquare adjusted is 86% based on this\npredictor 2 now when I use my predictor\n3 predictor basically means number of\nfeatures that I'm going to use and now\nin this one one feature is nowhere\nrelated like gender but what we are\ngetting we are basically getting R\nsquare increased to\n91% now for the R square\nadjusted this will not increase this\nwill in turn decrease right now it will\nbecome 82% how it will become I'll show\nyou I've just considered some value 8682\nhere you can see that there is an\nincrease here an increase is there here\ndecrease is there now how this is\nbasically happening see this P value\nthat I will be putting okay if I put a p\nisal 3 obviously with n minus P minus 1\nthis will become a little bit smaller\nnumber or sorry little bit uh smaller\nnumber right so now in this particular\ncase if it is not correlated obviously\nthis will be high when I'm increasing\nthis so this will also be high let me\nwrite the equation something like this\njust a second so this will basically\nbe okay now why probably this value may\nhave decreased let me talk about this\none what is r squ I hope everybody\nunderstood n is the number of data\npoints p is the number of\npredictors if p is increasing then what\nwill happen as P keeps on increasing\nthis value will keep on\ndecreasing this value will keep on\ndecreasing if this values keep on\ndecreasing this will be a bigger number\nthis will obviously be a big number a\nbig number divided by a small number\nwhat it will be obviously this will be a\nlittle bit bigger number 1 minus bigger\nnumber we will basically get some values\nwhich will be decreasing if my P value\nis two in this particular case it will\nbe less smaller than this right at least\nit will be greater than this this\nparticular value right when p is equal\nto\n3 so with the help of P obviously R\nsquare is there to support you okay\nwhether it is correlated or not always\nremember when the features are highly\ncorrelated your R square value will\nincrease tremendously if it is less\ncorrelated then it will be there will be\na small increase but there will not be a\nvery huge increase now if I consider p\nis equal to 2 obviously when I'm trying\nto find out this uh calculation n minus\nP minus 1 it will obviously be greater\nthan p is equal to 3 when p is equal to\n3 then this value will be still more\nsmaller and when we are dividing a\nbigger number by a smaller number\nobviously we are subtracting with one so\nthat basically means even though my R\nsquare is 86 over here there may be a\nscenario since this is nowhere\ncorrelated I'm basically getting an 82%\nbecause of this entire equation so I\nhope you are understanding this this is\nvery much important to understand a very\nvery important property simple way to\ndefine is that as my P value keeps on\nincreasing the number of predictors\nkeeps on increasing my R squ gets\nadjusted whatever R square I'm getting\nwith respect to this it will always be\nless than this particular R square there\nwas one interview question that was\nasked one of my student between R square\nand adjusted R square which will always\nbe bigger definitely the student said R\nsquare then he told him to explain about\nadjusted R square why does that specific\nhappen agenda one is about Ridge lasso\nregression second is assumptions of\nlinear regression the third point that\nwe are probably going to discuss about\nis logistic regression then the fourth\nthing that we are going to discuss about\nis something called as confusion\nMatrix the fifth thing that we are going\nto consider about\nis practicals\nfor lead lineer Ridge lasso and logistic\nso first topic uh that we are probably\ngoing to discuss is something called as\nRidge and lasso\nregression so let's understand about\nRidge and lasso regression if you\nremember in our previous session what\nall things we discussed linear\nregression and then we had discussed\nabout the cost function we have\ndiscussed about R square adjusted\nadjusted R square sorry R square and\nadjusted R square we have discussed\nabout it gradient descent we have\ndiscussed about it it was nothing but 1\nby 2 m summation of I = 1 2 m h Theta of\nx i -\ny - y i s so this is the cost function\nthat we had discussed right yesterday\nand this cost function was able to give\nus a\ngradient descent with respect to the J\nof\ntheta J of theta Zer or Theta not so I\ncan also write this as J of theta comma\nTheta 0 comma Theta 1 now let me give\nyou a scenario let's say that I have a\nscenario over here and I have this\nspecific scenario let's say that I just\nhave two points which looks like this\nokay now if I have these two specific\npoints what will happen I will probably\ntry to create a best fit line the best\nfit line will definitely pass through\nall the points like this if I try to\ncalculate the cost function what will be\nthe value of J of theta 0 comma Theta 1\nlet's say that in this particular case\nsince it is passing through the origin\nmy Theta 0 will be zero okay so what\nwill be the value of theta 0 comma Theta\n1 so here obviously you can see that\nthere is no difference so it will\nobviously become zero Now understand\nthis data that you see right right this\ndata is basically called as training\ndata so this data that I have actually\nplotted with two points these are\nspecifically called as training\ndata now what is the problem in this\ndata right now see right now exactly\nwhatever line is basically getting\ncreated over here which is through the\nuh hypothesis over here you can see that\nit is passing through every point so\nthat is the reason your cost is zero and\nour main aim is to basically minimize\nthe cost function that is absolutely\nfine now in this particular case in\nwhich my model this if this model is\ngetting trained initially this data is\nbasically called as training data now\njust imagine that tomorrow new data\npoints comes so if my new data points\nare here let's consider that I I want to\nbasically uh come up with this new data\npoint now in this particular scenario if\nI want to predict with respect to this\nparticular Point let's say my predicted\npoint is here\nis this the difference between the\npredicted and the real Point quite\nhuge yes or no so this is basically\ncreating a condition which is called as\noverfitting that basically means even\nthough my\nmodel has given or trained well with the\ntraining\ndata or let me write it down properly\nover here so this condition since since\nyou can see that over here my each and\nevery point is basically passing through\nthe best fit line so because of that\nwhat happens it causes something called\nas\noverfitting so you really need to\nunderstand what is overfitting now what\ndoes overfitting mean overfitting\nbasically means my model performs well\nwith training data but it fails to\nperform well with test data now what is\nthe test data over here the test data is\nbasically this points the real test data\nanswer was this points but because the\nmy line is like this I'm actually\ngetting the predicted point over here so\nthis distance if I try to calculate it\nis quite huge so in this scenario\nwhenever I say my model performs well\nwith training data and it fails to\nperform well with test data then this\nscenario we say it as overfitting so\nthis scenario when the model performs\nwell with training data I have a\ncondition which is called as low bias\nand when it fails to perform with the\ntest data then it is basically called as\nhigh High variance very important okay I\nwill make each and everyone understand\none by one if it is performing well with\nthe training data that is basically low\nbias and whenever it performs well with\nthe test sorry fails to perform well\nwith the fails to perform well with the\ntest data then it is basically High\nvariance now similarly I may have\nanother scenario which is called as\nunderfitting so let's say that I have\nsomething called as\nunderfitting now in this underfitting\nwhat is the scenario the\nmodel fails to perform it gives bad\naccuracy I say that model always\nremember whenever I talk about bias then\nyou can understand that it is something\nrelated to the training data whenever I\ntalk about test data at that point of\ntime you talk about variance and that\nspecifically whenever you talk about\nvariance that basically means we are\ntalking about the test data so for an\noverfitting you will basically have low\nbias and high variance low bias with\nrespect to the training data and high\nvariance with respect to the test data\nnow if the model accuracy is bad with\ntraining data and the model accuracy is\nalso bad with test data in this scenario\nwe basically say it as underfitting so\nthese are the two conditions that are\nwith respect to underfitting that\nbasically means that both for the\ntraining data also the model is giving\nbad accuracy and again for the test data\nalso it is basically having a bad\naccuracy so in this particular scenario\nwe can definitely say two things out of\nunderfitting one is high bias and high\nvariance so this is the condition with\nrespect to underfitting very super\nimportant let me just explain you once\nagain suppose let's consider I have one\nmodel I have model two this is model one\nthis is model one this is model two and\nthis is model 3 okay guys so suppose\nlet's say that I have my model my\ntraining accuracy is let's say\n90% And my let's say that my test\naccuracy is 80% now in this particular\ncase let's say that my training accuracy\nis\n92% and my test accuracy is 91% and\nlet's say my model three is basically\nhaving training accuracy as\n70% and my test accuracy is 65% so if I\ntake this particular case it is\nbasically overfitting if I take this\nparticular thing this basically becomes\nmy generalized model and when I talk\nabout this this is my I'll just say that\nokay I'll also put nice color so that uh\nyou'll be able to understand this this\nbecomes our generalized model and this\nfinally becomes our underfitting right\nunder under fitting so here is my red\ncolor I will just say it as underfitting\nwhat are the main properties of this\noverfitting as I said in this scenario\nsince it is performing well with the\ntraining data so it will be low bias\nHigh variance in this particular case it\nwill be low bias low variance and this\nparticular case it will be high bias and\nhigh variance understand in this\nterminology in this particular way\nyou'll be able to understand so why do\nwe require always a generalized model\nbecause whenever our new data will\ndefinitely come generalized model will\nbe able to give us very good output\nlet's go back to this particular example\nhere you'll be able to see this straight\nline the red line that I have actually\ncreated is basically overfitting so that\nwhenever I probably get the new points\nwhich is having this real value and the\npredicted points here you'll be able to\nsee the difference is quite huge so\nbecause of this it will definitely be a\nscenario of overfitting where it has low\nbias and high weight\nso again let me go ahead and take this\nexample so this was my line which I have\nactually drawn I had two points and when\nI draw this line which was a best fit\nline to which is passing through both\nthe points this scenario is basically\ncausing a overfitting problem and I've\nalso shown you my J of theta 1 will be\nzero in this scenario since it is\npassing exactly and the predicted point\nis also over there now understand one\nthing is that what can can we take out\nfrom this what assumptions we can take\nout from this definitely if I talk about\nour cost function our cost function here\nis nothing but 1X 2 m summation of I = 1\n2 m h Theta of X of i - y of I whole s\nnow let's consider that I am going to\nuse this H Theta X and I'm going to\nbasically write it as y hat okay let's\nfocus on this specific point so when I\ntake this I'm I'm just going to focus on\nthis particular point so here I will\ndefinitely write it as y hat minus y of\nI whole squ so this is my y y hat of I\nminus y hat y i whole Square so this is\nnothing but the difference between the\npredicted value and the real value okay\nthis is what I'm actually trying to get\nnow in this scenario if I am adding this\nvalues obviously I'm going to get the\nvalue as zero now I have to make sure\nthat this value does not come to zero\nbecause this is still over fitting so\nthat is where your Ridge regression will\ncome into picture Ridge and lasso will\ncome into picture now when I use Ridge\nand lasso suppose if I use Ridge now in\nRidge what we say this this is also\ncalled as L2\nregularization now L2 regularization\nwhat it does is that it basically adds a\nunique\nparameter add a One More Sample value\nwhich is like Lambda multiplied by slope\nSquare now what is this slope whatever\nslope of this particular line it is we\nare just going to square it off now\nsuppose if I take my equation which\nlooks like this H Theta of X is equal to\nTheta 0 + Theta 1 x now in this\nparticular case my Theta 0 was zero so\nmy H Theta of X is nothing but Theta 1\nwhat is Theta 1 this is specifically\ncalled as slope and I am basically\ntaking this Theta 1 I'm actually making\nit as a square Square so always\nunderstand I don't want to make this as\nzero because if it becomes zero it may\nlead to overfitting condition now what\nwill happen if I add this particular\nequation if I add this particular\nequation this will obviously come as\nzero let's consider my Lambda value over\nhere my Lambda value is one I'll talk\nabout how do you set up Lambda value\nokay let's consider that I'm\ninitializing it to one let's say my\nLambda value is 1 now what I will do is\nthat this l Lambda value is 1 Let's\nconsider our slope value initially is\ntwo and because of this two I got this\nbest fit line I'm just going to consider\nit so if I do the total sum over here if\nI'm just considering this this value is\nthree now the cost function will not\nstop over here because still it has to\nminimize it has to reduce this three\nvalue so what it will do it will again\nchange the Theta 1 value and let's say\nthat my Theta van value has changed now\nit got another best fit line which looks\nsomething like like this this is my next\nbest fit line I'll talk about Lambda\nLambda is a hyper parameter guys what\nexactly is Lambda I'll just talk about\nit now when I basically change this line\nnow see why I'm getting this line let's\nconsider I have changed my Theta 1 value\nsince we need to minimize now when we\nneed to minimize what it will do we'll\nagain calculate the slope of this\nparticular line and then we will try to\ncreate a new line when we sorry it is\ntwo two not three just a second guys 0 +\n1 multiplied by 2 s which is nothing but\n4 so now my cost function will not stop\nover here so we are going to still\nreduce this now in order to reduce this\nagain Theta 1 value will get changed and\nthen we will get a next best fit line\nfor this point now what will happen in\nthis scenario once we have this best fit\nline we will definitely get a kind of\nsmall difference so now if I go ahead\nand consider the new equation my y hat I\nminus y\ni² + Lambda of slope squar this value\nwill be a small value now because I have\nsome difference and then plus again 1\nmultiplied by now understand whether the\nslope will increase in this particular\ncase or whether it will decrease in this\nparticular case there will be some slope\nvalue let's say that I have got some\nslope of this particular line in this\nparticular scenario again your slope\nwill definitely decrease so let's say in\nthe case of two initially it was now it\nis basically\n1.36 whole squ now this small Value\nPlus 1 + 1.3 squ or let me consider that\nmy slope is now one simple value that is\n5 so if I get this it is 2.25 2.25 plus\nsmall value it will be less than three\nonly right it will obviously be less\nthan three or equal to 3 but understand\nwhat is happening the value is getting\nreduced from 4 to 3 so this is is the\nimportance of Ridge now what will happen\nis that you will try to get a\ngeneralized model which has low bias and\nlow variance instead of this overfitting\ncondition you know why specifically we\nare adding Ridge L2 regularization it is\nbasically to prevent\noverfitting because here you are not\nstopping here you are trying to reduce\nit unless and until you get a line you\nget a line which will be able to handle\nthe which will be able to handle as a uh\ngeneralized model now here you can see\nnow if I have my new points like how I\ndrew over here now the distance will be\nless so now you'll be able to see that\nit will be able to create a generalized\nmodel guys this will be a small value\nonly see initially when we have this\nline obviously we have zero if we try to\nslightly move here and there so here\nyou'll be able to see that it will just\na slight movement but what this movement\nis basically specifying it is specifying\nthat the slope should not be steep if we\nprobably have a steep slope it obviously\nleads to most of the time overfitting\ncondition it should not be steep it\nshould be very very it should be less\nsteeper but it should actually help you\nto create a generalized model so you\nwill be seeing that after playing for\nsome amount of time this value will not\nreduce after some point of time it'll\nget almost it'll be a minimal value\nit'll be a smaller value and for this\nalso you have to specify iterations how\nmany times you probably have to train\nthem now this iterations is also a\nhyperparameter based on number of\niterations you will probably see your R\nsquare or adjusted R square over here so\nthis iterations based on the number of\niterations it will never become zero\nguys understand because zero it is not\npossible if it becomes zero trust me it\nis an overfitting model you cannot get\nthat is something zero now what is\nLambda coming to this Lambda this Lambda\nis a\nhyperparameter this is basically to\ncheck how fast you want to lessen the\nsteepness or how fast you want to make a\nsteepness grow higher right and this\nLambda will also be selected by using\nhyper parameter and this also I'll show\nyou today in Practical what do you mean\nby iterations iteration basically means\nhow many time I want to change the Theta\n1 value how many times you want to\nchange the Theta value that is the\nconvergence algorithm right\nconvergence algorithm over here L2\nregularization or Ridge is basically\nused in such a way that you should never\noverfit why we assume Theta 0 is equal\nto 0 because I'm considering that it\npasses through a origin right origin\nover here Lambda is a hyper\nparameter steep basically means how\nsteep the line is if I have this line\nthis line is quite steep if I have this\nline This is less steep now if I go to\nthe next regularization which is called\nas lasso raso R lasso regression this is\nalso called as L1\nregularization now here the formula will\nbe changing little bit here you will be\nhaving y hat of minus of Y whole Square\nhere you'll be adding a parameter Lambda\nbut understand here you'll not be adding\nslope Square no here you'll be adding\nmode of slope here you'll be adding mode\nof slope and this mode of slope will\nwork is that it will actually help you\nto do feature selection now you may be\nthinking how feature selection crash\nlet's consider a equation over here\nlet's say that I have many many features\nI have many many many features okay so\nmy H Theta of X which I'm indicating\nhere as y hat let's say that I'm I'm\nwriting this equation apart from\npreventing for overfitting it will also\nhelp you to do feature selection here\nlet me just show you over here with an\nexample this H Theta of X which I'm\nprobably writing as y hat will basically\nbe indicated by something over here\nyou'll be able to see that it is nothing\nbut let's say that I have multiple\nfeatures like this now in this\nparticular features obviously there are\nso many coefficients over here so many\nslopes over here now mod of slope will\nbe what it will be nothing but mod of X1\nplus X2 plus X3 plus X4 plus X5 like\nthis up to xn now in this particular\ncase how it is basically helping you to\nsorry not X1 sorry just a second this\nmod of I have taken the data point this\nis not data points this should be your\nmod of theta 0 + Theta 1 + Theta 2 +\ntheta 3 + Theta 4 + Theta 5 like this up\nto Theta n so here you'll be able to see\nthat this is how I will basically uh\nI'll basically be calculating the slope\nnow as we go ahead guys whichever\nfeatures are probably not playing an\namazing role the Theta value the\ncoefficient value the slope value will\nbe very very small it is just like that\nentire feature is neglected that entire\nfeature is neglected now in this\nparticular case we were doing squaring\nbecause of the squaring that value was\nalso increasing but here because of the\nmode that value will not increase\ninstead it will be a condition wherein\nwe are basically neglecting those\nfeatures that are not at all important\nin this specific problem statement so\nwith the help of L1 regularization that\nis lasso you are able to do two\nimportant things one is preventing\noverfitting and the second case is that\nif you have many features and many of\nthe features are not that important okay\nin basically finding out your slope or\nyour line or the best fit line in that\nparticular case it will also help you to\nperform feature selection so this is the\nimportance of the entire what is the\nimportance of this this is the\nimportance of the uh Ridge and the lasso\nregression that we are doing here I'm\njust going to write L1\nregularization and obviously we have\ndiscussed about L2 regularization also\nnow you have probably understood Lambda\nis one hyperparameter okay which we will\nspecifically using okay and based on\nthis Lambda this will be found out\nthrough cross\nvalidation cross validation is a\ntechnique wherein we will try to\nprobably train our model and try to find\nout the specific things okay what should\nbe the exact value and there also we\nplay with multiple values in short what\nwe are doing we just trying to reduce\nthe cost function in such a way that uh\nit will definitely never become zero but\nit will basically reduce based on the\nLambda and the slope value in most of\nthe scenario if you ask me we should\ndefinitely try both the regularization\nand see that wherever the performance\nMatrix is good we should use that what\nis cross validation basically means I\nwill try to use different different\nLambda value and basically Ally use it\nso in a short let me write it down again\nfor Ridge regression which is an L2 Norm\nhere I'm simply writing my cost function\nin this particular case will be little\nbit different here I can definitely\nwrite my cost function as H Theta X of i\n- y of I S Plus Lambda multiplied slope\nSquare what is the purpose of this the\npurpose is very simple here we are\npreventing overfitting this was with\nrespect to the Ridge Recreation that is\nL2 nor now if I go ahead and discuss\nabout the next one which is called as\nlasso regression which is also called as\nL1 regularization in the case of lasso\nregression your cost function will be H\nTheta of X of\nIUS y of\ni² plus Lambda ultied mode of flow so\nhere you have this specific thing and\nwhat is the purpose the purpose are two\none is prevent overfitting and the\nsecond one is something called as\nfeature selection so these two are the\noutcomes of the entire thing see with\nrespect to this lasso right you have\nslopes slopes here you'll be having\nTheta 0 plus Theta 1 plus Theta 2 plus\ntheta 3 like this up to Theta n now when\nyou'll have this many number of thetas\nwhen you have many number of features\nand when you have many number of\nfeatures that basically means you'll\nhave multiple slopes right those\nfeatures that are not performing well or\nthat has no contribution in finding out\nyour output that coefficient value will\nbe almost nil right it will be very much\nnear to zero in short you neglecting\nthat value by using modulus you're not\nsquaring them up you're not increasing\nthose values now I will continue and uh\nprobably I will also discuss about the\nassumptions of linear regressions so\nwhat are the assumptions of linear\nregression in this particular scenario\nso assumption is that number one point\nlinear regression if our features are in\nnormal or gion\ndistribution if our features follows\nthis particular distribution it is\nobviously good our model will get\ntrained well so there is one concept\nwhich is called as feature\ntransformation now in future\ntransformation always understand what\nwill happen if a model does not fall\nfollow a gan distribution then we apply\nsome kind of mathematical equation onto\nthe data and try to convert them into\nnormal orian distribution the second\nassumption that I would definitely like\nto make is that standard scalar or\nstandard digestion standard dig is\nnothing but it is a kind of scaling your\ndata by using Z score I hope everybody\nremembers Z score this is what we\nbasically apply there your mean is equal\nto zero and standard deviation equal to\n1 see guys wherever you have gradient\ndescent involved it is good to basically\ndo\nstandardization because if our initial\npoint is a small Point somewhere here\nthen to reach the global Minima or\ntraining will happen quickly otherwise\nwhat will happen if your values are\nquite huge then your graph may be very\nbig and the point can come over any over\nthere and the third point is that this\nlinear regression works with respect to\nlinearity it works if your data is\nlinearly separable\nI'll not say linearly separable but this\nlinearity will come into picture if your\ndata is too much linear it will\nobviously be able to give a very good\nanswer like logistic regression also\nwhich we are going to discuss today this\nalso has the same property now you may\nbe asking is it compulsory to do\nstandardization guys if you want to\nincrease the training time of your model\nor if you want to optimize your model I\nwould suggest go ahead and do\nstandardization now coming to the fourth\nPoint here you really need to check\nabout multicolinearity\nthis is also one kind of check we\nbasically do what is multicol\nlinearities let's say I have X1 I have\nX2 and this is my output feature I have\nlet's say X3 also now let's say that if\nI try to see the colinearity of this two\nfeature how how correlated these two\nfeature are let's say that these two\nfeature are 95% correlated is it is it a\nwise decision to use both the features\nand let's say that let's let's say that\nthese two features are 95% correlated\nbut it is highly correlated with Y is it\nnecessary that we should use both the\nfeature in this particular scenario the\nanswer should be no we can drop this\nparticular feature okay we can drop this\nparticular feature any one of the\nfeature we can definitely drop it and\nbased on that I can just use one single\nfeature and basically we do the\nprediction there is also a concept which\nis called as variation inflation factor\nI will try to make a dedicated video\nabout this multical is also solved with\nthe help of variation inflation Factor\none more term is there homos orc so that\nkind of terminologies also we use one\nmore condition in this but if you almost\nsatisfied with this assumptions you will\ndefinitely be able to outperform in\nlinear regression so you have got an\nidea of the assumptions you have also\ngot an idea of multiple things okay now\nlet's go towards something called as\nlogistic regression now logistic\nregression what logistic regression is\nthe first type of algorithm that we are\ngoing to learn in classification let's\nsay that in classification I have one\nexample you know so suppose I have say\nnumber of hours study hours and number\nof play hours based on this I want to\npredict whether a child is passing or\nfailing suppose these two are my\nfeatures I want to predict whether it is\npass or fail so here you'll be able to\nsee that I have some fixed number of\ncategories specifically in this\nparticular scenario I have two\ncategories binary logistic regression\nworks very well with binary\nclassification now the uh question comes\nthat can we solve multiclass\nclassification using logistic the answer\nis simply yes you can definitely do it\nso let's go ahead and let's try to\ndiscuss about uh logistic regression now\nwhat is the main purpose of the logistic\nregression first of all let's let's uh\nunderstand one scenario okay suppose I\nhave a feature which basically says um\nnumber of study hours and this is like 1\n2 3 4 5 6 7 and let's say that I have\npass this point is basically pass and\nthis point is basically\nfail so I have this two conditions these\nare my outcomes now what I'll do I will\njust try to make some data points let's\nsay that if I study Less Than 3 hours I\nwill probably be fail if I study more\nthan 3 hours then probably I will pass\nthis I'll make it as fail and this I\nwill make it as pass so I will be having\npoints over here this 1 2 3 let's say\nthat this is my training data set now\nthe first question says that okay Chris\nfine you have some data over here\nwhenever it is less than three you are\nbasically the person is failing if it is\ngreater than five greater than three it\nis basically showing data points points\nwith respect to pass now can't we solve\nthis problem first with linear\nregression now with the help of linear\nregression here the first point will be\nthat yes I can definitely draw a best\nfit line my best fit line in this\nparticular scenario may be something\nlike this it may it may look something\nlike this so here fail is nothing but\nzero pass is one the middle point is\nbasically 0.5 so obviously with the help\nof linear\nregression I'm able to create this best\nfit line and I'll put a scenario that\nwhenever the value is less\nthan5 whenever the value is less than\n0.5 whenever the output is less than5\nlet's say that new data point is this\nand based on this I'll try to do the\nprediction I'm actually able to get the\noutput over here now when I'm getting\nthe output over here this basically is\n0.25 now in this particular scenario\nobviously I'm able to say that yes the\nperson I'll write a condition over here\nsaying that if my H Theta of x value is\nless than 0.5 then my output should be\nzero let's say less than 0.5 I'll say\nnot less than or equal to less than5\nthen my output will be zero right so in\nthis particular case Zero basically\nmeans fail similarly I'll have a\nscenario where I'll say that when if my\nS of theta of X is greater than or equal\nto 5 then this will basically be one\nwhich is nothing but pass so this two\ncondition I can definitely write over\nhere this is my center point so that any\npoint that will probably come over here\nlet's say that this point is coming over\nhere right let's say new data point is\nsomewhere coming over here with this red\npoint\nnow what I'll do I'll basically draw a\nstraight line it will come over here I\nwill just extend this line\nlong I will extend this line over here\nand I will extend this line over here\nand here you can see that based on this\nI'm actually getting this particular\nprediction which is greater than 0.5 so\nI will say that okay the person has\npassed obviously this is fine this is\nobviously working better this is\nobviously working better so what what is\nthe problem why we are not using linear\nregression okay in order to solve this\nparticular problem why you are\nspecifically having logistic regression\nthe answer is very much simple guys the\nanswer is that whenever let's say that\nif I have an outlier which looks\nsomething like this suppose I have an\noutlier which comes like this over here\nwhat is this value let's say that this\nvalue is nothing but 7 8 9 10 let's say\nthat the number of study hours and I'm\nstudying for nine it is obviously pass\nnow in this particular scenario when I\nhave an outlier this entire line will\nchange now I will probably get my line\nwhich looks something like this okay my\nline will basically move something like\nthis it will now get moved something\nlike this now when it gets moves\ncompletely like this now for even five\nor even at any point that I am actually\npredicting let's say that at this\nparticular point if I try to find out\nit'll be showing less than 0. five so\nhere this particular value or answer\nwill be wrong right because if we are\nstudying more than 5 hours OB viously B\nbased on the previous line the person\nhad to pass but in this scenario it is\nfailing it is coming less than 0.5 but\nthe real value for this is basically\npassed so I hope you are understanding\nbecause of the outlier the entire line\nis getting changed so how do we fix this\nparticular problem now in this two\nscenarios are there first of all\nobviously because of just an outlier\nyour entire line is getting shifted here\nand there the second point is that over\nhere sometimes you're also getting\ngreater than one you you're also getting\nless than one suppose if I try to\ncalculate for this particular point if I\nproject it in behind I'll be getting\nsome negative value so we have to squash\nthis function if I squash this function\nthen it'll become a plain line right how\ndo we squash it and for this we use\nsomething called as sigmoid activation\nfunction or sigmoid function if somebody\nask you why don't you use linear\nregession in order to solve this\nclassification problem then your answer\nshould be very much simple you should\nsay this to specific points so we will\ntry to go ahead and solve some linear\nregression now with the help of cost\nfunction everything as such and we'll\ntry to understand how the cost function\nwill look for logistic regression second\nreason I told you right it is greater\nthan zero over here the line is going\ngreater than zero right greater than\nzero I have only Z and one and it is\nbecoming greater than zero but I have\nalready told that our maximum and\nminimum value are 1 and zero so I hope\nyou have understood why linear Reg\ncannot be used okay I showed you all the\nscenarios why linear regression should\nnot be used now we'll continue and\nprobably discuss about the other things\nover here and uh we will now try to\nunderstand fine what exactly logistic\nregression is all about and how the\ndecision boundaries basically created\nnow we'll go ahead and discuss about\nthat specific thing so let's go ahead\nour values should be always between 0 to\none over here in this particular case\nbecause it is a binary classification\nproblem only this should be the answer\nso let's go ahead and let's define our\ndecision boundary so my decision\nboundary decision boundary in the case\nof logistic regression first of all as\nusual in logistic regression we defined\nour hypothesis okay guys first of all\nlet's see if I'm writing my my h of\ntheta my H Theta of X as Theta 0 + Theta\n1 into x + Theta 2 into X like this X1\nX2 + Theta n into xn\nnow in this scenario can I write this\nentire equation as Theta transpose X\nobviously I can definitely write this\nway right and this is what is the\nnotation that you will probably seeing\nin many places so with respect to the\ndecision boundary of logistic regression\nour Theta see like this we can write I'm\nsaying okay but since we have to\nconsider two things one is squashing the\nline okay how that squashing will\nbasically happen see if I have this if I\nhave this line\nwe saw in the above right if I have this\nline suppose I have some data points\nover here and I have some data points\nover here if I want to create the best\nfit line how will I create I will\nbasically create like this but I have to\nalso do two things one is squash over\nhere and squash over here right squash\nover here and squash over here now in\norder to squash I'm saying squash squash\nmeans\nokay now in order to do this I use a\nfunction which is called as sigmoid\nactivation function\nthat basically means what happens\nobviously you know this line is\nbasically denoted by H Theta of x equal\nto how do you denote this straight line\nlet me write it down nicely for you so\nhow do you denote this straight line the\nstraight line is obviously denoted by\nTheta 0 + Theta 1 * X1 let's say now on\ntop of this on top of this I have to\napply something on top of this value I\nhave to apply something so that I can\nmake this line straight instead of just\nexpanding in this way so my hypothesis\nwill basically be now G of G is\nbasically a function on Theta 0 and\nTheta 1 * X1 so here I'm trying to\nbasically what I'm trying to do I will\napply a mathematical formula on top of\nthis linear regression to squash this\nline now let's go ahead and let's try to\nfind out what is this G okay what is\nthis G I will say let Z equal to Theta 0\n+ Theta 1 * X I'm just initializing this\nnow my H Theta of X is nothing but G of\nZ now we need to understand what is this\nz g of Z and how do we basically specify\nwhat is the G function so my G function\nis nothing but H Theta of x equal to 1\nby 1 + e ^ of minus Z which in short if\nI try to initialize Zed now it is 1 ^ of\ne ^ of minus Theta 0 + Theta 1 * X so\nthis is what is my H Theta of X which is\nmy hypothesis and this obviously works\nwell because it is being able to squash\nthe function so this is basically my\nhypothesis which I am definitely trying\nto use it and this function that you are\nactually able to see is called as\nsigmoid or logistic function now you\nneed to understand what does this\nsigmoid function look like in graph in\ngraph it looks something like this so\nthis this is my Zed value and this is my\nG of Z this is my 05 your sigmoid\nfunction will have this curve so this is\nyour one this is zero your value when\nnow from this we can make a lot of\nassumptions what are the assumptions\nthat we can basically make your G of Zed\nyour G of Zed is greater than or equal\nto\n5.5 is obviously greater than or equal\nto 0.5 when your Zed value is greater\nthan or equal to zero this is the major\nassumptions that we can basically make\nthat is whenever your G of Z is greater\nthan your G of Z is greater than or\nequal to 0.5 whenever your Zed is\ngreater than or equal to Z so obviously\nwhenever your Zed value is greater than\nZ it is greater than 0.5 if your Zed\nvalue is less than zero what it will\nbecome it will basically be less than\n0.5 so you can write that specific\ncondition also you want so this is the\nmost important condition\nover here why it is called as logistic\nregression see guys with the help of\nregression you creating this straight\nline and with the help of the concept of\nsigmo you are able to squash it so they\nhave probably combined that name and uh\nbasically have written in this way will\nsquashing of the best fit L line help to\novercome the outlier issues yes\nobviously it'll be able to help you so\nlet's go ahead and let's try to solve\nthe problem statement now usually let's\nconsider my training set let's consider\nmy training set suppose I have some\ntraining points like this x of 1 comma y\nof 1\nlet's say x of 2A y of 2 okay X of 3A y\nof 3 like this I have lot of training\npoints and finally X of n comma y of n\nlet's say that this is my training data\nso here uh my y y will belong to what\nzero or 1 because I will only have two\noutputs since we are solving a binary\nclassification problem here is my\ntraining set with two outputs and I hope\neverybody knows about J Theta of Z\nit is nothing but 1 + e ^ of minus Z\nhere your Z is nothing but Theta 0 +\nTheta 1 * X1 so this is your Theta 0 now\nwhat we have to do we have to select\nthis Theta now in this particular case\nlet's consider that my Theta 0 is 0\nbecause it is passing through the origin\njust for time pass sake suppose my Z is\nTheta 1 into X so now I need to change\nwhat is my parameter my parameter is\nTheta 1\nI have to change parameter Theta 1 in\nsuch a way that I get the best fit line\nand along that I apply this sigmoid\nactivation function now let's go ahead\nand let's first of all Define our cost\nfunction because for this we definitely\nrequire our cost\nfunction now everything will be same\nobviously you know the cost function of\nlinear regression because the first best\nfit line that you are probably creating\nis with the help of linear\nregression now in this particular case\nin the case of linear regression so here\nyou can basically write J J of theta 1\nis nothing but 1 by m summation of I = 1\n2 m 1X 2 and here you have H Theta of x\nminus y of I I whole Square so this is\nyour entire thing of if you remember\nlinear regression whatever things we\nhave discussed yesterday okay so this is\nthe cost function let's consider that\nfor linear regression for this is for\nthe linear regression now for the\nlogistic regression what will happen for\nyour logistic regression I will take the\nsame cost function H Theta of X now you\nknow what is s Theta of X it is nothing\nbut 1 + 1 + e ^ of minus Theta 0 + Theta\nsorry Theta 1 multiplied by X right this\nis my with respect to logistic\nregression this is my entire equation\nnow similarly I will try to only put\nthis H Theta of X let's consider that\nthis is my cost function only only my H\nTheta of X is changing in this\nparticular case so if I go ahead and\nwrite my cost function I can basically\nsay 1x2 h Theta of X of i - y of\ni² and in this particular scenario what\nis h Theta of X it is nothing but 1 + 1\n+ e ^ minus Theta 1 x so this is what\nthis is getting replaced and this is my\nlogistic regression cost function I'm\njust considering this cost function part\nthis part later on if you replace this\nto this see if I replace this to this\nand if I replace this to this it becomes\na logistic regression cost function\nintercept I'm considering it as zero\nguys now when I'm replacing this to this\nthis to this then it becomes a logistic\nuh regression cost function but there is\none problem we cannot we cannot use we\ncannot use this cost function there is a\nreason for this because this equation\nthat you're seeing 1/ 1 + e^ of minus\nTheta 1 * X this is a non-convex\nfunction now you may be considering what\nis a non-convex function so let me write\nit down so here this this term this\nterminology right it is a non-convex\nfunction now what is this non-convex\nfunction let me show you and let me\ndifferentiate it with convex function\nokay we'll try to understand what is the\ndifference between non-convex function\nand convex function this is related to\ngradient descent very important this is\nrelated to gradient desent if you\nremember with the help of linear\nregression whatever gradient Dent we are\nactually getting it is a convex function\nlike this this is the convex function\nwhich looks like a parabola curve\nParabola curve because of this Parabola\ncurve whenever we use this linear\nregression cost function specifically\nbecause here my H Theta of X is what it\nis nothing but Theta 0 + Theta 1 into X\nbecause of this this equ\nwill always give you a parabola curve\nthis kind of cost function or convex\nfunction you can say but here your s\nTheta of X is changing so in the case of\nif I use that cost function you will be\ngetting some curves which looks like\nthis now what is the problem with this\ncurve here you have lot of local Minima\nif local Minima is there you will never\nreach This Global Minima so that is the\nreason we cannot use that c function now\nmathematically you can also go and\nprobably search in the Google what is\nthe\nwhat is the graph or what is a convex or\nnon-convex function but always remember\nwhenever we updates Theta 1 with this\nwithin this particular equation by\nfinding the slope then this way it will\nnot be differentiable and here you have\nlot of local Minima and because of this\nlocal Minima you will never be able to\nreach the global Minima this is your\nGlobal Minima right in case\nof in case of linear regression you'll\nreach This Global Minima but in this\ncase you will never reach never never\nyou'll be stuck over here or you may get\nstuck over here you may get stuck over\nhere okay so this has a local Minima\nproblem so how do we solve this\nunderstand in local Minima these are my\npoints right I have to come over here\nthis is my deepest point in this\nparticular case I don't have any local\nMinima now in local Minima also you'll\nget slope is equal to Z so that is the\nreason your Theta 1 will never get\nupdated so in order to solve this\nproblem you can see this diagram we have\nsomething called as logistic regression\ncost function so I can now write my\nlogistic regression cost function in a\ndifferent way so this researcher\nresearcher thought of it and basically\ncame up with this proposal that the\nlogistic cost function should look\nsomething like this so the entire cost\nfunction of logistic regression that is\nspecifically H Theta of X of I comma y\nthis should be written something like\nthis and it should be written like this\nsee here I'm just going to write cost\nfunction of J of theta 1 let's say that\nI'm writing J of theta 1 okay so J of\ntheta 1 what are the different different\noutput that I'll be getting I'll be get\nI'll be getting yal 1 or y equal to 0 So\nbased on this two scenarios our cost\nfunction will look something like this\nminus log of H of theta of X and I know\nI hope you all know what is h Theta of x\nh Theta of X is nothing but 1 + 1 ^ of -\nTheta 1 x so this is what is my H Theta\nof X and whenever Y is Zer then you\nbasically have minus log * 1 - H Theta\nof X of I of I okay so this is how you\nbasically write your cost function in\nthis particular scenario now with the\nhelp of this cost function it is always\npossible since it is getting log log is\nbasically getting used in this scenario\nyou'll always get a global Minima that\nis the reason why they have completely\nneglected this cost function and utiliz\nthis cost function now what does this\ncost function basically mean two\nscenarios if Y is equal to 1 Let's\nconsider this is my cost function\ngraph I have H Theta of X and you know\nthat H Theta of x value will be ranging\nbetween 0 to 1 since it is a\nclassification problem so it will be\nranging between 0 to 1 and this is\nbasically of J of theta 1 which is my\ncost function so if Y is equal to 1 this\nspecific equation will be used and\nwhenever this equation is is basically\nused you get a you get a curve see minus\nlog s of X of I you get a curve which\nlooks something like this okay which\nyou'll get a curve which looks like this\nnow what does this curve basically\nspecify the curve come up with two\nassumptions the cost will be zero if Y\nis = 1 and H Theta of x equal to 1 that\nbasically when your s Theta of X is 1\nand the Y is output is one that\nbasically means you're going to assign\nover here one right so in this\nparticular case you will be seeing that\nyour cost function will be zero cost is\nzero so here is my zero it is meeting\nover here if you of x equal to 1 and Y\nis equal to 1 so this is this is again a\nconvex function only then the next point\nthat you can probably discuss over here\nis with respect to Y is equal to 0 if\nyour Y is Z then what kind of curve you\nwill be getting you'll get a different\nkind of curve which will look like this\nH Theta of x here your value will be 0\nto one and here you'll be having a curve\nwhich looks like this so when you\ncombine this two you'll be able to see\nthat you are able to get a kind of\ngradient descent so this will definitely\nhelp us to create a cost function so I\nhope everybody is able to understand\ntill here with respect to this and this\nwill definitely work so finally I can\nalso write my cost function in a\ndifferent way the cost function that I\nwill probably write over here so this\nwill be my J of theta 1\nso I can come up with a cost function\nwhich looks like this\ncost of H of theta of X of I comma Yus\nlog of H Theta of x if Y is equal\n1 and then minus\nlog 1 - H Theta of x if Y is equal\n0 now I can combine this both and\nprobably write something like like this\nI can combine this both and I can\nbasically write cost of H Theta of X of\nIA Y is equal to - y log H Theta of X of\nI minus log 1 -\ny okay 1 - y log of 1 - H Theta of X so\nthis will be my final cost\nfunction and here also you can see that\nif I\nreplace if I replace y with one then\nwhat will remain only this particular\nvalue will remain right this value when\nY is equal to 1 this thing only will\ncome you see over here replace y with\none probably replace y with one and then\nyou'll be able to see so here I can now\nwrite if Y is equal to 1 my cost\nfunction will Rook something like this\nwhich is nothing\nbut see Y is 1 then what will happen my\nlog of H Theta of X of I will come and\nthis 1 - 1 is 0 so 0 multili by anything\nwill be 0 if Y is equal to 0 then what\nwill happen my cost function will be so\nwhen it is zero this will - y will\nbecome 0 0 multili by anything is z so\nhere you'll be able to see that I am\nI'll be having minus log 1 - H Theta of\nx i so this both the condition has been\nproved by this cost function\nso this is my cost function yes cost\nfunction and loss function with respect\nto the number of parameters will be\nalmost same so finally if I try to write\nJ of theta because I have that 1X 2 m\nalso right so 1X 2 m also I have so what\nI'm actually going to do here you will\nbe able to see that I can write J of\ntheta 1 is equal to 1 by 2 m summation\nof IAL 1 to M and then write down the\nentire equation that you have probably\nover here so here you have minus y or I\nI'll just remove this minus and put it\nover here and this will become plus\nsorry y of I\n* log H Theta of X of I 1 - y of i y\nlog 1 - H Theta of X of I so this\nbecomes my entire first function and\nobviously you know what is h thet of x H\nTheta of X of I is nothing but 1 + 1 e^\nminus Theta 1 * X and finally my\nconvergence algorithm I have to repeat\nthis to update Theta 1 repeat until this\nupdation that is Theta Theta\nJ is equal to Theta J minus learning\nrate derivative with respect to Theta J\nand this will be my J of theta 1 this is\nmy repeat until conversion so this is my\ncost function this is my repeat\nalgorithm and here I will be updating my\nentire Theta\n1 and this solves your problem with\nrespect to logistic regression simple\nsimple questions may come like how it is\ndifferent from linear regression how it\nis not different from linear regression\ncan we say log likelihood a topic from\nprobabilistic yes this is uh this is log\nlikelihood if now I will discuss about\nperformance metrics and this is specific\nto classification problem and binary\nclassification I'm talking let's\nconsider let's consider I have a data\nset which has X1 X2 and this is y and\nobviously in logistic uh classification\nyou have outputs like 0 1 0 1 1 0 1 and\nyour y hat y hat is basically the output\nof the predicted model now in this\nparticular scenario my y hat will\nprobably be 1 1 0 uh 1 1 1 Z so in this\nparticular scenario this is my predicted\noutput and this is my actual output so\ncan we come to some kind of conclusions\nwherein probably we will be able to\nidentify what may be the accuracy of\nthis specific model with respect to this\nmany data points because confusion\nMatrix is all dealt with this is called\nas we will first of all have to create a\nconfusion Matrix now for a binary\nclassification problem the confusion\nMatrix will look like this so here you\nhave 1 0 1 0 Let's say that this is\nprediction let's say that these are my\nactual value and these are my prediction\nvalue okay these both are prediction\nvalue these are my output value when my\nactual value is zero my predicted value\nis one does this what does this mean\nwrong prediction right so when my actual\nvalue is zero my predicted value is 1 so\nhere my count will increase to one let's\ngo to the second scenario when the\nactual value is one and my predicted\nvalue is one that basically means one\nand one so here I'm going to increase my\ncount similarly when my actual value is\nzero my predicted value is zero so that\nbasically mean when my actual value is z\nmy predicted value is zero I'm going to\nincrease the count by one if I go over\nhere 1 one again it is so instead of\nwriting one now this will become two I'm\ngoing to increase the count similarly\nI'll go over here one more one is there\nso I'm going to increase the count three\nthen I have 01 01 basically means when\nmy actual value is zero I'm actually\ngetting it as one so I'm also going to\nincrease this particular value as two\nand then finally I have 1 and zero where\nI'm going to increase like this now what\ndoes this basically mean now what does\nthis basically mean see with respect to\nthis kind of predictions whenever we are\ndiscussing this basically basically says\nso this is my actual values and I have Z\n1 and zero and this is my predicted\nvalues I also have 1 and zero this value\nwhen one and one are there this is\ncalled as true positive this value when\n0 and Zer are there this is called as\nfalse negative whenever your actual\nvalue is zero and you have predicted one\nthis becomes false positive and whenever\nyour actual value is one you have\npredicted zero this becomes false\nnegative now coming to this I really\nneed to find out the accuracy of this\nmodel now if I really want to find out\nand this is what is called as confusion\nMatrix now in this confusion Matrix if I\nreally want to find out the accuracy the\naccuracy of this model it is very much\nsimple this middle elements that you are\nable to see will basically give us the\nright output so this and this if I add\nit up it will give us the right output\nso here I'm going to get TP + TN divided\nby TP + FP + FN + TN so once I calculate\nthis so I have 3 + 1\n/ 3 + 2 + 1 + 1 so this is nothing but 4\nby 7 what is 4 by\n757 so am I getting 57 percentage\naccuracy so I'm actually getting 57%\naccuracy over here with respect to the\naccuracy so this is how we basically\ncalculate with respect to basic accuracy\nwith the help of uh the confusion Matrix\nokay so this is specifically called as\nconfusion Matrix now there are some more\nthings that you really need to specify\nalways remember our model aim should be\nthat we should try to reduce false\npositive and false negative now let's\nsay that I want to discuss about two\ntopics what one is suppose in our data\nset I have zeros and one category let's\nsay in my output if I say Zer are 900\nand ones are 100 this becomes an\nimbalanced data very clear right so this\nbecome an imbalanced data set it is a\nbiased data suppose if I say zeros are\nprobab\n600 and ones are probably 400 in this\nparticular scenario I will say that this\nis the balance data because yes you have\n100 less but it's okay the it may not\nimpact many of the algorithm now see\nguys most of the algorithm that we will\nbe probably discussing imbalanced if we\nhave an imbalanced data set it will\nobviously affect the algorithms let me\ntalk about this let's say that I have\nnumber of zeros as 900 and number of\nones is 100 now let's say that my model\nI have created which will directly\npredict\nzero it'll I'll just say that all my\ninputs that it is probably getting with\nrespect to this training data it'll just\noutput zero now in this particular\nscenario what will be my accuracy my\naccuracy will be 900 divid by 1,000\nright so this is nothing but 90% so is\nthis a good\naccuracy obviously it is a good accuracy\nbut this is a biased data if my model is\nbasically just outputting 00000000 0 if\nit is outputting 00 00 0 obviously most\nof the answer will be zeros but this\nwill be a scenario like you know where\nit is just outputting one thing then\nalso it is able to get 90% accuracy so\nyou should only not be dependent on\naccuracy so there are lot of\nterminologies that we will basically use\none terminology that we specifically use\nis something called as Precision then\nwe'll also use recall what is precision\nwhat is recall I'll write the formula\nover here in Precision what do we need\nto focus and then finally we will\ndiscuss about f score so we have to use\ndifferent kind of parametrics of sorry\ndifferent kind of formulas whenever you\nhave an imbalanced data set you can also\ndo oversampling but again understand in\nmost of the scenarios in some of the\nscenarios oversampling may work but we\nhave to focus on the type of performance\nmetrics that we are focusing on right\nnow I'll not say F1 score I'll say F\nscore the reason why I'm saying I'll\njust let you know so let's talk about\nrecall recall formula is basically given\nby true positive divided by true\npositive plus false negative\nPrecision is given by true positive\ndivided by true positive plus false\npositive and then I will probably\ndiscuss about F sore also or we\nbasically say fbaa also now I'll just\ndraw this confusion Matrix again okay\nwhich is having true positive true\nnegative so let me draw it over here so\nthis is my ones and zeros these are my\nactual values and these are my predicted\nvalues I have true positive I have true\nnegative false positive and false\nnegative now in this particular scenario\nwhen I'm actually discussing understand\nwhat is recall and what focus it is\nbasically given on so here whenever I\ntalk about recall recall basically says\nthat TP TP divided by TP plus FN so I'm\nactually focusing on this so what does\nthis basically say true uh recall out of\nall the actual true positives how many\nhave been predicted correctly that is\nbasically mentioned by TP out of all the\npositive values how many of them have\npredicted as positive so this is what it\nis basically saying and this scenario is\ncalled as recall in this the false\nnegative is basically given more\npriority and our focus should be that we\nshould try to reduce false positive\nfalse negative sorry we should try to\nreduce this now let's go ahead and let's\ndiscuss about Precision in Precision\nwhat we are doing we are basically\ntaking out of all the predicted values\nout of all the predicted positive values\nhow many of them are actual true or\npositive okay this is what Precision\nbasically means now suppose if I\nconsider spam classification suppose\nthis is my task tell me in this\nparticular case should we use Precision\nor recall and one more use case I'm\nsaying that whether the person has\ncancer or not in which case we have to\nsupport recall and in which case we have\nto go ahead with Precision has cancer or\nnot in spam what is important okay guys\nthe recall is also called as true\npositive rate I can also say recall as\nsensitivity so if I go with Spam\nclassification it should definitely go\nwith Precision why it should go with\nPrecision if I probably get a Spam ma\nthe main aim should be that whenever I\nget a Spam Mill it should be identified\nas spam okay in that specific scenario\nmy positive false positive we should try\nto reduce and in this scenario my false\npository talks about the spam\nclassification a lot in a better way in\nthe case of cancer I should definitely\nuse recall let's let's focus on the\nrecall formula tp/ by TP plus FN if a\nperson has a cancer see one actually he\nhas a cancer it should be predicted as\none otherwise if we have FN it is\nbasically predicting it does not have a\ncancer that is really a big situation in\nthis case if a person does not have a\nCancer and if he's predict if the model\npredicts okay fine he has a cancer he\nmay go and further do the test and then\nhe'll come to know whether he has a\ncancer or not but this scenario is very\ndangerous if a person has a cancer but\nhe is being indicated that he does not\nhave that cancer\nso here false negative is given more\npriority over here in the case of spam\nclassification false positive is given\nmore priority so this is something\nimportant over here and you really need\nto understand with respect to different\ndifferent problem statement let me give\nyou one more example tomorrow the stock\nmarket is going to crash in this what we\nneed to focus on should we focus on\nPrecision or should we focus on recall\nnow here two things are there who is\nsolving what kind of problem see many\npeople will say recall or Precision but\nhere two things are there on whose point\nof view you are creating this model are\nyou creating this model for the industry\nor are you creating this model for the\npeople for the people he should\ndefinitely get identified that okay in\nthis particular scenario you need to\nsell your stock because tomorrow stock\nmarket is going to crash but for\ncompanies this is very bad okay I hope\neverybody is able to understand for\ncompanies it is very very bad so in this\nparticular case sometime we need to\nfocus both on false positive and false\nnegative and again I'm telling you for\nwhich problem statement you are solving\nthat indicates if you are solving for\npeople then they should be able to get\nthe notification saying that it is going\nto crash if you're probably uh doing it\nfor companies at that time your\nPrecision recall may change but if I\nconsider for both the scenarios at that\npoint of time I will definitely use\nsomething called as F score F score or\nI'll also say it as F beta now how is\nfbaa Formula given as I will talk about\nit and here in the F score you have\nthree different formulas the first\nFormula I will say basically as when\nyour beta value is 1 okay first of all\nI'll just give a generic definition of f\ns or F beta here you are basically going\nto consider 1 + beta squ Precision\nmultiplied by recall divided beta Square\n* Precision plus recall whenever your\nboth false positive and false negative\nare important we select beta as one so\nif I select beta as 1 it becomes 1 + 4\nPrecision multiplied by recall then you\nhave Precision plus recall so here sorry\n1 + 1 so this becomes 2 multiplied by\nPrecision into recall divided by\nPrecision plus recall so here you have\nthis is basically called as harmonic\nmean harmonic mean probably you have\nseen this kind of equation where you\nhave written 2x y / x + y same type you\nare able to see this this is called as\nharmonic mean here the focus is on both\nfalse positive and false negative let's\nsay that your false positive is more\nimportant than false negative at that\npoint of time you will try to decrease\nor you will try to decrease your beta\nvalue let's say that I'm decreasing my\nBeta value to 0.5 then what will happen\n1 +5 whole\ns and then you have P * R Precision\nrecall and here also you have 25 p + r\nnow in this particular scenario I'm\ndecreasing my Beta decreasing the beta\nbasically means that you are providing\nmore importance to false positive than\nfalse negative and finally you'll be\nable to see that if I consider beta\nvalue as let me just say my notes if I\nconsider beta value as two that\nbasically means you are giving more\nimportance to false negative than false\npositive so with this specific case you\ncan come up to a conclusion what value\nyou basically want to use now whenever I\nuse beta is equal to 1 it becomes fub1\nscore if I use beta as .5 then this\nbasically becomes f.5 score and this\nbecomes your F2 score So based on which\nis important okay which is important\nwhether your Precision or false positive\nor false negative is important you can\nconsider those things F score will have\ndifferent values if you're using beta is\nequal to 1 that basically means you are\ngiving importance to both precision and\nrecall if your false positive is more\nimportant then at that point of time you\nreduce beta value if false negative is\ngreater than false bet uh false positive\nthen your beta value is\nincreasing beta is a deciding parameter\nto decide your F1 score or F2 score or F\nPoint score now first thing first what\nis the agenda of today's session first\nof all we will complete practicals for\nall the algorithms that we have\ndiscussed these all algorithms that we\nhave discussed we will cover the\npracticals probably we will be doing\nhyper parameter tuning everything the\nsecond thing and again here we are going\nto take just simple examples so yes uh\nso today's session I said practicals\nwith simple examples where I'll probably\ndiscuss about all the hyper parameter\ntuning then the second one the second\nalgorithm that I'm going to discuss\nabout is something called as n bias this\nis a classification algorithm so we are\ngoing to understand the intuition and\nthe third one that we are going to\nprobably discusses KNN algorithm so KNN\nalgorithms is definitely there\nso this our today's plan I know I've\nwritten very less but this much maths\nand involved in na bias right we'll\nunderstand the probability theorem again\nover there there is something called as\nbias theorem we'll try to understand and\nthen we'll try to solve a problem on\nthat so let's proceed and let's enjoy\ntoday's session how do we enjoy first of\nall we enjoy by creating a practical\nproblem so I am actually opening a\nnotebook file in front of you so here uh\nwe will try to Sol solve it with the\nhelp of linear regression Ridge lasso\nand try to solve some problems let's see\nhow much we will be able to solve it but\nagain the aim is that we learn in a\nbetter way okay uh so that everybody\nunderstands some basic basic things okay\nso first of all as usual uh everybody\nopen your jupyter notebook file the\nfirst algorithm that I'm going to\ndiscuss about is something called as SK\nlearn linear regression so everybody I\nhope everybody knows about this SK learn\nlet's see what all things are basically\nthere in this we will be using fit\nintercept everything as such but here\nthe main aim is to find out the\ncoefficients which is basically\nindicated by Theta 0 Theta 1 and all the\nfirst thing we'll start with linear\nregression and then we will go ahead and\ndiscuss with r and lassor I'm just going\nto make this as\nmarkdown how many different libraries of\nfor linear regression you can do with\nstats you can do with skyi you can do\nwith many things okay so first thing\nfirst let's first of all we require a\ndata set so for the data set what we are\ngoing to do is that we are going to\nbasically take up some smaller smaller\ndata just let me do this so for this uh\nwe are going to take the house pricing\ndata set so we are going to solve house\npricing data set problem a simple data\nset which is already present in SK learn\nonly now in order to import the data set\nI will write a line of code which is\nlike from SK learn dot data sets data\nsets\nimport load uncore Boston so we have\nsome Boston house pricing data set so\nI'm just going to execute this I'm also\ngoing to make a lot of Sals so that I\ndon't have to again go ahead and create\nall the sales again some basic libraries\nthat I probably want is pro import numai\nas\nNP\nimport pandas\nSPD okay import cbon as\nSNS and then I will also import Matt\nMatt plot lib do p plot as PLT and then\npercentile matplot lib matlot lib do\ninline and I will try to execute this\nsee this my typing speed has become a\nlittle bit faster by writing by\nexecuting this queries again and again\nand uh let's go ahead uh so I have\nimported all the necessary libraries\nthat is required which which will be\nmore than sufficient for you all to\nstart with now in order to load this\nparticular data set I will just use this\nLibrary called as load uncore Boston and\nI'm going to just initialize this so if\nyou press shift tab you will be able to\nsee that return load and return the\nBoston house prices data set it is a\nregression problem it is saying and then\nprobably I'm just going to execute it\nnow once I execute it I will go and\nprobably see the type of DF so it is\nbasically saying skarn dos. bunch now if\nI go and probably execute DF you'll be\nable to see that this will be in the\nform of key value pairs okay like Target\nis here data is here okay so data is\nhere Target is here and probably you'll\nbe able to find out feature names is\nhere so we definitely require feature\nnames we require our Target value and\nour data value so we really need to\ncombine this specific thing in a proper\nway in the form of a data frame so that\nyou will be able to see so what I'm\nactually going to do over here I'm just\ngoing to say PD do data frame I'll\nconvert this entirely into a data frame\nand I will say DF do data see this is a\nkey value pair right so DF do data is\nbasically giving me all the features\nvalue so if I write DF do data and just\nexecute it you'll be able to see that I\nyou will be able to get my entire data\nset in this way my entire data set in\nthis way this is my feature one feature\ntwo feature three feature 4 this feature\n12 I have 12 features over here and\nbased on that I have that specific value\nnow the next thing thing that I'm going\nto do probably I should also be able to\nadd the target feature name over here so\nwhat I will do I will just convert this\ninto DF and then I will also say DF do\ncolumns and I'll set it to DF do Target\nokay and let me change this to data set\nso I'm going to change this to data set\nand I'm going to say data set. columns\nis equal to DF do Target so if I execute\nthis and now if I probably\nprint my data set do head you will be\nable to see this specific thing okay it\nis an error let's see expected axis has\n13 element new values has\n506 so Target okay I should not use\nTarget over here instead I had a column\nwhich is called as features feature\nnames like if I go and probably see\nDF DF over here you'll be able to see\nthere is one thing which is called as\nfeature names so I'm going to use DF do\nfeature names over here so here it is DF\ndo feature names I'm just going to paste\nit over here and now if I go and write\nhere you can see print DF data set. head\nif I go and execute without print you'll\nbe able to see my entire data set so\nthese are my features with respect to\ndifferent different things and this is\nbasically a house pricing data set so\ninitially I have this features CRM ZN\nindust CH nox RM age distance radius tax\nPT ratio b l stack that so I have my\nentire data set over here the same data\nset I have basically put it over here\nnow here also you'll be able to see what\nall this feature basically means this is\nshowing wasted weighted distance to five\ndo uh Five Boston employment center rad\nbasically means index of accessibility\nto radial Highway tax basically means\nfull value property tax rate this much\nPT rate basically means pupil teacher\nratio I don't know what the hell it\nmeans but it's fine we have some kind of\ndata over here properly in front of you\nso these are my independent features\nwhat are these these all are my\nindependent features if you want the\nfeatures detail here you can see it\nright everything what is CRM this\nbasically means per capita crime rate by\ntown which is important ZN it is\nproportional of residential land zone\nfor Lots over 25,000 Square ft so this\nis my DF I did not do much I'm just\nusing data frame DF do data column\nfeatures name I'm getting this value\nvery much simple now let's go a little\nbit slowly so that many people will be\nable to also understand now this is my\ndata set. head now the thing is that I\nobviously have taken all these\nparticular values but this is my\nindependent feature I still have my\ndependent feature so what I'm actually\ngoing to do I will create a new feature\nwhich is like data set of price I'll\ncreate my feature name price price of\nthe house and what I will assign this\nparticular value this value will be\nassigned with this target this target\nvalue this target value is basically the\nsale the price of the houses right it is\nagain in the form of array so I'm going\nto take this and put it as a dependent\nfeature so here you'll be able to see\nthat my price will be my dependent\nfeature so here I'll basically write DF\ndo Target so once I execute it and now\nif I probably go and see my data set do\nhead you'll be able to see features over\nhere and one more feature is getting\nadded that is price now this price may\nbe the units may be in\nmillions somewhere Target should be here\nor there it should be probably in\nmillions\nor I cannot see it but it should be\nsomewhere here it should have definitely\nsaid that it is probably in millions or\nokay but that is not a problem I think\nbut mostly it'll be in millions\nsomewhere I think it should be\nhere okay I cannot see it but probably\nif I put more time I'll be able to\nunderstand it okay so over here what is\nthe thing main thing this all are my\nindependent features and this is my\ndependent feature right so if I'm trying\nto solve linear regression I have to\ndivide my independent and dependent\nfeatures properly now let's go to the\nnext step that\nis\ndividing the data\nset dividing the oh my God dividing the\ndata\nset\ninto\ntrain into first of all I'll try try to\ndivide into\nindependent and dependent\nfeatures so I want my entire features\ndata set divided into independent and\ndependent features X I will be using as\nmy independent featur so I will write\ndata set dot I will use an iock which is\npresent in data frames and understand\nfrom which feature to which feature I\nwill be taking as my independent feature\nto this feature till lat so the best way\nthat basically means that I just need to\nskip the last feature in order to skip\nthe last feature what I'm actually going\nto do from all the columns I will just\nskip the last column so this is how you\nbasically do an indexing with respect to\njust skipping the last feature and this\nwill basically be my independent\nfeatures and here I will basically say Y\nis equal to data set do iock and here I\njust want the last feature so I will\nwrite colon all the records I want and\nsee the first term that we are probably\nWR writing over here this basically\nspecifies with respect to records here\nthis specifies with respect to columns\nfrom all the columns I'm taking the last\ncolumn here I will just take the last\ncolumn and this will basically be my\ndependent features dependent features so\nhere I have basically executed now if\nyou can go and probably see x. head here\nyou'll be able to find all my\nindependent features in y do head you'll\nbe able to find the dependent feature\nnow let's go to the first algorithm that\nis called as linear regression\nalways remember whenever I definitely\nstart with linear regression I'll\ndefinitely not go directly with linear\nregression instead what I will do is\nthat I'll try to go with Ridge\nregression and uh lasso regression\nbecause there you are lot of options\nwith respect to hyper pment T but I'll\njust show you how linear regression is\ndone so basically you really really need\nto use a lot of libraries okay over here\nand based on this libraries this\nlibraries will try to install okay and\nwhat are these libraries these are\nbasically the linear regression Library\nso here I'm basically going to use two\nspecific thing one is linear regression\nLibrary so I will just use from SK learn\ndo linear uncore model import linear\nregression do you need to remember this\nthe answer is no because I also do the\nGoogle and I try to find out where in\nescal and it is present okay so here is\nmy linear regression so I will try to\ninitialize linear reg is equal to\ninitialize with linear regression and\nthen here what I'm actually going to do\nI'm going to basically apply something\ncalled as cross validation cross\nvalidation is very much important\nbecause in Cross validation we divide\nout train and test data in such a way\nthat every combination of the train and\ntest data is basically taken by care is\ntaken by the model and whoever accuracy\nis better that all entire thing is\nbasically combined so here what I'm\ngoing to do I'm going to say mean square\nerror is equal to here I will import one\nmore Library let's say from SK learn\ndot model selection I'm going to import\ncross Val\nscore so cross Val score cross\nvalidation score basically means it is\ngoing to do a lot of train and test\nsplit it's something like this one\nexample I will show it to you here only\nso what does cross validation basically\ndo okay so in Cross validation what\nhappens what you do suppose this is your\nentire data\nset suppose this is 100 records if you\ndo five cross validation then in the\nfirst this will be your test data and\nremaining all will be your training data\nif in the second cross validation this\nwill be your test data and remaining all\nwill be your test uh training data like\nthis five times you'll be doing cross\nvalidation by taking the different\ncombination of train and test but I'm\nnot going to discuss much about it in\nthe future if you want a separate\nsession I will include that in one of\nthe session itself so this was uh\nbasically the plan with respect to cross\nvalidation or cross Val score so here\nI'm going to basically take cross\nVal\nscore and here the first parameter that\nI give is my model so linear regression\nis my model and here I will take X and Y\nI'm not doing a train test split\nspecifically over here I'm giving the\nentire X and Y and probably based on\nthat I'm going to do a cross validation\nover here you can also do train test\nplate initially and then just give the X\ntrain and Y train over here to do the\ncross validation it is up to you but the\nbest practices will be that first you do\nthe train test split and then only give\nthe train data over here to do the cross\nvalidation I'm just going to use scoring\nis equal to you can use mean squared\nerror negative mean squared error let's\nsay that I'm going to use negative mean\nsqu error again where do you find all\nthese things you will be able to see in\nthe SK learn page of L uh cross Val\nscore and then finally in the cross Val\nscore you give cross validation value as\n5 10 whatever you want so after this\nwhat I'm actually going to do I'm just\ngoing to basically from this how many\nscores I will get the mean squar error\nwill be five since I'm doing five cross\nvalidation if you don't believe me just\nsee over here print msse so here you'll\nbe able to see five different values 1 2\n3 4 5 right five different mean values\nbecause we are doing cross five five\ncross validation so here what I'm going\nto write I'm just going to say np. mean\nI want to take the average of all the\nfive so here will basically be my\nmeanor\nmsse okay and then probably I'll print I\nwill print my Ms meanor MSC so this will\nbe my average score with respect to this\nthe negative value is there because we\nhave used negative mean squ error but if\nyou just consider mean square error then\nit is only 37.1 3 okay so this I have\nactually shown you how to do cross\nvalidation see with respect to linear\nregression you can't modify much with\nthe parameter so that is the reason why\nspecifically in order to overcome\noverfitting and do the feature selection\nwe use uh R and lasso regression so here\nI will show show you how to do ridge\nridge regression\nnow now in order to do the prediction\nall you have to do is that just go over\nhere take the model okay what is the\nmodel linear R and just say do\npredict so here you can see uh you'll be\ngetting a function called as do predict\nand give the test value whatever you\nwant to predict automatically the\nprediction will be done so I'm just\ngoing to remove this and focus on Ridge\nregression right now because I I want to\nshow how hyperparameter tuning is done\nin R regression so for R regression the\nsimple thing is that I'll be using two\ndifferent libraries from skarn do\nlinear linear uncore model I'm going to\nimport Ridge so for the ridge it is also\npresent in linear underscore model for\ndoing the hyperparameter tuning I will\nbe using from SK learn do modore\nselection and then I'm going to import\ngrid SE CV so these are the two\nlibraries that I'm actually going to use\ngrid SE CV will be able to help you out\nwith the um okay will be able to help\nyou out with Hyper parameter tuning and\nthen probably you'll be able to do\nthat uh difference between MSE and\nnegative MSE not big thing guys if you\nuse MSE here mean squ error you'll be\ngetting 37 I've just used negation of\nMSE it's okay anything is fine you can\ngo with MSE also means square error\nthere is also another uh another scoring\narea which is like which focuses on\nsquare root square mean Square uh sorry\nroot means Square eror okay so there are\ndifferent different things which you can\nbasically focus on okay now in order to\ngive you this specific good value I'm\nactually going to do hyper Peter tuning\nnow let's go ahead with uh grid s CV so\nhere what I'm going to do again I'm\ngoing to basically Define my model which\nwill be\nRidge okay so this this is what I have\nactually imported now uh let me open the\nridge skarn so SK learn\nRidge we need need to understand what\nall parameters are basically\nused do you remember this Alpha value\nguys do you remember this Alpha value\nwhy do we use Alpha I I told you now\nAlpha multiplied by slope square if you\nremember in Ridge we specifically use\nthis right Ridge and lasso regression\nAlpha so this is the alpha the this is\nprobably the best parameter we can\nperform hyper parameter tuning the next\nparameter that we can probably perform\nis basically uh this Max iteration okay\nMax iteration basically means how many\nnumber of iteration how many number of\ntimes we may probably change the Theta 1\nvalue to get the right value so we can\ndo this so what I'm actually going to do\nI'm going to select some Alpha values\nI'm going to play with this apart from\nthat if I want I can also play with the\nother parameters which are uh like kind\nof uh you know probably you can you can\nalso play with the iteration parameter\nit is up to you try whichever parameter\nyou want to change you can go ahead and\nchange it now let me show you how do we\nwrite this and how do we make sure that\nthis specific thing is done now uh\nbefore doing grid s CV uh let me do one\nthing I will Define my parameters okay\nso here is my Ridge now what I'm going\nto do I'm going to say parameters and in\nthis parameter two important value that\nI'm probably going to take is this one\nthat is my C value and I will try to\nDefine this in the form of dictionaries\nso here the C value that I sorry not C\njust a second\nguys my mistake it is not C it is\nAlpha let's see so how do I Define my\nAlpha value we'll try to see so here the\nparameters will be Alpha C is basically\nfor uh logistic regression I'll show you\nso the alpha value I will just mention\nsome values like\n1 e to the power of -5 that basically Me\n00000000 0 0 0 1 similarly I I can write\n1 E to the^ of - 10 that again means 0 0\n0 0 0 0 0 0 10 * 0 1 I'm just making fun\nokay so that you will also get\nentertained 1 E to the^ of minus 8 okay\nsimilarly I can write 1 E to the^ of\nminus 3 from this particular value now\nI'm increasing this value see 1 E to\nthe^ of minus 2 and then probably I can\nhave 1 5 10 um 20 something like this so\nI'm going to play with all this\nparticular parameters for right now\nbecause in grit or CV what they do is\nthat they take all the combination of\nthis Alpha value and wherever your uh\nyour your model performs well it is\ngoing to take that specific parameter\nand it is going to give you that okay\nthis is the best fit parameter that is\ngot selected so here I have got all\nthese things now what I'm going to do\nI'm going to basically apply the grid C\nTV so here I have uh gridge uh sorry\nRidge GD I'm\nsaying ridore regressor so I'm going to\nuse git s\nCV git s CV and here I'm basically going\nto take the parameters regge okay Ridge\nis my first model and then I will take\nup all this params that I have actually\ndefined see in git CV if I press shift\ntab I have to first of all execute this\nthen only it will be able to press shift\ntab so here if I press shift tab here\nyou'll be able to see estimator and\nparameter grid is my second parameter\nthen scoring and then all the other\nparameters so here the first thing that\ngoes is your model then your parameters\nwhich what you are actually playing then\nthe third parameter is basically your\nscoring\nscoring and again here I'm going to use\nnegative mean squ error some people are\nsaying that mean squared error is not\npresent so that is the reason why\nnegative mean squ error is done why it\nmay not be present because\nuh they try to always create a generic\nLibrary probably this kind of uh scoring\nparameter may also get used in other\nalgorithms so that is the reason they\nmay not have created but if you want to\nDeep dive into it Google\nGoogle then what is r regress dot fit on\nX comma y again I'm telling you you can\nfirst of all do train test split on X\nand Y and then probably only do this on\nX train and Y train parameter is not oh\nsorry\nokay I get this okay parameter is not\nand why it is not and oh yeah it has\nbecome a\nlist I'm going to make this as\ndictionary right now I'm fully focused\non implementing things if I get an error\nI'll definitely make sure that it'll get\nfixed anyhow if I get that error I will\nnot say oh Kish why why this error came\nyou\nknow why this error came I I'll not get\nworried I'll get the error down only you\ncannot give this as the one okay so try\nto understand okay so this is your gitar\nCV I've also done the fit and let's go\nand select the best parameter so what I\ncan do I will write print\nridore\nregressor dot\nparams sorry there will be a parameter\ncalled as best params I'm going to print\nthis and I'm going to print ridore\nregressor Dot\nbest\nscore so these are all the values that\nare got selected one is Alpha is equal\nto 20 and the best score is - 32 so\ninitially I gotus 37 but because of\nRidge regression you can see that our\nnegative mean square error has\ndefinitely become better there is a\nminus sign don't worry but from 37 it\nhas come to 32 cross validation guys\nover here inside grids s CV also when it\nis probably taking the entire\ncombination over there the CV Value\nCross validation also we can use\nso probably if I am probably considering\nall these\nthings many people has a question Chris\nis this minus value increased that\nbasically means you cannot use Ridge\nregression you are right in this\nparticular case Ridge regression is not\nhelping you out so guys let me again\nwrite it down everybody don't worry yeah\nprevious I got minus 32 right now I'm\ngetting - 37\nright sorry previously I got what - 37\n- 37 now I got - 32 so here you can see\nthis I got it from linear regression\nthis I got it from what Ridge which one\nshould I select I should select this\nmodel only because it is performing well\nthan this but again understand Ridge\nalso tries to reduce the overfitting so\nprobably in this particular scenario we\ncannot use Ridge because the performance\nis becoming more bad so what I will do I\nwill go and try with lasso regression\nnow I'll copy and paste the same thing\nso linear model import lasso then this\nwill basically be my\nlasso let's see with lasso whether it\nwill increase or not let's\nsee this is my parameter that got\nselected now let me write lasto\nregressor\ndot best params so this is Alpha is\nequal to one is got selected over here\nI'm just going to print it okay and then\nI'm going to print with last one\nregression DOT score will be the best so\nhere I'm actually getting - 35 - 35 here\nI'm actually getting - 32 so minus 35\nstill I will focus on linear regression\nnow see what will happen if I add more\nparameters if I add more parameters see\nwhat will happen so now I'm going to\ntake Alpha different different values\nsee this I'm just going to remove this\nand probably add Alpha value in this\nway see here I have added more values 5\n10 20 30 35 40 45 100 okay let's see\nwhether we our performance will increase\nor not so here\nuh first of all let me remove from here\nin Ridge just take it down guys I'm I'm\nadding more parameters like this just\ntake it down yeah CV is equal to 5\nnobody okay you're not able to see it um\nCV is equal to 5 now here it is uh what\nyou can basically focus on so here you\ncan see I have added some values like\nthis you can also\nadd and just try to execute and now if I\ngo and probably see this is my see first\nI have tried for Ridge I'm getting minus\n29 do you see after adding more\nparameters what happened in Ridge after\nadding more parameters what happened in\nRidge you can see om minus 29 and the\nalpha value that is got selected is 100\nif you want try with cross validation\n10 and just try to execute now\nnow so these are are some hyper\nparameters that we will definitely play\nwith here you can see - 29 so here you\ncan see minus 29 you can also increase\nthe cross validation\nvalue over here also and probably\nexecute it but with lasso I don't know\nwhether it is improving or not it is\ncoming to minus 34 you just have to play\nwith this parameters as now for a bigger\nproblem statement the thing is not\nlimited to here right we try to take\nmultiples and many parameters multiples\nand many parameters and try to do these\nthings it is up to you we play with\nmultiple parameters whichever gives us\nthe best result we are basically taking\nit it's okay error is increased I know\nthat no error is increasing definitely\nerror is increasing even though by\ntrying with different different\nparameters but about most of the\nscenario see here I gotus 37 probably\nwhat I can actually do is that uh try to\nget better one with respect to this\nnow the best way what I can also do is\nthat I can basically take up train and\ntest split also and probably do these\nthings let's see let's see one example\nso how do we do train and test from SK\nscalar dot I think model selection\nimport train test split okay it's okay\nguys you may get a different value okay\nlet's do one thing okay let's make your\nproblem statement little bit simpler now\nwhat I'm going to do just tell me in\ntrain test plate what we need to do so\nI'm going to take the same code I'm\ngoing to paste it over here or let me do\none thing let me insert a cell below and\nlet me do it for train test split so in\ntrain test plate what we can do so I'm\njust going to take the syntax paste it\nover here let's say that I'm taking XT\ntrain y train and then I'm using train\ntest split with 33% now if I execute\nwith respect to X train and Y train so\nhere is my you can see this I have\nwritten this code from SK learn. model\nselection uh train test plate random\nState can be anything whatever you write\nit is fine then you basically give X and\nY with test sizes 33 uh this is\nbasically saying that the test will have\n33% and the train data will be 77% so\nthis is what I'm actually getting with\nrespect to X train and Y train here what\nI'm going to do I'm going to basically\ntake X train comma y train and now if I\ngo and probably see this here you can\nsee minus 25 understand this value\nshould go towards zero if it is going\ntowards zero that basically means the\nperformance is better now similarly I do\nit for Ridge in Ridge what I'm actually\ngoing to do here I'm going to write X\ntrain and Y train and if I go and\nprobably select the best score than this\nhere you'll be able to see I'm getting\nhow much I'm getting minus\n2.47 okay here I'm getting\n25.8 here 25. 47 that basically means\nnow still the Improvement is little bit\nbad because here we are not going\ntowards zero so the next part again here\nalso you can basically do it for X train\nand Y train X train and Y train so here\nyou have this one and let's go and\nexecute this so here you can see minus\n2.47 now what you can also do is that\nyou can use this\nlasso regressor do predict and you can\nbasically predict with respect to X test\nso this is your white test value suppose\nlet's say that this is my y PR Yore PR\nthen what I can do from SK\nlearn I will be using R square and\nadjusted R square if you remember SK\nlearn R square r² so this is my R2 score\nso where it is present in SK learn.\nMatrix so I'm going to write from SK\nlearn import let's say I'm saying from\nskarn do Matrix import r² R2 score now\nwhat I'm going to do over here I'm\nbasically going to say my R2 score which\nis my variable I'll say this is nothing\nbut R2 score here I'm just going to give\nmy y PR comma Yore test so if I go and\nprobably see the output here I will be\nable to see print R2 score this is all I\nhave discussed guys there is also\nadjusted rant score is there where is R2\nR2 score one adjusted r² okay R2 score\nis there but adjusted R square should be\nhere somewhere in some manner so this is\nhow your output looks like with respect\nto by using this lasso regressor okay\nwhich is very good okay it should be I\ntold it should be near 100% right now\nI'm getting 67% if I want to tie with\nthe ridge you can also try that so you\ncan say Ridge regressor do predict and\nhere you can see 7 68% then you can also\ntry linear regressor and\npredict what is the error saying the\nregression is not fitted yet why why it\nis not fitted why it is not\nfitted let's say that I have fitted here\nlinear\nregression dot fit on X train and Y\ntrain X train and comma y train so I'm\njust going to fit it now if I go and\nprobably try to do the\ncalculation so if I go and see my R2\nscore it is also coming somewhere around\n68% 67% now since this is just a linear\nregression you won't be able to get 100%\nbecause you're drawing a straight line\nright so for that you basically have to\nother use other algorithms like XG boost\nand all n bias so many algorithms are\nthere it's okay see you give y test over\nhere y PR over here both are same right\nthey're\ncomparing by see at one limit you can\nyou can increase the performance after\nthat you cannot see again I'm telling\nyou in linear regression what we do\nthese are my points right I will be only\nable to create one best line I cannot\ncreate a curve line right over here so\nobviously my accuracy will be only\nlimited let's go and do it logistic\npractical\nquickly and here uh in logistic also we\ncan do git SE CV now what I'm actually\ngoing to do first of all let's go ahead\nwith the data set so I will quickly\nImplement logistic so from LC learn.\nlinear\nmodel I'm going to import logistic\nregression so I'm going to use logistic\nregression and apart from that we know\nthat let's take a new data set because\nfor logistic we need to solve using\nclassification problem so this is\nbasically my logistic regression I'll\ntake one data set so from SK learn. data\nsets import we'll take a data set which\nis like uh breast cancer data set so\nthat is also present in SK learn with\nrespect to the breast cancer data set\nI'm just going to use this see load best\ncancer data set I'm loading it and all\nthe independent features are in data and\nmy columns are feature names the same\nthing like how we did previously okay so\nthis will basically be my\ncomplete uh complete independent feature\nso if I go and probably see this x. head\nhere you'll be able to see that based on\nthis input features the independent\nfeature we need to determine whether the\nperson is having cancer or not these are\nsome of the features over here and this\nis like many many features are actually\npresent so next thing I this that was my\nindependent feature now I'll take my\ndependent feature dependent feature will\nalready present in DF Target okay this\nparticular data set that we have taken\nin DF in DF do Target we will basically\nhave all our dependent feature these are\nmy independent features so what I'm\nactually going to do I'm going to create\nY and I'm going to say PD do data frame\nand here I'm going to say DF do Target\nTarget and this column name should be\nTarget right so this will be my column\nname and now if I go and see my y y is\nbasically having zeros and one in the\ntarget feature now the next thing that\nwe are going to do is that uh apply\nbasically apply the first of all we need\nto check whether this data set is uh\nthis particular y column is balanced or\nimbalanced okay in order to do that I\nwill just write F\nTarget if the data set is imbalanced\ndefinitely we need to work on that and\ntry to perform upsampling so if I write\ny target. Valore counts if I execute\nthis so here you'll be able to see that\nvalue SC counts will basically give that\nhow many number of ones are and how many\nnumber of zeros are so now total number\nof ones are 357 and total number of\nzeros are 22 so is this a imbalanced\ndata set probably this is a balanced\ndata set so here I'm actually going to\nnow do train test spit train test spit I\nwill try to do again train test spit how\ndo we do we can quickly do copy the same\nthing entirely I'll copy this entirely\nover here and then I will get my X and Y\nso here is my X train X test y train y\ntest so train test plate obviously I'll\nbe doing it now in logistic regression\nif I go and search for\nlogistic regression escalar I will be\nable to see this what all parameters are\nthere this is basically the L1 Norm or\nL2 Norm or L1 regularization or L2\nregularization with respect to whatever\nthings we have discussed in logistic and\nthen the C value these two parameter\nvalues are very much important if I\nprobably show you over here the penalty\nwhat kind of penalty whether you want to\nadd L2 penalty L1 penalty you can use L2\nor L1 the next thing is C this is\nnothing but inverse of regularization\nstrength this basically says 1 by Lambda\nsomething like that this parameter is\nalso very much important guys class\nweight suppose if your data set is not\nbalanced at that point of time you can\napply weights to your classes if\nprobably your data set is balanced you\ncan directly use class weight is equal\nto balanced other than that you can use\nother other weight which you basically\nwant so this is specifically some of\nthis right no this is not Ridge or lasso\nokay this is logistic in logistic also\nyou have L1 norm and L2\nNorms understand probably I missed that\nparticular part in the theory but here\nalso you have an L2 penalty norm and L1\npenalty Norm I probably did not teach\nyou in theory because if you look see\nlogistic regression can be learned by\ntwo different ways one is through\nprobabilistic method and one is through\ngeometric method if you go and probably\nsee my video that is present with\nrespect to logistic regression right now\nin my YouTube channel there I have\nexplained you about this L1 and L2 Norms\nalso over there so in this also it is\nbasically present it is a kind of\npenalty again just for uh using for this\nkind of classification problem so what\nI'm actually going to do let's go and\nplay with the parameters that I am\nlooking at so I will play with two\nparameters one is params C value here\nI'm defining 1 10 20 anything that you\ncan Define one set of values you can\nDefine and there was one more parameter\nwhich is called as Max iteration this is\nspecifically for grits or CV okay that\nI'm specifically going to apply so I\nwill just try to execute this this will\nbe my params now I'm going to quickly\nDefine my model one which will be my\nlogistic regression model so my logistic\nregression here by default one value\nI'll give for C and Max itra let's say\nI'm giving this value later on what I\nwill do for this model I'll apply it to\ngrid sear CV so I'm just going to say\ngrid s CV and I'm going to apply it for\nmodel one param grid is equal to params\nthis parameter that I'm specifically\ntrying to apply since this is a\nclassification problem and I am not\npretty sure that whether true positive\nis important or true negative is\nimportant I'm going to use F1 scoring\nokay F1 scoring is basically again the\nparametric term which we discussed\nyesterday which is nothing but\nperformance metrics and then I'm going\nto use CV is equal to 5 so this will be\nentirely my model with respect to grid s\nCV and I'll be executing this then I\nwill do model. fit on my X train and Y\ntrain data so once I execute it here you\ncan see all the output along with\nwarnings a lot of warnings will be\ncoming I don't know because this many\nparameters are there and finally you can\nsee that this has got selected now if\nyou really want to find out what is your\nbest param score model\ndot best params so here you can see Max\niteration as\n150 and what you can actually do with\nrespect to your best score model do best\nscore is 95 percentage but still we want\nto test it with test data so can we do\nit yes we can definitely do it I'll say\nmodel do core or I'll say model dot\npredict on my X test data and this will\nbasically be my y red so this will be my\ny red all the Y prediction that I'm\nactually getting so if you go and see y\nred so these are my ones and zeros with\nrespect to the Y\nprediction at finally after getting the\nprediction values I can apply confusion\nMatrix I hope I have taught you about\nconfusion Matrix so from sklearn do\nconfusion Matrix sorry sklearn do metrix\nI'm going to import confusion metrix\nclassification report and the next thing\nthat I would like to do is this two I\nwill try to import confusion Matrix and\nclassification report now if you want to\nsee the confusion Matrix with respect to\nyour I can just write\nYore frad or Yore test whatever you want\ngo ahead with it and this is basically\nmy confusion Matrix if I put this\nforward no difference will be there only\nthis thing will be moving that also I\nshowed you 63 118 3 and 4 now finally if\nI want to accuracy score I can also\nimport accuracy score over here so here\nyou can see accuracy score is imported I\ncan also find out my accuracy score\nwhich is my the total accuracy with\nrespect to this I we can give y test and\nYore PR which we have discussed\nyesterday this is giving\n96% if you want detailed Precision\nrecall all the score then at that point\nof time I can use this classification\nreport and here I can give white test\nand wied here is what I'm actually\ngetting so here you can see with respect\nto F1 F1 score Precision recall since\nthis is a balanced data set obviously\nthe performance will be best yes you can\nalso use Roc see I'll also show you how\nto use Roc and probably you'll be able\nto see this you have to probably\ncalculate false positive rate two\npositive rate but don't worry about Roc\nI will first of all explain you the\ntheoretical part now let's go ahead and\ndiscuss about n bias n bias is an\nimportant algorithm so here I'm just\ngoing to go ahead so now let's go ahead\nand discuss about na bias and here we\nare going to discuss about the intuition\nso na bias is an another amazing\nalgorithm which is specifically used for\nclassification and this specifically\nworks on something called as base\ntheorem now what exactly is base theorem\nfirst of all we need to understand about\nbase theorem let's say that guys I have\nbase theorem let's say that I have an\nexperiment which is called as rolling a\ndis now in rolling a dis how many number\nof elements I have have so if I say what\nis the probability of 1 then obviously\nyou'll be saying 1X 6 if I say\nprobability of two then also here you'll\nsay 1X 6 if I say probability of three\nthen I will definitely say it is 1x 6 so\nhere you know that this kind of events\nare basically called as independent\nevents now rolling a dice why it is\ncalled as an independent event because\ngetting one or two in every experiment\none is not dependent on two two is not\ndependent on three so they are all\nindependent that is the reason why we\nspecifically say is an independent event\nbut if I take an example of dependent\nevents let's consider that I have a bag\nof marbles okay in this marble I\nbasically have three red marbles and I\nhave two green marbles now tell me what\nis the probability of suppose I have a\nevent in the first event I take out a\nred marble so what is the probability of\ntaking out a red marble so here you can\ndefinitely say that it is\n3x5 okay so this is my first event now\nin the second event let's say that in\nthis you have taken out the red marble\nnow what is the second second time again\nyou are taking out the second red marble\nor forget about second Rand marble now\nyou want to take out the green marble\nnow what is the probability with respect\nto taking out a green marble so here\nyou'll be definitely saying that okay\none red marble has been removed then the\ntotal number of marbles that are left\nare four so here you can definitely\nwrite that probability of getting a\ngreen marble is nothing but 2x4 which is\nnothing but 1x2 so here what is\nhappening first first element you took\nout first marble that you took out first\nevent from from the first event you took\nout red marble from the second event you\ntook out green marble this two are in\nthese two are dependent events because\nthe number of marbles are getting\nreduced as you take out from them so if\nI tell you what is the probability of\ntaking out a red marble and then a green\nmarble so it's the simple the formula\nwill be very much simple right which we\nhave already discussed in stats it is\nnothing but probability of probability\nof red multiplied by probability of\ngreen given Red so this specific thing\nis called as conditional probability\nhere understand what is happening\nprobability of green marble given the\nred marble event has occurred here both\nthe events are independent now let me\nwrite it down very nicely so I can write\nprobability of A and B is equal to\nprobability of a multiplied probability\nof B divided by probability of a let's\ngo and derive something can can I write\nprobability of A and B is equal to\nprobability of b and a so answer is yes\nwe can definitely say we can definitely\nsay if you go and do the calculation\nyou'll be able to get the answer you\nshould not say no now what is the\nformula for probability of A and B so\nhere you can basically write probability\nof a multiplied by probability of B\ngiven a if I take out probability of\ngreen what is probability of green in\nthis particular case 2x 5 what is\nprobability of red 3x 4 for right now\nlet's consider this now this part I can\ndefinitely write as this part I can\ndefinitely write as probability of B\nmultiplied by probability of B\nprobability of B this one probability of\nB and this will be probability of a\ngiven B so I can definitely write this\nmuch with respect to all this\ninformation now can I derive probability\nof a is equal to probability of B\nmultiplied by probability of a / B me\nprobability of a given B divided by\nprobability of sorry I'll write this as\nprobability of B given a divided by\nprobability of a and this is\nspecifically called as base theorem and\nthis is the Crux behind na bias\nunderstand this is the Crux behind the\nbase theorem now let's go ahead and\nlet's discuss about how we are using\nthis to solve let's take some examples\nand probably make you understand let's\nsay that I have some features like X1 X2\nX3 X4 X5 like this till xn and I have my\noutput y so these are my independent\nfeatures these all are my independent\nfeatures these all are my independent\nfeatures so here I'm going to write\nindependent features and this is my\noutput feature which is also my\ndependent feature now what is happening\nif I say probability of b or a what does\nthis basically mean I need to really\nfind what is the probability of Y and\nyou know that guys I will have some\nvalues over here and basically I'll have\nsome output value over here so based on\nthis input values I need to predict what\nis the output initially on a training\ndata set I will have your input and then\nyour output initially my model will get\ntrained on this now let's consider what\nthis entire terminology is I will try to\nwrite in terms of this equation so I\nwill say probability of Y given x1a x2a\nX3 up till xn then this equation will\nbecome probability of Y see probability\nof Y given X X1 X2 X3 xn this a is\nnothing but X1 X2 X3 xn and I'm trying\nto find out what is the probability of Y\nand then I will write probability of b b\nis nothing but y but before that what\nI'll write probability of a / B right a\ngiven b or probability of B probability\nof B is nothing but y multiplied by\nprobability of a given B probability of\na given B basically means probability of\nx1a X2 comma xn and given b b is given\nright so I'm able to find this entire\nvalue now just a second I made some\nmistakes I guess now it is correct sorry\nI I just missed one term that is this\ngiven y this is how it will become and\nthis will be equal to probability of a\nthat is X1 comma X2 like this up to XL\nso probability of Y multiplied by\nprobability of a given y now if I try to\nexpand this then this will basically\nbecome something like this see\nprobability of Y multiplied by\nprobability of X1 given yes a given y\nsorry given y multiplied by probability\nof X2 given y probability of x3 given Y\nand like this it will be probability of\nxn given y so this will also be y1 Y2 Y3\nYN this I can expand it like this and\nthen this will basically become\nprobability of X Y 1 multiplied by\nprobability of X2 multiplied by\nprobability of x3 like this up to\nprobability of xn so this is with\nrespect to all the probability y will be\ndifferent see here for this particular\nrecord y will be different for this y\nwill be different for this y will be\ndifferent but why output it may be yes\nor no right it may be yes or no okay I\nI'll solve a problem it will make\neverything understand and this will\nprobably be probability of Y it can be\nbinary multiclass whatever things you\nwant I'll solve a problem in front of\nyou now let's say that I have my y as\nlet's say that I have a lot of features\nX1 X2 X3 X X4 with respect to this let's\nsay in my one of my data set I have this\nmany x1s this many features and this is\nmy y so these are my feature number and\nthis is my y let's say that in y I have\nyes or no so how I will probably write\nwe really need to understand this okay I\nwill basically\nsay what is the probability of Y is\nequal to yes given this x of I this is\nmy first record first record of X of I\nthis is my second record of X of I so I\nmay write like this what is the\nprobability of Y being yes if x of I is\ngiven to you X of I basically means X1\nX2 X3 X4 so here you'll obviously write\nwhat kind of equation you'll basically\nsay probability of yes multiplied by\nprobability of yes multiplied by\nprobability of X of 1 given\nyes multiplied by probability of X2\ngiven yes probability of x3 given yes\nand probability of X4 given yes divided\nby probability of X1 multiplied by\nprobability of X2 multiplied by\nprobability of x3 multiplied by\nprobability of X4 Y is fixed it may be\nyes or it may be no but with respect to\ndifferent different records this value\nmay change similarly if I write\nprobability of Y is equal to no given X\nof I what it will be then it will be\nprobability of no multiplied by\nprobability of X1 given no then\nprobability of\nX2 given\nno probability of\nx3 given\nno and probability of X4 given no so\nhere because every any input that I give\nany input X of I that I give I may\neither get yes or no so I need to find\nboth the probability so probability of\nX1 multiplied by probability of X2\nmultiplied by probability of x3\nmultiplied by probability of X4 see with\nrespect to Any X of I the output can be\nyes or no and I really need to find out\nthe probabilities so both the formula is\nwritten over here what is the\nprobability of with respect to yes and\nwhat is the probability with respect to\nno now in this case one common thing you\nsee that this this denominator is fixed\nthis is definitely fixed it is fixed it\nis it is not going to change for both of\nthem and I can consider that this is a\nconstant so what I can do I can\ndefinitely ignore so here I can\ndefinitely ignore these things ignore\nthis also ignore this Al because see\nthis is constant so I don't want to\nconsider this in the next time I'll just\nuse this specific formula to calculate\nthe probability now let's say that if my\nfirst probability for a specific data\nset yes of X of I is let's say that I'm\ngetting\nas13 and similarly probability of no\nwith respect to X of I if I get\n05 you know that in a binary\nclassification any values if it get\ngreater than or equal to 5 we are going\nto consider it as 1 and if it is less\nthan 0.5 I'm going to consider it as\nzero now I'm getting values like this 13\nand .1 05 obviously I'm getting .13 05\nso we do something called as\nnormalization it says that if I really\nwant to find out the probability of X\nwith X of I if I do normalization it is\nnothing but .13 divided by .13 +\n05 72 this is nothing but\n72% and similarly if I do for\nprobability of no given X of I here\nobviously it will say 1 - 72 which will\nbe your remaining answer that is 28\nwhich is nothing but 28% so your final\nanswer will be this one this formulas\nyou have to remember now we'll solve a\nproblem let's solve a problem this will\nbe a very very interesting problem let's\nsay I have a data set which has like\nthis feature day let me just copy this\ndata set okay for you all now in this\ndata set I want to take out some\ninformation let's take out Outlook\ntable now based on this output Outlook\nfeature see over here Outlook my day\noutlook temperature humidity wind are\nthe input features independent feature\nthis is my output feature this one that\nyou are probably seeing play tennis is\nmy output feature which is specifically\na binary\nclassification so what I'm actually\ngoing to do I'm basically going to take\nmy Outlook feature and based on this\nOutlook feature I will just try to\ncreate a smaller table which will give\nsome information now based on Outlook\nfirst of all try to find out how many\ncategories are there in Outlook one is\nsunny one is\novercast and one is rain right three\ncategories are there so I'm going to\nwrite it down over here Sunny overcast\nand rain so these three are my features\nwith respect to Sunny uh with Outlook I\nhave three categories one is sunny one\nis overcast and one is RA here I'm going\nto basically say with respect to Sunny\nhow many yes are there and how many no\nare there and what is the probability of\nyes and probability of no so I'm going\nto again write it over here so this is\nmy Outlook feature\nand then I have categories first yes no\nSunny overcast rain yes no then\nprobability of yes and probability of no\nnow the next thing that we need to find\nout is that with respect to Sunny how\nmany of them are yes see yes we have so\nwhen we have sunny over here the answer\nis no so I will increase the count over\nhere one then again I have sunny again\nanswer is no so I'm going to increase\nthe count to two with this sunny this is\nbasically no okay so again I'm going to\nincrease the count to three now with\nsunny how many of them are yes one and\ntwo so I have this one and this one so I\nhave two so I'm going to say with\nrespect to Sunny I have two\nyes understand Outlook is my X1 X1\nfeature let's consider now the next\nthing is that let's see with respect to\novercost with overcast how many of them\nare yes so this overcast is there yes 1\n2 3 and four so total four yes are there\nwith respect to overcast then with\nrespect to overcast how many are on no\nyou can go ah and find out it is\nbasically zero NOS then with respect to\nrain how many of them are yes so here\nyou can see with respect to one rain yes\nyes no no so this is nothing but 3 2\nlet's try to find out there are three is\ntwo or\nnot one here also one yes is there right\nso 3 yes two NOS so the total number of\nyes and NOS if you count it there are\nnine yes and five NOS this is my total\ncount so if you totally count this 9 + 5\nis 14 you'll be able to compare that\nthere will be 9 yes and five NOS what is\nthe probability of yes when Sunny is\ngiven so here you have 2X 9 here you\nhave 4X 9 here you have 3x 9 now if if I\nsay what is the probability of no given\nSunny now see probability of yes given\nSunny probability of yes given forecast\nprobability of yes given rain so it is\nbasically that I will just try to write\nit in a simpler manner so that you'll\nnot get confused okay so this is my\nprobability of yes and this is my\nprobability of no but understand what\ndoes this basically mean this\nterminology basically means probability\nof yes given Sunny probability of yes\ngiven overcast probability of yes given\nrain similarly what is probability of no\nprobability of no obviously you know\nthat 3x 5 is my first probability then\nyou have 0x 5 and then you have 2X 5 now\nwith respect to the next feature let's\nconsider that I'm going to consider one\nmore feature and in this feature I will\nsay let's consider\ntemperature okay let's consider\ntemperature now in temperature how many\nfeatures I have or how many categories I\nhave I have hot you can see hot mild and\nand cold now with respect to hot mild\ncold here also I will be having yes no\nprobability of yes and probability of no\nnow try to find out with respect to hot\nhow many are yes so here no is there\nhere also no is there two NOS uh 1 yes\nuh 2 yes so two yes and two NOS probably\nthen similarly with respect to mild mild\nhow many are there 1 yes 1 No 2 yes 3s\n4s 4S and two knows okay so here you\nbasically go and calculate 4 yes and two\nknows with respect to cold how many are\nthere cool cool or cold 1 yes 1 No 2 yes\n3 S 3 S and 1 no so here I have\nspecifically have 3s and 1 no again the\ntotal number is 9 and five which will be\nequal to the same thing that what we\nhave got now really go ahead with\nfinding probability of yes given hot so\nit will be 2x 9 over here then here it\nwill be how much 4X 9 here it will be 3x\n9 again here what will be the\nprobability of no given given hot so\nit'll be 2x 5 2x 5 1X 5 so this two\ntables has already been created and\nfinally with respect to play the total\nnumber of plays are yes is 9 no is five\nand the answer is total 14 if if I say\nwhat is the probability of yes only yes\nthen it is nothing but 9 by4 what is the\nprobability of no it is nothing but\n5x4 okay so this two values also you\nrequire now let's say that you get a new\ndata set you need get a new data set\nlet's say you get a new test data where\nit says that suppose if you are having\nsunny and hot tell me what is the output\nso this is my problem statement so let\nme write it down so here I will write\nprobability of yes given Sunny comma hot\nthen here I will write probability of\nyes multiplied by probability of so here\nI will write probability of Sunny given\nyes multiplied by probability of hot\ngiven yes divided by what is it\nprobability of Sunny multiplied by\nprobability of hot\nequation because it is a\nconstant because probability of no also\nI'll be getting the same value 9 by4 so\nprobability of yes I'm going to replace\nit with 9\nby4 multiplied by 2x 9 then probability\nof hot given yes so I am going to get 2\nby 9 so\nhere 99 cancel or 2 1 7 then this is\nnothing but 2 by\n6331 I read this statement little bit\nwrong it should be probability of Sunny\ngiven yes now go ahead and calculate go\nahead and calculate what is probability\nof no given sunny and hot so here you\nhave probability of no multiplied by\nprobability of Sunny given\nno multiplied by probability of hot\ngiven\nno divided by probability of Sunny\nmultiplied by probability of heart this\nwill get cancelled denominator is a\nconstant guys this is a constant so what\nis probability of no so probability of\nno is nothing but 5 by4 so I will write\nover here 5 by4 multiplied by\nprobability of Sunny given no what is\nprobability of Sunny given no what is\nprobability of Sunny given no is nothing\nbut probability of Sunny given no is\nnothing but 3x 5 so here I'm going to\nget 3x 5 multiplied probability of H\ngiven no that is nothing but 2x 5 so 2x\n5 is here 3x 5 is there five and five\nwill get cancelled 2 1 2 7 and then I'm\ngetting 3x 35 which is nothing but\ncalculator uh if I'm actually getting\nthree ID by 35 it's nothing but\n857 I will write it down again\nprobability of yes given Sunny comma hot\nwhich is my independent feature is\nnothing but\n031\n031 and this is probability of no given\nSunny comma hot 85 now we'll try to\nnormalize this 85 + Point divided by 031\n+ 085 73 this is nothing but 73% and\nhere I can basically say 1 -73 which is\nmy27 which is nothing but 27% if the\ninput comes as sunny and hot if the\nweather is sunny and hot what will the\nperson do whether he will play or not\nthe answer is no okay now my next\nquestion will be that if your new data\nis overcast and Mild now tell me what\nwill be the probability using name bias\nnow you can add any number of features\nlet's say that I will say that okay\nlet's let's say that I will I will\nprobably say we can consider humidity\nmind wind also you basically create this\nkind of table to find it out but this\nwill be an assignment just do\nit overcast and Mild if it is with\nrespect to NB try to solve it so the\nsecond algorithm that we are going to\ndiscuss about is something called as KNN\nalgorithm KNN algorithm is a very simple\nproblem statement okay which can be used\nto solve both classification and\nregression so KNN basically means K\nnearest neighbor let's first of all\ndiscuss about classification problem\nnumber one classification problem let's\nsay that I have a binary classification\nproblem which looks like this I have two\ndata points like this one and this is\nanother one suppose a new data point\nsuppose a new data point which comes\nover\nhere then how do I say that whether this\nbelongs to this category or whether it\nbelongs to this category if I probably\ncreate a logistic regression I may\ndivide a line but in this particular\nscenario how do we Define or how do we\ncome to a conclusion that\nwhether this will belong to this\ncategory or this category so for here we\nbasically use something called as K\nnearest neighbor let's say that I say\nthat my K value is five so what it is\ngoing to do it is going to basically\ntake the five nearest closest point\nlet's say from this you have two nearest\nclosest point and from here you have\nthree nearest closest point so here we\nbasically see from the distance the\ndistance that which is my nearest point\nnow in this particular case you see that\nmaximum number of points are from Red\ncategories from Red from Red categories\nI'm getting three points and from White\ncategories I'm getting two points now in\nthis particular scenario maximum number\nof categories from where it is coming we\nbasically categorize that into that\nparticular class just with the help of\ndistance which all distance we\nspecifically use we use two distance one\nis ukan distance and the other one is\nsomething called as Manhattan distance\nso ukan and Manhattan distance now what\ndoes ukan distance basically say suppose\nif this is your two points which is\ndenoted by X1 y1\nX2 Y2 ukine distance in order to\ncalculate we apply a formula which looks\nlike this X2 - X1 s + Y2 - y1 s whereas\nin the case of magetan distance suppose\nthis are my two points then we calculate\nthe distance in this way we calculate\nthe distance from here then here right\nthis is the distance we calculate we\ndon't calculate the hypothenuse distance\nso this is the basic difference between\nukan and magetan distance now you may be\nthinking Chris then fine that is for\nclassification problem for regression\nwhat do we do for regression also it is\nvery much simple suppose I have all the\ndata points which looks like this now\nfor a new data point like this if I want\nto calculate then we basically take up\nthe nearest Five Points let's say my K\nis five k is a hyper parameter which we\nplay now suppose let's say that K it\nfinds the nearest point over here here\nhere here and here so if we need to find\nout the point for this particular output\nwith respect to the K is equal to 5 it\nwill try to calculate the average of all\nthe points once it calculates the\naverage of all the points that becomes\nyour output so regression and\nclassification that is the only\ndifference because this K is actually an\nhyper parameter we try with K is equal\nto 1 to 50 and then we probably try to\ncheck the error rate and if the error\nrate is less then only we select the\nmodel now two more things with respect\nto K nearish neighbor K nearest neighbor\nworks very bad with respect to two\nthings one is outliers and and one is\nimbalanced data set now if I have an\noutlier let's say I have an outlier over\nhere this is one of my categories like\nthis and this is my another category\nlet's consider that I have some outliers\nwhich looks like this now if I'm trying\nto find out the point for this you can\nsee that the nearest point is basically\nblue only and it belongs to the blue\ncategory but because this outlier you\nknow it'll consider that the nearest\nneighbor is this so then this will be\nbasically treated in this group only\nformula for Manhattan distance it uses\nmodulus X2 - X1 + Y2 - y1 mode X2 - X1\nY2 - y1 uh this was it from my side guys\nand yes I've also made detailed videos\nabout whatever topics we have discussed\ntoday you can directly go and search for\nthat particular\ntopic so this is the agenda of this\nsession we will try to complete this all\nthings again here we are going to\nunderstand the mathematical equations\nand all uh so today's session we are\nbasically going to discuss about uh\ndecision tree okay and uh in this\nsession we are going to basically\nunderstand what is the exact purpose of\ndecision tree with the help of decision\ntree you are actually solving two\ndifferent problems one is regression and\nthe other one is\nclassification so we'll try to\nunderstand both this particular part\nwell we will take a specific data set\nand try to solve those problems now\ncoming to the decision tree one thing\nyou need to understand I'll say that if\nage is less than 8 let's say I'm writing\nthis condition if age is less than or\nequal to 18 I'm going to say print go to\ncollege here I'm printing print college\nand then I'll write else if age is\ngreater than 18 and pag is less than or\nequal to 35 I'll say print work then\nagain I'll write else if age is let me\nlet me put this condition little bit\nbetter then I'll write here L if if age\nis greater than 18 and age is less than\nor equal to 35 I'm going to say print\nwork basically people needs to work in\nthis age else I'm just going to consider\nprint retire so here is my ifls\ncondition over here now whenever we have\nthis kind of nested if Els condition\nwhat we can do is that we can also\nrepresent this in the form of decision\ntrees we'll also we can actually form\nthis in the form of decision and the\ndecision tree here first of all we will\nhave a specific root node let's say this\nis my root node now in this root node\nthe first condition is less than or\nequal to 18 so here obviously I will be\nhaving two conditions saying that if it\nis less than or equal to 18 and one\ncondition will be yes one condition will\nbe no so if this is yes and if this is\nno right if this condition is true that\nbasically means we'll go in this side if\nit is true then here we will basically\nhave something like college so this is\nyour Leaf node similarly when I have no\nokay no no in this particular case we\nwill go to the next condition in this\nnext condition I will again create a\nnode and I'll say that okay this is less\nthan 18 and greater than sorry less than\nor equal to 35 so if this is also there\nthen again I'll have two conditions\nwhich is basically yes or no now when I\ncreate this yes or no over here you'll\nbe able to see that basically means here\nagain two condition will be there if it\nis yes I will say print work so this\nwill again be my leaf\nnode and again for no again I will do\nthe further splitting which is retire so\nhere you can see that this entire\nalgorithm this entire code that I have\nactually written you can see that it has\ngot converted to this kind of\ntrees where you specifically able to\ntake decisions yes or no so can we solve\na classification\nproblem sorry this is greater than 18\nagain if it is greater than 18 and less\nthan or 35 so can we solve a\nregression and a classification problem\nregression and classification problem\nusing this decision trees by creating\nthis kind of\nnodes so in short whenever we talk about\ndecision\ntrees whenever we talk about decision\ntrees\nyou will be seeing that decision trees\nare nothing but decision trees are\nnothing but by using this nested if El\ncondition we can definitely solve some\nspecific problem statement but here in\nthe visualized way we will specifically\ncreate this decision tree in the form of\nnodes now you need to understand that\nwhat type of maths we will probably use\nokay so let's do one thing let's take a\nspecific data set which I will\ndefinitely do it over here in front of\nyou\nokay and we will try to solve this\nparticular data set and this will\nbasically give you an idea like how we\ncan probably solve these problems so uh\nlet me just open my snippet tool so this\nis my data set that I have let's\nconsider that I have this specific data\nset now this data set are pretty much\nimportant because this probably in\nresearch papers also probably people who\nhave come up with this algorithm they\nusually take this they take this thing\nbut but right now this particular\nproblem statement if I talk about this\nis a classification problem statement\nokay but don't worry I will also help\nyou to explain I'll also explain you\nabout regression also how decision tree\nregression will definitely work so let's\ngo ahead and let's try to understand\nsuppose if I have this specific problem\nstatement how do we solve this this is\nmy output feature play tennis yes or no\nokay whether the person is going to pay\ntennis or not yesterday or there after\nyesterday or whenever you want so if I\nhave this input features like Outlook\ntemperature humidity and wind is the\nperson going to play tennis or not this\nis what my model should predict with the\nhelp of decision tree so how decision\ntree will work in this particular case\nfirst of all let's consider any any any\nspecific uh feature let's say that\nOutlook is my feature so this will be my\nfirst\nfeature which is specifically Outlook\nnow just tell me how many are basically\nhaving no and how many are basically\nhaving yes in the case of Outlook there\nyou'll be able to find out there are\nnine yes see 1 2 3 4 5 6 7 8 9 and how\nmany NOS are there 1 2 3 4 5 I think 1 2\n3 4 5 so nine yes and five NOS what we\nare going to do in this specific thing\nnow we have N9 yes and five Nos and the\nfirst node that I have actually taken\nis basically Outlook so Outlook feature\nnow just try to find out we are focusing\non this specific feature now in this\nfeature how many categories I have I\nhave one Sunny category you can see over\nhere I have Sunny one category then I\nhave another category called as\novercast then I have another category as\nrain so I have three unique categories\nSo based on these three categories I\nwill try to create three nodes so here\nis my one node here is my second node\nhere is my third node so these are my\nthree categories so this category is\nbasically called as Sunny this category\nis basically called as overcast and this\ncategory is basically called as rain\nbased on these three categories so I'm\nsplitting it now just go ahead and see\nin Sunny how many yes and how many no\nare there how many yes with respect to\nSunny are there see in sunny I have two\nNOS see one and two no uh one more no is\nthere three NOS so here you can see this\nis my one no then this is my two no this\nis my three no and yes are two so this\none and this one so how many total\nnumber of yes so here you can see that\nthere are 1 2 2 yes and three no let's\nsay that I have randomly selected one\nfeature which is Outlook why can't I\nwhen like see it is up to it it is up to\nthe decision tree to select any of the\nfeature here I have specifically taken\nOutlook later on I'll explain why it it\ncan basically select how it selects the\nfeature okay I'll I'll talk about it\ndon't worry so in the Outlook we have\ntwo yes sorry in the case of Sunny we\nhave two yes and three NOS now the next\nthing is that let's go and see for\novercast in overcast I have 1 yes uh 2s\num 3s and 4 yes I don't have any no in\novercast so over here my thing will be\nthat four yes and Zer Nos and then\nfinally when we go to the Rain part see\nin Rain how many features are there in\nrain if you go and probably see it how\nmany number of yes and NOS are there go\nand see in one one yes in row rain two\nyes then one no then again you have one\nyes and one no right so here you can\nbasically say that in rain in the case\nof rain if I take a as an example how\nmany number of yes and NOS are there it\nwill be 3 yes and two\nNOS understand understanding\nalgorithm then everything will you'll be\nable to understand now let's go ahead\nand try to cease for sunny sunny\ndefinitely has 2 yes and three NOS this\nhas four yes and zero NOS here you have\nthree Y and two NOS now if I probably\ntake overcast here you need to\nunderstand understand about two things\none is pure\nsplit and one is impure split now what\ndoes pure split basically mean pure spit\nbasically means that now see in this\nparticular scenario in overcast in\novercast I have either yes or no so here\nyou can see that I have four yes and Zer\nNOS so that basically means this is a\npure split anybody tomorrow in my data\nset if I just take this Outlook feature\nsuppose in one day in day 15 the Outlook\nis Outlook is basically overcast then I\nknow directly it is the person is going\nto play so this part is already created\nand this node is called as pure\nnode understand this why it is called as\npure node because either you have all\nYes or zeros NOS or zero yes or all NOS\nlike that in this particular case I have\nall yes so if I take this specific path\nI know that with respect to overcast my\nfinal decision which is yes it is always\ngoing to become yes so this is what it\nbasically says so I don't have to split\nfurther so from here I will probably not\nsplit I will definitely not split more\nbecause I don't require it because I\nhave it is a pure leaf node okay you can\nalso say that this is a pure leaf node\nso I'm just going to mention it again\nthis one I'm specifically talking about\nnow let's talk about sunny in the case\nof Sunny you have two yes and three NOS\nso this is obviously impure so what we\ndo we take next feature and again how do\nwe calculate that which feature we\nshould take next I'll discuss about it\nlet's say that after this I take up\ntemperature I take up temperature and I\nstart splitting again since this is\nimpure okay and this split will happen\nuntil we get finally a pure split\nsimilarly with respect to rain we will\ngo ahead and take another feature and\nwe'll keep on splitting unless and until\nwe get a leaf node which is completely\npure I hope you understood how this\nexactly work now two questions two\nquestions is that Kish the first thing\nis that how do we calculate this\nPurity and how do we come to know that\nthis is a pure split just by seeing\ndefinitely I can say I can definitely\nsay by just seeing that how many number\nof yes or NOS are there based on that I\ncan def itely say it is a pure split or\nnot so for this we use two different\nthings one is\nentropy and the other one is something\ncalled as guine coefficient so we will\ntry to understand how does entropy work\nand how does Guinea coefficient work in\ndecision tree which will help us to\ndetermine whether the split is pure\nsplit or not or whether this node is\nleaf node or not then coming to the\nsecond thing okay coming to the second\nthing one is with respect to Purity\nsecond thing your first most important\nquestion which you had asked why did I\nprobably select Outlook how the features\nare selected and here you have a topic\nwhich is called as Information Gain and\nif you know this both your problem is\nsolved so now let's go ahead and let's\nunderstand about entropy or guinea\ncoefficient or Information Gain entropy\nor guine coefficient oh sorry Guinea\ncoefficient I'm saying guine impurity\nalso you can say over here\nI'll write it as guine impurity not\ncoefficient also I'll just say it as\nGuinea impurity but I hope everybody is\nunderstood till here let's go ahead and\nlet's discuss about the first thing that\nis\nentropy how does entropy work and how we\nare going to use the formula so entropy\nhere I will just write guine so we are\ngoing to discuss about this both the\nthings let's say that the entropy\nformula which is given by I will write h\nof s is equal to so h of s is equal to\nminus P plus I'll talk about what is\nminus what is p plus log base 2 p\n+- p\nminus log base 2 p minus so this is the\nformula and in guine impurity the\nformula is 1 minus summation of I equal\n1 2 N p² I even talk about when you\nshould use guine impurity when you\nshould not use guine impurity\nwhen you should use entropy you know by\ndefault the decision tree regression or\nclassific sorry decision tree\nclassification uses Guinea impurity now\nlet's take one specific example so my\nexample is that I have a feature one my\nroot node I have a feature one which is\nmy root node and let's say that in this\nroot node I have six yes and three NOS\nvery simple let's say that this has two\ncategories and based on this two\ncategories of split has happened that is\na C1 let's say in this I have 3 S3 Nos\nand here I have 3 s0 Nos and this is my\nsecond category always understand if I\ndo the sumission 3s and 3s is 6s see\nthis this sumission if I do 3 + 3 is\nobviously 6 3 + 0 is obviously so this\nyou need to understand based on the\nnumber of root nodes only almost it'll\nbe same now let's go ahead and let's\nunderstand how do we Cal calculate let's\ntake this example how do we calculate\nthe entropy of this so I have already\nshown you the entropy formula over here\nnow let's understand the components I\nwill write h of s is equal to minus sign\nis there what is p+ p+ basically means\nthat what is the probability of yes what\nis the probability of yes this is a\nsimple thing for you all out of this\nwhat is the probability of yes yes out\nof this so obviously how you'll write if\nyou want to find out the probability of\nyes out of this see when I say plus that\nbasically means yes when I say minus\nthat basically means no so what is the\nprobability of yes so it is be nothing\nbut yes plus and minus are specifically\nfor binary\nclass this can be positive negative so\nthe probability with respect to yes can\nI write 3x 3 only for this what is the\nprobability out of this total number of\nthis is there 3x3 similarly if I go and\nsee the next term log to the base 2 p+\nso again if I go ahead and write over\nhere log to the base 2 p+ p+ is again\n3x3 so then again we have minus and this\nis now P minus what is p minus 0 by 3\nlog base 2 0 by 3 this obviously will\nbecome zero this will obviously become 0\nbecause 0 divid by anything is zero what\nwill this be 1 log to the base 1 what is\nthis this is nothing but zero log to the\nbase 1 is nothing but zero tell me\nwhether this is a pure split or impure\nsplit so this is a pure split whenever\nwe have a pure split the answer of the\nentropy is going to come to zero so here\nI'm going to Define one graph\nthis is H of s and let's say this is p+\nor P minus if my probability of plus see\nwhen I say probability of plus is 0.5\nwhat will be probability of minus it\nwill also be 0. five right because it's\njust like P is equal to 1 - Q right if p\nis .5 then Q will be 1 - P same thing\nright so when it\nis5 obviously my h of s will be 1 let's\nsay so this is this is the graph that\nwill basically get formed let's go ahead\nand try to calculate the entropy of this\nguys what will be the entropy of this\nnode so here I'm going to just make a\ngraph h of s minus what is p+ p+ is\nnothing but 3x 6 log base 2 3x 6\nminus three no are there 3x 6 log base 2\n3x 6 so if you compute this\nlog base 2 to the^ of 1 if you do the\ncalculation here I'm actually going to\nget one so when I'm getting one when I'm\nactually getting one when you have three\nyes and three NOS what is the\nprobability it is 50/50% right so when\nyour p+ is5 that basically means your h\nof s is coming as one so from this graph\nyou can see that I'm getting one if this\nis zero this is one this is zero and\nthis is one I hope everybody is able to\nto understand guys 0o and one if your p+\nis\nzero or if your p+ is one that basically\nmeans it becomes a pure split so in h of\ns you are going to get\nzero so always understand your entropy\nwill be between 0 to\n1 if I have a impure this is a\ncompletely impure split because here you\nhave 50% probability of getting yes 50%\nprobability of getting no h ofs is\nentropy this is entropy for the sample H\nofs notation that I'm using is H ofs so\nif whenever the split is happening the\nfirst thing is done the purity test the\npurity test is done with the help of\nentropy right now I'll also show guinea\nguinea impurity don't worry so with the\nentropy you'll be able to find if I am\ngetting one that basically means it is a\nimpure split and if I'm getting zero it\nis pure split so this is the graph okay\nthis is the graph and this graph is\nbasically the entropy graph again\nunderstand if your probability of\ngetting yes or no is 0.5 that basically\nmeans 50/50 is there 3s and three NOS\nthen your entropy is going to be 1 h of\ns if your probability is completely one\nthat basically means either you're\ngetting completely yes or completely no\nso your your entropy will be zero that\nbasically means it is pure split so in\nthe case of probability .5 you're\ngetting plus one then it'll keep on\nreducing now let's go ahead and let's\ntry to understand so here you have\nunderstood about purity test definitely\nyou'll use entropy try to find out\nwhether it is pure or impure if it is\nimpure you go ahead with the further\nshift further division of the categories\nagain you take another feature divide it\nbecause here from this two which split\nyou will do further you will do this\nsplit as further if you are getting 6 6\nis this specific value then you probably\ngo and draw over here this is your\nentropy if your probability is here\nwhich\nis.3 then you will go here and create\nthis this may be0 4 or3 something like\nthis it will be between 0 to 1 let's go\nahead and discuss about the second issue\nI hope everybody is discussed about we\nhave discussed about checking the pure\nsplit or not and we have understood this\nmuch but the next thing is that okay\nfine chish this is very good we have\nexplained well I know many people will\nsay that but there are some people I\ncan't help let's say that I have some\nfeatures okay now coming to the second\nproblem how do we consider which node to\ncap which which feature to take and\nsplit because here I may have one one\nsplit so again let's see that what is\nthe second problem which feature to take\nto split right this is the second\nproblem that we are trying to solve\nlet's say that I have one feature one\nover here and I have two categories\nlet's say this is there C1 and C2 here\nlet's say that I have 9 years 5 Nos and\nthen I have 6 years 2 NOS here I have\nbasically three yes and three NOS let's\nsay and in my data set I have features\nlike F1 FS2 F3 now let's say that\nanother split I can actually start with\nfeature two also and in feature two I\nmay have probably three categories like\nC1 C2 C3 so with respect to the root\nnode and all the other features because\nafter this also I may have to split\nright I may have to take another feature\nand keep on splitting right based on the\nPure or impure split how do I decide\nshould I take fub1 first or F2 first or\nF3 first or any other feature first how\nshould I decide that which feature\nshould I take and probably do the split\nthat is the major question so for this\nwe specifically use something called as\nInformation Gain so here I'm just going\nto say here we basically use Information\nGain now what is this Information Gain\nI'll talk about it so Information Gain\nfirst of all I will write the formula we\nbasically write gain with sample first\nwith feature one I will compute so first\nwith feature one I will compute suppose\nthis is my first split of my data and\nprobably I'm Computing over here this\ncan be written as h of s I'll discuss\nabout each and every parameter don't\nworry summation of V belong to values s\nof V don't worry guys if you have not\nunderstood the formula I will explain it\nthen the sample size H of SV I'll\ndiscuss about each and every parameter\nlet's say that I'm taking this feature\none split I have you have already seen\nwhat is feature one so this is my\nfeature one I have two categories C1 C2\nthis has 9 yes 5 NOS this has 6s and two\nNos and this has 3 yes and three NOS now\nI will try to calculate the information\ngain of this specific split now I will\ngo ahead and probably take this up now\nsee over here we'll try to understand\nwhat is this now if I want to compute\nthe gain of s of F1 first is first first\nthing that I need to find out is H of s\nnow this h of s is specifically of the\nroot node so I need to first of all\ncalculate what is h of s h ofs is\nnothing but entropy entropy of the root\nnode so if I want to compute the entropy\nof the node node tell me how should I\ncompute h of s is equal to minus p + log\nbase 2 p+ calculate guys along with me -\nP minus log base to P minus so I hope\neverybody knows this so here I'm going\nto compute by what is ability of plus\nover here in this specific root node it\nis nothing but 9 by4 then I have log\nbase 2 again 9\nby4 then I have P minus what is p minus\n5x4 log base 2 5 by4 so this calculation\nI will probably get it as\n94 approximately equal to 94 just check\nit whether you're getting this or not\nagain you can use calculator if you want\nnow now I have definitely found out this\nthis is specifically for the root node\nnow let's see the next thing the next\nimportant thing which is this part what\nis s of v and what is s and what is h of\nSV now very important just have a look\neverybody see this graph okay see this\ngraph I will talk about h of SV first of\nall I'll talk about h of SV okay this\none this is the entropy of category one\nyou need to find and entropy of category\n2 you need to find so if I write h of SV\nof category 1 so what is category 1 for\nthis I'll write SC1 let's say I'm going\nto write like this quickly calculate the\nH of SV of this and this separately you\nneed to calculate so h of SV of C1 okay\nso here again you'll write - 6X 8 log\nbase 2 6X\n8us 2x 8 log base to 2x 8 I hope\neverybody knows this how we got it so h\nof SV basically means I'm going to\ncompute the entropy of this category and\nthis category so for that I will\nbasically write h of so here I will\nwrite - 6 by8 log base 2 6X 8 - 2x 8 log\nbase 2 2x 8 so if I get it I'm actually\ngoing to get 81 and similarly if I if I\ncalculate h of C2 quickly calculate how\nmuch you are going to get guys 6X 8 6X 8\nwith respect to this we need to find out\nso now we have all these values we'll\nstart equating them to this equation so\nhere we have finally gain of s comma\nfub1 so let's say that here I'm going to\nbasically add\n94 minus see minus summation of okay\nsummation of what is s s of V understand\ns of V basically means that how many\nsamples I have over here let's say for\ncategory one how many samples I have for\ncategory one over here simple if you\nreally want to just calculate it is\nnothing but eight and total number of\nsample is how much if I go and see over\nhere there are 9 years five NOS okay 9\nyears and five NOS that basically means\n14 total sample here you have eight\nsample Okay so this will become\n8x4 then you multiply by what see see\nfrom this equation you multiply by h of\nSV so h of SV is nothing but the entropy\nof category 1 so entropy of category 1\nis nothing but 81 plus then you go again\nback to the graph and try to see that\nfor C2 how much how many total number of\nsamples are there 3 + 3 is 6 so 6 by 14\nit will\nbecome multiplied by 1 right so this is\nyour entire thing so here after all the\ncalculation you are going to get\n0.041 so this is my gain with s comma F1\nso here I have got this value amazing I\ndid this with feature one only what\nabout feature two let's say that this\nwas my split for feature two and suppose\nI get the gain for S comma feature 2 as\n.51 if I get this now tell\nme in using which feature should I start\nsplitting first whether it should be\nfub1 or whether it should be FS2 based\non this value you know that over here\nthe gain the information gain of s comma\nF2 is greater than gain of s comma fub1\nso your answer is very much simple we\nwill definitely use feature 2 to start\nthe split the thing over here you are\ntrying to understand that if I really\nwant to select which feature to select\nto start my splitting then I have to\nbasically calculate the information gain\nand go throughout the all the paths and\nwhichever path has the highest\nInformation\nGain then we will select that specific\nthing now the question Rises Kish\nobviously this is good but you had\nwritten about guinea impurity what is\nthe purpose of that please explain us\nand why Guinea impurity is basically\nused so let me go ahead with guine\nimpurity I told that yes you can\nobviously\nuse you can obviously use entropy but\nwhy Guinea impurity so guine impurity\nformula which I have specifically\nwritten as 1 minus summation of IAL 1\n2 N\np² now what is this p² suppose let's say\nthat in my n n is the number of outputs\nright now how many outputs I have I have\ntwo outputs yes or no so I will expand\nthis 1 minus since this is summation I\nequal to 1 to n I'm basically going to\nbasically say that okay fine I will\nwrite probability of plus whole\nSquare uh plus probability of minus\nwhole Square so this is the formula for\nguinea impurity now you may be thinking\nokay fine the calculation will be\nobviously very much equal easy right\nsuppose if I have a node sorry if I have\na node which which has 2 yes two NOS now\nin this particular case how do I\ncalculate my this probability if I have\ntwo yes or two NOS suppose let's say\nthat I have a node over here which is my\nsplit and this is having two yes and two\nno so how do I calculate I will write 1\nminus what is probability of square 1X 2\nsquare sorry not 1 by two\nyeah 1X 2 squ + 1 by 2\nsqu right then I will say 1 by 1X 4 + 1X\n4 is nothing but 2x 4 which is nothing\nbut 1X 2 so I will be getting 0.5 now\nhere here you understand this is a\ncomplete impure split right if you have\nan impure split in entropy the output\nyou getting it as one whereas in the\ncase of Guinea impurity\nas Z sorry\n0.5 so if I go ahead with the graph that\nI probably had created here so my Guinea\nimpurity line will look something like\nthis so it will be looking something\nlike this for zero obviously I'll be\ngetting zero but whenever my probability\nof plus is 0.5 I'm going to get 0.5 over\nhere and that is the difference between\nGuinea\nimpurity and entropy but the re but you\nmay be seeing Kish when to use what now\nlet's understand that when to use Guinea\nand when to use entropy tell me guys if\nI consider this formula of guine\nimpurity and if I probably\nconsider if I consider entropy this\nformula where do you think more time\nwill take for execution for this\nparticular formula whether for entropy\nit will take or for guinea impurity it\nwill take more time where it will\nprobably take for the execution purpose\nsee understand decision tree is having a\nworst time complexity because if you\nhave 100 features probably you'll keep\non comparing by dividing many many\nfeatures then probably compute a\nInformation Gain like this if you have\njust 100 features so which is faster\nentrop\nor guine impurity understand in entropy\nyou have log function here you have log\nfunction here you have simple maths the\nmore amount of time out of entropy and\nguine impurity the more amount of time\nbasically is taken\nby\nentropy so if you have huge number of\nfeatures like 100 200 features and you\nare planning to apply decision Tre I\nwould suggest try to use Guinea impurity\nthen entropy if you have small set of\nfeatures then you can go ahead with\nentropy so over here definitely with\nrespect to fast Guinea is greater than\nentropy now let's go ahead and\nunderstand with respect to you may be\nthinking Kish okay fine you have\nbasically explained us about categorical\nvariables over here see over here you\nhave you have explained about\ncategorical variables what if I have\nnumerical feature let's say I have F1\nover here which is a numerical\nfeature I have an F1 feature which is\nnumerical feature and I may have values\nlet's say that I have sorted all the\nvalues over here okay let's say that I\nhave F1 and output okay so this F1 let's\nsay that I have values\nlike ass sorted order values I'm sorting\nthis features I'm basically doing this\nlet's say that initially I have this\nfeatures like this and let's say I have\nvalues like 2.3 1.3 4 5 7 3 let's say I\nhave this features now this is a\ncontinuous\nfeature this is a continuous feature so\nfor a continuous feature how probably\nthe decision tree entropy will be\ncalculated and the Information Gain will\nget calculated so here you'll be able to\nsee that I will first of all sort these\nvalues so in F1 the decision tree will B\nbasically first of all sort this values\nso I have 1.3 then you have 2.3 then you\nhave four then you have three three then\nyou have four then you have five and\nthen you have six now whenever you have\na continuous feature so how the\ncontinuous feature will basically work\nin this case first of all your decision\ntree node will say\nthat we'll take this one only one first\nrecord and say that if it is less than\nor equal to 1.3\nokay if it is less than or equal to 1.3\nso you here you'll be getting two\nbranches yes or no so yes and no\ndefinitely your output over here will be\nput over here right and then for the no\nhere you'll be having another node over\nhere how many number of Records you'll\nbe having in this particular case you'll\nbe having one record in this particular\ncase you will be having around five to\nsix records and here also you'll be able\nto see right how many yes and NOS are\nthere definitely this will be a leaf\nnode so in the first instance they will\ngo ahead and calculate the information\ngain of this then probably once the\nInformation Gain Is got then what\nthey'll do they will take the first two\nrecords and again create a new decision\ntree let's say that this will be my\nsuggestion where they'll say it is less\nthan or equal to 2.3 so I will get one\nand one over here so in this now you'll\nbe having two records which will\nbasically say how many yes and no are\nthere and remaining all records will\ncome over here then again Information\nGain will be computed here then again\nwhat will happen they'll go to the next\nrecord then then again they'll create\nanother feature where they'll say less\nthan or equal to three and they will\ncreate this many nodes again they'll try\nto understand that how many yes or no\nare there and then they'll again compute\nThe Information Gain like this they'll\ndo it for each and every record and\nfinally whichever Information Gain is\nhigher they will select that specific\nvalue in that feature and they'll split\nthe node so in a continuous feature\nwhenever you have a continuous feature\nthis is how it will basically have and\nthen it will try to compute who is\nhaving the highest Information Gain the\nbest Information Gain will get selected\nand from there the splitting will\nhappen now let's go ahead and understand\nabout the next topic is that how this\nentirely things work in decision tree\nregressor because in decision tree\nregressor my output is an continuous\nvariable so suppose if I have one\nfeature one feature two and this output\nis a continuous feature it will be\ncontinuous any value can be there so in\nthis particular case how do I split it\nso let's say that f1c feature is getting\nselected now in this f1c feature what\nvalue will come when it is getting\nselected first of all the entire mean\nwill get calculated of the output mean\nwill get calculated so here I will have\nthe mean and here here the cost function\nthat is used is not Guinea coefficient\nor guinea impurity or entropy here we\nuse mean squared\nerror or you can also use mean absolute\nerror now what is mean squared error if\nyou remember from our logistic linear\nregression how do we calculate 1 by 2 m\nsummation of I = 1 to n y hat minus y\nwhole Square y hat of i y - y whole\nSquare this is what is mean square error\nso what it will do first based on F1\nfeature it will try to assign a mean\nvalue and then it will compute the MSE\nvalue and then it'll go ahead and do the\nsplitting now when it is doing splitting\nbased on categories of continuous\nvariable I will be having different\ndifferent categories now in this\ncategories what will happen after split\nsome records will go over\nhere then I will be having a mean value\nof this over here\nthat will be my output and then again\nthe MSC will get calculated over here as\nthe msse gets reduced that basically\nmeans we are reaching near the leaf\nnote and the same thing will happen over\nhere so finally when you follow this\npath whatever mean value is present over\nhere that will be your output this is\nthe difference between the decision tree\nregressor and the classifier here\ninstead of using entropy and all you use\nmean squar error or mean absolute error\nand this is the formula of mean square\nerror now let's go to the one more topic\nwhich is called as the hyperparameters\ntell me decision tree if I keep on\ngrowing this to any depth what kind of\nproblem it will face regressor part you\nwant me to explain okay let's\nsee okay let's let's do the\nregression decision\ntree\nregressor let's say I have feature F1\nand this is my output let's say I have\nvalues like 20 24 26 28 30 and this is\nmy feature one with category one\ncategory one let's\nsay some categories are there let's say\nI have done\nthe division by\nF1 that is this feature initially tell\nme what is the mean of this that mean\nvalue will get assigned over here then\nusing msse that is mean squar error here\nyou will try to calculate suppose I get\nan msse of some 37 47 something like\nthis and then I will try to split this\nthen I will be getting two more nodes or\nthree more nodes it depends then that\nspecific nodes will be the part of this\nagain the mean will change again the\nmean will change over here suppose this\ntwo is there this two records goes here\nright then again MC will get calculated\nI'm just taking as an example over here\njust try to assume this thing now if I\ntalk about hyper parameters see this is\nwhat is the formula that gets applied\nover MSC now let's see in this hyper\nparameter always understand decision\ntree leads to overfitting because we are\njust going to divide the nodes to\nwhatever level we want so this obviously\nwill lead to\noverfitting now in order to prevent\noverfitting we perform two important\nsteps one is post pruning and one is\npre- pruning so this two post pruning\nand pre pruning is a condition let's say\nthat I have done some\nsplits I have done some splits let's say\nover here I have seven yes and two\nno and again probably I do the further\nsplit like this now in this particular\nscenario you know that if 7 yes and two\nNOS are there there is a maximum there\nis more than 80% chances that this node\nis saying that the output is yes so\nshould we further do more\npruning the answer is no we can close it\nand we can cut the branch from here this\ntechnique is basically called as post\npruning that basically means first of\nall you create your decision tree then\nprobably see the decision tree and see\nthat whether there is an extra Branch or\nnot and just try to cut it there is one\nmore thing which is called as\npre-pruning now pre-pruning is decided\nby hyperparameters what kind of hyper\nparameters you can basically say that\nhow many number of decision tree needs\nto be used not number of decision tree\nsorry over here you may say that what is\nthe max\ndepth what is the max depth how many Max\nLeaf you can\nhave so this all parameters you can set\nit with grid SE\nCV and you can try it and you can\nbasically come up with a pre- pruning\ntechnique so this is the idea about\ndecision tree uh regressor yes yes it is\npossible your guinea value will be one\nno this graph is there\nno Guinea value are you talking about\nthis Guinea entropy it will not be one\nit will always be between 0\nto.5 so the first thing first as usual\nwhat we should do we should import the\nlibraries so here I will go ahead and\nimport the librar so I'll say\nimport pandas as NP PD import matplot\nli. pyplot as PLT\nuh\nimport so this basic things I have with\nme so I will go and take any data set\nthat I want from SK\nlearn. data sets import let's say that\nI'm going to take load Iris data set and\nthen I'm going to upload the iris data\nset so I'm going to write load Iris\nthere is my Iris data set then the next\nstep uh once you get your iris data set\nso this is my iris. dat\nokay these are all my features the four\nfeatures will be there these four\nfeatures are petal length petal width\nSLE length and SLE width this is my\nindependent features then if I really\nwant to apply\nfor classifier so decision tree\nclassifier so I can first of all import\nfrom\nskarn do tree import decision let's see\nwhere decision tree present in a scalon\ndecision tree\nclassifier the name is absolutely fine\nbut I was not getting over here\nso so this is got no module SK okay SK\nskar\nskn learn so here you have\nclassifier right now I'm just going to\noverfit the data then I'll probably show\nyou how you can go ahead with uh\npruning so by default what are the\nparameters over here if you probably go\nand see in in the classifier over here\nyou have Criterion see this the first P\nparameter is Criterion by default it is\nGuinea then you have Splitter Splitter\nbasically means how you're going to\nsplit and there also you have two types\nbest and random you can randomly select\nthe features and do it okay you should\nalways go with\nbest max depth is a hyper parameter\nminimum sample lift is a hyper parameter\nMax Fe features how many number of\nfeatures we are going to take in order\nto fix that that is also an hyper\nparameter so all these things are hyper\nparameter okay so I will just by default\nexecuted whatever is giving me in\ndecision tree and the next thing that\nI'm actually going to do is create a\ndecision tree so for this I will be\nusing plot. fig size plot. figure inside\nfigure I have this fix\nsize okay and I will probably show in\nsome better figure size so that\neverybody body will be able to see it so\nhere let me say that I'm going to take\nan area of\n1510 and then probably I'm going to say\ntree Dot\nPlot and here I'm going to say a\nclassifier and it should be filled the\ncoloring should be filled with this so\ntree sorry Tre Tre Tre Tre\nTre it should be classifi tree. plot\nokay I have to also import uh tree so I\nhave to basically import tree so from SK\nlearn\nimport three again I'm getting\nerror has no attribute plot\nwhy let me just see the documentation\nguys so this plot function is like plot\nuncore tree dot tab plot _ tree now what\nis the error we are getting okay not\nfitted yet\nsorry so I'm going to say\nclassifier do fit on data what data\niris.\ndata and then I'm going to fit with Iris\ndot\nTarget so once this is done I think now\nit will get\nexecuted so this is how your graph will\nlook like guys so here you can see this\nis how your graph looks like now if I\nshow you the graph over here see you can\nsee some amazing things over here three\noutputs are actually there in this when\nyou see in this left hand side this\nbecome a leaf node so this first one is\nprobably vers color uh versol flower\nokay if you go on the right hand side\nhere you can see 50/50 is there so based\non one feature based on one feature here\nyou'll be able to see that you are\ngetting a leaf node based on another\nBranch here you are getting\n05050 so again you have two more\nfeatures getting splitted over here so\nhere you have 495 here you have\n471 do we require this split anybody\ntell me from here do we require any any\nmore split just try to think this is\nafter post pruning I want to find out\nwhether more splits are required or not\nnow in this particular case you see this\nafter this do you require any\nsplit you do not require right here you\nare basically getting 47 and one I guess\nafter this also you require no split\nunderstand this so this is basically\npost pruning so you can then decide your\nlevel and probably do it gu value is\nmore than\n0.5 okay this side H this is coming as\n0.5 greater than 0.5 it should not had\nhere it is\n0.5 no maximum .5 can come 0 to.5 only\nshould come I don't know why this is\ncoming as 667\nI'll have a look onto this guys but\nanywhere you see other than that you're\neverywhere you're getting less\nthan5 the plotting graph is very much\neasy you use SK learn import tree then\nyou basically do this get classify and\nfield is equal to true and you can just\ndo this so the agenda let me Define the\nagenda what all things are there first\nwe'll understand about\nemble techniques in this assemble\ntechniques we are basically going to\ndiscuss about what is the difference\nbetween\nbagging and boosting\nsecond what we are basically going to\ndiscuss about is so uh the agenda of\nthis session is emble techniques bagging\nand boosting then we are probably going\nto cover random forest and then probably\nwe will try to cover adab boost and if I\nhave more energy I will also try to\ncover XG boost so all this Al lthms\nwe'll discuss about it so let's go ahead\nand let's start the\ntopics the first topic that we are going\nto discuss is about emble\ntechniques now what exactly is emble\ntechniques and we are going to discuss\nabout it okay so emble techniques what\nexactly is emble techniques till now we\nhave solved two different kind of\nproblem statement one is\nclassification and regression and you\nhave learned about different different\nalgorithms like uh linear regression\nlogistic regression we have discussed\nabout KNN we have discussed about\nyesterday what disc what did we discuss\nabout n bias different different\nalgorithms we have already finished now\nwith respect to classification\nregression Problem whatever algorithm we\nare discussing there was only one\nalgorithm at a time we were discussing\none algorithm at a time we are\ndiscussing and we are trying to either\nsolve a classification or a regression\nproblem now the next thing is over here\nis that can we use multiple algorithms\nmul multiple algorithm to solve a\nproblem multiple algorithms basically\nmeans can we I'll just talk about it\nokay now the if I ask this specific\nquestion can we use multiple algorithms\nto solve a problem at that point of time\nI will definitely say yes we can because\nwe are going to use something called as\nemble techniques there now what this\nemble techniques is okay so emble\ntechniques in emble techniques we\nspecifically use two different ways one\nis one one way is that we specifically\nuse and the other one I'll just go to\nwrite it over here so one that we\nbasically use is something called as\nbagging technique and the other one we\nspecifically use is something called as\nboosting technique so in bagging\nTechnique we what exactly we can do and\nin boosting technique what we can\nactually do and how we are combining\nmultiple models to solve a problem so\nlet's first of all discuss about bagging\nnow how does bagging work let's say that\nI have a specific data set so this is my\ndata set with uh with features rows\ncolumns everything like this I have this\nspecific data set just imagine I have\nmany many features over here like this\nfub1 F2 F3 and probably I have my output\nso this is my data set D let's consider\nit now what we do in bagging is that we\ncreate models and this model can be\nanything it can be logistic it can be\nlinear for a classification problem\nlet's say that this is logistic model so\nthis is my model M1 let's say I have\nanother model M2 then I may have another\nmodel M3 let's say that this is\nlogistic and this is probably the other\nmodel which is like decision tree and\nthen probably we use this model as KNN\nclassification and this model can again\nbe decision tree it's fine let's use\nanother decision tree so now here you\ncan see that we have used so many models\nokay so many models are there now with\nrespect to this particular model what I\nwill do is that the first step that I\nwill do from this particular data set I\nwill just take up some rows so I'll\nbasically do row\nsampling and I'll take a row sampling of\nD Dash D Das basically means this D Das\nis always less than D some of the rows\nI'll push it to M1 okay I can also use n\nfine so what I'll do is that some of the\nrows I'll push it to model one this\nmodel one will be training let's say\nthat for out of this 10,000 record th000\nrows I'm actually doing a row sampling\nof th rows and giving it to M1 to train\nit then what I'm actually going to do\nover here I'm basically going to give\nthis specific model M2 and again I'm\ngoing to do row row sampling and I'm\nagain going to sample some of the rows\nand give it to model two and again\nremember some of the rows may get\nrepeated from this D Dash to next dble\nDash similarly I will do row sampling\nand give it to this and again I may have\nd triple Dash and D4 Dash so different\ndifferent different different rows data\npoints when I say row sampling basically\nI'm talking about data points different\ndifferent data points I will give it to\nseparate separate model and this model\nwill specifically train when I say D\nDash that basically means uh suppose I\nsay th 10,000 are my total number of\ndata points when I say D Dash This D\nDash may be th000 points then D Double\nDash may be another th000 points and\nsome of the rows may get repeated over\nhere dle Dash here also I can basically\nuse so here specifically row sampling\nwill be used now when I have this many\nspecific each and every model will be\ntrained with different kind of data now\nhow the inferencing will happen for the\ntest data so first thing first let's say\nthat I'm going to get a new test data\nover here now new test data will be\npassed to M1 and this M1 suppose it\ngives zero as my output suppose let's\nsay that I'm doing a binary\nclassification it gives a Zer as an\noutput so this is my output of zero next\nM2 for the new test data gives one M3\ngives one and M4 also gives one as the\nthe output now in this particular case\nin this particular case what will happen\nnow you can see over here it's simple\nwhat what do you think the output may be\nin this particular case now M1 has\npredicted for this particular test data\nas zero the model M2 has predicted 1 M3\nhas predicted 1 and M4 has predicted one\nso finally all these outputs are going\nto get\naggregated are going to get aggregated\nand a simple thing that gets applied is\nmajority voting majority voting so tell\nme what will be the output for with\nrespect to this the output will\nobviously be one because the majority\nvoting that you can see three people are\nbasically saying it as one so my output\nover here will be one okay this is the\nconcept of bagging wherein you are\nproviding different different rows with\nprobably all the features in this case\nand giving it to different different\nmodel again which is a classification\nmodel and then finally you are combining\nthem based on majority voting and you're\ngetting the answer as one so this step\nis called as bootstrap aggregator that\nbasically means you're aggregating all\nthe output that is basically coming from\nall the specific models all the specific\nmodels now many people will say Krish\nwhat about Tai guys like this kind of\nsituation you know we will be having\nmore than 100 to 200 models so it is\nvery very difficult that it will be a\ntie who are repeating questions they\nwill be put up in time out so what if\nyou're saying that if the 50% of model\nsays yes 50% of our models says no\nalways understand guys we will be having\nmore than 100 to 200 plus models so in\nthis particular case there will be high\nprobability that always there will be a\nmajority voting available it will always\nnot be in that specific scenario so this\nwas the concept about bagging now some\npeople will be saying that Krish why are\nyou using different different models\nguys I'm not discussing about random\nForest over here random Forest uses only\none type of model that is decision tree\nbut if we think as an concept of bagging\nyou can have different different models\nover here and you can basically combine\nthem so this is a technique of emble\ntechniques and this is basically called\nas bagging okay now tell me one point I\nmissed out fine this is with respect to\nthe classification problem with respect\nto the regression problem what will\nhappen in case of a regression problem\nlet's say that I got here 120 here 140\nhere 122 here 148 as my output so in\nregression what will happen is that the\nentire mean will be taken mean will be\ntaken the output mean will be basically\ntaken and that will be your output of\nthe model average or mean very simple\nright so average or mean will be\nbasically taken up and here based on the\naverage you'll be able to solve the\nregression problem great now let's go\nahead and try to understand with respect\nto bagging and boosting how many\ndifferent types of algorithm are but\nbefore that I need to make you\nunderstand what exactly is boosting now\nhere in bagging you have seen that you\nhave parallel models right one one one\nindependent you have parallel models\nyou're giving some row samples in\ndifferent different models and basically\nare able to find out the output now in\ncase of boosting boosting is a\nsequential combination of models like\nthis you have lot of sequential models\nlike this and one after the model like\nfirst I'll give my training data to this\nparticular model then it will go to this\ndata then this model then this model so\nthis will be my M1 M2 M3 M4 and finally\nI will be getting my output so here you\ncan basically say that boosting is all\nabout and this M1 M2 M3 we basically\nmention it as weak Learners so this will\nbe weak learner weak learner weak\nlearner weak learner and finally when we\ngo till here it it'll if I combine all\nthese weak ners weak\nlearner weak learner okay once I combine\nall this weak learner it becomes a it\nbecomes a strong learner finally if I\ntry to combine this this will basically\nbecome a strong learner so here you have\nall the models sequentially one after\nthe other and then you will probably try\nto provide your uh input from one model\nto the next model to the next model and\nthese all models will be a very simpler\nweak learner model which will not be\nable to predict properly but when you\ncombine all this particular models\ntogether sequentially it becomes a\nstrong learner how this specifically\nworks I'll take an example example of AD\nboost XG boost I will show you that okay\nweek learner basically means the\nprediction is very bad but as you go\nsequentially you combine them they\nbecome a strong learner okay one example\nI want to give you let's say that you\nare a data scientist right let's say\nthat this model one may be a teacher\nwith respect to physics then this model\ntwo may be a teacher with respect to\nchemistry let's say model 3 is basically\na teacher of maths and model four is a\nteacher of geography now suppose if you\nare trying to solve one problem\nobviously if the physics teacher is not\nable to solve that particular problem\nthen probably chemistry can help or\nmaths can help or geography can help or\nsomeone can help so when we combine this\nmany expertise together they will be\nable to give you the output in an\nefficient way Sumit I'll talk about it\nwhere whether all the features are\nbasically passed to all the models or\nnot I'll just talk about it just give me\nsome time okay but I just want to give\nyou an idea about in short if someone\nasks you in an interview what exactly is\nboosting okay boosting is you can just\nsay that it is a sequential set of all\nthe models combined together and these\nall models that I initialized are\nusually weak Learners and when they are\ncombined together they become a strong\nlearner and based on the strong learner\nthey gives an amazing output and right\nnow if I say in most of the kaggle\ncompetition they use different types of\nboosting or bagging technique so we have\nbasically as I said\nbagging and boosting in bagging what\nkind of algorithm we specifically use we\nuse something called as random forest\nclassifier and the second model that we\nspecifically use is something called as\nrandom\nForest regress so we specifically use\nthese two kind of models which I'm\nactually going to discuss right now\nafter this and then in boosting we\nbasically use techniques like ad boost\ngradi Boost number three is Extreme\ngradient boost which we also say it as\nXG boost extreme gradient boost so let's\ngo ahead and let's discuss about the\nfirst algorithm which is called as\nrandom forest classifier and regressor\nnow first thing first let's understand\nsome things from the yesterday's class I\nhope uh what is the main problem with\nrespect to decision tree whenever we\ncreate a decision tree without any\nhyperparameter it does it not lead to\noverit\ndoes it not lead to overfitting uh\nwhenever you probably have a decision\ntree right it leads to something like\noverfitting why overfitting because it\ncompletely splits all the feature till\nit's complete depth overfitting\nbasically means for training data the\naccuracy is high for test data the\naccuracy is low so training data when\nthe accuracy is high I may basically say\nit as high bias and then I may basically\nsay it as sorry not high bias low bias\nand high V variance so low bias and high\nvariance yes obviously we can do pruning\nand all guys but again understand\npruning is an extensive task probably if\nyour if you have 100 features if you\nhave data points which is like 1 million\nto do pruning also it is very much\ndifficult yes pre pruning can be done\nbut again we cannot confirm that it may\nwork well or not so right now with\nrespect to decision tree you have this\nspecific problem that is low bias and\nhigh variance now in low Biance and high\nvariance you know that my model is\nbasically the generalized model that I\nshould get it should have low bias and\nlow variance so if somebody asks you why\ndo you use random Forest you can\nbasically explain about decision trees\nlike this now my main aim is to convert\nthis High variance to low variance now I\nwill be able to convert this High\nvariance to low variance using random\nforest classifier or random Forest\nregressor now what does random Forest do\nrandom Forest is a bagging technique\nsimilarly I have a data set over here\nlet's say that I have this data set\nand then here I will be having multiple\nmodels like\nM1\nM2\nM3 M4 let's say I have this four models\nlike this we have many many models now\nwith respect to this models this models\nall the models are actually decision\nTree in random forest all are decision\ntrees you don't have a different model\nover there so over here you can see that\nall the models are decision trees that\nis going to get used used in random\nForest so decision trees always gets\nused in random Forest the first thing\nthat you should know now whenever we are\nusing decision trees you know that\ndecision tree if I by default if we try\nto create it it may lead to overfitting\nand because of that every decision tree\nwill basically create low V low bias and\nhigh variance but if we combine in the\nform of bootstrap aggregator this High\nvariance will be getting converted to\nlow variance because why because\nmajority of voting we will be taking\nfrom this particular decision trees like\nthere will be many many decision tree so\nthey lot of outputs will be coming and\nwith the help of majority voting\nclassifier this High variance will get\nconverted to low variance now in random\nForest how it works in the first case if\nI talk about random Forest over here two\nthings basically happen with respect to\nthe D- data set let's say in first model\nwe do some kind of row\nsampling plus\nFeature Feature\nsampling that basically means we have to\nselect some set of rows and some set of\nfeatures and give it to M1 similarly you\ndo row sampling and feature sampling and\ngive it to M2 then you do row sampling\nand feature sampling you give it to M3\nand then you do row sampling and feature\nsampling you give it to M4 now when you\ndo this so what will happen\nindependently you're giving some\nfeatures along with some rows now there\nmay be a situation that your features\nmay also get repeated it may also get\nrepeated your records or data points may\nalso get repeated so when you are\nprobably training your model with this\nspecific data sets and specific features\nthis model become expert in predicting\nsomething right as I said one example\nover here I'm giving a physics model\nsome data I'm giving chemistry data\nchemistry model with some data similarly\nhere I'm giving some information to some\nmodel so the model will be an expert\nwith respect to that specific data So\nbased on all this particular data\nwhenever I get a new test data so what\nwill happen suppose let's say that this\nthis is a classification problem the M1\nmodel will be predicting zero this will\nbe predicting one this will be\npredicting zero and this will be\npredicting zero now in this particular\ncase again the majority voting\nclassifier or majority voting will\nhappen in the case of classification\nproblem and then here you will be\nspecifically able to get the output as\nzero so I hope everybody is able to\nunderstand all the models over here are\ndecision trees and based on that you\nwill be doing see when in I interview\nshould be very very uh things the things\nthat I'm telling you over here is all\nall the points are very much important\nand similarly if you tell the\ninterviewer definitely your interview is\ncracked in this kind of algorithm I've\nseen some of my students saying that\nokay uh Kish um when the interviewer\nasked me that which is my favorite\nalgorithm I said random Forest I told\nwhy did you say like that because he\nsaid that because that person let me let\nhim ask any questions in random Forest\nI'm very much confident about it and I'm\nalso going to prove him you know\nwhy they are very very good so with this\nspecific case here you can basically see\nthat because of the overfitting\ncondition of the decision tree you're\ncombining multiple decision tree so that\nyou get a generalized model which has\nlow bias and low variance so I hope\neverybody is able to understand boost\nfeature sampling basically means suppose\nif I have 1 2 3 four feature for the\nfirst model I may give two features for\nthe second model I may get three\nfeatures for the fourth model I may give\nfour features or uh any one feature ALS\nI can give to a specific model so\ninternally that random Forest it take\ncarees of over here these things are\nthere and this is how random Forest\nWorks only the difference between random\nForest classify and regression is that\nin regression again whatever output you\nare basically getting you basically do\nthe mean that's it average you just do\nthe average you'll be able to get the\noutput based on all the models output\nthat you are actually getting now let's\ntalk about some of the important points\nin random Forest the first thing first\nquestion is that is normalization\nrequired in random Forest then the next\nquestion is that in KNN is normalization\nwhen I say normalization or\nstandardization I I'll just talk about\nstandardization is standardization is\nrequired so this will be my another\nquestion so is normalization required in\nrandom forest or decision tree you here\nyou can also say it as decision tree is\nit required so for this the answer will\nbe no because understand decision tree\nwill basically do the splits if you Mini\nminimize the data also that split won't\nbe that much important but if I talk\nabout KNN whether standardization\nnormalization required over here the\nanswer is yes because here we use two\nthings one is ukan distance and\nManhattan distance because of this you\ndefinitely have to apply standardization\nso that the computation or distance\nbecomes easy so this is one of the most\ncommon interview questions that is\nbasically asked in random Forest coming\nto the third question is random Forest\nimpacted by outlier\nover here the answer will be no just\ncheck it out outside basically means\nGoogle and check it out check it out in\nGoogle okay perfect so I hope I've\ncovered most of the things in random\nForest is random Forest impacted by\noutliers this is the third question is\nKNN impacted by\noutliers is this KNN algorithm impacted\nby outliers is KNN impacted Byers the\nanswer is yes big yes perfect so so\nthese all are the interview questions\nthat needs to be covered now let's go\nahead and discuss about adab boost now\nin bagging most of the time we\nspecifically use random forest or you\ncan also create custom bagging\ntechniques custom bagging techniques\nmeans whatever algorithm you want use\nthe combination of them and try to give\nthe output this also you can do it\nmanually with the help of hands okay\nguys so second thing uh we are going to\ndiscuss about is boosting technique in\nthis\nthe first thing that uh first algorithm\nthat we are going to discuss about is\nadab Boost so adab boost we going to\ndiscuss about how does adab Boost uh\nwork now let's solve uh the first\nboosting technique which is called as\nadab boost okay and uh this is a\nboosting technique um in the boosting\ntechnique you have heard that we have to\nbasically solve in a sequential way this\nat least you know I know there is a lot\nof confusion within you all but we'll\ntry to solve a problem let's say so\nsuppose I have a data set which looks\nlike this fub1 F2 F3 F4 so these are my\nfeatures and probably these are my\noutput okay so let's say that I'm having\nthis features like this and this is my\noutput like yes or no like this so let's\nsay that how many records I have over\nhere three\n4 5 6 and one more is there 7 so this\nseven records are there now in adab\nboost the first thing is that\nspecifically with adab Boost uh you\nreally need to understand that what all\nthings we can basically do how do we\nsolve this classification problem that\nwe are going to understand the first\nthing first is that we Define a weight\nand the weight is very much simple\ninitially to all the records to all this\ninput records we provide an equal weight\nnow how do we provide an equal weight we\njust go and count how many number of\nrecords are there now in this particular\ncase the total number of records are one\n2 3 4 5 6 7 now every record I have to\nprovide an equal weight that is between\n0 to 1 so the overall sum should be one\nso in this particular case what I can do\nif I make 1X 7 1X 7 1X 7 to everyone\nthis will definitely become\na equal weights to all right and if I do\nthe total sum it will obviously be one\nlet's go to the next one now after this\nwhat do we do okay after this in adab\nthe first thing that we do is that we\ntake any of this feature how do you\ndecide which feature to take whether we\nshould go with F1 or whether we should\ngo with FS2 or whether we should go with\nF3 this we can do it with the help of\nInformation Gain and Information Gain\nand entropy or guinea right based on\nthis we can definitely understand\nwhether we should start making decision\nhere also you specifically make decision\ntrees so here what you do is that you\nprobably have to determine by using\nwhich feature I have to start my\ndecision tree so suppose out of all this\nfeature one feature two feature three\nyou have selected that okay the\ninformation gain and entropy of feature\none is higher so I'm going to use\nfeature one and probably divide this\ninto decision trees now when I divide\nthis into decision tree let's say that\nI'm dividing like this into decision\ntree this decision tree depth will be\nonly one one depth and this depth since\nit has only one depth we basically call\nit as stumps so what we do over here\nspecifically we will create a decision\nTre by taking only one feature and we\nwill only divide it to one level okay\none level or one depth that's\nand this is specifically called as stump\nwhat we are going to do next is that\nfrom this particular stump okay the\nstump is basically getting created only\none so that is adab Boost right we say\nit as weak Learners because this is weak\nlearner weak learner why there is a\nreason we say this as weak learner so\nonly weak learner so that is the first\nthing with respect to uh this particular\nadab boost so the first step is that\nthis is a weak learner so for the weak\nlearner we basically create a stump\nstump basically means one level decision\ntree that's it based on the information\ngain and entropy I have selected the\nfeature and then I just made a decision\ntree with only one level why it is\ncalled as it is called as weak learner\nokay so that is the reason we use only\nstum that is just a one level decision\ntree now the next step happens is that\nwe provide all the specific records to\nthis F1 and we train this specific model\nonly with one level decision tree we\ntrain them\nnow after we train them let's say that\nwe are going to pass all these\nparticular records to find out how many\nare correct and how many are wrong this\ndecision this decision tree is basically\ngiving so let's say that out of this\nentire records one\nrecord one record was just given as\nwrong let's say that this is the this is\nthe record which was given as wrong okay\nso let's say that this record output was\npredicted wrong from this particular\nmodel only one wrong was there after\ntraining the model now what we need to\ndo in this specific case understand a\nvery important thing so let's say that\nwe have done this and probably after\nthis what we are actually going to do we\nare going to calculate the total error\nso how many error this particular model\nmade let's say that in this particular\ncase only one was wrong so this was only\nwrong right one was wrong so if I want\nto calculate the total error how will I\ncalculate how many how many of them are\nwrong how many of them are wrong only\none is wrong what is the weight of this\nso I will go and write 1X 7 so this is\nspecifically my total error out of this\nspecific model which is my stump over\nhere okay which is my F1 stop now this\nis my first\nstep the second step is that I need to\nsee the performance of stump which stump\nthis specific stump and the performance\nis basically checked by a formula which\nis 1 by log e 1us total error divided\ntotal error why we are doing this\neverything will make sense okay in just\ntime every every in just a small time\neverything will make sense the first\nstep that we do in adaab boost is that\nwe try to find out the total error the\nsecond step we try to find out the\nperformance of stump now in this\nparticular case it will be 1 by log e 1\n- 1 by 7 / 1X 7 so once I calculate it\nit will be coming as\n895 F2 and F3 see again understand out\nof all these features I found out from\nInformation Gain and entropy that this\nis the best feature let's say that I\nhave calculated this\nas895 so this is my second step the\nfirst step is find out the total error\nthe second step is performance of stum\nwhat is te te basically means total\nerror te basically means total error now\nsee see the steps okay see the steps\nwhenever I'm discussing about boosting\nI'm going to combine weak Learners\ntogether to get a strong learner now\nwhat is the next step out of this now\nwhat what will be my third step\nunderstand over here my third step will\nbe to update all these weights and that\nis the reason why I'm calculating this\ntotal error and performance of Step so\nmy third step will basically be new\nsample weight from the decision tree one\nwhich is my stump so I'll say new sample\nweight is equal to I need to update all\nthese weights why I need to update all\nthese weights again understand I'll I'll\ntalk about it just a second so if I want\nto up update the sample weights first\nupdate I will do it for correct records\nsee for correct records whichever are\ncorrect like these all records are\ncorrect these all records are correct\nnow when I update the weights of this\nupdate the weights of this particular\nrecord it should reduce and when the the\nthe wrong records that I have this\nupdate should increase why because\nbecause if I increase this weights then\nthe wrong records that are there that\nrecord should go to the next week\nlearner that is the reason why I'm doing\nit now how to update this particular\nweights for correct records for correct\nrecords the formula looks something like\nthis weight multiplied by weight\nmultiplied by E to the^ of\nminus this specific performance okay\nthis specific performance so e to the\npower of PS I'll write performance of\nstump and then I will basically be able\nto write 1X 7 * e to the^ of minus\n895 if I do the calculation everybody\ntry to do it the answer will be\n05 now this is for correct records what\nabout incorrect records for the\nincorrect\nrecords the the weights that is going to\nthe formula that we going to apply is\nmultiplied by E to the^ of plus PS not\nminus PS plus PS so here I'll write 1 by\n7 multiplied e to the^ of\n895 so if I go and probably calcul this\nI'm going to get it\nas 349 so this two are the weights that\nI have got that basically means all\nthese records now which are correct 1X 7\nthe new updated weights will be 05 05\n05\n05 sorry not for the wrong\nrecords then this will be 05 then 05 and\n05 so let me just see what is 1x 7 so\nhere you can see initially it was. 142\nnow it has got reduced to 05 because all\nthese records are correct but the wrong\nrecord value is 349 so my weights will\nnow become over here as 349 now I will\njust go and go ahead and write over here\nmy new weight my new weight is nothing\nbut 05\n055\n05 05 05 1 2 how many 1 2 3 okay fourth\nrecord is here fourth record is there 1\n2 3 4 05 05 okay how many records are\nthere 1 2 3 4 5 6 7 so my fourth record\nwill basically become the new value that\nI'm having is something called as\n349 now tell me guys if I do the\nsummation of all these weights is this\nis it one so prob\nno I don't think so it is one because if\nI try to add it up it is not one but if\nI go and see over here these all are one\nif I combine all the things 1 2 3 4 5 6\n7 these all are one so here I have need\nto find out my normalized weight now in\norder to find out the normalized\nweight all I have to do is that what I\nhave to do because the entire sumission\nshould be one so we have to\nnormalize now in order to normalize all\nyou have to do is that go and find out\nwhat is the sum of all this things the\nsummation of all these things will be\n0 649 all you have to do is that divide\nall the numbers\nby 649 divided by\n649\n649 like this divide all the numbers by\n649 and tell me what will be the answer\nthat you'll be getting so here your\nnormalized weight will now look like\n077 07 and this value will be somewhere\naround uh\n537 I guess in this case then this will\nbe 07\n077 here we are going to divide by all\nthis 64 649 now this is my normalized\nweight now after you get a normalized\nweight we will try to create something\ncalled as buckets because see one\ndecision tree we have already created\nwhich is a stump and you know from this\nparticular stum what you're going to get\nokay as an output then in the sequential\nmodel we will go and combine another\nmodel over here now it's the time that I\nhave to create this specific model now\nin order to create this specific model I\nneed to provide some specific rows only\nto this model to train because this\nmodel is giving one wrong now what I\nhave to do is that whatever is wrong\nalong with other data points I need to\nprovide this specific model with those\nrecords so that this model will be able\nto train on this and probably be able to\nget the output now let's create buckets\nnow based on buckets how the buckets\nwill be created over here I will take 07\nuntil\nsorry whatever is the value over here\nnormal we value okay so I will start\ncreating my buckets buckets basically\nfrom 0 to\n07 what did I say now for this decision\ntree or stump I need to provide some\nrecords so the maximum number of record\nthat should be going should be the wrong\nrecords that should go over here now how\ndo we decide that okay there should be a\nway that we should be able to say that\nthat specific wrong number of Records\nshould go to that decision tree so for\nthat purpose what we do is that this\ndecision tree will randomly create some\nnumbers between 0 to 1 randomly create\nthose numbers between 0 to 1 and\nwhichever bucket it will come in like 07\nto 014 014 to 07 basically means 0 2 1\nthen 0 2 1 2 see how the bucket is\ngetting cre this value is getting added\nto this so that becomes this bucket 021\n+3 537 how much it is it is nothing but\n470 747 then 747\nto\n751 like this you create all the buckets\nokay you can create all the buckets now\ntell me which record is basically having\nthe biggest bucket size obviously this\nrecord so if I randomly create a number\nbetween 0 to one what is the highest\nprobability that the values will be\ngoing in so in this particular case most\nof the wrong records will be passed\nalong with the other records obviously\nother records there are chances that\nother records will go to the next\ndecision tree but understand maximum\nnumber will go with the wrong records\nbecause the bucket is high over here so\nthe bucket is high over here so most of\nthe time this specific record will get\ncreate selected and then it will be gone\nto the second tree now suppose I have\nthis all records\nso this is my first stump this is my\nsecond stump this is my third stump\nsimilarly the third stump from the\nsecond stump whichever wrong records\nwill be going maximum number of Records\nwill go over here then again it will be\ntrained like this we'll be having lot of\nstumps minimum 100 decision trees can be\nadded you know that every decision tree\nwill give one output for a new test data\nnew test data this week learner will\ngive one output this week learner will\ngive one output this week learner and\nthis will week learner will be giving\none output obviously the time complexity\nwill be more now from this particular\noutput suppose it is a binary\nclassification I will be getting 0 1 1 1\nso again over here majority voting will\nhappen and the output will be one in\ncase of regression problem I will be\nhaving a continuous value over here and\nfor this the average average will be\ncomputed and that will give me an output\nover here so for regression the average\nwill be done for classification what\nwill happen majority will be be\nhappening so everywhere that same part\nwill be going on buckets is very much\nsimple guys buckets basically means\nbased on this weights normalized weight\nwe are going to create bucket so that\nwhichever records has the highest bucket\nbased on this randomly creating code you\nknow it will select those specific\nbuckets and put it into random Forest\nunderstand why this bucket size is Big\nthe other wrong records which are\npresent right suppose they are have more\nthan four to five wrong records their\nbucket size will also be bigger and\nbecause based on this randomly creating\nnum between 0 to 1 most of the wrong\nrecords will be selected and given to\nthe second stum similarly this\nparticular decision tree will be doing\nsome mistakes then that wrong records\nwill get updated all the weights will\nget updated and it will be passed to the\nnext decision tree guys when I say wrong\nrecord the output will be same only no\nzero and one so interesting everyone I\nhope you understood so much of maths in\nadab boost and how adab boost actually\nwork three main things one is total\nerror one is performance of stump and\none is the new sample weight these\nthings are getting calculated extensive\nmax normalized weight was basically used\nbecause the sum of all these weights are\napproximately equal to one when boosting\nwhy not take the last output no no no we\nhave to give the importance of every\ndecision tree output every decision tree\noutput are important okay let me talk\nabout one model which is called as\nblackbox model versus white box what is\nthe difference between blackbox model\nand white box if I take an example of\nlinear regression tell me what kind of\nmodel it is is is it a white box model\nor black box if I take an example of\nrandom\nForest is this a white box or black box\nif I take an example of decision tree it\nis a white box of blackbox model if I\ntake an example of a Ann is it a white\nbox of blackbox model linear regression\nis basically called as an wide Box model\nbecause here you can basically visualize\nhow the Theta value is basically\nchanging and how it is coming to a\nglobal Minima and all those things in\nrandom Forest I will say this as\nblackbox model because it is impossible\nto see all the decision tree how it is\nworking so that is the reason the maths\nis so complex inside this if I talk\nabout decision tree this is basically a\nwhite box model because in decision tree\nwe know how the split are basically\nhappening with the help of paper and pen\nyou'll be able to do it in the case of\nan Ann this is a blackbox model because\nhere you don't know like how many\nneurons are there how they are\nperforming and how the weights are\ngetting updated so this is the basic\ndifference between the blackbox and uh\nuh white box model this entire thing is\nthe agenda of today's session so let's\nstart uh the first algorithm that we are\nprobably going to discuss today is\nsomething called as K\nmeans\nclustering K means clustering and this\nis a kind of unsupervised machine\nlearning now always remember\nunsupervised machine learning basically\nmeans that uh the one and the most\nimportant thing is that in unsupervised\nmachine learning\nin unsupervised ml you don't have any\nspecific output so you don't have any\nspecific output so suppose you have\nfeature one and feature two and suppose\nyou have datas different different data\nyou know and based on this data what we\ndo we basically try to create clusters\nthis clusters basically says what are\nthe similar kind of data so this is what\nwe basically do from uh clustering and\nthere are various techniques like K\nMains uh it is hierle clustering and all\nso first of all we'll try to understand\nabout K means and how does it\nspecifically work it's simple uh suppose\nyou have a data points like this okay\nlet's say that this is your F1 feature\nF2 feature and based on this in two\ndimensional probably I will be plotting\nthis points and suppose this is my\nanother points so our main purpose is\nbasically to Cluster together in\ndifferent different groups okay so this\nwill be my one group and probably the\nother group will be this group right so\ntwo groups because obviously you can see\nfrom this clusters here you have two\nsimilar kind of data which is basically\ngrouped together right this is my\ncluster one and this is my cluster 2 let\nme talk about this and why specifically\nit'll be very much useful then we'll try\nto understand about math intuition also\nnow always understand guys uh where does\nclustering gets used okay in most of the\nEnsemble techniques I told you about\ncustom emble technique right so custom\nemble techniques in custom assemble\ntechniques you know whenever we are\nprobably creating a model first of all\non our data set what we do is that we\ncreate clusters so suppose this is my\ndata set during my model creation the\nfirst algorithm we will probably apply\nwill be clustering algorithm and after\nthat it is obviously good that we can\napply regression or classification\nproblem suppose in this clustering I\nhave two or three groups let's say that\nI have two or three groups over here for\neach group we can apply a separate\nsupervis machine learning algorithm if\nwe know the specific output that we\nreally want to take ahead I'll talk\nabout this and uh give you some of the\nexamples as I go ahead now let's go on\ngo ahead and focus more on understanding\nhow does kin's clustering algorithm work\nso let's go over here the word K means\nhas this K value this K are nothing but\nthis K basically means centroids K\nbasically means centroids so suppose if\nI have a data set which looks like this\nlet's say that this is my data set now\nover here just by seeing the data set\nwhat are the possible groups you think\ndefinitely you'll be saying K is equal\nto 2 So when you say k is equal to two\nthat basically means you will be able to\nget two groups like this and each and\nevery group will be having a centroid a\ncentroid Point here also there will be a\ncentroid point so this centroid will\ndetermine basically this is a separate\ngroup over here this is a separate group\nover here so over here here you can\ndefinitely say that fine this is two\ngroups but but how do we come to a\nconclusion that there is only two groups\nokay we cannot just directly say that\nokay we'll try to just by seeing the\ndata because your data will be having a\nhigh dimension data right right now I'm\njust showing your two Dimension data but\nfor a high dimension data definitely\nyou'll not be able to see the data\npoints how it is plotted so how do you\ncome to a conclusion that only two\ngroups are there so for this there is\nsome steps that we basically perform in\nK mins the first step is that we try\nwith different K values we try with\ndifferent K values and which is the\nsuitable K value K is nothing but\ncentroids okay it is nothing but\ncentroids we try with different\ndifferent centroids in this particular\ncase let's say that I have this\nparticular data point and I actually\nstart with k is equal 1 or 2 or 3 any\none you want let's say that I'm going to\nstart with k is equal 2 how to come up\nwith this K is equal to 2 as a perfect\nvalue that I'll talk about it we need to\nknow there is a concept which is called\nas within cluster sum of square so when\nwe try different K values let's say that\nfor K is equal to 2 what will happen the\nfirst step we select a we try K values\nso let's say that we are considering K\nis equal to 2 the second step is that we\ninitialize K number of centroids now in\nthis particular case I know my K value\nis 2 so we will be initializing randomly\nlet's say that K is equal to 2 so what\nwe can actually do let's say that this\nis this is my one centroid I will I'll\nput it in another color so this will be\nmy one centroid and let's say that this\nis my another centroid so I have\ninitialized two centroids randomly in\nthis space now after this particular\ncentroid what we have to do is that\nafter initializing this centroid what we\nhave to do is that we have to basically\nfind out which points are near to the\ncentroid and which points are near to\nthis centroid now in order to find out\nit is a very easy step we can basically\nuse ukan distance to find out the\ndistance between the points in an easy\nway if I really want to show you that\nyou know like how many points I want to\nin an easy way what I can do I can\nbasically draw a straight line over here\nlet's say that I'm drawing a straight\nline over here in another color I can\ndraw a straight line and I can also draw\none parallel line like this so This\nbasically indicates that whichever\npoints you see over here suppose if I\ndraw a straight line in between all\nthese points you will be able to see\nthat let's say that I'm drawing one more\nparallel line\nwhich is intersecting together so from\nthis you can definitely find out let's\nsay that these are all my points that\nare nearer to this green line Green\nPoint so what I'm actually going to do\nin this particular case all these points\nthat you are seeing near the green it\nwill become green color so that\nbasically means this is basically nearer\nto this centroid and whichever points\nare nearer to this particular point that\nwill become red point so that basically\nmeans this belongs to this group okay\nthis belongs to this group so I hope\neverybody's clear till here then what\nwill happen is that this summation of\nall the values then we initialize the K\nnumber of centroids that is done then we\ntry to calculate the distance we try to\nfind out which all points is nearer to\nthe centroid let's say that this is my\none centroid this is my another centroid\nand we have seen that okay these all\npoints belong to this centroid it near\nto this particular centroid so this is\nbecoming red so that is based on the\nshortage distance and here it is\nbecoming green now the next step let's\nsee what is the next step after this so\nI am going to remove this thing now the\nnext step will be that the entire points\nthat is in red color all the average\nwill be taken so here again the average\nwill be taken now third step here I'm\ngoing to write here we are going to\ncompute the average the reason we\ncompute the average is that because we\nneed to update the centroid so compute\nthe average to update centroid to update\ncentroids so here you'll be able to see\nthat what I'm actually doing as soon as\nwe compute the average this centroid is\ngoing to move to some other location so\nwhat location it will move it will\nobviously become somewhere in Center so\nhere now I'm going to rub this and now\nmy new centroid will be this point where\nI am actually going to draw like this\nlet's say this is my new centroid now\nsimilarly this thing will happen with\nrespect to the green color so with\nrespect to the green color also it will\nhappen and this green will also Al get\nupdated so I'm going to rub this and\nthis will be my new Green Point which\nwill get updated over here then again\nwhat will happen again the distance will\nbe calculated and again a perpendicular\nline will be calculated here you can see\nthat now all the points are towards\nthere okay again the centroid based on\nthis particular distance again it will\nbe calculated and here you can see that\nall the points are in its own location\nso here now no update will actually\nhappen let's say that there was one\npoint which was red color over here\nthen this would have become green color\nbut since the updation has happened\nperfectly we are not going to update it\nand we are not going to update the\ncentroid right so now you can understand\nthat yes now we have actually got the\nperfect centroid and now this will be\nconsidered as one group and this will be\nbasically considered as the another\ngroup it will not intersect but right by\ndefault here intersection is happening\nso I hope everybody's understood the\nsteps that you have actually followed in\ninitializing the centroids in updating\nthe centroids and in updating the points\nis it clear everybody with respect to K\nmeans now let's discuss about one\npoint how do we decide this K value okay\nhow do we decide this K value so for\ndeciding the K value there is a concept\nwhich is called as elbow method so here\nI'm going to basically Define my elbow\nmethod now elbow method says something\nvery much important because this will\nactually help us to find out what is the\noptimized K value whether the K value\nshould be two whether uh the K value is\ngoing to be three whether the K value is\ngoing to become four and always\nunderstand suppose this is my data set\nsuppose this is my data set initially\nlet's say that I have my data points\nlike this we cannot go ahead and\ndirectly say say that okay K is equal to\n2 is going to work so obviously we are\ngoing to go with iteration for I is\nequal to probably 1 to 10 I'm going to\nmove towards iteration from 1 to 10\nlet's say so for every iteration we will\nconstruct a graph with respect to K\nvalue and with respect to something\ncalled as W CSS now what is this W CSS W\nCSS basically means within cluster sum\nof\nsquare okay this is the meaning of wcss\nwithin cluster sum of square now let's\nsay that initially we start with one\ncentroid so one centroid let's say it is\ninitialized here one centroid is\nbasically initialized here if we go and\ncompute the distance\nbetween each and every points to the\ncentroid and if we try to find out the\ndistance will the distance value be\ngreater or it will be smaller will it be\nsmaller or greater tell me if you try to\ncalculate this distance from this\ncentroid to every point this is what is\nwithin cluster sum of square it will\nalways be very very much greater so\nlet's say that my first point has come\nsomewhere here it is going to be\nobviously greater let's say that my\nfirst point is coming over here find\nSo within K is equal to 1 initially we\ntook and we found out the distance of w\nCSS and it is a very huge value okay\nbecause we're going to compute the\ndistance between each and every point to\nthe centroid now the next thing that I'm\nactually going to do is that now we'll\ngo with next value that is K is equal to\n2 now in K is equal to 2 I will\ninitialize two points okay I will\ninitialize two points and then probably\nI will do the entire process which I\nhave written on the top now tell me me\nwhichever points is nearer to this green\npoint if we compute the distance and\nwhichever points is nearer to the red\npoint if you compute the distance like\nthis now this summation of the distance\nwill be lesser than the previous W CSS\nor not obviously it is going to be\nlesser than the previous W CSS so what\nI'm actually going to do probably with K\nis equal to 2 your value may come\nsomewhere here then with K is equal to 3\nyour value May come somewhere here then\nK is equal to 4 will come here to 5 6\nlike this it will go so here if I\nprobably join this line you'll be able\nto see that there will be an Abrupt\nchanges in the W CSS value in the wcss\nvalue there will be an Abrupt changes\nand this this is basically called as\nelbow curve now why we say it as elbow\ncurve because it is in the shape of\nelbow and here at one specific point\nthere will be an Abrupt change and then\nit will be straight so that is the\nreason why we basically say this as\nelbow okay so this is a very important\nthing see in finding the K value we use\nelbow method but for validating purpose\nhow do we validate that this model is\nperforming well we use silard score that\nI'll show you just in some time but\nunderstand that in K means clustering we\nneed to update the centroids and based\non that we calculate the distance and as\nthe K value keep on increasing you'll be\nable to see that the distance will\nbecome normal or the wcss value will\nbecome normal and then we really need to\nfind out which is the phys K value where\nthe abrupt change see over here suppose\nabrupt change is there and then it is\nnormal then I will probably take this as\nmy K value so obviously the model\ncomplexity will be high because we are\ngoing to check with respect to different\ndifferent K values and wcss values and\nthis basically means that the value that\nwe'll probably get first of all we need\nto construct this elbow curve then see\nthe changes where it is basically\nhappening we'll need to find out the\nabrupt change and once we get the abrupt\nchange we basically say that this may be\nthe K value so K is equal to 4 as an\nexample I'm telling you so unless and\nuntil if you really want to find the\ncluster it is very much simple we take a\nk value we initialize K number of\ncentroids we compute the average to\nupdate the centroids then again we try\nto find out the distance try to see that\nwhether any points has changed and\ncontinue that process unless and until\nwe get separate groups okay so this is\nthe entire funa of claim in clustering\nso finally you'll be able to see that\nwith respect to the K value we will be\nable to get that many number of groups\nif my K value is four that basically\nmeans I will be probably getting four\ndifferent groups like this 1 two right\nthree like this and four I will be\ngetting four groups like this with K is\nequal to 4 that basically means K is\nequal to four clusters and every group\nwill be having its own centroids okay\nevery group will be having okay\ncentroids are very much important yes\nI'll try to show you in the coding also\nguys let's go towards the second\nalgorithm the second algorithm that we\nwill be probably discussing is called as\nhierarchical clustering now hierarchal\nclustering is very much simple guys all\nyou have to do is that let's say this is\nyour data points this is your data\npoints and this is my P1 let's say P2\nnow hierle clustering says that we will\ngo step by step the first thing is that\nwe will try to find out the most nearest\nValue let's say this is my X and Y let's\nsay these are my points like this is my\nP1 point this is my P2 point this is my\nP3 point this is my P4 Point P5 Point P6\npoint p7 point okay so these are my\npoints that I have actually named over\nhere let's say that this may be the\nnearest point to each other so what it\nwill do it will combine this together\ninto one cluster this we have computed\nthe distance so it will C create one\ncluster now what will happen on the\nright hand side there will be another\nnotation which you may be using in\nconnecting all the points one so suppose\nthis is my P1 this is my P2 this is my\nP3 P4 let's say that I have this many\npoints and probably I will also try to\nmake\np7 so these are my points p7 now you\nknow that the nearest point that we are\nhaving okay this will probably be\ndistance 1 2 3 this is distance okay 4 5\n6 like this we have lot of distance so\nhierle clustering will first of all find\nout the nearest point and try to compute\nthe distance between them and just try\nto combine them together into one what\ndo we do we basically combine them into\none group okay so P1 and P2 has been\ncombined let's say then it'll go and\nfind out the other nearest point so\nlet's say P6 and p7 are near so they are\nalso going to combine into one group so\nonce they combine into one group then we\nhave P6 and p7 which will be obviously L\ngreater than the previous distance and\nwe may get this kind of computation and\nanother combination or cluster will form\nget formed over here then you have seen\nthat okay P3 and P5 are nearer to each\nother so we are going to combine this so\nI'm going to basically combine P3 and\nP5 okay and let's say that this distance\nis greater than the previous one because\nwe are basically going to sh start with\nthe shortest distance and then we are\ngoing to capture the longest distance\nnow this is done now you can see that\nthe next point that is near right to\nthis particular group is P4 so we are\ngoing to combine this together into one\ngroup so once we combine this into one\ngroup this P4 will get connected like\nthis let's say it is getting connected\nlike this P4 has got connected then what\nis the nearest Point whether it is P6 p7\ngroup or P1 P2 obviously here you can\nsee that P1 P2 is there so I am probably\ngoing to combine this group together\nthat basically means P1 P2 let's say I'm\njust going to combine this group group\ntogether again circle is coming so I\nwill make a dot let's say I'm going to\ncombine this group together because\nthese are my nearest groups so what will\nhappen P1 and P2 will get combined to P5\nsorry P4 P5 this one so I will be\ngetting another line like this and then\nfinally you'll be seeing that P6 p7 is\nthe nearest group to this so this will\ntotally get combined and it may look\nsomething like this so this will become\na total group like\nthis so all the groups are combined so\nfinally you'll be able to see that there\nwill be one more line which will get\ncombined like\nthis this is basically called as\ndendogram dendogram okay which is like\nbottom root to top now the question\narises is that how do you find that how\nmany groups should be here how do you\nfind out that how many groups should be\nhere the funa is very much Clear guys in\nthis is that you need\nto find the longest\nvertical line you need to find out the\nlongest vertical line that has no\nhorizontal line pass through it no\nhorizontal\nline passed through it this is very much\nimportant that has no horizontal line\npass through it now what this is\nbasically meaning is that I will try to\nfind out the longest line longest\nvertical line in such a way that none of\nthe horizontal line passes through it\nwhat is horizontal line suppose if I\nconsider this vertical line This\nvertical line over here if you see that\nif I extend this green line it is\npassing through this if I extend this\nline it is passing through this right if\nI'm extending this line it is passing\nthrough this right so out of this the\nlongest line that may be passing in such\na way that no horizontal line probably\nis this line that I can actually see so\nwhat you do over here is that you\nbasically just create a straight line\nover this and then you try to find out\nthat how many clusters it will be there\nby understanding that how many lines it\nis passing through if it is passing\nthrough this one line two line three\nline four line that basically means your\nclusters will be four\nclusters this is how we basically do the\ncalculation in heral clustering again\nhere it may not be the perfect line I've\njust drawn with some assumptions but if\nyou are trying to do this probably you\nhave to do in this specific way okay\nI've already uploaded a lot of practical\nvideos with respect to highill\nclustering and all now now tell me\nmaximum effort or maximum time is taken\nby is taken\nby K\nmeans or hierle clustering this is a\nquestion for you yes guys number of\nclusters may be three but here I'm just\nshowing you that how many lines it may\nbe passed by how do you basically\ndetermine whether maximum time will be\ntaken by kin or Hier clustering this is\nan interview question the maximum time\nthat will be taken is by hierarchical\nclustering why because let's say that I\nhave many many many data points at that\npoint of time hierle clustering will\nkeep on constructing this kind of\ndendograms and it will be taking many\nmany many time lot time right so hierle\nclustering will take more time maximum\ntime that it is going to basically take\nso it is very much important that that\nyou understand which is making basically\ntaking more time so if your data set is\nsmall you may go ahead with hierle\nclustering if your data set is large go\nwith K means clustering go with K means\nclustering in short both will take more\ntime but K Min will perform better than\nhle clustering see guys you will be\nforming this kind of dendograms right\nand just imagine if you have 10 features\nand many data points how you're going to\ndo it it will be a cubers some process\nyou'll not be even able to see this\ndendogram properly and manually\nobviously you cannot do it so this was\nwith respect to K means clust swing and\nH mean clust swing I hope everybody's\nunderstood now the next topic that we'll\nfocus on is that how do we\nvalidate see how do we validate a\nclassification problem we use\nperformance metric like confusion Matrix\naccuracy um different different true\npositive rate Precision recall but how\ndo we validate clustering model Model S\nwe are going to use something called as\nso we are going to basically use\nsomething called as\nSil score I'll show you what Sid score\nis I'm going to just open the Wikipedia\nso this is how a CID score looks like a\nvery very amazing topic okay how do we\nvalidate whether my model basically has\nperfect three or four model perfect\nthree suppose if I find out my K value\nis three how do we find out now see one\nmore one more issue with K means one\nissue with K means which I forgot to\ntell you let's say that I have a data\npoint which looks like this and suppose\nI have some data points like this I have\nsome data points which looks like this\nlet's say I have like this now in this\none issue will be that suppose I try to\nmake a cluster over here obviously\nyou'll be saying my K value will be two\nokay in this particular case suppose\nthis is one cluster this is my another\ncluster\nright because of my wrong initialization\nof the points okay understand because\nsuppose if I initialize just randomly\nsome centroids like this then what may\nhappen is that there is a possibility\nthat we may also have three clusters\nlike like like this kind of clusters one\ncluster will be here one cluster will be\nhere one cluster will be here so this\ninitialization of the centroids one\ncondition is that it should be very very\nfar if we initialize our centroids very\nvery far at that point of time we will\nbe able to find the centroid exactly in\nthe center because it will keep on\nupdating it'll keep on going ahead right\nbut if we don't initialize that very far\nthen there will be a situation that\nprobably if I wanted to get only the\nreal thing was to get only two centroids\nI was probably getting three centroids\nright so this is a problem so for this\nthere is an algorithm which is called as\nK means Plus+ and what this K means\nPlus+ will do which I will probably show\nyou in Practical this will make sure\nthat all the centroids that are\ninitialized it is very very\nfar okay all the in centroids that is\nbasically there it is initialized very\nvery far we'll see that in practical\napplication where specifically those\ncentroids are basically used now let me\ngo ahead and let me show\nyou with respect to Sid clust string now\nwhat is the solo color string I'm going\nto explain you in an amazing way this is\nimportant\nif someone says you how do we validate\nhow do we validate cluster\nmodel then at that point of time we\nbasically use this site it will be used\nin it will be used with respect\nto it will be used with respect to K\nmeans it can be used in hierle mean\nright if you want to validate how do we\nvalidate okay that is what we are\nbasically going to see over here now in\nC's clustering\nwhat are the most important things the\nfirst and the most important thing is\nthat we will try to find out we will try\nto find out a ofi we will try to find\nout a of I now what is this a ofi see\nthis a ofi that you basically see a ofi\nis nothing but see three major steps\nhappens in order to validate cluster\nmodel with the help of solo first thing\nis that I will probably take one cluster\nokay there will be one point\nwhich will be my centroid let's say and\nthen what I'm going to do I'm just going\nto whatever points are there inside this\ncluster I'm going to compute the\ndistance between them so I'm going to do\nthe summation and I'm also going to do\nthe average of all this distance so here\nyou can see that when I said distance of\nI comma J I basically means this point J\nbasically means all these points I is\nnothing but it is the centroid so here\nis nothing but this this is the centroid\nlet's say that I'm having the centroid\nso I'm going to compute all the distance\nover here which is mentioned by this and\nthis value that you see that I'm\nactually dividing by C of I minus one in\nShort I am actually trying to calculate\nthe average\ndistance so this is the first point\nwhere I'm actually Computing the a ofi\nnow similarly what I will do is\nthat what I will do is that the next\npoint will be that suppose I have\ncomputed a ofi the next the next that we\nneed to compute is B ofi now what is b\nofi b ofi is nothing but there will be\nmultiple clusters in a k means problem\nstatement we will try to find out the\nnearest cluster okay suppose let's say\nthat this is the nearest cluster and in\nthis I have all the variety of points\nthen B ofi basically says that I will\ntry to compute the distance between each\npoint and the other point in this\ncentroid sorry in this cluster so this\nis my cluster one this is my cluster two\nso what I'm actually going to do is that\nhere I'm going to compute the distance\nbetween this point to this point then\nthis point to this point then this point\nto this point this point to this point\nthis point to this point this point to\nthis point every point I'm actually\ngoing to compute the distance once this\npoint is done we will go ahead with the\nnext point and we'll try to compute the\ndistance and once we get all this\nparticular distance what we are going to\ndo we are going to do the average of\nthem average\nnow tell me if I try to find out the\nrelationship between a of I and B of I\nif my cluster model is good will a of\nI will be greater than b of I or\nwill B of I will be greater than a ofi\nif I have a good clustering model if I\nhave a good clustering model will a of I\nis greater than b of I will be greater\nthan b of I or whether B of I will be\ngreater than a of I out of this if we\nhave a really good model obviously the\ndistance between B of I will be greater\nthan a of I in a good model that\nbasically means if I talk about sloid\nclustering the values will be between -1\nto +1 the more the value is towards +1\nthat basically means the good the model\nis the good the clustering model is the\nmore the values towards negative one\nthat basically means this condition is\ngetting applied now what does this\ncondition basically say that basically\nmeans that this distance is far than the\ncluster distance this is what this\ninformation is getting portrayed and\nthis is the importance of CID\nclustering finally when we apply the\nformula of CID clustering you'll be able\nto see that sloid clustering is nothing\nbut let me rub this everything guys for\nyou let me just show you what is CID\nclustering CID clustering formula will\nbe something like this this B of I so\nhere you have solid clustering this is\nthe formula B of I minus a of I Max of a\nof I comma B of I if C of I is greater\nthan one right so by this you will be\ngetting the value between -1 to + 1 and\nmore the value is towards + one the more\ngood your model is more the values\ntowards minus1 more bad your model is\nbecause if it is towards minus1 that\nbasically means your a of I is obviously\ngreater than b of I so this is the\noutcome with respect to cot crust string\nif s is equal to zero that basically\nmeans still your model needs to be uh\nper basically the clustering needs to be\nimproved what is I over here I is\nnothing but one data point you you can\njust read this guys data point in I in\nthe cluster C of I so I hope everybody's\nunderstood this now let's go ahead and\nlet's discuss about the next topic we\nhave obviously finished up solart\nclustering over here let's discuss about\nsomething called as DB\nscan so for DB scan clustering this is\nan amazing clustering algorithm we'll\ntry to understand how to actually do DB\nclustering and probably you'll be able\nto understand a lot of things from this\nnow in DB scan clustering what are the\nimportant things so let's start with\nrespect to DB scan clustering and let's\nunderstand some of the important points\nover here the first point that you\nreally need to remember is something\ncalled as score point points I'll also\ntalk about when do you say core points\nor when do you say other points as such\nso the first point that I will probably\ndiscuss about is something called as Min\npoints the second point that I will\nprobably discuss about is something\ncalled as score points the third thing\nthat I will probably discuss about is\nsomething called as border points and\nthe fourth point that I will definitely\ntalk about is something called as noise\nPoint okay guys now tell me in C's\nclustering\nif I have this kind of groups don't you\nthink with the help of two different\nclusters I may combine this two like\nthis with the help of two different\nclusters I may combine something like\nthis right but understand over here what\nwhat problem is basically happening with\nthe second clustering this is actually\nan outliers let's say that let's say one\nthing very nicely I will put okay let's\nsay I have one point over here I have\none point over here here so if I do\nclustering probably I will get one\ncluster\nhere and I may get another cluster which\nis somewhere here now understand one\nthing this point is definitely an\noutlier even though this is an outlier\nwith the help of K means what I'm\nactually doing I'm actually grouping\nthis into another group so can we have a\nscenario wherein a kind of clustering\nalgorithm is there where we can leave\nthe outlier separately and this outlier\nin this particular algorithm and this is\nB basically uh we will be using DB scan\nto relieve the outlier and this point\nwill be called as a noisy Point noisy\npoint or I can also say it as an outlier\nso this will be a noise point for this\nkind of algorithm where you want to skip\nthe outliers we can definitely use DB\nscan that is density based spatial\nclustering of application with noise a\nvery amazing algorithm and definitely I\nhave tried using this a lot nowadays I\ndon't use K means or Hier means instead\nuse this kind of algorithm now see this\nwhat are the important things over here\nfirst of all you need to go ahead with\nMin points Min points so first thing is\nthat you need to have Min points this\nMin points is a kind of\nhyperparameter this basically says what\ndoes hyper parameter says and there is\nalso a value which is called as\nEpsilon which I forgot I will write it\ndown over here this is called as Epsilon\nnow what does epsilon mean Epsilon\nbasically means if I have a point like\nthis\nand if I take Epsilon this is nothing\nbut the radius of that specific Circle\nradius of that specific Circle okay so\nEpsilon is nothing but radius over here\nin this specific T what does minimum\npoints is equal to 4 mean let's say that\nI have I have taken a point over here\nlet's say that this is my\npoint and I have drawn a circle which\nlooks like this and let's say that this\nis my Epsilon\nvalue okay this is my Epsilon value if I\nsay my Min point point is equal to 4\nwhich is again a hyper\nparameter that basically means I can if\nI have four at least four points over\nhere near to this particular Circle\nbased on this Epsilon value then what\nwill happen is that this point this red\npoint will actually become a core\npoint a core point which is basically\ngiven over here if it has at least that\nmany number of Min points inside or near\nto this particular within this\nEpsilon okay within this particular\ncluster suppose this is my cluster with\nthe help of Epsilon I have actually\ncreated it is there a particular unit of\nEpsilon or we simply take the unit of\ndistance no Epsilon value will also get\nselected through some way I I'll show\nyou I'll show you in the practical\napplication don't worry now the next\nthing is that let's say let's say I have\nanother another point over here let's\nsay that I have another point over here\nand this is my circle with respect to\nEpsilon I have created it let's say that\nhere I have only one\npoint I have only one point inside this\nparticular cluster at that point this\npoint becomes something called as border\nPoint border Point border point also we\nhave discussed over here right so border\npoint is also there so here I'm saying\nthat at least one at least one if it is\nonly one it is present then it will\nbecome a border point if it has Force\ndefinitely this will become a core Point\ncore Point like how we have this red\ncolor so and there will be one more\nscenario suppose I have this one cluster\nlet's say this is my Epsilon and suppose\nif I don't have any points near this\nthen this will definitely become my\nnoise point and this noise point will\nnothing be but this will be a\ncluster okay so here I have actually\ndiscussed about the noise point also so\nI hope everybody is able to understand\nthe key terms now what is basically\nhappening is that whenever we have a\nnoise Point like in this particular\nscenario we have a noise point and we\ndon't find any points inside this any\ncore point or border point if you don't\nfind inside this then it is going to\njust get neglected that basically means\nthis is basically treated as an outlier\nI hope everybody is able to understand\nhere this point will be treated as an\noutlier or it can also be treated as a\nnoise point and this will never be taken\ninside a group okay it will never never\nbe taken inside a group suppose I have\nthis set of points which you see\nbasically over here red core and all and\nthere is also a border Point by making\nmultiple circles over here here you can\ndefinitely say that how we are defining\ncore points and the Border points and\nthis can be combined into a single group\nokay this can be combined into a single\ngroup because how the connection is now\nsee this this yellow line is basically\ncreated by one sorry this yellow point\nis basically created by one Epsilon and\nwe have one One Core point over here\nremember over here it should be at least\none core Point okay not one point but\none core point at least if it is having\none core point then it will become a\nborder point this will become a border\npoint that basically means yes this can\nbe the part of this specific group so\nwhat we are doing Whenever there is a\nnoise we are going to neglect it\nwherever there is a broader and core\npoints we are going to combine it so\nI'll show you one more diagram which is\nan amazing diagram which will help you\nunderstand more in this a k means\nclustering and Hier mean clustering now\nsee this everybody now the right hand\nside of diagram that you see is based on\nDB scan clustering and the left hand\nside is basically your traditional\nclustering method let's say that this is\nK means which one do you think is better\nover here you see this these all\noutliers are not combined inside a group\nBut whichever are nearer as a core point\nand the broader point separate separate\ngroups are actually\ncreated right so this is how amazing a\nDB scan clustering is a DB scan\nclustering is pretty much amazing that\nis basically the outcome of this here in\nC's clustering you can see this all\nthese points has also been taken as blue\ncolor as one group because I'll be\nconsidering this as one group but here\nwe are able to determine this in a\namazing groups so in I'm saying you guys\ndirectly use DB scan with without\nworrying about anything so now let's\nfocus on the Practical part uh I'm just\ngoing to give you a GitHub link\neverybody download the code guys I've\ngiven you the GitHub link quickly\ndownload and keep your file ready I'm\ngoing to open my anaconda prompt\nprobably open my jupyter notbook we'll\ndo one practical problem I've given you\nthe link guys please open it so this is\nwhat we are going to do today this will\nbe amazing here you'll be able to see\namazing things how do you come to know\nthat over fitting or underfitting is\nhappening you don't know the real value\nright so in in clustering there will not\nbe any underfitting or overfitting so uh\nwhat all things we'll be importing first\nis that we'll try cin clustering we'll\ndo silot scoring and then probably we'll\nsee the output and um and we'll do DB\nscan Also let's say DB scan is also\nthere so uh what are the things we have\nbasically imported one is the cin\nclustering one is the Sout samples and\nSout scores these all are present in the\nSK learn and it is present in metrics\nthat basically means we use this\nspecific parameter to validate\nclustering models okay now we'll try to\nexecute this and apart from that mat\nplot lib we are just trying to import\nnumai we are trying to import and all\nhere we are executing it perfectly the\nnext thing is that here the next step is\nthat generating the sample data from\nmake underscore blobs first of all we\nare just trying to generate some samples\nwith some two features and we are saying\nthat okay should have four centroids or\nC centroids itself with some features\nI'm trying to generate some X and Y data\nrandomly and this particular data set\nwill basically be used in performing\nclustering algorithms okay forget about\nrange undor ncore clusters because we\nneed to try with different different\nclusters and try to find out the solid\nscore so right now I just initialized\nwith 2 3 4 5 6 values it is very simple\nso if I go and probably see my X data so\nmy X data will look something like this\nso this is my X data with two features\nand this is my Y data with one feature\nwhich is my output which belongs to a\nspecific class okay so that you can\nactually do with the help of make\nunderscore blobs let's say how to apply\nkin's clustering algorithm so as I said\nthat I will be using W CSS W CSS\nbasically means within cluster sum of\nsquare so I'm going to import K means\nover here for I in range 1A 11 that\nbasically means I'm going to use\ndifferent different K values or centroid\nvalues and try to C which is having the\nminimal wcss value and I'll try to draw\nthat graph which I had actually shown\nyou with respect to Elbow method so here\nI will basically be also using K means\nnumber of clusters will be I and\ninitialization technique I will will be\nusing K means Plus+ so that the points\nthe centroids that are initialized those\nthose points are very very far and then\nyou have random state is equal to zero\nthen we do fit and finally we do wcss do\nupend cins doin inertia okay this dot\ninertia will give you the distance\nbetween the centroids and all the other\npoints and this is what I'm going to\nappend in this wcss value and finally\nI'll just plot it now here you can see\nthat I'm just plotting it obviously by\nseeing this graph this graph looks like\nan elbow okay this graph looks like an\nelbow so the point that I'm actually\ngoing to consider over here see which is\nthe last abrupt change so if I talk\nabout the last abrupt change here I have\nthe specific value with respect to this\nokay I have one specific value with\nrespect to this this is my abrupt change\nfrom here the changes are normal so I'm\ngoing to basically select K is equal to\n4 now what I'm actually going to do with\nthe help of sart with the help of s CL\nscore we are going to compare whether K\nis equal to 4 is valid or not so that is\nwhat we are going to do valid or not so\nhere we are going to do this now let's\ngo ahead and let's try to see it how we\nare going to do it so here you can see n\nclusters is equal to 4 then I'm actually\nable to find out the prediction and this\nis specifically my output okay this is\ndone now see this code okay this code is\na huge code I have actually taken this\ncode directly from the SK learn page of\nSilo if you go and see this this code is\ndirectly given over there but I'm just\ngoing to talk about like what are the\nimportant things we need to see over\nhere with respect to different different\nclusters see see this clusters 2 3 4 5 6\nI'm going to basically compare whether\nthe K value should be four or not with\nthe help of solid scoring so let's go\nhere and here you can see that I'm\napplying this one first I will go with\nrespect to for Loop for ncore clusters\nin range underscore clusters different\ndifferent cluster values are there first\nwe'll start with two so here you can see\ninitialize the cluster with and cluster\nvalue and a random generator seed of 10\nfor reproducibility so ncore clusters\nfirst I take took it as two and then I\ndid fit predict on X after I did fit\npredictor on X I'm using this score on X\ncomma cluster label now what this is\ngoing to do understand in Solo what did\nwe discuss it will it will try to find\nout all the Clusters the Clusters over\nhere like this and it'll try to\ncalculate the distance between them\nwhich is the a of I then it'll try to\ncompute the B of I then finally it'll\ntry to compute the score and if the\nvalue is between minus1 to +1 the more\nthe Valu is towards + one the more\nbetter it is right so these all things\nwe have already discussed and that is\nwhat this specific function will do and\nthis will give my solo average value\nover here solid value will be over here\nokay this we have done and then we can\ncontinuously do it for another another\nthings you can actually find it over\nhere and this value that you see this\ncode that you see is nothing nothing so\ncomplex okay this is just to display the\ndata properly in the form of graphs okay\nin the form of graphs so again I'm\ntelling you I did not write this code\nI've directly taken it from the uh SK\nlearn page of solid okay so just try to\nsee this particular uh plotting diagrams\nand all that you can definitely figure\nout but let's see I will try to execute\nit and try to find out the output now\nsee for ncore cluster is equal to 2 the\naverage solid score is 70 I told you the\nvalue will be between -1 to +1 and I'm\nactually getting 704 which is very very\ngood and then for ncore cluster is equal\nto 3 588 then ncore cluster is equal to\n4 I'm getting 65 which is pretty much\namazing and then for ncore cluster equal\nto 5 the average score is 563 and ncore\ncluster is equal to 6 you are saying\n.45 here directly you can actually say\nthat fine for _ cluster equal to 2 I'm\ngetting an amazing score of\n704 obviously you're you're getting the\nhighest value over this so should we\nselect ncore cluster isal to two Okay we\nshould not directly conclude from it\nbecause here we need to also see that\nany feature value or any cluster value\nis also coming as negative value that\nalso we need to check so here we will go\ndown over here you will see the first\none over here with respect to the first\none you see that I'm get getting the\nvalue from 0 to 1 it is not going going\nto Min -.1 so definitely two clusters\nwas able to solve the problem so I'll\nkeep it like this with me I definitely\nhave a chance that this may this may\nperform well I may have a chance that\nthis K uh K is equal to 2 May perform\nwell okay so I may have a chance let's\nsee to the next one to the next one over\nhere you can see that for one of the\ncluster the value is negative if the\nvalue is negative that basically means\nthe AI is obviously greater than b ofi\nso I'm not going to prer this because it\nis having some negative values even\nthough my cluster looks better but again\nunderstand what is the problem with\nrespect to this cluster is that if I\ntake this cluster and probably compute\nthe distance between this point to this\npoint and if I probably compute from\nthis point to this point or this point\nto this point this point is obviously\nnearer to this right it is obviously\nnearer to this so that is the reason why\nI'm getting a negative value over here\nokay negative value over here this is my\nuh output my score this point that you\nsee dotted points this is my score 58\nwhat whatever it is this is basically my\nscore so obviously this basically\nindicates that this point is near the\nother cluster point is nearer to this so\nI'm actually getting a negative value\nright so this you really need to\nunderstand okay now similarly if I go\nwith respect to ncore Cluster is equal\nto 4 this looks good because here I\ndon't have any negative value and here\nyou can see how cooly it has basically\ndivided the points amazing inly with the\nhelp of k equal to 4 right and similarly\nif I go with five obviously you can see\nsome negative values are here some\ndotted line negative value are there\nwith respect to six you also have some\nnegative values so definitely I'll not\ngo with six I may either go with four or\nI may either go with two now whenever\nyou have this options always take a\nbigger number instead of two take four\nbecause four is greater than two because\nit will be able to create a generalized\nmodel so from this I'm actually going to\ntake and is equal to 4 K is equal to 4\nnow should we compare with this with the\nelbow method here also I got four right\nso both are actually matching so this\nindicates that with the help of this\nclustering this siluette score we can\ndefinitely come to a conclusion and\nvalidate our clustering model in an\namazing way so I hope everybody is able\nto understand and this way you basically\nvalidate a model and definitely you can\ntry it out you can understand this code\ndefinitely I but till here you have\nunderstood that here I'm going to get\nthe average value then for iore clusters\nwhatever cluster this is matching it is\njust mapping over there and it is\nbasically giving so this was the session\nand uh yes in today's session we\nefficiently covered many topics we\ncovered kin hierle clustering solid\nscore DB clustering in tomorrow's\nsession the topics that are probably\npending is first I'll start with svm and\nsvr second I will go ahead with XG boost\nand and third I will cover up PCA let's\nsee whether I'll be able to complete\nthis session uh one one amazing thing\nthat I want to teach you guys because\nmany people ask me the definition of\nbias and variance so guys uh many people\nget confused when we talk about bias and\nvariance you know because let's say that\nuh I have a model for the training data\nset it gives us somewhere around 90%\naccuracy let's say I'm getting a 90%\naccuracy for the test data I may\nprobably getting somewhere around 70%\naccuracy now tell me which scenario is\nbasically this most of the people will\nbe saying that okay fine it is\noverfitting now when I say overfitting I\nbasically mention overfitting by low\nbias and high\nvariance right so many people get\nconfused Krish tell me just the exact\ndefinition of bias and variance low bias\nobviously you are saying that because\nthe training is performed like the model\nis performing well with the help of\ntraining data set but with respect to\nthe test data set the model is not\nperforming well with respect to training\ndata set why do we always say bias and\nwith respect to test data set why do we\nalways say variance so for this you need\nto understand the definition of bias so\nlet me write down the definition of bias\nover here so here I can definitely write\nthat bias it is a\nphenomena that\nskews the\nresult of an\nalgorithm in\nfavor in favor or against an\nidea against an idea I'll make you\nunderstand the definition uh um but\nunderstand the understand understand\nwhat I have actually written over here\nit is a phenomena that skewes the result\nof an algorithm in favor or against an\nidea whenever I say this specific idea\nthis idea I will just talk about the\ntraining data set initially now when we\ntrain a specific model suppose if I have\nthis specific model over\nhere and I'm training with this specific\ntraining data set so this is my training\ndata set now based on the definition\nwhat does it basically say it is a\nphenomenon that skews the result of an\nalgorithm in favor or against an idea or\na this specific training data set so\neven though I'm training this particular\nmodel with this training data set\nwith this data set it may it may be in\nfavor of that or it may be against of\nthat that basically means it may perform\nwell it may not perform well if it is\nnot performing well that basically means\nthe accuracy is down if the accuracy is\nbetter at that point of time what will\nsay see if the accuracy is better that\ntime what we'll say we we'll come up\nwith two terms from here obviously you\nunderstand okay there are two scenarios\nof bias now here if it is in favor that\nbasically means it is performing well\nwith respect to the training data set I\nwill basically say that it has high bu\nif it is not able to perform well with\nthe training data set then here I will\nsay it as low\nbias I hope everybody is able to\nunderstand in this specific thing\nbecause many many many people has this\nkind kind of confusion now similarly if\nI talk about variance let's say about\nvariance because you need to understand\nthe definition a definition is very much\nimportant okay if I if I just talk about\nthe definition of variance I'm just\ngoing to refer like this the variance\nrefers to the changes in the model when\nusing when using different\nportion of the\ntraining or test\ndata now let's understand this\nparticular\ndefinition variance refers to the\nchanges in the model when using\ndifferent proportion of the test\ntraining data or test data we obviously\nknow that whenever initially if I have a\nmodel understand from the definition\neverything will make sense I am\nbasically training initially with the\ntraining\ndata okay because we divide our data set\nsee our data set whenever we are working\nwith we divide this into two parts one\nis our train data and test data okay\nbecause this is a tra test data is a\npart of that particular data set right\nand suppose in this particular training\ndata it gets trained and performs well\nhere I'm actually talking about bias but\nwhen we come with respect to the\nprediction of the specific model at that\npoint of time I can use other training\ndata that basically means that training\ndata may not be similar or I can also\nuse test data now in this test data what\nwe do we do some kind of predictions\nthese are my predictions and in this\nprediction again I may get two\nscenario I may get two scenario which is\nbasically mentioned by variance it\nrefers to the changes in the model when\nusing when using different portion of\nthe training or test data refers to the\nchanges basically means whether it is\nable to give a good prediction or wrong\npredictions that's it so in this\nparticular scenario if it gives a good\nprediction I may definitely say it as\nlow variance that basically means the\naccuracy with the accuracy with respect\nto the test data is also very good if I\nprobably get a bad if I probably get a\nbad accuracy at that time I basically\nsay it as high variance so if I talk\nabout three scenarios over here let's\nsay this is my model one and this is my\nmodel\ntwo and this is my model\nthree now in this scenario let's\nconsider that my model one has the\ntraining\naccuracy of 90% and test accuracy of\n75% similarly I have here as my train\naccuracy of 60% and my test accuracy\nof\n55% now similarly if I have my train\naccuracy of 90% And my test accuracy of\n92% now tell me what what things you\nwill be getting here obviously you can\ndirectly say that fine your training\naccuracy is better now you're talking\nabout bias so this basically indicates\nthat this has low\nbias and since your test accuracy is bad\nbecause it is when compared to the train\naccuracy it is less so here you are\nbasically going to say high\nvariance understand with respect to the\ndefinition similarly over here what\nyou'll say high\nbias High variance because obviously it\nis not performing\nwell this is another scenario last the\nlast scenario is that this is the\nscenario that we want because it is low\nbias and low variance\nokay many many people have basically\nasked me the definition with respect to\nbias and variance and here I've actually\ndiscussed and this indicates this gives\nme a generalized model and this is what\nis our aim when we are working as a data\nscientist so I hope you have understood\nthe basic difference between V bias and\nvariance and I was able to give you lot\nof examples lot of understanding with\nrespect to this so I hope you have\nactually got this particular uh\nunderstanding of this uh two terms which\nwe specifically talk about high bias low\nbias High variance low variance right so\nthis was it from my side guys uh and uh\nI hope you have understood\nthis\nokay so let's take let's consider a data\nset credit\nand let's say this is a\napproval so we are going to take this\nsample data set and understand how does\nXG boost work suppose salary is less\nthan or equal to 50 and the credit is\nbad so approval the loan approval will\nbe zero that basically means he he or\nshe will not get if it is less than or\nequal to 50 if the credit score is good\nthen probably approval will be one if it\nis less than or equal to 50 if it is\ngood\nagain then it is going to get one if it\nis greater than\n50 and if it is bad then obviously\napproval will be\nzero if it is greater than\n50 if it is good we are going to get it\nas one if it is greater than\n50k and probably if it is normal then\nalso we are going to get\nit so this is this is my data set so how\ndoes XG boost classifier work understand\nthe full form of XG boost is\nExtreme gradient\nboosting extreme gradient boosting so we\nwill basically understand about extreme\ngradient boosting now extreme gradient\nboosting uh will be actually used to\nsolve both classification and the\nregression problem statement so first of\nall let's understand how it is basically\nexib basically how it actually if you if\nyou just talk about XG boost you\nunderstand that it is a boosting\ntechnique and internally it tries to use\ndecision tree so how does this decision\nTre is basically getting constructed in\nthe case of XV boost and how it is\nbasically solved we are going to discuss\nabout it so whenever we start exib boost\nclassifier understand that first of all\nwe create a specific base model suppose\nif I say this is my base model and this\nbase model will be a weak learner okay\nand this base model will always give an\noutput of probability of 0.5 in the case\nof classification problem so suppose if\nI say this is probability 0.5 then I\nwill try to create a field over here\nthis field is called as residual field\nso first base model what I'm going to do\nany data set that you give from here to\ntrain it will always give you the output\nas 0.5 so this is just a dummy base\nmodel now tell me if my probability\noutput is is 0.5 if I want to calculate\nthe residual that basically means I need\nto subtract approval minus this\nparticular value so what will be the\nvalue over here 0 -.5 will be\n-.5 1 -.5 will be5 1 -.5 will\nbe5 and 0 -.5 will be -.5 and this 1 -.5\nwill\nbe uh 0.5 and this will also be 0.5\nlet's consider that I have one more\nrecord uh and this specific record can\nbe anything uh because I want to keep\nsome more records over here so let's\nconsider that I have one more record\nwhich is less than or equal to 50K and\nif the credit scod is normal you're\ngoing to get zero so here also if I try\nto find out the residual it will be\nminus5 now the first step I hope\neverybody's understood we have to create\na base model okay this base model is\nvery much important because we have to\ncreate all the decision Tree in a\nsequential manner so the first\nsequential base tree which is again this\nis also a decision tree kind of thing\nyou can consider but this is a base\nmodel which takes any inputs and gives\nby default the probability as 05 now\nlet's go ahead and understand what are\nthe steps in constructing decision tree\nafter creating the base model the first\nstep is that create uh binary decision\ntree so I'm going to write it down all\nthe steps please make sure that you note\nit down so so create a binary tree\nbinary decision tree using the features\nsecond step we basically Define we we we\nsay it as okay Second Step what we do we\nactually calculate the similarity weight\nwe calculate the similarity weight I'll\ntalk about this similarity weight what\nexactly it is if I want to use this a\nformula it is summation of residual\nSquare\ndivided\nby summation of probability 1 minus\nprobability plus Lambda I'll talk about\nthis what is exactly Lambda it is the\nkind of hyperparameter again so that it\ndoes not overfit the third thing is that\nwe calculate the Information Gain okay\nInformation Gain so these are the steps\nwe basically use in constructing or in\nsolving uh in creating an HD boost\nclassifier the first step is that we\ncreate a inary decision tree using the\nfeature then we go ahead with\ncalculating the similarity weight and\nfinally we go ahead and calculate the\ninformation gain so how does it go ahead\nlet's understand over here and let's try\nto find out okay now let's go ahead and\nlet's try to construct the decision tree\nas I said that let's consider that I'm\nconsidering salary feature So based on\nusing salary feature what I'm actually\ngoing to do I am going to take this as\nmy node and I'm going to split this up\nand remember whenever we are creating\ndecision Tree in this particular case it\nwill be a binary decision tree let's say\nthat in salary one is less than or equal\nto one is greater than 50 so this two\nyou obviously have in the case of binary\nin case of credit where there are three\ncategories I'll also show you how that\nfurther split will happen and how that\nwill get converted into a binary team so\nhere you have less than or equal to 50K\nand greater than 50k now let's go ahead\nand understand how many vales are there\nin this salary so if I see before the\nsplit you can definitely see that I'm\ngoing to use this residual and probably\ntrain this entire model now if I really\nwanted to find out the residual\ninitially these are my residuals over\nhere so one resid is -.5 then I have 0.5\nover here then I have .5 then again I\nhave -.5 then again I have 0.5 then\nagain I have 0.5 and finally I have\nminus .5 so these are my total residuals\nthat are there suppose if I make this\nsplit less than or equal to 50 First\nless than or equal to 50 the residuals\nwhat are things are there so here I'm\ngoing to have minus5 then less than or\nequal to 50 again I'm going to have 05\nthen again less than or equal to 50 I'm\ngoing to have 0.5 and less than or equal\nto again one more 0.5 is there I'm just\ngoing to remove this the last5 which is\nnothing but Min -.5 so I hope you\nunderstood this split so half of the\nthings came over here the remaining half\nwill be greater than or equal to greater\nthan 50 so you have one value here one\nvalue here one value here so it will be\nMin -.5 then you have 0.5 and then\nfinally you have 0.5 residuals how do we\nget it guys see from the base model\nwhich is by default giving 0.5 first my\ndata goes over here by default\nprobability I'm going to get 0.5 so\nresidual is basically calculated from\nthis probability and approval so this\nprobability minus approval so if you\nsubtract 0 -.5 sorry I'm just going to\nrub this so if you subtract 0 -.5 you're\ngoing to get -.5 1 -.5 you're going to\nget .5 1 -.5 you're going to get .5 so\neverybody I hope is very much clear with\nrespect to this so this is the first\nstep we constructed a binary tree now in\nthe second step it says calculate the\nsimilarity weight now how to calculate\nthe similarity weight similarity weight\nformula is sum of residual Square now\nwhat is residual Square let's say that\nI'm going to calculate the the the uh\nI'm going to calculate for this okay\nsimilarity weight now in this particular\ncase if I go and calculate my similarity\nweight it will be summation of residual\nSquare this is my residual values this\nis my residual Valu so I'm going to do\nthe summation of this Square okay this\nvalue square you can see over here sum\nof residual Square everybody you can see\nsum of of residual squares so what do\nyou think sum of residual squares will\nbe in this particular case how I have to\ndo it I will just take up this all\nvalues like\n-.5\n+5\n+5 and\n-.5 whole square right I'm just going to\ndo the squaring of this divided by\nunderstand what it is divided by it is\ndivided by probability of 1 minus\nprobability now where do we get this\nprobability value where do we get this\nprobability value value we get this\nprobability value from our base model\nright so here I'm basically going to say\nthat we are going to do the summation of\nprobability of 1 minus probability 1\nminus probability that basically means\nfor each and every point for each and\nevery Point what is the probability see\nprobability is basically coming from the\nbase model so for each Pro each point\nI'm going to come compute two things one\nis the probability and then 1 minus\nprobability and this I'm going to do the\nsumm\nlike this I will do it four times 1 -.5\nthen .5 * 1 -.5 and finally you'll be\nable to see one more will be there which\nis\n+5 1 -.5 so this will be your total\nthings with respect to this so I hope\nyou have understood till here uh where\nyou are able to understand that what we\nhave done this is summation of uh\nresidual square and this is the\nremaining probability multiplied by 1\nminus probability now tell me what are\nyou able to find out from this if you\ncancel this and this this and this this\nvalue is going to become zero so this\nentire value is going to become Zer\nbecause 0 divided by anything is 0er so\nhere I hope everybody is understood what\nis the similarity weight of this\nspecific node if I want to write it is\nnothing but zero now you may be\nconsidering where is Lambda\nvalue okay we will initially initialize\nLambda by 1 I'll talk about this hyper\nparameter let's consider it as 1 so here\n+ 1 or plus 0 let's let's consider\nLambda value 0 let's say for right now\nokay I'm just going to make it Lambda is\nequal to0 I'm just going to talk about\nit because it is a kind of hyper\nparameter by Z -.5 -.5 +5 +5 if I do the\nsummation if I do the summation here you\nwill be able to see that I'm going to\nget zero so this calculation we have\ndone and we have got uh the sumission of\nweight is equal to Z and let's go ahead\nand calculate the sumission of the\nweight of the next node no no no it's\nnot first Square it is whole squar so\nhere also if I do so it is5 +5 now let's\ndo it for this if I want to find out the\nsimilarity weight again see I'm going to\nrepeat it .5 +5 whole squ and since\nthere are three points so I'm going to\nbasically use probability 1 minus\nprobability for one point then plus\nprobability 1 minus probability second\npoint and then probability and 1 minus\nprobability for the third point and\nLambda is zero so I'm not going to write\nanything now go let's go and do the\ncalculation for this node so - 5 - 5 it\nbecomes zero then .5 whole square right\nso here I'm going to get 0.25 here if\nyou do the calculation here you are\ngoing to get 75 so this value is going\nto be 1x3 and which is nothing at33 so\nthe similarity weight for this node for\nthis node\nis33 so here you can see probability of\nmultiplied by 1 minus\nprobability okay now the next step that\nwe do is that calculate the information\ngain now you know how to calculate the\ninformation gain but before that let's\ndo the computation for this also for\nthis root node also go ahead and\ncalculate the similarity weight of\nthis okay they\nwhy the base model probability is5\nbecause it is just understand that it is\na dummy dummy model I have just put a if\ncondition there saying that it is going\nto give 0.5 now do it for this one guys\nroot node what it will be see I can\ncalculate from here only minus1 gone\nthis is also gone this is also gone this\nwill be .25 divided by something now\ntell me guys what should be for the root\nnode what is the similarity similarity\nweight what is the similarity weight for\nfor this do this calculation everyone up\none I know it will be. 25 divided by\nthis will be 1.75 are you getting this\nsimilarity weight which will be nothing\nbut 1 by 7 and if I divide 1 by 7 if I\nsay what is 1 by 7 it\nis42 so it is nothing but .14 if I want\nto calculate the root node similarity\nweight over here\nis4 so I know 0.14 here 0 here 33 now\nsee over here we calculate the\nInformation Gain Next Step the third\nstep what we do is that we calculate the\ninformation gain now Information Gain is\nnothing but in this particular case the\nroot node similarity weight we'll try to\nadd up so I will be getting\n0.33 minus this particular Top Root node\nwhatever split has happened that\nsimilarity weight I'll take 0 +33\n-14 so Point\n-14 and if I do it it is nothing but\njust open your calculator again and\n33\n-14 so it is nothing but .19 I'm getting\n.19 as my information gain the\ninformation gain of this specific tree I\ngot it\nas19 obviously you know how the features\nwill get selected based on the\nInformation Gain but let's say that the\nhighest Information Gain that is given\nby salary okay now we will go ahead and\ndo the further split let's go ahead and\ndo the further split so I I know my\ninformation gain now it is1 n and\nInformation Gain is basically used to\nselect that specific node through which\nthe split will happen now I'll further\ngo and do the split let's say that I'm\ngoing to do the further split with the\nnext feature that is which one credit so\nI'm going to take credit over here I'm\ngoing to take credit over here and again\nI have to do a binary split again but\nyou may be considering chish here are\nonly three categories how we are going\nto basically do this particular split\nright because we don't know how to do\nthe split because we have three\ncategories over here so in this case\nwhat I will do is that we what we can\ndefinitely do is that in this particular\ncase the split that we are probably\ngoing to do is that let's consider two\ncategories like good and normal at one\nside bad at one side so here it becomes\na binary split again now let's go ahead\nand let's try to see that how many data\npoints will fall here and how many data\npoints will fall here so for writing\ndown the data points let's say if it is\nless than or see go to the path if it is\nless than or equal to 50 it'll go this\npath and if it is B then we are probably\ngoing to get how much is the residual we\nare going to get one residual over here\nfirst of all so this is my one residual\nthat is -.5 then similarly if I see less\nthan or equal to 50 good is there right\ngood or normal is there so here again 0.\nfive will come I hope everybody is able\nto understand see the second record less\nthan or equal to 50 we go in this path\nbut it is good we come over here again\nless than or equal to 50 good again we\nare going to get 1\nmore5 then go with respect to greater\nthan or equal to 50 which is coming over\nhere we'll not worry about it right now\nagain less than or equal to 50 normal\nagain it is\n-.5 right so this many records\ndefinitely coming over here only one\nrecord is basically coming over here\nthen again we will start the same\nprocess again we will start the same\nprocess now for the same process what we\nare going to do again try to calculate\nthe similarity weight now in order to\ncalculate the similarity weight what I\nwill do I will basically say this is my\nsimilarity weight this will become .25\ndivided 025 why because this whole\nsquare right this whole Square residual\nsquare right summation of residual\nsquare but here I have only one residual\nso this Square it will become and then\nwhat I'm actually going to do I'm going\nto basically write .5 - 1 -.5 this is\nnothing for only for one data point so\nthis is nothing but .5 * .5 which is\nnothing but 0.25 right now in this\nparticular case I will get similarity\nweight as I hope everybody I'm getting\nit as one now what about this similarity\nweight if you want to compute it is\nagain very very simple this and this\nwill get cancelled then again it will be\n025 divided by um if I say one like this\n.25 then again it will be 75 then this\nwill also be 1 by3 that is nothing but\n33 so similarity weight will\nbe33 then again I have to calculate the\ninformation gain of this node what I\nwill do I will add this up see 1\n+33 I'll add like 1\n+33 minus 0 why zero because the\ninformation gain the similarity weight\nof this uh the up one is basically 0\nright for this particular credit node\nsimilarity weight is zero so 1\n+33 minus 0 this will be 1.33 so like\nthis further split will again happen\nover here with different different node\nand we will only be getting a binary\nsplit but we will be comparing based on\nInformation Gain which one is coming\ngood now let's say that I have created\nthis path I have I have designed I have\ndeveloped my entire binary decision tree\nwhich is a speciality in XG boost now\nwhat I'm going to do over here is that\nsee everybody what I'm going to do let's\nconsider the inferencing part let's say\nthis record is going to go how we are\ngoing to calculate the output so this\nfirst of all went to this base model now\nlet's go ahead and see how the\ninferencing will happen suppose This\nRecord is going right so first of all\nthis record will go to this base model\nthe base model is giving the probability\nas 0.5 so the first base model is\nbasically giving 0.5 now base based on\nthis 05 how do we calculate the real\nprobability how do we calculate the real\nprobability in this okay so we apply\nsomething called as logs so we basically\nsay log of P / 1us P so this is the\nformula we basically apply in only the\ncase of base model so if we try to see\nthis it is nothing but log\nof5 / .5 which is nothing but zero log\nof one is nothing but zero so in the\nfirst case whenever any record goes I\nwill be getting the zero value over here\nokay zero value over here then plus why\nplus I'm doing because it will now go to\nthe binary decision tree now this record\nwill go to my binary decision Tre\nwhatever value I'm getting from this I'm\nactually adding that up and now it will\ngo over here now when it goes over here\nfirst of all let's see which branch it\nis following it is following less than\nor equal to 50 Branch first Branch over\nhere then this is bad it'll go and\nfollow here so here I can see that the\nsimilarity weight is one now the\nsimilarity weight is basically one in\nthis case so what we do in the case of\nthis we pass it to a learning rate\nparameter so this specifically is my\nlearning rate multiplied by 1 one\nbecause why similarity weight is one\nover here so this will basically be my\nfirst references and Alpha over here is\nmy learning rate it can be a very small\nvalue based on the learning parameter\nthat we use like how we have defined\nlearning parameters elsewhere on top of\nthis we apply an activation function\nwhich is called as sigmoid since this is\na classification problem we apply an\nactivation function which is called as\nsigmoid and I hope you know what is the\nuse of sigmoid based on this based on\nthe alpha value based on this the output\nwill be between 0 to 1 now I hope you\ngetting it guys this is how the entire\ninferencing will probably happen now\nsimilarly what I will do I will try to\nconstruct this kind of decision tree\nparall so we we can also write our\nentire function will look something like\nthis Alpha 0 + alpha 1 and this will be\nyour decision tree 1 output then Alpha 2\nyour decision tree output Alpha 3 your\ndecision 3 output like this Alpha 4 your\ndecision 3 output fourth decision tree\nlike this it will be alpha n your\ndecision tree n output and this will be\nyour output finally when you're trying\nto inference from any new\nrecord now the reason why we say this as\nboosting because see understand we are\ngoing to add each and every decision\ntree output slowly to finally get our\noutput with respect to the working of\nthe decision tree this is how XG boost\nactually work don't credit further needs\nto be simplified yes see like this\nsimilarly we can split credit with the\nhelp of like we can make blue green one\nside normal at one side But whichever\nwill be giving the information gain more\nthat will be taken into consideration\nright and this is how your entire X\nboost classifier works it is very very\ndifficult to basically calculate all\nthose things so that is the reason we\nsay that XG boost is also a blackbox\nmodel so this is basically a blackb\nmodel it is it prone to overfitting see\nat one stage we also need to perform\nhyperparameter tuning and this we\nspecifically say pre- pruning we tend to\ndo pre pruning and since we are\ncombining multiple decision trees no no\nthis decision tree this decision tree is\nthis one this independent decision tree\nwhich I have created now parall after\nthis what I'll do I'll create one more\ndecision tree so it'll be looking like\nthis see finally how it will look so\nthis is my base model then my data then\nmy data will go to this decision tree\nwhich I have actually done as a binary\nsplit on different different records\nthen again we will make another decision\ntree which will again be a binary tree\nthe splits will look like this then this\nis my base model where I'm getting the\nvalue as zero this will be alpha 1\nmultiplied by decision tree 1 which is\nthis then this is Alpha 2 multiplied by\ndecision tree 2 which is this and like\nthis we will keep on continuously adding\nmore decision trees unless and until\nthis entire things becomes a very strong\nlearner so this is how how we basically\ndo the combination of all these things\nso I hope everybody is able to\nunderstand about the XG boost classifier\nnow you may be thinking how does\nregressor work do you want a regressor\nproblem statement also the decision tree\nwill get constructed based on\nIndependent features and again Lambda\nvalue is a hyperparameter we basically\nset up Lambda value with the help of\ncross validation now uh let's go ahead\nand discuss about ex boost regressor the\nsecond algorithm that we we will\nprobably discuss about is something\ncalled as XG boost regressor and how\ndoes X boost regressor actually work\nsome fundamental is follow in random\nForest no in random Forest it is\ncompletely different there bagging\nhappens bagging happens so over here\nlet's go ahead with the regressor so\nhere I'm going to take some example\nlet's say that I have this many\nexperience this many Gap and based on\nthat we need to determine the salary my\nsalary is my output feature let's say\nthe experience is 2 2.5 3 4 4.5 okay now\nin this Gap let's say it is yes\nyes no no yes and let's say that the\nsalary is somewhere around 40K it is\n41k\n52k and uh let's see some more data set\nover here 60k and 62k now the first step\nin classifier we created a base model\nhere also we'll try to create a base\nmodel first of all this base model what\noutput it will give it will give the\naverage of all these values what is the\naverage of all these values okay what is\nthe average of all these value 40 81 52\n60 62 if I just do the average it is\nnothing but 51k so by default I will\ncreate a base model which will take any\ninput and just give the output as 51\nthis is the first step now based on this\nI will try to calculate my residual now\nhow do I calculate my residual I will\njust subtract 40 by 51k so this will\nbasically be - 11k\nand uh this will be 10 K - 10 K - 10 and\nthis will be 1 this will be 9 and this\nwill be 11 I hope everybody's able to\nget this let's say that I I make this as\n42k okay for just making my calculation\nlittle bit easy so I have 9 over here so\nthis is my residual then again the first\nstep is that I construct my uh decision\ntree now let's say say that I'm going to\nuse The Experience over here so this is\nmy experience node and based on this\nexperience node I have my features over\nhere so here I will take up all my\nresiduals - 11 99 1 99 11 and then how\ndo I do the split based on experience\nthis is a continuous feature so I have\nto basically do split with respect to\ncontinuous feature which I have already\nshown you in decision tree how do we do\nso here is my residual here it is 40\nminus this\nis - 11 K - 9 K uh this is 1 K this is 9\nK and\n11k - 9k so now I will just create take\nup my first node here I'm going to use\nmy experience feature I know my values\nwhat all things are going to come 11k in\nthe root node - 9 1 9 and 11 now what we\nare going to do over here is that so I'm\ngoing to do again a binary split over\nhere now the binary split will happen\nbased on the continuous feature that is\nexperienced so two types of Records I\nmay get one is less than or equal to two\nand one is greater than 2 less than or\nequal to two and one is greater than two\nnow less than or equal to two when I do\nthe split let's see how many values we\nare getting less than or equal to two I\nwill get only one value that is -1 and\nhere I'm actually going to get all the\nother values - 9 1 9 11 now what we are\ngoing to do after this is that calculate\nthe similarity weight now here the\nsimilarity weight will little bit the\nformula will change with respect to\nregression so similarity weight is\nnothing but summation of residual\nsquares divided by number of residuals\nplus Lambda again here we are going to\nconsider Lambda is zero because this is\na hyper parameter tuning more the value\nof Lambda that basically means more more\nwe are penalizing with respect to the\nresiduals so this will be the formula\nthat we are going to apply okay so let's\nsee for the first number that that we\nwant to apply so how this will get\napplied again I'm going to write this\nformula here it'll be better let's say\nhere similarity weight is equal to\nsummation of residual square and here\nyou have number of residuals plus Lambda\nsee previously we were using probability\nand then all those things we are using\nso if you want to calculate the\nsimilarity weight of this this will\nbecome 121 divided by number of residual\nis 1 plus Lambda is 0 so this is going\nto be 121 so here we are going to\ncalculate the similarity weight which is\nnothing but 121 if if we probably take\nAlpha let's let's do one thing if we\nprobably take uh if if we probably take\nAlpha is equal to 1 then what will\nhappen if you take Alpha is equal to 1\njust think over here what will what may\nhappen we may directly penalize the\nsimilarity weight right by just adding\none okay so let's do that also suppose I\nsay I'm going to take Alpha is equal to\n1 so what will happen this will not be\nthe formula now now what will become 121\ndivided number of residual is 1 + 1 this\nis nothing but 65.5 let's say that I now\nhave 65.5 as my similarity weight now\nsimilarly I will go ahead and compute\nthe similarity weight for the next one\nso here it will become - 9 + 9 + 9 + 11\nwhole Square divided 4 + 1 so this and\nthis will get subtracted 12 squ is\nnothing but 14 4 144 divid 5 so if I go\nahead and calculate 144 ID 5 it is\nnothing but 28.5 so here I get\n28.5 so the similarity weight for this\nis\n28.5 similarly I can go ahead and\ncalculate the similarity weight for this\nfor the top one so it'll be nothing but\nwhat it will be 11 + sorry - 11 - 11 - 9\n+ + 1 + 9 + 11 divided 1 2 3 4 5 5 + 1\nis 6 so this is getting subtracted this\nwill be 1X 6 anyhow this will be whole\nsquare right so anyhow it will be 1X 6\nonly so 1X 6 will be my similarity\nweight over here okay 28.8 hits okay now\nfinally The Information Gain that we\nneed to compute will be very much simple\nwhat will be the Information Gain 65.5 +\n28.8\nminus 1X 6 so try to get it whatever we\nare trying to get it over here just tell\nme what will be the output is it 98.34%\n60.5 60.5 + 28 88 then this will change\njust a second 89.1 3 understand you\ndon't have to worry about calculation\nautomatically that things will be doing\nit okay so you don't have to worry now\nsee we have now further the decision\ntree can be splitted into any number of\ntimes probably the next split what we\ncan do is that we can we can do next\nsplit something like this this will be\nmy experience the two splits that may\nhappen with respect to less than or\nequal to 2.5 less than or equal to 2.5\nor greater than 2.5 now if this probably\ngives the Information Gain better then\nthe split will happen like this\notherwise whichever gives the better\ninformation again the split will\nbasically happen like this I hope like\nlet's say that this is this is the split\nthat is required - 11 - 11 is 9 is over\nhere and then we have 1 comma 9A 11 okay\nbecause less than or equal to 2.5 this\ntwo records will definitely go over here\nand this two This Record will definitely\ngo over here now if I try to calculate\nthe similarity weight for this it will\nbe nothing but - 11 - 9 - 11 - 9 whole S\nided 2 + 1 right now in this particular\ncase it will be - 20 s / 3 which is\nnothing but 400 2 20 into 20 is 400\nwhich is nothing but 3 so if I go and\nprobably use a\ncalculator and show it to you\n400 / 3 which is nothing but\n133.33 so the similarity weight for this\nis\n133.33 similarly I can go ahead and\ncompute for this it will be 1 + 9 + 11\nwhole s / 3 + 1 right so it will be 10 +\n11 10 + 11 is nothing but 21 whole s/ 4\nso what it is 21 whole square if I open\nmy calculator 21 s 21 * 21 which is\nnothing but 441 divid by 4 divid by 4 so\nthis will probably 110 110.\n2.25 and similarly I can go ahead and\ncompute for this so if I want to compute\nfor this what it will be the same thing\nthat we have got over here that is 1x 6\nso this will basically be 1X 6 so\nfinally if I compute the information\nagain it will be what it will be 133\n1333 +\n1.25 - 1X 6 obviously this value will be\ngreater than the previous one what we\nhave got that is\n8913 so definitely we are going to use\nthis split which is better than the\nprevious split right let's say that this\nsplit has been considered finally how do\nwe see the output okay I hope everybody\nis able to understand right let's say\nthat this split has worked well so I'm\ngoing to rub all these things\n11.25 is there now suppose I want to do\nthe inferencing how the inferencing will\nbe done\n11.25 here 110.2 now suppose any record\ncomes from here first of all any record\nthat will go it will go to the base\nmodel so the base model whenever it goes\nthe value is 51 51 plus alpha 1 this is\nmy learning rate one suppose if it goes\nin this route then what we have we have\n- 11 - 9 whenever we go in this rote\nwhich has - 11 and - 9 the average of\nboth these numbers will be considered\nwhat is average of both these numbers -\n11 - 1 9/ 2 this is nothing but - 10\nright so - 10 will get multiplied here\nsuppose if it goes in this route then\nhere what will happen here will 1 + 9 +\n11 divide by 3 average will be taken so\n21 divid 3 7 will be there so this will\nget replaced by 7 so similarly anything\nthat you are doing this is with respect\nto decision tree 1 like this we will\nagain construct decision tree separately\nand again it will become Alpha 2 by\ndecision Tre 2 Alpha 3 by decision 3 3\nand like this you will be doing till\nAlpha and decision 3 n and once you\ncalculate this this will be your\nspecific output in a regression tree so\nin this particular case what will happen\nyou're just trying to play with\nparameters and you're trying to use in a\ndifferent way to compute all this things\neverybody clear but again it is a\nblackbox model you cannot visualize all\nthis things now let's go to the third\nalgorithm which is called as s VM see\nsvm is almost like decision uh logistic\nregression okay so the major aim of svm\nis\nthat major aim of svm is that suppose if\nI have a do data points like this okay\nwe obviously use uh logistic regression\nto split this data points right like\nthis we try to create a best fit line\nwhich looks like this and probably based\non this best fit line we try to divide\nthe point now in svm what we do is that\nwe not only create a best fit line but\ninstead we also create a point which is\ncalled as marginal\nplanes so like this we create some\nmarginal\nplane so this is your hyper plane and\nthis is your marginal plane and\nwhichever plane has this maximum\ndistance will be able to divide the\npoints more efficiently but usually in\nin a normal scenario you know whenever\nwe talk about hyper plane or whenever we\ntalk about marginal plane there will be\nlot of overlapping of points right\nsuppose if I have some specific points I\nhave one point which looks like this I\nmay also have another points which may\noverlap so it is very difficult to get\nan exact straight marginal planes and\nsplit the point based on this now this\nspecific marginal plane should be\nmaximum because we can create any type\nbest fit line and probably\nuh use this marginal plane now if we\nhave this overlapping right if for what\ndo we call for this kind of plane this\nkind of plane is basically called as\nhard marginal plane so this is basically\ncalled as hardge marginal plane okay and\nsimilarly if any points are overlapping\nsuppose this yellow points can also get\noverlapped over here and there may be\nsome kind of Errors so for this\nparticular case we basically say as soft\nmarginal plane because here we will be\nable to see that errors will be there\nnow in asvm what we focus on doing is\nthat we focus on creating this marginal\nplane with maximum distance even though\nthere are some errors we consider it in\nsolving it by providing some kind of\nhyper parameter now how do we go ahead\nand basically create this all marginal\nplanes and how do we go ahead with this\nit's very much simple uh just imagine in\nthis specific way that initially let's\nconsider that I have this data point\nsuppose this is my\nbest fit line how do we give this best\nfit line as equation we basically say\nyal mx + C right we we basically say\nthis equation as y mx + C no hard hard\nmarginal it is impossible in a normal\ndata set obviously you'll not be able to\nget it but definitely we go ahead with\ncreating a soft marginal plan now Y is\nequal to MX plus C what does this m\nindicate m is nothing but slope and C\nindicates nothing but intercept\ncan I say that this both equations are\nsame ax + b y + C isal 0 can I also say\nthat this is the equation of a straight\nline can I say that this is also the\nequation of straight line I will say\nthat both of them are equal can I say\nboth of them are equal see if I try to\nprove this to you if I take this\nequation and try to find out y it will\nbe nothing but minus C Min - c\nminus a sorry - a x and this will be\ndivided by B this will be divided by\nB this will be divided by B so here you\ncan see that it is almost the same in\nthis particular case my M value will be\n- A by B and my C will basically be\nminus C by B so both the equation are\nalmost same\nso let's consider that this is my\nequation and I am actually and whenever\nI say Y is equal to mx + C can I also\nwrite something like this Y is equal to\nW1\nX1 + W2 X2 plus like this plus C or plus\nb same thing no so here also we can\nwrite y w transpose x + B same equation\nright we are basically using same\nequation yes we can also write it in a\ndifferent way but at the end of the day\nwe are also treating something like this\nlet's say that this slope is in this\ndirection if this slope is in this\ndirection then I can basically say that\nlet's consider that the slope is minus\none\nlet's say that this slope is minus one\nsee it is in the negative Direction\nlet's say that this slope is minus one\nI'm just trying to prove that this slope\nis negative value let's consider this\nnow suppose this is one of my point - 4a\n0 and obviously this particular equation\nis given by this particular line is\ngiven by this equation now if I really\nwant to find out the Y value let's say\nthat this is my\nX1 this is my X1 and this is my X2 let's\nsay that\nI want to find out I want to find out\nthis W transpose x + b the Y value based\non this line if I want to compute the y-\nvalue based on this line how will I\ncompute W transpose X basically means\nwhat w value what all things will be\nthere one value is B right B is\nintercept right now intercept is passing\nfrom origin can I say my B will be zero\nobviously I can assume that b will be\nzero now in this particular case if I\ntalk about w w in this case is minus one\nwhich I have initialized over here so if\nI want to do this matrix multiplication\nit will be W transpose can be written as\nlike this and this x value can be\nwritten as -4 comma - 4 and 0 -4 and 0\nright so I can basically write like this\nnow if I do this multiplication what\nwill my value I get I will basically get\nfour right so this is a positive\nvalue this is a positive value Now\nunderstand since this is a positive\nvalue any points that are below this\nline any points that I consider below\nthis line and if I try to calculate the\nY can I say that it will always be\npositive yes or no similarly if I could\nprobably consider one point over here as\n4A 4A 4 now tell me in this 4A 4 if I\ncalculate the Y value what will you get\nwhether you'll get a positive value or a\nnegative value if I try to calculate the\nY value in this case because here only\npositive values will'll be getting right\nso if I calculate the Y value will the Y\nvalue be negative or positive just try\nto calculate how do you calculate again\nI will use y equation this time again my\nslope is minus1 my intercept is zero and\nhere I will have 4 comma\n4 now here Min\n-4 and then this is + 0 this will be Min\n-4 right so this will be a negative\nvalue negative value guys negative see -\n4 + 0 negative so any point that I will\nprobably have in top of this any\npoints Above This Plane right and if I\ntry to calculate the Y value it will\nalways be negative so what two things\nyou are able to get positive and\nnegative so you can consider this\nentirely one category this another\ncategory at least these two things you\ncan basically\nconsider guys I hope everybody is able\nto understand this so this will be my\none\ncategory and this will be my another\ncategory obviously so that basically\nmeans I can definitely use a plane and\nsplit this point I hope everybody is\nable to understand now let's go ahead\nand let's see how this marginal plane\nwill get created and what is the cost\nfunction to basically do this or what is\nthe cost function in making sure that\nthe marginal plane will definitely work\nright it becomes difficult right so\nsuppose let's consider an\nexample suppose I say that this is my\nlines let's say uh I want to basically\ncreate a kind of I have two variety of\npoints one is this point let's say I\nhave all this points like this and the\nother points I have somewhere here let's\nconsider I am just using directly good\nnumber of points so that I can split it\nokay because I will try to talk about it\nwhat I'm actually trying to prove so\nobviously this is my best fit line that\nsplits and apart from that what I will\ndo is that I'll also create a marginal\npoints so in order to create the\nmarginal point I may use some different\ncolor let's see which color this will be\nmy one marginal point remember it will\nbe to the nearest point over here and\nbasically we will construct like like\nthis and similarly here we will be\nconstructing like this I've already told\nyou guys this equation can be mentioned\nat w transpose x + B = 0 right I can\ndefinitely say this because ax + b y + C\nis equal to 0 so this I can also write\nit as W transpose x equal to 0 sorry\nplus b plus b equal to 0 so both are\nsame okay this I don't have to prove it\nI hope everybody's clear with this now\nwhat I'm going to do let's represent\nthis line also with some equation so\nthis line if I want to represent this\nwill be W transpose x + B what value\nwill come over here positive or negative\nC from this line anything above this\nplane right any any any distance that we\ntry to find out it will always be\nnegative so let's say that I'm using it\nas minus one to just read as it is a\nnegative value and this line that I am\ngoing to mention it it will be W\ntranspose x + B is equal to + 1 Min -1\nabove + 1 because we have already\ndiscussed from this point if you're\ntrying to calculate the Y value it is\nalways going to be + one this is going\nto be minus one here I should definitely\nsay this as K okay but I'm not\nmentioning K in many articles you'll see\nit as minus one uh many research paper\nalso they use it as minus one but I\nwould like to specify uh minus and plus\nK but here let's go and write minus1 and\nplus now my aim is to increase this\ndistance okay this distance I really\nwant to increase this distance now in\norder to increase this if I increase\nthis distance that basically means my\nmodel is performing well so let's say I\nwant to find this distance first of all\nso if I write w transpose X Plus Bal to\n1 and here I will write w transpose x +\nB isal minus1 so what I'm going to do\nI'm going to do the computation and\nsubtract it like this so here obviously\nthis will be my X1 this will be my X2\nokay because these are my another points\nX2 and X1 so I can write w transpose X1\n-\nX2 B and B will get cancell and here I\nwill be writing two right so from here\nwe can definitely write two different\nthings let's see what all things we can\nwrite so here this is nothing but the\ndifference between my this plane and\nthis plane which is given by like this\nokay now always understand whenever we\nconsider any any vector vors right any\nvectors right it also has something\ncalled as\nmagnitude so if I want to remove this\nmagnitude I can divide this by W this\nmagnitude of w then only my Vector will\nremain which is indicated like this so\nI'm going to basically divide by this\nparticular operation both both the side\nI'm dividing by this magnitude of w and\nI don't care about the directions over\nhere right now we just care about the\nvectors now when I write like this what\nis our aim our aim is to can I say our\naim is to our aim is to\nmaximize 2 byw can I say this guys yes\nor\nno what is our aim our aim is to\nbasically maximize this right by\nupdating W comma B value I need to\nmaximize this yes everybody's clear with\nthis can I say that yes I want to\nmaximize this yes or no everybody I want\nto maximize this if I maximize this that\nbasically means my marginal plane will\nbecome bigger my marginal plane will be\nbigger okay now can I write along with\nthis that such that y of I my output\nwill be dependent on two different\nthings one is I can say that my y y of I\nis plus of uh is + one when w transpose\nx + B is greater than or equal to 1\neverybody see in this equation what I'm\nactually trying to specify such that y\nof I is + 1 when w transpose x + B is\ngreater than 1 and when it is minus 1\nthat basically means w transpose of X is\nB is less than or equal to minus now\nwhat does this basically mean see all my\nvalues whenever I compute W transpose x\n+ B is greater than or equal to 1 I'm\nobviously going to get this + one when w\ntranspose X+ B is less than or equal to\n1 I'm always going to get the output as\nminus one I hope that is the reason why\nI have actually written like this so\nthis two we have already discussed why\nwe are specifically writing we want to\nincrease the marginal plane which is\nthis this is my marginal plane and I'm\nwriting one condition that my Yi value\nwill be+ one when w transpose X plus b\nis greater than or equal to 1 otherwise\nit when it is less than or equal to\nminus one it is going to be very much\nclear with this transpose condition we\nhave already done it everybody clear\nwith this now on top of it we can add\none more very important Point instead of\nwriting such that and all you can also\nsay that our major\naim our major aim is that if I multiply\ny i multiplied by W transpose X of I + B\nIf I multiply this two this will always\nbe able greater than or equal to 1 for\ncorrect points right for correct points\nbecause understand if it is minus one if\nI'm multiplying with this and if it is a\ncorrect Point minus into minus will\nobviously be greater than or equal to\none only right similarly for this it\nwill be greater than 1 so I can also\ndefinitely say that my major M If I\nmultiply y of I with this it will be\nalways greater than or equal to + 1 U\nwhich is definitely saying that it will\nbe a positive value so this is just a\nrepresentation guys but understand what\nis the minimized cost function this is\nmy minimized cost function maximized\ncost function now I'm going to again\nwrite it down\nmaximize W comma B maximize W comma b 2\nby magnitude of w I can also write\nsomething like this minimize W comma B\nand I can just inverse this which looks\nlike this are these both are same or not\nbecause always understand in machine\nlearning algorithm why do we write\nminimize things because we are trying to\nminimize something okay both are\nequivalent these both are equivalent and\nwhy we specifically write minimization\nbecause in the back propagation when we\nwe are continuously updating the weights\nof w and B so we can definitely write\nlike this so here my main target is to\nminimize this particular value by\nchanging W and B and I will start adding\nsome more parameters over here this is\nfine till here I think everybody has got\nit this is our aim and we are going to\ndo this but I'm going to add two more\nparameters in this Optimizer one is C of\nI and one is summation of I equal 1 to n\nand here I will use something called as\nEA EA of I first of all I'll tell what\nis C of I see if I have this specific\ndata point let's say if some of my\npoints are over here then is it a right\nright prediction or wrong prediction if\nsome of my points are over here is it a\nright prediction or wrong prediction\nobviously it is a wrong prediction if my\npoints are somewhere here is it a WR\nprediction wrong wrong incorrect\nprediction right so this C value\nbasically says that how many errors we\ncan have how many errors we can have if\nit says that fine we can have six errors\nor seven errors how many errors we can\nhave even though we are using the\nmarginal plane how many errors we can\nhave so here I'm specifically writing\nhow many errors we can have this is what\nis specified by C ofi EA of I basically\nsays that what is the summation of I'm\ngoing to write it down since we are\ndoing the sumission this entire term\nbasically mentions that sumission\nof the distance of the values distance\nof the wrong points and how do we\ncalculate the distance from here to here\nsuppose this is a wrong point I will try\nto calculate the distance from here to\nhere I will do the sumission of this\nI'll do the sumission of this I will do\nthe sumission of this similarly for the\nGreen Point another sumission will\nhappen from here to here like this here\nto here and we going to do that specific\nsumission so we are telling that fine if\nyou are not able to fit properly try to\napply this two hyperparameters and try\nto make sure that this many errors are\nalso there it is well and good no\nproblem we will go ahead with that try\nto do the submission of the data points\nand based on that try to construct the\nbest fit line along with the marginal\nplane like this even though there are\nsome errors over here or errors over\nhere we are good to go with respect one\nmore thing is there which is called as\nAl svr svr only one thing is getting\nchanged in svr only this value will get\nchanged so I want you all to explore and\njust let me know this will be one\nassignment for you only this value will\nbe changing remaining everything are\nsame so just try to if you change this\nparticular value that becomes an svr\njust try to explore and just try to find\nout and just try to let me know so\noverall uh did you like the entire\nsession everyone okay in this one more\nthing is there which is called as kernel\nMatrix svm kernel we say it as svm\nkernel now in s VM kernel what happens\nsuppose if I have a specific data points\nwhich looks like this which looks like\nthis so we obviously cannot use a\nstraight line and try to divide it so\nwhat we do we convert this two Dimension\ninto three dimensions and then probably\nwe push our Point like this one point\nwill go like this and the white point\nwill go down and then we can basically\nuse a plane to split it so I uploaded a\nvideo around uh around that and uh you\ncan definitely have a look onto that and\nI have also shown you practically how to\ndo it that is the reason I've created\nthat specific video so great uh this was\nit from my side I hope you like this\nsession so thank you everyone have a\ngreat day keep on rocking keep on\nlearning and never give up\n",
  "words": [
    "today",
    "session",
    "things",
    "basically",
    "going",
    "discuss",
    "first",
    "going",
    "discuss",
    "different",
    "types",
    "machine",
    "learning",
    "algorithm",
    "like",
    "many",
    "different",
    "types",
    "machine",
    "learning",
    "algor",
    "understand",
    "purpose",
    "taking",
    "session",
    "clear",
    "interviews",
    "okay",
    "clear",
    "interviews",
    "go",
    "data",
    "science",
    "interviews",
    "main",
    "purpose",
    "clear",
    "interviews",
    "seen",
    "people",
    "knew",
    "machine",
    "learning",
    "algorithms",
    "proper",
    "way",
    "okay",
    "definitely",
    "able",
    "clear",
    "explain",
    "algorithms",
    "better",
    "way",
    "recruiter",
    "got",
    "hired",
    "first",
    "introduction",
    "machine",
    "learning",
    "specifically",
    "going",
    "talk",
    "ai",
    "versus",
    "ml",
    "versus",
    "dl",
    "versus",
    "data",
    "sign",
    "second",
    "thing",
    "going",
    "talk",
    "difference",
    "supervised",
    "ms",
    "unsupervised",
    "ml",
    "third",
    "thing",
    "probably",
    "going",
    "discuss",
    "something",
    "called",
    "linear",
    "regression",
    "going",
    "clearly",
    "understand",
    "maths",
    "geometric",
    "intuition",
    "next",
    "thing",
    "probably",
    "going",
    "discuss",
    "r",
    "square",
    "adjusted",
    "r",
    "square",
    "fifth",
    "topic",
    "going",
    "discuss",
    "ridge",
    "lasso",
    "regression",
    "first",
    "topic",
    "going",
    "discuss",
    "ai",
    "versus",
    "ml",
    "versus",
    "dl",
    "versus",
    "data",
    "science",
    "first",
    "topic",
    "probably",
    "going",
    "discuss",
    "really",
    "want",
    "understand",
    "difference",
    "ai",
    "versus",
    "ml",
    "versus",
    "dl",
    "versus",
    "data",
    "science",
    "go",
    "specific",
    "format",
    "imagine",
    "entire",
    "universe",
    "entire",
    "universe",
    "probably",
    "call",
    "ai",
    "specifically",
    "say",
    "ai",
    "basically",
    "means",
    "ai",
    "artificial",
    "intelligence",
    "whatever",
    "role",
    "machine",
    "learning",
    "developer",
    "working",
    "deep",
    "learning",
    "developer",
    "vision",
    "developer",
    "data",
    "scientist",
    "ai",
    "engineer",
    "end",
    "day",
    "actually",
    "creating",
    "ai",
    "application",
    "really",
    "want",
    "define",
    "artificial",
    "intelligence",
    "say",
    "process",
    "wherein",
    "create",
    "kind",
    "applications",
    "able",
    "task",
    "without",
    "human",
    "intervention",
    "basically",
    "means",
    "person",
    "need",
    "monitor",
    "ai",
    "application",
    "automatically",
    "able",
    "make",
    "decisions",
    "able",
    "perform",
    "task",
    "able",
    "many",
    "things",
    "ai",
    "application",
    "examples",
    "would",
    "definitely",
    "like",
    "consider",
    "first",
    "example",
    "would",
    "like",
    "consider",
    "ai",
    "application",
    "ai",
    "module",
    "netflix",
    "ai",
    "module",
    "suppose",
    "see",
    "kind",
    "action",
    "movie",
    "time",
    "kind",
    "ai",
    "work",
    "ai",
    "work",
    "basically",
    "implemented",
    "something",
    "called",
    "recommendation",
    "application",
    "happens",
    "continuously",
    "seeing",
    "action",
    "movies",
    "automatically",
    "ai",
    "module",
    "present",
    "inside",
    "netflix",
    "make",
    "sure",
    "gives",
    "us",
    "recommendation",
    "action",
    "movies",
    "second",
    "take",
    "example",
    "comedy",
    "movie",
    "continuously",
    "see",
    "comedy",
    "movie",
    "also",
    "give",
    "us",
    "recommendation",
    "comedy",
    "movie",
    "happens",
    "understands",
    "behavior",
    "able",
    "task",
    "without",
    "asking",
    "anything",
    "second",
    "example",
    "would",
    "like",
    "take",
    "buy",
    "iphone",
    "may",
    "recommend",
    "headphones",
    "kind",
    "recommendation",
    "also",
    "part",
    "ai",
    "module",
    "integrated",
    "website",
    "ads",
    "see",
    "probably",
    "opening",
    "channel",
    "get",
    "paid",
    "little",
    "bit",
    "hard",
    "work",
    "youtube",
    "right",
    "ads",
    "recommended",
    "uh",
    "also",
    "ai",
    "engine",
    "included",
    "youtube",
    "channel",
    "really",
    "plays",
    "goal",
    "understand",
    "business",
    "driven",
    "things",
    "basically",
    "help",
    "ai",
    "one",
    "example",
    "would",
    "like",
    "give",
    "consider",
    "cars",
    "able",
    "see",
    "cars",
    "take",
    "example",
    "tesla",
    "cars",
    "happens",
    "based",
    "road",
    "able",
    "ble",
    "drive",
    "automatically",
    "ai",
    "application",
    "integrated",
    "car",
    "right",
    "consider",
    "things",
    "ai",
    "application",
    "end",
    "day",
    "whatever",
    "role",
    "going",
    "create",
    "ai",
    "application",
    "common",
    "mistake",
    "people",
    "know",
    "like",
    "ceo",
    "sudhansu",
    "kumar",
    "written",
    "profile",
    "ai",
    "engineer",
    "basically",
    "means",
    "goal",
    "create",
    "ai",
    "application",
    "probably",
    "product",
    "based",
    "companies",
    "seeing",
    "kind",
    "roles",
    "called",
    "ai",
    "engineer",
    "let",
    "go",
    "next",
    "role",
    "called",
    "machine",
    "learning",
    "machine",
    "learning",
    "comes",
    "existence",
    "try",
    "create",
    "machine",
    "learning",
    "subset",
    "ai",
    "role",
    "machine",
    "learning",
    "provides",
    "stats",
    "tools",
    "analyze",
    "data",
    "visualize",
    "data",
    "apart",
    "predictions",
    "forecasting",
    "seeing",
    "lot",
    "machine",
    "learning",
    "algorithms",
    "internally",
    "machine",
    "learning",
    "algorithm",
    "equation",
    "basically",
    "using",
    "basically",
    "using",
    "kind",
    "stats",
    "tool",
    "stat",
    "techniques",
    "whenever",
    "work",
    "data",
    "statistics",
    "definitely",
    "much",
    "important",
    "exactly",
    "called",
    "machine",
    "learning",
    "subset",
    "ai",
    "much",
    "important",
    "understand",
    "ml",
    "subset",
    "ai",
    "see",
    "part",
    "let",
    "go",
    "next",
    "one",
    "called",
    "called",
    "deep",
    "learning",
    "deep",
    "learning",
    "subset",
    "ml",
    "let",
    "consider",
    "deep",
    "learning",
    "came",
    "existence",
    "1950s",
    "60s",
    "scientists",
    "thought",
    "make",
    "machine",
    "learn",
    "like",
    "human",
    "learn",
    "particular",
    "purpose",
    "deep",
    "learning",
    "came",
    "existence",
    "plan",
    "basically",
    "mimic",
    "human",
    "brain",
    "say",
    "mimicking",
    "human",
    "brain",
    "basically",
    "means",
    "trying",
    "mimic",
    "human",
    "brain",
    "implement",
    "something",
    "learn",
    "something",
    "use",
    "something",
    "called",
    "neural",
    "networks",
    "deep",
    "learning",
    "subset",
    "machine",
    "learning",
    "main",
    "aim",
    "mimic",
    "human",
    "brain",
    "actually",
    "create",
    "neural",
    "network",
    "neural",
    "network",
    "basically",
    "help",
    "train",
    "machines",
    "applications",
    "whatever",
    "trying",
    "create",
    "deep",
    "learning",
    "really",
    "really",
    "done",
    "amazing",
    "work",
    "help",
    "deep",
    "learning",
    "able",
    "solve",
    "complex",
    "complex",
    "complex",
    "use",
    "cases",
    "probably",
    "discussing",
    "go",
    "ahead",
    "come",
    "data",
    "science",
    "see",
    "thing",
    "guys",
    "want",
    "say",
    "data",
    "scientist",
    "tomorrow",
    "given",
    "business",
    "use",
    "case",
    "situation",
    "comes",
    "probably",
    "solve",
    "use",
    "case",
    "help",
    "machine",
    "learning",
    "algorithms",
    "deep",
    "learning",
    "algorithms",
    "final",
    "goal",
    "create",
    "ai",
    "application",
    "right",
    "say",
    "data",
    "scientist",
    "work",
    "machine",
    "learning",
    "work",
    "deep",
    "learning",
    "may",
    "know",
    "analyze",
    "data",
    "working",
    "panasonic",
    "got",
    "various",
    "different",
    "kind",
    "task",
    "sometime",
    "told",
    "use",
    "w",
    "powerbi",
    "visualize",
    "analyze",
    "data",
    "sometime",
    "given",
    "machine",
    "learning",
    "project",
    "sometime",
    "given",
    "deep",
    "learning",
    "project",
    "data",
    "scientist",
    "consider",
    "data",
    "scientist",
    "fall",
    "part",
    "everything",
    "talk",
    "machine",
    "learning",
    "deep",
    "learning",
    "respect",
    "kind",
    "problem",
    "statement",
    "solve",
    "majority",
    "business",
    "use",
    "cases",
    "falling",
    "two",
    "sections",
    "one",
    "supervised",
    "machine",
    "learning",
    "one",
    "unsupervised",
    "machine",
    "learning",
    "problems",
    "basically",
    "solving",
    "respect",
    "two",
    "problem",
    "statement",
    "two",
    "different",
    "types",
    "machine",
    "learning",
    "algorithms",
    "supervised",
    "machine",
    "learning",
    "deep",
    "learning",
    "talk",
    "supervised",
    "machine",
    "learning",
    "two",
    "major",
    "problem",
    "statements",
    "basically",
    "solving",
    "also",
    "one",
    "regression",
    "problem",
    "one",
    "something",
    "called",
    "classification",
    "problem",
    "case",
    "unsupervised",
    "machine",
    "learning",
    "problem",
    "statement",
    "basically",
    "solving",
    "two",
    "different",
    "types",
    "problem",
    "one",
    "clustering",
    "one",
    "dimensionality",
    "reduction",
    "also",
    "one",
    "type",
    "called",
    "reinforcement",
    "learning",
    "reinforcement",
    "learning",
    "definitely",
    "talk",
    "right",
    "right",
    "focusing",
    "things",
    "understand",
    "happens",
    "supervised",
    "machine",
    "learning",
    "let",
    "consider",
    "consider",
    "data",
    "set",
    "data",
    "set",
    "says",
    "age",
    "weight",
    "suppose",
    "two",
    "specific",
    "features",
    "let",
    "say",
    "values",
    "like",
    "24",
    "62",
    "25",
    "63",
    "21",
    "72",
    "257",
    "uh",
    "62",
    "many",
    "data",
    "let",
    "say",
    "task",
    "basically",
    "take",
    "particular",
    "data",
    "create",
    "model",
    "wherein",
    "suppose",
    "task",
    "need",
    "create",
    "model",
    "whenever",
    "takes",
    "new",
    "age",
    "first",
    "train",
    "model",
    "data",
    "whenever",
    "take",
    "age",
    "new",
    "age",
    "able",
    "give",
    "us",
    "output",
    "weight",
    "particular",
    "model",
    "also",
    "called",
    "hypothesis",
    "okay",
    "discuss",
    "today",
    "discussing",
    "linear",
    "regression",
    "important",
    "components",
    "whenever",
    "kind",
    "problem",
    "statement",
    "first",
    "need",
    "understand",
    "two",
    "important",
    "things",
    "one",
    "independent",
    "features",
    "one",
    "something",
    "called",
    "dependent",
    "features",
    "let",
    "go",
    "ahead",
    "discuss",
    "independent",
    "feature",
    "independent",
    "feature",
    "basically",
    "means",
    "particular",
    "case",
    "since",
    "input",
    "basically",
    "training",
    "features",
    "becomes",
    "independent",
    "feature",
    "particular",
    "case",
    "age",
    "independent",
    "feature",
    "whatever",
    "actually",
    "predicting",
    "say",
    "predicting",
    "know",
    "output",
    "okay",
    "basically",
    "make",
    "model",
    "uh",
    "give",
    "output",
    "particular",
    "casee",
    "dependent",
    "feature",
    "becomes",
    "weight",
    "specifically",
    "say",
    "dependent",
    "feature",
    "completely",
    "dependent",
    "value",
    "whenever",
    "increasing",
    "decreasing",
    "value",
    "basically",
    "getting",
    "changed",
    "reason",
    "basically",
    "say",
    "independent",
    "dependent",
    "feature",
    "whenever",
    "solving",
    "problem",
    "right",
    "case",
    "supervised",
    "machine",
    "learning",
    "remember",
    "one",
    "dependent",
    "feature",
    "number",
    "independent",
    "features",
    "let",
    "go",
    "ahead",
    "let",
    "discuss",
    "regression",
    "classification",
    "difference",
    "let",
    "let",
    "go",
    "ahead",
    "let",
    "discuss",
    "two",
    "things",
    "one",
    "let",
    "say",
    "want",
    "regression",
    "problem",
    "statement",
    "suppose",
    "take",
    "example",
    "age",
    "weight",
    "values",
    "like",
    "discussed",
    "24",
    "72",
    "23",
    "71",
    "uh",
    "24",
    "25",
    "okay",
    "kind",
    "data",
    "see",
    "output",
    "variable",
    "dependent",
    "feature",
    "particular",
    "dependent",
    "feature",
    "whenever",
    "trying",
    "find",
    "output",
    "particular",
    "output",
    "continuous",
    "variable",
    "continuous",
    "variable",
    "becomes",
    "regression",
    "problem",
    "statement",
    "one",
    "example",
    "would",
    "like",
    "give",
    "suppose",
    "data",
    "set",
    "right",
    "age",
    "weight",
    "suppose",
    "populating",
    "particular",
    "data",
    "set",
    "help",
    "scatter",
    "plot",
    "order",
    "basically",
    "solve",
    "problem",
    "suppose",
    "take",
    "example",
    "linear",
    "regression",
    "try",
    "draw",
    "straight",
    "line",
    "particular",
    "line",
    "equation",
    "called",
    "yal",
    "mx",
    "c",
    "help",
    "particular",
    "equation",
    "try",
    "find",
    "predicted",
    "points",
    "predicted",
    "point",
    "predicted",
    "point",
    "new",
    "points",
    "see",
    "basically",
    "predicted",
    "point",
    "respect",
    "way",
    "basically",
    "solve",
    "regression",
    "problem",
    "statement",
    "much",
    "important",
    "understand",
    "let",
    "go",
    "always",
    "understand",
    "regression",
    "problem",
    "statement",
    "output",
    "continuous",
    "variable",
    "second",
    "one",
    "basically",
    "classification",
    "problem",
    "classification",
    "problem",
    "suppose",
    "data",
    "set",
    "let",
    "say",
    "number",
    "hours",
    "study",
    "number",
    "study",
    "hours",
    "number",
    "play",
    "hours",
    "independent",
    "feature",
    "let",
    "say",
    "number",
    "sleeping",
    "hours",
    "finally",
    "output",
    "pass",
    "fail",
    "independent",
    "features",
    "dependent",
    "feature",
    "values",
    "like",
    "either",
    "pass",
    "fail",
    "pass",
    "fail",
    "whenever",
    "output",
    "fixed",
    "number",
    "categories",
    "becomes",
    "classification",
    "problem",
    "suppose",
    "two",
    "outputs",
    "becomes",
    "binary",
    "classification",
    "two",
    "different",
    "categories",
    "time",
    "becomes",
    "multiclass",
    "classification",
    "difference",
    "regression",
    "problem",
    "statement",
    "classification",
    "problem",
    "statement",
    "let",
    "go",
    "ahead",
    "let",
    "discuss",
    "something",
    "called",
    "unsupervised",
    "machine",
    "learning",
    "unsupervised",
    "machine",
    "learning",
    "second",
    "main",
    "topic",
    "going",
    "write",
    "unsupervised",
    "machine",
    "learning",
    "exactly",
    "unsupervised",
    "machine",
    "learning",
    "whenever",
    "talk",
    "two",
    "main",
    "problem",
    "statement",
    "solve",
    "one",
    "clustering",
    "one",
    "dimensionality",
    "reduction",
    "let",
    "take",
    "one",
    "example",
    "specific",
    "data",
    "set",
    "let",
    "say",
    "data",
    "set",
    "something",
    "called",
    "salary",
    "age",
    "scenario",
    "output",
    "variable",
    "output",
    "variable",
    "dependent",
    "variable",
    "kind",
    "assumptions",
    "take",
    "particular",
    "data",
    "set",
    "suppose",
    "salary",
    "age",
    "values",
    "particular",
    "case",
    "would",
    "like",
    "something",
    "called",
    "clustering",
    "clustering",
    "used",
    "understand",
    "let",
    "say",
    "going",
    "something",
    "called",
    "customer",
    "segmentation",
    "customer",
    "segmentation",
    "clustering",
    "basically",
    "means",
    "based",
    "data",
    "try",
    "find",
    "similar",
    "groups",
    "groups",
    "people",
    "suppose",
    "one",
    "group",
    "another",
    "group",
    "third",
    "group",
    "let",
    "say",
    "able",
    "create",
    "many",
    "groups",
    "many",
    "groups",
    "clusters",
    "say",
    "cluster",
    "1",
    "2",
    "three",
    "every",
    "cluster",
    "specifying",
    "information",
    "cluster",
    "may",
    "specify",
    "person",
    "uh",
    "young",
    "able",
    "get",
    "amazing",
    "salary",
    "person",
    "may",
    "specify",
    "people",
    "basically",
    "age",
    "getting",
    "good",
    "salary",
    "people",
    "like",
    "middle",
    "class",
    "background",
    "respect",
    "age",
    "salary",
    "much",
    "increasing",
    "clustering",
    "grouping",
    "together",
    "main",
    "thing",
    "grouping",
    "word",
    "much",
    "important",
    "use",
    "suppose",
    "company",
    "launches",
    "product",
    "want",
    "target",
    "particular",
    "product",
    "rich",
    "people",
    "let",
    "say",
    "product",
    "one",
    "rich",
    "people",
    "product",
    "two",
    "middle",
    "class",
    "people",
    "make",
    "kind",
    "clusters",
    "able",
    "target",
    "ads",
    "kind",
    "people",
    "let",
    "say",
    "rich",
    "people",
    "middle",
    "class",
    "people",
    "able",
    "target",
    "particular",
    "ads",
    "particular",
    "product",
    "send",
    "particular",
    "things",
    "specific",
    "group",
    "people",
    "basically",
    "called",
    "ad",
    "marketing",
    "uses",
    "something",
    "called",
    "customer",
    "segmentation",
    "important",
    "example",
    "based",
    "customer",
    "segmentation",
    "later",
    "apply",
    "regression",
    "classification",
    "kind",
    "problem",
    "statement",
    "coming",
    "second",
    "one",
    "clustering",
    "called",
    "dimensionality",
    "reduction",
    "dimensionality",
    "reduction",
    "focusing",
    "suppose",
    "th000",
    "features",
    "reduce",
    "features",
    "lower",
    "dimensions",
    "let",
    "say",
    "want",
    "convert",
    "uh",
    "th000",
    "feature",
    "100",
    "features",
    "lower",
    "dimension",
    "yes",
    "possible",
    "help",
    "dimensionality",
    "deduction",
    "algorithm",
    "algorithms",
    "like",
    "pca",
    "also",
    "try",
    "cover",
    "go",
    "ahead",
    "understand",
    "clustering",
    "classification",
    "problem",
    "clustering",
    "grouping",
    "algorithm",
    "output",
    "feature",
    "dependent",
    "variable",
    "clustering",
    "sorry",
    "unsupervised",
    "ml",
    "yes",
    "also",
    "try",
    "cover",
    "lda",
    "cover",
    "pca",
    "go",
    "ahead",
    "respect",
    "supervised",
    "unsupervised",
    "first",
    "thing",
    "going",
    "cover",
    "something",
    "called",
    "linear",
    "regression",
    "second",
    "algorithm",
    "try",
    "cover",
    "linear",
    "regression",
    "something",
    "called",
    "ridge",
    "lasso",
    "third",
    "going",
    "cover",
    "something",
    "called",
    "logistic",
    "regression",
    "fourth",
    "basically",
    "going",
    "cover",
    "something",
    "called",
    "decision",
    "tree",
    "decision",
    "tree",
    "includes",
    "classification",
    "regression",
    "four",
    "fifth",
    "going",
    "cover",
    "something",
    "called",
    "adab",
    "boost",
    "sixth",
    "going",
    "cover",
    "something",
    "called",
    "random",
    "forest",
    "seventh",
    "going",
    "cover",
    "something",
    "called",
    "gradient",
    "boosting",
    "eighth",
    "going",
    "cover",
    "something",
    "called",
    "xg",
    "boost",
    "n9",
    "going",
    "cover",
    "something",
    "called",
    "n",
    "bias",
    "go",
    "unsupervised",
    "machine",
    "learning",
    "algorithm",
    "first",
    "algorithm",
    "going",
    "something",
    "called",
    "k",
    "means",
    "k",
    "means",
    "algorithm",
    "also",
    "dv",
    "scan",
    "also",
    "going",
    "higher",
    "c",
    "clustering",
    "also",
    "something",
    "called",
    "k",
    "nearest",
    "neighbor",
    "clustering",
    "fifth",
    "try",
    "see",
    "pca",
    "lda",
    "different",
    "different",
    "things",
    "try",
    "cover",
    "yes",
    "svm",
    "missed",
    "going",
    "include",
    "svm",
    "knn",
    "also",
    "get",
    "covered",
    "list",
    "probably",
    "may",
    "miss",
    "one",
    "two",
    "going",
    "cover",
    "everything",
    "let",
    "start",
    "first",
    "algorithm",
    "linear",
    "regression",
    "let",
    "go",
    "ahead",
    "discuss",
    "linear",
    "regression",
    "linear",
    "regression",
    "problem",
    "statement",
    "simple",
    "guys",
    "suppose",
    "let",
    "say",
    "two",
    "features",
    "one",
    "x",
    "feature",
    "one",
    "feature",
    "let",
    "say",
    "x",
    "nothing",
    "age",
    "nothing",
    "weight",
    "based",
    "two",
    "features",
    "data",
    "points",
    "present",
    "linear",
    "regression",
    "try",
    "try",
    "create",
    "model",
    "help",
    "training",
    "data",
    "set",
    "training",
    "data",
    "set",
    "actually",
    "going",
    "going",
    "basically",
    "train",
    "model",
    "model",
    "nothing",
    "kind",
    "hypothesis",
    "testing",
    "kind",
    "hypothesis",
    "takes",
    "new",
    "age",
    "gives",
    "output",
    "weights",
    "help",
    "performance",
    "metrics",
    "try",
    "verify",
    "whether",
    "model",
    "performing",
    "well",
    "short",
    "going",
    "linear",
    "regression",
    "try",
    "find",
    "best",
    "fit",
    "line",
    "actually",
    "help",
    "us",
    "prediction",
    "basically",
    "means",
    "get",
    "new",
    "age",
    "output",
    "respect",
    "okay",
    "respect",
    "output",
    "particular",
    "case",
    "whenever",
    "drawing",
    "diagram",
    "like",
    "basically",
    "say",
    "linear",
    "function",
    "x",
    "going",
    "understand",
    "going",
    "create",
    "best",
    "fit",
    "line",
    "much",
    "important",
    "whenever",
    "say",
    "linear",
    "regression",
    "basically",
    "means",
    "going",
    "create",
    "linear",
    "line",
    "may",
    "thinking",
    "sir",
    "create",
    "linear",
    "line",
    "nonlinear",
    "line",
    "discuss",
    "go",
    "ahead",
    "see",
    "algorithms",
    "begin",
    "let",
    "consider",
    "line",
    "see",
    "right",
    "line",
    "equation",
    "given",
    "multiple",
    "equations",
    "someone",
    "people",
    "people",
    "write",
    "yal",
    "mx",
    "c",
    "people",
    "write",
    "uh",
    "h",
    "people",
    "write",
    "yal",
    "beta",
    "0",
    "beta",
    "1",
    "x",
    "people",
    "write",
    "h",
    "theta",
    "xal",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "many",
    "many",
    "equations",
    "straight",
    "line",
    "straight",
    "line",
    "many",
    "many",
    "equations",
    "respect",
    "many",
    "many",
    "different",
    "kind",
    "notations",
    "first",
    "algorithm",
    "probably",
    "learned",
    "linear",
    "regression",
    "andrew",
    "ng",
    "definitely",
    "would",
    "like",
    "give",
    "entire",
    "credits",
    "based",
    "notation",
    "whatever",
    "explained",
    "try",
    "explain",
    "credits",
    "algorithm",
    "specifically",
    "goes",
    "andrew",
    "ng",
    "let",
    "consider",
    "one",
    "order",
    "create",
    "straight",
    "line",
    "basically",
    "use",
    "equation",
    "called",
    "h",
    "theta",
    "equation",
    "straight",
    "line",
    "know",
    "equation",
    "straight",
    "line",
    "whatever",
    "write",
    "write",
    "many",
    "things",
    "yal",
    "mx",
    "c",
    "yal",
    "beta",
    "0",
    "beta",
    "1",
    "x",
    "also",
    "write",
    "one",
    "h",
    "theta",
    "xal",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "also",
    "basically",
    "say",
    "x",
    "also",
    "say",
    "x",
    "let",
    "go",
    "ahead",
    "let",
    "take",
    "equation",
    "let",
    "take",
    "equation",
    "going",
    "take",
    "equation",
    "write",
    "one",
    "equation",
    "also",
    "studied",
    "definitely",
    "adding",
    "points",
    "probably",
    "andrew",
    "could",
    "mention",
    "mention",
    "video",
    "try",
    "level",
    "best",
    "obviously",
    "best",
    "even",
    "compare",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "let",
    "understand",
    "theta",
    "0",
    "theta",
    "1",
    "said",
    "let",
    "say",
    "problem",
    "statement",
    "let",
    "say",
    "x",
    "data",
    "points",
    "trying",
    "create",
    "best",
    "fit",
    "line",
    "like",
    "best",
    "fit",
    "line",
    "uh",
    "say",
    "best",
    "fit",
    "line",
    "basically",
    "given",
    "equation",
    "theta",
    "0",
    "basically",
    "indicate",
    "theta",
    "0",
    "something",
    "called",
    "intercept",
    "exactly",
    "intercept",
    "intercept",
    "basically",
    "means",
    "x",
    "zero",
    "h",
    "theta",
    "x",
    "equal",
    "theta",
    "0",
    "particular",
    "case",
    "intercept",
    "basically",
    "indicates",
    "point",
    "meeting",
    "ais",
    "particular",
    "point",
    "basically",
    "intercept",
    "x",
    "equal",
    "0",
    "point",
    "time",
    "seeing",
    "line",
    "intersecting",
    "ais",
    "whatever",
    "value",
    "intercept",
    "second",
    "thing",
    "theta",
    "1",
    "theta",
    "1",
    "nothing",
    "slope",
    "coefficient",
    "basically",
    "indicate",
    "indicates",
    "let",
    "let",
    "say",
    "unit",
    "one",
    "unit",
    "probably",
    "respect",
    "find",
    "one",
    "point",
    "one",
    "point",
    "try",
    "draw",
    "unit",
    "movement",
    "basically",
    "say",
    "slope",
    "unit",
    "movement",
    "one",
    "one",
    "unit",
    "movement",
    "towards",
    "unit",
    "movement",
    "axis",
    "basically",
    "slope",
    "coefficient",
    "theta",
    "0",
    "theta",
    "1",
    "two",
    "things",
    "x",
    "definitely",
    "data",
    "points",
    "main",
    "aim",
    "create",
    "best",
    "fit",
    "line",
    "way",
    "try",
    "show",
    "main",
    "aim",
    "let",
    "let",
    "understand",
    "aim",
    "linear",
    "regression",
    "take",
    "example",
    "linear",
    "regression",
    "need",
    "find",
    "best",
    "fit",
    "line",
    "way",
    "distance",
    "data",
    "points",
    "predicted",
    "points",
    "less",
    "suppose",
    "creating",
    "best",
    "fit",
    "line",
    "okay",
    "creating",
    "best",
    "fit",
    "line",
    "respect",
    "data",
    "points",
    "initially",
    "right",
    "predicted",
    "point",
    "point",
    "particular",
    "case",
    "predicted",
    "point",
    "point",
    "summation",
    "points",
    "distance",
    "minimal",
    "able",
    "say",
    "best",
    "fit",
    "line",
    "definitely",
    "say",
    "exactly",
    "best",
    "fit",
    "line",
    "say",
    "try",
    "calculate",
    "difference",
    "point",
    "predicted",
    "point",
    "predicted",
    "point",
    "right",
    "try",
    "calculate",
    "distance",
    "basically",
    "aim",
    "minimal",
    "summation",
    "distance",
    "minimal",
    "see",
    "may",
    "also",
    "thinking",
    "krish",
    "one",
    "thing",
    "okay",
    "suppose",
    "data",
    "points",
    "play",
    "create",
    "multiple",
    "lines",
    "try",
    "compare",
    "compare",
    "multiple",
    "create",
    "multiple",
    "lines",
    "right",
    "like",
    "whoever",
    "giving",
    "best",
    "minimal",
    "point",
    "go",
    "select",
    "many",
    "iteration",
    "come",
    "know",
    "okay",
    "line",
    "best",
    "line",
    "specific",
    "purpose",
    "start",
    "one",
    "point",
    "lead",
    "towards",
    "finding",
    "best",
    "fit",
    "line",
    "start",
    "one",
    "point",
    "go",
    "towards",
    "finding",
    "best",
    "fit",
    "line",
    "particular",
    "purpose",
    "create",
    "something",
    "called",
    "uh",
    "cost",
    "function",
    "already",
    "shown",
    "hypothesis",
    "function",
    "best",
    "fit",
    "line",
    "equation",
    "basically",
    "given",
    "h",
    "theta",
    "x",
    "equal",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "hypothesis",
    "right",
    "coming",
    "cost",
    "function",
    "super",
    "super",
    "important",
    "super",
    "important",
    "cost",
    "function",
    "basically",
    "cost",
    "function",
    "told",
    "right",
    "right",
    "distance",
    "summation",
    "distance",
    "summation",
    "minimal",
    "really",
    "want",
    "find",
    "particular",
    "distance",
    "using",
    "one",
    "equation",
    "use",
    "distance",
    "formula",
    "predicted",
    "real",
    "point",
    "say",
    "h",
    "theta",
    "x",
    "say",
    "h",
    "theta",
    "x",
    "basically",
    "mean",
    "real",
    "point",
    "predicted",
    "point",
    "predicted",
    "point",
    "basically",
    "given",
    "h",
    "theta",
    "x",
    "going",
    "going",
    "basically",
    "squaring",
    "may",
    "get",
    "negative",
    "value",
    "really",
    "want",
    "squaring",
    "part",
    "understand",
    "one",
    "thing",
    "need",
    "also",
    "summation",
    "1",
    "compl",
    "complete",
    "let",
    "say",
    "taking",
    "number",
    "data",
    "points",
    "need",
    "calculate",
    "distance",
    "points",
    "right",
    "respect",
    "predicted",
    "predict",
    "respect",
    "real",
    "points",
    "also",
    "need",
    "divide",
    "1x",
    "2m",
    "reason",
    "dividing",
    "first",
    "let",
    "show",
    "dividing",
    "1",
    "1",
    "give",
    "us",
    "average",
    "values",
    "specific",
    "reason",
    "dividing",
    "1",
    "2",
    "derivation",
    "purpose",
    "helps",
    "us",
    "make",
    "equation",
    "much",
    "simpler",
    "later",
    "updating",
    "weights",
    "say",
    "weights",
    "basically",
    "updating",
    "theta",
    "0",
    "theta",
    "1",
    "theta",
    "0",
    "theta",
    "1",
    "point",
    "time",
    "able",
    "see",
    "particular",
    "value",
    "probably",
    "derivative",
    "help",
    "us",
    "going",
    "repeat",
    "going",
    "write",
    "first",
    "order",
    "find",
    "find",
    "best",
    "fit",
    "line",
    "need",
    "keep",
    "changing",
    "theta",
    "0",
    "theta",
    "1",
    "unless",
    "get",
    "best",
    "fit",
    "line",
    "unless",
    "get",
    "best",
    "fit",
    "line",
    "need",
    "keep",
    "updating",
    "theta",
    "0",
    "theta",
    "1",
    "need",
    "keep",
    "updating",
    "theta",
    "0",
    "theta",
    "1",
    "probably",
    "require",
    "cost",
    "function",
    "okay",
    "cost",
    "function",
    "tell",
    "cost",
    "function",
    "specify",
    "j",
    "theta",
    "0",
    "comma",
    "theta",
    "1",
    "equal",
    "cost",
    "fun",
    "function",
    "distance",
    "told",
    "right",
    "distance",
    "h",
    "theta",
    "x",
    "summation",
    "things",
    "needs",
    "minimal",
    "needs",
    "less",
    "respect",
    "x",
    "point",
    "point",
    "right",
    "similarly",
    "respect",
    "x",
    "point",
    "point",
    "actually",
    "going",
    "going",
    "use",
    "cost",
    "function",
    "cost",
    "function",
    "main",
    "aim",
    "basically",
    "write",
    "h",
    "theta",
    "x",
    "respect",
    "saying",
    "moving",
    "equal",
    "1",
    "points",
    "basically",
    "points",
    "apart",
    "actually",
    "going",
    "going",
    "divide",
    "1x",
    "2",
    "tell",
    "specifically",
    "dividing",
    "1x",
    "2",
    "first",
    "dividing",
    "getting",
    "average",
    "output",
    "average",
    "cost",
    "function",
    "iterating",
    "reason",
    "dividing",
    "two",
    "help",
    "us",
    "derivation",
    "let",
    "say",
    "x²",
    "try",
    "find",
    "derivative",
    "x²",
    "respect",
    "x",
    "get",
    "basically",
    "get",
    "2x",
    "right",
    "formula",
    "derivation",
    "x",
    "n",
    "nothing",
    "n",
    "x",
    "n",
    "minus1",
    "reason",
    "actually",
    "making",
    "1",
    "two",
    "two",
    "comes",
    "two",
    "two",
    "get",
    "cancelled",
    "hope",
    "everybody",
    "able",
    "understand",
    "cost",
    "function",
    "understand",
    "called",
    "entire",
    "equation",
    "basically",
    "called",
    "squared",
    "error",
    "function",
    "yes",
    "mathematical",
    "simplicity",
    "basically",
    "means",
    "updating",
    "theta",
    "0",
    "theta",
    "1",
    "basically",
    "find",
    "derivation",
    "cost",
    "function",
    "reason",
    "specifically",
    "squaring",
    "basically",
    "done",
    "get",
    "negative",
    "values",
    "squared",
    "error",
    "function",
    "let",
    "go",
    "towards",
    "need",
    "solve",
    "cost",
    "function",
    "okay",
    "need",
    "minimize",
    "minimize",
    "particular",
    "value",
    "1x",
    "2",
    "summation",
    "1",
    "2",
    "basically",
    "h",
    "theta",
    "x",
    "minus",
    "whole",
    "square",
    "need",
    "minimize",
    "adjusting",
    "parameter",
    "theta",
    "0",
    "theta",
    "1",
    "entirely",
    "nothing",
    "j",
    "theta",
    "0",
    "comma",
    "theta",
    "1",
    "really",
    "need",
    "minimize",
    "task",
    "okay",
    "task",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "compare",
    "two",
    "different",
    "thing",
    "one",
    "hypothesis",
    "testing",
    "one",
    "respect",
    "cost",
    "function",
    "okay",
    "let",
    "take",
    "example",
    "right",
    "equation",
    "hypothesis",
    "nothing",
    "h",
    "theta",
    "x",
    "equal",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "theta",
    "0",
    "0",
    "basically",
    "indicate",
    "say",
    "basically",
    "line",
    "line",
    "best",
    "fit",
    "line",
    "passes",
    "origin",
    "nothing",
    "theta",
    "xal",
    "theta",
    "1",
    "multiplied",
    "x",
    "say",
    "like",
    "obviously",
    "definitely",
    "say",
    "like",
    "right",
    "equation",
    "like",
    "right",
    "let",
    "consider",
    "theta",
    "0",
    "equal",
    "0",
    "done",
    "till",
    "minimized",
    "written",
    "equation",
    "everything",
    "yes",
    "passing",
    "origin",
    "equation",
    "actually",
    "getting",
    "let",
    "take",
    "one",
    "example",
    "let",
    "try",
    "solve",
    "h",
    "theta",
    "x",
    "new",
    "hypothesis",
    "considering",
    "intercept",
    "passing",
    "region",
    "respect",
    "let",
    "say",
    "create",
    "one",
    "line",
    "let",
    "say",
    "data",
    "points",
    "like",
    "x1",
    "y1",
    "1",
    "2",
    "3",
    "1",
    "2",
    "3",
    "let",
    "consider",
    "data",
    "points",
    "like",
    "data",
    "points",
    "like",
    "let",
    "say",
    "three",
    "data",
    "points",
    "1",
    "comma",
    "1",
    "2a",
    "2",
    "3",
    "comma",
    "3",
    "1a",
    "1",
    "nothing",
    "data",
    "point",
    "2a",
    "2",
    "nothing",
    "data",
    "point",
    "3",
    "comma",
    "3",
    "data",
    "point",
    "data",
    "points",
    "data",
    "set",
    "2",
    "comma",
    "2",
    "point",
    "3",
    "comma",
    "3",
    "basically",
    "point",
    "let",
    "consider",
    "points",
    "data",
    "points",
    "consider",
    "theta",
    "1",
    "1",
    "think",
    "straight",
    "line",
    "pass",
    "think",
    "straight",
    "line",
    "pass",
    "straight",
    "line",
    "definitely",
    "pass",
    "like",
    "right",
    "straight",
    "line",
    "definitely",
    "pass",
    "points",
    "point",
    "becomes",
    "prediction",
    "point",
    "also",
    "right",
    "point",
    "let",
    "consider",
    "also",
    "getting",
    "pass",
    "passes",
    "points",
    "theta",
    "1",
    "equal",
    "1",
    "theta",
    "1",
    "nothing",
    "slope",
    "slope",
    "equal",
    "1",
    "scenario",
    "passes",
    "points",
    "go",
    "ahead",
    "calculate",
    "j",
    "theta",
    "form",
    "j",
    "theta",
    "1",
    "become",
    "theta",
    "0",
    "0",
    "okay",
    "basically",
    "write",
    "1",
    "2",
    "summation",
    "1",
    "2",
    "three",
    "many",
    "points",
    "three",
    "right",
    "j",
    "h",
    "theta",
    "x1",
    "sorry",
    "x",
    "theta",
    "x",
    "right",
    "let",
    "go",
    "ahead",
    "compute",
    "particular",
    "scenario",
    "happen",
    "1x",
    "2",
    "point",
    "minus",
    "see",
    "h",
    "x",
    "also",
    "1",
    "also",
    "one",
    "point",
    "1",
    "become",
    "1",
    "1",
    "whole",
    "plus",
    "summation",
    "next",
    "point",
    "also",
    "falling",
    "2a",
    "2",
    "become",
    "2",
    "2",
    "3",
    "3",
    "total",
    "become",
    "zero",
    "j",
    "theta",
    "theta",
    "1",
    "1",
    "theta",
    "1",
    "1",
    "j",
    "theta",
    "1",
    "much",
    "z",
    "right",
    "j",
    "theta",
    "1",
    "cost",
    "function",
    "let",
    "draw",
    "cost",
    "function",
    "graph",
    "let",
    "say",
    "theta",
    "1",
    "theta",
    "two",
    "okay",
    "similarly",
    "five",
    "1",
    "2",
    "j",
    "theta",
    "1",
    "right",
    "theta",
    "1",
    "theta",
    "1",
    "1",
    "particular",
    "point",
    "get",
    "j",
    "theta",
    "1",
    "nothing",
    "zero",
    "first",
    "point",
    "first",
    "point",
    "guys",
    "discussed",
    "value",
    "1x",
    "2m",
    "basically",
    "make",
    "calculation",
    "simpler",
    "dividing",
    "1x",
    "2",
    "basically",
    "used",
    "average",
    "aage",
    "sumission",
    "actually",
    "let",
    "go",
    "ahead",
    "let",
    "take",
    "second",
    "scenario",
    "second",
    "scenario",
    "let",
    "consider",
    "theta",
    "1",
    "let",
    "say",
    "theta",
    "1",
    "theta",
    "1",
    "tell",
    "points",
    "get",
    "x",
    "equal",
    "1",
    "come",
    "right",
    "similarly",
    "x",
    "equal",
    "2",
    "nothing",
    "1",
    "similarly",
    "uh",
    "x",
    "equal",
    "35",
    "multiplied",
    "3",
    "see",
    "multiplying",
    "right5",
    "multi",
    "3",
    "next",
    "point",
    "come",
    "create",
    "best",
    "fit",
    "line",
    "happen",
    "next",
    "best",
    "fit",
    "line",
    "probably",
    "create",
    "green",
    "color",
    "okay",
    "second",
    "one",
    "green",
    "color",
    "definitely",
    "slope",
    "decreasing",
    "go",
    "ahead",
    "calculate",
    "j",
    "theta",
    "let",
    "see",
    "get",
    "j",
    "theta",
    "1",
    "nothing",
    "1x",
    "2",
    "equation",
    "summation",
    "1",
    "2",
    "3",
    "h",
    "theta",
    "x",
    "i²",
    "nothing",
    "1x",
    "2",
    "let",
    "summation",
    "point",
    "point",
    "nothing",
    "predicted",
    "point",
    "point",
    "real",
    "point",
    "right",
    "particular",
    "scenario",
    "first",
    "point",
    "get",
    "nothing",
    "5",
    "1",
    "whole",
    "getting",
    "5",
    "1",
    "whole",
    "square",
    "1",
    "real",
    "point",
    "1",
    "predicted",
    "point",
    "getting",
    "5",
    "1",
    "whole",
    "square",
    "second",
    "point",
    "1",
    "2",
    "whole",
    "right",
    "2",
    "1",
    "2",
    "whole",
    "finally",
    "get",
    "3",
    "whole",
    "finally",
    "calculation",
    "much",
    "actually",
    "getting",
    "1x",
    "2",
    "3",
    "6",
    "getting",
    "5",
    "square",
    "getting",
    "1",
    "getting",
    "whole",
    "square",
    "final",
    "output",
    "already",
    "calculated",
    "nothing",
    "point",
    "approximately",
    "equal",
    "58",
    "58",
    "theta",
    "nothing",
    "theta",
    "theta",
    "1",
    "right",
    "theta",
    "1",
    "able",
    "get",
    "58",
    "theta",
    "1",
    "58",
    "coming",
    "somewhere",
    "right",
    "next",
    "point",
    "green",
    "color",
    "let",
    "go",
    "ahead",
    "calculate",
    "third",
    "condition",
    "third",
    "condition",
    "actually",
    "going",
    "write",
    "going",
    "basically",
    "say",
    "theta",
    "1",
    "0",
    "point",
    "time",
    "go",
    "assume",
    "0",
    "multiplied",
    "x",
    "obviously",
    "zero",
    "getting",
    "three",
    "points",
    "next",
    "line",
    "line",
    "basically",
    "points",
    "go",
    "ahead",
    "calculate",
    "j",
    "theta",
    "1",
    "j",
    "theta",
    "1",
    "particular",
    "case",
    "theta",
    "1",
    "equal",
    "0",
    "1x",
    "2",
    "part",
    "able",
    "see",
    "0",
    "1",
    "0",
    "2",
    "0",
    "3",
    "okay",
    "become",
    "0",
    "1",
    "0",
    "2",
    "0",
    "3",
    "okay",
    "become",
    "1x",
    "6",
    "1",
    "4",
    "9",
    "nothing",
    "approximately",
    "equal",
    "happen",
    "respect",
    "theta",
    "1",
    "0",
    "getting",
    "draw",
    "nothing",
    "respect",
    "zero",
    "getting",
    "2",
    "2",
    "point",
    "similarly",
    "start",
    "constructing",
    "theta",
    "1",
    "equal",
    "2",
    "may",
    "get",
    "point",
    "join",
    "points",
    "together",
    "seeing",
    "getting",
    "kind",
    "curve",
    "okay",
    "curve",
    "something",
    "called",
    "gradient",
    "descent",
    "gradient",
    "descent",
    "play",
    "important",
    "role",
    "making",
    "sure",
    "making",
    "sure",
    "get",
    "right",
    "theta",
    "1",
    "value",
    "light",
    "slope",
    "value",
    "suitable",
    "point",
    "suitable",
    "point",
    "come",
    "point",
    "basically",
    "called",
    "global",
    "minima",
    "see",
    "three",
    "lines",
    "best",
    "fit",
    "line",
    "best",
    "fit",
    "line",
    "right",
    "best",
    "fit",
    "line",
    "best",
    "fit",
    "line",
    "point",
    "came",
    "point",
    "came",
    "right",
    "want",
    "basically",
    "come",
    "region",
    "global",
    "minima",
    "basically",
    "distance",
    "predicted",
    "real",
    "point",
    "less",
    "right",
    "specific",
    "point",
    "basically",
    "called",
    "global",
    "minimum",
    "still",
    "discuss",
    "krish",
    "assumed",
    "theta",
    "1",
    "1",
    "theta",
    "1",
    "theta",
    "1",
    "0",
    "also",
    "assuming",
    "many",
    "things",
    "right",
    "probably",
    "calculating",
    "creating",
    "gradient",
    "descent",
    "thing",
    "probably",
    "come",
    "one",
    "point",
    "reach",
    "towards",
    "specific",
    "reason",
    "first",
    "come",
    "point",
    "move",
    "towards",
    "global",
    "minima",
    "specific",
    "case",
    "using",
    "one",
    "convergence",
    "algorithm",
    "come",
    "one",
    "specific",
    "point",
    "need",
    "keep",
    "updating",
    "theta",
    "1",
    "instead",
    "using",
    "different",
    "different",
    "theta",
    "1",
    "value",
    "use",
    "something",
    "called",
    "convergence",
    "algorithm",
    "convergence",
    "algorithm",
    "basically",
    "says",
    "repeat",
    "convergence",
    "basically",
    "means",
    "loop",
    "let",
    "say",
    "basically",
    "going",
    "update",
    "theta",
    "value",
    "given",
    "notation",
    "continuous",
    "updation",
    "say",
    "theta",
    "j",
    "minus",
    "talk",
    "alpha",
    "worry",
    "derivative",
    "theta",
    "j",
    "respect",
    "j",
    "theta",
    "0",
    "theta",
    "1",
    "happen",
    "basically",
    "means",
    "reach",
    "specific",
    "point",
    "theta",
    "performing",
    "particular",
    "operation",
    "able",
    "come",
    "global",
    "minima",
    "specific",
    "thing",
    "able",
    "see",
    "called",
    "derivative",
    "called",
    "derivative",
    "derivative",
    "basically",
    "means",
    "trying",
    "find",
    "slope",
    "derivative",
    "also",
    "say",
    "slope",
    "equation",
    "definitely",
    "work",
    "guys",
    "trust",
    "definitely",
    "work",
    "work",
    "draw",
    "show",
    "let",
    "say",
    "cost",
    "function",
    "let",
    "say",
    "got",
    "gradient",
    "descent",
    "let",
    "say",
    "first",
    "point",
    "somewhere",
    "reach",
    "somewhere",
    "right",
    "reach",
    "theta",
    "1",
    "j",
    "theta",
    "1",
    "suppose",
    "reach",
    "specific",
    "point",
    "also",
    "another",
    "gradient",
    "descent",
    "looks",
    "like",
    "let",
    "say",
    "initial",
    "time",
    "reach",
    "point",
    "coming",
    "minimal",
    "global",
    "minima",
    "using",
    "equation",
    "talk",
    "alpha",
    "also",
    "worry",
    "also",
    "theta",
    "1",
    "also",
    "j",
    "theta",
    "1",
    "let",
    "say",
    "suppose",
    "came",
    "particular",
    "point",
    "right",
    "coming",
    "particular",
    "point",
    "basically",
    "apply",
    "derivative",
    "j",
    "theta",
    "1",
    "okay",
    "find",
    "derivative",
    "basically",
    "means",
    "trying",
    "find",
    "slope",
    "order",
    "find",
    "slope",
    "create",
    "straight",
    "line",
    "like",
    "look",
    "like",
    "try",
    "create",
    "try",
    "create",
    "slope",
    "like",
    "slope",
    "try",
    "find",
    "respect",
    "positive",
    "slope",
    "indicate",
    "understand",
    "right",
    "hand",
    "side",
    "line",
    "pointing",
    "top",
    "wordss",
    "direction",
    "best",
    "easy",
    "way",
    "find",
    "whether",
    "positive",
    "slope",
    "negative",
    "slope",
    "particular",
    "case",
    "positive",
    "slope",
    "get",
    "positive",
    "slope",
    "basically",
    "means",
    "update",
    "weights",
    "theta",
    "1",
    "theta",
    "1",
    "let",
    "say",
    "writing",
    "apply",
    "convergence",
    "algorithm",
    "see",
    "theta",
    "1",
    "colon",
    "theta",
    "1",
    "minus",
    "learning",
    "rate",
    "called",
    "alpha",
    "learning",
    "rate",
    "talk",
    "learning",
    "rate",
    "worry",
    "derivative",
    "value",
    "particular",
    "case",
    "since",
    "positive",
    "slope",
    "getting",
    "positive",
    "value",
    "let",
    "say",
    "theta",
    "value",
    "got",
    "slope",
    "initially",
    "need",
    "come",
    "location",
    "reduce",
    "theta",
    "1",
    "come",
    "main",
    "point",
    "see",
    "subtracting",
    "theta",
    "1",
    "something",
    "positive",
    "number",
    "right",
    "positive",
    "number",
    "definitely",
    "know",
    "n",
    "number",
    "iteration",
    "able",
    "come",
    "global",
    "minima",
    "similarly",
    "take",
    "right",
    "hand",
    "side",
    "try",
    "draw",
    "slope",
    "particular",
    "case",
    "slope",
    "negative",
    "similarly",
    "write",
    "equation",
    "theta",
    "1",
    "theta",
    "1",
    "minus",
    "learning",
    "rate",
    "multiplied",
    "negative",
    "number",
    "minus",
    "minus",
    "positive",
    "right",
    "suppose",
    "initially",
    "1",
    "theta",
    "1",
    "keep",
    "updating",
    "weight",
    "come",
    "global",
    "minima",
    "minus",
    "minus",
    "positive",
    "basically",
    "get",
    "theta",
    "1",
    "alpha",
    "positive",
    "number",
    "minus",
    "minus",
    "plus",
    "definitely",
    "work",
    "able",
    "come",
    "global",
    "minima",
    "whether",
    "positive",
    "slope",
    "negative",
    "slope",
    "learning",
    "learning",
    "rate",
    "learning",
    "rate",
    "based",
    "learning",
    "rate",
    "suppose",
    "want",
    "come",
    "point",
    "global",
    "minima",
    "speed",
    "coming",
    "speed",
    "learning",
    "rate",
    "value",
    "bigger",
    "speed",
    "may",
    "coming",
    "suppose",
    "say",
    "usually",
    "select",
    "learning",
    "rate",
    "01",
    "select",
    "small",
    "number",
    "start",
    "taking",
    "small",
    "small",
    "steps",
    "move",
    "towards",
    "optimal",
    "minima",
    "take",
    "alpha",
    "value",
    "huge",
    "value",
    "huge",
    "huge",
    "value",
    "happen",
    "uh",
    "updation",
    "theta",
    "1",
    "keep",
    "jumping",
    "situation",
    "never",
    "meet",
    "never",
    "reach",
    "global",
    "minima",
    "good",
    "decision",
    "take",
    "alpha",
    "small",
    "value",
    "also",
    "small",
    "value",
    "becomes",
    "small",
    "value",
    "happen",
    "tiny",
    "steps",
    "take",
    "forever",
    "reach",
    "global",
    "minima",
    "basically",
    "means",
    "model",
    "keep",
    "training",
    "definitely",
    "al",
    "going",
    "work",
    "let",
    "talk",
    "one",
    "scenario",
    "one",
    "scenario",
    "cost",
    "function",
    "local",
    "minima",
    "local",
    "minima",
    "come",
    "come",
    "local",
    "minima",
    "suppose",
    "one",
    "points",
    "come",
    "finally",
    "reaching",
    "happen",
    "particular",
    "case",
    "case",
    "seeing",
    "equation",
    "equation",
    "simply",
    "theta",
    "1",
    "theta",
    "1",
    "minus",
    "alpha",
    "point",
    "local",
    "minima",
    "slope",
    "zero",
    "particular",
    "case",
    "theta",
    "1",
    "equal",
    "theta",
    "1",
    "may",
    "thinking",
    "scenario",
    "stuck",
    "local",
    "minima",
    "called",
    "local",
    "minima",
    "usually",
    "respect",
    "gradient",
    "descent",
    "equation",
    "using",
    "get",
    "stuck",
    "local",
    "minima",
    "gradient",
    "descent",
    "particular",
    "scenar",
    "iio",
    "always",
    "look",
    "like",
    "yes",
    "deep",
    "learning",
    "learning",
    "grade",
    "descent",
    "ann",
    "point",
    "time",
    "lot",
    "local",
    "minima",
    "different",
    "different",
    "g",
    "decent",
    "algorithm",
    "like",
    "rms",
    "prop",
    "adam",
    "optimizers",
    "solve",
    "specific",
    "problem",
    "one",
    "point",
    "also",
    "wanted",
    "mention",
    "tomorrow",
    "someone",
    "asks",
    "interview",
    "question",
    "uh",
    "see",
    "local",
    "minima",
    "linear",
    "regression",
    "cost",
    "function",
    "use",
    "definitely",
    "give",
    "us",
    "local",
    "minima",
    "deep",
    "learning",
    "techniques",
    "trying",
    "use",
    "like",
    "ann",
    "different",
    "different",
    "kind",
    "optimizers",
    "solve",
    "particular",
    "problem",
    "answer",
    "basically",
    "give",
    "let",
    "go",
    "ahead",
    "write",
    "respect",
    "gradient",
    "descent",
    "algorithm",
    "going",
    "write",
    "gradient",
    "descent",
    "algorithm",
    "gradient",
    "descent",
    "algorithm",
    "remember",
    "guys",
    "gradient",
    "descent",
    "amazing",
    "algorithm",
    "definitely",
    "using",
    "please",
    "make",
    "sure",
    "know",
    "perfectly",
    "questions",
    "convergence",
    "stop",
    "convergence",
    "stop",
    "come",
    "near",
    "area",
    "uh",
    "j",
    "theta",
    "less",
    "gradient",
    "descent",
    "algorithm",
    "repeat",
    "say",
    "said",
    "repeat",
    "convergence",
    "told",
    "right",
    "written",
    "algorithm",
    "let",
    "take",
    "theta",
    "0",
    "theta",
    "1",
    "write",
    "theta",
    "0",
    "j",
    "equal",
    "theta",
    "j",
    "minus",
    "learning",
    "rate",
    "derivative",
    "theta",
    "j",
    "j",
    "theta",
    "0",
    "theta",
    "1",
    "repeat",
    "convergence",
    "really",
    "need",
    "find",
    "try",
    "equate",
    "try",
    "first",
    "find",
    "really",
    "want",
    "find",
    "derivative",
    "derivative",
    "derivative",
    "theta",
    "j",
    "respect",
    "j",
    "theta",
    "0",
    "theta",
    "1",
    "write",
    "definitely",
    "write",
    "easy",
    "way",
    "okay",
    "derivative",
    "theta",
    "j",
    "remember",
    "j",
    "0",
    "1",
    "right",
    "need",
    "find",
    "0",
    "theta",
    "0",
    "theta",
    "1",
    "1",
    "2",
    "j",
    "theta",
    "0a",
    "theta",
    "1",
    "obviously",
    "cost",
    "function",
    "write",
    "summation",
    "ial",
    "1",
    "basically",
    "write",
    "j",
    "theta",
    "x",
    "minus",
    "whole",
    "squar",
    "j",
    "equal",
    "z",
    "happen",
    "specifically",
    "say",
    "derivative",
    "derivative",
    "theta",
    "0",
    "j",
    "theta",
    "0a",
    "1",
    "simple",
    "simply",
    "applying",
    "derivative",
    "function",
    "see",
    "guys",
    "derivative",
    "let",
    "consider",
    "something",
    "like",
    "1x",
    "2",
    "x²",
    "try",
    "find",
    "derivative",
    "2x",
    "2",
    "mx",
    "2",
    "2",
    "get",
    "cancel",
    "similarly",
    "1",
    "specifically",
    "writing",
    "summation",
    "1",
    "2",
    "h",
    "theta",
    "x",
    "x",
    "x",
    "i²",
    "derivative",
    "respect",
    "theta",
    "0",
    "got",
    "second",
    "thing",
    "j",
    "equal",
    "1",
    "derivative",
    "derivative",
    "theta",
    "1",
    "j",
    "theta",
    "0",
    "comma",
    "theta",
    "1",
    "particular",
    "case",
    "1",
    "summation",
    "1",
    "see",
    "particular",
    "case",
    "theta",
    "1",
    "right",
    "theta",
    "1",
    "basically",
    "means",
    "try",
    "replace",
    "let",
    "say",
    "trying",
    "replace",
    "h",
    "theta",
    "x",
    "something",
    "else",
    "theta",
    "x",
    "know",
    "right",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "trying",
    "find",
    "derivative",
    "respect",
    "theta",
    "0",
    "obviously",
    "become",
    "able",
    "get",
    "much",
    "right",
    "respect",
    "second",
    "derivative",
    "writing",
    "writing",
    "h",
    "thet",
    "x",
    "multiplied",
    "x",
    "square",
    "also",
    "went",
    "understand",
    "h",
    "theta",
    "x",
    "see",
    "h",
    "theta",
    "x",
    "nothing",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "trying",
    "find",
    "derivative",
    "respect",
    "theta",
    "0",
    "nothing",
    "going",
    "come",
    "okay",
    "theta",
    "1",
    "x",
    "become",
    "constant",
    "particular",
    "case",
    "case",
    "theta",
    "1",
    "x",
    "try",
    "find",
    "derivative",
    "theta",
    "1",
    "x",
    "getting",
    "x",
    "square",
    "easy",
    "right",
    "x",
    "squ",
    "means",
    "2x",
    "derivative",
    "x",
    "square",
    "right",
    "square",
    "went",
    "1x",
    "2",
    "1",
    "2",
    "two",
    "got",
    "cancelled",
    "convergence",
    "algorithm",
    "discussed",
    "linear",
    "regression",
    "oh",
    "sorry",
    "remove",
    "square",
    "also",
    "let",
    "write",
    "okay",
    "repeat",
    "conver",
    "con",
    "let",
    "write",
    "repeat",
    "convergence",
    "finally",
    "two",
    "updates",
    "happening",
    "one",
    "theta",
    "0",
    "theta",
    "0",
    "minus",
    "alpha",
    "learning",
    "rate",
    "1",
    "summation",
    "ial",
    "1",
    "basically",
    "h",
    "theta",
    "x",
    "minus",
    "similarly",
    "want",
    "update",
    "theta",
    "1",
    "alpha",
    "1",
    "summation",
    "1",
    "h",
    "theta",
    "x",
    "oh",
    "god",
    "uh",
    "multiplied",
    "x",
    "alpha",
    "learning",
    "rate",
    "guys",
    "alpha",
    "nothing",
    "learning",
    "rate",
    "initialize",
    "value",
    "like",
    "see",
    "theta",
    "x",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "right",
    "derivative",
    "theta",
    "1",
    "x",
    "derivative",
    "theta",
    "1",
    "theta",
    "1",
    "x",
    "nothing",
    "x",
    "x",
    "come",
    "let",
    "discuss",
    "two",
    "important",
    "thing",
    "one",
    "r",
    "square",
    "adjusted",
    "r",
    "square",
    "similarly",
    "happen",
    "lot",
    "convex",
    "functions",
    "see",
    "talk",
    "uh",
    "like",
    "multiple",
    "features",
    "like",
    "x1",
    "x2",
    "x3",
    "x4",
    "point",
    "time",
    "3d",
    "curve",
    "curve",
    "looks",
    "like",
    "gradient",
    "decent",
    "something",
    "like",
    "gradient",
    "like",
    "coming",
    "mountain",
    "let",
    "discuss",
    "two",
    "performance",
    "metrics",
    "important",
    "particular",
    "case",
    "one",
    "r",
    "square",
    "adjusted",
    "r",
    "square",
    "usually",
    "use",
    "performance",
    "metrix",
    "verify",
    "model",
    "good",
    "model",
    "respect",
    "linear",
    "regression",
    "r",
    "square",
    "basically",
    "given",
    "r",
    "square",
    "performance",
    "matrix",
    "check",
    "good",
    "specific",
    "model",
    "basically",
    "formula",
    "like",
    "1",
    "minus",
    "sum",
    "residual",
    "divided",
    "sum",
    "total",
    "formula",
    "r",
    "squ",
    "sum",
    "residual",
    "basically",
    "write",
    "like",
    "summation",
    "min",
    "hat",
    "whole",
    "square",
    "yi",
    "hat",
    "nothing",
    "h",
    "theta",
    "x",
    "consider",
    "way",
    "divided",
    "summation",
    "mean",
    "mean",
    "formula",
    "formula",
    "try",
    "explain",
    "formula",
    "definitely",
    "says",
    "okay",
    "first",
    "thing",
    "first",
    "let",
    "consider",
    "problem",
    "statement",
    "trying",
    "solve",
    "suppose",
    "data",
    "points",
    "try",
    "create",
    "best",
    "fit",
    "line",
    "yi",
    "hat",
    "yi",
    "hat",
    "basically",
    "means",
    "specific",
    "point",
    "trying",
    "find",
    "difference",
    "things",
    "difference",
    "things",
    "let",
    "say",
    "points",
    "trying",
    "find",
    "difference",
    "predicted",
    "predicted",
    "point",
    "green",
    "color",
    "predicted",
    "points",
    "denoted",
    "hat",
    "always",
    "understand",
    "su",
    "sum",
    "residual",
    "sum",
    "residual",
    "nothing",
    "difference",
    "point",
    "point",
    "point",
    "point",
    "point",
    "point",
    "point",
    "point",
    "summation",
    "next",
    "point",
    "much",
    "important",
    "x",
    "ius",
    "bar",
    "bar",
    "nothing",
    "mean",
    "mean",
    "calculate",
    "mean",
    "probably",
    "get",
    "line",
    "looks",
    "like",
    "get",
    "line",
    "something",
    "like",
    "probably",
    "try",
    "calculate",
    "distance",
    "every",
    "point",
    "specific",
    "point",
    "respect",
    "distance",
    "point",
    "point",
    "denominator",
    "definitely",
    "high",
    "right",
    "value",
    "obviously",
    "value",
    "higher",
    "value",
    "right",
    "reason",
    "higher",
    "mean",
    "particular",
    "value",
    "distance",
    "obviously",
    "higher",
    "1",
    "minus",
    "high",
    "low",
    "value",
    "high",
    "value",
    "try",
    "divide",
    "low",
    "high",
    "low",
    "high",
    "obviously",
    "entire",
    "number",
    "become",
    "small",
    "number",
    "small",
    "number",
    "1",
    "minus",
    "small",
    "number",
    "big",
    "number",
    "basically",
    "shows",
    "r",
    "square",
    "fitted",
    "properly",
    "right",
    "basically",
    "got",
    "good",
    "r",
    "square",
    "tell",
    "get",
    "entire",
    "r",
    "square",
    "negative",
    "number",
    "let",
    "say",
    "particular",
    "case",
    "got",
    "90",
    "get",
    "r",
    "square",
    "negative",
    "number",
    "situation",
    "guys",
    "create",
    "best",
    "fit",
    "line",
    "looks",
    "like",
    "create",
    "best",
    "fit",
    "line",
    "looks",
    "like",
    "value",
    "quite",
    "high",
    "possible",
    "value",
    "higher",
    "higher",
    "value",
    "okay",
    "usual",
    "scenario",
    "happen",
    "obviously",
    "try",
    "fit",
    "line",
    "least",
    "good",
    "like",
    "pulling",
    "one",
    "line",
    "somewhere",
    "want",
    "create",
    "best",
    "fit",
    "line",
    "worse",
    "right",
    "worse",
    "particular",
    "scenario",
    "saying",
    "r",
    "square",
    "able",
    "see",
    "one",
    "one",
    "amazing",
    "feature",
    "r",
    "square",
    "let",
    "say",
    "let",
    "say",
    "one",
    "scenario",
    "suppose",
    "features",
    "like",
    "let",
    "say",
    "feature",
    "something",
    "like",
    "uh",
    "let",
    "say",
    "price",
    "house",
    "okay",
    "suppose",
    "bedrooms",
    "many",
    "bedrooms",
    "basically",
    "price",
    "house",
    "probably",
    "solve",
    "pro",
    "problem",
    "definitely",
    "get",
    "r",
    "square",
    "value",
    "let",
    "say",
    "r",
    "square",
    "value",
    "85",
    "let",
    "say",
    "r",
    "square",
    "85",
    "add",
    "one",
    "feature",
    "one",
    "feature",
    "basically",
    "says",
    "okay",
    "add",
    "location",
    "location",
    "house",
    "definitely",
    "correlated",
    "price",
    "definite",
    "chance",
    "r",
    "square",
    "value",
    "increase",
    "let",
    "say",
    "r",
    "square",
    "become",
    "90",
    "probably",
    "two",
    "specific",
    "feature",
    "obviously",
    "basically",
    "increasing",
    "r",
    "square",
    "also",
    "correlated",
    "price",
    "let",
    "change",
    "example",
    "see",
    "first",
    "case",
    "got",
    "r",
    "square",
    "85",
    "let",
    "say",
    "soon",
    "added",
    "location",
    "got",
    "90",
    "let",
    "say",
    "added",
    "one",
    "feature",
    "gender",
    "going",
    "stay",
    "gender",
    "like",
    "male",
    "female",
    "going",
    "stay",
    "know",
    "gender",
    "way",
    "correlated",
    "price",
    "even",
    "though",
    "add",
    "one",
    "feature",
    "scenario",
    "r",
    "square",
    "still",
    "increase",
    "may",
    "become",
    "91",
    "even",
    "though",
    "feature",
    "important",
    "even",
    "gender",
    "important",
    "r",
    "square",
    "formula",
    "works",
    "way",
    "keep",
    "adding",
    "features",
    "nowhere",
    "correlated",
    "obviously",
    "nowhere",
    "correlated",
    "correlated",
    "price",
    "also",
    "basically",
    "increasing",
    "r²",
    "specific",
    "thing",
    "happen",
    "whether",
    "male",
    "stay",
    "female",
    "stay",
    "matter",
    "still",
    "calculation",
    "r",
    "square",
    "still",
    "increase",
    "order",
    "impact",
    "model",
    "see",
    "right",
    "particular",
    "model",
    "got",
    "90",
    "soon",
    "see",
    "r",
    "square",
    "91",
    "considering",
    "particular",
    "gender",
    "model",
    "picked",
    "right",
    "performing",
    "well",
    "giving",
    "better",
    "r",
    "square",
    "value",
    "happen",
    "corelated",
    "model",
    "picked",
    "order",
    "prevent",
    "situation",
    "basically",
    "ally",
    "use",
    "something",
    "called",
    "adjusted",
    "r",
    "square",
    "adjusted",
    "r",
    "square",
    "work",
    "also",
    "show",
    "nice",
    "concept",
    "adjusted",
    "r",
    "square",
    "adjusted",
    "r",
    "square",
    "r",
    "square",
    "adjusted",
    "given",
    "formula",
    "given",
    "formula",
    "1",
    "1",
    "r²",
    "n",
    "1",
    "n",
    "total",
    "number",
    "samples",
    "n",
    "minus",
    "p",
    "minus",
    "1",
    "p",
    "p",
    "nothing",
    "number",
    "features",
    "predictors",
    "also",
    "say",
    "predictors",
    "suppose",
    "initially",
    "number",
    "predictors",
    "particular",
    "scenario",
    "scenario",
    "saw",
    "number",
    "predictors",
    "two",
    "particular",
    "case",
    "number",
    "predictor",
    "three",
    "predictor",
    "2",
    "got",
    "r",
    "squ",
    "90",
    "particular",
    "scenario",
    "calculation",
    "happen",
    "okay",
    "calculation",
    "happen",
    "let",
    "say",
    "r",
    "square",
    "adjusted",
    "little",
    "bit",
    "less",
    "little",
    "bit",
    "less",
    "let",
    "say",
    "it8",
    "6",
    "let",
    "say",
    "r",
    "square",
    "adjusted",
    "86",
    "based",
    "predictor",
    "2",
    "use",
    "predictor",
    "3",
    "predictor",
    "basically",
    "means",
    "number",
    "features",
    "going",
    "use",
    "one",
    "one",
    "feature",
    "nowhere",
    "related",
    "like",
    "gender",
    "getting",
    "basically",
    "getting",
    "r",
    "square",
    "increased",
    "91",
    "r",
    "square",
    "adjusted",
    "increase",
    "turn",
    "decrease",
    "right",
    "become",
    "82",
    "become",
    "show",
    "considered",
    "value",
    "8682",
    "see",
    "increase",
    "increase",
    "decrease",
    "basically",
    "happening",
    "see",
    "p",
    "value",
    "putting",
    "okay",
    "put",
    "p",
    "isal",
    "3",
    "obviously",
    "n",
    "minus",
    "p",
    "minus",
    "1",
    "become",
    "little",
    "bit",
    "smaller",
    "number",
    "sorry",
    "little",
    "bit",
    "uh",
    "smaller",
    "number",
    "right",
    "particular",
    "case",
    "correlated",
    "obviously",
    "high",
    "increasing",
    "also",
    "high",
    "let",
    "write",
    "equation",
    "something",
    "like",
    "second",
    "basically",
    "okay",
    "probably",
    "value",
    "may",
    "decreased",
    "let",
    "talk",
    "one",
    "r",
    "squ",
    "hope",
    "everybody",
    "understood",
    "n",
    "number",
    "data",
    "points",
    "p",
    "number",
    "predictors",
    "p",
    "increasing",
    "happen",
    "p",
    "keeps",
    "increasing",
    "value",
    "keep",
    "decreasing",
    "value",
    "keep",
    "decreasing",
    "values",
    "keep",
    "decreasing",
    "bigger",
    "number",
    "obviously",
    "big",
    "number",
    "big",
    "number",
    "divided",
    "small",
    "number",
    "obviously",
    "little",
    "bit",
    "bigger",
    "number",
    "1",
    "minus",
    "bigger",
    "number",
    "basically",
    "get",
    "values",
    "decreasing",
    "p",
    "value",
    "two",
    "particular",
    "case",
    "less",
    "smaller",
    "right",
    "least",
    "greater",
    "particular",
    "value",
    "right",
    "p",
    "equal",
    "3",
    "help",
    "p",
    "obviously",
    "r",
    "square",
    "support",
    "okay",
    "whether",
    "correlated",
    "always",
    "remember",
    "features",
    "highly",
    "correlated",
    "r",
    "square",
    "value",
    "increase",
    "tremendously",
    "less",
    "correlated",
    "small",
    "increase",
    "huge",
    "increase",
    "consider",
    "p",
    "equal",
    "2",
    "obviously",
    "trying",
    "find",
    "uh",
    "calculation",
    "n",
    "minus",
    "p",
    "minus",
    "1",
    "obviously",
    "greater",
    "p",
    "equal",
    "3",
    "p",
    "equal",
    "3",
    "value",
    "still",
    "smaller",
    "dividing",
    "bigger",
    "number",
    "smaller",
    "number",
    "obviously",
    "subtracting",
    "one",
    "basically",
    "means",
    "even",
    "though",
    "r",
    "square",
    "86",
    "may",
    "scenario",
    "since",
    "nowhere",
    "correlated",
    "basically",
    "getting",
    "82",
    "entire",
    "equation",
    "hope",
    "understanding",
    "much",
    "important",
    "understand",
    "important",
    "property",
    "simple",
    "way",
    "define",
    "p",
    "value",
    "keeps",
    "increasing",
    "number",
    "predictors",
    "keeps",
    "increasing",
    "r",
    "squ",
    "gets",
    "adjusted",
    "whatever",
    "r",
    "square",
    "getting",
    "respect",
    "always",
    "less",
    "particular",
    "r",
    "square",
    "one",
    "interview",
    "question",
    "asked",
    "one",
    "student",
    "r",
    "square",
    "adjusted",
    "r",
    "square",
    "always",
    "bigger",
    "definitely",
    "student",
    "said",
    "r",
    "square",
    "told",
    "explain",
    "adjusted",
    "r",
    "square",
    "specific",
    "happen",
    "agenda",
    "one",
    "ridge",
    "lasso",
    "regression",
    "second",
    "assumptions",
    "linear",
    "regression",
    "third",
    "point",
    "probably",
    "going",
    "discuss",
    "logistic",
    "regression",
    "fourth",
    "thing",
    "going",
    "discuss",
    "something",
    "called",
    "confusion",
    "matrix",
    "fifth",
    "thing",
    "going",
    "consider",
    "practicals",
    "lead",
    "lineer",
    "ridge",
    "lasso",
    "logistic",
    "first",
    "topic",
    "uh",
    "probably",
    "going",
    "discuss",
    "something",
    "called",
    "ridge",
    "lasso",
    "regression",
    "let",
    "understand",
    "ridge",
    "lasso",
    "regression",
    "remember",
    "previous",
    "session",
    "things",
    "discussed",
    "linear",
    "regression",
    "discussed",
    "cost",
    "function",
    "discussed",
    "r",
    "square",
    "adjusted",
    "adjusted",
    "r",
    "square",
    "sorry",
    "r",
    "square",
    "adjusted",
    "r",
    "square",
    "discussed",
    "gradient",
    "descent",
    "discussed",
    "nothing",
    "1",
    "2",
    "summation",
    "1",
    "2",
    "h",
    "theta",
    "x",
    "cost",
    "function",
    "discussed",
    "right",
    "yesterday",
    "cost",
    "function",
    "able",
    "give",
    "us",
    "gradient",
    "descent",
    "respect",
    "j",
    "theta",
    "j",
    "theta",
    "zer",
    "theta",
    "also",
    "write",
    "j",
    "theta",
    "comma",
    "theta",
    "0",
    "comma",
    "theta",
    "1",
    "let",
    "give",
    "scenario",
    "let",
    "say",
    "scenario",
    "specific",
    "scenario",
    "let",
    "say",
    "two",
    "points",
    "looks",
    "like",
    "okay",
    "two",
    "specific",
    "points",
    "happen",
    "probably",
    "try",
    "create",
    "best",
    "fit",
    "line",
    "best",
    "fit",
    "line",
    "definitely",
    "pass",
    "points",
    "like",
    "try",
    "calculate",
    "cost",
    "function",
    "value",
    "j",
    "theta",
    "0",
    "comma",
    "theta",
    "1",
    "let",
    "say",
    "particular",
    "case",
    "since",
    "passing",
    "origin",
    "theta",
    "0",
    "zero",
    "okay",
    "value",
    "theta",
    "0",
    "comma",
    "theta",
    "1",
    "obviously",
    "see",
    "difference",
    "obviously",
    "become",
    "zero",
    "understand",
    "data",
    "see",
    "right",
    "right",
    "data",
    "basically",
    "called",
    "training",
    "data",
    "data",
    "actually",
    "plotted",
    "two",
    "points",
    "specifically",
    "called",
    "training",
    "data",
    "problem",
    "data",
    "right",
    "see",
    "right",
    "exactly",
    "whatever",
    "line",
    "basically",
    "getting",
    "created",
    "uh",
    "hypothesis",
    "see",
    "passing",
    "every",
    "point",
    "reason",
    "cost",
    "zero",
    "main",
    "aim",
    "basically",
    "minimize",
    "cost",
    "function",
    "absolutely",
    "fine",
    "particular",
    "case",
    "model",
    "model",
    "getting",
    "trained",
    "initially",
    "data",
    "basically",
    "called",
    "training",
    "data",
    "imagine",
    "tomorrow",
    "new",
    "data",
    "points",
    "comes",
    "new",
    "data",
    "points",
    "let",
    "consider",
    "want",
    "basically",
    "uh",
    "come",
    "new",
    "data",
    "point",
    "particular",
    "scenario",
    "want",
    "predict",
    "respect",
    "particular",
    "point",
    "let",
    "say",
    "predicted",
    "point",
    "difference",
    "predicted",
    "real",
    "point",
    "quite",
    "huge",
    "yes",
    "basically",
    "creating",
    "condition",
    "called",
    "overfitting",
    "basically",
    "means",
    "even",
    "though",
    "model",
    "given",
    "trained",
    "well",
    "training",
    "data",
    "let",
    "write",
    "properly",
    "condition",
    "since",
    "since",
    "see",
    "every",
    "point",
    "basically",
    "passing",
    "best",
    "fit",
    "line",
    "happens",
    "causes",
    "something",
    "called",
    "overfitting",
    "really",
    "need",
    "understand",
    "overfitting",
    "overfitting",
    "mean",
    "overfitting",
    "basically",
    "means",
    "model",
    "performs",
    "well",
    "training",
    "data",
    "fails",
    "perform",
    "well",
    "test",
    "data",
    "test",
    "data",
    "test",
    "data",
    "basically",
    "points",
    "real",
    "test",
    "data",
    "answer",
    "points",
    "line",
    "like",
    "actually",
    "getting",
    "predicted",
    "point",
    "distance",
    "try",
    "calculate",
    "quite",
    "huge",
    "scenario",
    "whenever",
    "say",
    "model",
    "performs",
    "well",
    "training",
    "data",
    "fails",
    "perform",
    "well",
    "test",
    "data",
    "scenario",
    "say",
    "overfitting",
    "scenario",
    "model",
    "performs",
    "well",
    "training",
    "data",
    "condition",
    "called",
    "low",
    "bias",
    "fails",
    "perform",
    "test",
    "data",
    "basically",
    "called",
    "high",
    "high",
    "variance",
    "important",
    "okay",
    "make",
    "everyone",
    "understand",
    "one",
    "one",
    "performing",
    "well",
    "training",
    "data",
    "basically",
    "low",
    "bias",
    "whenever",
    "performs",
    "well",
    "test",
    "sorry",
    "fails",
    "perform",
    "well",
    "fails",
    "perform",
    "well",
    "test",
    "data",
    "basically",
    "high",
    "variance",
    "similarly",
    "may",
    "another",
    "scenario",
    "called",
    "underfitting",
    "let",
    "say",
    "something",
    "called",
    "underfitting",
    "underfitting",
    "scenario",
    "model",
    "fails",
    "perform",
    "gives",
    "bad",
    "accuracy",
    "say",
    "model",
    "always",
    "remember",
    "whenever",
    "talk",
    "bias",
    "understand",
    "something",
    "related",
    "training",
    "data",
    "whenever",
    "talk",
    "test",
    "data",
    "point",
    "time",
    "talk",
    "variance",
    "specifically",
    "whenever",
    "talk",
    "variance",
    "basically",
    "means",
    "talking",
    "test",
    "data",
    "overfitting",
    "basically",
    "low",
    "bias",
    "high",
    "variance",
    "low",
    "bias",
    "respect",
    "training",
    "data",
    "high",
    "variance",
    "respect",
    "test",
    "data",
    "model",
    "accuracy",
    "bad",
    "training",
    "data",
    "model",
    "accuracy",
    "also",
    "bad",
    "test",
    "data",
    "scenario",
    "basically",
    "say",
    "underfitting",
    "two",
    "conditions",
    "respect",
    "underfitting",
    "basically",
    "means",
    "training",
    "data",
    "also",
    "model",
    "giving",
    "bad",
    "accuracy",
    "test",
    "data",
    "also",
    "basically",
    "bad",
    "accuracy",
    "particular",
    "scenario",
    "definitely",
    "say",
    "two",
    "things",
    "underfitting",
    "one",
    "high",
    "bias",
    "high",
    "variance",
    "condition",
    "respect",
    "underfitting",
    "super",
    "important",
    "let",
    "explain",
    "suppose",
    "let",
    "consider",
    "one",
    "model",
    "model",
    "two",
    "model",
    "one",
    "model",
    "one",
    "model",
    "two",
    "model",
    "3",
    "okay",
    "guys",
    "suppose",
    "let",
    "say",
    "model",
    "training",
    "accuracy",
    "let",
    "say",
    "90",
    "let",
    "say",
    "test",
    "accuracy",
    "80",
    "particular",
    "case",
    "let",
    "say",
    "training",
    "accuracy",
    "92",
    "test",
    "accuracy",
    "91",
    "let",
    "say",
    "model",
    "three",
    "basically",
    "training",
    "accuracy",
    "70",
    "test",
    "accuracy",
    "65",
    "take",
    "particular",
    "case",
    "basically",
    "overfitting",
    "take",
    "particular",
    "thing",
    "basically",
    "becomes",
    "generalized",
    "model",
    "talk",
    "say",
    "okay",
    "also",
    "put",
    "nice",
    "color",
    "uh",
    "able",
    "understand",
    "becomes",
    "generalized",
    "model",
    "finally",
    "becomes",
    "underfitting",
    "right",
    "fitting",
    "red",
    "color",
    "say",
    "underfitting",
    "main",
    "properties",
    "overfitting",
    "said",
    "scenario",
    "since",
    "performing",
    "well",
    "training",
    "data",
    "low",
    "bias",
    "high",
    "variance",
    "particular",
    "case",
    "low",
    "bias",
    "low",
    "variance",
    "particular",
    "case",
    "high",
    "bias",
    "high",
    "variance",
    "understand",
    "terminology",
    "particular",
    "way",
    "able",
    "understand",
    "require",
    "always",
    "generalized",
    "model",
    "whenever",
    "new",
    "data",
    "definitely",
    "come",
    "generalized",
    "model",
    "able",
    "give",
    "us",
    "good",
    "output",
    "let",
    "go",
    "back",
    "particular",
    "example",
    "able",
    "see",
    "straight",
    "line",
    "red",
    "line",
    "actually",
    "created",
    "basically",
    "overfitting",
    "whenever",
    "probably",
    "get",
    "new",
    "points",
    "real",
    "value",
    "predicted",
    "points",
    "able",
    "see",
    "difference",
    "quite",
    "huge",
    "definitely",
    "scenario",
    "overfitting",
    "low",
    "bias",
    "high",
    "weight",
    "let",
    "go",
    "ahead",
    "take",
    "example",
    "line",
    "actually",
    "drawn",
    "two",
    "points",
    "draw",
    "line",
    "best",
    "fit",
    "line",
    "passing",
    "points",
    "scenario",
    "basically",
    "causing",
    "overfitting",
    "problem",
    "also",
    "shown",
    "j",
    "theta",
    "1",
    "zero",
    "scenario",
    "since",
    "passing",
    "exactly",
    "predicted",
    "point",
    "also",
    "understand",
    "one",
    "thing",
    "take",
    "assumptions",
    "take",
    "definitely",
    "talk",
    "cost",
    "function",
    "cost",
    "function",
    "nothing",
    "1x",
    "2",
    "summation",
    "1",
    "2",
    "h",
    "theta",
    "x",
    "whole",
    "let",
    "consider",
    "going",
    "use",
    "h",
    "theta",
    "x",
    "going",
    "basically",
    "write",
    "hat",
    "okay",
    "let",
    "focus",
    "specific",
    "point",
    "take",
    "going",
    "focus",
    "particular",
    "point",
    "definitely",
    "write",
    "hat",
    "minus",
    "whole",
    "squ",
    "hat",
    "minus",
    "hat",
    "whole",
    "square",
    "nothing",
    "difference",
    "predicted",
    "value",
    "real",
    "value",
    "okay",
    "actually",
    "trying",
    "get",
    "scenario",
    "adding",
    "values",
    "obviously",
    "going",
    "get",
    "value",
    "zero",
    "make",
    "sure",
    "value",
    "come",
    "zero",
    "still",
    "fitting",
    "ridge",
    "regression",
    "come",
    "picture",
    "ridge",
    "lasso",
    "come",
    "picture",
    "use",
    "ridge",
    "lasso",
    "suppose",
    "use",
    "ridge",
    "ridge",
    "say",
    "also",
    "called",
    "l2",
    "regularization",
    "l2",
    "regularization",
    "basically",
    "adds",
    "unique",
    "parameter",
    "add",
    "one",
    "sample",
    "value",
    "like",
    "lambda",
    "multiplied",
    "slope",
    "square",
    "slope",
    "whatever",
    "slope",
    "particular",
    "line",
    "going",
    "square",
    "suppose",
    "take",
    "equation",
    "looks",
    "like",
    "h",
    "theta",
    "x",
    "equal",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "particular",
    "case",
    "theta",
    "0",
    "zero",
    "h",
    "theta",
    "x",
    "nothing",
    "theta",
    "1",
    "theta",
    "1",
    "specifically",
    "called",
    "slope",
    "basically",
    "taking",
    "theta",
    "1",
    "actually",
    "making",
    "square",
    "square",
    "always",
    "understand",
    "want",
    "make",
    "zero",
    "becomes",
    "zero",
    "may",
    "lead",
    "overfitting",
    "condition",
    "happen",
    "add",
    "particular",
    "equation",
    "add",
    "particular",
    "equation",
    "obviously",
    "come",
    "zero",
    "let",
    "consider",
    "lambda",
    "value",
    "lambda",
    "value",
    "one",
    "talk",
    "set",
    "lambda",
    "value",
    "okay",
    "let",
    "consider",
    "initializing",
    "one",
    "let",
    "say",
    "lambda",
    "value",
    "1",
    "l",
    "lambda",
    "value",
    "1",
    "let",
    "consider",
    "slope",
    "value",
    "initially",
    "two",
    "two",
    "got",
    "best",
    "fit",
    "line",
    "going",
    "consider",
    "total",
    "sum",
    "considering",
    "value",
    "three",
    "cost",
    "function",
    "stop",
    "still",
    "minimize",
    "reduce",
    "three",
    "value",
    "change",
    "theta",
    "1",
    "value",
    "let",
    "say",
    "theta",
    "van",
    "value",
    "changed",
    "got",
    "another",
    "best",
    "fit",
    "line",
    "looks",
    "something",
    "like",
    "like",
    "next",
    "best",
    "fit",
    "line",
    "talk",
    "lambda",
    "lambda",
    "hyper",
    "parameter",
    "guys",
    "exactly",
    "lambda",
    "talk",
    "basically",
    "change",
    "line",
    "see",
    "getting",
    "line",
    "let",
    "consider",
    "changed",
    "theta",
    "1",
    "value",
    "since",
    "need",
    "minimize",
    "need",
    "minimize",
    "calculate",
    "slope",
    "particular",
    "line",
    "try",
    "create",
    "new",
    "line",
    "sorry",
    "two",
    "two",
    "three",
    "second",
    "guys",
    "0",
    "1",
    "multiplied",
    "2",
    "nothing",
    "4",
    "cost",
    "function",
    "stop",
    "going",
    "still",
    "reduce",
    "order",
    "reduce",
    "theta",
    "1",
    "value",
    "get",
    "changed",
    "get",
    "next",
    "best",
    "fit",
    "line",
    "point",
    "happen",
    "scenario",
    "best",
    "fit",
    "line",
    "definitely",
    "get",
    "kind",
    "small",
    "difference",
    "go",
    "ahead",
    "consider",
    "new",
    "equation",
    "hat",
    "minus",
    "i²",
    "lambda",
    "slope",
    "squar",
    "value",
    "small",
    "value",
    "difference",
    "plus",
    "1",
    "multiplied",
    "understand",
    "whether",
    "slope",
    "increase",
    "particular",
    "case",
    "whether",
    "decrease",
    "particular",
    "case",
    "slope",
    "value",
    "let",
    "say",
    "got",
    "slope",
    "particular",
    "line",
    "particular",
    "scenario",
    "slope",
    "definitely",
    "decrease",
    "let",
    "say",
    "case",
    "two",
    "initially",
    "basically",
    "whole",
    "squ",
    "small",
    "value",
    "plus",
    "1",
    "squ",
    "let",
    "consider",
    "slope",
    "one",
    "simple",
    "value",
    "5",
    "get",
    "plus",
    "small",
    "value",
    "less",
    "three",
    "right",
    "obviously",
    "less",
    "three",
    "equal",
    "3",
    "understand",
    "happening",
    "value",
    "getting",
    "reduced",
    "4",
    "3",
    "importance",
    "ridge",
    "happen",
    "try",
    "get",
    "generalized",
    "model",
    "low",
    "bias",
    "low",
    "variance",
    "instead",
    "overfitting",
    "condition",
    "know",
    "specifically",
    "adding",
    "ridge",
    "l2",
    "regularization",
    "basically",
    "prevent",
    "overfitting",
    "stopping",
    "trying",
    "reduce",
    "unless",
    "get",
    "line",
    "get",
    "line",
    "able",
    "handle",
    "able",
    "handle",
    "uh",
    "generalized",
    "model",
    "see",
    "new",
    "points",
    "like",
    "drew",
    "distance",
    "less",
    "able",
    "see",
    "able",
    "create",
    "generalized",
    "model",
    "guys",
    "small",
    "value",
    "see",
    "initially",
    "line",
    "obviously",
    "zero",
    "try",
    "slightly",
    "move",
    "able",
    "see",
    "slight",
    "movement",
    "movement",
    "basically",
    "specifying",
    "specifying",
    "slope",
    "steep",
    "probably",
    "steep",
    "slope",
    "obviously",
    "leads",
    "time",
    "overfitting",
    "condition",
    "steep",
    "less",
    "steeper",
    "actually",
    "help",
    "create",
    "generalized",
    "model",
    "seeing",
    "playing",
    "amount",
    "time",
    "value",
    "reduce",
    "point",
    "time",
    "get",
    "almost",
    "minimal",
    "value",
    "smaller",
    "value",
    "also",
    "specify",
    "iterations",
    "many",
    "times",
    "probably",
    "train",
    "iterations",
    "also",
    "hyperparameter",
    "based",
    "number",
    "iterations",
    "probably",
    "see",
    "r",
    "square",
    "adjusted",
    "r",
    "square",
    "iterations",
    "based",
    "number",
    "iterations",
    "never",
    "become",
    "zero",
    "guys",
    "understand",
    "zero",
    "possible",
    "becomes",
    "zero",
    "trust",
    "overfitting",
    "model",
    "get",
    "something",
    "zero",
    "lambda",
    "coming",
    "lambda",
    "lambda",
    "hyperparameter",
    "basically",
    "check",
    "fast",
    "want",
    "lessen",
    "steepness",
    "fast",
    "want",
    "make",
    "steepness",
    "grow",
    "higher",
    "right",
    "lambda",
    "also",
    "selected",
    "using",
    "hyper",
    "parameter",
    "also",
    "show",
    "today",
    "practical",
    "mean",
    "iterations",
    "iteration",
    "basically",
    "means",
    "many",
    "time",
    "want",
    "change",
    "theta",
    "1",
    "value",
    "many",
    "times",
    "want",
    "change",
    "theta",
    "value",
    "convergence",
    "algorithm",
    "right",
    "convergence",
    "algorithm",
    "l2",
    "regularization",
    "ridge",
    "basically",
    "used",
    "way",
    "never",
    "overfit",
    "assume",
    "theta",
    "0",
    "equal",
    "0",
    "considering",
    "passes",
    "origin",
    "right",
    "origin",
    "lambda",
    "hyper",
    "parameter",
    "steep",
    "basically",
    "means",
    "steep",
    "line",
    "line",
    "line",
    "quite",
    "steep",
    "line",
    "less",
    "steep",
    "go",
    "next",
    "regularization",
    "called",
    "lasso",
    "raso",
    "r",
    "lasso",
    "regression",
    "also",
    "called",
    "l1",
    "regularization",
    "formula",
    "changing",
    "little",
    "bit",
    "hat",
    "minus",
    "whole",
    "square",
    "adding",
    "parameter",
    "lambda",
    "understand",
    "adding",
    "slope",
    "square",
    "adding",
    "mode",
    "slope",
    "adding",
    "mode",
    "slope",
    "mode",
    "slope",
    "work",
    "actually",
    "help",
    "feature",
    "selection",
    "may",
    "thinking",
    "feature",
    "selection",
    "crash",
    "let",
    "consider",
    "equation",
    "let",
    "say",
    "many",
    "many",
    "features",
    "many",
    "many",
    "many",
    "features",
    "okay",
    "h",
    "theta",
    "x",
    "indicating",
    "hat",
    "let",
    "say",
    "writing",
    "equation",
    "apart",
    "preventing",
    "overfitting",
    "also",
    "help",
    "feature",
    "selection",
    "let",
    "show",
    "example",
    "h",
    "theta",
    "x",
    "probably",
    "writing",
    "hat",
    "basically",
    "indicated",
    "something",
    "able",
    "see",
    "nothing",
    "let",
    "say",
    "multiple",
    "features",
    "like",
    "particular",
    "features",
    "obviously",
    "many",
    "coefficients",
    "many",
    "slopes",
    "mod",
    "slope",
    "nothing",
    "mod",
    "x1",
    "plus",
    "x2",
    "plus",
    "x3",
    "plus",
    "x4",
    "plus",
    "x5",
    "like",
    "xn",
    "particular",
    "case",
    "basically",
    "helping",
    "sorry",
    "x1",
    "sorry",
    "second",
    "mod",
    "taken",
    "data",
    "point",
    "data",
    "points",
    "mod",
    "theta",
    "0",
    "theta",
    "1",
    "theta",
    "2",
    "theta",
    "3",
    "theta",
    "4",
    "theta",
    "5",
    "like",
    "theta",
    "n",
    "able",
    "see",
    "basically",
    "uh",
    "basically",
    "calculating",
    "slope",
    "go",
    "ahead",
    "guys",
    "whichever",
    "features",
    "probably",
    "playing",
    "amazing",
    "role",
    "theta",
    "value",
    "coefficient",
    "value",
    "slope",
    "value",
    "small",
    "like",
    "entire",
    "feature",
    "neglected",
    "entire",
    "feature",
    "neglected",
    "particular",
    "case",
    "squaring",
    "squaring",
    "value",
    "also",
    "increasing",
    "mode",
    "value",
    "increase",
    "instead",
    "condition",
    "wherein",
    "basically",
    "neglecting",
    "features",
    "important",
    "specific",
    "problem",
    "statement",
    "help",
    "l1",
    "regularization",
    "lasso",
    "able",
    "two",
    "important",
    "things",
    "one",
    "preventing",
    "overfitting",
    "second",
    "case",
    "many",
    "features",
    "many",
    "features",
    "important",
    "okay",
    "basically",
    "finding",
    "slope",
    "line",
    "best",
    "fit",
    "line",
    "particular",
    "case",
    "also",
    "help",
    "perform",
    "feature",
    "selection",
    "importance",
    "entire",
    "importance",
    "importance",
    "uh",
    "ridge",
    "lasso",
    "regression",
    "going",
    "write",
    "l1",
    "regularization",
    "obviously",
    "discussed",
    "l2",
    "regularization",
    "also",
    "probably",
    "understood",
    "lambda",
    "one",
    "hyperparameter",
    "okay",
    "specifically",
    "using",
    "okay",
    "based",
    "lambda",
    "found",
    "cross",
    "validation",
    "cross",
    "validation",
    "technique",
    "wherein",
    "try",
    "probably",
    "train",
    "model",
    "try",
    "find",
    "specific",
    "things",
    "okay",
    "exact",
    "value",
    "also",
    "play",
    "multiple",
    "values",
    "short",
    "trying",
    "reduce",
    "cost",
    "function",
    "way",
    "uh",
    "definitely",
    "never",
    "become",
    "zero",
    "basically",
    "reduce",
    "based",
    "lambda",
    "slope",
    "value",
    "scenario",
    "ask",
    "definitely",
    "try",
    "regularization",
    "see",
    "wherever",
    "performance",
    "matrix",
    "good",
    "use",
    "cross",
    "validation",
    "basically",
    "means",
    "try",
    "use",
    "different",
    "different",
    "lambda",
    "value",
    "basically",
    "ally",
    "use",
    "short",
    "let",
    "write",
    "ridge",
    "regression",
    "l2",
    "norm",
    "simply",
    "writing",
    "cost",
    "function",
    "particular",
    "case",
    "little",
    "bit",
    "different",
    "definitely",
    "write",
    "cost",
    "function",
    "h",
    "theta",
    "x",
    "plus",
    "lambda",
    "multiplied",
    "slope",
    "square",
    "purpose",
    "purpose",
    "simple",
    "preventing",
    "overfitting",
    "respect",
    "ridge",
    "recreation",
    "l2",
    "go",
    "ahead",
    "discuss",
    "next",
    "one",
    "called",
    "lasso",
    "regression",
    "also",
    "called",
    "l1",
    "regularization",
    "case",
    "lasso",
    "regression",
    "cost",
    "function",
    "h",
    "theta",
    "x",
    "ius",
    "i²",
    "plus",
    "lambda",
    "ultied",
    "mode",
    "flow",
    "specific",
    "thing",
    "purpose",
    "purpose",
    "two",
    "one",
    "prevent",
    "overfitting",
    "second",
    "one",
    "something",
    "called",
    "feature",
    "selection",
    "two",
    "outcomes",
    "entire",
    "thing",
    "see",
    "respect",
    "lasso",
    "right",
    "slopes",
    "slopes",
    "theta",
    "0",
    "plus",
    "theta",
    "1",
    "plus",
    "theta",
    "2",
    "plus",
    "theta",
    "3",
    "like",
    "theta",
    "n",
    "many",
    "number",
    "thetas",
    "many",
    "number",
    "features",
    "many",
    "number",
    "features",
    "basically",
    "means",
    "multiple",
    "slopes",
    "right",
    "features",
    "performing",
    "well",
    "contribution",
    "finding",
    "output",
    "coefficient",
    "value",
    "almost",
    "nil",
    "right",
    "much",
    "near",
    "zero",
    "short",
    "neglecting",
    "value",
    "using",
    "modulus",
    "squaring",
    "increasing",
    "values",
    "continue",
    "uh",
    "probably",
    "also",
    "discuss",
    "assumptions",
    "linear",
    "regressions",
    "assumptions",
    "linear",
    "regression",
    "particular",
    "scenario",
    "assumption",
    "number",
    "one",
    "point",
    "linear",
    "regression",
    "features",
    "normal",
    "gion",
    "distribution",
    "features",
    "follows",
    "particular",
    "distribution",
    "obviously",
    "good",
    "model",
    "get",
    "trained",
    "well",
    "one",
    "concept",
    "called",
    "feature",
    "transformation",
    "future",
    "transformation",
    "always",
    "understand",
    "happen",
    "model",
    "fall",
    "follow",
    "gan",
    "distribution",
    "apply",
    "kind",
    "mathematical",
    "equation",
    "onto",
    "data",
    "try",
    "convert",
    "normal",
    "orian",
    "distribution",
    "second",
    "assumption",
    "would",
    "definitely",
    "like",
    "make",
    "standard",
    "scalar",
    "standard",
    "digestion",
    "standard",
    "dig",
    "nothing",
    "kind",
    "scaling",
    "data",
    "using",
    "z",
    "score",
    "hope",
    "everybody",
    "remembers",
    "z",
    "score",
    "basically",
    "apply",
    "mean",
    "equal",
    "zero",
    "standard",
    "deviation",
    "equal",
    "1",
    "see",
    "guys",
    "wherever",
    "gradient",
    "descent",
    "involved",
    "good",
    "basically",
    "standardization",
    "initial",
    "point",
    "small",
    "point",
    "somewhere",
    "reach",
    "global",
    "minima",
    "training",
    "happen",
    "quickly",
    "otherwise",
    "happen",
    "values",
    "quite",
    "huge",
    "graph",
    "may",
    "big",
    "point",
    "come",
    "third",
    "point",
    "linear",
    "regression",
    "works",
    "respect",
    "linearity",
    "works",
    "data",
    "linearly",
    "separable",
    "say",
    "linearly",
    "separable",
    "linearity",
    "come",
    "picture",
    "data",
    "much",
    "linear",
    "obviously",
    "able",
    "give",
    "good",
    "answer",
    "like",
    "logistic",
    "regression",
    "also",
    "going",
    "discuss",
    "today",
    "also",
    "property",
    "may",
    "asking",
    "compulsory",
    "standardization",
    "guys",
    "want",
    "increase",
    "training",
    "time",
    "model",
    "want",
    "optimize",
    "model",
    "would",
    "suggest",
    "go",
    "ahead",
    "standardization",
    "coming",
    "fourth",
    "point",
    "really",
    "need",
    "check",
    "multicolinearity",
    "also",
    "one",
    "kind",
    "check",
    "basically",
    "multicol",
    "linearities",
    "let",
    "say",
    "x1",
    "x2",
    "output",
    "feature",
    "let",
    "say",
    "x3",
    "also",
    "let",
    "say",
    "try",
    "see",
    "colinearity",
    "two",
    "feature",
    "correlated",
    "two",
    "feature",
    "let",
    "say",
    "two",
    "feature",
    "95",
    "correlated",
    "wise",
    "decision",
    "use",
    "features",
    "let",
    "say",
    "let",
    "let",
    "say",
    "two",
    "features",
    "95",
    "correlated",
    "highly",
    "correlated",
    "necessary",
    "use",
    "feature",
    "particular",
    "scenario",
    "answer",
    "drop",
    "particular",
    "feature",
    "okay",
    "drop",
    "particular",
    "feature",
    "one",
    "feature",
    "definitely",
    "drop",
    "based",
    "use",
    "one",
    "single",
    "feature",
    "basically",
    "prediction",
    "also",
    "concept",
    "called",
    "variation",
    "inflation",
    "factor",
    "try",
    "make",
    "dedicated",
    "video",
    "multical",
    "also",
    "solved",
    "help",
    "variation",
    "inflation",
    "factor",
    "one",
    "term",
    "homos",
    "orc",
    "kind",
    "terminologies",
    "also",
    "use",
    "one",
    "condition",
    "almost",
    "satisfied",
    "assumptions",
    "definitely",
    "able",
    "outperform",
    "linear",
    "regression",
    "got",
    "idea",
    "assumptions",
    "also",
    "got",
    "idea",
    "multiple",
    "things",
    "okay",
    "let",
    "go",
    "towards",
    "something",
    "called",
    "logistic",
    "regression",
    "logistic",
    "regression",
    "logistic",
    "regression",
    "first",
    "type",
    "algorithm",
    "going",
    "learn",
    "classification",
    "let",
    "say",
    "classification",
    "one",
    "example",
    "know",
    "suppose",
    "say",
    "number",
    "hours",
    "study",
    "hours",
    "number",
    "play",
    "hours",
    "based",
    "want",
    "predict",
    "whether",
    "child",
    "passing",
    "failing",
    "suppose",
    "two",
    "features",
    "want",
    "predict",
    "whether",
    "pass",
    "fail",
    "able",
    "see",
    "fixed",
    "number",
    "categories",
    "specifically",
    "particular",
    "scenario",
    "two",
    "categories",
    "binary",
    "logistic",
    "regression",
    "works",
    "well",
    "binary",
    "classification",
    "uh",
    "question",
    "comes",
    "solve",
    "multiclass",
    "classification",
    "using",
    "logistic",
    "answer",
    "simply",
    "yes",
    "definitely",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "discuss",
    "uh",
    "logistic",
    "regression",
    "main",
    "purpose",
    "logistic",
    "regression",
    "first",
    "let",
    "let",
    "uh",
    "understand",
    "one",
    "scenario",
    "okay",
    "suppose",
    "feature",
    "basically",
    "says",
    "um",
    "number",
    "study",
    "hours",
    "like",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "let",
    "say",
    "pass",
    "point",
    "basically",
    "pass",
    "point",
    "basically",
    "fail",
    "two",
    "conditions",
    "outcomes",
    "try",
    "make",
    "data",
    "points",
    "let",
    "say",
    "study",
    "less",
    "3",
    "hours",
    "probably",
    "fail",
    "study",
    "3",
    "hours",
    "probably",
    "pass",
    "make",
    "fail",
    "make",
    "pass",
    "points",
    "1",
    "2",
    "3",
    "let",
    "say",
    "training",
    "data",
    "set",
    "first",
    "question",
    "says",
    "okay",
    "chris",
    "fine",
    "data",
    "whenever",
    "less",
    "three",
    "basically",
    "person",
    "failing",
    "greater",
    "five",
    "greater",
    "three",
    "basically",
    "showing",
    "data",
    "points",
    "points",
    "respect",
    "pass",
    "ca",
    "solve",
    "problem",
    "first",
    "linear",
    "regression",
    "help",
    "linear",
    "regression",
    "first",
    "point",
    "yes",
    "definitely",
    "draw",
    "best",
    "fit",
    "line",
    "best",
    "fit",
    "line",
    "particular",
    "scenario",
    "may",
    "something",
    "like",
    "may",
    "may",
    "look",
    "something",
    "like",
    "fail",
    "nothing",
    "zero",
    "pass",
    "one",
    "middle",
    "point",
    "basically",
    "obviously",
    "help",
    "linear",
    "regression",
    "able",
    "create",
    "best",
    "fit",
    "line",
    "put",
    "scenario",
    "whenever",
    "value",
    "less",
    "than5",
    "whenever",
    "value",
    "less",
    "whenever",
    "output",
    "less",
    "than5",
    "let",
    "say",
    "new",
    "data",
    "point",
    "based",
    "try",
    "prediction",
    "actually",
    "able",
    "get",
    "output",
    "getting",
    "output",
    "basically",
    "particular",
    "scenario",
    "obviously",
    "able",
    "say",
    "yes",
    "person",
    "write",
    "condition",
    "saying",
    "h",
    "theta",
    "x",
    "value",
    "less",
    "output",
    "zero",
    "let",
    "say",
    "less",
    "say",
    "less",
    "equal",
    "less",
    "than5",
    "output",
    "zero",
    "right",
    "particular",
    "case",
    "zero",
    "basically",
    "means",
    "fail",
    "similarly",
    "scenario",
    "say",
    "theta",
    "x",
    "greater",
    "equal",
    "5",
    "basically",
    "one",
    "nothing",
    "pass",
    "two",
    "condition",
    "definitely",
    "write",
    "center",
    "point",
    "point",
    "probably",
    "come",
    "let",
    "say",
    "point",
    "coming",
    "right",
    "let",
    "say",
    "new",
    "data",
    "point",
    "somewhere",
    "coming",
    "red",
    "point",
    "basically",
    "draw",
    "straight",
    "line",
    "come",
    "extend",
    "line",
    "long",
    "extend",
    "line",
    "extend",
    "line",
    "see",
    "based",
    "actually",
    "getting",
    "particular",
    "prediction",
    "greater",
    "say",
    "okay",
    "person",
    "passed",
    "obviously",
    "fine",
    "obviously",
    "working",
    "better",
    "obviously",
    "working",
    "better",
    "problem",
    "using",
    "linear",
    "regression",
    "okay",
    "order",
    "solve",
    "particular",
    "problem",
    "specifically",
    "logistic",
    "regression",
    "answer",
    "much",
    "simple",
    "guys",
    "answer",
    "whenever",
    "let",
    "say",
    "outlier",
    "looks",
    "something",
    "like",
    "suppose",
    "outlier",
    "comes",
    "like",
    "value",
    "let",
    "say",
    "value",
    "nothing",
    "7",
    "8",
    "9",
    "10",
    "let",
    "say",
    "number",
    "study",
    "hours",
    "studying",
    "nine",
    "obviously",
    "pass",
    "particular",
    "scenario",
    "outlier",
    "entire",
    "line",
    "change",
    "probably",
    "get",
    "line",
    "looks",
    "something",
    "like",
    "okay",
    "line",
    "basically",
    "move",
    "something",
    "like",
    "get",
    "moved",
    "something",
    "like",
    "gets",
    "moves",
    "completely",
    "like",
    "even",
    "five",
    "even",
    "point",
    "actually",
    "predicting",
    "let",
    "say",
    "particular",
    "point",
    "try",
    "find",
    "showing",
    "less",
    "five",
    "particular",
    "value",
    "answer",
    "wrong",
    "right",
    "studying",
    "5",
    "hours",
    "ob",
    "viously",
    "b",
    "based",
    "previous",
    "line",
    "person",
    "pass",
    "scenario",
    "failing",
    "coming",
    "less",
    "real",
    "value",
    "basically",
    "passed",
    "hope",
    "understanding",
    "outlier",
    "entire",
    "line",
    "getting",
    "changed",
    "fix",
    "particular",
    "problem",
    "two",
    "scenarios",
    "first",
    "obviously",
    "outlier",
    "entire",
    "line",
    "getting",
    "shifted",
    "second",
    "point",
    "sometimes",
    "also",
    "getting",
    "greater",
    "one",
    "also",
    "getting",
    "less",
    "one",
    "suppose",
    "try",
    "calculate",
    "particular",
    "point",
    "project",
    "behind",
    "getting",
    "negative",
    "value",
    "squash",
    "function",
    "squash",
    "function",
    "become",
    "plain",
    "line",
    "right",
    "squash",
    "use",
    "something",
    "called",
    "sigmoid",
    "activation",
    "function",
    "sigmoid",
    "function",
    "somebody",
    "ask",
    "use",
    "linear",
    "regession",
    "order",
    "solve",
    "classification",
    "problem",
    "answer",
    "much",
    "simple",
    "say",
    "specific",
    "points",
    "try",
    "go",
    "ahead",
    "solve",
    "linear",
    "regression",
    "help",
    "cost",
    "function",
    "everything",
    "try",
    "understand",
    "cost",
    "function",
    "look",
    "logistic",
    "regression",
    "second",
    "reason",
    "told",
    "right",
    "greater",
    "zero",
    "line",
    "going",
    "greater",
    "zero",
    "right",
    "greater",
    "zero",
    "z",
    "one",
    "becoming",
    "greater",
    "zero",
    "already",
    "told",
    "maximum",
    "minimum",
    "value",
    "1",
    "zero",
    "hope",
    "understood",
    "linear",
    "reg",
    "used",
    "okay",
    "showed",
    "scenarios",
    "linear",
    "regression",
    "used",
    "continue",
    "probably",
    "discuss",
    "things",
    "uh",
    "try",
    "understand",
    "fine",
    "exactly",
    "logistic",
    "regression",
    "decision",
    "boundaries",
    "basically",
    "created",
    "go",
    "ahead",
    "discuss",
    "specific",
    "thing",
    "let",
    "go",
    "ahead",
    "values",
    "always",
    "0",
    "one",
    "particular",
    "case",
    "binary",
    "classification",
    "problem",
    "answer",
    "let",
    "go",
    "ahead",
    "let",
    "define",
    "decision",
    "boundary",
    "decision",
    "boundary",
    "decision",
    "boundary",
    "case",
    "logistic",
    "regression",
    "first",
    "usual",
    "logistic",
    "regression",
    "defined",
    "hypothesis",
    "okay",
    "guys",
    "first",
    "let",
    "see",
    "writing",
    "h",
    "theta",
    "h",
    "theta",
    "x",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "theta",
    "2",
    "x",
    "like",
    "x1",
    "x2",
    "theta",
    "n",
    "xn",
    "scenario",
    "write",
    "entire",
    "equation",
    "theta",
    "transpose",
    "x",
    "obviously",
    "definitely",
    "write",
    "way",
    "right",
    "notation",
    "probably",
    "seeing",
    "many",
    "places",
    "respect",
    "decision",
    "boundary",
    "logistic",
    "regression",
    "theta",
    "see",
    "like",
    "write",
    "saying",
    "okay",
    "since",
    "consider",
    "two",
    "things",
    "one",
    "squashing",
    "line",
    "okay",
    "squashing",
    "basically",
    "happen",
    "see",
    "line",
    "saw",
    "right",
    "line",
    "suppose",
    "data",
    "points",
    "data",
    "points",
    "want",
    "create",
    "best",
    "fit",
    "line",
    "create",
    "basically",
    "create",
    "like",
    "also",
    "two",
    "things",
    "one",
    "squash",
    "squash",
    "right",
    "squash",
    "squash",
    "order",
    "squash",
    "saying",
    "squash",
    "squash",
    "means",
    "okay",
    "order",
    "use",
    "function",
    "called",
    "sigmoid",
    "activation",
    "function",
    "basically",
    "means",
    "happens",
    "obviously",
    "know",
    "line",
    "basically",
    "denoted",
    "h",
    "theta",
    "x",
    "equal",
    "denote",
    "straight",
    "line",
    "let",
    "write",
    "nicely",
    "denote",
    "straight",
    "line",
    "straight",
    "line",
    "obviously",
    "denoted",
    "theta",
    "0",
    "theta",
    "1",
    "x1",
    "let",
    "say",
    "top",
    "top",
    "apply",
    "something",
    "top",
    "value",
    "apply",
    "something",
    "make",
    "line",
    "straight",
    "instead",
    "expanding",
    "way",
    "hypothesis",
    "basically",
    "g",
    "g",
    "basically",
    "function",
    "theta",
    "0",
    "theta",
    "1",
    "x1",
    "trying",
    "basically",
    "trying",
    "apply",
    "mathematical",
    "formula",
    "top",
    "linear",
    "regression",
    "squash",
    "line",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "find",
    "g",
    "okay",
    "g",
    "say",
    "let",
    "z",
    "equal",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "initializing",
    "h",
    "theta",
    "x",
    "nothing",
    "g",
    "z",
    "need",
    "understand",
    "z",
    "g",
    "z",
    "basically",
    "specify",
    "g",
    "function",
    "g",
    "function",
    "nothing",
    "h",
    "theta",
    "x",
    "equal",
    "1",
    "1",
    "e",
    "minus",
    "z",
    "short",
    "try",
    "initialize",
    "zed",
    "1",
    "e",
    "minus",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "h",
    "theta",
    "x",
    "hypothesis",
    "obviously",
    "works",
    "well",
    "able",
    "squash",
    "function",
    "basically",
    "hypothesis",
    "definitely",
    "trying",
    "use",
    "function",
    "actually",
    "able",
    "see",
    "called",
    "sigmoid",
    "logistic",
    "function",
    "need",
    "understand",
    "sigmoid",
    "function",
    "look",
    "like",
    "graph",
    "graph",
    "looks",
    "something",
    "like",
    "zed",
    "value",
    "g",
    "z",
    "05",
    "sigmoid",
    "function",
    "curve",
    "one",
    "zero",
    "value",
    "make",
    "lot",
    "assumptions",
    "assumptions",
    "basically",
    "make",
    "g",
    "zed",
    "g",
    "zed",
    "greater",
    "equal",
    "obviously",
    "greater",
    "equal",
    "zed",
    "value",
    "greater",
    "equal",
    "zero",
    "major",
    "assumptions",
    "basically",
    "make",
    "whenever",
    "g",
    "z",
    "greater",
    "g",
    "z",
    "greater",
    "equal",
    "whenever",
    "zed",
    "greater",
    "equal",
    "z",
    "obviously",
    "whenever",
    "zed",
    "value",
    "greater",
    "z",
    "greater",
    "zed",
    "value",
    "less",
    "zero",
    "become",
    "basically",
    "less",
    "write",
    "specific",
    "condition",
    "also",
    "want",
    "important",
    "condition",
    "called",
    "logistic",
    "regression",
    "see",
    "guys",
    "help",
    "regression",
    "creating",
    "straight",
    "line",
    "help",
    "concept",
    "sigmo",
    "able",
    "squash",
    "probably",
    "combined",
    "name",
    "uh",
    "basically",
    "written",
    "way",
    "squashing",
    "best",
    "fit",
    "l",
    "line",
    "help",
    "overcome",
    "outlier",
    "issues",
    "yes",
    "obviously",
    "able",
    "help",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "solve",
    "problem",
    "statement",
    "usually",
    "let",
    "consider",
    "training",
    "set",
    "let",
    "consider",
    "training",
    "set",
    "suppose",
    "training",
    "points",
    "like",
    "x",
    "1",
    "comma",
    "1",
    "let",
    "say",
    "x",
    "2a",
    "2",
    "okay",
    "x",
    "3a",
    "3",
    "like",
    "lot",
    "training",
    "points",
    "finally",
    "x",
    "n",
    "comma",
    "n",
    "let",
    "say",
    "training",
    "data",
    "uh",
    "belong",
    "zero",
    "1",
    "two",
    "outputs",
    "since",
    "solving",
    "binary",
    "classification",
    "problem",
    "training",
    "set",
    "two",
    "outputs",
    "hope",
    "everybody",
    "knows",
    "j",
    "theta",
    "z",
    "nothing",
    "1",
    "e",
    "minus",
    "z",
    "z",
    "nothing",
    "theta",
    "0",
    "theta",
    "1",
    "x1",
    "theta",
    "0",
    "select",
    "theta",
    "particular",
    "case",
    "let",
    "consider",
    "theta",
    "0",
    "0",
    "passing",
    "origin",
    "time",
    "pass",
    "sake",
    "suppose",
    "z",
    "theta",
    "1",
    "x",
    "need",
    "change",
    "parameter",
    "parameter",
    "theta",
    "1",
    "change",
    "parameter",
    "theta",
    "1",
    "way",
    "get",
    "best",
    "fit",
    "line",
    "along",
    "apply",
    "sigmoid",
    "activation",
    "function",
    "let",
    "go",
    "ahead",
    "let",
    "first",
    "define",
    "cost",
    "function",
    "definitely",
    "require",
    "cost",
    "function",
    "everything",
    "obviously",
    "know",
    "cost",
    "function",
    "linear",
    "regression",
    "first",
    "best",
    "fit",
    "line",
    "probably",
    "creating",
    "help",
    "linear",
    "regression",
    "particular",
    "case",
    "case",
    "linear",
    "regression",
    "basically",
    "write",
    "j",
    "j",
    "theta",
    "1",
    "nothing",
    "1",
    "summation",
    "1",
    "2",
    "1x",
    "2",
    "h",
    "theta",
    "x",
    "minus",
    "whole",
    "square",
    "entire",
    "thing",
    "remember",
    "linear",
    "regression",
    "whatever",
    "things",
    "discussed",
    "yesterday",
    "okay",
    "cost",
    "function",
    "let",
    "consider",
    "linear",
    "regression",
    "linear",
    "regression",
    "logistic",
    "regression",
    "happen",
    "logistic",
    "regression",
    "take",
    "cost",
    "function",
    "h",
    "theta",
    "x",
    "know",
    "theta",
    "x",
    "nothing",
    "1",
    "1",
    "e",
    "minus",
    "theta",
    "0",
    "theta",
    "sorry",
    "theta",
    "1",
    "multiplied",
    "x",
    "right",
    "respect",
    "logistic",
    "regression",
    "entire",
    "equation",
    "similarly",
    "try",
    "put",
    "h",
    "theta",
    "x",
    "let",
    "consider",
    "cost",
    "function",
    "h",
    "theta",
    "x",
    "changing",
    "particular",
    "case",
    "go",
    "ahead",
    "write",
    "cost",
    "function",
    "basically",
    "say",
    "1x2",
    "h",
    "theta",
    "x",
    "i²",
    "particular",
    "scenario",
    "h",
    "theta",
    "x",
    "nothing",
    "1",
    "1",
    "e",
    "minus",
    "theta",
    "1",
    "x",
    "getting",
    "replaced",
    "logistic",
    "regression",
    "cost",
    "function",
    "considering",
    "cost",
    "function",
    "part",
    "part",
    "later",
    "replace",
    "see",
    "replace",
    "replace",
    "becomes",
    "logistic",
    "regression",
    "cost",
    "function",
    "intercept",
    "considering",
    "zero",
    "guys",
    "replacing",
    "becomes",
    "logistic",
    "uh",
    "regression",
    "cost",
    "function",
    "one",
    "problem",
    "use",
    "use",
    "cost",
    "function",
    "reason",
    "equation",
    "seeing",
    "1",
    "minus",
    "theta",
    "1",
    "x",
    "function",
    "may",
    "considering",
    "function",
    "let",
    "write",
    "term",
    "terminology",
    "right",
    "function",
    "function",
    "let",
    "show",
    "let",
    "differentiate",
    "convex",
    "function",
    "okay",
    "try",
    "understand",
    "difference",
    "function",
    "convex",
    "function",
    "related",
    "gradient",
    "descent",
    "important",
    "related",
    "gradient",
    "desent",
    "remember",
    "help",
    "linear",
    "regression",
    "whatever",
    "gradient",
    "dent",
    "actually",
    "getting",
    "convex",
    "function",
    "like",
    "convex",
    "function",
    "looks",
    "like",
    "parabola",
    "curve",
    "parabola",
    "curve",
    "parabola",
    "curve",
    "whenever",
    "use",
    "linear",
    "regression",
    "cost",
    "function",
    "specifically",
    "h",
    "theta",
    "x",
    "nothing",
    "theta",
    "0",
    "theta",
    "1",
    "x",
    "equ",
    "always",
    "give",
    "parabola",
    "curve",
    "kind",
    "cost",
    "function",
    "convex",
    "function",
    "say",
    "theta",
    "x",
    "changing",
    "case",
    "use",
    "cost",
    "function",
    "getting",
    "curves",
    "looks",
    "like",
    "problem",
    "curve",
    "lot",
    "local",
    "minima",
    "local",
    "minima",
    "never",
    "reach",
    "global",
    "minima",
    "reason",
    "use",
    "c",
    "function",
    "mathematically",
    "also",
    "go",
    "probably",
    "search",
    "google",
    "graph",
    "convex",
    "function",
    "always",
    "remember",
    "whenever",
    "updates",
    "theta",
    "1",
    "within",
    "particular",
    "equation",
    "finding",
    "slope",
    "way",
    "differentiable",
    "lot",
    "local",
    "minima",
    "local",
    "minima",
    "never",
    "able",
    "reach",
    "global",
    "minima",
    "global",
    "minima",
    "right",
    "case",
    "case",
    "linear",
    "regression",
    "reach",
    "global",
    "minima",
    "case",
    "never",
    "reach",
    "never",
    "never",
    "stuck",
    "may",
    "get",
    "stuck",
    "may",
    "get",
    "stuck",
    "okay",
    "local",
    "minima",
    "problem",
    "solve",
    "understand",
    "local",
    "minima",
    "points",
    "right",
    "come",
    "deepest",
    "point",
    "particular",
    "case",
    "local",
    "minima",
    "local",
    "minima",
    "also",
    "get",
    "slope",
    "equal",
    "z",
    "reason",
    "theta",
    "1",
    "never",
    "get",
    "updated",
    "order",
    "solve",
    "problem",
    "see",
    "diagram",
    "something",
    "called",
    "logistic",
    "regression",
    "cost",
    "function",
    "write",
    "logistic",
    "regression",
    "cost",
    "function",
    "different",
    "way",
    "researcher",
    "researcher",
    "thought",
    "basically",
    "came",
    "proposal",
    "logistic",
    "cost",
    "function",
    "look",
    "something",
    "like",
    "entire",
    "cost",
    "function",
    "logistic",
    "regression",
    "specifically",
    "h",
    "theta",
    "x",
    "comma",
    "written",
    "something",
    "like",
    "written",
    "like",
    "see",
    "going",
    "write",
    "cost",
    "function",
    "j",
    "theta",
    "1",
    "let",
    "say",
    "writing",
    "j",
    "theta",
    "1",
    "okay",
    "j",
    "theta",
    "1",
    "different",
    "different",
    "output",
    "getting",
    "get",
    "getting",
    "yal",
    "1",
    "equal",
    "0",
    "based",
    "two",
    "scenarios",
    "cost",
    "function",
    "look",
    "something",
    "like",
    "minus",
    "log",
    "h",
    "theta",
    "x",
    "know",
    "hope",
    "know",
    "h",
    "theta",
    "x",
    "h",
    "theta",
    "x",
    "nothing",
    "1",
    "1",
    "theta",
    "1",
    "x",
    "h",
    "theta",
    "x",
    "whenever",
    "zer",
    "basically",
    "minus",
    "log",
    "1",
    "h",
    "theta",
    "x",
    "okay",
    "basically",
    "write",
    "cost",
    "function",
    "particular",
    "scenario",
    "help",
    "cost",
    "function",
    "always",
    "possible",
    "since",
    "getting",
    "log",
    "log",
    "basically",
    "getting",
    "used",
    "scenario",
    "always",
    "get",
    "global",
    "minima",
    "reason",
    "completely",
    "neglected",
    "cost",
    "function",
    "utiliz",
    "cost",
    "function",
    "cost",
    "function",
    "basically",
    "mean",
    "two",
    "scenarios",
    "equal",
    "1",
    "let",
    "consider",
    "cost",
    "function",
    "graph",
    "h",
    "theta",
    "x",
    "know",
    "h",
    "theta",
    "x",
    "value",
    "ranging",
    "0",
    "1",
    "since",
    "classification",
    "problem",
    "ranging",
    "0",
    "1",
    "basically",
    "j",
    "theta",
    "1",
    "cost",
    "function",
    "equal",
    "1",
    "specific",
    "equation",
    "used",
    "whenever",
    "equation",
    "basically",
    "used",
    "get",
    "get",
    "curve",
    "see",
    "minus",
    "log",
    "x",
    "get",
    "curve",
    "looks",
    "something",
    "like",
    "okay",
    "get",
    "curve",
    "looks",
    "like",
    "curve",
    "basically",
    "specify",
    "curve",
    "come",
    "two",
    "assumptions",
    "cost",
    "zero",
    "1",
    "h",
    "theta",
    "x",
    "equal",
    "1",
    "basically",
    "theta",
    "x",
    "1",
    "output",
    "one",
    "basically",
    "means",
    "going",
    "assign",
    "one",
    "right",
    "particular",
    "case",
    "seeing",
    "cost",
    "function",
    "zero",
    "cost",
    "zero",
    "zero",
    "meeting",
    "x",
    "equal",
    "1",
    "equal",
    "1",
    "convex",
    "function",
    "next",
    "point",
    "probably",
    "discuss",
    "respect",
    "equal",
    "0",
    "z",
    "kind",
    "curve",
    "getting",
    "get",
    "different",
    "kind",
    "curve",
    "look",
    "like",
    "h",
    "theta",
    "x",
    "value",
    "0",
    "one",
    "curve",
    "looks",
    "like",
    "combine",
    "two",
    "able",
    "see",
    "able",
    "get",
    "kind",
    "gradient",
    "descent",
    "definitely",
    "help",
    "us",
    "create",
    "cost",
    "function",
    "hope",
    "everybody",
    "able",
    "understand",
    "till",
    "respect",
    "definitely",
    "work",
    "finally",
    "also",
    "write",
    "cost",
    "function",
    "different",
    "way",
    "cost",
    "function",
    "probably",
    "write",
    "j",
    "theta",
    "1",
    "come",
    "cost",
    "function",
    "looks",
    "like",
    "cost",
    "h",
    "theta",
    "x",
    "comma",
    "yus",
    "log",
    "h",
    "theta",
    "x",
    "equal",
    "1",
    "minus",
    "log",
    "1",
    "h",
    "theta",
    "x",
    "equal",
    "0",
    "combine",
    "probably",
    "write",
    "something",
    "like",
    "like",
    "combine",
    "basically",
    "write",
    "cost",
    "h",
    "theta",
    "x",
    "ia",
    "equal",
    "log",
    "h",
    "theta",
    "x",
    "minus",
    "log",
    "1",
    "okay",
    "1",
    "log",
    "1",
    "h",
    "theta",
    "x",
    "final",
    "cost",
    "function",
    "also",
    "see",
    "replace",
    "replace",
    "one",
    "remain",
    "particular",
    "value",
    "remain",
    "right",
    "value",
    "equal",
    "1",
    "thing",
    "come",
    "see",
    "replace",
    "one",
    "probably",
    "replace",
    "one",
    "able",
    "see",
    "write",
    "equal",
    "1",
    "cost",
    "function",
    "rook",
    "something",
    "like",
    "nothing",
    "see",
    "1",
    "happen",
    "log",
    "h",
    "theta",
    "x",
    "come",
    "1",
    "1",
    "0",
    "0",
    "multili",
    "anything",
    "0",
    "equal",
    "0",
    "happen",
    "cost",
    "function",
    "zero",
    "become",
    "0",
    "0",
    "multili",
    "anything",
    "z",
    "able",
    "see",
    "minus",
    "log",
    "1",
    "h",
    "theta",
    "x",
    "condition",
    "proved",
    "cost",
    "function",
    "cost",
    "function",
    "yes",
    "cost",
    "function",
    "loss",
    "function",
    "respect",
    "number",
    "parameters",
    "almost",
    "finally",
    "try",
    "write",
    "j",
    "theta",
    "1x",
    "2",
    "also",
    "right",
    "1x",
    "2",
    "also",
    "actually",
    "going",
    "able",
    "see",
    "write",
    "j",
    "theta",
    "1",
    "equal",
    "1",
    "2",
    "summation",
    "ial",
    "1",
    "write",
    "entire",
    "equation",
    "probably",
    "minus",
    "remove",
    "minus",
    "put",
    "become",
    "plus",
    "sorry",
    "log",
    "h",
    "theta",
    "x",
    "1",
    "log",
    "1",
    "h",
    "theta",
    "x",
    "becomes",
    "entire",
    "first",
    "function",
    "obviously",
    "know",
    "h",
    "thet",
    "x",
    "h",
    "theta",
    "x",
    "nothing",
    "1",
    "1",
    "minus",
    "theta",
    "1",
    "x",
    "finally",
    "convergence",
    "algorithm",
    "repeat",
    "update",
    "theta",
    "1",
    "repeat",
    "updation",
    "theta",
    "theta",
    "j",
    "equal",
    "theta",
    "j",
    "minus",
    "learning",
    "rate",
    "derivative",
    "respect",
    "theta",
    "j",
    "j",
    "theta",
    "1",
    "repeat",
    "conversion",
    "cost",
    "function",
    "repeat",
    "algorithm",
    "updating",
    "entire",
    "theta",
    "1",
    "solves",
    "problem",
    "respect",
    "logistic",
    "regression",
    "simple",
    "simple",
    "questions",
    "may",
    "come",
    "like",
    "different",
    "linear",
    "regression",
    "different",
    "linear",
    "regression",
    "say",
    "log",
    "likelihood",
    "topic",
    "probabilistic",
    "yes",
    "uh",
    "log",
    "likelihood",
    "discuss",
    "performance",
    "metrics",
    "specific",
    "classification",
    "problem",
    "binary",
    "classification",
    "talking",
    "let",
    "consider",
    "let",
    "consider",
    "data",
    "set",
    "x1",
    "x2",
    "obviously",
    "logistic",
    "uh",
    "classification",
    "outputs",
    "like",
    "0",
    "1",
    "0",
    "1",
    "1",
    "0",
    "1",
    "hat",
    "hat",
    "basically",
    "output",
    "predicted",
    "model",
    "particular",
    "scenario",
    "hat",
    "probably",
    "1",
    "1",
    "0",
    "uh",
    "1",
    "1",
    "1",
    "z",
    "particular",
    "scenario",
    "predicted",
    "output",
    "actual",
    "output",
    "come",
    "kind",
    "conclusions",
    "wherein",
    "probably",
    "able",
    "identify",
    "may",
    "accuracy",
    "specific",
    "model",
    "respect",
    "many",
    "data",
    "points",
    "confusion",
    "matrix",
    "dealt",
    "called",
    "first",
    "create",
    "confusion",
    "matrix",
    "binary",
    "classification",
    "problem",
    "confusion",
    "matrix",
    "look",
    "like",
    "1",
    "0",
    "1",
    "0",
    "let",
    "say",
    "prediction",
    "let",
    "say",
    "actual",
    "value",
    "prediction",
    "value",
    "okay",
    "prediction",
    "value",
    "output",
    "value",
    "actual",
    "value",
    "zero",
    "predicted",
    "value",
    "one",
    "mean",
    "wrong",
    "prediction",
    "right",
    "actual",
    "value",
    "zero",
    "predicted",
    "value",
    "1",
    "count",
    "increase",
    "one",
    "let",
    "go",
    "second",
    "scenario",
    "actual",
    "value",
    "one",
    "predicted",
    "value",
    "one",
    "basically",
    "means",
    "one",
    "one",
    "going",
    "increase",
    "count",
    "similarly",
    "actual",
    "value",
    "zero",
    "predicted",
    "value",
    "zero",
    "basically",
    "mean",
    "actual",
    "value",
    "z",
    "predicted",
    "value",
    "zero",
    "going",
    "increase",
    "count",
    "one",
    "go",
    "1",
    "one",
    "instead",
    "writing",
    "one",
    "become",
    "two",
    "going",
    "increase",
    "count",
    "similarly",
    "go",
    "one",
    "one",
    "going",
    "increase",
    "count",
    "three",
    "01",
    "01",
    "basically",
    "means",
    "actual",
    "value",
    "zero",
    "actually",
    "getting",
    "one",
    "also",
    "going",
    "increase",
    "particular",
    "value",
    "two",
    "finally",
    "1",
    "zero",
    "going",
    "increase",
    "like",
    "basically",
    "mean",
    "basically",
    "mean",
    "see",
    "respect",
    "kind",
    "predictions",
    "whenever",
    "discussing",
    "basically",
    "basically",
    "says",
    "actual",
    "values",
    "z",
    "1",
    "zero",
    "predicted",
    "values",
    "also",
    "1",
    "zero",
    "value",
    "one",
    "one",
    "called",
    "true",
    "positive",
    "value",
    "0",
    "zer",
    "called",
    "false",
    "negative",
    "whenever",
    "actual",
    "value",
    "zero",
    "predicted",
    "one",
    "becomes",
    "false",
    "positive",
    "whenever",
    "actual",
    "value",
    "one",
    "predicted",
    "zero",
    "becomes",
    "false",
    "negative",
    "coming",
    "really",
    "need",
    "find",
    "accuracy",
    "model",
    "really",
    "want",
    "find",
    "called",
    "confusion",
    "matrix",
    "confusion",
    "matrix",
    "really",
    "want",
    "find",
    "accuracy",
    "accuracy",
    "model",
    "much",
    "simple",
    "middle",
    "elements",
    "able",
    "see",
    "basically",
    "give",
    "us",
    "right",
    "output",
    "add",
    "give",
    "us",
    "right",
    "output",
    "going",
    "get",
    "tp",
    "tn",
    "divided",
    "tp",
    "fp",
    "fn",
    "tn",
    "calculate",
    "3",
    "1",
    "3",
    "2",
    "1",
    "1",
    "nothing",
    "4",
    "7",
    "4",
    "757",
    "getting",
    "57",
    "percentage",
    "accuracy",
    "actually",
    "getting",
    "57",
    "accuracy",
    "respect",
    "accuracy",
    "basically",
    "calculate",
    "respect",
    "basic",
    "accuracy",
    "help",
    "uh",
    "confusion",
    "matrix",
    "okay",
    "specifically",
    "called",
    "confusion",
    "matrix",
    "things",
    "really",
    "need",
    "specify",
    "always",
    "remember",
    "model",
    "aim",
    "try",
    "reduce",
    "false",
    "positive",
    "false",
    "negative",
    "let",
    "say",
    "want",
    "discuss",
    "two",
    "topics",
    "one",
    "suppose",
    "data",
    "set",
    "zeros",
    "one",
    "category",
    "let",
    "say",
    "output",
    "say",
    "zer",
    "900",
    "ones",
    "100",
    "becomes",
    "imbalanced",
    "data",
    "clear",
    "right",
    "become",
    "imbalanced",
    "data",
    "set",
    "biased",
    "data",
    "suppose",
    "say",
    "zeros",
    "probab",
    "600",
    "ones",
    "probably",
    "400",
    "particular",
    "scenario",
    "say",
    "balance",
    "data",
    "yes",
    "100",
    "less",
    "okay",
    "may",
    "impact",
    "many",
    "algorithm",
    "see",
    "guys",
    "algorithm",
    "probably",
    "discussing",
    "imbalanced",
    "imbalanced",
    "data",
    "set",
    "obviously",
    "affect",
    "algorithms",
    "let",
    "talk",
    "let",
    "say",
    "number",
    "zeros",
    "900",
    "number",
    "ones",
    "100",
    "let",
    "say",
    "model",
    "created",
    "directly",
    "predict",
    "zero",
    "say",
    "inputs",
    "probably",
    "getting",
    "respect",
    "training",
    "data",
    "output",
    "zero",
    "particular",
    "scenario",
    "accuracy",
    "accuracy",
    "900",
    "divid",
    "right",
    "nothing",
    "90",
    "good",
    "accuracy",
    "obviously",
    "good",
    "accuracy",
    "biased",
    "data",
    "model",
    "basically",
    "outputting",
    "00000000",
    "0",
    "outputting",
    "00",
    "00",
    "0",
    "obviously",
    "answer",
    "zeros",
    "scenario",
    "like",
    "know",
    "outputting",
    "one",
    "thing",
    "also",
    "able",
    "get",
    "90",
    "accuracy",
    "dependent",
    "accuracy",
    "lot",
    "terminologies",
    "basically",
    "use",
    "one",
    "terminology",
    "specifically",
    "use",
    "something",
    "called",
    "precision",
    "also",
    "use",
    "recall",
    "precision",
    "recall",
    "write",
    "formula",
    "precision",
    "need",
    "focus",
    "finally",
    "discuss",
    "f",
    "score",
    "use",
    "different",
    "kind",
    "parametrics",
    "sorry",
    "different",
    "kind",
    "formulas",
    "whenever",
    "imbalanced",
    "data",
    "set",
    "also",
    "oversampling",
    "understand",
    "scenarios",
    "scenarios",
    "oversampling",
    "may",
    "work",
    "focus",
    "type",
    "performance",
    "metrics",
    "focusing",
    "right",
    "say",
    "f1",
    "score",
    "say",
    "f",
    "score",
    "reason",
    "saying",
    "let",
    "know",
    "let",
    "talk",
    "recall",
    "recall",
    "formula",
    "basically",
    "given",
    "true",
    "positive",
    "divided",
    "true",
    "positive",
    "plus",
    "false",
    "negative",
    "precision",
    "given",
    "true",
    "positive",
    "divided",
    "true",
    "positive",
    "plus",
    "false",
    "positive",
    "probably",
    "discuss",
    "f",
    "sore",
    "also",
    "basically",
    "say",
    "fbaa",
    "also",
    "draw",
    "confusion",
    "matrix",
    "okay",
    "true",
    "positive",
    "true",
    "negative",
    "let",
    "draw",
    "ones",
    "zeros",
    "actual",
    "values",
    "predicted",
    "values",
    "true",
    "positive",
    "true",
    "negative",
    "false",
    "positive",
    "false",
    "negative",
    "particular",
    "scenario",
    "actually",
    "discussing",
    "understand",
    "recall",
    "focus",
    "basically",
    "given",
    "whenever",
    "talk",
    "recall",
    "recall",
    "basically",
    "says",
    "tp",
    "tp",
    "divided",
    "tp",
    "plus",
    "fn",
    "actually",
    "focusing",
    "basically",
    "say",
    "true",
    "uh",
    "recall",
    "actual",
    "true",
    "positives",
    "many",
    "predicted",
    "correctly",
    "basically",
    "mentioned",
    "tp",
    "positive",
    "values",
    "many",
    "predicted",
    "positive",
    "basically",
    "saying",
    "scenario",
    "called",
    "recall",
    "false",
    "negative",
    "basically",
    "given",
    "priority",
    "focus",
    "try",
    "reduce",
    "false",
    "positive",
    "false",
    "negative",
    "sorry",
    "try",
    "reduce",
    "let",
    "go",
    "ahead",
    "let",
    "discuss",
    "precision",
    "precision",
    "basically",
    "taking",
    "predicted",
    "values",
    "predicted",
    "positive",
    "values",
    "many",
    "actual",
    "true",
    "positive",
    "okay",
    "precision",
    "basically",
    "means",
    "suppose",
    "consider",
    "spam",
    "classification",
    "suppose",
    "task",
    "tell",
    "particular",
    "case",
    "use",
    "precision",
    "recall",
    "one",
    "use",
    "case",
    "saying",
    "whether",
    "person",
    "cancer",
    "case",
    "support",
    "recall",
    "case",
    "go",
    "ahead",
    "precision",
    "cancer",
    "spam",
    "important",
    "okay",
    "guys",
    "recall",
    "also",
    "called",
    "true",
    "positive",
    "rate",
    "also",
    "say",
    "recall",
    "sensitivity",
    "go",
    "spam",
    "classification",
    "definitely",
    "go",
    "precision",
    "go",
    "precision",
    "probably",
    "get",
    "spam",
    "main",
    "aim",
    "whenever",
    "get",
    "spam",
    "mill",
    "identified",
    "spam",
    "okay",
    "specific",
    "scenario",
    "positive",
    "false",
    "positive",
    "try",
    "reduce",
    "scenario",
    "false",
    "pository",
    "talks",
    "spam",
    "classification",
    "lot",
    "better",
    "way",
    "case",
    "cancer",
    "definitely",
    "use",
    "recall",
    "let",
    "let",
    "focus",
    "recall",
    "formula",
    "tp",
    "plus",
    "fn",
    "person",
    "cancer",
    "see",
    "one",
    "actually",
    "cancer",
    "predicted",
    "one",
    "otherwise",
    "fn",
    "basically",
    "predicting",
    "cancer",
    "really",
    "big",
    "situation",
    "case",
    "person",
    "cancer",
    "predict",
    "model",
    "predicts",
    "okay",
    "fine",
    "cancer",
    "may",
    "go",
    "test",
    "come",
    "know",
    "whether",
    "cancer",
    "scenario",
    "dangerous",
    "person",
    "cancer",
    "indicated",
    "cancer",
    "false",
    "negative",
    "given",
    "priority",
    "case",
    "spam",
    "classification",
    "false",
    "positive",
    "given",
    "priority",
    "something",
    "important",
    "really",
    "need",
    "understand",
    "respect",
    "different",
    "different",
    "problem",
    "statement",
    "let",
    "give",
    "one",
    "example",
    "tomorrow",
    "stock",
    "market",
    "going",
    "crash",
    "need",
    "focus",
    "focus",
    "precision",
    "focus",
    "recall",
    "two",
    "things",
    "solving",
    "kind",
    "problem",
    "see",
    "many",
    "people",
    "say",
    "recall",
    "precision",
    "two",
    "things",
    "whose",
    "point",
    "view",
    "creating",
    "model",
    "creating",
    "model",
    "industry",
    "creating",
    "model",
    "people",
    "people",
    "definitely",
    "get",
    "identified",
    "okay",
    "particular",
    "scenario",
    "need",
    "sell",
    "stock",
    "tomorrow",
    "stock",
    "market",
    "going",
    "crash",
    "companies",
    "bad",
    "okay",
    "hope",
    "everybody",
    "able",
    "understand",
    "companies",
    "bad",
    "particular",
    "case",
    "sometime",
    "need",
    "focus",
    "false",
    "positive",
    "false",
    "negative",
    "telling",
    "problem",
    "statement",
    "solving",
    "indicates",
    "solving",
    "people",
    "able",
    "get",
    "notification",
    "saying",
    "going",
    "crash",
    "probably",
    "uh",
    "companies",
    "time",
    "precision",
    "recall",
    "may",
    "change",
    "consider",
    "scenarios",
    "point",
    "time",
    "definitely",
    "use",
    "something",
    "called",
    "f",
    "score",
    "f",
    "score",
    "also",
    "say",
    "f",
    "beta",
    "fbaa",
    "formula",
    "given",
    "talk",
    "f",
    "score",
    "three",
    "different",
    "formulas",
    "first",
    "formula",
    "say",
    "basically",
    "beta",
    "value",
    "1",
    "okay",
    "first",
    "give",
    "generic",
    "definition",
    "f",
    "f",
    "beta",
    "basically",
    "going",
    "consider",
    "1",
    "beta",
    "squ",
    "precision",
    "multiplied",
    "recall",
    "divided",
    "beta",
    "square",
    "precision",
    "plus",
    "recall",
    "whenever",
    "false",
    "positive",
    "false",
    "negative",
    "important",
    "select",
    "beta",
    "one",
    "select",
    "beta",
    "1",
    "becomes",
    "1",
    "4",
    "precision",
    "multiplied",
    "recall",
    "precision",
    "plus",
    "recall",
    "sorry",
    "1",
    "1",
    "becomes",
    "2",
    "multiplied",
    "precision",
    "recall",
    "divided",
    "precision",
    "plus",
    "recall",
    "basically",
    "called",
    "harmonic",
    "mean",
    "harmonic",
    "mean",
    "probably",
    "seen",
    "kind",
    "equation",
    "written",
    "2x",
    "x",
    "type",
    "able",
    "see",
    "called",
    "harmonic",
    "mean",
    "focus",
    "false",
    "positive",
    "false",
    "negative",
    "let",
    "say",
    "false",
    "positive",
    "important",
    "false",
    "negative",
    "point",
    "time",
    "try",
    "decrease",
    "try",
    "decrease",
    "beta",
    "value",
    "let",
    "say",
    "decreasing",
    "beta",
    "value",
    "happen",
    "1",
    "whole",
    "p",
    "r",
    "precision",
    "recall",
    "also",
    "25",
    "p",
    "r",
    "particular",
    "scenario",
    "decreasing",
    "beta",
    "decreasing",
    "beta",
    "basically",
    "means",
    "providing",
    "importance",
    "false",
    "positive",
    "false",
    "negative",
    "finally",
    "able",
    "see",
    "consider",
    "beta",
    "value",
    "let",
    "say",
    "notes",
    "consider",
    "beta",
    "value",
    "two",
    "basically",
    "means",
    "giving",
    "importance",
    "false",
    "negative",
    "false",
    "positive",
    "specific",
    "case",
    "come",
    "conclusion",
    "value",
    "basically",
    "want",
    "use",
    "whenever",
    "use",
    "beta",
    "equal",
    "1",
    "becomes",
    "fub1",
    "score",
    "use",
    "beta",
    "basically",
    "becomes",
    "score",
    "becomes",
    "f2",
    "score",
    "based",
    "important",
    "okay",
    "important",
    "whether",
    "precision",
    "false",
    "positive",
    "false",
    "negative",
    "important",
    "consider",
    "things",
    "f",
    "score",
    "different",
    "values",
    "using",
    "beta",
    "equal",
    "1",
    "basically",
    "means",
    "giving",
    "importance",
    "precision",
    "recall",
    "false",
    "positive",
    "important",
    "point",
    "time",
    "reduce",
    "beta",
    "value",
    "false",
    "negative",
    "greater",
    "false",
    "bet",
    "uh",
    "false",
    "positive",
    "beta",
    "value",
    "increasing",
    "beta",
    "deciding",
    "parameter",
    "decide",
    "f1",
    "score",
    "f2",
    "score",
    "f",
    "point",
    "score",
    "first",
    "thing",
    "first",
    "agenda",
    "today",
    "session",
    "first",
    "complete",
    "practicals",
    "algorithms",
    "discussed",
    "algorithms",
    "discussed",
    "cover",
    "practicals",
    "probably",
    "hyper",
    "parameter",
    "tuning",
    "everything",
    "second",
    "thing",
    "going",
    "take",
    "simple",
    "examples",
    "yes",
    "uh",
    "today",
    "session",
    "said",
    "practicals",
    "simple",
    "examples",
    "probably",
    "discuss",
    "hyper",
    "parameter",
    "tuning",
    "second",
    "one",
    "second",
    "algorithm",
    "going",
    "discuss",
    "something",
    "called",
    "n",
    "bias",
    "classification",
    "algorithm",
    "going",
    "understand",
    "intuition",
    "third",
    "one",
    "going",
    "probably",
    "discusses",
    "knn",
    "algorithm",
    "knn",
    "algorithms",
    "definitely",
    "today",
    "plan",
    "know",
    "written",
    "less",
    "much",
    "maths",
    "involved",
    "na",
    "bias",
    "right",
    "understand",
    "probability",
    "theorem",
    "something",
    "called",
    "bias",
    "theorem",
    "try",
    "understand",
    "try",
    "solve",
    "problem",
    "let",
    "proceed",
    "let",
    "enjoy",
    "today",
    "session",
    "enjoy",
    "first",
    "enjoy",
    "creating",
    "practical",
    "problem",
    "actually",
    "opening",
    "notebook",
    "file",
    "front",
    "uh",
    "try",
    "sol",
    "solve",
    "help",
    "linear",
    "regression",
    "ridge",
    "lasso",
    "try",
    "solve",
    "problems",
    "let",
    "see",
    "much",
    "able",
    "solve",
    "aim",
    "learn",
    "better",
    "way",
    "okay",
    "uh",
    "everybody",
    "understands",
    "basic",
    "basic",
    "things",
    "okay",
    "first",
    "usual",
    "uh",
    "everybody",
    "open",
    "jupyter",
    "notebook",
    "file",
    "first",
    "algorithm",
    "going",
    "discuss",
    "something",
    "called",
    "sk",
    "learn",
    "linear",
    "regression",
    "everybody",
    "hope",
    "everybody",
    "knows",
    "sk",
    "learn",
    "let",
    "see",
    "things",
    "basically",
    "using",
    "fit",
    "intercept",
    "everything",
    "main",
    "aim",
    "find",
    "coefficients",
    "basically",
    "indicated",
    "theta",
    "0",
    "theta",
    "1",
    "first",
    "thing",
    "start",
    "linear",
    "regression",
    "go",
    "ahead",
    "discuss",
    "r",
    "lassor",
    "going",
    "make",
    "markdown",
    "many",
    "different",
    "libraries",
    "linear",
    "regression",
    "stats",
    "skyi",
    "many",
    "things",
    "okay",
    "first",
    "thing",
    "first",
    "let",
    "first",
    "require",
    "data",
    "set",
    "data",
    "set",
    "going",
    "going",
    "basically",
    "take",
    "smaller",
    "smaller",
    "data",
    "let",
    "uh",
    "going",
    "take",
    "house",
    "pricing",
    "data",
    "set",
    "going",
    "solve",
    "house",
    "pricing",
    "data",
    "set",
    "problem",
    "simple",
    "data",
    "set",
    "already",
    "present",
    "sk",
    "learn",
    "order",
    "import",
    "data",
    "set",
    "write",
    "line",
    "code",
    "like",
    "sk",
    "learn",
    "dot",
    "data",
    "sets",
    "data",
    "sets",
    "import",
    "load",
    "uncore",
    "boston",
    "boston",
    "house",
    "pricing",
    "data",
    "set",
    "going",
    "execute",
    "also",
    "going",
    "make",
    "lot",
    "sals",
    "go",
    "ahead",
    "create",
    "sales",
    "basic",
    "libraries",
    "probably",
    "want",
    "pro",
    "import",
    "numai",
    "np",
    "import",
    "pandas",
    "spd",
    "okay",
    "import",
    "cbon",
    "sns",
    "also",
    "import",
    "matt",
    "matt",
    "plot",
    "lib",
    "p",
    "plot",
    "plt",
    "percentile",
    "matplot",
    "lib",
    "matlot",
    "lib",
    "inline",
    "try",
    "execute",
    "see",
    "typing",
    "speed",
    "become",
    "little",
    "bit",
    "faster",
    "writing",
    "executing",
    "queries",
    "uh",
    "let",
    "go",
    "ahead",
    "uh",
    "imported",
    "necessary",
    "libraries",
    "required",
    "sufficient",
    "start",
    "order",
    "load",
    "particular",
    "data",
    "set",
    "use",
    "library",
    "called",
    "load",
    "uncore",
    "boston",
    "going",
    "initialize",
    "press",
    "shift",
    "tab",
    "able",
    "see",
    "return",
    "load",
    "return",
    "boston",
    "house",
    "prices",
    "data",
    "set",
    "regression",
    "problem",
    "saying",
    "probably",
    "going",
    "execute",
    "execute",
    "go",
    "probably",
    "see",
    "type",
    "df",
    "basically",
    "saying",
    "skarn",
    "dos",
    "bunch",
    "go",
    "probably",
    "execute",
    "df",
    "able",
    "see",
    "form",
    "key",
    "value",
    "pairs",
    "okay",
    "like",
    "target",
    "data",
    "okay",
    "data",
    "target",
    "probably",
    "able",
    "find",
    "feature",
    "names",
    "definitely",
    "require",
    "feature",
    "names",
    "require",
    "target",
    "value",
    "data",
    "value",
    "really",
    "need",
    "combine",
    "specific",
    "thing",
    "proper",
    "way",
    "form",
    "data",
    "frame",
    "able",
    "see",
    "actually",
    "going",
    "going",
    "say",
    "pd",
    "data",
    "frame",
    "convert",
    "entirely",
    "data",
    "frame",
    "say",
    "df",
    "data",
    "see",
    "key",
    "value",
    "pair",
    "right",
    "df",
    "data",
    "basically",
    "giving",
    "features",
    "value",
    "write",
    "df",
    "data",
    "execute",
    "able",
    "see",
    "able",
    "get",
    "entire",
    "data",
    "set",
    "way",
    "entire",
    "data",
    "set",
    "way",
    "feature",
    "one",
    "feature",
    "two",
    "feature",
    "three",
    "feature",
    "4",
    "feature",
    "12",
    "12",
    "features",
    "based",
    "specific",
    "value",
    "next",
    "thing",
    "thing",
    "going",
    "probably",
    "also",
    "able",
    "add",
    "target",
    "feature",
    "name",
    "convert",
    "df",
    "also",
    "say",
    "df",
    "columns",
    "set",
    "df",
    "target",
    "okay",
    "let",
    "change",
    "data",
    "set",
    "going",
    "change",
    "data",
    "set",
    "going",
    "say",
    "data",
    "set",
    "columns",
    "equal",
    "df",
    "target",
    "execute",
    "probably",
    "print",
    "data",
    "set",
    "head",
    "able",
    "see",
    "specific",
    "thing",
    "okay",
    "error",
    "let",
    "see",
    "expected",
    "axis",
    "13",
    "element",
    "new",
    "values",
    "506",
    "target",
    "okay",
    "use",
    "target",
    "instead",
    "column",
    "called",
    "features",
    "feature",
    "names",
    "like",
    "go",
    "probably",
    "see",
    "df",
    "df",
    "able",
    "see",
    "one",
    "thing",
    "called",
    "feature",
    "names",
    "going",
    "use",
    "df",
    "feature",
    "names",
    "df",
    "feature",
    "names",
    "going",
    "paste",
    "go",
    "write",
    "see",
    "print",
    "df",
    "data",
    "set",
    "head",
    "go",
    "execute",
    "without",
    "print",
    "able",
    "see",
    "entire",
    "data",
    "set",
    "features",
    "respect",
    "different",
    "different",
    "things",
    "basically",
    "house",
    "pricing",
    "data",
    "set",
    "initially",
    "features",
    "crm",
    "zn",
    "indust",
    "ch",
    "nox",
    "rm",
    "age",
    "distance",
    "radius",
    "tax",
    "pt",
    "ratio",
    "b",
    "l",
    "stack",
    "entire",
    "data",
    "set",
    "data",
    "set",
    "basically",
    "put",
    "also",
    "able",
    "see",
    "feature",
    "basically",
    "means",
    "showing",
    "wasted",
    "weighted",
    "distance",
    "five",
    "uh",
    "five",
    "boston",
    "employment",
    "center",
    "rad",
    "basically",
    "means",
    "index",
    "accessibility",
    "radial",
    "highway",
    "tax",
    "basically",
    "means",
    "full",
    "value",
    "property",
    "tax",
    "rate",
    "much",
    "pt",
    "rate",
    "basically",
    "means",
    "pupil",
    "teacher",
    "ratio",
    "know",
    "hell",
    "means",
    "fine",
    "kind",
    "data",
    "properly",
    "front",
    "independent",
    "features",
    "independent",
    "features",
    "want",
    "features",
    "detail",
    "see",
    "right",
    "everything",
    "crm",
    "basically",
    "means",
    "per",
    "capita",
    "crime",
    "rate",
    "town",
    "important",
    "zn",
    "proportional",
    "residential",
    "land",
    "zone",
    "lots",
    "square",
    "ft",
    "df",
    "much",
    "using",
    "data",
    "frame",
    "df",
    "data",
    "column",
    "features",
    "name",
    "getting",
    "value",
    "much",
    "simple",
    "let",
    "go",
    "little",
    "bit",
    "slowly",
    "many",
    "people",
    "able",
    "also",
    "understand",
    "data",
    "set",
    "head",
    "thing",
    "obviously",
    "taken",
    "particular",
    "values",
    "independent",
    "feature",
    "still",
    "dependent",
    "feature",
    "actually",
    "going",
    "create",
    "new",
    "feature",
    "like",
    "data",
    "set",
    "price",
    "create",
    "feature",
    "name",
    "price",
    "price",
    "house",
    "assign",
    "particular",
    "value",
    "value",
    "assigned",
    "target",
    "target",
    "value",
    "target",
    "value",
    "basically",
    "sale",
    "price",
    "houses",
    "right",
    "form",
    "array",
    "going",
    "take",
    "put",
    "dependent",
    "feature",
    "able",
    "see",
    "price",
    "dependent",
    "feature",
    "basically",
    "write",
    "df",
    "target",
    "execute",
    "probably",
    "go",
    "see",
    "data",
    "set",
    "head",
    "able",
    "see",
    "features",
    "one",
    "feature",
    "getting",
    "added",
    "price",
    "price",
    "may",
    "units",
    "may",
    "millions",
    "somewhere",
    "target",
    "probably",
    "millions",
    "see",
    "somewhere",
    "definitely",
    "said",
    "probably",
    "millions",
    "okay",
    "problem",
    "think",
    "mostly",
    "millions",
    "somewhere",
    "think",
    "okay",
    "see",
    "probably",
    "put",
    "time",
    "able",
    "understand",
    "okay",
    "thing",
    "main",
    "thing",
    "independent",
    "features",
    "dependent",
    "feature",
    "right",
    "trying",
    "solve",
    "linear",
    "regression",
    "divide",
    "independent",
    "dependent",
    "features",
    "properly",
    "let",
    "go",
    "next",
    "step",
    "dividing",
    "data",
    "set",
    "dividing",
    "oh",
    "god",
    "dividing",
    "data",
    "set",
    "train",
    "first",
    "try",
    "try",
    "divide",
    "independent",
    "dependent",
    "features",
    "want",
    "entire",
    "features",
    "data",
    "set",
    "divided",
    "independent",
    "dependent",
    "features",
    "x",
    "using",
    "independent",
    "featur",
    "write",
    "data",
    "set",
    "dot",
    "use",
    "iock",
    "present",
    "data",
    "frames",
    "understand",
    "feature",
    "feature",
    "taking",
    "independent",
    "feature",
    "feature",
    "till",
    "lat",
    "best",
    "way",
    "basically",
    "means",
    "need",
    "skip",
    "last",
    "feature",
    "order",
    "skip",
    "last",
    "feature",
    "actually",
    "going",
    "columns",
    "skip",
    "last",
    "column",
    "basically",
    "indexing",
    "respect",
    "skipping",
    "last",
    "feature",
    "basically",
    "independent",
    "features",
    "basically",
    "say",
    "equal",
    "data",
    "set",
    "iock",
    "want",
    "last",
    "feature",
    "write",
    "colon",
    "records",
    "want",
    "see",
    "first",
    "term",
    "probably",
    "wr",
    "writing",
    "basically",
    "specifies",
    "respect",
    "records",
    "specifies",
    "respect",
    "columns",
    "columns",
    "taking",
    "last",
    "column",
    "take",
    "last",
    "column",
    "basically",
    "dependent",
    "features",
    "dependent",
    "features",
    "basically",
    "executed",
    "go",
    "probably",
    "see",
    "head",
    "able",
    "find",
    "independent",
    "features",
    "head",
    "able",
    "find",
    "dependent",
    "feature",
    "let",
    "go",
    "first",
    "algorithm",
    "called",
    "linear",
    "regression",
    "always",
    "remember",
    "whenever",
    "definitely",
    "start",
    "linear",
    "regression",
    "definitely",
    "go",
    "directly",
    "linear",
    "regression",
    "instead",
    "try",
    "go",
    "ridge",
    "regression",
    "uh",
    "lasso",
    "regression",
    "lot",
    "options",
    "respect",
    "hyper",
    "pment",
    "show",
    "linear",
    "regression",
    "done",
    "basically",
    "really",
    "really",
    "need",
    "use",
    "lot",
    "libraries",
    "okay",
    "based",
    "libraries",
    "libraries",
    "try",
    "install",
    "okay",
    "libraries",
    "basically",
    "linear",
    "regression",
    "library",
    "basically",
    "going",
    "use",
    "two",
    "specific",
    "thing",
    "one",
    "linear",
    "regression",
    "library",
    "use",
    "sk",
    "learn",
    "linear",
    "uncore",
    "model",
    "import",
    "linear",
    "regression",
    "need",
    "remember",
    "answer",
    "also",
    "google",
    "try",
    "find",
    "escal",
    "present",
    "okay",
    "linear",
    "regression",
    "try",
    "initialize",
    "linear",
    "reg",
    "equal",
    "initialize",
    "linear",
    "regression",
    "actually",
    "going",
    "going",
    "basically",
    "apply",
    "something",
    "called",
    "cross",
    "validation",
    "cross",
    "validation",
    "much",
    "important",
    "cross",
    "validation",
    "divide",
    "train",
    "test",
    "data",
    "way",
    "every",
    "combination",
    "train",
    "test",
    "data",
    "basically",
    "taken",
    "care",
    "taken",
    "model",
    "whoever",
    "accuracy",
    "better",
    "entire",
    "thing",
    "basically",
    "combined",
    "going",
    "going",
    "say",
    "mean",
    "square",
    "error",
    "equal",
    "import",
    "one",
    "library",
    "let",
    "say",
    "sk",
    "learn",
    "dot",
    "model",
    "selection",
    "going",
    "import",
    "cross",
    "val",
    "score",
    "cross",
    "val",
    "score",
    "cross",
    "validation",
    "score",
    "basically",
    "means",
    "going",
    "lot",
    "train",
    "test",
    "split",
    "something",
    "like",
    "one",
    "example",
    "show",
    "cross",
    "validation",
    "basically",
    "okay",
    "cross",
    "validation",
    "happens",
    "suppose",
    "entire",
    "data",
    "set",
    "suppose",
    "100",
    "records",
    "five",
    "cross",
    "validation",
    "first",
    "test",
    "data",
    "remaining",
    "training",
    "data",
    "second",
    "cross",
    "validation",
    "test",
    "data",
    "remaining",
    "test",
    "uh",
    "training",
    "data",
    "like",
    "five",
    "times",
    "cross",
    "validation",
    "taking",
    "different",
    "combination",
    "train",
    "test",
    "going",
    "discuss",
    "much",
    "future",
    "want",
    "separate",
    "session",
    "include",
    "one",
    "session",
    "uh",
    "basically",
    "plan",
    "respect",
    "cross",
    "validation",
    "cross",
    "val",
    "score",
    "going",
    "basically",
    "take",
    "cross",
    "val",
    "score",
    "first",
    "parameter",
    "give",
    "model",
    "linear",
    "regression",
    "model",
    "take",
    "x",
    "train",
    "test",
    "split",
    "specifically",
    "giving",
    "entire",
    "x",
    "probably",
    "based",
    "going",
    "cross",
    "validation",
    "also",
    "train",
    "test",
    "plate",
    "initially",
    "give",
    "x",
    "train",
    "train",
    "cross",
    "validation",
    "best",
    "practices",
    "first",
    "train",
    "test",
    "split",
    "give",
    "train",
    "data",
    "cross",
    "validation",
    "going",
    "use",
    "scoring",
    "equal",
    "use",
    "mean",
    "squared",
    "error",
    "negative",
    "mean",
    "squared",
    "error",
    "let",
    "say",
    "going",
    "use",
    "negative",
    "mean",
    "squ",
    "error",
    "find",
    "things",
    "able",
    "see",
    "sk",
    "learn",
    "page",
    "l",
    "uh",
    "cross",
    "val",
    "score",
    "finally",
    "cross",
    "val",
    "score",
    "give",
    "cross",
    "validation",
    "value",
    "5",
    "10",
    "whatever",
    "want",
    "actually",
    "going",
    "going",
    "basically",
    "many",
    "scores",
    "get",
    "mean",
    "squar",
    "error",
    "five",
    "since",
    "five",
    "cross",
    "validation",
    "believe",
    "see",
    "print",
    "msse",
    "able",
    "see",
    "five",
    "different",
    "values",
    "1",
    "2",
    "3",
    "4",
    "5",
    "right",
    "five",
    "different",
    "mean",
    "values",
    "cross",
    "five",
    "five",
    "cross",
    "validation",
    "going",
    "write",
    "going",
    "say",
    "np",
    "mean",
    "want",
    "take",
    "average",
    "five",
    "basically",
    "meanor",
    "msse",
    "okay",
    "probably",
    "print",
    "print",
    "ms",
    "meanor",
    "msc",
    "average",
    "score",
    "respect",
    "negative",
    "value",
    "used",
    "negative",
    "mean",
    "squ",
    "error",
    "consider",
    "mean",
    "square",
    "error",
    "3",
    "okay",
    "actually",
    "shown",
    "cross",
    "validation",
    "see",
    "respect",
    "linear",
    "regression",
    "ca",
    "modify",
    "much",
    "parameter",
    "reason",
    "specifically",
    "order",
    "overcome",
    "overfitting",
    "feature",
    "selection",
    "use",
    "uh",
    "r",
    "lasso",
    "regression",
    "show",
    "show",
    "ridge",
    "ridge",
    "regression",
    "order",
    "prediction",
    "go",
    "take",
    "model",
    "okay",
    "model",
    "linear",
    "r",
    "say",
    "predict",
    "see",
    "uh",
    "getting",
    "function",
    "called",
    "predict",
    "give",
    "test",
    "value",
    "whatever",
    "want",
    "predict",
    "automatically",
    "prediction",
    "done",
    "going",
    "remove",
    "focus",
    "ridge",
    "regression",
    "right",
    "want",
    "show",
    "hyperparameter",
    "tuning",
    "done",
    "r",
    "regression",
    "r",
    "regression",
    "simple",
    "thing",
    "using",
    "two",
    "different",
    "libraries",
    "skarn",
    "linear",
    "linear",
    "uncore",
    "model",
    "going",
    "import",
    "ridge",
    "ridge",
    "also",
    "present",
    "linear",
    "underscore",
    "model",
    "hyperparameter",
    "tuning",
    "using",
    "sk",
    "learn",
    "modore",
    "selection",
    "going",
    "import",
    "grid",
    "se",
    "cv",
    "two",
    "libraries",
    "actually",
    "going",
    "use",
    "grid",
    "se",
    "cv",
    "able",
    "help",
    "um",
    "okay",
    "able",
    "help",
    "hyper",
    "parameter",
    "tuning",
    "probably",
    "able",
    "uh",
    "difference",
    "mse",
    "negative",
    "mse",
    "big",
    "thing",
    "guys",
    "use",
    "mse",
    "mean",
    "squ",
    "error",
    "getting",
    "37",
    "used",
    "negation",
    "mse",
    "okay",
    "anything",
    "fine",
    "go",
    "mse",
    "also",
    "means",
    "square",
    "error",
    "also",
    "another",
    "uh",
    "another",
    "scoring",
    "area",
    "like",
    "focuses",
    "square",
    "root",
    "square",
    "mean",
    "square",
    "uh",
    "sorry",
    "root",
    "means",
    "square",
    "eror",
    "okay",
    "different",
    "different",
    "things",
    "basically",
    "focus",
    "okay",
    "order",
    "give",
    "specific",
    "good",
    "value",
    "actually",
    "going",
    "hyper",
    "peter",
    "tuning",
    "let",
    "go",
    "ahead",
    "uh",
    "grid",
    "cv",
    "going",
    "going",
    "basically",
    "define",
    "model",
    "ridge",
    "okay",
    "actually",
    "imported",
    "uh",
    "let",
    "open",
    "ridge",
    "skarn",
    "sk",
    "learn",
    "ridge",
    "need",
    "need",
    "understand",
    "parameters",
    "basically",
    "used",
    "remember",
    "alpha",
    "value",
    "guys",
    "remember",
    "alpha",
    "value",
    "use",
    "alpha",
    "told",
    "alpha",
    "multiplied",
    "slope",
    "square",
    "remember",
    "ridge",
    "specifically",
    "use",
    "right",
    "ridge",
    "lasso",
    "regression",
    "alpha",
    "alpha",
    "probably",
    "best",
    "parameter",
    "perform",
    "hyper",
    "parameter",
    "tuning",
    "next",
    "parameter",
    "probably",
    "perform",
    "basically",
    "uh",
    "max",
    "iteration",
    "okay",
    "max",
    "iteration",
    "basically",
    "means",
    "many",
    "number",
    "iteration",
    "many",
    "number",
    "times",
    "may",
    "probably",
    "change",
    "theta",
    "1",
    "value",
    "get",
    "right",
    "value",
    "actually",
    "going",
    "going",
    "select",
    "alpha",
    "values",
    "going",
    "play",
    "apart",
    "want",
    "also",
    "play",
    "parameters",
    "uh",
    "like",
    "kind",
    "uh",
    "know",
    "probably",
    "also",
    "play",
    "iteration",
    "parameter",
    "try",
    "whichever",
    "parameter",
    "want",
    "change",
    "go",
    "ahead",
    "change",
    "let",
    "show",
    "write",
    "make",
    "sure",
    "specific",
    "thing",
    "done",
    "uh",
    "grid",
    "cv",
    "uh",
    "let",
    "one",
    "thing",
    "define",
    "parameters",
    "okay",
    "ridge",
    "going",
    "going",
    "say",
    "parameters",
    "parameter",
    "two",
    "important",
    "value",
    "probably",
    "going",
    "take",
    "one",
    "c",
    "value",
    "try",
    "define",
    "form",
    "dictionaries",
    "c",
    "value",
    "sorry",
    "c",
    "second",
    "guys",
    "mistake",
    "c",
    "alpha",
    "let",
    "see",
    "define",
    "alpha",
    "value",
    "try",
    "see",
    "parameters",
    "alpha",
    "c",
    "basically",
    "uh",
    "logistic",
    "regression",
    "show",
    "alpha",
    "value",
    "mention",
    "values",
    "like",
    "1",
    "e",
    "power",
    "basically",
    "00000000",
    "0",
    "0",
    "0",
    "1",
    "similarly",
    "write",
    "1",
    "e",
    "10",
    "means",
    "0",
    "0",
    "0",
    "0",
    "0",
    "0",
    "0",
    "0",
    "10",
    "0",
    "1",
    "making",
    "fun",
    "okay",
    "also",
    "get",
    "entertained",
    "1",
    "e",
    "minus",
    "8",
    "okay",
    "similarly",
    "write",
    "1",
    "e",
    "minus",
    "3",
    "particular",
    "value",
    "increasing",
    "value",
    "see",
    "1",
    "e",
    "minus",
    "2",
    "probably",
    "1",
    "5",
    "10",
    "um",
    "20",
    "something",
    "like",
    "going",
    "play",
    "particular",
    "parameters",
    "right",
    "grit",
    "cv",
    "take",
    "combination",
    "alpha",
    "value",
    "wherever",
    "uh",
    "model",
    "performs",
    "well",
    "going",
    "take",
    "specific",
    "parameter",
    "going",
    "give",
    "okay",
    "best",
    "fit",
    "parameter",
    "got",
    "selected",
    "got",
    "things",
    "going",
    "going",
    "basically",
    "apply",
    "grid",
    "c",
    "tv",
    "uh",
    "gridge",
    "uh",
    "sorry",
    "ridge",
    "gd",
    "saying",
    "ridore",
    "regressor",
    "going",
    "use",
    "git",
    "cv",
    "git",
    "cv",
    "basically",
    "going",
    "take",
    "parameters",
    "regge",
    "okay",
    "ridge",
    "first",
    "model",
    "take",
    "params",
    "actually",
    "defined",
    "see",
    "git",
    "cv",
    "press",
    "shift",
    "tab",
    "first",
    "execute",
    "able",
    "press",
    "shift",
    "tab",
    "press",
    "shift",
    "tab",
    "able",
    "see",
    "estimator",
    "parameter",
    "grid",
    "second",
    "parameter",
    "scoring",
    "parameters",
    "first",
    "thing",
    "goes",
    "model",
    "parameters",
    "actually",
    "playing",
    "third",
    "parameter",
    "basically",
    "scoring",
    "scoring",
    "going",
    "use",
    "negative",
    "mean",
    "squ",
    "error",
    "people",
    "saying",
    "mean",
    "squared",
    "error",
    "present",
    "reason",
    "negative",
    "mean",
    "squ",
    "error",
    "done",
    "may",
    "present",
    "uh",
    "try",
    "always",
    "create",
    "generic",
    "library",
    "probably",
    "kind",
    "uh",
    "scoring",
    "parameter",
    "may",
    "also",
    "get",
    "used",
    "algorithms",
    "reason",
    "may",
    "created",
    "want",
    "deep",
    "dive",
    "google",
    "google",
    "r",
    "regress",
    "dot",
    "fit",
    "x",
    "comma",
    "telling",
    "first",
    "train",
    "test",
    "split",
    "x",
    "probably",
    "x",
    "train",
    "train",
    "parameter",
    "oh",
    "sorry",
    "okay",
    "get",
    "okay",
    "parameter",
    "oh",
    "yeah",
    "become",
    "list",
    "going",
    "make",
    "dictionary",
    "right",
    "fully",
    "focused",
    "implementing",
    "things",
    "get",
    "error",
    "definitely",
    "make",
    "sure",
    "get",
    "fixed",
    "anyhow",
    "get",
    "error",
    "say",
    "oh",
    "kish",
    "error",
    "came",
    "know",
    "error",
    "came",
    "get",
    "worried",
    "get",
    "error",
    "give",
    "one",
    "okay",
    "try",
    "understand",
    "okay",
    "gitar",
    "cv",
    "also",
    "done",
    "fit",
    "let",
    "go",
    "select",
    "best",
    "parameter",
    "write",
    "print",
    "ridore",
    "regressor",
    "dot",
    "params",
    "sorry",
    "parameter",
    "called",
    "best",
    "params",
    "going",
    "print",
    "going",
    "print",
    "ridore",
    "regressor",
    "dot",
    "best",
    "score",
    "values",
    "got",
    "selected",
    "one",
    "alpha",
    "equal",
    "20",
    "best",
    "score",
    "32",
    "initially",
    "gotus",
    "37",
    "ridge",
    "regression",
    "see",
    "negative",
    "mean",
    "square",
    "error",
    "definitely",
    "become",
    "better",
    "minus",
    "sign",
    "worry",
    "37",
    "come",
    "32",
    "cross",
    "validation",
    "guys",
    "inside",
    "grids",
    "cv",
    "also",
    "probably",
    "taking",
    "entire",
    "combination",
    "cv",
    "value",
    "cross",
    "validation",
    "also",
    "use",
    "probably",
    "probably",
    "considering",
    "things",
    "many",
    "people",
    "question",
    "chris",
    "minus",
    "value",
    "increased",
    "basically",
    "means",
    "use",
    "ridge",
    "regression",
    "right",
    "particular",
    "case",
    "ridge",
    "regression",
    "helping",
    "guys",
    "let",
    "write",
    "everybody",
    "worry",
    "yeah",
    "previous",
    "got",
    "minus",
    "32",
    "right",
    "getting",
    "37",
    "right",
    "sorry",
    "previously",
    "got",
    "37",
    "37",
    "got",
    "32",
    "see",
    "got",
    "linear",
    "regression",
    "got",
    "ridge",
    "one",
    "select",
    "select",
    "model",
    "performing",
    "well",
    "understand",
    "ridge",
    "also",
    "tries",
    "reduce",
    "overfitting",
    "probably",
    "particular",
    "scenario",
    "use",
    "ridge",
    "performance",
    "becoming",
    "bad",
    "go",
    "try",
    "lasso",
    "regression",
    "copy",
    "paste",
    "thing",
    "linear",
    "model",
    "import",
    "lasso",
    "basically",
    "lasso",
    "let",
    "see",
    "lasso",
    "whether",
    "increase",
    "let",
    "see",
    "parameter",
    "got",
    "selected",
    "let",
    "write",
    "lasto",
    "regressor",
    "dot",
    "best",
    "params",
    "alpha",
    "equal",
    "one",
    "got",
    "selected",
    "going",
    "print",
    "okay",
    "going",
    "print",
    "last",
    "one",
    "regression",
    "dot",
    "score",
    "best",
    "actually",
    "getting",
    "35",
    "35",
    "actually",
    "getting",
    "32",
    "minus",
    "35",
    "still",
    "focus",
    "linear",
    "regression",
    "see",
    "happen",
    "add",
    "parameters",
    "add",
    "parameters",
    "see",
    "happen",
    "going",
    "take",
    "alpha",
    "different",
    "different",
    "values",
    "see",
    "going",
    "remove",
    "probably",
    "add",
    "alpha",
    "value",
    "way",
    "see",
    "added",
    "values",
    "5",
    "10",
    "20",
    "30",
    "35",
    "40",
    "45",
    "100",
    "okay",
    "let",
    "see",
    "whether",
    "performance",
    "increase",
    "uh",
    "first",
    "let",
    "remove",
    "ridge",
    "take",
    "guys",
    "adding",
    "parameters",
    "like",
    "take",
    "yeah",
    "cv",
    "equal",
    "5",
    "nobody",
    "okay",
    "able",
    "see",
    "um",
    "cv",
    "equal",
    "5",
    "uh",
    "basically",
    "focus",
    "see",
    "added",
    "values",
    "like",
    "also",
    "add",
    "try",
    "execute",
    "go",
    "probably",
    "see",
    "see",
    "first",
    "tried",
    "ridge",
    "getting",
    "minus",
    "29",
    "see",
    "adding",
    "parameters",
    "happened",
    "ridge",
    "adding",
    "parameters",
    "happened",
    "ridge",
    "see",
    "om",
    "minus",
    "29",
    "alpha",
    "value",
    "got",
    "selected",
    "100",
    "want",
    "try",
    "cross",
    "validation",
    "10",
    "try",
    "execute",
    "hyper",
    "parameters",
    "definitely",
    "play",
    "see",
    "29",
    "see",
    "minus",
    "29",
    "also",
    "increase",
    "cross",
    "validation",
    "value",
    "also",
    "probably",
    "execute",
    "lasso",
    "know",
    "whether",
    "improving",
    "coming",
    "minus",
    "34",
    "play",
    "parameters",
    "bigger",
    "problem",
    "statement",
    "thing",
    "limited",
    "right",
    "try",
    "take",
    "multiples",
    "many",
    "parameters",
    "multiples",
    "many",
    "parameters",
    "try",
    "things",
    "play",
    "multiple",
    "parameters",
    "whichever",
    "gives",
    "us",
    "best",
    "result",
    "basically",
    "taking",
    "okay",
    "error",
    "increased",
    "know",
    "error",
    "increasing",
    "definitely",
    "error",
    "increasing",
    "even",
    "though",
    "trying",
    "different",
    "different",
    "parameters",
    "scenario",
    "see",
    "gotus",
    "37",
    "probably",
    "actually",
    "uh",
    "try",
    "get",
    "better",
    "one",
    "respect",
    "best",
    "way",
    "also",
    "basically",
    "take",
    "train",
    "test",
    "split",
    "also",
    "probably",
    "things",
    "let",
    "see",
    "let",
    "see",
    "one",
    "example",
    "train",
    "test",
    "sk",
    "scalar",
    "dot",
    "think",
    "model",
    "selection",
    "import",
    "train",
    "test",
    "split",
    "okay",
    "okay",
    "guys",
    "may",
    "get",
    "different",
    "value",
    "okay",
    "let",
    "one",
    "thing",
    "okay",
    "let",
    "make",
    "problem",
    "statement",
    "little",
    "bit",
    "simpler",
    "going",
    "tell",
    "train",
    "test",
    "plate",
    "need",
    "going",
    "take",
    "code",
    "going",
    "paste",
    "let",
    "one",
    "thing",
    "let",
    "insert",
    "cell",
    "let",
    "train",
    "test",
    "split",
    "train",
    "test",
    "plate",
    "going",
    "take",
    "syntax",
    "paste",
    "let",
    "say",
    "taking",
    "xt",
    "train",
    "train",
    "using",
    "train",
    "test",
    "split",
    "33",
    "execute",
    "respect",
    "x",
    "train",
    "train",
    "see",
    "written",
    "code",
    "sk",
    "learn",
    "model",
    "selection",
    "uh",
    "train",
    "test",
    "plate",
    "random",
    "state",
    "anything",
    "whatever",
    "write",
    "fine",
    "basically",
    "give",
    "x",
    "test",
    "sizes",
    "33",
    "uh",
    "basically",
    "saying",
    "test",
    "33",
    "train",
    "data",
    "77",
    "actually",
    "getting",
    "respect",
    "x",
    "train",
    "train",
    "going",
    "going",
    "basically",
    "take",
    "x",
    "train",
    "comma",
    "train",
    "go",
    "probably",
    "see",
    "see",
    "minus",
    "25",
    "understand",
    "value",
    "go",
    "towards",
    "zero",
    "going",
    "towards",
    "zero",
    "basically",
    "means",
    "performance",
    "better",
    "similarly",
    "ridge",
    "ridge",
    "actually",
    "going",
    "going",
    "write",
    "x",
    "train",
    "train",
    "go",
    "probably",
    "select",
    "best",
    "score",
    "able",
    "see",
    "getting",
    "much",
    "getting",
    "minus",
    "okay",
    "getting",
    "25",
    "47",
    "basically",
    "means",
    "still",
    "improvement",
    "little",
    "bit",
    "bad",
    "going",
    "towards",
    "zero",
    "next",
    "part",
    "also",
    "basically",
    "x",
    "train",
    "train",
    "x",
    "train",
    "train",
    "one",
    "let",
    "go",
    "execute",
    "see",
    "minus",
    "also",
    "use",
    "lasso",
    "regressor",
    "predict",
    "basically",
    "predict",
    "respect",
    "x",
    "test",
    "white",
    "test",
    "value",
    "suppose",
    "let",
    "say",
    "pr",
    "yore",
    "pr",
    "sk",
    "learn",
    "using",
    "r",
    "square",
    "adjusted",
    "r",
    "square",
    "remember",
    "sk",
    "learn",
    "r",
    "square",
    "r²",
    "r2",
    "score",
    "present",
    "sk",
    "learn",
    "matrix",
    "going",
    "write",
    "sk",
    "learn",
    "import",
    "let",
    "say",
    "saying",
    "skarn",
    "matrix",
    "import",
    "r²",
    "r2",
    "score",
    "going",
    "basically",
    "going",
    "say",
    "r2",
    "score",
    "variable",
    "say",
    "nothing",
    "r2",
    "score",
    "going",
    "give",
    "pr",
    "comma",
    "yore",
    "test",
    "go",
    "probably",
    "see",
    "output",
    "able",
    "see",
    "print",
    "r2",
    "score",
    "discussed",
    "guys",
    "also",
    "adjusted",
    "rant",
    "score",
    "r2",
    "r2",
    "score",
    "one",
    "adjusted",
    "r²",
    "okay",
    "r2",
    "score",
    "adjusted",
    "r",
    "square",
    "somewhere",
    "manner",
    "output",
    "looks",
    "like",
    "respect",
    "using",
    "lasso",
    "regressor",
    "okay",
    "good",
    "okay",
    "told",
    "near",
    "100",
    "right",
    "getting",
    "67",
    "want",
    "tie",
    "ridge",
    "also",
    "try",
    "say",
    "ridge",
    "regressor",
    "predict",
    "see",
    "7",
    "68",
    "also",
    "try",
    "linear",
    "regressor",
    "predict",
    "error",
    "saying",
    "regression",
    "fitted",
    "yet",
    "fitted",
    "fitted",
    "let",
    "say",
    "fitted",
    "linear",
    "regression",
    "dot",
    "fit",
    "x",
    "train",
    "train",
    "x",
    "train",
    "comma",
    "train",
    "going",
    "fit",
    "go",
    "probably",
    "try",
    "calculation",
    "go",
    "see",
    "r2",
    "score",
    "also",
    "coming",
    "somewhere",
    "around",
    "68",
    "67",
    "since",
    "linear",
    "regression",
    "wo",
    "able",
    "get",
    "100",
    "drawing",
    "straight",
    "line",
    "right",
    "basically",
    "use",
    "algorithms",
    "like",
    "xg",
    "boost",
    "n",
    "bias",
    "many",
    "algorithms",
    "okay",
    "see",
    "give",
    "test",
    "pr",
    "right",
    "comparing",
    "see",
    "one",
    "limit",
    "increase",
    "performance",
    "see",
    "telling",
    "linear",
    "regression",
    "points",
    "right",
    "able",
    "create",
    "one",
    "best",
    "line",
    "create",
    "curve",
    "line",
    "right",
    "obviously",
    "accuracy",
    "limited",
    "let",
    "go",
    "logistic",
    "practical",
    "quickly",
    "uh",
    "logistic",
    "also",
    "git",
    "se",
    "cv",
    "actually",
    "going",
    "first",
    "let",
    "go",
    "ahead",
    "data",
    "set",
    "quickly",
    "implement",
    "logistic",
    "lc",
    "learn",
    "linear",
    "model",
    "going",
    "import",
    "logistic",
    "regression",
    "going",
    "use",
    "logistic",
    "regression",
    "apart",
    "know",
    "let",
    "take",
    "new",
    "data",
    "set",
    "logistic",
    "need",
    "solve",
    "using",
    "classification",
    "problem",
    "basically",
    "logistic",
    "regression",
    "take",
    "one",
    "data",
    "set",
    "sk",
    "learn",
    "data",
    "sets",
    "import",
    "take",
    "data",
    "set",
    "like",
    "uh",
    "breast",
    "cancer",
    "data",
    "set",
    "also",
    "present",
    "sk",
    "learn",
    "respect",
    "breast",
    "cancer",
    "data",
    "set",
    "going",
    "use",
    "see",
    "load",
    "best",
    "cancer",
    "data",
    "set",
    "loading",
    "independent",
    "features",
    "data",
    "columns",
    "feature",
    "names",
    "thing",
    "like",
    "previously",
    "okay",
    "basically",
    "complete",
    "uh",
    "complete",
    "independent",
    "feature",
    "go",
    "probably",
    "see",
    "head",
    "able",
    "see",
    "based",
    "input",
    "features",
    "independent",
    "feature",
    "need",
    "determine",
    "whether",
    "person",
    "cancer",
    "features",
    "like",
    "many",
    "many",
    "features",
    "actually",
    "present",
    "next",
    "thing",
    "independent",
    "feature",
    "take",
    "dependent",
    "feature",
    "dependent",
    "feature",
    "already",
    "present",
    "df",
    "target",
    "okay",
    "particular",
    "data",
    "set",
    "taken",
    "df",
    "df",
    "target",
    "basically",
    "dependent",
    "feature",
    "independent",
    "features",
    "actually",
    "going",
    "going",
    "create",
    "going",
    "say",
    "pd",
    "data",
    "frame",
    "going",
    "say",
    "df",
    "target",
    "target",
    "column",
    "name",
    "target",
    "right",
    "column",
    "name",
    "go",
    "see",
    "basically",
    "zeros",
    "one",
    "target",
    "feature",
    "next",
    "thing",
    "going",
    "uh",
    "apply",
    "basically",
    "apply",
    "first",
    "need",
    "check",
    "whether",
    "data",
    "set",
    "uh",
    "particular",
    "column",
    "balanced",
    "imbalanced",
    "okay",
    "order",
    "write",
    "f",
    "target",
    "data",
    "set",
    "imbalanced",
    "definitely",
    "need",
    "work",
    "try",
    "perform",
    "upsampling",
    "write",
    "target",
    "valore",
    "counts",
    "execute",
    "able",
    "see",
    "value",
    "sc",
    "counts",
    "basically",
    "give",
    "many",
    "number",
    "ones",
    "many",
    "number",
    "zeros",
    "total",
    "number",
    "ones",
    "357",
    "total",
    "number",
    "zeros",
    "22",
    "imbalanced",
    "data",
    "set",
    "probably",
    "balanced",
    "data",
    "set",
    "actually",
    "going",
    "train",
    "test",
    "spit",
    "train",
    "test",
    "spit",
    "try",
    "train",
    "test",
    "spit",
    "quickly",
    "copy",
    "thing",
    "entirely",
    "copy",
    "entirely",
    "get",
    "x",
    "x",
    "train",
    "x",
    "test",
    "train",
    "test",
    "train",
    "test",
    "plate",
    "obviously",
    "logistic",
    "regression",
    "go",
    "search",
    "logistic",
    "regression",
    "escalar",
    "able",
    "see",
    "parameters",
    "basically",
    "l1",
    "norm",
    "l2",
    "norm",
    "l1",
    "regularization",
    "l2",
    "regularization",
    "respect",
    "whatever",
    "things",
    "discussed",
    "logistic",
    "c",
    "value",
    "two",
    "parameter",
    "values",
    "much",
    "important",
    "probably",
    "show",
    "penalty",
    "kind",
    "penalty",
    "whether",
    "want",
    "add",
    "l2",
    "penalty",
    "l1",
    "penalty",
    "use",
    "l2",
    "l1",
    "next",
    "thing",
    "c",
    "nothing",
    "inverse",
    "regularization",
    "strength",
    "basically",
    "says",
    "1",
    "lambda",
    "something",
    "like",
    "parameter",
    "also",
    "much",
    "important",
    "guys",
    "class",
    "weight",
    "suppose",
    "data",
    "set",
    "balanced",
    "point",
    "time",
    "apply",
    "weights",
    "classes",
    "probably",
    "data",
    "set",
    "balanced",
    "directly",
    "use",
    "class",
    "weight",
    "equal",
    "balanced",
    "use",
    "weight",
    "basically",
    "want",
    "specifically",
    "right",
    "ridge",
    "lasso",
    "okay",
    "logistic",
    "logistic",
    "also",
    "l1",
    "norm",
    "l2",
    "norms",
    "understand",
    "probably",
    "missed",
    "particular",
    "part",
    "theory",
    "also",
    "l2",
    "penalty",
    "norm",
    "l1",
    "penalty",
    "norm",
    "probably",
    "teach",
    "theory",
    "look",
    "see",
    "logistic",
    "regression",
    "learned",
    "two",
    "different",
    "ways",
    "one",
    "probabilistic",
    "method",
    "one",
    "geometric",
    "method",
    "go",
    "probably",
    "see",
    "video",
    "present",
    "respect",
    "logistic",
    "regression",
    "right",
    "youtube",
    "channel",
    "explained",
    "l1",
    "l2",
    "norms",
    "also",
    "also",
    "basically",
    "present",
    "kind",
    "penalty",
    "uh",
    "using",
    "kind",
    "classification",
    "problem",
    "actually",
    "going",
    "let",
    "go",
    "play",
    "parameters",
    "looking",
    "play",
    "two",
    "parameters",
    "one",
    "params",
    "c",
    "value",
    "defining",
    "1",
    "10",
    "20",
    "anything",
    "define",
    "one",
    "set",
    "values",
    "define",
    "one",
    "parameter",
    "called",
    "max",
    "iteration",
    "specifically",
    "grits",
    "cv",
    "okay",
    "specifically",
    "going",
    "apply",
    "try",
    "execute",
    "params",
    "going",
    "quickly",
    "define",
    "model",
    "one",
    "logistic",
    "regression",
    "model",
    "logistic",
    "regression",
    "default",
    "one",
    "value",
    "give",
    "c",
    "max",
    "itra",
    "let",
    "say",
    "giving",
    "value",
    "later",
    "model",
    "apply",
    "grid",
    "sear",
    "cv",
    "going",
    "say",
    "grid",
    "cv",
    "going",
    "apply",
    "model",
    "one",
    "param",
    "grid",
    "equal",
    "params",
    "parameter",
    "specifically",
    "trying",
    "apply",
    "since",
    "classification",
    "problem",
    "pretty",
    "sure",
    "whether",
    "true",
    "positive",
    "important",
    "true",
    "negative",
    "important",
    "going",
    "use",
    "f1",
    "scoring",
    "okay",
    "f1",
    "scoring",
    "basically",
    "parametric",
    "term",
    "discussed",
    "yesterday",
    "nothing",
    "performance",
    "metrics",
    "going",
    "use",
    "cv",
    "equal",
    "5",
    "entirely",
    "model",
    "respect",
    "grid",
    "cv",
    "executing",
    "model",
    "fit",
    "x",
    "train",
    "train",
    "data",
    "execute",
    "see",
    "output",
    "along",
    "warnings",
    "lot",
    "warnings",
    "coming",
    "know",
    "many",
    "parameters",
    "finally",
    "see",
    "got",
    "selected",
    "really",
    "want",
    "find",
    "best",
    "param",
    "score",
    "model",
    "dot",
    "best",
    "params",
    "see",
    "max",
    "iteration",
    "150",
    "actually",
    "respect",
    "best",
    "score",
    "model",
    "best",
    "score",
    "95",
    "percentage",
    "still",
    "want",
    "test",
    "test",
    "data",
    "yes",
    "definitely",
    "say",
    "model",
    "core",
    "say",
    "model",
    "dot",
    "predict",
    "x",
    "test",
    "data",
    "basically",
    "red",
    "red",
    "prediction",
    "actually",
    "getting",
    "go",
    "see",
    "red",
    "ones",
    "zeros",
    "respect",
    "prediction",
    "finally",
    "getting",
    "prediction",
    "values",
    "apply",
    "confusion",
    "matrix",
    "hope",
    "taught",
    "confusion",
    "matrix",
    "sklearn",
    "confusion",
    "matrix",
    "sorry",
    "sklearn",
    "metrix",
    "going",
    "import",
    "confusion",
    "metrix",
    "classification",
    "report",
    "next",
    "thing",
    "would",
    "like",
    "two",
    "try",
    "import",
    "confusion",
    "matrix",
    "classification",
    "report",
    "want",
    "see",
    "confusion",
    "matrix",
    "respect",
    "write",
    "yore",
    "frad",
    "yore",
    "test",
    "whatever",
    "want",
    "go",
    "ahead",
    "basically",
    "confusion",
    "matrix",
    "put",
    "forward",
    "difference",
    "thing",
    "moving",
    "also",
    "showed",
    "63",
    "118",
    "3",
    "4",
    "finally",
    "want",
    "accuracy",
    "score",
    "also",
    "import",
    "accuracy",
    "score",
    "see",
    "accuracy",
    "score",
    "imported",
    "also",
    "find",
    "accuracy",
    "score",
    "total",
    "accuracy",
    "respect",
    "give",
    "test",
    "yore",
    "pr",
    "discussed",
    "yesterday",
    "giving",
    "96",
    "want",
    "detailed",
    "precision",
    "recall",
    "score",
    "point",
    "time",
    "use",
    "classification",
    "report",
    "give",
    "white",
    "test",
    "wied",
    "actually",
    "getting",
    "see",
    "respect",
    "f1",
    "f1",
    "score",
    "precision",
    "recall",
    "since",
    "balanced",
    "data",
    "set",
    "obviously",
    "performance",
    "best",
    "yes",
    "also",
    "use",
    "roc",
    "see",
    "also",
    "show",
    "use",
    "roc",
    "probably",
    "able",
    "see",
    "probably",
    "calculate",
    "false",
    "positive",
    "rate",
    "two",
    "positive",
    "rate",
    "worry",
    "roc",
    "first",
    "explain",
    "theoretical",
    "part",
    "let",
    "go",
    "ahead",
    "discuss",
    "n",
    "bias",
    "n",
    "bias",
    "important",
    "algorithm",
    "going",
    "go",
    "ahead",
    "let",
    "go",
    "ahead",
    "discuss",
    "na",
    "bias",
    "going",
    "discuss",
    "intuition",
    "na",
    "bias",
    "another",
    "amazing",
    "algorithm",
    "specifically",
    "used",
    "classification",
    "specifically",
    "works",
    "something",
    "called",
    "base",
    "theorem",
    "exactly",
    "base",
    "theorem",
    "first",
    "need",
    "understand",
    "base",
    "theorem",
    "let",
    "say",
    "guys",
    "base",
    "theorem",
    "let",
    "say",
    "experiment",
    "called",
    "rolling",
    "dis",
    "rolling",
    "dis",
    "many",
    "number",
    "elements",
    "say",
    "probability",
    "1",
    "obviously",
    "saying",
    "1x",
    "6",
    "say",
    "probability",
    "two",
    "also",
    "say",
    "1x",
    "6",
    "say",
    "probability",
    "three",
    "definitely",
    "say",
    "1x",
    "6",
    "know",
    "kind",
    "events",
    "basically",
    "called",
    "independent",
    "events",
    "rolling",
    "dice",
    "called",
    "independent",
    "event",
    "getting",
    "one",
    "two",
    "every",
    "experiment",
    "one",
    "dependent",
    "two",
    "two",
    "dependent",
    "three",
    "independent",
    "reason",
    "specifically",
    "say",
    "independent",
    "event",
    "take",
    "example",
    "dependent",
    "events",
    "let",
    "consider",
    "bag",
    "marbles",
    "okay",
    "marble",
    "basically",
    "three",
    "red",
    "marbles",
    "two",
    "green",
    "marbles",
    "tell",
    "probability",
    "suppose",
    "event",
    "first",
    "event",
    "take",
    "red",
    "marble",
    "probability",
    "taking",
    "red",
    "marble",
    "definitely",
    "say",
    "3x5",
    "okay",
    "first",
    "event",
    "second",
    "event",
    "let",
    "say",
    "taken",
    "red",
    "marble",
    "second",
    "second",
    "time",
    "taking",
    "second",
    "red",
    "marble",
    "forget",
    "second",
    "rand",
    "marble",
    "want",
    "take",
    "green",
    "marble",
    "probability",
    "respect",
    "taking",
    "green",
    "marble",
    "definitely",
    "saying",
    "okay",
    "one",
    "red",
    "marble",
    "removed",
    "total",
    "number",
    "marbles",
    "left",
    "four",
    "definitely",
    "write",
    "probability",
    "getting",
    "green",
    "marble",
    "nothing",
    "2x4",
    "nothing",
    "1x2",
    "happening",
    "first",
    "first",
    "element",
    "took",
    "first",
    "marble",
    "took",
    "first",
    "event",
    "first",
    "event",
    "took",
    "red",
    "marble",
    "second",
    "event",
    "took",
    "green",
    "marble",
    "two",
    "two",
    "dependent",
    "events",
    "number",
    "marbles",
    "getting",
    "reduced",
    "take",
    "tell",
    "probability",
    "taking",
    "red",
    "marble",
    "green",
    "marble",
    "simple",
    "formula",
    "much",
    "simple",
    "right",
    "already",
    "discussed",
    "stats",
    "nothing",
    "probability",
    "probability",
    "red",
    "multiplied",
    "probability",
    "green",
    "given",
    "red",
    "specific",
    "thing",
    "called",
    "conditional",
    "probability",
    "understand",
    "happening",
    "probability",
    "green",
    "marble",
    "given",
    "red",
    "marble",
    "event",
    "occurred",
    "events",
    "independent",
    "let",
    "write",
    "nicely",
    "write",
    "probability",
    "b",
    "equal",
    "probability",
    "multiplied",
    "probability",
    "b",
    "divided",
    "probability",
    "let",
    "go",
    "derive",
    "something",
    "write",
    "probability",
    "b",
    "equal",
    "probability",
    "b",
    "answer",
    "yes",
    "definitely",
    "say",
    "definitely",
    "say",
    "go",
    "calculation",
    "able",
    "get",
    "answer",
    "say",
    "formula",
    "probability",
    "b",
    "basically",
    "write",
    "probability",
    "multiplied",
    "probability",
    "b",
    "given",
    "take",
    "probability",
    "green",
    "probability",
    "green",
    "particular",
    "case",
    "2x",
    "5",
    "probability",
    "red",
    "3x",
    "4",
    "right",
    "let",
    "consider",
    "part",
    "definitely",
    "write",
    "part",
    "definitely",
    "write",
    "probability",
    "b",
    "multiplied",
    "probability",
    "b",
    "probability",
    "b",
    "one",
    "probability",
    "b",
    "probability",
    "given",
    "b",
    "definitely",
    "write",
    "much",
    "respect",
    "information",
    "derive",
    "probability",
    "equal",
    "probability",
    "b",
    "multiplied",
    "probability",
    "b",
    "probability",
    "given",
    "b",
    "divided",
    "probability",
    "sorry",
    "write",
    "probability",
    "b",
    "given",
    "divided",
    "probability",
    "specifically",
    "called",
    "base",
    "theorem",
    "crux",
    "behind",
    "na",
    "bias",
    "understand",
    "crux",
    "behind",
    "base",
    "theorem",
    "let",
    "go",
    "ahead",
    "let",
    "discuss",
    "using",
    "solve",
    "let",
    "take",
    "examples",
    "probably",
    "make",
    "understand",
    "let",
    "say",
    "features",
    "like",
    "x1",
    "x2",
    "x3",
    "x4",
    "x5",
    "like",
    "till",
    "xn",
    "output",
    "independent",
    "features",
    "independent",
    "features",
    "independent",
    "features",
    "going",
    "write",
    "independent",
    "features",
    "output",
    "feature",
    "also",
    "dependent",
    "feature",
    "happening",
    "say",
    "probability",
    "b",
    "basically",
    "mean",
    "need",
    "really",
    "find",
    "probability",
    "know",
    "guys",
    "values",
    "basically",
    "output",
    "value",
    "based",
    "input",
    "values",
    "need",
    "predict",
    "output",
    "initially",
    "training",
    "data",
    "set",
    "input",
    "output",
    "initially",
    "model",
    "get",
    "trained",
    "let",
    "consider",
    "entire",
    "terminology",
    "try",
    "write",
    "terms",
    "equation",
    "say",
    "probability",
    "given",
    "x1a",
    "x2a",
    "x3",
    "till",
    "xn",
    "equation",
    "become",
    "probability",
    "see",
    "probability",
    "given",
    "x",
    "x1",
    "x2",
    "x3",
    "xn",
    "nothing",
    "x1",
    "x2",
    "x3",
    "xn",
    "trying",
    "find",
    "probability",
    "write",
    "probability",
    "b",
    "b",
    "nothing",
    "write",
    "probability",
    "b",
    "right",
    "given",
    "b",
    "probability",
    "b",
    "probability",
    "b",
    "nothing",
    "multiplied",
    "probability",
    "given",
    "b",
    "probability",
    "given",
    "b",
    "basically",
    "means",
    "probability",
    "x1a",
    "x2",
    "comma",
    "xn",
    "given",
    "b",
    "b",
    "given",
    "right",
    "able",
    "find",
    "entire",
    "value",
    "second",
    "made",
    "mistakes",
    "guess",
    "correct",
    "sorry",
    "missed",
    "one",
    "term",
    "given",
    "become",
    "equal",
    "probability",
    "x1",
    "comma",
    "x2",
    "like",
    "xl",
    "probability",
    "multiplied",
    "probability",
    "given",
    "try",
    "expand",
    "basically",
    "become",
    "something",
    "like",
    "see",
    "probability",
    "multiplied",
    "probability",
    "x1",
    "given",
    "yes",
    "given",
    "sorry",
    "given",
    "multiplied",
    "probability",
    "x2",
    "given",
    "probability",
    "x3",
    "given",
    "like",
    "probability",
    "xn",
    "given",
    "also",
    "y1",
    "y2",
    "y3",
    "yn",
    "expand",
    "like",
    "basically",
    "become",
    "probability",
    "x",
    "1",
    "multiplied",
    "probability",
    "x2",
    "multiplied",
    "probability",
    "x3",
    "like",
    "probability",
    "xn",
    "respect",
    "probability",
    "different",
    "see",
    "particular",
    "record",
    "different",
    "different",
    "different",
    "output",
    "may",
    "yes",
    "right",
    "may",
    "yes",
    "okay",
    "solve",
    "problem",
    "make",
    "everything",
    "understand",
    "probably",
    "probability",
    "binary",
    "multiclass",
    "whatever",
    "things",
    "want",
    "solve",
    "problem",
    "front",
    "let",
    "say",
    "let",
    "say",
    "lot",
    "features",
    "x1",
    "x2",
    "x3",
    "x",
    "x4",
    "respect",
    "let",
    "say",
    "one",
    "data",
    "set",
    "many",
    "x1s",
    "many",
    "features",
    "feature",
    "number",
    "let",
    "say",
    "yes",
    "probably",
    "write",
    "really",
    "need",
    "understand",
    "okay",
    "basically",
    "say",
    "probability",
    "equal",
    "yes",
    "given",
    "x",
    "first",
    "record",
    "first",
    "record",
    "x",
    "second",
    "record",
    "x",
    "may",
    "write",
    "like",
    "probability",
    "yes",
    "x",
    "given",
    "x",
    "basically",
    "means",
    "x1",
    "x2",
    "x3",
    "x4",
    "obviously",
    "write",
    "kind",
    "equation",
    "basically",
    "say",
    "probability",
    "yes",
    "multiplied",
    "probability",
    "yes",
    "multiplied",
    "probability",
    "x",
    "1",
    "given",
    "yes",
    "multiplied",
    "probability",
    "x2",
    "given",
    "yes",
    "probability",
    "x3",
    "given",
    "yes",
    "probability",
    "x4",
    "given",
    "yes",
    "divided",
    "probability",
    "x1",
    "multiplied",
    "probability",
    "x2",
    "multiplied",
    "probability",
    "x3",
    "multiplied",
    "probability",
    "x4",
    "fixed",
    "may",
    "yes",
    "may",
    "respect",
    "different",
    "different",
    "records",
    "value",
    "may",
    "change",
    "similarly",
    "write",
    "probability",
    "equal",
    "given",
    "x",
    "probability",
    "multiplied",
    "probability",
    "x1",
    "given",
    "probability",
    "x2",
    "given",
    "probability",
    "x3",
    "given",
    "probability",
    "x4",
    "given",
    "every",
    "input",
    "give",
    "input",
    "x",
    "give",
    "may",
    "either",
    "get",
    "yes",
    "need",
    "find",
    "probability",
    "probability",
    "x1",
    "multiplied",
    "probability",
    "x2",
    "multiplied",
    "probability",
    "x3",
    "multiplied",
    "probability",
    "x4",
    "see",
    "respect",
    "x",
    "output",
    "yes",
    "really",
    "need",
    "find",
    "probabilities",
    "formula",
    "written",
    "probability",
    "respect",
    "yes",
    "probability",
    "respect",
    "case",
    "one",
    "common",
    "thing",
    "see",
    "denominator",
    "fixed",
    "definitely",
    "fixed",
    "fixed",
    "going",
    "change",
    "consider",
    "constant",
    "definitely",
    "ignore",
    "definitely",
    "ignore",
    "things",
    "ignore",
    "also",
    "ignore",
    "al",
    "see",
    "constant",
    "want",
    "consider",
    "next",
    "time",
    "use",
    "specific",
    "formula",
    "calculate",
    "probability",
    "let",
    "say",
    "first",
    "probability",
    "specific",
    "data",
    "set",
    "yes",
    "x",
    "let",
    "say",
    "getting",
    "as13",
    "similarly",
    "probability",
    "respect",
    "x",
    "get",
    "05",
    "know",
    "binary",
    "classification",
    "values",
    "get",
    "greater",
    "equal",
    "5",
    "going",
    "consider",
    "1",
    "less",
    "going",
    "consider",
    "zero",
    "getting",
    "values",
    "like",
    "13",
    "05",
    "obviously",
    "getting",
    "05",
    "something",
    "called",
    "normalization",
    "says",
    "really",
    "want",
    "find",
    "probability",
    "x",
    "x",
    "normalization",
    "nothing",
    "divided",
    "05",
    "72",
    "nothing",
    "72",
    "similarly",
    "probability",
    "given",
    "x",
    "obviously",
    "say",
    "1",
    "72",
    "remaining",
    "answer",
    "28",
    "nothing",
    "28",
    "final",
    "answer",
    "one",
    "formulas",
    "remember",
    "solve",
    "problem",
    "let",
    "solve",
    "problem",
    "interesting",
    "problem",
    "let",
    "say",
    "data",
    "set",
    "like",
    "feature",
    "day",
    "let",
    "copy",
    "data",
    "set",
    "okay",
    "data",
    "set",
    "want",
    "take",
    "information",
    "let",
    "take",
    "outlook",
    "table",
    "based",
    "output",
    "outlook",
    "feature",
    "see",
    "outlook",
    "day",
    "outlook",
    "temperature",
    "humidity",
    "wind",
    "input",
    "features",
    "independent",
    "feature",
    "output",
    "feature",
    "one",
    "probably",
    "seeing",
    "play",
    "tennis",
    "output",
    "feature",
    "specifically",
    "binary",
    "classification",
    "actually",
    "going",
    "basically",
    "going",
    "take",
    "outlook",
    "feature",
    "based",
    "outlook",
    "feature",
    "try",
    "create",
    "smaller",
    "table",
    "give",
    "information",
    "based",
    "outlook",
    "first",
    "try",
    "find",
    "many",
    "categories",
    "outlook",
    "one",
    "sunny",
    "one",
    "overcast",
    "one",
    "rain",
    "right",
    "three",
    "categories",
    "going",
    "write",
    "sunny",
    "overcast",
    "rain",
    "three",
    "features",
    "respect",
    "sunny",
    "uh",
    "outlook",
    "three",
    "categories",
    "one",
    "sunny",
    "one",
    "overcast",
    "one",
    "ra",
    "going",
    "basically",
    "say",
    "respect",
    "sunny",
    "many",
    "yes",
    "many",
    "probability",
    "yes",
    "probability",
    "going",
    "write",
    "outlook",
    "feature",
    "categories",
    "first",
    "yes",
    "sunny",
    "overcast",
    "rain",
    "yes",
    "probability",
    "yes",
    "probability",
    "next",
    "thing",
    "need",
    "find",
    "respect",
    "sunny",
    "many",
    "yes",
    "see",
    "yes",
    "sunny",
    "answer",
    "increase",
    "count",
    "one",
    "sunny",
    "answer",
    "going",
    "increase",
    "count",
    "two",
    "sunny",
    "basically",
    "okay",
    "going",
    "increase",
    "count",
    "three",
    "sunny",
    "many",
    "yes",
    "one",
    "two",
    "one",
    "one",
    "two",
    "going",
    "say",
    "respect",
    "sunny",
    "two",
    "yes",
    "understand",
    "outlook",
    "x1",
    "x1",
    "feature",
    "let",
    "consider",
    "next",
    "thing",
    "let",
    "see",
    "respect",
    "overcost",
    "overcast",
    "many",
    "yes",
    "overcast",
    "yes",
    "1",
    "2",
    "3",
    "four",
    "total",
    "four",
    "yes",
    "respect",
    "overcast",
    "respect",
    "overcast",
    "many",
    "go",
    "ah",
    "find",
    "basically",
    "zero",
    "nos",
    "respect",
    "rain",
    "many",
    "yes",
    "see",
    "respect",
    "one",
    "rain",
    "yes",
    "yes",
    "nothing",
    "3",
    "2",
    "let",
    "try",
    "find",
    "three",
    "two",
    "one",
    "also",
    "one",
    "yes",
    "right",
    "3",
    "yes",
    "two",
    "nos",
    "total",
    "number",
    "yes",
    "nos",
    "count",
    "nine",
    "yes",
    "five",
    "nos",
    "total",
    "count",
    "totally",
    "count",
    "9",
    "5",
    "14",
    "able",
    "compare",
    "9",
    "yes",
    "five",
    "nos",
    "probability",
    "yes",
    "sunny",
    "given",
    "2x",
    "9",
    "4x",
    "9",
    "3x",
    "9",
    "say",
    "probability",
    "given",
    "sunny",
    "see",
    "probability",
    "yes",
    "given",
    "sunny",
    "probability",
    "yes",
    "given",
    "forecast",
    "probability",
    "yes",
    "given",
    "rain",
    "basically",
    "try",
    "write",
    "simpler",
    "manner",
    "get",
    "confused",
    "okay",
    "probability",
    "yes",
    "probability",
    "understand",
    "basically",
    "mean",
    "terminology",
    "basically",
    "means",
    "probability",
    "yes",
    "given",
    "sunny",
    "probability",
    "yes",
    "given",
    "overcast",
    "probability",
    "yes",
    "given",
    "rain",
    "similarly",
    "probability",
    "probability",
    "obviously",
    "know",
    "3x",
    "5",
    "first",
    "probability",
    "0x",
    "5",
    "2x",
    "5",
    "respect",
    "next",
    "feature",
    "let",
    "consider",
    "going",
    "consider",
    "one",
    "feature",
    "feature",
    "say",
    "let",
    "consider",
    "temperature",
    "okay",
    "let",
    "consider",
    "temperature",
    "temperature",
    "many",
    "features",
    "many",
    "categories",
    "hot",
    "see",
    "hot",
    "mild",
    "cold",
    "respect",
    "hot",
    "mild",
    "cold",
    "also",
    "yes",
    "probability",
    "yes",
    "probability",
    "try",
    "find",
    "respect",
    "hot",
    "many",
    "yes",
    "also",
    "two",
    "nos",
    "uh",
    "1",
    "yes",
    "uh",
    "2",
    "yes",
    "two",
    "yes",
    "two",
    "nos",
    "probably",
    "similarly",
    "respect",
    "mild",
    "mild",
    "many",
    "1",
    "yes",
    "1",
    "2",
    "yes",
    "3s",
    "4s",
    "4s",
    "two",
    "knows",
    "okay",
    "basically",
    "go",
    "calculate",
    "4",
    "yes",
    "two",
    "knows",
    "respect",
    "cold",
    "many",
    "cool",
    "cool",
    "cold",
    "1",
    "yes",
    "1",
    "2",
    "yes",
    "3",
    "3",
    "1",
    "specifically",
    "3s",
    "1",
    "total",
    "number",
    "9",
    "five",
    "equal",
    "thing",
    "got",
    "really",
    "go",
    "ahead",
    "finding",
    "probability",
    "yes",
    "given",
    "hot",
    "2x",
    "9",
    "much",
    "4x",
    "9",
    "3x",
    "9",
    "probability",
    "given",
    "given",
    "hot",
    "2x",
    "5",
    "2x",
    "5",
    "1x",
    "5",
    "two",
    "tables",
    "already",
    "created",
    "finally",
    "respect",
    "play",
    "total",
    "number",
    "plays",
    "yes",
    "9",
    "five",
    "answer",
    "total",
    "14",
    "say",
    "probability",
    "yes",
    "yes",
    "nothing",
    "9",
    "by4",
    "probability",
    "nothing",
    "5x4",
    "okay",
    "two",
    "values",
    "also",
    "require",
    "let",
    "say",
    "get",
    "new",
    "data",
    "set",
    "need",
    "get",
    "new",
    "data",
    "set",
    "let",
    "say",
    "get",
    "new",
    "test",
    "data",
    "says",
    "suppose",
    "sunny",
    "hot",
    "tell",
    "output",
    "problem",
    "statement",
    "let",
    "write",
    "write",
    "probability",
    "yes",
    "given",
    "sunny",
    "comma",
    "hot",
    "write",
    "probability",
    "yes",
    "multiplied",
    "probability",
    "write",
    "probability",
    "sunny",
    "given",
    "yes",
    "multiplied",
    "probability",
    "hot",
    "given",
    "yes",
    "divided",
    "probability",
    "sunny",
    "multiplied",
    "probability",
    "hot",
    "equation",
    "constant",
    "probability",
    "also",
    "getting",
    "value",
    "9",
    "by4",
    "probability",
    "yes",
    "going",
    "replace",
    "9",
    "by4",
    "multiplied",
    "2x",
    "9",
    "probability",
    "hot",
    "given",
    "yes",
    "going",
    "get",
    "2",
    "9",
    "99",
    "cancel",
    "2",
    "1",
    "7",
    "nothing",
    "2",
    "6331",
    "read",
    "statement",
    "little",
    "bit",
    "wrong",
    "probability",
    "sunny",
    "given",
    "yes",
    "go",
    "ahead",
    "calculate",
    "go",
    "ahead",
    "calculate",
    "probability",
    "given",
    "sunny",
    "hot",
    "probability",
    "multiplied",
    "probability",
    "sunny",
    "given",
    "multiplied",
    "probability",
    "hot",
    "given",
    "divided",
    "probability",
    "sunny",
    "multiplied",
    "probability",
    "heart",
    "get",
    "cancelled",
    "denominator",
    "constant",
    "guys",
    "constant",
    "probability",
    "probability",
    "nothing",
    "5",
    "by4",
    "write",
    "5",
    "by4",
    "multiplied",
    "probability",
    "sunny",
    "given",
    "probability",
    "sunny",
    "given",
    "probability",
    "sunny",
    "given",
    "nothing",
    "probability",
    "sunny",
    "given",
    "nothing",
    "3x",
    "5",
    "going",
    "get",
    "3x",
    "5",
    "multiplied",
    "probability",
    "h",
    "given",
    "nothing",
    "2x",
    "5",
    "2x",
    "5",
    "3x",
    "5",
    "five",
    "five",
    "get",
    "cancelled",
    "2",
    "1",
    "2",
    "7",
    "getting",
    "3x",
    "35",
    "nothing",
    "calculator",
    "uh",
    "actually",
    "getting",
    "three",
    "id",
    "35",
    "nothing",
    "857",
    "write",
    "probability",
    "yes",
    "given",
    "sunny",
    "comma",
    "hot",
    "independent",
    "feature",
    "nothing",
    "031",
    "031",
    "probability",
    "given",
    "sunny",
    "comma",
    "hot",
    "85",
    "try",
    "normalize",
    "85",
    "point",
    "divided",
    "031",
    "085",
    "73",
    "nothing",
    "73",
    "basically",
    "say",
    "1",
    "my27",
    "nothing",
    "27",
    "input",
    "comes",
    "sunny",
    "hot",
    "weather",
    "sunny",
    "hot",
    "person",
    "whether",
    "play",
    "answer",
    "okay",
    "next",
    "question",
    "new",
    "data",
    "overcast",
    "mild",
    "tell",
    "probability",
    "using",
    "name",
    "bias",
    "add",
    "number",
    "features",
    "let",
    "say",
    "say",
    "okay",
    "let",
    "let",
    "say",
    "probably",
    "say",
    "consider",
    "humidity",
    "mind",
    "wind",
    "also",
    "basically",
    "create",
    "kind",
    "table",
    "find",
    "assignment",
    "overcast",
    "mild",
    "respect",
    "nb",
    "try",
    "solve",
    "second",
    "algorithm",
    "going",
    "discuss",
    "something",
    "called",
    "knn",
    "algorithm",
    "knn",
    "algorithm",
    "simple",
    "problem",
    "statement",
    "okay",
    "used",
    "solve",
    "classification",
    "regression",
    "knn",
    "basically",
    "means",
    "k",
    "nearest",
    "neighbor",
    "let",
    "first",
    "discuss",
    "classification",
    "problem",
    "number",
    "one",
    "classification",
    "problem",
    "let",
    "say",
    "binary",
    "classification",
    "problem",
    "looks",
    "like",
    "two",
    "data",
    "points",
    "like",
    "one",
    "another",
    "one",
    "suppose",
    "new",
    "data",
    "point",
    "suppose",
    "new",
    "data",
    "point",
    "comes",
    "say",
    "whether",
    "belongs",
    "category",
    "whether",
    "belongs",
    "category",
    "probably",
    "create",
    "logistic",
    "regression",
    "may",
    "divide",
    "line",
    "particular",
    "scenario",
    "define",
    "come",
    "conclusion",
    "whether",
    "belong",
    "category",
    "category",
    "basically",
    "use",
    "something",
    "called",
    "k",
    "nearest",
    "neighbor",
    "let",
    "say",
    "say",
    "k",
    "value",
    "five",
    "going",
    "going",
    "basically",
    "take",
    "five",
    "nearest",
    "closest",
    "point",
    "let",
    "say",
    "two",
    "nearest",
    "closest",
    "point",
    "three",
    "nearest",
    "closest",
    "point",
    "basically",
    "see",
    "distance",
    "distance",
    "nearest",
    "point",
    "particular",
    "case",
    "see",
    "maximum",
    "number",
    "points",
    "red",
    "categories",
    "red",
    "red",
    "categories",
    "getting",
    "three",
    "points",
    "white",
    "categories",
    "getting",
    "two",
    "points",
    "particular",
    "scenario",
    "maximum",
    "number",
    "categories",
    "coming",
    "basically",
    "categorize",
    "particular",
    "class",
    "help",
    "distance",
    "distance",
    "specifically",
    "use",
    "use",
    "two",
    "distance",
    "one",
    "ukan",
    "distance",
    "one",
    "something",
    "called",
    "manhattan",
    "distance",
    "ukan",
    "manhattan",
    "distance",
    "ukan",
    "distance",
    "basically",
    "say",
    "suppose",
    "two",
    "points",
    "denoted",
    "x1",
    "y1",
    "x2",
    "y2",
    "ukine",
    "distance",
    "order",
    "calculate",
    "apply",
    "formula",
    "looks",
    "like",
    "x2",
    "x1",
    "y2",
    "y1",
    "whereas",
    "case",
    "magetan",
    "distance",
    "suppose",
    "two",
    "points",
    "calculate",
    "distance",
    "way",
    "calculate",
    "distance",
    "right",
    "distance",
    "calculate",
    "calculate",
    "hypothenuse",
    "distance",
    "basic",
    "difference",
    "ukan",
    "magetan",
    "distance",
    "may",
    "thinking",
    "chris",
    "fine",
    "classification",
    "problem",
    "regression",
    "regression",
    "also",
    "much",
    "simple",
    "suppose",
    "data",
    "points",
    "looks",
    "like",
    "new",
    "data",
    "point",
    "like",
    "want",
    "calculate",
    "basically",
    "take",
    "nearest",
    "five",
    "points",
    "let",
    "say",
    "k",
    "five",
    "k",
    "hyper",
    "parameter",
    "play",
    "suppose",
    "let",
    "say",
    "k",
    "finds",
    "nearest",
    "point",
    "need",
    "find",
    "point",
    "particular",
    "output",
    "respect",
    "k",
    "equal",
    "5",
    "try",
    "calculate",
    "average",
    "points",
    "calculates",
    "average",
    "points",
    "becomes",
    "output",
    "regression",
    "classification",
    "difference",
    "k",
    "actually",
    "hyper",
    "parameter",
    "try",
    "k",
    "equal",
    "1",
    "50",
    "probably",
    "try",
    "check",
    "error",
    "rate",
    "error",
    "rate",
    "less",
    "select",
    "model",
    "two",
    "things",
    "respect",
    "k",
    "nearish",
    "neighbor",
    "k",
    "nearest",
    "neighbor",
    "works",
    "bad",
    "respect",
    "two",
    "things",
    "one",
    "outliers",
    "one",
    "imbalanced",
    "data",
    "set",
    "outlier",
    "let",
    "say",
    "outlier",
    "one",
    "categories",
    "like",
    "another",
    "category",
    "let",
    "consider",
    "outliers",
    "looks",
    "like",
    "trying",
    "find",
    "point",
    "see",
    "nearest",
    "point",
    "basically",
    "blue",
    "belongs",
    "blue",
    "category",
    "outlier",
    "know",
    "consider",
    "nearest",
    "neighbor",
    "basically",
    "treated",
    "group",
    "formula",
    "manhattan",
    "distance",
    "uses",
    "modulus",
    "x2",
    "x1",
    "y2",
    "y1",
    "mode",
    "x2",
    "x1",
    "y2",
    "y1",
    "uh",
    "side",
    "guys",
    "yes",
    "also",
    "made",
    "detailed",
    "videos",
    "whatever",
    "topics",
    "discussed",
    "today",
    "directly",
    "go",
    "search",
    "particular",
    "topic",
    "agenda",
    "session",
    "try",
    "complete",
    "things",
    "going",
    "understand",
    "mathematical",
    "equations",
    "uh",
    "today",
    "session",
    "basically",
    "going",
    "discuss",
    "uh",
    "decision",
    "tree",
    "okay",
    "uh",
    "session",
    "going",
    "basically",
    "understand",
    "exact",
    "purpose",
    "decision",
    "tree",
    "help",
    "decision",
    "tree",
    "actually",
    "solving",
    "two",
    "different",
    "problems",
    "one",
    "regression",
    "one",
    "classification",
    "try",
    "understand",
    "particular",
    "part",
    "well",
    "take",
    "specific",
    "data",
    "set",
    "try",
    "solve",
    "problems",
    "coming",
    "decision",
    "tree",
    "one",
    "thing",
    "need",
    "understand",
    "say",
    "age",
    "less",
    "8",
    "let",
    "say",
    "writing",
    "condition",
    "age",
    "less",
    "equal",
    "18",
    "going",
    "say",
    "print",
    "go",
    "college",
    "printing",
    "print",
    "college",
    "write",
    "else",
    "age",
    "greater",
    "18",
    "pag",
    "less",
    "equal",
    "35",
    "say",
    "print",
    "work",
    "write",
    "else",
    "age",
    "let",
    "let",
    "put",
    "condition",
    "little",
    "bit",
    "better",
    "write",
    "l",
    "age",
    "greater",
    "18",
    "age",
    "less",
    "equal",
    "35",
    "going",
    "say",
    "print",
    "work",
    "basically",
    "people",
    "needs",
    "work",
    "age",
    "else",
    "going",
    "consider",
    "print",
    "retire",
    "ifls",
    "condition",
    "whenever",
    "kind",
    "nested",
    "els",
    "condition",
    "also",
    "represent",
    "form",
    "decision",
    "trees",
    "also",
    "actually",
    "form",
    "form",
    "decision",
    "decision",
    "tree",
    "first",
    "specific",
    "root",
    "node",
    "let",
    "say",
    "root",
    "node",
    "root",
    "node",
    "first",
    "condition",
    "less",
    "equal",
    "18",
    "obviously",
    "two",
    "conditions",
    "saying",
    "less",
    "equal",
    "18",
    "one",
    "condition",
    "yes",
    "one",
    "condition",
    "yes",
    "right",
    "condition",
    "true",
    "basically",
    "means",
    "go",
    "side",
    "true",
    "basically",
    "something",
    "like",
    "college",
    "leaf",
    "node",
    "similarly",
    "okay",
    "particular",
    "case",
    "go",
    "next",
    "condition",
    "next",
    "condition",
    "create",
    "node",
    "say",
    "okay",
    "less",
    "18",
    "greater",
    "sorry",
    "less",
    "equal",
    "35",
    "also",
    "two",
    "conditions",
    "basically",
    "yes",
    "create",
    "yes",
    "able",
    "see",
    "basically",
    "means",
    "two",
    "condition",
    "yes",
    "say",
    "print",
    "work",
    "leaf",
    "node",
    "splitting",
    "retire",
    "see",
    "entire",
    "algorithm",
    "entire",
    "code",
    "actually",
    "written",
    "see",
    "got",
    "converted",
    "kind",
    "trees",
    "specifically",
    "able",
    "take",
    "decisions",
    "yes",
    "solve",
    "classification",
    "problem",
    "sorry",
    "greater",
    "18",
    "greater",
    "18",
    "less",
    "35",
    "solve",
    "regression",
    "classification",
    "problem",
    "regression",
    "classification",
    "problem",
    "using",
    "decision",
    "trees",
    "creating",
    "kind",
    "nodes",
    "short",
    "whenever",
    "talk",
    "decision",
    "trees",
    "whenever",
    "talk",
    "decision",
    "trees",
    "seeing",
    "decision",
    "trees",
    "nothing",
    "decision",
    "trees",
    "nothing",
    "using",
    "nested",
    "el",
    "condition",
    "definitely",
    "solve",
    "specific",
    "problem",
    "statement",
    "visualized",
    "way",
    "specifically",
    "create",
    "decision",
    "tree",
    "form",
    "nodes",
    "need",
    "understand",
    "type",
    "maths",
    "probably",
    "use",
    "okay",
    "let",
    "one",
    "thing",
    "let",
    "take",
    "specific",
    "data",
    "set",
    "definitely",
    "front",
    "okay",
    "try",
    "solve",
    "particular",
    "data",
    "set",
    "basically",
    "give",
    "idea",
    "like",
    "probably",
    "solve",
    "problems",
    "uh",
    "let",
    "open",
    "snippet",
    "tool",
    "data",
    "set",
    "let",
    "consider",
    "specific",
    "data",
    "set",
    "data",
    "set",
    "pretty",
    "much",
    "important",
    "probably",
    "research",
    "papers",
    "also",
    "probably",
    "people",
    "come",
    "algorithm",
    "usually",
    "take",
    "take",
    "thing",
    "right",
    "particular",
    "problem",
    "statement",
    "talk",
    "classification",
    "problem",
    "statement",
    "okay",
    "worry",
    "also",
    "help",
    "explain",
    "also",
    "explain",
    "regression",
    "also",
    "decision",
    "tree",
    "regression",
    "definitely",
    "work",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "understand",
    "suppose",
    "specific",
    "problem",
    "statement",
    "solve",
    "output",
    "feature",
    "play",
    "tennis",
    "yes",
    "okay",
    "whether",
    "person",
    "going",
    "pay",
    "tennis",
    "yesterday",
    "yesterday",
    "whenever",
    "want",
    "input",
    "features",
    "like",
    "outlook",
    "temperature",
    "humidity",
    "wind",
    "person",
    "going",
    "play",
    "tennis",
    "model",
    "predict",
    "help",
    "decision",
    "tree",
    "decision",
    "tree",
    "work",
    "particular",
    "case",
    "first",
    "let",
    "consider",
    "specific",
    "uh",
    "feature",
    "let",
    "say",
    "outlook",
    "feature",
    "first",
    "feature",
    "specifically",
    "outlook",
    "tell",
    "many",
    "basically",
    "many",
    "basically",
    "yes",
    "case",
    "outlook",
    "able",
    "find",
    "nine",
    "yes",
    "see",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "many",
    "nos",
    "1",
    "2",
    "3",
    "4",
    "5",
    "think",
    "1",
    "2",
    "3",
    "4",
    "5",
    "nine",
    "yes",
    "five",
    "nos",
    "going",
    "specific",
    "thing",
    "n9",
    "yes",
    "five",
    "nos",
    "first",
    "node",
    "actually",
    "taken",
    "basically",
    "outlook",
    "outlook",
    "feature",
    "try",
    "find",
    "focusing",
    "specific",
    "feature",
    "feature",
    "many",
    "categories",
    "one",
    "sunny",
    "category",
    "see",
    "sunny",
    "one",
    "category",
    "another",
    "category",
    "called",
    "overcast",
    "another",
    "category",
    "rain",
    "three",
    "unique",
    "categories",
    "based",
    "three",
    "categories",
    "try",
    "create",
    "three",
    "nodes",
    "one",
    "node",
    "second",
    "node",
    "third",
    "node",
    "three",
    "categories",
    "category",
    "basically",
    "called",
    "sunny",
    "category",
    "basically",
    "called",
    "overcast",
    "category",
    "basically",
    "called",
    "rain",
    "based",
    "three",
    "categories",
    "splitting",
    "go",
    "ahead",
    "see",
    "sunny",
    "many",
    "yes",
    "many",
    "many",
    "yes",
    "respect",
    "sunny",
    "see",
    "sunny",
    "two",
    "nos",
    "see",
    "one",
    "two",
    "uh",
    "one",
    "three",
    "nos",
    "see",
    "one",
    "two",
    "three",
    "yes",
    "two",
    "one",
    "one",
    "many",
    "total",
    "number",
    "yes",
    "see",
    "1",
    "2",
    "2",
    "yes",
    "three",
    "let",
    "say",
    "randomly",
    "selected",
    "one",
    "feature",
    "outlook",
    "ca",
    "like",
    "see",
    "decision",
    "tree",
    "select",
    "feature",
    "specifically",
    "taken",
    "outlook",
    "later",
    "explain",
    "basically",
    "select",
    "selects",
    "feature",
    "okay",
    "talk",
    "worry",
    "outlook",
    "two",
    "yes",
    "sorry",
    "case",
    "sunny",
    "two",
    "yes",
    "three",
    "nos",
    "next",
    "thing",
    "let",
    "go",
    "see",
    "overcast",
    "overcast",
    "1",
    "yes",
    "uh",
    "2s",
    "um",
    "3s",
    "4",
    "yes",
    "overcast",
    "thing",
    "four",
    "yes",
    "zer",
    "nos",
    "finally",
    "go",
    "rain",
    "part",
    "see",
    "rain",
    "many",
    "features",
    "rain",
    "go",
    "probably",
    "see",
    "many",
    "number",
    "yes",
    "nos",
    "go",
    "see",
    "one",
    "one",
    "yes",
    "row",
    "rain",
    "two",
    "yes",
    "one",
    "one",
    "yes",
    "one",
    "right",
    "basically",
    "say",
    "rain",
    "case",
    "rain",
    "take",
    "example",
    "many",
    "number",
    "yes",
    "nos",
    "3",
    "yes",
    "two",
    "nos",
    "understand",
    "understanding",
    "algorithm",
    "everything",
    "able",
    "understand",
    "let",
    "go",
    "ahead",
    "try",
    "cease",
    "sunny",
    "sunny",
    "definitely",
    "2",
    "yes",
    "three",
    "nos",
    "four",
    "yes",
    "zero",
    "nos",
    "three",
    "two",
    "nos",
    "probably",
    "take",
    "overcast",
    "need",
    "understand",
    "understand",
    "two",
    "things",
    "one",
    "pure",
    "split",
    "one",
    "impure",
    "split",
    "pure",
    "split",
    "basically",
    "mean",
    "pure",
    "spit",
    "basically",
    "means",
    "see",
    "particular",
    "scenario",
    "overcast",
    "overcast",
    "either",
    "yes",
    "see",
    "four",
    "yes",
    "zer",
    "nos",
    "basically",
    "means",
    "pure",
    "split",
    "anybody",
    "tomorrow",
    "data",
    "set",
    "take",
    "outlook",
    "feature",
    "suppose",
    "one",
    "day",
    "day",
    "15",
    "outlook",
    "outlook",
    "basically",
    "overcast",
    "know",
    "directly",
    "person",
    "going",
    "play",
    "part",
    "already",
    "created",
    "node",
    "called",
    "pure",
    "node",
    "understand",
    "called",
    "pure",
    "node",
    "either",
    "yes",
    "zeros",
    "nos",
    "zero",
    "yes",
    "nos",
    "like",
    "particular",
    "case",
    "yes",
    "take",
    "specific",
    "path",
    "know",
    "respect",
    "overcast",
    "final",
    "decision",
    "yes",
    "always",
    "going",
    "become",
    "yes",
    "basically",
    "says",
    "split",
    "probably",
    "split",
    "definitely",
    "split",
    "require",
    "pure",
    "leaf",
    "node",
    "okay",
    "also",
    "say",
    "pure",
    "leaf",
    "node",
    "going",
    "mention",
    "one",
    "specifically",
    "talking",
    "let",
    "talk",
    "sunny",
    "case",
    "sunny",
    "two",
    "yes",
    "three",
    "nos",
    "obviously",
    "impure",
    "take",
    "next",
    "feature",
    "calculate",
    "feature",
    "take",
    "next",
    "discuss",
    "let",
    "say",
    "take",
    "temperature",
    "take",
    "temperature",
    "start",
    "splitting",
    "since",
    "impure",
    "okay",
    "split",
    "happen",
    "get",
    "finally",
    "pure",
    "split",
    "similarly",
    "respect",
    "rain",
    "go",
    "ahead",
    "take",
    "another",
    "feature",
    "keep",
    "splitting",
    "unless",
    "get",
    "leaf",
    "node",
    "completely",
    "pure",
    "hope",
    "understood",
    "exactly",
    "work",
    "two",
    "questions",
    "two",
    "questions",
    "kish",
    "first",
    "thing",
    "calculate",
    "purity",
    "come",
    "know",
    "pure",
    "split",
    "seeing",
    "definitely",
    "say",
    "definitely",
    "say",
    "seeing",
    "many",
    "number",
    "yes",
    "nos",
    "based",
    "def",
    "itely",
    "say",
    "pure",
    "split",
    "use",
    "two",
    "different",
    "things",
    "one",
    "entropy",
    "one",
    "something",
    "called",
    "guine",
    "coefficient",
    "try",
    "understand",
    "entropy",
    "work",
    "guinea",
    "coefficient",
    "work",
    "decision",
    "tree",
    "help",
    "us",
    "determine",
    "whether",
    "split",
    "pure",
    "split",
    "whether",
    "node",
    "leaf",
    "node",
    "coming",
    "second",
    "thing",
    "okay",
    "coming",
    "second",
    "thing",
    "one",
    "respect",
    "purity",
    "second",
    "thing",
    "first",
    "important",
    "question",
    "asked",
    "probably",
    "select",
    "outlook",
    "features",
    "selected",
    "topic",
    "called",
    "information",
    "gain",
    "know",
    "problem",
    "solved",
    "let",
    "go",
    "ahead",
    "let",
    "understand",
    "entropy",
    "guinea",
    "coefficient",
    "information",
    "gain",
    "entropy",
    "guine",
    "coefficient",
    "oh",
    "sorry",
    "guinea",
    "coefficient",
    "saying",
    "guine",
    "impurity",
    "also",
    "say",
    "write",
    "guine",
    "impurity",
    "coefficient",
    "also",
    "say",
    "guinea",
    "impurity",
    "hope",
    "everybody",
    "understood",
    "till",
    "let",
    "go",
    "ahead",
    "let",
    "discuss",
    "first",
    "thing",
    "entropy",
    "entropy",
    "work",
    "going",
    "use",
    "formula",
    "entropy",
    "write",
    "guine",
    "going",
    "discuss",
    "things",
    "let",
    "say",
    "entropy",
    "formula",
    "given",
    "write",
    "h",
    "equal",
    "h",
    "equal",
    "minus",
    "p",
    "plus",
    "talk",
    "minus",
    "p",
    "plus",
    "log",
    "base",
    "2",
    "p",
    "p",
    "minus",
    "log",
    "base",
    "2",
    "p",
    "minus",
    "formula",
    "guine",
    "impurity",
    "formula",
    "1",
    "minus",
    "summation",
    "equal",
    "1",
    "2",
    "n",
    "p²",
    "even",
    "talk",
    "use",
    "guine",
    "impurity",
    "use",
    "guine",
    "impurity",
    "use",
    "entropy",
    "know",
    "default",
    "decision",
    "tree",
    "regression",
    "classific",
    "sorry",
    "decision",
    "tree",
    "classification",
    "uses",
    "guinea",
    "impurity",
    "let",
    "take",
    "one",
    "specific",
    "example",
    "example",
    "feature",
    "one",
    "root",
    "node",
    "feature",
    "one",
    "root",
    "node",
    "let",
    "say",
    "root",
    "node",
    "six",
    "yes",
    "three",
    "nos",
    "simple",
    "let",
    "say",
    "two",
    "categories",
    "based",
    "two",
    "categories",
    "split",
    "happened",
    "c1",
    "let",
    "say",
    "3",
    "s3",
    "nos",
    "3",
    "s0",
    "nos",
    "second",
    "category",
    "always",
    "understand",
    "sumission",
    "3s",
    "3s",
    "6s",
    "see",
    "sumission",
    "3",
    "3",
    "obviously",
    "6",
    "3",
    "0",
    "obviously",
    "need",
    "understand",
    "based",
    "number",
    "root",
    "nodes",
    "almost",
    "let",
    "go",
    "ahead",
    "let",
    "understand",
    "cal",
    "calculate",
    "let",
    "take",
    "example",
    "calculate",
    "entropy",
    "already",
    "shown",
    "entropy",
    "formula",
    "let",
    "understand",
    "components",
    "write",
    "h",
    "equal",
    "minus",
    "sign",
    "basically",
    "means",
    "probability",
    "yes",
    "probability",
    "yes",
    "simple",
    "thing",
    "probability",
    "yes",
    "yes",
    "obviously",
    "write",
    "want",
    "find",
    "probability",
    "yes",
    "see",
    "say",
    "plus",
    "basically",
    "means",
    "yes",
    "say",
    "minus",
    "basically",
    "means",
    "probability",
    "yes",
    "nothing",
    "yes",
    "plus",
    "minus",
    "specifically",
    "binary",
    "class",
    "positive",
    "negative",
    "probability",
    "respect",
    "yes",
    "write",
    "3x",
    "3",
    "probability",
    "total",
    "number",
    "3x3",
    "similarly",
    "go",
    "see",
    "next",
    "term",
    "log",
    "base",
    "2",
    "go",
    "ahead",
    "write",
    "log",
    "base",
    "2",
    "3x3",
    "minus",
    "p",
    "minus",
    "p",
    "minus",
    "0",
    "3",
    "log",
    "base",
    "2",
    "0",
    "3",
    "obviously",
    "become",
    "zero",
    "obviously",
    "become",
    "0",
    "0",
    "divid",
    "anything",
    "zero",
    "1",
    "log",
    "base",
    "1",
    "nothing",
    "zero",
    "log",
    "base",
    "1",
    "nothing",
    "zero",
    "tell",
    "whether",
    "pure",
    "split",
    "impure",
    "split",
    "pure",
    "split",
    "whenever",
    "pure",
    "split",
    "answer",
    "entropy",
    "going",
    "come",
    "zero",
    "going",
    "define",
    "one",
    "graph",
    "h",
    "let",
    "say",
    "p",
    "minus",
    "probability",
    "plus",
    "see",
    "say",
    "probability",
    "plus",
    "probability",
    "minus",
    "also",
    "five",
    "right",
    "like",
    "p",
    "equal",
    "1",
    "q",
    "right",
    "p",
    "q",
    "1",
    "p",
    "thing",
    "right",
    "is5",
    "obviously",
    "h",
    "1",
    "let",
    "say",
    "graph",
    "basically",
    "get",
    "formed",
    "let",
    "go",
    "ahead",
    "try",
    "calculate",
    "entropy",
    "guys",
    "entropy",
    "node",
    "going",
    "make",
    "graph",
    "h",
    "minus",
    "nothing",
    "3x",
    "6",
    "log",
    "base",
    "2",
    "3x",
    "6",
    "minus",
    "three",
    "3x",
    "6",
    "log",
    "base",
    "2",
    "3x",
    "6",
    "compute",
    "log",
    "base",
    "2",
    "1",
    "calculation",
    "actually",
    "going",
    "get",
    "one",
    "getting",
    "one",
    "actually",
    "getting",
    "one",
    "three",
    "yes",
    "three",
    "nos",
    "probability",
    "right",
    "is5",
    "basically",
    "means",
    "h",
    "coming",
    "one",
    "graph",
    "see",
    "getting",
    "one",
    "zero",
    "one",
    "zero",
    "one",
    "hope",
    "everybody",
    "able",
    "understand",
    "guys",
    "0o",
    "one",
    "zero",
    "one",
    "basically",
    "means",
    "becomes",
    "pure",
    "split",
    "h",
    "going",
    "get",
    "zero",
    "always",
    "understand",
    "entropy",
    "0",
    "1",
    "impure",
    "completely",
    "impure",
    "split",
    "50",
    "probability",
    "getting",
    "yes",
    "50",
    "probability",
    "getting",
    "h",
    "ofs",
    "entropy",
    "entropy",
    "sample",
    "h",
    "ofs",
    "notation",
    "using",
    "h",
    "ofs",
    "whenever",
    "split",
    "happening",
    "first",
    "thing",
    "done",
    "purity",
    "test",
    "purity",
    "test",
    "done",
    "help",
    "entropy",
    "right",
    "also",
    "show",
    "guinea",
    "guinea",
    "impurity",
    "worry",
    "entropy",
    "able",
    "find",
    "getting",
    "one",
    "basically",
    "means",
    "impure",
    "split",
    "getting",
    "zero",
    "pure",
    "split",
    "graph",
    "okay",
    "graph",
    "graph",
    "basically",
    "entropy",
    "graph",
    "understand",
    "probability",
    "getting",
    "yes",
    "basically",
    "means",
    "3s",
    "three",
    "nos",
    "entropy",
    "going",
    "1",
    "h",
    "probability",
    "completely",
    "one",
    "basically",
    "means",
    "either",
    "getting",
    "completely",
    "yes",
    "completely",
    "entropy",
    "zero",
    "basically",
    "means",
    "pure",
    "split",
    "case",
    "probability",
    "getting",
    "plus",
    "one",
    "keep",
    "reducing",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "understand",
    "understood",
    "purity",
    "test",
    "definitely",
    "use",
    "entropy",
    "try",
    "find",
    "whether",
    "pure",
    "impure",
    "impure",
    "go",
    "ahead",
    "shift",
    "division",
    "categories",
    "take",
    "another",
    "feature",
    "divide",
    "two",
    "split",
    "split",
    "getting",
    "6",
    "6",
    "specific",
    "value",
    "probably",
    "go",
    "draw",
    "entropy",
    "probability",
    "go",
    "create",
    "may",
    "be0",
    "4",
    "or3",
    "something",
    "like",
    "0",
    "1",
    "let",
    "go",
    "ahead",
    "discuss",
    "second",
    "issue",
    "hope",
    "everybody",
    "discussed",
    "discussed",
    "checking",
    "pure",
    "split",
    "understood",
    "much",
    "next",
    "thing",
    "okay",
    "fine",
    "chish",
    "good",
    "explained",
    "well",
    "know",
    "many",
    "people",
    "say",
    "people",
    "ca",
    "help",
    "let",
    "say",
    "features",
    "okay",
    "coming",
    "second",
    "problem",
    "consider",
    "node",
    "cap",
    "feature",
    "take",
    "split",
    "may",
    "one",
    "one",
    "split",
    "let",
    "see",
    "second",
    "problem",
    "feature",
    "take",
    "split",
    "right",
    "second",
    "problem",
    "trying",
    "solve",
    "let",
    "say",
    "one",
    "feature",
    "one",
    "two",
    "categories",
    "let",
    "say",
    "c1",
    "c2",
    "let",
    "say",
    "9",
    "years",
    "5",
    "nos",
    "6",
    "years",
    "2",
    "nos",
    "basically",
    "three",
    "yes",
    "three",
    "nos",
    "let",
    "say",
    "data",
    "set",
    "features",
    "like",
    "f1",
    "fs2",
    "f3",
    "let",
    "say",
    "another",
    "split",
    "actually",
    "start",
    "feature",
    "two",
    "also",
    "feature",
    "two",
    "may",
    "probably",
    "three",
    "categories",
    "like",
    "c1",
    "c2",
    "c3",
    "respect",
    "root",
    "node",
    "features",
    "also",
    "may",
    "split",
    "right",
    "may",
    "take",
    "another",
    "feature",
    "keep",
    "splitting",
    "right",
    "based",
    "pure",
    "impure",
    "split",
    "decide",
    "take",
    "fub1",
    "first",
    "f2",
    "first",
    "f3",
    "first",
    "feature",
    "first",
    "decide",
    "feature",
    "take",
    "probably",
    "split",
    "major",
    "question",
    "specifically",
    "use",
    "something",
    "called",
    "information",
    "gain",
    "going",
    "say",
    "basically",
    "use",
    "information",
    "gain",
    "information",
    "gain",
    "talk",
    "information",
    "gain",
    "first",
    "write",
    "formula",
    "basically",
    "write",
    "gain",
    "sample",
    "first",
    "feature",
    "one",
    "compute",
    "first",
    "feature",
    "one",
    "compute",
    "suppose",
    "first",
    "split",
    "data",
    "probably",
    "computing",
    "written",
    "h",
    "discuss",
    "every",
    "parameter",
    "worry",
    "summation",
    "v",
    "belong",
    "values",
    "v",
    "worry",
    "guys",
    "understood",
    "formula",
    "explain",
    "sample",
    "size",
    "h",
    "sv",
    "discuss",
    "every",
    "parameter",
    "let",
    "say",
    "taking",
    "feature",
    "one",
    "split",
    "already",
    "seen",
    "feature",
    "one",
    "feature",
    "one",
    "two",
    "categories",
    "c1",
    "c2",
    "9",
    "yes",
    "5",
    "nos",
    "6s",
    "two",
    "nos",
    "3",
    "yes",
    "three",
    "nos",
    "try",
    "calculate",
    "information",
    "gain",
    "specific",
    "split",
    "go",
    "ahead",
    "probably",
    "take",
    "see",
    "try",
    "understand",
    "want",
    "compute",
    "gain",
    "f1",
    "first",
    "first",
    "first",
    "thing",
    "need",
    "find",
    "h",
    "h",
    "specifically",
    "root",
    "node",
    "need",
    "first",
    "calculate",
    "h",
    "h",
    "ofs",
    "nothing",
    "entropy",
    "entropy",
    "root",
    "node",
    "want",
    "compute",
    "entropy",
    "node",
    "node",
    "tell",
    "compute",
    "h",
    "equal",
    "minus",
    "p",
    "log",
    "base",
    "2",
    "calculate",
    "guys",
    "along",
    "p",
    "minus",
    "log",
    "base",
    "p",
    "minus",
    "hope",
    "everybody",
    "knows",
    "going",
    "compute",
    "ability",
    "plus",
    "specific",
    "root",
    "node",
    "nothing",
    "9",
    "by4",
    "log",
    "base",
    "2",
    "9",
    "by4",
    "p",
    "minus",
    "p",
    "minus",
    "5x4",
    "log",
    "base",
    "2",
    "5",
    "by4",
    "calculation",
    "probably",
    "get",
    "94",
    "approximately",
    "equal",
    "94",
    "check",
    "whether",
    "getting",
    "use",
    "calculator",
    "want",
    "definitely",
    "found",
    "specifically",
    "root",
    "node",
    "let",
    "see",
    "next",
    "thing",
    "next",
    "important",
    "thing",
    "part",
    "v",
    "h",
    "sv",
    "important",
    "look",
    "everybody",
    "see",
    "graph",
    "okay",
    "see",
    "graph",
    "talk",
    "h",
    "sv",
    "first",
    "talk",
    "h",
    "sv",
    "okay",
    "one",
    "entropy",
    "category",
    "one",
    "need",
    "find",
    "entropy",
    "category",
    "2",
    "need",
    "find",
    "write",
    "h",
    "sv",
    "category",
    "1",
    "category",
    "1",
    "write",
    "sc1",
    "let",
    "say",
    "going",
    "write",
    "like",
    "quickly",
    "calculate",
    "h",
    "sv",
    "separately",
    "need",
    "calculate",
    "h",
    "sv",
    "c1",
    "okay",
    "write",
    "6x",
    "8",
    "log",
    "base",
    "2",
    "6x",
    "8us",
    "2x",
    "8",
    "log",
    "base",
    "2x",
    "8",
    "hope",
    "everybody",
    "knows",
    "got",
    "h",
    "sv",
    "basically",
    "means",
    "going",
    "compute",
    "entropy",
    "category",
    "category",
    "basically",
    "write",
    "h",
    "write",
    "6",
    "by8",
    "log",
    "base",
    "2",
    "6x",
    "8",
    "2x",
    "8",
    "log",
    "base",
    "2",
    "2x",
    "8",
    "get",
    "actually",
    "going",
    "get",
    "81",
    "similarly",
    "calculate",
    "h",
    "c2",
    "quickly",
    "calculate",
    "much",
    "going",
    "get",
    "guys",
    "6x",
    "8",
    "6x",
    "8",
    "respect",
    "need",
    "find",
    "values",
    "start",
    "equating",
    "equation",
    "finally",
    "gain",
    "comma",
    "fub1",
    "let",
    "say",
    "going",
    "basically",
    "add",
    "94",
    "minus",
    "see",
    "minus",
    "summation",
    "okay",
    "summation",
    "v",
    "understand",
    "v",
    "basically",
    "means",
    "many",
    "samples",
    "let",
    "say",
    "category",
    "one",
    "many",
    "samples",
    "category",
    "one",
    "simple",
    "really",
    "want",
    "calculate",
    "nothing",
    "eight",
    "total",
    "number",
    "sample",
    "much",
    "go",
    "see",
    "9",
    "years",
    "five",
    "nos",
    "okay",
    "9",
    "years",
    "five",
    "nos",
    "basically",
    "means",
    "14",
    "total",
    "sample",
    "eight",
    "sample",
    "okay",
    "become",
    "8x4",
    "multiply",
    "see",
    "see",
    "equation",
    "multiply",
    "h",
    "sv",
    "h",
    "sv",
    "nothing",
    "entropy",
    "category",
    "1",
    "entropy",
    "category",
    "1",
    "nothing",
    "81",
    "plus",
    "go",
    "back",
    "graph",
    "try",
    "see",
    "c2",
    "much",
    "many",
    "total",
    "number",
    "samples",
    "3",
    "3",
    "6",
    "6",
    "14",
    "become",
    "multiplied",
    "1",
    "right",
    "entire",
    "thing",
    "calculation",
    "going",
    "get",
    "gain",
    "comma",
    "f1",
    "got",
    "value",
    "amazing",
    "feature",
    "one",
    "feature",
    "two",
    "let",
    "say",
    "split",
    "feature",
    "two",
    "suppose",
    "get",
    "gain",
    "comma",
    "feature",
    "2",
    "get",
    "tell",
    "using",
    "feature",
    "start",
    "splitting",
    "first",
    "whether",
    "fub1",
    "whether",
    "fs2",
    "based",
    "value",
    "know",
    "gain",
    "information",
    "gain",
    "comma",
    "f2",
    "greater",
    "gain",
    "comma",
    "fub1",
    "answer",
    "much",
    "simple",
    "definitely",
    "use",
    "feature",
    "2",
    "start",
    "split",
    "thing",
    "trying",
    "understand",
    "really",
    "want",
    "select",
    "feature",
    "select",
    "start",
    "splitting",
    "basically",
    "calculate",
    "information",
    "gain",
    "go",
    "throughout",
    "paths",
    "whichever",
    "path",
    "highest",
    "information",
    "gain",
    "select",
    "specific",
    "thing",
    "question",
    "rises",
    "kish",
    "obviously",
    "good",
    "written",
    "guinea",
    "impurity",
    "purpose",
    "please",
    "explain",
    "us",
    "guinea",
    "impurity",
    "basically",
    "used",
    "let",
    "go",
    "ahead",
    "guine",
    "impurity",
    "told",
    "yes",
    "obviously",
    "use",
    "obviously",
    "use",
    "entropy",
    "guinea",
    "impurity",
    "guine",
    "impurity",
    "formula",
    "specifically",
    "written",
    "1",
    "minus",
    "summation",
    "ial",
    "1",
    "2",
    "n",
    "p²",
    "p²",
    "suppose",
    "let",
    "say",
    "n",
    "n",
    "number",
    "outputs",
    "right",
    "many",
    "outputs",
    "two",
    "outputs",
    "yes",
    "expand",
    "1",
    "minus",
    "since",
    "summation",
    "equal",
    "1",
    "n",
    "basically",
    "going",
    "basically",
    "say",
    "okay",
    "fine",
    "write",
    "probability",
    "plus",
    "whole",
    "square",
    "uh",
    "plus",
    "probability",
    "minus",
    "whole",
    "square",
    "formula",
    "guinea",
    "impurity",
    "may",
    "thinking",
    "okay",
    "fine",
    "calculation",
    "obviously",
    "much",
    "equal",
    "easy",
    "right",
    "suppose",
    "node",
    "sorry",
    "node",
    "2",
    "yes",
    "two",
    "nos",
    "particular",
    "case",
    "calculate",
    "probability",
    "two",
    "yes",
    "two",
    "nos",
    "suppose",
    "let",
    "say",
    "node",
    "split",
    "two",
    "yes",
    "two",
    "calculate",
    "write",
    "1",
    "minus",
    "probability",
    "square",
    "1x",
    "2",
    "square",
    "sorry",
    "1",
    "two",
    "yeah",
    "1x",
    "2",
    "squ",
    "1",
    "2",
    "squ",
    "right",
    "say",
    "1",
    "1x",
    "4",
    "1x",
    "4",
    "nothing",
    "2x",
    "4",
    "nothing",
    "1x",
    "2",
    "getting",
    "understand",
    "complete",
    "impure",
    "split",
    "right",
    "impure",
    "split",
    "entropy",
    "output",
    "getting",
    "one",
    "whereas",
    "case",
    "guinea",
    "impurity",
    "z",
    "sorry",
    "go",
    "ahead",
    "graph",
    "probably",
    "created",
    "guinea",
    "impurity",
    "line",
    "look",
    "something",
    "like",
    "looking",
    "something",
    "like",
    "zero",
    "obviously",
    "getting",
    "zero",
    "whenever",
    "probability",
    "plus",
    "going",
    "get",
    "difference",
    "guinea",
    "impurity",
    "entropy",
    "may",
    "seeing",
    "kish",
    "use",
    "let",
    "understand",
    "use",
    "guinea",
    "use",
    "entropy",
    "tell",
    "guys",
    "consider",
    "formula",
    "guine",
    "impurity",
    "probably",
    "consider",
    "consider",
    "entropy",
    "formula",
    "think",
    "time",
    "take",
    "execution",
    "particular",
    "formula",
    "whether",
    "entropy",
    "take",
    "guinea",
    "impurity",
    "take",
    "time",
    "probably",
    "take",
    "execution",
    "purpose",
    "see",
    "understand",
    "decision",
    "tree",
    "worst",
    "time",
    "complexity",
    "100",
    "features",
    "probably",
    "keep",
    "comparing",
    "dividing",
    "many",
    "many",
    "features",
    "probably",
    "compute",
    "information",
    "gain",
    "like",
    "100",
    "features",
    "faster",
    "entrop",
    "guine",
    "impurity",
    "understand",
    "entropy",
    "log",
    "function",
    "log",
    "function",
    "simple",
    "maths",
    "amount",
    "time",
    "entropy",
    "guine",
    "impurity",
    "amount",
    "time",
    "basically",
    "taken",
    "entropy",
    "huge",
    "number",
    "features",
    "like",
    "100",
    "200",
    "features",
    "planning",
    "apply",
    "decision",
    "tre",
    "would",
    "suggest",
    "try",
    "use",
    "guinea",
    "impurity",
    "entropy",
    "small",
    "set",
    "features",
    "go",
    "ahead",
    "entropy",
    "definitely",
    "respect",
    "fast",
    "guinea",
    "greater",
    "entropy",
    "let",
    "go",
    "ahead",
    "understand",
    "respect",
    "may",
    "thinking",
    "kish",
    "okay",
    "fine",
    "basically",
    "explained",
    "us",
    "categorical",
    "variables",
    "see",
    "explained",
    "categorical",
    "variables",
    "numerical",
    "feature",
    "let",
    "say",
    "f1",
    "numerical",
    "feature",
    "f1",
    "feature",
    "numerical",
    "feature",
    "may",
    "values",
    "let",
    "say",
    "sorted",
    "values",
    "okay",
    "let",
    "say",
    "f1",
    "output",
    "okay",
    "f1",
    "let",
    "say",
    "values",
    "like",
    "ass",
    "sorted",
    "order",
    "values",
    "sorting",
    "features",
    "basically",
    "let",
    "say",
    "initially",
    "features",
    "like",
    "let",
    "say",
    "values",
    "like",
    "4",
    "5",
    "7",
    "3",
    "let",
    "say",
    "features",
    "continuous",
    "feature",
    "continuous",
    "feature",
    "continuous",
    "feature",
    "probably",
    "decision",
    "tree",
    "entropy",
    "calculated",
    "information",
    "gain",
    "get",
    "calculated",
    "able",
    "see",
    "first",
    "sort",
    "values",
    "f1",
    "decision",
    "tree",
    "b",
    "basically",
    "first",
    "sort",
    "values",
    "four",
    "three",
    "three",
    "four",
    "five",
    "six",
    "whenever",
    "continuous",
    "feature",
    "continuous",
    "feature",
    "basically",
    "work",
    "case",
    "first",
    "decision",
    "tree",
    "node",
    "say",
    "take",
    "one",
    "one",
    "first",
    "record",
    "say",
    "less",
    "equal",
    "okay",
    "less",
    "equal",
    "getting",
    "two",
    "branches",
    "yes",
    "yes",
    "definitely",
    "output",
    "put",
    "right",
    "another",
    "node",
    "many",
    "number",
    "records",
    "particular",
    "case",
    "one",
    "record",
    "particular",
    "case",
    "around",
    "five",
    "six",
    "records",
    "also",
    "able",
    "see",
    "right",
    "many",
    "yes",
    "nos",
    "definitely",
    "leaf",
    "node",
    "first",
    "instance",
    "go",
    "ahead",
    "calculate",
    "information",
    "gain",
    "probably",
    "information",
    "gain",
    "got",
    "take",
    "first",
    "two",
    "records",
    "create",
    "new",
    "decision",
    "tree",
    "let",
    "say",
    "suggestion",
    "say",
    "less",
    "equal",
    "get",
    "one",
    "one",
    "two",
    "records",
    "basically",
    "say",
    "many",
    "yes",
    "remaining",
    "records",
    "come",
    "information",
    "gain",
    "computed",
    "happen",
    "go",
    "next",
    "record",
    "create",
    "another",
    "feature",
    "say",
    "less",
    "equal",
    "three",
    "create",
    "many",
    "nodes",
    "try",
    "understand",
    "many",
    "yes",
    "compute",
    "information",
    "gain",
    "like",
    "every",
    "record",
    "finally",
    "whichever",
    "information",
    "gain",
    "higher",
    "select",
    "specific",
    "value",
    "feature",
    "split",
    "node",
    "continuous",
    "feature",
    "whenever",
    "continuous",
    "feature",
    "basically",
    "try",
    "compute",
    "highest",
    "information",
    "gain",
    "best",
    "information",
    "gain",
    "get",
    "selected",
    "splitting",
    "happen",
    "let",
    "go",
    "ahead",
    "understand",
    "next",
    "topic",
    "entirely",
    "things",
    "work",
    "decision",
    "tree",
    "regressor",
    "decision",
    "tree",
    "regressor",
    "output",
    "continuous",
    "variable",
    "suppose",
    "one",
    "feature",
    "one",
    "feature",
    "two",
    "output",
    "continuous",
    "feature",
    "continuous",
    "value",
    "particular",
    "case",
    "split",
    "let",
    "say",
    "f1c",
    "feature",
    "getting",
    "selected",
    "f1c",
    "feature",
    "value",
    "come",
    "getting",
    "selected",
    "first",
    "entire",
    "mean",
    "get",
    "calculated",
    "output",
    "mean",
    "get",
    "calculated",
    "mean",
    "cost",
    "function",
    "used",
    "guinea",
    "coefficient",
    "guinea",
    "impurity",
    "entropy",
    "use",
    "mean",
    "squared",
    "error",
    "also",
    "use",
    "mean",
    "absolute",
    "error",
    "mean",
    "squared",
    "error",
    "remember",
    "logistic",
    "linear",
    "regression",
    "calculate",
    "1",
    "2",
    "summation",
    "1",
    "n",
    "hat",
    "minus",
    "whole",
    "square",
    "hat",
    "whole",
    "square",
    "mean",
    "square",
    "error",
    "first",
    "based",
    "f1",
    "feature",
    "try",
    "assign",
    "mean",
    "value",
    "compute",
    "mse",
    "value",
    "go",
    "ahead",
    "splitting",
    "splitting",
    "based",
    "categories",
    "continuous",
    "variable",
    "different",
    "different",
    "categories",
    "categories",
    "happen",
    "split",
    "records",
    "go",
    "mean",
    "value",
    "output",
    "msc",
    "get",
    "calculated",
    "msse",
    "gets",
    "reduced",
    "basically",
    "means",
    "reaching",
    "near",
    "leaf",
    "note",
    "thing",
    "happen",
    "finally",
    "follow",
    "path",
    "whatever",
    "mean",
    "value",
    "present",
    "output",
    "difference",
    "decision",
    "tree",
    "regressor",
    "classifier",
    "instead",
    "using",
    "entropy",
    "use",
    "mean",
    "squar",
    "error",
    "mean",
    "absolute",
    "error",
    "formula",
    "mean",
    "square",
    "error",
    "let",
    "go",
    "one",
    "topic",
    "called",
    "hyperparameters",
    "tell",
    "decision",
    "tree",
    "keep",
    "growing",
    "depth",
    "kind",
    "problem",
    "face",
    "regressor",
    "part",
    "want",
    "explain",
    "okay",
    "let",
    "see",
    "okay",
    "let",
    "let",
    "regression",
    "decision",
    "tree",
    "regressor",
    "let",
    "say",
    "feature",
    "f1",
    "output",
    "let",
    "say",
    "values",
    "like",
    "20",
    "24",
    "26",
    "28",
    "30",
    "feature",
    "one",
    "category",
    "one",
    "category",
    "one",
    "let",
    "say",
    "categories",
    "let",
    "say",
    "done",
    "division",
    "f1",
    "feature",
    "initially",
    "tell",
    "mean",
    "mean",
    "value",
    "get",
    "assigned",
    "using",
    "msse",
    "mean",
    "squar",
    "error",
    "try",
    "calculate",
    "suppose",
    "get",
    "msse",
    "37",
    "47",
    "something",
    "like",
    "try",
    "split",
    "getting",
    "two",
    "nodes",
    "three",
    "nodes",
    "depends",
    "specific",
    "nodes",
    "part",
    "mean",
    "change",
    "mean",
    "change",
    "suppose",
    "two",
    "two",
    "records",
    "goes",
    "right",
    "mc",
    "get",
    "calculated",
    "taking",
    "example",
    "try",
    "assume",
    "thing",
    "talk",
    "hyper",
    "parameters",
    "see",
    "formula",
    "gets",
    "applied",
    "msc",
    "let",
    "see",
    "hyper",
    "parameter",
    "always",
    "understand",
    "decision",
    "tree",
    "leads",
    "overfitting",
    "going",
    "divide",
    "nodes",
    "whatever",
    "level",
    "want",
    "obviously",
    "lead",
    "overfitting",
    "order",
    "prevent",
    "overfitting",
    "perform",
    "two",
    "important",
    "steps",
    "one",
    "post",
    "pruning",
    "one",
    "pruning",
    "two",
    "post",
    "pruning",
    "pre",
    "pruning",
    "condition",
    "let",
    "say",
    "done",
    "splits",
    "done",
    "splits",
    "let",
    "say",
    "seven",
    "yes",
    "two",
    "probably",
    "split",
    "like",
    "particular",
    "scenario",
    "know",
    "7",
    "yes",
    "two",
    "nos",
    "maximum",
    "80",
    "chances",
    "node",
    "saying",
    "output",
    "yes",
    "pruning",
    "answer",
    "close",
    "cut",
    "branch",
    "technique",
    "basically",
    "called",
    "post",
    "pruning",
    "basically",
    "means",
    "first",
    "create",
    "decision",
    "tree",
    "probably",
    "see",
    "decision",
    "tree",
    "see",
    "whether",
    "extra",
    "branch",
    "try",
    "cut",
    "one",
    "thing",
    "called",
    "decided",
    "hyperparameters",
    "kind",
    "hyper",
    "parameters",
    "basically",
    "say",
    "many",
    "number",
    "decision",
    "tree",
    "needs",
    "used",
    "number",
    "decision",
    "tree",
    "sorry",
    "may",
    "say",
    "max",
    "depth",
    "max",
    "depth",
    "many",
    "max",
    "leaf",
    "parameters",
    "set",
    "grid",
    "se",
    "cv",
    "try",
    "basically",
    "come",
    "pruning",
    "technique",
    "idea",
    "decision",
    "tree",
    "uh",
    "regressor",
    "yes",
    "yes",
    "possible",
    "guinea",
    "value",
    "one",
    "graph",
    "guinea",
    "value",
    "talking",
    "guinea",
    "entropy",
    "one",
    "always",
    "0",
    "first",
    "thing",
    "first",
    "usual",
    "import",
    "libraries",
    "go",
    "ahead",
    "import",
    "librar",
    "say",
    "import",
    "pandas",
    "np",
    "pd",
    "import",
    "matplot",
    "li",
    "pyplot",
    "plt",
    "uh",
    "import",
    "basic",
    "things",
    "go",
    "take",
    "data",
    "set",
    "want",
    "sk",
    "learn",
    "data",
    "sets",
    "import",
    "let",
    "say",
    "going",
    "take",
    "load",
    "iris",
    "data",
    "set",
    "going",
    "upload",
    "iris",
    "data",
    "set",
    "going",
    "write",
    "load",
    "iris",
    "iris",
    "data",
    "set",
    "next",
    "step",
    "uh",
    "get",
    "iris",
    "data",
    "set",
    "iris",
    "dat",
    "okay",
    "features",
    "four",
    "features",
    "four",
    "features",
    "petal",
    "length",
    "petal",
    "width",
    "sle",
    "length",
    "sle",
    "width",
    "independent",
    "features",
    "really",
    "want",
    "apply",
    "classifier",
    "decision",
    "tree",
    "classifier",
    "first",
    "import",
    "skarn",
    "tree",
    "import",
    "decision",
    "let",
    "see",
    "decision",
    "tree",
    "present",
    "scalon",
    "decision",
    "tree",
    "classifier",
    "name",
    "absolutely",
    "fine",
    "getting",
    "got",
    "module",
    "sk",
    "okay",
    "sk",
    "skar",
    "skn",
    "learn",
    "classifier",
    "right",
    "going",
    "overfit",
    "data",
    "probably",
    "show",
    "go",
    "ahead",
    "uh",
    "pruning",
    "default",
    "parameters",
    "probably",
    "go",
    "see",
    "classifier",
    "criterion",
    "see",
    "first",
    "p",
    "parameter",
    "criterion",
    "default",
    "guinea",
    "splitter",
    "splitter",
    "basically",
    "means",
    "going",
    "split",
    "also",
    "two",
    "types",
    "best",
    "random",
    "randomly",
    "select",
    "features",
    "okay",
    "always",
    "go",
    "best",
    "max",
    "depth",
    "hyper",
    "parameter",
    "minimum",
    "sample",
    "lift",
    "hyper",
    "parameter",
    "max",
    "fe",
    "features",
    "many",
    "number",
    "features",
    "going",
    "take",
    "order",
    "fix",
    "also",
    "hyper",
    "parameter",
    "things",
    "hyper",
    "parameter",
    "okay",
    "default",
    "executed",
    "whatever",
    "giving",
    "decision",
    "tree",
    "next",
    "thing",
    "actually",
    "going",
    "create",
    "decision",
    "tree",
    "using",
    "plot",
    "fig",
    "size",
    "plot",
    "figure",
    "inside",
    "figure",
    "fix",
    "size",
    "okay",
    "probably",
    "show",
    "better",
    "figure",
    "size",
    "everybody",
    "body",
    "able",
    "see",
    "let",
    "say",
    "going",
    "take",
    "area",
    "1510",
    "probably",
    "going",
    "say",
    "tree",
    "dot",
    "plot",
    "going",
    "say",
    "classifier",
    "filled",
    "coloring",
    "filled",
    "tree",
    "sorry",
    "tre",
    "tre",
    "tre",
    "tre",
    "tre",
    "classifi",
    "tree",
    "plot",
    "okay",
    "also",
    "import",
    "uh",
    "tree",
    "basically",
    "import",
    "tree",
    "sk",
    "learn",
    "import",
    "three",
    "getting",
    "error",
    "attribute",
    "plot",
    "let",
    "see",
    "documentation",
    "guys",
    "plot",
    "function",
    "like",
    "plot",
    "uncore",
    "tree",
    "dot",
    "tab",
    "plot",
    "tree",
    "error",
    "getting",
    "okay",
    "fitted",
    "yet",
    "sorry",
    "going",
    "say",
    "classifier",
    "fit",
    "data",
    "data",
    "iris",
    "data",
    "going",
    "fit",
    "iris",
    "dot",
    "target",
    "done",
    "think",
    "get",
    "executed",
    "graph",
    "look",
    "like",
    "guys",
    "see",
    "graph",
    "looks",
    "like",
    "show",
    "graph",
    "see",
    "see",
    "amazing",
    "things",
    "three",
    "outputs",
    "actually",
    "see",
    "left",
    "hand",
    "side",
    "become",
    "leaf",
    "node",
    "first",
    "one",
    "probably",
    "vers",
    "color",
    "uh",
    "versol",
    "flower",
    "okay",
    "go",
    "right",
    "hand",
    "side",
    "see",
    "based",
    "one",
    "feature",
    "based",
    "one",
    "feature",
    "able",
    "see",
    "getting",
    "leaf",
    "node",
    "based",
    "another",
    "branch",
    "getting",
    "05050",
    "two",
    "features",
    "getting",
    "splitted",
    "495",
    "471",
    "require",
    "split",
    "anybody",
    "tell",
    "require",
    "split",
    "try",
    "think",
    "post",
    "pruning",
    "want",
    "find",
    "whether",
    "splits",
    "required",
    "particular",
    "case",
    "see",
    "require",
    "split",
    "require",
    "right",
    "basically",
    "getting",
    "47",
    "one",
    "guess",
    "also",
    "require",
    "split",
    "understand",
    "basically",
    "post",
    "pruning",
    "decide",
    "level",
    "probably",
    "gu",
    "value",
    "okay",
    "side",
    "h",
    "coming",
    "greater",
    "maximum",
    "come",
    "0",
    "come",
    "know",
    "coming",
    "667",
    "look",
    "onto",
    "guys",
    "anywhere",
    "see",
    "everywhere",
    "getting",
    "less",
    "than5",
    "plotting",
    "graph",
    "much",
    "easy",
    "use",
    "sk",
    "learn",
    "import",
    "tree",
    "basically",
    "get",
    "classify",
    "field",
    "equal",
    "true",
    "agenda",
    "let",
    "define",
    "agenda",
    "things",
    "first",
    "understand",
    "emble",
    "techniques",
    "assemble",
    "techniques",
    "basically",
    "going",
    "discuss",
    "difference",
    "bagging",
    "boosting",
    "second",
    "basically",
    "going",
    "discuss",
    "uh",
    "agenda",
    "session",
    "emble",
    "techniques",
    "bagging",
    "boosting",
    "probably",
    "going",
    "cover",
    "random",
    "forest",
    "probably",
    "try",
    "cover",
    "adab",
    "boost",
    "energy",
    "also",
    "try",
    "cover",
    "xg",
    "boost",
    "al",
    "lthms",
    "discuss",
    "let",
    "go",
    "ahead",
    "let",
    "start",
    "topics",
    "first",
    "topic",
    "going",
    "discuss",
    "emble",
    "techniques",
    "exactly",
    "emble",
    "techniques",
    "going",
    "discuss",
    "okay",
    "emble",
    "techniques",
    "exactly",
    "emble",
    "techniques",
    "till",
    "solved",
    "two",
    "different",
    "kind",
    "problem",
    "statement",
    "one",
    "classification",
    "regression",
    "learned",
    "different",
    "different",
    "algorithms",
    "like",
    "uh",
    "linear",
    "regression",
    "logistic",
    "regression",
    "discussed",
    "knn",
    "discussed",
    "yesterday",
    "disc",
    "discuss",
    "n",
    "bias",
    "different",
    "different",
    "algorithms",
    "already",
    "finished",
    "respect",
    "classification",
    "regression",
    "problem",
    "whatever",
    "algorithm",
    "discussing",
    "one",
    "algorithm",
    "time",
    "discussing",
    "one",
    "algorithm",
    "time",
    "discussing",
    "trying",
    "either",
    "solve",
    "classification",
    "regression",
    "problem",
    "next",
    "thing",
    "use",
    "multiple",
    "algorithms",
    "mul",
    "multiple",
    "algorithm",
    "solve",
    "problem",
    "multiple",
    "algorithms",
    "basically",
    "means",
    "talk",
    "okay",
    "ask",
    "specific",
    "question",
    "use",
    "multiple",
    "algorithms",
    "solve",
    "problem",
    "point",
    "time",
    "definitely",
    "say",
    "yes",
    "going",
    "use",
    "something",
    "called",
    "emble",
    "techniques",
    "emble",
    "techniques",
    "okay",
    "emble",
    "techniques",
    "emble",
    "techniques",
    "specifically",
    "use",
    "two",
    "different",
    "ways",
    "one",
    "one",
    "one",
    "way",
    "specifically",
    "use",
    "one",
    "go",
    "write",
    "one",
    "basically",
    "use",
    "something",
    "called",
    "bagging",
    "technique",
    "one",
    "specifically",
    "use",
    "something",
    "called",
    "boosting",
    "technique",
    "bagging",
    "technique",
    "exactly",
    "boosting",
    "technique",
    "actually",
    "combining",
    "multiple",
    "models",
    "solve",
    "problem",
    "let",
    "first",
    "discuss",
    "bagging",
    "bagging",
    "work",
    "let",
    "say",
    "specific",
    "data",
    "set",
    "data",
    "set",
    "uh",
    "features",
    "rows",
    "columns",
    "everything",
    "like",
    "specific",
    "data",
    "set",
    "imagine",
    "many",
    "many",
    "features",
    "like",
    "fub1",
    "f2",
    "f3",
    "probably",
    "output",
    "data",
    "set",
    "let",
    "consider",
    "bagging",
    "create",
    "models",
    "model",
    "anything",
    "logistic",
    "linear",
    "classification",
    "problem",
    "let",
    "say",
    "logistic",
    "model",
    "model",
    "m1",
    "let",
    "say",
    "another",
    "model",
    "m2",
    "may",
    "another",
    "model",
    "m3",
    "let",
    "say",
    "logistic",
    "probably",
    "model",
    "like",
    "decision",
    "tree",
    "probably",
    "use",
    "model",
    "knn",
    "classification",
    "model",
    "decision",
    "tree",
    "fine",
    "let",
    "use",
    "another",
    "decision",
    "tree",
    "see",
    "used",
    "many",
    "models",
    "okay",
    "many",
    "models",
    "respect",
    "particular",
    "model",
    "first",
    "step",
    "particular",
    "data",
    "set",
    "take",
    "rows",
    "basically",
    "row",
    "sampling",
    "take",
    "row",
    "sampling",
    "dash",
    "das",
    "basically",
    "means",
    "das",
    "always",
    "less",
    "rows",
    "push",
    "m1",
    "okay",
    "also",
    "use",
    "n",
    "fine",
    "rows",
    "push",
    "model",
    "one",
    "model",
    "one",
    "training",
    "let",
    "say",
    "record",
    "th000",
    "rows",
    "actually",
    "row",
    "sampling",
    "th",
    "rows",
    "giving",
    "m1",
    "train",
    "actually",
    "going",
    "basically",
    "going",
    "give",
    "specific",
    "model",
    "m2",
    "going",
    "row",
    "row",
    "sampling",
    "going",
    "sample",
    "rows",
    "give",
    "model",
    "two",
    "remember",
    "rows",
    "may",
    "get",
    "repeated",
    "dash",
    "next",
    "dble",
    "dash",
    "similarly",
    "row",
    "sampling",
    "give",
    "may",
    "triple",
    "dash",
    "d4",
    "dash",
    "different",
    "different",
    "different",
    "different",
    "rows",
    "data",
    "points",
    "say",
    "row",
    "sampling",
    "basically",
    "talking",
    "data",
    "points",
    "different",
    "different",
    "data",
    "points",
    "give",
    "separate",
    "separate",
    "model",
    "model",
    "specifically",
    "train",
    "say",
    "dash",
    "basically",
    "means",
    "uh",
    "suppose",
    "say",
    "th",
    "total",
    "number",
    "data",
    "points",
    "say",
    "dash",
    "dash",
    "may",
    "th000",
    "points",
    "double",
    "dash",
    "may",
    "another",
    "th000",
    "points",
    "rows",
    "may",
    "get",
    "repeated",
    "dle",
    "dash",
    "also",
    "basically",
    "use",
    "specifically",
    "row",
    "sampling",
    "used",
    "many",
    "specific",
    "every",
    "model",
    "trained",
    "different",
    "kind",
    "data",
    "inferencing",
    "happen",
    "test",
    "data",
    "first",
    "thing",
    "first",
    "let",
    "say",
    "going",
    "get",
    "new",
    "test",
    "data",
    "new",
    "test",
    "data",
    "passed",
    "m1",
    "m1",
    "suppose",
    "gives",
    "zero",
    "output",
    "suppose",
    "let",
    "say",
    "binary",
    "classification",
    "gives",
    "zer",
    "output",
    "output",
    "zero",
    "next",
    "m2",
    "new",
    "test",
    "data",
    "gives",
    "one",
    "m3",
    "gives",
    "one",
    "m4",
    "also",
    "gives",
    "one",
    "output",
    "particular",
    "case",
    "particular",
    "case",
    "happen",
    "see",
    "simple",
    "think",
    "output",
    "may",
    "particular",
    "case",
    "m1",
    "predicted",
    "particular",
    "test",
    "data",
    "zero",
    "model",
    "m2",
    "predicted",
    "1",
    "m3",
    "predicted",
    "1",
    "m4",
    "predicted",
    "one",
    "finally",
    "outputs",
    "going",
    "get",
    "aggregated",
    "going",
    "get",
    "aggregated",
    "simple",
    "thing",
    "gets",
    "applied",
    "majority",
    "voting",
    "majority",
    "voting",
    "tell",
    "output",
    "respect",
    "output",
    "obviously",
    "one",
    "majority",
    "voting",
    "see",
    "three",
    "people",
    "basically",
    "saying",
    "one",
    "output",
    "one",
    "okay",
    "concept",
    "bagging",
    "wherein",
    "providing",
    "different",
    "different",
    "rows",
    "probably",
    "features",
    "case",
    "giving",
    "different",
    "different",
    "model",
    "classification",
    "model",
    "finally",
    "combining",
    "based",
    "majority",
    "voting",
    "getting",
    "answer",
    "one",
    "step",
    "called",
    "bootstrap",
    "aggregator",
    "basically",
    "means",
    "aggregating",
    "output",
    "basically",
    "coming",
    "specific",
    "models",
    "specific",
    "models",
    "many",
    "people",
    "say",
    "krish",
    "tai",
    "guys",
    "like",
    "kind",
    "situation",
    "know",
    "100",
    "200",
    "models",
    "difficult",
    "tie",
    "repeating",
    "questions",
    "put",
    "time",
    "saying",
    "50",
    "model",
    "says",
    "yes",
    "50",
    "models",
    "says",
    "always",
    "understand",
    "guys",
    "100",
    "200",
    "plus",
    "models",
    "particular",
    "case",
    "high",
    "probability",
    "always",
    "majority",
    "voting",
    "available",
    "always",
    "specific",
    "scenario",
    "concept",
    "bagging",
    "people",
    "saying",
    "krish",
    "using",
    "different",
    "different",
    "models",
    "guys",
    "discussing",
    "random",
    "forest",
    "random",
    "forest",
    "uses",
    "one",
    "type",
    "model",
    "decision",
    "tree",
    "think",
    "concept",
    "bagging",
    "different",
    "different",
    "models",
    "basically",
    "combine",
    "technique",
    "emble",
    "techniques",
    "basically",
    "called",
    "bagging",
    "okay",
    "tell",
    "one",
    "point",
    "missed",
    "fine",
    "respect",
    "classification",
    "problem",
    "respect",
    "regression",
    "problem",
    "happen",
    "case",
    "regression",
    "problem",
    "let",
    "say",
    "got",
    "120",
    "140",
    "122",
    "148",
    "output",
    "regression",
    "happen",
    "entire",
    "mean",
    "taken",
    "mean",
    "taken",
    "output",
    "mean",
    "basically",
    "taken",
    "output",
    "model",
    "average",
    "mean",
    "simple",
    "right",
    "average",
    "mean",
    "basically",
    "taken",
    "based",
    "average",
    "able",
    "solve",
    "regression",
    "problem",
    "great",
    "let",
    "go",
    "ahead",
    "try",
    "understand",
    "respect",
    "bagging",
    "boosting",
    "many",
    "different",
    "types",
    "algorithm",
    "need",
    "make",
    "understand",
    "exactly",
    "boosting",
    "bagging",
    "seen",
    "parallel",
    "models",
    "right",
    "one",
    "one",
    "one",
    "independent",
    "parallel",
    "models",
    "giving",
    "row",
    "samples",
    "different",
    "different",
    "models",
    "basically",
    "able",
    "find",
    "output",
    "case",
    "boosting",
    "boosting",
    "sequential",
    "combination",
    "models",
    "like",
    "lot",
    "sequential",
    "models",
    "like",
    "one",
    "model",
    "like",
    "first",
    "give",
    "training",
    "data",
    "particular",
    "model",
    "go",
    "data",
    "model",
    "model",
    "m1",
    "m2",
    "m3",
    "m4",
    "finally",
    "getting",
    "output",
    "basically",
    "say",
    "boosting",
    "m1",
    "m2",
    "m3",
    "basically",
    "mention",
    "weak",
    "learners",
    "weak",
    "learner",
    "weak",
    "learner",
    "weak",
    "learner",
    "weak",
    "learner",
    "finally",
    "go",
    "till",
    "combine",
    "weak",
    "ners",
    "weak",
    "learner",
    "weak",
    "learner",
    "okay",
    "combine",
    "weak",
    "learner",
    "becomes",
    "becomes",
    "strong",
    "learner",
    "finally",
    "try",
    "combine",
    "basically",
    "become",
    "strong",
    "learner",
    "models",
    "sequentially",
    "one",
    "probably",
    "try",
    "provide",
    "uh",
    "input",
    "one",
    "model",
    "next",
    "model",
    "next",
    "model",
    "models",
    "simpler",
    "weak",
    "learner",
    "model",
    "able",
    "predict",
    "properly",
    "combine",
    "particular",
    "models",
    "together",
    "sequentially",
    "becomes",
    "strong",
    "learner",
    "specifically",
    "works",
    "take",
    "example",
    "example",
    "ad",
    "boost",
    "xg",
    "boost",
    "show",
    "okay",
    "week",
    "learner",
    "basically",
    "means",
    "prediction",
    "bad",
    "go",
    "sequentially",
    "combine",
    "become",
    "strong",
    "learner",
    "okay",
    "one",
    "example",
    "want",
    "give",
    "let",
    "say",
    "data",
    "scientist",
    "right",
    "let",
    "say",
    "model",
    "one",
    "may",
    "teacher",
    "respect",
    "physics",
    "model",
    "two",
    "may",
    "teacher",
    "respect",
    "chemistry",
    "let",
    "say",
    "model",
    "3",
    "basically",
    "teacher",
    "maths",
    "model",
    "four",
    "teacher",
    "geography",
    "suppose",
    "trying",
    "solve",
    "one",
    "problem",
    "obviously",
    "physics",
    "teacher",
    "able",
    "solve",
    "particular",
    "problem",
    "probably",
    "chemistry",
    "help",
    "maths",
    "help",
    "geography",
    "help",
    "someone",
    "help",
    "combine",
    "many",
    "expertise",
    "together",
    "able",
    "give",
    "output",
    "efficient",
    "way",
    "sumit",
    "talk",
    "whether",
    "features",
    "basically",
    "passed",
    "models",
    "talk",
    "give",
    "time",
    "okay",
    "want",
    "give",
    "idea",
    "short",
    "someone",
    "asks",
    "interview",
    "exactly",
    "boosting",
    "okay",
    "boosting",
    "say",
    "sequential",
    "set",
    "models",
    "combined",
    "together",
    "models",
    "initialized",
    "usually",
    "weak",
    "learners",
    "combined",
    "together",
    "become",
    "strong",
    "learner",
    "based",
    "strong",
    "learner",
    "gives",
    "amazing",
    "output",
    "right",
    "say",
    "kaggle",
    "competition",
    "use",
    "different",
    "types",
    "boosting",
    "bagging",
    "technique",
    "basically",
    "said",
    "bagging",
    "boosting",
    "bagging",
    "kind",
    "algorithm",
    "specifically",
    "use",
    "use",
    "something",
    "called",
    "random",
    "forest",
    "classifier",
    "second",
    "model",
    "specifically",
    "use",
    "something",
    "called",
    "random",
    "forest",
    "regress",
    "specifically",
    "use",
    "two",
    "kind",
    "models",
    "actually",
    "going",
    "discuss",
    "right",
    "boosting",
    "basically",
    "use",
    "techniques",
    "like",
    "ad",
    "boost",
    "gradi",
    "boost",
    "number",
    "three",
    "extreme",
    "gradient",
    "boost",
    "also",
    "say",
    "xg",
    "boost",
    "extreme",
    "gradient",
    "boost",
    "let",
    "go",
    "ahead",
    "let",
    "discuss",
    "first",
    "algorithm",
    "called",
    "random",
    "forest",
    "classifier",
    "regressor",
    "first",
    "thing",
    "first",
    "let",
    "understand",
    "things",
    "yesterday",
    "class",
    "hope",
    "uh",
    "main",
    "problem",
    "respect",
    "decision",
    "tree",
    "whenever",
    "create",
    "decision",
    "tree",
    "without",
    "hyperparameter",
    "lead",
    "overit",
    "lead",
    "overfitting",
    "uh",
    "whenever",
    "probably",
    "decision",
    "tree",
    "right",
    "leads",
    "something",
    "like",
    "overfitting",
    "overfitting",
    "completely",
    "splits",
    "feature",
    "till",
    "complete",
    "depth",
    "overfitting",
    "basically",
    "means",
    "training",
    "data",
    "accuracy",
    "high",
    "test",
    "data",
    "accuracy",
    "low",
    "training",
    "data",
    "accuracy",
    "high",
    "may",
    "basically",
    "say",
    "high",
    "bias",
    "may",
    "basically",
    "say",
    "sorry",
    "high",
    "bias",
    "low",
    "bias",
    "high",
    "v",
    "variance",
    "low",
    "bias",
    "high",
    "variance",
    "yes",
    "obviously",
    "pruning",
    "guys",
    "understand",
    "pruning",
    "extensive",
    "task",
    "probably",
    "100",
    "features",
    "data",
    "points",
    "like",
    "1",
    "million",
    "pruning",
    "also",
    "much",
    "difficult",
    "yes",
    "pre",
    "pruning",
    "done",
    "confirm",
    "may",
    "work",
    "well",
    "right",
    "respect",
    "decision",
    "tree",
    "specific",
    "problem",
    "low",
    "bias",
    "high",
    "variance",
    "low",
    "biance",
    "high",
    "variance",
    "know",
    "model",
    "basically",
    "generalized",
    "model",
    "get",
    "low",
    "bias",
    "low",
    "variance",
    "somebody",
    "asks",
    "use",
    "random",
    "forest",
    "basically",
    "explain",
    "decision",
    "trees",
    "like",
    "main",
    "aim",
    "convert",
    "high",
    "variance",
    "low",
    "variance",
    "able",
    "convert",
    "high",
    "variance",
    "low",
    "variance",
    "using",
    "random",
    "forest",
    "classifier",
    "random",
    "forest",
    "regressor",
    "random",
    "forest",
    "random",
    "forest",
    "bagging",
    "technique",
    "similarly",
    "data",
    "set",
    "let",
    "say",
    "data",
    "set",
    "multiple",
    "models",
    "like",
    "m1",
    "m2",
    "m3",
    "m4",
    "let",
    "say",
    "four",
    "models",
    "like",
    "many",
    "many",
    "models",
    "respect",
    "models",
    "models",
    "models",
    "actually",
    "decision",
    "tree",
    "random",
    "forest",
    "decision",
    "trees",
    "different",
    "model",
    "see",
    "models",
    "decision",
    "trees",
    "going",
    "get",
    "used",
    "used",
    "random",
    "forest",
    "decision",
    "trees",
    "always",
    "gets",
    "used",
    "random",
    "forest",
    "first",
    "thing",
    "know",
    "whenever",
    "using",
    "decision",
    "trees",
    "know",
    "decision",
    "tree",
    "default",
    "try",
    "create",
    "may",
    "lead",
    "overfitting",
    "every",
    "decision",
    "tree",
    "basically",
    "create",
    "low",
    "v",
    "low",
    "bias",
    "high",
    "variance",
    "combine",
    "form",
    "bootstrap",
    "aggregator",
    "high",
    "variance",
    "getting",
    "converted",
    "low",
    "variance",
    "majority",
    "voting",
    "taking",
    "particular",
    "decision",
    "trees",
    "like",
    "many",
    "many",
    "decision",
    "tree",
    "lot",
    "outputs",
    "coming",
    "help",
    "majority",
    "voting",
    "classifier",
    "high",
    "variance",
    "get",
    "converted",
    "low",
    "variance",
    "random",
    "forest",
    "works",
    "first",
    "case",
    "talk",
    "random",
    "forest",
    "two",
    "things",
    "basically",
    "happen",
    "respect",
    "data",
    "set",
    "let",
    "say",
    "first",
    "model",
    "kind",
    "row",
    "sampling",
    "plus",
    "feature",
    "feature",
    "sampling",
    "basically",
    "means",
    "select",
    "set",
    "rows",
    "set",
    "features",
    "give",
    "m1",
    "similarly",
    "row",
    "sampling",
    "feature",
    "sampling",
    "give",
    "m2",
    "row",
    "sampling",
    "feature",
    "sampling",
    "give",
    "m3",
    "row",
    "sampling",
    "feature",
    "sampling",
    "give",
    "m4",
    "happen",
    "independently",
    "giving",
    "features",
    "along",
    "rows",
    "may",
    "situation",
    "features",
    "may",
    "also",
    "get",
    "repeated",
    "may",
    "also",
    "get",
    "repeated",
    "records",
    "data",
    "points",
    "may",
    "also",
    "get",
    "repeated",
    "probably",
    "training",
    "model",
    "specific",
    "data",
    "sets",
    "specific",
    "features",
    "model",
    "become",
    "expert",
    "predicting",
    "something",
    "right",
    "said",
    "one",
    "example",
    "giving",
    "physics",
    "model",
    "data",
    "giving",
    "chemistry",
    "data",
    "chemistry",
    "model",
    "data",
    "similarly",
    "giving",
    "information",
    "model",
    "model",
    "expert",
    "respect",
    "specific",
    "data",
    "based",
    "particular",
    "data",
    "whenever",
    "get",
    "new",
    "test",
    "data",
    "happen",
    "suppose",
    "let",
    "say",
    "classification",
    "problem",
    "m1",
    "model",
    "predicting",
    "zero",
    "predicting",
    "one",
    "predicting",
    "zero",
    "predicting",
    "zero",
    "particular",
    "case",
    "majority",
    "voting",
    "classifier",
    "majority",
    "voting",
    "happen",
    "case",
    "classification",
    "problem",
    "specifically",
    "able",
    "get",
    "output",
    "zero",
    "hope",
    "everybody",
    "able",
    "understand",
    "models",
    "decision",
    "trees",
    "based",
    "see",
    "interview",
    "uh",
    "things",
    "things",
    "telling",
    "points",
    "much",
    "important",
    "similarly",
    "tell",
    "interviewer",
    "definitely",
    "interview",
    "cracked",
    "kind",
    "algorithm",
    "seen",
    "students",
    "saying",
    "okay",
    "uh",
    "kish",
    "um",
    "interviewer",
    "asked",
    "favorite",
    "algorithm",
    "said",
    "random",
    "forest",
    "told",
    "say",
    "like",
    "said",
    "person",
    "let",
    "let",
    "ask",
    "questions",
    "random",
    "forest",
    "much",
    "confident",
    "also",
    "going",
    "prove",
    "know",
    "good",
    "specific",
    "case",
    "basically",
    "see",
    "overfitting",
    "condition",
    "decision",
    "tree",
    "combining",
    "multiple",
    "decision",
    "tree",
    "get",
    "generalized",
    "model",
    "low",
    "bias",
    "low",
    "variance",
    "hope",
    "everybody",
    "able",
    "understand",
    "boost",
    "feature",
    "sampling",
    "basically",
    "means",
    "suppose",
    "1",
    "2",
    "3",
    "four",
    "feature",
    "first",
    "model",
    "may",
    "give",
    "two",
    "features",
    "second",
    "model",
    "may",
    "get",
    "three",
    "features",
    "fourth",
    "model",
    "may",
    "give",
    "four",
    "features",
    "uh",
    "one",
    "feature",
    "als",
    "give",
    "specific",
    "model",
    "internally",
    "random",
    "forest",
    "take",
    "carees",
    "things",
    "random",
    "forest",
    "works",
    "difference",
    "random",
    "forest",
    "classify",
    "regression",
    "regression",
    "whatever",
    "output",
    "basically",
    "getting",
    "basically",
    "mean",
    "average",
    "average",
    "able",
    "get",
    "output",
    "based",
    "models",
    "output",
    "actually",
    "getting",
    "let",
    "talk",
    "important",
    "points",
    "random",
    "forest",
    "first",
    "thing",
    "first",
    "question",
    "normalization",
    "required",
    "random",
    "forest",
    "next",
    "question",
    "knn",
    "normalization",
    "say",
    "normalization",
    "standardization",
    "talk",
    "standardization",
    "standardization",
    "required",
    "another",
    "question",
    "normalization",
    "required",
    "random",
    "forest",
    "decision",
    "tree",
    "also",
    "say",
    "decision",
    "tree",
    "required",
    "answer",
    "understand",
    "decision",
    "tree",
    "basically",
    "splits",
    "mini",
    "minimize",
    "data",
    "also",
    "split",
    "wo",
    "much",
    "important",
    "talk",
    "knn",
    "whether",
    "standardization",
    "normalization",
    "required",
    "answer",
    "yes",
    "use",
    "two",
    "things",
    "one",
    "ukan",
    "distance",
    "manhattan",
    "distance",
    "definitely",
    "apply",
    "standardization",
    "computation",
    "distance",
    "becomes",
    "easy",
    "one",
    "common",
    "interview",
    "questions",
    "basically",
    "asked",
    "random",
    "forest",
    "coming",
    "third",
    "question",
    "random",
    "forest",
    "impacted",
    "outlier",
    "answer",
    "check",
    "outside",
    "basically",
    "means",
    "google",
    "check",
    "check",
    "google",
    "okay",
    "perfect",
    "hope",
    "covered",
    "things",
    "random",
    "forest",
    "random",
    "forest",
    "impacted",
    "outliers",
    "third",
    "question",
    "knn",
    "impacted",
    "outliers",
    "knn",
    "algorithm",
    "impacted",
    "outliers",
    "knn",
    "impacted",
    "byers",
    "answer",
    "yes",
    "big",
    "yes",
    "perfect",
    "interview",
    "questions",
    "needs",
    "covered",
    "let",
    "go",
    "ahead",
    "discuss",
    "adab",
    "boost",
    "bagging",
    "time",
    "specifically",
    "use",
    "random",
    "forest",
    "also",
    "create",
    "custom",
    "bagging",
    "techniques",
    "custom",
    "bagging",
    "techniques",
    "means",
    "whatever",
    "algorithm",
    "want",
    "use",
    "combination",
    "try",
    "give",
    "output",
    "also",
    "manually",
    "help",
    "hands",
    "okay",
    "guys",
    "second",
    "thing",
    "uh",
    "going",
    "discuss",
    "boosting",
    "technique",
    "first",
    "thing",
    "uh",
    "first",
    "algorithm",
    "going",
    "discuss",
    "adab",
    "boost",
    "adab",
    "boost",
    "going",
    "discuss",
    "adab",
    "boost",
    "uh",
    "work",
    "let",
    "solve",
    "uh",
    "first",
    "boosting",
    "technique",
    "called",
    "adab",
    "boost",
    "okay",
    "uh",
    "boosting",
    "technique",
    "um",
    "boosting",
    "technique",
    "heard",
    "basically",
    "solve",
    "sequential",
    "way",
    "least",
    "know",
    "know",
    "lot",
    "confusion",
    "within",
    "try",
    "solve",
    "problem",
    "let",
    "say",
    "suppose",
    "data",
    "set",
    "looks",
    "like",
    "fub1",
    "f2",
    "f3",
    "f4",
    "features",
    "probably",
    "output",
    "okay",
    "let",
    "say",
    "features",
    "like",
    "output",
    "like",
    "yes",
    "like",
    "let",
    "say",
    "many",
    "records",
    "three",
    "4",
    "5",
    "6",
    "one",
    "7",
    "seven",
    "records",
    "adab",
    "boost",
    "first",
    "thing",
    "specifically",
    "adab",
    "boost",
    "uh",
    "really",
    "need",
    "understand",
    "things",
    "basically",
    "solve",
    "classification",
    "problem",
    "going",
    "understand",
    "first",
    "thing",
    "first",
    "define",
    "weight",
    "weight",
    "much",
    "simple",
    "initially",
    "records",
    "input",
    "records",
    "provide",
    "equal",
    "weight",
    "provide",
    "equal",
    "weight",
    "go",
    "count",
    "many",
    "number",
    "records",
    "particular",
    "case",
    "total",
    "number",
    "records",
    "one",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "every",
    "record",
    "provide",
    "equal",
    "weight",
    "0",
    "1",
    "overall",
    "sum",
    "one",
    "particular",
    "case",
    "make",
    "1x",
    "7",
    "1x",
    "7",
    "1x",
    "7",
    "everyone",
    "definitely",
    "become",
    "equal",
    "weights",
    "right",
    "total",
    "sum",
    "obviously",
    "one",
    "let",
    "go",
    "next",
    "one",
    "okay",
    "adab",
    "first",
    "thing",
    "take",
    "feature",
    "decide",
    "feature",
    "take",
    "whether",
    "go",
    "f1",
    "whether",
    "go",
    "fs2",
    "whether",
    "go",
    "f3",
    "help",
    "information",
    "gain",
    "information",
    "gain",
    "entropy",
    "guinea",
    "right",
    "based",
    "definitely",
    "understand",
    "whether",
    "start",
    "making",
    "decision",
    "also",
    "specifically",
    "make",
    "decision",
    "trees",
    "probably",
    "determine",
    "using",
    "feature",
    "start",
    "decision",
    "tree",
    "suppose",
    "feature",
    "one",
    "feature",
    "two",
    "feature",
    "three",
    "selected",
    "okay",
    "information",
    "gain",
    "entropy",
    "feature",
    "one",
    "higher",
    "going",
    "use",
    "feature",
    "one",
    "probably",
    "divide",
    "decision",
    "trees",
    "divide",
    "decision",
    "tree",
    "let",
    "say",
    "dividing",
    "like",
    "decision",
    "tree",
    "decision",
    "tree",
    "depth",
    "one",
    "one",
    "depth",
    "depth",
    "since",
    "one",
    "depth",
    "basically",
    "call",
    "stumps",
    "specifically",
    "create",
    "decision",
    "tre",
    "taking",
    "one",
    "feature",
    "divide",
    "one",
    "level",
    "okay",
    "one",
    "level",
    "one",
    "depth",
    "specifically",
    "called",
    "stump",
    "going",
    "next",
    "particular",
    "stump",
    "okay",
    "stump",
    "basically",
    "getting",
    "created",
    "one",
    "adab",
    "boost",
    "right",
    "say",
    "weak",
    "learners",
    "weak",
    "learner",
    "weak",
    "learner",
    "reason",
    "say",
    "weak",
    "learner",
    "weak",
    "learner",
    "first",
    "thing",
    "respect",
    "uh",
    "particular",
    "adab",
    "boost",
    "first",
    "step",
    "weak",
    "learner",
    "weak",
    "learner",
    "basically",
    "create",
    "stump",
    "stump",
    "basically",
    "means",
    "one",
    "level",
    "decision",
    "tree",
    "based",
    "information",
    "gain",
    "entropy",
    "selected",
    "feature",
    "made",
    "decision",
    "tree",
    "one",
    "level",
    "called",
    "called",
    "weak",
    "learner",
    "okay",
    "reason",
    "use",
    "stum",
    "one",
    "level",
    "decision",
    "tree",
    "next",
    "step",
    "happens",
    "provide",
    "specific",
    "records",
    "f1",
    "train",
    "specific",
    "model",
    "one",
    "level",
    "decision",
    "tree",
    "train",
    "train",
    "let",
    "say",
    "going",
    "pass",
    "particular",
    "records",
    "find",
    "many",
    "correct",
    "many",
    "wrong",
    "decision",
    "decision",
    "tree",
    "basically",
    "giving",
    "let",
    "say",
    "entire",
    "records",
    "one",
    "record",
    "one",
    "record",
    "given",
    "wrong",
    "let",
    "say",
    "record",
    "given",
    "wrong",
    "okay",
    "let",
    "say",
    "record",
    "output",
    "predicted",
    "wrong",
    "particular",
    "model",
    "one",
    "wrong",
    "training",
    "model",
    "need",
    "specific",
    "case",
    "understand",
    "important",
    "thing",
    "let",
    "say",
    "done",
    "probably",
    "actually",
    "going",
    "going",
    "calculate",
    "total",
    "error",
    "many",
    "error",
    "particular",
    "model",
    "made",
    "let",
    "say",
    "particular",
    "case",
    "one",
    "wrong",
    "wrong",
    "right",
    "one",
    "wrong",
    "want",
    "calculate",
    "total",
    "error",
    "calculate",
    "many",
    "many",
    "wrong",
    "many",
    "wrong",
    "one",
    "wrong",
    "weight",
    "go",
    "write",
    "1x",
    "7",
    "specifically",
    "total",
    "error",
    "specific",
    "model",
    "stump",
    "okay",
    "f1",
    "stop",
    "first",
    "step",
    "second",
    "step",
    "need",
    "see",
    "performance",
    "stump",
    "stump",
    "specific",
    "stump",
    "performance",
    "basically",
    "checked",
    "formula",
    "1",
    "log",
    "e",
    "1us",
    "total",
    "error",
    "divided",
    "total",
    "error",
    "everything",
    "make",
    "sense",
    "okay",
    "time",
    "every",
    "every",
    "small",
    "time",
    "everything",
    "make",
    "sense",
    "first",
    "step",
    "adaab",
    "boost",
    "try",
    "find",
    "total",
    "error",
    "second",
    "step",
    "try",
    "find",
    "performance",
    "stump",
    "particular",
    "case",
    "1",
    "log",
    "e",
    "1",
    "1",
    "7",
    "1x",
    "7",
    "calculate",
    "coming",
    "895",
    "f2",
    "f3",
    "see",
    "understand",
    "features",
    "found",
    "information",
    "gain",
    "entropy",
    "best",
    "feature",
    "let",
    "say",
    "calculated",
    "as895",
    "second",
    "step",
    "first",
    "step",
    "find",
    "total",
    "error",
    "second",
    "step",
    "performance",
    "stum",
    "te",
    "te",
    "basically",
    "means",
    "total",
    "error",
    "te",
    "basically",
    "means",
    "total",
    "error",
    "see",
    "see",
    "steps",
    "okay",
    "see",
    "steps",
    "whenever",
    "discussing",
    "boosting",
    "going",
    "combine",
    "weak",
    "learners",
    "together",
    "get",
    "strong",
    "learner",
    "next",
    "step",
    "third",
    "step",
    "understand",
    "third",
    "step",
    "update",
    "weights",
    "reason",
    "calculating",
    "total",
    "error",
    "performance",
    "step",
    "third",
    "step",
    "basically",
    "new",
    "sample",
    "weight",
    "decision",
    "tree",
    "one",
    "stump",
    "say",
    "new",
    "sample",
    "weight",
    "equal",
    "need",
    "update",
    "weights",
    "need",
    "update",
    "weights",
    "understand",
    "talk",
    "second",
    "want",
    "update",
    "sample",
    "weights",
    "first",
    "update",
    "correct",
    "records",
    "see",
    "correct",
    "records",
    "whichever",
    "correct",
    "like",
    "records",
    "correct",
    "records",
    "correct",
    "update",
    "weights",
    "update",
    "weights",
    "particular",
    "record",
    "reduce",
    "wrong",
    "records",
    "update",
    "increase",
    "increase",
    "weights",
    "wrong",
    "records",
    "record",
    "go",
    "next",
    "week",
    "learner",
    "reason",
    "update",
    "particular",
    "weights",
    "correct",
    "records",
    "correct",
    "records",
    "formula",
    "looks",
    "something",
    "like",
    "weight",
    "multiplied",
    "weight",
    "multiplied",
    "e",
    "minus",
    "specific",
    "performance",
    "okay",
    "specific",
    "performance",
    "e",
    "power",
    "ps",
    "write",
    "performance",
    "stump",
    "basically",
    "able",
    "write",
    "1x",
    "7",
    "e",
    "minus",
    "895",
    "calculation",
    "everybody",
    "try",
    "answer",
    "05",
    "correct",
    "records",
    "incorrect",
    "records",
    "incorrect",
    "records",
    "weights",
    "going",
    "formula",
    "going",
    "apply",
    "multiplied",
    "e",
    "plus",
    "ps",
    "minus",
    "ps",
    "plus",
    "ps",
    "write",
    "1",
    "7",
    "multiplied",
    "e",
    "895",
    "go",
    "probably",
    "calcul",
    "going",
    "get",
    "349",
    "two",
    "weights",
    "got",
    "basically",
    "means",
    "records",
    "correct",
    "1x",
    "7",
    "new",
    "updated",
    "weights",
    "05",
    "05",
    "05",
    "05",
    "sorry",
    "wrong",
    "records",
    "05",
    "05",
    "05",
    "let",
    "see",
    "1x",
    "7",
    "see",
    "initially",
    "142",
    "got",
    "reduced",
    "05",
    "records",
    "correct",
    "wrong",
    "record",
    "value",
    "349",
    "weights",
    "become",
    "349",
    "go",
    "go",
    "ahead",
    "write",
    "new",
    "weight",
    "new",
    "weight",
    "nothing",
    "05",
    "055",
    "05",
    "05",
    "05",
    "1",
    "2",
    "many",
    "1",
    "2",
    "3",
    "okay",
    "fourth",
    "record",
    "fourth",
    "record",
    "1",
    "2",
    "3",
    "4",
    "05",
    "05",
    "okay",
    "many",
    "records",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "fourth",
    "record",
    "basically",
    "become",
    "new",
    "value",
    "something",
    "called",
    "349",
    "tell",
    "guys",
    "summation",
    "weights",
    "one",
    "prob",
    "think",
    "one",
    "try",
    "add",
    "one",
    "go",
    "see",
    "one",
    "combine",
    "things",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "one",
    "need",
    "find",
    "normalized",
    "weight",
    "order",
    "find",
    "normalized",
    "weight",
    "entire",
    "sumission",
    "one",
    "normalize",
    "order",
    "normalize",
    "go",
    "find",
    "sum",
    "things",
    "summation",
    "things",
    "0",
    "649",
    "divide",
    "numbers",
    "649",
    "divided",
    "649",
    "649",
    "like",
    "divide",
    "numbers",
    "649",
    "tell",
    "answer",
    "getting",
    "normalized",
    "weight",
    "look",
    "like",
    "077",
    "07",
    "value",
    "somewhere",
    "around",
    "uh",
    "537",
    "guess",
    "case",
    "07",
    "077",
    "going",
    "divide",
    "64",
    "649",
    "normalized",
    "weight",
    "get",
    "normalized",
    "weight",
    "try",
    "create",
    "something",
    "called",
    "buckets",
    "see",
    "one",
    "decision",
    "tree",
    "already",
    "created",
    "stump",
    "know",
    "particular",
    "stum",
    "going",
    "get",
    "okay",
    "output",
    "sequential",
    "model",
    "go",
    "combine",
    "another",
    "model",
    "time",
    "create",
    "specific",
    "model",
    "order",
    "create",
    "specific",
    "model",
    "need",
    "provide",
    "specific",
    "rows",
    "model",
    "train",
    "model",
    "giving",
    "one",
    "wrong",
    "whatever",
    "wrong",
    "along",
    "data",
    "points",
    "need",
    "provide",
    "specific",
    "model",
    "records",
    "model",
    "able",
    "train",
    "probably",
    "able",
    "get",
    "output",
    "let",
    "create",
    "buckets",
    "based",
    "buckets",
    "buckets",
    "created",
    "take",
    "07",
    "sorry",
    "whatever",
    "value",
    "normal",
    "value",
    "okay",
    "start",
    "creating",
    "buckets",
    "buckets",
    "basically",
    "0",
    "07",
    "say",
    "decision",
    "tree",
    "stump",
    "need",
    "provide",
    "records",
    "maximum",
    "number",
    "record",
    "going",
    "wrong",
    "records",
    "go",
    "decide",
    "okay",
    "way",
    "able",
    "say",
    "specific",
    "wrong",
    "number",
    "records",
    "go",
    "decision",
    "tree",
    "purpose",
    "decision",
    "tree",
    "randomly",
    "create",
    "numbers",
    "0",
    "1",
    "randomly",
    "create",
    "numbers",
    "0",
    "1",
    "whichever",
    "bucket",
    "come",
    "like",
    "07",
    "014",
    "014",
    "07",
    "basically",
    "means",
    "0",
    "2",
    "1",
    "0",
    "2",
    "1",
    "2",
    "see",
    "bucket",
    "getting",
    "cre",
    "value",
    "getting",
    "added",
    "becomes",
    "bucket",
    "021",
    "537",
    "much",
    "nothing",
    "470",
    "747",
    "747",
    "751",
    "like",
    "create",
    "buckets",
    "okay",
    "create",
    "buckets",
    "tell",
    "record",
    "basically",
    "biggest",
    "bucket",
    "size",
    "obviously",
    "record",
    "randomly",
    "create",
    "number",
    "0",
    "one",
    "highest",
    "probability",
    "values",
    "going",
    "particular",
    "case",
    "wrong",
    "records",
    "passed",
    "along",
    "records",
    "obviously",
    "records",
    "chances",
    "records",
    "go",
    "next",
    "decision",
    "tree",
    "understand",
    "maximum",
    "number",
    "go",
    "wrong",
    "records",
    "bucket",
    "high",
    "bucket",
    "high",
    "time",
    "specific",
    "record",
    "get",
    "create",
    "selected",
    "gone",
    "second",
    "tree",
    "suppose",
    "records",
    "first",
    "stump",
    "second",
    "stump",
    "third",
    "stump",
    "similarly",
    "third",
    "stump",
    "second",
    "stump",
    "whichever",
    "wrong",
    "records",
    "going",
    "maximum",
    "number",
    "records",
    "go",
    "trained",
    "like",
    "lot",
    "stumps",
    "minimum",
    "100",
    "decision",
    "trees",
    "added",
    "know",
    "every",
    "decision",
    "tree",
    "give",
    "one",
    "output",
    "new",
    "test",
    "data",
    "new",
    "test",
    "data",
    "week",
    "learner",
    "give",
    "one",
    "output",
    "week",
    "learner",
    "give",
    "one",
    "output",
    "week",
    "learner",
    "week",
    "learner",
    "giving",
    "one",
    "output",
    "obviously",
    "time",
    "complexity",
    "particular",
    "output",
    "suppose",
    "binary",
    "classification",
    "getting",
    "0",
    "1",
    "1",
    "1",
    "majority",
    "voting",
    "happen",
    "output",
    "one",
    "case",
    "regression",
    "problem",
    "continuous",
    "value",
    "average",
    "average",
    "computed",
    "give",
    "output",
    "regression",
    "average",
    "done",
    "classification",
    "happen",
    "majority",
    "happening",
    "everywhere",
    "part",
    "going",
    "buckets",
    "much",
    "simple",
    "guys",
    "buckets",
    "basically",
    "means",
    "based",
    "weights",
    "normalized",
    "weight",
    "going",
    "create",
    "bucket",
    "whichever",
    "records",
    "highest",
    "bucket",
    "based",
    "randomly",
    "creating",
    "code",
    "know",
    "select",
    "specific",
    "buckets",
    "put",
    "random",
    "forest",
    "understand",
    "bucket",
    "size",
    "big",
    "wrong",
    "records",
    "present",
    "right",
    "suppose",
    "four",
    "five",
    "wrong",
    "records",
    "bucket",
    "size",
    "also",
    "bigger",
    "based",
    "randomly",
    "creating",
    "num",
    "0",
    "1",
    "wrong",
    "records",
    "selected",
    "given",
    "second",
    "stum",
    "similarly",
    "particular",
    "decision",
    "tree",
    "mistakes",
    "wrong",
    "records",
    "get",
    "updated",
    "weights",
    "get",
    "updated",
    "passed",
    "next",
    "decision",
    "tree",
    "guys",
    "say",
    "wrong",
    "record",
    "output",
    "zero",
    "one",
    "interesting",
    "everyone",
    "hope",
    "understood",
    "much",
    "maths",
    "adab",
    "boost",
    "adab",
    "boost",
    "actually",
    "work",
    "three",
    "main",
    "things",
    "one",
    "total",
    "error",
    "one",
    "performance",
    "stump",
    "one",
    "new",
    "sample",
    "weight",
    "things",
    "getting",
    "calculated",
    "extensive",
    "max",
    "normalized",
    "weight",
    "basically",
    "used",
    "sum",
    "weights",
    "approximately",
    "equal",
    "one",
    "boosting",
    "take",
    "last",
    "output",
    "give",
    "importance",
    "every",
    "decision",
    "tree",
    "output",
    "every",
    "decision",
    "tree",
    "output",
    "important",
    "okay",
    "let",
    "talk",
    "one",
    "model",
    "called",
    "blackbox",
    "model",
    "versus",
    "white",
    "box",
    "difference",
    "blackbox",
    "model",
    "white",
    "box",
    "take",
    "example",
    "linear",
    "regression",
    "tell",
    "kind",
    "model",
    "white",
    "box",
    "model",
    "black",
    "box",
    "take",
    "example",
    "random",
    "forest",
    "white",
    "box",
    "black",
    "box",
    "take",
    "example",
    "decision",
    "tree",
    "white",
    "box",
    "blackbox",
    "model",
    "take",
    "example",
    "ann",
    "white",
    "box",
    "blackbox",
    "model",
    "linear",
    "regression",
    "basically",
    "called",
    "wide",
    "box",
    "model",
    "basically",
    "visualize",
    "theta",
    "value",
    "basically",
    "changing",
    "coming",
    "global",
    "minima",
    "things",
    "random",
    "forest",
    "say",
    "blackbox",
    "model",
    "impossible",
    "see",
    "decision",
    "tree",
    "working",
    "reason",
    "maths",
    "complex",
    "inside",
    "talk",
    "decision",
    "tree",
    "basically",
    "white",
    "box",
    "model",
    "decision",
    "tree",
    "know",
    "split",
    "basically",
    "happening",
    "help",
    "paper",
    "pen",
    "able",
    "case",
    "ann",
    "blackbox",
    "model",
    "know",
    "like",
    "many",
    "neurons",
    "performing",
    "weights",
    "getting",
    "updated",
    "basic",
    "difference",
    "blackbox",
    "uh",
    "uh",
    "white",
    "box",
    "model",
    "entire",
    "thing",
    "agenda",
    "today",
    "session",
    "let",
    "start",
    "uh",
    "first",
    "algorithm",
    "probably",
    "going",
    "discuss",
    "today",
    "something",
    "called",
    "k",
    "means",
    "clustering",
    "k",
    "means",
    "clustering",
    "kind",
    "unsupervised",
    "machine",
    "learning",
    "always",
    "remember",
    "unsupervised",
    "machine",
    "learning",
    "basically",
    "means",
    "uh",
    "one",
    "important",
    "thing",
    "unsupervised",
    "machine",
    "learning",
    "unsupervised",
    "ml",
    "specific",
    "output",
    "specific",
    "output",
    "suppose",
    "feature",
    "one",
    "feature",
    "two",
    "suppose",
    "datas",
    "different",
    "different",
    "data",
    "know",
    "based",
    "data",
    "basically",
    "try",
    "create",
    "clusters",
    "clusters",
    "basically",
    "says",
    "similar",
    "kind",
    "data",
    "basically",
    "uh",
    "clustering",
    "various",
    "techniques",
    "like",
    "k",
    "mains",
    "uh",
    "hierle",
    "clustering",
    "first",
    "try",
    "understand",
    "k",
    "means",
    "specifically",
    "work",
    "simple",
    "uh",
    "suppose",
    "data",
    "points",
    "like",
    "okay",
    "let",
    "say",
    "f1",
    "feature",
    "f2",
    "feature",
    "based",
    "two",
    "dimensional",
    "probably",
    "plotting",
    "points",
    "suppose",
    "another",
    "points",
    "main",
    "purpose",
    "basically",
    "cluster",
    "together",
    "different",
    "different",
    "groups",
    "okay",
    "one",
    "group",
    "probably",
    "group",
    "group",
    "right",
    "two",
    "groups",
    "obviously",
    "see",
    "clusters",
    "two",
    "similar",
    "kind",
    "data",
    "basically",
    "grouped",
    "together",
    "right",
    "cluster",
    "one",
    "cluster",
    "2",
    "let",
    "talk",
    "specifically",
    "much",
    "useful",
    "try",
    "understand",
    "math",
    "intuition",
    "also",
    "always",
    "understand",
    "guys",
    "uh",
    "clustering",
    "gets",
    "used",
    "okay",
    "ensemble",
    "techniques",
    "told",
    "custom",
    "emble",
    "technique",
    "right",
    "custom",
    "emble",
    "techniques",
    "custom",
    "assemble",
    "techniques",
    "know",
    "whenever",
    "probably",
    "creating",
    "model",
    "first",
    "data",
    "set",
    "create",
    "clusters",
    "suppose",
    "data",
    "set",
    "model",
    "creation",
    "first",
    "algorithm",
    "probably",
    "apply",
    "clustering",
    "algorithm",
    "obviously",
    "good",
    "apply",
    "regression",
    "classification",
    "problem",
    "suppose",
    "clustering",
    "two",
    "three",
    "groups",
    "let",
    "say",
    "two",
    "three",
    "groups",
    "group",
    "apply",
    "separate",
    "supervis",
    "machine",
    "learning",
    "algorithm",
    "know",
    "specific",
    "output",
    "really",
    "want",
    "take",
    "ahead",
    "talk",
    "uh",
    "give",
    "examples",
    "go",
    "ahead",
    "let",
    "go",
    "go",
    "ahead",
    "focus",
    "understanding",
    "kin",
    "clustering",
    "algorithm",
    "work",
    "let",
    "go",
    "word",
    "k",
    "means",
    "k",
    "value",
    "k",
    "nothing",
    "k",
    "basically",
    "means",
    "centroids",
    "k",
    "basically",
    "means",
    "centroids",
    "suppose",
    "data",
    "set",
    "looks",
    "like",
    "let",
    "say",
    "data",
    "set",
    "seeing",
    "data",
    "set",
    "possible",
    "groups",
    "think",
    "definitely",
    "saying",
    "k",
    "equal",
    "2",
    "say",
    "k",
    "equal",
    "two",
    "basically",
    "means",
    "able",
    "get",
    "two",
    "groups",
    "like",
    "every",
    "group",
    "centroid",
    "centroid",
    "point",
    "also",
    "centroid",
    "point",
    "centroid",
    "determine",
    "basically",
    "separate",
    "group",
    "separate",
    "group",
    "definitely",
    "say",
    "fine",
    "two",
    "groups",
    "come",
    "conclusion",
    "two",
    "groups",
    "okay",
    "directly",
    "say",
    "okay",
    "try",
    "seeing",
    "data",
    "data",
    "high",
    "dimension",
    "data",
    "right",
    "right",
    "showing",
    "two",
    "dimension",
    "data",
    "high",
    "dimension",
    "data",
    "definitely",
    "able",
    "see",
    "data",
    "points",
    "plotted",
    "come",
    "conclusion",
    "two",
    "groups",
    "steps",
    "basically",
    "perform",
    "k",
    "mins",
    "first",
    "step",
    "try",
    "different",
    "k",
    "values",
    "try",
    "different",
    "k",
    "values",
    "suitable",
    "k",
    "value",
    "k",
    "nothing",
    "centroids",
    "okay",
    "nothing",
    "centroids",
    "try",
    "different",
    "different",
    "centroids",
    "particular",
    "case",
    "let",
    "say",
    "particular",
    "data",
    "point",
    "actually",
    "start",
    "k",
    "equal",
    "1",
    "2",
    "3",
    "one",
    "want",
    "let",
    "say",
    "going",
    "start",
    "k",
    "equal",
    "2",
    "come",
    "k",
    "equal",
    "2",
    "perfect",
    "value",
    "talk",
    "need",
    "know",
    "concept",
    "called",
    "within",
    "cluster",
    "sum",
    "square",
    "try",
    "different",
    "k",
    "values",
    "let",
    "say",
    "k",
    "equal",
    "2",
    "happen",
    "first",
    "step",
    "select",
    "try",
    "k",
    "values",
    "let",
    "say",
    "considering",
    "k",
    "equal",
    "2",
    "second",
    "step",
    "initialize",
    "k",
    "number",
    "centroids",
    "particular",
    "case",
    "know",
    "k",
    "value",
    "2",
    "initializing",
    "randomly",
    "let",
    "say",
    "k",
    "equal",
    "2",
    "actually",
    "let",
    "say",
    "one",
    "centroid",
    "put",
    "another",
    "color",
    "one",
    "centroid",
    "let",
    "say",
    "another",
    "centroid",
    "initialized",
    "two",
    "centroids",
    "randomly",
    "space",
    "particular",
    "centroid",
    "initializing",
    "centroid",
    "basically",
    "find",
    "points",
    "near",
    "centroid",
    "points",
    "near",
    "centroid",
    "order",
    "find",
    "easy",
    "step",
    "basically",
    "use",
    "ukan",
    "distance",
    "find",
    "distance",
    "points",
    "easy",
    "way",
    "really",
    "want",
    "show",
    "know",
    "like",
    "many",
    "points",
    "want",
    "easy",
    "way",
    "basically",
    "draw",
    "straight",
    "line",
    "let",
    "say",
    "drawing",
    "straight",
    "line",
    "another",
    "color",
    "draw",
    "straight",
    "line",
    "also",
    "draw",
    "one",
    "parallel",
    "line",
    "like",
    "basically",
    "indicates",
    "whichever",
    "points",
    "see",
    "suppose",
    "draw",
    "straight",
    "line",
    "points",
    "able",
    "see",
    "let",
    "say",
    "drawing",
    "one",
    "parallel",
    "line",
    "intersecting",
    "together",
    "definitely",
    "find",
    "let",
    "say",
    "points",
    "nearer",
    "green",
    "line",
    "green",
    "point",
    "actually",
    "going",
    "particular",
    "case",
    "points",
    "seeing",
    "near",
    "green",
    "become",
    "green",
    "color",
    "basically",
    "means",
    "basically",
    "nearer",
    "centroid",
    "whichever",
    "points",
    "nearer",
    "particular",
    "point",
    "become",
    "red",
    "point",
    "basically",
    "means",
    "belongs",
    "group",
    "okay",
    "belongs",
    "group",
    "hope",
    "everybody",
    "clear",
    "till",
    "happen",
    "summation",
    "values",
    "initialize",
    "k",
    "number",
    "centroids",
    "done",
    "try",
    "calculate",
    "distance",
    "try",
    "find",
    "points",
    "nearer",
    "centroid",
    "let",
    "say",
    "one",
    "centroid",
    "another",
    "centroid",
    "seen",
    "okay",
    "points",
    "belong",
    "centroid",
    "near",
    "particular",
    "centroid",
    "becoming",
    "red",
    "based",
    "shortage",
    "distance",
    "becoming",
    "green",
    "next",
    "step",
    "let",
    "see",
    "next",
    "step",
    "going",
    "remove",
    "thing",
    "next",
    "step",
    "entire",
    "points",
    "red",
    "color",
    "average",
    "taken",
    "average",
    "taken",
    "third",
    "step",
    "going",
    "write",
    "going",
    "compute",
    "average",
    "reason",
    "compute",
    "average",
    "need",
    "update",
    "centroid",
    "compute",
    "average",
    "update",
    "centroid",
    "update",
    "centroids",
    "able",
    "see",
    "actually",
    "soon",
    "compute",
    "average",
    "centroid",
    "going",
    "move",
    "location",
    "location",
    "move",
    "obviously",
    "become",
    "somewhere",
    "center",
    "going",
    "rub",
    "new",
    "centroid",
    "point",
    "actually",
    "going",
    "draw",
    "like",
    "let",
    "say",
    "new",
    "centroid",
    "similarly",
    "thing",
    "happen",
    "respect",
    "green",
    "color",
    "respect",
    "green",
    "color",
    "also",
    "happen",
    "green",
    "also",
    "al",
    "get",
    "updated",
    "going",
    "rub",
    "new",
    "green",
    "point",
    "get",
    "updated",
    "happen",
    "distance",
    "calculated",
    "perpendicular",
    "line",
    "calculated",
    "see",
    "points",
    "towards",
    "okay",
    "centroid",
    "based",
    "particular",
    "distance",
    "calculated",
    "see",
    "points",
    "location",
    "update",
    "actually",
    "happen",
    "let",
    "say",
    "one",
    "point",
    "red",
    "color",
    "would",
    "become",
    "green",
    "color",
    "since",
    "updation",
    "happened",
    "perfectly",
    "going",
    "update",
    "going",
    "update",
    "centroid",
    "right",
    "understand",
    "yes",
    "actually",
    "got",
    "perfect",
    "centroid",
    "considered",
    "one",
    "group",
    "basically",
    "considered",
    "another",
    "group",
    "intersect",
    "right",
    "default",
    "intersection",
    "happening",
    "hope",
    "everybody",
    "understood",
    "steps",
    "actually",
    "followed",
    "initializing",
    "centroids",
    "updating",
    "centroids",
    "updating",
    "points",
    "clear",
    "everybody",
    "respect",
    "k",
    "means",
    "let",
    "discuss",
    "one",
    "point",
    "decide",
    "k",
    "value",
    "okay",
    "decide",
    "k",
    "value",
    "deciding",
    "k",
    "value",
    "concept",
    "called",
    "elbow",
    "method",
    "going",
    "basically",
    "define",
    "elbow",
    "method",
    "elbow",
    "method",
    "says",
    "something",
    "much",
    "important",
    "actually",
    "help",
    "us",
    "find",
    "optimized",
    "k",
    "value",
    "whether",
    "k",
    "value",
    "two",
    "whether",
    "uh",
    "k",
    "value",
    "going",
    "three",
    "whether",
    "k",
    "value",
    "going",
    "become",
    "four",
    "always",
    "understand",
    "suppose",
    "data",
    "set",
    "suppose",
    "data",
    "set",
    "initially",
    "let",
    "say",
    "data",
    "points",
    "like",
    "go",
    "ahead",
    "directly",
    "say",
    "say",
    "okay",
    "k",
    "equal",
    "2",
    "going",
    "work",
    "obviously",
    "going",
    "go",
    "iteration",
    "equal",
    "probably",
    "1",
    "10",
    "going",
    "move",
    "towards",
    "iteration",
    "1",
    "10",
    "let",
    "say",
    "every",
    "iteration",
    "construct",
    "graph",
    "respect",
    "k",
    "value",
    "respect",
    "something",
    "called",
    "w",
    "css",
    "w",
    "css",
    "w",
    "css",
    "basically",
    "means",
    "within",
    "cluster",
    "sum",
    "square",
    "okay",
    "meaning",
    "wcss",
    "within",
    "cluster",
    "sum",
    "square",
    "let",
    "say",
    "initially",
    "start",
    "one",
    "centroid",
    "one",
    "centroid",
    "let",
    "say",
    "initialized",
    "one",
    "centroid",
    "basically",
    "initialized",
    "go",
    "compute",
    "distance",
    "every",
    "points",
    "centroid",
    "try",
    "find",
    "distance",
    "distance",
    "value",
    "greater",
    "smaller",
    "smaller",
    "greater",
    "tell",
    "try",
    "calculate",
    "distance",
    "centroid",
    "every",
    "point",
    "within",
    "cluster",
    "sum",
    "square",
    "always",
    "much",
    "greater",
    "let",
    "say",
    "first",
    "point",
    "come",
    "somewhere",
    "going",
    "obviously",
    "greater",
    "let",
    "say",
    "first",
    "point",
    "coming",
    "find",
    "within",
    "k",
    "equal",
    "1",
    "initially",
    "took",
    "found",
    "distance",
    "w",
    "css",
    "huge",
    "value",
    "okay",
    "going",
    "compute",
    "distance",
    "every",
    "point",
    "centroid",
    "next",
    "thing",
    "actually",
    "going",
    "go",
    "next",
    "value",
    "k",
    "equal",
    "2",
    "k",
    "equal",
    "2",
    "initialize",
    "two",
    "points",
    "okay",
    "initialize",
    "two",
    "points",
    "probably",
    "entire",
    "process",
    "written",
    "top",
    "tell",
    "whichever",
    "points",
    "nearer",
    "green",
    "point",
    "compute",
    "distance",
    "whichever",
    "points",
    "nearer",
    "red",
    "point",
    "compute",
    "distance",
    "like",
    "summation",
    "distance",
    "lesser",
    "previous",
    "w",
    "css",
    "obviously",
    "going",
    "lesser",
    "previous",
    "w",
    "css",
    "actually",
    "going",
    "probably",
    "k",
    "equal",
    "2",
    "value",
    "may",
    "come",
    "somewhere",
    "k",
    "equal",
    "3",
    "value",
    "may",
    "come",
    "somewhere",
    "k",
    "equal",
    "4",
    "come",
    "5",
    "6",
    "like",
    "go",
    "probably",
    "join",
    "line",
    "able",
    "see",
    "abrupt",
    "changes",
    "w",
    "css",
    "value",
    "wcss",
    "value",
    "abrupt",
    "changes",
    "basically",
    "called",
    "elbow",
    "curve",
    "say",
    "elbow",
    "curve",
    "shape",
    "elbow",
    "one",
    "specific",
    "point",
    "abrupt",
    "change",
    "straight",
    "reason",
    "basically",
    "say",
    "elbow",
    "okay",
    "important",
    "thing",
    "see",
    "finding",
    "k",
    "value",
    "use",
    "elbow",
    "method",
    "validating",
    "purpose",
    "validate",
    "model",
    "performing",
    "well",
    "use",
    "silard",
    "score",
    "show",
    "time",
    "understand",
    "k",
    "means",
    "clustering",
    "need",
    "update",
    "centroids",
    "based",
    "calculate",
    "distance",
    "k",
    "value",
    "keep",
    "increasing",
    "able",
    "see",
    "distance",
    "become",
    "normal",
    "wcss",
    "value",
    "become",
    "normal",
    "really",
    "need",
    "find",
    "phys",
    "k",
    "value",
    "abrupt",
    "change",
    "see",
    "suppose",
    "abrupt",
    "change",
    "normal",
    "probably",
    "take",
    "k",
    "value",
    "obviously",
    "model",
    "complexity",
    "high",
    "going",
    "check",
    "respect",
    "different",
    "different",
    "k",
    "values",
    "wcss",
    "values",
    "basically",
    "means",
    "value",
    "probably",
    "get",
    "first",
    "need",
    "construct",
    "elbow",
    "curve",
    "see",
    "changes",
    "basically",
    "happening",
    "need",
    "find",
    "abrupt",
    "change",
    "get",
    "abrupt",
    "change",
    "basically",
    "say",
    "may",
    "k",
    "value",
    "k",
    "equal",
    "4",
    "example",
    "telling",
    "unless",
    "really",
    "want",
    "find",
    "cluster",
    "much",
    "simple",
    "take",
    "k",
    "value",
    "initialize",
    "k",
    "number",
    "centroids",
    "compute",
    "average",
    "update",
    "centroids",
    "try",
    "find",
    "distance",
    "try",
    "see",
    "whether",
    "points",
    "changed",
    "continue",
    "process",
    "unless",
    "get",
    "separate",
    "groups",
    "okay",
    "entire",
    "funa",
    "claim",
    "clustering",
    "finally",
    "able",
    "see",
    "respect",
    "k",
    "value",
    "able",
    "get",
    "many",
    "number",
    "groups",
    "k",
    "value",
    "four",
    "basically",
    "means",
    "probably",
    "getting",
    "four",
    "different",
    "groups",
    "like",
    "1",
    "two",
    "right",
    "three",
    "like",
    "four",
    "getting",
    "four",
    "groups",
    "like",
    "k",
    "equal",
    "4",
    "basically",
    "means",
    "k",
    "equal",
    "four",
    "clusters",
    "every",
    "group",
    "centroids",
    "okay",
    "every",
    "group",
    "okay",
    "centroids",
    "much",
    "important",
    "yes",
    "try",
    "show",
    "coding",
    "also",
    "guys",
    "let",
    "go",
    "towards",
    "second",
    "algorithm",
    "second",
    "algorithm",
    "probably",
    "discussing",
    "called",
    "hierarchical",
    "clustering",
    "hierarchal",
    "clustering",
    "much",
    "simple",
    "guys",
    "let",
    "say",
    "data",
    "points",
    "data",
    "points",
    "p1",
    "let",
    "say",
    "p2",
    "hierle",
    "clustering",
    "says",
    "go",
    "step",
    "step",
    "first",
    "thing",
    "try",
    "find",
    "nearest",
    "value",
    "let",
    "say",
    "x",
    "let",
    "say",
    "points",
    "like",
    "p1",
    "point",
    "p2",
    "point",
    "p3",
    "point",
    "p4",
    "point",
    "p5",
    "point",
    "p6",
    "point",
    "p7",
    "point",
    "okay",
    "points",
    "actually",
    "named",
    "let",
    "say",
    "may",
    "nearest",
    "point",
    "combine",
    "together",
    "one",
    "cluster",
    "computed",
    "distance",
    "c",
    "create",
    "one",
    "cluster",
    "happen",
    "right",
    "hand",
    "side",
    "another",
    "notation",
    "may",
    "using",
    "connecting",
    "points",
    "one",
    "suppose",
    "p1",
    "p2",
    "p3",
    "p4",
    "let",
    "say",
    "many",
    "points",
    "probably",
    "also",
    "try",
    "make",
    "p7",
    "points",
    "p7",
    "know",
    "nearest",
    "point",
    "okay",
    "probably",
    "distance",
    "1",
    "2",
    "3",
    "distance",
    "okay",
    "4",
    "5",
    "6",
    "like",
    "lot",
    "distance",
    "hierle",
    "clustering",
    "first",
    "find",
    "nearest",
    "point",
    "try",
    "compute",
    "distance",
    "try",
    "combine",
    "together",
    "one",
    "basically",
    "combine",
    "one",
    "group",
    "okay",
    "p1",
    "p2",
    "combined",
    "let",
    "say",
    "go",
    "find",
    "nearest",
    "point",
    "let",
    "say",
    "p6",
    "p7",
    "near",
    "also",
    "going",
    "combine",
    "one",
    "group",
    "combine",
    "one",
    "group",
    "p6",
    "p7",
    "obviously",
    "l",
    "greater",
    "previous",
    "distance",
    "may",
    "get",
    "kind",
    "computation",
    "another",
    "combination",
    "cluster",
    "form",
    "get",
    "formed",
    "seen",
    "okay",
    "p3",
    "p5",
    "nearer",
    "going",
    "combine",
    "going",
    "basically",
    "combine",
    "p3",
    "p5",
    "okay",
    "let",
    "say",
    "distance",
    "greater",
    "previous",
    "one",
    "basically",
    "going",
    "sh",
    "start",
    "shortest",
    "distance",
    "going",
    "capture",
    "longest",
    "distance",
    "done",
    "see",
    "next",
    "point",
    "near",
    "right",
    "particular",
    "group",
    "p4",
    "going",
    "combine",
    "together",
    "one",
    "group",
    "combine",
    "one",
    "group",
    "p4",
    "get",
    "connected",
    "like",
    "let",
    "say",
    "getting",
    "connected",
    "like",
    "p4",
    "got",
    "connected",
    "nearest",
    "point",
    "whether",
    "p6",
    "p7",
    "group",
    "p1",
    "p2",
    "obviously",
    "see",
    "p1",
    "p2",
    "probably",
    "going",
    "combine",
    "group",
    "together",
    "basically",
    "means",
    "p1",
    "p2",
    "let",
    "say",
    "going",
    "combine",
    "group",
    "group",
    "together",
    "circle",
    "coming",
    "make",
    "dot",
    "let",
    "say",
    "going",
    "combine",
    "group",
    "together",
    "nearest",
    "groups",
    "happen",
    "p1",
    "p2",
    "get",
    "combined",
    "p5",
    "sorry",
    "p4",
    "p5",
    "one",
    "getting",
    "another",
    "line",
    "like",
    "finally",
    "seeing",
    "p6",
    "p7",
    "nearest",
    "group",
    "totally",
    "get",
    "combined",
    "may",
    "look",
    "something",
    "like",
    "become",
    "total",
    "group",
    "like",
    "groups",
    "combined",
    "finally",
    "able",
    "see",
    "one",
    "line",
    "get",
    "combined",
    "like",
    "basically",
    "called",
    "dendogram",
    "dendogram",
    "okay",
    "like",
    "bottom",
    "root",
    "top",
    "question",
    "arises",
    "find",
    "many",
    "groups",
    "find",
    "many",
    "groups",
    "funa",
    "much",
    "clear",
    "guys",
    "need",
    "find",
    "longest",
    "vertical",
    "line",
    "need",
    "find",
    "longest",
    "vertical",
    "line",
    "horizontal",
    "line",
    "pass",
    "horizontal",
    "line",
    "passed",
    "much",
    "important",
    "horizontal",
    "line",
    "pass",
    "basically",
    "meaning",
    "try",
    "find",
    "longest",
    "line",
    "longest",
    "vertical",
    "line",
    "way",
    "none",
    "horizontal",
    "line",
    "passes",
    "horizontal",
    "line",
    "suppose",
    "consider",
    "vertical",
    "line",
    "vertical",
    "line",
    "see",
    "extend",
    "green",
    "line",
    "passing",
    "extend",
    "line",
    "passing",
    "right",
    "extending",
    "line",
    "passing",
    "right",
    "longest",
    "line",
    "may",
    "passing",
    "way",
    "horizontal",
    "line",
    "probably",
    "line",
    "actually",
    "see",
    "basically",
    "create",
    "straight",
    "line",
    "try",
    "find",
    "many",
    "clusters",
    "understanding",
    "many",
    "lines",
    "passing",
    "passing",
    "one",
    "line",
    "two",
    "line",
    "three",
    "line",
    "four",
    "line",
    "basically",
    "means",
    "clusters",
    "four",
    "clusters",
    "basically",
    "calculation",
    "heral",
    "clustering",
    "may",
    "perfect",
    "line",
    "drawn",
    "assumptions",
    "trying",
    "probably",
    "specific",
    "way",
    "okay",
    "already",
    "uploaded",
    "lot",
    "practical",
    "videos",
    "respect",
    "highill",
    "clustering",
    "tell",
    "maximum",
    "effort",
    "maximum",
    "time",
    "taken",
    "taken",
    "k",
    "means",
    "hierle",
    "clustering",
    "question",
    "yes",
    "guys",
    "number",
    "clusters",
    "may",
    "three",
    "showing",
    "many",
    "lines",
    "may",
    "passed",
    "basically",
    "determine",
    "whether",
    "maximum",
    "time",
    "taken",
    "kin",
    "hier",
    "clustering",
    "interview",
    "question",
    "maximum",
    "time",
    "taken",
    "hierarchical",
    "clustering",
    "let",
    "say",
    "many",
    "many",
    "many",
    "data",
    "points",
    "point",
    "time",
    "hierle",
    "clustering",
    "keep",
    "constructing",
    "kind",
    "dendograms",
    "taking",
    "many",
    "many",
    "many",
    "time",
    "lot",
    "time",
    "right",
    "hierle",
    "clustering",
    "take",
    "time",
    "maximum",
    "time",
    "going",
    "basically",
    "take",
    "much",
    "important",
    "understand",
    "making",
    "basically",
    "taking",
    "time",
    "data",
    "set",
    "small",
    "may",
    "go",
    "ahead",
    "hierle",
    "clustering",
    "data",
    "set",
    "large",
    "go",
    "k",
    "means",
    "clustering",
    "go",
    "k",
    "means",
    "clustering",
    "short",
    "take",
    "time",
    "k",
    "min",
    "perform",
    "better",
    "hle",
    "clustering",
    "see",
    "guys",
    "forming",
    "kind",
    "dendograms",
    "right",
    "imagine",
    "10",
    "features",
    "many",
    "data",
    "points",
    "going",
    "cubers",
    "process",
    "even",
    "able",
    "see",
    "dendogram",
    "properly",
    "manually",
    "obviously",
    "respect",
    "k",
    "means",
    "clust",
    "swing",
    "h",
    "mean",
    "clust",
    "swing",
    "hope",
    "everybody",
    "understood",
    "next",
    "topic",
    "focus",
    "validate",
    "see",
    "validate",
    "classification",
    "problem",
    "use",
    "performance",
    "metric",
    "like",
    "confusion",
    "matrix",
    "accuracy",
    "um",
    "different",
    "different",
    "true",
    "positive",
    "rate",
    "precision",
    "recall",
    "validate",
    "clustering",
    "model",
    "model",
    "going",
    "use",
    "something",
    "called",
    "going",
    "basically",
    "use",
    "something",
    "called",
    "sil",
    "score",
    "show",
    "sid",
    "score",
    "going",
    "open",
    "wikipedia",
    "cid",
    "score",
    "looks",
    "like",
    "amazing",
    "topic",
    "okay",
    "validate",
    "whether",
    "model",
    "basically",
    "perfect",
    "three",
    "four",
    "model",
    "perfect",
    "three",
    "suppose",
    "find",
    "k",
    "value",
    "three",
    "find",
    "see",
    "one",
    "one",
    "issue",
    "k",
    "means",
    "one",
    "issue",
    "k",
    "means",
    "forgot",
    "tell",
    "let",
    "say",
    "data",
    "point",
    "looks",
    "like",
    "suppose",
    "data",
    "points",
    "like",
    "data",
    "points",
    "looks",
    "like",
    "let",
    "say",
    "like",
    "one",
    "issue",
    "suppose",
    "try",
    "make",
    "cluster",
    "obviously",
    "saying",
    "k",
    "value",
    "two",
    "okay",
    "particular",
    "case",
    "suppose",
    "one",
    "cluster",
    "another",
    "cluster",
    "right",
    "wrong",
    "initialization",
    "points",
    "okay",
    "understand",
    "suppose",
    "initialize",
    "randomly",
    "centroids",
    "like",
    "may",
    "happen",
    "possibility",
    "may",
    "also",
    "three",
    "clusters",
    "like",
    "like",
    "like",
    "kind",
    "clusters",
    "one",
    "cluster",
    "one",
    "cluster",
    "one",
    "cluster",
    "initialization",
    "centroids",
    "one",
    "condition",
    "far",
    "initialize",
    "centroids",
    "far",
    "point",
    "time",
    "able",
    "find",
    "centroid",
    "exactly",
    "center",
    "keep",
    "updating",
    "keep",
    "going",
    "ahead",
    "right",
    "initialize",
    "far",
    "situation",
    "probably",
    "wanted",
    "get",
    "real",
    "thing",
    "get",
    "two",
    "centroids",
    "probably",
    "getting",
    "three",
    "centroids",
    "right",
    "problem",
    "algorithm",
    "called",
    "k",
    "means",
    "k",
    "means",
    "probably",
    "show",
    "practical",
    "make",
    "sure",
    "centroids",
    "initialized",
    "far",
    "okay",
    "centroids",
    "basically",
    "initialized",
    "far",
    "see",
    "practical",
    "application",
    "specifically",
    "centroids",
    "basically",
    "used",
    "let",
    "go",
    "ahead",
    "let",
    "show",
    "respect",
    "sid",
    "clust",
    "string",
    "solo",
    "color",
    "string",
    "going",
    "explain",
    "amazing",
    "way",
    "important",
    "someone",
    "says",
    "validate",
    "validate",
    "cluster",
    "model",
    "point",
    "time",
    "basically",
    "use",
    "site",
    "used",
    "used",
    "respect",
    "used",
    "respect",
    "k",
    "means",
    "used",
    "hierle",
    "mean",
    "right",
    "want",
    "validate",
    "validate",
    "okay",
    "basically",
    "going",
    "see",
    "c",
    "clustering",
    "important",
    "things",
    "first",
    "important",
    "thing",
    "try",
    "find",
    "try",
    "find",
    "ofi",
    "try",
    "find",
    "ofi",
    "see",
    "ofi",
    "basically",
    "see",
    "ofi",
    "nothing",
    "see",
    "three",
    "major",
    "steps",
    "happens",
    "order",
    "validate",
    "cluster",
    "model",
    "help",
    "solo",
    "first",
    "thing",
    "probably",
    "take",
    "one",
    "cluster",
    "okay",
    "one",
    "point",
    "centroid",
    "let",
    "say",
    "going",
    "going",
    "whatever",
    "points",
    "inside",
    "cluster",
    "going",
    "compute",
    "distance",
    "going",
    "summation",
    "also",
    "going",
    "average",
    "distance",
    "see",
    "said",
    "distance",
    "comma",
    "j",
    "basically",
    "means",
    "point",
    "j",
    "basically",
    "means",
    "points",
    "nothing",
    "centroid",
    "nothing",
    "centroid",
    "let",
    "say",
    "centroid",
    "going",
    "compute",
    "distance",
    "mentioned",
    "value",
    "see",
    "actually",
    "dividing",
    "c",
    "minus",
    "one",
    "short",
    "actually",
    "trying",
    "calculate",
    "average",
    "distance",
    "first",
    "point",
    "actually",
    "computing",
    "ofi",
    "similarly",
    "next",
    "point",
    "suppose",
    "computed",
    "ofi",
    "next",
    "next",
    "need",
    "compute",
    "b",
    "ofi",
    "b",
    "ofi",
    "b",
    "ofi",
    "nothing",
    "multiple",
    "clusters",
    "k",
    "means",
    "problem",
    "statement",
    "try",
    "find",
    "nearest",
    "cluster",
    "okay",
    "suppose",
    "let",
    "say",
    "nearest",
    "cluster",
    "variety",
    "points",
    "b",
    "ofi",
    "basically",
    "says",
    "try",
    "compute",
    "distance",
    "point",
    "point",
    "centroid",
    "sorry",
    "cluster",
    "cluster",
    "one",
    "cluster",
    "two",
    "actually",
    "going",
    "going",
    "compute",
    "distance",
    "point",
    "point",
    "point",
    "point",
    "point",
    "point",
    "point",
    "point",
    "point",
    "point",
    "point",
    "point",
    "every",
    "point",
    "actually",
    "going",
    "compute",
    "distance",
    "point",
    "done",
    "go",
    "ahead",
    "next",
    "point",
    "try",
    "compute",
    "distance",
    "get",
    "particular",
    "distance",
    "going",
    "going",
    "average",
    "average",
    "tell",
    "try",
    "find",
    "relationship",
    "b",
    "cluster",
    "model",
    "good",
    "greater",
    "b",
    "b",
    "greater",
    "ofi",
    "good",
    "clustering",
    "model",
    "good",
    "clustering",
    "model",
    "greater",
    "b",
    "greater",
    "b",
    "whether",
    "b",
    "greater",
    "really",
    "good",
    "model",
    "obviously",
    "distance",
    "b",
    "greater",
    "good",
    "model",
    "basically",
    "means",
    "talk",
    "sloid",
    "clustering",
    "values",
    "value",
    "towards",
    "basically",
    "means",
    "good",
    "model",
    "good",
    "clustering",
    "model",
    "values",
    "towards",
    "negative",
    "one",
    "basically",
    "means",
    "condition",
    "getting",
    "applied",
    "condition",
    "basically",
    "say",
    "basically",
    "means",
    "distance",
    "far",
    "cluster",
    "distance",
    "information",
    "getting",
    "portrayed",
    "importance",
    "cid",
    "clustering",
    "finally",
    "apply",
    "formula",
    "cid",
    "clustering",
    "able",
    "see",
    "sloid",
    "clustering",
    "nothing",
    "let",
    "rub",
    "everything",
    "guys",
    "let",
    "show",
    "cid",
    "clustering",
    "cid",
    "clustering",
    "formula",
    "something",
    "like",
    "b",
    "solid",
    "clustering",
    "formula",
    "b",
    "minus",
    "max",
    "comma",
    "b",
    "c",
    "greater",
    "one",
    "right",
    "getting",
    "value",
    "1",
    "value",
    "towards",
    "one",
    "good",
    "model",
    "values",
    "towards",
    "minus1",
    "bad",
    "model",
    "towards",
    "minus1",
    "basically",
    "means",
    "obviously",
    "greater",
    "b",
    "outcome",
    "respect",
    "cot",
    "crust",
    "string",
    "equal",
    "zero",
    "basically",
    "means",
    "still",
    "model",
    "needs",
    "uh",
    "per",
    "basically",
    "clustering",
    "needs",
    "improved",
    "nothing",
    "one",
    "data",
    "point",
    "read",
    "guys",
    "data",
    "point",
    "cluster",
    "c",
    "hope",
    "everybody",
    "understood",
    "let",
    "go",
    "ahead",
    "let",
    "discuss",
    "next",
    "topic",
    "obviously",
    "finished",
    "solart",
    "clustering",
    "let",
    "discuss",
    "something",
    "called",
    "db",
    "scan",
    "db",
    "scan",
    "clustering",
    "amazing",
    "clustering",
    "algorithm",
    "try",
    "understand",
    "actually",
    "db",
    "clustering",
    "probably",
    "able",
    "understand",
    "lot",
    "things",
    "db",
    "scan",
    "clustering",
    "important",
    "things",
    "let",
    "start",
    "respect",
    "db",
    "scan",
    "clustering",
    "let",
    "understand",
    "important",
    "points",
    "first",
    "point",
    "really",
    "need",
    "remember",
    "something",
    "called",
    "score",
    "point",
    "points",
    "also",
    "talk",
    "say",
    "core",
    "points",
    "say",
    "points",
    "first",
    "point",
    "probably",
    "discuss",
    "something",
    "called",
    "min",
    "points",
    "second",
    "point",
    "probably",
    "discuss",
    "something",
    "called",
    "score",
    "points",
    "third",
    "thing",
    "probably",
    "discuss",
    "something",
    "called",
    "border",
    "points",
    "fourth",
    "point",
    "definitely",
    "talk",
    "something",
    "called",
    "noise",
    "point",
    "okay",
    "guys",
    "tell",
    "c",
    "clustering",
    "kind",
    "groups",
    "think",
    "help",
    "two",
    "different",
    "clusters",
    "may",
    "combine",
    "two",
    "like",
    "help",
    "two",
    "different",
    "clusters",
    "may",
    "combine",
    "something",
    "like",
    "right",
    "understand",
    "problem",
    "basically",
    "happening",
    "second",
    "clustering",
    "actually",
    "outliers",
    "let",
    "say",
    "let",
    "say",
    "one",
    "thing",
    "nicely",
    "put",
    "okay",
    "let",
    "say",
    "one",
    "point",
    "one",
    "point",
    "clustering",
    "probably",
    "get",
    "one",
    "cluster",
    "may",
    "get",
    "another",
    "cluster",
    "somewhere",
    "understand",
    "one",
    "thing",
    "point",
    "definitely",
    "outlier",
    "even",
    "though",
    "outlier",
    "help",
    "k",
    "means",
    "actually",
    "actually",
    "grouping",
    "another",
    "group",
    "scenario",
    "wherein",
    "kind",
    "clustering",
    "algorithm",
    "leave",
    "outlier",
    "separately",
    "outlier",
    "particular",
    "algorithm",
    "b",
    "basically",
    "uh",
    "using",
    "db",
    "scan",
    "relieve",
    "outlier",
    "point",
    "called",
    "noisy",
    "point",
    "noisy",
    "point",
    "also",
    "say",
    "outlier",
    "noise",
    "point",
    "kind",
    "algorithm",
    "want",
    "skip",
    "outliers",
    "definitely",
    "use",
    "db",
    "scan",
    "density",
    "based",
    "spatial",
    "clustering",
    "application",
    "noise",
    "amazing",
    "algorithm",
    "definitely",
    "tried",
    "using",
    "lot",
    "nowadays",
    "use",
    "k",
    "means",
    "hier",
    "means",
    "instead",
    "use",
    "kind",
    "algorithm",
    "see",
    "important",
    "things",
    "first",
    "need",
    "go",
    "ahead",
    "min",
    "points",
    "min",
    "points",
    "first",
    "thing",
    "need",
    "min",
    "points",
    "min",
    "points",
    "kind",
    "hyperparameter",
    "basically",
    "says",
    "hyper",
    "parameter",
    "says",
    "also",
    "value",
    "called",
    "epsilon",
    "forgot",
    "write",
    "called",
    "epsilon",
    "epsilon",
    "mean",
    "epsilon",
    "basically",
    "means",
    "point",
    "like",
    "take",
    "epsilon",
    "nothing",
    "radius",
    "specific",
    "circle",
    "radius",
    "specific",
    "circle",
    "okay",
    "epsilon",
    "nothing",
    "radius",
    "specific",
    "minimum",
    "points",
    "equal",
    "4",
    "mean",
    "let",
    "say",
    "taken",
    "point",
    "let",
    "say",
    "point",
    "drawn",
    "circle",
    "looks",
    "like",
    "let",
    "say",
    "epsilon",
    "value",
    "okay",
    "epsilon",
    "value",
    "say",
    "min",
    "point",
    "point",
    "equal",
    "4",
    "hyper",
    "parameter",
    "basically",
    "means",
    "four",
    "least",
    "four",
    "points",
    "near",
    "particular",
    "circle",
    "based",
    "epsilon",
    "value",
    "happen",
    "point",
    "red",
    "point",
    "actually",
    "become",
    "core",
    "point",
    "core",
    "point",
    "basically",
    "given",
    "least",
    "many",
    "number",
    "min",
    "points",
    "inside",
    "near",
    "particular",
    "within",
    "epsilon",
    "okay",
    "within",
    "particular",
    "cluster",
    "suppose",
    "cluster",
    "help",
    "epsilon",
    "actually",
    "created",
    "particular",
    "unit",
    "epsilon",
    "simply",
    "take",
    "unit",
    "distance",
    "epsilon",
    "value",
    "also",
    "get",
    "selected",
    "way",
    "show",
    "show",
    "practical",
    "application",
    "worry",
    "next",
    "thing",
    "let",
    "say",
    "let",
    "say",
    "another",
    "another",
    "point",
    "let",
    "say",
    "another",
    "point",
    "circle",
    "respect",
    "epsilon",
    "created",
    "let",
    "say",
    "one",
    "point",
    "one",
    "point",
    "inside",
    "particular",
    "cluster",
    "point",
    "point",
    "becomes",
    "something",
    "called",
    "border",
    "point",
    "border",
    "point",
    "border",
    "point",
    "also",
    "discussed",
    "right",
    "border",
    "point",
    "also",
    "saying",
    "least",
    "one",
    "least",
    "one",
    "one",
    "present",
    "become",
    "border",
    "point",
    "force",
    "definitely",
    "become",
    "core",
    "point",
    "core",
    "point",
    "like",
    "red",
    "color",
    "one",
    "scenario",
    "suppose",
    "one",
    "cluster",
    "let",
    "say",
    "epsilon",
    "suppose",
    "points",
    "near",
    "definitely",
    "become",
    "noise",
    "point",
    "noise",
    "point",
    "nothing",
    "cluster",
    "okay",
    "actually",
    "discussed",
    "noise",
    "point",
    "also",
    "hope",
    "everybody",
    "able",
    "understand",
    "key",
    "terms",
    "basically",
    "happening",
    "whenever",
    "noise",
    "point",
    "like",
    "particular",
    "scenario",
    "noise",
    "point",
    "find",
    "points",
    "inside",
    "core",
    "point",
    "border",
    "point",
    "find",
    "inside",
    "going",
    "get",
    "neglected",
    "basically",
    "means",
    "basically",
    "treated",
    "outlier",
    "hope",
    "everybody",
    "able",
    "understand",
    "point",
    "treated",
    "outlier",
    "also",
    "treated",
    "noise",
    "point",
    "never",
    "taken",
    "inside",
    "group",
    "okay",
    "never",
    "never",
    "taken",
    "inside",
    "group",
    "suppose",
    "set",
    "points",
    "see",
    "basically",
    "red",
    "core",
    "also",
    "border",
    "point",
    "making",
    "multiple",
    "circles",
    "definitely",
    "say",
    "defining",
    "core",
    "points",
    "border",
    "points",
    "combined",
    "single",
    "group",
    "okay",
    "combined",
    "single",
    "group",
    "connection",
    "see",
    "yellow",
    "line",
    "basically",
    "created",
    "one",
    "sorry",
    "yellow",
    "point",
    "basically",
    "created",
    "one",
    "epsilon",
    "one",
    "one",
    "core",
    "point",
    "remember",
    "least",
    "one",
    "core",
    "point",
    "okay",
    "one",
    "point",
    "one",
    "core",
    "point",
    "least",
    "one",
    "core",
    "point",
    "become",
    "border",
    "point",
    "become",
    "border",
    "point",
    "basically",
    "means",
    "yes",
    "part",
    "specific",
    "group",
    "whenever",
    "noise",
    "going",
    "neglect",
    "wherever",
    "broader",
    "core",
    "points",
    "going",
    "combine",
    "show",
    "one",
    "diagram",
    "amazing",
    "diagram",
    "help",
    "understand",
    "k",
    "means",
    "clustering",
    "hier",
    "mean",
    "clustering",
    "see",
    "everybody",
    "right",
    "hand",
    "side",
    "diagram",
    "see",
    "based",
    "db",
    "scan",
    "clustering",
    "left",
    "hand",
    "side",
    "basically",
    "traditional",
    "clustering",
    "method",
    "let",
    "say",
    "k",
    "means",
    "one",
    "think",
    "better",
    "see",
    "outliers",
    "combined",
    "inside",
    "group",
    "whichever",
    "nearer",
    "core",
    "point",
    "broader",
    "point",
    "separate",
    "separate",
    "groups",
    "actually",
    "created",
    "right",
    "amazing",
    "db",
    "scan",
    "clustering",
    "db",
    "scan",
    "clustering",
    "pretty",
    "much",
    "amazing",
    "basically",
    "outcome",
    "c",
    "clustering",
    "see",
    "points",
    "also",
    "taken",
    "blue",
    "color",
    "one",
    "group",
    "considering",
    "one",
    "group",
    "able",
    "determine",
    "amazing",
    "groups",
    "saying",
    "guys",
    "directly",
    "use",
    "db",
    "scan",
    "without",
    "worrying",
    "anything",
    "let",
    "focus",
    "practical",
    "part",
    "uh",
    "going",
    "give",
    "github",
    "link",
    "everybody",
    "download",
    "code",
    "guys",
    "given",
    "github",
    "link",
    "quickly",
    "download",
    "keep",
    "file",
    "ready",
    "going",
    "open",
    "anaconda",
    "prompt",
    "probably",
    "open",
    "jupyter",
    "notbook",
    "one",
    "practical",
    "problem",
    "given",
    "link",
    "guys",
    "please",
    "open",
    "going",
    "today",
    "amazing",
    "able",
    "see",
    "amazing",
    "things",
    "come",
    "know",
    "fitting",
    "underfitting",
    "happening",
    "know",
    "real",
    "value",
    "right",
    "clustering",
    "underfitting",
    "overfitting",
    "uh",
    "things",
    "importing",
    "first",
    "try",
    "cin",
    "clustering",
    "silot",
    "scoring",
    "probably",
    "see",
    "output",
    "um",
    "db",
    "scan",
    "also",
    "let",
    "say",
    "db",
    "scan",
    "also",
    "uh",
    "things",
    "basically",
    "imported",
    "one",
    "cin",
    "clustering",
    "one",
    "sout",
    "samples",
    "sout",
    "scores",
    "present",
    "sk",
    "learn",
    "present",
    "metrics",
    "basically",
    "means",
    "use",
    "specific",
    "parameter",
    "validate",
    "clustering",
    "models",
    "okay",
    "try",
    "execute",
    "apart",
    "mat",
    "plot",
    "lib",
    "trying",
    "import",
    "numai",
    "trying",
    "import",
    "executing",
    "perfectly",
    "next",
    "thing",
    "next",
    "step",
    "generating",
    "sample",
    "data",
    "make",
    "underscore",
    "blobs",
    "first",
    "trying",
    "generate",
    "samples",
    "two",
    "features",
    "saying",
    "okay",
    "four",
    "centroids",
    "c",
    "centroids",
    "features",
    "trying",
    "generate",
    "x",
    "data",
    "randomly",
    "particular",
    "data",
    "set",
    "basically",
    "used",
    "performing",
    "clustering",
    "algorithms",
    "okay",
    "forget",
    "range",
    "undor",
    "ncore",
    "clusters",
    "need",
    "try",
    "different",
    "different",
    "clusters",
    "try",
    "find",
    "solid",
    "score",
    "right",
    "initialized",
    "2",
    "3",
    "4",
    "5",
    "6",
    "values",
    "simple",
    "go",
    "probably",
    "see",
    "x",
    "data",
    "x",
    "data",
    "look",
    "something",
    "like",
    "x",
    "data",
    "two",
    "features",
    "data",
    "one",
    "feature",
    "output",
    "belongs",
    "specific",
    "class",
    "okay",
    "actually",
    "help",
    "make",
    "underscore",
    "blobs",
    "let",
    "say",
    "apply",
    "kin",
    "clustering",
    "algorithm",
    "said",
    "using",
    "w",
    "css",
    "w",
    "css",
    "basically",
    "means",
    "within",
    "cluster",
    "sum",
    "square",
    "going",
    "import",
    "k",
    "means",
    "range",
    "1a",
    "11",
    "basically",
    "means",
    "going",
    "use",
    "different",
    "different",
    "k",
    "values",
    "centroid",
    "values",
    "try",
    "c",
    "minimal",
    "wcss",
    "value",
    "try",
    "draw",
    "graph",
    "actually",
    "shown",
    "respect",
    "elbow",
    "method",
    "basically",
    "also",
    "using",
    "k",
    "means",
    "number",
    "clusters",
    "initialization",
    "technique",
    "using",
    "k",
    "means",
    "points",
    "centroids",
    "initialized",
    "points",
    "far",
    "random",
    "state",
    "equal",
    "zero",
    "fit",
    "finally",
    "wcss",
    "upend",
    "cins",
    "doin",
    "inertia",
    "okay",
    "dot",
    "inertia",
    "give",
    "distance",
    "centroids",
    "points",
    "going",
    "append",
    "wcss",
    "value",
    "finally",
    "plot",
    "see",
    "plotting",
    "obviously",
    "seeing",
    "graph",
    "graph",
    "looks",
    "like",
    "elbow",
    "okay",
    "graph",
    "looks",
    "like",
    "elbow",
    "point",
    "actually",
    "going",
    "consider",
    "see",
    "last",
    "abrupt",
    "change",
    "talk",
    "last",
    "abrupt",
    "change",
    "specific",
    "value",
    "respect",
    "okay",
    "one",
    "specific",
    "value",
    "respect",
    "abrupt",
    "change",
    "changes",
    "normal",
    "going",
    "basically",
    "select",
    "k",
    "equal",
    "4",
    "actually",
    "going",
    "help",
    "sart",
    "help",
    "cl",
    "score",
    "going",
    "compare",
    "whether",
    "k",
    "equal",
    "4",
    "valid",
    "going",
    "valid",
    "going",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "see",
    "going",
    "see",
    "n",
    "clusters",
    "equal",
    "4",
    "actually",
    "able",
    "find",
    "prediction",
    "specifically",
    "output",
    "okay",
    "done",
    "see",
    "code",
    "okay",
    "code",
    "huge",
    "code",
    "actually",
    "taken",
    "code",
    "directly",
    "sk",
    "learn",
    "page",
    "silo",
    "go",
    "see",
    "code",
    "directly",
    "given",
    "going",
    "talk",
    "like",
    "important",
    "things",
    "need",
    "see",
    "respect",
    "different",
    "different",
    "clusters",
    "see",
    "see",
    "clusters",
    "2",
    "3",
    "4",
    "5",
    "6",
    "going",
    "basically",
    "compare",
    "whether",
    "k",
    "value",
    "four",
    "help",
    "solid",
    "scoring",
    "let",
    "go",
    "see",
    "applying",
    "one",
    "first",
    "go",
    "respect",
    "loop",
    "ncore",
    "clusters",
    "range",
    "underscore",
    "clusters",
    "different",
    "different",
    "cluster",
    "values",
    "first",
    "start",
    "two",
    "see",
    "initialize",
    "cluster",
    "cluster",
    "value",
    "random",
    "generator",
    "seed",
    "10",
    "reproducibility",
    "ncore",
    "clusters",
    "first",
    "take",
    "took",
    "two",
    "fit",
    "predict",
    "x",
    "fit",
    "predictor",
    "x",
    "using",
    "score",
    "x",
    "comma",
    "cluster",
    "label",
    "going",
    "understand",
    "solo",
    "discuss",
    "try",
    "find",
    "clusters",
    "clusters",
    "like",
    "try",
    "calculate",
    "distance",
    "try",
    "compute",
    "b",
    "finally",
    "try",
    "compute",
    "score",
    "value",
    "minus1",
    "valu",
    "towards",
    "one",
    "better",
    "right",
    "things",
    "already",
    "discussed",
    "specific",
    "function",
    "give",
    "solo",
    "average",
    "value",
    "solid",
    "value",
    "okay",
    "done",
    "continuously",
    "another",
    "another",
    "things",
    "actually",
    "find",
    "value",
    "see",
    "code",
    "see",
    "nothing",
    "nothing",
    "complex",
    "okay",
    "display",
    "data",
    "properly",
    "form",
    "graphs",
    "okay",
    "form",
    "graphs",
    "telling",
    "write",
    "code",
    "directly",
    "taken",
    "uh",
    "sk",
    "learn",
    "page",
    "solid",
    "okay",
    "try",
    "see",
    "particular",
    "uh",
    "plotting",
    "diagrams",
    "definitely",
    "figure",
    "let",
    "see",
    "try",
    "execute",
    "try",
    "find",
    "output",
    "see",
    "ncore",
    "cluster",
    "equal",
    "2",
    "average",
    "solid",
    "score",
    "70",
    "told",
    "value",
    "actually",
    "getting",
    "704",
    "good",
    "ncore",
    "cluster",
    "equal",
    "3",
    "588",
    "ncore",
    "cluster",
    "equal",
    "4",
    "getting",
    "65",
    "pretty",
    "much",
    "amazing",
    "ncore",
    "cluster",
    "equal",
    "5",
    "average",
    "score",
    "563",
    "ncore",
    "cluster",
    "equal",
    "6",
    "saying",
    "directly",
    "actually",
    "say",
    "fine",
    "cluster",
    "equal",
    "2",
    "getting",
    "amazing",
    "score",
    "704",
    "obviously",
    "getting",
    "highest",
    "value",
    "select",
    "ncore",
    "cluster",
    "isal",
    "two",
    "okay",
    "directly",
    "conclude",
    "need",
    "also",
    "see",
    "feature",
    "value",
    "cluster",
    "value",
    "also",
    "coming",
    "negative",
    "value",
    "also",
    "need",
    "check",
    "go",
    "see",
    "first",
    "one",
    "respect",
    "first",
    "one",
    "see",
    "get",
    "getting",
    "value",
    "0",
    "1",
    "going",
    "going",
    "min",
    "definitely",
    "two",
    "clusters",
    "able",
    "solve",
    "problem",
    "keep",
    "like",
    "definitely",
    "chance",
    "may",
    "may",
    "perform",
    "well",
    "may",
    "chance",
    "k",
    "uh",
    "k",
    "equal",
    "2",
    "may",
    "perform",
    "well",
    "okay",
    "may",
    "chance",
    "let",
    "see",
    "next",
    "one",
    "next",
    "one",
    "see",
    "one",
    "cluster",
    "value",
    "negative",
    "value",
    "negative",
    "basically",
    "means",
    "ai",
    "obviously",
    "greater",
    "b",
    "ofi",
    "going",
    "prer",
    "negative",
    "values",
    "even",
    "though",
    "cluster",
    "looks",
    "better",
    "understand",
    "problem",
    "respect",
    "cluster",
    "take",
    "cluster",
    "probably",
    "compute",
    "distance",
    "point",
    "point",
    "probably",
    "compute",
    "point",
    "point",
    "point",
    "point",
    "point",
    "obviously",
    "nearer",
    "right",
    "obviously",
    "nearer",
    "reason",
    "getting",
    "negative",
    "value",
    "okay",
    "negative",
    "value",
    "uh",
    "output",
    "score",
    "point",
    "see",
    "dotted",
    "points",
    "score",
    "58",
    "whatever",
    "basically",
    "score",
    "obviously",
    "basically",
    "indicates",
    "point",
    "near",
    "cluster",
    "point",
    "nearer",
    "actually",
    "getting",
    "negative",
    "value",
    "right",
    "really",
    "need",
    "understand",
    "okay",
    "similarly",
    "go",
    "respect",
    "ncore",
    "cluster",
    "equal",
    "4",
    "looks",
    "good",
    "negative",
    "value",
    "see",
    "cooly",
    "basically",
    "divided",
    "points",
    "amazing",
    "inly",
    "help",
    "k",
    "equal",
    "4",
    "right",
    "similarly",
    "go",
    "five",
    "obviously",
    "see",
    "negative",
    "values",
    "dotted",
    "line",
    "negative",
    "value",
    "respect",
    "six",
    "also",
    "negative",
    "values",
    "definitely",
    "go",
    "six",
    "may",
    "either",
    "go",
    "four",
    "may",
    "either",
    "go",
    "two",
    "whenever",
    "options",
    "always",
    "take",
    "bigger",
    "number",
    "instead",
    "two",
    "take",
    "four",
    "four",
    "greater",
    "two",
    "able",
    "create",
    "generalized",
    "model",
    "actually",
    "going",
    "take",
    "equal",
    "4",
    "k",
    "equal",
    "4",
    "compare",
    "elbow",
    "method",
    "also",
    "got",
    "four",
    "right",
    "actually",
    "matching",
    "indicates",
    "help",
    "clustering",
    "siluette",
    "score",
    "definitely",
    "come",
    "conclusion",
    "validate",
    "clustering",
    "model",
    "amazing",
    "way",
    "hope",
    "everybody",
    "able",
    "understand",
    "way",
    "basically",
    "validate",
    "model",
    "definitely",
    "try",
    "understand",
    "code",
    "definitely",
    "till",
    "understood",
    "going",
    "get",
    "average",
    "value",
    "iore",
    "clusters",
    "whatever",
    "cluster",
    "matching",
    "mapping",
    "basically",
    "giving",
    "session",
    "uh",
    "yes",
    "today",
    "session",
    "efficiently",
    "covered",
    "many",
    "topics",
    "covered",
    "kin",
    "hierle",
    "clustering",
    "solid",
    "score",
    "db",
    "clustering",
    "tomorrow",
    "session",
    "topics",
    "probably",
    "pending",
    "first",
    "start",
    "svm",
    "svr",
    "second",
    "go",
    "ahead",
    "xg",
    "boost",
    "third",
    "cover",
    "pca",
    "let",
    "see",
    "whether",
    "able",
    "complete",
    "session",
    "uh",
    "one",
    "one",
    "amazing",
    "thing",
    "want",
    "teach",
    "guys",
    "many",
    "people",
    "ask",
    "definition",
    "bias",
    "variance",
    "guys",
    "uh",
    "many",
    "people",
    "get",
    "confused",
    "talk",
    "bias",
    "variance",
    "know",
    "let",
    "say",
    "uh",
    "model",
    "training",
    "data",
    "set",
    "gives",
    "us",
    "somewhere",
    "around",
    "90",
    "accuracy",
    "let",
    "say",
    "getting",
    "90",
    "accuracy",
    "test",
    "data",
    "may",
    "probably",
    "getting",
    "somewhere",
    "around",
    "70",
    "accuracy",
    "tell",
    "scenario",
    "basically",
    "people",
    "saying",
    "okay",
    "fine",
    "overfitting",
    "say",
    "overfitting",
    "basically",
    "mention",
    "overfitting",
    "low",
    "bias",
    "high",
    "variance",
    "right",
    "many",
    "people",
    "get",
    "confused",
    "krish",
    "tell",
    "exact",
    "definition",
    "bias",
    "variance",
    "low",
    "bias",
    "obviously",
    "saying",
    "training",
    "performed",
    "like",
    "model",
    "performing",
    "well",
    "help",
    "training",
    "data",
    "set",
    "respect",
    "test",
    "data",
    "set",
    "model",
    "performing",
    "well",
    "respect",
    "training",
    "data",
    "set",
    "always",
    "say",
    "bias",
    "respect",
    "test",
    "data",
    "set",
    "always",
    "say",
    "variance",
    "need",
    "understand",
    "definition",
    "bias",
    "let",
    "write",
    "definition",
    "bias",
    "definitely",
    "write",
    "bias",
    "phenomena",
    "skews",
    "result",
    "algorithm",
    "favor",
    "favor",
    "idea",
    "idea",
    "make",
    "understand",
    "definition",
    "uh",
    "um",
    "understand",
    "understand",
    "understand",
    "actually",
    "written",
    "phenomena",
    "skewes",
    "result",
    "algorithm",
    "favor",
    "idea",
    "whenever",
    "say",
    "specific",
    "idea",
    "idea",
    "talk",
    "training",
    "data",
    "set",
    "initially",
    "train",
    "specific",
    "model",
    "suppose",
    "specific",
    "model",
    "training",
    "specific",
    "training",
    "data",
    "set",
    "training",
    "data",
    "set",
    "based",
    "definition",
    "basically",
    "say",
    "phenomenon",
    "skews",
    "result",
    "algorithm",
    "favor",
    "idea",
    "specific",
    "training",
    "data",
    "set",
    "even",
    "though",
    "training",
    "particular",
    "model",
    "training",
    "data",
    "set",
    "data",
    "set",
    "may",
    "may",
    "favor",
    "may",
    "basically",
    "means",
    "may",
    "perform",
    "well",
    "may",
    "perform",
    "well",
    "performing",
    "well",
    "basically",
    "means",
    "accuracy",
    "accuracy",
    "better",
    "point",
    "time",
    "say",
    "see",
    "accuracy",
    "better",
    "time",
    "say",
    "come",
    "two",
    "terms",
    "obviously",
    "understand",
    "okay",
    "two",
    "scenarios",
    "bias",
    "favor",
    "basically",
    "means",
    "performing",
    "well",
    "respect",
    "training",
    "data",
    "set",
    "basically",
    "say",
    "high",
    "bu",
    "able",
    "perform",
    "well",
    "training",
    "data",
    "set",
    "say",
    "low",
    "bias",
    "hope",
    "everybody",
    "able",
    "understand",
    "specific",
    "thing",
    "many",
    "many",
    "many",
    "people",
    "kind",
    "kind",
    "confusion",
    "similarly",
    "talk",
    "variance",
    "let",
    "say",
    "variance",
    "need",
    "understand",
    "definition",
    "definition",
    "much",
    "important",
    "okay",
    "talk",
    "definition",
    "variance",
    "going",
    "refer",
    "like",
    "variance",
    "refers",
    "changes",
    "model",
    "using",
    "using",
    "different",
    "portion",
    "training",
    "test",
    "data",
    "let",
    "understand",
    "particular",
    "definition",
    "variance",
    "refers",
    "changes",
    "model",
    "using",
    "different",
    "proportion",
    "test",
    "training",
    "data",
    "test",
    "data",
    "obviously",
    "know",
    "whenever",
    "initially",
    "model",
    "understand",
    "definition",
    "everything",
    "make",
    "sense",
    "basically",
    "training",
    "initially",
    "training",
    "data",
    "okay",
    "divide",
    "data",
    "set",
    "see",
    "data",
    "set",
    "whenever",
    "working",
    "divide",
    "two",
    "parts",
    "one",
    "train",
    "data",
    "test",
    "data",
    "okay",
    "tra",
    "test",
    "data",
    "part",
    "particular",
    "data",
    "set",
    "right",
    "suppose",
    "particular",
    "training",
    "data",
    "gets",
    "trained",
    "performs",
    "well",
    "actually",
    "talking",
    "bias",
    "come",
    "respect",
    "prediction",
    "specific",
    "model",
    "point",
    "time",
    "use",
    "training",
    "data",
    "basically",
    "means",
    "training",
    "data",
    "may",
    "similar",
    "also",
    "use",
    "test",
    "data",
    "test",
    "data",
    "kind",
    "predictions",
    "predictions",
    "prediction",
    "may",
    "get",
    "two",
    "scenario",
    "may",
    "get",
    "two",
    "scenario",
    "basically",
    "mentioned",
    "variance",
    "refers",
    "changes",
    "model",
    "using",
    "using",
    "different",
    "portion",
    "training",
    "test",
    "data",
    "refers",
    "changes",
    "basically",
    "means",
    "whether",
    "able",
    "give",
    "good",
    "prediction",
    "wrong",
    "predictions",
    "particular",
    "scenario",
    "gives",
    "good",
    "prediction",
    "may",
    "definitely",
    "say",
    "low",
    "variance",
    "basically",
    "means",
    "accuracy",
    "accuracy",
    "respect",
    "test",
    "data",
    "also",
    "good",
    "probably",
    "get",
    "bad",
    "probably",
    "get",
    "bad",
    "accuracy",
    "time",
    "basically",
    "say",
    "high",
    "variance",
    "talk",
    "three",
    "scenarios",
    "let",
    "say",
    "model",
    "one",
    "model",
    "two",
    "model",
    "three",
    "scenario",
    "let",
    "consider",
    "model",
    "one",
    "training",
    "accuracy",
    "90",
    "test",
    "accuracy",
    "75",
    "similarly",
    "train",
    "accuracy",
    "60",
    "test",
    "accuracy",
    "55",
    "similarly",
    "train",
    "accuracy",
    "90",
    "test",
    "accuracy",
    "92",
    "tell",
    "things",
    "getting",
    "obviously",
    "directly",
    "say",
    "fine",
    "training",
    "accuracy",
    "better",
    "talking",
    "bias",
    "basically",
    "indicates",
    "low",
    "bias",
    "since",
    "test",
    "accuracy",
    "bad",
    "compared",
    "train",
    "accuracy",
    "less",
    "basically",
    "going",
    "say",
    "high",
    "variance",
    "understand",
    "respect",
    "definition",
    "similarly",
    "say",
    "high",
    "bias",
    "high",
    "variance",
    "obviously",
    "performing",
    "well",
    "another",
    "scenario",
    "last",
    "last",
    "scenario",
    "scenario",
    "want",
    "low",
    "bias",
    "low",
    "variance",
    "okay",
    "many",
    "many",
    "people",
    "basically",
    "asked",
    "definition",
    "respect",
    "bias",
    "variance",
    "actually",
    "discussed",
    "indicates",
    "gives",
    "generalized",
    "model",
    "aim",
    "working",
    "data",
    "scientist",
    "hope",
    "understood",
    "basic",
    "difference",
    "v",
    "bias",
    "variance",
    "able",
    "give",
    "lot",
    "examples",
    "lot",
    "understanding",
    "respect",
    "hope",
    "actually",
    "got",
    "particular",
    "uh",
    "understanding",
    "uh",
    "two",
    "terms",
    "specifically",
    "talk",
    "high",
    "bias",
    "low",
    "bias",
    "high",
    "variance",
    "low",
    "variance",
    "right",
    "side",
    "guys",
    "uh",
    "uh",
    "hope",
    "understood",
    "okay",
    "let",
    "take",
    "let",
    "consider",
    "data",
    "set",
    "credit",
    "let",
    "say",
    "approval",
    "going",
    "take",
    "sample",
    "data",
    "set",
    "understand",
    "xg",
    "boost",
    "work",
    "suppose",
    "salary",
    "less",
    "equal",
    "50",
    "credit",
    "bad",
    "approval",
    "loan",
    "approval",
    "zero",
    "basically",
    "means",
    "get",
    "less",
    "equal",
    "50",
    "credit",
    "score",
    "good",
    "probably",
    "approval",
    "one",
    "less",
    "equal",
    "50",
    "good",
    "going",
    "get",
    "one",
    "greater",
    "50",
    "bad",
    "obviously",
    "approval",
    "zero",
    "greater",
    "50",
    "good",
    "going",
    "get",
    "one",
    "greater",
    "50k",
    "probably",
    "normal",
    "also",
    "going",
    "get",
    "data",
    "set",
    "xg",
    "boost",
    "classifier",
    "work",
    "understand",
    "full",
    "form",
    "xg",
    "boost",
    "extreme",
    "gradient",
    "boosting",
    "extreme",
    "gradient",
    "boosting",
    "basically",
    "understand",
    "extreme",
    "gradient",
    "boosting",
    "extreme",
    "gradient",
    "boosting",
    "uh",
    "actually",
    "used",
    "solve",
    "classification",
    "regression",
    "problem",
    "statement",
    "first",
    "let",
    "understand",
    "basically",
    "exib",
    "basically",
    "actually",
    "talk",
    "xg",
    "boost",
    "understand",
    "boosting",
    "technique",
    "internally",
    "tries",
    "use",
    "decision",
    "tree",
    "decision",
    "tre",
    "basically",
    "getting",
    "constructed",
    "case",
    "xv",
    "boost",
    "basically",
    "solved",
    "going",
    "discuss",
    "whenever",
    "start",
    "exib",
    "boost",
    "classifier",
    "understand",
    "first",
    "create",
    "specific",
    "base",
    "model",
    "suppose",
    "say",
    "base",
    "model",
    "base",
    "model",
    "weak",
    "learner",
    "okay",
    "base",
    "model",
    "always",
    "give",
    "output",
    "probability",
    "case",
    "classification",
    "problem",
    "suppose",
    "say",
    "probability",
    "try",
    "create",
    "field",
    "field",
    "called",
    "residual",
    "field",
    "first",
    "base",
    "model",
    "going",
    "data",
    "set",
    "give",
    "train",
    "always",
    "give",
    "output",
    "dummy",
    "base",
    "model",
    "tell",
    "probability",
    "output",
    "want",
    "calculate",
    "residual",
    "basically",
    "means",
    "need",
    "subtract",
    "approval",
    "minus",
    "particular",
    "value",
    "value",
    "0",
    "1",
    "be5",
    "1",
    "be5",
    "0",
    "1",
    "uh",
    "also",
    "let",
    "consider",
    "one",
    "record",
    "uh",
    "specific",
    "record",
    "anything",
    "uh",
    "want",
    "keep",
    "records",
    "let",
    "consider",
    "one",
    "record",
    "less",
    "equal",
    "50k",
    "credit",
    "scod",
    "normal",
    "going",
    "get",
    "zero",
    "also",
    "try",
    "find",
    "residual",
    "minus5",
    "first",
    "step",
    "hope",
    "everybody",
    "understood",
    "create",
    "base",
    "model",
    "okay",
    "base",
    "model",
    "much",
    "important",
    "create",
    "decision",
    "tree",
    "sequential",
    "manner",
    "first",
    "sequential",
    "base",
    "tree",
    "also",
    "decision",
    "tree",
    "kind",
    "thing",
    "consider",
    "base",
    "model",
    "takes",
    "inputs",
    "gives",
    "default",
    "probability",
    "05",
    "let",
    "go",
    "ahead",
    "understand",
    "steps",
    "constructing",
    "decision",
    "tree",
    "creating",
    "base",
    "model",
    "first",
    "step",
    "create",
    "uh",
    "binary",
    "decision",
    "tree",
    "going",
    "write",
    "steps",
    "please",
    "make",
    "sure",
    "note",
    "create",
    "binary",
    "tree",
    "binary",
    "decision",
    "tree",
    "using",
    "features",
    "second",
    "step",
    "basically",
    "define",
    "say",
    "okay",
    "second",
    "step",
    "actually",
    "calculate",
    "similarity",
    "weight",
    "calculate",
    "similarity",
    "weight",
    "talk",
    "similarity",
    "weight",
    "exactly",
    "want",
    "use",
    "formula",
    "summation",
    "residual",
    "square",
    "divided",
    "summation",
    "probability",
    "1",
    "minus",
    "probability",
    "plus",
    "lambda",
    "talk",
    "exactly",
    "lambda",
    "kind",
    "hyperparameter",
    "overfit",
    "third",
    "thing",
    "calculate",
    "information",
    "gain",
    "okay",
    "information",
    "gain",
    "steps",
    "basically",
    "use",
    "constructing",
    "solving",
    "uh",
    "creating",
    "hd",
    "boost",
    "classifier",
    "first",
    "step",
    "create",
    "inary",
    "decision",
    "tree",
    "using",
    "feature",
    "go",
    "ahead",
    "calculating",
    "similarity",
    "weight",
    "finally",
    "go",
    "ahead",
    "calculate",
    "information",
    "gain",
    "go",
    "ahead",
    "let",
    "understand",
    "let",
    "try",
    "find",
    "okay",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "construct",
    "decision",
    "tree",
    "said",
    "let",
    "consider",
    "considering",
    "salary",
    "feature",
    "based",
    "using",
    "salary",
    "feature",
    "actually",
    "going",
    "going",
    "take",
    "node",
    "going",
    "split",
    "remember",
    "whenever",
    "creating",
    "decision",
    "tree",
    "particular",
    "case",
    "binary",
    "decision",
    "tree",
    "let",
    "say",
    "salary",
    "one",
    "less",
    "equal",
    "one",
    "greater",
    "50",
    "two",
    "obviously",
    "case",
    "binary",
    "case",
    "credit",
    "three",
    "categories",
    "also",
    "show",
    "split",
    "happen",
    "get",
    "converted",
    "binary",
    "team",
    "less",
    "equal",
    "50k",
    "greater",
    "50k",
    "let",
    "go",
    "ahead",
    "understand",
    "many",
    "vales",
    "salary",
    "see",
    "split",
    "definitely",
    "see",
    "going",
    "use",
    "residual",
    "probably",
    "train",
    "entire",
    "model",
    "really",
    "wanted",
    "find",
    "residual",
    "initially",
    "residuals",
    "one",
    "resid",
    "finally",
    "minus",
    "total",
    "residuals",
    "suppose",
    "make",
    "split",
    "less",
    "equal",
    "50",
    "first",
    "less",
    "equal",
    "50",
    "residuals",
    "things",
    "going",
    "minus5",
    "less",
    "equal",
    "50",
    "going",
    "05",
    "less",
    "equal",
    "50",
    "going",
    "less",
    "equal",
    "one",
    "going",
    "remove",
    "last5",
    "nothing",
    "min",
    "hope",
    "understood",
    "split",
    "half",
    "things",
    "came",
    "remaining",
    "half",
    "greater",
    "equal",
    "greater",
    "50",
    "one",
    "value",
    "one",
    "value",
    "one",
    "value",
    "min",
    "finally",
    "residuals",
    "get",
    "guys",
    "see",
    "base",
    "model",
    "default",
    "giving",
    "first",
    "data",
    "goes",
    "default",
    "probability",
    "going",
    "get",
    "residual",
    "basically",
    "calculated",
    "probability",
    "approval",
    "probability",
    "minus",
    "approval",
    "subtract",
    "0",
    "sorry",
    "going",
    "rub",
    "subtract",
    "0",
    "going",
    "get",
    "1",
    "going",
    "get",
    "1",
    "going",
    "get",
    "everybody",
    "hope",
    "much",
    "clear",
    "respect",
    "first",
    "step",
    "constructed",
    "binary",
    "tree",
    "second",
    "step",
    "says",
    "calculate",
    "similarity",
    "weight",
    "calculate",
    "similarity",
    "weight",
    "similarity",
    "weight",
    "formula",
    "sum",
    "residual",
    "square",
    "residual",
    "square",
    "let",
    "say",
    "going",
    "calculate",
    "uh",
    "going",
    "calculate",
    "okay",
    "similarity",
    "weight",
    "particular",
    "case",
    "go",
    "calculate",
    "similarity",
    "weight",
    "summation",
    "residual",
    "square",
    "residual",
    "values",
    "residual",
    "valu",
    "going",
    "summation",
    "square",
    "okay",
    "value",
    "square",
    "see",
    "sum",
    "residual",
    "square",
    "everybody",
    "see",
    "sum",
    "residual",
    "squares",
    "think",
    "sum",
    "residual",
    "squares",
    "particular",
    "case",
    "take",
    "values",
    "like",
    "whole",
    "square",
    "right",
    "going",
    "squaring",
    "divided",
    "understand",
    "divided",
    "divided",
    "probability",
    "1",
    "minus",
    "probability",
    "get",
    "probability",
    "value",
    "get",
    "probability",
    "value",
    "value",
    "get",
    "probability",
    "value",
    "base",
    "model",
    "right",
    "basically",
    "going",
    "say",
    "going",
    "summation",
    "probability",
    "1",
    "minus",
    "probability",
    "1",
    "minus",
    "probability",
    "basically",
    "means",
    "every",
    "point",
    "every",
    "point",
    "probability",
    "see",
    "probability",
    "basically",
    "coming",
    "base",
    "model",
    "pro",
    "point",
    "going",
    "come",
    "compute",
    "two",
    "things",
    "one",
    "probability",
    "1",
    "minus",
    "probability",
    "going",
    "summ",
    "like",
    "four",
    "times",
    "1",
    "1",
    "finally",
    "able",
    "see",
    "one",
    "1",
    "total",
    "things",
    "respect",
    "hope",
    "understood",
    "till",
    "uh",
    "able",
    "understand",
    "done",
    "summation",
    "uh",
    "residual",
    "square",
    "remaining",
    "probability",
    "multiplied",
    "1",
    "minus",
    "probability",
    "tell",
    "able",
    "find",
    "cancel",
    "value",
    "going",
    "become",
    "zero",
    "entire",
    "value",
    "going",
    "become",
    "zer",
    "0",
    "divided",
    "anything",
    "0er",
    "hope",
    "everybody",
    "understood",
    "similarity",
    "weight",
    "specific",
    "node",
    "want",
    "write",
    "nothing",
    "zero",
    "may",
    "considering",
    "lambda",
    "value",
    "okay",
    "initially",
    "initialize",
    "lambda",
    "1",
    "talk",
    "hyper",
    "parameter",
    "let",
    "consider",
    "1",
    "1",
    "plus",
    "0",
    "let",
    "let",
    "consider",
    "lambda",
    "value",
    "0",
    "let",
    "say",
    "right",
    "okay",
    "going",
    "make",
    "lambda",
    "equal",
    "to0",
    "going",
    "talk",
    "kind",
    "hyper",
    "parameter",
    "z",
    "summation",
    "summation",
    "able",
    "see",
    "going",
    "get",
    "zero",
    "calculation",
    "done",
    "got",
    "uh",
    "sumission",
    "weight",
    "equal",
    "z",
    "let",
    "go",
    "ahead",
    "calculate",
    "sumission",
    "weight",
    "next",
    "node",
    "first",
    "square",
    "whole",
    "squar",
    "also",
    "is5",
    "let",
    "want",
    "find",
    "similarity",
    "weight",
    "see",
    "going",
    "repeat",
    "whole",
    "squ",
    "since",
    "three",
    "points",
    "going",
    "basically",
    "use",
    "probability",
    "1",
    "minus",
    "probability",
    "one",
    "point",
    "plus",
    "probability",
    "1",
    "minus",
    "probability",
    "second",
    "point",
    "probability",
    "1",
    "minus",
    "probability",
    "third",
    "point",
    "lambda",
    "zero",
    "going",
    "write",
    "anything",
    "go",
    "let",
    "go",
    "calculation",
    "node",
    "5",
    "5",
    "becomes",
    "zero",
    "whole",
    "square",
    "right",
    "going",
    "get",
    "calculation",
    "going",
    "get",
    "75",
    "value",
    "going",
    "1x3",
    "nothing",
    "at33",
    "similarity",
    "weight",
    "node",
    "node",
    "is33",
    "see",
    "probability",
    "multiplied",
    "1",
    "minus",
    "probability",
    "okay",
    "next",
    "step",
    "calculate",
    "information",
    "gain",
    "know",
    "calculate",
    "information",
    "gain",
    "let",
    "computation",
    "also",
    "root",
    "node",
    "also",
    "go",
    "ahead",
    "calculate",
    "similarity",
    "weight",
    "okay",
    "base",
    "model",
    "probability",
    "is5",
    "understand",
    "dummy",
    "dummy",
    "model",
    "put",
    "condition",
    "saying",
    "going",
    "give",
    "one",
    "guys",
    "root",
    "node",
    "see",
    "calculate",
    "minus1",
    "gone",
    "also",
    "gone",
    "also",
    "gone",
    "divided",
    "something",
    "tell",
    "guys",
    "root",
    "node",
    "similarity",
    "similarity",
    "weight",
    "similarity",
    "weight",
    "calculation",
    "everyone",
    "one",
    "know",
    "25",
    "divided",
    "getting",
    "similarity",
    "weight",
    "nothing",
    "1",
    "7",
    "divide",
    "1",
    "7",
    "say",
    "1",
    "7",
    "is42",
    "nothing",
    "want",
    "calculate",
    "root",
    "node",
    "similarity",
    "weight",
    "is4",
    "know",
    "0",
    "33",
    "see",
    "calculate",
    "information",
    "gain",
    "next",
    "step",
    "third",
    "step",
    "calculate",
    "information",
    "gain",
    "information",
    "gain",
    "nothing",
    "particular",
    "case",
    "root",
    "node",
    "similarity",
    "weight",
    "try",
    "add",
    "getting",
    "minus",
    "particular",
    "top",
    "root",
    "node",
    "whatever",
    "split",
    "happened",
    "similarity",
    "weight",
    "take",
    "0",
    "point",
    "nothing",
    "open",
    "calculator",
    "33",
    "nothing",
    "getting",
    "information",
    "gain",
    "information",
    "gain",
    "specific",
    "tree",
    "got",
    "as19",
    "obviously",
    "know",
    "features",
    "get",
    "selected",
    "based",
    "information",
    "gain",
    "let",
    "say",
    "highest",
    "information",
    "gain",
    "given",
    "salary",
    "okay",
    "go",
    "ahead",
    "split",
    "let",
    "go",
    "ahead",
    "split",
    "know",
    "information",
    "gain",
    "is1",
    "n",
    "information",
    "gain",
    "basically",
    "used",
    "select",
    "specific",
    "node",
    "split",
    "happen",
    "go",
    "split",
    "let",
    "say",
    "going",
    "split",
    "next",
    "feature",
    "one",
    "credit",
    "going",
    "take",
    "credit",
    "going",
    "take",
    "credit",
    "binary",
    "split",
    "may",
    "considering",
    "chish",
    "three",
    "categories",
    "going",
    "basically",
    "particular",
    "split",
    "right",
    "know",
    "split",
    "three",
    "categories",
    "case",
    "definitely",
    "particular",
    "case",
    "split",
    "probably",
    "going",
    "let",
    "consider",
    "two",
    "categories",
    "like",
    "good",
    "normal",
    "one",
    "side",
    "bad",
    "one",
    "side",
    "becomes",
    "binary",
    "split",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "see",
    "many",
    "data",
    "points",
    "fall",
    "many",
    "data",
    "points",
    "fall",
    "writing",
    "data",
    "points",
    "let",
    "say",
    "less",
    "see",
    "go",
    "path",
    "less",
    "equal",
    "50",
    "go",
    "path",
    "b",
    "probably",
    "going",
    "get",
    "much",
    "residual",
    "going",
    "get",
    "one",
    "residual",
    "first",
    "one",
    "residual",
    "similarly",
    "see",
    "less",
    "equal",
    "50",
    "good",
    "right",
    "good",
    "normal",
    "five",
    "come",
    "hope",
    "everybody",
    "able",
    "understand",
    "see",
    "second",
    "record",
    "less",
    "equal",
    "50",
    "go",
    "path",
    "good",
    "come",
    "less",
    "equal",
    "50",
    "good",
    "going",
    "get",
    "1",
    "more5",
    "go",
    "respect",
    "greater",
    "equal",
    "50",
    "coming",
    "worry",
    "right",
    "less",
    "equal",
    "50",
    "normal",
    "right",
    "many",
    "records",
    "definitely",
    "coming",
    "one",
    "record",
    "basically",
    "coming",
    "start",
    "process",
    "start",
    "process",
    "process",
    "going",
    "try",
    "calculate",
    "similarity",
    "weight",
    "order",
    "calculate",
    "similarity",
    "weight",
    "basically",
    "say",
    "similarity",
    "weight",
    "become",
    "divided",
    "025",
    "whole",
    "square",
    "right",
    "whole",
    "square",
    "residual",
    "square",
    "right",
    "summation",
    "residual",
    "square",
    "one",
    "residual",
    "square",
    "become",
    "actually",
    "going",
    "going",
    "basically",
    "write",
    "1",
    "nothing",
    "one",
    "data",
    "point",
    "nothing",
    "nothing",
    "right",
    "particular",
    "case",
    "get",
    "similarity",
    "weight",
    "hope",
    "everybody",
    "getting",
    "one",
    "similarity",
    "weight",
    "want",
    "compute",
    "simple",
    "get",
    "cancelled",
    "025",
    "divided",
    "um",
    "say",
    "one",
    "like",
    "75",
    "also",
    "1",
    "by3",
    "nothing",
    "33",
    "similarity",
    "weight",
    "be33",
    "calculate",
    "information",
    "gain",
    "node",
    "add",
    "see",
    "1",
    "add",
    "like",
    "1",
    "minus",
    "0",
    "zero",
    "information",
    "gain",
    "similarity",
    "weight",
    "uh",
    "one",
    "basically",
    "0",
    "right",
    "particular",
    "credit",
    "node",
    "similarity",
    "weight",
    "zero",
    "1",
    "minus",
    "0",
    "like",
    "split",
    "happen",
    "different",
    "different",
    "node",
    "getting",
    "binary",
    "split",
    "comparing",
    "based",
    "information",
    "gain",
    "one",
    "coming",
    "good",
    "let",
    "say",
    "created",
    "path",
    "designed",
    "developed",
    "entire",
    "binary",
    "decision",
    "tree",
    "speciality",
    "xg",
    "boost",
    "going",
    "see",
    "everybody",
    "going",
    "let",
    "consider",
    "inferencing",
    "part",
    "let",
    "say",
    "record",
    "going",
    "go",
    "going",
    "calculate",
    "output",
    "first",
    "went",
    "base",
    "model",
    "let",
    "go",
    "ahead",
    "see",
    "inferencing",
    "happen",
    "suppose",
    "record",
    "going",
    "right",
    "first",
    "record",
    "go",
    "base",
    "model",
    "base",
    "model",
    "giving",
    "probability",
    "first",
    "base",
    "model",
    "basically",
    "giving",
    "base",
    "based",
    "05",
    "calculate",
    "real",
    "probability",
    "calculate",
    "real",
    "probability",
    "okay",
    "apply",
    "something",
    "called",
    "logs",
    "basically",
    "say",
    "log",
    "p",
    "1us",
    "p",
    "formula",
    "basically",
    "apply",
    "case",
    "base",
    "model",
    "try",
    "see",
    "nothing",
    "log",
    "of5",
    "nothing",
    "zero",
    "log",
    "one",
    "nothing",
    "zero",
    "first",
    "case",
    "whenever",
    "record",
    "goes",
    "getting",
    "zero",
    "value",
    "okay",
    "zero",
    "value",
    "plus",
    "plus",
    "go",
    "binary",
    "decision",
    "tree",
    "record",
    "go",
    "binary",
    "decision",
    "tre",
    "whatever",
    "value",
    "getting",
    "actually",
    "adding",
    "go",
    "goes",
    "first",
    "let",
    "see",
    "branch",
    "following",
    "following",
    "less",
    "equal",
    "50",
    "branch",
    "first",
    "branch",
    "bad",
    "go",
    "follow",
    "see",
    "similarity",
    "weight",
    "one",
    "similarity",
    "weight",
    "basically",
    "one",
    "case",
    "case",
    "pass",
    "learning",
    "rate",
    "parameter",
    "specifically",
    "learning",
    "rate",
    "multiplied",
    "1",
    "one",
    "similarity",
    "weight",
    "one",
    "basically",
    "first",
    "references",
    "alpha",
    "learning",
    "rate",
    "small",
    "value",
    "based",
    "learning",
    "parameter",
    "use",
    "like",
    "defined",
    "learning",
    "parameters",
    "elsewhere",
    "top",
    "apply",
    "activation",
    "function",
    "called",
    "sigmoid",
    "since",
    "classification",
    "problem",
    "apply",
    "activation",
    "function",
    "called",
    "sigmoid",
    "hope",
    "know",
    "use",
    "sigmoid",
    "based",
    "based",
    "alpha",
    "value",
    "based",
    "output",
    "0",
    "1",
    "hope",
    "getting",
    "guys",
    "entire",
    "inferencing",
    "probably",
    "happen",
    "similarly",
    "try",
    "construct",
    "kind",
    "decision",
    "tree",
    "parall",
    "also",
    "write",
    "entire",
    "function",
    "look",
    "something",
    "like",
    "alpha",
    "0",
    "alpha",
    "1",
    "decision",
    "tree",
    "1",
    "output",
    "alpha",
    "2",
    "decision",
    "tree",
    "output",
    "alpha",
    "3",
    "decision",
    "3",
    "output",
    "like",
    "alpha",
    "4",
    "decision",
    "3",
    "output",
    "fourth",
    "decision",
    "tree",
    "like",
    "alpha",
    "n",
    "decision",
    "tree",
    "n",
    "output",
    "output",
    "finally",
    "trying",
    "inference",
    "new",
    "record",
    "reason",
    "say",
    "boosting",
    "see",
    "understand",
    "going",
    "add",
    "every",
    "decision",
    "tree",
    "output",
    "slowly",
    "finally",
    "get",
    "output",
    "respect",
    "working",
    "decision",
    "tree",
    "xg",
    "boost",
    "actually",
    "work",
    "credit",
    "needs",
    "simplified",
    "yes",
    "see",
    "like",
    "similarly",
    "split",
    "credit",
    "help",
    "like",
    "make",
    "blue",
    "green",
    "one",
    "side",
    "normal",
    "one",
    "side",
    "whichever",
    "giving",
    "information",
    "gain",
    "taken",
    "consideration",
    "right",
    "entire",
    "x",
    "boost",
    "classifier",
    "works",
    "difficult",
    "basically",
    "calculate",
    "things",
    "reason",
    "say",
    "xg",
    "boost",
    "also",
    "blackbox",
    "model",
    "basically",
    "blackb",
    "model",
    "prone",
    "overfitting",
    "see",
    "one",
    "stage",
    "also",
    "need",
    "perform",
    "hyperparameter",
    "tuning",
    "specifically",
    "say",
    "pruning",
    "tend",
    "pre",
    "pruning",
    "since",
    "combining",
    "multiple",
    "decision",
    "trees",
    "decision",
    "tree",
    "decision",
    "tree",
    "one",
    "independent",
    "decision",
    "tree",
    "created",
    "parall",
    "create",
    "one",
    "decision",
    "tree",
    "looking",
    "like",
    "see",
    "finally",
    "look",
    "base",
    "model",
    "data",
    "data",
    "go",
    "decision",
    "tree",
    "actually",
    "done",
    "binary",
    "split",
    "different",
    "different",
    "records",
    "make",
    "another",
    "decision",
    "tree",
    "binary",
    "tree",
    "splits",
    "look",
    "like",
    "base",
    "model",
    "getting",
    "value",
    "zero",
    "alpha",
    "1",
    "multiplied",
    "decision",
    "tree",
    "1",
    "alpha",
    "2",
    "multiplied",
    "decision",
    "tree",
    "2",
    "like",
    "keep",
    "continuously",
    "adding",
    "decision",
    "trees",
    "unless",
    "entire",
    "things",
    "becomes",
    "strong",
    "learner",
    "basically",
    "combination",
    "things",
    "hope",
    "everybody",
    "able",
    "understand",
    "xg",
    "boost",
    "classifier",
    "may",
    "thinking",
    "regressor",
    "work",
    "want",
    "regressor",
    "problem",
    "statement",
    "also",
    "decision",
    "tree",
    "get",
    "constructed",
    "based",
    "independent",
    "features",
    "lambda",
    "value",
    "hyperparameter",
    "basically",
    "set",
    "lambda",
    "value",
    "help",
    "cross",
    "validation",
    "uh",
    "let",
    "go",
    "ahead",
    "discuss",
    "ex",
    "boost",
    "regressor",
    "second",
    "algorithm",
    "probably",
    "discuss",
    "something",
    "called",
    "xg",
    "boost",
    "regressor",
    "x",
    "boost",
    "regressor",
    "actually",
    "work",
    "fundamental",
    "follow",
    "random",
    "forest",
    "random",
    "forest",
    "completely",
    "different",
    "bagging",
    "happens",
    "bagging",
    "happens",
    "let",
    "go",
    "ahead",
    "regressor",
    "going",
    "take",
    "example",
    "let",
    "say",
    "many",
    "experience",
    "many",
    "gap",
    "based",
    "need",
    "determine",
    "salary",
    "salary",
    "output",
    "feature",
    "let",
    "say",
    "experience",
    "2",
    "3",
    "4",
    "okay",
    "gap",
    "let",
    "say",
    "yes",
    "yes",
    "yes",
    "let",
    "say",
    "salary",
    "somewhere",
    "around",
    "40k",
    "41k",
    "52k",
    "uh",
    "let",
    "see",
    "data",
    "set",
    "60k",
    "62k",
    "first",
    "step",
    "classifier",
    "created",
    "base",
    "model",
    "also",
    "try",
    "create",
    "base",
    "model",
    "first",
    "base",
    "model",
    "output",
    "give",
    "give",
    "average",
    "values",
    "average",
    "values",
    "okay",
    "average",
    "value",
    "40",
    "81",
    "52",
    "60",
    "62",
    "average",
    "nothing",
    "51k",
    "default",
    "create",
    "base",
    "model",
    "take",
    "input",
    "give",
    "output",
    "51",
    "first",
    "step",
    "based",
    "try",
    "calculate",
    "residual",
    "calculate",
    "residual",
    "subtract",
    "40",
    "51k",
    "basically",
    "11k",
    "uh",
    "10",
    "k",
    "10",
    "k",
    "10",
    "1",
    "9",
    "11",
    "hope",
    "everybody",
    "able",
    "get",
    "let",
    "say",
    "make",
    "42k",
    "okay",
    "making",
    "calculation",
    "little",
    "bit",
    "easy",
    "9",
    "residual",
    "first",
    "step",
    "construct",
    "uh",
    "decision",
    "tree",
    "let",
    "say",
    "say",
    "going",
    "use",
    "experience",
    "experience",
    "node",
    "based",
    "experience",
    "node",
    "features",
    "take",
    "residuals",
    "11",
    "99",
    "1",
    "99",
    "11",
    "split",
    "based",
    "experience",
    "continuous",
    "feature",
    "basically",
    "split",
    "respect",
    "continuous",
    "feature",
    "already",
    "shown",
    "decision",
    "tree",
    "residual",
    "40",
    "minus",
    "11",
    "k",
    "9",
    "k",
    "uh",
    "1",
    "k",
    "9",
    "k",
    "11k",
    "9k",
    "create",
    "take",
    "first",
    "node",
    "going",
    "use",
    "experience",
    "feature",
    "know",
    "values",
    "things",
    "going",
    "come",
    "11k",
    "root",
    "node",
    "9",
    "1",
    "9",
    "11",
    "going",
    "going",
    "binary",
    "split",
    "binary",
    "split",
    "happen",
    "based",
    "continuous",
    "feature",
    "experienced",
    "two",
    "types",
    "records",
    "may",
    "get",
    "one",
    "less",
    "equal",
    "two",
    "one",
    "greater",
    "2",
    "less",
    "equal",
    "two",
    "one",
    "greater",
    "two",
    "less",
    "equal",
    "two",
    "split",
    "let",
    "see",
    "many",
    "values",
    "getting",
    "less",
    "equal",
    "two",
    "get",
    "one",
    "value",
    "actually",
    "going",
    "get",
    "values",
    "9",
    "1",
    "9",
    "11",
    "going",
    "calculate",
    "similarity",
    "weight",
    "similarity",
    "weight",
    "little",
    "bit",
    "formula",
    "change",
    "respect",
    "regression",
    "similarity",
    "weight",
    "nothing",
    "summation",
    "residual",
    "squares",
    "divided",
    "number",
    "residuals",
    "plus",
    "lambda",
    "going",
    "consider",
    "lambda",
    "zero",
    "hyper",
    "parameter",
    "tuning",
    "value",
    "lambda",
    "basically",
    "means",
    "penalizing",
    "respect",
    "residuals",
    "formula",
    "going",
    "apply",
    "okay",
    "let",
    "see",
    "first",
    "number",
    "want",
    "apply",
    "get",
    "applied",
    "going",
    "write",
    "formula",
    "better",
    "let",
    "say",
    "similarity",
    "weight",
    "equal",
    "summation",
    "residual",
    "square",
    "number",
    "residuals",
    "plus",
    "lambda",
    "see",
    "previously",
    "using",
    "probability",
    "things",
    "using",
    "want",
    "calculate",
    "similarity",
    "weight",
    "become",
    "121",
    "divided",
    "number",
    "residual",
    "1",
    "plus",
    "lambda",
    "0",
    "going",
    "121",
    "going",
    "calculate",
    "similarity",
    "weight",
    "nothing",
    "121",
    "probably",
    "take",
    "alpha",
    "let",
    "let",
    "one",
    "thing",
    "probably",
    "take",
    "uh",
    "probably",
    "take",
    "alpha",
    "equal",
    "1",
    "happen",
    "take",
    "alpha",
    "equal",
    "1",
    "think",
    "may",
    "happen",
    "may",
    "directly",
    "penalize",
    "similarity",
    "weight",
    "right",
    "adding",
    "one",
    "okay",
    "let",
    "also",
    "suppose",
    "say",
    "going",
    "take",
    "alpha",
    "equal",
    "1",
    "happen",
    "formula",
    "become",
    "121",
    "divided",
    "number",
    "residual",
    "1",
    "1",
    "nothing",
    "let",
    "say",
    "similarity",
    "weight",
    "similarly",
    "go",
    "ahead",
    "compute",
    "similarity",
    "weight",
    "next",
    "one",
    "become",
    "9",
    "9",
    "9",
    "11",
    "whole",
    "square",
    "divided",
    "4",
    "1",
    "get",
    "subtracted",
    "12",
    "squ",
    "nothing",
    "14",
    "4",
    "144",
    "divid",
    "5",
    "go",
    "ahead",
    "calculate",
    "144",
    "id",
    "5",
    "nothing",
    "get",
    "similarity",
    "weight",
    "similarly",
    "go",
    "ahead",
    "calculate",
    "similarity",
    "weight",
    "top",
    "one",
    "nothing",
    "11",
    "sorry",
    "11",
    "11",
    "9",
    "1",
    "9",
    "11",
    "divided",
    "1",
    "2",
    "3",
    "4",
    "5",
    "5",
    "1",
    "6",
    "getting",
    "subtracted",
    "1x",
    "6",
    "anyhow",
    "whole",
    "square",
    "right",
    "anyhow",
    "1x",
    "6",
    "1x",
    "6",
    "similarity",
    "weight",
    "okay",
    "hits",
    "okay",
    "finally",
    "information",
    "gain",
    "need",
    "compute",
    "much",
    "simple",
    "information",
    "gain",
    "minus",
    "1x",
    "6",
    "try",
    "get",
    "whatever",
    "trying",
    "get",
    "tell",
    "output",
    "28",
    "88",
    "change",
    "second",
    "3",
    "understand",
    "worry",
    "calculation",
    "automatically",
    "things",
    "okay",
    "worry",
    "see",
    "decision",
    "tree",
    "splitted",
    "number",
    "times",
    "probably",
    "next",
    "split",
    "next",
    "split",
    "something",
    "like",
    "experience",
    "two",
    "splits",
    "may",
    "happen",
    "respect",
    "less",
    "equal",
    "less",
    "equal",
    "greater",
    "probably",
    "gives",
    "information",
    "gain",
    "better",
    "split",
    "happen",
    "like",
    "otherwise",
    "whichever",
    "gives",
    "better",
    "information",
    "split",
    "basically",
    "happen",
    "like",
    "hope",
    "like",
    "let",
    "say",
    "split",
    "required",
    "11",
    "11",
    "9",
    "1",
    "comma",
    "9a",
    "11",
    "okay",
    "less",
    "equal",
    "two",
    "records",
    "definitely",
    "go",
    "two",
    "record",
    "definitely",
    "go",
    "try",
    "calculate",
    "similarity",
    "weight",
    "nothing",
    "11",
    "9",
    "11",
    "9",
    "whole",
    "ided",
    "2",
    "1",
    "right",
    "particular",
    "case",
    "20",
    "3",
    "nothing",
    "400",
    "2",
    "20",
    "20",
    "400",
    "nothing",
    "3",
    "go",
    "probably",
    "use",
    "calculator",
    "show",
    "400",
    "3",
    "nothing",
    "similarity",
    "weight",
    "similarly",
    "go",
    "ahead",
    "compute",
    "1",
    "9",
    "11",
    "whole",
    "3",
    "1",
    "right",
    "10",
    "11",
    "10",
    "11",
    "nothing",
    "21",
    "whole",
    "4",
    "21",
    "whole",
    "square",
    "open",
    "calculator",
    "21",
    "21",
    "21",
    "nothing",
    "441",
    "divid",
    "4",
    "divid",
    "4",
    "probably",
    "110",
    "110",
    "similarly",
    "go",
    "ahead",
    "compute",
    "want",
    "compute",
    "thing",
    "got",
    "1x",
    "6",
    "basically",
    "1x",
    "6",
    "finally",
    "compute",
    "information",
    "133",
    "1333",
    "1x",
    "6",
    "obviously",
    "value",
    "greater",
    "previous",
    "one",
    "got",
    "8913",
    "definitely",
    "going",
    "use",
    "split",
    "better",
    "previous",
    "split",
    "right",
    "let",
    "say",
    "split",
    "considered",
    "finally",
    "see",
    "output",
    "okay",
    "hope",
    "everybody",
    "able",
    "understand",
    "right",
    "let",
    "say",
    "split",
    "worked",
    "well",
    "going",
    "rub",
    "things",
    "suppose",
    "want",
    "inferencing",
    "inferencing",
    "done",
    "suppose",
    "record",
    "comes",
    "first",
    "record",
    "go",
    "go",
    "base",
    "model",
    "base",
    "model",
    "whenever",
    "goes",
    "value",
    "51",
    "51",
    "plus",
    "alpha",
    "1",
    "learning",
    "rate",
    "one",
    "suppose",
    "goes",
    "route",
    "11",
    "9",
    "whenever",
    "go",
    "rote",
    "11",
    "9",
    "average",
    "numbers",
    "considered",
    "average",
    "numbers",
    "11",
    "1",
    "2",
    "nothing",
    "10",
    "right",
    "10",
    "get",
    "multiplied",
    "suppose",
    "goes",
    "route",
    "happen",
    "1",
    "9",
    "11",
    "divide",
    "3",
    "average",
    "taken",
    "21",
    "divid",
    "3",
    "7",
    "get",
    "replaced",
    "7",
    "similarly",
    "anything",
    "respect",
    "decision",
    "tree",
    "1",
    "like",
    "construct",
    "decision",
    "tree",
    "separately",
    "become",
    "alpha",
    "2",
    "decision",
    "tre",
    "2",
    "alpha",
    "3",
    "decision",
    "3",
    "3",
    "like",
    "till",
    "alpha",
    "decision",
    "3",
    "n",
    "calculate",
    "specific",
    "output",
    "regression",
    "tree",
    "particular",
    "case",
    "happen",
    "trying",
    "play",
    "parameters",
    "trying",
    "use",
    "different",
    "way",
    "compute",
    "things",
    "everybody",
    "clear",
    "blackbox",
    "model",
    "visualize",
    "things",
    "let",
    "go",
    "third",
    "algorithm",
    "called",
    "vm",
    "see",
    "svm",
    "almost",
    "like",
    "decision",
    "uh",
    "logistic",
    "regression",
    "okay",
    "major",
    "aim",
    "svm",
    "major",
    "aim",
    "svm",
    "suppose",
    "data",
    "points",
    "like",
    "okay",
    "obviously",
    "use",
    "uh",
    "logistic",
    "regression",
    "split",
    "data",
    "points",
    "right",
    "like",
    "try",
    "create",
    "best",
    "fit",
    "line",
    "looks",
    "like",
    "probably",
    "based",
    "best",
    "fit",
    "line",
    "try",
    "divide",
    "point",
    "svm",
    "create",
    "best",
    "fit",
    "line",
    "instead",
    "also",
    "create",
    "point",
    "called",
    "marginal",
    "planes",
    "like",
    "create",
    "marginal",
    "plane",
    "hyper",
    "plane",
    "marginal",
    "plane",
    "whichever",
    "plane",
    "maximum",
    "distance",
    "able",
    "divide",
    "points",
    "efficiently",
    "usually",
    "normal",
    "scenario",
    "know",
    "whenever",
    "talk",
    "hyper",
    "plane",
    "whenever",
    "talk",
    "marginal",
    "plane",
    "lot",
    "overlapping",
    "points",
    "right",
    "suppose",
    "specific",
    "points",
    "one",
    "point",
    "looks",
    "like",
    "may",
    "also",
    "another",
    "points",
    "may",
    "overlap",
    "difficult",
    "get",
    "exact",
    "straight",
    "marginal",
    "planes",
    "split",
    "point",
    "based",
    "specific",
    "marginal",
    "plane",
    "maximum",
    "create",
    "type",
    "best",
    "fit",
    "line",
    "probably",
    "uh",
    "use",
    "marginal",
    "plane",
    "overlapping",
    "right",
    "call",
    "kind",
    "plane",
    "kind",
    "plane",
    "basically",
    "called",
    "hard",
    "marginal",
    "plane",
    "basically",
    "called",
    "hardge",
    "marginal",
    "plane",
    "okay",
    "similarly",
    "points",
    "overlapping",
    "suppose",
    "yellow",
    "points",
    "also",
    "get",
    "overlapped",
    "may",
    "kind",
    "errors",
    "particular",
    "case",
    "basically",
    "say",
    "soft",
    "marginal",
    "plane",
    "able",
    "see",
    "errors",
    "asvm",
    "focus",
    "focus",
    "creating",
    "marginal",
    "plane",
    "maximum",
    "distance",
    "even",
    "though",
    "errors",
    "consider",
    "solving",
    "providing",
    "kind",
    "hyper",
    "parameter",
    "go",
    "ahead",
    "basically",
    "create",
    "marginal",
    "planes",
    "go",
    "ahead",
    "much",
    "simple",
    "uh",
    "imagine",
    "specific",
    "way",
    "initially",
    "let",
    "consider",
    "data",
    "point",
    "suppose",
    "best",
    "fit",
    "line",
    "give",
    "best",
    "fit",
    "line",
    "equation",
    "basically",
    "say",
    "yal",
    "mx",
    "c",
    "right",
    "basically",
    "say",
    "equation",
    "mx",
    "c",
    "hard",
    "hard",
    "marginal",
    "impossible",
    "normal",
    "data",
    "set",
    "obviously",
    "able",
    "get",
    "definitely",
    "go",
    "ahead",
    "creating",
    "soft",
    "marginal",
    "plan",
    "equal",
    "mx",
    "plus",
    "c",
    "indicate",
    "nothing",
    "slope",
    "c",
    "indicates",
    "nothing",
    "intercept",
    "say",
    "equations",
    "ax",
    "b",
    "c",
    "isal",
    "0",
    "also",
    "say",
    "equation",
    "straight",
    "line",
    "say",
    "also",
    "equation",
    "straight",
    "line",
    "say",
    "equal",
    "say",
    "equal",
    "see",
    "try",
    "prove",
    "take",
    "equation",
    "try",
    "find",
    "nothing",
    "minus",
    "c",
    "min",
    "c",
    "minus",
    "sorry",
    "x",
    "divided",
    "b",
    "divided",
    "b",
    "divided",
    "b",
    "see",
    "almost",
    "particular",
    "case",
    "value",
    "b",
    "c",
    "basically",
    "minus",
    "c",
    "b",
    "equation",
    "almost",
    "let",
    "consider",
    "equation",
    "actually",
    "whenever",
    "say",
    "equal",
    "mx",
    "c",
    "also",
    "write",
    "something",
    "like",
    "equal",
    "w1",
    "x1",
    "w2",
    "x2",
    "plus",
    "like",
    "plus",
    "c",
    "plus",
    "b",
    "thing",
    "also",
    "write",
    "w",
    "transpose",
    "x",
    "b",
    "equation",
    "right",
    "basically",
    "using",
    "equation",
    "yes",
    "also",
    "write",
    "different",
    "way",
    "end",
    "day",
    "also",
    "treating",
    "something",
    "like",
    "let",
    "say",
    "slope",
    "direction",
    "slope",
    "direction",
    "basically",
    "say",
    "let",
    "consider",
    "slope",
    "minus",
    "one",
    "let",
    "say",
    "slope",
    "minus",
    "one",
    "see",
    "negative",
    "direction",
    "let",
    "say",
    "slope",
    "minus",
    "one",
    "trying",
    "prove",
    "slope",
    "negative",
    "value",
    "let",
    "consider",
    "suppose",
    "one",
    "point",
    "4a",
    "0",
    "obviously",
    "particular",
    "equation",
    "given",
    "particular",
    "line",
    "given",
    "equation",
    "really",
    "want",
    "find",
    "value",
    "let",
    "say",
    "x1",
    "x1",
    "x2",
    "let",
    "say",
    "want",
    "find",
    "want",
    "find",
    "w",
    "transpose",
    "x",
    "b",
    "value",
    "based",
    "line",
    "want",
    "compute",
    "value",
    "based",
    "line",
    "compute",
    "w",
    "transpose",
    "x",
    "basically",
    "means",
    "w",
    "value",
    "things",
    "one",
    "value",
    "b",
    "right",
    "b",
    "intercept",
    "right",
    "intercept",
    "passing",
    "origin",
    "say",
    "b",
    "zero",
    "obviously",
    "assume",
    "b",
    "zero",
    "particular",
    "case",
    "talk",
    "w",
    "w",
    "case",
    "minus",
    "one",
    "initialized",
    "want",
    "matrix",
    "multiplication",
    "w",
    "transpose",
    "written",
    "like",
    "x",
    "value",
    "written",
    "comma",
    "4",
    "0",
    "0",
    "right",
    "basically",
    "write",
    "like",
    "multiplication",
    "value",
    "get",
    "basically",
    "get",
    "four",
    "right",
    "positive",
    "value",
    "positive",
    "value",
    "understand",
    "since",
    "positive",
    "value",
    "points",
    "line",
    "points",
    "consider",
    "line",
    "try",
    "calculate",
    "say",
    "always",
    "positive",
    "yes",
    "similarly",
    "could",
    "probably",
    "consider",
    "one",
    "point",
    "4a",
    "4a",
    "4",
    "tell",
    "4a",
    "4",
    "calculate",
    "value",
    "get",
    "whether",
    "get",
    "positive",
    "value",
    "negative",
    "value",
    "try",
    "calculate",
    "value",
    "case",
    "positive",
    "values",
    "getting",
    "right",
    "calculate",
    "value",
    "value",
    "negative",
    "positive",
    "try",
    "calculate",
    "calculate",
    "use",
    "equation",
    "time",
    "slope",
    "minus1",
    "intercept",
    "zero",
    "4",
    "comma",
    "4",
    "min",
    "0",
    "min",
    "right",
    "negative",
    "value",
    "negative",
    "value",
    "guys",
    "negative",
    "see",
    "4",
    "0",
    "negative",
    "point",
    "probably",
    "top",
    "points",
    "plane",
    "right",
    "try",
    "calculate",
    "value",
    "always",
    "negative",
    "two",
    "things",
    "able",
    "get",
    "positive",
    "negative",
    "consider",
    "entirely",
    "one",
    "category",
    "another",
    "category",
    "least",
    "two",
    "things",
    "basically",
    "consider",
    "guys",
    "hope",
    "everybody",
    "able",
    "understand",
    "one",
    "category",
    "another",
    "category",
    "obviously",
    "basically",
    "means",
    "definitely",
    "use",
    "plane",
    "split",
    "point",
    "hope",
    "everybody",
    "able",
    "understand",
    "let",
    "go",
    "ahead",
    "let",
    "see",
    "marginal",
    "plane",
    "get",
    "created",
    "cost",
    "function",
    "basically",
    "cost",
    "function",
    "making",
    "sure",
    "marginal",
    "plane",
    "definitely",
    "work",
    "right",
    "becomes",
    "difficult",
    "right",
    "suppose",
    "let",
    "consider",
    "example",
    "suppose",
    "say",
    "lines",
    "let",
    "say",
    "uh",
    "want",
    "basically",
    "create",
    "kind",
    "two",
    "variety",
    "points",
    "one",
    "point",
    "let",
    "say",
    "points",
    "like",
    "points",
    "somewhere",
    "let",
    "consider",
    "using",
    "directly",
    "good",
    "number",
    "points",
    "split",
    "okay",
    "try",
    "talk",
    "actually",
    "trying",
    "prove",
    "obviously",
    "best",
    "fit",
    "line",
    "splits",
    "apart",
    "also",
    "create",
    "marginal",
    "points",
    "order",
    "create",
    "marginal",
    "point",
    "may",
    "use",
    "different",
    "color",
    "let",
    "see",
    "color",
    "one",
    "marginal",
    "point",
    "remember",
    "nearest",
    "point",
    "basically",
    "construct",
    "like",
    "like",
    "similarly",
    "constructing",
    "like",
    "already",
    "told",
    "guys",
    "equation",
    "mentioned",
    "w",
    "transpose",
    "x",
    "b",
    "0",
    "right",
    "definitely",
    "say",
    "ax",
    "b",
    "c",
    "equal",
    "0",
    "also",
    "write",
    "w",
    "transpose",
    "x",
    "equal",
    "0",
    "sorry",
    "plus",
    "b",
    "plus",
    "b",
    "equal",
    "0",
    "okay",
    "prove",
    "hope",
    "everybody",
    "clear",
    "going",
    "let",
    "represent",
    "line",
    "also",
    "equation",
    "line",
    "want",
    "represent",
    "w",
    "transpose",
    "x",
    "b",
    "value",
    "come",
    "positive",
    "negative",
    "c",
    "line",
    "anything",
    "plane",
    "right",
    "distance",
    "try",
    "find",
    "always",
    "negative",
    "let",
    "say",
    "using",
    "minus",
    "one",
    "read",
    "negative",
    "value",
    "line",
    "going",
    "mention",
    "w",
    "transpose",
    "x",
    "b",
    "equal",
    "1",
    "min",
    "1",
    "already",
    "discussed",
    "point",
    "trying",
    "calculate",
    "value",
    "always",
    "going",
    "one",
    "going",
    "minus",
    "one",
    "definitely",
    "say",
    "k",
    "okay",
    "mentioning",
    "k",
    "many",
    "articles",
    "see",
    "minus",
    "one",
    "uh",
    "many",
    "research",
    "paper",
    "also",
    "use",
    "minus",
    "one",
    "would",
    "like",
    "specify",
    "uh",
    "minus",
    "plus",
    "k",
    "let",
    "go",
    "write",
    "minus1",
    "plus",
    "aim",
    "increase",
    "distance",
    "okay",
    "distance",
    "really",
    "want",
    "increase",
    "distance",
    "order",
    "increase",
    "increase",
    "distance",
    "basically",
    "means",
    "model",
    "performing",
    "well",
    "let",
    "say",
    "want",
    "find",
    "distance",
    "first",
    "write",
    "w",
    "transpose",
    "x",
    "plus",
    "bal",
    "1",
    "write",
    "w",
    "transpose",
    "x",
    "b",
    "isal",
    "minus1",
    "going",
    "going",
    "computation",
    "subtract",
    "like",
    "obviously",
    "x1",
    "x2",
    "okay",
    "another",
    "points",
    "x2",
    "x1",
    "write",
    "w",
    "transpose",
    "x1",
    "x2",
    "b",
    "b",
    "get",
    "cancell",
    "writing",
    "two",
    "right",
    "definitely",
    "write",
    "two",
    "different",
    "things",
    "let",
    "see",
    "things",
    "write",
    "nothing",
    "difference",
    "plane",
    "plane",
    "given",
    "like",
    "okay",
    "always",
    "understand",
    "whenever",
    "consider",
    "vector",
    "vors",
    "right",
    "vectors",
    "right",
    "also",
    "something",
    "called",
    "magnitude",
    "want",
    "remove",
    "magnitude",
    "divide",
    "w",
    "magnitude",
    "w",
    "vector",
    "remain",
    "indicated",
    "like",
    "going",
    "basically",
    "divide",
    "particular",
    "operation",
    "side",
    "dividing",
    "magnitude",
    "w",
    "care",
    "directions",
    "right",
    "care",
    "vectors",
    "write",
    "like",
    "aim",
    "aim",
    "say",
    "aim",
    "aim",
    "maximize",
    "2",
    "byw",
    "say",
    "guys",
    "yes",
    "aim",
    "aim",
    "basically",
    "maximize",
    "right",
    "updating",
    "w",
    "comma",
    "b",
    "value",
    "need",
    "maximize",
    "yes",
    "everybody",
    "clear",
    "say",
    "yes",
    "want",
    "maximize",
    "yes",
    "everybody",
    "want",
    "maximize",
    "maximize",
    "basically",
    "means",
    "marginal",
    "plane",
    "become",
    "bigger",
    "marginal",
    "plane",
    "bigger",
    "okay",
    "write",
    "along",
    "output",
    "dependent",
    "two",
    "different",
    "things",
    "one",
    "say",
    "plus",
    "uh",
    "one",
    "w",
    "transpose",
    "x",
    "b",
    "greater",
    "equal",
    "1",
    "everybody",
    "see",
    "equation",
    "actually",
    "trying",
    "specify",
    "1",
    "w",
    "transpose",
    "x",
    "b",
    "greater",
    "1",
    "minus",
    "1",
    "basically",
    "means",
    "w",
    "transpose",
    "x",
    "b",
    "less",
    "equal",
    "minus",
    "basically",
    "mean",
    "see",
    "values",
    "whenever",
    "compute",
    "w",
    "transpose",
    "x",
    "b",
    "greater",
    "equal",
    "1",
    "obviously",
    "going",
    "get",
    "one",
    "w",
    "transpose",
    "b",
    "less",
    "equal",
    "1",
    "always",
    "going",
    "get",
    "output",
    "minus",
    "one",
    "hope",
    "reason",
    "actually",
    "written",
    "like",
    "two",
    "already",
    "discussed",
    "specifically",
    "writing",
    "want",
    "increase",
    "marginal",
    "plane",
    "marginal",
    "plane",
    "writing",
    "one",
    "condition",
    "yi",
    "value",
    "one",
    "w",
    "transpose",
    "x",
    "plus",
    "b",
    "greater",
    "equal",
    "1",
    "otherwise",
    "less",
    "equal",
    "minus",
    "one",
    "going",
    "much",
    "clear",
    "transpose",
    "condition",
    "already",
    "done",
    "everybody",
    "clear",
    "top",
    "add",
    "one",
    "important",
    "point",
    "instead",
    "writing",
    "also",
    "say",
    "major",
    "aim",
    "major",
    "aim",
    "multiply",
    "multiplied",
    "w",
    "transpose",
    "x",
    "b",
    "multiply",
    "two",
    "always",
    "able",
    "greater",
    "equal",
    "1",
    "correct",
    "points",
    "right",
    "correct",
    "points",
    "understand",
    "minus",
    "one",
    "multiplying",
    "correct",
    "point",
    "minus",
    "minus",
    "obviously",
    "greater",
    "equal",
    "one",
    "right",
    "similarly",
    "greater",
    "1",
    "also",
    "definitely",
    "say",
    "major",
    "multiply",
    "always",
    "greater",
    "equal",
    "1",
    "u",
    "definitely",
    "saying",
    "positive",
    "value",
    "representation",
    "guys",
    "understand",
    "minimized",
    "cost",
    "function",
    "minimized",
    "cost",
    "function",
    "maximized",
    "cost",
    "function",
    "going",
    "write",
    "maximize",
    "w",
    "comma",
    "b",
    "maximize",
    "w",
    "comma",
    "b",
    "2",
    "magnitude",
    "w",
    "also",
    "write",
    "something",
    "like",
    "minimize",
    "w",
    "comma",
    "b",
    "inverse",
    "looks",
    "like",
    "always",
    "understand",
    "machine",
    "learning",
    "algorithm",
    "write",
    "minimize",
    "things",
    "trying",
    "minimize",
    "something",
    "okay",
    "equivalent",
    "equivalent",
    "specifically",
    "write",
    "minimization",
    "back",
    "propagation",
    "continuously",
    "updating",
    "weights",
    "w",
    "b",
    "definitely",
    "write",
    "like",
    "main",
    "target",
    "minimize",
    "particular",
    "value",
    "changing",
    "w",
    "b",
    "start",
    "adding",
    "parameters",
    "fine",
    "till",
    "think",
    "everybody",
    "got",
    "aim",
    "going",
    "going",
    "add",
    "two",
    "parameters",
    "optimizer",
    "one",
    "c",
    "one",
    "summation",
    "equal",
    "1",
    "n",
    "use",
    "something",
    "called",
    "ea",
    "ea",
    "first",
    "tell",
    "c",
    "see",
    "specific",
    "data",
    "point",
    "let",
    "say",
    "points",
    "right",
    "right",
    "prediction",
    "wrong",
    "prediction",
    "points",
    "right",
    "prediction",
    "wrong",
    "prediction",
    "obviously",
    "wrong",
    "prediction",
    "points",
    "somewhere",
    "wr",
    "prediction",
    "wrong",
    "wrong",
    "incorrect",
    "prediction",
    "right",
    "c",
    "value",
    "basically",
    "says",
    "many",
    "errors",
    "many",
    "errors",
    "says",
    "fine",
    "six",
    "errors",
    "seven",
    "errors",
    "many",
    "errors",
    "even",
    "though",
    "using",
    "marginal",
    "plane",
    "many",
    "errors",
    "specifically",
    "writing",
    "many",
    "errors",
    "specified",
    "c",
    "ofi",
    "ea",
    "basically",
    "says",
    "summation",
    "going",
    "write",
    "since",
    "sumission",
    "entire",
    "term",
    "basically",
    "mentions",
    "sumission",
    "distance",
    "values",
    "distance",
    "wrong",
    "points",
    "calculate",
    "distance",
    "suppose",
    "wrong",
    "point",
    "try",
    "calculate",
    "distance",
    "sumission",
    "sumission",
    "sumission",
    "similarly",
    "green",
    "point",
    "another",
    "sumission",
    "happen",
    "like",
    "going",
    "specific",
    "sumission",
    "telling",
    "fine",
    "able",
    "fit",
    "properly",
    "try",
    "apply",
    "two",
    "hyperparameters",
    "try",
    "make",
    "sure",
    "many",
    "errors",
    "also",
    "well",
    "good",
    "problem",
    "go",
    "ahead",
    "try",
    "submission",
    "data",
    "points",
    "based",
    "try",
    "construct",
    "best",
    "fit",
    "line",
    "along",
    "marginal",
    "plane",
    "like",
    "even",
    "though",
    "errors",
    "errors",
    "good",
    "go",
    "respect",
    "one",
    "thing",
    "called",
    "al",
    "svr",
    "svr",
    "one",
    "thing",
    "getting",
    "changed",
    "svr",
    "value",
    "get",
    "changed",
    "want",
    "explore",
    "let",
    "know",
    "one",
    "assignment",
    "value",
    "changing",
    "remaining",
    "everything",
    "try",
    "change",
    "particular",
    "value",
    "becomes",
    "svr",
    "try",
    "explore",
    "try",
    "find",
    "try",
    "let",
    "know",
    "overall",
    "uh",
    "like",
    "entire",
    "session",
    "everyone",
    "okay",
    "one",
    "thing",
    "called",
    "kernel",
    "matrix",
    "svm",
    "kernel",
    "say",
    "svm",
    "kernel",
    "vm",
    "kernel",
    "happens",
    "suppose",
    "specific",
    "data",
    "points",
    "looks",
    "like",
    "looks",
    "like",
    "obviously",
    "use",
    "straight",
    "line",
    "try",
    "divide",
    "convert",
    "two",
    "dimension",
    "three",
    "dimensions",
    "probably",
    "push",
    "point",
    "like",
    "one",
    "point",
    "go",
    "like",
    "white",
    "point",
    "go",
    "basically",
    "use",
    "plane",
    "split",
    "uploaded",
    "video",
    "around",
    "uh",
    "around",
    "uh",
    "definitely",
    "look",
    "onto",
    "also",
    "shown",
    "practically",
    "reason",
    "created",
    "specific",
    "video",
    "great",
    "uh",
    "side",
    "hope",
    "like",
    "session",
    "thank",
    "everyone",
    "great",
    "day",
    "keep",
    "rocking",
    "keep",
    "learning",
    "never",
    "give"
  ],
  "keywords": [
    "session",
    "things",
    "basically",
    "going",
    "discuss",
    "first",
    "different",
    "machine",
    "learning",
    "algorithm",
    "like",
    "many",
    "understand",
    "purpose",
    "taking",
    "okay",
    "go",
    "data",
    "main",
    "people",
    "algorithms",
    "way",
    "definitely",
    "able",
    "better",
    "got",
    "specifically",
    "talk",
    "ai",
    "second",
    "thing",
    "difference",
    "third",
    "probably",
    "something",
    "called",
    "linear",
    "regression",
    "next",
    "r",
    "square",
    "adjusted",
    "ridge",
    "lasso",
    "really",
    "want",
    "specific",
    "entire",
    "say",
    "means",
    "whatever",
    "deep",
    "actually",
    "creating",
    "define",
    "create",
    "kind",
    "person",
    "need",
    "make",
    "perform",
    "consider",
    "example",
    "suppose",
    "see",
    "time",
    "work",
    "seeing",
    "present",
    "gives",
    "us",
    "take",
    "also",
    "give",
    "may",
    "part",
    "get",
    "little",
    "bit",
    "right",
    "uh",
    "help",
    "one",
    "based",
    "know",
    "written",
    "let",
    "try",
    "lot",
    "equation",
    "using",
    "techniques",
    "whenever",
    "much",
    "important",
    "exactly",
    "learn",
    "particular",
    "trying",
    "use",
    "aim",
    "train",
    "done",
    "amazing",
    "solve",
    "ahead",
    "come",
    "guys",
    "given",
    "case",
    "w",
    "everything",
    "respect",
    "problem",
    "statement",
    "two",
    "classification",
    "clustering",
    "set",
    "says",
    "age",
    "weight",
    "features",
    "values",
    "model",
    "new",
    "output",
    "independent",
    "dependent",
    "feature",
    "since",
    "training",
    "becomes",
    "value",
    "increasing",
    "getting",
    "reason",
    "remember",
    "number",
    "discussed",
    "find",
    "continuous",
    "order",
    "draw",
    "straight",
    "line",
    "c",
    "predicted",
    "points",
    "point",
    "always",
    "play",
    "finally",
    "pass",
    "categories",
    "binary",
    "write",
    "scenario",
    "used",
    "groups",
    "group",
    "another",
    "clusters",
    "cluster",
    "1",
    "2",
    "three",
    "every",
    "information",
    "good",
    "together",
    "target",
    "apply",
    "coming",
    "reduce",
    "100",
    "yes",
    "cover",
    "sorry",
    "logistic",
    "decision",
    "tree",
    "four",
    "boost",
    "random",
    "forest",
    "gradient",
    "boosting",
    "n",
    "bias",
    "k",
    "nearest",
    "start",
    "simple",
    "x",
    "nothing",
    "weights",
    "performance",
    "whether",
    "performing",
    "well",
    "best",
    "fit",
    "prediction",
    "function",
    "multiple",
    "h",
    "beta",
    "0",
    "theta",
    "obviously",
    "even",
    "zero",
    "equal",
    "slope",
    "towards",
    "show",
    "distance",
    "less",
    "initially",
    "summation",
    "calculate",
    "giving",
    "select",
    "cost",
    "already",
    "formula",
    "mean",
    "negative",
    "predict",
    "divide",
    "1x",
    "average",
    "derivative",
    "keep",
    "tell",
    "j",
    "comma",
    "similarly",
    "saying",
    "2x",
    "hope",
    "everybody",
    "error",
    "minus",
    "whole",
    "parameter",
    "multiplied",
    "passing",
    "x1",
    "3",
    "think",
    "become",
    "compute",
    "happen",
    "plus",
    "total",
    "z",
    "graph",
    "five",
    "calculation",
    "green",
    "color",
    "5",
    "6",
    "somewhere",
    "condition",
    "4",
    "9",
    "curve",
    "descent",
    "global",
    "minima",
    "update",
    "alpha",
    "looks",
    "look",
    "positive",
    "side",
    "writing",
    "rate",
    "small",
    "local",
    "question",
    "answer",
    "squ",
    "x2",
    "matrix",
    "sum",
    "residual",
    "divided",
    "min",
    "hat",
    "high",
    "low",
    "add",
    "increase",
    "change",
    "p",
    "put",
    "understood",
    "greater",
    "confusion",
    "created",
    "fine",
    "overfitting",
    "test",
    "variance",
    "bad",
    "accuracy",
    "red",
    "focus",
    "lambda",
    "hyper",
    "selected",
    "taken",
    "whichever",
    "cross",
    "validation",
    "technique",
    "score",
    "7",
    "outlier",
    "10",
    "wrong",
    "b",
    "maximum",
    "transpose",
    "e",
    "05",
    "log",
    "combine",
    "parameters",
    "true",
    "false",
    "category",
    "directly",
    "precision",
    "recall",
    "f1",
    "probability",
    "sk",
    "import",
    "dot",
    "execute",
    "df",
    "print",
    "step",
    "records",
    "split",
    "cv",
    "root",
    "regressor",
    "base",
    "marble",
    "record",
    "outlook",
    "sunny",
    "overcast",
    "rain",
    "nos",
    "hot",
    "50",
    "trees",
    "node",
    "pure",
    "entropy",
    "guinea",
    "gain",
    "impurity",
    "classifier",
    "pruning",
    "bagging",
    "models",
    "sampling",
    "weak",
    "learner",
    "stump",
    "centroids",
    "centroid",
    "epsilon",
    "11",
    "similarity",
    "marginal",
    "plane"
  ]
}