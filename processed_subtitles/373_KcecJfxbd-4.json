{
  "text": "[Music]\ndid you know that there are around 4.4\nbillion internet users today and all\nthese users undoubtedly generate massive\namounts of data and all of this data is\ncollectively termed as big data but how\ndo you think all of this data is\nprocessed well frameworks like hadoop\nand spark are used to do this so hey\nguys welcome to this big data tutorial\nin this video we'll be covering the\nfundamentals of big data hadoop and\nspark so we will be talking about hadoop\ninstallation and the various components\nof hadoop such as hdfs map reduce and\nyard then we look into apache spark its\ninstallation spark m-led and spark sql\nfirst off we'll have an introduction to\nbig data and look into the hadoop\ninstallation by our instructor ajay now\nwhen we talk about evolution of big data\nwe have known that data has evolved in\nlast five years like never before now in\nfact before going to big data or before\nunderstanding these solutions and the\nneed and why there is a rush towards big\ndata technology in solution i would like\nto ask a question take a couple of\nminutes and think why are organizations\ninterested in big data why is there\ncertain rush in industry where everyone\nwould want to ramp up their current\ninfrastructure or would want to be\nworking on technologies which allow them\nto use this big data think about it what\nis happening and why are organizations\ninterested in this and if you think on\nthis\nyou will start thinking about what\norganizations have been doing in past\nwhat organizations have not done and why\nare organizations interested in big data\nnow before we learn on big data we can\nalways look into internet and check for\nuse cases where organizations have\nfailed to use legacy systems or\nrelational databases to work on their\ndata requirements now over in recent or\nover past five years or in recent decade\nwhat has happened is organizations have\nstarted understanding the value of data\nand they have decided not to ignore any\ndata as being uneconomical now we can\ntalk about different platforms through\nwhich data is generated take an example\nof social media like twitter facebook\ninstagram whatsapp youtube you have\ne-commerce and various portals say ebay\namazon flipkart alibaba.com and then you\nhave various tech giants such as google\noracle sap amazon microsoft and so on so\nlots of data is getting generated every\nday in every business sector the point\nhere is that organizations have slowly\nstarted realizing that they would be\ninterested in working on all the data\nnow the question which i asked was why\nare organizations interested in big data\nand some of you might have already\nanswered or thought about that\norganizations are interested in doing\nprecise analysis or they want to work on\ndifferent formats of data such as\nstructured unstructured semi-structured\ndata organizations are interested in\ngaining insights or finding the hidden\ntreasure in the so called big data and\nthis is the main reason where\norganizations are interested in big data\nnow there are various use cases there\nare various use cases we can compare\nthat organizations from past 50 or more\nthan 50 years have been handling huge\namount of data they have been working on\nhuge volume of data but the question\nhere is have they worked on all the data\nor have they worked on some portion of\nit what have they used to store this\ndata and if they have used something to\nstore this data what is happening what\nis what is changing now when we talk\nabout the businesses we cannot avoid\ntalking about the dynamism involved now\nany organization would want to have a\nsolution which allows them to store data\nand store huge amount of data capture it\nprocess it analyze it and also look into\nthe data to give more value to the data\norganizations have then been looking for\nsolutions now let's look at some facts\nthat can convince you or that would\nconvince you that data is exploding and\nneeds your attention right 55 billion\nmessages and 4.5 billion photos are sent\neach day on whatsapp 300 hours of video\nare uploaded every minute on youtube did\nyou guys know that youtube is the second\nlargest search engine after google every\nminute users send\n31.25 million messages and watch 2.77\nmillion videos on facebook walmart\nhandles more than 1 million customer\ntransactions every hour google 40 000\nsearch queries are performed on google\nper second that is 3.46 million searches\na day in fact you could also say that a\nlot of times people when they are\nloading up the google page is basically\njust to check their internet connection\nhowever that is also generating data idc\nreports that by 2025 real-time data will\nbe more than a quarter of all the data\nand by 2025 the volume of digital data\nwill increase to 163 zeta bytes that is\nwe are not even talking about gigabytes\nor terabytes anymore we are talking\nabout petabytes exabytes and zeta bytes\nand zeta bytes means 10 to the power 21\nbytes so this is how data has evolved\nnow you can talk about different\ncompanies which would want to use their\ndata to take business decisions they\nwould want to collect the data store it\nand analyze it and that's how they would\nbe interested in drawing insights for\nthe business now this is just a simple\nexample about facebook and what it does\nto work on the data now before we go to\nfacebook you could always check in\ngoogle by just typing in companies using\nbig data and if we say companies using\nbig data we should be able to find a\nlist of different companies which are\nusing big data for different use cases\nthere are various sources from where you\ncan find we could also search for\nsolution that is hadoop which we'll\ndiscuss later but you could always say\ncompanies using hadoop and that should\ntake you to the wiki page which will\nbasically help you know what are the\ndifferent companies which are using this\nso-called solution called hadoop okay\nnow coming back to what we were\ndiscussing about so organizations are\ninterested in big data as we discussed\nin gaining insights they would want to\nuse the data to find hidden information\nwhich probably they ignored earlier now\ntake an example of rdbms what is biggest\ndrawback in using an rdbms now you might\nthink that rdbms is known for stability\nand consistency and organizations would\nbe interested in storing their data in\noracle or db2 or mysql or microsoft sql\nserver and they have been doing that for\nmany years now so what has changed now\nnow when we talk about rdbms the first\nquestion which i would ask is do we have\naccess to 100 of data being online in\nrdbms the answer is no we would only\nhave 10 or 20 or 30 percent of data\nonline and rest of the data would be\narchived which means that if an\norganization is interested in working on\nall the data they would have to move the\ndata from the archived storage to the\nprocessing layer and that would involve\nbandwidth consumption now this is one of\nthe biggest drawbacks of rdbms you do\nnot have access to 100 of data online in\nmany of the cases organizations started\nrealizing that the data which they were\nignoring as being uneconomical had\nhidden value which they had never\nexploited i had read a presentation\nsomewhere which said torture the data\nand it will confess to anything now\nthat's the value of data which\norganizations have realized in recent\npast take an example of facebook now\nthis shows what facebook does with its\nbig data and we'll come to what is big\ndata but let's understand the use case\nnow facebook collects huge volumes of\nuser data whether that is sms whether\nthat is likes whether that is\nadvertisements whether that is features\nwhich people are liking or photographs\nor even user profiles now by collecting\nthis data and providing a portal which\npeople can use to connect facebook is\nalso accumulating huge volume of data\nand that's way beyond petabytes they\nwould also be interested in analyzing\nthis data and one of the reasons would\nbe they would want to personalize the\nexperience take an example of\npersonalized news feed depending on a\nuser behavior depending on what a user\nlikes what a user would want to know\nabout they can recommend a personalized\nnews feed to every particular user\nthat's just one example of what facebook\ndoes with its data take an example of\nphoto tag suggestions now when you log\ninto facebook account you could also get\nsuggestions on different friends whom\nyou would like to connect to or you\nwould want to tag so that they could be\nknown by others some more examples which\nshow how facebook uses its data are as\nfollows so the flashback collection of\nphotos and posts that received the most\ncomments and likes okay there was\nsomething called as i voted that was\nused for 2016 elections with reminders\nand directions to tell users their time\nand place of polling also something\ncalled as safety checks in incidents\nsuch as earthquake hurricane or mass\nshooting facebook gives you safety\nchecks now these are some examples where\nfacebook is using big data and that\nbrings us to the question what is big\ndata this was just an example where we\ndiscussed about one company which is\nmaking use of that data which has been\naccumulated and it's not only for\ncompanies which are social media\noriented like facebook where data is\nimportant take an example of ibm take an\nexample of jpmorgan chase take an\nexample of ge or any other organization\nwhich is collecting huge amount of data\nthey would all want to gather insights\nthey would want to analyze the data they\nwould want to be more precise in\nbuilding their services or solutions\nwhich can take care of their customers\nso what is big data big data is\nbasically a term it is used to describe\nthe data that is too large and complex\nto store in traditional databases and as\ni gave an example it's not just about\nstoring the data it is also about what\nyou can do with the data it also means\nthat if there is a lot of dynamism\ninvolved can you change the underlying\nstorage and handle any kind of data that\ncomes in now before we get into that\nlet's just understand what is big data\nso big data is basically a term which\nhas been given to categorize the data if\nit has different characteristics\norganizations would want to have the big\ndata stored processed and then analyzed\nto get whatever useful information they\ncan get from this data now there are\nfive\nv's of big data volume velocity variety\nvalue veracity although these are five\nv's but then there are other v's which\nalso categorize the data as big data\nsuch as volatility validity viscosity\nvirality of data okay so these are five\nv's of big data and if the data has one\nor all of these characteristics then it\ncan be considered as big data including\nthe other ways which i just mentioned so\nvolume basically means incredible amount\nof data huge volumes of data data\ngenerated every second now that could be\nused for batch processing that could be\nused for real-time stream processing\nokay you might have data being generated\nfrom different kind of devices like your\ncell phones your social media websites\nonline transactions variable devices\nservers and these days with iot we are\nalso talking about data of getting\ngenerated via internet of things that is\nyou could have different devices which\ncould be communicating to each other you\ncould be getting data from radars or\nleaders or even camera sensors so there\nis a huge volume of data which is\ngetting generated and if we are talking\nabout data which has huge volume which\nis getting generated constantly or has\nbeen accumulated over a period of time\nwe would say that is big data velocity\nnow this is one more important aspect of\nbig data speed with which the data is\ngetting generated think about stock\nmarkets think about social media\nwebsites think about online surveys or\nmarketing campaigns or airline industry\nso if the data is getting generated with\na lot of speed where it becomes\ndifficult to capture collect process\ncure\nmine or analyze the data then we are\ncertainly talking about big data the\nnext aspect of big data is variety now\nthis is where we talk about structured\ndata semi-structured data or\nunstructured data and here i would like\nto ask a question what is the difference\nwhen do you call the data is structured\nsemi-structured or unstructured now\nlet's look at an example before we\ntheoretically discuss about this i\nalways would like to use some examples\nlet's look at a log file and let's see\nwhat is it so if i look at this log file\nand if i would say what kind of data is\nthis which is the highlighted one the\nanswer would be\nit is structured data it has specific\ndelimiters such as space it has data\nwhich is separated by space and if i had\na hundred or thousand or million rows\nwhich had similar kind of data i could\ncertainly store that in a table i could\nhave a predefined schema to store this\ndata so i would call the one which is\nhighlighted and structured but if i look\nat this portion where i would look at a\ncombination of this kind of data where\nsome data has a pattern and some data\ndoesn't now this is an example of\nsemi-structured data so if i would have\na predefined structure to store this\ndata probably the pattern of data would\nbreak the structure and if i look at all\nthe data then i would certainly call it\nunstructured data because there is no\nclear schema which can define this data\nnow this is what i mean by variety of\ndata that is structured data which\nbasically has a schema or has a format\nwhich could be easily understood you\nhave semi-structured which could be like\nan xml or json or even your excel sheets\nwhere you could have some data which is\nstructured and the other is unstructured\nand when we talk about unstructured we\nare talking about absence of schema it\ndoes not have a format it does not have\na schema and it is hard to analyze which\nbrings its own challenges the next\naspect is value now value refers to the\nability to turn your data useful for\nbusiness you would have lot of data\nwhich is being collected as we mentioned\nin previous slides right there would be\na lot of data wrangling or data\npre-processing or cleaning up of data\nhappening\nand then finally you would want to draw\nvalue from that data but from all the\ndata collected what percentage of data\ngives us value and if all my data can\ngive me value then why wouldn't i use it\nthis is an aspect of big data right\nveracity now this means the quality of\ndata billions of dollars are lost every\nyear by organizations because the data\nwhich was collected was not of good\nquality or probably they collected a lot\nof data and then it was erroneous take\nan example of autonomous driving\nprojects which are\nhappening in europe or u.s where there\nare car fleets which are on the road\ncollecting data via radar sensors and\ncamera sensors and when this data has to\nbe processed to train algorithms it has\nrealized that sometimes the data which\nwas collected was missing in some values\nmight be was not appropriate or had a\nlot of errors and all this process of\ncollecting the data becomes a repetitive\ntask because the quality of data was not\ngood this is just one example we can\ntake example from healthcare industry or\nstock markets or financial institutions\nand so on so extracting loads of data\nis not useful if the data is messy or\npoor in quality and that basically means\nthat velocity is a very important v of\nbig data now apart from veracity volume\nvariety velocity and value we have the\nother v such as viscosity how dense the\ndata is\nvalidity is the data still valid\nvolatility is my data volatile or\nvirality is the data viral now all of\nthese different v's categorize the data\nas big data we would like to talk on a\nbig data case study and we have taken an\nexample of google which obviously is one\nof the companies which is\nchurning and working on huge amount of\ndata now it's actually said that if you\ncompare one grain of sand with one byte\nof data then google is processing or\ngoogle is handling whole worlds sand\nevery week that is the kind of data\nwhich google is processing now in early\n2000 and since then when the number of\ninternet users started growing google\nalso faced lot of problems\nin storing increasing user data and\nusing the traditional servers to manage\nthat now that was a challenge which\ngoogle started facing could they use\ntraditional data server to store the\ndata well yes they could right storage\ndevices have been getting cheaper day by\nday but then how much time does it take\nto retrieve that data what is the seek\ntime what is the time taken to read and\nprocess that data thousands of search\nqueries were raised per second no doubt\nnow we could say millions and billions\nof queries are raised per second every\nquery read 100 mbs\nof data and consumed tens of billions of\ncpu cycles based on these queries so the\nrequirement was that they wanted to have\na large\ndistributed highly fault tolerant file\nsystem large to store to capture process\nhuge amount of data distributed because\nthey could not rely just on one server\neven if that had multiple disks stacked\nup that was not an efficient choice what\nwould happen if this particular machine\nfailed what would happen if the whole\nserver was down so they needed a\ndistributed storage and a distributed\ncomputing\nenvironment they needed something which\ncan be highly fault tolerant right so\nthis was the requirement which google\nhad and the solution which came out as a\nresult was gfs google file system now\nlet's look at how gfs works so normally\nin any particular linux system or linux\nserver you would have a file system you\nwould have set of processors you would\nhave set of files and directories which\ncould store the data gfs was different\nso to facilitate gfs which could store\nhuge amount of data there was an\narchitecture an architecture which had\none master and multiple chunk servers or\nyou can say slave servers or slave\nmachines master machine was to contain\nmetadata was to contain data about data\nwhen we say metadata we are talking\nabout information about data and then\nyou have the chung servers or the slave\nmachines which could be storing data in\na distributed fashion now any client or\nan api or an application which would\nwant to read the data would first\ncontact the master server it would\ncontact the machine where the master\nprocess was running and client would\nplace a request of reading the data or\nshowing an interest of reading the data\ninternally what it is doing is it is\nrequesting for metadata your api or an\napplication would want to know from\nwhere it can read the data master server\nwhich has metadata whether that is in\nram or disk we can discuss that later\nbut then master server would have the\nmetadata and it would know which are the\nchunk servers or the slave machines\nwhere the data was stored in a\ndistributed fashion master would respond\nback with the metadata information to\nthe client and then client could use\nthat information to read or write to\nthese slave machines where actually the\ndata was stored now this is what the\nprocess or set of processes work\ntogether to make gfs so when you say a\nchunk server we would basically have the\nfiles getting divided into fixed size\nchunks now how would they get divided so\nthere would be some kind of chunk size\nor a block size which would determine\nthat if the file is bigger than the\npre-decided chunk size then it would be\nsplit into smaller chunks and be\ndistributed across the chunk servers or\nthe slave machines if the file was\nsmaller then it would still use one\nchunk or a block to get stored on the\nunderlying slave machines so these junk\nservers or slave machines are the ones\nwhich actually store the data on local\ndisks as your linux files client which\nis interacting with master for metadata\nand then interacting with junk servers\nfor read write operations would be the\none which would be externally connecting\nto the cluster so this is how it would\nlook so you have a master which would\nobviously be receiving some kind of\nheartbeats from the chunk servers to\nknow their status and receive\ninformation in the form of packets which\nwould let the master know which machines\nwere available for storage which\nmachines already had data and master\nwould build up the metadata within\nitself the files would be broken down\ninto chunks for example we can look at\nfile one it is broken down into chunk\none and chunk two and file two has one\nchunk which is one portion of it and\nthen you have file to reciting on some\nother chunk server which also lets us\nknow that there is some kind of auto\nreplication for this file system right\nand the data which is getting stored in\nthe chunk could have a data of 64 mb now\nthat chunk size could be changed based\non the data size but google file system\nhad the basic size of the chunk as 64 mb\neach chunk would be replicated on\nmultiple servers the default replication\nwas three and that could again be\nincreased or decreased as per\nrequirement this would also mean that if\na particular slave machine or a chunk\nserver would die or would get killed or\nwould crash there would never be any\ndata loss because a replica of data\nresiding on the failed machine would\nstill be available on some other slave\nserver chunk server or slave machine now\nthis helped google to store and process\nhuge volumes of data in a distributed\nmanner and thus have a fault tolerant\ndistributed scalable storage which could\nallow them to store a huge amount of\ndata now that was just one example which\nactually led to the solution which today\nwe call as hadoop now when we talk about\nbig data here i would like to ask you\nsome questions that if we were talking\nabout the rdbms case take an example of\nuh something like nasa which was working\non a a project called setai search of\nextraterrestrial intelligence now this\nwas a project where they were looking\nfor\na solution to take care of their problem\nthe problem was that they would roughly\nsend some waves in space capture those\nwaves back and then analyze this data to\nfind if there was any extraterrestrial\nobject in space now they had two options\nfor it they could either have a huge\nserver built which could take care of\nstoring the data and processing it or\nthey could go for volunteer computing\nnow volunteer computing basically means\nthat you could have a lot of people\nvolunteering and being part of this\nproject and what they would in turn do\nis they would be donating their ram and\nstorage from their machines when they\nare not using it how would that happen\nbasically download some kind of patch on\ntheir machine which would run as a\nscreen saver and if the user is not\nusing his machine some portion of data\ncould be transferred to these machines\nfor intermittent storage and processing\nusing ram now this sounds very\ninteresting and this sounds very easy\nhowever it would have its own challenges\nright think about security think about\nintegrity but those those problems are\nnot bigger as much as is the requirement\nof bandwidth and this is the same thing\nwhich happens in rdbms if you would have\nto move data from archived solution to\nthe processing layer that would consume\nhuge amount of bandwidth big data brings\nits own challenges\nhuge amount of data is getting generated\nevery day now the biggest challenge is\nstoring this huge volume of data and\nespecially when this data is getting\ngenerated with lot of variety when it\ncan have different kind of formats where\nit could be viral it could be having a\nlot of value and nobody has looked into\nthe veracity of data but the primary\nproblem would be handling this huge\nvolume of data variety of the data would\nbring in challenges of storing it in\nlegacy systems if processing of the data\nwas required now here again i would\nsuggest you need to think what is the\ndifference between reading a data and\nprocessing a data so reading might just\nmean bringing in the data from disk and\ndoing some io operations and processing\nwould mean reading the data probably\ndoing some transformations on it\nextracting some useful information from\nit and then storing it in the same\nformat or probably in a different format\nso processing this massive volume of\ndata is the second challenge\norganizations don't just store their big\ndata they would eventually want to use\nit to process it to gather some insights\nnow processing and extracting insights\nfrom big data would take huge amount of\ntime unless and until there was an\nefficient solution to handle and process\nthis big data securing the data that's\nagain a concern for organizations right\nencryption of big data is difficult to\nperform if you would think about\ndifferent compression mechanisms then\nthat would also mean decompressing of\ndata which would also mean that you\ncould take a hit on the cpu cycles or on\ndisk usage providing user authentication\nfor every team member now that could\nalso be dangerous so that led to hadoop\nas a solution so big data brings its own\nchallenges big data brings its own\nbenefits and here we have a solution\nwhich is hadoop now what is hadoop it's\nan open source framework for storing\ndata and running applications on\nclusters of commodity hardware hadoop is\nan open source framework and before we\ndiscuss on two main components of hadoop\nit would be good to look into the link\nwhich i was suggesting earlier that is\ncompanies using hadoop and any person\nwho would be interested in learning big\ndata should start somewhere here where\nyou could list down different companies\nwhat kind of setup they have why are\nthey having hadoop what kind of\nprocessing they are doing and how are\nthey using these so called hadoop\nclusters to process and in fact store\ncapture and process huge amount of data\nanother link which i would suggest is\nlooking at different\ndistributions of hadoop any person who\nis interested in learning in big data\nshould know about different\ndistributions of hadoop now in linux we\nhave different different distributions\nlike ubuntu centos red hat susie debian\nin the same way you have different\ndistributions of hadoop which we can\nlook on the wiki page and this is the\nlink which talks about products that\ninclude apache hadoop or derivative\nworks and commercial support which\nbasically means that apache hadoop the\nsole products that can be called a\nrelease of apache hadoop come from\napache.org that's an open source\ncommunity and then you have various\nvendor-specific distributions like\namazon web services you have cloud era\nyou have hotend works you have ibm's big\ninside you have mapper all these are\ndifferent distributions of hadoop so\nbasically all of these vendor-specific\ndistributions are depending on using on\ncore apache hadoop in brief we can say\nthat these are the vendors which take up\nthe apache hadoop package it within a\ncluster management solution so that\nusers who intend to use apache hadoop\nwould not have difficulties of setting\nup a cluster setting up a framework they\ncould just use a vendor-specific\ndistribution with its cluster\ninstallation solutions clustered\nmanagement solution and easily plan\ndeploy install and manage a cluster here\nis a quick demo on setting up cloudera\nquickstart vm in case you are interested\nin working on a standalone cluster you\ncan download the cloudera quick start vm\nso you can just type in download\ncloudera quick start vm and you can\nsearch for package now this can be used\nto set up a quick start vm which would\nbe a single node cloudera based cluster\nso you can click on this link and then\nbasically based on the platform which\nyou would be choosing to install such as\nusing a vm box or which version of\ncloudera you would install so here i can\nselect a platform so i can choose\nvirtualbox and then you can click on get\nit now so give your details and\nbasically then it should allow you to\ndownload the quick start vm which would\nlook something like this and once you\nhave the zip file which is downloaded\nyou can unzip it which can then be used\nto set up a single node cloud error\ncluster so once you have downloaded the\nzip file that would look something like\nthis so you would have a quick start\nvirtual box and then a virtual boss disk\nnow this can be used to set up a cluster\nignore these files which are related to\namazon machines and you don't need to\nhave that so you would just have this\nand this can be used to set up a cloud\ndata cluster so for this to be set up\nyou can click on file import appliance\nand here you can choose your quick start\nvm by looking into downloads quick start\nvm select this and click on open now you\ncan click on next and that shows you the\nspecifications of cpu ram which we can\nthen change later and click on import\nthis will start importing virtual disk\nimage dot vmdk file into your vm box\nonce this is done we will have to change\nthe specifications or machines to use\ntwo cpu cores minimum and give a little\nmore ram because cloudera quick start vm\nis very cpu intensive and it needs good\namount of ram so to survive i will give\n2 cpu cores and 5 gb ram and that should\nbe enough for us to bring up a quick\nstart vm which gives us a cloudera\ndistribution of hadoop in a single node\ncluster setup which can be used for\nworking learning about different\ndistributions in cloudera clusters\nworking with sdfs and other hadoop\necosystem components let's just wait for\nthis importing to finish and then we\nwill go ahead and set up a quick start\nvm for our practice here the importing\nof appliance is done and we see cloudera\nquickstart machine is added to my list\nof machines i can click on this and\nclick on settings as mentioned i would\nlike to give it more ram and more cpu\ncores so click on system and here let's\nincrease the ram to at least\nfive and click on processor and let's\ngive it two cpu cores which would at\nleast be better than using one cpu core\nnetwork it goes for nat and that's fine\nclick on ok and we would want to start\nthis machine so that it uses two cpu\ncores 5gb ram and it should bring up my\ncloudera quick start vm now let's go\nahead and start this machine which has\nour quick start vm it might take\ninitially some time to start up because\ninternally there will be various\ncloudera services which will be starting\nup and those services need to be up for\nour cloud era quick start vm to be\naccessible so unlike your apache hadoop\ncluster where we start our cluster and\nwe will be starting all our processes in\ncase of cloudera it is your cloudera scm\nserver and agents which take care of\nstarting up of your services and\nstarting up of your different roles for\nthose services i explained in my\nprevious session that for a cloud era\ncluster it would be these services let\nme just show you that so in case of\napache cluster we start our services\nthat is we start our cluster by running\nscript and then basically those scripts\nwill individually start the different\nprocesses on different nodes in case of\ncloud era we would always have a\ncloudera scm server which would be\nrunning on one machine and then\nincluding that machine we would have\nclouded icm agents which would be\nrunning on multiple machines similarly\nif we had a hortonworks cluster we would\nhave ambari server starting up on the\nfirst machine and then ambari agents\nrunning on other machines so your server\ncomponent knows what are the services\nwhich are set up what are their\nconfigurations and agents running on\nevery node are responsible to send\nheartbeats to the server receive\ninstructions and then take care of\nstarting and stopping off of individual\nroles on different machines in case of\nour single node cluster setup in quick\nstart vm we would just have one scm\nserver and one sem agent which will\nstart on the machine which will then\ntake care of all the roles which need to\nbe started for your different services\nso we will just wait for our machine to\ncome up and basically have clouded sem\nserver and agent running and once we\nhave that we need to follow few steps so\nthat we can have the cloudera admin\nconsole accessible which allows you to\nbrowse the cluster look at different\nservices look at the roles for different\nservices and also work with your cluster\neither using command line or using the\nweb interface that is\nnow that my machine has come up and it\nalready is connected to the internet\nwhich we can see here we need to do\ncertain things so that we can have our\nadmin console accessible at this point\nof time you can click on terminal and\ncheck if you have access to the cluster\nso here type in host name and that shows\nyou your host name which is\nquickstart.cloudera we can also type in\nhdfs command to see if we have access\nand if my cluster is working these\ncommands are same as you would give them\nin an apache hadoop cluster or in any\nother distribution of a loop sometimes\nwhen your cluster is up and you have\naccess to the terminal it might take few\nseconds or few minutes before there is a\nconnection established between cloudera\ncm server and cloudera cm agent running\nin the background which takes care of\nyour cluster i have given a sdfs dfs\nlist command which basically should show\nme what by default exists on my sdfs\nlet's just give it a couple of seconds\nbefore it shows us out we can also check\nby giving a service cloudera scm server\nstatus and here it tells me that if you\nwould want to use cloudera express\nfree run this command it needs 8 gb of\nram and it leads to virtual cpu cores\nand it also mentions it may take several\nminutes before cloudera manager has\nstarted i can login as root here and\nthen give the command service cloud\nerror scm server status remember the\npassword for root is clouded so it\nbasically says that if you would want to\ncheck these settings it is good to have\nexpress edition running so we can close\nthis my sdfs access is working fine\nlet's close the terminal and here we\nhave launch cloud error express click on\nthis and that will give you that you\nneed to give a command which is\nforce let's copy this command let's open\na different terminal and let's give this\ncommand like this which will then go\nahead and shut down your cloudera based\nservices and then it will restart it\nonly after which you will be able to\naccess your admin console so let's just\ngive it a couple of minutes before it\ndoes this and then we will have access\nto our admin console here if you see it\nis starting the cloudera manager server\nagain it is waiting for cloudera manager\napi then starting the cloudera manager\nagents and then configuring the\ndeployment as per the new settings which\nwe have given as to use the express\nedition of cloudera once all this is\ndone it will say the cluster has been\nrestarted and the admin console can be\naccessed by id and password as cloudera\nwe'll give it a couple of more minutes\nand once this is done we are ready to\nuse our admin console now that\ndeployment has been configured client\nconfigurations have also been deployed\nand it has restarted the cloudera\nmanagement service it gives you an\naccess to quick start admin console\nusing username and password as cloud\nerror let's try accessing it so we can\nopen up the browser here and let's\nchange this to 7 1 8 0 that's the\ndefault port and that shows the admin\nconsole which is coming up now here we\ncan log in as cloud error cloud error\nand then let's click on login now as i\nsaid cloudera is very cpu intensive and\nmemory intensive so it would slow down\nsince we have not given enough gb ram to\nour cloud error cluster and thus it will\nbe advisable to stop or even remove the\nservices which we don't need now as of\nnow if we look at the services all of\nthem look in a stop status and that's\ngood in one way because we can then go\nahead and remove the services which we\nwill not use in the beginning and later\nwe can anytime add services to the\ncluster so for example i can click on\nkey value store here and then i can\nscroll down where it says delete to\nremove this service from the admin\nconsole now anytime you are removing a\nparticular service it will only remove\nthe service from the management by cloud\ndirect manager all the role groups under\nthis service will be removed from host\ntemplates so we can click on delete now\nif this service was depending on some\nother service it would have prompted me\nwith a message that remove the relevant\nservices on which this particular\nservice depends if the service was\nalready running then it would have given\nme a message that the service has to be\nstopped before it can be deleted from\nthe cloudera admin console now this is\nmy admin console which allows you to\nclick on services look at the different\nroles and processes which are running\nfor this service we anyways have access\nto our cloudera cluster from the\nterminal using our regular sdfs or yarn\nor map red commands now i removed a\nservice i will also remove solar which\nwe will not be using for the beginning\nbut then it depends on your choice so we\ncan here scroll down to delete it and\nthat says that before deleting the solar\nservice you must remove the dependencies\non the service from the configuration of\nfollowing services that is hue now hue\nis a web interface which allows you to\nwork with your sdfs and that is\ndepending on this so click on configure\nservice dependency and here we can make\nsure that our hue service does not\ndepend on a particular service we are\nremoving so that then we can have a\nclean removal of the service so i'll\nclick on none and i will say save\nchanges once this is done then we can go\nahead and try removing the solar service\nfrom our admin console which will reduce\nsome load on my management console which\nwill also allow me to work faster on my\ncluster now here we have removed the\ndependency of hue on solar so we can\nclick on this and then we can delete it\nremember i'm only doing this so that my\ncluster becomes little lighter and i can\nwork on my focus services at any point\nof time if you want to add more services\nto your cluster you can anytime do that\nyou can fix different configuration\nissues like what we see here with\ndifferent warning messages and here we\nhave these services which are already\nexisting now if we don't need any of the\nservice i can click on the drop down and\nclick on delete again this says that\nscoop 2 also has\nrelevance to hue so hue as a web\ninterface also depends on scope 2. as of\nnow we'll make it none at any point of\ntime later you can add the services by\nclicking the add service option now this\nis a cluster to which you have admin\naccess and this is a quick start vm\nwhich gives you a single node cloud\nerror cluster which you can use for\nlearning and practicing so here we'll\nclick on scope 2 and then we will say\ndelete as we have configured the\ndependency now and we will remove scope\n2 also from the list of services which\nyour admin console is managing right so\nonce this is done we have removed three\nservices which we did not need we can\neven remove scope as a client and if we\nneed we can add that later now there are\nvarious other alerts which your cloudera\nadmin console shows and we can always\nfix them by clicking on the health\nissues or configuration issues we can\nclick here and see what is the health\nissue it is pointing to if that is a\ncritical one or if that can be ignored\nso it says there is an issue with a\nclock offset which basically relates to\nan ntp service network time protocol\nwhich makes sure that one or multiple\nmachines are in the same time zone and\nare in sync so for now we can click on\nsuppress and we can just say suppress\nfor all hosts and we can say look into\nit later and confirm so now we will not\nhave that health issue reported that\nprobably the ntp service and the\nmachines\nmight not be in sync now that does not\nhave an impact for our use case as of\nnow but if we have a kerberos kind of\nsetup which is for security then\nbasically this offset and time zone\nbecomes important so we can ignore this\nmessage and we are still good to use the\ncluster we also have other configuration\nissues and you can click on this which\nmight talk about the heap size or the\nram which is available for machines it\ntalks about zookeeper should be in odd\nnumbers queue does not have a load\nbalancer sdfs only has one data node but\nall of these issues are not to be\nworried upon because this is a single\nnode cluster setup so if you want to\navoid all of these warnings you can\nalways click on suppress and you can\navoid and let your cluster be in all\ngreen status but that's nothing to worry\nso we can click on cluster and basically\nwe can look at the services so we have\nremoved some services which we don't\nintend to use now i have also suppressed\na offset warning which is not very\ncritical for my use case and basically i\nam good to start the cluster at any\npoint of time as i said if you would\nwant to add services this is the actions\nbutton which you can use to add service\nso we will just say restart my cluster\nwhich will restart all the services one\nby one starting from zookeeper as the\nfirst service to come up we can always\nclick on this arrow mark and see what is\nhappening in the services what services\nare coming up and in which order if you\nhave any issues you can always click on\nthe link next to it which will take you\nto the logs and we can click on close to\nlet it happen in the background so this\nwill basically let my services restart\none by one and my cluster will then\nbecome completely accessible either\nusing hue as a web interface or quick\nstart terminal which allows you to give\nyour commands now while my machines are\ncoming up you can click on hosts and you\ncan have a look at all the hosts we have\nas of now only one which will also tell\nyou how many roles or processes are\nrunning on this machine so that is 25\nrolls it tells you what is the disk\nusage it tells you what is the physical\nmemory being used and using this host\ntab we can add new host to the cluster\nwe can check the configuration we can\ncheck all the hosts in diagnostics you\ncan look at the logs which will give you\naccess to all the logs you can even\nselect the sources from which you would\nwant to have the logs or you can give\nthe host name you can click on search\nyou can build your own charts you can\nalso do the admin stuff by adding\ndifferent users or enabling security\nusing the administration tab so since we\nhave clicked on restart of a cluster we\nwill slowly start seeing all the\nservices one by one coming up starting\nwith zookeeper to begin with and once we\nhave our cluster up and running whether\nthat is showing all services in green or\nin a different status we still should be\nable to access the service now as we saw\nin apache hadoop cluster even here we\ncan click on sdfs and we can access the\nweb ui once our sdfs service is up by\nclicking on quick links so the service\nis not yet up once it is up we should be\nable to see the web ui link which will\nallow you to check things from sdfs web\ninterface similarly yarn as a service\nalso has a web interface so as soon as\nthe service comes up under your quick\nlinks we will have access to the yarn ui\nand similarly once the service comes up\nwe will have access to hue which will\ngive you the web interface which allows\nyou to work with your sdfs which allows\nyou to work with your different other\ncomponents within the cluster without\neven using the command line tools or\ncommand line options so we will have to\ngive it some time while the cloud error\nscm agent on every machine will be able\nto restart the roles which are\nresponsible for your cluster to come up\nwe can always click here it tells that\nthere are some running commands in the\nbackground which are trying to start my\ncluster we can go to the terminal and we\ncan switch as hdfs user remember sdfs\nuser is the admin user and it does not\nhave a password unless you have set one\nso you can just log in as sdfs which\nmight ask you for a password initially\nwhich we do not have so the best way to\ndo this is by logging in as root where\nthe password is cloud error and then you\ncan log in as hdfs so that then onwards\nyou can give your sdfs commands to work\nwith your file system now since my\nservices are coming up right now when i\ntry to give a sdfs dfs command it might\nnot work or it might also say that it is\ntrying to connect to the name node which\nis not up yet so we will have to give it\nsome time and only once the name node is\nup we will be able to access our sdfs\nusing commands so this is how you can\nquickly set up your quick start and then\nyou can be working using the command\nline options from the terminal like what\nyou would do in apache hadoop cluster\nyou could use the web interfaces which\nallow you to work with your cluster now\nthis usually takes more time so you will\nhave to give it some time before your\nservices are up and running and for any\nreason if you have issues it might\nrequire you to restart your cluster\nseveral times in the beginning before it\ngets accustomed to the settings what you\nhave given and it starts up the services\nat any point of time if you have any\nerror message then you can always go\nback and look in logs and see what is\nhappening and try starting your cluster\nso this is how we set up a quick start\nvm and you can be using this to work\nwith your cloudera cluster and welcome\nto the tutorial where we will learn\ntoday on setting up a apache hadoop\ncluster although there are various\ndistributions of hadoop as we have\nalready discussed and you could set up a\napache hadoop cluster which is the core\ndistribution you could be setting up a\ncloud error distribution of hadoop or\neven a hortonworks however for cloudera\nyou would need machines which are really\npowerful at least machines which have 16\ngb of ram and in which you can set up\nmultiple virtual machines each virtual\nmachine of at least four to five gb same\nthing applies for hortonworks or you\ncould always download and set up a\ncloudera quick start vm or a hortonworks\nsandbox in this session we will learn\nhow to set up apache hadoop cluster\nwhich is the core distribution and if\none has learned how to set up apache\nhadoop cluster then they would\nunderstand the internals of setting up a\ncluster different configuration\nproperties which are needed for your\ncluster setup we can say that these are\nthe vendors which take up the apache\nhadoop package it within a cluster\nmanagement solution so that users who\nintend to use apache hadoop would not\nhave difficulties of setting up a\ncluster setting up a framework they\ncould just use a vendor-specific\ndistribution with its cluster\ninstallation solutions clustered\nmanagement solution and easily plan\ndeploy install and manage a cluster so\nlook into this link which talks about\ndifferent distributions of hadoop coming\nback what is hadoop hadoop is a open\nsource framework for storing huge amount\nof data and processing it it also\ndepends on commodity machines and it\ndoes not need carrier class hardware now\nwhen we say commodity machines we are\nstill talking about machines which are\nhigher than a basic laptop or a basic\nserver but then they are not at the\nlevel of a carrier class hardware and\ncosting huge to the organization hadoop\nas a framework has two main components\nor i would say two processing layers it\nhas a storage layer which is called hdfs\nhadoop distributed file system and it\nhas a processing layer which is called\nyarn which can handle different\nprocessing frameworks in which hadoop\nmap reduce is the oldest most mature and\nmost evolved processing framework sdfs\nderives its existence from gfs it solves\nthe issue of storing huge amount of data\nin a distributed fashion and mapreduce\nis a programming model which allows to\nprocess data which is distributed across\na cluster we will learn on sdfs and\nmapreduce in further slides sdfs is a\nstorage layer now compare your laptop\ncompare your desktop that also has a\nprocessing layer and a storage layer\nyour disks are your storage layer your\nram and cpu core is your processing\nlayer similarly hadoop as a solution has\na storage layer that is a distributed\nfile system which can store huge amount\nof data in multiple machines\ninstead of a single machine or a single\nserver which basically makes it highly\nreliable highly fault tolerant and\ndistributed for parallel processing\nconsider a data file of size one\ngigabyte now if that was getting stored\nin one single machine we would still\nhave the data getting stored in one or\nmultiple disks and if you would try to\nread the data the seek time time taken\nto read the data would be very high in\ncomparison to reading it from multiple\nmachines now sdfs like your gfs divides\nthe file into smaller chunks and stores\nthe data across the hadoop cluster in\nprevious versions of hadoop the default\nblock size was 64 mb however from hadoop\nversion 2 onwards the block size is 128\nmb now that's customizable depending on\nthe average size of the data which is\ncoming into the cluster the block size\ncan be changed the minimum block size is\n128 mb which basically means that every\nfile which is\nless than or equal to 128 mb would use\none block and any file which goes beyond\nthe size of block will use multiple\nblocks and these blocks would be\nresiding on the underlying slave\nmachines files are automatically split\nby hadoop framework into blocks of\ndefault block size and stored across the\ncluster each node in your hadoop cluster\nwould have a bunch of disks that would\nbe storing these blocks now once we have\nthe data stored\nin a distributed fashion that also gives\nus the flexibility of going for\ndistributed computing and when we say\ndistributed computing we are talking\nabout performing distributed parallel\nprocessing on large volumes of data take\nan example where we are talking about\none single machine or one huge server\nwhich has four i o channels and each\ninput output channel can take care of\n100 megabytes of per second 100\nmegabytes of data per second so if we\nhave a data file of one terabyte it\nwould take approximately 43 minutes for\none machine to process one terabyte of\ndata if you compare that with\ndistributed computing where we are\ntalking about hundreds of machines each\nmachine having similar number of input\noutput channels in each channel having\nthe same capacity to handle 100\nmegabytes of data per second if a data\nfile of one terabyte had to be processed\nin a parallel processing fashion using\ndistributed computing it would take just\n26 seconds for 100 machines to process\none terabyte of data in this environment\nnow all of you might be thinking that\nyeah this is right but then what about\nthe processing logic that might be\ncomplicated and yes that would be but\nthen we have a mapreduce programming\nmodel and a processing framework to help\nus doing that now when we look at one of\nthe distributions of hadoop that is\nhortonworks it also has different\nresources that provide security to your\nbig data now before we start talking on\nthese different services like apache nox\napache ranger which basically allow you\nto monitor authorize and audit your\nhadoop clusters let's also look at the\nhistory of hadoop which started in 1999\nwith apache software foundation in 2002\na nuts created by doug cutting and mike\ncaprilla then dug cutting joined hadoop\nand took\nnuts product with him 2008 nuts was\ndivided and hadoop was born in 2008\nyahoo released hadoop as an open source\nproject to asf in 2008 also hadoop based\nstartup cloud era incorporated cutting\nleft yahoo for cloudera yahoo was then\ncoming out with hortonworks as a\ncommercial hadoop distribution map r\ntechnology also released hadoop\ndistribution green plum released hadoop\ndistribution and pivotal hd so this is a\nbrief on history of hadoop you can\nalways read about history of evolution\nof hadoop how it started and today there\nare more than 10 distributions which are\nexisting in market however cloudera is\nthe dominant one and as per recent news\nitems cloudera and hortonworks have\nmerged as one entity which will bring up\nbetter solutions to the market\nnow here we will want to learn on hadoop\necosystem so far we have learned on big\ndata so what is big data we have learned\non what are the different use cases\nwhere big data can be used different\norganizations which use big data and we\ntook an example of facebook we also\nlooked at google file system which laid\nthe foundation for sdfs the hadoop's\ndistributed file system and we have not\nyet spoken about the processing layer\nwhich is yarn and the processing\nframework which is mapreduce which we\nwill discuss in future slides or further\nslides let's spend some time in\nunderstanding the hadoop ecosystem or\nyou might have heard as ecosphere which\nhas various products or you can say\ntools or you can say packages which work\ntogether to comprise your hadoop cluster\nnow this picture shows the different\ncomponents which are used for different\nrequirements to work on your big data\nstarting from bottom to top we would\nlook at sdfs which is hadoop distributed\nfile system basically a file system\nwhich would use multiple machines to\nstore the data you have a processing\nlayer that is yarn we have a programming\nmodel that is mapreduce which could be\nwritten in any programming language\nprimarily in java but then that could be\nalso written in python or scala or any\nother programming language so these are\nthe main components so you have sdfs and\nyou have yarn these are the processing\nlayers and for each processing layer we\nwould have multiple processes which\nwould be running on different machines\nwithin a cluster now what would a hadoop\ncluster do if we did not have any data\nand where would that data come from so\nthere might be a lot of sources where\nthe data is getting generated and that\ndata might already be stored in rdbms\nand if you are interested in bringing in\nthe structured data from your rdbms\nlayer to your hadoop ecosystem then you\nwould use data injection tools like\nscrew so scope would be used for data\ningestion transformation and pushing it\ninto sdfs now that could also be used\nfor exporting the data from sdfs back to\nrdbms but scope mainly is for structured\ndata now you have other data injection\ntools like flume which is used for\ningestion of streaming or unstructured\nkind of data data could directly be\ningested into sdfs or it could be going\ninto a data warehousing package called\nhive now hive is a data warehousing\npackage on top of sdfs where users could\nwrite their sql queries to process the\ndata they could be creating their tables\nand databases to store the data and that\ndata would eventually be getting stored\nin sdfs and then in the underlying\nsystems hive could be accessed through\nvarious services and edge catalog is one\nof them your data which is being\ningested could also be stored in a\nunstructured data store such as apache\nhbase now there are lots of nosql\ndatabases there are a lot of data\ningestion tools so if we go to google\nand if we search for data ingestion\ntools if i say data ingestion tools so\nyou could look at this link which shows\naround 18 different data ingestion tools\nwhich are existing in market you have\nthese tools for import and export for\ntransforming for governance for even\nprocessing the data while the data is\nbeing ingested so you have apache kafka\nwhich is a published subscribing\nmessaging system you have apache nifi\nwhich is not only for streaming or\nstructured data but it also helps with\ngovernance you have wavefront you have\ndata torrent amazon's kinesis\napache storm samsa sync sort goblin\nflume scoop samsara fluenty mufflins\nwhite elephant chokwa heka scribe\ndatabus and many more these are some\ndata ingestion tools which can be used\nnow we could also look at nosql\ndatabases and if you just type in nosql\ndatabases that can take you to\nnosqldb.org and that would show you that\nthere are more than\n225 nosql databases existing in market\nnosql stands for not only sql so these\nare databases which can be created\nwithout defining any schema they could\nbe key value stores they could be\ndocument based databases which allow a\nlot of dynamism which have different\nkind of constraints and can allow you to\nstore your structured unstructured or\nsemi-structured data so have a look at\nthis exhaustive list of nosql databases\nso coming back to your hadoop ecosystem\nyour data could be ingested using one of\nthe data ingestion tools into sdfs or\ninto a hive table or into hive that\nwould be using sdfs as the underlying\nlayer your data could also be imported\ninto in nosql databases such as hbase\nwhich is a four dimensional database\nit's a key value store which allows for\nfaster random access across billions of\nrecords we learn on edge base on hive in\nother sessions now when you talk about\nhadoop echo system you could have a\nworkflow manager such as uzi now there\nare other workflow managers like azkaban\nyou have luji right which can be used\nbut uzi comes with hadoop now not with\napache hadoop as i said apache hadoop is\nan open source and to set up apache\nhadoop cluster and to set up hive or\nedge base or any of these components we\nwould have to manually download the\npackages edit the configs set up the\nprocesses and start the cluster and\nservices\nhowever if you were using a\nvendor-specific distribution many of\nthese packages would come as a packaged\nsolution so oozy is a workflow scheduler\nwhich allows you to create a workflow of\ndifferent jobs which could be run on a\nhadoop cluster you could have a service\nsuch as zookeeper which is mainly for\nmanagement and monitoring so it's a\ncentralized coordination service which\nespecially plays a very important role\nwhen it is managing a high availability\ncluster of hadoop or hbase you have pick\nscripting which allows for processing of\ndata in a very concise way you have hive\nquerying which allows users to write\ntheir sql queries to process the data\nyou could also do interactive analysis\nusing packages like apache drill you\nhave apache mahout for machine learning\nyou have kafka apache storm and even\nspark which can be used for streaming\nnow spark again has different components\nand it is a in-memory computing\nframework which allows you to work on\nstreaming data structure data graph\nbased data and even machine learning\nalgorithms so hadoop as such is a\nsolution which allows organizations to\nwork on big data and hadoop ecosystem\nand the components which are involved\nwhich can be part of your hadoop\necosystem are growing so there can be\nlots and lots of other components which\ncan be part of your adobe echo system\nwhich can be overall used to store\nprocess capture and analyze big data\nthis is where we come to\na point where we should now start\nlearning on hadoop distributed file\nsystem which allows you to store data in\na distributed fashion sdfs basically\nmakes it easier for organizations to\nstore huge amount of data in previous\nsession we were learning on big data its\ncharacteristics what are organizations\ninterested in when they talk about big\ndata and different use cases today we\nwill learn on hadoop distributed file\nsystem which is basically the logical\nstorage layer for a hadoop cluster sdfs\nis fault tolerant is reliable and is\nscalable so let's understand and learn\non the working of sdfs that is the\nlogical storage layer for a hadoop\ncluster sdfs is a distributed file\nsystem which is designed to store large\nvolumes of data on commodity machines it\nis designed with a low cost hardware and\nthat is what i mean when i say commodity\nmachines so we could have a cluster of\ntens and hundreds and thousands of nodes\nwhich can still scale and accommodate\nhuge volume of data it provides access\nto data across multiple hadoop clusters\nit has a high fault tolerance and\nthroughput and sdfs stores data in\nmultiple servers instead of a central\nserver now earlier when we were\ndiscussing on how storing data on a\nsingle machine can lead to more read or\nprocessing times and basically lead to\nhigh seek time that can be taken care by\na distributed file system such as sdfs\nwhich would rely on file systems of\nmultiple machines which form your hadoop\ncluster now let's understand how sdfs\nworks for example we have a large file\nwhich is to be stored on sdfs now that\nfile would be broken down into blocks\nand the block size by default is 128 mb\nthese blocks would then be stored across\ndifferent slave machines or different\nnodes which are responsible for storing\ndata sdfs has a default block size of\n128 mb which can be increased as per\nrequirement now that could be in\nmegabytes gigabytes terabytes or even\nbigger than that the default block size\nis 128 mb from hadoop version 2 onwards\nand your previous versions of hadoop had\nthe block size of 64 mb here the point\nto be remembered is block is a logical\nentity which basically takes the data\nand stores it on the underlying machines\nin their disks now this is how it would\nlook in a normal hadoop cluster we would\nhave one single master unless it is a\nhigh availability cluster where we could\nhave multiple masters that has two\nmasters one being active and one being\nstandby in a normal hadoop cluster you\nwould have one master and multiple slave\nnodes where these data blocks are\ngetting stored now before we understand\nthe mapreduce algorithm let's also\nunderstand the working of sdfs and how\ndata gets stored overall how does it\nwork so let me just open up a notepad\nfile and i would explain about sdfs now\nlet's imagine that this is i would like\nto have say five machines which could be\nforming my cluster so i would have\nmachine one machine two machine three\nmachine four and machine five now if i\nsay my hadoop cluster is running on\nthese machines\nwhich could be of a apache hadoop core\ndistribution or having a vendor specific\ndistribution such as cloudera or\nhortonworks or map r or ibm big insight\netc so these are some vendor-specific\ndistributions of a hadoop cluster now\nhere we can discuss later on how or what\nprocesses would be running on these\nmachines now in case of apache hadoop\ncluster in case of apache hadoop cluster\nto have a hadoop cluster running we\nwould have to download the hadoop\nrelated packages we would have to untar\nthen we would have to edit config files\nto specify which processes would run on\nwhich nodes and then basically we would\nbe formatting hdfs and then onwards we\ncan go ahead and start our cluster so it\nlooks like in apache adobe cluster we\nwould have to manually do various things\nsuch as downloading the hadoop related\ntar file untiling it editing the config\nfiles formatting sdfs and then going\nahead and starting your cluster in case\nof a vendor-specific distribution such\nas cloudera or hortonworks let me split\nit here so you would have basically a\ncluster management solution that would\nhelp you in doing various things to set\nup your cluster in case of a cloud error\ndistribution of hadoop you would have a\ncloud installer file and then you would\nhave a cloudera manager which will help\nin deploying a cloudera cluster so\ndownloading hadoop related parcels or\npackages untiling them editing config\nfiles formatting sdfs and basically\nstarting your cluster would be taken\ncare automatically by cloudera manager\nin case of hortonworks data platform it\nwould be ambari software that would be\nused for deploying your cluster now no\nmatter which distribution of hadoop you\nwould be running you would still have\nthe same roles or processes which would\nbe running let me just briefly discuss\nhere about the terminologies so let me\nbring up a file here which could explain\nthese details so let's look at this now\nhere i have documented how it would look\nin different distributions of hadoop so\nwe have hadoop version one and two and\nlet me just look for files here so this\nis the one now if you would look in\nversion one apache hadoop we would call\ndemons or processes and if you look at\nthe names we have the name node we have\ndata nodes we have job tracker we have\nsecondary name node and we have task\nrecords in case of version 2 you would\nstill call them demons you could have\none or multiple name nodes if it was a\nfederation or a high availability kind\nof setup you would have data nodes\ninstead of job tracker you would have\nresource manager and then there is a\ntemporary diamond called mr app master\nif you had a high availability then\nsecondary name node would not be allowed\nand if there was a federation kind of\nsetup secondary name node would still be\nexisting instead of task trackers you\nwould have node managers now in case of\nvendor-specific distribution such as\ncloudera whether you are talking about\nolder version or newer version which is\nfive point something you would have what\nwe call as roles so in apache hadoop we\ncall them demons in cloudera we call\nthem roles in hortonworks we call them\ncomponents but we are talking about the\nsame thing in case of cloudera you would\nhave services which would be part of the\ncluster such as impala oozy spark hive\nhbase storm hue sdfs mapreduce yarn and\nso on similarly in hortonworks also you\nwould have these services which would be\nrunning in case of apache hadoop we\nwould not have so called services seen\nbut then the functionality still exists\nand how is that that is depending on\nthese demons so basically if you look in\napache hadoop or cloudera or hortonworks\neverywhere we have these demons and we\nhave these would be running on one or\nmultiple nodes so we could have\ndifferent kind of setups and just to\nbriefly discuss on that so we could have\na name node running here a resource\nmanager running here and you could have\na secondary name node now don't go by\nname it's not a second name node we'll\ndiscuss about that later a secondary\nname node and then we could have data\nnodes running in machines so this is one\nkind of setup where i am talking about\ndata node node manager and here we have\na data node node manager and similarly\nwe would have that on one more machine\nnow this is one kind of setup this is\nnot the only setup possible you could\nhave a different setup where on the same\nmachines you could decide to have data\nnode and node manager running on every\nnode however that would not be preferred\nbecause you wouldn't want the slaves and\nthe masters to run on the same machine\nhowever this setup is also possible you\ncould have a high availability kind of\nsetup which basically is good for\ncluster and for an organization where\nyou could have name node 1 which would\nbe running here and say we can have a\nname node 2 running here we could have a\nresource\nmanager 2 running here and then we could\nhave a resource manager one running here\nand i could still have my data node and\nnode manager running in the same fashion\nnow this would not be allowed in a high\navailability cluster because we have a\nstandby name node so this is one more\nsetup which is possible and then you\ncould have a single node setup which\ncould be everything having on the same\nmachine that is named node resource\nmanager secondary name node data node\nand node manager so these are different\nsetups and if somebody asks which is a\npreferable setup then i would still go\nfor either this one which has multiple\ndata nodes and node managers not this\none or your high availability setup now\na single node or a sudo distributed\ncluster is basically good for testing or\nsome development kind of activities and\nthese are your distributed clusters now\nif we closely see we basically have the\nroles or components or demons running on\nthese machines to take care of mainly\ntwo functionalities or two services that\nis your hdfs\nwhich is your storage layer and yarn\nwhich is your processing layer so in a\nhadoop cluster we have a storage layer\nand we have a processing layer for\nstorage layer that is sdfs you basically\nhave one single master and you have\nmultiple slave processes running so i\ncould say for hdfs to work fine i should\nhave a name node running multiple data\nnodes running and then a secondary name\nnote running for yarn as a service that\nis my processing layer i would need\nminimum one resource manager and\nmultiple node managers running on one or\nmultiple machines so these are the two\nprocessing these are the two layers one\nbeing the storage layer and one being\nthe processing layer and for your\nstorage layer you have these processes\nwhere in your name node is the master\nand your data nodes and secondary name\nnode can be considered as the slaves you\nhave your resource manager as the master\nfor your processing layer and node\nmanagers are the slaves now this is\nbriefly about the setup which could be\nin a particular environment now say the\nhadoop related packages or parcels have\nbeen downloaded whether it is a\nvendor-specific distribution or apache\nhadoop cluster say untarring of that is\ndone editing of config files is done\nformatting is done and your cluster is\nstarted in that case your cluster would\nbe up and running and then we could\nbasically have and once your cluster is\nup we would have the sdfs up and running\nready to allow to store data now let's\nimagine that this is my sdfs which is up\nand running or sdfs as a service is up\nand running let's imagine that these are\nmy nodes which is m1 m2 m3 m4 and m5 now\nwhile the config files were edited for\neach of these nodes it would have been\ndefined which processes will run and we\nwould have also given path where these\nprocesses will store the data so let's\nsay that this is my m1 this is m2 m3\nm4\nand m5 m3 m4 and m5 have data nodes\nrunning so we would have defined a path\nlet's call it say abc slash data abc\nslash\nagain data or d and abc so this is the\npath which has been defined on every\nnode to store hadoop related data and on\nm1 where we would have name node running\nwe would have still defined a path let's\ncall it abc slash n where name node\ncould store its data so once your\ncluster is up and running and in your\nsdfs is available we can have our data\nstored on sdfs now let's understand how\ndoes that work so any client or an\napplication which would want to write\ndata to the cluster needs to interact\nwith the master to begin with say let's\nsay client machine or an api or an\napplication would be interested in\nwriting the data now how would name node\nknow about available data nodes so as\nsoon as the cluster comes up the first\nthing which happens is these data nodes\nwould be sending in their heartbeat to\nthe name node every three seconds so\ndata nodes send the heartbeat to name\nnode every three seconds to let the name\nnode know to let the master know that\nthey are available what does name node\ndo with this name node is building a\nmetadata in its ram so it already has\nsome metadata in the disk which was\ncreated as a result of formatting sdfs\nand now we would have metadata in ram\nwhich is getting built so initially name\nnode would have information of which\ndata nodes are available in the cluster\nnow if a client or an application wants\nto write data to the cluster it gets in\ntouch with the name node inquiring about\nthe slave machines are enquiring about\nthe data nodes where the data can\nactually be written your name node\nrefers to the metadata collected so far\nand responds back mentioning the data\nnodes or the slave machines which are\navailable for data storage based on this\nresponse your client takes the file to\nsdfs intending to write it to the\ncluster now in hadoop we basically have\ntwo things one is the default block size\nwhich we discussed earlier is 128 mb so\nyou have 128 mb as the default block\nsize which is customizable and that can\nbe changed depending on the average data\nsize and you have the default\nreplication which is three now imagine\nthe application or api or client would\nwant to write a file which is 256 mb\nwhich is bigger than the block size and\nthis file has to be broken down into\nwhat we call as blocks which are logical\nentities so the file is taken to the\nframework framework will split the file\ninto blocks so let's imagine that this\nfile is 256 mb it will be broken down\ninto two blocks b1 and b2 and these\nblocks will then be randomly distributed\nto the slave machines that is the data\nnodes so we would have block one say\ngetting stored here now you could also\nhave block one getting stored on any of\nthese data nodes so we would have block\none and then we could have block two\nwhich could be written here and then you\nwould also have their replicas sitting\non the machine so the rule of\nreplication is you will never have two\nidentical blocks sitting on the same\nnode so if i have block one here then i\nwill never have a replica of block one\nagain on the same machine so we will\nhave a replica of block 2 here block 1\nhere and you have a replica of block 2\nhere there is no differentiation between\nwhich one is the original and which one\nis the replica for the cluster all the\nblocks are same and what also happens is\nafter the blocks are written your data\nnodes are repeatedly sending their block\nreport to name node every 10 seconds and\nthis information is again updated in the\nname nodes metadata so now name node\nwill have information what are the files\nthe files are broken down into what\nblocks the blocks are residing on which\ndata nodes and this metadata is getting\nbuilt in the name node as data nodes\nrespond with their heartbeats and also\nevery 10 seconds with their block report\nnow we need to remember one point here\nthat name node is basically a master and\ndata nodes are the slave hadoop as a\nframework has been coded in java which\nbasically means that every directory\nevery file and file related block is\nconsidered as an object which is getting\nstored and for every object which name\nnode is tracking 150 bytes of name nodes\nram is utilized which also tells us that\nif the block sizes were smaller than 128\nmb then for the same file we would have\nmore number of blocks which would mean\nthat name nodes are ram would be over\nutilized and overload name node so block\nsize has to be chosen with a lot of\nconsideration on the masters ram and\nwhat is the average size of the data now\nthis is how the data is distributed in\nblocks and distributed across the data\nnodes does sdfs is very fault tolerant\nbecause if any of the data nodes fail\nthen you still have other data nodes\nwhich have the replica of data does\nmaking it possible to read the data and\naccess the data even if multiple data\nnodes go down now there are other\ninternals that is how data nodes are\nplaced in racks or what has rack\nawareness to do in block placement but\nwe will learn on all that later so this\nis how your sdfs works now there are\nmany other things such as the read and\nwrite process\nwhich is taken care by your methods such\nas open and create which internally are\nplaying a role when a read or write\noperation happens also facts like\nreplication is a sequential process but\nright activity is parallel so we can\nlearn more functionality of sdfs in\nlater sessions but this is in brief on\nhow sdfs works how your file is split\ninto blocks and the blocks are getting\nstored on the disks note that the block\ncontains the data so even if you have a\nfile which is one kilobyte and if this\nfile is to be stored on sdfs it will\nstill use a block of 128 mb which does\nnot mean that there is a disk space\ngetting wasted when the block resides on\nthe data node's disk it will only occupy\none kilobyte of space for the data it\nhas which means that block is a logical\nentity to take your data to the disk and\nget it stored on the disk this is how\nyour sdfs works and basically takes care\nof reliable data storage across machines\nin distributed fashion\nnow that we have set up a two node\ncluster which has two data nodes two\nnode managers one master machine let's\ntry working with sdfs let's look at\nquick commands which allow you to write\ndata to the cluster although there are\ndifferent ways the first way is using\ncommands the second is using an ide and\nwhere you could write your code to use\nhadoop's file system api and the third\nis a graphical user interface called hue\nwhich normally does not come with apache\nalthough it can work with apache it\ncomes as a service with cloud error and\nother distributions to work with hdfs we\nneed some sample data now there are\nvarious websites from where you can get\nsome sample data so you can open up your\nbrowser and let me show you some sample\nplaces or sample data set links from\nwhere you can download bigger or smaller\nfiles which then can be used to write\ndata to your sdfs although you could\njust pick up a local file or you could\ncreate your own files and write them to\nsdfs there are various data set sites\nwhich can be used so for example you can\ntype in machine learning uci data sets\nand this is one link which has more than\n200 data sets which can be used for even\ndata science kind of activities which\ncan be used for testing with your\nalgorithms and you can download data\nfrom here on to your local machine and\nthen from your local machine we will be\nwriting the data to hdfs which then\nbased on the size of the data splits the\ndata into blocks and your blocks would\nget stored on your underlying machines\nwherever data nodes run so i can click\non this uci machine learning repository\nor even just on data sets to be very\nspecific all we need is some sample data\nsets you can get kegel data sets you can\nget data sets from yelp website which\nare bigger data sets zipped and you can\nbe using them to write data to see how\nyour files split into multiple blocks\nnow here we have a lot of data we can\nclick on any one link to begin with and\nhere i have the data folder shows me the\nsum file and i can\njust right click on this\nsave link as\nand save the file this is a simple text\nfile and we can save it and you can get\nvarious data sets from here you can even\ngo to yelp data set challenge and that\nhas bigger files which are zipped and\nthey can be downloaded so you can click\non download data set although i will not\nstart the downloading here you can just\ngiven your name here just some email id\nsome initials and you can click on\ndownload so this is one more website\nwhere you can download the json data\nsets which are 3.6 gigabytes in\ncompressed format and then you have\nphotos which are in 7.2 gigabytes you\ncan also say gutenberg download books\nand that is again a website from where\nyou can download free data sets which\nare not very huge so try to get some\nvariety of data sets in different\nformats which then you can write to hdfs\nyou can even use your sdfs log files\nwhich already exist on your machines and\nyou don't need to download anything from\ninternet now let's close this for now so\ni already have some file let's copy this\nfrom home htc downloads\nthat was my file let's copy to home htc\nand here's my file i can check the\npermissions so that's owned by sdc i can\neven look into user local hadoop\nlogs and there are some already log\nfiles so you can even copy some of these\nfiles using copy\nuser local hadoop logs and let's take\nhadoop\ndata node related log and let's copy to\nhome sdc for our testing and now we have\nsome other files now to work with sdfs\nas i said you can always type in hdfs\nand that shows you various options now\nwe already used the first option which\nis name node iphone format we also used\nhd dfs admin to look at how many nodes\nwe have and to work with your hdfs file\nsystem the option is dfs so you would\nsay hdfs dfs and then you can always hit\non enter it shows you various options to\nwork with your distributed file system\nlike creating a directory copying some\nfiles getting data from your sdfs to\nyour local machine changing the\npermissions changing the replication and\nmany other options so here let's click\non hdfs dfs let's create it directly and\nlet's call it my data so this is a\ndirectory i'm creating on hdfs remember\nthis directory will not be seen on your\nlocal machine it is a hdfs directory now\nif i look in my root i don't see my data\nbecause that's a sdfs directory which is\ntechnically getting created on sdfs and\nonce you start writing data to sdfs the\nrelevant blocks will start getting\nstored under abc slash data so as of now\nwe can always keep a browser open to see\nas we work with sdfs how does the data\npropagate or come to your distributed\nfile system using the web interface so\nhere let's say http\nslash m1 five zero zero seven zero and\nthat's my browser interface let's click\non utilities and let's click on browse\nthe file system which should show the\ndirectory which we have created although\nthat does not have any data as of now\nlet's go in here and now we know that we\nhave some files here so i will say hdfs\ndfs and this has two options either i\ncan say put or i can say copy from local\nboth of these do the same thing and both\nof these you can find here so we have\nput and we should have copy from local\nthat is you're copying the data from\nyour local machine to your hdfs so now i\nwill say copy from local i will say take\nthe aba directory or file and put it in\nmy data so now i'm technically copying a\nfile from my local file system to my\nhdfs\nwhich is then using again my local file\nsystems path so this is done let's\nquickly check by clicking on my data if\nmy file is written now if you see this\nfile is 187 kilobytes however this will\nstill use a block of 128 and in my\nprevious sessions i already explained\nwhy a block of 128 but when it goes to\nthe disk it will only occupy the space\nfor the data the block has we can\nquickly go in here and we can do a hdfs\ndfs listing of my directory which should\nhave my file now and we can quickly\ncheck if my file is replicated either\nusing the web interface which says\nreplication is true that means the file\nrelated blocks should be getting stored\non the underlying data nodes we can\nalways click on overview if we would\nwant to browse and scroll down which\nsays there are three files in a\ndirectory the file has one block so\ntotally you have four file system\nobjects we can scroll down to show or\nsee we have two live nodes and if you\nsee each node has one block we can\nalways look from our data path\nby looking into abc slash data slash\ncurrent that has block pool directory\nthat has current that has finalized that\nas subdirectory subdirectory and the\nblock look at the size of the block it\nis 191 which relates to the kilobyte\nsize which we are talking about\nsimilarly we can check on m2 machine\nwhich also has a data node running to\nmake sure our file related block is\nreplicated so look in block pool current\nfinalized subdirectory sub directory and\nthat shows me the same block which is\nreplicated with its meta file and with\nthe same size this ensures that my data\nis getting replicated as per whatever\nfactor we have set and my file is\nwritten to htfs and now i can also do a\ncomplete listing i can just say a\nrecursive listing to show me all the\ndirectories and all the files which i\nhave in my hdfs i think we are\nforgetting something we are logged in as\nroot which is not right so we need to be\nlogged in as htc and then we can try our\ncommands that is hpfs dfs iphone ls\nslash which should show me my directory\nbut then if you would want to do a\nrecursive listing of showing all the\ndirectories and files you can just add\nhyphen r so we have written a file to\nhdfs in my data directory similarly i\ncan write using a put command last time\nwe used a copy from local i will use a\nput command i will say let's take this\nlog file and let us write it to the my\ndata directory instead of copy from\nlocal i'm using a put command which also\nallows me to write the file to my htfs\nonce this is done let's go to the web ui\nand let's go to browse the file system\nlet's look in our directory and that\nshows me that i have a log file which is\neven smaller than the first file which\nwe wrote it is in kilobytes however it\nis replicated it is using still the\nblock size and we can always go back and\nlook at the terminal to see if for my\nnew file i have a new block which should\nbe in my data path and if you see this\nthis is my block which will also have an\napplication on machine 2. so this is how\nyou write one file or you could write\nmultiple files you could write multiple\nfiles let's create some files here so i\nhave these log files let's say hadoop\nlog and let's give it a name like this\nso now if you see i just created\nmultiple files and i can also say sdfs\ndfs\nmake directory let's say my data tool so\ni'm creating a new directory on my sdfs\nand then i can even copy multiple files\nor a complete directory so all i need to\ndo here is give a matching pattern or a\ndirectory so i'll say sdfs dfs put and\nthen i can say take all the directories\nof files and put it to my data too\nremember this one file or multiple files\nare following the replication what has\nbeen set at a cluster level now if we do\na recursive listing where i would want\nto see all the directories and the files\nwhich are within my hdfs i see that i\nhave a my data directory and i also have\na mydata2 directory which has multiple\nfiles and all of the files within the\ndirectory are following the replication\nset at a cluster level so all of them\nare having their blocks replicated twice\nwe can always go back here we can just\ndo a back and that shows me what i have\nin my data too everything is replicated\ntwice what i have in my data is also\nreplicated twice now sdfs also has a\nfunctionality where you can write data\nwith a different replication so let's\ncreate one more directory and now let's\nwrite data with a different replication\nso the way i write my files i follow the\nsame thing however now i will say hyphen\ndfs dot replication and i will specify\nit as one so i am passing in as argument\nand i am specifying that i would want to\nwrite data with a replication of just\none so my blocks will not be considered\nunder replicated and i am choosing to\nwrite these files in this directory with\nan application of one we can check from\nour browser interface by just doing a\nquick refresh and here if i click on my\ndata three it shows it follows a\nreplication of one there is also a\ncommand to check your replication status\nusing hdfs so we can say stfs fsck slash\nfiles slash\nblocks and if i would want to see only\nblocks or only files i could choose one\nof them now here i have given a complete\nsdfs file system i can replace that with\nslash my data so i can say my data three\nthis is what i would want to see and\nthis will show me within this directory\nhow many files i have how many blocks\ndoes each file use and is it replicated\ncorrectly so my fsck command gives me an\ninformation that in my data 3 i have one\nfile which is\n24 kilobytes uses one block because it's\nvery smaller than 128 and it is\nreplicated only once so there is no\nsituation of over under misreplicated\nblocks we have our files correctly\nreplicated as they were specified while\nwriting same thing can be seen for other\nfiles say mydata2 and that will tell me\nthat all the files within mydata2 were\nreplicated as per the default\nreplication which was at a cluster level\nand if you see here everything is\nreplicated twice we don't have any over\nunder misreplicated blocks and my file\nsystem status is healthy so this is the\nsdfs command to make sure that your\nfiles are replicated correctly so we\nhave seen how your files are written\nwith default replication we have seen\nhow your files follow a dynamic\nreplication what you specify on the\ncommand line now we can also see a\ncommand which allows us to change\nreplication after the data has been\nwritten so i can say set rep and i can\nsay path that is my data 3 where i\ninitially had all my file related blocks\nreplicated only once and now i'll say\nmydata3 and i can specify that this\nshould follow a replication of 2 which\nbasically means that it will look into\nthe directory it will check all the\nfiles which had only one block and now\nit will create a replica of those blocks\nacross other data nodes we can always go\nback and look into the data node logs to\nsee the replication happening we can\nalso look from the browser interface\nwhich initially was showing an\napplication of one and now it shows a\nreplication of two so this is the\ncommand to change the replication after\nthe data has been written so our rights\ncan have default replication we can\nwrite files with a different replication\nwe can change the replication after the\ndata is written not only that we can\neven get the data from our hdfs to my\nlocal machine and for that i'll give\nhdfs get command or copy to local so i\ncan say my data 3 and then i can bring\nit to my local machine in home hdc using\na get command so this is to get data\nfrom your sdfs to your local machine\nusing a hdfs command now if you see i\nshould have a mydata3 directory which is\nin my local path and we can look in\nmydata3 it has all the files which we\nhad on hdfs so we were not only putting\ndata from our local machine to sdfs we\nalso successfully brought data from our\nhdfs to local machine we have written\ndata with different replication we have\nchecked the status of nodes we have\nchecked the status of blocks and if we\nwould want to delete some data sdfs also\nhas options for that so i can say hdfs\ndfs remove and i could also do a\nrecursive remove to remove everything\nincluding the directory from mydata3 so\nthis will delete data from my hdfs\nhowever if thrash is enabled now thrash\nbasically means recycle bin which\ndepends on a property in the core hyphen\nsite file we have not enabled it if\nthrash is enabled then your data will be\ndeleted from hdfs however it will not be\npermanently deleted it will sit in\nthrash for a time interval what admin\nhas specified in the configuration file\nso right now i deleted thrash and this\nseems to be gone from sdfs because we\nhave not set up any thresh however if a\nthrash is set up then either you can\ndecide to have your data deleted and\nstill be in thrash so that it could be\nrestored or you could delete it\npermanently using a skip thrash option\nhowever remember this skip thrash option\nmakes more sense if the hash is enabled\nin your cluster and you do not want your\ndeleted data to be string in thrash even\nfor whatever time interval has been set\nbecause even if the data sits in thrash\nand your thrash is a directory structure\non hdfs all the data will still be\nreplicated so we have seen how you can\nwrite data to the cluster how you can\nget data from cluster to your local\nmachine how you can write it with\ndifferent replications how you can\ndelete it from your cluster and do a\nrecursive listing of your directories\nand files now there are various other\ncommands which can be used for example\nsomething like copy or move or change\npermissions and many more\nthanks ajay now we have richard and ajay\ntaking us through the hadoop ecosystem\nwe will look into hdfs mapreduce yarn\nscoop hive pig and hbase suppose you\nhave a library that has a collection of\nhuge number of books on each floor and\nyou want to count the total number of\nbooks present on each floor what would\nbe your approach you could say i will do\nit myself but then don't you think that\nwill take a lot of time and that's\nobviously not an efficient way of\ncounting the number of books in this\nhuge collection on every floor by\nyourself now there could be a different\napproach or an alternative to that you\ncould think of asking three of your\nfriends or three of your colleagues and\nyou could then say if each friend could\ncount the books on every floor then\nobviously that would make your work\nfaster and easier to count the books on\nevery floor now this is what we mean by\nparallel processing so when you say\nparallel processing in technical terms\nyou're talking about using multiple\nmachines and each machine would be\ncontributing its ram and cpu cores for\nprocessing and your data would be\nprocessed on multiple machines at the\nsame time now this type of process\ninvolves parallel processing in our case\nor in our library example where you\nwould have person one who would be\ntaking care of books on floor 1 and\ncounting them person 2 on floor 2 then\nyou have someone on floor 3 and someone\non floor 4. so every individual would be\ncounting the books on every floor in\nparallel so that reduces the time\nconsumed for this activity and then\nthere should be some mechanism where all\nthese counts from every floor can be\naggregated so what is each person doing\nhere each person is mapping the data of\na particular floor or you can say each\nperson is doing a kind of activity or\nbasically a task on every floor and the\ntask is counting the books on every\nfloor now then you could have some\naggregation mechanism that could\nbasically reduce or summarize this total\ncount and in terms of map reduce we\nwould say that's the work of reducer so\nwhen you talk about hadoop map reduce it\nprocesses data on different node\nmachines now this is the whole concept\nof hadoop framework right that you not\nonly have your data stored across\nmachines but you would also want to\nprocess the data locally so instead of\ntransferring the data from one machine\nto other machine or bringing all the\ndata together into some central\nprocessing unit and then processing it\nyou would rather have the data processed\non the machines wherever that is stored\nso we know in case of hadoop cluster we\nwould have our data stored on multiple\ndata nodes on their multiple disks and\nthat is the data which needs to be\nprocessed but the requirement is that we\nwant to process this data as fast as\npossible and that could be achieved by\nusing parallel processing now in case of\nmapreduce we basically have the first\nphase which is your mapping phase so in\ncase of mapreduce programming model you\nbasically have two phases one is mapping\nand one is reducing now who takes care\nof things in mapping phase it is a\nmapper class and this mapper class has\nthe function which is provided by the\ndeveloper which takes care of these\nindividual map tasks which will work on\nmultiple nodes in parallel your reducer\nclass belongs to the reducing phase so a\nreducing phase basically uses a reducer\nclass which provides a function that\nwill aggregate and reduce the output of\ndifferent data nodes to generate the\nfinal output now that's how your\nmapreduce works using mapping and then\nobviously reducing now you could have\nsome other kind of jobs which are map\nonly jobs wherein there is no reducing\nrequired but we are not talking about\nthose we are talking about our\nrequirement where we would want to\nprocess the data using mapping and\nreducing especially when data is huge\nwhen data is stored across multiple\nmachines and you would want to process\nthe data in parallel so when you talk\nabout mapreduce you could say it's a\nprogramming model you could say\ninternally it's a processing engine of\nhadoop that allows you to process and\ncompute huge volumes of data and when we\nsay huge volumes of data we can talk\nabout terabytes we can talk about\npetabytes exabytes and that amount of\ndata which needs to be processed on a\nhuge cluster we could also use mapreduce\nprogramming model and run a mapreduce\nalgorithm in a local mode but what does\nthat mean if you would go for a local\nmode it basically means it would do all\nthe mapping and reducing on the same\nnode using the processing capacity that\nis ram and cpu cores on the same machine\nwhich is not really efficient in fact we\nwould want to have our map reduce work\non multiple nodes which would obviously\nhave mapping phase followed by a\nreducing phase and intermittently there\nwould be data generated there would be\ndifferent other phases which help this\nwhole processing so when you talk about\nhadoop map reduce you are mainly talking\nabout two main components or two main\nphases that is mapping and reducing\nmapping taking care of map tasks\nreducing taking care of reduced tasks so\nyou would have your data which would be\nstored on multiple machines now when we\ntalk about data data could be in\ndifferent formats we could or the\ndeveloper could specify what is the\nformat which needs to be used to\nunderstand the data which is coming in\nthat data then goes through the mapping\ninternally there would be some shuffling\nsorting and then reducing which gives\nyou your final output so the way we\naccess data from sdfs or the way our\ndata is getting stored on sdfs we have\nour input data which would have one or\nmultiple files in one or multiple\ndirectories and your final output is\nalso stored on sdfs to be accessed to be\nlooked into and to see if the processing\nwas done correctly so this is how it\nlooks so you have the input data which\nwould then be worked upon by multiple\nmap tasks now how many map tasks that\nbasically depends on the file that\ndepends on the input format so normally\nwe know that in a hadoop cluster you\nwould have a file which is broken down\ninto blocks depending on its size so the\ndefault block size is 128 mb which can\nthen still be customized based on your\naverage size of data which is getting\nstored on the cluster so if i have\nreally huge files which are getting\nstored on the cluster i would certainly\nset a higher block size so that my every\nfile does not have huge number of blocks\ncreating a load on name notes ram\nbecause that's tracking the number of\nelements in your cluster or number of\nobjects in your cluster so depending on\nyour file size your file would be split\ninto multiple chunks and for every chunk\nwe would have a map task running now\nwhat is this map task doing that is\nspecified within the mapper class so\nwithin the mapper class you have the\nmapper function which basically says\nwhat each of these map tasks has to do\non each of the chunks which has to be\nprocessed this data intermittently is\nwritten to sdfs where it is sorted and\nshuffled and then you have internal\nphases such as partitioner which decides\nhow many reduced tasks would be used or\nwhat data goes to which reducer you\ncould also have a combiner phase which\nis like a mini reducer doing the same\nreduce operation before it reaches\nreduce then you have your reducing phase\nwhich is taken care by a reducer class\nand internally the reducer function\nprovided by developer which would have\nreduced task running on the data which\ncomes as an output from map tasks\nfinally your output is then generated\nwhich is stored on sdfs now in case of\nhadoop it accepts data in different\nformats your data could be in compressed\nformat your data could be in part k your\ndata could be in afro text csv psv\nbinary format and all of these formats\nare supported however remember if you\nare talking about data being compressed\nthen you have to also look into what\nkind of splitability the compression\nmechanism supports otherwise when\nmapreduce processing happens it would\ntake the complete file as one chunk to\nbe processed so sdfs accepts input data\nin different formats this data is stored\nin sdfs and that is basically our input\nwhich is then passing through the\nmapping phase now what is mapping phase\ndoing as i said it reads record by\nrecord depending on the input format it\nreads the data so we have multiple map\ntasks running on multiple chunks once\nthis data is being read this is broken\ndown into individual elements and when i\nsay individual element i could say this\nis my list of key value pairs so your\nrecords based on some kind of delimiter\nor without delimiter are broken down\ninto individual elements and thus your\nmac creates key value pairs now these\nkey value pairs are not my final output\nthese key value pairs are basically a\nlist of elements which will then be\nsubjected to further processing so you\nwould have internally shuffling and\nsorting of data so that all the relevant\nkey value pairs are brought together\nwhich basically benefits the processing\nand then you have your reducing which\naggregates the key value pairs into set\nof smaller tuples or tuples as you would\nsay finally your output is getting\nstored in the\ndesignated directory as a list of\naggregated key value pairs which gives\nyou your output so when we talk about\nmapreduce one of the key factors here is\nthe parallel processing which it can\noffer so we know that we our data is\ngetting stored across multiple data\nnodes and you would have huge volume of\ndata which is split and randomly\ndistributed across data nodes and this\nis the data which needs to be processed\nand the best way would be parallel\nprocessing so you could have your data\ngetting stored on multiple data nodes or\nmultiple slave nodes and each slave node\nwould have again one or multiple disks\nto process this data basically we have\nto go for parallel processing approach\nwe have to use the mapreduce now let's\nlook at the mapreduce workflow to\nunderstand how it works so basically you\nhave your input data stored on sdfs now\nthis is the data which needs to be\nprocessed it is stored in input files\nand the processing which you want can be\ndone on one single file or it can be\ndone on a directory which has multiple\nfiles you could also later have multiple\noutputs merged which we achieve by using\nsomething called as chaining of mappers\nso here you have your data getting\nstored on sdfs now input format is\nbasically something to define the input\nspecification and how the input files\nwill be split so there are various input\nformats now we can search for that so we\ncan go to google and we can basically\nsearch for\nhadoop map reduce\nyahoo tutorial this is one of the good\nlinks and if i look into this link i can\nsearch for different input formats and\noutput formats so let's search for input\nformat so when we talk about input\nformat you basically have something to\ndefine how input files are split so\ninput files are split up and read based\non what input format is specified so\nthis is a class that provides following\nfunctionality it selects the files or\nother objects that should be used for\ninput it defines the input split that\nbreak a file into tasks provides a\nfactory for record reader objects that\nread the file so there are different\nformats if you look in the table here\nand you can see that the text input\nformat is the default format which reads\nlines of a text file and each line is\nconsidered as a record here the key is\nthe byte offset of the line and the\nvalue is the line content it says you\ncan have key value input format which\npasses lines into key value pairs\neverything up to the first tab character\nis the key and the remainder is the line\nyou could also have sequence file input\nformat which basically works on binary\nformat so you have input format and in\nthe same way you can also search for\noutput format which takes care of how\nthe data is handled after the processing\nis done so the key value pairs provided\nto this output collector are then\nwritten to output files the way they are\nwritten is governed by output format so\nit functions pretty much like input\nformat as described in earlier right so\nwe could set what is the output format\nto be followed and again you have text\noutput sequence file output format null\noutput format and so on so these are\ndifferent classes which take care of how\nyour data is handled when it is being\nread for processing or how is the data\nbeing written when the processing is\ndone so based on the input format the\nfile is broken down into splits and this\nlogically represents the data to be\nprocessed by individual map tasks or you\ncould say individual mapper functions so\nyou could have one or multiple splits\nwhich need to be processed depending on\nthe file size depending on what\nproperties have been set now once this\nis done you have your input splits which\nare subjected to mapping phase\ninternally you have a record reader\nwhich communicates with the input split\nand converts the data into key value\npairs suitable to be read by mapper and\nwhat is mapper doing it is basically\nworking on these key value pairs the map\ntask giving you an intermittent output\nwhich would then be forwarded for\nfurther processing now once that is done\nand we have these key value pairs which\nis being worked upon my map your map\ntasks as a part of your mapper function\nare generating your key value pairs\nwhich are your intermediate outputs to\nbe processed further now you could have\nas i said a combiner face or internally\na mini radio surface now combiner does\nnot have its own class so combiner\nbasically uses the same class as the\nreducer class provided by the developer\nand its main work is to do the reducing\nor its main work is to do some kind of\nmini aggregation on the key value pairs\nwhich were generated by map so once the\ndata is coming in from the combiner then\nwe have internally a partitioner phase\nwhich decides how outputs from combiners\nare sent to the reducers or you could\nalso say that even if i did not have a\ncombiner partitioner would decide based\non the keys and values based on the type\nof keys how many reducers would be\nrequired or how many reduced tasks would\nbe required to work on your output which\nwas generated by map task now once\npartitioner has decided that then your\ndata would be then sorted and shuffled\nwhich is then fed into the reducer so\nwhen you talk about your reducer it\nwould basically have one or multiple\nreduced tasks now that depends on what\nor what partitioner decided or\ndetermined for your data to be processed\nit can also depend on the configuration\nproperties which have been set to decide\nhow many radio stars should be used now\ninternally all this data is obviously\ngoing through sorting and shuffling so\nthat your reducing or aggregation\nbecomes an easier task once we have this\ndone we basically have the reducer which\nis the code for the reducer is provided\nby the developer and all the\nintermediate data has then to be\naggregated to give you a final output\nwhich would then be stored on sdfs and\nwho does this you have an internal\nrecord writer which writes these output\nkey value pairs from reducer to the\noutput files now this is how your\nmapreduce works wherein the final output\ndata can be not only stored but then\nread or accessed from sdfs or even used\nas an input for further mapreduce kind\nof processing so this is how it overall\nlooks so you basically have your data\nstored on sdfs based on input format you\nhave the splits then you have record\nreader which gives your data to the\nmapping phase which is then taken care\nby your mapper function and mapper\nfunction basically means one or multiple\nmap tasks working on your chunks of data\nyou could have a combiner phase which is\noptional which is not mandatory then you\nhave a partitioner phase which decides\non how many reduced tasks or how many\nreducers would be used to work on your\ndata internally there is sorting and\nshuffling of data happening and then\nbasically based on your output format\nyour record reader will write the output\nto sdfs directory now internally you\ncould also remember that data is being\nprocessed locally so you would have the\noutput of each task which is being\nworked upon stored locally however we do\nnot access the data directly from data\nnodes we access it from sdfs so our\noutput is stored on sdfs so that is your\nmapreduce workflow when you talk about\nmapreduce architecture now this is how\nit would look so you would have\nbasically a edge node or a client\nprogram or an api which intends to\nprocess some data so it submits the job\nto the job tracker or you can say\nresource manager in case of hadoop yarn\nframework right now before this step we\ncan also say that an interaction with\nname node would have already happened\nwhich would have given information of\ndata nodes which have the relevant data\nstored then your master processor so in\nhadoop version 1 we had job tracker and\nthen the slaves were called task\ntrackers in hadoop version 2 instead of\njob tracker you have resource manager\nanswer of task trackers you have node\nmanagers so basically your resource\nmanager has to assign the job to the\ntask trackers or node managers so your\nnode managers as we discussed in yarn\nare basically taking care of processing\nwhich happens on every node so\ninternally there is all of this work\nhappening by resource manager node\nmanagers and application master then you\ncan refer to the yarn based tutorial to\nunderstand more about that so here your\nprocessing master is basically breaking\ndown the application into tasks what it\ndoes internally is once your application\nis submitted your application to be run\non yarn processing framework is handled\nby resource manager now forget about the\nyarn part as of now i mean who does the\nnegotiating of resources who allocates\nthem how does the processing happen on\nthe nodes right so that's all to do with\nhow yarn handles the processing request\nso you have your data which is stored in\nsdfs broken down into one or multiple\nsplits depending on the input format\nwhich has been specified by the\ndeveloper your input splits are to be\nworked upon by your one or multiple map\ntasks which will be running within the\ncontainer on the nodes basically you\nhave the resources which are being\nutilized so for each map task you would\nhave some amount of ram which will be\nutilized and then further the same data\nwhich has to go through reducing phase\nthat is your reduced task will also be\nutilizing some ram and cpu cores now\ninternally you have these functions\nwhich take care of deciding on number of\nreducers doing a mini reduce and\nbasically reading and processing the\ndata from multiple data nodes now this\nis how your mapreduce programming model\nmakes parallel processing work or\nprocesses your data which is stored\nacross multiple machines finally you\nhave your output which is getting stored\non his dfs\nso let's have a quick demo on mapreduce\nand see how it works on a hadoop cluster\nnow we have discussed briefly about\nmapreduce which contains mainly two\nphases that is your mapping phase and\nyour reducing phase and mapping phase is\ntaken care by your mapper function and\nyour reducing phase is taken care by\nyour reducer function now in between we\nalso have sorting and shuffling and then\nyou have other phases which is\npartitioner and combiner and we will\ndiscuss about all those in detail in\nlater sessions but let's have a quick\ndemo on how we can run a mapreduce which\nis already existing as a package jar\nfile within your apache hadoop cluster\nor even in your cloudera cluster now we\ncan build our own map reduce programs we\ncan package them as jar transfer them to\nthe cluster and then run it on a hadoop\ncluster on yarn or we could be using\nalready provided default program so\nlet's see where they are now these are\nmy two machines which i have brought up\nand basically this would have my apache\nhadoop cluster running now we can just\ndo a simple start hyphen all dot sh now\ni know that this script is deprecated\nand it says instead use start dfs and\nstart yarn but then it will still take\ncare of starting off my cluster on these\ntwo nodes where i would have one single\nname node two data nodes one secondary\nname node one resource manager and two\nnode managers now if you have any doubt\nin how this cluster came up you can\nalways look at the previous sessions\nwhere we had a walkthrough in setting up\na cluster on apache and then you could\nalso have your cluster running using\nless than 3gb of your total machine ram\nand you could have apache cluster\nrunning on your machine now once this\ncluster comes up we will also have a\nlook at the web ui which is available\nfor name node and resource manager now\nbased on the settings what we have given\nour uis will show us details of our\ncluster but remember the ui is only to\nbrowse now here my cluster has come up i\ncan just do a jps to look at java\nrelated processes and that will show me\nwhat are the processes which are running\non c1 which is your data node resource\nmanager node manager and name node and\non my m1 machine which is my second\nmachine which i have configured here i\ncan always do a jps and that shows me\nthe processes running which also means\nthat my cluster is up with two data\nnodes with two node managers and here i\ncan have a look at my\nweb ui so i can just do a refresh and\nthe same thing with this one just to\nrefresh so i had already opened the web\npages so you can always access the web\nui using your name notes host name and\n570 port it tells me what is my cluster\nid what is my block pull id it gives you\ninformation of what is the space usage\nhow many live nodes you have and you can\neven browse your file system so i have\nput in a lot of data here i can click on\nbrowse the file system and this\nbasically shows me multiple directories\nand these directories have one or\nmultiple files which we will use for our\nmapreduce example now if you see here\nthese are my directories which have some\nsample files although these files are\nvery small like 8.7 kilobytes if you\nlook into this directory if you look\ninto this i have just pulled in some of\nmy hadoop logs and i have put it on my\nsdfs these are a little bigger files and\nthen we also have some other data which\nwe can see here and this is data which\ni've downloaded from web now we can\neither run a mapreduce on a single file\nor in a directory which contains\nmultiple files let's look at that before\nlooking at demo and mapreduce also\nremember mapreduce will create a output\ndirectory and we need to have that\ndirectory created plus we need to have\nthe permissions to run the mapreduce job\nso by default since i'm running it using\nadmin id i should not have any problem\nbut then if you intend to run mapreduce\nwith a different user then obviously you\nwill have to ask the admin or you will\nhave to give the user permission to read\nand write from sdfs so this is the\ndirectory which i have created which\nwill contain my output once the\nmapreduce job finishes and this is my\ncluster file system if you look on this\nui this shows me about my yarn which is\navailable for taking care of any\nprocessing it as of now shows that i\nhave total of 8 gb memory and i have 8 v\ncores now that can be depending on what\nconfiguration we have set or how many\nnodes are available we can look at nodes\nwhich are available and that shows me i\nhave two node managers running each has\n8 gb memory and 8 v cores now that's not\ntrue actually but then we have not set\nthe configurations for node managers and\nthat's why it takes the default\nproperties that is 8gb laminate vcos now\nthis is my yarn ui we can also look at\nscheduler which basically shows me the\ndifferent queues if they have been\nconfigured where you will have to run\nthe jobs we'll discuss about all these\nin later in detail now let's go back to\nour terminal and let's see where we can\nfind some sample applications which we\ncan run on the cluster so once i go to\nthe terminal i can well submit the\nmapreduce job from any terminal now here\ni know that my hadoop related directory\nis here and within hadoop you have\nvarious directories we have discussed\nthat in binaries you have the commands\nwhich you can run in s bin you basically\nhave the startup scripts and here you\nalso notice there is a share directory\nin the end if you look in the shared\ndirectory you would find hadoop and\nwithin hadoop you have various sub\ndirectories in which we will look for\nmapreduce now this mapreduce directory\nhas some sample jar files which we can\nuse to run a mapreduce on the cluster\nsimilarly if you are working on a\ncloudera cluster you would have to go\ninto opt cloudera parcel cdh slash lib\nand in that you would have directories\nfor sdfs mapreduce or sdfs yarn where\nyou can still find the same jars it is\nbasically a package which contains your\nmultiple applications now how do we run\na mapreduce we can just type in hadoop\nand hit enter and that shows me that i\nhave an option called jar which can be\nused to run a jar file now at any point\nof time if you would want to see what\nare the different classes which are\navailable in a particular\njar you could always do a jar minus xvf\nfor example i could say jar xv f and i\ncould say user local hadoop\nshare hadoop mapreduce and then list\ndown your jar file so i'll say hadoop\nmapreduce examples and if i do this this\nshould basically unpack it to show me\nwhat classes are available within this\nparticular jar and it has done this it\nhas created a meta file and it has\ncreated a org directory we can see that\nby doing a ls and here if you look in ls\norg since i ran the command from your\nphone directory i can look into org\npatchy hadoop examples which shows me\nthe classes which i have and those\nclasses contain which mapper or reducer\nclasses so it might not be just mapper\nand reducer but you can always have a\nlook so for example i am targeting to\nuse word count program which does a word\ncount on files and gives me a list of\nwords and how many times they occur in a\nparticular file or in set of files and\nthis shows me that what are the classes\nwhich belong to word count so we have a\nin sum reducer so this is my reducer\nclass i have tokenizer mapper that is my\nmapper class right and basically this is\nwhat is used these classes are used if\nyou run a word code now there are many\nother programs which are part of this\njar file and we can expand and see that\nso i can say hadoop jar and give your\npath so i'll say hadoop jar user local\nhadoop share hadoop map reduce hadoop\nmapreduce examples and if i hit on enter\nthat will show me what are the inbuilt\nclasses which are already available now\nthese are certain things which we can\nuse now there are other jar files also\nfor example i can look at hadoop and\nhere\nwe can look at the jar files which we\nhave in this particular path so this is\none hadoop mapreduce examples which you\ncan use you can always look in other jar\nfiles like you can look for\nhadoop mapreduce client job client and\nthen you can look at the test one so\nthat is also an interesting one so you\ncan always look into hadoop mapreduce\nclient job client and then you have\nsomething ending with this so if i would\nhave tried this one using my hadoop jar\ncommand so in my previous example when\nwe did this it was showing me all the\nclasses which are available and that\nalready has a word count now there are\nother good programs which you can try\nlike teragen to generate dummy data\nterasort to check your sorting\nperformance and so on and terra validate\nto validate the results similarly we can\nalso do a hadoop jar as i said on hadoop\nmapreduce i think that was client and\nthen we have\njob client and then test star now this\nhas a lot of other classes which can be\nused or programs which can be used for\ndoing a stress testing or checking your\ncluster status and so on one of them\ninteresting one is test dfs io but let's\nnot get into all the details in first\ninstance let's see how we can run a\nmapreduce now if i would want to run a\nmapreduce i need to give hadoop jar and\nthen my jar file and if i hit on enter\nit would say it needs the input and\noutput it needs which class you want to\nrun so for example i would say word\ncount and again if i hit on enter it\ntells me that you need to give me some\ninput and output to process and\nobviously this processing will be\nhappening on cluster that is our yarn\nprocessing framework unless and until\nyou would want to run this job in a\nlocal mode so there is a possibility\nthat you can run the job in a local mode\nbut let's first try how it runs on the\ncluster so how do we do that now here i\ncan do a hdfs\nls slash command to see what i have on\nmy sdfs now through my ui i was already\nshowing you that we have set of files\nand directories which we can use to\nprocess now we can take up one single\nfile so for example if i pick up new\ndata and i can look into the files here\nwhat we have and we can basically run a\nmapreduce on a single file or multiple\nfiles so let's take this file whatever\nthat contains and i would like to do a\nword count so that i get a list of words\nand their occurrence in this file so let\nme just copy this now i also need my\noutput to be written and that will be\nwritten here so here if i want to run a\nmapreduce i can say hadoop which we can\npull out from history so hadoop jar word\ncount now i need to give my input so\nthat will be new data and then i will\ngive this file which we just copied now\ni am going to run the word count only on\na single file and i will basically have\nmy output which will be stored in this\ndirectory the directory which i have\ncreated already mr output so let's do\nthis\noutput and this is fair enough now you\ncan give many other properties you can\nspecify how many map jobs you want to\nrun how many reduced jobs you want to\nrun do you want your output to be\ncompressed do you want your output to be\nmerged or many other properties can be\ndefined when you are specifying word\ncount and then you can pass in an\nargument to pass properties from the\ncommand line which will affect your\noutput now once i go ahead and submit\nthis this is basically running a simple\ninbuilt mapreduce job on our hadoop\ncluster now obviously internally it will\nbe looking for name node now we have\nsome issue here and it says the output\nalready exists what does that mean so it\nbasically means that hadoop will create\nan output for you you just need to give\na name but then you don't need to create\nit so let's give let's append the output\nwith number one and then let's go ahead\nand run this so i've submitted this\ncommand now this can also be done in\nbackground if you would want to run\nmultiple jobs on your cluster at the\nsame time so it takes total input paths\nto processes one so that is there is\nonly one split on which your job has to\nwork now it will internally try to\ncontact your resource manager and\nbasically this is done so here we can\nhave a look and we can see some counters\nhere now what i also see is for some\nproperty which it is missing it has run\nthe job but it has run in a local mode\nit has run in a local mode so although\nwe have submitted so this might be\nrelated to my yarn settings and we can\ncheck that\nso if i do a refresh when i have run my\napplication it has completed it would\nhave created an output but the only\nthing is it did not interact with your\nyarn it did not interact with your\nresource manager we can check those\nproperties and here if we look into the\njob it basically tells me that it went\nfor mapping and reducing it would have\ncreated an output it worked on my file\nbut then it ran in a local mode it ran\nin a local mode so mapreduce remember is\na programming model right now if you run\nit on yarn you get the facilities of\nrunning it on a cluster where yarn takes\ncare of resource management if you don't\nrun it on yarn and run it on a local\nmode it will use your machines ram and\ncpu cores for processing but then we can\nquickly look at the output and then we\ncan also try running this on yarn so if\ni look into my hdfs and if i look into\nmy output mr output that's the directory\nwhich was not used actually let's look\ninto the other directory which is ending\nwith one and that should show me the\noutput created by this mapreduce\nalthough it ran in the local mode it\nfetched an input file from usdfs and it\nwould have created output in hdfs now\nthat's my part file which is created and\nif you look at part minus r minus these\nzeros if you would have more than one\nreducer running then you would have\nmultiple such files created we can look\ninto this what does this file contain\nwhich should have my word count and here\ni can say cat which basically shows me\nwhat is the output created by my\nmapreduce let's have a look into this so\nthe file which we gave for processing\nhas been broken down and now we have the\nlist of words which occur in the file\nplus a count of those words so if there\nis some word which is in is more then it\nshows me the count so this is a list of\nmy words and the count for that so this\nis how we run a sample mapreduce job i\nwill also show you how we can run it on\nyeah now let's run mapreduce on yarn and\ninitially when we tried running a\nmapreduce it did not hit yarn but it ran\nin a local mode and that was because\nthere was a property which had to be\nchanged in mapreduce hyphen site file so\nbasically if you look into this file the\nerror was that i had given a property\nwhich says\nmapred.framework.name and that was not\nthe right property name and it was\nignored and that's why it ran in a local\nmode so i changed the property to\nmapreduce.framework.name\nrestarted my cluster and everything\nshould be fine now and that mapred\nhyphen site file has also been copied\nacross the nodes now to run a mapreduce\non a hadoop cluster so that it uses yarn\nand yarn takes care of resource\nallocation on one or multiple machines\nso i'm just changing the output here and\nnow i will submit this job which should\nfirst connect to the resource manager\nand if it connects to the resource\nmanager that means our job will be run\nusing yarn on the cluster rather than in\na local mode so now we have to wait for\nthis application to internally connect\nto resource manager and once it starts\nthere we can always go back to the web\nui and check if our application has\nreached yarn so it shows me that there\nis one input part to be processed that's\nmy job id that's my application id which\nyou can even monitor status from the\ncommand line now here the job has been\nsubmitted so let's go back here and just\nto a refresh on my yarn ui which should\nshow me the new application which is\nsubmitted it tells me that it is an\naccepted state application master has\nalready started\nand if you click on this link it will\nalso give you more details of how many\nmap and reduce tasks would run so as of\nnow it says the application master is\nrunning it would be using this node\nwhich is m1 we can always look into the\nlogs we can see that there is a one task\nattempt which is being made and now if i\ngo back to my terminal i will see that\nit is waiting to get some resources from\nthe cluster and once it gets the\nresources it will first start with the\nmapping phase where the mapper function\nruns it does the map tasks one or\nmultiple depending on the splits so\nright now we have one file and one split\nso we will have just one map task\nrunning and once the mapping phase\ncompletes then it will get into reducing\nwhich will finally give me my output so\nwe can be toggling through these\nsessions so here i can just do a refresh\nto see what is happening with my\napplication is it proceeding is it still\nwaiting for resource manager to allocate\nsome resources now just couple of\nminutes back i tested this application\non yarn and we can see that my first\napplication completed successfully and\nhere we will have to give some time so\nthat yarn can allocate the resources now\nif the resources were used by some other\napplication they will have to be freed\nup now internally yan takes care of all\nthat which we will learn more detail in\nyarn or you might have already followed\nthe yarn based session now here we will\nhave to just give it some more time and\nlet's see if my application proceeds\nwith the resources what yarn can\nallocate to it sometimes you can also\nsee a slowness in what web ui shows up\nand that can be related to the amount of\nmemory you have allocated to your nodes\nnow for apache we can have less amount\nof memory and we can still run the\ncluster and as i said the memory which\nshows up here 16 gb and 16 cores is not\nthe true one those are the default\nsettings right but then my yarn should\nbe able to facilitate running of this\napplication let's just give it a couple\nof seconds and then let's look into the\noutput here again i had to make some\nchanges in the settings because our\napplication was not getting enough\nresources and then basically i restarted\nmy cluster now let's submit the\napplication again to the cluster which\nfirst should contact the resource\nmanager and then basically the map and\nreduce process should start so here i've\nsubmitted an application it is\nconnecting to the resource manager and\nthen basically it will start internally\nan app master that is application master\nit is looking for the number of splits\nwhich is one it's getting the\napplication id and it basically then\nstarts running the job it also gives you\na tracking url to look at the output and\nnow we should go back and look at our\nyarn ui if our application shows up here\nand we will have to give it a couple of\nseconds when it can get the final status\nchange to running and that's where my\napplication will be getting resources\nnow if you closely notice here i have\nallocated specific amount of memory that\nis 1.5 gb for node manager on every node\nand i have basically given two cores\neach which my machines also have and my\nyarn should be utilizing these resources\nrather than going for default now the\napplication has started moving\nand we can see the progress bar here\nwhich basically will show what is\nhappening and if we go back to the\nterminal it will show that first it went\nin deciding map and reduce it goes for\nmap once the mapping phase completes\nthen the reducing phase will come into\nexistence and here my job has completed\nso now it has basically used we can\nalways look at how many map and reduce\nstars were run it shows me that there\nwas one map and one reduced task now\nwith the number of map tasks depends on\nthe number of splits and we had just one\nfile which is less than 128 mb so that\nwas one split to be processed and\nreduced task is internally decided by\nthe reducer or depending on what kind of\nproperty has been set in hadoop config\nfiles now it also tells me how many\ninput records were read which basically\nmeans these were the number of lines in\nthe file it tells me output records\nwhich gives me the number of total words\nin the file now there might be\nduplicates and that which is processed\nby internal combiner further processing\nor forwarding that information to\nreducer and basically reducer works on 3\n35 records gives us a list of words and\ntheir count now if i do a refresh here\nthis would obviously show my application\nis completed it says succeeded you can\nalways click on the application to look\nfor more information it tells me where\nit ran now we do not have a history\nserver running as of now otherwise we\ncan always access more information so\nthis leads to history server where all\nyour applications are stored but i can\nclick on this attempt tasks and this\nwill basically show me the history url\nor you can always look into the logs so\nthis is how you can submit a sample\napplication which is inbuilt which is\navailable in the jar on your hadoop\ncluster and that will utilize your\ncluster to run now you could always as i\nsaid when you are running a particular\njob remember to change the output\ndirectory and if you would not want it\nto be processing is a single individual\nfile you could also point it to a\ndirectory that basically means it will\nhave multiple files and depending on the\nfile sizes there would be multiple\nsplits and according to that multiple\nmap tasks will be selected so if i click\non this this would submit my second\napplication to the cluster which should\nfirst connect to resource manager then\nresource manager has to start an\napplication master now here we are\ntargeting 10 splits now you have to\nsometimes give couple of seconds in your\nmachines so that the resources which\nwere used are internally already freed\nup so that your cluster can pick it up\nand then your yarn can take care of\nresources so right now my application is\nan undefined status but then as soon as\nmy yarn provides it the resources we\nwill have the application running on our\nyarn cluster so it has already started\nif you see it is going further then it\nwould launch 10 map tasks and it would\nthe number of reduced tasks would be\ndecided on either the way your data is\nor based on the properties which have\nbeen set at your cluster level let's\njust do a quick refresh here on my yarn\nui to show me the progress also take\ncare that when you are submitting your\napplication you need to have the output\ndirectory mentioned however do not\ncreate it hadoop will create that for\nyou now this is how you run a map reduce\nwithout specifying properties but then\nyou can specify more properties you can\nlook into what are the things which can\nbe changed for your mapper and reducer\nor basically having a combiner class\nwhich can do a mini reducing and all\nthose things can be done so we will\nlearn about that in the later sessions\nnow we will compare hadoop version one\nthat is with mapreduce version one we\nwill understand and learn about the\nlimitations of hadoop version one what\nis the need of yarn what is yarn what\nkind of workloads can be running on yarn\nwhat are yarn components what is yarn\narchitecture and finally we will see a\ndemo on yarn so hadoop version one or\nmapreduce version 1. well that's\noutdated now and nobody is using hadoop\nversion 1 but it would be good to\nunderstand what was in hadoop version 1\nand what were the limitations of hadoop\nversion 1 which brought in the thought\nfor\nthe future processing layer that is yarn\nnow when we talk about hadoop we already\nknow that hadoop is a framework and\nhadoop has two layers one is your\nstorage layer that is your sdfs hadoop\ndistributed file system which allows for\ndistributed storage and processing which\nallows fault tolerance by inbuilt\nreplication and which basically allows\nyou to store huge amount of data across\nmultiple commodity machines when we talk\nabout processing we know that mapreduce\nis the oldest and the most mature\nprocessing programming model which\nbasically takes care of your data\nprocessing on your distributed file\nsystem so in hadoop version 1 mapreduce\nperformed both data processing and\nresource management and that's how it\nwas problematic in mapreduce we had\nbasically when we talk about the\nprocessing layer we had the master which\nwas called job tracker and then you had\nthe slaves which were the task records\nso your job tracker was taking care of\nallocating resources it was performing\nscheduling and even monitoring the jobs\nit basically was taking care of\nassigning map and reduced tasks to the\njobs running on task trackers and task\ntrackers which were co-located with data\nnodes were responsible for processing\nthe jobs so task trackers were the\nslaves for the processing layer which\nreported their progress to the job\ntracker so this is what was happening in\nhadoop version 1. now when we talk about\nhadoop version 1 we would have say\nclient machines or an api or an\napplication which basically submits the\njob to the master that is job tracker\nnow obviously we cannot forget that\nthere would already be an involvement\nfrom name node which basically tells\nwhich are the machines or which are the\ndata nodes where the data is already\nstored now once the job submission\nhappens to the job tracker job tracker\nbeing the master demon for taking care\nof your processing request and also\nresource management job scheduling would\nthen be interacting with your multiple\ntask trackers which would be running on\nmultiple machines so each machine would\nhave a task tracker running and that\ntask tracker which is a processing slave\nwould be co-located with the data nodes\nnow we know that in case of hadoop you\nhave the concept of moving the\nprocessing to wherever the data is\nstored rather than moving the data to\nthe processing layer so we would have\ntask trackers which would be running on\nmultiple machines and these task\ntrackers would be responsible for\nhandling the tasks what are these tasks\nthese are the application which is\nbroken down into smaller tasks which\nwould work on the data which is\nrespectively stored on that particular\nnode now these were your slave demons\nright so your job tracker was not only\ntracking the resources so your task\ntrackers were sending heartbeats they\nwere sending in packets and information\nto the job tracker which would then be\nknowing how many resources and when we\ntalk about resources we are talking\nabout the cpu cores we are talking about\nthe ram which would be available on\nevery node so task trackers would be\nsending in their resource information to\njob tracker and your job tracker would\nbe already aware of what amount of\nresources are available on a particular\nnode how loaded a particular node is\nwhat kind of work could be given to the\ntask tracker so job tracker was taking\ncare of resource management and it was\nalso breaking the application into tasks\nand doing the job scheduling part assign\ndifferent tasks to these slave demons\nthat is your task trackers so job\ntracker was eventually overburdened\nright because it was managing jobs it\nwas tracking the resources from multiple\ntask trackers and basically it was\ntaking care of job scheduling so job\ntracker would be overburdened and in a\ncase if job tracker would fail then it\nwould affect the overall processing so\nif the master is skilled if the master\ndemon dies then the processing cannot\nproceed now this was one of the\nlimitations of hadoop version one so\nwhen you talk about scalability that is\nthe capability to scale due to a single\njob tracker scalability would be hitting\na bottle link you cannot have a cluster\nsize of more than 4000 nodes and cannot\nrun more than 40 000 concurrent tasks\nnow that's just a number we could always\nlook into the individual resources which\neach machine was having and then we can\ncome up with an appropriate number\nhowever with a single job tracker there\nwas no horizontal scalability for the\nprocessing layer because we had single\nprocessing master now when we talk about\navailability job tracker as i mentioned\nwould be a single point of failure now\nany failure kills all the queued and\nrunning jobs and jobs would have to be\nresubmitted now why would we want that\nin a distributed platform in a cluster\nwhich has hundreds and thousands of\nmachines we would want a processing\nlayer which can handle huge amount of\nprocessing which could be more scalable\nwhich could be more available and could\nhandle different kind of workloads when\nit comes to resource utilization now if\nyou would have a predefined number of\nmap and reduce slots for each task\ntracker you would have issues which\nwould relate to resource utilization and\nthat again is putting a burden on the\nmaster which is tracking these resources\nwhich has to assign jobs which can run\non multiple machines in parallel so\nlimitations in running non-map reduce\napplications now that was one more\nlimitation of hadoop version 1 and\nmapreduce that the only kind of\nprocessing you could do is mapreduce and\nmap reduce programming model although it\nis good it is oldest it has matured over\na period of time but then it is very\nrigid you will have to go for mapping\nand reducing approach and that was the\nonly kind of processing which could be\ndone in hadoop person one so when it\ncomes to doing a real time analysis or\ndoing ad hoc query or doing a graph\nbased processing or massive parallel\nprocessing there were limitations\nbecause that could not be done in hadoop\nversion 1 which was having mapreduce\nversion 1 as the processing component\nnow that brings us to the need for yarn\nso yarn it stands for yet another\nresource negotiator so as i mentioned\nbefore yarn in hadoop version one well\nyou could have applications which could\nbe written in different programming\nlanguages but then the only kind of\nprocessing which was possible was\nmapreduce we had the storage layer we\nhad the processing but then kind of\nlimited processing which could be done\nnow this was one thing which brought in\na thought that why shouldn't we have a\nprocessing layer which can handle\ndifferent kind of workloads as i\nmentioned might be graph processing\nmight be real time processing might be\nmassive parallel processing or any other\nkind of processing which would be a\nrequirement of an organization now\ndesigned to run mapreduce jobs only and\nhaving issues in scalability resource\nutilization job tracking etc that led to\nthe need of something what we call as\nyarn now from hadoop version 2 onwards\nwe have the two main layers have changed\na little bit you have the storage layer\nwhich is intact that is your hdfs and\nthen you have the processing layer which\nis called yarn yet another resource\nnegotiator now we will understand how\nyarn works but then yarn is taking care\nof your processing layer it does support\nmapreduce so mapreduce processing can\nstill be done but then now you can have\na support to other processing frameworks\nyarn can be used to solve the issues\nwhich hadoop version 1 was posing\nsomething like resource management\nsomething like different kind of\nworkload processing something like\nscalability resource utilization all\nthat is now taken care by yarn now when\nwe talk about yarn we can have now a\ncluster size of more than 10 000 nodes\nand can run more than 100 000 concurrent\ntasks that's just to take care of your\nscalability when you talk about\ncompatibility applications which were\ndeveloped for hadoop version 1 which\nwere primarily mapreduce kind of\nprocessing can run on yarn without any\ndisruption or availability issues when\nyou talk about resource utilization\nthere is a mechanism which takes care of\ndynamic allocation of cluster resources\nand this basically improves the resource\nutilization when we talk about\nmulti-tenancy so basically now the\ncluster can handle different kind of\nworkloads so you can use open source and\nproprietary data access engines you can\nperform real-time analysis you can be\ndoing graph processing you can be doing\nad hoc querying and this can be\nsupported for multiple workloads which\ncan run in parallel so this is what yarn\noffers so what is yan as i mentioned\nyarn stands for yet another resource\nnegotiator so it is the cluster resource\nmanagement layer for your apache hadoop\necosystem which takes care of scheduling\njobs and assigning resources\nnow just imagine when you would want to\nrun a particular application you would\nbasically be telling the cluster that i\nwould want resources to run my\napplications that application might be a\nmapreduce application that might be a\nhive query which is triggering a\nmapreduce that might be a big script\nwhich is triggering a mapreduce that\ncould be hive with days as an execution\nengine that could be a spark application\nthat could be a graph processing\napplication in any of these cases you\nwould still you as in in sense client or\nbasically an api or the application\nwould be requesting for resources yan\nwould take care of that so yarn would\nprovide the desired resources now when\nwe talk about resources we are mainly\ntalking about the network related\nresources we are talking about the cpu\ncores or as in terms of yarn we say\nvirtual cpu cores we would talk about\nram that is in gb or mb or in terabytes\nwhich would be offered from multiple\nmachines and yarn would take care of\nthis so with yarn you could basically\nhandle different workloads now these are\nsome of the workloads which are showing\nup here you have the traditional\nmapreduce which is\nmainly batch oriented you could have an\ninteractive execution engine something\nas space you could have hbase which is a\ncolumn oriented or a four dimensional\ndatabase and that would be not only\nstoring data on sdfs but would also need\nsome kind of processing you could have\nstreaming functionalities which would be\nfrom storm or kafka or spark you could\nhave graph processing you could have\nin-memory processing such as spark and\nits components and you could have many\nothers so these are different frameworks\nwhich could now run and which can run on\ntop of er so how does the ant do that\nnow when we talk about\nyarn this is how a overall yarn\narchitecture looks so at one end you\nhave the client now client could be\nbasically your edge node where you have\nsome applications which are running it\ncould be an api which would want to\ninteract with your cluster it could be a\nuser triggered application which wants\nto run some jobs which are doing some\nprocessing so this client would submit a\njob request now what is resource manager\ndoing resource manager is the master of\nyour processing layer in hadoop version\n1 we basically had job tracker and then\nwe had task trackers which were running\non individual nodes so your task\ntrackers were sending your heart beats\nto the job tracker your task trackers\nwere sending it their resource\ninformation and job tracker was the one\nwhich was tracking the resources and it\nwas doing the job scheduling and that's\nhow as i mentioned earlier job tracker\nwas overburdened so job tracker is now\nreplaced by your resource manager which\nis the master for your processing layer\nyour task trackers are replaced by node\nmanagers which would be then running on\nevery node and we have a temporary demon\nwhich you see here in blue and that's\nyour app master so this is what we\nmentioned when we say yet another\nresource negotiator so appmaster would\nbe existing in a hadoop version 2. now\nwhen we talk about your resource manager\nresource manager is the master for\nprocessing layer so it would already be\nreceiving heartbeats and you can say\nresource information from multiple node\nmanagers which would be running on one\nor multiple machines and these node\nmanagers are not only updating their\nstatus but they are also giving an\ninformation of the amount of resources\nthey have now when we talk about\nresources we should understand that if\ni'm talking about this node manager then\nthis has been allocated some amount of\nram for processing and some amount of\ncpu cores and that is just a portion of\nwhat the complete node has so if my node\nhas say imagine my node has around 100\ngb ram and i have saved 60 cores all of\nthat cannot be allocated to node manager\nso node manager is just one of the\ncomponents of hadoop ecosystem it is the\nslave of the processing layer so we\ncould say keeping in all the aspects\nsuch as different services which are\nrunning might be cloudera or hortonworks\nrelated services running system\nprocesses running on a particular node\nsome portion of this would be assigned\nto node manager for processing so we\ncould say for example say 60 gb ram per\nnode and say 40 cpu cores so this is\nwhat is allocated for the node manager\non every machine similarly we would have\nhere similarly we would have here so\nnode manager is constantly giving an\nupdate to resource manager about the\nresources what it has probably there\nmight be some other applications running\nand node manager is already occupied so\nit gives an update now we also have a\nconcept of containers which is basically\nwe will we will talk about which is\nabout these resources being broken down\ninto smaller parts so resource manager\nis keeping a track of the resources\nwhich every node manager has and it is\nalso responsible for taking care of the\njob request how do these things happen\nnow as we see here resource manager at a\nhigher level you can always say this is\nthe processing master which does\neverything but in reality it is not the\nresource manager which is doing it but\nit has internally different services or\ncomponents which are helping it to do\nwhat it is supposed to do now let's look\nfurther now as i mentioned your resource\nmanager has these\nservices or components which basically\nhelps it to do the things it is\nbasically a an architecture where\nmultiple components are working together\nto achieve what yarn allows so resource\nmanager has mainly two components that\nis your scheduler and applications\nmanager and these are\nat high level four main components here\nso we talk about resource manager which\nis the processing master you have node\nmanagers which are the processing slaves\nwhich are running on every nodes you\nhave the concept of container and you\nhave the concept of application master\nhow do all these things work now let's\nlook at yarn components so resource\nmanager basically has two main\ncomponents you can say which assist\nresource manager in doing what it is\ncapable of so you have scheduler and\napplications manager now there is when\nyou talk about\nresources there is always a requirement\nfor the applications which need to run\non cluster of resources so your\napplication which has to run which was\nsubmitted by client needs resources and\nthese resources are coming in from\nmultiple machines wherever the relevant\ndata is stored and a node manager is\nrunning so we always know that node\nmanager is co-located with data nodes\nnow what does the scheduler do so we\nhave different kind of schedulers here\nwe have basically a capacity scheduler\nwe have a fair scheduler or we could\nhave a fifo scheduler so there are\ndifferent schedulers which take care of\nresource allocation so your scheduler is\nresponsible for allocating resources to\nvarious running applications\nnow imagine a particular environment\nwhere you have different teams or\ndifferent departments which are working\non the same cluster so we would call the\ncluster as a multi-tenant cluster and on\nthe multi-tenant cluster you would have\ndifferent applications which would want\nto run simultaneously accessing the\nresources of the cluster how is that\nmanaged so there has to be some\ncomponent which has a concept of pooling\nor queuing so that different departments\nor different users can get dedicated\nresources or can share resources on the\ncluster so scheduler is responsible for\nallocating resources to various running\napplications now it does not perform\nmonitoring or tracking of the status of\napplications that's not the part of\nscheduler it does not offer any\nguarantee about restarting the failed\ntasks due to hardware or network or any\nother failures scheduler is mainly\nresponsible for allocating resources now\nas i mentioned you could have different\nkind of schedulers you could have a fifo\nscheduler which was mainly in older\nversion of hadoop which stands for first\nin first out you could have a fair\nscheduler which basically means\nmultiple applications could be running\nin the cluster and they would have a\nfair share of the resources you could\nhave a capacity scheduler which would\nbasically have dedicated or fixed amount\nof resources across the cluster now\nwhichever scheduler is being used\nscheduler is mainly responsible for\nallocating resources then it's your\napplications manager now this is\nresponsible for accepting job\nsubmissions now as i said at higher\nlevel we could always say resource\nmanager state doing everything it is\nallocating the resources it is\nnegotiating the resources it is also\ntaking care of listening to the clients\nand taking care of job submissions but\nwho is doing it in real it is these\ncomponents so you have applications\nmanager which is responsible for\naccepting job submissions it negotiates\nthe first container for executing the\napplication specific application master\nit provides the service for restarting\nthe application master now how does this\nwork how do these things happen in\ncoordination now as i said your node\nmanager is the slave process which would\nbe running on every machine slave is\ntracking the resources what it has it is\ntracking the processes it is taking care\nof running the jobs and basically it is\ntracking each container resource\nutilization so let's understand what is\nthis container so normally when you talk\nabout a application request which comes\nfrom a client so let's say this is my\nclient which is requesting or which is\ncoming up with an application which\nneeds to run on the cluster now this\napplication could be anything it first\ncontacts your master that's your\nresource manager which is the master for\nyour processing layer now as i mentioned\nand as we already know that your name\nnode which is the master of your cluster\nhas the metadata in its ram which is\naware of the data being split into\nblocks the blocks when stored on\nmultiple machines and other information\nso obviously there was a interaction\nwith the master which has given this\ninformation of the relevant nodes where\nthe data exists now for the processing\nneed your client basically the\napplication which needs to run on the\ncluster so your resource manager which\nbasically has the scheduler which takes\ncare of allocating resources and\nresource manager has mainly these two\ncomponents which are helping it to do\nits work now for a particular\napplication which might be needing data\nfrom multiple machines now we know that\nwe would have multiple machines where we\nwould have node manager running we would\nhave a data node running and data nodes\nare responsible for storing the data on\ndisk so your resource manager has to\nnegotiate the resources now when i say\nnegotiating the resources it could\nbasically ask each of these node\nmanagers for some amount of resources\nfor example it would be saying can i\nhave one gb of ram and one cpu core from\nyou because there is some data residing\non your machine and that needs to be\nprocessed as part of my application can\ni again have one gb and one cpu core\nfrom you and this is again because some\nrelevant data is stored and this request\nwhich resource manager makes of holding\nthe resources of total resources which\nthe node manager has your resource\nmanager is negotiating or is asking for\nresources from the processing slave so\nthis request of holding resources can be\nconsidered as a container so resource\nmanager now we know it is not actually\nthe resource manager but it is the\napplication manager which is negotiating\nthe resources so it negotiates the\nresources which\nare called container so this request of\nholding resource can be considered as a\ncontainer so basically a container can\nbe of different sizes we will talk about\nthat so resource manager negotiates the\nresources with node manager now node\nmanager which is already giving an\nupdate of the resources it has what\namount of resources it holds how much\nbusy it is can basically approve or deny\nthis request so node manager would\nbasically approve in saying yes i could\nhold these resources i could give you\nthis container of this particular size\nnow once the container has been approved\nor allocated or you can say granted by\nyour node manager resource manager now\nknows that resources to process the\napplication are available and guaranteed\nby the node manager so resource manager\nstarts a temporary demon called\nappmaster so this is a piece of code\nwhich would also be running in one of\nthe containers it would be running in\none of the containers which would then\ntake care of execution of tasks in other\ncontainers so your application master is\nper application so if i would have 10\ndifferent applications coming in from\nthe client then we would have 10 app\nmasters one app master being responsible\nfor per application now what does this\napp master do it basically is a piece of\ncode which is responsible for execution\nof the application so your app master\nwould run in one of the containers and\nthen it would use the other containers\nwhich node manager is guaranteed that it\nwill give when the request application\nrequest comes to it and using these\ncontainers the app master will run the\nprocessing tasks within these designated\nresources so it is mainly the\nresponsibility of application master to\nget the execution done and then\ncommunicate it to the master so resource\nmanager is tracking the resources it is\nnegotiating the resources and once the\nresources have been negotiated it\nbasically gives the control to\napplication master application master is\nthen running within one of the\ncontainers on one of the nodes and using\nthe other containers to take care of\nexecution this is how it looks so\nbasically container as i said is a\ncollection of resources like cpu memory\nyour disk which would be used or which\nalready has the data and network so your\nnode manager is basically looking into\nthe request from application master and\nit basically is granting this request or\nbasically is allocating these containers\nnow again we could have different sizing\nof the containers let's take an example\nhere so as i mentioned from the total\nresources which are available for a\nparticular node some portion of\nresources are allocated to the node\nmanager so let's imagine this is my node\nwhere node manager as a processing slave\nis running so from the total resources\nwhich the node has some portion of ram\nand cpu cores is basically allocated to\nthe node manager so i could say out of\ntotal 100 gb ram we can say around\n60 cores which the particular node has\nso this is my ram which the node has and\nthese are the cpu cores which the node\nhas some portion of it right so we can\nsay might be\n70 percent or 60 percent of the total\nresources so we could say around 60 gb\nram and then we could say around 40 v\ncores have been allocated to node\nmanager so there are these settings\nwhich are given in the yarn hyphen site\nfile now apart from this allocation that\nis 60 gb ram and 40v cores we also have\nsome properties which say what will be\nthe container sizes so for example we\ncould have a small container setting\nwhich could say my every container could\nhave 2gb ram and say one virtual cpu\ncore so this is my smallest container so\nbased on the total resources you could\ncalculate how many such small containers\ncould be running so if i say 2gb ram\nthen i could have around 30 containers\nbut then i am talking about one virtual\ncpu core so totally i could have around\n30\nsmall containers which could be running\nin parallel on a particular node and as\nof that calculation you would say 10 cpu\ncores are not being utilized you could\nhave a bigger container size which could\nsay i would go for 2 cpu cores and 3gb\nram\nso 3gb ram and 2 cpu cores so that would\ngive me around 20 containers of bigger\nsize so this is the container sizing\nwhich is again defined in the yarn\nhyphen site file so what we know is on a\nparticular node which has this kind of\nallocation either we could have 30 small\ncontainers running or we could have 20\nbig containers running and same would\napply to multiple nodes so node manager\nbased on the request from application\nmaster can allocate these containers now\nremember it is within this one of these\ncontainers you would have an application\nmaster running and other containers\ncould be used for your processing\nrequirement application master which is\nper application it is the one which uses\nthese resources it basically manages or\nuses these resources for individual\napplication so remember if we have 10\napplications running on yarn then it\nwould be 10 application masters one\nresponsible for each application your\napplication master is the one which also\ninteracts with the scheduler to\nbasically know how much amount of\nresources could be allocated for one\napplication and your application master\nis the one which uses these resources\nbut it can never\nnegotiate for more resources to node\nmanager application master cannot do\nthat application master has to always go\nback to resource manager if it needs\nmore resources so it is always the\nresource manager an internally resource\nmanager component that is application\nmanager which negotiates the resources\nat any point of time due to some node\nfailures or due to any other\nrequirements if application master\nneeds\nmore resources on one or multiple nodes\nit will always be contacting the\nresource manager internally the\napplications manager for more containers\nnow this is how it looks so your client\nsubmits the job request to resource\nmanager now we know that resource\nmanager internally has scheduler an\napplications manager node managers which\nare running on multiple machines are the\nones which are tracking their resources\ngiving this information to the source\nmanager so that resource manager or i\nwould say its component applications\nmanager could request resources from\nmultiple node managers when i say\nrequest resources it is these containers\nso your resource manager basically will\nrequest for the resources on one or\nmultiple nodes node manager is the one\nwhich approves these containers and once\nthe container has been approved your\nresource manager triggers a piece of\ncode that is application master which\nobviously needs some resources so it\nwould run in one of the containers and\nwill use other containers to do the\nexecution so your client submits an\napplication to resource manager resource\nmanager allocates a container or i would\nsay this is at a high level right\nresource manager is negotiating the\nresources and internally who is\nnegotiating the resources it is your\napplications manager who is granting\nthis request it is node manager and\nthat's how we can say resource manager\nlocates a container application master\nbasically contacts the related node\nmanager because it needs to use the\ncontainers node manager is the one which\nlaunches the container or basically\ngives those resources within which an\napplication can run and application\nmaster itself will then accommodate\nitself in one of the containers and then\nuse other containers for the processing\nand it is within these containers the\nactual execution happens now that could\nbe a map task that could be a reduced\ntask that could be a spark executor\ntaking care of smart tasks and many\nother processing\nso before we look into the demo on how\nyarn works i would suggest looking into\none of the blogs from cloudera so you\ncan just look for yarn untangling and\nthis is really a good blog which\nbasically talks about the overall\nfunctionality which i explained just now\nso as we mentioned here so you basically\nhave the master process you have the\nworker process which basically takes\ncare of your processing your resource\nmanager being the master and node\nmanager being the slave this also talks\nabout the resources which each node\nmanager has\nit talks about the yarn configuration\nfile where you give all these properties\nit basically shows you node manager\nwhich reports the amount of resources it\nhas to resource manager now remember if\nworker node shows 18 to 8 cpu cores and\n128 gb ram and if your node manager says\n64 v cores and ram 128 gb then that's\nnot the total capacity of your node it\nis some portion of your node which is\nallocated to node manager now once your\nnode manager reports that your resource\nmanager is requesting for containers\nbased on the application what is a\ncontainer it is basically a logical name\ngiven to a combination of vcore and\nram\nit is within this container where you\nwould have basically the process running\nso once your application starts and once\nnode manager is guaranteed these\ncontainers your application or your\nresource manager has basically already\nstarted an application master within the\ncontainer and what does that application\nmaster do it uses the other containers\nwhere the tasks would run so this is a\nvery good blog which you can refer to\nand this also talks about mapreduce if\nyou have already followed the mapreduce\ntutorials in past then you would know\nabout the different kind of tasks that\nis map and reduce and these map and\nreduce tasks could be running within the\ncontainer in one or multiple as i said\nit could be map task it could be reduced\ntask it could be a spark based task\nwhich would be running within the\ncontainer now once the task finishes\nbasically that resources can be freed up\nso the container is released and the\nresources are given back to yarn so that\nit can take care of further processing\nif you'll further look in this blog you\ncan also look into the part two of it\nwhere you talk mainly about\nconfiguration settings you can always\nlook into this which talks about why and\nhow much resources are allocated to the\nnode manager it basically talks about\nyour operating system overhead it talks\nabout other services it talks about\nclouded or hot and works related\nservices running and other processes\nwhich might be running and based on that\nsome portion of ram and cpu cores would\nbe allocated to node manager so that's\nhow it would be done in the yarn hyphen\nsite file and this basically shows you\nwhat is the total amount of memory and\ncpu cores which is allocated to node\nmanager then within every machine where\nyou have a node manager running on every\nmachine in the yarn hyphen site file you\nwould have such properties which would\nsay what is the minimum container size\nwhat is the maximum container size in\nterms of ram what is the minimum for cpu\ncores what is the maximum for cpu cores\nand what is the incremental size in\nwhere ram and cpu cores can increment so\nthese are some of the properties which\ndefine how containers are allocated for\nyour application request so have a look\nat this and this could be a good\ninformation which talks about different\nproperties now you can look further\nwhich talks about scheduling if you look\nin this particular blog which also talks\nabout scheduling where it talks about\nscheduling in yarn which talks about\nfair scheduler or you basically having\ndifferent cues in which allocations can\nbe done you also have different ways in\nwhich queues can be managed and\ndifferent schedulers can be used so you\ncan always look at this series of blog\nyou can also be checking for yarn\nschedulers and then search for uh\nhadoop\ndefinitive\nguide and that could give you some\ninformation on how it looks when you\nlook for hadoop definitive guide so if\nyou look into this book which talks\nabout the different resources\nas i mentioned so you could have a fee\nfor scheduler that is first in first out\nwhich basically means if a long running\napplication is submitted to the cluster\nall other small running applications\nwill have to wait there is no other way\nbut that would not be a preferred option\nif you look in fifo scheduler if you\nlook for capacity scheduler it basically\nmeans that you could have different\nqueues created and those queues would\nhave resources allocated so then you\ncould have a production queue where\nproduction jobs are running in a\nparticular queue which has fixed amount\nof resources allocated you could have a\ndevelopment queue where development jobs\nare running and both of them are running\nin parallel you could then also look\ninto fair scheduler which basically\nmeans again multiple applications could\nbe running on the cluster however they\nwould have a fair share so when i say\nfair share in brief what it means is\nif i had given 50 percent of resources\nto a queue for production and 50 percent\nof resources for a queue of development\nand if both of them are running in\nparallel then they would have access to\n50 percent of cluster resources however\nif one of the queue is unutilized then\nsecond queue can utilize all cluster\nresources so look into the fair\nscheduling part it also shows you about\nhow allocations can be given and you can\nlearn more about schedulers and how cues\ncan be used for managing multiple\napplications now we will spend some time\nin looking into few ways or few quick\nways in interacting with yarn in the\nform of a demo to understand and learn\non how yarn works we can look into a\nparticular cluster now here we have a\ndesignated cluster which can be used you\ncould be using the similar kind of\ncommands on your apache based cluster or\na cloudera quick start vm if you already\nhave or if you have a cloud error or a\nhortonworks cluster running there are\ndifferent ways in which we can interact\nwith yarn and we can look at the\ninformation one is basically looking\ninto the admin console so if i would\nlook into cloud data manager which is\nbasically an admin console for a\ncloudera's distribution of hadoop\nsimilarly you could have a hortonworks\ncluster than access to the admin console\nso if you have even read access for your\ncluster and if you have the admin\nconsole then you can search for yarn as\na service which is running you can click\non yarn as a service and that gives you\ndifferent tabs so you have the instances\nwhich tells basically what are the\ndifferent roles for your yarn service\nrunning so we have here multiple node\nmanagers now some of them show in stop\nstatus but that's nothing to worry so we\nhave three and six node managers we have\nresource manager which is one but then\nthat can also be in a high availability\nwhere you can have active and standby\nyou also have a job history server which\nwould show you the applications once\nthey have completed now you can look at\nthe yarn configurations and as i was\nexplaining you can always look for the\nproperties which are related to the\nallocation so you can here search for\ncourse and that should show you the\nproperties which talk about the\nallocations so here if we see we can be\nlooking for\nyarn app mapreduce application master\nresource cpu course what is the cpu\ncourse allocated for mapreduce map task\nreduce task you can be looking at yarn\nnode manager resource cpu course which\nbasically says every node manager\non every node would be allocated with\nsix cpu cores and the container sizing\nis with minimum allocation of one cpu\ncore and the maximum could be two cpu\ncores similarly you could also be\nsearching for memory allocation and here\nyou could then scroll down to see what\nkind of memory allocation has been done\nfor the node manager so if we look\nfurther it should give me information of\nnode manager which basically says here\nthat the container minimum allocation is\n2 gb the maximum is 3 gb and we can look\nat node manager which has been given 25\ngb per node so it's a combination of\nthis memory and cpu cores which is the\ntotal amount of resources which have\nbeen allocated to every node manager now\nwe can always look into applications tab\nthat would show us different\napplications which are submitted on yarn\nfor example right now we see there is a\nspark application running which is\nbasically a user who is using spark\nshell which has triggered a application\non spark and that is running on yarn you\ncan look at different applications\nworkload information you can always do a\nsearch based on the number of days how\nmany applications have run and so on you\ncan always go to the web ui and you can\nbe searching for the resource manager\nweb ui and if you have access to that it\nwill give you overall information of\nyour cluster so this basically says that\nhere we have 100 gb memory allocated so\nthat could be say 25 gb per node and if\nwe have four node managers running and\nwe have 24 cores which is six cores per\nnode if we look further here into nodes\ni could get more information so this\ntells me that i have four node managers\nrunning and node managers basically have\n25 gb memory allocated per node and six\nscores out of which some portion is\nbeing utilized we can always look at the\nscheduler here which can give us\ninformation what kind of scheduler has\nbeen allocated so we basically see that\nthere is just a root cue and within root\nyou have default queue and you have\nbasically user skew based on different\nusers we can always scroll here and that\ncan give us information if it is a fair\nshare so here we see that my root dot\ndefault has 50 percent of resources and\nthe other queue also has 50 percent of\nresources which also gives me an idea\nthat a fair scheduler is being used we\ncan always confirm that if we are using\na fair scheduler or a capacity scheduler\nwhich takes care of a location so search\nfor scheduler and that should give you\nsome understanding of what kind of\nscheduler is being used and what are the\nallocations given for that particular\nscheduler so here we have fair scheduler\nit shows me you have under root you have\nthe root q which has been given hundred\npercent capacity and then you have\nwithin that default which also takes\nhundred percent so this is how you can\nunderstand about yarn by looking into\nthe yarn web ui you can be looking into\nthe configurations you can look at\napplications you can always look at\ndifferent actions now since we do not\nhave admin access the only information\nwe have is to download the client\nconfiguration we can always look at the\nhistory server which can give us\ninformation of all the applications\nwhich have successfully completed now\nthis is from your yarn ui what i can\nalso do is i can be going into hue which\nis the web interface and your web\ninterface also basically allows you to\nlook into the jobs so you can click on\nhue web ui and if you have access to\nthat it should show up or you should\nhave a way to get to your hue which is a\ngraphical user interface mainly comes\nwith your cloud error you can also\nconfigure that with apache hortonworks\nhas a different way of giving you the\nweb ui access you can click and get into\nhue and that is also one way where you\ncan look at yarn you can look at the\njobs which are running if there are some\nissues with it and these these are your\nweb interfaces so either you look from\nyarn web ui or here in hue you have\nsomething called as job browser which\ncan also give you information of your\ndifferent applications which might have\nrun so here i can just remove this one\nwhich will basically give me a list of\nall the different kind of jobs or\nworkflows which were run so either it\nwas a spark based application or it was\na map reduce or it was coming from hive\nso here i have list of all the\napplications and it says this was a\nmapreduce this was a spark something was\nkilled something was successful and this\nwas basically a probably a hive query\nwhich triggered a mapreduce job you can\nclick on the application and that tells\nyou how many tasks were run for it so\nthere was a map task which ran for it\nyou can get into the metadata\ninformation which you can obviously you\ncan also look from the yarn ui to look\ninto your applications which can give\nyou a detailed information of if it was\na map reduce how many map and reduce\ntasks were run what were the different\ncounters if it was a spark application\nit can let you follow through spark\nhistory server or job history server so\nyou can always use the web ui to look\ninto the jobs you can be finding in lot\nof useful information here you can also\nbe looking at how many resources were\nused and what happened to the job was it\nsuccessful did it fail and what was the\njob status now apart from web ui which\nalways you might not have access to so\nin a particular cluster in a production\ncluster there might be restrictions and\nthe organization might not have\naccess given to all the users to\ngraphical user interface like you or\nmight be you would not have access to\nthe cloud era manager or admin console\nbecause probably organization is\nmanaging multiple clusters using this\nadmin console so the one way which you\nwould have access is your web console or\nbasically your edge node or client\nmachine from where you can connect to\nthe cluster and then you can be working\nso let's log in here and now here we can\ngive different commands so this is the\ncommand line from where you can have\naccess to different details you can\nalways check by just typing in map red\nwhich gives you different options where\nyou can look at the map reduce related\njobs you can look at different queues if\nthere are queues configured you can look\nat the history server or you can also be\ndoing some admin stuff provided you have\naccess so for example if i just say map\nred and q here this basically gives me\nan option says what would you want to do\nwould you want to list all the queues do\nyou want information on a particular\nqueue so let's try a list\nand that should give you different\nqueues which were being used now here we\nknow that per user a queue dynamically\ngets created which is under root dot\nusers and that gives me what is the\nstatus of the queue what is the capacity\nhas there been any kind of maximum\ncapacity or capping done so we get to\nsee a huge list of queues which\ndynamically get configured in this\nenvironment and then you also look at\nyour root dot default i could have also\npicked up one particular queue and i\ncould have said show me the jobs so i\ncould do that now here we can\nalso give a yarn command so let me just\nclear the screen and i will say yarn and\nthat shows me different options so apart\nfrom your web interface something like\nweb ui apart from your yarn's web ui you\ncould also be looking for information\nusing yarn commands here so these are\nsome list of commands which we can check\nnow you can just type in yarn and\nversion if you would want to see the\nversion which basically gives you\ninformation of what is the hadoop\nversion being used and what is the\nvendor specific distribution version so\nhere we see we are working on cloudera's\ndistribution 5.14 which is internally\nusing hadoop 2.6 now similarly you can\nbe doing a yarn application list so if\nyou give this that could be an\nexhaustive list of all the applications\nwhich are running or applications which\nhave completed so here we don't see any\napplications because right now probably\nthere are no applications which are\nrunning it also shows you you could be\npulling out different status such as\nsubmitted accepted or running now you\ncould also say i would want to see the\nservices that have finished running so i\ncould say yarn application list and app\nstates as finished so here we could be\nusing our command so i could say yarn\napplication\nlist\nand then i would want to see the app\nstates which gives me the applications\nwhich have finished and we would want to\nlist all the applications which finished\nnow there that might be applications\nwhich succeeded right and there is a\nhuge list of application which is coming\nin from the history server which is\nbasically showing you the huge list of\napplications which have completed so\nthis is one way and then you could also\nbe searching for one particular\napplication if you would want to search\na particular application if you have the\napplication id you could always be doing\na grip that's a simple way i could say\nbasically let's pick up this one and if\ni would want to search for this if i\nwould want more details on this i could\nobviously do that\nby calling in my previous command and\nyou could do a grip if that's what you\nwant to do and if you would want to\nsearch is there an application which is\nin the list of my applications that\nshows my application i could pull out\nmore information about my application so\ni could look at the log files for a\nparticular application by giving the\napplication id so i could say yarn logs\nnow that's an option and every time\nanytime you have a doubt just hit enter\nit will always give you options what you\nneed to give with a particular command\nso i can say yarn logs application\nid now we copied an application id and\nwe could just give it here we could give\nother options like app owner or if you\nwould want to get into the container\ndetails or if you would want to check on\na particular node now here i am giving\nyarn logs and then i am pointing it to\nan application id and it says the log\naggregation has not completed might be\nthis was might be this was an\napplication which was triggered based on\na particular interactive shell or based\non a particular query so there is no log\nexisting for this particular application\nyou can always look at the status of an\napplication you can kill an application\nso here you can be saying yarn yarn\napplication and then what would you want\nto do with an application hit and enter\nit shows you the different options so we\njust tried app states you could always\nlook at the last one which says status\nand then for my status i could be giving\nmy application id so that tells me what\nis the status of this application it\nconnects to the resource manager it\ntells me what's the application id what\nkind of application it was who ran it\nwhich was the queue where the job was\nrunning what was the start and end time\nwhat is the progress the status of it if\nit is finished or if it has succeeded\nand then it basically gives me also an\ninformation of where the application\nmaster was running it gives me the\ninformation where you can find this job\ndetails in history server if you are\ninterested in looking into it also gives\nyou a aggregate resource allocation\nwhich tells how much gb memory and how\nmany c4 seconds it used so this is\nbasically looking out at the application\ndetails now i could kill an application\nif the application was already running i\ncould always do a yarn application\nminus skill and then i could be giving\nmy application now i could try killing\nthis however it would say the\napplication is already finished if i had\nan application running and if my\napplication was already given an\napplication id by resource manager i\ncould just kill it i can also say yarn\nnode list which should give me a list of\nthe node managers now this is what we\nwere looking from the yarn web ui and we\nwere pulling out the information so we\ncan get this and kind of information\nfrom your command line always remember\nand always try to be well accustomed\nwith the command line so you can do\nvarious things from the command line and\nthen obviously you have the web uis\nwhich can help you with a graphical\ninterface easily able to access things\nnow you could be also starting the\nresource manager which we would not be\ndoing here because we are already\nrunning in a cluster so you could give a\nyarn resource manager you could get the\nlogs of resource manager if you would\nwant by giving yarn demon so we can try\nthat so you can say yarn and then demon\nso it says it does not find the demon so\nyou can give something like this get\nlevel and here i will have to give the\nnode and the ip address where you want\nto check the logs of resource manager so\nyou could be giving this for which we\nwill have to then get into cloud error\nmanager to look into the nodes and the\nip address you could be giving a command\nsomething like this which basically\ngives you the level of the log which you\nhave and i got this resource manager\naddress from the web ui now i can be\ngiving in this command to look into the\ndemand log and it basically says you\nwould want to look at the resource\nmanager related log and you have the log\n4j which is being used for logging the\nkind of level which has been set as info\nwhich can again be changed in the way\nyou're logging the information now you\ncan try any other commands also from\nyarn for example looking at the yarn rm\nadmin so you can always do a yarn rm\nadmin and this basically gives you a lot\nof other informations like refreshing\nthe cues or refreshing the nodes or\nbasically looking at the admin acls or\ngetting groups so you could always get\ngroup names for a particular user now we\ncould search for a particular user such\nas yarn or hdfs itself so i could just\nsay here i would want\nget\ngroups and then i could be searching for\nsay username hdfs so that tells me sdfs\nbelongs to a hadoop group similarly you\ncould search for say map red or you\ncould search for yarn so these are\nservice related users which\nautomatically get created and you can\npull out information related to these\nyou can always do a refresh nodes kind\nof command and that is mainly done\ninternally this can be useful when\nyou're doing commissioning\ndecommissioning but then in case of\ncloudera or hortonworks kind of cluster\nyou would not be manually giving this\ncommand because if you are doing a\ncommissioning decommissioning from an\nadmin console and if you are an\nadministrator then you could just\nrestart the services which are affected\nand that will take care of this but if\nyou were working in an apache cluster\nand if you were doing commissioning\ndecommissioning then you would be using\nin two commands refresh nodes and\nbasically that's for refreshing the\nnodes which should not be used for\nprocessing and similarly you could have\na command refresh nodes which comes with\nsdfs so these are different options\nwhich you can use with your yarn on the\ncommand line you could also be using\ncurl commands to get more information\nabout your cluster by giving curl minus\nx and then basically pointing out to\nyour resource manager web ui address now\nhere i would like to print out the\ncluster related metrics and i could just\nsimply do this which basically gives me\na high level information of how many\napplications were submitted how many are\npending what is the reserved resources\nwhat is the available amount of memory\nor cpu cores and all the information\nsimilarly you can be using the same curl\ncommands to get more information like\nscheduler information so you would just\nreplace the metrics with scheduler and\nyou could get the information of the\ndifferent queues now that's a huge list\nwe can cancel this and that would give\nme the list of all the queues which are\nallocated and what are the resources\nallocated for each queue you could also\nget cluster information on application\nids and status running of applications\nrunning in yarn so you would have to\nreplace the last bit of it and you would\nsay i would want to look at the\napplications and that gives me a huge\nlist of applications then you can do a\ngrip and you can be filtering out\nspecific application related information\nsimilarly you can be looking at the\nnodes so you can always be looking at\nnode specific information which gives\nyou how many nodes you have but this\ncould be mainly used when you have an\napplication which wants to or a web\napplication which wants to use a curl\ncommand then would want to get\ninformation about your cluster from an\nhttp interface now when it comes to\napplication we can basically try running\na simple or a sample map reduced job\nwhich could then be triggered on yarn\nand it would use the resources now i can\nlook at my application here and i can be\nlooking into my specific directory which\nis this one which should have lot of\nfiles and directories which we have here\nnow i could pick up one of these and i\ncould be using a simple example to do\nsome processing let's take up this file\nso there is a file and i could run a\nsimple word count or i could be running\na hive query which triggers a mapreduce\njob i could even run a spark application\nwhich would then show that the\napplication is running on the cluster so\nfor example if i would say spark to\nshell now i know that this is an\ninteractive way of working with spark\nbut this internally triggers a spark\nsubmit and this runs an application so\nhere when you do a spark to shell by\ndefault it will contact yarn so it gets\nan application id it is running on yarn\nwith the master being yarn and now i\nhave access to the interactive way of\nworking with spark now if i go and look\ninto applications i should be able to\nsee my application which has been\nstarted here and it shows up here so\nthis is my application 3827\nwhich has been started on yarn and as of\nnow we can also look into the yarn ui\nand that shows me the application which\nhas been started which basically has one\nrunning container which has one cpu core\nallocated 2gb ram and it's in progress\nalthough we are not doing anything there\nso we can always look at our application\nfrom the yarn ui or as i mentioned from\nyour applications tab within yarn\nservices which gives us the information\nand you can even click on this\napplication to follow and see more\ninformation but you should be given\naccess to that now this is just a simple\napplication which i triggered using\nspark shell similarly i can basically be\nrunning a mapreduce now to run a\nmapreduce i can say hadoop\njar and that basically needs a class so\nwe can look for the default path which\nis opt cloud error parcels cdh\nlib hadoop map reduce hadoop mapreduce\nexamples and then we can look at this\nparticular jar file and if i hit on\nenter it shows me the different classes\nwhich are part of this jar and here i\nwould like to use word count so i could\njust give this i could say word count\nnow remember i could run the job in a\nparticular queue by giving in an\nargument here so i could say minus d map\nred dot job dot q\ndot name and then i can point my job to\na particular queue i can even give\ndifferent arguments in saying i would\nwant my mapreduce output to be\ncompressed or i want it to be stored in\na particular directory and so on so here\ni have the word count and then basically\nwhat i can be doing is i can be pointing\nit to a particular input path\nand then i can have my output which can\nbe getting stored here again a directory\nwhich we need to choose and i will say\noutput\nnew and i can submit my job now once i\nhave submitted my job it connects to\nresource manager it basically gets a job\nid it gets an application id it shows\nyou from where you can track your\napplication you can always go to the\nyarn ui and you can be looking at your\napplication and the resources it is\nusing so my application was not a big\none and it has already completed it\ntriggered one map task it launched one\nreduced task it was working on around 12\n466 records where you have then the\noutput of map which is these many number\nof output records which was then taken\nby a combiner and finally by a reducer\nwhich basically gives you the output so\nthis is my on application which has\ncompleted now i could be looking into\nthe yarn ui and if my job has completed\nyou might not see your application here\nso as of now it shows up here the word\ncount which i ran it also shows me my\nprevious spark shell job it shows me my\napplication is completed and if you\nwould want further information on this\nyou can click and go to the history\nserver if you have been given access to\nit or directly go to the history server\nweb ui where your application shows up\nit shows how many map and reduce tasks\nit was running you can click on this\nparticular application which basically\ngives you information of your map and\nreduce tasks you can look at different\ncounters for your application right you\ncan always look at map specific tasks\nyou can always look into one particular\ntask what it did on which node it was\nrunning or you can be looking at the\ncomplete application log so you can\nalways click on the logs and here you\nhave click here for full log which gives\nyou the information and you can always\nlook for your application which can give\nyou information of app master being\nlaunched or you could have\nsearch for the word container so you\ncould see a job which needs one or\nmultiple containers and then you could\nsay container is being requested then\nyou could see container is being\nallocated then you can see what is the\ncontainer size and then basically your\ntask moves from initializing to running\nin the container and finally you can\neven search for release which will tell\nyou that the container was released so\nyou can always look into the log for\nmore information so this is how you can\ninteract with yarn this is how you can\ninteract with your command line to look\nfor more information or using your yarn\nweb ui my name is richard kirschner i am\nwith the simply learn team that's\nwww.simplylearn.com\nget certified get ahead today we're\ngoing to dive in on scoop one of the\nmany features of the hadoop ecosystem\nfor the hadoop file system what's in it\nfor you today we're going to cover the\nneed for scoop what is scoop scoop\nfeatures scoop architecture scoop import\nscoop export scoop processing and then\nfinally we'll have a little hands-on\ndemo on scoop so you can see what it\nlooks like so where does the need for\nscoop come in in our big data hadoop\nfile system processing huge volumes of\ndata requires loading data from diverse\nsources into hadoop cluster you can see\nhere we have our data processing and\nthis process of loading data from the\nheterogeneous sources comes with a set\nof challenges so what are the challenges\nmaintaining data consistency ensuring\nefficient utilization of resources\nespecially when you're talking about big\ndata we can certainly use up the\nresources when importing terabytes and\npetabytes of data over the course of\ntime loading bulk data to hadoop was not\npossible it's one of the big challenges\nthat came up when they first had the\nhadoop file system going and loading\ndata using script was very slow in other\nwords you'd write a script in whatever\nlanguage you're in and then it would\nvery slowly load each piece and parse it\nin so the solution scoop scooped helped\nin overcoming all the challenges to\ntraditional approach and could lead bulk\ndata from rdbms to hadoop very easily so\nthank your enterprise server you want to\ntake the from mysql or sql and you want\nto bring that data into your hadoop\nwarehouse your data filing system and\nthat's where scoop comes in so what\nexactly is scoop scoop is a tool used to\ntransfer bulk of data between hadoop and\nexternal data stores such as relational\ndatabases and mysql server or\nthe microsoft sql server or my sql\nserver so scoop equals sql plus hadoop\nand you can see here we have our rdbms\nall the data we have stored on there and\nthen your scoop is the middle ground and\nbrings the import into the hadoop file\nsystem it also is one of the features\nthat goes out and grabs the data from\nhadoop and exports it back out into an\nrdbms let's take a look at scoop\nfeatures scoop features has parallel\nimport and export it has import results\nof sql query connectors for all major\nrdbms databases kerberos security\nintegration provides full and\nincremental load so we look at parallel\nimport and export scoop uses yarn yet\nanother resource negotiator framework to\nimport and export data this provides\nfault tolerance on a top of parallelism\nscoop allows us to import the result\nreturned from an sql carry into the\nhadoop file system or the hdfs and you\ncan see here where the import results of\nsql query come in scoop provides\nconnectors for multiple relational\ndatabase management system rdbms's\ndatabases such as mysql and microsoft\nsql server and it has connectors for all\nmajor rdbms databases scoop supports\nkerberos computer network authentication\nprotocol that allows nodes communicating\nover a non-secure network to prove their\nidentity to one another in a secure\nmanner scoop can load the whole table or\nparts of the table by a single command\nhence it supports full and incremental\nload let's dig a little deeper into the\nscoop architecture we have our client in\nthis case a hooded wizard behind his\nlaptop you never know who's going to be\naccessing the hadoop cluster and the\nclient comes in and sends their command\nwhich goes into scoop the client submits\nthe import export command to import or\nexport data data from different\ndatabases is fetched by scoop and so we\nhave\nenterprise data warehouse document based\nsystems you have connect connector for\nyour data warehouse a connector for\ndocument based systems which reaches out\nto those two entities and we have our\nconnector for the rdbms so connectors\nhelp in working with a range of popular\ndatabases multiple mappers perform map\ntasks to load the data onto hdfs a\nhadoop file system and you can see here\nwe have the map task if you remember\nfrom hadoop hadoop is based on map\nreduce because we're not reducing the\ndata we're just mapping it over it only\naccesses the mappers and it opens up\nmultiple mappers to do parallel\nprocessing and you can see here the hdfs\nhbase high is where the target is for\nthis particular one similarly multiple\nmap tests will export the data from hdfs\nonto rdbms using scoop export command so\njust like you can import it you can now\nexport it using the multiple map\nroutines scoop import so here we have\nour dbms data store and we have the\nfolders on there so maybe it's your\ncompany's database maybe it's an archive\nat google with all the searches going on\nwhatever it is usually you think with\nscoop you think sql you think my sequel\nserver or microsoft sql server that kind\nof setup\nso it gathers the metadata and you see\nthe scoop import so introspect database\ntogether metadata primary key\ninformation and then it submits so you\ncan see submits map only job remember we\ntalked about mapreduce it only needs the\nmap side of it because we're not\nreducing the data we're just mapping it\nover scoop divides the input data set\ninto splits and uses individual map\ntests to push the splits into hdfs so\nright into the hadoop file system and\nyou can see down on the right is kind of\na small depiction of a hadoop cluster\nand then you have scoop export so we're\ngoing to go the other direction and with\nthe other direction you have your hadoop\nfile system storage which is your hadoop\ncluster you have your scoop job and each\none of those clusters then gets a map\nmapper comes out to each one of the\ncomputers that has data on it\nso the first step is you've got to\ngather the metadata so step one you\ngather the metadata step two submits map\nonly job introspect database to gather\nmetadata primary key information scoop\ndivides the input data set into splits\nand uses individual map tests to push\nthe splits to rdbms scoop will export\nhadoop files back to rdms tables you can\nthink of this in a number of different\nmanners one of them would be if you're\nrestoring a backup from the hadoop file\nsystem into your enterprise machines\nthere's certainly many others as far as\nexploring data and data science so as we\ndig a little deeper into scoop input we\nhave our connect our jdbc and our url so\nspecified the jdbc connect string\nconnecting manager we specify the\nconnection manager class to use you can\nsee here driver with the class name\nmanually specify the jdbc driver class\nto use hadoop map reduce home directory\noverride hadoop mapped home username set\nauthentication username and of course\nhelp print uses instructions and with\nthe export you'll see that we can\nspecify the jdbc connect string specify\nthe connection manager class to use\nmanually specify jdbc driver class to\nuse we do have to let it know to\noverride the hadoop map reduce home and\nthat's true on both of these and set\nauthentication username and finally you\ncan print out all your help setup so you\ncan see the format for scoop is pretty\nstraightforward both import and export\nso let's uh continue on our path and\nlook at scoop processing and what the\ncomputer goes through for that and we\ntalk about scoop processing first scoop\nruns in the hadoop cluster it imports\ndata from the rdbms the nosql database\nto the hadoop file system so remember we\nmight not be importing the data from a\nrdbms it might actually be coming from a\nnosql and there's many out there it uses\nmappers to slice the incoming data into\nmultiple formats and load the data into\nhdfs it exports data back into an rdbms\nwhile making sure that the schema of the\ndata in the database is maintained so\nnow that we've looked at the basic\ncommands in our scoop in the scoop\nprocessing or at least the basics as far\nas theory is concerned let's just jump\nin and take a look at a demo on scoop\nfor this demo i'm going to use our\ncloudera quick start if you've been\nwatching our other demos we've done\nyou'll see that we've been using that\npretty consistently certainly this will\nwork in any of your\nyour horton sandbox which is also a\nsingle node testing machine cloudera is\none of um there's a docker version\ninstead of virtualbox and you can also\nset up your own hadoop cluster plan a\nlittle extra time if you're not an admin\nit's actually a pretty uh significant\nendeavor for an admin if you've been\nadmitting linux machines for a very long\ntime and you know a lot of the commands\ni find for most admins it takes them\nabout two to four hours the first time\nthey go in and create a virtual machine\nand set up their own hadoop in this case\nthough when you're just learning and\ngetting set up best to start with\ncloudera cloudera also includes an\ninstall version of mysql that way you\ndon't have to install the the sql\nversion for importing data from into\nonce you're in the cloudera quick start\nyou'll see it opens a nice centos linux\ninterface and it has the desktop setup\non there this is really nice for\nlearnings here not just looking at\ncommand lines and from in here it should\nopen up by default to hue if not you can\nclick on hue here's a kind of a fun\nlittle web-based interface under hue i\ncan go under query i can pick an editor\nand we'll go right down to scoop so now\ni'm just going to load the scoop editor\ninner hue now i'm going to switch over\nand do this all in command line i just\nwant to show that you can actually do\nthis in a hue through the web-based\ninterface the reason i like to do the\ncommand line is specifically on my\ncomputer it runs much quicker or if i do\nthe command line here and i run it it\ntends to have an extra lag or an added\nlayer in it so for this we're going to\ngo ahead and open our command line the\nsecond reason i do this is we're going\nto need to go ahead and edit our mysql\nso we have something to scoop in\notherwise i don't have anything going in\nthere and of course we zoom in zoom in\nthis and increase the size of our screen\nso for this demo our hands-on i'm going\nto use oracle virtualbox manager and the\ncloudera quickstart if you're not\nfamiliar with this we do have another\ntutorial we put out and you can send a\nnote in the youtube video below and let\nour team know they'll send you a link or\ncome visit\nwww.simplylearn.com now this creates a\nlinux box on my windows computer so\nwe're going to be in linux and it'll be\nthe cloudera version\nwith scoop and we'll also be using mysql\nmy sql server once inside the cloudera\nvirtualbox we'll go under the hue editor\nnow we're going to do everything in\nterminal window but i just want you to\nbe aware that under the hue editor you\ncan go under query editor and you'll see\nas we come down here here's our scoop on\nthis so you can run your scoop from in\nhere now before we do this we have to do\na little exploration in my sql and my\nsql server that way we know what data is\ncoming in so let me go ahead and open up\na terminal window in cloudera you have a\nterminal window at the top here that you\ncan just click on it open it up and let\nme just go ahead and zoom in on here go\nview and zoom in now to get into my sql\nserver you simply type in mysql and this\npart will depend on your setup now the\ncloudera quickstart comes up that the\nusername is root and the password is\ncloudera kind of a strange quirk is that\nyou can put a space between the minus u\nand the root but not between the minus p\nand the cloudera usually you put in a\nminus capital p and then it prompts you\nfor your password on here for this demo\ni don't worry too much about you knowing\nthe password on that so we'll just go\nright into my sql server since this is\nthe standard password for this quick\nstart and you can see we're now into\nmysql and we're going to do just a\ncouple of quick\ncommands in here there's show databases\nand you follow by the semicolon that's\nstandard in most of these shell commands\nso it knows it's the end of your shell\ncommand and you'll see in here in the\nquick start cloudera quickstart the\nmysql comes with a standard set of\ndatabases these are just\nsome of these have to do like with the\nuzi which is the uzi part of hadoop\nwhere others of these like customers and\nemployees and stuff like that those are\njust for demo purposes they come as a\nstandard setup in there so that people\ngoing in for the first time have a\ndatabase to play with which is really\ngood for us so we don't have to recreate\nthose databases and you will see in the\nlist here we have a retail underscore db\nand then we can simply do uh use\nretail underscore db this will set that\nas a default in mysql and then we want\nto go ahead and show the tables and if\nwe show the tables you can see under the\ndatabase the retail db database we have\ncategories customers departments order\nitems orders products so there's a\nnumber of tables in here and we're going\nto go ahead and just use a standard\nsql command and if you did our hive\nlanguage you'll note remember it's the\nsame for hql also on this we're just\ngoing to select star everything from\ndepartments\nso there's our departments table and\nwe're going to list everything on the\ndepartments table and you'll see we've\ngot six lines in here and it has a\ndepartment id and a department name\ntwo for fitness three for footwear so on\nand so forth now at this point i can\njust go ahead and exit but it's kind of\nnice to have this data up here so we can\nlook at it and flip back and forth\nbetween the screens so i'm going to open\nup another terminal window and we'll go\nahead and zoom in on this also and it\nisn't too important for this particular\nsetup but it's always kind of fine fun\nto know what your setup you're working\nwith what is your host name and so we'll\ngo ahead and just type that in this is a\nlinux command and it's a\nhostname minus f and you see we're on\nquick start cloudera no surprise there\nnow this next command is going to be a\nlittle bit longer because we're going to\nbe doing our first scoop command and i'm\ngoing to do two of them we're going to\nlist databases and lists tables it's\ngoing to take just a moment to get\nthrough this\nbecause there's a bunch of stuff going\non here so we have scoop we have list\ndatabases we have connect and under the\nconnect command we need to let it know\nhow we're connecting we're going to use\nthe jdbc this is a very standard one\njdbc mysql so you'll see that if you're\ndoing an sql database that's how you\nstarted off with and then the next part\nthis is where you have to go look it up\nit's however it was created so if your\nadmin created a mysql server with a\ncertain setup that's what you have to go\nby and you'll see that usually they list\nthis as localhost so you'll see\nsomething like localhost sometimes\nthere's a lot of different formats but\nthe most common is either local host or\nthe actual connection so in this case we\nwant to go ahead and do quick start 3306\nand so quick start here's the name of\nthe local host database and how it's\nhosted on here and when you set up the\nquick start for um for hadoop under\ncloudera it's port 3306 is where that's\ncoming in so that's where all that's\ncoming from uh and so there's our path\nfor that and then we have to put in our\npassword we typically type password if\nyou look it up password on the cloudera\nquickstart is cloudera and we have to\nalso let it know the username and again\nif you're doing this you'd probably put\nin a minus capital you can actually just\ndo it for a prompt\nfor the password so if you leave that\nout it'll prompt you but for this\ndoesn't really matter i don't care if\nyou see my password it's the default one\nfor cloudera quickstart and then the\nusername on here is simply root and then\nwe're going to put our semicolon at the\nend and so we have here our full setup\nand we go ahead and list the databases\nand you'll see you might get some\nwarnings on here i haven't run the\nupdates on the quick start i suggest\nyou're not running the updates either if\nyou're doing this for the first time\nbecause it'll do some reformatting on\nthere and it quickly pass up and you can\nsee here's all of our the tables we went\nin there and if we go back to\non the previous window we should see\nthat these tables match so here we come\nin and here we have our databases and\nyou can see back up here where we had\nthe cm customers employees and so on so\nthe databases match and then we want to\ngo ahead and list the\ntables for a specific database so let's\ngo ahead and do that i'm a very lazy\ntypist so i'll put the up arrow in and\nyou can see here scoop list databases\nwe're just going to go back and change\nthis from databases to list tables so we\nwant to list the tables in here same\nconnection so most the connection is the\nsame except we need to know which tables\nwe're listing an interesting fact is you\ncan create a table without being under a\ndatabase so if you left this blank it\nwill show the open tables that aren't\nconnected directly to a database or\nunder a database but what we want to do\nis right past this last slash on the\n3306 we want to put that retail\nunderscore db because that's the\ndatabase we're going to be working with\nand this will go in there and show the\ntables listed under that database and\nhere we go we got categories customers\ndepartments order items and products if\nwe flip back here real quick there it is\nthe same thing we had we had categories\ncustomers departments order items and so\non and so let's go ahead and run our\nfirst import command and again i'm that\nlazy typer so we're going to do scoop\nand instead of list tables we want to go\nahead and import so there's our import\ncommand and so once we have our import\ncommand in there then we need to tell it\nexactly what we're going to import so\neverything else is the same we're\nimporting from the retail db so we keep\nthat and then at the very end we're\ngoing to tag on dash dash table that\ntells us we can tell what table we're\nimporting from and we're going to import\ndepartments\nthere we go so this is pretty\nstraightforward because it what's nice\nabout this is you can see the commands\nare the same i got the same connection i\nchange it for the whatever database i'm\nin\nthen i come in here our password and the\nusername are going to be the same that's\nall under the mysql server setup and\nthen we let it know what table we're\nentering it let me run this and this is\ngoing to actually go through the mapper\nprocess in hadoop so this is a mapping\nprocess it takes the data and it maps it\nup to different parts in the setup in\nhadoop on there and then saves that data\ninto the hadoop file system and it does\ntake it a moment to zip through which i\nkind of skipped over for you since it is\nrunning a you know it's designed to run\nacross the cluster not on a single node\nso when you're running on a single node\nit's going to run slow even if you\ndedicate\na couple cores to it i think i put\ndedicated four cores to this one and so\nyou can see right down here we get to\nthe end it's now mapped in that\ninformation and then we can go in here\nwe can go under can flip back to our\nhue and under hue on the top i have\nthere's databases and the second icon\nover is your hadoop file system and we\ncan go in here and look at the hadoop\nfile system and you'll see it show up\nunderneath our documents there it is\ndepartments cloudera departments and you\ncan see there's always a delay when i'm\nworking in hue which i don't like\nand that's the quick start issue that's\nnot necessarily running out on a server\nwhen i'm running it on a server you\npretty much have to run through some\nkind of server interface i still prefer\nthe terminal window it still runs a lot\nquicker but we'll flip back on over here\nto the command line and we can do the\nhadoop type in the hadoop fs and then\nlist minus ls and if we run this you'll\nsee underneath our hadoop file system\nthere is our departments which has been\nadded in and we can also do\nhadoop\nfs and this is kind of interesting for\nthose who've gone through the hadoop\nfile system everything you'll you'll\nrecognize this on here i'm going to list\nit the contents of departments and\nyou'll see underneath departments uh we\nhave part part m000\nand so this is interesting because this\nis how hadoop saves these files this is\nin the file system this is not in hive\nso we didn't directly import this into\nhive we put this in the hadoop file\nsystem depending on what you're doing\nyou would then write the schema for hive\nto look at the hadoop file system\ncertainly visit our hive tutorial for\nmore information on hive specific\nso you can see in here are different\nfiles that it forms that are part of\ndepartments and we can do something like\nthis we can look at the contents of one\nof these files fs minus ls or a number\nof the files and we'll simply do the\nfull path which is user cloudera and\nthen we already know the next one is\ndepartments\nand then after departments we're going\nto put slash part star so this is going\nto see anything that has part in it\nso we have part dash m 0 0 0 and so on\nwe can go ahead and cat use that cut\ncommand or that list command to bring\nthose up and then we can use the cat\ncommand to actually display the contents\nand that's a linux command hadoop linux\ncommand the cat catenate not to be\nconfused with catatonic catastrophic\nthere's a lot of cat got your tongue and\nwe see here fitness footwear apparel\nthat should look really familiar because\nthat's what we had in our mysql server\nwe went in here we did a select all on\nhere there it is fitness footwear\napparel golf outdoors and fan shop and\nthen of course it's really important\nlet's look back on over here to be able\nto tell it where to put the data uh so\nwe go back to our import command so\nhere's our scoop import we have our\nconnect we have the db underneath our\nconnection our my sql server we have our\npassword our username the table going\nwhere it's going to i mean the table\nwhere it's coming from uh and then we\ncan add a target on here we can put in a\ntarget dash\ndirectory and you do have to put the\nfull path that's a hadoop thing it's a\ngood practice to be in and we're going\nto add it to department we'll just do\ndepartment one and so here we now add a\ntarget directory in here and user\ncloudera department one and this will\ntake just a moment before so i'll go\nahead and skip over the process since\nit's going to run very slowly it's only\nrunning on like i said a couple cores\nand it's also on a single node and now\nwe can do the uh hadoop let's just do\nthe up arrow file system list we want\njust straight list and when we do the\nhadoop file system\nminus ls or list you'll see that we now\nhave department one and we can of course\ndo uh list department one and you can\nsee we have the files inside department\none and they mirrored what we saw before\nwith the same files in there and the\npart\n0 and so on if we want to look at them\nit'd be the same thing we did before\nwith the cat so except instead of\ndepartments uh we'd be department one\nthere we go something that's going to\ncome up with the same data we had before\nnow one of the important things when\nyou're importing data and it's always a\nquestion to ask is do you filter the\ndata before it comes in do we want to\nfilter this data as it comes in so we're\nnot storing everything in our file\nsystem you would think hadoop big data\nput it all in there i know from\nexperience that putting it all in there\ncan turn a couple hundred terabytes into\na petabyte very rapidly and suddenly\nyou're having to really add on to that\ndata store and you're storing duplicate\ndata sometimes so you really need to be\nable to filter your data out and so\nlet's go ahead and use our up arrow to\ngo to our last import uh since it's\nstill a lot the same stuff so we have\nall of our commands under import we have\nthe target\nwe're going to change this to department\n2 so we're going to create a new\ndirectory for this one and then after\ndepartments uh there's another command\nthat we didn't really slide in here and\nthat's our mapping and i'll show you\nwhat this looks like in a minute but\nwe're going to put m3 in there that\ndoesn't have nothing to do with the\nfiltering i'll show you that in a second\nthough what that's for and we just want\nto put in where uh so where and what is\nthe where in this case we want to know\nwhere department\nid and if you want to know where that\ncame from we can flip back on over here\nwe have department underscore ids this\nis where that's coming from that's just\nthe name of the column on here so we\ncome in here to department id\nis greater than 4 simple\nlogic there you can see where you'd use\nthat for maybe creating buckets for ages\nuh you know aged from 10 to 15 20 to 30.\nyou might be looking for i mean there's\nall kinds of reasons why you could use\nthe where command on here and filter\ninformation out maybe you're doing word\ncounting and you want to know words that\nare used less than 100 times you want to\nget rid of the and is and and all the\nstuff that's used over and over again uh\nso we'll go ahead and put the where and\nthen department id is greater than four\nwe'll go ahead and hit enter on here and\nthis will create our department 2 setup\non this and i'll go ahead and skip over\nsome of the runtime again it runs really\nslow on a single node real quick page\nthrough our commands\nlet's see here we go\nour list and we should see underneath\nthe list the department two on here now\nand there it is department two and then\ni can go ahead and do list department\ntwo you'll see the contents in here uh\nand you'll see that there is only three\nmaps and it could be that the data\ncreated three maps but remember i set it\nup to only use three mappers uh so\nthere's zero one and two and we can go\nahead and do a cat on there remember\nthis is department two so we wanna look\nat all the contents of these three\ndifferent files and there it is it's\ngreater than four so we have golf is\nfive outdoor six\nfan shop is seven so we've effectively\nfiltered out our data and just storing\nthe data we want on our file system so\nif you're going to store data on here\nthe next stage is to export the data\nremember a lot of times you have my sql\nserver and we're continually dumping\nthat data into our long term storage and\naccess the hadoop file system but what\nhappens when you need to pull that data\nout and\nrestore a database or\nmaybe you have\nyou just merged with a new company a\nfavorite topic merging companies\nemerging databases that's listed under\nnightmare and how many different names\nfor company can you have so you can see\nwhere being able to export is also\nequally important and let's go ahead and\ndo that i'm going to flip back over to\nmy sql server here and we'll need to go\nahead and create our database we're\ngoing to export into now i'm not going\nto go too much in detail on this command\nwe're simply creating a table and the\ntable is going to have\nit's pretty much the same table we\nalready have in here from departments\nbut in this case we're going to create a\ntable called dept\nso it's the same setup but it's just\ngoing to we're just giving a different\nname a different schema and so we've\ndone that and we'll go ahead and do a\nselect star from dept\nthere we go and it's empty that's what\nwe expect a new database a new data\ntable and it's empty in there so now we\nneed to go ahead and export our data\nthat we just filtered out into there so\nlet's flip back on over here to our\nscoop setup which is just our linux\nterminal window and let's go back up to\none of our commands here's scoop import\nin this case instead of import we're\ngoing to take the scoop and we're going\nto export so we're going to just change\nthat export and the connection is going\nto remain the same so same connect same\ndatabase we're also we're still doing\nthe retail db we have the same password\nso none of that changes uh the big\nchange here is going to be the table\ninstead of departments uh remember we\nchanged it and gave it a new name and so\nwe want to change it here also d-e-p-t\nso department we're not going to worry\nabout the mapper count and the where was\npart of our import there we go and then\nfinally it needs to know where to export\nfrom so instead of target directory we\nhave an export directory that's where\nit's coming from still user cloudera and\nwe'll keep it as department 2. just so\nyou can see how that data is coming back\nwith that we filtered in and let's go\nahead and run this it'll take it just a\nmoment to go through with steps and\nagain because it's low i'm just going to\ngo and skip this so you don't have to\nsit through it and once we've wrapped up\nour export we'll flip back on over here\nto mysql use the up arrow and this time\nwe're going to select star from\ndepartment and we can see that there it\nis it exported the golf outdoors and fan\nshop and you can imagine also that you\nmight have to use the where command in\nyour export also so there's a lot of\nmixing the command line for scoop is\npretty straightforward you're changing\nthe different variables in there whether\nyou're creating a table listing a table\nlisting databases very powerful tool for\nbringing your data into the hadoop file\nsystem and exporting it so now that\nwe've wrapped up our demo on scoop and\ngone through a lot of basic commands\nthat wraps it up for us today first\nwe're going to start with the history of\nhive what is hive architecture of hive\ndata flow and hive hive data modeling\nhive data types different modes of hive\nand difference between hive and rdbms\nfinally we're going to look into the\nfeatures of hive and do a quick hands-on\ndemo on hive in the cloudera hadoop file\nsystem let's dive in with a brief\nhistory of hive so the history of hive\nbegins with facebook facebook began\nusing hadoop as a solution to handle the\ngrowing big data and we're not talking\nabout a data that fits on one or two or\neven five computers uh we're talking due\nto the fits on if you've looked at any\nof our other hadoop tutorials you'll\nknow we're talking about very big data\nand data pools and facebook certainly\nhas a lot of data it tracks as we know\nthe hadoop uses mapreduce for processing\ndata mapreduce required users to write\nlong codes and so you'd have these\nreally extensive java codes very\ncomplicated for the average person to\nuse not all users were versed in java\nand other coding languages this proved\nto be a disadvantage for them users were\ncomfortable with writing queries in sql\nsql has been around for a long time the\nstandard sql query language hive was\ndeveloped with the vision to incorporate\nthe concepts of tables columns just like\nsql so why hive well the problem was for\nprocessing and analyzing data users\nfound it difficult to code as not all of\nthem were well versed with the coding\nlanguages you have your processing ever\nanalyzing and so the solution was\nrequired a language similar to sql which\nwas well known to all the users and thus\nthe hive or hql language evolved what is\nhive hive is a data warehouse system\nwhich is used for querying and analyzing\nlarge data sets stored in the hdfs or\nthe hadoop file system hive uses a query\nlanguage that we call hive ql or hql\nwhich is similar to sql so if we take\nour user the user sends out their hive\nqueries and then that is converted into\na mapreduce tasks and then accesses the\nhadoop mapreduce system let's take a\nlook at the architecture of hive\narchitecture of hive we have the hive\nclient\nso that could be the programmer or maybe\nit's a manager who knows enough sql to\ndo a basic query to look up the data\nthey need the hive client supports\ndifferent types of client applications\nin different languages prefer for\nperforming queries and so we have our\nthrift application in the hive thrift\nclient thrift is a software framework\nhive server is based on thrift so it can\nserve the request from all programming\nlanguage that support thrift and then we\nhave our jdbc application and the hive\njdbc driver jdbc java database\nconnectivity jdbc application is\nconnected through the jdbc driver and\nthen you have the odbc application or\nthe hive odbc driver the odbc or open\ndatabase connectivity the odbc\napplication is connected through the\nodbc driver with the growing development\nof all of our different scripting\nlanguages python c plus plus spark java\nyou can find just about any connection\nin any of the main scripting languages\nand so we have our hive services as we\nlook at deeper into the architecture\nhive supports various services\nso you have your hive server basically\nyour thrift application or your hive\nthrift client or your jdbc or your hive\njdbc driver your odbc application or\nyour hive odbc driver they all connect\ninto the hive server and you have your\nhive web interface you also have your\ncli now the hive web interface is a gui\nis provided to execute hive queries and\nwe'll actually be using that later on\ntoday so you can see kind of what that\nlooks like and get a feel for what that\nmeans commands are executed directly in\ncli and then the cli is a direct\nterminal window and i'll also show you\nthat too so you can see how those two\ndifferent interfaces work these then\npush the code into the hive driver hive\ndriver is responsible for all the\nqueries submitted so everything goes\nthrough that driver let's take a closer\nlook at the hive driver the hive driver\nnow performs three steps internally one\nis a compiler hive driver passes query\nto compiler where it is checked and\nanalyzed then the optimizer kicks in and\nthe optimize logical plan in the form of\na graph of mapreduce and hdfs tasks is\nobtained and then finally in the\nexecutor in the final step the tasks are\nexecuted we look at the architecture we\nalso have to note the meta store\nmetastore is a repository for hive\nmetadata stores metadata for hive tables\nand you can think of this as your schema\nand where is it located and it's stored\non the apache derby db processing and\nresource management is all handled by\nthe mapreduce v1 you'll see mapreduce v2\nthe yarn and the tez these are all\ndifferent ways of managing these\nresources depending on what version of\nhadoop you're in hive uses mapreduce\nframework to process queries and then we\nhave our distributed storage which is\nthe hdfs and if you looked at our hadoop\ntutorials you'll know that these are on\ncommodity machines and are linearly\nscalable that means they're very\naffordable a lot of time when you're\ntalking about big data you're talking\nabout a tenth of the price of storing it\non enterprise computers and then we look\nat the data flow and hive\nso in our data flow and hive we have our\nhive in the hadoop system and underneath\nthe user interface or the ui we have our\ndriver our compiler our execution engine\nand our metastore that all goes into the\nmapreduce and the hadoop file system so\nwhen we execute a query you see it\ncoming in here it goes into the driver\nstep one step two we get a plan what are\nwe going to do refers to the query\nexecution then we go to the metadata\nit's like well what kind of metadata are\nwe actually looking at where is this\ndata located what is the schema on it\nthen this comes back with the metadata\ninto the compiler then the compiler\ntakes all that information and the syn\nplan returns it to the driver the driver\nthen sends the execute plan to the\nexecution engine once it's in the\nexecution engine the execution engine\nacts as a bridge between hive and hadoop\nto process the query and that's going\ninto your mapreduce in your hadoop file\nsystem or your hdfs and then we come\nback with the metadata operations it\ngoes back into the metastore to update\nor let it know what's going on which\nalso goes to the between it's a\ncommunication between the execution\nengine and the metastore execution\nengine communications is\nbi-directionally with the metastore to\nperform operations like create drop\ntables metastore stores information\nabout tables and columns so again we're\ntalking about the schema of your\ndatabase and once we have that we have a\nbi-directional\nsend results communication back into the\ndriver and then we have the fetch\nresults which goes back to the client so\nlet's take a little bit look at the hive\ndata modeling hive data modeling so you\nhave your high data modeling you have\nyour tables you have your partitions and\nyou have buckets the tables in hive are\ncreated the same way it is done in rdbms\nso when you're looking at your\ntraditional sql server or mysql server\nwhere you might have enterprise\nequipment and a lot of\npeople pulling and moving stuff off of\nthere the tables are going to look very\nsimilar and this makes it very easy to\ntake that information and let's say you\nneed to keep current information but you\nneed to store all of your years of\ntransactions back into the hadoop hive\nso you match those those all kind of\nlook the same the tables are the same\nyour databases look very similar and you\ncan easily import them back you can\neasily store them into the hive system\npartitions here tables are organized\ninto partitions for grouping same type\nof data based on partition key this can\nbecome very important for speeding up\nthe process of doing queries so if\nyou're looking at dates as far as like\nyour employment dates of employees if\nthat's what you're tracking you might\nadd a partition there because that might\nbe one of the key things that you're\nalways looking up as far as employees\nare concerned and finally we have\nbuckets\ndata present in partitions can be\nfurther divided into buckets for\nefficient querying again there's that\nefficiency at this level a lot of times\nyou're taught you're working with the\nprogrammer and the admin of your hadoop\nfile system to maximize the efficiency\nof that file system so it's usually a\ntwo-person job and we're talking about\nhive data modeling you want to make sure\nthat they work together and you're\nmaximizing your resources hive data\ntypes so we're talking about hive data\ntypes we have our primitive data types\nand our complex data types a lot of this\nwill look familiar because it mirrors a\nlot of stuff in sql in our primitive\ndata types we have the numerical data\ntypes string data type date time data\ntype and\nmiscellaneous data type and these should\nbe very they're kind of self-explanatory\nbut just in case numerical data is your\nfloats your integers your short integers\nall of that numerical data comes in as a\nnumber a string of course is characters\nand numbers and then you have your date\ntime stamp and then we have kind of a\ngeneral way of pulling your own created\ndata types in there that's your\nmiscellaneous data type and we have\ncomplex data types so you can store\narrays you can store maps you can store\nstructures uh and even units in there as\nwe dig into hive data types\nand we have the primitive data types and\nthe complex data types so we look at\nprimitive data types and we're looking\nat numeric data types data types like an\ninteger a float a decimal those are all\nstored as numbers in the hive data\nsystem a string data type data types\nlike characters and strings you store\nthe name of the person you're working\nwith you know john doe the city\nmemphis the state tennessee maybe it's\nboulder colorado usa or maybe it's hyper\nbad\nindia that's all going to be string and\nstored as a string character and of\ncourse we have our date time data type\ndata types like timestamp date interval\nthose are very common as far as tracking\nsales anything like that you just think\nif you can type a stamp of time on it or\nmaybe you're dealing with the race and\nyou want to know the interval how long\ndid the person take to complete whatever\ntask it was all that is date time data\ntype and then we talk miscellaneous data\ntype these are like boolean in binary\nand when you get into boolean and binary\nyou can actually almost create anything\nin there but your yes knows zero one now\nlet's take a look at complex data types\na little closer uh we have arrays so\nyour syntax is of data type and it's an\narray and you can just think of an array\nas a collection of same\nentities one two three four if they're\nall numbers and you have maps this is a\ncollection of key value pairs\nso understanding maps is so central to\nhadoop\nso when we store maps you have a key\nwhich is a set you can only have one key\nper mapped value and so you in hadoop of\ncourse you collect\nthe same keys and you can add them all\nup or do something with all the contents\nof the same key but this is our map as a\nprimitive type data type in our\ncollection of key value pairs and then\ncollection of complex data with comment\nso we can have a structure we have a\ncolumn name data type comment call a\ncolumn comment uh so you can get very\ncomplicated structures in here with your\ncollection of data and your commented\nsetup and then we have units and this is\na collection of heterogeneous data types\nso the syntax for this is union type\ndata type data type and so on so it's\nall going to be the same a little bit\ndifferent than the arrays where you can\nactually mix and match different modes\nof hive hive operates in two modes\ndepending on the number and size of data\nnodes we have our local mode and our map\nreduce mode when we talk about the local\nmode it is used when hadoop is having\none data node and the data is small\nprocessing will be very fast on a\nsmaller data sets which are present in\nlocal machine and this might be that you\nhave a local file stuff you're uploading\ninto the hive and you need to do some\nprocesses in there you can go ahead and\nrun those high processes and queries on\nit usually you don't see much in the way\nof a single node hadoop system if you're\ngoing to do that you might as well just\nuse like an sql database or even a java\nsqlite or something python sqlite so you\ndon't really see a lot of single node\nhadoop databases but you do see the\nlocal mode in hive where you're working\nwith a small amount of data that's going\nto be integrated into the larger\ndatabase and then we have the map reduce\nmode this is used when hadoop is having\nmultiple data nodes and the data is\nspread across various data nodes\nprocessing large data sets can be more\nefficient using this mode and this you\ncan think of instead of it being one two\nthree or even five computers we're\nusually talking with the hadoop file\nsystem we're looking at 10 computers 15\n100 where this data is spread across all\nthose different hadoop nodes difference\nbetween hive and\nrdbms remember rdbms stands for the\nrelational database management system\nlet's take a look at the difference\nbetween hive and the rdbms with hive\nhive enforces schema on read and it's\nvery important that whatever is coming\nin that's when hive's looking at it and\nmaking sure that it fits the model\nthe rdbms enforces a schema when it\nactually writes the data into the\ndatabase so it's read the data and then\nonce it starts to write it that's where\nit's going to give you the error tell\nyou something's incorrect about your\nscheme hive data size is in petabytes\nthat is hard to imagine um you know\nwe're looking at your personal computer\non your desk maybe you have 10 terabytes\nif it's a high-end computer we're\ntalking petabytes so that's hundreds of\ncomputers grouped together when a rdbms\ndata size is in terabytes very rarely do\nyou see an rdbms system that's spread\nover more than five computers and\nthere's a lot of reasons for that with\nthe rdbms it actually has a high-end\namount of writes to the hard drive\nthere's a lot more going on there you're\nwriting and polling stuff so you really\ndon't want to get too big with an rd bms\nor you're gonna run into a lot of\nproblems with hive you can take it as\nbig as you want hive is based on the\nnotion of write once and read many times\nthis is so important and they call it\nworm which is write w wants o read are\nmany times m they refer to it as worm\nand that's true of any of you a lot of\nyour hadoop setup it's it's altered a\nlittle bit but in general we're looking\nat archiving data that you want to do\ndata analysis on we're looking at\npulling all that stuff off your rd bms\nfrom years and years and years of\nbusiness or whatever your company does\nor scientific research and putting that\ninto a huge data pool so that you can\nnow do queries on it and get that\ninformation out of it with the rdbms\nit's based on the notion of read and\nwrite many times so you're continually\nupdating this database you're\ncontinually bringing up new stuff new\nsales\nthe account changes because they have a\ndifferent licensing now whatever\nsoftware you're selling all that kind of\nstuff where the data is continually\nfluctuating and then hive resembles a\ntraditional database by supporting sql\nbut it is not a database it is a data\nwarehouse this is very important it goes\nwith all the other stuff we've talked\nabout that we're not looking at a\ndatabase but a data warehouse to store\nthe data and still have fast and easy\naccess to it for doing queries you can\nthink of\ntwitter and facebook they have so many\nposts that are archived back\nhistorically those posts aren't going to\nchange they made the post they're posted\nthey're there and they're in their\ndatabase but they have to store it in a\nwarehouse in case they want to pull it\nback up with the rdbms it's a type of\ndatabase management system which is\nbased on the relational model of data\nand then with hive easily scalable at a\nlow cost\nagain we're talking maybe a thousand\ndollars per terabyte um the rdbms is not\nscalable at a low cost when you first\nstart on the lower end you're talking\nabout 10 000 per terabyte of data\nincluding all the backup on the models\nand all the added necessities to support\nit as you scale it up you have to scale\nthose computers and hardware up uh so\nyou might start off with a basic server\nand then you upgrade to a sun computer\nto run it and you spend you know tens of\nthousands of dollars for that hardware\nupgrade with hive you just put another\ncomputer into your hadoop file system so\nlet's look at some of the features of\nhive uh when we're looking at the\nfeatures of hive we're talking about the\nuse of sql like language called hive ql\na lot of times you'll see that as hql\nwhich is easier than long codes this is\nnice if you're working with your\nshareholders you come to them and you\nsay hey you can do a basic sql query on\nhere and pull up the information you\nneed this way you don't have to take off\nhave your programmers jump in every time\nthey want to look up something in the\ndatabase they actually now can easily do\nthat if they're not\nskilled in programming and script\nwriting tables are used which are\nsimilar to the rdbms hence easier to\nunderstand and one of the things i like\nabout this is when i'm bringing tables\nin from a mysql server or sql server\nthere's almost a direct reflection\nbetween the two so when you're looking\nat one which is the data which is\ncontinually changing and then you're\ngoing into the archive database it's not\nthis huge jump where you have to learn a\nwhole new language\nyou mirror that same schema into the\nhdfs into the hive making it very easy\nto go between the two and then using\nhive ql multiple users can\nsimultaneously query data so again you\nhave multiple clients in there and they\nsend in their query that's also true\nwith the rdbms which kind of queues them\nup because it's running so fast you\ndon't notice the lag time well you get\nthat also with the hql as you add more\ncomputers in the query can go very\nquickly depending on how many computers\nand how much resources each machine has\nto pull the information and hive\nsupports a variety of data types\nso with hive it's designed to be on the\nhadoop system which you can put almost\nanything into the hadoop file system so\nwith all that let's take a look at a\ndemo on hive ql or hql before i dive\ninto the hands-on demo let's take a look\nat the website\nhive.apache.org that's the main website\nsince apache it's an apache open source\nsoftware this is the main software for\nthe main site for the build and if you\ngo in here you'll see that they're\nslowly migrating hive into beehive and\nso if you see beehive versus hive note\nthe beehive as the new release is coming\nout that's all it is it reflects a lot\nof the same functionality of hive it's\nthe same thing and then we like to pull\nup some kind of documentation on\ncommands and for this i'm actually going\nto go to hortonworks hive cheat sheet\nand that's because hortonworks and\ncloudera are two of the most common used\nbuilds for hadoop and four which include\nhive and all the different tools in\nthere and so hortonworks has a pretty\ngood pdf you can download cheat sheet on\nthere i believe cloudera does too but\nwe'll go ahead and just look at the\nhorton one because it's the one that\ncomes up really good and you can see\nwhen we look at the query language it\ncompares mysql server to hive ql or hql\nand you can see the basic select we\nselect from columns from table where\nconditions exist the most basic command\non there and they have different things\nyou can do with it just like you do with\nyour sql and if you scroll down you'll\nsee data types so here's your integer\nyour flow your binary double string\ntimestamp and all the different data\ntypes you can use some different\nsemantics different keys features\nfunctions uh for running a hive query\ncommand line setup and of course the\nhive shell uh set up in here uh so you\ncan see right here if we loop through it\nit has a lot of your basic stuff and it\nis we're basically looking at sql across\na horton database we're going to go\nahead and run our hadoop cluster hive\ndemo and i'm going to go ahead and use\nthe cloudera quick start this is in the\nvirtual box so again we have an oracle\nvirtual box which is open source and\nthen we have our cloudera quick start\nwhich is the hadoop setup on a single\nnode now obviously hadoop and hive are\ndesigned to run across a cluster of\ncomputers so we talk about a single node\nis for education testing that kind of\nthing and if you have a chance you can\nalways go back and look at our demo we\nhad on\nsetting up a hadoop system in a single\ncluster just set a note down below in\nthe youtube video and our team will get\nin contact with you and send you that\nlink if you don't already have it or you\ncan contact us at the\nwww.simplylearn.com now in here it's\nalways important to note that you do\nneed\non your computer if you're running on\nwindows because i'm on a windows machine\nyou're going to need probably about 12\ngigabytes to actually run this uh used\nto be by with a lot less but as things\nhave evolved they take up more and more\nresources and you need the professional\nversion if you have the home version i\nwas able to get that to run but boy did\nit take a lot of extra work to get the\nhome version to let me use the virtual\nsetup on there and we'll simply click on\nthe cloudera quickstart and i'm going to\ngo and just start that up and this is\nstarting up our linux so we have our\nwindows 10 which is a computer i'm on\nand then i have the virtual box which is\ngoing to have a linux operating system\nin it and we'll skip ahead so you don't\nhave to watch the whole install\nsomething interesting to know about the\ncloudera is that it's running on\nlinuxcentos and for whatever reason i've\nalways had to click on it and hit the\nescape button for it to spin up and then\nyou'll see the dos come in here now that\nour cloudera is spun up on our virtual\nmachine with the linux on uh we can see\nhere we have our it uses the\nthunderbird browser on here by default\nand automatically opens up a number of\ndifferent tabs for us and a quick note\nbecause i mentioned like the\nrestrictions on getting set up on your\nown computer if you have a home edition\ncomputer and you're worried about\nsetting it up on there you can also go\nin there and spin up a one month free\nservice on amazon web service to play\nwith this uh so there's other options\nyou're not stuck with just doing it on\nthe quick start menu you can spin this\nup in many other ways now the first\nthing we want to note is that we've come\nin here into cloudera and i'm going to\naccess this in two ways\nthe first one is we're going to use hue\nand i'm going to open up hue and i'll\ntake it a moment to load from the setup\non here and hue is nice if i go in and\nuse hue as an editor into hive or into\nthe hadoop setup usually i'm doing it as\na\nfrom an admin side because it has a lot\nmore information a lot of visuals less\nto do with you know actually diving in\nthere and just executing code and you\ncan also write this code into files and\nscripts and there's other things you can\notherwise you can upload it into hive\nbut today we're going to look at the\ncommand lines and we'll upload it into\nhue and then we'll go into and actually\ndo our work in a terminal window under\nthe hive shell now in the hue browser\nwindow if you go under query and click\non the pull down menu and then you go\nunder editor and you'll see hive there\nwe go there's our hive setup i go and\nclick on hive and this will open up our\nquery down here and now it has a nice\nlittle b that shows our hive going and\nwe can go something very simple down\nhere like show\ndatabases and we follow it with the\nsemicolon and that's the standard in\nhive is you always add our punctuation\nat the end there and i'll go ahead and\nrun this and the query will show up\nunderneath and you'll see down here\nsince this is a new quick start i just\nput on here you'll see it has the\ndefault down here for the databases\nthat's the database name i haven't\nactually created any databases on here\nand then there's a lot of other like\nassistant function tables\nyour databases up here there's all kinds\nof things you can research you can look\nat through hue as far as a bigger\npicture the downside of this is it\nalways seems to lag for me whenever i'm\ndoing this i always seem to run slow so\nif you're in cloudera you can open up a\nterminal window they actually have an\nicon at the top you can also go under\napplications and under applications\nsystem tools and terminal either one\nwill work it's just a regular terminal\nwindow and this terminal window is now\nrunning underneath our linux so this is\na linux terminal window or on our\nvirtual machine which is resting on our\nregular windows 10 machine and we'll go\nahead and zoom this in so you can see\nthe text better on your own video and i\nsimply just clicked on view and zoom in\nand then all we have to do is type in\nhive and this will open up the shell on\nhere and it takes it just a moment to\nload when starting up hive i also want\nto note that depending on your rights on\nthe computer you're on interaction you\nmight have to do pseudo hive and put in\nyour password and username most\ncomputers are usually set up with the\nhive login again it just depends on how\nyou're accessing the linux system and\nthe hive shell once we're in here we can\ngo ahead and do a simple uh hql command\nshow databases and if we do that we'll\nsee here that we don't have any\ndatabases so we can go ahead and create\na database and we'll just call it office\nfor today for this moment now if i do\nshow we'll just do the up arrow up arrow\nis a hotkey that works in both linux and\nin hive so i can go back and paste\nthrough all the commands i've typed in\nand we can see now that i have my\nthere's of course a default database and\nthen there's the office database so now\nwe've created a database it's pretty\nquick and easy and we can go ahead and\ndrop the database we can do drop\ndatabase\noffice now this will work on this\ndatabase because it's empty if your\ndatabase was not empty you would have to\ndo cascade and that drops all the tables\nin the database and the database itself\nnow if we do show database and we'll go\nahead and recreate our database because\nwe're going to use the office database\nfor the rest of this hands-on demo a\nreally handy command to now\nset with the sql or hql is to use office\nand what that does is that sets office\nas a default database so instead of\nhaving to reference the database every\ntime we work with a table it now\nautomatically assumes that's the\ndatabase being used whatever tables\nwe're working on the difference is you\nput the database name period table and\ni'll show you in just a minute what that\nlooks like and how that's different if\nwe're going to have a table and a\ndatabase we should probably load some\ndata into it so let me go ahead and\nswitch gears here and open up a terminal\nwindow you can just open another\nterminal window and it'll open up right\non top of the one that you have hive\nshell running in and when we're in this\nterminal window first we're going to go\nahead and just do a list which is of\ncourse a linux command you can see all\nthe files i have in here this is the\ndefault load we can change directory to\ndocuments we can list in documents and\nwe're actually going to be looking at\nemployee.csv a linux command is the cat\nyou can use this actually to combine\ndocuments there's all kinds of things\nthat cat does but if we want to just\ndisplay the contents of our\nemployee.csv file we can simply do cat\nemployee csv and when we're looking at\nthis we want to know a couple things one\nthere's a line at the top okay so the\nvery first thing we notice is that we\nhave a header line the next thing we\nnotice is that the data is comma\nseparated and in this particular case\nyou'll see a space here generally with\nthese you've got to be real careful with\nspaces there's all kinds of things you\ngot to watch out for because they can\ncause issues these spaces won't because\nthese are all strings that the space is\nconnected to if this was a space next to\nthe integer you would get a null value\nthat comes into the database without\ndoing something extra in there now with\nmost of hadoop that's important to know\nthat you're writing the data once\nreading it many times and that's true of\nalmost all your hadoop things coming in\nso you really want to process the data\nbefore it gets into the database and for\nthose who of you have studied data\ntransformation that's the adult where\nyou extract transfer form and then load\nthe data so you really want to extract\nand transform before putting it into the\nhive then you load it into the hive with\nthe transformed data and of course we\nalso want to note the schema we have an\ninteger string string integer integer so\nwe kept it pretty simple in here as far\nas the way the data is set up the last\nthing that you're going to want to look\nup\nis the source since we're doing local\nuploads we want to know what the path is\nwe have the whole path in this case it's\nhome\ncloudera slash documents and these are\njust text documents we're working with\nright now we're not doing anything fancy\nso we can do a simple get edit\nemployee.csv\nand you'll see it comes up here it's\njust a text document so i can easily\nremove these added spaces there we go\nand then we go and just save it and so\nnow it has a new setup in there we've\nedited it the g edit is usually one of\nthe default that loads into linux so any\ntext editor will do back to the hive\nshell so let's go ahead and create a\ntable employee and what i want you to\nnote here is i did not put the semicolon\non the end here semicolon tells it to\nexecute that line so this is kind of\nnice if you're you can actually just\npaste it in if you have it written on\nanother sheet and you can see right here\nwhere i have create table employee and\nit goes into the next line on there so i\ncan do all of my commands at once now\njust so i don't have any typo errors i\nwent ahead and just pasted the next\nthree lines in and the next one is our\nschema if you remember correctly from\nthe other side we had the different\nvalues in here which was id name\ndepartment year of joining and salary\nand the id is an integer name is a\nstring department string air joining\nenergy salary an integer and they're in\nbrackets we put close brackets around\nthem and you could do this all as one\nline and then we have row format\ndelimited fields terminated by comma and\nthis is important because the default is\ntabs so if i do it now it won't find any\nterminated fields so you'll get a bunch\nof null values loaded into your table\nand then finally our table properties we\nwant to skip the header line count\nequals one now this is a lot of work for\nuploading a single file it's kind of\ngoofy when you're uploading a single\nfile that you have to put all this in\nhere but keep in mind hive and hadoop is\ndesigned for writing many files into the\ndatabase you write them all in there and\nthen you can they're saved it's an\narchive it's a data warehouse and then\nyou're able to do all your queries on\nthem so a lot of times we're not looking\nat just the one file coming up we're\nloading hundreds of files you have your\nreports coming off of your main database\nall those reports are being loaded you\nhave your log files you have i mean all\nthis different data is being dumped into\nhadoop and in this case hive on top of\nhadoop and so we need to let it know hey\nhow do i handle these files coming in\nand then we have the semicolon at the\nend which lets us know to go ahead and\nrun this line and so we'll go ahead and\nrun that and now if we do a show tables\nyou can see there's our employee on\nthere we can also describe if we do\ndescribe employee you can see that we\nhave our id integer name string\ndepartment string year of joining\ninteger and salary integer and then\nfinally let's just do a select star from\nemployee very basic sql nhql command\nselecting data it's going to come up and\nwe haven't put anything in it so as we\nexpect there's no data in it so if we\nflip back to our\nlinux terminal window you can see where\nwe did the cat\nemployee.csv and you can see all the\ndata we expect to come into it and we\nalso did our pwd and right here you see\nthe path you need that full path when\nyou are loading data you know you can do\na browse and if i did it right now with\njust the employee.csv as a name it will\nwork but that is a really bad habit in\ngeneral when you're loading data because\nit's you don't know what else is going\non in the computer you want to do the\nfull path almost in all your data loads\nso let's go ahead and flip back over\nhere to our hive shell we're working in\nand the command for this is load data so\nthat says hey we're loading data that's\na hive command hql and we want local\ndata so you got to put down local in\npath so now it needs to know where the\npath is now to make this more legible\ni'm just going to go ahead and hit enter\nthen we'll just paste the full path in\nthere which i have stored over on the\nside like a good prepared demo and\nyou'll see here we have home cloudera\ndocuments employee.csv so it's a whole\npath for this text document in here and\nwe go ahead and hit enter in there and\nthen we have to let it know where the\ndata is going so now we have a source\nand we need a destination and it's going\nto go into the table and we'll just call\nit employee we'll just match the table\nin there and because i wanted to execute\nwe put the semicolon on the end it goes\nahead and executes all three lines now\nif we go back if you remember we did the\nselect star from employee just using the\nup error to page through my different\ncommands i've already typed in you can\nsee right here we have as we expect we\nhave rows sam mike and nick and we have\nall their information showing in our\nfour rows and then let's go ahead and do\nuh select\nand count let's look at a couple of\nthese different select options you can\ndo we're going to count everything from\nemployee now this is kind of interesting\nbecause the first one just pops up with\nthe basic select because it doesn't need\nto go through the full map reduce phase\nbut when you start doing a count it does\ngo through the full map redo setup in\nthe hive in hadoop and because i'm doing\nthis demo on a single node cloudera\nvirtual box on top of a windows 10 all\nthe benefits of running it on a cluster\nare gone and instead it's now going\nthrough all those added layers so it\ntakes longer to run you know like i said\nwhen you do a single node as i said\nearlier it doesn't do any good as an\nactual distribution because you're only\nrunning it on one computer and then\nyou've added all these different layers\nto run it and we see it comes up with\nfour and that's what we expect we have\nfour rows we expect four at the end and\nif you remember from\nour cheat sheet which we brought up here\nfrom hortons it's a pretty good one\nthere's all these different commands we\ncan do we'll look at one more command\nwhere we do the\nwhat they call sub queries right down\nhere because that's really common to do\na lot of sub queries and so we'll do\nselect\nstar or all different columns from\nemployee now if we weren't using the\noffice database it would look like this\nfrom office dot employee and either one\nwill work on this particular one because\nwe have office set as a default on there\nso from office employee and then the\ncommand where creates a subset and in\nthis case we want to know where the\nsalary is greater than 25\n000. there we go and of course we end\nwith our semicolon and if we run this\nquery you can see it pops up and there's\nour salaries of people top earners uh we\nhave rose and i t and mike and hr kudos\nto them of course they're fictional i\ndon't actually we don't actually have a\nrose and a mic in those positions or\nmaybe we do so finally we want to go\nahead and do is we're done with this\ntable now remember you're dealing with\nthe data warehouse so you usually don't\ndo a lot of dropping of tables and\ndatabases but we're going to go ahead\nand drop this table here before we drop\nit one more quick note is we can change\nit so what we're going to do is we're\ngoing to alter table office employee and\nwe want to go ahead and rename it\nthere's some other commands you can do\nin here but rename is pretty common and\nwe're going to rename it to\nand it's going to stay in office and uh\nturns out one of our\nshareholders really doesn't like the\nword employee he wants employees plural\nit's a big deal to him so let's go ahead\nand change that name for the table it's\nthat easy because it's just changing the\nmetadata on there and now if we do show\ntables you'll see we now have employees\nnot employee and then at this point\nmaybe we're doing some house cleaning\nbecause this is all practice so we're\ngoing to go ahead and drop table and\nwe'll drop table employees because we\nchanged the name in there so if we did\nemployee just give us an error and now\nif we do show tables you'll see all the\ntables are gone now the next thing we\nwant to go and take a look at and we're\ngoing to walk back through the loading\nof data just real quick because we're\ngoing to load two tables in here and let\nme just float back to our terminal\nwindow so we can see what those tables\nare that we're loading and so up here we\nhave customer we have a customer\nfile and we have an order file we want\nto go ahead and put the customers and\nthe orders into here so those are the\ntwo we're doing and of course it's\nalways nice to see what you're working\nwith so let's do our cat\ncustomer.csv we could always do g edit\nbut we don't really need to edit these\nwe just want to take a look at the data\nin customer and important in here is\nagain we have a header so we have to\nskip a line comma separated uh nothing\nodd with the data we have our schema\nwhich is\ninteger string integer string integer so\nyou know you'd want to take that note\nthat down or flip back and forth when\nyou're doing it and then let's go ahead\nand do cat order dot csv and we can see\nwe have oid which i'm guessing is the\norder id we have a date up something new\nwe've done integers and strings but we\nhaven't done date when you're importing\nnew and you never worked with the date\ndate's always one of the more trickier\nfields to port in when that's true of\njust about any scripting language i've\nworked with all of them have their own\nidea of how date's supposed to be\nformatted what the default is this\nparticular format or it's year and it\nhas all four uh digits dash month two\ndigits dash day is the standard import\nfor the hive so you'll have to look up\nand see what the different formats are\nif you're going to do a different format\nand they're coming in or you're not able\nto pre-process the data but this would\nbe a pre-processing of the data thing\ncoming in if you remember correctly from\nour edel which is uh e just in case you\nweren't able to hear me last time etl\nwhich stands for extract transform then\nload so you want to make sure you're\ntransforming this data before it gets\ninto here and so we're going to go ahead\nand bring\nboth this data in here and really we're\ndoing this so we can show you the basic\njoin there is if you remember from our\nsetup merge join all kinds of different\nthings you can do but joining different\ndata sets is so common so it's really\nimportant to know how to do this we need\nto go ahead and bring in these two data\nsets and you can see where i just\ncreated a table customer here's our\nschema the integer name age address\nsalary here's our eliminated by commas\nand our table properties where we skip a\nline well let's go ahead and load the\ndata first and then we'll do that with\nour order and let's go ahead and put\nthat in here and i've got it split into\nthree lines so you can see it easily\nwe've got load data local in path so we\nknow we're loading data we know it's\nlocal and we have the path here's the\ncomplete path for\noops this is supposed to be order csv\ngrab the wrong one of course it's going\nto give me errors because you can't\nrecreate the same table on there and\nhere we go create table here's our\ninteger date customer the basic setup\nthat we had coming in here for our\nschema row format commas table\nproperties skip header line and then\nfinally let's load the data into\nour order table load data local in path\nhome cloudera documents order.csv into\ntable order now if we did everything\nright we should be able to do select\nstar from customer and you can see we\nhave all seven customers and then we can\ndo select star from order and we have uh\nfour orders uh so this is just like a\nquick frame we have a lot of times when\nyou have your customer databases in\nbusiness you have thousands of customers\nfrom years and years and some of them\nyou know they move they close their\nbusiness they change names all kinds of\nthings happen uh so we want to do is we\nwant to go ahead and find just the\ninformation connected to these orders\nand who's connected to them and so let's\ngo ahead and do it's a select because\nwe're going to display information so\nselect and this is kind of interesting\nwe're going to do c dot id\nand i'm going to define c as customer as\na customer table in just a minute then\nwe're going to do c dot name and again\nwe're going to define the c c dot age so\nthis means from the customer we want to\nknow their id their name their age and\nthen you know i'd also like to know the\norder amount uh so let's do o for dot\namount and then this is where we need to\ngo ahead and define uh what we're doing\nand i'm going to capitalize from\ncustomer so we're going to take the\ncustomer table in here and we're going\nto name it c that's where the c comes\nfrom so that's the customer table c and\nwe want to join order as o that's where\nour o comes from so the o dot amount is\nwhat we're joining in there and then we\nwant to do this on we got to tell it how\nto connect the two tables c dot id\nequals o dot customer underscore id so\nnow we know how they're joined and\nremember we have seven customers in here\nwe have four orders and as a processes\nwe should get a return of four different\nnames joined together and they're joined\nbased on of course the orders on there\nand once we're done we now have the\norder number the person who made the\norder their age and the amount of the\norder which came from the order table uh\nso you have your different information\nand you can see how the join works here\nvery common use of tables and hql and\nsql and let's do one more thing with our\ndatabase and then i'll show you a couple\nother hive commands\nand let's go ahead and do a drop and\nwe're going to drop database office\nand if you're looking at this and you\nremember from earlier this will give me\nan error and let's just see what that\nlooks like it says fail to execute\nexception one or more tables exist so if\nyou remember from before you can't just\ndrop a database unless you tell it to\ncascade that lets it know i don't care\nhow many tables are in it let's get rid\nof it and in hadoop since it's an art\nit's a warehouse a data warehouse you\nusually don't do a lot of dropping uh\nmaybe at the beginning when you're\ndeveloping the schemas and you realize\nyou messed up you might drop some stuff\nuh but down the road you're really just\nadding commodity machines to take up so\nyou can store more stuff on it so you\nusually don't do a lot of database\ndropping and some other uh fun commands\nto know is you can do select round 2.3\nis round value you can do a round off in\nhive we can do as floor value which is\ngoing to give us a 2 so it turns it into\nan integer versus a float it goes down\nyou know basically truncates it but it\ngoes down and we can also do ceiling\nwhich is going to round it up so we're\nlooking for the next integer above\nthere's a few commands we didn't show in\nhere because we're on a single node as\nas an admin to help spediate the process\nyou usually add in partitions for the\ndata and buckets you can't do that on a\nsingle node because the when you add a\npartition it partitions it across\nseparate nodes but beyond that you can\nsee that it's very straightforward we\nhave sql coming in and all your basic\nqueries that are in sql are very similar\nto hql let's get started with pig why\npig what is pig mapreduce versus hive\nversus pig hopefully you've had a chance\nto do our hive tutorial and our\nmapreduce tutorial if you haven't send a\nnote over to simplylearn and we'll\nfollow up with a link to you we'll look\nat pig architecture working a pig pig\nlatin data model pig execution modes a\nuse case twitter and features a pig and\nthen we'll tag on a short demo so you\ncan see pig in action so why pig as we\nall know hadoop uses mapreduce to\nanalyze and process big data processing\nbig data consumed more time so before we\nhad the hadoop system they'd have to\nspend a lot of money on a huge set of\ncomputers and enterprise machines so he\nintroduced the hadoop map reduce and so\nafterwards processing big data was\nfaster using mapreduce then what is the\nproblem with map reduce prior to 2006\nall mapreduce programs were written in\njava non-programmers found it difficult\nto write lengthy java codes they faced\nissues in incorporating maps sort\nreduced to fundamentals of mapreduce\nwhile creating a program you can see\nhere map face shuffle and sort reduce\nphase eventually it became a difficult\ntask to maintain and optimize a code due\nto which the processing time increased\nyou can imagine a manager trying to go\nin there and needed a simple query to\nfind out data and he has to go talk to\nthe programmers anytime he wants\nanything so that was a big problem not\neverybody wants to have a on-call\nprogrammer for every manager on their\nteam yahoo faced problems to process and\nanalyze large data sets using java as\nthe codes were complex and lengthy there\nwas a necessity to develop an easier way\nto analyze large data sets without using\ntime-consuming complex java modes and\ncodes and scripts and all that fun stuff\napache pig was developed by yahoo it was\ndeveloped with the vision to analyze and\nprocess large datasets without using\ncomplex java codes pig was developed\nespecially for non-programmers pig used\nsimple steps to analyze data sets which\nwas time efficient so what exactly is\npik pig is a scripting platform that\nruns on hadoop clusters designed to\nprocess and analyze large data sets and\nso you have your pig which uses sql like\nqueries they're definitely not sql but\nsome of them resemble sql queries and\nthen we use that to analyze our data pig\noperates on various types of data like\nstructured semi-structured and\nunstructured data let's take a closer\nlook at map reduce versus hive versus\npig so we start with a compiled language\nyour mapreduce and we have hive which is\nyour sql like query and then we have pig\nwhich is a scripting language it has\nsome similarities to sql but it has a\nlot of its own stuff remember sql like\nquery which is what hive is based off\nlooks for structured data and so when\nyou get into scripting languages like\npig now we're dealing more with\nsemi-structured and even unstructured\ndata with a hadoop map reduced we have a\nneed to write long complex codes with\nhive no need to write complex codes you\ncould just put it in a simple sql query\nor hql hive ql and in pig no need to\nwrite complex codes as we have pig latin\nnow remember in the map reduce it can\nproduce structured semi-structured and\nunstructured data and as i mentioned\nbefore hive can process only structured\ndata think rows and columns where pig\ncan process structured semi-structured\nand unstructured data you can think of\nstructured data as rows and columns\nsemi-structured as your html xml\ndocuments like you have on your web\npages and unstructured could be anything\nfrom groups of documents and written\nformat twitter tweets any of those\nthings come in as very unstructured data\nand with our hadoop map reduce we have a\nlower level of abstraction with both\nhive and pig we have a higher level\nabstraction so it's much more easy for\nsomeone to use without having to dive in\ndeep and write a very lengthy map reduce\ncode and those map and reduce codes can\ntake 70 80 lines of code when you can do\nthe same thing in one or two lines with\nhive or pig this is the advantage pig\nhas over hive it can process only\nstructured data and hive while in pig it\ncan process structured semi-structured\nand unstructured data some other\nfeatures to note that separates the\ndifferent query languages as we look at\nmap and reduce mapreduce supports\npartitioning features as does hive pig\nno concept of partitioning in pix it\ndoesn't support your partitioning\nfeature your partitioning features allow\nyou to partition the data in such a way\nthat it can be queried quicker you're\nnot able to do that in pig mapreduce\nuses java and python while hive uses an\nsql like query language known as hive ql\nor hql pig latin is used which is a\nprocedural data flow language mapreduce\nis used by programmers pretty much as\nstraightforward on java hive is used by\ndata analysts pig is used by researchers\nand programmers certainly there's a lot\nof mix between all three programmers\nhave been known to go in and use a hive\nfor quick query and anybody's been able\nto use pig for quick query or research\nunder map and reduce code performance is\nreally good under hive code performance\nis lesser than map and reduce and pig\nunder pig code performance is lesser\nthan mapreduce but better than hive so\nif we're going to look at speed and time\nthe map reduce is going to be the\nfastest performance on all of those\nwhere pig will have second and high\nfollows in the back let's look at\ncomponents of pig pig has two main\ncomponents we have pig latin pig latin\nis the procedural data flow language\nused in pig to analyze data it is easy\nto program using piglet and it is\nsimilar to sql and then we have the\nruntime engine runtime engine represents\nthe execution environment created to run\npig latin programs it is also a compiler\nthat produces mapreduce programs uses\nhdfs or your hadoop file system for\nstoring and retrieving data and as we\ndig deeper into the pig architecture\nwe'll see that we have pig latin scripts\nprogrammers write a script in piglet to\nanalyze data using pig then you have the\ngrunt shell and it actually says grunt\nwhen we start it up and we'll show you\nthat here in a little bit which goes\ninto the pig server and this is where we\nhave our parser parser checks the syntax\nof the pig script after checking the\noutput will be a dag directed acylic\ngraph and then we have an optimizer\nwhich optimizes after your dag your\nlogical plan is passed to the logical\noptimizer where an optimization takes\nplace finally the compiler converts the\ndag into map reduce jobs and then that\nis executed on the map reduce under the\nexecution engine the results are\ndisplayed using dump statement and\nstored in hdfs using store statement and\nagain we'll show you that\nthe kind of end you always want to\nexecute everything once you've created\nit and so dump is kind of our execution\nstatement and you can see right here as\nwe were talking about earlier once we\nget to the execution engine and it's\ncoded into mapreduce then the map reduce\nprocesses it onto the hdfs\nworking of pig pig latin script is\nwritten by the users so you have load\ndata and write pig script and pig\noperations so we look at the working of\npig pig latin script is written by the\nusers there's step one we load data and\nwrite pig script and step two in this\nstep all the pig operations are\nperformed by parser optimizer and\ncompiler so we go into the pig\noperations and then we get to step three\nexecution of the plan in this days the\nresults are shown on the screen\notherwise stored in the hdfs as per the\ncode so it might be of a small amount of\ndata you're reducing it to and you want\nto put that on the screen or you might\nbe converting a huge amount of data\nwhich you want to put back into the\nhadoop file system for other use let's\ntake a look at the pig latin data model\nthe data model of pig latin helps pig to\nhandle various types of data for example\nwe have adam rob or 50. atom represents\nany single value of primitive data type\nin pig latin like integer float string\nit is stored as a string tuple so we go\nfrom our atom which are most basic\nthings so if you look at just rob or\njust 50 that's an atom that's our most\nbasic object we have in pig latin then\nyou have a tuple tuple represents\nsequence of fields that can be of any\ndata type it is the same as a row in\nrdbms for example a set of data from a\nsingle row and you can see here we have\nrob comma five and you can imagine with\nmany of our other examples we've used we\nmight have the id number the name where\nthey live their age their date of\nstarting the job that would all be one\nrow and stored as a tuple and then we\ncreate a bag a bag is a collection of\ntuples it is the same as a table in\nrdbms and is represented by brackets and\nyou can see here we have our table with\nrob5 mic 10 and we also have a map a map\nis a set of key value pairs key is of\ncharacter array type and a value can be\nof any type it is represented by the\nbrackets and so we have name and age\nwhere the key value is mic and 10. pig\nlatin has a fully nestable data model\nthat means one data type can be nested\nwithin another here's a diagram\nrepresentation of pig latin data model\nand in this particular example we have\nbasically an id number a name and age\nand a place and we break this apart we\nlook at this model from pig latin\nperspective we start with our field and\nif you remember a field contains\nbasically an atom it is one\nparticular data type and the atom is\nstored as a string which then converts\nit into either an integer or number or\ncharacter string next we have our tuple\nand in this case you can see that it\nrepresents a row so our tuple would be\nthree comma joe comma 29 comma\ncalifornia and finally we have our bag\nwhich contains three rows in it in this\nparticular example let's take a quick\nlook at pig execution modes pig works in\ntwo execution modes depending on where\nthe data is reciting and where the pig\nscript is going to run we have local\nmode here the pig engine takes input\nfrom the linux file system and the\noutput is stored in the same file system\nlocal mold local mode is useful in\nanalyzing small data sets using pig and\nwe have the map reduced mode here the\npig engine directly interacts and\nexecutes in hdfs and mapreduce in the\nmap reduce mode queries written in pig\nlatin are translated into mapreduce jobs\nand are run on a hadoop cluster by\ndefault pig runs in this mode there are\nthree modes in pig depending on how a\npig latin code can be written we have\nour interactive mode batch mode and\nembedded mode the interactive mode means\ncoding and executing the script line by\nline when we do our example we'll be in\nthe interactive mode in badge mode all\nscripts are coded in a file with the\nextension.pig and the file is directly\nexecuted and then there's embedded mode\npig lets its users define their own\nfunctions udfss in a programming\nlanguage such as java so let's take a\nlook and see how this works in a use\ncase in this case use case twitter users\non twitter generate about 500 million\ntweets on a daily basis the hadoop\nmapreduce was used to process and\nanalyze this data analyzing the number\nof tweets created by a user in the tweet\ntable was done using mapreduce and java\nprogramming language and you can see the\nproblem it was difficult to perform\nmapreduce operations as users were not\nwell versed with written complex java\ncodes so twitter used apache pig to\novercome these problems and let's see\nhow let's start with the problem\nstatement analyze the user table and\ntweet table and find out how many tweets\nare created by a person and here you can\nsee we have a user table we have alice\ntim and john with their id numbers one\ntwo three and we have a tweet table in\nthe tweet table you have your um the id\nof the user and then what they tweeted\nuh google was a good whatever it was\ntennis dot dot spacecraft olympics\npolitics whatever they're tweeting about\nthe following operations were performed\nfor analyzing given data first the\ntwitter data is loaded into the pig\nstorage using load command and you can\nsee here we have our data coming in and\nthen that's going into pig storage and\nthis data is probably on enterprise\ncomputer so this is actually active\ntwitter's going on and then it goes into\nhadoop file system remember the hadoop\nfile system is a data warehouse for\nstoring data and so the first step is we\nwant to go ahead and load it into the\npig storage into our data storage system\nthe remaining operations performed are\nshown below in join and group operation\nthe tweet and user tables are joined and\ngrouped using co-group command and you\ncan see here where we add a whole column\nwhen we go from\nuser names and tweet to the id link\ndirectly to the name so alice was user 1\n10 was 2 and john 3. and so now they're\nlisted with their actual tweet the next\noperation is the aggregation the tweets\nare counted according to the names the\ncommand used is count so it's very\nstraightforward we just want to count\nhow many tweets each user is doing and\nfinally the result after the count\noperation is joined with the user table\nto find out the username and you can see\nhere where alice had 3 tim 2 and john 1.\npig reduces the complexity of the\noperations which would have been lengthy\nusing mapreduce in joining group\noperation the tweet and user tables are\njoined and grouped using co-group\ncommand the next operation is the\naggregation the tweets are counted\naccording to the names the command used\nis count the result after the count\noperation is joined with the user table\nto find out the username and you can see\nwe're talking about three lines of\nscript versus a mapreduce code of about\n80 lines finally we could find out the\nnumber of tweets created by a user in a\nsimple way so let's go quickly over some\nof the features of pig that we already\nwent through most of these\nfirst ease of programming is pig latin\nis similar to sql lesser lines of code\nneed to be written short development\ntime as the code is simpler so we can\nget our queries out rather quickly\ninstead of having to have a programmer\nspend hours on it handles all kinds of\ndata like structured semi-structured and\nunstructured pig lets us create user\ndefined functions pig offers a large set\nof operators such as join filter and so\non it allows for multiple queries to\nprocess in parallel and optimization and\ncompilation is easy as it is done\nautomatically and internally\nso enough theory let's dive in and show\nyou a quick demo on some of the commands\nyou can do in pick today's setup will\ncontinue as we have in the last three\ndemos to go and use cloudera quickstart\nand we'll be doing this in virtualbox we\ndo have a tutorial in setting that up\nyou can send a note to our simplylearn\nteam and then get that linked to you\nonce your cloudera quickstart has spun\nup and remember this is virtualbox we've\ncreated a virtual machine and this\nvirtual machine is centos linux once\nit's spun up you'll be in a full linux\nsystem here and as you can see we have\nthunderbird browser which opens up to\nthe hadoop basic system browser and we\ncan go underneath the hue where it comes\nup by default if you click on the pull\ndown menu and go under editor you can\nsee there's our impala our hive\npig along with a bunch of other query\nlanguages you can use and we're going\nunder pig and then once you're in pig we\ncan go ahead and use our command line\nhere and just click that little blue\nbutton to start it up and running we\nwill actually be working in terminal\nwindow and so if you're in the cloud era\nquick start you can open up the terminal\nwindow up top or if you're in your own\nsetup and you're logged in you can\neasily use all of your commands here in\nterminal window and we'll zoom in that\nway you get a nice view of what's going\non there we go now for our first command\nwe're going to do a hadoop command and\nimport some data into the hadoop system\nin this case a pig input let's take a\nlook at this we have a hadoop now let's\nknow it's going to be a hadoop command\ndfs there's actually four variations of\ndfs so if you have hdfs or whatever\nthat's fine all four of them point used\nto be different setups underneath\ndifferent things and now they all do the\nsame thing and we want to put this file\nwhich in this case is under home\ncloudera documents and sample and we\njust want to take that and put it into\nthe pig input now let's take a look at\nthat file if i go under my document\nbrowsers and open this up you'll see\nit's got a simple id name profession and\nage we have one jack engineer 25 and\nthat was in one of our earlier things we\nhad in there and so let's go ahead and\nhit enter and execute this and now we've\nuploaded that data and it's gone into\nour pig input and then a lot of the\nhadoop commands mimic the linux commands\nand so you'll see we have cat as one of\nour commands or it has a hyphen before\nit so we execute that with hadoop dfs\nhyphen cat slash pig input because\nthat's what we called it that's where we\nput our sample csv at and we execute\nthis you can see from our hadoop system\nit's going to go in and pull that up and\nsure enough it pulls out the data file\nwe just put in there and then we can\nsimply enter the pig latin or pig editor\nmode by typing in pig and we can see\nhere by our grunt i told you that's how\nit was going to tell you we're in pig\nlatin there's our grunt command line so\nwe are now in the pig shell and then\nwe'll go ahead and put our load command\nin here and the way this works is i'm\ngoing to have office equals load and\nhere's my load in this case it's going\nto be pig input we have that in single\nbrackets you remember that's where the\ndata is in the hadoop file system where\nwe dumped it into there we're going to\nusing pig storage our data was separated\nas with a comma so there's our comma\nseparator and then we have as in this\ncase we have an id character array name\ncharacter array profession character a\nand age character ray and we're just\ngoing to do them all as character arrays\njust to keep this simple for this one\nand then when i hit put this all in here\nyou can see that's our full command line\ngoing in and we have our semicolon at\nthe end so when i hit enter it's now set\noffice up but it hasn't actually done\nanything yet it doesn't do anything\nuntil we do dump office so there's our\ncommand to execute whatever we've loaded\nor whatever setup we have in here and we\nrun that you can see it go through the\ndifferent languages and this is going\nthrough the map reduce remember we're\nnot doing this locally we're doing this\non the hadoop setup and once we finished\nour dump you can see we have id name\nprofession age and all the information\nthat we just dumped into our pick oh we\ncan now do let's say\noh let's say we have a request just for\nwe'll keep it simple in here but just\nfor the name and age and so we can go\noffice we'll call it each as our\nvariable underscore each and we'll say\nfor each office\ngenerate name come h and for each means\nthat we're going to do this for each row\nand if you're thinking map reduce you\nknow that this is a map function because\nit's mapping each row and generating\nname and age on here and of course we\nwant to go ahead and close it with a\nsemicolon and then once we've created\nour query or the command line in here\nlet's go ahead and dump office\nunderscore each in with our semicolon\nand this will go through our reduce\nsetup on here and if we were on a large\ncluster the same processing time would\nhappen in fact it's really slow because\ni have multiple things on this computer\nand this particular virtual box is only\nusing a quarter of my processor it's\nonly dedicated to this and you can see\nhere there it is name and age and it\nalso included the top row since we\ndidn't delete that out of there or tell\nit not to and that's fine for this\nexample but you need to be aware of\nthose things when you're processing a\nsignificantly large amount of data or\nany data then we can also do office and\nwe'll call this dsc for descending so\nmaybe the boss comes to you and says hey\ncan we order office by id descending and\nof course your boss you've taught them\nhow to uh your shareholder sounds a\nlittle drug-attorney say boss you've\ntalked to the shareholder and you said\nand you've taught them a little bit of\npig latin and they know that they can\nnow create office description and we can\norder office by id description and of\ncourse once we do that we have to dump\noffice underscore description so that\nit'll actually execute and there goes\ninto our map reduce it'll take just a\nmoment for it to come up because again\ni'm running on only a quarter of my\nprocessor and you can see we now have\nour ids in descending order returned\nlet's also look at and this is so\nimportant with anytime you're dealing\nwith big data let's create office with a\nlimit and you can of course do any of\nthis instead of with office we could do\nthis with office descending so you get\njust the top two ids on there we're\ngonna limit just to two and of course to\nexecute that we have to dump office\nunderscore limit and you can just think\nof dumping your garbage into the pig pin\nfor the pig to eat there we go dump\noffice limit two and that's going to\njust limit our office to the top two and\nfor our output we get our first row\nwhich had our id name profession and age\nand our second row which is jack who's\nan engineer let's do an uh filter we'll\ncall it office underscore filter you\nguessed it equals filter office by\nprofession equals and keep note this is\nuh similar to how python does it with\nthe double equal signs for equal for\ndoing a true false statement so for your\nlogic statement remember to use two\nequal signs in pig and we're going to\nsay it equals doctor so we want to find\nout how many doctors do we have on our\nlist and we'll go ahead and do our dump\nwe're dumping all our garbage into the\npig pen and we're letting pig take over\nand see what it can find out and see\nwho's a doctor on our list and we find\nuh employee id number two bob is a\ndoctor 30 years old for this next\nsection uh we're going to cover\nsomething we see a lot nowadays in data\nanalysis and that's word counting\ntokenization that is one of the next big\nsteps as we move forward in our data\nanalysis where we go from say stock\nmarket analysis of highs and lows and\nall the numbers to what are people\nsaying about companies on twitter what\nare they saying on the web pages and on\nfacebook suddenly you need to start\ncounting words and finding out how many\nwords are totaled how many are in the\nfirst part of the document and so on\nwe're going to cover a very basic word\ncount\nexample and in this case i've created a\ndocument called wordrose.txt\nand you can see here we have simplylearn\nis a company supporting online learning\nsimplylearn helps people attain their\ncertifications simply learn is an online\ncommunity i love simply learn i love\nprogramming i love data analysis and i\nwent and saved this into my documents\nfolder so we could use it and let me go\nahead and open up a new terminal window\nfor our word count let me go and close\nthe old one so we're going to go in here\nand instead of doing this as pig we're\ngoing to do pig minus x local and what\ni'm doing is i'm telling the pig to\nstart the pig shell but we're going to\nbe looking at files local to our\nvirtualbox or this centos machine and\nlet me go ahead and hit enter on there\njust maximize this up there we go and\nit'll load pig up and it's going to look\njust the same as the pig we're doing\nwhich was defaulted to hi to our hadoop\nsystem to our hdfs this is now defaulted\nto the local system now we're going to\ncreate lines we're going to load it\nstraight from the file remember last\ntime we took the hdfs and loaded it into\nthere and then loaded it into pig since\nwe're gone the local we're just going to\nrun a local script we have lines equals\nload home the actual full path home\ncloudera documents and i called it\nwordrose.txt\nand as line is a character array so each\nline and i've actually you can change\nthis to read each document i certainly\nhave done a lot of document analysis and\nthen you go through and do word counts\nand different kind of counts in there so\nonce we go ahead and create our line\ninstead of doing the dump we're going to\ngo ahead and start entering all of our\ndifferent setups for each of our steps\nwe want to go through and let's just\ntake a look at this next one because the\nload is straightforward we're loading\nfrom this particular file since we're\nlocals loading it directly from here\ninstead of going into the hadoop file\nsystem and it says as and then each line\nis read as a character array now we're\ngoing to do words equal for each of the\nlines generate flat tokenize line space\nas word now there's a lot of ways to do\nthis this is if you're a programmer\nyou're just splitting the line up by\nspaces there's actual ways to tokenize\nit you look for periods capitalization\nthere's all kinds of other things you\nplay with with this but for the most\nbasic word count we're just going to\nseparate it by spaces the\nflatten takes the line and just creates\na it flattens each of the words out so\nthis is we're just going to generate a\nbunch of words for each line and then\neach each of those words is as a word a\nlittle confusing in there but if you\nreally think about it we're just going\ndown each line separating it out and\nwe're generating a list of words one\nthing to note is the default for\ntokenize you can just do tokenized line\nwithout the space in there if you do\nthat it will automatically tokenize it\nby space you can do either one and then\nwe're going to do group we're going to\ngroup it by words so we're going to\ngroup words by word so when we we split\nit up each token is a word and it's a\nlist of words and so we're going to\ngroup equals group words by words so\nwe're going to group all the same words\ntogether and if we're going to group\nthem then we want to go ahead and count\nthem and so for count we'll go ahead and\ncreate a word count variable and here's\nour four each so for each grouped\ngrouped is our line where we group all\nthe words in the line that are similar\nwe're going to generate a group and then\nwe're going to count the words for each\ngroup so for each line we group the\nwords together we're going to generate a\ngroup and that's going to count the\nwords we want to know the word count in\neach of those and that comes back in our\nword count and finally we want to take\nthis and we want to go ahead and dump\nword count and this is a little bit more\nwhat you see when you start looking at\ngrunt scripts you'll see right here\nthese these lines right here we have\neach of the steps you take to get there\nso we load our file for each of our\nlines we're going to generate and\ntokenize it into words then we're going\nto take the words and we're going to\ngroup them by same words for each group\nwe're going to generate a group and\nwe're just going to count the words so\nwe're going to summarize all the words\nin here and let's go ahead and do our\ndump word count which executes all this\nand it goes through our map reduce it's\nactually a local runner you'll see down\nhere you start seeing where they still\nhave map reduced but it's a special\nrunner we're mapping it that's a part of\neach row being counted and grouped and\nthen when we do the word count that's a\nreducer the reducer crazy's keys and you\ncan see i is used three times a came up\nonce and came up once is to continue on\ndown here to attain online people\ncompany analysis simply learn they took\nthe top rating with four certification\nso all these things are then counted in\nthe how many words are used in in data\nanalysis this is probably the very the\nbeginnings of data analysis where you\nmight look at it and say oh they\nmentioned love\nthree times so whatever's going on in\nthis post it's about love and\nwhat do they love and then you might\nattach that to the different objects in\nhere so you can see that pig latin is\nfairly easy to use there's nothing\nreally you know it takes a little bit to\nlearn the script uh depending on how\ngood your memory is as i get older my\nmemory leaks a little bit more so i\ndon't memorize it as much but that was\npretty straightforward the script we put\nin there and then it goes through the\nfull map reduce localized run comes out\nand like i said it's very easy to use\nthat's why people like pig latin is\nbecause it's intuitive one of the things\ni like about pig latin is when i'm\ntroubleshooting when we're\ntroubleshooting a lot of times you're\nworking with a small amount of data and\nyou start doing one line at a time and\nso i can go lines equal load and there's\nmy loaded text and maybe i'll just dump\nlines and then it's going to run it's\ngoing to show me all the lines that i'm\nworking on in the small amount of data\nand that way i can test that if i got an\nerror on there that said oh this isn't\nworking maybe i'll be like oh my gosh\ni'm in map reduce or i'm in the basic\ngrunt shell instead of the local path\ngrunt let's start with an introduction\nto hbase back in the days data used to\nbe less and was mostly structured you\nsee we have structured data here we\nusually had it like in a database where\nyou had uh every field was exactly the\ncorrect length so if you had a name\nfield exactly 32 characters remember the\nold access database and microsoft the\nfiles are small if we had you know\nhundreds of people in one database that\nwas considered big data this data could\nbe easily stored in relational database\nor rdbms when we talk about relational\ndatabase you might think of oracle you\nmight think of sql microsoft sql mysql\nall of these have evolved even from back\nthen to do a lot more today than they\ndid but they still fall short in a lot\nof ways and they're all examples of an\nrdms or relationship database then\ninternet evolved and huge volumes of\nstructured and semi-structured data got\ngenerated and you can see here with the\nsemi-structured data we have email if\nyou look at my spam folder you know\nwe're talking about all the html pages\nxml which is a lot of time is displayed\non our html and help desk pages json all\nof this really has just even in the last\neach year it almost doubles from the\nyear before how much of this is\ngenerated so storing and processing this\ndata on an rdbms has become a major\nproblem and so the solution is we use\napache age base apache hbase was the\nsolution for this let's take a look at\nthe history the hbase history and we\nlook at the hbase history we're going to\nstart back in 2006 november google\nreleased the paper on big table and then\nin 2017 just a few months later age\nbased prototype was created as a hadoop\ncontribution later on in the year 2007\nin october first usable hbase along with\nthe hadoop .15 was released and then in\njanuary of 2008 hbase became the\nsubproject of hadoop and later on that\nyear in october all the way into\nseptember the next year hbase was\nreleased the 0.81 version the 0.19\nversion and 0.20 and finally in may of\n2010 hbase became apache top level\nproject and so you can see in the course\nof about four years hbase started off as\njust an idea on paper and has evolved\nall the way till 2010 as a solid project\nunder the apache and since 2010 has\ncontinued to evolve and grow as a major\nsource for storing data in\nsemi-structured data so what is hbase\nhbase is a column oriented database\nmanagement system derived from google's\nnosql database bigtable that runs on top\nof the hadoop file system or the hdfs\nit's an open source project that is\nhorizontally scalable and that's very\nimportant to understand that you don't\nhave to buy a bunch of huge expensive\ncomputers you're expanding it by\ncontinually adding commodity machines\nand so it's a linear cost expansion as\nopposed to being exponential no sql\ndatabase written in java which permits\nfaster querying so java is the back end\nfor the hbase setup and it's well suited\nfor sparse data sets so it can contain\nmissing or n a values and this doesn't\nboggle it down like it would in other\ndatabase companies using hbase so let's\ntake a look and see who is using this uh\nnosql database for their servers and for\nstoring their data and we have\nhortonworks which isn't a surprise\nbecause they're one of the like cloudera\nhortonworks they are behind hadoop and\none of the big developments and backing\nof it and of course apache hbase is the\nopen source behind it and we have\ncapital one as banks you also see bank\nof america where they're collecting\ninformation on people and tracking it uh\nso their information might be very\nsparse they might have one bank way back\nwhen they collected information as far\nas the person's family and what their\nincome for the whole family is and their\npersonal income and maybe another one\ndoesn't collect the family income as you\nstart seeing where you have data that is\nuh very difficult to store where it's\nmissing a bunch of data hubspot's using\nit facebook uh certainly all of your\nfacebook twitter most of your social\nmedias are using it and then of course\nthere's jp morgan chase and company\nanother bank that uses the hbase as\ntheir data warehouse for nos sql let's\ntake a look at an hbase use case so we\ncan dig a little bit more into it to see\nhow it functions telecommunication\ncompany that provides mobile voice and\nmultimedia services across china the\nchina mobile and china mobile they\ngenerate billions of call detailed\nrecords or cdr and so these cdrs and all\nthese records are these calls and how\nlong they are and different aspects of\nthe call maybe the tower they're\nbroadcasted from all that is being\nrecorded so they can track it a\ntraditional database systems were unable\nto scale up to the vast volumes of data\nand provide a cost-effective solution no\ngood so storing and real-time analysis\nof billions of call records was a major\nproblem for this company solution apache\nhbase hbase stores billions of rows of\ndetailed call records hbc performs fast\nprocessing of records using sql queries\nso you can mix your sql and nosql\nqueries and usually just say no sql\nqueries because of the way the query\nworks applications of hbase one of them\nwould be in the medical industry hbase\nis used for storing genome sequences\nstoring disease history of people of an\narea and you can imagine how sparse that\nis as far as both of those a genome\nsequence might be only have pieces to it\nthat each person is unique or is unique\nto different people and the same thing\nwith disease you really don't need a\ncolumn for every possible disease a\nperson could get you just want to know\nwhat those diseases those people have\nhad to deal with in that area e-commerce\nhbase is used for storing logs about\ncustomer search history performs\nanalytics and target advertisement for\nbetter business insights sports hba\nstores match details in the history of\neach match uses this data for better\nprediction so when we look at hbase we\nall want to know what's the difference\nbetween hbase versus rdbms that is a\nrelational database management system\nhbase versus rdbms so the hbase does not\nhave a fixed schema it's schema-less\ndefines only column families and we'll\nshow you what that means later on rdbms\nhas a fixed schema which describes the\nstructure of the tables and you can\nthink of this as you have a row and you\nhave columns and each column is a very\nspecific structure how much data can go\nin there and what it does with the hbase\nit works well with structured and\nsemi-structured data with the rdbms it\nworks only well with structured data\nwith the hbase it can have denormalized\ndata it can contain missing or null\nvalues with the rdbms it can store only\nnormalized data now you can still store\na null value in the rdbms but it still\ntakes up the same space as if you're\nstoring a regular value in many cases\nand it also for the hbase is built for y\ntables it can be scaled horizontally for\ninstance if you were doing a tokenizer\nof words and word clusters you might\nhave 1.4 million different words that\nyou're pulling up and combinations of\nwords so with an rdbms it's built for\nthin tables that are hard to scale you\ndon't want to store 1.4 million columns\nin your sql it's going to crash and it's\ngoing to be very hard to do searches\nwith the age base it only stores that\ndata which is part of whatever row\nyou're working on let's look at some of\nthe features of the hbase it's scalable\ndata can be scaled across various nodes\nas it is stored in the hdfs and always\nthink about this it's a linear add-on\nfor each terabyte of data i'm adding on\nroughly a thousand dollars in commodity\ncomputing with an enterprise machine\nwe're looking at about 10 000 at the\nlower end for each terabyte of data and\nthat includes all your backup and\nredundancy so it's a big difference it's\nlike a tenth of the cost to store it\nacross the hbase it has automatic\nfailure support right ahead log across\nclusters which provides automatic\nsupport against failure consistent read\nand write hbase provides consistent read\nand write of the data it's a java api\nfor client access provides easy to use\njava api for clients block cache and\nbloom filters so the hbase supports\nblock caching and bloom filters for high\nvolume query optimization let's dig a\nlittle deeper into the hbase storage\nhbase column oriented storage and i told\nyou we're going to look into this to see\nhow it stores the data and here you can\nsee you have a row key this is really\none of the important references is each\nrow has to have its own key or your row\nid and then you have your column family\nand in here you can see we have column\nfamily one column family two column\nfamily three and you have your column\nqualifiers so you can have in column\nfamily one you can have three columns in\nthere and there might not be any data in\nthat so when you go into column family\none and do a query for every column that\ncontains a certain thing that row might\nnot have anything in there and not be\nqueried where in column family 2 maybe\nyou have column 1 filled out and column\n3 filled out and so on and so forth and\nthen each cell is connected to the row\nwhere the data is actually stored let's\ntake a look at this at what it looks\nlike when you fill the data in so in\nhere we have a row key with a row id and\nwe have our employee id one two three\nthat's pretty straightforward you\nprobably would even have that on an sql\nserver and then you have your column\nfamily this is where it starts really\nseparating out your column family you\nmight have personal data and under\npersonal data you would have name city\nage you might have a lot more than just\nthat you might have number of children\nyou might have degree all those kinds of\ndifferent things that go under personal\ndata and some of them might be missing\nyou might only have the name and the age\nof an employee you might only have the\nname the city and how many children and\nnot the age and so you can see with the\npersonal data you can now collect a\nlarge variety of data and store it in\nthe hbase very easily and then maybe you\nhave a family of professional data your\ndesignation your salary all the stuff\nthat the employee is doing for you in\nthat company let's dig a little deeper\ninto the hbase architecture and so you\ncan see here what looks to be a\ncomplicated chart it's not as\ncomplicated as you think from the apache\na space we have the zookeeper which is\nused for monitoring what's going on and\nyou have your h master this is the hbase\nmaster assigns regions and load\nbalancing and then underneath the region\nor the hbase master then under the\nh-master or h-base master you have your\nread your server serves data for read\nand write and the region server which is\nall your different computers you have in\nyour hadoop cluster you'll have a region\nan h-log you'll have a store memory\nstore and then you have your different\nfiles for h file that are stored on\nthere and those are separated across the\ndifferent computers and that's all part\nof the hdfs storage system so we look at\nthe architectural components or regions\nand we're looking at we're drilling down\na little bit hbase tables are divided\nhorizontally by a row so you have a key\nrange into regions so each of those ids\nyou might have ids 1 to 20 21 to 50 or\nwhatever they are regions are assigned\nto the nodes in the cluster called\nregion servers a region contains all\nrows in the table between the region's\nstart key and the end key again 1 to 10\n11 to 20 and so forth these servers\nserve data for read and write and you\ncan see here we have the client and the\nget and the git sends it out and it\nfinds out where that start if it's\nbetween which start keys and n keys and\nthen it pulls the data from that\ndifferent region server and so the\nregion signed data definition language\noperation create delete are handled by\nthe h-master so the h-master is telling\nit what are we doing with this data\nwhat's going out there assigning and\nreassigning regions for recovery or load\nbalancing and monitoring all servers so\nthat's also part of it so you know if\nyour ids if you have 500 ids across\nthree servers you're not going to put\n400 ids on server 1 and 100 on the\nserver 2 and leaves region 3 and region\n4 empty you're going to split that up\nand that's all handled by the h-master\nand you can see here it monitors region\nservers assigns regions to region\nservers assigns regions to reason\nservers and so forth and so forth hbase\nhas a distributed environment where\nh-master alone is not sufficient to\nmanage everything hence zookeeper was\nintroduced it works with h-master so you\nhave an active h-master which sends a\nheartbeat signal to zookeeper indicating\nthat it's active and the zookeeper also\nhas a heartbeat to the region servers so\nthe region servers send their status to\nzoo keeper indicating they are ready for\nread and write operation inactive server\nacts as a backup if the active hmaster\nfails it will come to the rescue active\nhmaster and region servers connect with\na session to zookeeper so you see your\nactive hmaster selection region server\nsession they're all looking at the\nzookeeper keeping that pulse an active\nh-master region server connects with a\nsession to the zookeeper and you can see\nhere where we have ephemeral nodes for\nactive sessions via heartbeats to\nindicate that the region servers are up\nand running so let's take a look at\nhbase read or write going on there's a\nspecial hbase catalog table called the\nmeta table which holds a location of the\nregions in the cluster here's what\nhappens the first time a client reads or\nwrites data to hbase the client gets the\nregion server the host the meta table\nfrom zookeeper and you can see right\nhere the client has a request for your\nregion server and goes hey zookeeper can\nyou handle this the zookeeper takes a\nlook at it and goes ah metal location is\nstored in zookeeper so it looks at his\nmetadata on there and then the metadata\ntable location is sent back to the\nclient the client will query the meta\nserver to get the region server\ncorresponding to the row key if it wants\nto access the client caches this\ninformation along with the midi table\nlocation and you can see here the client\ngoing back and forth to the region\nserver with the information and it might\nbe going across multiple region servers\ndepending on what you're querying so we\nget the region server for row key from\nthe meta table that's where that row key\ncomes in and says hey this is where\nwe're going with this and so once it\ngets a row key from the corresponding\nregion server we can now put row or git\nrow from that region server let's take a\nlook at the hbase meta table special\nhbase catalog table that maintains a\nlist of all the region servers in the\nhbase storage system so you see here we\nhave the meta table we have a row key\nand a value table key region region\nserver so the meta table is used to find\nthe region for the given table key and\nyou can see down here you know meta\ntable comes in is going to fire out\nwhere it's going with the region server\nand we look a little closer at the right\nmechanism in hbase we have right ahead\nlog or wall as you abbreviate it kind of\na way to remember wall is right ahead\nlog is a file used to store new data\nthat is yet to be put on permanent\nstorage it is used for recovery in the\ncase of failure so you can see here\nwhere the client comes in and it\nliterally puts the new data coming in\ninto this kind of temporary storage or\nthe wall on there once it's gone into\nthe wall then the memory store memstor\nis the right cache that stores a new\ndata that has not yet been written to\ndisk there is one mem store per column\nfamily per region and once we've done\nthat we have three ack once the data is\nplaced in mems store the client then\nreceives the acknowledgement when the\nminister reaches the threshold it dumps\nor commits to data into h file as you\ncan see right here we've taken our our\ngun into the wall the wall then source\nit into the different memory stores uh\nand then the memory stores it says hey\nwe've reached we're ready to dump that\ninto our h files and then it moves it\ninto the h files h files store the rows\nas data as stored key value on disk so\nhere we've done a lot of theory let's\ndive in and just take a look and see\nwhat some of these commands look like\nand what happens in our hbase when we're\nmanipulating a nosql setup\n[Music]\nso if you're learning a new setup it's\nalways good to start with where is this\ncoming from it's open source by apache\nand you can go to\nhbase.apache.org and you'll see that it\nhas a lot of information you can\nactually download the hbase separate\nfrom the hadoop although most people\njust install the hadoop because it's\nbundled with it and if you go in here\nyou'll find a reference guide and so you\ncan go through the apache reference\nguide and there's a number of things to\nlook at but we're going to be going\nthrough apache h base shell that's what\nwe're going to be working with and\nthere's a lot of other interfaces on the\nsetup and you can look up a lot of the\ndifferent commands on here so we go into\nthe apache hbase reference guide we go\ndown to read hbase shell commands from a\ncommand file you can see here where it\ngives you different options of formats\nfor putting the data in and listing the\ndata certainly can also create files and\nscripts to do this too but we're going\nto look at the basics we're going to go\nthrough this on a basic hbase shell and\none last thing to look at is of course\nif you continue down the setup you can\nsee here where they have more detail as\nfar as how to create and how to get to\nyour data on your hbase now i will be\nworking in a virtual box and this is by\noracle you can download the oracle\nvirtualbox you can put a note in below\nfor the youtube as we did have a\nprevious session on setting up virtual\nsetup to run your hadoop system in there\ni'm using the cloudera quick start\ninstalled in here there's hortons you\ncan also use the amazon web service\nthere's a number of options for trying\nthis out in this case we have cloudera\non the oracle virtualbox the virtual box\nhas linux centos installed on it and\nthen the hadoop that has all the\ndifferent hadoop flavors including hbase\nand i bring this up because my computer\nis a windows 10 the operating system of\nthe virtualbox is linux and we're\nlooking at the hbase data warehouse and\nso we have three very different entities\nall running on my computer and that can\nbe confusing if it's the first time in\nand working with this kind of setup now\nyou'll notice in our cloudera setup they\nactually have some hbase monitoring so i\ncan go underneath here and click on\nhbase and master and it'll tell me\nwhat's going on with my region servers\nit'll tell me what's going on with our\nbackup tables right now i don't have any\nuser tables because we haven't created\nany and this is only a single node and a\nsingle hbase tour so you're not going to\nexpect anything too extensive in here\nsince this is for practice and education\nand perhaps testing out package you're\nworking on it's not for really you can\ndeploy cloudera of course but when you\ntalk about a quick start or a single\nnode setup that's what it's really for\nso we can go through all the different\nhbase and you'll see all kinds of\ndifferent information with zookeeper if\nyou saw flash by down here what version\nwe're working in since zookeeper is part\nof the hbase setup where we want to go\nis we want to open up a terminal window\nand in cloudera it happens to be up at\nthe top and when you click on here\nyou'll see your cloudera terminal window\nopen and let me just expand this so we\nhave a nice full screen and then i'm\nalso going to zoom in that way you have\na nice big picture and you can see what\ni'm typing what's going out on and to\nopen up your hbase shell simply type\nhbase shell to get in and hit enter and\nyou'll see it takes just a moment to\nload and we'll be in our hbase shell for\ndoing hbase commands once we've gotten\ninto our hbase shell you'll see it'll\nhave the hbase prompt information ahead\nof it we can do something simple like\nlist this is going to list whatever\ntables we have and it so happens that\nthere's a base table that comes with\nhbase now we can go ahead and create and\ni'm going to type in just create what's\nnice about this is it's going to throw\nme kind of a it's going to say hey\nthere's no just straight create but it\ndoes come up and tell me all these\ndifferent formats we can use for create\nso we can create our table and one of\nour families and add splits names\nversions all kinds of things you can do\nwith this let's just start with a very\nbasic one on here and let's go ahead and\ncreate and we'll call it new\ntable now let's just call it new tbl for\ntable new table and then we also want to\ndo let's do knowledge so let's take a\nlook at this i'm creating a new table\nand it's going to have a family of\nknowledge in it and let me hit enter\nit's going to come up it's going to take\nit a second to go ahead and create it\nnow we have our new table in here so if\ni go list you'll now see table and new\ntable so you can now see that we have\nthe new table and of course the default\ntable that's set up in here and we can\ndo something like uh describe we can\ndescribe and then we're going to do new\ntbl and when we describe it it's going\nto come up it's going to say hey name i\nhave knowledge data block encoding none\nbloom filter row or replication scope\nversion all the different information\nyou need new\nminimum version zero forever deleted\ncells false block size in memory you can\nlook this stuff up on apache.org to\nreally track it down one of the things\nthat's important to note is versions so\nyou have your different versions of the\ndata that's stored and that's always\nimportant to understand that we might\ntalk about that a little bit later on\nand then we have to describe it we can\nalso do a status the status says i have\none active master going on that's our\nhbase as a whole we can do status\nsummary it should do the same thing as\nstatus so we got the same thing coming\nup and now that we've created let's go\nahead and put something in it so we're\ngonna put new tbl and then we want row\none you know what before i even do this\nlet's just type in put and you can see\nwhen i type in put it gives us like a\nlot of different options of how it works\nand different ways of formatting our\ndata as it goes in and all of them\nusually begin with the new table new tbl\nthen we have in this case we'll call it\nrow one and then we'll have knowledge\nremember we created knowledge already\nand we'll do knowledge\nsports and then in knowledge and sports\nwe're going to set that equal to cricket\nso we're going to put underneath this uh\nour knowledge set up that we have a\nthing called sports in there and we'll\nsee what this looks like in just a\nsecond let's go ahead and put in we'll\ndo a couple of these let's see let's do\nanother row one and this time instead of\nsports let's do science you know this\nperson not only you know we have row one\nwhich is both knowledgeable and cricket\nand also in chemistry so it's a chemist\nwho plays cricket in row one and uh\nlet's see if we have let's do another\nrow one just to keep it going and we'll\ndo science in this case let's do physics\nnot only in chemistry but also physicist\ni have quite a joy in physics myself so\nhere we go we have row one there we go\nand then let's do uh row two let's see\nwhat that looks like when we start\nputting in row two and in row two this\nperson is has knowledge in economics\nthis is a master of business and how or\nmaybe it's global economics maybe it's\njust for the business and how it fits in\nwith the country's economics and we call\nit macro economics so i guess it is for\nthe whole country there so we have\nknowledge economics macro economics and\nthen let's just do one more we'll keep\nit as row two and this time our\neconomist is also a musician so we'll\nput music and they happen to have\nknowledge and they enjoy oh let's do pop\nmusic they're into the current pop music\ngoing on so we've loaded our database\nand you'll see we have two rows row one\nand row two in here and we can do is we\ncan list the contents of our database by\nsimply doing scan scan and then let's\njust do scan by itself so you can see\nhow that looks you can always just type\nin there and it tells you all the\ndifferent setups you can do with scan\nand how it works in this case we want to\ndo scan new tbl and in our scan new tbl\nwe have row one row one row two row two\nand you'll see row one has a column\ncalled knowledge science time step value\ncrickets value physics so it has\ninformation as when it was created when\nthe time stamp is row one also has\nknowledge sports and a value of cricket\nso we have sports and science and this\nis interesting because if you remember\nup here we also gave it originally we\ntold it to come in here and have\nchemistry we had science chemistry and\nscience physics and we come down here i\ndon't see the chemistry why because\nwe've now replaced chemistry with\nphysics so the new value is physics on\nhere let me go ahead and clear down a\nlittle bit and in this we're going to\nask the question is enabled new table\nwhen i hit enter in here you're going to\nsee it comes out true and then we'll go\nahead and disable it let's go ahead\ndisable new\ntable make sure i have our quotes around\nit and now that we've disabled it what\nhappens when we do the scan we do the\nscan new table and hit enter you're\ngonna see that we get an error coming up\nso once it's disabled you can't do\nanything with it until we re-enable it\nnow before we enable the table let's do\nan alteration on it and here's our new\ntable and this should look a little\nfamiliar because it's very similar to\ncreate we'll call this test info we'll\nhit enter in there it'll take just a\nmoment for updating and then we want to\ngo ahead and enable it so let's go ahead\nand enable our new table so it's back up\nand running and then we want to describe\ndescribe\nnew table and we come in here you'll now\nsee we have name knowledge and under\nthere we have our data encoding and all\nthe information under knowledge and then\nwe also have down below test info so now\nwe have the name test info and all the\ninformation concerning the test info on\nhere and we'll simply enable it new\ntable so now it's enabled oops already\ndid that i guess we'll enable it twice\nand so let's start looking at well we\nhad scan new table and you can see here\nit brings up the information like this\nwhat if we want to go ahead and get a\nrow so we'll do r1 and when we do hbase\nr1 you can see we have knowledge science\nand it has a time stamp value physics\nand we have knowledge sports and it has\na time stamp on it and value cricket and\nthen let's see what happens when we put\ninto our\nnew\ntable and in here we want row one and if\nyou can guess from earlier because we\ndid something similar uh we're going to\ndo knowledge economics and then it's\ngoing to be a sort of i think it was\nwhat uh macro economics is now market\neconomics and we'll go back and do our\nget command and now see what it looks\nlike and we can see here where we have\nknowledge economics it has a timestamp\nvalue market economics physics and\ncricket and this is because we have\neconomics science and sports those are\nthe three different columns that we have\nand then each one has different\ninformation in it and so if you've\nmanaged to go through all these commands\nand look at basics on here you'll now\nhave the ability to create a very basic\nhbase setup nosql setup based on your\ncolumns and your rows\nthank you guys now that we are done with\nhadoop ecosystem we have shruti who will\ntake us to big data applications\nbefore we move on to the applications\nlet's have a quick look at the big data\nmarket revenue forecast worldwide from\n2011 to 2027.\nso here's a graph in which the y-axis\nrepresents the revenue in billion us\ndollars and the x-axis represents the\nyears as it is seen clearly from the\ngraph big data has grown until 2019 and\nstatistics predict that this growth will\ncontinue even in the future this growth\nis made possible as numerous companies\nuse big data in various domains to boost\ntheir revenue we will look into few of\nsuch applications the first big data\napplication we will look into is weather\nforecast imagine there is a sudden storm\nand you're not even prepared that would\nbe a terrifying situation isn't it\ndealing with any calamities such as\nhurricane storms floods would be very\ninconvenient if we are caught off guard\nthe solution is to have a tool that\npredicts the weather of the coming days\nwell in advance this tool needs to be\naccurate and to make such a tool big\ndata is used so how does big data help\nhere well it allows us to gather all the\ninformation required to predict the\nweather information such as the climate\nchange details wind direction\nprecipitation previous weather reports\nand so on after all this data is\ncollected it becomes easier for us to\nspot a trend and identify what's going\nto happen next by analyzing all of this\nbig data a weather prediction engine\nworks on this analysis it predicts the\nweather of every region across the world\nfor any given time by using such a tool\nwe can be well prepared to face any\nclimate change or any natural calamity\nlet's take an example of a landslide and\ntry to understand how big data is used\nto tackle such a situation predicting a\nlandslide is very difficult with just\nthe basic warning signs lack of this\nprediction can cause a huge damage to\nlife and property this challenge was\nstudied by the university of melbourne\nand they developed a tool which is\ncapable of predicting a landslide this\ntool predicts the boundary where a\nlandslide is likely to occur two weeks\nbefore this magical tool works on both\nbig data and applied mathematics an\naccurate prediction like this which is\nmade two weeks before can save lives and\nhelp in relocating people in that\nparticular region it also gives us an\ninsight into the magnitude of the\nupcoming destruction this is how big\ndata is used in weather forecast and in\npredicting any natural calamities across\nthe world let us now move on to our next\napplication that is big data application\nin the field of media and entertainment\nthe media and the entertainment industry\nis a massive one leveraging big data\nhere can produce sky-high results and\nboost the revenue for any company let us\nsee the different ways in which big data\nis used in this industry have you ever\nnoticed that you come across relevant\nadvertisements in your social media\nsites and in your mailboxes well this is\ndone by analyzing all your data such as\nyour previous browsing history and your\npurchase data publishers then display\nwhat you like in the form of ads which\nwill in turn catch your interest in\nlooking into it next up is customer\nsentiment analysis customers are very\nimportant for a company the happier the\ncustomer the greater the company's\nrevenue big data helps in gathering all\nthe emotions of a customer through their\nposts messages conversations etc these\nemotions are then analyzed to arrive at\na conclusion regarding the customer\nsatisfaction if the customer is unhappy\nthe company strives to do better the\nnext time and provides their customers a\nbetter experience while purchasing an\nitem from an e-commerce site or while\nwatching videos on an entertainment site\nyou might have noticed a segment which\nsays most recommended list for you this\nlist is a personalized list which is\nmade available to you by analyzing all\nthe data such as your previous watch\nhistory your subscriptions your likes\nand so on recommendation engine is a\ntool that filters and analyzes all this\ndata and provides you with a list that\nyou would most likely be interested in\nby doing so the site is able to retain\nand engage its customer for a longer\ntime next is customer churn analysis in\nsimple words customer churn happens when\na customer stops a subscription with a\nservice predicting and preventing this\nis of paramount importance to any\norganization by analyzing the behavioral\npatterns of previously churned customers\nan organization can identify which of\ntheir current customers are likely to\nchurn by analyzing all of this data the\norganization can then implement\neffective programs for customer\nretention let us now look into an use\ncase of starbucks big data is\neffectively used by the starbucks app 17\nmillion users use this app and you can\nimagine how much data they generate data\nin the form of their coffee buying\nhabits the store they visit and to the\ntime they purchase all of this data is\nfed into the app so when a customer\nenters a new starbucks location the\nsystem analyzes all their data and they\nare provided with their preferred order\nthis app also suggests new products to\nthe customer in addition to this they\nalso provide personalized offer and\ndiscounts on special occasions moving on\nto our next sector which is healthcare\nit is one of the most important sectors\nbig data is widely used here to save\nlives with all the available big data\nmedical researchers are done very\neffectively they are performed\naccurately by analyzing all the previous\nmedical histories and new treatments and\nmedicines are discovered cure can be\nfound out even for few of the incurable\ndiseases there are cases when one\nmedication need not be effective for\nevery patient hence personal care is\nvery important personal care is provided\nto each patient depending on their past\nmedical history and individuals medical\nhistory along with their body parameters\nare analyzed and personal attention is\ngiven to each of them as we all know\nmedical treatments are not very pocket\nfriendly every time a medical treatment\nis taken the amount increases this can\nbe reduced if readmissions are brought\ndown analyzing all the data precisely\nwill deliver a long-term efficient\nresult which will in turn prevent a\npatient's readmission frequently with\nglobalization came an increase in the\nease for infectious diseases to spread\nwidely based on geography and\ndemographics big data helps in\npredicting where an outbreak of epidemic\nviruses are most likely to occur an\namerican healthcare company united\nhealthcare uses big data to detect any\nonline medical fraud activities such as\npayment of unauthorized benefits\nintentional misrepresentation of data\nand so on the healthcare company runs\ndisease management programs the success\nrates of these programs are predicted\nusing big data depending on how patients\nrespond to it the next sector we will\nlook into is logistics logistics looks\ninto the process of transportation and\nstorage of goods the movement of a\nproduct from its supplier to a consumer\nis very important big data is used to\nmake this process faster and efficient\nthe most important factor in logistics\nis the time taken for the products to\nreach their destination to achieve\nminimum time sensors within the vehicle\nanalyze the fastest route this analysis\nis based on various data such as the\nweather traffic the list of orders and\nso on by doing so the fastest route is\nobtained and the delivery time is\nreduced capacity planning is another\nfactor which needs to be taken into\nconsideration details regarding the\nworkforce and the number of vehicles are\nanalyzed thoroughly and each vehicle is\nallocated a different route this is done\nas there is no need for many trucks to\ntravel in the same direction which will\nbe pointless depending on the analysis\nof the available workforce and resources\nthis decision is taken big data\nanalytics also finds its use in managing\nwarehouses efficiently this analysis\nalong with tracking sensors provide\ninformation regarding the underutilized\nspace which results in efficient\nresource allocation and eventually\nreduces the cost customer satisfaction\nis important in logistics just like it\nis in any other sector customer\nreactions are analyzed from the\navailable data which will eventually\ncreate an instant feedback loop a happy\ncustomer will always help the company\ngain more customers let us now look into\na use case of ups as you know ups is one\nof the biggest shipping company in the\nworld they have a huge customer database\nand they work on data every minute ups\nuses big data to gather different kinds\nof data regarding the weather the\ntraffic jams the geography the locations\nand so on after collecting all this data\nthey analyze it to discover the best and\nthe fastest route to the destination in\naddition to this they also use big data\nto change the routes in real time this\nis how efficiently ups leverages big\ndata next up we have a very interesting\nsector that is the travel and tourism\nsector the global tourism market is\nexpected to grow in the near future big\ndata is used in various ways in this\nsector let us look into a few of them\nhotels can increase their revenue by\nadjusting the room tariffs depending on\nthe peak seasons such as holiday seasons\nfestive seasons and so on the tourism\nindustry uses all of this data to\nanticipate the demand and maximize their\nrevenue big data is also used by resorts\nand hotels to analyze various details\nregarding their competitors this\nanalysis result helps them to\nincorporate all the good facilities\ntheir competitors are providing and by\ndoing so the hotel is able to flourish\nfurther a customer always comes back if\nthey are offered good packages which are\nmore than just the basic ones looking\ninto a customer's past travel history\nlikes and preferences hotels can provide\nits customers with personalized\nexperiences which will interest them\nhighly investing in an area which could\nbe the hub of tourism is very wise few\ncountries use big data to examine the\ntourism activities in their country and\nthis in turn helps them discover new and\nfruitful investment opportunities let us\nlook into one of the best online\nhomestay networks airbnb and see how big\ndata is used by them airbnb undoubtedly\nprovides its customers with the best\naccommodation across the world big data\nis used by it to analyze the different\nkinds of available properties depending\non the customer's preferences the\npricing the keywords previous customers\nratings and experiences airbnb filters\nout the best result big data works its\nmagic yet again now we will move on to\nour final sector which is the government\nand law enforcement sector maintaining\nlaw and order is of utmost importance to\nany government it is a huge task by\nitself big data plays an active role\nhere and in addition to this it also\nhelps governments bring in new policies\nand schemes for the welfare of its\ncitizens the police department is able\nto predict criminal activities way\nbefore it happens by analyzing big data\ninformation such as the previous crime\nrecords in a particular region the\nsafety aspect in that region and so on\nby analyzing these factors they are able\nto predict any activity which breaks the\nlaw and order of the region governments\nare able to tackle unemployment to a\ngreat extent by using big data by\nanalyzing the number of students\ngraduating every year to the number of\nrelevant job openings the government can\nhave an idea of the unemployment rate in\nthe country and then take necessary\nmeasures to tackle it our next factor is\npoverty in large countries it is\ndifficult to analyze which area requires\nattention and development big data\nanalytics makes it easier for\ngovernments to discover such areas\npoverty gradually decreases once these\nareas begin to develop\ngovernments have to always be on the\nlookout for better development a public\nsurvey voices the opinion of a country's\ncitizens analyzing all the data\ncollected from such surveys can help\ngovernments build better policies and\nservices which will benefit its citizens\nlet us now move on to our use case did\nyou know that the new york police\ndepartment uses big data analytics to\nprotect its citizens the department\nprevents and identifies crimes by\nanalyzing a huge amount of data which\nincludes fingerprints certain emails and\nrecords from previous police\ninvestigations and so on after analyzing\nall of this data meaningful insights are\ndrawn from it which will help the police\nin taking the required preventive\nmeasures against crimes thank you srithi\nlet us now have a look at the next data\nprocessing framework spark we have our\ninstructor ajay who will take us through\napache spark its installation welcome to\nthis tutorial on apache spark one of the\nmost in demand technologies and\nprocessing frameworks in the big data\nworld and here we will learn on apache\nspark history of spark what is spark\nhadoop which is a framework again wes\nspark components of apache spark that is\nspark core spark sql spark streaming\nspark ml lab and graphics then we will\nlearn on spark architecture applications\nof spark spark use cases so let's begin\nwith understanding about history of\napache spark it all started in 2009 as a\nproject at uc berkeley amp labs by mate\nin 2010 it was open source under a bsd\nlicense in 2013 spark became an apache\ntop level project and in 2014\nused by databricks to sort large-scale\ndata sets and it set a new world record\nso that's how apache spark started and\ntoday it is one of the most in demand\nprocessing framework or i would say in\nmemory computing framework which is used\nacross the big data industry so what is\napache spark let's learn about this\napache spark is a open source in-memory\ncomputing framework or you could say\ndata processing engine which is used to\nprocess data in batch and also in real\ntime across various cluster computers\nand it has a very simple programming\nlanguage behind the scenes that is scala\nwhich is used although if users would\nwant to work on spark they can work with\npython they can work with scala they can\nwork with java and so on even r for that\nmatter so it supports all these\nprogramming languages and that's one of\nthe reasons that it is called polyglot\nwherein you have good set of libraries\nand support from all the programming\nlanguages and developers and data\nscientists incorporate spark into their\napplications or build spark based\napplications to process analyze query\nand transform data at a very large scale\nso these are key features of apache\nspark now if you compare hadoop west\nspark we know that hadoop is a framework\nand it basically has map reduce which\ncomes with hadoop for processing data\nhowever processing data using mapreduce\nin hadoop is quite slow because it is a\nbatch oriented operation and it is time\nconsuming if you if you talk about spark\nspark can process the same data 100\ntimes faster than mapreduce as it is a\nin-memory computing framework well there\ncan always be conflicting ideas saying\nwhat if my spark application is not\nreally efficiently coded and my\nmapreduce application has been very\nefficiently coded well then it's a\ndifferent case however\nnormally if you talk about code which is\nefficiently written for mapreduce or for\nspark based processing spark will win\nthe battle by doing almost 100 times\nfaster than mapreduce so as i mentioned\nhadoop performs batch processing and\nthat is one of the paradigms of map\nreduced programming model which involves\nmapping and reducing and that's quite\nrigid so it performs batch processing\nthe intermittent data is written to sdfs\nand written red back from sdfs and that\nmakes hadoop's mapreduce processing\nslower in case of spark it can perform\nboth batch and real-time processing\nhowever a lot of use cases are based on\nreal-time processing take an example of\nmacy's take an example of retail giant\nsuch as walmart and there are many use\ncases who would prefer to do real time\nprocessing or i would say near real time\nprocessing so when we say real time or\nnear real time it is about processing\nthe data as it comes in or you're\ntalking about streaming kind of data now\nhadoop or hadoop's mapreduce obviously\nwas started to be written in java now\nyou could also write it in scala or in\npython however if you talk about\nmapreduce it will have more lines of\ncode since it is written in java and it\nwill take more times to execute you have\nto manage the dependencies you have to\ndo the right declarations you have to\ncreate your mapper and reducer and\ndriver classes however if you compare\nspark it has few lines of code as it is\nimplemented in scala and scala is a\nstatically typed dynamically inferred\nlanguage it's very very concise and the\nbenefit is it has features from both\nfunctional programming and\nobject-oriented language and in case of\nscala whatever code is written that is\nconverted into byte codes and then it\nruns in the jvm now hadoop supports\nkerberos authentication there are\ndifferent kind of authentication\nmechanisms kerberos is one of the\nwell-known ones and it can really get\ndifficult to manage now spark supports\nauthentication via a shared secret it\ncan also run on yarn leveraging the\ncapability of kerberos so what are spark\nfeatures which really makes it unique or\nin demand processing framework when we\ntalk about spark features one of the key\nfeatures is fast processing so spark\ncontains\nresilient distributed data sets so rdds\nare the building blocks for spark and\nwe'll learn more about rdds later so\nspark contains rdds which saves huge\ntime taken in reading and writing\noperations so it can be 100 times or you\ncan say 10 to 100 times faster than\nhadoop when we say in memory computing\nhere i would like to make a note that\nthere is a difference between caching\nand in memory computing think about it\ncaching is mainly to support read ahead\nmechanism where you have your data\npre-loaded so that it can benefit\nfurther queries however when we say in\nmemory computing we are talking about\nlazy valuation we are talking about data\nbeing loaded into memory only and only\nwhen a specific kind of action is\ninvoked so data is stored in ram so here\nwe can say ram is not only used for\nprocessing but it can also be used for\nstorage and we can again decide whether\nwe would want that ram to be used for\npersistence or just for computing so it\ncan access the data quickly and\naccelerate the speed of analytics now\nspark is quite flexible it supports\nmultiple languages as i already\nmentioned and it allows the developers\nto write applications in java scala r or\npython it's quite fault tolerance so\nspark contains these rdds or you could\nsay execution logic or you could say\ntemporary data sets which initially do\nnot have any data loaded and the data\nwill be loaded into rdds only when\nexecution is happening so these can be\nfault tolerant as\nthese rdds are distributed across\nmultiple nodes so failure of one worker\nnode in the cluster will really not\naffect the rdds because that portion can\nbe recomputed so it ensures loss of data\nit ensures that there is no data loss\nand it is absolutely fault tolerant it\nis for better analytics so sparked has\nrich set of sql queries machine learning\nalgorithms complex analytics all of this\nsupported by various par components\nwhich we will learn in coming slides\nwith all these functionalities analytics\ncan be performed better in terms of\nspark so these are some of the key\nfeatures of spark however there are many\nmore features which are related to\ndifferent components of spark and we\nwill learn about them so what are these\ncomponents of spark which i am talking\nabout spark core so this is the core\ncomponent which basically has rdds which\nhas a core engine which takes care of\nyour processing now you also have spark\nsql so people who would be interested in\nworking on structured data or data which\ncan be structuralized would want to\nprefer using spark sql and spark sql\ninternally has components or features\nlike data frames and data sets which can\nbe used to process your structured data\nin a much much faster way you have spark\nstreaming now that's again an important\ncomponent of spark which allows you to\ncreate your spark streaming applications\nwhich not only works on data which is\nbeing streamed in or data which is\nconstantly getting generated but you\nwould also or you could also transform\nthe data you could analyze or process\nthe data as it comes in in smaller\nchunks you have sparks mlib now this is\nbasically a set of libraries which\nallows developers or data scientists to\nbuild their machine learning algorithms\nso that they can do predictive analytics\nor prescriptive descriptive pre-emptive\nanalytics or they could build their\nrecommendation systems or bigger smarter\nmachine learning algorithms using these\nlibraries and then you have graphics so\nthink about organizations like linkedin\nor say twitter where you have data which\nnaturally has a network kind of flow so\ndata which could be represented in the\nform of graphs now here when i talk\nabout graphs i'm not talking about pie\ncharts or bar charts but i'm talking\nabout network related data that is data\nwhich can be networked together which\ncan have some kind of relationship think\nabout facebook think about linkedin\nwhere you have one person connected to\nother person or one company connected to\nother companies so if we have our data\nwhich can be represented in the form of\nnetwork graphs then spark has a\ncomponent called graphics which allows\nyou to do graph based processing so\nthese are some of the components of\napache spark spark core spark sql spark\nstreaming spark mlib and graphics so to\nlearn more about components of spark\nlet's learn here about spark core now\nthis is the base engine and this is used\nfor large scale parallel and distributed\ndata processing so when you work with\nspark at least and the minimum you would\nwork with a spark core which has rdds as\nthe building blocks of your spark so it\nis responsible for your memory\nmanagement your fault recovery\nscheduling distributing and monitoring\njobs on a cluster and interacting with\nstorage systems so here i would like to\nmake a key point that spark by itself\ndoes not have its own storage\nit relies on storage now that storage\ncould be your sdfs that is hadoop's\ndistributed file system it could be\na database like nosql database such as\nhbase or it could be any other database\nsay rdbms from where you could connect\nyour spark and then fetch the data\nextract the data process it analyze it\nso let's learn a little bit about your\nrdds resilient distributed data sets now\nspark core which is the base engine or\nthe core engine is embedded with the\nbuilding blocks of spark which is\nnothing but your resilient distributed\ndata set so as the name says it is\nresilient so it is existing for a\nshortest period of time distributed so\nit is distributed across nodes and it is\na data set where the data will be loaded\nor where the data will be existing for\nprocessing so it is immutable fault\ntolerant distributed collection of\nobjects so that's what your rdd is and\nthere are mainly two operations which\ncan be performed on an rdd now to take\nan example of this\nsay i want to process a particular file\nnow here i could write a simple code in\nscala and that would basically mean\nsomething like this so if i say val\nwhich is to declare a variable i would\nsay val x and then i could use what we\ncall a spark context which is basically\nthe most important entry point of your\napplication so then i could use a method\nof spark context for example that is\ntext file and then i could point it to a\nparticular file so this is just a method\nof your spark context and spark context\nis the entry point of your application\nnow here i could just give a path in\nthis method so what does this step do it\ndoes not do any evaluation so when i say\nval x i'm creating an immutable variable\nand to that variable i'm assigning a\nfile now what this step does is it\nactually creates a rdd resilient\ndistributed data set so we can imagine\nthis as a simple execution logic a empty\ndata set which is created in memory of\nyour node so if i would say i have\nmultiple nodes in which my data is split\nand stored imagining that your yarn your\nspark is working with hadoop so i have\nhadoop which is using say two nodes and\nthis is my distributed file system sdfs\nwhich basically means my file is written\nto hdfs and it also means that the file\nrelated blocks are stored in the\nunderlying disk of these machines so\nwhen i say val x equals sc.text file\nthat is using a method of spark context\nnow there are various other methods like\nwhole text files parallel eyes and so on\nthis step will create an rdd so you can\nimagine this as a logical data set which\nis created in memory across these nodes\nbecause these nodes have the data\nhowever no data is loaded here so this\nis the first rdd and i can say first\nstep in what we call as a dag a dag\nwhich will have series of steps which\nwill get executed at later stage now\nlater i could do further processing on\nthis i could say val y and then i could\ndo something on x so i could say x\ndot\nmap and i would want to apply a function\nto every record or every element in this\nfile and i could give a logic here x dot\nmap now this second step is again\ncreating an rdd a resilient distributed\ndata set you can say second step in my\ndag okay and here you have a external\nrdd one more rdd created which depends\non the first rtd so my first rdd becomes\nthe base rdd or parent rdd and the\nresultant rtd becomes the child rdd then\nwe can go further and we could say val\nzed and i would say okay now i would\nwant to do some filter on y so this\nfilter which i am doing here and then i\ncould give a logic might be i'm\nsearching for a word i am searching for\nsome pattern so i could say val\nzed equals\ny dot filter which again creates one\nmore rdd a resilient distributed data\nset in memory and a you can say this is\nnothing but one more step in the dag so\nthis is my tag which is a series of\nsteps which will be executed now here\nwhen does the execution happen when the\ndata get when will the data get loaded\ninto these rdds so all of this that is\nusing a method using a transformation\nlike map using a transformation like\nfilter or flat map or anything else\nthese are your\ntransformations so the operations such\nas map filter join union and many others\nwill only create rdds which basically\nmeans it is only creating execution\nlogic no data is evaluated no operation\nis happening right now only and only\nwhen you invoke an action that is might\nbe you want to print some result might\nbe you want to take some elements and\nsee that might be you want to do a count\nso those are actions which will actually\ntrigger the execution of this dag right\nfrom the beginning so if i here say z\ndot count where i would want to just\ncount the number of words which i am\nfiltering this is an action which is\ninvoked and this will trigger the\nexecution of dag right from the\nbeginning so this is what happens in a\nspark now if i do a z dot count again\nit will start the whole execution of dag\nagain right from the beginning so my z\ndot com second time in action is invoked\nagain the data will be loaded in the\nfirst rtd then you will have map then\nyou will have filter and finally you\nwill have result\nso this is the core concept of your rdds\nand this is how rtd works so mainly in\nspark there are two kind of operations\none is your transformations and one is\nyour actions transformations are using a\nmethod of spark context will always and\nalways create an rtd or you could say a\nstep in the tag actions are something\nwhich will invoke the execution which\nwill invoke the execution from the first\nrdd till the last rdd where you can get\nyour result so this is how your rdds\nwork now when we talk about components\nof spark let's learn a little bit about\nspark sql so spark sql is a component\ntype processing framework which is used\nfor structured and semi-structured data\nprocessing so usually people might have\ntheir structured data stored in rdbms or\nin files where data is structured with\nbut particular delimiters and has a\npattern and if one wants to process this\nstructured data if one wants to\nuse spark to do in-memory processing and\nwork on their structured data they would\nprefer to use spark sql so you can work\non different data formats say csv json\nyou can even work on smarter formats\nlike avro parquet even your binary files\nor sequence files you could have your\ndata coming in from an rdbms which can\nthen be extracted using a jdbc\nconnection so at the bottom level when\nyou talk about spark sql it has a data\nsource api which basically allows you to\nget the data in whichever format it is\nnow spark sql has something called as\ndata frame api so what are data frames\ndata frames in short you can visualize\nor imagine as rows and columns or if\nyour data can be represented in the form\nof rows and columns with some column\nheadings so data frame api allows you to\ncreate data frames so like my previous\nexample when you work on a file when you\nwant to process it you would convert\nthat into an rdd using a method of smart\ncontext or by doing some transformations\nso in the similar way when you use data\nframes or when you want to use spark sql\nyou would use\nsparks context which is sql context or\nhive context or spark which allows you\nto work with data frames so like in my\nearlier example we were saying val x\nequals sc dot text file now in case of\ndata frames instead of sc you would be\nusing say spark dot something so spark\ncontext is available for your data\nframes api to be used in older versions\nlike spark 1.6 and so on we were using\nhive context or sql context so if you\nwere working with spark 1.6 you would be\nsaying val x equals sql context dot here\nwe would be using spark dot so data\nframe api basically allows you to create\ndata frames out of your structured data\nwhich also lets spark know that data is\nalready in a particular structure it\nfollows a format and based on that your\nsparks back-end dag scheduler right so\nwhen i say about dag i talk about your\nsequence of steps so spark is already\naware of\nwhat are the different steps involved in\nyour application so your data frame api\nbasically allows you to create data\nframes out of your data and data frames\nwhen i say i'm talking about rows and\ncolumns with some headings\nand then you have your data frame dsl\nlanguage or you can use spark sql or\nhive query language any of these options\ncan be used to work with your data\nframes so to learn more about data\nframes follow in the next sessions when\nyou talk about spark streaming now this\nis very interesting for organizations\nwho would want to work on streaming data\nimagine a store like macy's where they\nwould want to have machine learning\nalgorithms now what would these machine\nlearning algorithms do suppose you have\na lot of customers walking in the store\nand they are\nsearching for particular product or\nparticular item so there could be\ncameras placed in the store and this is\nbeing already done there are cameras\nplaced in the store which will keep\nmonitoring in which corner of the store\nthere are more customers now once camera\ncaptures this information this\ninformation can be streamed in to be\nprocessed by algorithms and those\nalgorithms will see which product or\nwhich series of product customers might\nbe interested in and if this algorithm\nin real time can process based on the\nnumber of customers based on the\navailable product in the store it can\ncome up with a attractive alternative\nprice so that which the price can be\ndisplayed on the screen and probably\ncustomers would buy the product now this\nis a real-time processing where the data\ncomes in algorithms work on it do some\ncomputation and give out some result and\nwhich can then result in customers\nbuying a particular product so the whole\nessence of this machine learning and\nreal-time processing will really hold\ngood if and when customers are in the\nstore or this could relate to even an\nonline shopping portal where there might\nbe machine learning algorithms which\nmight be doing real-time processing\nbased on the clicks which customer is\ndoing based on the clicks based on\ncustomer history based on customer\nbehavior algorithms can come up with\nrecommendation of products or better\naltered price so that the sale happens\nnow in this case we would be\nseeing the essence of real time\nprocessing only in a fixed or in a\nparticular duration of time and this\nalso means that you should have\nsomething which can process the data as\nit comes in so spark streaming is a\nlightweight api that allows developers\nto perform batch processing and also\nreal-time streaming and processing of\ndata so it provides secure reliable fast\nprocessing of live data streams so what\nhappens here in spark streaming in brief\nso you have a input data stream now that\ndata stream could be a file which is\nconstantly getting appended it could be\nsome kind of metrics it could be some\nkind of events based on the clicks which\ncustomers are doing or based on the\nproducts which they are choosing in a\nstore this input data stream is then\npushed in through a spark streaming\napplication now spark streaming\napplication will broke break this\ncontent into smaller streams what we\ncall as discriticized streams or batches\nof smaller data on which processing can\nhappen in frames so you could say\nprocess my file every five seconds for\nthe latest data which has come in now\nthere are also some windows based uh\noptions like when i say windows i mean a\nwindow of past three events window of\npast three events each event being of\nfive seconds so your batches of smaller\ndata is processed by spark engine and\nthis process data can then be stored or\ncan be used for further processing so\nthat's what spark streaming does when\nyou talk about mlib it's a low level\nmachine learning library that is simple\nto use scalable and compatible with\nvarious programming languages now hadoop\nalso has some libraries like you have\napache mahout which can be used for\nmachine learning algorithms however in\nterms of spark we are talking about\nmachine learning algorithms which can be\nbuilt using ml labs libraries and then\nspark can be used for processing so mlib\neases the deployment and development of\nscalable machine learning algorithms i\nmean think about your clustering\ntechniques\nso think about your classification where\nyou would want to classify the data\nwhere you would want to do supervised or\nunsupervised learning think about\ncollaborative filtering and many other\ndata science related techniques or\ntechniques which are required to build\nyour recommendation engines or machine\nlearning algorithms can be built using\nsparks mlip graphics is spark's own\ngraph computation engine so this is\nmainly if you are interested in doing a\ngraph based processing think about\nfacebook think about linkedin where you\ncan have your data which can be stored\nand that data\nhas some kind of network connections or\nyou could say it is well networked i\ncould say x is connected to y y is\nconnected to z\nz is connected to a so x y z a all of\nthese are in terms of graph\nterminologies we call as vertices or\nvertex which are basically being\nconnected and the connection between\nthese are called edges so i could say a\nis friend to b so a and b are vertices\nand friend a relation between them is\nthe edge now if i have my data which can\nbe represented in the form of graphs if\ni would want to do a processing in such\nway this could be not only for social\nmedia it could be for your network\ndevices it could be a cloud platform it\ncould be about different applications\nwhich are connected in a particular\nenvironment so if you have data which\ncan be represented in the form of graph\nthen graphics can be used to do etl that\nis extraction transformation load to do\nyour data analysis and also do\ninteractive graph computation so\ngraphics is quite powerful now when you\ntalk about spark your spark can work\nwith your different clustering\ntechnologies so it can work with apache\nmesos that's how spark came in where it\nwas initially to prove the credibility\nof apache mesos spark can work with yarn\nwhich is usually you will see in\ndifferent working environments spark can\nalso work as standalone that means\nwithout hadoop spar can have its own\nsetup with master and worker processes\nso usually or you can say technically\nspark uses a master slave architecture\nnow that consists of a driver program\nthat can run on a master node it can\nalso run on a client node it depends on\nhow you have configured or what your\napplication is\nand then you have multiple executors\nwhich can run on worker nodes so your\nmaster node has a driver program and\nthis driver program internally has the\nspark context so your spark every spark\napplication will have a driver program\nand that driver program has a inbuilt or\ninternally used spark context which is\nbasically your entry point of\napplication for any spark functionality\nso your driver or your driver program\ninteracts with your cluster manager now\nwhen i say interacts with cluster\nmanager so you have your spark context\nwhich is the entry point that takes your\napplication request to the cluster\nmanager now as i said your cluster\nmanager could be say apache mesos it\ncould be yarn it could be spark\nstandalone master itself so your cluster\nmanager in terms of yarn is your\nresource manager so your spark\napplication internally runs as series or\nset of tasks and processes your driver\nprogram wherever that is run will have a\nspark context and spark context will\ntake care of your application execution\nhow does that do it spark context will\ntalk to cluster manager so your cluster\nmanager could be yarn and in terms of\nwhen i say cluster manager for yarn\nwould be resource manager so at high\nlevel we can say a job is split into\nmultiple tasks and those tasks will be\ndistributed over the slave nodes or\nworker nodes so whenever you do some\nkind of transformation or you use a\nmethod of spark context and rdd is\ncreated and this rdd is distributed\nacross\nmultiple nodes as i explained earlier\nworker nodes are the slaves that run\ndifferent tasks so this is how a spark\narchitecture looks like now we can learn\nmore about spark architecture and its\ninteraction with yarn so usually what\nhappens when your spark context\ninteracts with the cluster manager so in\nterms of yarn i could say resource\nmanager now we already know about yarn\nso you would have say node managers\nrunning on multiple machines and each\nmachine has some ram and cpu cores\nallocated for your node manager on the\nsame machine you have the data nodes\nrunning which obviously are there to\nhave the hadoop related data so whenever\nthe application wants to process the\ndata your application via spark contacts\ncontacts the cluster managers that is\nresource manager now what does resource\nmanager do resource manager makes a\nrequest so resource manager makes\nrequests to the node manager of the\nmachines wherever the relevant data\nresides asking for containers so your\nresource manager is negotiating or\nasking for containers from node manager\nsaying hey can i have a container of 1gb\nram and one cpu core can i have a\ncontainer of 1gb ram and 1 cpu core and\nyour node manager based on the kind of\nprocessing it is doing will approve or\ndeny it so node manager would say fine i\ncan give you the container and once this\ncontainer is allocated or approved by\nnode manager resource manager will\nbasically start an extra piece of code\ncalled appmaster so appmaster is\nresponsible for execution of your\napplications whether those are spark\napplications or mapreduce so your\napplication master which is a piece of\ncode will run in one of the containers\nthat is it will use the ram and cpu core\nand then it will use the other\ncontainers which were allocated by node\nmanager to run the tasks so it is within\nthis container which can take care of\nexecution so what is a container a\ncombination of ram and cpu core so it is\nwithin this container we will have a\nexecutable process which would run and\nthis executor process is taking care of\nyour application related tasks so that's\nhow overall spark works in integration\nwith yarn now let's learn about this\nspark cluster managers as i said spark\ncan work in a standalone mode so that is\nwithout hadoop so by default application\nsubmitted to spark standalone mode\ncluster will run in fifo order and each\napplication will try to use all the\navailable nodes so you could have a\nspark standalone cluster which basically\nmeans you could have multiple nodes on\none of the nodes you would have the\nmaster process running and on the other\nnodes you would have the spark worker\nprocesses running so here we would not\nhave any distributed file system because\nspark is standalone and it will rely on\nan external storage to get the data or\nprobably the file system of the nodes\nwhere the data is stored and processing\nwill happen across the nodes where your\nworker processes are running you could\nhave spark working with apache mesos now\nas i said apache mesos is a open source\nproject to manage your computer clusters\nand can also run hadoop applications\napache mesos was introduced earlier and\nspark came in and as existence to prove\nthe credibility of apache mesos you can\nhave spark working with hadoop's yarn\nthis is something which widely you will\nsee in different working environments so\nyarn which takes care of your processing\nand can take care of different\nprocessing frameworks also supports\nspark you could have kubernetes now that\nis something which is making a lot of\nnews in today's world it is an open\nsource system for automating deployment\nscaling and management of containerized\napplications so where you could have\nmultiple docker based images which can\nbe connecting to each other so spark\nalso works with kubernetes now let's\nlook at some applications of spark so\njpmorgan chase and company uses spark to\ndetect fraudulent transactions analyze\nthe business spends of an individual to\nsuggest offers and identify patterns to\ndecide how much to invest and where to\ninvest so this this is one of the\nexamples of banking a lot of banking\nenvironments are using spark due to its\nreal-time processing capabilities and\nin-memory faster processing where they\ncould be working on fraud detection or\ncredit analysis or pattern\nidentification and many other use cases\nalibaba group that uses also spark to\nanalyze large data sets of data such as\nreal-time transaction details now that\nmight be based online or in the stores\nof looking at the browsing history in\nthe form of spark jobs and then provides\nrecommendations to its users so alibaba\ngroup is using spark in its e-commerce\ndomain you have iq here now this is a\nleading healthcare company that uses\nspark to analyze patients data identify\npossible health issues and diagnose it\nbased on their medical history so there\nis a lot of work happening in healthcare\nindustry where real-time processing is\nfinding lot of importance and real-time\nand faster processing is what is\nrequired so healthcare industry and iqvi\nis also using spark you have netflix\nwhich is known and you have riot games\nso entertainment and gaming companies\nlike netflix and ride games use apache\nspark to showcase relevant\nadvertisements to their users based on\nthe videos that they have watched shared\nor liked so these are few domains which\nfind use cases of spark that is banking\ne-commerce health care entertainment and\nthen there are many more which are using\nspark in their day to day activities for\nreal time in memory faster processing\nnow let's discuss about the sparks use\ncase and let's talk about conviva which\nis world's leading video streaming\ncompanies so video streaming is a\nchallenge now if you talk about youtube\nwhich has data you could always read\nabout it so youtube has data which is\nworth\nwatching 10 years so there is huge\namount of data where people are\nuploading their videos or companies are\ndoing advertisements and this\nvideos\nare streamed in or can be watched by\nusers so video streaming is a challenge\nand especially with increasing demand\nfor high quality streaming experiences\nconviva collects data about video\nstreaming quality to give their\ncustomers visibility into the end user\nexperience they are delivering now how\ndo they do it apache spark again using\napache spark conviva delivers a better\nquality of service to its customers by\nremoving the screen buffering and\nlearning in detail about network\nconditions in real time this information\nis then stored in the video player to\nmanage live video traffic coming in from\n4 million video feeds every month to\nensure maximum retention now using\napache spark conviva has created an auto\ndiagnostics alert it automatically\ndetects anomalies along the video\nstreaming pipeline and diagnoses the\nroot cause of the issue now this really\nmakes it one of the leading video\nstreaming companies based on auto\ndiagnostic alerts it reduces waiting\ntime before the video starts it avoids\nbuffering and recovers the video from a\ntechnical error and the whole goal is to\nmaximize the viewer engagement so this\nis sparks use case where conviva is\nusing spark in different ways to\nstay ahead in video streaming related\ndeliveries so let's have a quick demo on\nsetting up spark on windows and trying\nout the sparks interactive way of\nworking for this first thing is you will\nhave to download spark now you can just\ngo to google and type spark download and\nonce you click on this link it shows you\na spark release a package type which\nsays pre-built so we will have to get a\npre-built apache hadoop related spark so\nhere i will choose say spark 2.4.3 and i\nwill choose pre-built for apache 2.6 and\nthen i can just click and download on\nthis park 2.4.3\nbin hadoop 2.6 star file once you click\non this link takes you to the mirror\nsite and you can just click on this link\nand download spark 2.4.3\nonce this is done which i have already\ndownloaded here i can go to my downloads\nand it shows that i have a tar file now\nwe will have to unturn it or unzip it so\ni can click on this link and i already\nhave winzip which allows me to unzip\nspark\n2.4.3 i can choose a location so i can\nsay unzip click on unzip choose local\nand then i can choose one of my folders\nso i have already unzipped it here and\nthat's how my spar 2.4.4\ndirectory exists here 2.4.3 i'm sorry\nnow let's go and look into this\ndirectory and what it contains so i can\nclick on c drive and then spark 2.4.3\nand this has the folders which are\nrequired for us to use spark if you look\nin bin we have different applications\nand commands which we can use and if you\nalso see i would have added\none of the utilities here either we can\nhave the win utils because spark will\nneed hadoop and we are planning to use\nspark on windows so what we need is this\nso i have in my desktop a hadoop\ndirectory which has a bin folder and\nhere i have downloaded a win utils\nexecutable file which you can always\nsearch on internet and you can say\ndownload when\nutils.exe for spark 64-bit and then you\ncan find the link from where you can\ndownload this for example we can click\non this and here we can search for\nwin-utils so here we have the link i can\nopen this link in a different tab and\nthis basically shows me when utils for\nhadoop 2.7.1 similarly you can search\nfor hadoop 2.6 and you can download\nhowever 2.7 will also work fine now i\nhave downloaded winutils.exe\nand that's in this hadoop folder within\nbin i have this so we need two things\nhere one is spark which is untired and\nin a particular location and then your\nhadoop which has been and this has been\nutils now once you have these things you\ncan set your environment variables by\njust type in envir go to your edit\nsystem environment variables click on\nenvironment variables and here if you\nsee i have added two variables so you\ncan just click on new and then you can\ngive your variable name as hadoop\nunderscore home and then you can give\nthe path where you have your hadoop\ndirectory that contains bin and that\ncontains\nwin.util.exe i also have added spark\nhome which points to myspark directory\nnow once you have this you are ready to\nuse your spark on windows in local mode\nonce this is done i can open up my\ncommand prompt and here i need to go to\nmy spark directory i can just say cd\nspar 2.4.3\nand then i can do a dir slash p which\nshows me the folders where we have been\nso to execute spark we can look into\nwhat our bin has and it has different\nprograms such as you have spark shell to\nwork with scala and an interactive way\nof working with spark you can also use\npi spark which allows you to use python\nto work with spark and you also have\nspark submit if you have packaged your\napplication in a jar file to submit it\non a cluster so in this case we would be\nusing spark in a local mode so let's use\nspark shell and test it so i can just\nsay spark shell in this way to start my\nspark in an interactive way now based on\nthe path which you have set for your\nvinnutils.exe\nand spark shell you should be able to\nuse spark so it says welcome to spark\nversion 2.4.3 it also shows that spark\ncontext which is the entry point of your\napplication\nis available as sc wherein it is\nconnecting to master as local and it has\nan application id it also shows you a\nspark ui which you can look at and it\nsays spark session is available as spark\nso if you are interested in working with\nspark core that is with rdds we would be\nusing spark context and if you are\ninterested in working with spark sql\nthat is data frames and data sets then\nwe can be using spark session so let's\ntry it out so we can declare a variable\nthat is a immutable variable and now i\nwill use spark context with a method of\nspark context so i could just do a tab\nto see what are the available options\nwithin your spark context and here we\ncan use one of them to read a file so i\nwill use sc.text file and then i need to\npoint to a file so i already know that\nin my spark directory i have a file\ncalled readme dot rd and this is the\nmethod of spark context if i enter it\nwill create a rdd now as i've explained\nabout rdds these are resilient\ndistributed data sets there is no\nevaluation happening right now so even\nif this file does not exist i would\nstill have an rdd created now i can go\nfurther and do some more transformations\non this so i could say val y and then i\ncould say take x i would like to do a\nmap transformation on it and i would use\nto uppercase now this is a inbuilt\nfunction where i would want to convert\nthe content of this file into uppercase\nwhen i hit on enter it again creates an\nrdd and this rdd is basically a child\nrdd of the parent rtd created by text\nfile remember here we don't have any\nevaluation happening or any execution\nhappening it is just transformations\nwhich will lead to creation of further\nrdds now we can invoke an action to see\nthe result and whenever you invoke an\naction it will trigger the execution of\nyour dag starting from your first rdd\ntill the last transformation you did\nbefore invoking an action so i could\njust say count to see how many lines it\nhas and once i do this it says input\npath does not exist it is not able to\nfind a file in this location so this\nclearly explains that there was no\nevaluation happening when we did this\nwhen we tried to read a file and neither\nwhen we did a map transformation only\nand only when we invoked an action it\ntried to search for file in this\nlocation so let's quit this and check if\nthe file exists to quit you can just do\na ctrl d or you could do a colon quit\nnow here if you see some messages like\nthis which says unable to delete\ntemporary file so that's a known issue\nwe can fix that but my spark is working\nfine now let's go and check if the file\nexists so i will go in here and then i\nwill look in my spark directory to see\nif the file exists so this is the file\nreadme dot md however i had done readme\ndot rd so let's do it again and let's\ntest if our spark shell works fine so we\ncan go into the spark directory by just\ndoing this and then either i could be\ngoing into bin directory and then start\nspark shell or some people prefer to do\nit this way wherein you can say bin\nspark\nshell and that should start your spark\nshell but there was an error in the in\nthe slash so let's do it this way and\nthis will start your spark shell an\ninteractive way of working with your\nspark and using spark context or spark\nnow once we have done this i can again\nsay val x and i can say sc which is\nspark context which is already\ninitialized when you start your spark\nshell i can say text file and i would\nwant to read a file which is readme dot\nmd now we know that this file exists\nhowever the checking of the file\nexisting or not that means an evaluation\ndoes not happen when you do a\ntransformation or when you use a method\nof spark context it just creates an rdd\nclick on this and this has created an\nrdd of string now we can do further\ntransformation on x by saying map and\nthen i can say i would want to convert\nthe content into uppercase and that's\nalso done now finally let's invoke an\naction which will trigger the execution\nof my tag which contains these rdds so i\ncould just say y and i could say take 10\nand i could say for each print ln and\nthis should be able to invoke an action\nand it shows me the result from the file\nwhich is converted into uppercase\nremember if i do a y dot count which is\nanother action it will trigger the\nexecution of dag right from the\nbeginning where the first rtd was\ncreated the transformation will be done\nand then it will show me the count of\nlines which exist as a result of this so\nwe can do this and this says me that\nthere are 105 lines this is a simple\nexample of using spark shell and using\nspar in an interactive way on your\nwindows machine now to quit we can\nalways do a colon quit or we could do a\ncontrol d and that basically takes you\nout of your spark shell similarly we can\nwork on pi spark by just saying dot\nslash\nbin\nslash pi spark and that's python's way\nof working with spark which brings your\npython shell it starts the spark context\nwhich is available as sc and if you are\ninterested in working on data frames or\ndata sets using spark sql then we would\nbe using spark now here i can just\ndeclare a variable x and i can just do\nsc.text file like what we did using\nscala we can be referring to the same\nfile readme dot md and this\nhas created an rdd you can just type in\nx and that shows the rdd which was\ncreated you can then further do some\ntransformation on it using x dot\nmap and then i could say i would want to\nconvert this to uppercase now in case of\npython you would normally use\nlambda if you would want to do any kind\nof transformation so i would say lambda\nx and then i would want to convert this\nto uppercase so here we can check if my\nrdd was correctly created and as i was\nsaying we can then do some\ntransformation on it so i can say again\ny equals and then i would want to do\nsome transformation on x i would say x\ndot map so in case of python as i\nexplained whenever we want to use an\nanonymous function or we want to do some\ntransformation then we can pass in the\nfunction to the transformation so here i\ncan say lambda and then i can just say\nline or i could say x or a whatever that\nis and i could say line i would use a\ninbuilt function that is upper which\nshould convert the content of x to\nuppercase now remember we are just doing\nsome transformations here so if i click\non y again and enter sorry if i enter y\nit will show me that that has created an\nrdd to see the content i can just say y\ndot collect and that should show me the\ncontent of first few lines which is\nconverted in uppercase so this clearly\nshows that we can even use python to\nwork with spark by using pi spark now to\nquit this we can just say quit and that\nshould take you out of your pi spark\nshell so this is how you work on spark\nshell or pi spark on your windows\nmachine so this has a local spark setup\nor basically spark running in a local\nmode and you could be using the\ninteractive way of working with spark\nnow if we have packaged our application\nin a jar file then i could be using\nspark submit to submit my application\nhow do we do that for this we can\nbasically bring up one of our ides so in\nmy case i'm using eclipse and i can get\ninto my eclipse now there are two\noptions here one we can be writing code\nin your ide that is eclipse we can be\ncompiling the code based on the spark\nrelated jars and then we can run our\napplication from ide which would\ninteract with your spark that's one way\nthe second way is we can use packaging\ntools like sbt to package our\napplication as jar and then push the jar\nto your local spark or a hadoop base\npark and then run it using spark submit\nnow this is a simple sample application\nfor which i have\nset up certain things here the first\nthing is when you have downloaded\neclipse for all the people who are new\nthey might have to add the scala plug-in\nnow if you see here in my eclipse it\nshows up java perspective and it shows\nscalar perspective now how did i get\nthis you can click on help and you can\nsay install new software and here you\ncan say add you can say scala ide and\nthen you will have to give a link from\nwhere you can get the scala ide how do i\nget that so i can go to my browser and i\ncan say scala ide dot org\nand once you go to this link just scroll\ndown click on stable and that shows you\ndifferent releases for your different\nides now for eclipse although this shows\nfor oxygen and i have eclipse neon but\nthat works fine you can just copy this\nlink and here once you have copied the\nlink give the location here and you can\nsay okay now once you do that it shows\nyou different options from where you can\nchoose color ide for eclipse now in my\ncase it is already installed so these\noptions don't show up however if you do\nnot have this option for example let's\nchoose something else say i will say\nscala\nsearch and once i do this next gets\nactivated so scala id is already\ninstalled in my case so i can click on\nnext and this should basically get scala\nsearch plugin also which can be added to\nmy eclipse now once this is done you\nwill be prompted to restart your eclipse\nand then you should have your scala\nperspective so for now i'll just cancel\nthis because i already have my scala id\ndot eclipse so this is done i'll just do\na cancel here now once you have your\nscala id or scala plugin added to your\nide you can always click on this icon\nhere and here you can just click double\nclick on scala that will open up scala\nperspective so that's the first thing\nthen you can always click on file new\nand create a scala project for example i\ncould say my test apps and then i can\njust say finish so it has added a\nproject now what we need to do is we\nneed to make sure that our setup is fine\nso first thing i would suggest is click\non this go to your build path and here\nclick on configure build path so where\nit says color compiler it would be good\nto choose for your project settings\ninstead of 2.12 go for 2.11 so sometimes\nyou might have problems with scala 2.12\nwhich might not be able to compile for\nyour scala 2.11 so use latest 2.11\nbundle and then you can say apply it\nsays compiler settings have changed a\nfull rebuild is required for changes to\ntake effect shall all projects be\ncleaned now we can say okay and we can\nsay okay so this will build your\nworkspace to use scala 2.11 bundle now\nonce that is done second thing what we\nneed is we would want to write our code\nand we would want our code to compile\nfor that as i said if we want to write\nour code using an ide and then run it\nfrom our ide rather than using a build\ntool like meven or sbt i can just add\nthe spark related jars to my build path\nso i can right click here i can say\nbuild path and i can click on configure\nbuild path and here where it says\nlibraries i can say add external charts\nnow this needs my spark related jars now\nwhere do i find it so remember we have\ndownloaded spark so you can even find it\nthere so you can click on\nwherever your spark is and then you can\nclick for jars which shows you all spark\nrelated jars you can just do a control a\nand just add it to your build path say\napply and then just say okay so once\nthis is done you have already added the\nspark related jars which might be good\nenough for you to write an application\nonce this is done we need to basically\ngo ahead and write our code now how do i\ndo that i can just click on source and i\ncan say new and then i can\nsay package\nand here you can say main dot scala\nclick on finish and that has created\nmain dots color so that would be source\nmain slash scala in the folders we can\nalways check that so i can go to c drive\ni can look in users win 10 and then look\nin my workspace for my project and if\nyou see in source we have main and we\nhave scala now this is where we will be\nwriting our code so for this then i can\njust right click and i can say new and i\ncan just create an object and i can give\nit a name so we can say test app and\nthen we can say finish so in case of\nscala and spark we need to create an\nobject and then we need to define main\nclass here once that is done we also\nneed to import some packages and we need\nto initialize the spark context now for\nthat you can take an example here so i\nhave already written the code and i can\nlook in source\nmain scala and i have created an app\ncalled first app.scala now here we have\nthe package name we are importing some\npackages which are required for us to\ninitialize path context and spark\nconfiguration so it is spark context\nspark conf and here is my object and for\na particular project at least one of\nyour object or one of your application\nneeds to be the main so we define the\nmain here by saying def main args and\narray of string then we need to\nbasically define our spark context so\ni'm creating a configuration object here\nand then i say new spark conf i am\nsetting my application name to hello\nspark and then this is very important if\nyou intend to run your spark application\neither using spark submit or from your\nid you need to specify your master as\nlocal now that could be given with one\nthread or multiple threads so this is my\nconfiguration and then i initialize my\nspark context pointing to my\nconfiguration so this\nis the most important part here now if\nyou see this is something which i showed\nyou in the command line so i'm doing a\nval x and i'm pointing it to a file in\nmy project directory which is\nabc1.txt so this is just to make sure\nthat you are giving a windows path and\nyou are giving the right escape\ncharacters now if it would be if you\nintend to run this application on a\nhadoop based cluster then you will have\nto give the path of the file accordingly\nwhich we can see later so here i have\nval x i'm pointing it to a file which i\nhave created here and this just contains\nthree lines this is my sample file which\ni want to test this is a new file now\nhere i've created a variable called x\nfurther i would say val y and i would\nuse spark context method text file like\nwhat we did in interactive way and i'm\npointing it to x i'm also using some\nextra features like caching so that i\ncan cache the rdd which gets created\nfurther i create a variable called\ncounts and then i do it flat map\ntransformation on y where i would split\nthe content based on space i would do a\nmap transformation where i would get\nevery word from the file and map it with\nnumber one and finally i do a reduce by\nkey operation once this is done i do a\nsaves as text file now i could do a\ncollect i could do a take i could do a\ncount and i could also do a save as text\nfile where i would want my output to be\nsaved in this location plus i'm using a\njava utility to append a random number\nto my output directory finally i do a\nspark context stop so this is my\napplication and if you see here my\napplication completely compiles because\ni've already added the relevant jar\nfiles in the build path now once this\napplication is done we need to run this\nbut before doing that we need to look\ninto our run configurations so you can\nclick on run configurations and here i\nam giving my project name i am\nspecifying my main class which is first\napp then you can straight away go to\nenvironment and here i have added two\nvariables one is spark underscore local\nunderscore ip which points to this\nmachine's host and then i have said\nhadoop underscore home because spark\neven when you are running locally would\nwant if hadoop is existing so we can\njust set hadoop underscore home and i am\npointing it to my desktop hadoop\ndirectory which contains bin folder and\nthat bin folder contains a\nwinneutils.exe\nonce this is done we are fine and then i\ncan test my application by just clicking\non run now this should trigger my\napplication on my windows machine using\nmy ide i didn't have to package this as\njar and here\nmy application is completed which should\nhave created one additional directory\nhere so i can click on this i can right\nclick and i can try doing a refresh and\nhere you see a new output is created we\ncan look into this it has a part file\nwhich shows my word count so this is a\nsimple example of setting up your ide\nand running your applications from ide\non a local spark now what you also see\nis a build file here what is that for so\nin case you would want to\ncreate a project and you would want to\npackage the application as a jar file\nand then run it on the cluster using\nspark submit now in that case we can\navoid adding the jars to my build path\nbut what i would need is i would need\nbuild dot sbt file within my project\nfolder remember if you intend to package\nyour application as a jar and then run\nit on a cluster or on a local setup you\nwould not need the build related charts\nhowever the build related jars which are\nalready here will not cause any harm let\nthem be there now we have a build.sbt\nfile also which is existing in my\nproject folder now just for this run i\ncan in fact delete this target directory\nit will be anyways recreated and if you\nwould want we can also clean up these\njust to avoid the confusion i will\ndelete these now i just have my project\nfolder and i have the libraries i have\nabc1.txt and i have a build.sbt file\nwhat does that contain so this basically\nhas a name i'm giving a version which\nwill be appended to my jar i'm saying\nthe scala version which i'm using which\nis 2.11.8 and i'm saying spark version\nwhich is\n2.2.0 you can replace these with the\nversion which you intend to use other\nthan that we are pointing to the\ndifferent spark components and the\nrelevant dependency related jars which\nyour sbt can get for you so i have given\nspark core spark sql mlib spark\nstreaming and spark hive which will be\nthen fetched from the repository so this\nis my build.sbt file which exists in my\nproject folder we have our code written\nwe have our build dot sbt file which is\nalready existing in my project folder my\ncode is already fine now we need to use\nsbt to package my applications into jar\nfor which you will have to download sbt\nso you can just say download sbt windows\nand that should take you to the\ninstalling svt on windows page now here\nyou can just download the msi installer\nand run through the installer which will\nbasically install sbt on your machine\nnow i have already done that so my build\nfile is ready my code is ready all i\nneed to do is now use sbt so what i can\ndo is from my command line i can just\nsay\ncd and i will go into users i'll go into\nwin10 and i'll go into my workspace now\nhere i can see for my projects and i can\ngo into my spark apps now once i'm in my\nproject folder we can double check that\nit has a build.sbt which is fine sbt is\nalready installed on this machine and i\ncan just say sbt if you would want to\ncheck you can always do sbt version to\ncheck if you have svt and this command\nis also done once you install sbt for\nthe first type you can always do sbt\nversion and before displaying it was\ndisplaying svds version it will try to\nfix all your dependencies it will try to\nget all the relevant dependencies so\nthis is just to check that sbt is\ninstalled on my machine as i said my\ncode is already ready i have a build.sbt\nfile and i can just say sbt package\nwhich then will refer to build.sbt will\nlook for your code in source main scala\nand then if everything is fine it will\npackage it as jar and it will create\nfolders within your project folder we\ncan click on this we can try doing a\nrefresh to see now if you see here the\nproject was again created a target\nfolder is created within scala 2.11 i\nwill have my jar created so we can see\nthat my sbt was fine it has packaged the\ncode and it has created a jar file as\nsimple project 2.11 -1 which can be used\nto run on the cluster or in a local mode\nlet's check in eclipse i'll click on\nthis and i'll just do a refresh and\ni see my chart is already existing which\nbasically means then we can do a spark\nsubmit so this is where we see that i am\nalready running a spark submit command\nnow this might show up some error\nmessages but we can see how we did this\nso if you see here once my packaging was\ndone i went into my spark folder i did a\nbin slash spark submit i mentioned my\njar file and then i said my class main\ndot scala dot first app so this is my\napp and this is how i'm using spark\nsubmit to submit an application which is\nrunning in local mode so we can see\nfurther if it starts processing so here\nit says created local directory then it\nstarts an executor on localhost it goes\nfor execution and then if we see further\nwe could also see it will add our jar to\nthe class path and then finally it\nshould be doing the execution now if\nthere is any error message that might be\nrelated to your file not being deleted\nor if there is any problem with executor\nso we can basically come back and check\nhere if there was any specific error so\nit the error was that it tries to delete\na temp directory but it does not have\nthe permissions so we have not done that\nbut as per my application it should be\ncreating a output in this folder as park\nout so let's click on this and see i'll\ndo a refresh right and if you see a new\nspark output has been created previous\nsessions we have seen how we can set up\nspark on windows or set up our spark\nrelated ide so that we can run our\napplications on windows now here we have\na quick demo on setting up spark as a\nstandalone cluster on ubuntu machines\nand then trying out spark way of working\nfor which we need at least two machines\nand here i have ubuntu machines set up\num1 and um2 just to give insights on how\nmy machines look so if i look in my etc\nhost i have the ip address of multiple\nmachines so i can ping one machine to\nother machine by just doing a ping um2\nand that should work fine similarly i\nhave also set up ssh access for these\nmachines both of the machines have hdu\nas the user and i can just do a ssh um2\nand that locks me into the second\nmachine without a password similarly i\ncan check the same thing from my second\nmachine by doing a ssh um one and that\nworks fine too so we need two machines\nwhich can ping each other firewall\ndisabled able to ssh each other so that\nwe can set up a standalone cluster of\nspark how do we do it so first thing is\nwe will have to download the spark\nrelated tar file now i can just go to\ngoogle and i can type in spark 2.2.1\ndownload now that's what i'm interested\nin and this takes you to spark release\n2.2.1 however here it shows you the\nlatest release is 2.4 and then\n2.4.2 so what we can do is we can go to\narchives so i can basically look at\nexisting versions now here i can click\non download and this shows me a spark\nrelease and also pre-built for apache\nhadoop now what we can also do is we can\ngo to release archives for an older\nstable version i click on this and here\ni have spark\n2.2.1 now i can click on this and we can\neither install something which is not\nalready built or we can download this\none which is spark 2.2.1 bin hadoop 2.6\npoint t g z so you click on this link\nand then save this file which i have\nalready done and here if you see in my\nmachine i have home hdu downloads which\nhas my different packages which i have\ndownloaded also if you notice i have\ndownloaded jdk8 so you can check on my\nmachines so apart from machines being\nable to ping each other and being able\nto do a ssh we also need machines to\nhave java which is already installed\nonce this is done now i have my spark\nrelated package so i go into user local\ndirectory and then you can give a\ncommand here that is sudo star xvf home\nsdu downloads and then give your spark\npackage now once you do this this will\nuntar the spark directory and create a\ndirectory in my user local location if\nyou see here this is the directory which\nis created but you also see that there\nis a spark link which is pointing to\nspark now that is because if you would\nwant to work on a newer version of spark\nyou could just do the same thing for\nnewer version and then make your link\npointing to the newer version of spark\nhow do you create a link you can just\nsay sudo ln minus s you can give your\nspark directory and then you can create\na link so i've already created a link\nand my spark path will then become this\nnow what do we do with this once we have\ndone this once we have created a link we\ncan go into the bash file of my user and\nif you carefully see here i have given\nmy java path so that i can execute java\nrelated commands and i have also added\nmy spark related path here which says\nuser local spark so in case you would be\nchanging your spark version to a latest\none you will not have to change things\nin your bash file only thing you will\nhave to do is unlink the existing spark\nlink and create a new link to your newer\nversion so this is done on this machine\nand now i can basically be using spark\nnow before doing that we can go into\nuser local spark and this has different\ndirectories so if you look in bin these\nhas a binaries programs like your pi\nspark your spark shell spark sql and\nspark submit which we will see how we\ncan use here if you look in sbin it has\nother startup scripts to start your\nhistory server or to start your master\nor worker processes if you look in conf\nthis has your config directories now\nhere by default you might see spark\nminus default.conf.template\ni have renamed that to dot conf slaves\ntemplate has been renamed to slaves so\nlet's look into this so i can go into\nconf and then i will look in spark\ndefault conf now here based on our setup\nnow here we intend to set up a spark\nstandalone cluster that is without\nhadoop but i would want to have a spark\nstandalone distributed cluster so i have\nuncommented this property it says\nspark.master and i say spark and i also\nmention that my master will run on this\nmachine which is um1 spark event log dot\nenabled is true because we would want to\ntrack the events i have mentioned a\ndirectory so this is a local directory\nin user local spark and we will have to\ncreate it it talks about the default\nserializer it talks about the driver\nmemory which was by default 5 gigabyte i\nhave reduced it to 2 gigabyte gigabyte\nbased on my machine configuration you\nhave java options and then if you intend\nto run a history server so that whenever\nyour spark application is complete you\nwill have your application stored in\nhistory server i have given the log\ndirectory which is user local spark and\nthen application history and this also\nneeds to be created we have spark\nhistory provider for the class which\ntakes care of history server and the\nupdate interval of looking for your\napplications so this is what we have in\nour spark default and if you look in\nslaves i have given the machines where i\nwould want my worker processes to run so\nthat is um1 and um now once you have\nmade your changes in your spark default\nconf and slaves file what you can simply\ndo is scp assuming that whatever was\ndone on this machine that is untarring\nyour spark directory creating a link\nupdating your bash rc and\nrenaming your config files same steps\nneed to be done on the second machine\nand your second machine would also be\nprepared to be used for your spark\ncluster so once you have both the\nmachines which can ping each other which\ncan ssh each other both of the machines\nhave java both of the machines have the\nsame spark version downloaded and the\nbasic setup done i can just easily copy\nthe spark related config from here into\nmy other machine that is sdu um2 and\nthen i can give a path which is user\nlocal spark conf and in this way i don't\nneed to edit my config files again and\nsimilarly i can even copy the slaves\nfile this is all we need to basically\nhave our spark standalone cluster now\nsince we have updated our bash rc we can\ngive our spark command from anywhere we\nhave set up our config files so i can\njust do a start minus all dot sh and\nthat based on my config files based on\nthe directories which are mentioned it\nwill start my master process and my\nworker process on both the machines so\nhere we have a spark standalone cluster\nwhich has two workers and one master now\nwe can always go to the browser\ninterface and we can check for the spark\nui which should be available by typing\nin http\nslash my master and then the port is\neight zero eight zero so this shows that\ni have a spark master running i have two\nworker processes right now there are no\napplications which are running but my\nspark ui is already available when we\nstart using spark either by spark shell\nor pi spark or even spark submit we will\nsee our applications getting populated\nhere additionally we can also start the\nstart history server by just saying\nstart history server dot sh and that\nwill start my history server once this\nis done we have a history server also\nrunning and we can go back and then we\ncan again pull out the history server\nuser interface by giving in the port\nwhich is default 18080 so i have a spark\nstandalone cluster with one master two\nworkers and also a history server which\nis running once we have this we can try\nworking with spark either with spark\nshell or pi spark or if you have\npackaged your application as a jar using\nsbt or mevin you could also use spark\nsubmit additionally you could also use\nspark sql so to work with spark i would\njust say spark shell on one of my\nmachines both my machines which have\nworker processes running we can start a\nspark shell on any of these machines\nremember here our spark will rely on the\nstorage of these machines that is the\nfile system because it does not have any\nhadoop or distributed file system we\nhave started our spark shell and that\nshows spark context which is available\nas sc spark session is available as\nspark so to work with spark core and\nrdds we would be using spark context and\nif you intend to work with spark sql\nthat is data frames and data sets you\ncan be using spark let's try this quick\nsimple example so i'll say val x sc dot\ntext file and now i would want to point\nit to a file so i will say the complete\npath remember i am giving the spark\ncommand from my home directory so i will\nhave to say user local spark and then i\ncan point to a file which is existing\nhere and this is what i would want to\nuse to do some processing we are using\nthe first step here wherein we are using\na method of spark context which will not\nrelate in any evaluation and it will\njust create an rdt now we are using the\nscala's way so let's create one more\nvariable and then we can do some\ntransformation on it by saying to upper\ncase i would just want to convert the\ncontent into upper case and this is\nagain a transformation so that will just\ncreate an rdd once this is done we can\ninvoke an action to basically see some\nresults so i could just simply do a\ncollect which is an action and this\naction will trigger the execution of dag\nwhich starts from the farthest rtd that\nis the first step where your file is\nloaded into rdd now once we do this we\nshould be able to see our result and\nthat basically is execution of dag so\nfrom the time we have used a text file\nwhich is a method of spark context 2 we\ndid a transformation until we invoked an\naction becomes my one job now if i would\ndo y dot count which is again a\ndifferent action this will trigger the\nexecution of dag right from the\nbeginning that is sc.text file where the\ndata will again be loaded in rdd a map\ntransformation will happen and then you\nwill have a count now we can always see\nthis in the spark ui by just going in\nhere and just doing a refresh so that\nwill show me that there is an\napplication which is running via spark\nshell it has utilized three cores it has\nutilized memory per executor one\ngigabyte and we can click on this\napplication which shows that it used\nboth the workers one core and one gb on\nthis worker and two cores and memory on\nthis worker i can click on application\ndetail ui wherein i can see what are the\nactions i invoked we also see the number\nof tasks which are run and that is\nbecause every rdd which is created by\ndefault is having two partitions and for\neach partition you will have one task\nwell we can change the partitions and\nmany more things can be done but that\nyou can learn in later sessions here we\nhave two jobs one ended with collect and\none ended with count now i can click on\ncollect and i can see the dag\nvisualization which tells me we started\na text file we did a map and finally we\ndid a collect it shows that everything\nwas done in state 0 that is just one\nstage i can click on count and that\nagain shows me the dag visualization\nwhich is a different stage a different\njob id which started with text file map\nand then we did account if you would\nwant to see more details you can always\nuse these tabs to look at different\nstages within your application if you\nwere doing some caching looking at\nenvironment variables and also to see\nhow many executors were used including\nthe one for the driver on which nodes\nthese executor ran how many cores how\nmany tasks were run was there any\nshuffling involved and so on so this is\nhow i have used spark shell that is an\ninteractive way of running my\napplication and since we have a spark ui\nwe can always look at applications when\nthey have run and we can always drill\ndown and know more details now since my\napplication is completed i can do a\nrefresh on history server but that does\nnot show anything because when we\nstarted a spark shell it started an\napplication and that application is\nstill in running status if you see here\nunder running applications so this is\nhow we use spark shell on a standalone\nspark cluster now we can just do a colon\nquit and that takes me out of spark\nshell similarly if i would want to work\non pi spark that is python's way of\nworking with spark i can just type in pi\nspark which should bring my python shell\nand then i can continue working with\nspark and again go back and look at my\nui while my pi spark comes up i can go\nhere since i quit my spark shell i can\njust do a refresh to see if my\napplication is yet coming up in history\nserver it might take some time and you\ncan always go back and look at\nincomplete applications or wait till\nyour application is populated in history\nserver now if i do a refresh here on\nspark ui it says this application was\ncompleted it says finished and then now\nwe have started pi spark so it has\nstarted a new application pi spark shell\nand that is in running status we can\ncome back here and now we have our\npython's way so i can do the same thing\nwhich i did in scala using sc dot text\nfile i will point it to a file which\nexists in user local spark read me dot\nmd so that's the file which i'm\ninterested in now this would have\ncreated an rdd which we can confirm by\njust typing in x now i can create a\ndifferent variable and i can say i would\nwant to do a transformation of map now\nin case of python when you use anonymous\nfunctions or when you want to apply some\nfunctions to your transformations you\nuse lambda functions so we say lambda\nline and then i would say what i want to\ndo to the line i will say i will use a\ninbuilt uppercase upper function which\nshould convert my content into uppercase\nbut as of now this is a transformation\nso it only creates an rtt to see the\nresult i can do a y dot collect and that\nshould bring up my result now in case\nif it says the file does not exist that\nis because there was a typo here and we\ncan repeat this step so let's bring up\nhere again x and i can give the right\nname of the file i can do a\ntransformation and finally i can see the\nresult so this is using pi spark on a\nstandalone spark cluster which is\nworking without hadoop which has two\nworker nodes and one master node we can\nalways come and do a refresh here look\nat more details so we have our\napplication we can click on this again\nsimilarly like we did for scala\napplication detail ui i see my\napplication ended at collect it ran to\ntasks and i can look at my tag so this\nis how we can use\nspark shell or pi spark thank you ajay\nnow we have rahul who will tell us how\nto become a big data engineer now let's\nfind out who is a big data engineer a\nbig data engineer is a professional who\ndevelops\nmaintains tests and evaluates the\ncompany's big data infrastructure in\nother words develop big data solutions\nbased on a company's requirements they\nmaintain these solutions they test out\nthese solutions as to the company's\nrequirements they integrate this\nsolution with the various tools and\nsystems of the organization and finally\nthey evaluate how well the solution is\nworking to fulfill the company's\nrequirements next up let's have a look\nat the responsibilities of a big data\nengineer now they need to be able to\ndesign implement verify and maintain\nsoftware systems now for the process of\ningesting data as well as processing it\nthey need to be able to build highly\nscalable as well as robust systems they\nneed to be able to extract data from one\ndatabase transform it as well as load it\ninto another data store with the process\nof etl or the extract transform load\nprocess and they need to research as\nwell as propose new ways to acquire data\nimprove the overall data quality and the\nefficiency of the system now to ensure\nthat all the business requirements are\nmet they need to build a suitable data\narchitecture they need to be able to\nintegrate several programming languages\nand tools together so that they can\ngenerate a structured solution they need\nto build models that reduce the overall\ncomplexity and increase the efficiency\nof the whole system by mining data from\nvarious sources and finally they need to\nwork well with other teams ones that\ninclude data architects data analysts\nand data scientists next up let's have a\nlook at the skills required to become a\nbig data engineer the first step is to\nhave programming knowledge one of the\nmost important skills required to become\na big data engineer is that of\nexperience with programming languages\nespecially hands-on experience now the\nbig data solutions that organizations\nwould want you to create will not be\npossible without experience in\nprogramming languages i can even tell\nyou an easy way through which you can\nget experience with programming\nlanguages practice practice and more\npractice some of the most commonly used\nprogramming languages used for big data\nengineering are python java and c plus\nplus the second skill that you require\nis to have in-depth knowledge about dbms\nand sql you need to know how data is\nmaintained as well as managed in a\ndatabase you need to know how sql can be\nused to transform as well as perform\nactions on a database and by extension\nknow how to write sql queries for any\nrelational database management systems\nsome of the commonly used database\nmanagement systems for big data\nengineering mysql rackle database and\nthe microsoft sql server the third skill\nthat you require is to have experience\nworking with etl and warehousing tools\nnow you need to know how to construct as\nwell as use a data warehouse so that you\ncan perform the etl operation or the\nextract transform and load operations\nnow as a big data engineer you'll be\nconstantly tasked with extracting\nunstructured data from a number of\ndifferent sources transforming it into\nmeaningful information and loading it\ninto other data storages databases or\ndata warehouses now what this is is\nbasically aggregating unstructured data\nfrom multiple sources analyzing it so\nthat you can take better business\ndecisions some of the tools used for\nthis purpose are talent ibm data stage\npentaho and informatica next up we have\nthe fourth skill that you require which\nis knowledge about operating systems now\nsince most big data tools have unique\ndemands such as root access to operating\nsystem functionality as well as hardware\nhaving a strong understanding of\noperating systems like linux and unix is\nabsolutely mandatory some of the\noperating systems used by big data\nengineers are unix linux and solaris now\nthe fifth skill that you require to have\nexperience with hadoop based analytics\nsince hadoop is one of the most commonly\nused tools when it comes to big data\nengineering it's understood that you\nneed to have experience with apache\nhadoop based technologies technologies\nlike hdfs hadoop mapreduce apache ah\nbase hive and pig the sixth skill that\nyou require is to have worked with\nreal-time processing frameworks like\napache spark now as a big data engineer\nyou'll have to deal with vast volumes of\ndata so for this data you need an\nanalytics engine like spark which can be\nused for large-scale real-time data\nprocessing now spark can process live\nstreaming data from a number of\ndifferent sources like facebook\ninstagram twitter and so on it can also\nperform interactive analysis and data\nintegration and now we're at our final\nskill requirement which is to have\nexperience with data mining and modeling\nso as a data engineer you'll have to\nexamine massive pre-existing data so\nthat you can discover patterns as well\nas new information with this you will\ncreate predictive models for your\nbusiness so that you can make better\ninformed decisions some of the tools\nused for this are r rapidminer weka and\nnime now let's talk about a big data\nengineer's salary as well as other roles\nthey can look forward to now the average\nsalary of a big data engineer in the\nunited states is approximately ninety\nthousand dollars per year now this\nranges from sixty six thousand dollars\nall the way to hundred and thirty\nthousand dollars per annum in india the\naverage salary is around 7 lakh rupees\nand ranges from 4 lakhs to 13 lakhs per\nannum after you've become a big data\nengineer some of the job roles that you\ncan look forward to are that of a senior\nbig data engineer a business\nintelligence architect and a data\narchitect now let's talk about\ncertifications that a big data engineer\ncan opt for first off we have the\ncloudera ccp data engineer a cloudera\ncertified professional data engineer\npossesses the skills to develop reliable\nand scalable data pipelines that result\nin optimized data sets for a variety of\nworkloads it is one of the industry's\nmost demanding performance-based\ncertification ccp evaluates and\nrecognizes a candidate's mastery of the\ntechnical skills most sought after by\nemployers the time limit for this exam\nis 240 minutes and it costs 400 next we\nhave the ibm certified data architect\nbig data certification an ibm certified\nbig data architect understands the\ncomplexity of data and can design\nsystems and models to handle different\ndata variety including structured\nsemi-structured unstructured volume\nvelocity veracity and so on a big data\narchitect is also able to effectively\naddress information governance and\nsecurity challenges associated with the\nsystem this exam is 75 minutes long and\nfinally we have the google cloud\ncertified data engineer a google\ncertified data engineer enables data\ndriven decision making by collecting\ntransforming and publishing data they\nshould also be able to leverage deploy\nand continuously train pre-existing\nmachine learning models the length of\nthe certification exam is two hours and\nits registration fee is 200 now let's\nhave a look at how simply learn can help\nyou become a big data engineer\nsimply learn provides the big data\narchitect masters program this includes\na number of different courses like big\ndata hadoop and spark developer apache\nspark and scala mongodb developer and\nadministrator big data and hadoop\nadministrator and so much more this\ncourse goes through 50 plus in demand\nskills and tools 12 plus real life\nprojects and the possibility of an\nannual average salary of 19 to 26 lakh\nrupees per annum it will also help you\nget noticed by the top hiring companies\nthis course will also go through some\nmajor tools like kafka apache spark\nflume edge base mongodb hive pig\nmapreduce java scala and much more now\nwhy don't you head over to\nsimplylearn.com and get started on your\njourney to get certified and get thank\nyou rahul now we will have a look at the\nhadoop interview questions and answers\nour instructor ajay will guide us\nthrough it\none single machine with huge amount of\ndisks and huge amount of ram but then\nthe time taken to read that data when\nall the data is stored in one machine\nwould be very high and that would be\nwith least fault tolerance if you talk\nabout sdfs your data is distributed so\nsdfs stands for hadoop distributed file\nsystem so here your data is distributed\nand maintained on multiple systems so it\nis never one single machine it is also\nsupporting reliability so whatever is\nstored in hdfs say a file being stored\ndepending on its size is split into\nblocks and those blocks will be spread\nacross multiple nodes not only that\nevery block which is stored on a node\nwill have its replicas stored on other\nnodes replication factor depends but\nthis makes sdfs more reliable in cases\nof your slave nodes or data nodes\ncrashing you will rarely have data loss\nbecause of auto replication feature now\ntime taken to read the data is\ncomparatively more as you might have\nsituations where your data is\ndistributed across the nodes and even if\nyou are doing a parallel read your data\nread might take more time because it\nneeds coordination from multiple\nmachines however if you are working with\nhuge data which is getting stored it\nwill still be beneficial in comparison\nto reading from a single machine so you\nshould always think about its\nreliability through auto replication\nfeature its fault tolerance because of\nyour data getting stored across multiple\nmachines and its capability to scale so\nwhen you talk about sdfs we are talking\nabout horizontal scalability or scaling\nout when you talk about regular file\nsystem you are talking about vertical\nscalability which is scaling up\nnow let's look at some specific sdfs\nquestions what is this why is sdfs fault\ntolerant now as i just explained in\nprevious slides your sdfs is fault\ntolerant as it replicates data on\ndifferent data nodes so you have a\nmaster node and you have multiple slave\nnodes or data nodes where actually the\ndata is getting stored now we also have\na default block size of 128 mb that's\nthe minimum since hadoop version 2 so\nany file which is up to 128 mb would be\nusing one logical block and if the file\nsize is bigger than 128 mb then it will\nbe split into blocks and those blocks\nwill be stored across multiple machines\nnow since these blocks are stored across\nmultiple machines it makes it more fault\ntolerant because even if your machines\nfail you would still have a copy of your\nblock existing on some other machine now\nthere are two aspects here one we talk\nabout the first rule of replication\nwhich basically means you will never\nhave two identical blocks sitting on the\nsame machine and the second rule of\nreplication is in terms of rack\nawareness so if your machines are placed\nin racks as we see in the right image\nyou will never have all the replicas\nplaced on the same rack even if they are\non different machines so it has to be\nfault tolerant and it has to maintain\nredundancy so at least one replica will\nbe placed on some other node on some\nother rack that's how sdfs is fault\ntolerant now here let's understand the\narchitecture of sdfs now as i mentioned\nearlier you would in a hadoop cluster\nthe main service is your hdfs\nso for your sdfs service you would have\na name node which is your master process\nrunning on one of the machines and you\nwould have data nodes which are your\nslave machines getting stored across or\ngetting or the process processes running\nacross multiple machines each one of\nthese processes has an important role to\nplay when you talk about sdfs whatever\ndata is written to hdfs\nthat data is split into blocks depending\non its size and the blocks are randomly\ndistributed across nodes with auto\nreplication feature these blocks are\nalso\nauto replicated across multiple machines\nwith the first condition that no two\nidentical blocks will sit on the same\nmachine now as soon as the cluster comes\nup your data nodes which are part of the\ncluster and based on config files would\nstart sending their heartbeat to the\nname node and this would be every three\nseconds what does name note do with that\nname node will store this information in\nits ram so name node starts building a\nmetadata in its ram and that metadata\nhas information of what are the data\nnodes which are available in the\nbeginning now when a data\nwriting activity starts and the blocks\nare distributed across data nodes data\nnodes every 10 seconds will also send a\nblock report to name node so name node\nis again adding up this information in\nits ram or the metadata in ram which\nearlier had only data node information\nnow name node will also have information\nabout what are the files the files are\nsplit in which blocks the blocks are\nstored on which machines and what are\nthe file permissions now while name node\nis maintaining this metadata in ram name\nnode is also maintaining metadata in\ndisk now that is what we see in the red\nbox which basically has information of\nwhatever information was written to hdfs\nso to summarize your name node has\nmetadata in ram and metadata in disk\nyour data nodes are the machines where\nyour blocks or data is actually getting\nstored and then there is an auto\nreplication feature which is always\nexisting unless and until you have\ndisabled it and your read and write\nactivity is a parallel activity however\nreplication is a sequential activity now\nthis is what i mentioned about when you\ntalk about name node which is the master\nprocess hosting metadata in disk and ram\nso when we talk about disk it basically\nhas a edit log which is your transaction\nlog and your fs image which is your file\nsystem image right from the time the\ncluster was started this metadata and\ndisk was existing and this gets appended\nevery time read write or any other\noperations happen on hdfs metadata in\nram is dynamically built every time the\ncluster comes up which basically means\nthat if your cluster is coming up name\nnode in the initial few seconds or few\nminutes would be in a safe mode which\nbasically means it is busy registering\nthe information from data nodes so name\nnode is one of the most critical\nprocesses if name node is down and if\nall other processes are running you will\nnot be able to access the cluster name\nnode's metadata in disk is very\nimportant for name node to come up and\nmaintain the cluster name nodes metadata\nin ram is basically for all or\nsatisfying all your client requests now\nwhen we look at data nodes as i\nmentioned data nodes hold the actual\ndata blocks and they are sending these\nblock reports every 10 seconds so the\nmetadata in name notes ram is constantly\ngetting updated and metadata in disk is\nalso constantly getting updated based on\nany kind of write activity happening on\nthe cluster now data node which is\nstoring the block will also help in any\nkind of read activity whenever a client\nrequests so whenever a client or an\napplication or an api would want to read\nthe data it would first talk to name\nnode name node would look into its\nmetadata on ram and confirm to the\nclient which machines could be reached\nto get to that data that's where your\nclient would try to read the data from\nsdfs which is actually getting the data\nfrom data nodes and that's how your read\nwrite requests are satisfied now what\nare the two types of metadata in name\nnode server holds as i mentioned earlier\nmetadata in disk very important to\nremember edit log nfs image metadata in\nram which is information about your data\nnodes files files being split into\nblocks blocks residing on data nodes and\nfile permissions so i will share a very\ngood link on this and you can always\nlook for more detailed information about\nyour metadata so you can search for sdfs\nmetadata directories explained now this\nis from hortonworks however it talks\nabout the metadata in disk which name\nnode manages and details about this so\nhave a look at this link if you are more\ninterested in learning about metadata on\ndisk coming back\nlet's look at the next question what is\nthe difference between federation and\nhigh availability now these are the\nfeatures which were introduced in hadoop\nversion 2. both of these features are\nabout\nhorizontal scalability of name node so\nprior to version 2 the only possibility\nwas that you could have one single\nmaster which basically me means that\nyour cluster could become unavailable if\nname node would crash so hadoop version\n2 introduced two new features federation\nand high availability however high\navailability is a popular one so when\nyou talk about federation it basically\nmeans any number of name nodes so there\nis no limitation to the number of name\nnodes your name nodes are in a federated\ncluster which basically means name nodes\nstill belong to the same cluster but\nthey are not coordinating with each\nother so whenever a write request comes\nin one of the name node picks up that\nrequest and it guides that request for\nthe blocks to be written on data nodes\nbut for this your name node does not\nhave to coordinate with other name node\nto find out if the block id which was\nbeing assigned was the same one as\nassigned by other name node so all of\nthem belong to a federated cluster they\nare linked via a cluster id so whenever\nan application or an api is trying to\ntalk to cluster it is always going via\nan cluster id and one of the name node\nwould pick up the read activity or write\nactivity or processing activity so all\nthe name nodes are sharing a pool of\nmetadata in which each name node will\nhave its own dedicated pool and we can\nremember that by a term called namespace\nor name service so this also provides\nhigh fault tolerance supposed your one\nname node goes down it will not affect\nor make your cluster unavailable you\nwill still have your cluster reachable\nbecause there are other name nodes\nrunning and they are available now when\nit comes to heartbeats all your data\nnodes are sending their heart beats to\nall the name nodes and all the name\nnodes are aware of all the data nodes\nwhen you talk about high availability\nthis is where you would only have two\nname nodes so you would have an active\nand you would have a standby now\nnormally in any environment you would\nsee a\nhigh availability setup with zookeeper\nso zookeeper is a centralized\ncoordination service so when you talk\nabout your active and standby name notes\nelection of a name node to be made as\nactive and taking care of an automatic\nfailover is done by your zookeeper high\navailability can be set up without\nzookeeper but that would mean that the\nadmins intervention would be required to\nmake a name node as active from standby\nor also to take care of failover\nnow at any point of time in high\navailability a active name node would be\ntaking care of storing the edits about\nwhatever updates are happening on sdfs\nand it is also writing these edits to a\nshared location standby name node is the\none which is constantly looking for\nthese latest updates and applying to its\nmetadata which is actually a copy of\nwhatever your active name node has so in\nthis way your standby name node is\nalways in sync with the active name node\nand if for any reason active name node\nfails your standby name node will take\nover and become the active remember\nzookeeper plays a very important role\nhere it's a centralized coordination\nservice one more thing to remember here\nis that in your high availability\nsecondary name node will not be allowed\nso you would have a active name node and\nthen you will have a standby name node\nwhich will be configured on a separate\nmachine and both of these will be having\naccess to a shared location now that\nshared location could be nfs or it could\nbe a quorum of general nodes so for more\ninformation refer to the tutorial where\ni have explained about sdfs high\navailability and federation now let's\nlook at some logical question here so if\nyou have a input file of 350 mb which is\nobviously bigger than 128 mb how many\ninput splits would be created by sdfs\nand what would be the size of each input\nsplit so for this you need to remember\nthat by default the minimum block size\nis 128 mb now that's customizable if\nyour environment has more number of\nlarger files written on an average then\nobviously you have to go for a bigger\nblock size if your environment has a lot\nof files being written but these files\nare of smaller size you could be okay\nwith 128 mb remember in hadoop every\nentity that is your directory on sdfs\nfile on sdfs and a file having multiple\nblocks each of these are considered as\nobjects and for each object hadoop's\nname nodes ram 150 bytes is utilized so\nif your block size is very small then\nyou would have more number of blocks\nwhich would directly affect the name\nnode's ram if you keep a block size very\nhigh that will reduce the number of\nblocks but remember that might affect in\nprocessing because processing also\ndepends on splits more number of splits\nmore the parallel processing so\nsetting of block size has to be done\nwith consideration about your\nparallelism requirement and your name\nnodes ram which is available now coming\nto the question if you have a file of\n350 mb that would be split into three\nblocks and here two blocks would have\n128 mb data and the third block although\nthe block size would still be 128 it\nwould have only 94 mb of data so this\nwould be the split of this particular\nfile now let's\nunderstand about rack awareness how does\nrack awareness work or why do we even\nhave racks so organizations always would\nwant to place their nodes or machines in\na systematic way there can be different\napproaches you could have a rack which\nwould have machines running on the\nmaster processes and the intention would\nbe that this particular rack could have\nhigher bandwidth more cooling dedicated\npower supply top of rack switch and so\non the second approach could be that you\ncould have one master process running on\none machine of every rack and then you\ncould have other slave processes running\nnow when you talk about your rack\nawareness one thing to understand is\nthat if your machines are placed within\nracks and we are aware that hadoop\nfollows auto replication the rule of\nreplication in a rack aware cluster\nwould be that you would never have all\nthe replicas placed on the same rack so\nif we look at this if we have block a in\nblue color you will never have all the\nthree blue boxes in the same rack even\nif they are on different notes because\nthat makes us that makes it less fault\ntolerant so you would have at least one\ncopy of block which would be stored on a\ndifferent track on a different note now\nlet's look at this so basically here we\nare talking about replicas being placed\nin such a way now somebody could ask a\nquestion can i have my block and its\nreplicas spread across three lakhs and\nyes you can do that but then in order to\nmake it more redundant you are\nincreasing your bandwidth requirement so\nthe better approach would be two blocks\non the same rack on different machines\nand one copy on a different track now\nlet's proceed how can you restart name\nnode and all demons in hadoop so if you\nwere working on an apache hadoop cluster\nthen you could be doing a start and stop\nusing hadoop demonscripts so there are\nthese hadoop demons scripts which would\nbe used to start and stop your hadoop\nand this is when you talk about your\napache hadoop so let's look at one\nparticular file which i would like to\nshow you more information here and this\ntalks about your different clusters so\nlet's look into this and\nso let's look at the start and stop and\nhere i have a file let's look at this\none and this gives you highlight so if\nyou talk about apache hadoop this is how\nthe setup would be done so you would\nhave it download the adobe tar file you\nwould have to untie it edit the config\nfiles you would have to do formatting\nand then start your cluster and here i\nhave said using scripts so this is in\ncase of apache hadoop you could be using\na start all script that internally\ntriggers start dfs and start yarn and\nthese scripts start dfs internally would\nrun hadoop demon multiple times based on\nyour configs to start your different\nprocesses then your start yarn would run\nyarn demon script to start your\nprocessing related processes so this is\nhow it happens in apache hadoop now in\ncase of cloud era or hortonworks which\nis basically a vendor-specific\ndistribution you would have say multiple\nservices which would have one or\nmultiple demands running across the\nmachines let's take an example here that\nyou would have machine 1 machine 2 and\nmachine 3 with your processor spread\nacross however in case of cloudera and\nhortonworks these are cluster management\nsolutions so you would never be involved\nin running a script individually to\nstart and stop your processes in fact in\ncase of cloudera you would have a\nclouded scm server running on one of the\nmachines and then cloudera cm agents\nrunning on every machine if you talk\nabout hortonworks you would have ambari\nserver an ambari agent running so your\nagents which are running on every\nmachine are responsible to monitor the\nprocesses send also their heartbeat to\nthe master that is your server and your\nserver is the one or a service which\nbasically will give instructions to the\nagents so in case of vendor specific\ndistribution your start and stop of\nprocesses is automatically taken care by\nthese underlying services and these\nservices internally are still running\nthese commands however only in apache\nhadoop you have to manually follow these\nto start and stop coming back we can\nlook into\nsome command related questions so which\ncommand will help you find the status of\nblocks and file system health so you can\nalways go for a file system check\ncommand now that can show you the files\nfor a particular sdfs path it can show\nyou the blocks and it can also give you\ninformation on\nstatus such as under replicated blocks\nover replicated blocks misreplicated\nblocks default replication and so on so\nyour fsck file system check utility does\nnot repair if there is any problem with\nthe blocks but it can give you\ninformation of\nblocks related to the files on which\nmachines they have stored if they are\nreplicated as per the replication factor\nor if there is any problem with any\nparticular replica now what would happen\nif you store too many small files in a\ncluster and this relates to the block\ninformation which i gave some time back\nso remember hadoop is coded in java so\nhere every directory every file and file\nrelated block is considered as an object\nand for every object within your hadoop\ncluster name node's ram gets utilized so\nmore number of blocks you have more\nwould be usage of name nodes ram and if\nyou're storing too many small files it\nwould not affect your disk it would\ndirectly affect your name nodes ram\nthat's why in production clusters admin\nguys or infrastructure specialist will\ntake care that everyone who is writing\ndata to hdfs follows a quota system so\nthat you could be controlled in the\namount of data you write plus the count\nof data and individual rights on hdfs\nnow how do you copy data from local\nsystem onto sdfs so you can use a put\ncommand or a copy from local and then\ngiven your local path which is your\nsource and then your destination which\nis your sdfs path remember you can\nalways do a copy from local using a\nminus f option that's a flag option and\nthat also helps you in writing the same\nfile or a new file to hdfs\nso with your minus f you have a chance\nof overwriting or rewriting the data\nwhich is existing on sdfs so copy from\nlocal or minus put both of them do the\nsame thing and you can also pass an\nargument when you're copying to control\nyour replication or other aspects of\nyour file now when do you use dfs admin\nrefresh nodes or rm admin refresh nodes\nso as the command says this is basically\nto do with refreshing the node\ninformation so your refresh nodes is\nmainly used\nwhen say a commissioning or\ndecommissioning of nodes is done so when\na node is added into the cluster or when\na node is removed from the cluster you\nare actually informing hadoop master\nthat this particular node would not be\nused for storage and would not be used\nfor processing now in that case you\nwould be once you are done with the\nprocess of commissioning or\ndecommissioning you would be giving\nthese commands that is refreshed nodes\nand rm admin refresh nodes so internally\nwhen you talk about commissioning\ndecommissioning there are include and\nexclude files which are updated and\nthese include and exclude files will\nhave entry of machines which are being\nadded to the cluster or machines which\nare being removed from the cluster and\nwhile this is being done the cluster is\nstill running so you do not have to\nrestart your master process however you\ncan just use these refresh commands to\ntake care of your commissioning\ndecommissioning activities now is there\nany way to change replication of files\non sdfs after they are already written\nand the answer is of course yes so if\nyou would want to set a replication\nfactor at a cluster level and if you\nhave admin access then you could edit\nyour sdfs hyphen site file or you could\nsay hadoop hyphen site file and that\nwould take care of replication factor\nbeing set at a cluster level however if\nyou would want to change the replication\nafter the data has been written you\ncould always use a setrep command so\nsetrep command is basically to change\nthe replication after the data is\nwritten\nyou could also\nwrite the data with a different\nreplication and for that you could use a\nminus d dfs dfs.replication\nand give your application factor when\nyou are writing data to the cluster so\nin hadoop you can let your data be\nreplicated as per the property set in\nthe config file you could write the data\nwith a different replication\nyou could change the replication after\nthe data is read so all these options\nare available now who takes care of\nreplication consistency in a hadoop\ncluster and what do you mean by under\nover replicated blocks now as i\nmentioned your fsck command can give you\ninformation of over or under replicated\nblocks now in a cluster it is always an\nalways name node which takes care of\nreplication consistency so for example\nif you have set up a replication of\nthree and since we know the first rule\nof replication which basically means\nthat you cannot have two replicas\nresiding on the same note it would mean\nthat if your application is three we\nwould need at least three data nodes\navailable now say for example you had a\ncluster with three nodes and replication\nwas set to three at one point of time\none of your name node crashed and if\nthat happens your blocks would be under\nreplicated that means there was a\nreplication factor set but now your\nblocks are not replicated or there are\nnot enough replicas as per the\nreplication factor set this is not a\nproblem your master process or name node\nwill wait for some time before it will\nstart the replication of data again so\nif a data road is not responding or if a\ndisk has crashed and if name node does\nnot get information of a replica name\nnode will wait for some time and then it\nwill start re-replication of those\nmissing blocks from the available nodes\nhowever while name node is doing it the\nblocks are in under replicated situation\nnow when you talk about over replicated\nthis is a situation where name node\nrealizes that there are extra copies of\nblock now this might be the case that\nyou had three nodes running with a\nreplication of three one of the node\nwent down due to a network failure or\nsome other issue within few minutes name\nnode re-replicated the data and then the\nfailed node is back with its set of\nblocks again name node is smart enough\nto understand that this is a over\nreplication situation and it will delete\nset of blocks from one of the nodes it\nmight be the node which has been\nrecently added it might be your old node\nwhich has joined your cluster again or\nany node that depends on the load on a\nparticular node now we discussed about\nhadoop we discussed about sdfs now we\nwill discuss about mapreduce which is\nthe programming model and you can say\nprocessing framework what is distributed\ncache in mapreduce now we know that when\nwe talk about mapreduce\nthe data which has to be processed might\nbe existing on multiple nodes so when\nyou would have your mapreduce program\nrunning it would basically read the data\nfrom the underlying disks\nnow this could be a costly operation if\nevery time the data has to be read from\ndisk\nso distributed cache is a mechanism\nwherein\ndata set or data which is coming from\nthe disk can be cached\nand available for all worker nodes now\nhow will this benefit so when a map\nreduce is running instead of every time\nreading the data from disk\nit would pick up the data from\ndistributed cache and this this will\nbenefit your mapreduce processing so\ndistributed cache can be set in your job\nconf where you can specify that a file\nshould be picked up from distributed\ncache now let's understand about these\nroles so what is a record reader what is\na combiner what is a partitioner and\nwhat kind of roles do they play in a map\nreduce processing paradigm or map reduce\noperation so record reader communicates\nwith the input split and it basically\nconverts the data into key value pairs\nand these key value pairs are the ones\nwhich will be worked upon by the mapper\nyour combiner is an optional face it's\nlike mini radius so combiner does not\nhave its own class it relies on the\nreducer class\nbasically your combiner would receive\nthe data from your map tasks\nwhich would have completed works on it\nbased on whatever reducer class mentions\nand then passes its output to the\nreducer phase partitioner is basically a\nphase which decides how many reduced\ntasks would be used aggregate or\nsummarize your data so partitioner is a\nphase which would decide based on the\nnumber of keys based on the number of\nmap tasks your partitioner would decide\nif one or multiple reduced tasks would\nbe used to take care of processing so\neither it could be partitioner which\ndecides on how many reduced tasks would\nrun or it could be based on the\nproperties which we have set within the\ncluster which will take care of the\nnumber of reduced tasks which would be\nused always remember your partitioner\ndecides how outputs from combiner are\nsent to reducer and to how many reducers\nit controls the partitioning of keys of\nyour intermediate map outputs so map\nphase whatever output it generates is an\nintermediate output\nand that has to be taken by your\npartitioner or by a combiner and then\npartitioner to be sent to one or\nmultiple reduced tasks this is one of\nthe common questions which you might\nface\nwhy is mapreduce lower\nin processing so we know mapreduce goes\nfor parallel processing we know we can\nhave multiple map tasks running on\nmultiple nodes at the same time we also\nknow that multiple reduced tasks could\nbe running now why does then mapreduce\nbecome a slower approach\nfirst of all your mapreduce is a batch\noriented operation now mapreduce\nis very rigid and it strictly uses\nmapping and reducing phases so no matter\nwhat kind of processing you would want\nto do you would have to still provide\nthe mapper function and the reducer\nfunction to work on data not only this\nwhenever your map phase completes\nthe output of your map face which is an\nintermittent output would be written to\nhdfs and thereafter underlying disks and\nthis data would then be shuffled and\nsorted and picked up for reducing phase\nso every time your data being written to\nhdfs and retrieved from sdfs makes\nmapreduce a slower approach the question\nis for a mapreduce job is it possible to\nchange the number of mappers to be\ncreated\nnow by default you cannot change the\nnumber of map tasks because number of\nmap tasks depends on the input splits\nhowever there are different ways in\nwhich you can either set a property to\nhave more number of map tasks which can\nbe used\nor you can\ncustomize your code or\nmake it use a different format which can\nthen control the number of map tasks\nby default number of map tasks\nare equal to the number of splits of\nfile you are processing\nso if you have a 1 gb of file that is\nsplit into 8 blocks of 128 mb there\nwould be 8 map tasks running on the\ncluster these map tasks are basically\nrunning your mapper function\nif you have hard coded properties in\nyour map red hyphen site file to specify\nmore number of map tasks then you could\ncontrol the number of map tasks\nlet's also talk about some data types so\nwhen you prepare for hadoop when you\nwant to get into big data field you\nshould start learning about different\ndata formats now there are different\ndata formats such as avro parquet\nyou have\nsequence file or binary format and these\nare different formats which are used now\nwhen you talk about your data types\nin hadoop these are implementation of\nyour writable and writable comparable\ninterfaces so for every data type\nin java you have a equivalent in hadoop\nso end in java would be intriguable in\nhadoop float would be float writable\nlong would be long writable double\nwritable boolean writable array writable\nmap writable and object credible so\nthese are your different data types that\ncould be used within your mapreduce\nprogram\nand these are implementation of writable\nand writable comparable interfaces what\nis speculative execution\nnow imagine you have a cluster which has\nhuge number of nodes and your data is\nspread across multiple slave machines or\nmultiple nodes now at a point of time\ndue to a disk degrade\nor network issues\nor machine heating up or more load being\non a particular node there can be a\nsituation where your data node will\nexecute task in a slower manner now in\nthis case if speculative execution is\nturned on there would be a shadow task\nor a another\nsimilar task running on some other node\nfor the same processing so whichever\ntask finishes first will be accepted and\nthe other task would be killed so\nspeculative execution could be good if\nyou are working in an intensive workload\nkind of environment where if a\nparticular node is slower you could\nbenefit from a unoccupied or a node\nwhich has less load to take care of your\nprocessing going further this is how we\ncan understand so node a which might be\nhaving a slower task you would have a\nscheduler which is maintaining or\nhaving knowledge of what are the\nresources available so if speculative\nexecution as a property is turned on\nthen the task which was running slow a\ncopy of that task or you can say shadow\ntask would run on some other node and\nwhichever task completes first will be\nconsidered this is what happens in your\nspeculative execution now how is\nidentity mapper different from chain\nmapper now this is where we are getting\ndeeper into mapreduce concepts so when\nyou talk about mapper identity mapper is\nthe default mapper which is chosen when\nno mapper is specified in mapreduce\ndriver class so for every mapreduce\nprogram you would have a map class\nwhich is taking care of your mapping\nphase which basically has a mapper\nfunction and which would run one or\nmultiple map tasks right your\nprogramming your program would also have\na reduce class which would be running a\nreducer function which takes care of\nreduced tasks running on multiple nodes\nnow if a mapper is not specified within\nyour driver class so driver class is\nsomething which has all information\nabout your flow what's your map class\nwhat is your reduced class what's your\ninput format what's your output format\nwhat are the job configurations and so\non so identity mapper is the default\nmapper which is chosen when no mapper\nclass is mentioned in your driver class\nit basically implements an identity\nfunction which directly writes all its\nkey pairs into output and it was defined\nin old mapreduce api in this particular\npackage but when you talk about chaining\nmappers or chain mapper this is\nbasically a class to run multiple\nmappers in a single map task or\nbasically you could say multiple map\ntasks would run as a part of your\nprocessing the output of first mapper\nwould become as an input to second\nmapper and so on and this can be defined\nin the under mentioned class or package\nwhat are the major configuration\nparameters required in a mapreduce\nprogram obviously we need to have the\ninput location we need to have the\noutput location so input location is\nwhere the files will be picked up from\nand this would preferably on sdfs\ndirectory output location is the path\nwhere your job output would be written\nby your mapreduce program you also need\nto specify input and output formats if\nyou don't specify the defaults are\nconsidered then we need to also have the\nclasses which have your map and reduce\nfunctions\nand if you intend to run the code on a\ncluster you need to package your class\nin a jar file export it to your cluster\nand then this jar file would have your\nmapper reducer and driver classes so\nthese are important configuration\nparameters\nwhich you need to consider for a\nmapreduce program now what is the\ndifference or what do you mean by map\nside join and reduce side join map side\njoin is basically when the join is\nperformed at the mapping level or at the\nmapping phase or is performed by the\nmapper so each input data which is being\nworked upon has to be divided into same\nnumber of partitions\ninput to each map is in the form of a\nstructured partition and is in sorted\norder so map site join you can\nunderstand it in a simpler way that if\nyou compare it with rdbms concepts where\nyou had two tables which were being\njoined it will always be advisable to\ngive your bigger table as the left side\ntable or the first table for your join\ncondition\nand it would be your smaller table on\nthe left side and your bigger table on\nthe right side which basically means the\nsmaller table could be loaded in memory\nand could be used for joining so map\nside drawing is a similar kind of\nmechanism where input data is divided\ninto same number of partitions when you\ntalk about reduced side join here the\njoin is performed by the reducer so it\nis easier to implement than website join\nas all the sorting and shuffling will\nsend the values\nor send all the values having identical\nkeys to the same reducer so you don't\nneed to have your data set in a\nstructured form so look into your map\nside join or reduce side join and other\njoins just to understand how mapreduce\nworks however i would suggest not to\nfocus more on this because mapreduce is\nstill being used for processing but the\namount of map reduced base processing\nhas decreased overall or across the\nindustry now what is the role of output\ncommitter class in a map reduced job so\noutput committer as the name says\ndescribes the commit of task output for\na mapreduce job so we could have this as\nmentioned or capacity hadoop mapreduce\noutput committer you could have a class\nwhich extends your output committer\nclass\nso mapreduce relies on this mapreduce\nrelies on the output committer of the\njob to\nset up the job initialization cleaning\nup the job after the job completion that\nmeans all the resources which were being\nused by a particular job setting up the\ntask temporary output checking whether a\ntask needs a commit committing the task\noutput and discarding the task out so\nthis is a very important class and can\nbe used within your mapreduce job what\nis the process of spilling in mapreduce\nwhat does that mean\nso spilling is basically a process of\ncopying the data from memory buffer to\ndisk when obviously the buffer usage\nreaches a certain threshold so if there\nis not enough memory in your buffer in\nyour memory then the content which is\nstored in buffer or memory has to be\nflushed out so by default a background\nthread starts spilling the content from\nmemory to disk after eighty percent of\nbuffer size is filled now when is the\nbuffer being used so when your mapreduce\nprocessing is happening the data from\ndata is being read from the disk loaded\ninto the buffer and then some processing\nhappens\nsame thing also happens when you are\nwriting data to the cluster so you can\nimagine for a 100 megabyte size buffer\nthe spilling will start after the\ncontent of buffer reaches 80 megabytes\nthis is customizable how can you set the\nmappers and reducers for a mapreduce job\nso these are the properties\nso number of mappers and reducers as i\nmentioned earlier can be customized so\nby default your number of map tasks\ndepends on the split and number of\nreduced tasks depends on the\npartitioning phase which decides number\nof reduced tasks which would be used\ndepending on the keys however we can set\nthese properties either in the config\nfiles or provide them on the command\nline or also make them part of our code\nand this can control the number of map\ntasks or reduce tasks which would be run\nfor a particular job let's look at one\nmore interesting question what happens\nwhen a node running a map task fails\nbefore sending the output to the reducer\nso there was a node which was running a\nmap task and we know that there could be\none or multiple map tasks running on one\nor multiple nodes and all the map tasks\nhave to be completed before the further\nstages that such as combiner or reducer\ncome into existence so in a case if a\nnode crashes where a map task was\nassigned to it the whole task will have\nto be run again on some other node so in\nhadoop version 2 yarn framework has a\ntemporary demon called application\nmaster so your application master is\ntaking care of execution of your\napplication and if a particular task on\na particular node failed due to\nunavailability of node it is the role of\napplication master to have this task\nscheduled on some other node now can we\nwrite the output of mapreduce in\ndifferent formats of course we can so\nhadoop supports various input and output\nformats so you can write the output of\nmapreduce in different formats so you\ncould have the default format that is\ntext output format wherein records are\nwritten as line of text you could have\nsequence file which is basically to\nwrite sequence files or your binary\nformat files where your output files\nneed to be fed into another mapreduce\njobs you could go for a map file output\nformat to write output as map files you\ncould go for a sequence file as a binary\noutput format so that's again a variant\nof your\nsequence file input format it basically\nwrites keys and values to a sequence\nfile so when we talk about binary format\nwe are talking about a non-human\nreadable format db output format now\nthis is basically used when you would\nwant to write data to say relational\ndatabases or say no sql databases such\nas hbase so this format also sends the\nreduce output to a sql table now let's\nlearn a little bit about yarn yarn which\nstands for yet another resource\nnegotiator it's the processing framework\nso what benefits did yan bring in hadoop\nversion 2 and how did it solve the\nissues of mapreduce version 1. so\nmapreduce version 1 had major issues\nwhen it comes to scalability or\navailability because sorry in hadoop\nversion 1 you had only one master\nprocess for processing layer and that is\nyour job tracker so your job tracker was\nlistening to all the task trackers which\nwere running on multiple machines so\nyour job tracker was\nresponsible for resource tracking and\njob scheduling in yarn you still have a\nprocessing master but that's called\nresource manager instead of job tracker\nand now with hadoop version 2 you could\neven have resource manager running in\nhigh availability mode you have node\nmanagers which would be running on\nmultiple machines and then you have a\ntemporary demon called application\nmaster so in case of hadoop version 2\nyour resource manager or master is only\nhandling the client connections and\ntaking care of tracking the resources\nthe jobs scheduling are basically taking\ncare of execution across multiple nodes\nis controlled by application master till\nthe application completes\nso in yarn you can have different kind\nof resource allocations that could be\ndone and there is a concept of container\nso container is basically a combination\nof ram and cpu cores yarn can run\ndifferent kind of workloads so it is not\njust mapreduce kind of workload which\ncan be run on auto version 2 but you\nwould have graph processing massive\nparallel processing you could have a\nreal-time processing and huge processing\napplications could run on a cluster\nbased on yarn so when we talk about\nscalability\nin case of your hadoop version 2 you can\nhave a cluster size of more than 10 000\nnodes and can run more than 100 000\nconcurrent tasks and this is because for\nevery application which is launched you\nhave this temporary demon called\napplication master so if i would have 10\napplications running i would have 10 app\nmasters running taking care of execution\nof these applications across multiple\nnodes compatibility so hadoop version 2\nis fully compatible with whatever was\ndeveloped as per hadoop version 1 and\nall your processing needs would be taken\ncare by yarn\nso dynamic allocation of cluster\nresources taking care of different\nworkloads\nallocating resources across multiple\nmachines and using them for execution\nall that is taken care by\nmulti-tenancy which basically means you\ncould have multiple users or multiple\nteams\nyou could have open source and\nproprietary data access engines and all\nof these could be basically hosted using\nthe same cluster now how does yarn\nallocate resources to an application\nwith help of its architecture so\nbasically you have a client or an\napplication or an api which talks to\nresource manager resource manager is as\ni mentioned\nmanaging the resource allocation in the\ncluster when you talk about resource\nmanager you have its internal two\ncomponents one is your scheduler and one\nis your applications manager so when we\nsay resource manager being the master is\ntracking the resources the source\nmanager is the one which is negotiating\nthe resources with slave it is not\nactually resource manager who is doing\nit but these internal components\nso you have a scheduler which allocates\nresources to various running\napplications so scheduler is not\nbothered about tracking your resources\nor basically tracking your applications\nso we can have different kind of\nschedulers such as fifo which is first\nin first out you could have a fair\nscheduler or you could have a capacity\nscheduler and these schedulers basically\ncontrol how resources are allocated to\nmultiple applications when they are\nrunning in parallel so there is a queue\nmechanism so scheduler will schedule\nresources based on requirements of\napplication but it is not monitoring or\ntracking the status of applications\nyour applications manager is the one\nwhich is accepting the job submissions\nit is monitoring and restarting the\napplication masters so it's application\nmanager which is basically then\nlaunching a application master which is\nresponsible for\nan application so this is how it looks\nso whenever a job submission happens we\nalready know that resource manager is\naware of the resources which are\navailable with every node manager so on\nevery node which has fixed amount of ram\nand cpu cores\nsome portion of resources that is your\nram and cpu cores are allocated to node\nmanager now resource manager is already\naware of how much resources are\navailable across nodes so whenever a\nclient request comes in resource manager\nwill make a request to node manager it\nwill basically request node manager to\nhold some resources for processing node\nmanager would basically approve or\ndisapprove this request of holding the\nsources\nand these resources that is a\ncombination of ram and cpu cores are\nnothing but containers we can allocate\ncontainers of different sizes within\nyarn hyphen site file so your node\nmanager based on a request from resource\nmanager guarantees the container which\nwould be available for processing that's\nwhen your resource manager starts a\ntemporary demand called application\nmaster to take care of your execution so\nyour app master which was launched by\nresource manager or we can say internal\naway applications manager will run in\none of the containers because\napplication master is also a piece of\ncode so it will run in one of the\ncontainers and then other containers\nwill be utilized for execution\nthis is how yarn is basically taking\ncare of your allocation your application\nmaster is managing resource needs it is\nthe one which is interacting with\nscheduler and if\na particular\nnode crashes it is the responsibility of\napp master to go back to the master\nwhich is resource manager and negotiate\nfor more resources\nso your app master will never ever\nnegotiate resources with node manager\ndirectly it will always talk to resource\nmanager and the source manager is the\none which negotiates the resources\ncontainer as i said is a collection of\nresources like your ram cpu network\nbandwidth and your container is\nallocated based on the availability of\nresources on a particular node so which\nof the following has occupied the place\nof a job tracker of mapreduce\nso it is your resource manager so\nresource manager is the name of the\nmaster process in adobe portion 2. now\nif you would have to\nwrite yarn commands to check the status\nof an application so we could just say\nyarn application minus status and then\nthe application id and you could kill it\nalso from the command line remember your\nyarn has a ui and you can even look at\nyour applications from the ui you can\neven kill your applications from the ui\nhowever knowing the command line\ncommands would be very useful can we\nhave more than one resource manager in a\nyoung base cluster yes we can that is\nwhat hadoop version 2 allows us to have\nso you can have a high availability yarn\ncluster where you have a active and\nstandby and the coordination is taking\ncare by your zookeeper at a particular\ntime there can only be one active\nresource manager and if active resource\nmanager fails your standby resource\nmanager comes\nand becomes active however zookeeper is\nplaying a very important role remember\nzookeeper is the one which is\ncoordinating the server state and it is\ndoing the election of active to standby\nfailover what are the different\nschedulers available in yarn so you have\na fifo scheduler that is\nfirst in first out and this is not a\ndesirable option because in this case a\nlonger running application might block\nall other small running applications\nyour capacity scheduler is basically a\nscheduler where dedicated queues are\ncreated and they have fixed amount of\nresources so you can have multiple\napplications accessing the cluster at\nthe same time and they would be using\ntheir own queues\nand the resources allocated to them if\nyou talk about fair scheduler you don't\nneed to have a fixed amount of\nresources you can just have a percentage\nand you could decide what kind of\nfairness is to be followed which\nbasically means that if you were\nallocated 20 gigabytes of memory however\nthe cluster has 100 gigabytes and the\nother team was assigned 80 gigabytes of\nmemory then you have 20 percent access\nto the cluster and other team has 80\nhowever if the other team does not come\nup or does not use the cluster in a fair\nscheduler you can go up to maximum 100\nof your cluster to find out more\ninformation about your schedulers you\ncould either look in hadoop definitive\nguide or what you could do is you could\njust go to google and you could type for\nexample yarn\nscheduler\nlet's search for yarn scheduler and then\nyou can look in\nhadoop definitive guide\nand so this is your hadoop definitive\nguide and it beautifully explains about\nyour different schedulers how do\nmultiple applications run and that could\nbe in your\nfifo kind of scheduling it could be in\ncapacity scheduler or it could be in a\nfair scheduling so have a look at this\nlink it's a very good link you can also\nsearch for yarn untangling and this is a\nblog of four or this is a series of four\nblocks where it's beautifully explained\nabout your yarn how it works how the\nresource allocation happens what is a\ncontainer and what runs within the\ncontainer so you can scroll down you can\nbe reading through this and you can then\nalso search for part two of it which\ntalks about allocation and so on so\ncoming back\nwe basically have these schedulers what\nhappens if a resource manager fails\nwhile executing an application in a high\navailability cluster so in a high\navailability cluster we know that we\nwould have two resource managers one\nbeing active one being standby and\nzookeeper which is keeping a track of\nthe server states so if a rm fails in\ncase of high availability the\nstandby will be elected as active and\nthen basically your resource manager or\nthe standby would become the active one\nand this one would instruct the\napplication master to abort in the\nbeginning then your resource manager\nrecovers its running state so there is\nsomething called as rm state store where\nall the applications which are running\ntheir status is stored so resource\nmanager recovers\nits running state by looking at your\nstate store\nby taking advantage of container\nstatuses and then continues to take care\nof your processing now in a cluster of\n10 data nodes each having 16 gb and 10\ncores what would be total processing\ncapacity of the cluster take a minute to\nthink 10 data nodes 16 gb ram per node\n10 cores\nso if you mention the answer as 160 gb\nram\nand 100 cores then\nyou went wrong now think of a cluster\nwhich has 10 data nodes each having 16\ngb ram and 10 cores remember on every\nnode in a hadoop cluster you would have\none or multiple processors running those\nprocesses would need ram the machine\nitself which has a linux file system\nwould have its own processes so that\nwould also be having some ram usage\nwhich basically means that if you talk\nabout 10 data nodes you should deduct at\nleast 20 to 30 percent towards the\noverheads towards the cloud database\nservices towards the other processes\nwhich are running and in that case i\ncould say that you could have 11 or 12\ngb available on every machine for\nprocessing and say 6 or 7 cores multiply\nthat by 10 and that's your processing\ncapacity remember the same thing applies\nto the disk usage also so if somebody\nasks you in a 10 data node cluster where\neach machine has 20 terabytes of disks\nwhat is my total storage capacity\navailable for sdfs so the answer would\nnot be 200 you have to consider the\noverheads and this is basically which\ngives you your processing capacity now\nlet's look at one more question so what\nhappens if requested memory or cpu cores\nbeyond or goes beyond the size of\ncontainer now as i said you can have\nyour configurations which can say that\nin a particular data node which has 100\ngb ram i could allocate say 50 gb\nfor the processing like out of 100 cores\ni could say 50 cores for processing so\nif you have 100 gb ram and 100 cores you\ncould ideally allocate 100 for\nprocessing but that's not ideally\npossible so if you have 100 gb ram you\nwould go for 50 gb and if you have 100\ncores you would go for 50 cores now\nwithin this ram and cpu course you have\nthe concept of containers right so\ncontainer is a combination of ram and\ncpu cores so you could have a minimum\nsize container and maximum size\ncontainer now at any point of time if\nyour application starts demanding\nmore memory or more cpu cores\nand this cannot fit into a container\nlocation your application will fail your\napplication will fail because you\nrequested for a memory or a combination\nof memory and cpu cores which is\nmore than the maximum container size now\nhere we will discuss on hive peg hbase\nand these components of do which are\nbeing used in the industry for various\nuse cases let's look at some questions\nhere and let's look how you should\nprepare for them so first of all we will\nlearn on hive which is a data\nwarehousing package so the question is\nwhat are the different components of a\nhive architecture now when we talk about\nhive we already know that hive is a data\nwarehousing package which basically\nallows you to work on structured data or\ndata which can be structuralized so\nnormally people are well versed with\nquerying or basically processing the\ndata using sql queries a lot of people\ncome from database backgrounds and they\nwould find it comfortable if they know\nstructured query language hive is one of\nthe data warehousing package which\nresides within a hadoop ecosystem it\nuses hadoop's distributed file system to\nstore the data and it uses rdbms usually\nto store the metadata although metadata\ncan be stored locally also so what are\nthe different components of an hive\narchitecture so it has a user interface\nso user interface calls the execute\ninterface to the driver this creates a\nsession to the query and then it sends\nthe query to the compiler to generate an\nexecution plan for it usually whenever\nhive is set up it would have its\nmetadata stored in an rdbms now to\nestablish the connection between rdbms\nand hadoop we need odbc or jdbc\nconnector jar file and that connector\njar file has a driver class now this\ndriver class is mandatory to create a\nconnection between hive and hadoop so\nuser interface creates this interface\nusing the driver now we have metastore\nmetastore stores the metadata\ninformation so any object which you\ncreate such as database table indexes\ntheir metadata is stored in metastore\nand usually this metastore is stored in\nan rdbms so that multiple users can\nconnect to hive so your metastore stores\nthe metadata information and sends that\nto the compiler for execution of a query\nwhat does the compiler do it generates\nthe execution plan it has a dag now dag\nstands for direct acyclic graph so it\nhas a dag of stages where each stage is\neither a metadata operation a map or\nreduced job or an operation on sdfs and\nfinally we have execution engine that\nacts as a bridge between hive and hadoop\nto process the query so execution engine\ncommunicates bi-directionally with\nmetastore to perform operations like\ncreate or drop tables so these are four\nimportant components of hive\narchitecture\nnow what is the difference between\nexternal table and manage table and\nheight\nso we have various kinds of table in\nheight such as external table manage\ntable partition table the major\ndifference between your managed and\nexternal table is in respect to what\nhappens to the data if the table is\ndropped usually whenever we create a\ntable in hive it creates a manage table\nor we could also call that as an\ninternal table now this manages the data\nand moves it into warehouse directory by\ndefault whether you create a manage\ntable or external table usually the data\ncan recite in hive's default warehouse\ndirectory or it could be residing in a\nlocation chosen however when we talk\nabout manage table if one drops a manage\ntable not only the metadata information\nis deleted but the table's data is also\ndeleted from sdfs if we talk about\nexternal table it is created with an\nexternal keyword explicitly and if an\nexternal table is dropped nothing\nhappens to the data which resides in\nsdfs so that's the main difference\nbetween your managed and external table\nwhat might be the use case if somebody\nasks you there might be a migration kind\nof activity or you are interested in\ncreating a lot of tables\nusing your queries so in that case you\ncould dump all the data on sdfs and then\nyou could create a table by pointing to\na particular directory or multiple\ndirectories now you could then do some\ntesting of your tables and would decide\nthat you might not need all the tables\nso in that case it would be advisable to\ncreate external tables so that even if\nthe table is later dropped the data on\nsdfs will be intact unlike your manage\ntable where dropping of table will\ndelete the data from sdfs also let's\nlearn a little bit on partition so what\nis partition and hive and why is\npartitioning required in hive if\nsomebody asks you that now normally in\nworld of rdbms partitioning is the\nprocess to group similar type of data\ntogether and that is usually done on\nbasis of a column or what we call as\npartitioning key now each table usually\nhas one column in context of rdbms which\ncould be used to partition the data and\nwhy do we do that so that we can avoid\nscanning the complete table for a query\nand restrict the scan to\nset of data or to a particular partition\nin hive we can have any number of\npartition keys so partitioning provides\ngranularity in hive table it reduces the\nquery latency by scanning only relevant\npartition data instead of whole data set\nwe can partition at various levels now\nif i compare rdbms with hive in case of\nrdbms you could have\none column which could be used for\npartitioning and then you could be\nsquaring the specific partition so in\ncase of rdbms your partition column is\nusually a part of the table definition\nso for example if i have an employee\ntable i might have employee id employee\nname employee age and employee salary as\nfour columns and i would decide to\npartition the table based on salary\ncolumn now why would i partition it\nbecause i feel that employee table is\ngrowing very fast it is or it will have\nhuge amount of data and later when we\nquery the table we don't want to scan\nthe complete table so i could split my\ndata into multiple partition based on a\nsalary column giving some ranges in hive\nit is a little different\nin hive you can do partitioning and\nthere is a concept of static and dynamic\npartitioning but in hive the partition\ncolumn is not part of table definition\nso you might have an employee table with\nemployee id\nname age\nand that that's it that would be the\ntable definition but you could then have\npartitioning done based on salary column\nwhich will then create a specific folder\non sdfs in that case when we query the\ndata we can see the partition column\nalso showing up so we can partition the\ntransaction data for a bank for example\nbased on month like chan feb etc and any\noperation regarding a particular month\nwill then allow us to query that\nparticular folder that is where\npartitioning is useful now why does hive\nnot store metadata information in sdfs\nif somebody asks you so we know that\nhives data is stored in sdfs which is\nhadoop distributed file system however\nthe metadata is either stored locally\nand that mode of hive would be called as\nembedded mode or you could have hives\nmetadata stored in rdbms so that\nmultiple clients can initiate a\nconnection now this metadata which is\nvery important for hive would not be\nstored in sdfs so we already know that\nsdfs read and write operations are time\nconsuming it is a distributed file\nsystem and it can accommodate huge\namount of data so hive stores metadata\ninformation in meta store using rdbms\ninstead of sdfs so this allows to\nachieve low latency and faster data\naccess\nnow if somebody asks what are the\ncomponents used in hive query processor\nso usually we have the main components\nare your parser your execution engine\nlogical plan generation optimizer and\ntype checking so whenever a query is\nsubmitted it will go through a parser\nand parser would check the syntax it\nwould check for objects which are being\nqueried and other things to see if the\nquery is fine now internally you have a\nsemantic analyzer which will also look\nat the query you have an execution\nengine which basically will work on the\nexecution part that is the best\ngenerated execution plan which could be\nused to get the results for the query\nyou could also have user defined\nfunctions which a user would want to use\nand these are normally created in\njava or java programming language and\nthen basically these user defined\nfunctions are added to the class path\nnow you would have a logical plan\ngeneration which basically looks at your\nquery and then generates a logical plan\nor the best execution path which would\nbe required to get to the results\ninternally there is a physical plan\ngenerated which is then looked in by\noptimizer to get the best path to get to\nthe data and that might also be checking\nyour different operators which you are\nusing within your query finally we would\nalso have type checking so these are\nimportant components in hype so somebody\nmight ask you if you are querying your\ndata using hive what are the different\ncomponents involved or if you could\nexplain what are the different\ncomponents which\nwork when a query is submitted so these\nare the components now let's look a\nscenario based question suppose there\nare a lot of small csv files which are\npresent in a sdfs directory and you want\nto create a single hive table from these\nfiles so data in these files have fields\nlike registration number name email\naddress\nso if this is what needs to be done what\nwill be your approach to solve it where\nwill you create a single hive table for\nlots of small files without degrading\nthe performance of the system so there\ncan be different approaches now we know\nthat there are a lot of small csv files\nwhich are present in a directory so we\nknow that when we create a table in hive\nwe can use a location parameter so i\ncould say create table give a table name\ngive the column and their data types i\ncould specify the delimiters and finally\ni could say location and then point it\nto a directory on sdfs and this\ndirectory might be the directory which\nhas lot of csv files so in this case i\nwill avoid loading the data in the table\nbecause table being point table pointing\nto the directory will directly pick up\nthe data from one or multiple files and\nwe also know that hive does schema check\non read so it does not do a schema check\non write so in case there were one or\ntwo files which did not follow the\nschema of the table it would not prevent\ndata loading data would anyways be\nloaded only when you query the data it\nmight show you null values if data which\nwas loaded does not follow the schema of\nthe table this is one approach what is\nthe other approach so let's look at that\nyou can think about sequence file format\nwhich is basically a smart format or a\nbinary format and you can group these\nsmall files together to form a sequence\nfile now this could be one other smarter\napproach so we could create a temporary\ntable so we could say create table give\na table name give the column names and\ntheir data types we could specify the\ndelimiters as it shows here that is row\nformat and fields terminated by and\nfinally we can store that as text file\nthen we can load data into this table by\ngiving a local file system path and then\nwe can create a table that will store\ndata in sequence file format so my point\none is storing the data instead text\nfile point three would be storing the\ndata in sequence file format so we say\ncreate table give the specifications we\nsay row format delimited fields are\nterminated by comma stored as sequence\nfile then we can move the data from test\ntable into test sequence file table so i\ncould just say insert overwrite my new\ntable as\nselect star from other table remember in\nhive you cannot do insert update delete\nhowever if the table is existing you can\ndo a insert overwrite from an existing\ntable into a new table so this could be\none approach where we could have a lot\nof csv files or smaller files club\ntogether as one big sequence file and\nthen store it in the table now if\nsomebody asks you write a query to\ninsert a new column that is\ninteger data type into a hive table and\nthe requirement might be that you would\nwant to insert this table at a position\nbefore an existing column now that's\npossible by doing an alter table giving\nyour table name and then specifying\nchange column giving you a new column\nwith the data type before an existing\ncolumn this is a simple way wherein you\ncan insert a new column into a hive\ntable what are the key differences\nbetween hive and pig\nnow some of you might have heard hive\nvisit data warehousing package and pig\nis more of a scripting language both of\nthem are used for data analysis or trend\ndetection hypothesis testing data\ntransformation and many other use cases\nso if we compare hive and pig hive uses\na declarative language called hiveql\nthat is hive querying language similar\nto sql and it is for reporting or for\ndata analysis even for data\ntransformation or for your data\nextraction pig uses a high level\nprocedural language called pig latin for\nprogramming both of them remember use\nmapreduce processing framework so when\nwe run a query in hive to process the\ndata or when we create and submit a big\nscript both of them trigger a mapreduce\njob unless and until we have set them to\nlocal mode hive operates on the server\nside of the cluster and basically works\non structured data or data which can be\nstructuralized pig usually works or\noperates on the client side of the\ncluster and allows both structured\nunstructured or even i could say\nsemi-structured data hive does not\nsupport avro file format by default\nhowever that can be done by using the\nwrite serializer d serializer so we can\nhave hive table related data stored in\navro format in sequence file format in\npart k format or even as a text file\nformat however when we are working on\nsmarter formats like avro or sequence\nfile or parquet we might have to use\nspecific serializers d serializers for\navro this is the package which allows us\nto use avro format pig supports avro\nformat by default hive was developed by\nfacebook and it supports partitioning\nand pig was developed by yahoo and it\ndoes not support partitioning so these\nare high level differences there are\nlots and lots of differences remember\nhive is more of a data housing package\nand pig is more of a scripting language\nor a\nstrictly procedural flow following\nscripting language which allows us to\nprocess the data now let's get more and\nlet's get more deeper and learn about\npig which is as i mentioned a scripting\nlanguage which can be used for your data\nprocessing it also uses mapreduce\nalthough we can even have big run in a\nlocal mode let's learn about pig in the\nnext section now let's learn on some\nquestions about pig which is a scripting\nlanguage and it is extensively used for\ndata processing and data analysis so the\nquestion is how is apache pig different\nfrom mapreduce now we all know that\nmapreduce is a programming model it is\nit's quite rigid when it comes to\nprocessing the data because you have to\ndo the mapping and reducing you have to\nwrite huge code usually mapreduce is\nwritten in java but now it can also be\nwritten in python it can be written in\nscala another programming languages so\nif we compare pig with mapreduce pig\nobviously is very concise it has less\nlines of code when compared to mapreduce\nnow we also know that pig script\ninternally will trigger a mapreduce job\nhowever user need not know about\nmapreduce programming model they can\nsimply write simple scripts in pig and\nthat will automatically be converted\ninto mapreduce however mapreduce has\nmore lines of code peak is high level\nlanguage which can easily perform join\noperations or other data processing\noperations mapreduce is a low level\nlanguage which cannot perform job join\noperations easily so we can do join\nusing mapreduce however it's not really\neasy in comparison to pick now as i said\non execution every pig operator is\nconverted internally into a mapreduce\njob so every big script which is run\nwhich would be converted into mapreduce\njob now map reduce\noverall is a batch oriented processing\nso it takes more time to compile it\ntakes more time to execute either when\nyou run a mapreduce job or when it is\ntriggered by pixscript pig works with\nall versions of hadoop and when we talk\nabout mapreduce program which is written\nin one hadoop version may not work with\nother versions it might work or it might\nnot it depends on what are the\ndependencies what is the compiler you\nare using what programming language you\nhave used and what version of hadoop you\nare working on so these are the main\ndifferences between apache pig and\nmapreduce what are the different ways of\nexecuting pig script so you could create\na script file store it in dot pic or dot\ntext and then you could execute it using\nthe pick command you could be bringing\nup the grunt shell that is pig's shell\nnow that usually starts with mapreduce\nmode but then we can also bring it up in\na local mode and we can also run pig\nembedded as an embedded script in other\nprogramming language so these are the\ndifferent ways of executing your big\nscript now what are the major components\nof pig execution environment this is\nthis is a very common question\ninterviewers would always want to know\ndifferent components of hive different\ncomponents of pig even different\ncomponents which are involved in hadoop\necosystem so when we want to learn about\nmajor components of pig execution\nenvironment here are some so you have\npig scripts now that is written in pig\nlatin using built-in operators and\nuser-defined functions and submitted to\nthe execution environment that's what\nhappens when you would want to process\nthe data using pic now there is a parser\nwhich does type checking and checks the\nsyntax of the script the output of\nparser is a tag direct cyclic graph so\nblock and wikipedia for dag so dag is\nbasically a sequence of steps which run\nin one direction then you have an\noptimizer now this optimizer performs\noptimization using merge transform split\netc it aims to reduce the amount of data\nin the pipeline that's the whole purpose\nof optimizer you have a internal\ncompiler so pick compiler converts the\noptimized code into a mapreduce job and\nhere user need not know the mapreduce\nprogramming model or how it works or how\nit is written they all need to know\nabout running the big script which would\nbe internally converted into a mapreduce\njob and finally we have an execution\nengine so mapreduce jobs are submitted\nto the execution engine to generate the\ndesired results so these are major\ncomponents of pig execution environment\nnow let's learn about different complex\ndata types in pig supports various data\ntypes the main ones are tuple bag and\nmap what is tuple or tuple as you might\nhave heard a tuple is an ordered set of\nfields which can contain different data\ntypes for each field so in array you\nwould have multiple elements but that\nwould be of same types list can also\nhave different types your tuple is a\ncollection which has different fields\nand each field can be of different type\nnow we could have an example as one\ncomma three or one comma three comma a\nstring or a float element and all of\nthat form a tuple bag is a set of tuples\nso that's represented by curly braces so\nyou could also imagine this like a\ndictionary which has various different\ncollection elements what is a map map is\na set of key value pairs used to\nrepresent data so when you work in big\ndata field you need to know about\ndifferent data types which are supported\nby pig which are supported by hive which\nare supported in other components of\nhadoop so tuple tag map array array\nbuffer you can think about list you can\nthink about dictionaries you can think\nabout map which is key value pair so\nthese are your different complex data\ntypes other than the primitive data type\nsuch as integer character string boolean\nfloat and so on now what are the various\ndiagnostic operators available in apache\npic so these are some of the operators\nor options which you can give in a pic\nscript you can do a dumb now dumb\noperator runs the pig latin scripts and\ndisplays the result on the screen so\neither i could do it dumb and see the\noutput on the screen or i can even do a\ndump into and i could store my output in\na particular file so we can load the\ndata using load operator in pig and then\npig also has different internal storage\nlike json loader or big storage which\ncan be used if you are working on\nspecific kind of data and then you could\ndo a dump either before processing or\nafter processing and dump would produce\nthe result the result could be stored in\na file or\nseen on the screen you also have a\ndescribe operator now that is used to\nview the schema of a relation so you can\nload the data and then you can view the\nschema of relation using describe\noperator x-plane as we might already\nknow displays the physical logical and\nmapreduce execution plans normally in\nrdbms when we use x-plane we would like\nto see what happens behind the scenes\nwhen a particular script or a query runs\nso we could load the data using load\noperator as in any other case and if we\nwould want to display the logical\nphysical and mapreduce execution plans\nwe could use explain operator there is\nalso an illustrate operator now that\ngives the step by step execution of\nsequence of statements so sometimes when\nwe would want to analyze our script to\nsee how good or bad they are or would\nthat really serve our purpose we could\nuse illustrate and again you can test\nthat by loading the data using load\noperator and you could just use a\nillustrate operator to have a look at\nthe step by step execution of the\nsequence of statements which you would\nwant to execute so these are different\ndiagnostic operators available in apache\npic now if somebody asks state the usage\nof group order by and distinct keywords\nin big script so as i said pig is a\nscripting language so you could use\nvarious operators so group basically\ncollects various records with the same\nkey and groups the data in one or more\nrelations here is an example you could\ndo a group data so that is basically a\nvariable or you can give some other name\nand you can say group relation name by\nage now say i have a file where i have\nfield various fields and one of the\nfield is relation name so i could group\nthat by a different field order by is\nused to display the contents of relation\nin a sorted order whether ascending or\ndescending so i could create a variable\ncalled relation two and then i could say\norder relation name one by ascending or\ndescending order distinct basically\nremoves the duplicate records and it is\nimplemented only on entire records\nnot on individual records so if you\nwould want to find out the distinct\nvalues and relation name field i could\nuse district what are the relational\noperators in pig so you have various\nrelational operators which help data\nscientists or data analysts or\ndevelopers who are analyzing the data\nsuch as co-group which joins two or more\ntables and then performs group operation\non the join table result you have cross\nit is used to compute the cross product\nthat is a cartesian product of two or\nmore relations for each is basically to\ndo some iteration so if it will iterate\nthrough tuples of a relation generating\na data transformation so for example if\ni say variable a equals and then i load\na file in a and then i could create a\nvariable called b where i could say for\neach a i would want to do something say\ngroup join is to join two or more tables\nin a relation limit is to limit the\nnumber of output tuples or output\nresults split is to split the relation\ninto two or more relations union is to\nget a combination that's it will merge\nthe contents of two or more relations\nand order is to get a sorted result so\nthese are some relational operators\nwhich are extensively used in pig for\nanalysis what is the use of having\nfilters in apache pic now say for\nexample i have some data which has three\nfields year product quantity and this is\nmy phone sales data so filter operator\ncould be used to select the required\nvalues from a relation based on a\ncondition it also allows you to remove\nunwanted records from data file so for\nexample filter the products where\nquantity is greater than thousand so i\nsee that i have one row wherein or\nmultiple rows where the quantity is\ngreater than thousand such as 1500 1700\n1200 so i could create a variable called\na i would load my file using pick\nstorage as i explained earlier pick\nstorage is an internal parameter which\ncan be used to\nspecify the delimiters now here my\ndelimiter is comma so i could say using\npick storage as and then i could specify\nthe data type for each field so here\nbeing integer product being character\narray and quantity being integer then b\ni could say filter a whatever we have in\na by quantity greater than thousand so\nit's very concise it's very simple and\nit allows us to extract and process data\nin a simpler way now suppose there is a\nfile called test.txt\nhaving 150 records in hdfs so this is a\nfile which is stored on his dfs and it\nhas 150 records where we can consider\nevery record being one line and if\nsomebody asks you to write a pick\ncommand to retrieve the first 10 records\nof the file first we will have to load\nthe data so i could create a variable\ncalled test underscore data and i would\nsay load my file using pick storage\nspecifying the delimiter as comma as and\nthen i could specify my fields whatever\nfields our file have and then i would\nwant to get only 10 records for which i\ncould use the limit operator so i could\nsay limit on test data and give me 10\nrecords this is very simple and we can\nextract 10 records from 150 records\nwhich are stored in the file on sdfs\nnow we have learnt on pig we have\nlearned some questions on hive you could\nalways look more in books like\nprogramming in hive or programming in\npig and look for some more examples and\ntry out these examples on a existing\nhadoop setup now let's learn on hbase\nwhich is a nosql database now edge base\nis a four dimensional database in\ncomparison to your rdbms which usually\nare dimensional so rdbms have rows and\ncolumns but hbase has four coordinates\nit has row key which is always unique\ncolumn family which can be any number\ncolumn qualifiers which can again be any\nnumber per column family and then you\nhave a version so these four coordinates\nmake edge base a four dimensional key\nvalue store or a column family store\nwhich is unique for storing huge amount\nof data and extracting data from hbase\nthere is a very good link which i would\nsuggest everyone can look at if you\nwould want to learn more on hbase and\nyou could just say hbase mapper and this\nbasically brings up a documentation\nwhich is from mapr but then that's not\nspecific to mapper and you can look at\nthis link which will give you a detailed\nexplanation of hbase how it works what\nare the architectural components and how\ndata is stored and how it makes hbase a\nvery powerful nosql database so let's\nlearn on some of the important or\ncritical questions on hbase which might\nbe asked by the interviewer in an\ninterview when you are applying for a\nbig data admin or a developer position\nrole so what are the key components of\nedge base now as i said this is one of\nthe favorite questions of interviewers\nwhere they would want to understand your\nknowledge on different components for a\nparticular service hbase as i said is a\nnosql database and that comes as a part\nof service with cloudera or hortonworks\nand with apache hadoop you could also\nset up edge base as an independent\npackage so what are the key components\nof hbase hbase has a region server now\nedge base follows the similar kind of\ntopology like hadoop now hadoop has a\nmaster process that is name node and\nslave processes such as data nodes and\nsecondary name node in the same way\nhbase also has a master which is h\nmaster and the slave processes are\ncalled region servers so these region\nservers are usually co-located with data\nnodes however it is not mandatory that\nif you have 100 data nodes you would\nhave 100 region servers so it purely\ndepends on admin so what does this\nregion server contain so region server\ncontains hbase tables that are divided\nhorizontally into regions or you could\nsay group of rows is called regions so\nin edge base you have two aspects one is\ngroup of columns which is called column\nfamily and one is group of rows which is\ncalled regions now these regions or\nthese rows are grouped based on the key\nvalues or i would say row keys which are\nalways unique when you store your data\nin each base you would have data in the\nform of rows and columns so group of\nrows are called regions or you could say\nthese are horizontal partitions of the\ntable so a region server manages these\nregions on the node where a data node is\nrunning a region server can have up to\nthousand regions it runs on every node\nand decides the size of region so region\nserver as i said is a slave process\nwhich is responsible for managing hbase\ndata on the node each region server is a\nworker node or a worker process\nco-located with data node which will\ntake care of your read write update\ndelete request from the clients now when\nwe talk about more components of edge\nbase as i said you have hp h master so\nyou would always have a connection\ncoming in from a client or an\napplication what does h master do it\nassigns regions it monitors the region\nservers it assigns regions to region\nservers for load balancing and it cannot\ndo that without the help of zookeeper so\nif we talk about components of hbase\nthere are three main components you have\nzookeeper you have h master and you have\nregion server region server being the\nslave process your edge master being the\nmaster process which takes care of all\nyour table operations assigning regions\nto the region servers taking care of\nread and write requests which come from\nclient and for all of this edge master\nwill take in help of zookeeper which is\na centralized coordination service so\nwhenever a client wants to read or write\nor change the schema or any other\nmetadata operations it will contact\nhmaster edge master internally will\ncontact zookeeper so you could have\nhbase setup also in high availability\nmode where you could have a active edge\nmaster than a backup etch master you\nwould have a zookeeper quorum which is\nthe way zookeeper works so zookeeper is\na centralized coordination service which\nwill always run with a quorum of\nprocesses so zookeeper would always run\nwith odd number of processes such as 3 5\nand 7 because zookeeper works on the\nconcept of majority consensus now\nzookeeper which is a centralized\ncoordination service is keeping a track\nof all the servers which are alive\navailable and also keeps a track of\ntheir status for every server with\nzookeeper is monitoring zookeeper keeps\na session alive with that particular\nserver edge master would always check\nwith zookeeper which region servers are\navailable alive so that regions can be\nassigned to the region server at one end\nyou have region server which are sending\ntheir status to the zookeeper indicating\nif they are ready for any kind of read\nor write operation and at other end h\nmaster is querying the zookeeper to\ncheck the status now zookeeper\ninternally manages a meta table now that\nmeta table will have information of\nwhich regions are residing on which\nregion server and what row keys those\nregions contain so in case of a read\nactivity hmaster will query zookeeper to\nfind out the region server which\ncontains that meta table once edge\nmaster gets the information of meta\ntable it can look into the meta table to\nfind out the row keys and the\ncorresponding region servers which\ncontain the regions for those row keys\nnow if we would want to understand\nrookie and column families in each base\nlet's look at this and it would be good\nif we could look this on an excel sheet\nso row key is always unique it acts as a\nprimary key for any hbase table it\nallows a logical grouping of cells and\nmake sure that all cells with the same\nrow key are co-located on the same\nserver so as i said you have four\ncoordinates for hbase you have a row key\nwhich is always unique you have column\nfamilies which is nothing but group of\ncolumns and when i say column families\none column family can have any number of\ncolumns so when i talk about h base h\nbase is four dimensional and in terms of\nedge base it is also called as a column\noriented database which basically means\nthat every row in one column could have\na different data type now you have a row\nkey which uniquely identifies the row\nyou have column families which could be\none or many depending on how the table\nhas been defined and a column family can\nhave any number of columns or i could\nsay for every row within a column family\nyou could have different number of\ncolumns so i could say for my row 1 i\ncould just have two columns such as name\nand city within the column family for my\nrow 2 i could have name city age\ndesignation salary for my third row i\ncould have thousand columns and all that\ncould belong to one column family so\nthis is a horizontally scalable database\nso column family consists of group of\ncolumns which is defined during table\ncreation and each column family can have\nany number of column qualifiers\nseparated by a delimiter now a\ncombination of row key column family\ncolumn qualifier such as name city age\nand the value within the cell is makes\nthe hbase a unique four dimensional\ndatabase for more information if you\nwould want to learn on hbase please\nrefer this link which is hbase mapper\nand this gives a complete hbase\narchitecture that is three components of\nname node\nthree components that is name node\nregion servers and zookeeper how it\nworks how each base edge master\ninteracts with zookeeper what zookeeper\ndoes in coordination how are the\ncomponents working together and how does\nhbs take care of read and write coming\nback and continuing why do we need to\ndisable a table so there are different\ntable operations what you can do in edge\nbase and one of them is disabling a\ntable now if you would want to check the\nstatus of table you could check that by\nis disabled and giving the table name\norder is enabled and the table name so\nthe question is why do we need to\ndisable a table now if we would want to\nmodify a table or we are doing some kind\nof maintenance activity in that case we\ncan disable the table so that we can\nmodify or change its settings when a\ntable is disabled it cannot be accessed\nthrough the scan command now if we have\nto write a code to open a connection in\neach base now to interact with hbs one\ncould either use a graphical user\ninterface such as hue or you could be\nusing the command line h based shell or\nyou could be using hbase admin api if\nyou are working with java or say happy\nbase if you are working with python\nwhere you may want to open a connection\nwith hbase so that you can work with\nthat base programmatically in that case\nwe have to create a configuration object\nthat is configuration my conf and then\ncreate a configuration object and then\nyou can use different classes like etch\ntable interface to work on a new table\nyou could use h column qualifier and\nmany other classes which are available\nin hbase admin api what does replication\nmean in terms of hbase so edge base as i\nsaid works in a cluster way and when you\ntalk about cluster you could always set\nup a replication from one edge base\ncluster to other hbase cluster so this\nreplication feature in edge base\nprovides a mechanism to copy data\nbetween clusters or sync the data\nbetween different clusters this feature\ncan be used as a disaster recovery\nsolution that provides high availability\nfor hbase so if i have a hbase cluster\none where i have one master and multiple\nregion servers running in a hadoop\ncluster i could use the same hadoop\ncluster to create a hbase replica\ncluster or i could have a totally\ndifferent hbase replica cluster where my\nintention is that if things are changing\nin a particular table in cluster 1 i\nwould want them to be replicated across\ndifferent clusters so i could alter the\nhbase table and set the replication\nscope to 1. now a replication scope of 0\nindicates that table is not replicated\nbut if we set the replication to 1 we\nbasically will have to set up ah base\ncluster where we can replicate hbase\ntables data from cluster one to cluster\nso these are the commands which can be\nused to enable replication and then\nreplicate the data of table across\nclusters can we import and export in\nhbase of course we can it is possible to\nimport and export tables from one hbase\ncluster to other hbase cluster or even\nwithin a cluster so we can use the hbase\nexport utility which comes in this\nparticular package give a table name and\nthen a target location so that will\nexport the data of hbase table into a\ndirectory on sdfs then i could create a\ndifferent table which would follow some\nkind of same definition as the table\nwhich was exported and then i could use\nimport to import the data from the\ndirectory on sdfs to my table if you\nwould want to learn more on hbase import\nand export you could look at hbase\nimport\noperations let's search for the link and\nthis is the link where you could learn\nmore about hbase import export utilities\nhow you could do a bulk import bulk\nexport which internally uses mapreduce\nand then you could do a import and\nexport into hbs tables moving further\nwhat do we mean by compaction in hbase\nnow we all know that hbase is a nosql\ndatabase which can be used to store huge\namount of data however whenever a data\nis written in hbase it is first returned\nto what we call as write ahead log and\nalso to mem store which is write cache\nnow once the data is written in wall and\nyour mem store it is offloaded to form\nan internal hbase format file which is\ncalled h5 and usually these edge files\nare very small in nature so we also know\nthat sdfs is good when we talk about few\nnumber of larger files in comparison to\nlarge number of smaller files due to the\nlimitation of name node's memory\ncompaction is process of merging hbase\nfiles that is these smaller edge files\ninto a single large file this is done to\nreduce the amount of memory required to\nstore the files and number of disk seeks\nneeded so we could have lot of edge\nfiles which get created when the data is\nwritten to hbase and these smaller files\ncan then be compacted through a major or\nminor compaction creating one big edge\nfile which internally would then be\nwritten to sdfs and sdfs format of\nblocks that is the benefit of compaction\nthere is also a feature called bloom\nfilter so how does bloom filter work so\nbloom filter or hbase bloom filter is a\nmechanism to test whether a h file\ncontains a specific row or a row column\ncell bloom filter is named after its\ncreator burton hovered bloom it is a\ndata structure which predicts whether a\ngiven element is a member of a set of\ndata it provides an in-memory index\nstructure that reduces the disk reads\nand determines the probability of\nfinding a row in a particular file this\nis one of very useful features of edge\nbase which allows for faster access and\navoids disk seeks\ndoes hbase have any concept of name\nspace so namespace is when you have\nsimilar elements grouped together so\nnamespace yes\nis support such name space so namespace\nis a logical grouping of tables\nanalogous to a database in rdbms so you\ncan create hp's namespace to the schema\nof rdbms database so you could create a\nnamespace by saying create namespace and\ngiving it a name and then you could also\nlist the tables within a namespace you\ncould create tables within a specific\nnamespace now this is usually done in\nproduction environment where a cluster\nmight be multi-tenant cluster and there\nmight be different users of the same\nnosql database in that case admin would\ncreate specific namespace and for\nspecific namespace you would have\ndifferent directories on sdfs and users\nof a particular business unit or a team\ncan work on their hbase objects within a\nspecific name space this is a question\nwhich is again very important to\nunderstand about the writes or reads so\nhow does right ahead log wall help when\na region server crashes now as i said\nwhen a write happens it will happen into\nmem store and wall that is your edit log\nor write ahead log so whenever a write\nhappens it will happen in two places mem\nstore which is the right cache and wall\nwhich is a edit log only when the data\nis written in both these places and\nbased on the limitation of mem store the\ndata will be flushed to create an\nedge-based format file called h-file\nthese files are then compacted and\ncreated into one bigger file which will\nthen be stored on sdfs and sdfs data as\nwe know is stored in the form of blocks\non the underlying data nodes so if a\nregion server hosting a mem store\ncrashes now where is region server\nrunning that would be co-located with\ndata node so if a data node crashes or\nif a region server which was hosting the\nmem store write cache crashes data in\nmemory the data that in memory which was\nnot persisted is lost now how does hbase\nrecover from this as i said your data is\nwritten into wall and mem store at the\nsame time hbase recovers against that by\nwriting to wall before the write\ncompletes so whenever a write happens it\nhappens in mem store and wall at the\nsame time hbase cluster keeps a wall to\nrecord changes as they happen and that's\nwhy we call it as also an edit log if\nhps goes down or the node that goes down\nthe data that was not flushed from mem\nstore to edge file can be recovered by\nreplaying the right ahead lock and\nthat's the benefit of your edit log or\nwrite ahead log now if we would have to\nwrite hbase command to list the contents\nand update the column families of a\ntable i could just do a scan and that\nwould give me complete data of a table\nif you are very specific and if you\nwould want to look at a particular row\nthen you could do a get table name and\nthen give the row key however you could\ndo a scan to get the complete data of a\nparticular table you could also do a\ndescribe to see what are the different\ncolumn families and if you would want to\nalter the table and add a new column\nfamily it is very simple you can just\nsay alter give the hbs table name and\nthen give you a new column family name\nwhich will then be added to the table\nwhat are catalog tables in each base so\nas i mentioned your zookeeper knows the\nlocation of this internal catalog table\nor what we call as the meta table now\ncatalog tables in edge base have two\ntables one is edge base meta table and\none is\nhyphen rule the catalog table edge base\nmeta exists as an edge base table and is\nfiltered out of hbase shells list\ncommand so if i give a list command on\nedge base it would list all the tables\nwhich space contains but not the meta\ntable it's an internal table this meta\ntable keeps a list of all regions in the\nsystem and location of hbase meta stored\nin zookeeper so if somebody wants to\nfind out or look for particular rows\nthey need to know the regions which\ncontain that data and those regions are\nlocated on region server to get all this\ninformation one has to look into this\nmeta table however we will not be\nlooking into meta table directly we\nwould just be giving a write or a read\noperation internally uh based master\nqueries the zookeeper zookeeper has the\ninformation of where the meta table\nexists and that meta table which is\nexisting on region server contains\ninformation of row keys and the region\nservers where those rows can be found\nyour root table keeps a track of\nlocation of the meta table what is hot\nspotting in edge base and how to avoid\nhot spotting now this is a common\nproblem and always admin guys or guys\nwho are managing the infrastructure\nwould think about it so one of the main\nidea is that edge base would be\nleveraging the benefit of sdfs your all\nread and write requests should be\nuniformly distributed across all of the\nregions in region servers otherwise\nwhat's the benefit of having a\ndistributed cluster so you would have\nyour data stored across region servers\nin the form of regions which are\nhorizontal partitions of the table and\nwhenever read and write requests happen\nthey should be uniformly distributed\nacross all the regions in the region\nservers now hot spotting occurs when a\ngiven region serviced by a region server\nreceives most or all of read and write\nrequest which is basically a unbalanced\nway of read write operations now hotspot\ncan be avoided by designing the row key\nin such a way that data being written\nshould go to multiple regions across the\ncluster so you could do techniques such\nas salting hashing reversing the key and\nmany other techniques which are employed\nby users of hbase we need to just make\nsure that when the regions are\ndistributed across region servers they\nshould be spread across region servers\nso that your read and write request can\nbe satisfied from different region\nservers in parallel rather than all read\nand write requests hitting the same\nregion server overloading the region\nserver which may also lead to the\ncrashing of a particular region server\nso these were some of the important\nquestions of hbase and then there are\nmany more please refer to the link which\ni specified in during my discussion and\nthat gives you a detailed explanation of\nhow each base works you can also look\ninto hp's definitive guide by o'reilly\nor hbase in action and these are really\ngood books to understand about hbase\ninternals and how it works now that we\nhave learnt on hive which is a data\nwarehousing package we have learnt on\npig which is a scripting or a scripting\nlanguage which allows you to do data\nanalysis and we have learned some\nquestions on a nosql database just note\nit that there are more than 225 nosql\ndatabases existing in market and if you\nwould want to learn and know about more\nnosql databases you can just go to\ngoogle and type no sql databases org and\nthat will take you to the link which is\nfor nosql databases and this shows there\nare more than 225\nnosql databases existing in market and\nthese are for different use cases used\nby different users and for with\ndifferent features so have a look at\nthis link now when you talk about data\ningestion so let's look at data\ningestion and this is one good link\nwhich i would suggest to have a look at\nwhich lists down around 18 different\ningestion tools so when you talk about\ndifferent data ingestion tools some are\nfor structured data some are for\nstreaming data some are for data\ngovernance some are for data ingestion\nand transformation and so on so have a\nlook at this link which also gives you a\ncomparison of different data ingestion\ntools so here let's learn about some\nquestions on scope which is one of the\ndata injection tools mainly used for\nstructured data or you could say data\nwhich is coming in from rdbms or data\nwhich is already structured and you\nwould want to ingest that you would want\nto store that on sdfs which could then\nbe used for hive which could be used for\nany kind of processing using mapreduce\nor hive or pig or spark or any other\nprocessing frameworks or you would want\nto load that data into say high voltage\nbased tables scope is mainly for\nstructured data it is extensively used\nwhen organizations are migrating from\nrdbms to a big data platform and they\nwould be interested in ingesting the\ndata that is doing import and export of\ndata from rdbms to sdfs or vice versa so\nlet's learn about some important\nquestions on scope which you may be\nasked by an interviewer when you apply\nfor a big data related position how is\nscoop different from flu so this is a\nvery common question which is asked\nscoop which is mainly for structured\ndata so scoop works with rdbms it also\nworks with no sql databases to import\nand export data so you can import data\ninto sdfs you can import data into data\nwarehousing package such as hive\ndirectly or also in hbase and you could\nalso export data from hadoop ecosystem\nto your rdbms however when it comes to\nflow flow is more of\na data injection tool which works with\nstreaming data or unstructured data so\ndata which is constantly getting\ngenerated for example log files or\nmetrics from server or some chat\nmessenger and so on so if you are\ninterested in working on capturing and\nstoring the streaming data in a storage\nlayer such as sdfs or hbase you could be\nusing flue there could be other tools\nalso like kafka or storm or chokwa or\nsamsa nifi and so on scoop however is\nmainly for structured data your loading\ndata in scope is not event driven so it\nis not based on event it basically works\non data which is already stored in rdbms\nin terms of flume it is completely event\ndriven that is as the messages or as the\nevents happen as the data is getting\ngenerated you can have that data\ningested using flow scope works with\nstructured data sources and you have\nvarious scope connectors which are used\nto fetch data from external data\nstructures or rdbms so for every rdbms\nsuch as mysql oracle db2 microsoft sql\nserver you have different connectors\nwhich are available flume it works on\nfetching streaming data such as tweets\nor log files or server metrics from your\ndifferent sources where the data is\ngetting generated and if you are\ninterested in not only ingesting that\ndata which is getting generated in a\nstreaming fashion but if you would be\ninterested in processing the data as it\narrives scoop can import data from rdbms\nonto sdfs and also export it back to\nrdbms flume is then used for streaming\ndata now you could have one to one one\ntoo many or many to one kind of relation\nso in terms of loom you have components\nsuch as your source sync and channel\nthat's the main difference between your\nscope and flow what are the different\nfile formats to import data using scope\nwell there are lots and lots of formats\nin which you can import data into scope\nwhen you talk about scope you can have\ndelimited text file format now that's\nthe default import format it can be\nspecified explicitly using as text file\nargument so when i want to import data\nfrom an rdbms i could get that data in\nsdfs using different compression schemes\nor in different formats using the\nspecific arguments so i could specify an\nargument which will write string based\nrepresentation of each record to output\nfiles with delimiters between individual\ncolumns and rows so that is the default\nformat which is used to import data\nusing scope so to learn more about your\nscope and different arguments which are\navailable you can click on\nscoop.apache.org\nyou can look into the documentation and\ni would suggest choosing one of the\nversions and looking into the user guide\nand here you can search for arguments\nand look for specific control arguments\nwhich show how you can import data using\nscope so here we have common arguments\nand then you also have import control\narguments wherein we have different\noptions like getting data as avro as\nsequence file as text file or parquet\nfile these are different formats you can\nalso get data in default compression\nscheme that is gzip or you can specify\ncompression codec and then you can\nspecify what compression mechanism you\nwould want to use when you are importing\nyour data using scope when it comes to\ndefault format for flume we could say\nsequence file which is a binary format\nthat stores individual records in record\nspecific data types so these data types\nare manifested as java classes and scope\nwill automatically generate these data\ntypes for you so scoop does that when we\ntalk about your sequence file format in\nterms of your scope you could be\nextracting storage of all data in binary\nrepresentation so as i mentioned you can\nimport data in different formats such as\navro parquet sequence file that is\nbinary format or machine readable format\nand then you could also have data in\ndifferent compression schemes let me\njust show you some quick examples here\nso if i look in\nthe content and here i could search for\na scoop based file where i have listed\ndown some examples so if i would want to\nuse different compression schemes here\nare some examples have a look at these\nso i'm doing a scoop import i'm also\ngiving an argument so that scoop which\nalso triggers a map reduce job or i\nwould say map only job so when you run a\nscoop import it triggers a map only job\nno reduce happens here and you could\nspecify this parameter or this argument\non the command line\nmapreduce.framework.name\nso that you could run your map only job\nin a local mode to save time or that\nwould interact with yarn and run a\nfull-fledged map only job we can give\nthe connection and then connect to\nwhatever rdbms we are connecting\nmentioning the database name give your\nuser name and password give the table\nname give a target directory or it would\ncreate a directory same as the table\nname which would work only once and then\ni could say minus z to get data in a\ncompressed format that is gzip or i\ncould be specifying compression codec\nand then i could specify what\ncompression codec i would want to use\nsay snappy b lz4 default i could also\nrun a query by giving a scope import and\nwhen i'm specifying a query if you\nnotice i'm not given any table name\nbecause that would be included in the\nquery i can get my data as a sequence\nfile format which is a binary format\nwhich will create a huge file so we\ncould also have compression enabled and\nthen i could say the output of my map\njob should use a compression at record\nlevel for my data coming in sequence\nfile so sequence file or a binary format\nsupports compression at record level or\nat block level i could get my data in an\navro file where data has embedded schema\nwithin the file or a parquet file also\nso these are different ways in which you\ncan set up different compression schemes\nor you can even get data in different\nformats and you could be doing a simple\nscope import for these looking further\nwhat is the importance of eval tool in\nscope so there is something called as\neval tool so scoop eval tool allows\nusers to execute user defined queries\nagainst respective database servers and\npreview the result in the console so\neither i could be running a straight\naway query to import the data into my\nsdfs or i could just use scoop eval\nconnect to my external rdbms specify my\nusername and password\nand then i could be giving in a query to\nsee what would be the result of the\nquery which we intend to import now\nlet's learn about how scope imports and\nexports data between rdbms and sdfs with\nits architecture so rdbms as we know has\nyour database structures your tables\nwhich all of them are logical and\ninternally there is always metadata\nwhich is stored your scope import\nconnects to an external rdbms and for\nthis connection it uses an internal\nconnector jar file which has a driver\nclass so that's something which needs to\nbe set up by admin but they need to make\nsure that whichever rdbms you intend to\nconnect to they need to have the jdbc\nconnector for that particular rdbms\nstored within the scope lib folder so\nscope import gets the metadata and then\nfor your scoop command it converts that\ninto a map only job which might have one\nor multiple map tasks now that depends\non your scoop command you could be\nspecifying that you would want to do a\nimport only in one task or in multiple\ntasks these multiple map tasks will then\nrun on a section of data from rdbms and\nthen store it in sdfs so at high level\nwe could say scoop will introspect\ndatabase to get gathered the metadata it\ndivides the input data set into splits\nand this division of data into splits\nmainly happens on primary key column of\nthe table now if somebody might ask what\nif my table in rdbms does not have a\nprimary key column then when you are\ndoing a scoop import either you will\nhave to import it using one mapper task\nby specifying hyphen hyphen m equals one\nor you would have to say split by\nparameter to specify a numeric column\nfrom rdbms and that's how you can import\nthe data let me just show you a quick\nexample on this so i could just look in\nagain into the scoop command file and\nhere we could be looking at an example\nso if you see this one here we are\nspecifying minus minus m equals 1 which\nbasically means i would want to import\nthe data using one map task now in this\ncase whether the table has a primary key\ncolumn or does not have a primary key\ncolumn will not matter but if i say a\nminus minus ms6 where i'm specifying\nmultiple map tasks to be imported then\nthis will look for a primary key column\nin the table which you are importing now\nif the table does not have a primary key\ncolumn then i could be specifying a\nsplit by and then specify the column so\nthat the data could be split into\nmultiple chunks and multiple map tasks\ncould take it now if the second scenario\nis your table does not have a primary\nkey column and it does not have a\nnumeric column on which you could do a\nsplit by in that case and if you would\nwant to use multiple mappers you could\nstill say split by on a textual column\nbut you will have to add this property\nso that it allows splitting the data\nwhich is non-numeric all of these\noptions are given in the scoop\napache.org link going further how scoop\nimports and exports data between rdbms\nand sdfs with its architecture so as i\nsaid it submits the map only job to the\ncluster and then it basically does a\nimport or export so if we are exporting\nthe data from sdfs in that case again\nthere would be a map only job it would\nlook at multiple splits of the data\nwhich is existing which your map only\njob would process through one or one\ntable map task and then export it to\nrdbms suppose you have a database sdb in\nmysql we if somebody asked you to write\na command to connect this database and\nimport tables to scoop so here is a\nquick example as i showed you in the\ncommand file so you could say scope\nimport this is what we would want to do\nyou connect using jdbc now this will\nonly work if the jdbc connector already\nexists within your scope lib directory\nadmin has to set up that so you can\nconnect to your rdbms you can point to\nthe database so\nhere our database name is test\nunderscore db i could give user name and\nthen either i could give password on the\ncommand line or just say capital p so\nthat i could be prompted for the\npassword and then i could give the table\nname which i would want to import i\ncould also be specifying minus minus m\nand specify how many map tasks do i want\nto use for this import as i showed in\nprevious screen how to export a table\nback to rdbms now for this we need the\ndata in a directory on hdfs so for\nexample there is a department stable in\nretail database which is already\nimported into scoop and you need to\nexport this table back to rdbms so this\nis the content of the table now create a\nnew department table in rdbms so i could\ncreate a table specifying the column\nnames whether that supports null or no\nif that has a primary key column which\nis always recommended and then i can do\na scoop export i can connect to the\nrdbms specifying my username and\npassword specify the table into which\nyou want to export the data and then you\ngive export directory pointing to a\ndirectory on sdfs which contains the\ndata this is how you can export data\ninto table seeing example on this so i\ncould again look into my file and here i\nhave an example of import this is where\nyou are importing data directly into\nhive and you have scope import where you\nare importing data directly into hbase\ntable and you can then query your hbase\ntable to look at the data you could also\ndo a export by\nrunning your map only job in a local\nmode connecting to the rdbms specifying\nyour username specifying the table where\nyou would want to export and the\ndirectory on sdfs where you have kept\nthe relevant data this is a simple\nexample of export looking further\nwhat is the role of jdbc driver in scoop\nsetup so as i said if you would want to\nuse scoop to connect to an external\nrdbms we need the jdbc odbc connector\njar file now one or admin could download\nthe jdbc connector jar file and then\nplace the jar file within the scoop lib\ndirectory wherever scoop is installed\nand this jdbc connector jar file\ncontains a driver now jdbc driver is a\nstandard java api which is used for\naccessing different databases in rdbms\nso this connector jar file is very much\nrequired and this connector jar file has\na driver class\nand this driver class enables the\nconnection between your rdbms and your\nhadoop structure each database vendor is\nresponsible for writing their own\nimplementation that will allow\ncommunication with the corresponding\ndatabase and we need to download the\ndrivers which allow our scoop to connect\nto external rdbms so your jdbc driver\nalone is not enough to connect to scope\nwe also need connectors to interact with\ndifferent database so a connector is a\nplugable piece that is used to fetch\nmetadata and allow scoop to overcome the\ndifferences in sql dialects so this is\nhow connection can be established so\nnormally your admins would when they are\nsetting up scope and hadoop they would\ndownload\nsay mysql jdbc connector and this is how\nthey would go to the mysql connectors if\nyou are connecting to mysql similarly\nfor your other rdbms you could be say\ngoing in here you could be looking for a\nprevious version depending you could be\ngoing for platform independent and then\nyou could be downloading the connected\njar file now if you enter this jar file\nyou would see a mysql connector jar and\nif we look in\ncom.mysql.jdbcom.mysql.jdbc.org\nso this is the package which is within\nthe connector jar file and this\nhas the driver class which allows the\nconnection of your scope with your rdbms\nso these things will have to be done by\nyour admin so that you can have your\nscope connecting to an external rdbms\nnow how do you update the columns that\nare already exported so if i do a export\nand i put my data in rdbms can i really\nupdate the columns that are already\nexported yes i can using a update key\nparameter so scoop export command\nremains the same the only thing i will\nhave to specify now is the table name\nyour fields terminated by if you have a\nspecific delimiter and then you can say\nupdate key and then the column name so\nthis allows us to update the columns\nthat are already exported in rdbms\nwhat is code gen so scope commands\ntranslate into your mapreduce job or map\nonly job so code gen is basically a tool\nin scope that generates data access\nobjects dao java classes that\nencapsulate and interpret imported\nrecords so if i do a scoop code gen\nconnect to an rdbms using my username\nand give a table this will generate a\njava code for employee table in the test\ndatabase so this code gen can be useful\nfor us to understand what data we have\nin this particular table finally can\nscoop be used to convert data in\ndifferent formats i think i already\nanswered that right if no which tools\ncan be used for this purpose so scoop\ncan be used to convert data in different\nformats and that depends on the\ndifferent arguments which you use when\nyou do a import such as avro file\nparquet file binary format with record\nor block level compression so if you are\ninterested in knowing more on different\ndata formats then i think i can suggest\na link for that and we can say hadoop\nformats\ni think it is tech\nmaggie avro\nparque let's see you can find the link\ntech mac e yeah this is a very good link\nwhich specifies or talks about different\ndata formats which you should know such\nas your text file format different\ncompression schemes how is data\norganization what are the common formats\nwhat do you have in text file structured\nbinary sequence files with compression\nwithout compression what is record level\nwhat is block level what is a avro data\nfile what is a square what is a\nparquet data file or a columnar format\nand other formats like orc rc and so on\nthanks ajay with that we've reached the\nend of this complete big data course i\nhope you enjoyed this video do like and\nshare it thank you for watching and stay\ntuned for more from simply learn\n[Music]\n",
  "words": [
    "music",
    "know",
    "around",
    "billion",
    "internet",
    "users",
    "today",
    "users",
    "undoubtedly",
    "generate",
    "massive",
    "amounts",
    "data",
    "data",
    "collectively",
    "termed",
    "big",
    "data",
    "think",
    "data",
    "processed",
    "well",
    "frameworks",
    "like",
    "hadoop",
    "spark",
    "used",
    "hey",
    "guys",
    "welcome",
    "big",
    "data",
    "tutorial",
    "video",
    "covering",
    "fundamentals",
    "big",
    "data",
    "hadoop",
    "spark",
    "talking",
    "hadoop",
    "installation",
    "various",
    "components",
    "hadoop",
    "hdfs",
    "map",
    "reduce",
    "yard",
    "look",
    "apache",
    "spark",
    "installation",
    "spark",
    "spark",
    "sql",
    "first",
    "introduction",
    "big",
    "data",
    "look",
    "hadoop",
    "installation",
    "instructor",
    "ajay",
    "talk",
    "evolution",
    "big",
    "data",
    "known",
    "data",
    "evolved",
    "last",
    "five",
    "years",
    "like",
    "never",
    "fact",
    "going",
    "big",
    "data",
    "understanding",
    "solutions",
    "need",
    "rush",
    "towards",
    "big",
    "data",
    "technology",
    "solution",
    "would",
    "like",
    "ask",
    "question",
    "take",
    "couple",
    "minutes",
    "think",
    "organizations",
    "interested",
    "big",
    "data",
    "certain",
    "rush",
    "industry",
    "everyone",
    "would",
    "want",
    "ramp",
    "current",
    "infrastructure",
    "would",
    "want",
    "working",
    "technologies",
    "allow",
    "use",
    "big",
    "data",
    "think",
    "happening",
    "organizations",
    "interested",
    "think",
    "start",
    "thinking",
    "organizations",
    "past",
    "organizations",
    "done",
    "organizations",
    "interested",
    "big",
    "data",
    "learn",
    "big",
    "data",
    "always",
    "look",
    "internet",
    "check",
    "use",
    "cases",
    "organizations",
    "failed",
    "use",
    "legacy",
    "systems",
    "relational",
    "databases",
    "work",
    "data",
    "requirements",
    "recent",
    "past",
    "five",
    "years",
    "recent",
    "decade",
    "happened",
    "organizations",
    "started",
    "understanding",
    "value",
    "data",
    "decided",
    "ignore",
    "data",
    "uneconomical",
    "talk",
    "different",
    "platforms",
    "data",
    "generated",
    "take",
    "example",
    "social",
    "media",
    "like",
    "twitter",
    "facebook",
    "instagram",
    "whatsapp",
    "youtube",
    "various",
    "portals",
    "say",
    "ebay",
    "amazon",
    "flipkart",
    "various",
    "tech",
    "giants",
    "google",
    "oracle",
    "sap",
    "amazon",
    "microsoft",
    "lots",
    "data",
    "getting",
    "generated",
    "every",
    "day",
    "every",
    "business",
    "sector",
    "point",
    "organizations",
    "slowly",
    "started",
    "realizing",
    "would",
    "interested",
    "working",
    "data",
    "question",
    "asked",
    "organizations",
    "interested",
    "big",
    "data",
    "might",
    "already",
    "answered",
    "thought",
    "organizations",
    "interested",
    "precise",
    "analysis",
    "want",
    "work",
    "different",
    "formats",
    "data",
    "structured",
    "unstructured",
    "data",
    "organizations",
    "interested",
    "gaining",
    "insights",
    "finding",
    "hidden",
    "treasure",
    "called",
    "big",
    "data",
    "main",
    "reason",
    "organizations",
    "interested",
    "big",
    "data",
    "various",
    "use",
    "cases",
    "various",
    "use",
    "cases",
    "compare",
    "organizations",
    "past",
    "50",
    "50",
    "years",
    "handling",
    "huge",
    "amount",
    "data",
    "working",
    "huge",
    "volume",
    "data",
    "question",
    "worked",
    "data",
    "worked",
    "portion",
    "used",
    "store",
    "data",
    "used",
    "something",
    "store",
    "data",
    "happening",
    "changing",
    "talk",
    "businesses",
    "avoid",
    "talking",
    "dynamism",
    "involved",
    "organization",
    "would",
    "want",
    "solution",
    "allows",
    "store",
    "data",
    "store",
    "huge",
    "amount",
    "data",
    "capture",
    "process",
    "analyze",
    "also",
    "look",
    "data",
    "give",
    "value",
    "data",
    "organizations",
    "looking",
    "solutions",
    "let",
    "look",
    "facts",
    "convince",
    "would",
    "convince",
    "data",
    "exploding",
    "needs",
    "attention",
    "right",
    "55",
    "billion",
    "messages",
    "billion",
    "photos",
    "sent",
    "day",
    "whatsapp",
    "300",
    "hours",
    "video",
    "uploaded",
    "every",
    "minute",
    "youtube",
    "guys",
    "know",
    "youtube",
    "second",
    "largest",
    "search",
    "engine",
    "google",
    "every",
    "minute",
    "users",
    "send",
    "million",
    "messages",
    "watch",
    "million",
    "videos",
    "facebook",
    "walmart",
    "handles",
    "1",
    "million",
    "customer",
    "transactions",
    "every",
    "hour",
    "google",
    "40",
    "000",
    "search",
    "queries",
    "performed",
    "google",
    "per",
    "second",
    "million",
    "searches",
    "day",
    "fact",
    "could",
    "also",
    "say",
    "lot",
    "times",
    "people",
    "loading",
    "google",
    "page",
    "basically",
    "check",
    "internet",
    "connection",
    "however",
    "also",
    "generating",
    "data",
    "idc",
    "reports",
    "2025",
    "data",
    "quarter",
    "data",
    "2025",
    "volume",
    "digital",
    "data",
    "increase",
    "163",
    "zeta",
    "bytes",
    "even",
    "talking",
    "gigabytes",
    "terabytes",
    "anymore",
    "talking",
    "petabytes",
    "exabytes",
    "zeta",
    "bytes",
    "zeta",
    "bytes",
    "means",
    "10",
    "power",
    "21",
    "bytes",
    "data",
    "evolved",
    "talk",
    "different",
    "companies",
    "would",
    "want",
    "use",
    "data",
    "take",
    "business",
    "decisions",
    "would",
    "want",
    "collect",
    "data",
    "store",
    "analyze",
    "would",
    "interested",
    "drawing",
    "insights",
    "business",
    "simple",
    "example",
    "facebook",
    "work",
    "data",
    "go",
    "facebook",
    "could",
    "always",
    "check",
    "google",
    "typing",
    "companies",
    "using",
    "big",
    "data",
    "say",
    "companies",
    "using",
    "big",
    "data",
    "able",
    "find",
    "list",
    "different",
    "companies",
    "using",
    "big",
    "data",
    "different",
    "use",
    "cases",
    "various",
    "sources",
    "find",
    "could",
    "also",
    "search",
    "solution",
    "hadoop",
    "discuss",
    "later",
    "could",
    "always",
    "say",
    "companies",
    "using",
    "hadoop",
    "take",
    "wiki",
    "page",
    "basically",
    "help",
    "know",
    "different",
    "companies",
    "using",
    "solution",
    "called",
    "hadoop",
    "okay",
    "coming",
    "back",
    "discussing",
    "organizations",
    "interested",
    "big",
    "data",
    "discussed",
    "gaining",
    "insights",
    "would",
    "want",
    "use",
    "data",
    "find",
    "hidden",
    "information",
    "probably",
    "ignored",
    "earlier",
    "take",
    "example",
    "rdbms",
    "biggest",
    "drawback",
    "using",
    "rdbms",
    "might",
    "think",
    "rdbms",
    "known",
    "stability",
    "consistency",
    "organizations",
    "would",
    "interested",
    "storing",
    "data",
    "oracle",
    "db2",
    "mysql",
    "microsoft",
    "sql",
    "server",
    "many",
    "years",
    "changed",
    "talk",
    "rdbms",
    "first",
    "question",
    "would",
    "ask",
    "access",
    "100",
    "data",
    "online",
    "rdbms",
    "answer",
    "would",
    "10",
    "20",
    "30",
    "percent",
    "data",
    "online",
    "rest",
    "data",
    "would",
    "archived",
    "means",
    "organization",
    "interested",
    "working",
    "data",
    "would",
    "move",
    "data",
    "archived",
    "storage",
    "processing",
    "layer",
    "would",
    "involve",
    "bandwidth",
    "consumption",
    "one",
    "biggest",
    "drawbacks",
    "rdbms",
    "access",
    "100",
    "data",
    "online",
    "many",
    "cases",
    "organizations",
    "started",
    "realizing",
    "data",
    "ignoring",
    "uneconomical",
    "hidden",
    "value",
    "never",
    "exploited",
    "read",
    "presentation",
    "somewhere",
    "said",
    "torture",
    "data",
    "confess",
    "anything",
    "value",
    "data",
    "organizations",
    "realized",
    "recent",
    "past",
    "take",
    "example",
    "facebook",
    "shows",
    "facebook",
    "big",
    "data",
    "come",
    "big",
    "data",
    "let",
    "understand",
    "use",
    "case",
    "facebook",
    "collects",
    "huge",
    "volumes",
    "user",
    "data",
    "whether",
    "sms",
    "whether",
    "likes",
    "whether",
    "advertisements",
    "whether",
    "features",
    "people",
    "liking",
    "photographs",
    "even",
    "user",
    "profiles",
    "collecting",
    "data",
    "providing",
    "portal",
    "people",
    "use",
    "connect",
    "facebook",
    "also",
    "accumulating",
    "huge",
    "volume",
    "data",
    "way",
    "beyond",
    "petabytes",
    "would",
    "also",
    "interested",
    "analyzing",
    "data",
    "one",
    "reasons",
    "would",
    "would",
    "want",
    "personalize",
    "experience",
    "take",
    "example",
    "personalized",
    "news",
    "feed",
    "depending",
    "user",
    "behavior",
    "depending",
    "user",
    "likes",
    "user",
    "would",
    "want",
    "know",
    "recommend",
    "personalized",
    "news",
    "feed",
    "every",
    "particular",
    "user",
    "one",
    "example",
    "facebook",
    "data",
    "take",
    "example",
    "photo",
    "tag",
    "suggestions",
    "log",
    "facebook",
    "account",
    "could",
    "also",
    "get",
    "suggestions",
    "different",
    "friends",
    "would",
    "like",
    "connect",
    "would",
    "want",
    "tag",
    "could",
    "known",
    "others",
    "examples",
    "show",
    "facebook",
    "uses",
    "data",
    "follows",
    "flashback",
    "collection",
    "photos",
    "posts",
    "received",
    "comments",
    "likes",
    "okay",
    "something",
    "called",
    "voted",
    "used",
    "2016",
    "elections",
    "reminders",
    "directions",
    "tell",
    "users",
    "time",
    "place",
    "polling",
    "also",
    "something",
    "called",
    "safety",
    "checks",
    "incidents",
    "earthquake",
    "hurricane",
    "mass",
    "shooting",
    "facebook",
    "gives",
    "safety",
    "checks",
    "examples",
    "facebook",
    "using",
    "big",
    "data",
    "brings",
    "us",
    "question",
    "big",
    "data",
    "example",
    "discussed",
    "one",
    "company",
    "making",
    "use",
    "data",
    "accumulated",
    "companies",
    "social",
    "media",
    "oriented",
    "like",
    "facebook",
    "data",
    "important",
    "take",
    "example",
    "ibm",
    "take",
    "example",
    "jpmorgan",
    "chase",
    "take",
    "example",
    "ge",
    "organization",
    "collecting",
    "huge",
    "amount",
    "data",
    "would",
    "want",
    "gather",
    "insights",
    "would",
    "want",
    "analyze",
    "data",
    "would",
    "want",
    "precise",
    "building",
    "services",
    "solutions",
    "take",
    "care",
    "customers",
    "big",
    "data",
    "big",
    "data",
    "basically",
    "term",
    "used",
    "describe",
    "data",
    "large",
    "complex",
    "store",
    "traditional",
    "databases",
    "gave",
    "example",
    "storing",
    "data",
    "also",
    "data",
    "also",
    "means",
    "lot",
    "dynamism",
    "involved",
    "change",
    "underlying",
    "storage",
    "handle",
    "kind",
    "data",
    "comes",
    "get",
    "let",
    "understand",
    "big",
    "data",
    "big",
    "data",
    "basically",
    "term",
    "given",
    "categorize",
    "data",
    "different",
    "characteristics",
    "organizations",
    "would",
    "want",
    "big",
    "data",
    "stored",
    "processed",
    "analyzed",
    "get",
    "whatever",
    "useful",
    "information",
    "get",
    "data",
    "five",
    "v",
    "big",
    "data",
    "volume",
    "velocity",
    "variety",
    "value",
    "veracity",
    "although",
    "five",
    "v",
    "v",
    "also",
    "categorize",
    "data",
    "big",
    "data",
    "volatility",
    "validity",
    "viscosity",
    "virality",
    "data",
    "okay",
    "five",
    "v",
    "big",
    "data",
    "data",
    "one",
    "characteristics",
    "considered",
    "big",
    "data",
    "including",
    "ways",
    "mentioned",
    "volume",
    "basically",
    "means",
    "incredible",
    "amount",
    "data",
    "huge",
    "volumes",
    "data",
    "data",
    "generated",
    "every",
    "second",
    "could",
    "used",
    "batch",
    "processing",
    "could",
    "used",
    "stream",
    "processing",
    "okay",
    "might",
    "data",
    "generated",
    "different",
    "kind",
    "devices",
    "like",
    "cell",
    "phones",
    "social",
    "media",
    "websites",
    "online",
    "transactions",
    "variable",
    "devices",
    "servers",
    "days",
    "iot",
    "also",
    "talking",
    "data",
    "getting",
    "generated",
    "via",
    "internet",
    "things",
    "could",
    "different",
    "devices",
    "could",
    "communicating",
    "could",
    "getting",
    "data",
    "radars",
    "leaders",
    "even",
    "camera",
    "sensors",
    "huge",
    "volume",
    "data",
    "getting",
    "generated",
    "talking",
    "data",
    "huge",
    "volume",
    "getting",
    "generated",
    "constantly",
    "accumulated",
    "period",
    "time",
    "would",
    "say",
    "big",
    "data",
    "velocity",
    "one",
    "important",
    "aspect",
    "big",
    "data",
    "speed",
    "data",
    "getting",
    "generated",
    "think",
    "stock",
    "markets",
    "think",
    "social",
    "media",
    "websites",
    "think",
    "online",
    "surveys",
    "marketing",
    "campaigns",
    "airline",
    "industry",
    "data",
    "getting",
    "generated",
    "lot",
    "speed",
    "becomes",
    "difficult",
    "capture",
    "collect",
    "process",
    "cure",
    "mine",
    "analyze",
    "data",
    "certainly",
    "talking",
    "big",
    "data",
    "next",
    "aspect",
    "big",
    "data",
    "variety",
    "talk",
    "structured",
    "data",
    "data",
    "unstructured",
    "data",
    "would",
    "like",
    "ask",
    "question",
    "difference",
    "call",
    "data",
    "structured",
    "unstructured",
    "let",
    "look",
    "example",
    "theoretically",
    "discuss",
    "always",
    "would",
    "like",
    "use",
    "examples",
    "let",
    "look",
    "log",
    "file",
    "let",
    "see",
    "look",
    "log",
    "file",
    "would",
    "say",
    "kind",
    "data",
    "highlighted",
    "one",
    "answer",
    "would",
    "structured",
    "data",
    "specific",
    "delimiters",
    "space",
    "data",
    "separated",
    "space",
    "hundred",
    "thousand",
    "million",
    "rows",
    "similar",
    "kind",
    "data",
    "could",
    "certainly",
    "store",
    "table",
    "could",
    "predefined",
    "schema",
    "store",
    "data",
    "would",
    "call",
    "one",
    "highlighted",
    "structured",
    "look",
    "portion",
    "would",
    "look",
    "combination",
    "kind",
    "data",
    "data",
    "pattern",
    "data",
    "example",
    "data",
    "would",
    "predefined",
    "structure",
    "store",
    "data",
    "probably",
    "pattern",
    "data",
    "would",
    "break",
    "structure",
    "look",
    "data",
    "would",
    "certainly",
    "call",
    "unstructured",
    "data",
    "clear",
    "schema",
    "define",
    "data",
    "mean",
    "variety",
    "data",
    "structured",
    "data",
    "basically",
    "schema",
    "format",
    "could",
    "easily",
    "understood",
    "could",
    "like",
    "xml",
    "json",
    "even",
    "excel",
    "sheets",
    "could",
    "data",
    "structured",
    "unstructured",
    "talk",
    "unstructured",
    "talking",
    "absence",
    "schema",
    "format",
    "schema",
    "hard",
    "analyze",
    "brings",
    "challenges",
    "next",
    "aspect",
    "value",
    "value",
    "refers",
    "ability",
    "turn",
    "data",
    "useful",
    "business",
    "would",
    "lot",
    "data",
    "collected",
    "mentioned",
    "previous",
    "slides",
    "right",
    "would",
    "lot",
    "data",
    "wrangling",
    "data",
    "cleaning",
    "data",
    "happening",
    "finally",
    "would",
    "want",
    "draw",
    "value",
    "data",
    "data",
    "collected",
    "percentage",
    "data",
    "gives",
    "us",
    "value",
    "data",
    "give",
    "value",
    "would",
    "use",
    "aspect",
    "big",
    "data",
    "right",
    "veracity",
    "means",
    "quality",
    "data",
    "billions",
    "dollars",
    "lost",
    "every",
    "year",
    "organizations",
    "data",
    "collected",
    "good",
    "quality",
    "probably",
    "collected",
    "lot",
    "data",
    "erroneous",
    "take",
    "example",
    "autonomous",
    "driving",
    "projects",
    "happening",
    "europe",
    "car",
    "fleets",
    "road",
    "collecting",
    "data",
    "via",
    "radar",
    "sensors",
    "camera",
    "sensors",
    "data",
    "processed",
    "train",
    "algorithms",
    "realized",
    "sometimes",
    "data",
    "collected",
    "missing",
    "values",
    "might",
    "appropriate",
    "lot",
    "errors",
    "process",
    "collecting",
    "data",
    "becomes",
    "repetitive",
    "task",
    "quality",
    "data",
    "good",
    "one",
    "example",
    "take",
    "example",
    "healthcare",
    "industry",
    "stock",
    "markets",
    "financial",
    "institutions",
    "extracting",
    "loads",
    "data",
    "useful",
    "data",
    "messy",
    "poor",
    "quality",
    "basically",
    "means",
    "velocity",
    "important",
    "v",
    "big",
    "data",
    "apart",
    "veracity",
    "volume",
    "variety",
    "velocity",
    "value",
    "v",
    "viscosity",
    "dense",
    "data",
    "validity",
    "data",
    "still",
    "valid",
    "volatility",
    "data",
    "volatile",
    "virality",
    "data",
    "viral",
    "different",
    "v",
    "categorize",
    "data",
    "big",
    "data",
    "would",
    "like",
    "talk",
    "big",
    "data",
    "case",
    "study",
    "taken",
    "example",
    "google",
    "obviously",
    "one",
    "companies",
    "churning",
    "working",
    "huge",
    "amount",
    "data",
    "actually",
    "said",
    "compare",
    "one",
    "grain",
    "sand",
    "one",
    "byte",
    "data",
    "google",
    "processing",
    "google",
    "handling",
    "whole",
    "worlds",
    "sand",
    "every",
    "week",
    "kind",
    "data",
    "google",
    "processing",
    "early",
    "2000",
    "since",
    "number",
    "internet",
    "users",
    "started",
    "growing",
    "google",
    "also",
    "faced",
    "lot",
    "problems",
    "storing",
    "increasing",
    "user",
    "data",
    "using",
    "traditional",
    "servers",
    "manage",
    "challenge",
    "google",
    "started",
    "facing",
    "could",
    "use",
    "traditional",
    "data",
    "server",
    "store",
    "data",
    "well",
    "yes",
    "could",
    "right",
    "storage",
    "devices",
    "getting",
    "cheaper",
    "day",
    "day",
    "much",
    "time",
    "take",
    "retrieve",
    "data",
    "seek",
    "time",
    "time",
    "taken",
    "read",
    "process",
    "data",
    "thousands",
    "search",
    "queries",
    "raised",
    "per",
    "second",
    "doubt",
    "could",
    "say",
    "millions",
    "billions",
    "queries",
    "raised",
    "per",
    "second",
    "every",
    "query",
    "read",
    "100",
    "mbs",
    "data",
    "consumed",
    "tens",
    "billions",
    "cpu",
    "cycles",
    "based",
    "queries",
    "requirement",
    "wanted",
    "large",
    "distributed",
    "highly",
    "fault",
    "tolerant",
    "file",
    "system",
    "large",
    "store",
    "capture",
    "process",
    "huge",
    "amount",
    "data",
    "distributed",
    "could",
    "rely",
    "one",
    "server",
    "even",
    "multiple",
    "disks",
    "stacked",
    "efficient",
    "choice",
    "would",
    "happen",
    "particular",
    "machine",
    "failed",
    "would",
    "happen",
    "whole",
    "server",
    "needed",
    "distributed",
    "storage",
    "distributed",
    "computing",
    "environment",
    "needed",
    "something",
    "highly",
    "fault",
    "tolerant",
    "right",
    "requirement",
    "google",
    "solution",
    "came",
    "result",
    "gfs",
    "google",
    "file",
    "system",
    "let",
    "look",
    "gfs",
    "works",
    "normally",
    "particular",
    "linux",
    "system",
    "linux",
    "server",
    "would",
    "file",
    "system",
    "would",
    "set",
    "processors",
    "would",
    "set",
    "files",
    "directories",
    "could",
    "store",
    "data",
    "gfs",
    "different",
    "facilitate",
    "gfs",
    "could",
    "store",
    "huge",
    "amount",
    "data",
    "architecture",
    "architecture",
    "one",
    "master",
    "multiple",
    "chunk",
    "servers",
    "say",
    "slave",
    "servers",
    "slave",
    "machines",
    "master",
    "machine",
    "contain",
    "metadata",
    "contain",
    "data",
    "data",
    "say",
    "metadata",
    "talking",
    "information",
    "data",
    "chung",
    "servers",
    "slave",
    "machines",
    "could",
    "storing",
    "data",
    "distributed",
    "fashion",
    "client",
    "api",
    "application",
    "would",
    "want",
    "read",
    "data",
    "would",
    "first",
    "contact",
    "master",
    "server",
    "would",
    "contact",
    "machine",
    "master",
    "process",
    "running",
    "client",
    "would",
    "place",
    "request",
    "reading",
    "data",
    "showing",
    "interest",
    "reading",
    "data",
    "internally",
    "requesting",
    "metadata",
    "api",
    "application",
    "would",
    "want",
    "know",
    "read",
    "data",
    "master",
    "server",
    "metadata",
    "whether",
    "ram",
    "disk",
    "discuss",
    "later",
    "master",
    "server",
    "would",
    "metadata",
    "would",
    "know",
    "chunk",
    "servers",
    "slave",
    "machines",
    "data",
    "stored",
    "distributed",
    "fashion",
    "master",
    "would",
    "respond",
    "back",
    "metadata",
    "information",
    "client",
    "client",
    "could",
    "use",
    "information",
    "read",
    "write",
    "slave",
    "machines",
    "actually",
    "data",
    "stored",
    "process",
    "set",
    "processes",
    "work",
    "together",
    "make",
    "gfs",
    "say",
    "chunk",
    "server",
    "would",
    "basically",
    "files",
    "getting",
    "divided",
    "fixed",
    "size",
    "chunks",
    "would",
    "get",
    "divided",
    "would",
    "kind",
    "chunk",
    "size",
    "block",
    "size",
    "would",
    "determine",
    "file",
    "bigger",
    "chunk",
    "size",
    "would",
    "split",
    "smaller",
    "chunks",
    "distributed",
    "across",
    "chunk",
    "servers",
    "slave",
    "machines",
    "file",
    "smaller",
    "would",
    "still",
    "use",
    "one",
    "chunk",
    "block",
    "get",
    "stored",
    "underlying",
    "slave",
    "machines",
    "junk",
    "servers",
    "slave",
    "machines",
    "ones",
    "actually",
    "store",
    "data",
    "local",
    "disks",
    "linux",
    "files",
    "client",
    "interacting",
    "master",
    "metadata",
    "interacting",
    "junk",
    "servers",
    "read",
    "write",
    "operations",
    "would",
    "one",
    "would",
    "externally",
    "connecting",
    "cluster",
    "would",
    "look",
    "master",
    "would",
    "obviously",
    "receiving",
    "kind",
    "heartbeats",
    "chunk",
    "servers",
    "know",
    "status",
    "receive",
    "information",
    "form",
    "packets",
    "would",
    "let",
    "master",
    "know",
    "machines",
    "available",
    "storage",
    "machines",
    "already",
    "data",
    "master",
    "would",
    "build",
    "metadata",
    "within",
    "files",
    "would",
    "broken",
    "chunks",
    "example",
    "look",
    "file",
    "one",
    "broken",
    "chunk",
    "one",
    "chunk",
    "two",
    "file",
    "two",
    "one",
    "chunk",
    "one",
    "portion",
    "file",
    "reciting",
    "chunk",
    "server",
    "also",
    "lets",
    "us",
    "know",
    "kind",
    "auto",
    "replication",
    "file",
    "system",
    "right",
    "data",
    "getting",
    "stored",
    "chunk",
    "could",
    "data",
    "64",
    "mb",
    "chunk",
    "size",
    "could",
    "changed",
    "based",
    "data",
    "size",
    "google",
    "file",
    "system",
    "basic",
    "size",
    "chunk",
    "64",
    "mb",
    "chunk",
    "would",
    "replicated",
    "multiple",
    "servers",
    "default",
    "replication",
    "three",
    "could",
    "increased",
    "decreased",
    "per",
    "requirement",
    "would",
    "also",
    "mean",
    "particular",
    "slave",
    "machine",
    "chunk",
    "server",
    "would",
    "die",
    "would",
    "get",
    "killed",
    "would",
    "crash",
    "would",
    "never",
    "data",
    "loss",
    "replica",
    "data",
    "residing",
    "failed",
    "machine",
    "would",
    "still",
    "available",
    "slave",
    "server",
    "chunk",
    "server",
    "slave",
    "machine",
    "helped",
    "google",
    "store",
    "process",
    "huge",
    "volumes",
    "data",
    "distributed",
    "manner",
    "thus",
    "fault",
    "tolerant",
    "distributed",
    "scalable",
    "storage",
    "could",
    "allow",
    "store",
    "huge",
    "amount",
    "data",
    "one",
    "example",
    "actually",
    "led",
    "solution",
    "today",
    "call",
    "hadoop",
    "talk",
    "big",
    "data",
    "would",
    "like",
    "ask",
    "questions",
    "talking",
    "rdbms",
    "case",
    "take",
    "example",
    "uh",
    "something",
    "like",
    "nasa",
    "working",
    "project",
    "called",
    "setai",
    "search",
    "extraterrestrial",
    "intelligence",
    "project",
    "looking",
    "solution",
    "take",
    "care",
    "problem",
    "problem",
    "would",
    "roughly",
    "send",
    "waves",
    "space",
    "capture",
    "waves",
    "back",
    "analyze",
    "data",
    "find",
    "extraterrestrial",
    "object",
    "space",
    "two",
    "options",
    "could",
    "either",
    "huge",
    "server",
    "built",
    "could",
    "take",
    "care",
    "storing",
    "data",
    "processing",
    "could",
    "go",
    "volunteer",
    "computing",
    "volunteer",
    "computing",
    "basically",
    "means",
    "could",
    "lot",
    "people",
    "volunteering",
    "part",
    "project",
    "would",
    "turn",
    "would",
    "donating",
    "ram",
    "storage",
    "machines",
    "using",
    "would",
    "happen",
    "basically",
    "download",
    "kind",
    "patch",
    "machine",
    "would",
    "run",
    "screen",
    "saver",
    "user",
    "using",
    "machine",
    "portion",
    "data",
    "could",
    "transferred",
    "machines",
    "intermittent",
    "storage",
    "processing",
    "using",
    "ram",
    "sounds",
    "interesting",
    "sounds",
    "easy",
    "however",
    "would",
    "challenges",
    "right",
    "think",
    "security",
    "think",
    "integrity",
    "problems",
    "bigger",
    "much",
    "requirement",
    "bandwidth",
    "thing",
    "happens",
    "rdbms",
    "would",
    "move",
    "data",
    "archived",
    "solution",
    "processing",
    "layer",
    "would",
    "consume",
    "huge",
    "amount",
    "bandwidth",
    "big",
    "data",
    "brings",
    "challenges",
    "huge",
    "amount",
    "data",
    "getting",
    "generated",
    "every",
    "day",
    "biggest",
    "challenge",
    "storing",
    "huge",
    "volume",
    "data",
    "especially",
    "data",
    "getting",
    "generated",
    "lot",
    "variety",
    "different",
    "kind",
    "formats",
    "could",
    "viral",
    "could",
    "lot",
    "value",
    "nobody",
    "looked",
    "veracity",
    "data",
    "primary",
    "problem",
    "would",
    "handling",
    "huge",
    "volume",
    "data",
    "variety",
    "data",
    "would",
    "bring",
    "challenges",
    "storing",
    "legacy",
    "systems",
    "processing",
    "data",
    "required",
    "would",
    "suggest",
    "need",
    "think",
    "difference",
    "reading",
    "data",
    "processing",
    "data",
    "reading",
    "might",
    "mean",
    "bringing",
    "data",
    "disk",
    "io",
    "operations",
    "processing",
    "would",
    "mean",
    "reading",
    "data",
    "probably",
    "transformations",
    "extracting",
    "useful",
    "information",
    "storing",
    "format",
    "probably",
    "different",
    "format",
    "processing",
    "massive",
    "volume",
    "data",
    "second",
    "challenge",
    "organizations",
    "store",
    "big",
    "data",
    "would",
    "eventually",
    "want",
    "use",
    "process",
    "gather",
    "insights",
    "processing",
    "extracting",
    "insights",
    "big",
    "data",
    "would",
    "take",
    "huge",
    "amount",
    "time",
    "unless",
    "efficient",
    "solution",
    "handle",
    "process",
    "big",
    "data",
    "securing",
    "data",
    "concern",
    "organizations",
    "right",
    "encryption",
    "big",
    "data",
    "difficult",
    "perform",
    "would",
    "think",
    "different",
    "compression",
    "mechanisms",
    "would",
    "also",
    "mean",
    "decompressing",
    "data",
    "would",
    "also",
    "mean",
    "could",
    "take",
    "hit",
    "cpu",
    "cycles",
    "disk",
    "usage",
    "providing",
    "user",
    "authentication",
    "every",
    "team",
    "member",
    "could",
    "also",
    "dangerous",
    "led",
    "hadoop",
    "solution",
    "big",
    "data",
    "brings",
    "challenges",
    "big",
    "data",
    "brings",
    "benefits",
    "solution",
    "hadoop",
    "hadoop",
    "open",
    "source",
    "framework",
    "storing",
    "data",
    "running",
    "applications",
    "clusters",
    "commodity",
    "hardware",
    "hadoop",
    "open",
    "source",
    "framework",
    "discuss",
    "two",
    "main",
    "components",
    "hadoop",
    "would",
    "good",
    "look",
    "link",
    "suggesting",
    "earlier",
    "companies",
    "using",
    "hadoop",
    "person",
    "would",
    "interested",
    "learning",
    "big",
    "data",
    "start",
    "somewhere",
    "could",
    "list",
    "different",
    "companies",
    "kind",
    "setup",
    "hadoop",
    "kind",
    "processing",
    "using",
    "called",
    "hadoop",
    "clusters",
    "process",
    "fact",
    "store",
    "capture",
    "process",
    "huge",
    "amount",
    "data",
    "another",
    "link",
    "would",
    "suggest",
    "looking",
    "different",
    "distributions",
    "hadoop",
    "person",
    "interested",
    "learning",
    "big",
    "data",
    "know",
    "different",
    "distributions",
    "hadoop",
    "linux",
    "different",
    "different",
    "distributions",
    "like",
    "ubuntu",
    "centos",
    "red",
    "hat",
    "susie",
    "debian",
    "way",
    "different",
    "distributions",
    "hadoop",
    "look",
    "wiki",
    "page",
    "link",
    "talks",
    "products",
    "include",
    "apache",
    "hadoop",
    "derivative",
    "works",
    "commercial",
    "support",
    "basically",
    "means",
    "apache",
    "hadoop",
    "sole",
    "products",
    "called",
    "release",
    "apache",
    "hadoop",
    "come",
    "open",
    "source",
    "community",
    "various",
    "distributions",
    "like",
    "amazon",
    "web",
    "services",
    "cloud",
    "era",
    "hotend",
    "works",
    "ibm",
    "big",
    "inside",
    "mapper",
    "different",
    "distributions",
    "hadoop",
    "basically",
    "distributions",
    "depending",
    "using",
    "core",
    "apache",
    "hadoop",
    "brief",
    "say",
    "vendors",
    "take",
    "apache",
    "hadoop",
    "package",
    "within",
    "cluster",
    "management",
    "solution",
    "users",
    "intend",
    "use",
    "apache",
    "hadoop",
    "would",
    "difficulties",
    "setting",
    "cluster",
    "setting",
    "framework",
    "could",
    "use",
    "distribution",
    "cluster",
    "installation",
    "solutions",
    "clustered",
    "management",
    "solution",
    "easily",
    "plan",
    "deploy",
    "install",
    "manage",
    "cluster",
    "quick",
    "demo",
    "setting",
    "cloudera",
    "quickstart",
    "vm",
    "case",
    "interested",
    "working",
    "standalone",
    "cluster",
    "download",
    "cloudera",
    "quick",
    "start",
    "vm",
    "type",
    "download",
    "cloudera",
    "quick",
    "start",
    "vm",
    "search",
    "package",
    "used",
    "set",
    "quick",
    "start",
    "vm",
    "would",
    "single",
    "node",
    "cloudera",
    "based",
    "cluster",
    "click",
    "link",
    "basically",
    "based",
    "platform",
    "would",
    "choosing",
    "install",
    "using",
    "vm",
    "box",
    "version",
    "cloudera",
    "would",
    "install",
    "select",
    "platform",
    "choose",
    "virtualbox",
    "click",
    "get",
    "give",
    "details",
    "basically",
    "allow",
    "download",
    "quick",
    "start",
    "vm",
    "would",
    "look",
    "something",
    "like",
    "zip",
    "file",
    "downloaded",
    "unzip",
    "used",
    "set",
    "single",
    "node",
    "cloud",
    "error",
    "cluster",
    "downloaded",
    "zip",
    "file",
    "would",
    "look",
    "something",
    "like",
    "would",
    "quick",
    "start",
    "virtual",
    "box",
    "virtual",
    "boss",
    "disk",
    "used",
    "set",
    "cluster",
    "ignore",
    "files",
    "related",
    "amazon",
    "machines",
    "need",
    "would",
    "used",
    "set",
    "cloud",
    "data",
    "cluster",
    "set",
    "click",
    "file",
    "import",
    "appliance",
    "choose",
    "quick",
    "start",
    "vm",
    "looking",
    "downloads",
    "quick",
    "start",
    "vm",
    "select",
    "click",
    "open",
    "click",
    "next",
    "shows",
    "specifications",
    "cpu",
    "ram",
    "change",
    "later",
    "click",
    "import",
    "start",
    "importing",
    "virtual",
    "disk",
    "image",
    "dot",
    "vmdk",
    "file",
    "vm",
    "box",
    "done",
    "change",
    "specifications",
    "machines",
    "use",
    "two",
    "cpu",
    "cores",
    "minimum",
    "give",
    "little",
    "ram",
    "cloudera",
    "quick",
    "start",
    "vm",
    "cpu",
    "intensive",
    "needs",
    "good",
    "amount",
    "ram",
    "survive",
    "give",
    "2",
    "cpu",
    "cores",
    "5",
    "gb",
    "ram",
    "enough",
    "us",
    "bring",
    "quick",
    "start",
    "vm",
    "gives",
    "us",
    "cloudera",
    "distribution",
    "hadoop",
    "single",
    "node",
    "cluster",
    "setup",
    "used",
    "working",
    "learning",
    "different",
    "distributions",
    "cloudera",
    "clusters",
    "working",
    "sdfs",
    "hadoop",
    "ecosystem",
    "components",
    "let",
    "wait",
    "importing",
    "finish",
    "go",
    "ahead",
    "set",
    "quick",
    "start",
    "vm",
    "practice",
    "importing",
    "appliance",
    "done",
    "see",
    "cloudera",
    "quickstart",
    "machine",
    "added",
    "list",
    "machines",
    "click",
    "click",
    "settings",
    "mentioned",
    "would",
    "like",
    "give",
    "ram",
    "cpu",
    "cores",
    "click",
    "system",
    "let",
    "increase",
    "ram",
    "least",
    "five",
    "click",
    "processor",
    "let",
    "give",
    "two",
    "cpu",
    "cores",
    "would",
    "least",
    "better",
    "using",
    "one",
    "cpu",
    "core",
    "network",
    "goes",
    "nat",
    "fine",
    "click",
    "ok",
    "would",
    "want",
    "start",
    "machine",
    "uses",
    "two",
    "cpu",
    "cores",
    "5gb",
    "ram",
    "bring",
    "cloudera",
    "quick",
    "start",
    "vm",
    "let",
    "go",
    "ahead",
    "start",
    "machine",
    "quick",
    "start",
    "vm",
    "might",
    "take",
    "initially",
    "time",
    "start",
    "internally",
    "various",
    "cloudera",
    "services",
    "starting",
    "services",
    "need",
    "cloud",
    "era",
    "quick",
    "start",
    "vm",
    "accessible",
    "unlike",
    "apache",
    "hadoop",
    "cluster",
    "start",
    "cluster",
    "starting",
    "processes",
    "case",
    "cloudera",
    "cloudera",
    "scm",
    "server",
    "agents",
    "take",
    "care",
    "starting",
    "services",
    "starting",
    "different",
    "roles",
    "services",
    "explained",
    "previous",
    "session",
    "cloud",
    "era",
    "cluster",
    "would",
    "services",
    "let",
    "show",
    "case",
    "apache",
    "cluster",
    "start",
    "services",
    "start",
    "cluster",
    "running",
    "script",
    "basically",
    "scripts",
    "individually",
    "start",
    "different",
    "processes",
    "different",
    "nodes",
    "case",
    "cloud",
    "era",
    "would",
    "always",
    "cloudera",
    "scm",
    "server",
    "would",
    "running",
    "one",
    "machine",
    "including",
    "machine",
    "would",
    "clouded",
    "icm",
    "agents",
    "would",
    "running",
    "multiple",
    "machines",
    "similarly",
    "hortonworks",
    "cluster",
    "would",
    "ambari",
    "server",
    "starting",
    "first",
    "machine",
    "ambari",
    "agents",
    "running",
    "machines",
    "server",
    "component",
    "knows",
    "services",
    "set",
    "configurations",
    "agents",
    "running",
    "every",
    "node",
    "responsible",
    "send",
    "heartbeats",
    "server",
    "receive",
    "instructions",
    "take",
    "care",
    "starting",
    "stopping",
    "individual",
    "roles",
    "different",
    "machines",
    "case",
    "single",
    "node",
    "cluster",
    "setup",
    "quick",
    "start",
    "vm",
    "would",
    "one",
    "scm",
    "server",
    "one",
    "sem",
    "agent",
    "start",
    "machine",
    "take",
    "care",
    "roles",
    "need",
    "started",
    "different",
    "services",
    "wait",
    "machine",
    "come",
    "basically",
    "clouded",
    "sem",
    "server",
    "agent",
    "running",
    "need",
    "follow",
    "steps",
    "cloudera",
    "admin",
    "console",
    "accessible",
    "allows",
    "browse",
    "cluster",
    "look",
    "different",
    "services",
    "look",
    "roles",
    "different",
    "services",
    "also",
    "work",
    "cluster",
    "either",
    "using",
    "command",
    "line",
    "using",
    "web",
    "interface",
    "machine",
    "come",
    "already",
    "connected",
    "internet",
    "see",
    "need",
    "certain",
    "things",
    "admin",
    "console",
    "accessible",
    "point",
    "time",
    "click",
    "terminal",
    "check",
    "access",
    "cluster",
    "type",
    "host",
    "name",
    "shows",
    "host",
    "name",
    "also",
    "type",
    "hdfs",
    "command",
    "see",
    "access",
    "cluster",
    "working",
    "commands",
    "would",
    "give",
    "apache",
    "hadoop",
    "cluster",
    "distribution",
    "loop",
    "sometimes",
    "cluster",
    "access",
    "terminal",
    "might",
    "take",
    "seconds",
    "minutes",
    "connection",
    "established",
    "cloudera",
    "cm",
    "server",
    "cloudera",
    "cm",
    "agent",
    "running",
    "background",
    "takes",
    "care",
    "cluster",
    "given",
    "sdfs",
    "dfs",
    "list",
    "command",
    "basically",
    "show",
    "default",
    "exists",
    "sdfs",
    "let",
    "give",
    "couple",
    "seconds",
    "shows",
    "us",
    "also",
    "check",
    "giving",
    "service",
    "cloudera",
    "scm",
    "server",
    "status",
    "tells",
    "would",
    "want",
    "use",
    "cloudera",
    "express",
    "free",
    "run",
    "command",
    "needs",
    "8",
    "gb",
    "ram",
    "leads",
    "virtual",
    "cpu",
    "cores",
    "also",
    "mentions",
    "may",
    "take",
    "several",
    "minutes",
    "cloudera",
    "manager",
    "started",
    "login",
    "root",
    "give",
    "command",
    "service",
    "cloud",
    "error",
    "scm",
    "server",
    "status",
    "remember",
    "password",
    "root",
    "clouded",
    "basically",
    "says",
    "would",
    "want",
    "check",
    "settings",
    "good",
    "express",
    "edition",
    "running",
    "close",
    "sdfs",
    "access",
    "working",
    "fine",
    "let",
    "close",
    "terminal",
    "launch",
    "cloud",
    "error",
    "express",
    "click",
    "give",
    "need",
    "give",
    "command",
    "force",
    "let",
    "copy",
    "command",
    "let",
    "open",
    "different",
    "terminal",
    "let",
    "give",
    "command",
    "like",
    "go",
    "ahead",
    "shut",
    "cloudera",
    "based",
    "services",
    "restart",
    "able",
    "access",
    "admin",
    "console",
    "let",
    "give",
    "couple",
    "minutes",
    "access",
    "admin",
    "console",
    "see",
    "starting",
    "cloudera",
    "manager",
    "server",
    "waiting",
    "cloudera",
    "manager",
    "api",
    "starting",
    "cloudera",
    "manager",
    "agents",
    "configuring",
    "deployment",
    "per",
    "new",
    "settings",
    "given",
    "use",
    "express",
    "edition",
    "cloudera",
    "done",
    "say",
    "cluster",
    "restarted",
    "admin",
    "console",
    "accessed",
    "id",
    "password",
    "cloudera",
    "give",
    "couple",
    "minutes",
    "done",
    "ready",
    "use",
    "admin",
    "console",
    "deployment",
    "configured",
    "client",
    "configurations",
    "also",
    "deployed",
    "restarted",
    "cloudera",
    "management",
    "service",
    "gives",
    "access",
    "quick",
    "start",
    "admin",
    "console",
    "using",
    "username",
    "password",
    "cloud",
    "error",
    "let",
    "try",
    "accessing",
    "open",
    "browser",
    "let",
    "change",
    "7",
    "1",
    "8",
    "0",
    "default",
    "port",
    "shows",
    "admin",
    "console",
    "coming",
    "log",
    "cloud",
    "error",
    "cloud",
    "error",
    "let",
    "click",
    "login",
    "said",
    "cloudera",
    "cpu",
    "intensive",
    "memory",
    "intensive",
    "would",
    "slow",
    "since",
    "given",
    "enough",
    "gb",
    "ram",
    "cloud",
    "error",
    "cluster",
    "thus",
    "advisable",
    "stop",
    "even",
    "remove",
    "services",
    "need",
    "look",
    "services",
    "look",
    "stop",
    "status",
    "good",
    "one",
    "way",
    "go",
    "ahead",
    "remove",
    "services",
    "use",
    "beginning",
    "later",
    "anytime",
    "add",
    "services",
    "cluster",
    "example",
    "click",
    "key",
    "value",
    "store",
    "scroll",
    "says",
    "delete",
    "remove",
    "service",
    "admin",
    "console",
    "anytime",
    "removing",
    "particular",
    "service",
    "remove",
    "service",
    "management",
    "cloud",
    "direct",
    "manager",
    "role",
    "groups",
    "service",
    "removed",
    "host",
    "templates",
    "click",
    "delete",
    "service",
    "depending",
    "service",
    "would",
    "prompted",
    "message",
    "remove",
    "relevant",
    "services",
    "particular",
    "service",
    "depends",
    "service",
    "already",
    "running",
    "would",
    "given",
    "message",
    "service",
    "stopped",
    "deleted",
    "cloudera",
    "admin",
    "console",
    "admin",
    "console",
    "allows",
    "click",
    "services",
    "look",
    "different",
    "roles",
    "processes",
    "running",
    "service",
    "anyways",
    "access",
    "cloudera",
    "cluster",
    "terminal",
    "using",
    "regular",
    "sdfs",
    "yarn",
    "map",
    "red",
    "commands",
    "removed",
    "service",
    "also",
    "remove",
    "solar",
    "using",
    "beginning",
    "depends",
    "choice",
    "scroll",
    "delete",
    "says",
    "deleting",
    "solar",
    "service",
    "must",
    "remove",
    "dependencies",
    "service",
    "configuration",
    "following",
    "services",
    "hue",
    "hue",
    "web",
    "interface",
    "allows",
    "work",
    "sdfs",
    "depending",
    "click",
    "configure",
    "service",
    "dependency",
    "make",
    "sure",
    "hue",
    "service",
    "depend",
    "particular",
    "service",
    "removing",
    "clean",
    "removal",
    "service",
    "click",
    "none",
    "say",
    "save",
    "changes",
    "done",
    "go",
    "ahead",
    "try",
    "removing",
    "solar",
    "service",
    "admin",
    "console",
    "reduce",
    "load",
    "management",
    "console",
    "also",
    "allow",
    "work",
    "faster",
    "cluster",
    "removed",
    "dependency",
    "hue",
    "solar",
    "click",
    "delete",
    "remember",
    "cluster",
    "becomes",
    "little",
    "lighter",
    "work",
    "focus",
    "services",
    "point",
    "time",
    "want",
    "add",
    "services",
    "cluster",
    "anytime",
    "fix",
    "different",
    "configuration",
    "issues",
    "like",
    "see",
    "different",
    "warning",
    "messages",
    "services",
    "already",
    "existing",
    "need",
    "service",
    "click",
    "drop",
    "click",
    "delete",
    "says",
    "scoop",
    "2",
    "also",
    "relevance",
    "hue",
    "hue",
    "web",
    "interface",
    "also",
    "depends",
    "scope",
    "make",
    "none",
    "point",
    "time",
    "later",
    "add",
    "services",
    "clicking",
    "add",
    "service",
    "option",
    "cluster",
    "admin",
    "access",
    "quick",
    "start",
    "vm",
    "gives",
    "single",
    "node",
    "cloud",
    "error",
    "cluster",
    "use",
    "learning",
    "practicing",
    "click",
    "scope",
    "2",
    "say",
    "delete",
    "configured",
    "dependency",
    "remove",
    "scope",
    "2",
    "also",
    "list",
    "services",
    "admin",
    "console",
    "managing",
    "right",
    "done",
    "removed",
    "three",
    "services",
    "need",
    "even",
    "remove",
    "scope",
    "client",
    "need",
    "add",
    "later",
    "various",
    "alerts",
    "cloudera",
    "admin",
    "console",
    "shows",
    "always",
    "fix",
    "clicking",
    "health",
    "issues",
    "configuration",
    "issues",
    "click",
    "see",
    "health",
    "issue",
    "pointing",
    "critical",
    "one",
    "ignored",
    "says",
    "issue",
    "clock",
    "offset",
    "basically",
    "relates",
    "ntp",
    "service",
    "network",
    "time",
    "protocol",
    "makes",
    "sure",
    "one",
    "multiple",
    "machines",
    "time",
    "zone",
    "sync",
    "click",
    "suppress",
    "say",
    "suppress",
    "hosts",
    "say",
    "look",
    "later",
    "confirm",
    "health",
    "issue",
    "reported",
    "probably",
    "ntp",
    "service",
    "machines",
    "might",
    "sync",
    "impact",
    "use",
    "case",
    "kerberos",
    "kind",
    "setup",
    "security",
    "basically",
    "offset",
    "time",
    "zone",
    "becomes",
    "important",
    "ignore",
    "message",
    "still",
    "good",
    "use",
    "cluster",
    "also",
    "configuration",
    "issues",
    "click",
    "might",
    "talk",
    "heap",
    "size",
    "ram",
    "available",
    "machines",
    "talks",
    "zookeeper",
    "odd",
    "numbers",
    "queue",
    "load",
    "balancer",
    "sdfs",
    "one",
    "data",
    "node",
    "issues",
    "worried",
    "upon",
    "single",
    "node",
    "cluster",
    "setup",
    "want",
    "avoid",
    "warnings",
    "always",
    "click",
    "suppress",
    "avoid",
    "let",
    "cluster",
    "green",
    "status",
    "nothing",
    "worry",
    "click",
    "cluster",
    "basically",
    "look",
    "services",
    "removed",
    "services",
    "intend",
    "use",
    "also",
    "suppressed",
    "offset",
    "warning",
    "critical",
    "use",
    "case",
    "basically",
    "good",
    "start",
    "cluster",
    "point",
    "time",
    "said",
    "would",
    "want",
    "add",
    "services",
    "actions",
    "button",
    "use",
    "add",
    "service",
    "say",
    "restart",
    "cluster",
    "restart",
    "services",
    "one",
    "one",
    "starting",
    "zookeeper",
    "first",
    "service",
    "come",
    "always",
    "click",
    "arrow",
    "mark",
    "see",
    "happening",
    "services",
    "services",
    "coming",
    "order",
    "issues",
    "always",
    "click",
    "link",
    "next",
    "take",
    "logs",
    "click",
    "close",
    "let",
    "happen",
    "background",
    "basically",
    "let",
    "services",
    "restart",
    "one",
    "one",
    "cluster",
    "become",
    "completely",
    "accessible",
    "either",
    "using",
    "hue",
    "web",
    "interface",
    "quick",
    "start",
    "terminal",
    "allows",
    "give",
    "commands",
    "machines",
    "coming",
    "click",
    "hosts",
    "look",
    "hosts",
    "one",
    "also",
    "tell",
    "many",
    "roles",
    "processes",
    "running",
    "machine",
    "25",
    "rolls",
    "tells",
    "disk",
    "usage",
    "tells",
    "physical",
    "memory",
    "used",
    "using",
    "host",
    "tab",
    "add",
    "new",
    "host",
    "cluster",
    "check",
    "configuration",
    "check",
    "hosts",
    "diagnostics",
    "look",
    "logs",
    "give",
    "access",
    "logs",
    "even",
    "select",
    "sources",
    "would",
    "want",
    "logs",
    "give",
    "host",
    "name",
    "click",
    "search",
    "build",
    "charts",
    "also",
    "admin",
    "stuff",
    "adding",
    "different",
    "users",
    "enabling",
    "security",
    "using",
    "administration",
    "tab",
    "since",
    "clicked",
    "restart",
    "cluster",
    "slowly",
    "start",
    "seeing",
    "services",
    "one",
    "one",
    "coming",
    "starting",
    "zookeeper",
    "begin",
    "cluster",
    "running",
    "whether",
    "showing",
    "services",
    "green",
    "different",
    "status",
    "still",
    "able",
    "access",
    "service",
    "saw",
    "apache",
    "hadoop",
    "cluster",
    "even",
    "click",
    "sdfs",
    "access",
    "web",
    "ui",
    "sdfs",
    "service",
    "clicking",
    "quick",
    "links",
    "service",
    "yet",
    "able",
    "see",
    "web",
    "ui",
    "link",
    "allow",
    "check",
    "things",
    "sdfs",
    "web",
    "interface",
    "similarly",
    "yarn",
    "service",
    "also",
    "web",
    "interface",
    "soon",
    "service",
    "comes",
    "quick",
    "links",
    "access",
    "yarn",
    "ui",
    "similarly",
    "service",
    "comes",
    "access",
    "hue",
    "give",
    "web",
    "interface",
    "allows",
    "work",
    "sdfs",
    "allows",
    "work",
    "different",
    "components",
    "within",
    "cluster",
    "without",
    "even",
    "using",
    "command",
    "line",
    "tools",
    "command",
    "line",
    "options",
    "give",
    "time",
    "cloud",
    "error",
    "scm",
    "agent",
    "every",
    "machine",
    "able",
    "restart",
    "roles",
    "responsible",
    "cluster",
    "come",
    "always",
    "click",
    "tells",
    "running",
    "commands",
    "background",
    "trying",
    "start",
    "cluster",
    "go",
    "terminal",
    "switch",
    "hdfs",
    "user",
    "remember",
    "sdfs",
    "user",
    "admin",
    "user",
    "password",
    "unless",
    "set",
    "one",
    "log",
    "sdfs",
    "might",
    "ask",
    "password",
    "initially",
    "best",
    "way",
    "logging",
    "root",
    "password",
    "cloud",
    "error",
    "log",
    "hdfs",
    "onwards",
    "give",
    "sdfs",
    "commands",
    "work",
    "file",
    "system",
    "since",
    "services",
    "coming",
    "right",
    "try",
    "give",
    "sdfs",
    "dfs",
    "command",
    "might",
    "work",
    "might",
    "also",
    "say",
    "trying",
    "connect",
    "name",
    "node",
    "yet",
    "give",
    "time",
    "name",
    "node",
    "able",
    "access",
    "sdfs",
    "using",
    "commands",
    "quickly",
    "set",
    "quick",
    "start",
    "working",
    "using",
    "command",
    "line",
    "options",
    "terminal",
    "like",
    "would",
    "apache",
    "hadoop",
    "cluster",
    "could",
    "use",
    "web",
    "interfaces",
    "allow",
    "work",
    "cluster",
    "usually",
    "takes",
    "time",
    "give",
    "time",
    "services",
    "running",
    "reason",
    "issues",
    "might",
    "require",
    "restart",
    "cluster",
    "several",
    "times",
    "beginning",
    "gets",
    "accustomed",
    "settings",
    "given",
    "starts",
    "services",
    "point",
    "time",
    "error",
    "message",
    "always",
    "go",
    "back",
    "look",
    "logs",
    "see",
    "happening",
    "try",
    "starting",
    "cluster",
    "set",
    "quick",
    "start",
    "vm",
    "using",
    "work",
    "cloudera",
    "cluster",
    "welcome",
    "tutorial",
    "learn",
    "today",
    "setting",
    "apache",
    "hadoop",
    "cluster",
    "although",
    "various",
    "distributions",
    "hadoop",
    "already",
    "discussed",
    "could",
    "set",
    "apache",
    "hadoop",
    "cluster",
    "core",
    "distribution",
    "could",
    "setting",
    "cloud",
    "error",
    "distribution",
    "hadoop",
    "even",
    "hortonworks",
    "however",
    "cloudera",
    "would",
    "need",
    "machines",
    "really",
    "powerful",
    "least",
    "machines",
    "16",
    "gb",
    "ram",
    "set",
    "multiple",
    "virtual",
    "machines",
    "virtual",
    "machine",
    "least",
    "four",
    "five",
    "gb",
    "thing",
    "applies",
    "hortonworks",
    "could",
    "always",
    "download",
    "set",
    "cloudera",
    "quick",
    "start",
    "vm",
    "hortonworks",
    "sandbox",
    "session",
    "learn",
    "set",
    "apache",
    "hadoop",
    "cluster",
    "core",
    "distribution",
    "one",
    "learned",
    "set",
    "apache",
    "hadoop",
    "cluster",
    "would",
    "understand",
    "internals",
    "setting",
    "cluster",
    "different",
    "configuration",
    "properties",
    "needed",
    "cluster",
    "setup",
    "say",
    "vendors",
    "take",
    "apache",
    "hadoop",
    "package",
    "within",
    "cluster",
    "management",
    "solution",
    "users",
    "intend",
    "use",
    "apache",
    "hadoop",
    "would",
    "difficulties",
    "setting",
    "cluster",
    "setting",
    "framework",
    "could",
    "use",
    "distribution",
    "cluster",
    "installation",
    "solutions",
    "clustered",
    "management",
    "solution",
    "easily",
    "plan",
    "deploy",
    "install",
    "manage",
    "cluster",
    "look",
    "link",
    "talks",
    "different",
    "distributions",
    "hadoop",
    "coming",
    "back",
    "hadoop",
    "hadoop",
    "open",
    "source",
    "framework",
    "storing",
    "huge",
    "amount",
    "data",
    "processing",
    "also",
    "depends",
    "commodity",
    "machines",
    "need",
    "carrier",
    "class",
    "hardware",
    "say",
    "commodity",
    "machines",
    "still",
    "talking",
    "machines",
    "higher",
    "basic",
    "laptop",
    "basic",
    "server",
    "level",
    "carrier",
    "class",
    "hardware",
    "costing",
    "huge",
    "organization",
    "hadoop",
    "framework",
    "two",
    "main",
    "components",
    "would",
    "say",
    "two",
    "processing",
    "layers",
    "storage",
    "layer",
    "called",
    "hdfs",
    "hadoop",
    "distributed",
    "file",
    "system",
    "processing",
    "layer",
    "called",
    "yarn",
    "handle",
    "different",
    "processing",
    "frameworks",
    "hadoop",
    "map",
    "reduce",
    "oldest",
    "mature",
    "evolved",
    "processing",
    "framework",
    "sdfs",
    "derives",
    "existence",
    "gfs",
    "solves",
    "issue",
    "storing",
    "huge",
    "amount",
    "data",
    "distributed",
    "fashion",
    "mapreduce",
    "programming",
    "model",
    "allows",
    "process",
    "data",
    "distributed",
    "across",
    "cluster",
    "learn",
    "sdfs",
    "mapreduce",
    "slides",
    "sdfs",
    "storage",
    "layer",
    "compare",
    "laptop",
    "compare",
    "desktop",
    "also",
    "processing",
    "layer",
    "storage",
    "layer",
    "disks",
    "storage",
    "layer",
    "ram",
    "cpu",
    "core",
    "processing",
    "layer",
    "similarly",
    "hadoop",
    "solution",
    "storage",
    "layer",
    "distributed",
    "file",
    "system",
    "store",
    "huge",
    "amount",
    "data",
    "multiple",
    "machines",
    "instead",
    "single",
    "machine",
    "single",
    "server",
    "basically",
    "makes",
    "highly",
    "reliable",
    "highly",
    "fault",
    "tolerant",
    "distributed",
    "parallel",
    "processing",
    "consider",
    "data",
    "file",
    "size",
    "one",
    "gigabyte",
    "getting",
    "stored",
    "one",
    "single",
    "machine",
    "would",
    "still",
    "data",
    "getting",
    "stored",
    "one",
    "multiple",
    "disks",
    "would",
    "try",
    "read",
    "data",
    "seek",
    "time",
    "time",
    "taken",
    "read",
    "data",
    "would",
    "high",
    "comparison",
    "reading",
    "multiple",
    "machines",
    "sdfs",
    "like",
    "gfs",
    "divides",
    "file",
    "smaller",
    "chunks",
    "stores",
    "data",
    "across",
    "hadoop",
    "cluster",
    "previous",
    "versions",
    "hadoop",
    "default",
    "block",
    "size",
    "64",
    "mb",
    "however",
    "hadoop",
    "version",
    "2",
    "onwards",
    "block",
    "size",
    "128",
    "mb",
    "customizable",
    "depending",
    "average",
    "size",
    "data",
    "coming",
    "cluster",
    "block",
    "size",
    "changed",
    "minimum",
    "block",
    "size",
    "128",
    "mb",
    "basically",
    "means",
    "every",
    "file",
    "less",
    "equal",
    "128",
    "mb",
    "would",
    "use",
    "one",
    "block",
    "file",
    "goes",
    "beyond",
    "size",
    "block",
    "use",
    "multiple",
    "blocks",
    "blocks",
    "would",
    "residing",
    "underlying",
    "slave",
    "machines",
    "files",
    "automatically",
    "split",
    "hadoop",
    "framework",
    "blocks",
    "default",
    "block",
    "size",
    "stored",
    "across",
    "cluster",
    "node",
    "hadoop",
    "cluster",
    "would",
    "bunch",
    "disks",
    "would",
    "storing",
    "blocks",
    "data",
    "stored",
    "distributed",
    "fashion",
    "also",
    "gives",
    "us",
    "flexibility",
    "going",
    "distributed",
    "computing",
    "say",
    "distributed",
    "computing",
    "talking",
    "performing",
    "distributed",
    "parallel",
    "processing",
    "large",
    "volumes",
    "data",
    "take",
    "example",
    "talking",
    "one",
    "single",
    "machine",
    "one",
    "huge",
    "server",
    "four",
    "channels",
    "input",
    "output",
    "channel",
    "take",
    "care",
    "100",
    "megabytes",
    "per",
    "second",
    "100",
    "megabytes",
    "data",
    "per",
    "second",
    "data",
    "file",
    "one",
    "terabyte",
    "would",
    "take",
    "approximately",
    "43",
    "minutes",
    "one",
    "machine",
    "process",
    "one",
    "terabyte",
    "data",
    "compare",
    "distributed",
    "computing",
    "talking",
    "hundreds",
    "machines",
    "machine",
    "similar",
    "number",
    "input",
    "output",
    "channels",
    "channel",
    "capacity",
    "handle",
    "100",
    "megabytes",
    "data",
    "per",
    "second",
    "data",
    "file",
    "one",
    "terabyte",
    "processed",
    "parallel",
    "processing",
    "fashion",
    "using",
    "distributed",
    "computing",
    "would",
    "take",
    "26",
    "seconds",
    "100",
    "machines",
    "process",
    "one",
    "terabyte",
    "data",
    "environment",
    "might",
    "thinking",
    "yeah",
    "right",
    "processing",
    "logic",
    "might",
    "complicated",
    "yes",
    "would",
    "mapreduce",
    "programming",
    "model",
    "processing",
    "framework",
    "help",
    "us",
    "look",
    "one",
    "distributions",
    "hadoop",
    "hortonworks",
    "also",
    "different",
    "resources",
    "provide",
    "security",
    "big",
    "data",
    "start",
    "talking",
    "different",
    "services",
    "like",
    "apache",
    "nox",
    "apache",
    "ranger",
    "basically",
    "allow",
    "monitor",
    "authorize",
    "audit",
    "hadoop",
    "clusters",
    "let",
    "also",
    "look",
    "history",
    "hadoop",
    "started",
    "1999",
    "apache",
    "software",
    "foundation",
    "2002",
    "nuts",
    "created",
    "doug",
    "cutting",
    "mike",
    "caprilla",
    "dug",
    "cutting",
    "joined",
    "hadoop",
    "took",
    "nuts",
    "product",
    "2008",
    "nuts",
    "divided",
    "hadoop",
    "born",
    "2008",
    "yahoo",
    "released",
    "hadoop",
    "open",
    "source",
    "project",
    "asf",
    "2008",
    "also",
    "hadoop",
    "based",
    "startup",
    "cloud",
    "era",
    "incorporated",
    "cutting",
    "left",
    "yahoo",
    "cloudera",
    "yahoo",
    "coming",
    "hortonworks",
    "commercial",
    "hadoop",
    "distribution",
    "map",
    "r",
    "technology",
    "also",
    "released",
    "hadoop",
    "distribution",
    "green",
    "plum",
    "released",
    "hadoop",
    "distribution",
    "pivotal",
    "hd",
    "brief",
    "history",
    "hadoop",
    "always",
    "read",
    "history",
    "evolution",
    "hadoop",
    "started",
    "today",
    "10",
    "distributions",
    "existing",
    "market",
    "however",
    "cloudera",
    "dominant",
    "one",
    "per",
    "recent",
    "news",
    "items",
    "cloudera",
    "hortonworks",
    "merged",
    "one",
    "entity",
    "bring",
    "better",
    "solutions",
    "market",
    "want",
    "learn",
    "hadoop",
    "ecosystem",
    "far",
    "learned",
    "big",
    "data",
    "big",
    "data",
    "learned",
    "different",
    "use",
    "cases",
    "big",
    "data",
    "used",
    "different",
    "organizations",
    "use",
    "big",
    "data",
    "took",
    "example",
    "facebook",
    "also",
    "looked",
    "google",
    "file",
    "system",
    "laid",
    "foundation",
    "sdfs",
    "hadoop",
    "distributed",
    "file",
    "system",
    "yet",
    "spoken",
    "processing",
    "layer",
    "yarn",
    "processing",
    "framework",
    "mapreduce",
    "discuss",
    "future",
    "slides",
    "slides",
    "let",
    "spend",
    "time",
    "understanding",
    "hadoop",
    "ecosystem",
    "might",
    "heard",
    "ecosphere",
    "various",
    "products",
    "say",
    "tools",
    "say",
    "packages",
    "work",
    "together",
    "comprise",
    "hadoop",
    "cluster",
    "picture",
    "shows",
    "different",
    "components",
    "used",
    "different",
    "requirements",
    "work",
    "big",
    "data",
    "starting",
    "bottom",
    "top",
    "would",
    "look",
    "sdfs",
    "hadoop",
    "distributed",
    "file",
    "system",
    "basically",
    "file",
    "system",
    "would",
    "use",
    "multiple",
    "machines",
    "store",
    "data",
    "processing",
    "layer",
    "yarn",
    "programming",
    "model",
    "mapreduce",
    "could",
    "written",
    "programming",
    "language",
    "primarily",
    "java",
    "could",
    "also",
    "written",
    "python",
    "scala",
    "programming",
    "language",
    "main",
    "components",
    "sdfs",
    "yarn",
    "processing",
    "layers",
    "processing",
    "layer",
    "would",
    "multiple",
    "processes",
    "would",
    "running",
    "different",
    "machines",
    "within",
    "cluster",
    "would",
    "hadoop",
    "cluster",
    "data",
    "would",
    "data",
    "come",
    "might",
    "lot",
    "sources",
    "data",
    "getting",
    "generated",
    "data",
    "might",
    "already",
    "stored",
    "rdbms",
    "interested",
    "bringing",
    "structured",
    "data",
    "rdbms",
    "layer",
    "hadoop",
    "ecosystem",
    "would",
    "use",
    "data",
    "injection",
    "tools",
    "like",
    "screw",
    "scope",
    "would",
    "used",
    "data",
    "ingestion",
    "transformation",
    "pushing",
    "sdfs",
    "could",
    "also",
    "used",
    "exporting",
    "data",
    "sdfs",
    "back",
    "rdbms",
    "scope",
    "mainly",
    "structured",
    "data",
    "data",
    "injection",
    "tools",
    "like",
    "flume",
    "used",
    "ingestion",
    "streaming",
    "unstructured",
    "kind",
    "data",
    "data",
    "could",
    "directly",
    "ingested",
    "sdfs",
    "could",
    "going",
    "data",
    "warehousing",
    "package",
    "called",
    "hive",
    "hive",
    "data",
    "warehousing",
    "package",
    "top",
    "sdfs",
    "users",
    "could",
    "write",
    "sql",
    "queries",
    "process",
    "data",
    "could",
    "creating",
    "tables",
    "databases",
    "store",
    "data",
    "data",
    "would",
    "eventually",
    "getting",
    "stored",
    "sdfs",
    "underlying",
    "systems",
    "hive",
    "could",
    "accessed",
    "various",
    "services",
    "edge",
    "catalog",
    "one",
    "data",
    "ingested",
    "could",
    "also",
    "stored",
    "unstructured",
    "data",
    "store",
    "apache",
    "hbase",
    "lots",
    "nosql",
    "databases",
    "lot",
    "data",
    "ingestion",
    "tools",
    "go",
    "google",
    "search",
    "data",
    "ingestion",
    "tools",
    "say",
    "data",
    "ingestion",
    "tools",
    "could",
    "look",
    "link",
    "shows",
    "around",
    "18",
    "different",
    "data",
    "ingestion",
    "tools",
    "existing",
    "market",
    "tools",
    "import",
    "export",
    "transforming",
    "governance",
    "even",
    "processing",
    "data",
    "data",
    "ingested",
    "apache",
    "kafka",
    "published",
    "subscribing",
    "messaging",
    "system",
    "apache",
    "nifi",
    "streaming",
    "structured",
    "data",
    "also",
    "helps",
    "governance",
    "wavefront",
    "data",
    "torrent",
    "amazon",
    "kinesis",
    "apache",
    "storm",
    "samsa",
    "sync",
    "sort",
    "goblin",
    "flume",
    "scoop",
    "samsara",
    "fluenty",
    "mufflins",
    "white",
    "elephant",
    "chokwa",
    "heka",
    "scribe",
    "databus",
    "many",
    "data",
    "ingestion",
    "tools",
    "used",
    "could",
    "also",
    "look",
    "nosql",
    "databases",
    "type",
    "nosql",
    "databases",
    "take",
    "would",
    "show",
    "225",
    "nosql",
    "databases",
    "existing",
    "market",
    "nosql",
    "stands",
    "sql",
    "databases",
    "created",
    "without",
    "defining",
    "schema",
    "could",
    "key",
    "value",
    "stores",
    "could",
    "document",
    "based",
    "databases",
    "allow",
    "lot",
    "dynamism",
    "different",
    "kind",
    "constraints",
    "allow",
    "store",
    "structured",
    "unstructured",
    "data",
    "look",
    "exhaustive",
    "list",
    "nosql",
    "databases",
    "coming",
    "back",
    "hadoop",
    "ecosystem",
    "data",
    "could",
    "ingested",
    "using",
    "one",
    "data",
    "ingestion",
    "tools",
    "sdfs",
    "hive",
    "table",
    "hive",
    "would",
    "using",
    "sdfs",
    "underlying",
    "layer",
    "data",
    "could",
    "also",
    "imported",
    "nosql",
    "databases",
    "hbase",
    "four",
    "dimensional",
    "database",
    "key",
    "value",
    "store",
    "allows",
    "faster",
    "random",
    "access",
    "across",
    "billions",
    "records",
    "learn",
    "edge",
    "base",
    "hive",
    "sessions",
    "talk",
    "hadoop",
    "echo",
    "system",
    "could",
    "workflow",
    "manager",
    "uzi",
    "workflow",
    "managers",
    "like",
    "azkaban",
    "luji",
    "right",
    "used",
    "uzi",
    "comes",
    "hadoop",
    "apache",
    "hadoop",
    "said",
    "apache",
    "hadoop",
    "open",
    "source",
    "set",
    "apache",
    "hadoop",
    "cluster",
    "set",
    "hive",
    "edge",
    "base",
    "components",
    "would",
    "manually",
    "download",
    "packages",
    "edit",
    "configs",
    "set",
    "processes",
    "start",
    "cluster",
    "services",
    "however",
    "using",
    "distribution",
    "many",
    "packages",
    "would",
    "come",
    "packaged",
    "solution",
    "oozy",
    "workflow",
    "scheduler",
    "allows",
    "create",
    "workflow",
    "different",
    "jobs",
    "could",
    "run",
    "hadoop",
    "cluster",
    "could",
    "service",
    "zookeeper",
    "mainly",
    "management",
    "monitoring",
    "centralized",
    "coordination",
    "service",
    "especially",
    "plays",
    "important",
    "role",
    "managing",
    "high",
    "availability",
    "cluster",
    "hadoop",
    "hbase",
    "pick",
    "scripting",
    "allows",
    "processing",
    "data",
    "concise",
    "way",
    "hive",
    "querying",
    "allows",
    "users",
    "write",
    "sql",
    "queries",
    "process",
    "data",
    "could",
    "also",
    "interactive",
    "analysis",
    "using",
    "packages",
    "like",
    "apache",
    "drill",
    "apache",
    "mahout",
    "machine",
    "learning",
    "kafka",
    "apache",
    "storm",
    "even",
    "spark",
    "used",
    "streaming",
    "spark",
    "different",
    "components",
    "computing",
    "framework",
    "allows",
    "work",
    "streaming",
    "data",
    "structure",
    "data",
    "graph",
    "based",
    "data",
    "even",
    "machine",
    "learning",
    "algorithms",
    "hadoop",
    "solution",
    "allows",
    "organizations",
    "work",
    "big",
    "data",
    "hadoop",
    "ecosystem",
    "components",
    "involved",
    "part",
    "hadoop",
    "ecosystem",
    "growing",
    "lots",
    "lots",
    "components",
    "part",
    "adobe",
    "echo",
    "system",
    "overall",
    "used",
    "store",
    "process",
    "capture",
    "analyze",
    "big",
    "data",
    "come",
    "point",
    "start",
    "learning",
    "hadoop",
    "distributed",
    "file",
    "system",
    "allows",
    "store",
    "data",
    "distributed",
    "fashion",
    "sdfs",
    "basically",
    "makes",
    "easier",
    "organizations",
    "store",
    "huge",
    "amount",
    "data",
    "previous",
    "session",
    "learning",
    "big",
    "data",
    "characteristics",
    "organizations",
    "interested",
    "talk",
    "big",
    "data",
    "different",
    "use",
    "cases",
    "today",
    "learn",
    "hadoop",
    "distributed",
    "file",
    "system",
    "basically",
    "logical",
    "storage",
    "layer",
    "hadoop",
    "cluster",
    "sdfs",
    "fault",
    "tolerant",
    "reliable",
    "scalable",
    "let",
    "understand",
    "learn",
    "working",
    "sdfs",
    "logical",
    "storage",
    "layer",
    "hadoop",
    "cluster",
    "sdfs",
    "distributed",
    "file",
    "system",
    "designed",
    "store",
    "large",
    "volumes",
    "data",
    "commodity",
    "machines",
    "designed",
    "low",
    "cost",
    "hardware",
    "mean",
    "say",
    "commodity",
    "machines",
    "could",
    "cluster",
    "tens",
    "hundreds",
    "thousands",
    "nodes",
    "still",
    "scale",
    "accommodate",
    "huge",
    "volume",
    "data",
    "provides",
    "access",
    "data",
    "across",
    "multiple",
    "hadoop",
    "clusters",
    "high",
    "fault",
    "tolerance",
    "throughput",
    "sdfs",
    "stores",
    "data",
    "multiple",
    "servers",
    "instead",
    "central",
    "server",
    "earlier",
    "discussing",
    "storing",
    "data",
    "single",
    "machine",
    "lead",
    "read",
    "processing",
    "times",
    "basically",
    "lead",
    "high",
    "seek",
    "time",
    "taken",
    "care",
    "distributed",
    "file",
    "system",
    "sdfs",
    "would",
    "rely",
    "file",
    "systems",
    "multiple",
    "machines",
    "form",
    "hadoop",
    "cluster",
    "let",
    "understand",
    "sdfs",
    "works",
    "example",
    "large",
    "file",
    "stored",
    "sdfs",
    "file",
    "would",
    "broken",
    "blocks",
    "block",
    "size",
    "default",
    "128",
    "mb",
    "blocks",
    "would",
    "stored",
    "across",
    "different",
    "slave",
    "machines",
    "different",
    "nodes",
    "responsible",
    "storing",
    "data",
    "sdfs",
    "default",
    "block",
    "size",
    "128",
    "mb",
    "increased",
    "per",
    "requirement",
    "could",
    "megabytes",
    "gigabytes",
    "terabytes",
    "even",
    "bigger",
    "default",
    "block",
    "size",
    "128",
    "mb",
    "hadoop",
    "version",
    "2",
    "onwards",
    "previous",
    "versions",
    "hadoop",
    "block",
    "size",
    "64",
    "mb",
    "point",
    "remembered",
    "block",
    "logical",
    "entity",
    "basically",
    "takes",
    "data",
    "stores",
    "underlying",
    "machines",
    "disks",
    "would",
    "look",
    "normal",
    "hadoop",
    "cluster",
    "would",
    "one",
    "single",
    "master",
    "unless",
    "high",
    "availability",
    "cluster",
    "could",
    "multiple",
    "masters",
    "two",
    "masters",
    "one",
    "active",
    "one",
    "standby",
    "normal",
    "hadoop",
    "cluster",
    "would",
    "one",
    "master",
    "multiple",
    "slave",
    "nodes",
    "data",
    "blocks",
    "getting",
    "stored",
    "understand",
    "mapreduce",
    "algorithm",
    "let",
    "also",
    "understand",
    "working",
    "sdfs",
    "data",
    "gets",
    "stored",
    "overall",
    "work",
    "let",
    "open",
    "notepad",
    "file",
    "would",
    "explain",
    "sdfs",
    "let",
    "imagine",
    "would",
    "like",
    "say",
    "five",
    "machines",
    "could",
    "forming",
    "cluster",
    "would",
    "machine",
    "one",
    "machine",
    "two",
    "machine",
    "three",
    "machine",
    "four",
    "machine",
    "five",
    "say",
    "hadoop",
    "cluster",
    "running",
    "machines",
    "could",
    "apache",
    "hadoop",
    "core",
    "distribution",
    "vendor",
    "specific",
    "distribution",
    "cloudera",
    "hortonworks",
    "map",
    "r",
    "ibm",
    "big",
    "insight",
    "etc",
    "distributions",
    "hadoop",
    "cluster",
    "discuss",
    "later",
    "processes",
    "would",
    "running",
    "machines",
    "case",
    "apache",
    "hadoop",
    "cluster",
    "case",
    "apache",
    "hadoop",
    "cluster",
    "hadoop",
    "cluster",
    "running",
    "would",
    "download",
    "hadoop",
    "related",
    "packages",
    "would",
    "untar",
    "would",
    "edit",
    "config",
    "files",
    "specify",
    "processes",
    "would",
    "run",
    "nodes",
    "basically",
    "would",
    "formatting",
    "hdfs",
    "onwards",
    "go",
    "ahead",
    "start",
    "cluster",
    "looks",
    "like",
    "apache",
    "adobe",
    "cluster",
    "would",
    "manually",
    "various",
    "things",
    "downloading",
    "hadoop",
    "related",
    "tar",
    "file",
    "untiling",
    "editing",
    "config",
    "files",
    "formatting",
    "sdfs",
    "going",
    "ahead",
    "starting",
    "cluster",
    "case",
    "distribution",
    "cloudera",
    "hortonworks",
    "let",
    "split",
    "would",
    "basically",
    "cluster",
    "management",
    "solution",
    "would",
    "help",
    "various",
    "things",
    "set",
    "cluster",
    "case",
    "cloud",
    "error",
    "distribution",
    "hadoop",
    "would",
    "cloud",
    "installer",
    "file",
    "would",
    "cloudera",
    "manager",
    "help",
    "deploying",
    "cloudera",
    "cluster",
    "downloading",
    "hadoop",
    "related",
    "parcels",
    "packages",
    "untiling",
    "editing",
    "config",
    "files",
    "formatting",
    "sdfs",
    "basically",
    "starting",
    "cluster",
    "would",
    "taken",
    "care",
    "automatically",
    "cloudera",
    "manager",
    "case",
    "hortonworks",
    "data",
    "platform",
    "would",
    "ambari",
    "software",
    "would",
    "used",
    "deploying",
    "cluster",
    "matter",
    "distribution",
    "hadoop",
    "would",
    "running",
    "would",
    "still",
    "roles",
    "processes",
    "would",
    "running",
    "let",
    "briefly",
    "discuss",
    "terminologies",
    "let",
    "bring",
    "file",
    "could",
    "explain",
    "details",
    "let",
    "look",
    "documented",
    "would",
    "look",
    "different",
    "distributions",
    "hadoop",
    "hadoop",
    "version",
    "one",
    "two",
    "let",
    "look",
    "files",
    "one",
    "would",
    "look",
    "version",
    "one",
    "apache",
    "hadoop",
    "would",
    "call",
    "demons",
    "processes",
    "look",
    "names",
    "name",
    "node",
    "data",
    "nodes",
    "job",
    "tracker",
    "secondary",
    "name",
    "node",
    "task",
    "records",
    "case",
    "version",
    "2",
    "would",
    "still",
    "call",
    "demons",
    "could",
    "one",
    "multiple",
    "name",
    "nodes",
    "federation",
    "high",
    "availability",
    "kind",
    "setup",
    "would",
    "data",
    "nodes",
    "instead",
    "job",
    "tracker",
    "would",
    "resource",
    "manager",
    "temporary",
    "diamond",
    "called",
    "mr",
    "app",
    "master",
    "high",
    "availability",
    "secondary",
    "name",
    "node",
    "would",
    "allowed",
    "federation",
    "kind",
    "setup",
    "secondary",
    "name",
    "node",
    "would",
    "still",
    "existing",
    "instead",
    "task",
    "trackers",
    "would",
    "node",
    "managers",
    "case",
    "distribution",
    "cloudera",
    "whether",
    "talking",
    "older",
    "version",
    "newer",
    "version",
    "five",
    "point",
    "something",
    "would",
    "call",
    "roles",
    "apache",
    "hadoop",
    "call",
    "demons",
    "cloudera",
    "call",
    "roles",
    "hortonworks",
    "call",
    "components",
    "talking",
    "thing",
    "case",
    "cloudera",
    "would",
    "services",
    "would",
    "part",
    "cluster",
    "impala",
    "oozy",
    "spark",
    "hive",
    "hbase",
    "storm",
    "hue",
    "sdfs",
    "mapreduce",
    "yarn",
    "similarly",
    "hortonworks",
    "also",
    "would",
    "services",
    "would",
    "running",
    "case",
    "apache",
    "hadoop",
    "would",
    "called",
    "services",
    "seen",
    "functionality",
    "still",
    "exists",
    "depending",
    "demons",
    "basically",
    "look",
    "apache",
    "hadoop",
    "cloudera",
    "hortonworks",
    "everywhere",
    "demons",
    "would",
    "running",
    "one",
    "multiple",
    "nodes",
    "could",
    "different",
    "kind",
    "setups",
    "briefly",
    "discuss",
    "could",
    "name",
    "node",
    "running",
    "resource",
    "manager",
    "running",
    "could",
    "secondary",
    "name",
    "node",
    "go",
    "name",
    "second",
    "name",
    "node",
    "discuss",
    "later",
    "secondary",
    "name",
    "node",
    "could",
    "data",
    "nodes",
    "running",
    "machines",
    "one",
    "kind",
    "setup",
    "talking",
    "data",
    "node",
    "node",
    "manager",
    "data",
    "node",
    "node",
    "manager",
    "similarly",
    "would",
    "one",
    "machine",
    "one",
    "kind",
    "setup",
    "setup",
    "possible",
    "could",
    "different",
    "setup",
    "machines",
    "could",
    "decide",
    "data",
    "node",
    "node",
    "manager",
    "running",
    "every",
    "node",
    "however",
    "would",
    "preferred",
    "would",
    "want",
    "slaves",
    "masters",
    "run",
    "machine",
    "however",
    "setup",
    "also",
    "possible",
    "could",
    "high",
    "availability",
    "kind",
    "setup",
    "basically",
    "good",
    "cluster",
    "organization",
    "could",
    "name",
    "node",
    "1",
    "would",
    "running",
    "say",
    "name",
    "node",
    "2",
    "running",
    "could",
    "resource",
    "manager",
    "2",
    "running",
    "could",
    "resource",
    "manager",
    "one",
    "running",
    "could",
    "still",
    "data",
    "node",
    "node",
    "manager",
    "running",
    "fashion",
    "would",
    "allowed",
    "high",
    "availability",
    "cluster",
    "standby",
    "name",
    "node",
    "one",
    "setup",
    "possible",
    "could",
    "single",
    "node",
    "setup",
    "could",
    "everything",
    "machine",
    "named",
    "node",
    "resource",
    "manager",
    "secondary",
    "name",
    "node",
    "data",
    "node",
    "node",
    "manager",
    "different",
    "setups",
    "somebody",
    "asks",
    "preferable",
    "setup",
    "would",
    "still",
    "go",
    "either",
    "one",
    "multiple",
    "data",
    "nodes",
    "node",
    "managers",
    "one",
    "high",
    "availability",
    "setup",
    "single",
    "node",
    "sudo",
    "distributed",
    "cluster",
    "basically",
    "good",
    "testing",
    "development",
    "kind",
    "activities",
    "distributed",
    "clusters",
    "closely",
    "see",
    "basically",
    "roles",
    "components",
    "demons",
    "running",
    "machines",
    "take",
    "care",
    "mainly",
    "two",
    "functionalities",
    "two",
    "services",
    "hdfs",
    "storage",
    "layer",
    "yarn",
    "processing",
    "layer",
    "hadoop",
    "cluster",
    "storage",
    "layer",
    "processing",
    "layer",
    "storage",
    "layer",
    "sdfs",
    "basically",
    "one",
    "single",
    "master",
    "multiple",
    "slave",
    "processes",
    "running",
    "could",
    "say",
    "hdfs",
    "work",
    "fine",
    "name",
    "node",
    "running",
    "multiple",
    "data",
    "nodes",
    "running",
    "secondary",
    "name",
    "note",
    "running",
    "yarn",
    "service",
    "processing",
    "layer",
    "would",
    "need",
    "minimum",
    "one",
    "resource",
    "manager",
    "multiple",
    "node",
    "managers",
    "running",
    "one",
    "multiple",
    "machines",
    "two",
    "processing",
    "two",
    "layers",
    "one",
    "storage",
    "layer",
    "one",
    "processing",
    "layer",
    "storage",
    "layer",
    "processes",
    "name",
    "node",
    "master",
    "data",
    "nodes",
    "secondary",
    "name",
    "node",
    "considered",
    "slaves",
    "resource",
    "manager",
    "master",
    "processing",
    "layer",
    "node",
    "managers",
    "slaves",
    "briefly",
    "setup",
    "could",
    "particular",
    "environment",
    "say",
    "hadoop",
    "related",
    "packages",
    "parcels",
    "downloaded",
    "whether",
    "distribution",
    "apache",
    "hadoop",
    "cluster",
    "say",
    "untarring",
    "done",
    "editing",
    "config",
    "files",
    "done",
    "formatting",
    "done",
    "cluster",
    "started",
    "case",
    "cluster",
    "would",
    "running",
    "could",
    "basically",
    "cluster",
    "would",
    "sdfs",
    "running",
    "ready",
    "allow",
    "store",
    "data",
    "let",
    "imagine",
    "sdfs",
    "running",
    "sdfs",
    "service",
    "running",
    "let",
    "imagine",
    "nodes",
    "m1",
    "m2",
    "m3",
    "m4",
    "m5",
    "config",
    "files",
    "edited",
    "nodes",
    "would",
    "defined",
    "processes",
    "run",
    "would",
    "also",
    "given",
    "path",
    "processes",
    "store",
    "data",
    "let",
    "say",
    "m1",
    "m2",
    "m3",
    "m4",
    "m5",
    "m3",
    "m4",
    "m5",
    "data",
    "nodes",
    "running",
    "would",
    "defined",
    "path",
    "let",
    "call",
    "say",
    "abc",
    "slash",
    "data",
    "abc",
    "slash",
    "data",
    "abc",
    "path",
    "defined",
    "every",
    "node",
    "store",
    "hadoop",
    "related",
    "data",
    "m1",
    "would",
    "name",
    "node",
    "running",
    "would",
    "still",
    "defined",
    "path",
    "let",
    "call",
    "abc",
    "slash",
    "n",
    "name",
    "node",
    "could",
    "store",
    "data",
    "cluster",
    "running",
    "sdfs",
    "available",
    "data",
    "stored",
    "sdfs",
    "let",
    "understand",
    "work",
    "client",
    "application",
    "would",
    "want",
    "write",
    "data",
    "cluster",
    "needs",
    "interact",
    "master",
    "begin",
    "say",
    "let",
    "say",
    "client",
    "machine",
    "api",
    "application",
    "would",
    "interested",
    "writing",
    "data",
    "would",
    "name",
    "node",
    "know",
    "available",
    "data",
    "nodes",
    "soon",
    "cluster",
    "comes",
    "first",
    "thing",
    "happens",
    "data",
    "nodes",
    "would",
    "sending",
    "heartbeat",
    "name",
    "node",
    "every",
    "three",
    "seconds",
    "data",
    "nodes",
    "send",
    "heartbeat",
    "name",
    "node",
    "every",
    "three",
    "seconds",
    "let",
    "name",
    "node",
    "know",
    "let",
    "master",
    "know",
    "available",
    "name",
    "node",
    "name",
    "node",
    "building",
    "metadata",
    "ram",
    "already",
    "metadata",
    "disk",
    "created",
    "result",
    "formatting",
    "sdfs",
    "would",
    "metadata",
    "ram",
    "getting",
    "built",
    "initially",
    "name",
    "node",
    "would",
    "information",
    "data",
    "nodes",
    "available",
    "cluster",
    "client",
    "application",
    "wants",
    "write",
    "data",
    "cluster",
    "gets",
    "touch",
    "name",
    "node",
    "inquiring",
    "slave",
    "machines",
    "enquiring",
    "data",
    "nodes",
    "data",
    "actually",
    "written",
    "name",
    "node",
    "refers",
    "metadata",
    "collected",
    "far",
    "responds",
    "back",
    "mentioning",
    "data",
    "nodes",
    "slave",
    "machines",
    "available",
    "data",
    "storage",
    "based",
    "response",
    "client",
    "takes",
    "file",
    "sdfs",
    "intending",
    "write",
    "cluster",
    "hadoop",
    "basically",
    "two",
    "things",
    "one",
    "default",
    "block",
    "size",
    "discussed",
    "earlier",
    "128",
    "mb",
    "128",
    "mb",
    "default",
    "block",
    "size",
    "customizable",
    "changed",
    "depending",
    "average",
    "data",
    "size",
    "default",
    "replication",
    "three",
    "imagine",
    "application",
    "api",
    "client",
    "would",
    "want",
    "write",
    "file",
    "256",
    "mb",
    "bigger",
    "block",
    "size",
    "file",
    "broken",
    "call",
    "blocks",
    "logical",
    "entities",
    "file",
    "taken",
    "framework",
    "framework",
    "split",
    "file",
    "blocks",
    "let",
    "imagine",
    "file",
    "256",
    "mb",
    "broken",
    "two",
    "blocks",
    "b1",
    "b2",
    "blocks",
    "randomly",
    "distributed",
    "slave",
    "machines",
    "data",
    "nodes",
    "would",
    "block",
    "one",
    "say",
    "getting",
    "stored",
    "could",
    "also",
    "block",
    "one",
    "getting",
    "stored",
    "data",
    "nodes",
    "would",
    "block",
    "one",
    "could",
    "block",
    "two",
    "could",
    "written",
    "would",
    "also",
    "replicas",
    "sitting",
    "machine",
    "rule",
    "replication",
    "never",
    "two",
    "identical",
    "blocks",
    "sitting",
    "node",
    "block",
    "one",
    "never",
    "replica",
    "block",
    "one",
    "machine",
    "replica",
    "block",
    "2",
    "block",
    "1",
    "replica",
    "block",
    "2",
    "differentiation",
    "one",
    "original",
    "one",
    "replica",
    "cluster",
    "blocks",
    "also",
    "happens",
    "blocks",
    "written",
    "data",
    "nodes",
    "repeatedly",
    "sending",
    "block",
    "report",
    "name",
    "node",
    "every",
    "10",
    "seconds",
    "information",
    "updated",
    "name",
    "nodes",
    "metadata",
    "name",
    "node",
    "information",
    "files",
    "files",
    "broken",
    "blocks",
    "blocks",
    "residing",
    "data",
    "nodes",
    "metadata",
    "getting",
    "built",
    "name",
    "node",
    "data",
    "nodes",
    "respond",
    "heartbeats",
    "also",
    "every",
    "10",
    "seconds",
    "block",
    "report",
    "need",
    "remember",
    "one",
    "point",
    "name",
    "node",
    "basically",
    "master",
    "data",
    "nodes",
    "slave",
    "hadoop",
    "framework",
    "coded",
    "java",
    "basically",
    "means",
    "every",
    "directory",
    "every",
    "file",
    "file",
    "related",
    "block",
    "considered",
    "object",
    "getting",
    "stored",
    "every",
    "object",
    "name",
    "node",
    "tracking",
    "150",
    "bytes",
    "name",
    "nodes",
    "ram",
    "utilized",
    "also",
    "tells",
    "us",
    "block",
    "sizes",
    "smaller",
    "128",
    "mb",
    "file",
    "would",
    "number",
    "blocks",
    "would",
    "mean",
    "name",
    "nodes",
    "ram",
    "would",
    "utilized",
    "overload",
    "name",
    "node",
    "block",
    "size",
    "chosen",
    "lot",
    "consideration",
    "masters",
    "ram",
    "average",
    "size",
    "data",
    "data",
    "distributed",
    "blocks",
    "distributed",
    "across",
    "data",
    "nodes",
    "sdfs",
    "fault",
    "tolerant",
    "data",
    "nodes",
    "fail",
    "still",
    "data",
    "nodes",
    "replica",
    "data",
    "making",
    "possible",
    "read",
    "data",
    "access",
    "data",
    "even",
    "multiple",
    "data",
    "nodes",
    "go",
    "internals",
    "data",
    "nodes",
    "placed",
    "racks",
    "rack",
    "awareness",
    "block",
    "placement",
    "learn",
    "later",
    "sdfs",
    "works",
    "many",
    "things",
    "read",
    "write",
    "process",
    "taken",
    "care",
    "methods",
    "open",
    "create",
    "internally",
    "playing",
    "role",
    "read",
    "write",
    "operation",
    "happens",
    "also",
    "facts",
    "like",
    "replication",
    "sequential",
    "process",
    "right",
    "activity",
    "parallel",
    "learn",
    "functionality",
    "sdfs",
    "later",
    "sessions",
    "brief",
    "sdfs",
    "works",
    "file",
    "split",
    "blocks",
    "blocks",
    "getting",
    "stored",
    "disks",
    "note",
    "block",
    "contains",
    "data",
    "even",
    "file",
    "one",
    "kilobyte",
    "file",
    "stored",
    "sdfs",
    "still",
    "use",
    "block",
    "128",
    "mb",
    "mean",
    "disk",
    "space",
    "getting",
    "wasted",
    "block",
    "resides",
    "data",
    "node",
    "disk",
    "occupy",
    "one",
    "kilobyte",
    "space",
    "data",
    "means",
    "block",
    "logical",
    "entity",
    "take",
    "data",
    "disk",
    "get",
    "stored",
    "disk",
    "sdfs",
    "works",
    "basically",
    "takes",
    "care",
    "reliable",
    "data",
    "storage",
    "across",
    "machines",
    "distributed",
    "fashion",
    "set",
    "two",
    "node",
    "cluster",
    "two",
    "data",
    "nodes",
    "two",
    "node",
    "managers",
    "one",
    "master",
    "machine",
    "let",
    "try",
    "working",
    "sdfs",
    "let",
    "look",
    "quick",
    "commands",
    "allow",
    "write",
    "data",
    "cluster",
    "although",
    "different",
    "ways",
    "first",
    "way",
    "using",
    "commands",
    "second",
    "using",
    "ide",
    "could",
    "write",
    "code",
    "use",
    "hadoop",
    "file",
    "system",
    "api",
    "third",
    "graphical",
    "user",
    "interface",
    "called",
    "hue",
    "normally",
    "come",
    "apache",
    "although",
    "work",
    "apache",
    "comes",
    "service",
    "cloud",
    "error",
    "distributions",
    "work",
    "hdfs",
    "need",
    "sample",
    "data",
    "various",
    "websites",
    "get",
    "sample",
    "data",
    "open",
    "browser",
    "let",
    "show",
    "sample",
    "places",
    "sample",
    "data",
    "set",
    "links",
    "download",
    "bigger",
    "smaller",
    "files",
    "used",
    "write",
    "data",
    "sdfs",
    "although",
    "could",
    "pick",
    "local",
    "file",
    "could",
    "create",
    "files",
    "write",
    "sdfs",
    "various",
    "data",
    "set",
    "sites",
    "used",
    "example",
    "type",
    "machine",
    "learning",
    "uci",
    "data",
    "sets",
    "one",
    "link",
    "200",
    "data",
    "sets",
    "used",
    "even",
    "data",
    "science",
    "kind",
    "activities",
    "used",
    "testing",
    "algorithms",
    "download",
    "data",
    "local",
    "machine",
    "local",
    "machine",
    "writing",
    "data",
    "hdfs",
    "based",
    "size",
    "data",
    "splits",
    "data",
    "blocks",
    "blocks",
    "would",
    "get",
    "stored",
    "underlying",
    "machines",
    "wherever",
    "data",
    "nodes",
    "run",
    "click",
    "uci",
    "machine",
    "learning",
    "repository",
    "even",
    "data",
    "sets",
    "specific",
    "need",
    "sample",
    "data",
    "sets",
    "get",
    "kegel",
    "data",
    "sets",
    "get",
    "data",
    "sets",
    "yelp",
    "website",
    "bigger",
    "data",
    "sets",
    "zipped",
    "using",
    "write",
    "data",
    "see",
    "files",
    "split",
    "multiple",
    "blocks",
    "lot",
    "data",
    "click",
    "one",
    "link",
    "begin",
    "data",
    "folder",
    "shows",
    "sum",
    "file",
    "right",
    "click",
    "save",
    "link",
    "save",
    "file",
    "simple",
    "text",
    "file",
    "save",
    "get",
    "various",
    "data",
    "sets",
    "even",
    "go",
    "yelp",
    "data",
    "set",
    "challenge",
    "bigger",
    "files",
    "zipped",
    "downloaded",
    "click",
    "download",
    "data",
    "set",
    "although",
    "start",
    "downloading",
    "given",
    "name",
    "email",
    "id",
    "initials",
    "click",
    "download",
    "one",
    "website",
    "download",
    "json",
    "data",
    "sets",
    "gigabytes",
    "compressed",
    "format",
    "photos",
    "gigabytes",
    "also",
    "say",
    "gutenberg",
    "download",
    "books",
    "website",
    "download",
    "free",
    "data",
    "sets",
    "huge",
    "try",
    "get",
    "variety",
    "data",
    "sets",
    "different",
    "formats",
    "write",
    "hdfs",
    "even",
    "use",
    "sdfs",
    "log",
    "files",
    "already",
    "exist",
    "machines",
    "need",
    "download",
    "anything",
    "internet",
    "let",
    "close",
    "already",
    "file",
    "let",
    "copy",
    "home",
    "htc",
    "downloads",
    "file",
    "let",
    "copy",
    "home",
    "htc",
    "file",
    "check",
    "permissions",
    "owned",
    "sdc",
    "even",
    "look",
    "user",
    "local",
    "hadoop",
    "logs",
    "already",
    "log",
    "files",
    "even",
    "copy",
    "files",
    "using",
    "copy",
    "user",
    "local",
    "hadoop",
    "logs",
    "let",
    "take",
    "hadoop",
    "data",
    "node",
    "related",
    "log",
    "let",
    "copy",
    "home",
    "sdc",
    "testing",
    "files",
    "work",
    "sdfs",
    "said",
    "always",
    "type",
    "hdfs",
    "shows",
    "various",
    "options",
    "already",
    "used",
    "first",
    "option",
    "name",
    "node",
    "iphone",
    "format",
    "also",
    "used",
    "hd",
    "dfs",
    "admin",
    "look",
    "many",
    "nodes",
    "work",
    "hdfs",
    "file",
    "system",
    "option",
    "dfs",
    "would",
    "say",
    "hdfs",
    "dfs",
    "always",
    "hit",
    "enter",
    "shows",
    "various",
    "options",
    "work",
    "distributed",
    "file",
    "system",
    "like",
    "creating",
    "directory",
    "copying",
    "files",
    "getting",
    "data",
    "sdfs",
    "local",
    "machine",
    "changing",
    "permissions",
    "changing",
    "replication",
    "many",
    "options",
    "let",
    "click",
    "hdfs",
    "dfs",
    "let",
    "create",
    "directly",
    "let",
    "call",
    "data",
    "directory",
    "creating",
    "hdfs",
    "remember",
    "directory",
    "seen",
    "local",
    "machine",
    "hdfs",
    "directory",
    "look",
    "root",
    "see",
    "data",
    "sdfs",
    "directory",
    "technically",
    "getting",
    "created",
    "sdfs",
    "start",
    "writing",
    "data",
    "sdfs",
    "relevant",
    "blocks",
    "start",
    "getting",
    "stored",
    "abc",
    "slash",
    "data",
    "always",
    "keep",
    "browser",
    "open",
    "see",
    "work",
    "sdfs",
    "data",
    "propagate",
    "come",
    "distributed",
    "file",
    "system",
    "using",
    "web",
    "interface",
    "let",
    "say",
    "http",
    "slash",
    "m1",
    "five",
    "zero",
    "zero",
    "seven",
    "zero",
    "browser",
    "interface",
    "let",
    "click",
    "utilities",
    "let",
    "click",
    "browse",
    "file",
    "system",
    "show",
    "directory",
    "created",
    "although",
    "data",
    "let",
    "go",
    "know",
    "files",
    "say",
    "hdfs",
    "dfs",
    "two",
    "options",
    "either",
    "say",
    "put",
    "say",
    "copy",
    "local",
    "thing",
    "find",
    "put",
    "copy",
    "local",
    "copying",
    "data",
    "local",
    "machine",
    "hdfs",
    "say",
    "copy",
    "local",
    "say",
    "take",
    "aba",
    "directory",
    "file",
    "put",
    "data",
    "technically",
    "copying",
    "file",
    "local",
    "file",
    "system",
    "hdfs",
    "using",
    "local",
    "file",
    "systems",
    "path",
    "done",
    "let",
    "quickly",
    "check",
    "clicking",
    "data",
    "file",
    "written",
    "see",
    "file",
    "187",
    "kilobytes",
    "however",
    "still",
    "use",
    "block",
    "128",
    "previous",
    "sessions",
    "already",
    "explained",
    "block",
    "128",
    "goes",
    "disk",
    "occupy",
    "space",
    "data",
    "block",
    "quickly",
    "go",
    "hdfs",
    "dfs",
    "listing",
    "directory",
    "file",
    "quickly",
    "check",
    "file",
    "replicated",
    "either",
    "using",
    "web",
    "interface",
    "says",
    "replication",
    "true",
    "means",
    "file",
    "related",
    "blocks",
    "getting",
    "stored",
    "underlying",
    "data",
    "nodes",
    "always",
    "click",
    "overview",
    "would",
    "want",
    "browse",
    "scroll",
    "says",
    "three",
    "files",
    "directory",
    "file",
    "one",
    "block",
    "totally",
    "four",
    "file",
    "system",
    "objects",
    "scroll",
    "show",
    "see",
    "two",
    "live",
    "nodes",
    "see",
    "node",
    "one",
    "block",
    "always",
    "look",
    "data",
    "path",
    "looking",
    "abc",
    "slash",
    "data",
    "slash",
    "current",
    "block",
    "pool",
    "directory",
    "current",
    "finalized",
    "subdirectory",
    "subdirectory",
    "block",
    "look",
    "size",
    "block",
    "191",
    "relates",
    "kilobyte",
    "size",
    "talking",
    "similarly",
    "check",
    "m2",
    "machine",
    "also",
    "data",
    "node",
    "running",
    "make",
    "sure",
    "file",
    "related",
    "block",
    "replicated",
    "look",
    "block",
    "pool",
    "current",
    "finalized",
    "subdirectory",
    "sub",
    "directory",
    "shows",
    "block",
    "replicated",
    "meta",
    "file",
    "size",
    "ensures",
    "data",
    "getting",
    "replicated",
    "per",
    "whatever",
    "factor",
    "set",
    "file",
    "written",
    "htfs",
    "also",
    "complete",
    "listing",
    "say",
    "recursive",
    "listing",
    "show",
    "directories",
    "files",
    "hdfs",
    "think",
    "forgetting",
    "something",
    "logged",
    "root",
    "right",
    "need",
    "logged",
    "htc",
    "try",
    "commands",
    "hpfs",
    "dfs",
    "iphone",
    "ls",
    "slash",
    "show",
    "directory",
    "would",
    "want",
    "recursive",
    "listing",
    "showing",
    "directories",
    "files",
    "add",
    "hyphen",
    "r",
    "written",
    "file",
    "hdfs",
    "data",
    "directory",
    "similarly",
    "write",
    "using",
    "put",
    "command",
    "last",
    "time",
    "used",
    "copy",
    "local",
    "use",
    "put",
    "command",
    "say",
    "let",
    "take",
    "log",
    "file",
    "let",
    "us",
    "write",
    "data",
    "directory",
    "instead",
    "copy",
    "local",
    "using",
    "put",
    "command",
    "also",
    "allows",
    "write",
    "file",
    "htfs",
    "done",
    "let",
    "go",
    "web",
    "ui",
    "let",
    "go",
    "browse",
    "file",
    "system",
    "let",
    "look",
    "directory",
    "shows",
    "log",
    "file",
    "even",
    "smaller",
    "first",
    "file",
    "wrote",
    "kilobytes",
    "however",
    "replicated",
    "using",
    "still",
    "block",
    "size",
    "always",
    "go",
    "back",
    "look",
    "terminal",
    "see",
    "new",
    "file",
    "new",
    "block",
    "data",
    "path",
    "see",
    "block",
    "also",
    "application",
    "machine",
    "write",
    "one",
    "file",
    "could",
    "write",
    "multiple",
    "files",
    "could",
    "write",
    "multiple",
    "files",
    "let",
    "create",
    "files",
    "log",
    "files",
    "let",
    "say",
    "hadoop",
    "log",
    "let",
    "give",
    "name",
    "like",
    "see",
    "created",
    "multiple",
    "files",
    "also",
    "say",
    "sdfs",
    "dfs",
    "make",
    "directory",
    "let",
    "say",
    "data",
    "tool",
    "creating",
    "new",
    "directory",
    "sdfs",
    "even",
    "copy",
    "multiple",
    "files",
    "complete",
    "directory",
    "need",
    "give",
    "matching",
    "pattern",
    "directory",
    "say",
    "sdfs",
    "dfs",
    "put",
    "say",
    "take",
    "directories",
    "files",
    "put",
    "data",
    "remember",
    "one",
    "file",
    "multiple",
    "files",
    "following",
    "replication",
    "set",
    "cluster",
    "level",
    "recursive",
    "listing",
    "would",
    "want",
    "see",
    "directories",
    "files",
    "within",
    "hdfs",
    "see",
    "data",
    "directory",
    "also",
    "mydata2",
    "directory",
    "multiple",
    "files",
    "files",
    "within",
    "directory",
    "following",
    "replication",
    "set",
    "cluster",
    "level",
    "blocks",
    "replicated",
    "twice",
    "always",
    "go",
    "back",
    "back",
    "shows",
    "data",
    "everything",
    "replicated",
    "twice",
    "data",
    "also",
    "replicated",
    "twice",
    "sdfs",
    "also",
    "functionality",
    "write",
    "data",
    "different",
    "replication",
    "let",
    "create",
    "one",
    "directory",
    "let",
    "write",
    "data",
    "different",
    "replication",
    "way",
    "write",
    "files",
    "follow",
    "thing",
    "however",
    "say",
    "hyphen",
    "dfs",
    "dot",
    "replication",
    "specify",
    "one",
    "passing",
    "argument",
    "specifying",
    "would",
    "want",
    "write",
    "data",
    "replication",
    "one",
    "blocks",
    "considered",
    "replicated",
    "choosing",
    "write",
    "files",
    "directory",
    "application",
    "one",
    "check",
    "browser",
    "interface",
    "quick",
    "refresh",
    "click",
    "data",
    "three",
    "shows",
    "follows",
    "replication",
    "one",
    "also",
    "command",
    "check",
    "replication",
    "status",
    "using",
    "hdfs",
    "say",
    "stfs",
    "fsck",
    "slash",
    "files",
    "slash",
    "blocks",
    "would",
    "want",
    "see",
    "blocks",
    "files",
    "could",
    "choose",
    "one",
    "given",
    "complete",
    "sdfs",
    "file",
    "system",
    "replace",
    "slash",
    "data",
    "say",
    "data",
    "three",
    "would",
    "want",
    "see",
    "show",
    "within",
    "directory",
    "many",
    "files",
    "many",
    "blocks",
    "file",
    "use",
    "replicated",
    "correctly",
    "fsck",
    "command",
    "gives",
    "information",
    "data",
    "3",
    "one",
    "file",
    "24",
    "kilobytes",
    "uses",
    "one",
    "block",
    "smaller",
    "128",
    "replicated",
    "situation",
    "misreplicated",
    "blocks",
    "files",
    "correctly",
    "replicated",
    "specified",
    "writing",
    "thing",
    "seen",
    "files",
    "say",
    "mydata2",
    "tell",
    "files",
    "within",
    "mydata2",
    "replicated",
    "per",
    "default",
    "replication",
    "cluster",
    "level",
    "see",
    "everything",
    "replicated",
    "twice",
    "misreplicated",
    "blocks",
    "file",
    "system",
    "status",
    "healthy",
    "sdfs",
    "command",
    "make",
    "sure",
    "files",
    "replicated",
    "correctly",
    "seen",
    "files",
    "written",
    "default",
    "replication",
    "seen",
    "files",
    "follow",
    "dynamic",
    "replication",
    "specify",
    "command",
    "line",
    "also",
    "see",
    "command",
    "allows",
    "us",
    "change",
    "replication",
    "data",
    "written",
    "say",
    "set",
    "rep",
    "say",
    "path",
    "data",
    "3",
    "initially",
    "file",
    "related",
    "blocks",
    "replicated",
    "say",
    "mydata3",
    "specify",
    "follow",
    "replication",
    "2",
    "basically",
    "means",
    "look",
    "directory",
    "check",
    "files",
    "one",
    "block",
    "create",
    "replica",
    "blocks",
    "across",
    "data",
    "nodes",
    "always",
    "go",
    "back",
    "look",
    "data",
    "node",
    "logs",
    "see",
    "replication",
    "happening",
    "also",
    "look",
    "browser",
    "interface",
    "initially",
    "showing",
    "application",
    "one",
    "shows",
    "replication",
    "two",
    "command",
    "change",
    "replication",
    "data",
    "written",
    "rights",
    "default",
    "replication",
    "write",
    "files",
    "different",
    "replication",
    "change",
    "replication",
    "data",
    "written",
    "even",
    "get",
    "data",
    "hdfs",
    "local",
    "machine",
    "give",
    "hdfs",
    "get",
    "command",
    "copy",
    "local",
    "say",
    "data",
    "3",
    "bring",
    "local",
    "machine",
    "home",
    "hdc",
    "using",
    "get",
    "command",
    "get",
    "data",
    "sdfs",
    "local",
    "machine",
    "using",
    "hdfs",
    "command",
    "see",
    "mydata3",
    "directory",
    "local",
    "path",
    "look",
    "mydata3",
    "files",
    "hdfs",
    "putting",
    "data",
    "local",
    "machine",
    "sdfs",
    "also",
    "successfully",
    "brought",
    "data",
    "hdfs",
    "local",
    "machine",
    "written",
    "data",
    "different",
    "replication",
    "checked",
    "status",
    "nodes",
    "checked",
    "status",
    "blocks",
    "would",
    "want",
    "delete",
    "data",
    "sdfs",
    "also",
    "options",
    "say",
    "hdfs",
    "dfs",
    "remove",
    "could",
    "also",
    "recursive",
    "remove",
    "remove",
    "everything",
    "including",
    "directory",
    "mydata3",
    "delete",
    "data",
    "hdfs",
    "however",
    "thrash",
    "enabled",
    "thrash",
    "basically",
    "means",
    "recycle",
    "bin",
    "depends",
    "property",
    "core",
    "hyphen",
    "site",
    "file",
    "enabled",
    "thrash",
    "enabled",
    "data",
    "deleted",
    "hdfs",
    "however",
    "permanently",
    "deleted",
    "sit",
    "thrash",
    "time",
    "interval",
    "admin",
    "specified",
    "configuration",
    "file",
    "right",
    "deleted",
    "thrash",
    "seems",
    "gone",
    "sdfs",
    "set",
    "thresh",
    "however",
    "thrash",
    "set",
    "either",
    "decide",
    "data",
    "deleted",
    "still",
    "thrash",
    "could",
    "restored",
    "could",
    "delete",
    "permanently",
    "using",
    "skip",
    "thrash",
    "option",
    "however",
    "remember",
    "skip",
    "thrash",
    "option",
    "makes",
    "sense",
    "hash",
    "enabled",
    "cluster",
    "want",
    "deleted",
    "data",
    "string",
    "thrash",
    "even",
    "whatever",
    "time",
    "interval",
    "set",
    "even",
    "data",
    "sits",
    "thrash",
    "thrash",
    "directory",
    "structure",
    "hdfs",
    "data",
    "still",
    "replicated",
    "seen",
    "write",
    "data",
    "cluster",
    "get",
    "data",
    "cluster",
    "local",
    "machine",
    "write",
    "different",
    "replications",
    "delete",
    "cluster",
    "recursive",
    "listing",
    "directories",
    "files",
    "various",
    "commands",
    "used",
    "example",
    "something",
    "like",
    "copy",
    "move",
    "change",
    "permissions",
    "many",
    "thanks",
    "ajay",
    "richard",
    "ajay",
    "taking",
    "us",
    "hadoop",
    "ecosystem",
    "look",
    "hdfs",
    "mapreduce",
    "yarn",
    "scoop",
    "hive",
    "pig",
    "hbase",
    "suppose",
    "library",
    "collection",
    "huge",
    "number",
    "books",
    "floor",
    "want",
    "count",
    "total",
    "number",
    "books",
    "present",
    "floor",
    "would",
    "approach",
    "could",
    "say",
    "think",
    "take",
    "lot",
    "time",
    "obviously",
    "efficient",
    "way",
    "counting",
    "number",
    "books",
    "huge",
    "collection",
    "every",
    "floor",
    "could",
    "different",
    "approach",
    "alternative",
    "could",
    "think",
    "asking",
    "three",
    "friends",
    "three",
    "colleagues",
    "could",
    "say",
    "friend",
    "could",
    "count",
    "books",
    "every",
    "floor",
    "obviously",
    "would",
    "make",
    "work",
    "faster",
    "easier",
    "count",
    "books",
    "every",
    "floor",
    "mean",
    "parallel",
    "processing",
    "say",
    "parallel",
    "processing",
    "technical",
    "terms",
    "talking",
    "using",
    "multiple",
    "machines",
    "machine",
    "would",
    "contributing",
    "ram",
    "cpu",
    "cores",
    "processing",
    "data",
    "would",
    "processed",
    "multiple",
    "machines",
    "time",
    "type",
    "process",
    "involves",
    "parallel",
    "processing",
    "case",
    "library",
    "example",
    "would",
    "person",
    "one",
    "would",
    "taking",
    "care",
    "books",
    "floor",
    "1",
    "counting",
    "person",
    "2",
    "floor",
    "2",
    "someone",
    "floor",
    "3",
    "someone",
    "floor",
    "every",
    "individual",
    "would",
    "counting",
    "books",
    "every",
    "floor",
    "parallel",
    "reduces",
    "time",
    "consumed",
    "activity",
    "mechanism",
    "counts",
    "every",
    "floor",
    "aggregated",
    "person",
    "person",
    "mapping",
    "data",
    "particular",
    "floor",
    "say",
    "person",
    "kind",
    "activity",
    "basically",
    "task",
    "every",
    "floor",
    "task",
    "counting",
    "books",
    "every",
    "floor",
    "could",
    "aggregation",
    "mechanism",
    "could",
    "basically",
    "reduce",
    "summarize",
    "total",
    "count",
    "terms",
    "map",
    "reduce",
    "would",
    "say",
    "work",
    "reducer",
    "talk",
    "hadoop",
    "map",
    "reduce",
    "processes",
    "data",
    "different",
    "node",
    "machines",
    "whole",
    "concept",
    "hadoop",
    "framework",
    "right",
    "data",
    "stored",
    "across",
    "machines",
    "would",
    "also",
    "want",
    "process",
    "data",
    "locally",
    "instead",
    "transferring",
    "data",
    "one",
    "machine",
    "machine",
    "bringing",
    "data",
    "together",
    "central",
    "processing",
    "unit",
    "processing",
    "would",
    "rather",
    "data",
    "processed",
    "machines",
    "wherever",
    "stored",
    "know",
    "case",
    "hadoop",
    "cluster",
    "would",
    "data",
    "stored",
    "multiple",
    "data",
    "nodes",
    "multiple",
    "disks",
    "data",
    "needs",
    "processed",
    "requirement",
    "want",
    "process",
    "data",
    "fast",
    "possible",
    "could",
    "achieved",
    "using",
    "parallel",
    "processing",
    "case",
    "mapreduce",
    "basically",
    "first",
    "phase",
    "mapping",
    "phase",
    "case",
    "mapreduce",
    "programming",
    "model",
    "basically",
    "two",
    "phases",
    "one",
    "mapping",
    "one",
    "reducing",
    "takes",
    "care",
    "things",
    "mapping",
    "phase",
    "mapper",
    "class",
    "mapper",
    "class",
    "function",
    "provided",
    "developer",
    "takes",
    "care",
    "individual",
    "map",
    "tasks",
    "work",
    "multiple",
    "nodes",
    "parallel",
    "reducer",
    "class",
    "belongs",
    "reducing",
    "phase",
    "reducing",
    "phase",
    "basically",
    "uses",
    "reducer",
    "class",
    "provides",
    "function",
    "aggregate",
    "reduce",
    "output",
    "different",
    "data",
    "nodes",
    "generate",
    "final",
    "output",
    "mapreduce",
    "works",
    "using",
    "mapping",
    "obviously",
    "reducing",
    "could",
    "kind",
    "jobs",
    "map",
    "jobs",
    "wherein",
    "reducing",
    "required",
    "talking",
    "talking",
    "requirement",
    "would",
    "want",
    "process",
    "data",
    "using",
    "mapping",
    "reducing",
    "especially",
    "data",
    "huge",
    "data",
    "stored",
    "across",
    "multiple",
    "machines",
    "would",
    "want",
    "process",
    "data",
    "parallel",
    "talk",
    "mapreduce",
    "could",
    "say",
    "programming",
    "model",
    "could",
    "say",
    "internally",
    "processing",
    "engine",
    "hadoop",
    "allows",
    "process",
    "compute",
    "huge",
    "volumes",
    "data",
    "say",
    "huge",
    "volumes",
    "data",
    "talk",
    "terabytes",
    "talk",
    "petabytes",
    "exabytes",
    "amount",
    "data",
    "needs",
    "processed",
    "huge",
    "cluster",
    "could",
    "also",
    "use",
    "mapreduce",
    "programming",
    "model",
    "run",
    "mapreduce",
    "algorithm",
    "local",
    "mode",
    "mean",
    "would",
    "go",
    "local",
    "mode",
    "basically",
    "means",
    "would",
    "mapping",
    "reducing",
    "node",
    "using",
    "processing",
    "capacity",
    "ram",
    "cpu",
    "cores",
    "machine",
    "really",
    "efficient",
    "fact",
    "would",
    "want",
    "map",
    "reduce",
    "work",
    "multiple",
    "nodes",
    "would",
    "obviously",
    "mapping",
    "phase",
    "followed",
    "reducing",
    "phase",
    "intermittently",
    "would",
    "data",
    "generated",
    "would",
    "different",
    "phases",
    "help",
    "whole",
    "processing",
    "talk",
    "hadoop",
    "map",
    "reduce",
    "mainly",
    "talking",
    "two",
    "main",
    "components",
    "two",
    "main",
    "phases",
    "mapping",
    "reducing",
    "mapping",
    "taking",
    "care",
    "map",
    "tasks",
    "reducing",
    "taking",
    "care",
    "reduced",
    "tasks",
    "would",
    "data",
    "would",
    "stored",
    "multiple",
    "machines",
    "talk",
    "data",
    "data",
    "could",
    "different",
    "formats",
    "could",
    "developer",
    "could",
    "specify",
    "format",
    "needs",
    "used",
    "understand",
    "data",
    "coming",
    "data",
    "goes",
    "mapping",
    "internally",
    "would",
    "shuffling",
    "sorting",
    "reducing",
    "gives",
    "final",
    "output",
    "way",
    "access",
    "data",
    "sdfs",
    "way",
    "data",
    "getting",
    "stored",
    "sdfs",
    "input",
    "data",
    "would",
    "one",
    "multiple",
    "files",
    "one",
    "multiple",
    "directories",
    "final",
    "output",
    "also",
    "stored",
    "sdfs",
    "accessed",
    "looked",
    "see",
    "processing",
    "done",
    "correctly",
    "looks",
    "input",
    "data",
    "would",
    "worked",
    "upon",
    "multiple",
    "map",
    "tasks",
    "many",
    "map",
    "tasks",
    "basically",
    "depends",
    "file",
    "depends",
    "input",
    "format",
    "normally",
    "know",
    "hadoop",
    "cluster",
    "would",
    "file",
    "broken",
    "blocks",
    "depending",
    "size",
    "default",
    "block",
    "size",
    "128",
    "mb",
    "still",
    "customized",
    "based",
    "average",
    "size",
    "data",
    "getting",
    "stored",
    "cluster",
    "really",
    "huge",
    "files",
    "getting",
    "stored",
    "cluster",
    "would",
    "certainly",
    "set",
    "higher",
    "block",
    "size",
    "every",
    "file",
    "huge",
    "number",
    "blocks",
    "creating",
    "load",
    "name",
    "notes",
    "ram",
    "tracking",
    "number",
    "elements",
    "cluster",
    "number",
    "objects",
    "cluster",
    "depending",
    "file",
    "size",
    "file",
    "would",
    "split",
    "multiple",
    "chunks",
    "every",
    "chunk",
    "would",
    "map",
    "task",
    "running",
    "map",
    "task",
    "specified",
    "within",
    "mapper",
    "class",
    "within",
    "mapper",
    "class",
    "mapper",
    "function",
    "basically",
    "says",
    "map",
    "tasks",
    "chunks",
    "processed",
    "data",
    "intermittently",
    "written",
    "sdfs",
    "sorted",
    "shuffled",
    "internal",
    "phases",
    "partitioner",
    "decides",
    "many",
    "reduced",
    "tasks",
    "would",
    "used",
    "data",
    "goes",
    "reducer",
    "could",
    "also",
    "combiner",
    "phase",
    "like",
    "mini",
    "reducer",
    "reduce",
    "operation",
    "reaches",
    "reduce",
    "reducing",
    "phase",
    "taken",
    "care",
    "reducer",
    "class",
    "internally",
    "reducer",
    "function",
    "provided",
    "developer",
    "would",
    "reduced",
    "task",
    "running",
    "data",
    "comes",
    "output",
    "map",
    "tasks",
    "finally",
    "output",
    "generated",
    "stored",
    "sdfs",
    "case",
    "hadoop",
    "accepts",
    "data",
    "different",
    "formats",
    "data",
    "could",
    "compressed",
    "format",
    "data",
    "could",
    "part",
    "k",
    "data",
    "could",
    "afro",
    "text",
    "csv",
    "psv",
    "binary",
    "format",
    "formats",
    "supported",
    "however",
    "remember",
    "talking",
    "data",
    "compressed",
    "also",
    "look",
    "kind",
    "splitability",
    "compression",
    "mechanism",
    "supports",
    "otherwise",
    "mapreduce",
    "processing",
    "happens",
    "would",
    "take",
    "complete",
    "file",
    "one",
    "chunk",
    "processed",
    "sdfs",
    "accepts",
    "input",
    "data",
    "different",
    "formats",
    "data",
    "stored",
    "sdfs",
    "basically",
    "input",
    "passing",
    "mapping",
    "phase",
    "mapping",
    "phase",
    "said",
    "reads",
    "record",
    "record",
    "depending",
    "input",
    "format",
    "reads",
    "data",
    "multiple",
    "map",
    "tasks",
    "running",
    "multiple",
    "chunks",
    "data",
    "read",
    "broken",
    "individual",
    "elements",
    "say",
    "individual",
    "element",
    "could",
    "say",
    "list",
    "key",
    "value",
    "pairs",
    "records",
    "based",
    "kind",
    "delimiter",
    "without",
    "delimiter",
    "broken",
    "individual",
    "elements",
    "thus",
    "mac",
    "creates",
    "key",
    "value",
    "pairs",
    "key",
    "value",
    "pairs",
    "final",
    "output",
    "key",
    "value",
    "pairs",
    "basically",
    "list",
    "elements",
    "subjected",
    "processing",
    "would",
    "internally",
    "shuffling",
    "sorting",
    "data",
    "relevant",
    "key",
    "value",
    "pairs",
    "brought",
    "together",
    "basically",
    "benefits",
    "processing",
    "reducing",
    "aggregates",
    "key",
    "value",
    "pairs",
    "set",
    "smaller",
    "tuples",
    "tuples",
    "would",
    "say",
    "finally",
    "output",
    "getting",
    "stored",
    "designated",
    "directory",
    "list",
    "aggregated",
    "key",
    "value",
    "pairs",
    "gives",
    "output",
    "talk",
    "mapreduce",
    "one",
    "key",
    "factors",
    "parallel",
    "processing",
    "offer",
    "know",
    "data",
    "getting",
    "stored",
    "across",
    "multiple",
    "data",
    "nodes",
    "would",
    "huge",
    "volume",
    "data",
    "split",
    "randomly",
    "distributed",
    "across",
    "data",
    "nodes",
    "data",
    "needs",
    "processed",
    "best",
    "way",
    "would",
    "parallel",
    "processing",
    "could",
    "data",
    "getting",
    "stored",
    "multiple",
    "data",
    "nodes",
    "multiple",
    "slave",
    "nodes",
    "slave",
    "node",
    "would",
    "one",
    "multiple",
    "disks",
    "process",
    "data",
    "basically",
    "go",
    "parallel",
    "processing",
    "approach",
    "use",
    "mapreduce",
    "let",
    "look",
    "mapreduce",
    "workflow",
    "understand",
    "works",
    "basically",
    "input",
    "data",
    "stored",
    "sdfs",
    "data",
    "needs",
    "processed",
    "stored",
    "input",
    "files",
    "processing",
    "want",
    "done",
    "one",
    "single",
    "file",
    "done",
    "directory",
    "multiple",
    "files",
    "could",
    "also",
    "later",
    "multiple",
    "outputs",
    "merged",
    "achieve",
    "using",
    "something",
    "called",
    "chaining",
    "mappers",
    "data",
    "getting",
    "stored",
    "sdfs",
    "input",
    "format",
    "basically",
    "something",
    "define",
    "input",
    "specification",
    "input",
    "files",
    "split",
    "various",
    "input",
    "formats",
    "search",
    "go",
    "google",
    "basically",
    "search",
    "hadoop",
    "map",
    "reduce",
    "yahoo",
    "tutorial",
    "one",
    "good",
    "links",
    "look",
    "link",
    "search",
    "different",
    "input",
    "formats",
    "output",
    "formats",
    "let",
    "search",
    "input",
    "format",
    "talk",
    "input",
    "format",
    "basically",
    "something",
    "define",
    "input",
    "files",
    "split",
    "input",
    "files",
    "split",
    "read",
    "based",
    "input",
    "format",
    "specified",
    "class",
    "provides",
    "following",
    "functionality",
    "selects",
    "files",
    "objects",
    "used",
    "input",
    "defines",
    "input",
    "split",
    "break",
    "file",
    "tasks",
    "provides",
    "factory",
    "record",
    "reader",
    "objects",
    "read",
    "file",
    "different",
    "formats",
    "look",
    "table",
    "see",
    "text",
    "input",
    "format",
    "default",
    "format",
    "reads",
    "lines",
    "text",
    "file",
    "line",
    "considered",
    "record",
    "key",
    "byte",
    "offset",
    "line",
    "value",
    "line",
    "content",
    "says",
    "key",
    "value",
    "input",
    "format",
    "passes",
    "lines",
    "key",
    "value",
    "pairs",
    "everything",
    "first",
    "tab",
    "character",
    "key",
    "remainder",
    "line",
    "could",
    "also",
    "sequence",
    "file",
    "input",
    "format",
    "basically",
    "works",
    "binary",
    "format",
    "input",
    "format",
    "way",
    "also",
    "search",
    "output",
    "format",
    "takes",
    "care",
    "data",
    "handled",
    "processing",
    "done",
    "key",
    "value",
    "pairs",
    "provided",
    "output",
    "collector",
    "written",
    "output",
    "files",
    "way",
    "written",
    "governed",
    "output",
    "format",
    "functions",
    "pretty",
    "much",
    "like",
    "input",
    "format",
    "described",
    "earlier",
    "right",
    "could",
    "set",
    "output",
    "format",
    "followed",
    "text",
    "output",
    "sequence",
    "file",
    "output",
    "format",
    "null",
    "output",
    "format",
    "different",
    "classes",
    "take",
    "care",
    "data",
    "handled",
    "read",
    "processing",
    "data",
    "written",
    "processing",
    "done",
    "based",
    "input",
    "format",
    "file",
    "broken",
    "splits",
    "logically",
    "represents",
    "data",
    "processed",
    "individual",
    "map",
    "tasks",
    "could",
    "say",
    "individual",
    "mapper",
    "functions",
    "could",
    "one",
    "multiple",
    "splits",
    "need",
    "processed",
    "depending",
    "file",
    "size",
    "depending",
    "properties",
    "set",
    "done",
    "input",
    "splits",
    "subjected",
    "mapping",
    "phase",
    "internally",
    "record",
    "reader",
    "communicates",
    "input",
    "split",
    "converts",
    "data",
    "key",
    "value",
    "pairs",
    "suitable",
    "read",
    "mapper",
    "mapper",
    "basically",
    "working",
    "key",
    "value",
    "pairs",
    "map",
    "task",
    "giving",
    "intermittent",
    "output",
    "would",
    "forwarded",
    "processing",
    "done",
    "key",
    "value",
    "pairs",
    "worked",
    "upon",
    "map",
    "map",
    "tasks",
    "part",
    "mapper",
    "function",
    "generating",
    "key",
    "value",
    "pairs",
    "intermediate",
    "outputs",
    "processed",
    "could",
    "said",
    "combiner",
    "face",
    "internally",
    "mini",
    "radio",
    "surface",
    "combiner",
    "class",
    "combiner",
    "basically",
    "uses",
    "class",
    "reducer",
    "class",
    "provided",
    "developer",
    "main",
    "work",
    "reducing",
    "main",
    "work",
    "kind",
    "mini",
    "aggregation",
    "key",
    "value",
    "pairs",
    "generated",
    "map",
    "data",
    "coming",
    "combiner",
    "internally",
    "partitioner",
    "phase",
    "decides",
    "outputs",
    "combiners",
    "sent",
    "reducers",
    "could",
    "also",
    "say",
    "even",
    "combiner",
    "partitioner",
    "would",
    "decide",
    "based",
    "keys",
    "values",
    "based",
    "type",
    "keys",
    "many",
    "reducers",
    "would",
    "required",
    "many",
    "reduced",
    "tasks",
    "would",
    "required",
    "work",
    "output",
    "generated",
    "map",
    "task",
    "partitioner",
    "decided",
    "data",
    "would",
    "sorted",
    "shuffled",
    "fed",
    "reducer",
    "talk",
    "reducer",
    "would",
    "basically",
    "one",
    "multiple",
    "reduced",
    "tasks",
    "depends",
    "partitioner",
    "decided",
    "determined",
    "data",
    "processed",
    "also",
    "depend",
    "configuration",
    "properties",
    "set",
    "decide",
    "many",
    "radio",
    "stars",
    "used",
    "internally",
    "data",
    "obviously",
    "going",
    "sorting",
    "shuffling",
    "reducing",
    "aggregation",
    "becomes",
    "easier",
    "task",
    "done",
    "basically",
    "reducer",
    "code",
    "reducer",
    "provided",
    "developer",
    "intermediate",
    "data",
    "aggregated",
    "give",
    "final",
    "output",
    "would",
    "stored",
    "sdfs",
    "internal",
    "record",
    "writer",
    "writes",
    "output",
    "key",
    "value",
    "pairs",
    "reducer",
    "output",
    "files",
    "mapreduce",
    "works",
    "wherein",
    "final",
    "output",
    "data",
    "stored",
    "read",
    "accessed",
    "sdfs",
    "even",
    "used",
    "input",
    "mapreduce",
    "kind",
    "processing",
    "overall",
    "looks",
    "basically",
    "data",
    "stored",
    "sdfs",
    "based",
    "input",
    "format",
    "splits",
    "record",
    "reader",
    "gives",
    "data",
    "mapping",
    "phase",
    "taken",
    "care",
    "mapper",
    "function",
    "mapper",
    "function",
    "basically",
    "means",
    "one",
    "multiple",
    "map",
    "tasks",
    "working",
    "chunks",
    "data",
    "could",
    "combiner",
    "phase",
    "optional",
    "mandatory",
    "partitioner",
    "phase",
    "decides",
    "many",
    "reduced",
    "tasks",
    "many",
    "reducers",
    "would",
    "used",
    "work",
    "data",
    "internally",
    "sorting",
    "shuffling",
    "data",
    "happening",
    "basically",
    "based",
    "output",
    "format",
    "record",
    "reader",
    "write",
    "output",
    "sdfs",
    "directory",
    "internally",
    "could",
    "also",
    "remember",
    "data",
    "processed",
    "locally",
    "would",
    "output",
    "task",
    "worked",
    "upon",
    "stored",
    "locally",
    "however",
    "access",
    "data",
    "directly",
    "data",
    "nodes",
    "access",
    "sdfs",
    "output",
    "stored",
    "sdfs",
    "mapreduce",
    "workflow",
    "talk",
    "mapreduce",
    "architecture",
    "would",
    "look",
    "would",
    "basically",
    "edge",
    "node",
    "client",
    "program",
    "api",
    "intends",
    "process",
    "data",
    "submits",
    "job",
    "job",
    "tracker",
    "say",
    "resource",
    "manager",
    "case",
    "hadoop",
    "yarn",
    "framework",
    "right",
    "step",
    "also",
    "say",
    "interaction",
    "name",
    "node",
    "would",
    "already",
    "happened",
    "would",
    "given",
    "information",
    "data",
    "nodes",
    "relevant",
    "data",
    "stored",
    "master",
    "processor",
    "hadoop",
    "version",
    "1",
    "job",
    "tracker",
    "slaves",
    "called",
    "task",
    "trackers",
    "hadoop",
    "version",
    "2",
    "instead",
    "job",
    "tracker",
    "resource",
    "manager",
    "answer",
    "task",
    "trackers",
    "node",
    "managers",
    "basically",
    "resource",
    "manager",
    "assign",
    "job",
    "task",
    "trackers",
    "node",
    "managers",
    "node",
    "managers",
    "discussed",
    "yarn",
    "basically",
    "taking",
    "care",
    "processing",
    "happens",
    "every",
    "node",
    "internally",
    "work",
    "happening",
    "resource",
    "manager",
    "node",
    "managers",
    "application",
    "master",
    "refer",
    "yarn",
    "based",
    "tutorial",
    "understand",
    "processing",
    "master",
    "basically",
    "breaking",
    "application",
    "tasks",
    "internally",
    "application",
    "submitted",
    "application",
    "run",
    "yarn",
    "processing",
    "framework",
    "handled",
    "resource",
    "manager",
    "forget",
    "yarn",
    "part",
    "mean",
    "negotiating",
    "resources",
    "allocates",
    "processing",
    "happen",
    "nodes",
    "right",
    "yarn",
    "handles",
    "processing",
    "request",
    "data",
    "stored",
    "sdfs",
    "broken",
    "one",
    "multiple",
    "splits",
    "depending",
    "input",
    "format",
    "specified",
    "developer",
    "input",
    "splits",
    "worked",
    "upon",
    "one",
    "multiple",
    "map",
    "tasks",
    "running",
    "within",
    "container",
    "nodes",
    "basically",
    "resources",
    "utilized",
    "map",
    "task",
    "would",
    "amount",
    "ram",
    "utilized",
    "data",
    "go",
    "reducing",
    "phase",
    "reduced",
    "task",
    "also",
    "utilizing",
    "ram",
    "cpu",
    "cores",
    "internally",
    "functions",
    "take",
    "care",
    "deciding",
    "number",
    "reducers",
    "mini",
    "reduce",
    "basically",
    "reading",
    "processing",
    "data",
    "multiple",
    "data",
    "nodes",
    "mapreduce",
    "programming",
    "model",
    "makes",
    "parallel",
    "processing",
    "work",
    "processes",
    "data",
    "stored",
    "across",
    "multiple",
    "machines",
    "finally",
    "output",
    "getting",
    "stored",
    "dfs",
    "let",
    "quick",
    "demo",
    "mapreduce",
    "see",
    "works",
    "hadoop",
    "cluster",
    "discussed",
    "briefly",
    "mapreduce",
    "contains",
    "mainly",
    "two",
    "phases",
    "mapping",
    "phase",
    "reducing",
    "phase",
    "mapping",
    "phase",
    "taken",
    "care",
    "mapper",
    "function",
    "reducing",
    "phase",
    "taken",
    "care",
    "reducer",
    "function",
    "also",
    "sorting",
    "shuffling",
    "phases",
    "partitioner",
    "combiner",
    "discuss",
    "detail",
    "later",
    "sessions",
    "let",
    "quick",
    "demo",
    "run",
    "mapreduce",
    "already",
    "existing",
    "package",
    "jar",
    "file",
    "within",
    "apache",
    "hadoop",
    "cluster",
    "even",
    "cloudera",
    "cluster",
    "build",
    "map",
    "reduce",
    "programs",
    "package",
    "jar",
    "transfer",
    "cluster",
    "run",
    "hadoop",
    "cluster",
    "yarn",
    "could",
    "using",
    "already",
    "provided",
    "default",
    "program",
    "let",
    "see",
    "two",
    "machines",
    "brought",
    "basically",
    "would",
    "apache",
    "hadoop",
    "cluster",
    "running",
    "simple",
    "start",
    "hyphen",
    "dot",
    "sh",
    "know",
    "script",
    "deprecated",
    "says",
    "instead",
    "use",
    "start",
    "dfs",
    "start",
    "yarn",
    "still",
    "take",
    "care",
    "starting",
    "cluster",
    "two",
    "nodes",
    "would",
    "one",
    "single",
    "name",
    "node",
    "two",
    "data",
    "nodes",
    "one",
    "secondary",
    "name",
    "node",
    "one",
    "resource",
    "manager",
    "two",
    "node",
    "managers",
    "doubt",
    "cluster",
    "came",
    "always",
    "look",
    "previous",
    "sessions",
    "walkthrough",
    "setting",
    "cluster",
    "apache",
    "could",
    "also",
    "cluster",
    "running",
    "using",
    "less",
    "3gb",
    "total",
    "machine",
    "ram",
    "could",
    "apache",
    "cluster",
    "running",
    "machine",
    "cluster",
    "comes",
    "also",
    "look",
    "web",
    "ui",
    "available",
    "name",
    "node",
    "resource",
    "manager",
    "based",
    "settings",
    "given",
    "uis",
    "show",
    "us",
    "details",
    "cluster",
    "remember",
    "ui",
    "browse",
    "cluster",
    "come",
    "jps",
    "look",
    "java",
    "related",
    "processes",
    "show",
    "processes",
    "running",
    "c1",
    "data",
    "node",
    "resource",
    "manager",
    "node",
    "manager",
    "name",
    "node",
    "m1",
    "machine",
    "second",
    "machine",
    "configured",
    "always",
    "jps",
    "shows",
    "processes",
    "running",
    "also",
    "means",
    "cluster",
    "two",
    "data",
    "nodes",
    "two",
    "node",
    "managers",
    "look",
    "web",
    "ui",
    "refresh",
    "thing",
    "one",
    "refresh",
    "already",
    "opened",
    "web",
    "pages",
    "always",
    "access",
    "web",
    "ui",
    "using",
    "name",
    "notes",
    "host",
    "name",
    "570",
    "port",
    "tells",
    "cluster",
    "id",
    "block",
    "pull",
    "id",
    "gives",
    "information",
    "space",
    "usage",
    "many",
    "live",
    "nodes",
    "even",
    "browse",
    "file",
    "system",
    "put",
    "lot",
    "data",
    "click",
    "browse",
    "file",
    "system",
    "basically",
    "shows",
    "multiple",
    "directories",
    "directories",
    "one",
    "multiple",
    "files",
    "use",
    "mapreduce",
    "example",
    "see",
    "directories",
    "sample",
    "files",
    "although",
    "files",
    "small",
    "like",
    "kilobytes",
    "look",
    "directory",
    "look",
    "pulled",
    "hadoop",
    "logs",
    "put",
    "sdfs",
    "little",
    "bigger",
    "files",
    "also",
    "data",
    "see",
    "data",
    "downloaded",
    "web",
    "either",
    "run",
    "mapreduce",
    "single",
    "file",
    "directory",
    "contains",
    "multiple",
    "files",
    "let",
    "look",
    "looking",
    "demo",
    "mapreduce",
    "also",
    "remember",
    "mapreduce",
    "create",
    "output",
    "directory",
    "need",
    "directory",
    "created",
    "plus",
    "need",
    "permissions",
    "run",
    "mapreduce",
    "job",
    "default",
    "since",
    "running",
    "using",
    "admin",
    "id",
    "problem",
    "intend",
    "run",
    "mapreduce",
    "different",
    "user",
    "obviously",
    "ask",
    "admin",
    "give",
    "user",
    "permission",
    "read",
    "write",
    "sdfs",
    "directory",
    "created",
    "contain",
    "output",
    "mapreduce",
    "job",
    "finishes",
    "cluster",
    "file",
    "system",
    "look",
    "ui",
    "shows",
    "yarn",
    "available",
    "taking",
    "care",
    "processing",
    "shows",
    "total",
    "8",
    "gb",
    "memory",
    "8",
    "v",
    "cores",
    "depending",
    "configuration",
    "set",
    "many",
    "nodes",
    "available",
    "look",
    "nodes",
    "available",
    "shows",
    "two",
    "node",
    "managers",
    "running",
    "8",
    "gb",
    "memory",
    "8",
    "v",
    "cores",
    "true",
    "actually",
    "set",
    "configurations",
    "node",
    "managers",
    "takes",
    "default",
    "properties",
    "8gb",
    "laminate",
    "vcos",
    "yarn",
    "ui",
    "also",
    "look",
    "scheduler",
    "basically",
    "shows",
    "different",
    "queues",
    "configured",
    "run",
    "jobs",
    "discuss",
    "later",
    "detail",
    "let",
    "go",
    "back",
    "terminal",
    "let",
    "see",
    "find",
    "sample",
    "applications",
    "run",
    "cluster",
    "go",
    "terminal",
    "well",
    "submit",
    "mapreduce",
    "job",
    "terminal",
    "know",
    "hadoop",
    "related",
    "directory",
    "within",
    "hadoop",
    "various",
    "directories",
    "discussed",
    "binaries",
    "commands",
    "run",
    "bin",
    "basically",
    "startup",
    "scripts",
    "also",
    "notice",
    "share",
    "directory",
    "end",
    "look",
    "shared",
    "directory",
    "would",
    "find",
    "hadoop",
    "within",
    "hadoop",
    "various",
    "sub",
    "directories",
    "look",
    "mapreduce",
    "mapreduce",
    "directory",
    "sample",
    "jar",
    "files",
    "use",
    "run",
    "mapreduce",
    "cluster",
    "similarly",
    "working",
    "cloudera",
    "cluster",
    "would",
    "go",
    "opt",
    "cloudera",
    "parcel",
    "cdh",
    "slash",
    "lib",
    "would",
    "directories",
    "sdfs",
    "mapreduce",
    "sdfs",
    "yarn",
    "still",
    "find",
    "jars",
    "basically",
    "package",
    "contains",
    "multiple",
    "applications",
    "run",
    "mapreduce",
    "type",
    "hadoop",
    "hit",
    "enter",
    "shows",
    "option",
    "called",
    "jar",
    "used",
    "run",
    "jar",
    "file",
    "point",
    "time",
    "would",
    "want",
    "see",
    "different",
    "classes",
    "available",
    "particular",
    "jar",
    "could",
    "always",
    "jar",
    "minus",
    "xvf",
    "example",
    "could",
    "say",
    "jar",
    "xv",
    "f",
    "could",
    "say",
    "user",
    "local",
    "hadoop",
    "share",
    "hadoop",
    "mapreduce",
    "list",
    "jar",
    "file",
    "say",
    "hadoop",
    "mapreduce",
    "examples",
    "basically",
    "unpack",
    "show",
    "classes",
    "available",
    "within",
    "particular",
    "jar",
    "done",
    "created",
    "meta",
    "file",
    "created",
    "org",
    "directory",
    "see",
    "ls",
    "look",
    "ls",
    "org",
    "since",
    "ran",
    "command",
    "phone",
    "directory",
    "look",
    "org",
    "patchy",
    "hadoop",
    "examples",
    "shows",
    "classes",
    "classes",
    "contain",
    "mapper",
    "reducer",
    "classes",
    "might",
    "mapper",
    "reducer",
    "always",
    "look",
    "example",
    "targeting",
    "use",
    "word",
    "count",
    "program",
    "word",
    "count",
    "files",
    "gives",
    "list",
    "words",
    "many",
    "times",
    "occur",
    "particular",
    "file",
    "set",
    "files",
    "shows",
    "classes",
    "belong",
    "word",
    "count",
    "sum",
    "reducer",
    "reducer",
    "class",
    "tokenizer",
    "mapper",
    "mapper",
    "class",
    "right",
    "basically",
    "used",
    "classes",
    "used",
    "run",
    "word",
    "code",
    "many",
    "programs",
    "part",
    "jar",
    "file",
    "expand",
    "see",
    "say",
    "hadoop",
    "jar",
    "give",
    "path",
    "say",
    "hadoop",
    "jar",
    "user",
    "local",
    "hadoop",
    "share",
    "hadoop",
    "map",
    "reduce",
    "hadoop",
    "mapreduce",
    "examples",
    "hit",
    "enter",
    "show",
    "inbuilt",
    "classes",
    "already",
    "available",
    "certain",
    "things",
    "use",
    "jar",
    "files",
    "also",
    "example",
    "look",
    "hadoop",
    "look",
    "jar",
    "files",
    "particular",
    "path",
    "one",
    "hadoop",
    "mapreduce",
    "examples",
    "use",
    "always",
    "look",
    "jar",
    "files",
    "like",
    "look",
    "hadoop",
    "mapreduce",
    "client",
    "job",
    "client",
    "look",
    "test",
    "one",
    "also",
    "interesting",
    "one",
    "always",
    "look",
    "hadoop",
    "mapreduce",
    "client",
    "job",
    "client",
    "something",
    "ending",
    "would",
    "tried",
    "one",
    "using",
    "hadoop",
    "jar",
    "command",
    "previous",
    "example",
    "showing",
    "classes",
    "available",
    "already",
    "word",
    "count",
    "good",
    "programs",
    "try",
    "like",
    "teragen",
    "generate",
    "dummy",
    "data",
    "terasort",
    "check",
    "sorting",
    "performance",
    "terra",
    "validate",
    "validate",
    "results",
    "similarly",
    "also",
    "hadoop",
    "jar",
    "said",
    "hadoop",
    "mapreduce",
    "think",
    "client",
    "job",
    "client",
    "test",
    "star",
    "lot",
    "classes",
    "used",
    "programs",
    "used",
    "stress",
    "testing",
    "checking",
    "cluster",
    "status",
    "one",
    "interesting",
    "one",
    "test",
    "dfs",
    "io",
    "let",
    "get",
    "details",
    "first",
    "instance",
    "let",
    "see",
    "run",
    "mapreduce",
    "would",
    "want",
    "run",
    "mapreduce",
    "need",
    "give",
    "hadoop",
    "jar",
    "jar",
    "file",
    "hit",
    "enter",
    "would",
    "say",
    "needs",
    "input",
    "output",
    "needs",
    "class",
    "want",
    "run",
    "example",
    "would",
    "say",
    "word",
    "count",
    "hit",
    "enter",
    "tells",
    "need",
    "give",
    "input",
    "output",
    "process",
    "obviously",
    "processing",
    "happening",
    "cluster",
    "yarn",
    "processing",
    "framework",
    "unless",
    "would",
    "want",
    "run",
    "job",
    "local",
    "mode",
    "possibility",
    "run",
    "job",
    "local",
    "mode",
    "let",
    "first",
    "try",
    "runs",
    "cluster",
    "hdfs",
    "ls",
    "slash",
    "command",
    "see",
    "sdfs",
    "ui",
    "already",
    "showing",
    "set",
    "files",
    "directories",
    "use",
    "process",
    "take",
    "one",
    "single",
    "file",
    "example",
    "pick",
    "new",
    "data",
    "look",
    "files",
    "basically",
    "run",
    "mapreduce",
    "single",
    "file",
    "multiple",
    "files",
    "let",
    "take",
    "file",
    "whatever",
    "contains",
    "would",
    "like",
    "word",
    "count",
    "get",
    "list",
    "words",
    "occurrence",
    "file",
    "let",
    "copy",
    "also",
    "need",
    "output",
    "written",
    "written",
    "want",
    "run",
    "mapreduce",
    "say",
    "hadoop",
    "pull",
    "history",
    "hadoop",
    "jar",
    "word",
    "count",
    "need",
    "give",
    "input",
    "new",
    "data",
    "give",
    "file",
    "copied",
    "going",
    "run",
    "word",
    "count",
    "single",
    "file",
    "basically",
    "output",
    "stored",
    "directory",
    "directory",
    "created",
    "already",
    "mr",
    "output",
    "let",
    "output",
    "fair",
    "enough",
    "give",
    "many",
    "properties",
    "specify",
    "many",
    "map",
    "jobs",
    "want",
    "run",
    "many",
    "reduced",
    "jobs",
    "want",
    "run",
    "want",
    "output",
    "compressed",
    "want",
    "output",
    "merged",
    "many",
    "properties",
    "defined",
    "specifying",
    "word",
    "count",
    "pass",
    "argument",
    "pass",
    "properties",
    "command",
    "line",
    "affect",
    "output",
    "go",
    "ahead",
    "submit",
    "basically",
    "running",
    "simple",
    "inbuilt",
    "mapreduce",
    "job",
    "hadoop",
    "cluster",
    "obviously",
    "internally",
    "looking",
    "name",
    "node",
    "issue",
    "says",
    "output",
    "already",
    "exists",
    "mean",
    "basically",
    "means",
    "hadoop",
    "create",
    "output",
    "need",
    "give",
    "name",
    "need",
    "create",
    "let",
    "give",
    "let",
    "append",
    "output",
    "number",
    "one",
    "let",
    "go",
    "ahead",
    "run",
    "submitted",
    "command",
    "also",
    "done",
    "background",
    "would",
    "want",
    "run",
    "multiple",
    "jobs",
    "cluster",
    "time",
    "takes",
    "total",
    "input",
    "paths",
    "processes",
    "one",
    "one",
    "split",
    "job",
    "work",
    "internally",
    "try",
    "contact",
    "resource",
    "manager",
    "basically",
    "done",
    "look",
    "see",
    "counters",
    "also",
    "see",
    "property",
    "missing",
    "run",
    "job",
    "run",
    "local",
    "mode",
    "run",
    "local",
    "mode",
    "although",
    "submitted",
    "might",
    "related",
    "yarn",
    "settings",
    "check",
    "refresh",
    "run",
    "application",
    "completed",
    "would",
    "created",
    "output",
    "thing",
    "interact",
    "yarn",
    "interact",
    "resource",
    "manager",
    "check",
    "properties",
    "look",
    "job",
    "basically",
    "tells",
    "went",
    "mapping",
    "reducing",
    "would",
    "created",
    "output",
    "worked",
    "file",
    "ran",
    "local",
    "mode",
    "ran",
    "local",
    "mode",
    "mapreduce",
    "remember",
    "programming",
    "model",
    "right",
    "run",
    "yarn",
    "get",
    "facilities",
    "running",
    "cluster",
    "yarn",
    "takes",
    "care",
    "resource",
    "management",
    "run",
    "yarn",
    "run",
    "local",
    "mode",
    "use",
    "machines",
    "ram",
    "cpu",
    "cores",
    "processing",
    "quickly",
    "look",
    "output",
    "also",
    "try",
    "running",
    "yarn",
    "look",
    "hdfs",
    "look",
    "output",
    "mr",
    "output",
    "directory",
    "used",
    "actually",
    "let",
    "look",
    "directory",
    "ending",
    "one",
    "show",
    "output",
    "created",
    "mapreduce",
    "although",
    "ran",
    "local",
    "mode",
    "fetched",
    "input",
    "file",
    "usdfs",
    "would",
    "created",
    "output",
    "hdfs",
    "part",
    "file",
    "created",
    "look",
    "part",
    "minus",
    "r",
    "minus",
    "zeros",
    "would",
    "one",
    "reducer",
    "running",
    "would",
    "multiple",
    "files",
    "created",
    "look",
    "file",
    "contain",
    "word",
    "count",
    "say",
    "cat",
    "basically",
    "shows",
    "output",
    "created",
    "mapreduce",
    "let",
    "look",
    "file",
    "gave",
    "processing",
    "broken",
    "list",
    "words",
    "occur",
    "file",
    "plus",
    "count",
    "words",
    "word",
    "shows",
    "count",
    "list",
    "words",
    "count",
    "run",
    "sample",
    "mapreduce",
    "job",
    "also",
    "show",
    "run",
    "yeah",
    "let",
    "run",
    "mapreduce",
    "yarn",
    "initially",
    "tried",
    "running",
    "mapreduce",
    "hit",
    "yarn",
    "ran",
    "local",
    "mode",
    "property",
    "changed",
    "mapreduce",
    "hyphen",
    "site",
    "file",
    "basically",
    "look",
    "file",
    "error",
    "given",
    "property",
    "says",
    "right",
    "property",
    "name",
    "ignored",
    "ran",
    "local",
    "mode",
    "changed",
    "property",
    "restarted",
    "cluster",
    "everything",
    "fine",
    "mapred",
    "hyphen",
    "site",
    "file",
    "also",
    "copied",
    "across",
    "nodes",
    "run",
    "mapreduce",
    "hadoop",
    "cluster",
    "uses",
    "yarn",
    "yarn",
    "takes",
    "care",
    "resource",
    "allocation",
    "one",
    "multiple",
    "machines",
    "changing",
    "output",
    "submit",
    "job",
    "first",
    "connect",
    "resource",
    "manager",
    "connects",
    "resource",
    "manager",
    "means",
    "job",
    "run",
    "using",
    "yarn",
    "cluster",
    "rather",
    "local",
    "mode",
    "wait",
    "application",
    "internally",
    "connect",
    "resource",
    "manager",
    "starts",
    "always",
    "go",
    "back",
    "web",
    "ui",
    "check",
    "application",
    "reached",
    "yarn",
    "shows",
    "one",
    "input",
    "part",
    "processed",
    "job",
    "id",
    "application",
    "id",
    "even",
    "monitor",
    "status",
    "command",
    "line",
    "job",
    "submitted",
    "let",
    "go",
    "back",
    "refresh",
    "yarn",
    "ui",
    "show",
    "new",
    "application",
    "submitted",
    "tells",
    "accepted",
    "state",
    "application",
    "master",
    "already",
    "started",
    "click",
    "link",
    "also",
    "give",
    "details",
    "many",
    "map",
    "reduce",
    "tasks",
    "would",
    "run",
    "says",
    "application",
    "master",
    "running",
    "would",
    "using",
    "node",
    "m1",
    "always",
    "look",
    "logs",
    "see",
    "one",
    "task",
    "attempt",
    "made",
    "go",
    "back",
    "terminal",
    "see",
    "waiting",
    "get",
    "resources",
    "cluster",
    "gets",
    "resources",
    "first",
    "start",
    "mapping",
    "phase",
    "mapper",
    "function",
    "runs",
    "map",
    "tasks",
    "one",
    "multiple",
    "depending",
    "splits",
    "right",
    "one",
    "file",
    "one",
    "split",
    "one",
    "map",
    "task",
    "running",
    "mapping",
    "phase",
    "completes",
    "get",
    "reducing",
    "finally",
    "give",
    "output",
    "toggling",
    "sessions",
    "refresh",
    "see",
    "happening",
    "application",
    "proceeding",
    "still",
    "waiting",
    "resource",
    "manager",
    "allocate",
    "resources",
    "couple",
    "minutes",
    "back",
    "tested",
    "application",
    "yarn",
    "see",
    "first",
    "application",
    "completed",
    "successfully",
    "give",
    "time",
    "yarn",
    "allocate",
    "resources",
    "resources",
    "used",
    "application",
    "freed",
    "internally",
    "yan",
    "takes",
    "care",
    "learn",
    "detail",
    "yarn",
    "might",
    "already",
    "followed",
    "yarn",
    "based",
    "session",
    "give",
    "time",
    "let",
    "see",
    "application",
    "proceeds",
    "resources",
    "yarn",
    "allocate",
    "sometimes",
    "also",
    "see",
    "slowness",
    "web",
    "ui",
    "shows",
    "related",
    "amount",
    "memory",
    "allocated",
    "nodes",
    "apache",
    "less",
    "amount",
    "memory",
    "still",
    "run",
    "cluster",
    "said",
    "memory",
    "shows",
    "16",
    "gb",
    "16",
    "cores",
    "true",
    "one",
    "default",
    "settings",
    "right",
    "yarn",
    "able",
    "facilitate",
    "running",
    "application",
    "let",
    "give",
    "couple",
    "seconds",
    "let",
    "look",
    "output",
    "make",
    "changes",
    "settings",
    "application",
    "getting",
    "enough",
    "resources",
    "basically",
    "restarted",
    "cluster",
    "let",
    "submit",
    "application",
    "cluster",
    "first",
    "contact",
    "resource",
    "manager",
    "basically",
    "map",
    "reduce",
    "process",
    "start",
    "submitted",
    "application",
    "connecting",
    "resource",
    "manager",
    "basically",
    "start",
    "internally",
    "app",
    "master",
    "application",
    "master",
    "looking",
    "number",
    "splits",
    "one",
    "getting",
    "application",
    "id",
    "basically",
    "starts",
    "running",
    "job",
    "also",
    "gives",
    "tracking",
    "url",
    "look",
    "output",
    "go",
    "back",
    "look",
    "yarn",
    "ui",
    "application",
    "shows",
    "give",
    "couple",
    "seconds",
    "get",
    "final",
    "status",
    "change",
    "running",
    "application",
    "getting",
    "resources",
    "closely",
    "notice",
    "allocated",
    "specific",
    "amount",
    "memory",
    "gb",
    "node",
    "manager",
    "every",
    "node",
    "basically",
    "given",
    "two",
    "cores",
    "machines",
    "also",
    "yarn",
    "utilizing",
    "resources",
    "rather",
    "going",
    "default",
    "application",
    "started",
    "moving",
    "see",
    "progress",
    "bar",
    "basically",
    "show",
    "happening",
    "go",
    "back",
    "terminal",
    "show",
    "first",
    "went",
    "deciding",
    "map",
    "reduce",
    "goes",
    "map",
    "mapping",
    "phase",
    "completes",
    "reducing",
    "phase",
    "come",
    "existence",
    "job",
    "completed",
    "basically",
    "used",
    "always",
    "look",
    "many",
    "map",
    "reduce",
    "stars",
    "run",
    "shows",
    "one",
    "map",
    "one",
    "reduced",
    "task",
    "number",
    "map",
    "tasks",
    "depends",
    "number",
    "splits",
    "one",
    "file",
    "less",
    "128",
    "mb",
    "one",
    "split",
    "processed",
    "reduced",
    "task",
    "internally",
    "decided",
    "reducer",
    "depending",
    "kind",
    "property",
    "set",
    "hadoop",
    "config",
    "files",
    "also",
    "tells",
    "many",
    "input",
    "records",
    "read",
    "basically",
    "means",
    "number",
    "lines",
    "file",
    "tells",
    "output",
    "records",
    "gives",
    "number",
    "total",
    "words",
    "file",
    "might",
    "duplicates",
    "processed",
    "internal",
    "combiner",
    "processing",
    "forwarding",
    "information",
    "reducer",
    "basically",
    "reducer",
    "works",
    "3",
    "35",
    "records",
    "gives",
    "us",
    "list",
    "words",
    "count",
    "refresh",
    "would",
    "obviously",
    "show",
    "application",
    "completed",
    "says",
    "succeeded",
    "always",
    "click",
    "application",
    "look",
    "information",
    "tells",
    "ran",
    "history",
    "server",
    "running",
    "otherwise",
    "always",
    "access",
    "information",
    "leads",
    "history",
    "server",
    "applications",
    "stored",
    "click",
    "attempt",
    "tasks",
    "basically",
    "show",
    "history",
    "url",
    "always",
    "look",
    "logs",
    "submit",
    "sample",
    "application",
    "inbuilt",
    "available",
    "jar",
    "hadoop",
    "cluster",
    "utilize",
    "cluster",
    "run",
    "could",
    "always",
    "said",
    "running",
    "particular",
    "job",
    "remember",
    "change",
    "output",
    "directory",
    "would",
    "want",
    "processing",
    "single",
    "individual",
    "file",
    "could",
    "also",
    "point",
    "directory",
    "basically",
    "means",
    "multiple",
    "files",
    "depending",
    "file",
    "sizes",
    "would",
    "multiple",
    "splits",
    "according",
    "multiple",
    "map",
    "tasks",
    "selected",
    "click",
    "would",
    "submit",
    "second",
    "application",
    "cluster",
    "first",
    "connect",
    "resource",
    "manager",
    "resource",
    "manager",
    "start",
    "application",
    "master",
    "targeting",
    "10",
    "splits",
    "sometimes",
    "give",
    "couple",
    "seconds",
    "machines",
    "resources",
    "used",
    "internally",
    "already",
    "freed",
    "cluster",
    "pick",
    "yarn",
    "take",
    "care",
    "resources",
    "right",
    "application",
    "undefined",
    "status",
    "soon",
    "yarn",
    "provides",
    "resources",
    "application",
    "running",
    "yarn",
    "cluster",
    "already",
    "started",
    "see",
    "going",
    "would",
    "launch",
    "10",
    "map",
    "tasks",
    "would",
    "number",
    "reduced",
    "tasks",
    "would",
    "decided",
    "either",
    "way",
    "data",
    "based",
    "properties",
    "set",
    "cluster",
    "level",
    "let",
    "quick",
    "refresh",
    "yarn",
    "ui",
    "show",
    "progress",
    "also",
    "take",
    "care",
    "submitting",
    "application",
    "need",
    "output",
    "directory",
    "mentioned",
    "however",
    "create",
    "hadoop",
    "create",
    "run",
    "map",
    "reduce",
    "without",
    "specifying",
    "properties",
    "specify",
    "properties",
    "look",
    "things",
    "changed",
    "mapper",
    "reducer",
    "basically",
    "combiner",
    "class",
    "mini",
    "reducing",
    "things",
    "done",
    "learn",
    "later",
    "sessions",
    "compare",
    "hadoop",
    "version",
    "one",
    "mapreduce",
    "version",
    "one",
    "understand",
    "learn",
    "limitations",
    "hadoop",
    "version",
    "one",
    "need",
    "yarn",
    "yarn",
    "kind",
    "workloads",
    "running",
    "yarn",
    "yarn",
    "components",
    "yarn",
    "architecture",
    "finally",
    "see",
    "demo",
    "yarn",
    "hadoop",
    "version",
    "one",
    "mapreduce",
    "version",
    "well",
    "outdated",
    "nobody",
    "using",
    "hadoop",
    "version",
    "1",
    "would",
    "good",
    "understand",
    "hadoop",
    "version",
    "1",
    "limitations",
    "hadoop",
    "version",
    "1",
    "brought",
    "thought",
    "future",
    "processing",
    "layer",
    "yarn",
    "talk",
    "hadoop",
    "already",
    "know",
    "hadoop",
    "framework",
    "hadoop",
    "two",
    "layers",
    "one",
    "storage",
    "layer",
    "sdfs",
    "hadoop",
    "distributed",
    "file",
    "system",
    "allows",
    "distributed",
    "storage",
    "processing",
    "allows",
    "fault",
    "tolerance",
    "inbuilt",
    "replication",
    "basically",
    "allows",
    "store",
    "huge",
    "amount",
    "data",
    "across",
    "multiple",
    "commodity",
    "machines",
    "talk",
    "processing",
    "know",
    "mapreduce",
    "oldest",
    "mature",
    "processing",
    "programming",
    "model",
    "basically",
    "takes",
    "care",
    "data",
    "processing",
    "distributed",
    "file",
    "system",
    "hadoop",
    "version",
    "1",
    "mapreduce",
    "performed",
    "data",
    "processing",
    "resource",
    "management",
    "problematic",
    "mapreduce",
    "basically",
    "talk",
    "processing",
    "layer",
    "master",
    "called",
    "job",
    "tracker",
    "slaves",
    "task",
    "records",
    "job",
    "tracker",
    "taking",
    "care",
    "allocating",
    "resources",
    "performing",
    "scheduling",
    "even",
    "monitoring",
    "jobs",
    "basically",
    "taking",
    "care",
    "assigning",
    "map",
    "reduced",
    "tasks",
    "jobs",
    "running",
    "task",
    "trackers",
    "task",
    "trackers",
    "data",
    "nodes",
    "responsible",
    "processing",
    "jobs",
    "task",
    "trackers",
    "slaves",
    "processing",
    "layer",
    "reported",
    "progress",
    "job",
    "tracker",
    "happening",
    "hadoop",
    "version",
    "talk",
    "hadoop",
    "version",
    "1",
    "would",
    "say",
    "client",
    "machines",
    "api",
    "application",
    "basically",
    "submits",
    "job",
    "master",
    "job",
    "tracker",
    "obviously",
    "forget",
    "would",
    "already",
    "involvement",
    "name",
    "node",
    "basically",
    "tells",
    "machines",
    "data",
    "nodes",
    "data",
    "already",
    "stored",
    "job",
    "submission",
    "happens",
    "job",
    "tracker",
    "job",
    "tracker",
    "master",
    "demon",
    "taking",
    "care",
    "processing",
    "request",
    "also",
    "resource",
    "management",
    "job",
    "scheduling",
    "would",
    "interacting",
    "multiple",
    "task",
    "trackers",
    "would",
    "running",
    "multiple",
    "machines",
    "machine",
    "would",
    "task",
    "tracker",
    "running",
    "task",
    "tracker",
    "processing",
    "slave",
    "would",
    "data",
    "nodes",
    "know",
    "case",
    "hadoop",
    "concept",
    "moving",
    "processing",
    "wherever",
    "data",
    "stored",
    "rather",
    "moving",
    "data",
    "processing",
    "layer",
    "would",
    "task",
    "trackers",
    "would",
    "running",
    "multiple",
    "machines",
    "task",
    "trackers",
    "would",
    "responsible",
    "handling",
    "tasks",
    "tasks",
    "application",
    "broken",
    "smaller",
    "tasks",
    "would",
    "work",
    "data",
    "respectively",
    "stored",
    "particular",
    "node",
    "slave",
    "demons",
    "right",
    "job",
    "tracker",
    "tracking",
    "resources",
    "task",
    "trackers",
    "sending",
    "heartbeats",
    "sending",
    "packets",
    "information",
    "job",
    "tracker",
    "would",
    "knowing",
    "many",
    "resources",
    "talk",
    "resources",
    "talking",
    "cpu",
    "cores",
    "talking",
    "ram",
    "would",
    "available",
    "every",
    "node",
    "task",
    "trackers",
    "would",
    "sending",
    "resource",
    "information",
    "job",
    "tracker",
    "job",
    "tracker",
    "would",
    "already",
    "aware",
    "amount",
    "resources",
    "available",
    "particular",
    "node",
    "loaded",
    "particular",
    "node",
    "kind",
    "work",
    "could",
    "given",
    "task",
    "tracker",
    "job",
    "tracker",
    "taking",
    "care",
    "resource",
    "management",
    "also",
    "breaking",
    "application",
    "tasks",
    "job",
    "scheduling",
    "part",
    "assign",
    "different",
    "tasks",
    "slave",
    "demons",
    "task",
    "trackers",
    "job",
    "tracker",
    "eventually",
    "overburdened",
    "right",
    "managing",
    "jobs",
    "tracking",
    "resources",
    "multiple",
    "task",
    "trackers",
    "basically",
    "taking",
    "care",
    "job",
    "scheduling",
    "job",
    "tracker",
    "would",
    "overburdened",
    "case",
    "job",
    "tracker",
    "would",
    "fail",
    "would",
    "affect",
    "overall",
    "processing",
    "master",
    "skilled",
    "master",
    "demon",
    "dies",
    "processing",
    "proceed",
    "one",
    "limitations",
    "hadoop",
    "version",
    "one",
    "talk",
    "scalability",
    "capability",
    "scale",
    "due",
    "single",
    "job",
    "tracker",
    "scalability",
    "would",
    "hitting",
    "bottle",
    "link",
    "cluster",
    "size",
    "4000",
    "nodes",
    "run",
    "40",
    "000",
    "concurrent",
    "tasks",
    "number",
    "could",
    "always",
    "look",
    "individual",
    "resources",
    "machine",
    "come",
    "appropriate",
    "number",
    "however",
    "single",
    "job",
    "tracker",
    "horizontal",
    "scalability",
    "processing",
    "layer",
    "single",
    "processing",
    "master",
    "talk",
    "availability",
    "job",
    "tracker",
    "mentioned",
    "would",
    "single",
    "point",
    "failure",
    "failure",
    "kills",
    "queued",
    "running",
    "jobs",
    "jobs",
    "would",
    "resubmitted",
    "would",
    "want",
    "distributed",
    "platform",
    "cluster",
    "hundreds",
    "thousands",
    "machines",
    "would",
    "want",
    "processing",
    "layer",
    "handle",
    "huge",
    "amount",
    "processing",
    "could",
    "scalable",
    "could",
    "available",
    "could",
    "handle",
    "different",
    "kind",
    "workloads",
    "comes",
    "resource",
    "utilization",
    "would",
    "predefined",
    "number",
    "map",
    "reduce",
    "slots",
    "task",
    "tracker",
    "would",
    "issues",
    "would",
    "relate",
    "resource",
    "utilization",
    "putting",
    "burden",
    "master",
    "tracking",
    "resources",
    "assign",
    "jobs",
    "run",
    "multiple",
    "machines",
    "parallel",
    "limitations",
    "running",
    "reduce",
    "applications",
    "one",
    "limitation",
    "hadoop",
    "version",
    "1",
    "mapreduce",
    "kind",
    "processing",
    "could",
    "mapreduce",
    "map",
    "reduce",
    "programming",
    "model",
    "although",
    "good",
    "oldest",
    "matured",
    "period",
    "time",
    "rigid",
    "go",
    "mapping",
    "reducing",
    "approach",
    "kind",
    "processing",
    "could",
    "done",
    "hadoop",
    "person",
    "one",
    "comes",
    "real",
    "time",
    "analysis",
    "ad",
    "hoc",
    "query",
    "graph",
    "based",
    "processing",
    "massive",
    "parallel",
    "processing",
    "limitations",
    "could",
    "done",
    "hadoop",
    "version",
    "1",
    "mapreduce",
    "version",
    "1",
    "processing",
    "component",
    "brings",
    "us",
    "need",
    "yarn",
    "yarn",
    "stands",
    "yet",
    "another",
    "resource",
    "negotiator",
    "mentioned",
    "yarn",
    "hadoop",
    "version",
    "one",
    "well",
    "could",
    "applications",
    "could",
    "written",
    "different",
    "programming",
    "languages",
    "kind",
    "processing",
    "possible",
    "mapreduce",
    "storage",
    "layer",
    "processing",
    "kind",
    "limited",
    "processing",
    "could",
    "done",
    "one",
    "thing",
    "brought",
    "thought",
    "processing",
    "layer",
    "handle",
    "different",
    "kind",
    "workloads",
    "mentioned",
    "might",
    "graph",
    "processing",
    "might",
    "real",
    "time",
    "processing",
    "might",
    "massive",
    "parallel",
    "processing",
    "kind",
    "processing",
    "would",
    "requirement",
    "organization",
    "designed",
    "run",
    "mapreduce",
    "jobs",
    "issues",
    "scalability",
    "resource",
    "utilization",
    "job",
    "tracking",
    "etc",
    "led",
    "need",
    "something",
    "call",
    "yarn",
    "hadoop",
    "version",
    "2",
    "onwards",
    "two",
    "main",
    "layers",
    "changed",
    "little",
    "bit",
    "storage",
    "layer",
    "intact",
    "hdfs",
    "processing",
    "layer",
    "called",
    "yarn",
    "yet",
    "another",
    "resource",
    "negotiator",
    "understand",
    "yarn",
    "works",
    "yarn",
    "taking",
    "care",
    "processing",
    "layer",
    "support",
    "mapreduce",
    "mapreduce",
    "processing",
    "still",
    "done",
    "support",
    "processing",
    "frameworks",
    "yarn",
    "used",
    "solve",
    "issues",
    "hadoop",
    "version",
    "1",
    "posing",
    "something",
    "like",
    "resource",
    "management",
    "something",
    "like",
    "different",
    "kind",
    "workload",
    "processing",
    "something",
    "like",
    "scalability",
    "resource",
    "utilization",
    "taken",
    "care",
    "yarn",
    "talk",
    "yarn",
    "cluster",
    "size",
    "10",
    "000",
    "nodes",
    "run",
    "100",
    "000",
    "concurrent",
    "tasks",
    "take",
    "care",
    "scalability",
    "talk",
    "compatibility",
    "applications",
    "developed",
    "hadoop",
    "version",
    "1",
    "primarily",
    "mapreduce",
    "kind",
    "processing",
    "run",
    "yarn",
    "without",
    "disruption",
    "availability",
    "issues",
    "talk",
    "resource",
    "utilization",
    "mechanism",
    "takes",
    "care",
    "dynamic",
    "allocation",
    "cluster",
    "resources",
    "basically",
    "improves",
    "resource",
    "utilization",
    "talk",
    "basically",
    "cluster",
    "handle",
    "different",
    "kind",
    "workloads",
    "use",
    "open",
    "source",
    "proprietary",
    "data",
    "access",
    "engines",
    "perform",
    "analysis",
    "graph",
    "processing",
    "ad",
    "hoc",
    "querying",
    "supported",
    "multiple",
    "workloads",
    "run",
    "parallel",
    "yarn",
    "offers",
    "yan",
    "mentioned",
    "yarn",
    "stands",
    "yet",
    "another",
    "resource",
    "negotiator",
    "cluster",
    "resource",
    "management",
    "layer",
    "apache",
    "hadoop",
    "ecosystem",
    "takes",
    "care",
    "scheduling",
    "jobs",
    "assigning",
    "resources",
    "imagine",
    "would",
    "want",
    "run",
    "particular",
    "application",
    "would",
    "basically",
    "telling",
    "cluster",
    "would",
    "want",
    "resources",
    "run",
    "applications",
    "application",
    "might",
    "mapreduce",
    "application",
    "might",
    "hive",
    "query",
    "triggering",
    "mapreduce",
    "might",
    "big",
    "script",
    "triggering",
    "mapreduce",
    "could",
    "hive",
    "days",
    "execution",
    "engine",
    "could",
    "spark",
    "application",
    "could",
    "graph",
    "processing",
    "application",
    "cases",
    "would",
    "still",
    "sense",
    "client",
    "basically",
    "api",
    "application",
    "would",
    "requesting",
    "resources",
    "yan",
    "would",
    "take",
    "care",
    "yarn",
    "would",
    "provide",
    "desired",
    "resources",
    "talk",
    "resources",
    "mainly",
    "talking",
    "network",
    "related",
    "resources",
    "talking",
    "cpu",
    "cores",
    "terms",
    "yarn",
    "say",
    "virtual",
    "cpu",
    "cores",
    "would",
    "talk",
    "ram",
    "gb",
    "mb",
    "terabytes",
    "would",
    "offered",
    "multiple",
    "machines",
    "yarn",
    "would",
    "take",
    "care",
    "yarn",
    "could",
    "basically",
    "handle",
    "different",
    "workloads",
    "workloads",
    "showing",
    "traditional",
    "mapreduce",
    "mainly",
    "batch",
    "oriented",
    "could",
    "interactive",
    "execution",
    "engine",
    "something",
    "space",
    "could",
    "hbase",
    "column",
    "oriented",
    "four",
    "dimensional",
    "database",
    "would",
    "storing",
    "data",
    "sdfs",
    "would",
    "also",
    "need",
    "kind",
    "processing",
    "could",
    "streaming",
    "functionalities",
    "would",
    "storm",
    "kafka",
    "spark",
    "could",
    "graph",
    "processing",
    "could",
    "processing",
    "spark",
    "components",
    "could",
    "many",
    "others",
    "different",
    "frameworks",
    "could",
    "run",
    "run",
    "top",
    "er",
    "ant",
    "talk",
    "yarn",
    "overall",
    "yarn",
    "architecture",
    "looks",
    "one",
    "end",
    "client",
    "client",
    "could",
    "basically",
    "edge",
    "node",
    "applications",
    "running",
    "could",
    "api",
    "would",
    "want",
    "interact",
    "cluster",
    "could",
    "user",
    "triggered",
    "application",
    "wants",
    "run",
    "jobs",
    "processing",
    "client",
    "would",
    "submit",
    "job",
    "request",
    "resource",
    "manager",
    "resource",
    "manager",
    "master",
    "processing",
    "layer",
    "hadoop",
    "version",
    "1",
    "basically",
    "job",
    "tracker",
    "task",
    "trackers",
    "running",
    "individual",
    "nodes",
    "task",
    "trackers",
    "sending",
    "heart",
    "beats",
    "job",
    "tracker",
    "task",
    "trackers",
    "sending",
    "resource",
    "information",
    "job",
    "tracker",
    "one",
    "tracking",
    "resources",
    "job",
    "scheduling",
    "mentioned",
    "earlier",
    "job",
    "tracker",
    "overburdened",
    "job",
    "tracker",
    "replaced",
    "resource",
    "manager",
    "master",
    "processing",
    "layer",
    "task",
    "trackers",
    "replaced",
    "node",
    "managers",
    "would",
    "running",
    "every",
    "node",
    "temporary",
    "demon",
    "see",
    "blue",
    "app",
    "master",
    "mentioned",
    "say",
    "yet",
    "another",
    "resource",
    "negotiator",
    "appmaster",
    "would",
    "existing",
    "hadoop",
    "version",
    "talk",
    "resource",
    "manager",
    "resource",
    "manager",
    "master",
    "processing",
    "layer",
    "would",
    "already",
    "receiving",
    "heartbeats",
    "say",
    "resource",
    "information",
    "multiple",
    "node",
    "managers",
    "would",
    "running",
    "one",
    "multiple",
    "machines",
    "node",
    "managers",
    "updating",
    "status",
    "also",
    "giving",
    "information",
    "amount",
    "resources",
    "talk",
    "resources",
    "understand",
    "talking",
    "node",
    "manager",
    "allocated",
    "amount",
    "ram",
    "processing",
    "amount",
    "cpu",
    "cores",
    "portion",
    "complete",
    "node",
    "node",
    "say",
    "imagine",
    "node",
    "around",
    "100",
    "gb",
    "ram",
    "saved",
    "60",
    "cores",
    "allocated",
    "node",
    "manager",
    "node",
    "manager",
    "one",
    "components",
    "hadoop",
    "ecosystem",
    "slave",
    "processing",
    "layer",
    "could",
    "say",
    "keeping",
    "aspects",
    "different",
    "services",
    "running",
    "might",
    "cloudera",
    "hortonworks",
    "related",
    "services",
    "running",
    "system",
    "processes",
    "running",
    "particular",
    "node",
    "portion",
    "would",
    "assigned",
    "node",
    "manager",
    "processing",
    "could",
    "say",
    "example",
    "say",
    "60",
    "gb",
    "ram",
    "per",
    "node",
    "say",
    "40",
    "cpu",
    "cores",
    "allocated",
    "node",
    "manager",
    "every",
    "machine",
    "similarly",
    "would",
    "similarly",
    "would",
    "node",
    "manager",
    "constantly",
    "giving",
    "update",
    "resource",
    "manager",
    "resources",
    "probably",
    "might",
    "applications",
    "running",
    "node",
    "manager",
    "already",
    "occupied",
    "gives",
    "update",
    "also",
    "concept",
    "containers",
    "basically",
    "talk",
    "resources",
    "broken",
    "smaller",
    "parts",
    "resource",
    "manager",
    "keeping",
    "track",
    "resources",
    "every",
    "node",
    "manager",
    "also",
    "responsible",
    "taking",
    "care",
    "job",
    "request",
    "things",
    "happen",
    "see",
    "resource",
    "manager",
    "higher",
    "level",
    "always",
    "say",
    "processing",
    "master",
    "everything",
    "reality",
    "resource",
    "manager",
    "internally",
    "different",
    "services",
    "components",
    "helping",
    "supposed",
    "let",
    "look",
    "mentioned",
    "resource",
    "manager",
    "services",
    "components",
    "basically",
    "helps",
    "things",
    "basically",
    "architecture",
    "multiple",
    "components",
    "working",
    "together",
    "achieve",
    "yarn",
    "allows",
    "resource",
    "manager",
    "mainly",
    "two",
    "components",
    "scheduler",
    "applications",
    "manager",
    "high",
    "level",
    "four",
    "main",
    "components",
    "talk",
    "resource",
    "manager",
    "processing",
    "master",
    "node",
    "managers",
    "processing",
    "slaves",
    "running",
    "every",
    "nodes",
    "concept",
    "container",
    "concept",
    "application",
    "master",
    "things",
    "work",
    "let",
    "look",
    "yarn",
    "components",
    "resource",
    "manager",
    "basically",
    "two",
    "main",
    "components",
    "say",
    "assist",
    "resource",
    "manager",
    "capable",
    "scheduler",
    "applications",
    "manager",
    "talk",
    "resources",
    "always",
    "requirement",
    "applications",
    "need",
    "run",
    "cluster",
    "resources",
    "application",
    "run",
    "submitted",
    "client",
    "needs",
    "resources",
    "resources",
    "coming",
    "multiple",
    "machines",
    "wherever",
    "relevant",
    "data",
    "stored",
    "node",
    "manager",
    "running",
    "always",
    "know",
    "node",
    "manager",
    "data",
    "nodes",
    "scheduler",
    "different",
    "kind",
    "schedulers",
    "basically",
    "capacity",
    "scheduler",
    "fair",
    "scheduler",
    "could",
    "fifo",
    "scheduler",
    "different",
    "schedulers",
    "take",
    "care",
    "resource",
    "allocation",
    "scheduler",
    "responsible",
    "allocating",
    "resources",
    "various",
    "running",
    "applications",
    "imagine",
    "particular",
    "environment",
    "different",
    "teams",
    "different",
    "departments",
    "working",
    "cluster",
    "would",
    "call",
    "cluster",
    "cluster",
    "cluster",
    "would",
    "different",
    "applications",
    "would",
    "want",
    "run",
    "simultaneously",
    "accessing",
    "resources",
    "cluster",
    "managed",
    "component",
    "concept",
    "pooling",
    "queuing",
    "different",
    "departments",
    "different",
    "users",
    "get",
    "dedicated",
    "resources",
    "share",
    "resources",
    "cluster",
    "scheduler",
    "responsible",
    "allocating",
    "resources",
    "various",
    "running",
    "applications",
    "perform",
    "monitoring",
    "tracking",
    "status",
    "applications",
    "part",
    "scheduler",
    "offer",
    "guarantee",
    "restarting",
    "failed",
    "tasks",
    "due",
    "hardware",
    "network",
    "failures",
    "scheduler",
    "mainly",
    "responsible",
    "allocating",
    "resources",
    "mentioned",
    "could",
    "different",
    "kind",
    "schedulers",
    "could",
    "fifo",
    "scheduler",
    "mainly",
    "older",
    "version",
    "hadoop",
    "stands",
    "first",
    "first",
    "could",
    "fair",
    "scheduler",
    "basically",
    "means",
    "multiple",
    "applications",
    "could",
    "running",
    "cluster",
    "would",
    "fair",
    "share",
    "resources",
    "could",
    "capacity",
    "scheduler",
    "would",
    "basically",
    "dedicated",
    "fixed",
    "amount",
    "resources",
    "across",
    "cluster",
    "whichever",
    "scheduler",
    "used",
    "scheduler",
    "mainly",
    "responsible",
    "allocating",
    "resources",
    "applications",
    "manager",
    "responsible",
    "accepting",
    "job",
    "submissions",
    "said",
    "higher",
    "level",
    "could",
    "always",
    "say",
    "resource",
    "manager",
    "state",
    "everything",
    "allocating",
    "resources",
    "negotiating",
    "resources",
    "also",
    "taking",
    "care",
    "listening",
    "clients",
    "taking",
    "care",
    "job",
    "submissions",
    "real",
    "components",
    "applications",
    "manager",
    "responsible",
    "accepting",
    "job",
    "submissions",
    "negotiates",
    "first",
    "container",
    "executing",
    "application",
    "specific",
    "application",
    "master",
    "provides",
    "service",
    "restarting",
    "application",
    "master",
    "work",
    "things",
    "happen",
    "coordination",
    "said",
    "node",
    "manager",
    "slave",
    "process",
    "would",
    "running",
    "every",
    "machine",
    "slave",
    "tracking",
    "resources",
    "tracking",
    "processes",
    "taking",
    "care",
    "running",
    "jobs",
    "basically",
    "tracking",
    "container",
    "resource",
    "utilization",
    "let",
    "understand",
    "container",
    "normally",
    "talk",
    "application",
    "request",
    "comes",
    "client",
    "let",
    "say",
    "client",
    "requesting",
    "coming",
    "application",
    "needs",
    "run",
    "cluster",
    "application",
    "could",
    "anything",
    "first",
    "contacts",
    "master",
    "resource",
    "manager",
    "master",
    "processing",
    "layer",
    "mentioned",
    "already",
    "know",
    "name",
    "node",
    "master",
    "cluster",
    "metadata",
    "ram",
    "aware",
    "data",
    "split",
    "blocks",
    "blocks",
    "stored",
    "multiple",
    "machines",
    "information",
    "obviously",
    "interaction",
    "master",
    "given",
    "information",
    "relevant",
    "nodes",
    "data",
    "exists",
    "processing",
    "need",
    "client",
    "basically",
    "application",
    "needs",
    "run",
    "cluster",
    "resource",
    "manager",
    "basically",
    "scheduler",
    "takes",
    "care",
    "allocating",
    "resources",
    "resource",
    "manager",
    "mainly",
    "two",
    "components",
    "helping",
    "work",
    "particular",
    "application",
    "might",
    "needing",
    "data",
    "multiple",
    "machines",
    "know",
    "would",
    "multiple",
    "machines",
    "would",
    "node",
    "manager",
    "running",
    "would",
    "data",
    "node",
    "running",
    "data",
    "nodes",
    "responsible",
    "storing",
    "data",
    "disk",
    "resource",
    "manager",
    "negotiate",
    "resources",
    "say",
    "negotiating",
    "resources",
    "could",
    "basically",
    "ask",
    "node",
    "managers",
    "amount",
    "resources",
    "example",
    "would",
    "saying",
    "one",
    "gb",
    "ram",
    "one",
    "cpu",
    "core",
    "data",
    "residing",
    "machine",
    "needs",
    "processed",
    "part",
    "application",
    "one",
    "gb",
    "one",
    "cpu",
    "core",
    "relevant",
    "data",
    "stored",
    "request",
    "resource",
    "manager",
    "makes",
    "holding",
    "resources",
    "total",
    "resources",
    "node",
    "manager",
    "resource",
    "manager",
    "negotiating",
    "asking",
    "resources",
    "processing",
    "slave",
    "request",
    "holding",
    "resources",
    "considered",
    "container",
    "resource",
    "manager",
    "know",
    "actually",
    "resource",
    "manager",
    "application",
    "manager",
    "negotiating",
    "resources",
    "negotiates",
    "resources",
    "called",
    "container",
    "request",
    "holding",
    "resource",
    "considered",
    "container",
    "basically",
    "container",
    "different",
    "sizes",
    "talk",
    "resource",
    "manager",
    "negotiates",
    "resources",
    "node",
    "manager",
    "node",
    "manager",
    "already",
    "giving",
    "update",
    "resources",
    "amount",
    "resources",
    "holds",
    "much",
    "busy",
    "basically",
    "approve",
    "deny",
    "request",
    "node",
    "manager",
    "would",
    "basically",
    "approve",
    "saying",
    "yes",
    "could",
    "hold",
    "resources",
    "could",
    "give",
    "container",
    "particular",
    "size",
    "container",
    "approved",
    "allocated",
    "say",
    "granted",
    "node",
    "manager",
    "resource",
    "manager",
    "knows",
    "resources",
    "process",
    "application",
    "available",
    "guaranteed",
    "node",
    "manager",
    "resource",
    "manager",
    "starts",
    "temporary",
    "demon",
    "called",
    "appmaster",
    "piece",
    "code",
    "would",
    "also",
    "running",
    "one",
    "containers",
    "would",
    "running",
    "one",
    "containers",
    "would",
    "take",
    "care",
    "execution",
    "tasks",
    "containers",
    "application",
    "master",
    "per",
    "application",
    "would",
    "10",
    "different",
    "applications",
    "coming",
    "client",
    "would",
    "10",
    "app",
    "masters",
    "one",
    "app",
    "master",
    "responsible",
    "per",
    "application",
    "app",
    "master",
    "basically",
    "piece",
    "code",
    "responsible",
    "execution",
    "application",
    "app",
    "master",
    "would",
    "run",
    "one",
    "containers",
    "would",
    "use",
    "containers",
    "node",
    "manager",
    "guaranteed",
    "give",
    "request",
    "application",
    "request",
    "comes",
    "using",
    "containers",
    "app",
    "master",
    "run",
    "processing",
    "tasks",
    "within",
    "designated",
    "resources",
    "mainly",
    "responsibility",
    "application",
    "master",
    "get",
    "execution",
    "done",
    "communicate",
    "master",
    "resource",
    "manager",
    "tracking",
    "resources",
    "negotiating",
    "resources",
    "resources",
    "negotiated",
    "basically",
    "gives",
    "control",
    "application",
    "master",
    "application",
    "master",
    "running",
    "within",
    "one",
    "containers",
    "one",
    "nodes",
    "using",
    "containers",
    "take",
    "care",
    "execution",
    "looks",
    "basically",
    "container",
    "said",
    "collection",
    "resources",
    "like",
    "cpu",
    "memory",
    "disk",
    "would",
    "used",
    "already",
    "data",
    "network",
    "node",
    "manager",
    "basically",
    "looking",
    "request",
    "application",
    "master",
    "basically",
    "granting",
    "request",
    "basically",
    "allocating",
    "containers",
    "could",
    "different",
    "sizing",
    "containers",
    "let",
    "take",
    "example",
    "mentioned",
    "total",
    "resources",
    "available",
    "particular",
    "node",
    "portion",
    "resources",
    "allocated",
    "node",
    "manager",
    "let",
    "imagine",
    "node",
    "node",
    "manager",
    "processing",
    "slave",
    "running",
    "total",
    "resources",
    "node",
    "portion",
    "ram",
    "cpu",
    "cores",
    "basically",
    "allocated",
    "node",
    "manager",
    "could",
    "say",
    "total",
    "100",
    "gb",
    "ram",
    "say",
    "around",
    "60",
    "cores",
    "particular",
    "node",
    "ram",
    "node",
    "cpu",
    "cores",
    "node",
    "portion",
    "right",
    "say",
    "might",
    "70",
    "percent",
    "60",
    "percent",
    "total",
    "resources",
    "could",
    "say",
    "around",
    "60",
    "gb",
    "ram",
    "could",
    "say",
    "around",
    "40",
    "v",
    "cores",
    "allocated",
    "node",
    "manager",
    "settings",
    "given",
    "yarn",
    "hyphen",
    "site",
    "file",
    "apart",
    "allocation",
    "60",
    "gb",
    "ram",
    "40v",
    "cores",
    "also",
    "properties",
    "say",
    "container",
    "sizes",
    "example",
    "could",
    "small",
    "container",
    "setting",
    "could",
    "say",
    "every",
    "container",
    "could",
    "2gb",
    "ram",
    "say",
    "one",
    "virtual",
    "cpu",
    "core",
    "smallest",
    "container",
    "based",
    "total",
    "resources",
    "could",
    "calculate",
    "many",
    "small",
    "containers",
    "could",
    "running",
    "say",
    "2gb",
    "ram",
    "could",
    "around",
    "30",
    "containers",
    "talking",
    "one",
    "virtual",
    "cpu",
    "core",
    "totally",
    "could",
    "around",
    "30",
    "small",
    "containers",
    "could",
    "running",
    "parallel",
    "particular",
    "node",
    "calculation",
    "would",
    "say",
    "10",
    "cpu",
    "cores",
    "utilized",
    "could",
    "bigger",
    "container",
    "size",
    "could",
    "say",
    "would",
    "go",
    "2",
    "cpu",
    "cores",
    "3gb",
    "ram",
    "3gb",
    "ram",
    "2",
    "cpu",
    "cores",
    "would",
    "give",
    "around",
    "20",
    "containers",
    "bigger",
    "size",
    "container",
    "sizing",
    "defined",
    "yarn",
    "hyphen",
    "site",
    "file",
    "know",
    "particular",
    "node",
    "kind",
    "allocation",
    "either",
    "could",
    "30",
    "small",
    "containers",
    "running",
    "could",
    "20",
    "big",
    "containers",
    "running",
    "would",
    "apply",
    "multiple",
    "nodes",
    "node",
    "manager",
    "based",
    "request",
    "application",
    "master",
    "allocate",
    "containers",
    "remember",
    "within",
    "one",
    "containers",
    "would",
    "application",
    "master",
    "running",
    "containers",
    "could",
    "used",
    "processing",
    "requirement",
    "application",
    "master",
    "per",
    "application",
    "one",
    "uses",
    "resources",
    "basically",
    "manages",
    "uses",
    "resources",
    "individual",
    "application",
    "remember",
    "10",
    "applications",
    "running",
    "yarn",
    "would",
    "10",
    "application",
    "masters",
    "one",
    "responsible",
    "application",
    "application",
    "master",
    "one",
    "also",
    "interacts",
    "scheduler",
    "basically",
    "know",
    "much",
    "amount",
    "resources",
    "could",
    "allocated",
    "one",
    "application",
    "application",
    "master",
    "one",
    "uses",
    "resources",
    "never",
    "negotiate",
    "resources",
    "node",
    "manager",
    "application",
    "master",
    "application",
    "master",
    "always",
    "go",
    "back",
    "resource",
    "manager",
    "needs",
    "resources",
    "always",
    "resource",
    "manager",
    "internally",
    "resource",
    "manager",
    "component",
    "application",
    "manager",
    "negotiates",
    "resources",
    "point",
    "time",
    "due",
    "node",
    "failures",
    "due",
    "requirements",
    "application",
    "master",
    "needs",
    "resources",
    "one",
    "multiple",
    "nodes",
    "always",
    "contacting",
    "resource",
    "manager",
    "internally",
    "applications",
    "manager",
    "containers",
    "looks",
    "client",
    "submits",
    "job",
    "request",
    "resource",
    "manager",
    "know",
    "resource",
    "manager",
    "internally",
    "scheduler",
    "applications",
    "manager",
    "node",
    "managers",
    "running",
    "multiple",
    "machines",
    "ones",
    "tracking",
    "resources",
    "giving",
    "information",
    "source",
    "manager",
    "resource",
    "manager",
    "would",
    "say",
    "component",
    "applications",
    "manager",
    "could",
    "request",
    "resources",
    "multiple",
    "node",
    "managers",
    "say",
    "request",
    "resources",
    "containers",
    "resource",
    "manager",
    "basically",
    "request",
    "resources",
    "one",
    "multiple",
    "nodes",
    "node",
    "manager",
    "one",
    "approves",
    "containers",
    "container",
    "approved",
    "resource",
    "manager",
    "triggers",
    "piece",
    "code",
    "application",
    "master",
    "obviously",
    "needs",
    "resources",
    "would",
    "run",
    "one",
    "containers",
    "use",
    "containers",
    "execution",
    "client",
    "submits",
    "application",
    "resource",
    "manager",
    "resource",
    "manager",
    "allocates",
    "container",
    "would",
    "say",
    "high",
    "level",
    "right",
    "resource",
    "manager",
    "negotiating",
    "resources",
    "internally",
    "negotiating",
    "resources",
    "applications",
    "manager",
    "granting",
    "request",
    "node",
    "manager",
    "say",
    "resource",
    "manager",
    "locates",
    "container",
    "application",
    "master",
    "basically",
    "contacts",
    "related",
    "node",
    "manager",
    "needs",
    "use",
    "containers",
    "node",
    "manager",
    "one",
    "launches",
    "container",
    "basically",
    "gives",
    "resources",
    "within",
    "application",
    "run",
    "application",
    "master",
    "accommodate",
    "one",
    "containers",
    "use",
    "containers",
    "processing",
    "within",
    "containers",
    "actual",
    "execution",
    "happens",
    "could",
    "map",
    "task",
    "could",
    "reduced",
    "task",
    "could",
    "spark",
    "executor",
    "taking",
    "care",
    "smart",
    "tasks",
    "many",
    "processing",
    "look",
    "demo",
    "yarn",
    "works",
    "would",
    "suggest",
    "looking",
    "one",
    "blogs",
    "cloudera",
    "look",
    "yarn",
    "untangling",
    "really",
    "good",
    "blog",
    "basically",
    "talks",
    "overall",
    "functionality",
    "explained",
    "mentioned",
    "basically",
    "master",
    "process",
    "worker",
    "process",
    "basically",
    "takes",
    "care",
    "processing",
    "resource",
    "manager",
    "master",
    "node",
    "manager",
    "slave",
    "also",
    "talks",
    "resources",
    "node",
    "manager",
    "talks",
    "yarn",
    "configuration",
    "file",
    "give",
    "properties",
    "basically",
    "shows",
    "node",
    "manager",
    "reports",
    "amount",
    "resources",
    "resource",
    "manager",
    "remember",
    "worker",
    "node",
    "shows",
    "18",
    "8",
    "cpu",
    "cores",
    "128",
    "gb",
    "ram",
    "node",
    "manager",
    "says",
    "64",
    "v",
    "cores",
    "ram",
    "128",
    "gb",
    "total",
    "capacity",
    "node",
    "portion",
    "node",
    "allocated",
    "node",
    "manager",
    "node",
    "manager",
    "reports",
    "resource",
    "manager",
    "requesting",
    "containers",
    "based",
    "application",
    "container",
    "basically",
    "logical",
    "name",
    "given",
    "combination",
    "vcore",
    "ram",
    "within",
    "container",
    "would",
    "basically",
    "process",
    "running",
    "application",
    "starts",
    "node",
    "manager",
    "guaranteed",
    "containers",
    "application",
    "resource",
    "manager",
    "basically",
    "already",
    "started",
    "application",
    "master",
    "within",
    "container",
    "application",
    "master",
    "uses",
    "containers",
    "tasks",
    "would",
    "run",
    "good",
    "blog",
    "refer",
    "also",
    "talks",
    "mapreduce",
    "already",
    "followed",
    "mapreduce",
    "tutorials",
    "past",
    "would",
    "know",
    "different",
    "kind",
    "tasks",
    "map",
    "reduce",
    "map",
    "reduce",
    "tasks",
    "could",
    "running",
    "within",
    "container",
    "one",
    "multiple",
    "said",
    "could",
    "map",
    "task",
    "could",
    "reduced",
    "task",
    "could",
    "spark",
    "based",
    "task",
    "would",
    "running",
    "within",
    "container",
    "task",
    "finishes",
    "basically",
    "resources",
    "freed",
    "container",
    "released",
    "resources",
    "given",
    "back",
    "yarn",
    "take",
    "care",
    "processing",
    "look",
    "blog",
    "also",
    "look",
    "part",
    "two",
    "talk",
    "mainly",
    "configuration",
    "settings",
    "always",
    "look",
    "talks",
    "much",
    "resources",
    "allocated",
    "node",
    "manager",
    "basically",
    "talks",
    "operating",
    "system",
    "overhead",
    "talks",
    "services",
    "talks",
    "clouded",
    "hot",
    "works",
    "related",
    "services",
    "running",
    "processes",
    "might",
    "running",
    "based",
    "portion",
    "ram",
    "cpu",
    "cores",
    "would",
    "allocated",
    "node",
    "manager",
    "would",
    "done",
    "yarn",
    "hyphen",
    "site",
    "file",
    "basically",
    "shows",
    "total",
    "amount",
    "memory",
    "cpu",
    "cores",
    "allocated",
    "node",
    "manager",
    "within",
    "every",
    "machine",
    "node",
    "manager",
    "running",
    "every",
    "machine",
    "yarn",
    "hyphen",
    "site",
    "file",
    "would",
    "properties",
    "would",
    "say",
    "minimum",
    "container",
    "size",
    "maximum",
    "container",
    "size",
    "terms",
    "ram",
    "minimum",
    "cpu",
    "cores",
    "maximum",
    "cpu",
    "cores",
    "incremental",
    "size",
    "ram",
    "cpu",
    "cores",
    "increment",
    "properties",
    "define",
    "containers",
    "allocated",
    "application",
    "request",
    "look",
    "could",
    "good",
    "information",
    "talks",
    "different",
    "properties",
    "look",
    "talks",
    "scheduling",
    "look",
    "particular",
    "blog",
    "also",
    "talks",
    "scheduling",
    "talks",
    "scheduling",
    "yarn",
    "talks",
    "fair",
    "scheduler",
    "basically",
    "different",
    "cues",
    "allocations",
    "done",
    "also",
    "different",
    "ways",
    "queues",
    "managed",
    "different",
    "schedulers",
    "used",
    "always",
    "look",
    "series",
    "blog",
    "also",
    "checking",
    "yarn",
    "schedulers",
    "search",
    "uh",
    "hadoop",
    "definitive",
    "guide",
    "could",
    "give",
    "information",
    "looks",
    "look",
    "hadoop",
    "definitive",
    "guide",
    "look",
    "book",
    "talks",
    "different",
    "resources",
    "mentioned",
    "could",
    "fee",
    "scheduler",
    "first",
    "first",
    "basically",
    "means",
    "long",
    "running",
    "application",
    "submitted",
    "cluster",
    "small",
    "running",
    "applications",
    "wait",
    "way",
    "would",
    "preferred",
    "option",
    "look",
    "fifo",
    "scheduler",
    "look",
    "capacity",
    "scheduler",
    "basically",
    "means",
    "could",
    "different",
    "queues",
    "created",
    "queues",
    "would",
    "resources",
    "allocated",
    "could",
    "production",
    "queue",
    "production",
    "jobs",
    "running",
    "particular",
    "queue",
    "fixed",
    "amount",
    "resources",
    "allocated",
    "could",
    "development",
    "queue",
    "development",
    "jobs",
    "running",
    "running",
    "parallel",
    "could",
    "also",
    "look",
    "fair",
    "scheduler",
    "basically",
    "means",
    "multiple",
    "applications",
    "could",
    "running",
    "cluster",
    "however",
    "would",
    "fair",
    "share",
    "say",
    "fair",
    "share",
    "brief",
    "means",
    "given",
    "50",
    "percent",
    "resources",
    "queue",
    "production",
    "50",
    "percent",
    "resources",
    "queue",
    "development",
    "running",
    "parallel",
    "would",
    "access",
    "50",
    "percent",
    "cluster",
    "resources",
    "however",
    "one",
    "queue",
    "unutilized",
    "second",
    "queue",
    "utilize",
    "cluster",
    "resources",
    "look",
    "fair",
    "scheduling",
    "part",
    "also",
    "shows",
    "allocations",
    "given",
    "learn",
    "schedulers",
    "cues",
    "used",
    "managing",
    "multiple",
    "applications",
    "spend",
    "time",
    "looking",
    "ways",
    "quick",
    "ways",
    "interacting",
    "yarn",
    "form",
    "demo",
    "understand",
    "learn",
    "yarn",
    "works",
    "look",
    "particular",
    "cluster",
    "designated",
    "cluster",
    "used",
    "could",
    "using",
    "similar",
    "kind",
    "commands",
    "apache",
    "based",
    "cluster",
    "cloudera",
    "quick",
    "start",
    "vm",
    "already",
    "cloud",
    "error",
    "hortonworks",
    "cluster",
    "running",
    "different",
    "ways",
    "interact",
    "yarn",
    "look",
    "information",
    "one",
    "basically",
    "looking",
    "admin",
    "console",
    "would",
    "look",
    "cloud",
    "data",
    "manager",
    "basically",
    "admin",
    "console",
    "cloudera",
    "distribution",
    "hadoop",
    "similarly",
    "could",
    "hortonworks",
    "cluster",
    "access",
    "admin",
    "console",
    "even",
    "read",
    "access",
    "cluster",
    "admin",
    "console",
    "search",
    "yarn",
    "service",
    "running",
    "click",
    "yarn",
    "service",
    "gives",
    "different",
    "tabs",
    "instances",
    "tells",
    "basically",
    "different",
    "roles",
    "yarn",
    "service",
    "running",
    "multiple",
    "node",
    "managers",
    "show",
    "stop",
    "status",
    "nothing",
    "worry",
    "three",
    "six",
    "node",
    "managers",
    "resource",
    "manager",
    "one",
    "also",
    "high",
    "availability",
    "active",
    "standby",
    "also",
    "job",
    "history",
    "server",
    "would",
    "show",
    "applications",
    "completed",
    "look",
    "yarn",
    "configurations",
    "explaining",
    "always",
    "look",
    "properties",
    "related",
    "allocation",
    "search",
    "course",
    "show",
    "properties",
    "talk",
    "allocations",
    "see",
    "looking",
    "yarn",
    "app",
    "mapreduce",
    "application",
    "master",
    "resource",
    "cpu",
    "course",
    "cpu",
    "course",
    "allocated",
    "mapreduce",
    "map",
    "task",
    "reduce",
    "task",
    "looking",
    "yarn",
    "node",
    "manager",
    "resource",
    "cpu",
    "course",
    "basically",
    "says",
    "every",
    "node",
    "manager",
    "every",
    "node",
    "would",
    "allocated",
    "six",
    "cpu",
    "cores",
    "container",
    "sizing",
    "minimum",
    "allocation",
    "one",
    "cpu",
    "core",
    "maximum",
    "could",
    "two",
    "cpu",
    "cores",
    "similarly",
    "could",
    "also",
    "searching",
    "memory",
    "allocation",
    "could",
    "scroll",
    "see",
    "kind",
    "memory",
    "allocation",
    "done",
    "node",
    "manager",
    "look",
    "give",
    "information",
    "node",
    "manager",
    "basically",
    "says",
    "container",
    "minimum",
    "allocation",
    "2",
    "gb",
    "maximum",
    "3",
    "gb",
    "look",
    "node",
    "manager",
    "given",
    "25",
    "gb",
    "per",
    "node",
    "combination",
    "memory",
    "cpu",
    "cores",
    "total",
    "amount",
    "resources",
    "allocated",
    "every",
    "node",
    "manager",
    "always",
    "look",
    "applications",
    "tab",
    "would",
    "show",
    "us",
    "different",
    "applications",
    "submitted",
    "yarn",
    "example",
    "right",
    "see",
    "spark",
    "application",
    "running",
    "basically",
    "user",
    "using",
    "spark",
    "shell",
    "triggered",
    "application",
    "spark",
    "running",
    "yarn",
    "look",
    "different",
    "applications",
    "workload",
    "information",
    "always",
    "search",
    "based",
    "number",
    "days",
    "many",
    "applications",
    "run",
    "always",
    "go",
    "web",
    "ui",
    "searching",
    "resource",
    "manager",
    "web",
    "ui",
    "access",
    "give",
    "overall",
    "information",
    "cluster",
    "basically",
    "says",
    "100",
    "gb",
    "memory",
    "allocated",
    "could",
    "say",
    "25",
    "gb",
    "per",
    "node",
    "four",
    "node",
    "managers",
    "running",
    "24",
    "cores",
    "six",
    "cores",
    "per",
    "node",
    "look",
    "nodes",
    "could",
    "get",
    "information",
    "tells",
    "four",
    "node",
    "managers",
    "running",
    "node",
    "managers",
    "basically",
    "25",
    "gb",
    "memory",
    "allocated",
    "per",
    "node",
    "six",
    "scores",
    "portion",
    "utilized",
    "always",
    "look",
    "scheduler",
    "give",
    "us",
    "information",
    "kind",
    "scheduler",
    "allocated",
    "basically",
    "see",
    "root",
    "cue",
    "within",
    "root",
    "default",
    "queue",
    "basically",
    "user",
    "skew",
    "based",
    "different",
    "users",
    "always",
    "scroll",
    "give",
    "us",
    "information",
    "fair",
    "share",
    "see",
    "root",
    "dot",
    "default",
    "50",
    "percent",
    "resources",
    "queue",
    "also",
    "50",
    "percent",
    "resources",
    "also",
    "gives",
    "idea",
    "fair",
    "scheduler",
    "used",
    "always",
    "confirm",
    "using",
    "fair",
    "scheduler",
    "capacity",
    "scheduler",
    "takes",
    "care",
    "location",
    "search",
    "scheduler",
    "give",
    "understanding",
    "kind",
    "scheduler",
    "used",
    "allocations",
    "given",
    "particular",
    "scheduler",
    "fair",
    "scheduler",
    "shows",
    "root",
    "root",
    "q",
    "given",
    "hundred",
    "percent",
    "capacity",
    "within",
    "default",
    "also",
    "takes",
    "hundred",
    "percent",
    "understand",
    "yarn",
    "looking",
    "yarn",
    "web",
    "ui",
    "looking",
    "configurations",
    "look",
    "applications",
    "always",
    "look",
    "different",
    "actions",
    "since",
    "admin",
    "access",
    "information",
    "download",
    "client",
    "configuration",
    "always",
    "look",
    "history",
    "server",
    "give",
    "us",
    "information",
    "applications",
    "successfully",
    "completed",
    "yarn",
    "ui",
    "also",
    "going",
    "hue",
    "web",
    "interface",
    "web",
    "interface",
    "also",
    "basically",
    "allows",
    "look",
    "jobs",
    "click",
    "hue",
    "web",
    "ui",
    "access",
    "show",
    "way",
    "get",
    "hue",
    "graphical",
    "user",
    "interface",
    "mainly",
    "comes",
    "cloud",
    "error",
    "also",
    "configure",
    "apache",
    "hortonworks",
    "different",
    "way",
    "giving",
    "web",
    "ui",
    "access",
    "click",
    "get",
    "hue",
    "also",
    "one",
    "way",
    "look",
    "yarn",
    "look",
    "jobs",
    "running",
    "issues",
    "web",
    "interfaces",
    "either",
    "look",
    "yarn",
    "web",
    "ui",
    "hue",
    "something",
    "called",
    "job",
    "browser",
    "also",
    "give",
    "information",
    "different",
    "applications",
    "might",
    "run",
    "remove",
    "one",
    "basically",
    "give",
    "list",
    "different",
    "kind",
    "jobs",
    "workflows",
    "run",
    "either",
    "spark",
    "based",
    "application",
    "map",
    "reduce",
    "coming",
    "hive",
    "list",
    "applications",
    "says",
    "mapreduce",
    "spark",
    "something",
    "killed",
    "something",
    "successful",
    "basically",
    "probably",
    "hive",
    "query",
    "triggered",
    "mapreduce",
    "job",
    "click",
    "application",
    "tells",
    "many",
    "tasks",
    "run",
    "map",
    "task",
    "ran",
    "get",
    "metadata",
    "information",
    "obviously",
    "also",
    "look",
    "yarn",
    "ui",
    "look",
    "applications",
    "give",
    "detailed",
    "information",
    "map",
    "reduce",
    "many",
    "map",
    "reduce",
    "tasks",
    "run",
    "different",
    "counters",
    "spark",
    "application",
    "let",
    "follow",
    "spark",
    "history",
    "server",
    "job",
    "history",
    "server",
    "always",
    "use",
    "web",
    "ui",
    "look",
    "jobs",
    "finding",
    "lot",
    "useful",
    "information",
    "also",
    "looking",
    "many",
    "resources",
    "used",
    "happened",
    "job",
    "successful",
    "fail",
    "job",
    "status",
    "apart",
    "web",
    "ui",
    "always",
    "might",
    "access",
    "particular",
    "cluster",
    "production",
    "cluster",
    "might",
    "restrictions",
    "organization",
    "might",
    "access",
    "given",
    "users",
    "graphical",
    "user",
    "interface",
    "like",
    "might",
    "would",
    "access",
    "cloud",
    "era",
    "manager",
    "admin",
    "console",
    "probably",
    "organization",
    "managing",
    "multiple",
    "clusters",
    "using",
    "admin",
    "console",
    "one",
    "way",
    "would",
    "access",
    "web",
    "console",
    "basically",
    "edge",
    "node",
    "client",
    "machine",
    "connect",
    "cluster",
    "working",
    "let",
    "log",
    "give",
    "different",
    "commands",
    "command",
    "line",
    "access",
    "different",
    "details",
    "always",
    "check",
    "typing",
    "map",
    "red",
    "gives",
    "different",
    "options",
    "look",
    "map",
    "reduce",
    "related",
    "jobs",
    "look",
    "different",
    "queues",
    "queues",
    "configured",
    "look",
    "history",
    "server",
    "also",
    "admin",
    "stuff",
    "provided",
    "access",
    "example",
    "say",
    "map",
    "red",
    "q",
    "basically",
    "gives",
    "option",
    "says",
    "would",
    "want",
    "would",
    "want",
    "list",
    "queues",
    "want",
    "information",
    "particular",
    "queue",
    "let",
    "try",
    "list",
    "give",
    "different",
    "queues",
    "used",
    "know",
    "per",
    "user",
    "queue",
    "dynamically",
    "gets",
    "created",
    "root",
    "dot",
    "users",
    "gives",
    "status",
    "queue",
    "capacity",
    "kind",
    "maximum",
    "capacity",
    "capping",
    "done",
    "get",
    "see",
    "huge",
    "list",
    "queues",
    "dynamically",
    "get",
    "configured",
    "environment",
    "also",
    "look",
    "root",
    "dot",
    "default",
    "could",
    "also",
    "picked",
    "one",
    "particular",
    "queue",
    "could",
    "said",
    "show",
    "jobs",
    "could",
    "also",
    "give",
    "yarn",
    "command",
    "let",
    "clear",
    "screen",
    "say",
    "yarn",
    "shows",
    "different",
    "options",
    "apart",
    "web",
    "interface",
    "something",
    "like",
    "web",
    "ui",
    "apart",
    "yarn",
    "web",
    "ui",
    "could",
    "also",
    "looking",
    "information",
    "using",
    "yarn",
    "commands",
    "list",
    "commands",
    "check",
    "type",
    "yarn",
    "version",
    "would",
    "want",
    "see",
    "version",
    "basically",
    "gives",
    "information",
    "hadoop",
    "version",
    "used",
    "vendor",
    "specific",
    "distribution",
    "version",
    "see",
    "working",
    "cloudera",
    "distribution",
    "internally",
    "using",
    "hadoop",
    "similarly",
    "yarn",
    "application",
    "list",
    "give",
    "could",
    "exhaustive",
    "list",
    "applications",
    "running",
    "applications",
    "completed",
    "see",
    "applications",
    "right",
    "probably",
    "applications",
    "running",
    "also",
    "shows",
    "could",
    "pulling",
    "different",
    "status",
    "submitted",
    "accepted",
    "running",
    "could",
    "also",
    "say",
    "would",
    "want",
    "see",
    "services",
    "finished",
    "running",
    "could",
    "say",
    "yarn",
    "application",
    "list",
    "app",
    "states",
    "finished",
    "could",
    "using",
    "command",
    "could",
    "say",
    "yarn",
    "application",
    "list",
    "would",
    "want",
    "see",
    "app",
    "states",
    "gives",
    "applications",
    "finished",
    "would",
    "want",
    "list",
    "applications",
    "finished",
    "might",
    "applications",
    "succeeded",
    "right",
    "huge",
    "list",
    "application",
    "coming",
    "history",
    "server",
    "basically",
    "showing",
    "huge",
    "list",
    "applications",
    "completed",
    "one",
    "way",
    "could",
    "also",
    "searching",
    "one",
    "particular",
    "application",
    "would",
    "want",
    "search",
    "particular",
    "application",
    "application",
    "id",
    "could",
    "always",
    "grip",
    "simple",
    "way",
    "could",
    "say",
    "basically",
    "let",
    "pick",
    "one",
    "would",
    "want",
    "search",
    "would",
    "want",
    "details",
    "could",
    "obviously",
    "calling",
    "previous",
    "command",
    "could",
    "grip",
    "want",
    "would",
    "want",
    "search",
    "application",
    "list",
    "applications",
    "shows",
    "application",
    "could",
    "pull",
    "information",
    "application",
    "could",
    "look",
    "log",
    "files",
    "particular",
    "application",
    "giving",
    "application",
    "id",
    "could",
    "say",
    "yarn",
    "logs",
    "option",
    "every",
    "time",
    "anytime",
    "doubt",
    "hit",
    "enter",
    "always",
    "give",
    "options",
    "need",
    "give",
    "particular",
    "command",
    "say",
    "yarn",
    "logs",
    "application",
    "id",
    "copied",
    "application",
    "id",
    "could",
    "give",
    "could",
    "give",
    "options",
    "like",
    "app",
    "owner",
    "would",
    "want",
    "get",
    "container",
    "details",
    "would",
    "want",
    "check",
    "particular",
    "node",
    "giving",
    "yarn",
    "logs",
    "pointing",
    "application",
    "id",
    "says",
    "log",
    "aggregation",
    "completed",
    "might",
    "might",
    "application",
    "triggered",
    "based",
    "particular",
    "interactive",
    "shell",
    "based",
    "particular",
    "query",
    "log",
    "existing",
    "particular",
    "application",
    "always",
    "look",
    "status",
    "application",
    "kill",
    "application",
    "saying",
    "yarn",
    "yarn",
    "application",
    "would",
    "want",
    "application",
    "hit",
    "enter",
    "shows",
    "different",
    "options",
    "tried",
    "app",
    "states",
    "could",
    "always",
    "look",
    "last",
    "one",
    "says",
    "status",
    "status",
    "could",
    "giving",
    "application",
    "id",
    "tells",
    "status",
    "application",
    "connects",
    "resource",
    "manager",
    "tells",
    "application",
    "id",
    "kind",
    "application",
    "ran",
    "queue",
    "job",
    "running",
    "start",
    "end",
    "time",
    "progress",
    "status",
    "finished",
    "succeeded",
    "basically",
    "gives",
    "also",
    "information",
    "application",
    "master",
    "running",
    "gives",
    "information",
    "find",
    "job",
    "details",
    "history",
    "server",
    "interested",
    "looking",
    "also",
    "gives",
    "aggregate",
    "resource",
    "allocation",
    "tells",
    "much",
    "gb",
    "memory",
    "many",
    "c4",
    "seconds",
    "used",
    "basically",
    "looking",
    "application",
    "details",
    "could",
    "kill",
    "application",
    "application",
    "already",
    "running",
    "could",
    "always",
    "yarn",
    "application",
    "minus",
    "skill",
    "could",
    "giving",
    "application",
    "could",
    "try",
    "killing",
    "however",
    "would",
    "say",
    "application",
    "already",
    "finished",
    "application",
    "running",
    "application",
    "already",
    "given",
    "application",
    "id",
    "resource",
    "manager",
    "could",
    "kill",
    "also",
    "say",
    "yarn",
    "node",
    "list",
    "give",
    "list",
    "node",
    "managers",
    "looking",
    "yarn",
    "web",
    "ui",
    "pulling",
    "information",
    "get",
    "kind",
    "information",
    "command",
    "line",
    "always",
    "remember",
    "always",
    "try",
    "well",
    "accustomed",
    "command",
    "line",
    "various",
    "things",
    "command",
    "line",
    "obviously",
    "web",
    "uis",
    "help",
    "graphical",
    "interface",
    "easily",
    "able",
    "access",
    "things",
    "could",
    "also",
    "starting",
    "resource",
    "manager",
    "would",
    "already",
    "running",
    "cluster",
    "could",
    "give",
    "yarn",
    "resource",
    "manager",
    "could",
    "get",
    "logs",
    "resource",
    "manager",
    "would",
    "want",
    "giving",
    "yarn",
    "demon",
    "try",
    "say",
    "yarn",
    "demon",
    "says",
    "find",
    "demon",
    "give",
    "something",
    "like",
    "get",
    "level",
    "give",
    "node",
    "ip",
    "address",
    "want",
    "check",
    "logs",
    "resource",
    "manager",
    "could",
    "giving",
    "get",
    "cloud",
    "error",
    "manager",
    "look",
    "nodes",
    "ip",
    "address",
    "could",
    "giving",
    "command",
    "something",
    "like",
    "basically",
    "gives",
    "level",
    "log",
    "got",
    "resource",
    "manager",
    "address",
    "web",
    "ui",
    "giving",
    "command",
    "look",
    "demand",
    "log",
    "basically",
    "says",
    "would",
    "want",
    "look",
    "resource",
    "manager",
    "related",
    "log",
    "log",
    "4j",
    "used",
    "logging",
    "kind",
    "level",
    "set",
    "info",
    "changed",
    "way",
    "logging",
    "information",
    "try",
    "commands",
    "also",
    "yarn",
    "example",
    "looking",
    "yarn",
    "rm",
    "admin",
    "always",
    "yarn",
    "rm",
    "admin",
    "basically",
    "gives",
    "lot",
    "informations",
    "like",
    "refreshing",
    "cues",
    "refreshing",
    "nodes",
    "basically",
    "looking",
    "admin",
    "acls",
    "getting",
    "groups",
    "could",
    "always",
    "get",
    "group",
    "names",
    "particular",
    "user",
    "could",
    "search",
    "particular",
    "user",
    "yarn",
    "hdfs",
    "could",
    "say",
    "would",
    "want",
    "get",
    "groups",
    "could",
    "searching",
    "say",
    "username",
    "hdfs",
    "tells",
    "sdfs",
    "belongs",
    "hadoop",
    "group",
    "similarly",
    "could",
    "search",
    "say",
    "map",
    "red",
    "could",
    "search",
    "yarn",
    "service",
    "related",
    "users",
    "automatically",
    "get",
    "created",
    "pull",
    "information",
    "related",
    "always",
    "refresh",
    "nodes",
    "kind",
    "command",
    "mainly",
    "done",
    "internally",
    "useful",
    "commissioning",
    "decommissioning",
    "case",
    "cloudera",
    "hortonworks",
    "kind",
    "cluster",
    "would",
    "manually",
    "giving",
    "command",
    "commissioning",
    "decommissioning",
    "admin",
    "console",
    "administrator",
    "could",
    "restart",
    "services",
    "affected",
    "take",
    "care",
    "working",
    "apache",
    "cluster",
    "commissioning",
    "decommissioning",
    "would",
    "using",
    "two",
    "commands",
    "refresh",
    "nodes",
    "basically",
    "refreshing",
    "nodes",
    "used",
    "processing",
    "similarly",
    "could",
    "command",
    "refresh",
    "nodes",
    "comes",
    "sdfs",
    "different",
    "options",
    "use",
    "yarn",
    "command",
    "line",
    "could",
    "also",
    "using",
    "curl",
    "commands",
    "get",
    "information",
    "cluster",
    "giving",
    "curl",
    "minus",
    "x",
    "basically",
    "pointing",
    "resource",
    "manager",
    "web",
    "ui",
    "address",
    "would",
    "like",
    "print",
    "cluster",
    "related",
    "metrics",
    "could",
    "simply",
    "basically",
    "gives",
    "high",
    "level",
    "information",
    "many",
    "applications",
    "submitted",
    "many",
    "pending",
    "reserved",
    "resources",
    "available",
    "amount",
    "memory",
    "cpu",
    "cores",
    "information",
    "similarly",
    "using",
    "curl",
    "commands",
    "get",
    "information",
    "like",
    "scheduler",
    "information",
    "would",
    "replace",
    "metrics",
    "scheduler",
    "could",
    "get",
    "information",
    "different",
    "queues",
    "huge",
    "list",
    "cancel",
    "would",
    "give",
    "list",
    "queues",
    "allocated",
    "resources",
    "allocated",
    "queue",
    "could",
    "also",
    "get",
    "cluster",
    "information",
    "application",
    "ids",
    "status",
    "running",
    "applications",
    "running",
    "yarn",
    "would",
    "replace",
    "last",
    "bit",
    "would",
    "say",
    "would",
    "want",
    "look",
    "applications",
    "gives",
    "huge",
    "list",
    "applications",
    "grip",
    "filtering",
    "specific",
    "application",
    "related",
    "information",
    "similarly",
    "looking",
    "nodes",
    "always",
    "looking",
    "node",
    "specific",
    "information",
    "gives",
    "many",
    "nodes",
    "could",
    "mainly",
    "used",
    "application",
    "wants",
    "web",
    "application",
    "wants",
    "use",
    "curl",
    "command",
    "would",
    "want",
    "get",
    "information",
    "cluster",
    "http",
    "interface",
    "comes",
    "application",
    "basically",
    "try",
    "running",
    "simple",
    "sample",
    "map",
    "reduced",
    "job",
    "could",
    "triggered",
    "yarn",
    "would",
    "use",
    "resources",
    "look",
    "application",
    "looking",
    "specific",
    "directory",
    "one",
    "lot",
    "files",
    "directories",
    "could",
    "pick",
    "one",
    "could",
    "using",
    "simple",
    "example",
    "processing",
    "let",
    "take",
    "file",
    "file",
    "could",
    "run",
    "simple",
    "word",
    "count",
    "could",
    "running",
    "hive",
    "query",
    "triggers",
    "mapreduce",
    "job",
    "could",
    "even",
    "run",
    "spark",
    "application",
    "would",
    "show",
    "application",
    "running",
    "cluster",
    "example",
    "would",
    "say",
    "spark",
    "shell",
    "know",
    "interactive",
    "way",
    "working",
    "spark",
    "internally",
    "triggers",
    "spark",
    "submit",
    "runs",
    "application",
    "spark",
    "shell",
    "default",
    "contact",
    "yarn",
    "gets",
    "application",
    "id",
    "running",
    "yarn",
    "master",
    "yarn",
    "access",
    "interactive",
    "way",
    "working",
    "spark",
    "go",
    "look",
    "applications",
    "able",
    "see",
    "application",
    "started",
    "shows",
    "application",
    "3827",
    "started",
    "yarn",
    "also",
    "look",
    "yarn",
    "ui",
    "shows",
    "application",
    "started",
    "basically",
    "one",
    "running",
    "container",
    "one",
    "cpu",
    "core",
    "allocated",
    "2gb",
    "ram",
    "progress",
    "although",
    "anything",
    "always",
    "look",
    "application",
    "yarn",
    "ui",
    "mentioned",
    "applications",
    "tab",
    "within",
    "yarn",
    "services",
    "gives",
    "us",
    "information",
    "even",
    "click",
    "application",
    "follow",
    "see",
    "information",
    "given",
    "access",
    "simple",
    "application",
    "triggered",
    "using",
    "spark",
    "shell",
    "similarly",
    "basically",
    "running",
    "mapreduce",
    "run",
    "mapreduce",
    "say",
    "hadoop",
    "jar",
    "basically",
    "needs",
    "class",
    "look",
    "default",
    "path",
    "opt",
    "cloud",
    "error",
    "parcels",
    "cdh",
    "lib",
    "hadoop",
    "map",
    "reduce",
    "hadoop",
    "mapreduce",
    "examples",
    "look",
    "particular",
    "jar",
    "file",
    "hit",
    "enter",
    "shows",
    "different",
    "classes",
    "part",
    "jar",
    "would",
    "like",
    "use",
    "word",
    "count",
    "could",
    "give",
    "could",
    "say",
    "word",
    "count",
    "remember",
    "could",
    "run",
    "job",
    "particular",
    "queue",
    "giving",
    "argument",
    "could",
    "say",
    "minus",
    "map",
    "red",
    "dot",
    "job",
    "dot",
    "q",
    "dot",
    "name",
    "point",
    "job",
    "particular",
    "queue",
    "even",
    "give",
    "different",
    "arguments",
    "saying",
    "would",
    "want",
    "mapreduce",
    "output",
    "compressed",
    "want",
    "stored",
    "particular",
    "directory",
    "word",
    "count",
    "basically",
    "pointing",
    "particular",
    "input",
    "path",
    "output",
    "getting",
    "stored",
    "directory",
    "need",
    "choose",
    "say",
    "output",
    "new",
    "submit",
    "job",
    "submitted",
    "job",
    "connects",
    "resource",
    "manager",
    "basically",
    "gets",
    "job",
    "id",
    "gets",
    "application",
    "id",
    "shows",
    "track",
    "application",
    "always",
    "go",
    "yarn",
    "ui",
    "looking",
    "application",
    "resources",
    "using",
    "application",
    "big",
    "one",
    "already",
    "completed",
    "triggered",
    "one",
    "map",
    "task",
    "launched",
    "one",
    "reduced",
    "task",
    "working",
    "around",
    "12",
    "466",
    "records",
    "output",
    "map",
    "many",
    "number",
    "output",
    "records",
    "taken",
    "combiner",
    "finally",
    "reducer",
    "basically",
    "gives",
    "output",
    "application",
    "completed",
    "could",
    "looking",
    "yarn",
    "ui",
    "job",
    "completed",
    "might",
    "see",
    "application",
    "shows",
    "word",
    "count",
    "ran",
    "also",
    "shows",
    "previous",
    "spark",
    "shell",
    "job",
    "shows",
    "application",
    "completed",
    "would",
    "want",
    "information",
    "click",
    "go",
    "history",
    "server",
    "given",
    "access",
    "directly",
    "go",
    "history",
    "server",
    "web",
    "ui",
    "application",
    "shows",
    "shows",
    "many",
    "map",
    "reduce",
    "tasks",
    "running",
    "click",
    "particular",
    "application",
    "basically",
    "gives",
    "information",
    "map",
    "reduce",
    "tasks",
    "look",
    "different",
    "counters",
    "application",
    "right",
    "always",
    "look",
    "map",
    "specific",
    "tasks",
    "always",
    "look",
    "one",
    "particular",
    "task",
    "node",
    "running",
    "looking",
    "complete",
    "application",
    "log",
    "always",
    "click",
    "logs",
    "click",
    "full",
    "log",
    "gives",
    "information",
    "always",
    "look",
    "application",
    "give",
    "information",
    "app",
    "master",
    "launched",
    "could",
    "search",
    "word",
    "container",
    "could",
    "see",
    "job",
    "needs",
    "one",
    "multiple",
    "containers",
    "could",
    "say",
    "container",
    "requested",
    "could",
    "see",
    "container",
    "allocated",
    "see",
    "container",
    "size",
    "basically",
    "task",
    "moves",
    "initializing",
    "running",
    "container",
    "finally",
    "even",
    "search",
    "release",
    "tell",
    "container",
    "released",
    "always",
    "look",
    "log",
    "information",
    "interact",
    "yarn",
    "interact",
    "command",
    "line",
    "look",
    "information",
    "using",
    "yarn",
    "web",
    "ui",
    "name",
    "richard",
    "kirschner",
    "simply",
    "learn",
    "team",
    "get",
    "certified",
    "get",
    "ahead",
    "today",
    "going",
    "dive",
    "scoop",
    "one",
    "many",
    "features",
    "hadoop",
    "ecosystem",
    "hadoop",
    "file",
    "system",
    "today",
    "going",
    "cover",
    "need",
    "scoop",
    "scoop",
    "scoop",
    "features",
    "scoop",
    "architecture",
    "scoop",
    "import",
    "scoop",
    "export",
    "scoop",
    "processing",
    "finally",
    "little",
    "demo",
    "scoop",
    "see",
    "looks",
    "like",
    "need",
    "scoop",
    "come",
    "big",
    "data",
    "hadoop",
    "file",
    "system",
    "processing",
    "huge",
    "volumes",
    "data",
    "requires",
    "loading",
    "data",
    "diverse",
    "sources",
    "hadoop",
    "cluster",
    "see",
    "data",
    "processing",
    "process",
    "loading",
    "data",
    "heterogeneous",
    "sources",
    "comes",
    "set",
    "challenges",
    "challenges",
    "maintaining",
    "data",
    "consistency",
    "ensuring",
    "efficient",
    "utilization",
    "resources",
    "especially",
    "talking",
    "big",
    "data",
    "certainly",
    "use",
    "resources",
    "importing",
    "terabytes",
    "petabytes",
    "data",
    "course",
    "time",
    "loading",
    "bulk",
    "data",
    "hadoop",
    "possible",
    "one",
    "big",
    "challenges",
    "came",
    "first",
    "hadoop",
    "file",
    "system",
    "going",
    "loading",
    "data",
    "using",
    "script",
    "slow",
    "words",
    "write",
    "script",
    "whatever",
    "language",
    "would",
    "slowly",
    "load",
    "piece",
    "parse",
    "solution",
    "scoop",
    "scooped",
    "helped",
    "overcoming",
    "challenges",
    "traditional",
    "approach",
    "could",
    "lead",
    "bulk",
    "data",
    "rdbms",
    "hadoop",
    "easily",
    "thank",
    "enterprise",
    "server",
    "want",
    "take",
    "mysql",
    "sql",
    "want",
    "bring",
    "data",
    "hadoop",
    "warehouse",
    "data",
    "filing",
    "system",
    "scoop",
    "comes",
    "exactly",
    "scoop",
    "scoop",
    "tool",
    "used",
    "transfer",
    "bulk",
    "data",
    "hadoop",
    "external",
    "data",
    "stores",
    "relational",
    "databases",
    "mysql",
    "server",
    "microsoft",
    "sql",
    "server",
    "sql",
    "server",
    "scoop",
    "equals",
    "sql",
    "plus",
    "hadoop",
    "see",
    "rdbms",
    "data",
    "stored",
    "scoop",
    "middle",
    "ground",
    "brings",
    "import",
    "hadoop",
    "file",
    "system",
    "also",
    "one",
    "features",
    "goes",
    "grabs",
    "data",
    "hadoop",
    "exports",
    "back",
    "rdbms",
    "let",
    "take",
    "look",
    "scoop",
    "features",
    "scoop",
    "features",
    "parallel",
    "import",
    "export",
    "import",
    "results",
    "sql",
    "query",
    "connectors",
    "major",
    "rdbms",
    "databases",
    "kerberos",
    "security",
    "integration",
    "provides",
    "full",
    "incremental",
    "load",
    "look",
    "parallel",
    "import",
    "export",
    "scoop",
    "uses",
    "yarn",
    "yet",
    "another",
    "resource",
    "negotiator",
    "framework",
    "import",
    "export",
    "data",
    "provides",
    "fault",
    "tolerance",
    "top",
    "parallelism",
    "scoop",
    "allows",
    "us",
    "import",
    "result",
    "returned",
    "sql",
    "carry",
    "hadoop",
    "file",
    "system",
    "hdfs",
    "see",
    "import",
    "results",
    "sql",
    "query",
    "come",
    "scoop",
    "provides",
    "connectors",
    "multiple",
    "relational",
    "database",
    "management",
    "system",
    "rdbms",
    "databases",
    "mysql",
    "microsoft",
    "sql",
    "server",
    "connectors",
    "major",
    "rdbms",
    "databases",
    "scoop",
    "supports",
    "kerberos",
    "computer",
    "network",
    "authentication",
    "protocol",
    "allows",
    "nodes",
    "communicating",
    "network",
    "prove",
    "identity",
    "one",
    "another",
    "secure",
    "manner",
    "scoop",
    "load",
    "whole",
    "table",
    "parts",
    "table",
    "single",
    "command",
    "hence",
    "supports",
    "full",
    "incremental",
    "load",
    "let",
    "dig",
    "little",
    "deeper",
    "scoop",
    "architecture",
    "client",
    "case",
    "hooded",
    "wizard",
    "behind",
    "laptop",
    "never",
    "know",
    "going",
    "accessing",
    "hadoop",
    "cluster",
    "client",
    "comes",
    "sends",
    "command",
    "goes",
    "scoop",
    "client",
    "submits",
    "import",
    "export",
    "command",
    "import",
    "export",
    "data",
    "data",
    "different",
    "databases",
    "fetched",
    "scoop",
    "enterprise",
    "data",
    "warehouse",
    "document",
    "based",
    "systems",
    "connect",
    "connector",
    "data",
    "warehouse",
    "connector",
    "document",
    "based",
    "systems",
    "reaches",
    "two",
    "entities",
    "connector",
    "rdbms",
    "connectors",
    "help",
    "working",
    "range",
    "popular",
    "databases",
    "multiple",
    "mappers",
    "perform",
    "map",
    "tasks",
    "load",
    "data",
    "onto",
    "hdfs",
    "hadoop",
    "file",
    "system",
    "see",
    "map",
    "task",
    "remember",
    "hadoop",
    "hadoop",
    "based",
    "map",
    "reduce",
    "reducing",
    "data",
    "mapping",
    "accesses",
    "mappers",
    "opens",
    "multiple",
    "mappers",
    "parallel",
    "processing",
    "see",
    "hdfs",
    "hbase",
    "high",
    "target",
    "particular",
    "one",
    "similarly",
    "multiple",
    "map",
    "tests",
    "export",
    "data",
    "hdfs",
    "onto",
    "rdbms",
    "using",
    "scoop",
    "export",
    "command",
    "like",
    "import",
    "export",
    "using",
    "multiple",
    "map",
    "routines",
    "scoop",
    "import",
    "dbms",
    "data",
    "store",
    "folders",
    "maybe",
    "company",
    "database",
    "maybe",
    "archive",
    "google",
    "searches",
    "going",
    "whatever",
    "usually",
    "think",
    "scoop",
    "think",
    "sql",
    "think",
    "sequel",
    "server",
    "microsoft",
    "sql",
    "server",
    "kind",
    "setup",
    "gathers",
    "metadata",
    "see",
    "scoop",
    "import",
    "introspect",
    "database",
    "together",
    "metadata",
    "primary",
    "key",
    "information",
    "submits",
    "see",
    "submits",
    "map",
    "job",
    "remember",
    "talked",
    "mapreduce",
    "needs",
    "map",
    "side",
    "reducing",
    "data",
    "mapping",
    "scoop",
    "divides",
    "input",
    "data",
    "set",
    "splits",
    "uses",
    "individual",
    "map",
    "tests",
    "push",
    "splits",
    "hdfs",
    "right",
    "hadoop",
    "file",
    "system",
    "see",
    "right",
    "kind",
    "small",
    "depiction",
    "hadoop",
    "cluster",
    "scoop",
    "export",
    "going",
    "go",
    "direction",
    "direction",
    "hadoop",
    "file",
    "system",
    "storage",
    "hadoop",
    "cluster",
    "scoop",
    "job",
    "one",
    "clusters",
    "gets",
    "map",
    "mapper",
    "comes",
    "one",
    "computers",
    "data",
    "first",
    "step",
    "got",
    "gather",
    "metadata",
    "step",
    "one",
    "gather",
    "metadata",
    "step",
    "two",
    "submits",
    "map",
    "job",
    "introspect",
    "database",
    "gather",
    "metadata",
    "primary",
    "key",
    "information",
    "scoop",
    "divides",
    "input",
    "data",
    "set",
    "splits",
    "uses",
    "individual",
    "map",
    "tests",
    "push",
    "splits",
    "rdbms",
    "scoop",
    "export",
    "hadoop",
    "files",
    "back",
    "rdms",
    "tables",
    "think",
    "number",
    "different",
    "manners",
    "one",
    "would",
    "restoring",
    "backup",
    "hadoop",
    "file",
    "system",
    "enterprise",
    "machines",
    "certainly",
    "many",
    "others",
    "far",
    "exploring",
    "data",
    "data",
    "science",
    "dig",
    "little",
    "deeper",
    "scoop",
    "input",
    "connect",
    "jdbc",
    "url",
    "specified",
    "jdbc",
    "connect",
    "string",
    "connecting",
    "manager",
    "specify",
    "connection",
    "manager",
    "class",
    "use",
    "see",
    "driver",
    "class",
    "name",
    "manually",
    "specify",
    "jdbc",
    "driver",
    "class",
    "use",
    "hadoop",
    "map",
    "reduce",
    "home",
    "directory",
    "override",
    "hadoop",
    "mapped",
    "home",
    "username",
    "set",
    "authentication",
    "username",
    "course",
    "help",
    "print",
    "uses",
    "instructions",
    "export",
    "see",
    "specify",
    "jdbc",
    "connect",
    "string",
    "specify",
    "connection",
    "manager",
    "class",
    "use",
    "manually",
    "specify",
    "jdbc",
    "driver",
    "class",
    "use",
    "let",
    "know",
    "override",
    "hadoop",
    "map",
    "reduce",
    "home",
    "true",
    "set",
    "authentication",
    "username",
    "finally",
    "print",
    "help",
    "setup",
    "see",
    "format",
    "scoop",
    "pretty",
    "straightforward",
    "import",
    "export",
    "let",
    "uh",
    "continue",
    "path",
    "look",
    "scoop",
    "processing",
    "computer",
    "goes",
    "talk",
    "scoop",
    "processing",
    "first",
    "scoop",
    "runs",
    "hadoop",
    "cluster",
    "imports",
    "data",
    "rdbms",
    "nosql",
    "database",
    "hadoop",
    "file",
    "system",
    "remember",
    "might",
    "importing",
    "data",
    "rdbms",
    "might",
    "actually",
    "coming",
    "nosql",
    "many",
    "uses",
    "mappers",
    "slice",
    "incoming",
    "data",
    "multiple",
    "formats",
    "load",
    "data",
    "hdfs",
    "exports",
    "data",
    "back",
    "rdbms",
    "making",
    "sure",
    "schema",
    "data",
    "database",
    "maintained",
    "looked",
    "basic",
    "commands",
    "scoop",
    "scoop",
    "processing",
    "least",
    "basics",
    "far",
    "theory",
    "concerned",
    "let",
    "jump",
    "take",
    "look",
    "demo",
    "scoop",
    "demo",
    "going",
    "use",
    "cloudera",
    "quick",
    "start",
    "watching",
    "demos",
    "done",
    "see",
    "using",
    "pretty",
    "consistently",
    "certainly",
    "work",
    "horton",
    "sandbox",
    "also",
    "single",
    "node",
    "testing",
    "machine",
    "cloudera",
    "one",
    "um",
    "docker",
    "version",
    "instead",
    "virtualbox",
    "also",
    "set",
    "hadoop",
    "cluster",
    "plan",
    "little",
    "extra",
    "time",
    "admin",
    "actually",
    "pretty",
    "uh",
    "significant",
    "endeavor",
    "admin",
    "admitting",
    "linux",
    "machines",
    "long",
    "time",
    "know",
    "lot",
    "commands",
    "find",
    "admins",
    "takes",
    "two",
    "four",
    "hours",
    "first",
    "time",
    "go",
    "create",
    "virtual",
    "machine",
    "set",
    "hadoop",
    "case",
    "though",
    "learning",
    "getting",
    "set",
    "best",
    "start",
    "cloudera",
    "cloudera",
    "also",
    "includes",
    "install",
    "version",
    "mysql",
    "way",
    "install",
    "sql",
    "version",
    "importing",
    "data",
    "cloudera",
    "quick",
    "start",
    "see",
    "opens",
    "nice",
    "centos",
    "linux",
    "interface",
    "desktop",
    "setup",
    "really",
    "nice",
    "learnings",
    "looking",
    "command",
    "lines",
    "open",
    "default",
    "hue",
    "click",
    "hue",
    "kind",
    "fun",
    "little",
    "interface",
    "hue",
    "go",
    "query",
    "pick",
    "editor",
    "go",
    "right",
    "scoop",
    "going",
    "load",
    "scoop",
    "editor",
    "inner",
    "hue",
    "going",
    "switch",
    "command",
    "line",
    "want",
    "show",
    "actually",
    "hue",
    "interface",
    "reason",
    "like",
    "command",
    "line",
    "specifically",
    "computer",
    "runs",
    "much",
    "quicker",
    "command",
    "line",
    "run",
    "tends",
    "extra",
    "lag",
    "added",
    "layer",
    "going",
    "go",
    "ahead",
    "open",
    "command",
    "line",
    "second",
    "reason",
    "going",
    "need",
    "go",
    "ahead",
    "edit",
    "mysql",
    "something",
    "scoop",
    "otherwise",
    "anything",
    "going",
    "course",
    "zoom",
    "zoom",
    "increase",
    "size",
    "screen",
    "demo",
    "going",
    "use",
    "oracle",
    "virtualbox",
    "manager",
    "cloudera",
    "quickstart",
    "familiar",
    "another",
    "tutorial",
    "put",
    "send",
    "note",
    "youtube",
    "video",
    "let",
    "team",
    "know",
    "send",
    "link",
    "come",
    "visit",
    "creates",
    "linux",
    "box",
    "windows",
    "computer",
    "going",
    "linux",
    "cloudera",
    "version",
    "scoop",
    "also",
    "using",
    "mysql",
    "sql",
    "server",
    "inside",
    "cloudera",
    "virtualbox",
    "go",
    "hue",
    "editor",
    "going",
    "everything",
    "terminal",
    "window",
    "want",
    "aware",
    "hue",
    "editor",
    "go",
    "query",
    "editor",
    "see",
    "come",
    "scoop",
    "run",
    "scoop",
    "little",
    "exploration",
    "sql",
    "sql",
    "server",
    "way",
    "know",
    "data",
    "coming",
    "let",
    "go",
    "ahead",
    "open",
    "terminal",
    "window",
    "cloudera",
    "terminal",
    "window",
    "top",
    "click",
    "open",
    "let",
    "go",
    "ahead",
    "zoom",
    "go",
    "view",
    "zoom",
    "get",
    "sql",
    "server",
    "simply",
    "type",
    "mysql",
    "part",
    "depend",
    "setup",
    "cloudera",
    "quickstart",
    "comes",
    "username",
    "root",
    "password",
    "cloudera",
    "kind",
    "strange",
    "quirk",
    "put",
    "space",
    "minus",
    "u",
    "root",
    "minus",
    "p",
    "cloudera",
    "usually",
    "put",
    "minus",
    "capital",
    "p",
    "prompts",
    "password",
    "demo",
    "worry",
    "much",
    "knowing",
    "password",
    "go",
    "right",
    "sql",
    "server",
    "since",
    "standard",
    "password",
    "quick",
    "start",
    "see",
    "mysql",
    "going",
    "couple",
    "quick",
    "commands",
    "show",
    "databases",
    "follow",
    "semicolon",
    "standard",
    "shell",
    "commands",
    "knows",
    "end",
    "shell",
    "command",
    "see",
    "quick",
    "start",
    "cloudera",
    "quickstart",
    "mysql",
    "comes",
    "standard",
    "set",
    "databases",
    "like",
    "uzi",
    "uzi",
    "part",
    "hadoop",
    "others",
    "like",
    "customers",
    "employees",
    "stuff",
    "like",
    "demo",
    "purposes",
    "come",
    "standard",
    "setup",
    "people",
    "going",
    "first",
    "time",
    "database",
    "play",
    "really",
    "good",
    "us",
    "recreate",
    "databases",
    "see",
    "list",
    "retail",
    "underscore",
    "db",
    "simply",
    "uh",
    "use",
    "retail",
    "underscore",
    "db",
    "set",
    "default",
    "mysql",
    "want",
    "go",
    "ahead",
    "show",
    "tables",
    "show",
    "tables",
    "see",
    "database",
    "retail",
    "db",
    "database",
    "categories",
    "customers",
    "departments",
    "order",
    "items",
    "orders",
    "products",
    "number",
    "tables",
    "going",
    "go",
    "ahead",
    "use",
    "standard",
    "sql",
    "command",
    "hive",
    "language",
    "note",
    "remember",
    "hql",
    "also",
    "going",
    "select",
    "star",
    "everything",
    "departments",
    "departments",
    "table",
    "going",
    "list",
    "everything",
    "departments",
    "table",
    "see",
    "got",
    "six",
    "lines",
    "department",
    "id",
    "department",
    "name",
    "two",
    "fitness",
    "three",
    "footwear",
    "forth",
    "point",
    "go",
    "ahead",
    "exit",
    "kind",
    "nice",
    "data",
    "look",
    "flip",
    "back",
    "forth",
    "screens",
    "going",
    "open",
    "another",
    "terminal",
    "window",
    "go",
    "ahead",
    "zoom",
    "also",
    "important",
    "particular",
    "setup",
    "always",
    "kind",
    "fine",
    "fun",
    "know",
    "setup",
    "working",
    "host",
    "name",
    "go",
    "ahead",
    "type",
    "linux",
    "command",
    "hostname",
    "minus",
    "f",
    "see",
    "quick",
    "start",
    "cloudera",
    "surprise",
    "next",
    "command",
    "going",
    "little",
    "bit",
    "longer",
    "going",
    "first",
    "scoop",
    "command",
    "going",
    "two",
    "going",
    "list",
    "databases",
    "lists",
    "tables",
    "going",
    "take",
    "moment",
    "get",
    "bunch",
    "stuff",
    "going",
    "scoop",
    "list",
    "databases",
    "connect",
    "connect",
    "command",
    "need",
    "let",
    "know",
    "connecting",
    "going",
    "use",
    "jdbc",
    "standard",
    "one",
    "jdbc",
    "mysql",
    "see",
    "sql",
    "database",
    "started",
    "next",
    "part",
    "go",
    "look",
    "however",
    "created",
    "admin",
    "created",
    "mysql",
    "server",
    "certain",
    "setup",
    "go",
    "see",
    "usually",
    "list",
    "localhost",
    "see",
    "something",
    "like",
    "localhost",
    "sometimes",
    "lot",
    "different",
    "formats",
    "common",
    "either",
    "local",
    "host",
    "actual",
    "connection",
    "case",
    "want",
    "go",
    "ahead",
    "quick",
    "start",
    "3306",
    "quick",
    "start",
    "name",
    "local",
    "host",
    "database",
    "hosted",
    "set",
    "quick",
    "start",
    "um",
    "hadoop",
    "cloudera",
    "port",
    "3306",
    "coming",
    "coming",
    "uh",
    "path",
    "put",
    "password",
    "typically",
    "type",
    "password",
    "look",
    "password",
    "cloudera",
    "quickstart",
    "cloudera",
    "also",
    "let",
    "know",
    "username",
    "probably",
    "put",
    "minus",
    "capital",
    "actually",
    "prompt",
    "password",
    "leave",
    "prompt",
    "really",
    "matter",
    "care",
    "see",
    "password",
    "default",
    "one",
    "cloudera",
    "quickstart",
    "username",
    "simply",
    "root",
    "going",
    "put",
    "semicolon",
    "end",
    "full",
    "setup",
    "go",
    "ahead",
    "list",
    "databases",
    "see",
    "might",
    "get",
    "warnings",
    "run",
    "updates",
    "quick",
    "start",
    "suggest",
    "running",
    "updates",
    "either",
    "first",
    "time",
    "reformatting",
    "quickly",
    "pass",
    "see",
    "tables",
    "went",
    "go",
    "back",
    "previous",
    "window",
    "see",
    "tables",
    "match",
    "come",
    "databases",
    "see",
    "back",
    "cm",
    "customers",
    "employees",
    "databases",
    "match",
    "want",
    "go",
    "ahead",
    "list",
    "tables",
    "specific",
    "database",
    "let",
    "go",
    "ahead",
    "lazy",
    "typist",
    "put",
    "arrow",
    "see",
    "scoop",
    "list",
    "databases",
    "going",
    "go",
    "back",
    "change",
    "databases",
    "list",
    "tables",
    "want",
    "list",
    "tables",
    "connection",
    "connection",
    "except",
    "need",
    "know",
    "tables",
    "listing",
    "interesting",
    "fact",
    "create",
    "table",
    "without",
    "database",
    "left",
    "blank",
    "show",
    "open",
    "tables",
    "connected",
    "directly",
    "database",
    "database",
    "want",
    "right",
    "past",
    "last",
    "slash",
    "3306",
    "want",
    "put",
    "retail",
    "underscore",
    "db",
    "database",
    "going",
    "working",
    "go",
    "show",
    "tables",
    "listed",
    "database",
    "go",
    "got",
    "categories",
    "customers",
    "departments",
    "order",
    "items",
    "products",
    "flip",
    "back",
    "real",
    "quick",
    "thing",
    "categories",
    "customers",
    "departments",
    "order",
    "items",
    "let",
    "go",
    "ahead",
    "run",
    "first",
    "import",
    "command",
    "lazy",
    "typer",
    "going",
    "scoop",
    "instead",
    "list",
    "tables",
    "want",
    "go",
    "ahead",
    "import",
    "import",
    "command",
    "import",
    "command",
    "need",
    "tell",
    "exactly",
    "going",
    "import",
    "everything",
    "else",
    "importing",
    "retail",
    "db",
    "keep",
    "end",
    "going",
    "tag",
    "dash",
    "dash",
    "table",
    "tells",
    "us",
    "tell",
    "table",
    "importing",
    "going",
    "import",
    "departments",
    "go",
    "pretty",
    "straightforward",
    "nice",
    "see",
    "commands",
    "got",
    "connection",
    "change",
    "whatever",
    "database",
    "come",
    "password",
    "username",
    "going",
    "mysql",
    "server",
    "setup",
    "let",
    "know",
    "table",
    "entering",
    "let",
    "run",
    "going",
    "actually",
    "go",
    "mapper",
    "process",
    "hadoop",
    "mapping",
    "process",
    "takes",
    "data",
    "maps",
    "different",
    "parts",
    "setup",
    "hadoop",
    "saves",
    "data",
    "hadoop",
    "file",
    "system",
    "take",
    "moment",
    "zip",
    "kind",
    "skipped",
    "since",
    "running",
    "know",
    "designed",
    "run",
    "across",
    "cluster",
    "single",
    "node",
    "running",
    "single",
    "node",
    "going",
    "run",
    "slow",
    "even",
    "dedicate",
    "couple",
    "cores",
    "think",
    "put",
    "dedicated",
    "four",
    "cores",
    "one",
    "see",
    "right",
    "get",
    "end",
    "mapped",
    "information",
    "go",
    "go",
    "flip",
    "back",
    "hue",
    "hue",
    "top",
    "databases",
    "second",
    "icon",
    "hadoop",
    "file",
    "system",
    "go",
    "look",
    "hadoop",
    "file",
    "system",
    "see",
    "show",
    "underneath",
    "documents",
    "departments",
    "cloudera",
    "departments",
    "see",
    "always",
    "delay",
    "working",
    "hue",
    "like",
    "quick",
    "start",
    "issue",
    "necessarily",
    "running",
    "server",
    "running",
    "server",
    "pretty",
    "much",
    "run",
    "kind",
    "server",
    "interface",
    "still",
    "prefer",
    "terminal",
    "window",
    "still",
    "runs",
    "lot",
    "quicker",
    "flip",
    "back",
    "command",
    "line",
    "hadoop",
    "type",
    "hadoop",
    "fs",
    "list",
    "minus",
    "ls",
    "run",
    "see",
    "underneath",
    "hadoop",
    "file",
    "system",
    "departments",
    "added",
    "also",
    "hadoop",
    "fs",
    "kind",
    "interesting",
    "gone",
    "hadoop",
    "file",
    "system",
    "everything",
    "recognize",
    "going",
    "list",
    "contents",
    "departments",
    "see",
    "underneath",
    "departments",
    "uh",
    "part",
    "part",
    "m000",
    "interesting",
    "hadoop",
    "saves",
    "files",
    "file",
    "system",
    "hive",
    "directly",
    "import",
    "hive",
    "put",
    "hadoop",
    "file",
    "system",
    "depending",
    "would",
    "write",
    "schema",
    "hive",
    "look",
    "hadoop",
    "file",
    "system",
    "certainly",
    "visit",
    "hive",
    "tutorial",
    "information",
    "hive",
    "specific",
    "see",
    "different",
    "files",
    "forms",
    "part",
    "departments",
    "something",
    "like",
    "look",
    "contents",
    "one",
    "files",
    "fs",
    "minus",
    "ls",
    "number",
    "files",
    "simply",
    "full",
    "path",
    "user",
    "cloudera",
    "already",
    "know",
    "next",
    "one",
    "departments",
    "departments",
    "going",
    "put",
    "slash",
    "part",
    "star",
    "going",
    "see",
    "anything",
    "part",
    "part",
    "dash",
    "0",
    "0",
    "0",
    "go",
    "ahead",
    "cat",
    "use",
    "cut",
    "command",
    "list",
    "command",
    "bring",
    "use",
    "cat",
    "command",
    "actually",
    "display",
    "contents",
    "linux",
    "command",
    "hadoop",
    "linux",
    "command",
    "cat",
    "catenate",
    "confused",
    "catatonic",
    "catastrophic",
    "lot",
    "cat",
    "got",
    "tongue",
    "see",
    "fitness",
    "footwear",
    "apparel",
    "look",
    "really",
    "familiar",
    "mysql",
    "server",
    "went",
    "select",
    "fitness",
    "footwear",
    "apparel",
    "golf",
    "outdoors",
    "fan",
    "shop",
    "course",
    "really",
    "important",
    "let",
    "look",
    "back",
    "able",
    "tell",
    "put",
    "data",
    "uh",
    "go",
    "back",
    "import",
    "command",
    "scoop",
    "import",
    "connect",
    "db",
    "underneath",
    "connection",
    "sql",
    "server",
    "password",
    "username",
    "table",
    "going",
    "going",
    "mean",
    "table",
    "coming",
    "uh",
    "add",
    "target",
    "put",
    "target",
    "dash",
    "directory",
    "put",
    "full",
    "path",
    "hadoop",
    "thing",
    "good",
    "practice",
    "going",
    "add",
    "department",
    "department",
    "one",
    "add",
    "target",
    "directory",
    "user",
    "cloudera",
    "department",
    "one",
    "take",
    "moment",
    "go",
    "ahead",
    "skip",
    "process",
    "since",
    "going",
    "run",
    "slowly",
    "running",
    "like",
    "said",
    "couple",
    "cores",
    "also",
    "single",
    "node",
    "uh",
    "hadoop",
    "let",
    "arrow",
    "file",
    "system",
    "list",
    "want",
    "straight",
    "list",
    "hadoop",
    "file",
    "system",
    "minus",
    "ls",
    "list",
    "see",
    "department",
    "one",
    "course",
    "uh",
    "list",
    "department",
    "one",
    "see",
    "files",
    "inside",
    "department",
    "one",
    "mirrored",
    "saw",
    "files",
    "part",
    "0",
    "want",
    "look",
    "thing",
    "cat",
    "except",
    "instead",
    "departments",
    "uh",
    "department",
    "one",
    "go",
    "something",
    "going",
    "come",
    "data",
    "one",
    "important",
    "things",
    "importing",
    "data",
    "always",
    "question",
    "ask",
    "filter",
    "data",
    "comes",
    "want",
    "filter",
    "data",
    "comes",
    "storing",
    "everything",
    "file",
    "system",
    "would",
    "think",
    "hadoop",
    "big",
    "data",
    "put",
    "know",
    "experience",
    "putting",
    "turn",
    "couple",
    "hundred",
    "terabytes",
    "petabyte",
    "rapidly",
    "suddenly",
    "really",
    "add",
    "data",
    "store",
    "storing",
    "duplicate",
    "data",
    "sometimes",
    "really",
    "need",
    "able",
    "filter",
    "data",
    "let",
    "go",
    "ahead",
    "use",
    "arrow",
    "go",
    "last",
    "import",
    "uh",
    "since",
    "still",
    "lot",
    "stuff",
    "commands",
    "import",
    "target",
    "going",
    "change",
    "department",
    "2",
    "going",
    "create",
    "new",
    "directory",
    "one",
    "departments",
    "uh",
    "another",
    "command",
    "really",
    "slide",
    "mapping",
    "show",
    "looks",
    "like",
    "minute",
    "going",
    "put",
    "m3",
    "nothing",
    "filtering",
    "show",
    "second",
    "though",
    "want",
    "put",
    "uh",
    "case",
    "want",
    "know",
    "department",
    "id",
    "want",
    "know",
    "came",
    "flip",
    "back",
    "department",
    "underscore",
    "ids",
    "coming",
    "name",
    "column",
    "come",
    "department",
    "id",
    "greater",
    "4",
    "simple",
    "logic",
    "see",
    "use",
    "maybe",
    "creating",
    "buckets",
    "ages",
    "uh",
    "know",
    "aged",
    "10",
    "15",
    "20",
    "might",
    "looking",
    "mean",
    "kinds",
    "reasons",
    "could",
    "use",
    "command",
    "filter",
    "information",
    "maybe",
    "word",
    "counting",
    "want",
    "know",
    "words",
    "used",
    "less",
    "100",
    "times",
    "want",
    "get",
    "rid",
    "stuff",
    "used",
    "uh",
    "go",
    "ahead",
    "put",
    "department",
    "id",
    "greater",
    "four",
    "go",
    "ahead",
    "hit",
    "enter",
    "create",
    "department",
    "2",
    "setup",
    "go",
    "ahead",
    "skip",
    "runtime",
    "runs",
    "really",
    "slow",
    "single",
    "node",
    "real",
    "quick",
    "page",
    "commands",
    "let",
    "see",
    "go",
    "list",
    "see",
    "underneath",
    "list",
    "department",
    "two",
    "department",
    "two",
    "go",
    "ahead",
    "list",
    "department",
    "two",
    "see",
    "contents",
    "uh",
    "see",
    "three",
    "maps",
    "could",
    "data",
    "created",
    "three",
    "maps",
    "remember",
    "set",
    "use",
    "three",
    "mappers",
    "uh",
    "zero",
    "one",
    "two",
    "go",
    "ahead",
    "cat",
    "remember",
    "department",
    "two",
    "wan",
    "na",
    "look",
    "contents",
    "three",
    "different",
    "files",
    "greater",
    "four",
    "golf",
    "five",
    "outdoor",
    "six",
    "fan",
    "shop",
    "seven",
    "effectively",
    "filtered",
    "data",
    "storing",
    "data",
    "want",
    "file",
    "system",
    "going",
    "store",
    "data",
    "next",
    "stage",
    "export",
    "data",
    "remember",
    "lot",
    "times",
    "sql",
    "server",
    "continually",
    "dumping",
    "data",
    "long",
    "term",
    "storage",
    "access",
    "hadoop",
    "file",
    "system",
    "happens",
    "need",
    "pull",
    "data",
    "restore",
    "database",
    "maybe",
    "merged",
    "new",
    "company",
    "favorite",
    "topic",
    "merging",
    "companies",
    "emerging",
    "databases",
    "listed",
    "nightmare",
    "many",
    "different",
    "names",
    "company",
    "see",
    "able",
    "export",
    "also",
    "equally",
    "important",
    "let",
    "go",
    "ahead",
    "going",
    "flip",
    "back",
    "sql",
    "server",
    "need",
    "go",
    "ahead",
    "create",
    "database",
    "going",
    "export",
    "going",
    "go",
    "much",
    "detail",
    "command",
    "simply",
    "creating",
    "table",
    "table",
    "going",
    "pretty",
    "much",
    "table",
    "already",
    "departments",
    "case",
    "going",
    "create",
    "table",
    "called",
    "dept",
    "setup",
    "going",
    "giving",
    "different",
    "name",
    "different",
    "schema",
    "done",
    "go",
    "ahead",
    "select",
    "star",
    "dept",
    "go",
    "empty",
    "expect",
    "new",
    "database",
    "new",
    "data",
    "table",
    "empty",
    "need",
    "go",
    "ahead",
    "export",
    "data",
    "filtered",
    "let",
    "flip",
    "back",
    "scoop",
    "setup",
    "linux",
    "terminal",
    "window",
    "let",
    "go",
    "back",
    "one",
    "commands",
    "scoop",
    "import",
    "case",
    "instead",
    "import",
    "going",
    "take",
    "scoop",
    "going",
    "export",
    "going",
    "change",
    "export",
    "connection",
    "going",
    "remain",
    "connect",
    "database",
    "also",
    "still",
    "retail",
    "db",
    "password",
    "none",
    "changes",
    "uh",
    "big",
    "change",
    "going",
    "table",
    "instead",
    "departments",
    "uh",
    "remember",
    "changed",
    "gave",
    "new",
    "name",
    "want",
    "change",
    "also",
    "department",
    "going",
    "worry",
    "mapper",
    "count",
    "part",
    "import",
    "go",
    "finally",
    "needs",
    "know",
    "export",
    "instead",
    "target",
    "directory",
    "export",
    "directory",
    "coming",
    "still",
    "user",
    "cloudera",
    "keep",
    "department",
    "see",
    "data",
    "coming",
    "back",
    "filtered",
    "let",
    "go",
    "ahead",
    "run",
    "take",
    "moment",
    "go",
    "steps",
    "low",
    "going",
    "go",
    "skip",
    "sit",
    "wrapped",
    "export",
    "flip",
    "back",
    "mysql",
    "use",
    "arrow",
    "time",
    "going",
    "select",
    "star",
    "department",
    "see",
    "exported",
    "golf",
    "outdoors",
    "fan",
    "shop",
    "imagine",
    "also",
    "might",
    "use",
    "command",
    "export",
    "also",
    "lot",
    "mixing",
    "command",
    "line",
    "scoop",
    "pretty",
    "straightforward",
    "changing",
    "different",
    "variables",
    "whether",
    "creating",
    "table",
    "listing",
    "table",
    "listing",
    "databases",
    "powerful",
    "tool",
    "bringing",
    "data",
    "hadoop",
    "file",
    "system",
    "exporting",
    "wrapped",
    "demo",
    "scoop",
    "gone",
    "lot",
    "basic",
    "commands",
    "wraps",
    "us",
    "today",
    "first",
    "going",
    "start",
    "history",
    "hive",
    "hive",
    "architecture",
    "hive",
    "data",
    "flow",
    "hive",
    "hive",
    "data",
    "modeling",
    "hive",
    "data",
    "types",
    "different",
    "modes",
    "hive",
    "difference",
    "hive",
    "rdbms",
    "finally",
    "going",
    "look",
    "features",
    "hive",
    "quick",
    "demo",
    "hive",
    "cloudera",
    "hadoop",
    "file",
    "system",
    "let",
    "dive",
    "brief",
    "history",
    "hive",
    "history",
    "hive",
    "begins",
    "facebook",
    "facebook",
    "began",
    "using",
    "hadoop",
    "solution",
    "handle",
    "growing",
    "big",
    "data",
    "talking",
    "data",
    "fits",
    "one",
    "two",
    "even",
    "five",
    "computers",
    "uh",
    "talking",
    "due",
    "fits",
    "looked",
    "hadoop",
    "tutorials",
    "know",
    "talking",
    "big",
    "data",
    "data",
    "pools",
    "facebook",
    "certainly",
    "lot",
    "data",
    "tracks",
    "know",
    "hadoop",
    "uses",
    "mapreduce",
    "processing",
    "data",
    "mapreduce",
    "required",
    "users",
    "write",
    "long",
    "codes",
    "really",
    "extensive",
    "java",
    "codes",
    "complicated",
    "average",
    "person",
    "use",
    "users",
    "versed",
    "java",
    "coding",
    "languages",
    "proved",
    "disadvantage",
    "users",
    "comfortable",
    "writing",
    "queries",
    "sql",
    "sql",
    "around",
    "long",
    "time",
    "standard",
    "sql",
    "query",
    "language",
    "hive",
    "developed",
    "vision",
    "incorporate",
    "concepts",
    "tables",
    "columns",
    "like",
    "sql",
    "hive",
    "well",
    "problem",
    "processing",
    "analyzing",
    "data",
    "users",
    "found",
    "difficult",
    "code",
    "well",
    "versed",
    "coding",
    "languages",
    "processing",
    "ever",
    "analyzing",
    "solution",
    "required",
    "language",
    "similar",
    "sql",
    "well",
    "known",
    "users",
    "thus",
    "hive",
    "hql",
    "language",
    "evolved",
    "hive",
    "hive",
    "data",
    "warehouse",
    "system",
    "used",
    "querying",
    "analyzing",
    "large",
    "data",
    "sets",
    "stored",
    "hdfs",
    "hadoop",
    "file",
    "system",
    "hive",
    "uses",
    "query",
    "language",
    "call",
    "hive",
    "ql",
    "hql",
    "similar",
    "sql",
    "take",
    "user",
    "user",
    "sends",
    "hive",
    "queries",
    "converted",
    "mapreduce",
    "tasks",
    "accesses",
    "hadoop",
    "mapreduce",
    "system",
    "let",
    "take",
    "look",
    "architecture",
    "hive",
    "architecture",
    "hive",
    "hive",
    "client",
    "could",
    "programmer",
    "maybe",
    "manager",
    "knows",
    "enough",
    "sql",
    "basic",
    "query",
    "look",
    "data",
    "need",
    "hive",
    "client",
    "supports",
    "different",
    "types",
    "client",
    "applications",
    "different",
    "languages",
    "prefer",
    "performing",
    "queries",
    "thrift",
    "application",
    "hive",
    "thrift",
    "client",
    "thrift",
    "software",
    "framework",
    "hive",
    "server",
    "based",
    "thrift",
    "serve",
    "request",
    "programming",
    "language",
    "support",
    "thrift",
    "jdbc",
    "application",
    "hive",
    "jdbc",
    "driver",
    "jdbc",
    "java",
    "database",
    "connectivity",
    "jdbc",
    "application",
    "connected",
    "jdbc",
    "driver",
    "odbc",
    "application",
    "hive",
    "odbc",
    "driver",
    "odbc",
    "open",
    "database",
    "connectivity",
    "odbc",
    "application",
    "connected",
    "odbc",
    "driver",
    "growing",
    "development",
    "different",
    "scripting",
    "languages",
    "python",
    "c",
    "plus",
    "plus",
    "spark",
    "java",
    "find",
    "connection",
    "main",
    "scripting",
    "languages",
    "hive",
    "services",
    "look",
    "deeper",
    "architecture",
    "hive",
    "supports",
    "various",
    "services",
    "hive",
    "server",
    "basically",
    "thrift",
    "application",
    "hive",
    "thrift",
    "client",
    "jdbc",
    "hive",
    "jdbc",
    "driver",
    "odbc",
    "application",
    "hive",
    "odbc",
    "driver",
    "connect",
    "hive",
    "server",
    "hive",
    "web",
    "interface",
    "also",
    "cli",
    "hive",
    "web",
    "interface",
    "gui",
    "provided",
    "execute",
    "hive",
    "queries",
    "actually",
    "using",
    "later",
    "today",
    "see",
    "kind",
    "looks",
    "like",
    "get",
    "feel",
    "means",
    "commands",
    "executed",
    "directly",
    "cli",
    "cli",
    "direct",
    "terminal",
    "window",
    "also",
    "show",
    "see",
    "two",
    "different",
    "interfaces",
    "work",
    "push",
    "code",
    "hive",
    "driver",
    "hive",
    "driver",
    "responsible",
    "queries",
    "submitted",
    "everything",
    "goes",
    "driver",
    "let",
    "take",
    "closer",
    "look",
    "hive",
    "driver",
    "hive",
    "driver",
    "performs",
    "three",
    "steps",
    "internally",
    "one",
    "compiler",
    "hive",
    "driver",
    "passes",
    "query",
    "compiler",
    "checked",
    "analyzed",
    "optimizer",
    "kicks",
    "optimize",
    "logical",
    "plan",
    "form",
    "graph",
    "mapreduce",
    "hdfs",
    "tasks",
    "obtained",
    "finally",
    "executor",
    "final",
    "step",
    "tasks",
    "executed",
    "look",
    "architecture",
    "also",
    "note",
    "meta",
    "store",
    "metastore",
    "repository",
    "hive",
    "metadata",
    "stores",
    "metadata",
    "hive",
    "tables",
    "think",
    "schema",
    "located",
    "stored",
    "apache",
    "derby",
    "db",
    "processing",
    "resource",
    "management",
    "handled",
    "mapreduce",
    "v1",
    "see",
    "mapreduce",
    "v2",
    "yarn",
    "tez",
    "different",
    "ways",
    "managing",
    "resources",
    "depending",
    "version",
    "hadoop",
    "hive",
    "uses",
    "mapreduce",
    "framework",
    "process",
    "queries",
    "distributed",
    "storage",
    "hdfs",
    "looked",
    "hadoop",
    "tutorials",
    "know",
    "commodity",
    "machines",
    "linearly",
    "scalable",
    "means",
    "affordable",
    "lot",
    "time",
    "talking",
    "big",
    "data",
    "talking",
    "tenth",
    "price",
    "storing",
    "enterprise",
    "computers",
    "look",
    "data",
    "flow",
    "hive",
    "data",
    "flow",
    "hive",
    "hive",
    "hadoop",
    "system",
    "underneath",
    "user",
    "interface",
    "ui",
    "driver",
    "compiler",
    "execution",
    "engine",
    "metastore",
    "goes",
    "mapreduce",
    "hadoop",
    "file",
    "system",
    "execute",
    "query",
    "see",
    "coming",
    "goes",
    "driver",
    "step",
    "one",
    "step",
    "two",
    "get",
    "plan",
    "going",
    "refers",
    "query",
    "execution",
    "go",
    "metadata",
    "like",
    "well",
    "kind",
    "metadata",
    "actually",
    "looking",
    "data",
    "located",
    "schema",
    "comes",
    "back",
    "metadata",
    "compiler",
    "compiler",
    "takes",
    "information",
    "syn",
    "plan",
    "returns",
    "driver",
    "driver",
    "sends",
    "execute",
    "plan",
    "execution",
    "engine",
    "execution",
    "engine",
    "execution",
    "engine",
    "acts",
    "bridge",
    "hive",
    "hadoop",
    "process",
    "query",
    "going",
    "mapreduce",
    "hadoop",
    "file",
    "system",
    "hdfs",
    "come",
    "back",
    "metadata",
    "operations",
    "goes",
    "back",
    "metastore",
    "update",
    "let",
    "know",
    "going",
    "also",
    "goes",
    "communication",
    "execution",
    "engine",
    "metastore",
    "execution",
    "engine",
    "communications",
    "metastore",
    "perform",
    "operations",
    "like",
    "create",
    "drop",
    "tables",
    "metastore",
    "stores",
    "information",
    "tables",
    "columns",
    "talking",
    "schema",
    "database",
    "send",
    "results",
    "communication",
    "back",
    "driver",
    "fetch",
    "results",
    "goes",
    "back",
    "client",
    "let",
    "take",
    "little",
    "bit",
    "look",
    "hive",
    "data",
    "modeling",
    "hive",
    "data",
    "modeling",
    "high",
    "data",
    "modeling",
    "tables",
    "partitions",
    "buckets",
    "tables",
    "hive",
    "created",
    "way",
    "done",
    "rdbms",
    "looking",
    "traditional",
    "sql",
    "server",
    "mysql",
    "server",
    "might",
    "enterprise",
    "equipment",
    "lot",
    "people",
    "pulling",
    "moving",
    "stuff",
    "tables",
    "going",
    "look",
    "similar",
    "makes",
    "easy",
    "take",
    "information",
    "let",
    "say",
    "need",
    "keep",
    "current",
    "information",
    "need",
    "store",
    "years",
    "transactions",
    "back",
    "hadoop",
    "hive",
    "match",
    "kind",
    "look",
    "tables",
    "databases",
    "look",
    "similar",
    "easily",
    "import",
    "back",
    "easily",
    "store",
    "hive",
    "system",
    "partitions",
    "tables",
    "organized",
    "partitions",
    "grouping",
    "type",
    "data",
    "based",
    "partition",
    "key",
    "become",
    "important",
    "speeding",
    "process",
    "queries",
    "looking",
    "dates",
    "far",
    "like",
    "employment",
    "dates",
    "employees",
    "tracking",
    "might",
    "add",
    "partition",
    "might",
    "one",
    "key",
    "things",
    "always",
    "looking",
    "far",
    "employees",
    "concerned",
    "finally",
    "buckets",
    "data",
    "present",
    "partitions",
    "divided",
    "buckets",
    "efficient",
    "querying",
    "efficiency",
    "level",
    "lot",
    "times",
    "taught",
    "working",
    "programmer",
    "admin",
    "hadoop",
    "file",
    "system",
    "maximize",
    "efficiency",
    "file",
    "system",
    "usually",
    "job",
    "talking",
    "hive",
    "data",
    "modeling",
    "want",
    "make",
    "sure",
    "work",
    "together",
    "maximizing",
    "resources",
    "hive",
    "data",
    "types",
    "talking",
    "hive",
    "data",
    "types",
    "primitive",
    "data",
    "types",
    "complex",
    "data",
    "types",
    "lot",
    "look",
    "familiar",
    "mirrors",
    "lot",
    "stuff",
    "sql",
    "primitive",
    "data",
    "types",
    "numerical",
    "data",
    "types",
    "string",
    "data",
    "type",
    "date",
    "time",
    "data",
    "type",
    "miscellaneous",
    "data",
    "type",
    "kind",
    "case",
    "numerical",
    "data",
    "floats",
    "integers",
    "short",
    "integers",
    "numerical",
    "data",
    "comes",
    "number",
    "string",
    "course",
    "characters",
    "numbers",
    "date",
    "time",
    "stamp",
    "kind",
    "general",
    "way",
    "pulling",
    "created",
    "data",
    "types",
    "miscellaneous",
    "data",
    "type",
    "complex",
    "data",
    "types",
    "store",
    "arrays",
    "store",
    "maps",
    "store",
    "structures",
    "uh",
    "even",
    "units",
    "dig",
    "hive",
    "data",
    "types",
    "primitive",
    "data",
    "types",
    "complex",
    "data",
    "types",
    "look",
    "primitive",
    "data",
    "types",
    "looking",
    "numeric",
    "data",
    "types",
    "data",
    "types",
    "like",
    "integer",
    "float",
    "decimal",
    "stored",
    "numbers",
    "hive",
    "data",
    "system",
    "string",
    "data",
    "type",
    "data",
    "types",
    "like",
    "characters",
    "strings",
    "store",
    "name",
    "person",
    "working",
    "know",
    "john",
    "doe",
    "city",
    "memphis",
    "state",
    "tennessee",
    "maybe",
    "boulder",
    "colorado",
    "usa",
    "maybe",
    "hyper",
    "bad",
    "india",
    "going",
    "string",
    "stored",
    "string",
    "character",
    "course",
    "date",
    "time",
    "data",
    "type",
    "data",
    "types",
    "like",
    "timestamp",
    "date",
    "interval",
    "common",
    "far",
    "tracking",
    "sales",
    "anything",
    "like",
    "think",
    "type",
    "stamp",
    "time",
    "maybe",
    "dealing",
    "race",
    "want",
    "know",
    "interval",
    "long",
    "person",
    "take",
    "complete",
    "whatever",
    "task",
    "date",
    "time",
    "data",
    "type",
    "talk",
    "miscellaneous",
    "data",
    "type",
    "like",
    "boolean",
    "binary",
    "get",
    "boolean",
    "binary",
    "actually",
    "almost",
    "create",
    "anything",
    "yes",
    "knows",
    "zero",
    "one",
    "let",
    "take",
    "look",
    "complex",
    "data",
    "types",
    "little",
    "closer",
    "uh",
    "arrays",
    "syntax",
    "data",
    "type",
    "array",
    "think",
    "array",
    "collection",
    "entities",
    "one",
    "two",
    "three",
    "four",
    "numbers",
    "maps",
    "collection",
    "key",
    "value",
    "pairs",
    "understanding",
    "maps",
    "central",
    "hadoop",
    "store",
    "maps",
    "key",
    "set",
    "one",
    "key",
    "per",
    "mapped",
    "value",
    "hadoop",
    "course",
    "collect",
    "keys",
    "add",
    "something",
    "contents",
    "key",
    "map",
    "primitive",
    "type",
    "data",
    "type",
    "collection",
    "key",
    "value",
    "pairs",
    "collection",
    "complex",
    "data",
    "comment",
    "structure",
    "column",
    "name",
    "data",
    "type",
    "comment",
    "call",
    "column",
    "comment",
    "uh",
    "get",
    "complicated",
    "structures",
    "collection",
    "data",
    "commented",
    "setup",
    "units",
    "collection",
    "heterogeneous",
    "data",
    "types",
    "syntax",
    "union",
    "type",
    "data",
    "type",
    "data",
    "type",
    "going",
    "little",
    "bit",
    "different",
    "arrays",
    "actually",
    "mix",
    "match",
    "different",
    "modes",
    "hive",
    "hive",
    "operates",
    "two",
    "modes",
    "depending",
    "number",
    "size",
    "data",
    "nodes",
    "local",
    "mode",
    "map",
    "reduce",
    "mode",
    "talk",
    "local",
    "mode",
    "used",
    "hadoop",
    "one",
    "data",
    "node",
    "data",
    "small",
    "processing",
    "fast",
    "smaller",
    "data",
    "sets",
    "present",
    "local",
    "machine",
    "might",
    "local",
    "file",
    "stuff",
    "uploading",
    "hive",
    "need",
    "processes",
    "go",
    "ahead",
    "run",
    "high",
    "processes",
    "queries",
    "usually",
    "see",
    "much",
    "way",
    "single",
    "node",
    "hadoop",
    "system",
    "going",
    "might",
    "well",
    "use",
    "like",
    "sql",
    "database",
    "even",
    "java",
    "sqlite",
    "something",
    "python",
    "sqlite",
    "really",
    "see",
    "lot",
    "single",
    "node",
    "hadoop",
    "databases",
    "see",
    "local",
    "mode",
    "hive",
    "working",
    "small",
    "amount",
    "data",
    "going",
    "integrated",
    "larger",
    "database",
    "map",
    "reduce",
    "mode",
    "used",
    "hadoop",
    "multiple",
    "data",
    "nodes",
    "data",
    "spread",
    "across",
    "various",
    "data",
    "nodes",
    "processing",
    "large",
    "data",
    "sets",
    "efficient",
    "using",
    "mode",
    "think",
    "instead",
    "one",
    "two",
    "three",
    "even",
    "five",
    "computers",
    "usually",
    "talking",
    "hadoop",
    "file",
    "system",
    "looking",
    "10",
    "computers",
    "15",
    "100",
    "data",
    "spread",
    "across",
    "different",
    "hadoop",
    "nodes",
    "difference",
    "hive",
    "rdbms",
    "remember",
    "rdbms",
    "stands",
    "relational",
    "database",
    "management",
    "system",
    "let",
    "take",
    "look",
    "difference",
    "hive",
    "rdbms",
    "hive",
    "hive",
    "enforces",
    "schema",
    "read",
    "important",
    "whatever",
    "coming",
    "hive",
    "looking",
    "making",
    "sure",
    "fits",
    "model",
    "rdbms",
    "enforces",
    "schema",
    "actually",
    "writes",
    "data",
    "database",
    "read",
    "data",
    "starts",
    "write",
    "going",
    "give",
    "error",
    "tell",
    "something",
    "incorrect",
    "scheme",
    "hive",
    "data",
    "size",
    "petabytes",
    "hard",
    "imagine",
    "um",
    "know",
    "looking",
    "personal",
    "computer",
    "desk",
    "maybe",
    "10",
    "terabytes",
    "computer",
    "talking",
    "petabytes",
    "hundreds",
    "computers",
    "grouped",
    "together",
    "rdbms",
    "data",
    "size",
    "terabytes",
    "rarely",
    "see",
    "rdbms",
    "system",
    "spread",
    "five",
    "computers",
    "lot",
    "reasons",
    "rdbms",
    "actually",
    "amount",
    "writes",
    "hard",
    "drive",
    "lot",
    "going",
    "writing",
    "polling",
    "stuff",
    "really",
    "want",
    "get",
    "big",
    "rd",
    "bms",
    "gon",
    "na",
    "run",
    "lot",
    "problems",
    "hive",
    "take",
    "big",
    "want",
    "hive",
    "based",
    "notion",
    "write",
    "read",
    "many",
    "times",
    "important",
    "call",
    "worm",
    "write",
    "w",
    "wants",
    "read",
    "many",
    "times",
    "refer",
    "worm",
    "true",
    "lot",
    "hadoop",
    "setup",
    "altered",
    "little",
    "bit",
    "general",
    "looking",
    "archiving",
    "data",
    "want",
    "data",
    "analysis",
    "looking",
    "pulling",
    "stuff",
    "rd",
    "bms",
    "years",
    "years",
    "years",
    "business",
    "whatever",
    "company",
    "scientific",
    "research",
    "putting",
    "huge",
    "data",
    "pool",
    "queries",
    "get",
    "information",
    "rdbms",
    "based",
    "notion",
    "read",
    "write",
    "many",
    "times",
    "continually",
    "updating",
    "database",
    "continually",
    "bringing",
    "new",
    "stuff",
    "new",
    "sales",
    "account",
    "changes",
    "different",
    "licensing",
    "whatever",
    "software",
    "selling",
    "kind",
    "stuff",
    "data",
    "continually",
    "fluctuating",
    "hive",
    "resembles",
    "traditional",
    "database",
    "supporting",
    "sql",
    "database",
    "data",
    "warehouse",
    "important",
    "goes",
    "stuff",
    "talked",
    "looking",
    "database",
    "data",
    "warehouse",
    "store",
    "data",
    "still",
    "fast",
    "easy",
    "access",
    "queries",
    "think",
    "twitter",
    "facebook",
    "many",
    "posts",
    "archived",
    "back",
    "historically",
    "posts",
    "going",
    "change",
    "made",
    "post",
    "posted",
    "database",
    "store",
    "warehouse",
    "case",
    "want",
    "pull",
    "back",
    "rdbms",
    "type",
    "database",
    "management",
    "system",
    "based",
    "relational",
    "model",
    "data",
    "hive",
    "easily",
    "scalable",
    "low",
    "cost",
    "talking",
    "maybe",
    "thousand",
    "dollars",
    "per",
    "terabyte",
    "um",
    "rdbms",
    "scalable",
    "low",
    "cost",
    "first",
    "start",
    "lower",
    "end",
    "talking",
    "10",
    "000",
    "per",
    "terabyte",
    "data",
    "including",
    "backup",
    "models",
    "added",
    "necessities",
    "support",
    "scale",
    "scale",
    "computers",
    "hardware",
    "uh",
    "might",
    "start",
    "basic",
    "server",
    "upgrade",
    "sun",
    "computer",
    "run",
    "spend",
    "know",
    "tens",
    "thousands",
    "dollars",
    "hardware",
    "upgrade",
    "hive",
    "put",
    "another",
    "computer",
    "hadoop",
    "file",
    "system",
    "let",
    "look",
    "features",
    "hive",
    "uh",
    "looking",
    "features",
    "hive",
    "talking",
    "use",
    "sql",
    "like",
    "language",
    "called",
    "hive",
    "ql",
    "lot",
    "times",
    "see",
    "hql",
    "easier",
    "long",
    "codes",
    "nice",
    "working",
    "shareholders",
    "come",
    "say",
    "hey",
    "basic",
    "sql",
    "query",
    "pull",
    "information",
    "need",
    "way",
    "take",
    "programmers",
    "jump",
    "every",
    "time",
    "want",
    "look",
    "something",
    "database",
    "actually",
    "easily",
    "skilled",
    "programming",
    "script",
    "writing",
    "tables",
    "used",
    "similar",
    "rdbms",
    "hence",
    "easier",
    "understand",
    "one",
    "things",
    "like",
    "bringing",
    "tables",
    "mysql",
    "server",
    "sql",
    "server",
    "almost",
    "direct",
    "reflection",
    "two",
    "looking",
    "one",
    "data",
    "continually",
    "changing",
    "going",
    "archive",
    "database",
    "huge",
    "jump",
    "learn",
    "whole",
    "new",
    "language",
    "mirror",
    "schema",
    "hdfs",
    "hive",
    "making",
    "easy",
    "go",
    "two",
    "using",
    "hive",
    "ql",
    "multiple",
    "users",
    "simultaneously",
    "query",
    "data",
    "multiple",
    "clients",
    "send",
    "query",
    "also",
    "true",
    "rdbms",
    "kind",
    "queues",
    "running",
    "fast",
    "notice",
    "lag",
    "time",
    "well",
    "get",
    "also",
    "hql",
    "add",
    "computers",
    "query",
    "go",
    "quickly",
    "depending",
    "many",
    "computers",
    "much",
    "resources",
    "machine",
    "pull",
    "information",
    "hive",
    "supports",
    "variety",
    "data",
    "types",
    "hive",
    "designed",
    "hadoop",
    "system",
    "put",
    "almost",
    "anything",
    "hadoop",
    "file",
    "system",
    "let",
    "take",
    "look",
    "demo",
    "hive",
    "ql",
    "hql",
    "dive",
    "demo",
    "let",
    "take",
    "look",
    "website",
    "main",
    "website",
    "since",
    "apache",
    "apache",
    "open",
    "source",
    "software",
    "main",
    "software",
    "main",
    "site",
    "build",
    "go",
    "see",
    "slowly",
    "migrating",
    "hive",
    "beehive",
    "see",
    "beehive",
    "versus",
    "hive",
    "note",
    "beehive",
    "new",
    "release",
    "coming",
    "reflects",
    "lot",
    "functionality",
    "hive",
    "thing",
    "like",
    "pull",
    "kind",
    "documentation",
    "commands",
    "actually",
    "going",
    "go",
    "hortonworks",
    "hive",
    "cheat",
    "sheet",
    "hortonworks",
    "cloudera",
    "two",
    "common",
    "used",
    "builds",
    "hadoop",
    "four",
    "include",
    "hive",
    "different",
    "tools",
    "hortonworks",
    "pretty",
    "good",
    "pdf",
    "download",
    "cheat",
    "sheet",
    "believe",
    "cloudera",
    "go",
    "ahead",
    "look",
    "horton",
    "one",
    "one",
    "comes",
    "really",
    "good",
    "see",
    "look",
    "query",
    "language",
    "compares",
    "mysql",
    "server",
    "hive",
    "ql",
    "hql",
    "see",
    "basic",
    "select",
    "select",
    "columns",
    "table",
    "conditions",
    "exist",
    "basic",
    "command",
    "different",
    "things",
    "like",
    "sql",
    "scroll",
    "see",
    "data",
    "types",
    "integer",
    "flow",
    "binary",
    "double",
    "string",
    "timestamp",
    "different",
    "data",
    "types",
    "use",
    "different",
    "semantics",
    "different",
    "keys",
    "features",
    "functions",
    "uh",
    "running",
    "hive",
    "query",
    "command",
    "line",
    "setup",
    "course",
    "hive",
    "shell",
    "uh",
    "set",
    "uh",
    "see",
    "right",
    "loop",
    "lot",
    "basic",
    "stuff",
    "basically",
    "looking",
    "sql",
    "across",
    "horton",
    "database",
    "going",
    "go",
    "ahead",
    "run",
    "hadoop",
    "cluster",
    "hive",
    "demo",
    "going",
    "go",
    "ahead",
    "use",
    "cloudera",
    "quick",
    "start",
    "virtual",
    "box",
    "oracle",
    "virtual",
    "box",
    "open",
    "source",
    "cloudera",
    "quick",
    "start",
    "hadoop",
    "setup",
    "single",
    "node",
    "obviously",
    "hadoop",
    "hive",
    "designed",
    "run",
    "across",
    "cluster",
    "computers",
    "talk",
    "single",
    "node",
    "education",
    "testing",
    "kind",
    "thing",
    "chance",
    "always",
    "go",
    "back",
    "look",
    "demo",
    "setting",
    "hadoop",
    "system",
    "single",
    "cluster",
    "set",
    "note",
    "youtube",
    "video",
    "team",
    "get",
    "contact",
    "send",
    "link",
    "already",
    "contact",
    "us",
    "always",
    "important",
    "note",
    "need",
    "computer",
    "running",
    "windows",
    "windows",
    "machine",
    "going",
    "need",
    "probably",
    "12",
    "gigabytes",
    "actually",
    "run",
    "uh",
    "used",
    "lot",
    "less",
    "things",
    "evolved",
    "take",
    "resources",
    "need",
    "professional",
    "version",
    "home",
    "version",
    "able",
    "get",
    "run",
    "boy",
    "take",
    "lot",
    "extra",
    "work",
    "get",
    "home",
    "version",
    "let",
    "use",
    "virtual",
    "setup",
    "simply",
    "click",
    "cloudera",
    "quickstart",
    "going",
    "go",
    "start",
    "starting",
    "linux",
    "windows",
    "10",
    "computer",
    "virtual",
    "box",
    "going",
    "linux",
    "operating",
    "system",
    "skip",
    "ahead",
    "watch",
    "whole",
    "install",
    "something",
    "interesting",
    "know",
    "cloudera",
    "running",
    "linuxcentos",
    "whatever",
    "reason",
    "always",
    "click",
    "hit",
    "escape",
    "button",
    "spin",
    "see",
    "dos",
    "come",
    "cloudera",
    "spun",
    "virtual",
    "machine",
    "linux",
    "uh",
    "see",
    "uses",
    "thunderbird",
    "browser",
    "default",
    "automatically",
    "opens",
    "number",
    "different",
    "tabs",
    "us",
    "quick",
    "note",
    "mentioned",
    "like",
    "restrictions",
    "getting",
    "set",
    "computer",
    "home",
    "edition",
    "computer",
    "worried",
    "setting",
    "also",
    "go",
    "spin",
    "one",
    "month",
    "free",
    "service",
    "amazon",
    "web",
    "service",
    "play",
    "uh",
    "options",
    "stuck",
    "quick",
    "start",
    "menu",
    "spin",
    "many",
    "ways",
    "first",
    "thing",
    "want",
    "note",
    "come",
    "cloudera",
    "going",
    "access",
    "two",
    "ways",
    "first",
    "one",
    "going",
    "use",
    "hue",
    "going",
    "open",
    "hue",
    "take",
    "moment",
    "load",
    "setup",
    "hue",
    "nice",
    "go",
    "use",
    "hue",
    "editor",
    "hive",
    "hadoop",
    "setup",
    "usually",
    "admin",
    "side",
    "lot",
    "information",
    "lot",
    "visuals",
    "less",
    "know",
    "actually",
    "diving",
    "executing",
    "code",
    "also",
    "write",
    "code",
    "files",
    "scripts",
    "things",
    "otherwise",
    "upload",
    "hive",
    "today",
    "going",
    "look",
    "command",
    "lines",
    "upload",
    "hue",
    "go",
    "actually",
    "work",
    "terminal",
    "window",
    "hive",
    "shell",
    "hue",
    "browser",
    "window",
    "go",
    "query",
    "click",
    "pull",
    "menu",
    "go",
    "editor",
    "see",
    "hive",
    "go",
    "hive",
    "setup",
    "go",
    "click",
    "hive",
    "open",
    "query",
    "nice",
    "little",
    "b",
    "shows",
    "hive",
    "going",
    "go",
    "something",
    "simple",
    "like",
    "show",
    "databases",
    "follow",
    "semicolon",
    "standard",
    "hive",
    "always",
    "add",
    "punctuation",
    "end",
    "go",
    "ahead",
    "run",
    "query",
    "show",
    "underneath",
    "see",
    "since",
    "new",
    "quick",
    "start",
    "put",
    "see",
    "default",
    "databases",
    "database",
    "name",
    "actually",
    "created",
    "databases",
    "lot",
    "like",
    "assistant",
    "function",
    "tables",
    "databases",
    "kinds",
    "things",
    "research",
    "look",
    "hue",
    "far",
    "bigger",
    "picture",
    "downside",
    "always",
    "seems",
    "lag",
    "whenever",
    "always",
    "seem",
    "run",
    "slow",
    "cloudera",
    "open",
    "terminal",
    "window",
    "actually",
    "icon",
    "top",
    "also",
    "go",
    "applications",
    "applications",
    "system",
    "tools",
    "terminal",
    "either",
    "one",
    "work",
    "regular",
    "terminal",
    "window",
    "terminal",
    "window",
    "running",
    "underneath",
    "linux",
    "linux",
    "terminal",
    "window",
    "virtual",
    "machine",
    "resting",
    "regular",
    "windows",
    "10",
    "machine",
    "go",
    "ahead",
    "zoom",
    "see",
    "text",
    "better",
    "video",
    "simply",
    "clicked",
    "view",
    "zoom",
    "type",
    "hive",
    "open",
    "shell",
    "takes",
    "moment",
    "load",
    "starting",
    "hive",
    "also",
    "want",
    "note",
    "depending",
    "rights",
    "computer",
    "interaction",
    "might",
    "pseudo",
    "hive",
    "put",
    "password",
    "username",
    "computers",
    "usually",
    "set",
    "hive",
    "login",
    "depends",
    "accessing",
    "linux",
    "system",
    "hive",
    "shell",
    "go",
    "ahead",
    "simple",
    "uh",
    "hql",
    "command",
    "show",
    "databases",
    "see",
    "databases",
    "go",
    "ahead",
    "create",
    "database",
    "call",
    "office",
    "today",
    "moment",
    "show",
    "arrow",
    "arrow",
    "hotkey",
    "works",
    "linux",
    "hive",
    "go",
    "back",
    "paste",
    "commands",
    "typed",
    "see",
    "course",
    "default",
    "database",
    "office",
    "database",
    "created",
    "database",
    "pretty",
    "quick",
    "easy",
    "go",
    "ahead",
    "drop",
    "database",
    "drop",
    "database",
    "office",
    "work",
    "database",
    "empty",
    "database",
    "empty",
    "would",
    "cascade",
    "drops",
    "tables",
    "database",
    "database",
    "show",
    "database",
    "go",
    "ahead",
    "recreate",
    "database",
    "going",
    "use",
    "office",
    "database",
    "rest",
    "demo",
    "really",
    "handy",
    "command",
    "set",
    "sql",
    "hql",
    "use",
    "office",
    "sets",
    "office",
    "default",
    "database",
    "instead",
    "reference",
    "database",
    "every",
    "time",
    "work",
    "table",
    "automatically",
    "assumes",
    "database",
    "used",
    "whatever",
    "tables",
    "working",
    "difference",
    "put",
    "database",
    "name",
    "period",
    "table",
    "show",
    "minute",
    "looks",
    "like",
    "different",
    "going",
    "table",
    "database",
    "probably",
    "load",
    "data",
    "let",
    "go",
    "ahead",
    "switch",
    "gears",
    "open",
    "terminal",
    "window",
    "open",
    "another",
    "terminal",
    "window",
    "open",
    "right",
    "top",
    "one",
    "hive",
    "shell",
    "running",
    "terminal",
    "window",
    "first",
    "going",
    "go",
    "ahead",
    "list",
    "course",
    "linux",
    "command",
    "see",
    "files",
    "default",
    "load",
    "change",
    "directory",
    "documents",
    "list",
    "documents",
    "actually",
    "going",
    "looking",
    "linux",
    "command",
    "cat",
    "use",
    "actually",
    "combine",
    "documents",
    "kinds",
    "things",
    "cat",
    "want",
    "display",
    "contents",
    "file",
    "simply",
    "cat",
    "employee",
    "csv",
    "looking",
    "want",
    "know",
    "couple",
    "things",
    "one",
    "line",
    "top",
    "okay",
    "first",
    "thing",
    "notice",
    "header",
    "line",
    "next",
    "thing",
    "notice",
    "data",
    "comma",
    "separated",
    "particular",
    "case",
    "see",
    "space",
    "generally",
    "got",
    "real",
    "careful",
    "spaces",
    "kinds",
    "things",
    "got",
    "watch",
    "cause",
    "issues",
    "spaces",
    "wo",
    "strings",
    "space",
    "connected",
    "space",
    "next",
    "integer",
    "would",
    "get",
    "null",
    "value",
    "comes",
    "database",
    "without",
    "something",
    "extra",
    "hadoop",
    "important",
    "know",
    "writing",
    "data",
    "reading",
    "many",
    "times",
    "true",
    "almost",
    "hadoop",
    "things",
    "coming",
    "really",
    "want",
    "process",
    "data",
    "gets",
    "database",
    "studied",
    "data",
    "transformation",
    "adult",
    "extract",
    "transfer",
    "form",
    "load",
    "data",
    "really",
    "want",
    "extract",
    "transform",
    "putting",
    "hive",
    "load",
    "hive",
    "transformed",
    "data",
    "course",
    "also",
    "want",
    "note",
    "schema",
    "integer",
    "string",
    "string",
    "integer",
    "integer",
    "kept",
    "pretty",
    "simple",
    "far",
    "way",
    "data",
    "set",
    "last",
    "thing",
    "going",
    "want",
    "look",
    "source",
    "since",
    "local",
    "uploads",
    "want",
    "know",
    "path",
    "whole",
    "path",
    "case",
    "home",
    "cloudera",
    "slash",
    "documents",
    "text",
    "documents",
    "working",
    "right",
    "anything",
    "fancy",
    "simple",
    "get",
    "edit",
    "see",
    "comes",
    "text",
    "document",
    "easily",
    "remove",
    "added",
    "spaces",
    "go",
    "go",
    "save",
    "new",
    "setup",
    "edited",
    "g",
    "edit",
    "usually",
    "one",
    "default",
    "loads",
    "linux",
    "text",
    "editor",
    "back",
    "hive",
    "shell",
    "let",
    "go",
    "ahead",
    "create",
    "table",
    "employee",
    "want",
    "note",
    "put",
    "semicolon",
    "end",
    "semicolon",
    "tells",
    "execute",
    "line",
    "kind",
    "nice",
    "actually",
    "paste",
    "written",
    "another",
    "sheet",
    "see",
    "right",
    "create",
    "table",
    "employee",
    "goes",
    "next",
    "line",
    "commands",
    "typo",
    "errors",
    "went",
    "ahead",
    "pasted",
    "next",
    "three",
    "lines",
    "next",
    "one",
    "schema",
    "remember",
    "correctly",
    "side",
    "different",
    "values",
    "id",
    "name",
    "department",
    "year",
    "joining",
    "salary",
    "id",
    "integer",
    "name",
    "string",
    "department",
    "string",
    "air",
    "joining",
    "energy",
    "salary",
    "integer",
    "brackets",
    "put",
    "close",
    "brackets",
    "around",
    "could",
    "one",
    "line",
    "row",
    "format",
    "delimited",
    "fields",
    "terminated",
    "comma",
    "important",
    "default",
    "tabs",
    "wo",
    "find",
    "terminated",
    "fields",
    "get",
    "bunch",
    "null",
    "values",
    "loaded",
    "table",
    "finally",
    "table",
    "properties",
    "want",
    "skip",
    "header",
    "line",
    "count",
    "equals",
    "one",
    "lot",
    "work",
    "uploading",
    "single",
    "file",
    "kind",
    "goofy",
    "uploading",
    "single",
    "file",
    "put",
    "keep",
    "mind",
    "hive",
    "hadoop",
    "designed",
    "writing",
    "many",
    "files",
    "database",
    "write",
    "saved",
    "archive",
    "data",
    "warehouse",
    "able",
    "queries",
    "lot",
    "times",
    "looking",
    "one",
    "file",
    "coming",
    "loading",
    "hundreds",
    "files",
    "reports",
    "coming",
    "main",
    "database",
    "reports",
    "loaded",
    "log",
    "files",
    "mean",
    "different",
    "data",
    "dumped",
    "hadoop",
    "case",
    "hive",
    "top",
    "hadoop",
    "need",
    "let",
    "know",
    "hey",
    "handle",
    "files",
    "coming",
    "semicolon",
    "end",
    "lets",
    "us",
    "know",
    "go",
    "ahead",
    "run",
    "line",
    "go",
    "ahead",
    "run",
    "show",
    "tables",
    "see",
    "employee",
    "also",
    "describe",
    "describe",
    "employee",
    "see",
    "id",
    "integer",
    "name",
    "string",
    "department",
    "string",
    "year",
    "joining",
    "integer",
    "salary",
    "integer",
    "finally",
    "let",
    "select",
    "star",
    "employee",
    "basic",
    "sql",
    "nhql",
    "command",
    "selecting",
    "data",
    "going",
    "come",
    "put",
    "anything",
    "expect",
    "data",
    "flip",
    "back",
    "linux",
    "terminal",
    "window",
    "see",
    "cat",
    "see",
    "data",
    "expect",
    "come",
    "also",
    "pwd",
    "right",
    "see",
    "path",
    "need",
    "full",
    "path",
    "loading",
    "data",
    "know",
    "browse",
    "right",
    "name",
    "work",
    "really",
    "bad",
    "habit",
    "general",
    "loading",
    "data",
    "know",
    "else",
    "going",
    "computer",
    "want",
    "full",
    "path",
    "almost",
    "data",
    "loads",
    "let",
    "go",
    "ahead",
    "flip",
    "back",
    "hive",
    "shell",
    "working",
    "command",
    "load",
    "data",
    "says",
    "hey",
    "loading",
    "data",
    "hive",
    "command",
    "hql",
    "want",
    "local",
    "data",
    "got",
    "put",
    "local",
    "path",
    "needs",
    "know",
    "path",
    "make",
    "legible",
    "going",
    "go",
    "ahead",
    "hit",
    "enter",
    "paste",
    "full",
    "path",
    "stored",
    "side",
    "like",
    "good",
    "prepared",
    "demo",
    "see",
    "home",
    "cloudera",
    "documents",
    "whole",
    "path",
    "text",
    "document",
    "go",
    "ahead",
    "hit",
    "enter",
    "let",
    "know",
    "data",
    "going",
    "source",
    "need",
    "destination",
    "going",
    "go",
    "table",
    "call",
    "employee",
    "match",
    "table",
    "wanted",
    "execute",
    "put",
    "semicolon",
    "end",
    "goes",
    "ahead",
    "executes",
    "three",
    "lines",
    "go",
    "back",
    "remember",
    "select",
    "star",
    "employee",
    "using",
    "error",
    "page",
    "different",
    "commands",
    "already",
    "typed",
    "see",
    "right",
    "expect",
    "rows",
    "sam",
    "mike",
    "nick",
    "information",
    "showing",
    "four",
    "rows",
    "let",
    "go",
    "ahead",
    "uh",
    "select",
    "count",
    "let",
    "look",
    "couple",
    "different",
    "select",
    "options",
    "going",
    "count",
    "everything",
    "employee",
    "kind",
    "interesting",
    "first",
    "one",
    "pops",
    "basic",
    "select",
    "need",
    "go",
    "full",
    "map",
    "reduce",
    "phase",
    "start",
    "count",
    "go",
    "full",
    "map",
    "redo",
    "setup",
    "hive",
    "hadoop",
    "demo",
    "single",
    "node",
    "cloudera",
    "virtual",
    "box",
    "top",
    "windows",
    "10",
    "benefits",
    "running",
    "cluster",
    "gone",
    "instead",
    "going",
    "added",
    "layers",
    "takes",
    "longer",
    "run",
    "know",
    "like",
    "said",
    "single",
    "node",
    "said",
    "earlier",
    "good",
    "actual",
    "distribution",
    "running",
    "one",
    "computer",
    "added",
    "different",
    "layers",
    "run",
    "see",
    "comes",
    "four",
    "expect",
    "four",
    "rows",
    "expect",
    "four",
    "end",
    "remember",
    "cheat",
    "sheet",
    "brought",
    "hortons",
    "pretty",
    "good",
    "one",
    "different",
    "commands",
    "look",
    "one",
    "command",
    "call",
    "sub",
    "queries",
    "right",
    "really",
    "common",
    "lot",
    "sub",
    "queries",
    "select",
    "star",
    "different",
    "columns",
    "employee",
    "using",
    "office",
    "database",
    "would",
    "look",
    "like",
    "office",
    "dot",
    "employee",
    "either",
    "one",
    "work",
    "particular",
    "one",
    "office",
    "set",
    "default",
    "office",
    "employee",
    "command",
    "creates",
    "subset",
    "case",
    "want",
    "know",
    "salary",
    "greater",
    "25",
    "go",
    "course",
    "end",
    "semicolon",
    "run",
    "query",
    "see",
    "pops",
    "salaries",
    "people",
    "top",
    "earners",
    "uh",
    "rose",
    "mike",
    "hr",
    "kudos",
    "course",
    "fictional",
    "actually",
    "actually",
    "rose",
    "mic",
    "positions",
    "maybe",
    "finally",
    "want",
    "go",
    "ahead",
    "done",
    "table",
    "remember",
    "dealing",
    "data",
    "warehouse",
    "usually",
    "lot",
    "dropping",
    "tables",
    "databases",
    "going",
    "go",
    "ahead",
    "drop",
    "table",
    "drop",
    "one",
    "quick",
    "note",
    "change",
    "going",
    "going",
    "alter",
    "table",
    "office",
    "employee",
    "want",
    "go",
    "ahead",
    "rename",
    "commands",
    "rename",
    "pretty",
    "common",
    "going",
    "rename",
    "going",
    "stay",
    "office",
    "uh",
    "turns",
    "one",
    "shareholders",
    "really",
    "like",
    "word",
    "employee",
    "wants",
    "employees",
    "plural",
    "big",
    "deal",
    "let",
    "go",
    "ahead",
    "change",
    "name",
    "table",
    "easy",
    "changing",
    "metadata",
    "show",
    "tables",
    "see",
    "employees",
    "employee",
    "point",
    "maybe",
    "house",
    "cleaning",
    "practice",
    "going",
    "go",
    "ahead",
    "drop",
    "table",
    "drop",
    "table",
    "employees",
    "changed",
    "name",
    "employee",
    "give",
    "us",
    "error",
    "show",
    "tables",
    "see",
    "tables",
    "gone",
    "next",
    "thing",
    "want",
    "go",
    "take",
    "look",
    "going",
    "walk",
    "back",
    "loading",
    "data",
    "real",
    "quick",
    "going",
    "load",
    "two",
    "tables",
    "let",
    "float",
    "back",
    "terminal",
    "window",
    "see",
    "tables",
    "loading",
    "customer",
    "customer",
    "file",
    "order",
    "file",
    "want",
    "go",
    "ahead",
    "put",
    "customers",
    "orders",
    "two",
    "course",
    "always",
    "nice",
    "see",
    "working",
    "let",
    "cat",
    "could",
    "always",
    "g",
    "edit",
    "really",
    "need",
    "edit",
    "want",
    "take",
    "look",
    "data",
    "customer",
    "important",
    "header",
    "skip",
    "line",
    "comma",
    "separated",
    "uh",
    "nothing",
    "odd",
    "data",
    "schema",
    "integer",
    "string",
    "integer",
    "string",
    "integer",
    "know",
    "want",
    "take",
    "note",
    "flip",
    "back",
    "forth",
    "let",
    "go",
    "ahead",
    "cat",
    "order",
    "dot",
    "csv",
    "see",
    "oid",
    "guessing",
    "order",
    "id",
    "date",
    "something",
    "new",
    "done",
    "integers",
    "strings",
    "done",
    "date",
    "importing",
    "new",
    "never",
    "worked",
    "date",
    "date",
    "always",
    "one",
    "trickier",
    "fields",
    "port",
    "true",
    "scripting",
    "language",
    "worked",
    "idea",
    "date",
    "supposed",
    "formatted",
    "default",
    "particular",
    "format",
    "year",
    "four",
    "uh",
    "digits",
    "dash",
    "month",
    "two",
    "digits",
    "dash",
    "day",
    "standard",
    "import",
    "hive",
    "look",
    "see",
    "different",
    "formats",
    "going",
    "different",
    "format",
    "coming",
    "able",
    "data",
    "would",
    "data",
    "thing",
    "coming",
    "remember",
    "correctly",
    "edel",
    "uh",
    "e",
    "case",
    "able",
    "hear",
    "last",
    "time",
    "etl",
    "stands",
    "extract",
    "transform",
    "load",
    "want",
    "make",
    "sure",
    "transforming",
    "data",
    "gets",
    "going",
    "go",
    "ahead",
    "bring",
    "data",
    "really",
    "show",
    "basic",
    "join",
    "remember",
    "setup",
    "merge",
    "join",
    "kinds",
    "different",
    "things",
    "joining",
    "different",
    "data",
    "sets",
    "common",
    "really",
    "important",
    "know",
    "need",
    "go",
    "ahead",
    "bring",
    "two",
    "data",
    "sets",
    "see",
    "created",
    "table",
    "customer",
    "schema",
    "integer",
    "name",
    "age",
    "address",
    "salary",
    "eliminated",
    "commas",
    "table",
    "properties",
    "skip",
    "line",
    "well",
    "let",
    "go",
    "ahead",
    "load",
    "data",
    "first",
    "order",
    "let",
    "go",
    "ahead",
    "put",
    "got",
    "split",
    "three",
    "lines",
    "see",
    "easily",
    "got",
    "load",
    "data",
    "local",
    "path",
    "know",
    "loading",
    "data",
    "know",
    "local",
    "path",
    "complete",
    "path",
    "oops",
    "supposed",
    "order",
    "csv",
    "grab",
    "wrong",
    "one",
    "course",
    "going",
    "give",
    "errors",
    "ca",
    "recreate",
    "table",
    "go",
    "create",
    "table",
    "integer",
    "date",
    "customer",
    "basic",
    "setup",
    "coming",
    "schema",
    "row",
    "format",
    "commas",
    "table",
    "properties",
    "skip",
    "header",
    "line",
    "finally",
    "let",
    "load",
    "data",
    "order",
    "table",
    "load",
    "data",
    "local",
    "path",
    "home",
    "cloudera",
    "documents",
    "table",
    "order",
    "everything",
    "right",
    "able",
    "select",
    "star",
    "customer",
    "see",
    "seven",
    "customers",
    "select",
    "star",
    "order",
    "uh",
    "four",
    "orders",
    "uh",
    "like",
    "quick",
    "frame",
    "lot",
    "times",
    "customer",
    "databases",
    "business",
    "thousands",
    "customers",
    "years",
    "years",
    "know",
    "move",
    "close",
    "business",
    "change",
    "names",
    "kinds",
    "things",
    "happen",
    "uh",
    "want",
    "want",
    "go",
    "ahead",
    "find",
    "information",
    "connected",
    "orders",
    "connected",
    "let",
    "go",
    "ahead",
    "select",
    "going",
    "display",
    "information",
    "select",
    "kind",
    "interesting",
    "going",
    "c",
    "dot",
    "id",
    "going",
    "define",
    "c",
    "customer",
    "customer",
    "table",
    "minute",
    "going",
    "c",
    "dot",
    "name",
    "going",
    "define",
    "c",
    "c",
    "dot",
    "age",
    "means",
    "customer",
    "want",
    "know",
    "id",
    "name",
    "age",
    "know",
    "also",
    "like",
    "know",
    "order",
    "amount",
    "uh",
    "let",
    "dot",
    "amount",
    "need",
    "go",
    "ahead",
    "define",
    "uh",
    "going",
    "capitalize",
    "customer",
    "going",
    "take",
    "customer",
    "table",
    "going",
    "name",
    "c",
    "c",
    "comes",
    "customer",
    "table",
    "c",
    "want",
    "join",
    "order",
    "comes",
    "dot",
    "amount",
    "joining",
    "want",
    "got",
    "tell",
    "connect",
    "two",
    "tables",
    "c",
    "dot",
    "id",
    "equals",
    "dot",
    "customer",
    "underscore",
    "id",
    "know",
    "joined",
    "remember",
    "seven",
    "customers",
    "four",
    "orders",
    "processes",
    "get",
    "return",
    "four",
    "different",
    "names",
    "joined",
    "together",
    "joined",
    "based",
    "course",
    "orders",
    "done",
    "order",
    "number",
    "person",
    "made",
    "order",
    "age",
    "amount",
    "order",
    "came",
    "order",
    "table",
    "uh",
    "different",
    "information",
    "see",
    "join",
    "works",
    "common",
    "use",
    "tables",
    "hql",
    "sql",
    "let",
    "one",
    "thing",
    "database",
    "show",
    "couple",
    "hive",
    "commands",
    "let",
    "go",
    "ahead",
    "drop",
    "going",
    "drop",
    "database",
    "office",
    "looking",
    "remember",
    "earlier",
    "give",
    "error",
    "let",
    "see",
    "looks",
    "like",
    "says",
    "fail",
    "execute",
    "exception",
    "one",
    "tables",
    "exist",
    "remember",
    "ca",
    "drop",
    "database",
    "unless",
    "tell",
    "cascade",
    "lets",
    "know",
    "care",
    "many",
    "tables",
    "let",
    "get",
    "rid",
    "hadoop",
    "since",
    "art",
    "warehouse",
    "data",
    "warehouse",
    "usually",
    "lot",
    "dropping",
    "uh",
    "maybe",
    "beginning",
    "developing",
    "schemas",
    "realize",
    "messed",
    "might",
    "drop",
    "stuff",
    "uh",
    "road",
    "really",
    "adding",
    "commodity",
    "machines",
    "take",
    "store",
    "stuff",
    "usually",
    "lot",
    "database",
    "dropping",
    "uh",
    "fun",
    "commands",
    "know",
    "select",
    "round",
    "round",
    "value",
    "round",
    "hive",
    "floor",
    "value",
    "going",
    "give",
    "us",
    "2",
    "turns",
    "integer",
    "versus",
    "float",
    "goes",
    "know",
    "basically",
    "truncates",
    "goes",
    "also",
    "ceiling",
    "going",
    "round",
    "looking",
    "next",
    "integer",
    "commands",
    "show",
    "single",
    "node",
    "admin",
    "help",
    "spediate",
    "process",
    "usually",
    "add",
    "partitions",
    "data",
    "buckets",
    "ca",
    "single",
    "node",
    "add",
    "partition",
    "partitions",
    "across",
    "separate",
    "nodes",
    "beyond",
    "see",
    "straightforward",
    "sql",
    "coming",
    "basic",
    "queries",
    "sql",
    "similar",
    "hql",
    "let",
    "get",
    "started",
    "pig",
    "pig",
    "pig",
    "mapreduce",
    "versus",
    "hive",
    "versus",
    "pig",
    "hopefully",
    "chance",
    "hive",
    "tutorial",
    "mapreduce",
    "tutorial",
    "send",
    "note",
    "simplylearn",
    "follow",
    "link",
    "look",
    "pig",
    "architecture",
    "working",
    "pig",
    "pig",
    "latin",
    "data",
    "model",
    "pig",
    "execution",
    "modes",
    "use",
    "case",
    "twitter",
    "features",
    "pig",
    "tag",
    "short",
    "demo",
    "see",
    "pig",
    "action",
    "pig",
    "know",
    "hadoop",
    "uses",
    "mapreduce",
    "analyze",
    "process",
    "big",
    "data",
    "processing",
    "big",
    "data",
    "consumed",
    "time",
    "hadoop",
    "system",
    "spend",
    "lot",
    "money",
    "huge",
    "set",
    "computers",
    "enterprise",
    "machines",
    "introduced",
    "hadoop",
    "map",
    "reduce",
    "afterwards",
    "processing",
    "big",
    "data",
    "faster",
    "using",
    "mapreduce",
    "problem",
    "map",
    "reduce",
    "prior",
    "2006",
    "mapreduce",
    "programs",
    "written",
    "java",
    "found",
    "difficult",
    "write",
    "lengthy",
    "java",
    "codes",
    "faced",
    "issues",
    "incorporating",
    "maps",
    "sort",
    "reduced",
    "fundamentals",
    "mapreduce",
    "creating",
    "program",
    "see",
    "map",
    "face",
    "shuffle",
    "sort",
    "reduce",
    "phase",
    "eventually",
    "became",
    "difficult",
    "task",
    "maintain",
    "optimize",
    "code",
    "due",
    "processing",
    "time",
    "increased",
    "imagine",
    "manager",
    "trying",
    "go",
    "needed",
    "simple",
    "query",
    "find",
    "data",
    "go",
    "talk",
    "programmers",
    "anytime",
    "wants",
    "anything",
    "big",
    "problem",
    "everybody",
    "wants",
    "programmer",
    "every",
    "manager",
    "team",
    "yahoo",
    "faced",
    "problems",
    "process",
    "analyze",
    "large",
    "data",
    "sets",
    "using",
    "java",
    "codes",
    "complex",
    "lengthy",
    "necessity",
    "develop",
    "easier",
    "way",
    "analyze",
    "large",
    "data",
    "sets",
    "without",
    "using",
    "complex",
    "java",
    "modes",
    "codes",
    "scripts",
    "fun",
    "stuff",
    "apache",
    "pig",
    "developed",
    "yahoo",
    "developed",
    "vision",
    "analyze",
    "process",
    "large",
    "datasets",
    "without",
    "using",
    "complex",
    "java",
    "codes",
    "pig",
    "developed",
    "especially",
    "pig",
    "used",
    "simple",
    "steps",
    "analyze",
    "data",
    "sets",
    "time",
    "efficient",
    "exactly",
    "pik",
    "pig",
    "scripting",
    "platform",
    "runs",
    "hadoop",
    "clusters",
    "designed",
    "process",
    "analyze",
    "large",
    "data",
    "sets",
    "pig",
    "uses",
    "sql",
    "like",
    "queries",
    "definitely",
    "sql",
    "resemble",
    "sql",
    "queries",
    "use",
    "analyze",
    "data",
    "pig",
    "operates",
    "various",
    "types",
    "data",
    "like",
    "structured",
    "unstructured",
    "data",
    "let",
    "take",
    "closer",
    "look",
    "map",
    "reduce",
    "versus",
    "hive",
    "versus",
    "pig",
    "start",
    "compiled",
    "language",
    "mapreduce",
    "hive",
    "sql",
    "like",
    "query",
    "pig",
    "scripting",
    "language",
    "similarities",
    "sql",
    "lot",
    "stuff",
    "remember",
    "sql",
    "like",
    "query",
    "hive",
    "based",
    "looks",
    "structured",
    "data",
    "get",
    "scripting",
    "languages",
    "like",
    "pig",
    "dealing",
    "even",
    "unstructured",
    "data",
    "hadoop",
    "map",
    "reduced",
    "need",
    "write",
    "long",
    "complex",
    "codes",
    "hive",
    "need",
    "write",
    "complex",
    "codes",
    "could",
    "put",
    "simple",
    "sql",
    "query",
    "hql",
    "hive",
    "ql",
    "pig",
    "need",
    "write",
    "complex",
    "codes",
    "pig",
    "latin",
    "remember",
    "map",
    "reduce",
    "produce",
    "structured",
    "unstructured",
    "data",
    "mentioned",
    "hive",
    "process",
    "structured",
    "data",
    "think",
    "rows",
    "columns",
    "pig",
    "process",
    "structured",
    "unstructured",
    "data",
    "think",
    "structured",
    "data",
    "rows",
    "columns",
    "html",
    "xml",
    "documents",
    "like",
    "web",
    "pages",
    "unstructured",
    "could",
    "anything",
    "groups",
    "documents",
    "written",
    "format",
    "twitter",
    "tweets",
    "things",
    "come",
    "unstructured",
    "data",
    "hadoop",
    "map",
    "reduce",
    "lower",
    "level",
    "abstraction",
    "hive",
    "pig",
    "higher",
    "level",
    "abstraction",
    "much",
    "easy",
    "someone",
    "use",
    "without",
    "dive",
    "deep",
    "write",
    "lengthy",
    "map",
    "reduce",
    "code",
    "map",
    "reduce",
    "codes",
    "take",
    "70",
    "80",
    "lines",
    "code",
    "thing",
    "one",
    "two",
    "lines",
    "hive",
    "pig",
    "advantage",
    "pig",
    "hive",
    "process",
    "structured",
    "data",
    "hive",
    "pig",
    "process",
    "structured",
    "unstructured",
    "data",
    "features",
    "note",
    "separates",
    "different",
    "query",
    "languages",
    "look",
    "map",
    "reduce",
    "mapreduce",
    "supports",
    "partitioning",
    "features",
    "hive",
    "pig",
    "concept",
    "partitioning",
    "pix",
    "support",
    "partitioning",
    "feature",
    "partitioning",
    "features",
    "allow",
    "partition",
    "data",
    "way",
    "queried",
    "quicker",
    "able",
    "pig",
    "mapreduce",
    "uses",
    "java",
    "python",
    "hive",
    "uses",
    "sql",
    "like",
    "query",
    "language",
    "known",
    "hive",
    "ql",
    "hql",
    "pig",
    "latin",
    "used",
    "procedural",
    "data",
    "flow",
    "language",
    "mapreduce",
    "used",
    "programmers",
    "pretty",
    "much",
    "straightforward",
    "java",
    "hive",
    "used",
    "data",
    "analysts",
    "pig",
    "used",
    "researchers",
    "programmers",
    "certainly",
    "lot",
    "mix",
    "three",
    "programmers",
    "known",
    "go",
    "use",
    "hive",
    "quick",
    "query",
    "anybody",
    "able",
    "use",
    "pig",
    "quick",
    "query",
    "research",
    "map",
    "reduce",
    "code",
    "performance",
    "really",
    "good",
    "hive",
    "code",
    "performance",
    "lesser",
    "map",
    "reduce",
    "pig",
    "pig",
    "code",
    "performance",
    "lesser",
    "mapreduce",
    "better",
    "hive",
    "going",
    "look",
    "speed",
    "time",
    "map",
    "reduce",
    "going",
    "fastest",
    "performance",
    "pig",
    "second",
    "high",
    "follows",
    "back",
    "let",
    "look",
    "components",
    "pig",
    "pig",
    "two",
    "main",
    "components",
    "pig",
    "latin",
    "pig",
    "latin",
    "procedural",
    "data",
    "flow",
    "language",
    "used",
    "pig",
    "analyze",
    "data",
    "easy",
    "program",
    "using",
    "piglet",
    "similar",
    "sql",
    "runtime",
    "engine",
    "runtime",
    "engine",
    "represents",
    "execution",
    "environment",
    "created",
    "run",
    "pig",
    "latin",
    "programs",
    "also",
    "compiler",
    "produces",
    "mapreduce",
    "programs",
    "uses",
    "hdfs",
    "hadoop",
    "file",
    "system",
    "storing",
    "retrieving",
    "data",
    "dig",
    "deeper",
    "pig",
    "architecture",
    "see",
    "pig",
    "latin",
    "scripts",
    "programmers",
    "write",
    "script",
    "piglet",
    "analyze",
    "data",
    "using",
    "pig",
    "grunt",
    "shell",
    "actually",
    "says",
    "grunt",
    "start",
    "show",
    "little",
    "bit",
    "goes",
    "pig",
    "server",
    "parser",
    "parser",
    "checks",
    "syntax",
    "pig",
    "script",
    "checking",
    "output",
    "dag",
    "directed",
    "acylic",
    "graph",
    "optimizer",
    "optimizes",
    "dag",
    "logical",
    "plan",
    "passed",
    "logical",
    "optimizer",
    "optimization",
    "takes",
    "place",
    "finally",
    "compiler",
    "converts",
    "dag",
    "map",
    "reduce",
    "jobs",
    "executed",
    "map",
    "reduce",
    "execution",
    "engine",
    "results",
    "displayed",
    "using",
    "dump",
    "statement",
    "stored",
    "hdfs",
    "using",
    "store",
    "statement",
    "show",
    "kind",
    "end",
    "always",
    "want",
    "execute",
    "everything",
    "created",
    "dump",
    "kind",
    "execution",
    "statement",
    "see",
    "right",
    "talking",
    "earlier",
    "get",
    "execution",
    "engine",
    "coded",
    "mapreduce",
    "map",
    "reduce",
    "processes",
    "onto",
    "hdfs",
    "working",
    "pig",
    "pig",
    "latin",
    "script",
    "written",
    "users",
    "load",
    "data",
    "write",
    "pig",
    "script",
    "pig",
    "operations",
    "look",
    "working",
    "pig",
    "pig",
    "latin",
    "script",
    "written",
    "users",
    "step",
    "one",
    "load",
    "data",
    "write",
    "pig",
    "script",
    "step",
    "two",
    "step",
    "pig",
    "operations",
    "performed",
    "parser",
    "optimizer",
    "compiler",
    "go",
    "pig",
    "operations",
    "get",
    "step",
    "three",
    "execution",
    "plan",
    "days",
    "results",
    "shown",
    "screen",
    "otherwise",
    "stored",
    "hdfs",
    "per",
    "code",
    "might",
    "small",
    "amount",
    "data",
    "reducing",
    "want",
    "put",
    "screen",
    "might",
    "converting",
    "huge",
    "amount",
    "data",
    "want",
    "put",
    "back",
    "hadoop",
    "file",
    "system",
    "use",
    "let",
    "take",
    "look",
    "pig",
    "latin",
    "data",
    "model",
    "data",
    "model",
    "pig",
    "latin",
    "helps",
    "pig",
    "handle",
    "various",
    "types",
    "data",
    "example",
    "adam",
    "rob",
    "atom",
    "represents",
    "single",
    "value",
    "primitive",
    "data",
    "type",
    "pig",
    "latin",
    "like",
    "integer",
    "float",
    "string",
    "stored",
    "string",
    "tuple",
    "go",
    "atom",
    "basic",
    "things",
    "look",
    "rob",
    "50",
    "atom",
    "basic",
    "object",
    "pig",
    "latin",
    "tuple",
    "tuple",
    "represents",
    "sequence",
    "fields",
    "data",
    "type",
    "row",
    "rdbms",
    "example",
    "set",
    "data",
    "single",
    "row",
    "see",
    "rob",
    "comma",
    "five",
    "imagine",
    "many",
    "examples",
    "used",
    "might",
    "id",
    "number",
    "name",
    "live",
    "age",
    "date",
    "starting",
    "job",
    "would",
    "one",
    "row",
    "stored",
    "tuple",
    "create",
    "bag",
    "bag",
    "collection",
    "tuples",
    "table",
    "rdbms",
    "represented",
    "brackets",
    "see",
    "table",
    "rob5",
    "mic",
    "10",
    "also",
    "map",
    "map",
    "set",
    "key",
    "value",
    "pairs",
    "key",
    "character",
    "array",
    "type",
    "value",
    "type",
    "represented",
    "brackets",
    "name",
    "age",
    "key",
    "value",
    "mic",
    "pig",
    "latin",
    "fully",
    "nestable",
    "data",
    "model",
    "means",
    "one",
    "data",
    "type",
    "nested",
    "within",
    "another",
    "diagram",
    "representation",
    "pig",
    "latin",
    "data",
    "model",
    "particular",
    "example",
    "basically",
    "id",
    "number",
    "name",
    "age",
    "place",
    "break",
    "apart",
    "look",
    "model",
    "pig",
    "latin",
    "perspective",
    "start",
    "field",
    "remember",
    "field",
    "contains",
    "basically",
    "atom",
    "one",
    "particular",
    "data",
    "type",
    "atom",
    "stored",
    "string",
    "converts",
    "either",
    "integer",
    "number",
    "character",
    "string",
    "next",
    "tuple",
    "case",
    "see",
    "represents",
    "row",
    "tuple",
    "would",
    "three",
    "comma",
    "joe",
    "comma",
    "29",
    "comma",
    "california",
    "finally",
    "bag",
    "contains",
    "three",
    "rows",
    "particular",
    "example",
    "let",
    "take",
    "quick",
    "look",
    "pig",
    "execution",
    "modes",
    "pig",
    "works",
    "two",
    "execution",
    "modes",
    "depending",
    "data",
    "reciting",
    "pig",
    "script",
    "going",
    "run",
    "local",
    "mode",
    "pig",
    "engine",
    "takes",
    "input",
    "linux",
    "file",
    "system",
    "output",
    "stored",
    "file",
    "system",
    "local",
    "mold",
    "local",
    "mode",
    "useful",
    "analyzing",
    "small",
    "data",
    "sets",
    "using",
    "pig",
    "map",
    "reduced",
    "mode",
    "pig",
    "engine",
    "directly",
    "interacts",
    "executes",
    "hdfs",
    "mapreduce",
    "map",
    "reduce",
    "mode",
    "queries",
    "written",
    "pig",
    "latin",
    "translated",
    "mapreduce",
    "jobs",
    "run",
    "hadoop",
    "cluster",
    "default",
    "pig",
    "runs",
    "mode",
    "three",
    "modes",
    "pig",
    "depending",
    "pig",
    "latin",
    "code",
    "written",
    "interactive",
    "mode",
    "batch",
    "mode",
    "embedded",
    "mode",
    "interactive",
    "mode",
    "means",
    "coding",
    "executing",
    "script",
    "line",
    "line",
    "example",
    "interactive",
    "mode",
    "badge",
    "mode",
    "scripts",
    "coded",
    "file",
    "file",
    "directly",
    "executed",
    "embedded",
    "mode",
    "pig",
    "lets",
    "users",
    "define",
    "functions",
    "udfss",
    "programming",
    "language",
    "java",
    "let",
    "take",
    "look",
    "see",
    "works",
    "use",
    "case",
    "case",
    "use",
    "case",
    "twitter",
    "users",
    "twitter",
    "generate",
    "500",
    "million",
    "tweets",
    "daily",
    "basis",
    "hadoop",
    "mapreduce",
    "used",
    "process",
    "analyze",
    "data",
    "analyzing",
    "number",
    "tweets",
    "created",
    "user",
    "tweet",
    "table",
    "done",
    "using",
    "mapreduce",
    "java",
    "programming",
    "language",
    "see",
    "problem",
    "difficult",
    "perform",
    "mapreduce",
    "operations",
    "users",
    "well",
    "versed",
    "written",
    "complex",
    "java",
    "codes",
    "twitter",
    "used",
    "apache",
    "pig",
    "overcome",
    "problems",
    "let",
    "see",
    "let",
    "start",
    "problem",
    "statement",
    "analyze",
    "user",
    "table",
    "tweet",
    "table",
    "find",
    "many",
    "tweets",
    "created",
    "person",
    "see",
    "user",
    "table",
    "alice",
    "tim",
    "john",
    "id",
    "numbers",
    "one",
    "two",
    "three",
    "tweet",
    "table",
    "tweet",
    "table",
    "um",
    "id",
    "user",
    "tweeted",
    "uh",
    "google",
    "good",
    "whatever",
    "tennis",
    "dot",
    "dot",
    "spacecraft",
    "olympics",
    "politics",
    "whatever",
    "tweeting",
    "following",
    "operations",
    "performed",
    "analyzing",
    "given",
    "data",
    "first",
    "twitter",
    "data",
    "loaded",
    "pig",
    "storage",
    "using",
    "load",
    "command",
    "see",
    "data",
    "coming",
    "going",
    "pig",
    "storage",
    "data",
    "probably",
    "enterprise",
    "computer",
    "actually",
    "active",
    "twitter",
    "going",
    "goes",
    "hadoop",
    "file",
    "system",
    "remember",
    "hadoop",
    "file",
    "system",
    "data",
    "warehouse",
    "storing",
    "data",
    "first",
    "step",
    "want",
    "go",
    "ahead",
    "load",
    "pig",
    "storage",
    "data",
    "storage",
    "system",
    "remaining",
    "operations",
    "performed",
    "shown",
    "join",
    "group",
    "operation",
    "tweet",
    "user",
    "tables",
    "joined",
    "grouped",
    "using",
    "command",
    "see",
    "add",
    "whole",
    "column",
    "go",
    "user",
    "names",
    "tweet",
    "id",
    "link",
    "directly",
    "name",
    "alice",
    "user",
    "1",
    "10",
    "2",
    "john",
    "listed",
    "actual",
    "tweet",
    "next",
    "operation",
    "aggregation",
    "tweets",
    "counted",
    "according",
    "names",
    "command",
    "used",
    "count",
    "straightforward",
    "want",
    "count",
    "many",
    "tweets",
    "user",
    "finally",
    "result",
    "count",
    "operation",
    "joined",
    "user",
    "table",
    "find",
    "username",
    "see",
    "alice",
    "3",
    "tim",
    "2",
    "john",
    "pig",
    "reduces",
    "complexity",
    "operations",
    "would",
    "lengthy",
    "using",
    "mapreduce",
    "joining",
    "group",
    "operation",
    "tweet",
    "user",
    "tables",
    "joined",
    "grouped",
    "using",
    "command",
    "next",
    "operation",
    "aggregation",
    "tweets",
    "counted",
    "according",
    "names",
    "command",
    "used",
    "count",
    "result",
    "count",
    "operation",
    "joined",
    "user",
    "table",
    "find",
    "username",
    "see",
    "talking",
    "three",
    "lines",
    "script",
    "versus",
    "mapreduce",
    "code",
    "80",
    "lines",
    "finally",
    "could",
    "find",
    "number",
    "tweets",
    "created",
    "user",
    "simple",
    "way",
    "let",
    "go",
    "quickly",
    "features",
    "pig",
    "already",
    "went",
    "first",
    "ease",
    "programming",
    "pig",
    "latin",
    "similar",
    "sql",
    "lesser",
    "lines",
    "code",
    "need",
    "written",
    "short",
    "development",
    "time",
    "code",
    "simpler",
    "get",
    "queries",
    "rather",
    "quickly",
    "instead",
    "programmer",
    "spend",
    "hours",
    "handles",
    "kinds",
    "data",
    "like",
    "structured",
    "unstructured",
    "pig",
    "lets",
    "us",
    "create",
    "user",
    "defined",
    "functions",
    "pig",
    "offers",
    "large",
    "set",
    "operators",
    "join",
    "filter",
    "allows",
    "multiple",
    "queries",
    "process",
    "parallel",
    "optimization",
    "compilation",
    "easy",
    "done",
    "automatically",
    "internally",
    "enough",
    "theory",
    "let",
    "dive",
    "show",
    "quick",
    "demo",
    "commands",
    "pick",
    "today",
    "setup",
    "continue",
    "last",
    "three",
    "demos",
    "go",
    "use",
    "cloudera",
    "quickstart",
    "virtualbox",
    "tutorial",
    "setting",
    "send",
    "note",
    "simplylearn",
    "team",
    "get",
    "linked",
    "cloudera",
    "quickstart",
    "spun",
    "remember",
    "virtualbox",
    "created",
    "virtual",
    "machine",
    "virtual",
    "machine",
    "centos",
    "linux",
    "spun",
    "full",
    "linux",
    "system",
    "see",
    "thunderbird",
    "browser",
    "opens",
    "hadoop",
    "basic",
    "system",
    "browser",
    "go",
    "underneath",
    "hue",
    "comes",
    "default",
    "click",
    "pull",
    "menu",
    "go",
    "editor",
    "see",
    "impala",
    "hive",
    "pig",
    "along",
    "bunch",
    "query",
    "languages",
    "use",
    "going",
    "pig",
    "pig",
    "go",
    "ahead",
    "use",
    "command",
    "line",
    "click",
    "little",
    "blue",
    "button",
    "start",
    "running",
    "actually",
    "working",
    "terminal",
    "window",
    "cloud",
    "era",
    "quick",
    "start",
    "open",
    "terminal",
    "window",
    "top",
    "setup",
    "logged",
    "easily",
    "use",
    "commands",
    "terminal",
    "window",
    "zoom",
    "way",
    "get",
    "nice",
    "view",
    "going",
    "go",
    "first",
    "command",
    "going",
    "hadoop",
    "command",
    "import",
    "data",
    "hadoop",
    "system",
    "case",
    "pig",
    "input",
    "let",
    "take",
    "look",
    "hadoop",
    "let",
    "know",
    "going",
    "hadoop",
    "command",
    "dfs",
    "actually",
    "four",
    "variations",
    "dfs",
    "hdfs",
    "whatever",
    "fine",
    "four",
    "point",
    "used",
    "different",
    "setups",
    "underneath",
    "different",
    "things",
    "thing",
    "want",
    "put",
    "file",
    "case",
    "home",
    "cloudera",
    "documents",
    "sample",
    "want",
    "take",
    "put",
    "pig",
    "input",
    "let",
    "take",
    "look",
    "file",
    "go",
    "document",
    "browsers",
    "open",
    "see",
    "got",
    "simple",
    "id",
    "name",
    "profession",
    "age",
    "one",
    "jack",
    "engineer",
    "25",
    "one",
    "earlier",
    "things",
    "let",
    "go",
    "ahead",
    "hit",
    "enter",
    "execute",
    "uploaded",
    "data",
    "gone",
    "pig",
    "input",
    "lot",
    "hadoop",
    "commands",
    "mimic",
    "linux",
    "commands",
    "see",
    "cat",
    "one",
    "commands",
    "hyphen",
    "execute",
    "hadoop",
    "dfs",
    "hyphen",
    "cat",
    "slash",
    "pig",
    "input",
    "called",
    "put",
    "sample",
    "csv",
    "execute",
    "see",
    "hadoop",
    "system",
    "going",
    "go",
    "pull",
    "sure",
    "enough",
    "pulls",
    "data",
    "file",
    "put",
    "simply",
    "enter",
    "pig",
    "latin",
    "pig",
    "editor",
    "mode",
    "typing",
    "pig",
    "see",
    "grunt",
    "told",
    "going",
    "tell",
    "pig",
    "latin",
    "grunt",
    "command",
    "line",
    "pig",
    "shell",
    "go",
    "ahead",
    "put",
    "load",
    "command",
    "way",
    "works",
    "going",
    "office",
    "equals",
    "load",
    "load",
    "case",
    "going",
    "pig",
    "input",
    "single",
    "brackets",
    "remember",
    "data",
    "hadoop",
    "file",
    "system",
    "dumped",
    "going",
    "using",
    "pig",
    "storage",
    "data",
    "separated",
    "comma",
    "comma",
    "separator",
    "case",
    "id",
    "character",
    "array",
    "name",
    "character",
    "array",
    "profession",
    "character",
    "age",
    "character",
    "ray",
    "going",
    "character",
    "arrays",
    "keep",
    "simple",
    "one",
    "hit",
    "put",
    "see",
    "full",
    "command",
    "line",
    "going",
    "semicolon",
    "end",
    "hit",
    "enter",
    "set",
    "office",
    "actually",
    "done",
    "anything",
    "yet",
    "anything",
    "dump",
    "office",
    "command",
    "execute",
    "whatever",
    "loaded",
    "whatever",
    "setup",
    "run",
    "see",
    "go",
    "different",
    "languages",
    "going",
    "map",
    "reduce",
    "remember",
    "locally",
    "hadoop",
    "setup",
    "finished",
    "dump",
    "see",
    "id",
    "name",
    "profession",
    "age",
    "information",
    "dumped",
    "pick",
    "oh",
    "let",
    "say",
    "oh",
    "let",
    "say",
    "request",
    "keep",
    "simple",
    "name",
    "age",
    "go",
    "office",
    "call",
    "variable",
    "underscore",
    "say",
    "office",
    "generate",
    "name",
    "come",
    "h",
    "means",
    "going",
    "row",
    "thinking",
    "map",
    "reduce",
    "know",
    "map",
    "function",
    "mapping",
    "row",
    "generating",
    "name",
    "age",
    "course",
    "want",
    "go",
    "ahead",
    "close",
    "semicolon",
    "created",
    "query",
    "command",
    "line",
    "let",
    "go",
    "ahead",
    "dump",
    "office",
    "underscore",
    "semicolon",
    "go",
    "reduce",
    "setup",
    "large",
    "cluster",
    "processing",
    "time",
    "would",
    "happen",
    "fact",
    "really",
    "slow",
    "multiple",
    "things",
    "computer",
    "particular",
    "virtual",
    "box",
    "using",
    "quarter",
    "processor",
    "dedicated",
    "see",
    "name",
    "age",
    "also",
    "included",
    "top",
    "row",
    "since",
    "delete",
    "tell",
    "fine",
    "example",
    "need",
    "aware",
    "things",
    "processing",
    "significantly",
    "large",
    "amount",
    "data",
    "data",
    "also",
    "office",
    "call",
    "dsc",
    "descending",
    "maybe",
    "boss",
    "comes",
    "says",
    "hey",
    "order",
    "office",
    "id",
    "descending",
    "course",
    "boss",
    "taught",
    "uh",
    "shareholder",
    "sounds",
    "little",
    "say",
    "boss",
    "talked",
    "shareholder",
    "said",
    "taught",
    "little",
    "bit",
    "pig",
    "latin",
    "know",
    "create",
    "office",
    "description",
    "order",
    "office",
    "id",
    "description",
    "course",
    "dump",
    "office",
    "underscore",
    "description",
    "actually",
    "execute",
    "goes",
    "map",
    "reduce",
    "take",
    "moment",
    "come",
    "running",
    "quarter",
    "processor",
    "see",
    "ids",
    "descending",
    "order",
    "returned",
    "let",
    "also",
    "look",
    "important",
    "anytime",
    "dealing",
    "big",
    "data",
    "let",
    "create",
    "office",
    "limit",
    "course",
    "instead",
    "office",
    "could",
    "office",
    "descending",
    "get",
    "top",
    "two",
    "ids",
    "gon",
    "na",
    "limit",
    "two",
    "course",
    "execute",
    "dump",
    "office",
    "underscore",
    "limit",
    "think",
    "dumping",
    "garbage",
    "pig",
    "pin",
    "pig",
    "eat",
    "go",
    "dump",
    "office",
    "limit",
    "two",
    "going",
    "limit",
    "office",
    "top",
    "two",
    "output",
    "get",
    "first",
    "row",
    "id",
    "name",
    "profession",
    "age",
    "second",
    "row",
    "jack",
    "engineer",
    "let",
    "uh",
    "filter",
    "call",
    "office",
    "underscore",
    "filter",
    "guessed",
    "equals",
    "filter",
    "office",
    "profession",
    "equals",
    "keep",
    "note",
    "uh",
    "similar",
    "python",
    "double",
    "equal",
    "signs",
    "equal",
    "true",
    "false",
    "statement",
    "logic",
    "statement",
    "remember",
    "use",
    "two",
    "equal",
    "signs",
    "pig",
    "going",
    "say",
    "equals",
    "doctor",
    "want",
    "find",
    "many",
    "doctors",
    "list",
    "go",
    "ahead",
    "dump",
    "dumping",
    "garbage",
    "pig",
    "pen",
    "letting",
    "pig",
    "take",
    "see",
    "find",
    "see",
    "doctor",
    "list",
    "find",
    "uh",
    "employee",
    "id",
    "number",
    "two",
    "bob",
    "doctor",
    "30",
    "years",
    "old",
    "next",
    "section",
    "uh",
    "going",
    "cover",
    "something",
    "see",
    "lot",
    "nowadays",
    "data",
    "analysis",
    "word",
    "counting",
    "tokenization",
    "one",
    "next",
    "big",
    "steps",
    "move",
    "forward",
    "data",
    "analysis",
    "go",
    "say",
    "stock",
    "market",
    "analysis",
    "highs",
    "lows",
    "numbers",
    "people",
    "saying",
    "companies",
    "twitter",
    "saying",
    "web",
    "pages",
    "facebook",
    "suddenly",
    "need",
    "start",
    "counting",
    "words",
    "finding",
    "many",
    "words",
    "totaled",
    "many",
    "first",
    "part",
    "document",
    "going",
    "cover",
    "basic",
    "word",
    "count",
    "example",
    "case",
    "created",
    "document",
    "called",
    "see",
    "simplylearn",
    "company",
    "supporting",
    "online",
    "learning",
    "simplylearn",
    "helps",
    "people",
    "attain",
    "certifications",
    "simply",
    "learn",
    "online",
    "community",
    "love",
    "simply",
    "learn",
    "love",
    "programming",
    "love",
    "data",
    "analysis",
    "went",
    "saved",
    "documents",
    "folder",
    "could",
    "use",
    "let",
    "go",
    "ahead",
    "open",
    "new",
    "terminal",
    "window",
    "word",
    "count",
    "let",
    "go",
    "close",
    "old",
    "one",
    "going",
    "go",
    "instead",
    "pig",
    "going",
    "pig",
    "minus",
    "x",
    "local",
    "telling",
    "pig",
    "start",
    "pig",
    "shell",
    "going",
    "looking",
    "files",
    "local",
    "virtualbox",
    "centos",
    "machine",
    "let",
    "go",
    "ahead",
    "hit",
    "enter",
    "maximize",
    "go",
    "load",
    "pig",
    "going",
    "look",
    "pig",
    "defaulted",
    "hi",
    "hadoop",
    "system",
    "hdfs",
    "defaulted",
    "local",
    "system",
    "going",
    "create",
    "lines",
    "going",
    "load",
    "straight",
    "file",
    "remember",
    "last",
    "time",
    "took",
    "hdfs",
    "loaded",
    "loaded",
    "pig",
    "since",
    "gone",
    "local",
    "going",
    "run",
    "local",
    "script",
    "lines",
    "equals",
    "load",
    "home",
    "actual",
    "full",
    "path",
    "home",
    "cloudera",
    "documents",
    "called",
    "line",
    "character",
    "array",
    "line",
    "actually",
    "change",
    "read",
    "document",
    "certainly",
    "done",
    "lot",
    "document",
    "analysis",
    "go",
    "word",
    "counts",
    "different",
    "kind",
    "counts",
    "go",
    "ahead",
    "create",
    "line",
    "instead",
    "dump",
    "going",
    "go",
    "ahead",
    "start",
    "entering",
    "different",
    "setups",
    "steps",
    "want",
    "go",
    "let",
    "take",
    "look",
    "next",
    "one",
    "load",
    "straightforward",
    "loading",
    "particular",
    "file",
    "since",
    "locals",
    "loading",
    "directly",
    "instead",
    "going",
    "hadoop",
    "file",
    "system",
    "says",
    "line",
    "read",
    "character",
    "array",
    "going",
    "words",
    "equal",
    "lines",
    "generate",
    "flat",
    "tokenize",
    "line",
    "space",
    "word",
    "lot",
    "ways",
    "programmer",
    "splitting",
    "line",
    "spaces",
    "actual",
    "ways",
    "tokenize",
    "look",
    "periods",
    "capitalization",
    "kinds",
    "things",
    "play",
    "basic",
    "word",
    "count",
    "going",
    "separate",
    "spaces",
    "flatten",
    "takes",
    "line",
    "creates",
    "flattens",
    "words",
    "going",
    "generate",
    "bunch",
    "words",
    "line",
    "words",
    "word",
    "little",
    "confusing",
    "really",
    "think",
    "going",
    "line",
    "separating",
    "generating",
    "list",
    "words",
    "one",
    "thing",
    "note",
    "default",
    "tokenize",
    "tokenized",
    "line",
    "without",
    "space",
    "automatically",
    "tokenize",
    "space",
    "either",
    "one",
    "going",
    "group",
    "going",
    "group",
    "words",
    "going",
    "group",
    "words",
    "word",
    "split",
    "token",
    "word",
    "list",
    "words",
    "going",
    "group",
    "equals",
    "group",
    "words",
    "words",
    "going",
    "group",
    "words",
    "together",
    "going",
    "group",
    "want",
    "go",
    "ahead",
    "count",
    "count",
    "go",
    "ahead",
    "create",
    "word",
    "count",
    "variable",
    "four",
    "grouped",
    "grouped",
    "line",
    "group",
    "words",
    "line",
    "similar",
    "going",
    "generate",
    "group",
    "going",
    "count",
    "words",
    "group",
    "line",
    "group",
    "words",
    "together",
    "going",
    "generate",
    "group",
    "going",
    "count",
    "words",
    "want",
    "know",
    "word",
    "count",
    "comes",
    "back",
    "word",
    "count",
    "finally",
    "want",
    "take",
    "want",
    "go",
    "ahead",
    "dump",
    "word",
    "count",
    "little",
    "bit",
    "see",
    "start",
    "looking",
    "grunt",
    "scripts",
    "see",
    "right",
    "lines",
    "right",
    "steps",
    "take",
    "get",
    "load",
    "file",
    "lines",
    "going",
    "generate",
    "tokenize",
    "words",
    "going",
    "take",
    "words",
    "going",
    "group",
    "words",
    "group",
    "going",
    "generate",
    "group",
    "going",
    "count",
    "words",
    "going",
    "summarize",
    "words",
    "let",
    "go",
    "ahead",
    "dump",
    "word",
    "count",
    "executes",
    "goes",
    "map",
    "reduce",
    "actually",
    "local",
    "runner",
    "see",
    "start",
    "seeing",
    "still",
    "map",
    "reduced",
    "special",
    "runner",
    "mapping",
    "part",
    "row",
    "counted",
    "grouped",
    "word",
    "count",
    "reducer",
    "reducer",
    "crazy",
    "keys",
    "see",
    "used",
    "three",
    "times",
    "came",
    "came",
    "continue",
    "attain",
    "online",
    "people",
    "company",
    "analysis",
    "simply",
    "learn",
    "took",
    "top",
    "rating",
    "four",
    "certification",
    "things",
    "counted",
    "many",
    "words",
    "used",
    "data",
    "analysis",
    "probably",
    "beginnings",
    "data",
    "analysis",
    "might",
    "look",
    "say",
    "oh",
    "mentioned",
    "love",
    "three",
    "times",
    "whatever",
    "going",
    "post",
    "love",
    "love",
    "might",
    "attach",
    "different",
    "objects",
    "see",
    "pig",
    "latin",
    "fairly",
    "easy",
    "use",
    "nothing",
    "really",
    "know",
    "takes",
    "little",
    "bit",
    "learn",
    "script",
    "uh",
    "depending",
    "good",
    "memory",
    "get",
    "older",
    "memory",
    "leaks",
    "little",
    "bit",
    "memorize",
    "much",
    "pretty",
    "straightforward",
    "script",
    "put",
    "goes",
    "full",
    "map",
    "reduce",
    "localized",
    "run",
    "comes",
    "like",
    "said",
    "easy",
    "use",
    "people",
    "like",
    "pig",
    "latin",
    "intuitive",
    "one",
    "things",
    "like",
    "pig",
    "latin",
    "troubleshooting",
    "troubleshooting",
    "lot",
    "times",
    "working",
    "small",
    "amount",
    "data",
    "start",
    "one",
    "line",
    "time",
    "go",
    "lines",
    "equal",
    "load",
    "loaded",
    "text",
    "maybe",
    "dump",
    "lines",
    "going",
    "run",
    "going",
    "show",
    "lines",
    "working",
    "small",
    "amount",
    "data",
    "way",
    "test",
    "got",
    "error",
    "said",
    "oh",
    "working",
    "maybe",
    "like",
    "oh",
    "gosh",
    "map",
    "reduce",
    "basic",
    "grunt",
    "shell",
    "instead",
    "local",
    "path",
    "grunt",
    "let",
    "start",
    "introduction",
    "hbase",
    "back",
    "days",
    "data",
    "used",
    "less",
    "mostly",
    "structured",
    "see",
    "structured",
    "data",
    "usually",
    "like",
    "database",
    "uh",
    "every",
    "field",
    "exactly",
    "correct",
    "length",
    "name",
    "field",
    "exactly",
    "32",
    "characters",
    "remember",
    "old",
    "access",
    "database",
    "microsoft",
    "files",
    "small",
    "know",
    "hundreds",
    "people",
    "one",
    "database",
    "considered",
    "big",
    "data",
    "data",
    "could",
    "easily",
    "stored",
    "relational",
    "database",
    "rdbms",
    "talk",
    "relational",
    "database",
    "might",
    "think",
    "oracle",
    "might",
    "think",
    "sql",
    "microsoft",
    "sql",
    "mysql",
    "evolved",
    "even",
    "back",
    "lot",
    "today",
    "still",
    "fall",
    "short",
    "lot",
    "ways",
    "examples",
    "rdms",
    "relationship",
    "database",
    "internet",
    "evolved",
    "huge",
    "volumes",
    "structured",
    "data",
    "got",
    "generated",
    "see",
    "data",
    "email",
    "look",
    "spam",
    "folder",
    "know",
    "talking",
    "html",
    "pages",
    "xml",
    "lot",
    "time",
    "displayed",
    "html",
    "help",
    "desk",
    "pages",
    "json",
    "really",
    "even",
    "last",
    "year",
    "almost",
    "doubles",
    "year",
    "much",
    "generated",
    "storing",
    "processing",
    "data",
    "rdbms",
    "become",
    "major",
    "problem",
    "solution",
    "use",
    "apache",
    "age",
    "base",
    "apache",
    "hbase",
    "solution",
    "let",
    "take",
    "look",
    "history",
    "hbase",
    "history",
    "look",
    "hbase",
    "history",
    "going",
    "start",
    "back",
    "2006",
    "november",
    "google",
    "released",
    "paper",
    "big",
    "table",
    "2017",
    "months",
    "later",
    "age",
    "based",
    "prototype",
    "created",
    "hadoop",
    "contribution",
    "later",
    "year",
    "2007",
    "october",
    "first",
    "usable",
    "hbase",
    "along",
    "hadoop",
    "released",
    "january",
    "2008",
    "hbase",
    "became",
    "subproject",
    "hadoop",
    "later",
    "year",
    "october",
    "way",
    "september",
    "next",
    "year",
    "hbase",
    "released",
    "version",
    "version",
    "finally",
    "may",
    "2010",
    "hbase",
    "became",
    "apache",
    "top",
    "level",
    "project",
    "see",
    "course",
    "four",
    "years",
    "hbase",
    "started",
    "idea",
    "paper",
    "evolved",
    "way",
    "till",
    "2010",
    "solid",
    "project",
    "apache",
    "since",
    "2010",
    "continued",
    "evolve",
    "grow",
    "major",
    "source",
    "storing",
    "data",
    "data",
    "hbase",
    "hbase",
    "column",
    "oriented",
    "database",
    "management",
    "system",
    "derived",
    "google",
    "nosql",
    "database",
    "bigtable",
    "runs",
    "top",
    "hadoop",
    "file",
    "system",
    "hdfs",
    "open",
    "source",
    "project",
    "horizontally",
    "scalable",
    "important",
    "understand",
    "buy",
    "bunch",
    "huge",
    "expensive",
    "computers",
    "expanding",
    "continually",
    "adding",
    "commodity",
    "machines",
    "linear",
    "cost",
    "expansion",
    "opposed",
    "exponential",
    "sql",
    "database",
    "written",
    "java",
    "permits",
    "faster",
    "querying",
    "java",
    "back",
    "end",
    "hbase",
    "setup",
    "well",
    "suited",
    "sparse",
    "data",
    "sets",
    "contain",
    "missing",
    "n",
    "values",
    "boggle",
    "like",
    "would",
    "database",
    "companies",
    "using",
    "hbase",
    "let",
    "take",
    "look",
    "see",
    "using",
    "uh",
    "nosql",
    "database",
    "servers",
    "storing",
    "data",
    "hortonworks",
    "surprise",
    "one",
    "like",
    "cloudera",
    "hortonworks",
    "behind",
    "hadoop",
    "one",
    "big",
    "developments",
    "backing",
    "course",
    "apache",
    "hbase",
    "open",
    "source",
    "behind",
    "capital",
    "one",
    "banks",
    "also",
    "see",
    "bank",
    "america",
    "collecting",
    "information",
    "people",
    "tracking",
    "uh",
    "information",
    "might",
    "sparse",
    "might",
    "one",
    "bank",
    "way",
    "back",
    "collected",
    "information",
    "far",
    "person",
    "family",
    "income",
    "whole",
    "family",
    "personal",
    "income",
    "maybe",
    "another",
    "one",
    "collect",
    "family",
    "income",
    "start",
    "seeing",
    "data",
    "uh",
    "difficult",
    "store",
    "missing",
    "bunch",
    "data",
    "hubspot",
    "using",
    "facebook",
    "uh",
    "certainly",
    "facebook",
    "twitter",
    "social",
    "medias",
    "using",
    "course",
    "jp",
    "morgan",
    "chase",
    "company",
    "another",
    "bank",
    "uses",
    "hbase",
    "data",
    "warehouse",
    "nos",
    "sql",
    "let",
    "take",
    "look",
    "hbase",
    "use",
    "case",
    "dig",
    "little",
    "bit",
    "see",
    "functions",
    "telecommunication",
    "company",
    "provides",
    "mobile",
    "voice",
    "multimedia",
    "services",
    "across",
    "china",
    "china",
    "mobile",
    "china",
    "mobile",
    "generate",
    "billions",
    "call",
    "detailed",
    "records",
    "cdr",
    "cdrs",
    "records",
    "calls",
    "long",
    "different",
    "aspects",
    "call",
    "maybe",
    "tower",
    "broadcasted",
    "recorded",
    "track",
    "traditional",
    "database",
    "systems",
    "unable",
    "scale",
    "vast",
    "volumes",
    "data",
    "provide",
    "solution",
    "good",
    "storing",
    "analysis",
    "billions",
    "call",
    "records",
    "major",
    "problem",
    "company",
    "solution",
    "apache",
    "hbase",
    "hbase",
    "stores",
    "billions",
    "rows",
    "detailed",
    "call",
    "records",
    "hbc",
    "performs",
    "fast",
    "processing",
    "records",
    "using",
    "sql",
    "queries",
    "mix",
    "sql",
    "nosql",
    "queries",
    "usually",
    "say",
    "sql",
    "queries",
    "way",
    "query",
    "works",
    "applications",
    "hbase",
    "one",
    "would",
    "medical",
    "industry",
    "hbase",
    "used",
    "storing",
    "genome",
    "sequences",
    "storing",
    "disease",
    "history",
    "people",
    "area",
    "imagine",
    "sparse",
    "far",
    "genome",
    "sequence",
    "might",
    "pieces",
    "person",
    "unique",
    "unique",
    "different",
    "people",
    "thing",
    "disease",
    "really",
    "need",
    "column",
    "every",
    "possible",
    "disease",
    "person",
    "could",
    "get",
    "want",
    "know",
    "diseases",
    "people",
    "deal",
    "area",
    "hbase",
    "used",
    "storing",
    "logs",
    "customer",
    "search",
    "history",
    "performs",
    "analytics",
    "target",
    "advertisement",
    "better",
    "business",
    "insights",
    "sports",
    "hba",
    "stores",
    "match",
    "details",
    "history",
    "match",
    "uses",
    "data",
    "better",
    "prediction",
    "look",
    "hbase",
    "want",
    "know",
    "difference",
    "hbase",
    "versus",
    "rdbms",
    "relational",
    "database",
    "management",
    "system",
    "hbase",
    "versus",
    "rdbms",
    "hbase",
    "fixed",
    "schema",
    "defines",
    "column",
    "families",
    "show",
    "means",
    "later",
    "rdbms",
    "fixed",
    "schema",
    "describes",
    "structure",
    "tables",
    "think",
    "row",
    "columns",
    "column",
    "specific",
    "structure",
    "much",
    "data",
    "go",
    "hbase",
    "works",
    "well",
    "structured",
    "data",
    "rdbms",
    "works",
    "well",
    "structured",
    "data",
    "hbase",
    "denormalized",
    "data",
    "contain",
    "missing",
    "null",
    "values",
    "rdbms",
    "store",
    "normalized",
    "data",
    "still",
    "store",
    "null",
    "value",
    "rdbms",
    "still",
    "takes",
    "space",
    "storing",
    "regular",
    "value",
    "many",
    "cases",
    "also",
    "hbase",
    "built",
    "tables",
    "scaled",
    "horizontally",
    "instance",
    "tokenizer",
    "words",
    "word",
    "clusters",
    "might",
    "million",
    "different",
    "words",
    "pulling",
    "combinations",
    "words",
    "rdbms",
    "built",
    "thin",
    "tables",
    "hard",
    "scale",
    "want",
    "store",
    "million",
    "columns",
    "sql",
    "going",
    "crash",
    "going",
    "hard",
    "searches",
    "age",
    "base",
    "stores",
    "data",
    "part",
    "whatever",
    "row",
    "working",
    "let",
    "look",
    "features",
    "hbase",
    "scalable",
    "data",
    "scaled",
    "across",
    "various",
    "nodes",
    "stored",
    "hdfs",
    "always",
    "think",
    "linear",
    "terabyte",
    "data",
    "adding",
    "roughly",
    "thousand",
    "dollars",
    "commodity",
    "computing",
    "enterprise",
    "machine",
    "looking",
    "10",
    "000",
    "lower",
    "end",
    "terabyte",
    "data",
    "includes",
    "backup",
    "redundancy",
    "big",
    "difference",
    "like",
    "tenth",
    "cost",
    "store",
    "across",
    "hbase",
    "automatic",
    "failure",
    "support",
    "right",
    "ahead",
    "log",
    "across",
    "clusters",
    "provides",
    "automatic",
    "support",
    "failure",
    "consistent",
    "read",
    "write",
    "hbase",
    "provides",
    "consistent",
    "read",
    "write",
    "data",
    "java",
    "api",
    "client",
    "access",
    "provides",
    "easy",
    "use",
    "java",
    "api",
    "clients",
    "block",
    "cache",
    "bloom",
    "filters",
    "hbase",
    "supports",
    "block",
    "caching",
    "bloom",
    "filters",
    "high",
    "volume",
    "query",
    "optimization",
    "let",
    "dig",
    "little",
    "deeper",
    "hbase",
    "storage",
    "hbase",
    "column",
    "oriented",
    "storage",
    "told",
    "going",
    "look",
    "see",
    "stores",
    "data",
    "see",
    "row",
    "key",
    "really",
    "one",
    "important",
    "references",
    "row",
    "key",
    "row",
    "id",
    "column",
    "family",
    "see",
    "column",
    "family",
    "one",
    "column",
    "family",
    "two",
    "column",
    "family",
    "three",
    "column",
    "qualifiers",
    "column",
    "family",
    "one",
    "three",
    "columns",
    "might",
    "data",
    "go",
    "column",
    "family",
    "one",
    "query",
    "every",
    "column",
    "contains",
    "certain",
    "thing",
    "row",
    "might",
    "anything",
    "queried",
    "column",
    "family",
    "2",
    "maybe",
    "column",
    "1",
    "filled",
    "column",
    "3",
    "filled",
    "forth",
    "cell",
    "connected",
    "row",
    "data",
    "actually",
    "stored",
    "let",
    "take",
    "look",
    "looks",
    "like",
    "fill",
    "data",
    "row",
    "key",
    "row",
    "id",
    "employee",
    "id",
    "one",
    "two",
    "three",
    "pretty",
    "straightforward",
    "probably",
    "would",
    "even",
    "sql",
    "server",
    "column",
    "family",
    "starts",
    "really",
    "separating",
    "column",
    "family",
    "might",
    "personal",
    "data",
    "personal",
    "data",
    "would",
    "name",
    "city",
    "age",
    "might",
    "lot",
    "might",
    "number",
    "children",
    "might",
    "degree",
    "kinds",
    "different",
    "things",
    "go",
    "personal",
    "data",
    "might",
    "missing",
    "might",
    "name",
    "age",
    "employee",
    "might",
    "name",
    "city",
    "many",
    "children",
    "age",
    "see",
    "personal",
    "data",
    "collect",
    "large",
    "variety",
    "data",
    "store",
    "hbase",
    "easily",
    "maybe",
    "family",
    "professional",
    "data",
    "designation",
    "salary",
    "stuff",
    "employee",
    "company",
    "let",
    "dig",
    "little",
    "deeper",
    "hbase",
    "architecture",
    "see",
    "looks",
    "complicated",
    "chart",
    "complicated",
    "think",
    "apache",
    "space",
    "zookeeper",
    "used",
    "monitoring",
    "going",
    "h",
    "master",
    "hbase",
    "master",
    "assigns",
    "regions",
    "load",
    "balancing",
    "underneath",
    "region",
    "hbase",
    "master",
    "master",
    "read",
    "server",
    "serves",
    "data",
    "read",
    "write",
    "region",
    "server",
    "different",
    "computers",
    "hadoop",
    "cluster",
    "region",
    "store",
    "memory",
    "store",
    "different",
    "files",
    "h",
    "file",
    "stored",
    "separated",
    "across",
    "different",
    "computers",
    "part",
    "hdfs",
    "storage",
    "system",
    "look",
    "architectural",
    "components",
    "regions",
    "looking",
    "drilling",
    "little",
    "bit",
    "hbase",
    "tables",
    "divided",
    "horizontally",
    "row",
    "key",
    "range",
    "regions",
    "ids",
    "might",
    "ids",
    "1",
    "20",
    "21",
    "50",
    "whatever",
    "regions",
    "assigned",
    "nodes",
    "cluster",
    "called",
    "region",
    "servers",
    "region",
    "contains",
    "rows",
    "table",
    "region",
    "start",
    "key",
    "end",
    "key",
    "1",
    "10",
    "11",
    "20",
    "forth",
    "servers",
    "serve",
    "data",
    "read",
    "write",
    "see",
    "client",
    "get",
    "git",
    "sends",
    "finds",
    "start",
    "start",
    "keys",
    "n",
    "keys",
    "pulls",
    "data",
    "different",
    "region",
    "server",
    "region",
    "signed",
    "data",
    "definition",
    "language",
    "operation",
    "create",
    "delete",
    "handled",
    "telling",
    "data",
    "going",
    "assigning",
    "reassigning",
    "regions",
    "recovery",
    "load",
    "balancing",
    "monitoring",
    "servers",
    "also",
    "part",
    "know",
    "ids",
    "500",
    "ids",
    "across",
    "three",
    "servers",
    "going",
    "put",
    "400",
    "ids",
    "server",
    "1",
    "100",
    "server",
    "2",
    "leaves",
    "region",
    "3",
    "region",
    "4",
    "empty",
    "going",
    "split",
    "handled",
    "see",
    "monitors",
    "region",
    "servers",
    "assigns",
    "regions",
    "region",
    "servers",
    "assigns",
    "regions",
    "reason",
    "servers",
    "forth",
    "forth",
    "hbase",
    "distributed",
    "environment",
    "alone",
    "sufficient",
    "manage",
    "everything",
    "hence",
    "zookeeper",
    "introduced",
    "works",
    "active",
    "sends",
    "heartbeat",
    "signal",
    "zookeeper",
    "indicating",
    "active",
    "zookeeper",
    "also",
    "heartbeat",
    "region",
    "servers",
    "region",
    "servers",
    "send",
    "status",
    "zoo",
    "keeper",
    "indicating",
    "ready",
    "read",
    "write",
    "operation",
    "inactive",
    "server",
    "acts",
    "backup",
    "active",
    "hmaster",
    "fails",
    "come",
    "rescue",
    "active",
    "hmaster",
    "region",
    "servers",
    "connect",
    "session",
    "zookeeper",
    "see",
    "active",
    "hmaster",
    "selection",
    "region",
    "server",
    "session",
    "looking",
    "zookeeper",
    "keeping",
    "pulse",
    "active",
    "region",
    "server",
    "connects",
    "session",
    "zookeeper",
    "see",
    "ephemeral",
    "nodes",
    "active",
    "sessions",
    "via",
    "heartbeats",
    "indicate",
    "region",
    "servers",
    "running",
    "let",
    "take",
    "look",
    "hbase",
    "read",
    "write",
    "going",
    "special",
    "hbase",
    "catalog",
    "table",
    "called",
    "meta",
    "table",
    "holds",
    "location",
    "regions",
    "cluster",
    "happens",
    "first",
    "time",
    "client",
    "reads",
    "writes",
    "data",
    "hbase",
    "client",
    "gets",
    "region",
    "server",
    "host",
    "meta",
    "table",
    "zookeeper",
    "see",
    "right",
    "client",
    "request",
    "region",
    "server",
    "goes",
    "hey",
    "zookeeper",
    "handle",
    "zookeeper",
    "takes",
    "look",
    "goes",
    "ah",
    "metal",
    "location",
    "stored",
    "zookeeper",
    "looks",
    "metadata",
    "metadata",
    "table",
    "location",
    "sent",
    "back",
    "client",
    "client",
    "query",
    "meta",
    "server",
    "get",
    "region",
    "server",
    "corresponding",
    "row",
    "key",
    "wants",
    "access",
    "client",
    "caches",
    "information",
    "along",
    "midi",
    "table",
    "location",
    "see",
    "client",
    "going",
    "back",
    "forth",
    "region",
    "server",
    "information",
    "might",
    "going",
    "across",
    "multiple",
    "region",
    "servers",
    "depending",
    "querying",
    "get",
    "region",
    "server",
    "row",
    "key",
    "meta",
    "table",
    "row",
    "key",
    "comes",
    "says",
    "hey",
    "going",
    "gets",
    "row",
    "key",
    "corresponding",
    "region",
    "server",
    "put",
    "row",
    "git",
    "row",
    "region",
    "server",
    "let",
    "take",
    "look",
    "hbase",
    "meta",
    "table",
    "special",
    "hbase",
    "catalog",
    "table",
    "maintains",
    "list",
    "region",
    "servers",
    "hbase",
    "storage",
    "system",
    "see",
    "meta",
    "table",
    "row",
    "key",
    "value",
    "table",
    "key",
    "region",
    "region",
    "server",
    "meta",
    "table",
    "used",
    "find",
    "region",
    "given",
    "table",
    "key",
    "see",
    "know",
    "meta",
    "table",
    "comes",
    "going",
    "fire",
    "going",
    "region",
    "server",
    "look",
    "little",
    "closer",
    "right",
    "mechanism",
    "hbase",
    "right",
    "ahead",
    "log",
    "wall",
    "abbreviate",
    "kind",
    "way",
    "remember",
    "wall",
    "right",
    "ahead",
    "log",
    "file",
    "used",
    "store",
    "new",
    "data",
    "yet",
    "put",
    "permanent",
    "storage",
    "used",
    "recovery",
    "case",
    "failure",
    "see",
    "client",
    "comes",
    "literally",
    "puts",
    "new",
    "data",
    "coming",
    "kind",
    "temporary",
    "storage",
    "wall",
    "gone",
    "wall",
    "memory",
    "store",
    "memstor",
    "right",
    "cache",
    "stores",
    "new",
    "data",
    "yet",
    "written",
    "disk",
    "one",
    "mem",
    "store",
    "per",
    "column",
    "family",
    "per",
    "region",
    "done",
    "three",
    "ack",
    "data",
    "placed",
    "mems",
    "store",
    "client",
    "receives",
    "acknowledgement",
    "minister",
    "reaches",
    "threshold",
    "dumps",
    "commits",
    "data",
    "h",
    "file",
    "see",
    "right",
    "taken",
    "gun",
    "wall",
    "wall",
    "source",
    "different",
    "memory",
    "stores",
    "uh",
    "memory",
    "stores",
    "says",
    "hey",
    "reached",
    "ready",
    "dump",
    "h",
    "files",
    "moves",
    "h",
    "files",
    "h",
    "files",
    "store",
    "rows",
    "data",
    "stored",
    "key",
    "value",
    "disk",
    "done",
    "lot",
    "theory",
    "let",
    "dive",
    "take",
    "look",
    "see",
    "commands",
    "look",
    "like",
    "happens",
    "hbase",
    "manipulating",
    "nosql",
    "setup",
    "music",
    "learning",
    "new",
    "setup",
    "always",
    "good",
    "start",
    "coming",
    "open",
    "source",
    "apache",
    "go",
    "see",
    "lot",
    "information",
    "actually",
    "download",
    "hbase",
    "separate",
    "hadoop",
    "although",
    "people",
    "install",
    "hadoop",
    "bundled",
    "go",
    "find",
    "reference",
    "guide",
    "go",
    "apache",
    "reference",
    "guide",
    "number",
    "things",
    "look",
    "going",
    "going",
    "apache",
    "h",
    "base",
    "shell",
    "going",
    "working",
    "lot",
    "interfaces",
    "setup",
    "look",
    "lot",
    "different",
    "commands",
    "go",
    "apache",
    "hbase",
    "reference",
    "guide",
    "go",
    "read",
    "hbase",
    "shell",
    "commands",
    "command",
    "file",
    "see",
    "gives",
    "different",
    "options",
    "formats",
    "putting",
    "data",
    "listing",
    "data",
    "certainly",
    "also",
    "create",
    "files",
    "scripts",
    "going",
    "look",
    "basics",
    "going",
    "go",
    "basic",
    "hbase",
    "shell",
    "one",
    "last",
    "thing",
    "look",
    "course",
    "continue",
    "setup",
    "see",
    "detail",
    "far",
    "create",
    "get",
    "data",
    "hbase",
    "working",
    "virtual",
    "box",
    "oracle",
    "download",
    "oracle",
    "virtualbox",
    "put",
    "note",
    "youtube",
    "previous",
    "session",
    "setting",
    "virtual",
    "setup",
    "run",
    "hadoop",
    "system",
    "using",
    "cloudera",
    "quick",
    "start",
    "installed",
    "hortons",
    "also",
    "use",
    "amazon",
    "web",
    "service",
    "number",
    "options",
    "trying",
    "case",
    "cloudera",
    "oracle",
    "virtualbox",
    "virtual",
    "box",
    "linux",
    "centos",
    "installed",
    "hadoop",
    "different",
    "hadoop",
    "flavors",
    "including",
    "hbase",
    "bring",
    "computer",
    "windows",
    "10",
    "operating",
    "system",
    "virtualbox",
    "linux",
    "looking",
    "hbase",
    "data",
    "warehouse",
    "three",
    "different",
    "entities",
    "running",
    "computer",
    "confusing",
    "first",
    "time",
    "working",
    "kind",
    "setup",
    "notice",
    "cloudera",
    "setup",
    "actually",
    "hbase",
    "monitoring",
    "go",
    "underneath",
    "click",
    "hbase",
    "master",
    "tell",
    "going",
    "region",
    "servers",
    "tell",
    "going",
    "backup",
    "tables",
    "right",
    "user",
    "tables",
    "created",
    "single",
    "node",
    "single",
    "hbase",
    "tour",
    "going",
    "expect",
    "anything",
    "extensive",
    "since",
    "practice",
    "education",
    "perhaps",
    "testing",
    "package",
    "working",
    "really",
    "deploy",
    "cloudera",
    "course",
    "talk",
    "quick",
    "start",
    "single",
    "node",
    "setup",
    "really",
    "go",
    "different",
    "hbase",
    "see",
    "kinds",
    "different",
    "information",
    "zookeeper",
    "saw",
    "flash",
    "version",
    "working",
    "since",
    "zookeeper",
    "part",
    "hbase",
    "setup",
    "want",
    "go",
    "want",
    "open",
    "terminal",
    "window",
    "cloudera",
    "happens",
    "top",
    "click",
    "see",
    "cloudera",
    "terminal",
    "window",
    "open",
    "let",
    "expand",
    "nice",
    "full",
    "screen",
    "also",
    "going",
    "zoom",
    "way",
    "nice",
    "big",
    "picture",
    "see",
    "typing",
    "going",
    "open",
    "hbase",
    "shell",
    "simply",
    "type",
    "hbase",
    "shell",
    "get",
    "hit",
    "enter",
    "see",
    "takes",
    "moment",
    "load",
    "hbase",
    "shell",
    "hbase",
    "commands",
    "gotten",
    "hbase",
    "shell",
    "see",
    "hbase",
    "prompt",
    "information",
    "ahead",
    "something",
    "simple",
    "like",
    "list",
    "going",
    "list",
    "whatever",
    "tables",
    "happens",
    "base",
    "table",
    "comes",
    "hbase",
    "go",
    "ahead",
    "create",
    "going",
    "type",
    "create",
    "nice",
    "going",
    "throw",
    "kind",
    "going",
    "say",
    "hey",
    "straight",
    "create",
    "come",
    "tell",
    "different",
    "formats",
    "use",
    "create",
    "create",
    "table",
    "one",
    "families",
    "add",
    "splits",
    "names",
    "versions",
    "kinds",
    "things",
    "let",
    "start",
    "basic",
    "one",
    "let",
    "go",
    "ahead",
    "create",
    "call",
    "new",
    "table",
    "let",
    "call",
    "new",
    "tbl",
    "table",
    "new",
    "table",
    "also",
    "want",
    "let",
    "knowledge",
    "let",
    "take",
    "look",
    "creating",
    "new",
    "table",
    "going",
    "family",
    "knowledge",
    "let",
    "hit",
    "enter",
    "going",
    "come",
    "going",
    "take",
    "second",
    "go",
    "ahead",
    "create",
    "new",
    "table",
    "go",
    "list",
    "see",
    "table",
    "new",
    "table",
    "see",
    "new",
    "table",
    "course",
    "default",
    "table",
    "set",
    "something",
    "like",
    "uh",
    "describe",
    "describe",
    "going",
    "new",
    "tbl",
    "describe",
    "going",
    "come",
    "going",
    "say",
    "hey",
    "name",
    "knowledge",
    "data",
    "block",
    "encoding",
    "none",
    "bloom",
    "filter",
    "row",
    "replication",
    "scope",
    "version",
    "different",
    "information",
    "need",
    "new",
    "minimum",
    "version",
    "zero",
    "forever",
    "deleted",
    "cells",
    "false",
    "block",
    "size",
    "memory",
    "look",
    "stuff",
    "really",
    "track",
    "one",
    "things",
    "important",
    "note",
    "versions",
    "different",
    "versions",
    "data",
    "stored",
    "always",
    "important",
    "understand",
    "might",
    "talk",
    "little",
    "bit",
    "later",
    "describe",
    "also",
    "status",
    "status",
    "says",
    "one",
    "active",
    "master",
    "going",
    "hbase",
    "whole",
    "status",
    "summary",
    "thing",
    "status",
    "got",
    "thing",
    "coming",
    "created",
    "let",
    "go",
    "ahead",
    "put",
    "something",
    "gon",
    "na",
    "put",
    "new",
    "tbl",
    "want",
    "row",
    "one",
    "know",
    "even",
    "let",
    "type",
    "put",
    "see",
    "type",
    "put",
    "gives",
    "us",
    "like",
    "lot",
    "different",
    "options",
    "works",
    "different",
    "ways",
    "formatting",
    "data",
    "goes",
    "usually",
    "begin",
    "new",
    "table",
    "new",
    "tbl",
    "case",
    "call",
    "row",
    "one",
    "knowledge",
    "remember",
    "created",
    "knowledge",
    "already",
    "knowledge",
    "sports",
    "knowledge",
    "sports",
    "going",
    "set",
    "equal",
    "cricket",
    "going",
    "put",
    "underneath",
    "uh",
    "knowledge",
    "set",
    "thing",
    "called",
    "sports",
    "see",
    "looks",
    "like",
    "second",
    "let",
    "go",
    "ahead",
    "put",
    "couple",
    "let",
    "see",
    "let",
    "another",
    "row",
    "one",
    "time",
    "instead",
    "sports",
    "let",
    "science",
    "know",
    "person",
    "know",
    "row",
    "one",
    "knowledgeable",
    "cricket",
    "also",
    "chemistry",
    "chemist",
    "plays",
    "cricket",
    "row",
    "one",
    "uh",
    "let",
    "see",
    "let",
    "another",
    "row",
    "one",
    "keep",
    "going",
    "science",
    "case",
    "let",
    "physics",
    "chemistry",
    "also",
    "physicist",
    "quite",
    "joy",
    "physics",
    "go",
    "row",
    "one",
    "go",
    "let",
    "uh",
    "row",
    "two",
    "let",
    "see",
    "looks",
    "like",
    "start",
    "putting",
    "row",
    "two",
    "row",
    "two",
    "person",
    "knowledge",
    "economics",
    "master",
    "business",
    "maybe",
    "global",
    "economics",
    "maybe",
    "business",
    "fits",
    "country",
    "economics",
    "call",
    "macro",
    "economics",
    "guess",
    "whole",
    "country",
    "knowledge",
    "economics",
    "macro",
    "economics",
    "let",
    "one",
    "keep",
    "row",
    "two",
    "time",
    "economist",
    "also",
    "musician",
    "put",
    "music",
    "happen",
    "knowledge",
    "enjoy",
    "oh",
    "let",
    "pop",
    "music",
    "current",
    "pop",
    "music",
    "going",
    "loaded",
    "database",
    "see",
    "two",
    "rows",
    "row",
    "one",
    "row",
    "two",
    "list",
    "contents",
    "database",
    "simply",
    "scan",
    "scan",
    "let",
    "scan",
    "see",
    "looks",
    "always",
    "type",
    "tells",
    "different",
    "setups",
    "scan",
    "works",
    "case",
    "want",
    "scan",
    "new",
    "tbl",
    "scan",
    "new",
    "tbl",
    "row",
    "one",
    "row",
    "one",
    "row",
    "two",
    "row",
    "two",
    "see",
    "row",
    "one",
    "column",
    "called",
    "knowledge",
    "science",
    "time",
    "step",
    "value",
    "crickets",
    "value",
    "physics",
    "information",
    "created",
    "time",
    "stamp",
    "row",
    "one",
    "also",
    "knowledge",
    "sports",
    "value",
    "cricket",
    "sports",
    "science",
    "interesting",
    "remember",
    "also",
    "gave",
    "originally",
    "told",
    "come",
    "chemistry",
    "science",
    "chemistry",
    "science",
    "physics",
    "come",
    "see",
    "chemistry",
    "replaced",
    "chemistry",
    "physics",
    "new",
    "value",
    "physics",
    "let",
    "go",
    "ahead",
    "clear",
    "little",
    "bit",
    "going",
    "ask",
    "question",
    "enabled",
    "new",
    "table",
    "hit",
    "enter",
    "going",
    "see",
    "comes",
    "true",
    "go",
    "ahead",
    "disable",
    "let",
    "go",
    "ahead",
    "disable",
    "new",
    "table",
    "make",
    "sure",
    "quotes",
    "around",
    "disabled",
    "happens",
    "scan",
    "scan",
    "new",
    "table",
    "hit",
    "enter",
    "gon",
    "na",
    "see",
    "get",
    "error",
    "coming",
    "disabled",
    "ca",
    "anything",
    "enable",
    "table",
    "let",
    "alteration",
    "new",
    "table",
    "look",
    "little",
    "familiar",
    "similar",
    "create",
    "call",
    "test",
    "info",
    "hit",
    "enter",
    "take",
    "moment",
    "updating",
    "want",
    "go",
    "ahead",
    "enable",
    "let",
    "go",
    "ahead",
    "enable",
    "new",
    "table",
    "back",
    "running",
    "want",
    "describe",
    "describe",
    "new",
    "table",
    "come",
    "see",
    "name",
    "knowledge",
    "data",
    "encoding",
    "information",
    "knowledge",
    "also",
    "test",
    "info",
    "name",
    "test",
    "info",
    "information",
    "concerning",
    "test",
    "info",
    "simply",
    "enable",
    "new",
    "table",
    "enabled",
    "oops",
    "already",
    "guess",
    "enable",
    "twice",
    "let",
    "start",
    "looking",
    "well",
    "scan",
    "new",
    "table",
    "see",
    "brings",
    "information",
    "like",
    "want",
    "go",
    "ahead",
    "get",
    "row",
    "r1",
    "hbase",
    "r1",
    "see",
    "knowledge",
    "science",
    "time",
    "stamp",
    "value",
    "physics",
    "knowledge",
    "sports",
    "time",
    "stamp",
    "value",
    "cricket",
    "let",
    "see",
    "happens",
    "put",
    "new",
    "table",
    "want",
    "row",
    "one",
    "guess",
    "earlier",
    "something",
    "similar",
    "uh",
    "going",
    "knowledge",
    "economics",
    "going",
    "sort",
    "think",
    "uh",
    "macro",
    "economics",
    "market",
    "economics",
    "go",
    "back",
    "get",
    "command",
    "see",
    "looks",
    "like",
    "see",
    "knowledge",
    "economics",
    "timestamp",
    "value",
    "market",
    "economics",
    "physics",
    "cricket",
    "economics",
    "science",
    "sports",
    "three",
    "different",
    "columns",
    "one",
    "different",
    "information",
    "managed",
    "go",
    "commands",
    "look",
    "basics",
    "ability",
    "create",
    "basic",
    "hbase",
    "setup",
    "nosql",
    "setup",
    "based",
    "columns",
    "rows",
    "thank",
    "guys",
    "done",
    "hadoop",
    "ecosystem",
    "shruti",
    "take",
    "us",
    "big",
    "data",
    "applications",
    "move",
    "applications",
    "let",
    "quick",
    "look",
    "big",
    "data",
    "market",
    "revenue",
    "forecast",
    "worldwide",
    "2011",
    "graph",
    "represents",
    "revenue",
    "billion",
    "us",
    "dollars",
    "represents",
    "years",
    "seen",
    "clearly",
    "graph",
    "big",
    "data",
    "grown",
    "2019",
    "statistics",
    "predict",
    "growth",
    "continue",
    "even",
    "future",
    "growth",
    "made",
    "possible",
    "numerous",
    "companies",
    "use",
    "big",
    "data",
    "various",
    "domains",
    "boost",
    "revenue",
    "look",
    "applications",
    "first",
    "big",
    "data",
    "application",
    "look",
    "weather",
    "forecast",
    "imagine",
    "sudden",
    "storm",
    "even",
    "prepared",
    "would",
    "terrifying",
    "situation",
    "dealing",
    "calamities",
    "hurricane",
    "storms",
    "floods",
    "would",
    "inconvenient",
    "caught",
    "guard",
    "solution",
    "tool",
    "predicts",
    "weather",
    "coming",
    "days",
    "well",
    "advance",
    "tool",
    "needs",
    "accurate",
    "make",
    "tool",
    "big",
    "data",
    "used",
    "big",
    "data",
    "help",
    "well",
    "allows",
    "us",
    "gather",
    "information",
    "required",
    "predict",
    "weather",
    "information",
    "climate",
    "change",
    "details",
    "wind",
    "direction",
    "precipitation",
    "previous",
    "weather",
    "reports",
    "data",
    "collected",
    "becomes",
    "easier",
    "us",
    "spot",
    "trend",
    "identify",
    "going",
    "happen",
    "next",
    "analyzing",
    "big",
    "data",
    "weather",
    "prediction",
    "engine",
    "works",
    "analysis",
    "predicts",
    "weather",
    "every",
    "region",
    "across",
    "world",
    "given",
    "time",
    "using",
    "tool",
    "well",
    "prepared",
    "face",
    "climate",
    "change",
    "natural",
    "calamity",
    "let",
    "take",
    "example",
    "landslide",
    "try",
    "understand",
    "big",
    "data",
    "used",
    "tackle",
    "situation",
    "predicting",
    "landslide",
    "difficult",
    "basic",
    "warning",
    "signs",
    "lack",
    "prediction",
    "cause",
    "huge",
    "damage",
    "life",
    "property",
    "challenge",
    "studied",
    "university",
    "melbourne",
    "developed",
    "tool",
    "capable",
    "predicting",
    "landslide",
    "tool",
    "predicts",
    "boundary",
    "landslide",
    "likely",
    "occur",
    "two",
    "weeks",
    "magical",
    "tool",
    "works",
    "big",
    "data",
    "applied",
    "mathematics",
    "accurate",
    "prediction",
    "like",
    "made",
    "two",
    "weeks",
    "save",
    "lives",
    "help",
    "relocating",
    "people",
    "particular",
    "region",
    "also",
    "gives",
    "us",
    "insight",
    "magnitude",
    "upcoming",
    "destruction",
    "big",
    "data",
    "used",
    "weather",
    "forecast",
    "predicting",
    "natural",
    "calamities",
    "across",
    "world",
    "let",
    "us",
    "move",
    "next",
    "application",
    "big",
    "data",
    "application",
    "field",
    "media",
    "entertainment",
    "media",
    "entertainment",
    "industry",
    "massive",
    "one",
    "leveraging",
    "big",
    "data",
    "produce",
    "results",
    "boost",
    "revenue",
    "company",
    "let",
    "us",
    "see",
    "different",
    "ways",
    "big",
    "data",
    "used",
    "industry",
    "ever",
    "noticed",
    "come",
    "across",
    "relevant",
    "advertisements",
    "social",
    "media",
    "sites",
    "mailboxes",
    "well",
    "done",
    "analyzing",
    "data",
    "previous",
    "browsing",
    "history",
    "purchase",
    "data",
    "publishers",
    "display",
    "like",
    "form",
    "ads",
    "turn",
    "catch",
    "interest",
    "looking",
    "next",
    "customer",
    "sentiment",
    "analysis",
    "customers",
    "important",
    "company",
    "happier",
    "customer",
    "greater",
    "company",
    "revenue",
    "big",
    "data",
    "helps",
    "gathering",
    "emotions",
    "customer",
    "posts",
    "messages",
    "conversations",
    "etc",
    "emotions",
    "analyzed",
    "arrive",
    "conclusion",
    "regarding",
    "customer",
    "satisfaction",
    "customer",
    "unhappy",
    "company",
    "strives",
    "better",
    "next",
    "time",
    "provides",
    "customers",
    "better",
    "experience",
    "purchasing",
    "item",
    "site",
    "watching",
    "videos",
    "entertainment",
    "site",
    "might",
    "noticed",
    "segment",
    "says",
    "recommended",
    "list",
    "list",
    "personalized",
    "list",
    "made",
    "available",
    "analyzing",
    "data",
    "previous",
    "watch",
    "history",
    "subscriptions",
    "likes",
    "recommendation",
    "engine",
    "tool",
    "filters",
    "analyzes",
    "data",
    "provides",
    "list",
    "would",
    "likely",
    "interested",
    "site",
    "able",
    "retain",
    "engage",
    "customer",
    "longer",
    "time",
    "next",
    "customer",
    "churn",
    "analysis",
    "simple",
    "words",
    "customer",
    "churn",
    "happens",
    "customer",
    "stops",
    "subscription",
    "service",
    "predicting",
    "preventing",
    "paramount",
    "importance",
    "organization",
    "analyzing",
    "behavioral",
    "patterns",
    "previously",
    "churned",
    "customers",
    "organization",
    "identify",
    "current",
    "customers",
    "likely",
    "churn",
    "analyzing",
    "data",
    "organization",
    "implement",
    "effective",
    "programs",
    "customer",
    "retention",
    "let",
    "us",
    "look",
    "use",
    "case",
    "starbucks",
    "big",
    "data",
    "effectively",
    "used",
    "starbucks",
    "app",
    "17",
    "million",
    "users",
    "use",
    "app",
    "imagine",
    "much",
    "data",
    "generate",
    "data",
    "form",
    "coffee",
    "buying",
    "habits",
    "store",
    "visit",
    "time",
    "purchase",
    "data",
    "fed",
    "app",
    "customer",
    "enters",
    "new",
    "starbucks",
    "location",
    "system",
    "analyzes",
    "data",
    "provided",
    "preferred",
    "order",
    "app",
    "also",
    "suggests",
    "new",
    "products",
    "customer",
    "addition",
    "also",
    "provide",
    "personalized",
    "offer",
    "discounts",
    "special",
    "occasions",
    "moving",
    "next",
    "sector",
    "healthcare",
    "one",
    "important",
    "sectors",
    "big",
    "data",
    "widely",
    "used",
    "save",
    "lives",
    "available",
    "big",
    "data",
    "medical",
    "researchers",
    "done",
    "effectively",
    "performed",
    "accurately",
    "analyzing",
    "previous",
    "medical",
    "histories",
    "new",
    "treatments",
    "medicines",
    "discovered",
    "cure",
    "found",
    "even",
    "incurable",
    "diseases",
    "cases",
    "one",
    "medication",
    "need",
    "effective",
    "every",
    "patient",
    "hence",
    "personal",
    "care",
    "important",
    "personal",
    "care",
    "provided",
    "patient",
    "depending",
    "past",
    "medical",
    "history",
    "individuals",
    "medical",
    "history",
    "along",
    "body",
    "parameters",
    "analyzed",
    "personal",
    "attention",
    "given",
    "know",
    "medical",
    "treatments",
    "pocket",
    "friendly",
    "every",
    "time",
    "medical",
    "treatment",
    "taken",
    "amount",
    "increases",
    "reduced",
    "readmissions",
    "brought",
    "analyzing",
    "data",
    "precisely",
    "deliver",
    "efficient",
    "result",
    "turn",
    "prevent",
    "patient",
    "readmission",
    "frequently",
    "globalization",
    "came",
    "increase",
    "ease",
    "infectious",
    "diseases",
    "spread",
    "widely",
    "based",
    "geography",
    "demographics",
    "big",
    "data",
    "helps",
    "predicting",
    "outbreak",
    "epidemic",
    "viruses",
    "likely",
    "occur",
    "american",
    "healthcare",
    "company",
    "united",
    "healthcare",
    "uses",
    "big",
    "data",
    "detect",
    "online",
    "medical",
    "fraud",
    "activities",
    "payment",
    "unauthorized",
    "benefits",
    "intentional",
    "misrepresentation",
    "data",
    "healthcare",
    "company",
    "runs",
    "disease",
    "management",
    "programs",
    "success",
    "rates",
    "programs",
    "predicted",
    "using",
    "big",
    "data",
    "depending",
    "patients",
    "respond",
    "next",
    "sector",
    "look",
    "logistics",
    "logistics",
    "looks",
    "process",
    "transportation",
    "storage",
    "goods",
    "movement",
    "product",
    "supplier",
    "consumer",
    "important",
    "big",
    "data",
    "used",
    "make",
    "process",
    "faster",
    "efficient",
    "important",
    "factor",
    "logistics",
    "time",
    "taken",
    "products",
    "reach",
    "destination",
    "achieve",
    "minimum",
    "time",
    "sensors",
    "within",
    "vehicle",
    "analyze",
    "fastest",
    "route",
    "analysis",
    "based",
    "various",
    "data",
    "weather",
    "traffic",
    "list",
    "orders",
    "fastest",
    "route",
    "obtained",
    "delivery",
    "time",
    "reduced",
    "capacity",
    "planning",
    "another",
    "factor",
    "needs",
    "taken",
    "consideration",
    "details",
    "regarding",
    "workforce",
    "number",
    "vehicles",
    "analyzed",
    "thoroughly",
    "vehicle",
    "allocated",
    "different",
    "route",
    "done",
    "need",
    "many",
    "trucks",
    "travel",
    "direction",
    "pointless",
    "depending",
    "analysis",
    "available",
    "workforce",
    "resources",
    "decision",
    "taken",
    "big",
    "data",
    "analytics",
    "also",
    "finds",
    "use",
    "managing",
    "warehouses",
    "efficiently",
    "analysis",
    "along",
    "tracking",
    "sensors",
    "provide",
    "information",
    "regarding",
    "underutilized",
    "space",
    "results",
    "efficient",
    "resource",
    "allocation",
    "eventually",
    "reduces",
    "cost",
    "customer",
    "satisfaction",
    "important",
    "logistics",
    "like",
    "sector",
    "customer",
    "reactions",
    "analyzed",
    "available",
    "data",
    "eventually",
    "create",
    "instant",
    "feedback",
    "loop",
    "happy",
    "customer",
    "always",
    "help",
    "company",
    "gain",
    "customers",
    "let",
    "us",
    "look",
    "use",
    "case",
    "ups",
    "know",
    "ups",
    "one",
    "biggest",
    "shipping",
    "company",
    "world",
    "huge",
    "customer",
    "database",
    "work",
    "data",
    "every",
    "minute",
    "ups",
    "uses",
    "big",
    "data",
    "gather",
    "different",
    "kinds",
    "data",
    "regarding",
    "weather",
    "traffic",
    "jams",
    "geography",
    "locations",
    "collecting",
    "data",
    "analyze",
    "discover",
    "best",
    "fastest",
    "route",
    "destination",
    "addition",
    "also",
    "use",
    "big",
    "data",
    "change",
    "routes",
    "real",
    "time",
    "efficiently",
    "ups",
    "leverages",
    "big",
    "data",
    "next",
    "interesting",
    "sector",
    "travel",
    "tourism",
    "sector",
    "global",
    "tourism",
    "market",
    "expected",
    "grow",
    "near",
    "future",
    "big",
    "data",
    "used",
    "various",
    "ways",
    "sector",
    "let",
    "us",
    "look",
    "hotels",
    "increase",
    "revenue",
    "adjusting",
    "room",
    "tariffs",
    "depending",
    "peak",
    "seasons",
    "holiday",
    "seasons",
    "festive",
    "seasons",
    "tourism",
    "industry",
    "uses",
    "data",
    "anticipate",
    "demand",
    "maximize",
    "revenue",
    "big",
    "data",
    "also",
    "used",
    "resorts",
    "hotels",
    "analyze",
    "various",
    "details",
    "regarding",
    "competitors",
    "analysis",
    "result",
    "helps",
    "incorporate",
    "good",
    "facilities",
    "competitors",
    "providing",
    "hotel",
    "able",
    "flourish",
    "customer",
    "always",
    "comes",
    "back",
    "offered",
    "good",
    "packages",
    "basic",
    "ones",
    "looking",
    "customer",
    "past",
    "travel",
    "history",
    "likes",
    "preferences",
    "hotels",
    "provide",
    "customers",
    "personalized",
    "experiences",
    "interest",
    "highly",
    "investing",
    "area",
    "could",
    "hub",
    "tourism",
    "wise",
    "countries",
    "use",
    "big",
    "data",
    "examine",
    "tourism",
    "activities",
    "country",
    "turn",
    "helps",
    "discover",
    "new",
    "fruitful",
    "investment",
    "opportunities",
    "let",
    "us",
    "look",
    "one",
    "best",
    "online",
    "homestay",
    "networks",
    "airbnb",
    "see",
    "big",
    "data",
    "used",
    "airbnb",
    "undoubtedly",
    "provides",
    "customers",
    "best",
    "accommodation",
    "across",
    "world",
    "big",
    "data",
    "used",
    "analyze",
    "different",
    "kinds",
    "available",
    "properties",
    "depending",
    "customer",
    "preferences",
    "pricing",
    "keywords",
    "previous",
    "customers",
    "ratings",
    "experiences",
    "airbnb",
    "filters",
    "best",
    "result",
    "big",
    "data",
    "works",
    "magic",
    "yet",
    "move",
    "final",
    "sector",
    "government",
    "law",
    "enforcement",
    "sector",
    "maintaining",
    "law",
    "order",
    "utmost",
    "importance",
    "government",
    "huge",
    "task",
    "big",
    "data",
    "plays",
    "active",
    "role",
    "addition",
    "also",
    "helps",
    "governments",
    "bring",
    "new",
    "policies",
    "schemes",
    "welfare",
    "citizens",
    "police",
    "department",
    "able",
    "predict",
    "criminal",
    "activities",
    "way",
    "happens",
    "analyzing",
    "big",
    "data",
    "information",
    "previous",
    "crime",
    "records",
    "particular",
    "region",
    "safety",
    "aspect",
    "region",
    "analyzing",
    "factors",
    "able",
    "predict",
    "activity",
    "breaks",
    "law",
    "order",
    "region",
    "governments",
    "able",
    "tackle",
    "unemployment",
    "great",
    "extent",
    "using",
    "big",
    "data",
    "analyzing",
    "number",
    "students",
    "graduating",
    "every",
    "year",
    "number",
    "relevant",
    "job",
    "openings",
    "government",
    "idea",
    "unemployment",
    "rate",
    "country",
    "take",
    "necessary",
    "measures",
    "tackle",
    "next",
    "factor",
    "poverty",
    "large",
    "countries",
    "difficult",
    "analyze",
    "area",
    "requires",
    "attention",
    "development",
    "big",
    "data",
    "analytics",
    "makes",
    "easier",
    "governments",
    "discover",
    "areas",
    "poverty",
    "gradually",
    "decreases",
    "areas",
    "begin",
    "develop",
    "governments",
    "always",
    "lookout",
    "better",
    "development",
    "public",
    "survey",
    "voices",
    "opinion",
    "country",
    "citizens",
    "analyzing",
    "data",
    "collected",
    "surveys",
    "help",
    "governments",
    "build",
    "better",
    "policies",
    "services",
    "benefit",
    "citizens",
    "let",
    "us",
    "move",
    "use",
    "case",
    "know",
    "new",
    "york",
    "police",
    "department",
    "uses",
    "big",
    "data",
    "analytics",
    "protect",
    "citizens",
    "department",
    "prevents",
    "identifies",
    "crimes",
    "analyzing",
    "huge",
    "amount",
    "data",
    "includes",
    "fingerprints",
    "certain",
    "emails",
    "records",
    "previous",
    "police",
    "investigations",
    "analyzing",
    "data",
    "meaningful",
    "insights",
    "drawn",
    "help",
    "police",
    "taking",
    "required",
    "preventive",
    "measures",
    "crimes",
    "thank",
    "srithi",
    "let",
    "us",
    "look",
    "next",
    "data",
    "processing",
    "framework",
    "spark",
    "instructor",
    "ajay",
    "take",
    "us",
    "apache",
    "spark",
    "installation",
    "welcome",
    "tutorial",
    "apache",
    "spark",
    "one",
    "demand",
    "technologies",
    "processing",
    "frameworks",
    "big",
    "data",
    "world",
    "learn",
    "apache",
    "spark",
    "history",
    "spark",
    "spark",
    "hadoop",
    "framework",
    "wes",
    "spark",
    "components",
    "apache",
    "spark",
    "spark",
    "core",
    "spark",
    "sql",
    "spark",
    "streaming",
    "spark",
    "ml",
    "lab",
    "graphics",
    "learn",
    "spark",
    "architecture",
    "applications",
    "spark",
    "spark",
    "use",
    "cases",
    "let",
    "begin",
    "understanding",
    "history",
    "apache",
    "spark",
    "started",
    "2009",
    "project",
    "uc",
    "berkeley",
    "amp",
    "labs",
    "mate",
    "2010",
    "open",
    "source",
    "bsd",
    "license",
    "2013",
    "spark",
    "became",
    "apache",
    "top",
    "level",
    "project",
    "2014",
    "used",
    "databricks",
    "sort",
    "data",
    "sets",
    "set",
    "new",
    "world",
    "record",
    "apache",
    "spark",
    "started",
    "today",
    "one",
    "demand",
    "processing",
    "framework",
    "would",
    "say",
    "memory",
    "computing",
    "framework",
    "used",
    "across",
    "big",
    "data",
    "industry",
    "apache",
    "spark",
    "let",
    "learn",
    "apache",
    "spark",
    "open",
    "source",
    "computing",
    "framework",
    "could",
    "say",
    "data",
    "processing",
    "engine",
    "used",
    "process",
    "data",
    "batch",
    "also",
    "real",
    "time",
    "across",
    "various",
    "cluster",
    "computers",
    "simple",
    "programming",
    "language",
    "behind",
    "scenes",
    "scala",
    "used",
    "although",
    "users",
    "would",
    "want",
    "work",
    "spark",
    "work",
    "python",
    "work",
    "scala",
    "work",
    "java",
    "even",
    "r",
    "matter",
    "supports",
    "programming",
    "languages",
    "one",
    "reasons",
    "called",
    "polyglot",
    "wherein",
    "good",
    "set",
    "libraries",
    "support",
    "programming",
    "languages",
    "developers",
    "data",
    "scientists",
    "incorporate",
    "spark",
    "applications",
    "build",
    "spark",
    "based",
    "applications",
    "process",
    "analyze",
    "query",
    "transform",
    "data",
    "large",
    "scale",
    "key",
    "features",
    "apache",
    "spark",
    "compare",
    "hadoop",
    "west",
    "spark",
    "know",
    "hadoop",
    "framework",
    "basically",
    "map",
    "reduce",
    "comes",
    "hadoop",
    "processing",
    "data",
    "however",
    "processing",
    "data",
    "using",
    "mapreduce",
    "hadoop",
    "quite",
    "slow",
    "batch",
    "oriented",
    "operation",
    "time",
    "consuming",
    "talk",
    "spark",
    "spark",
    "process",
    "data",
    "100",
    "times",
    "faster",
    "mapreduce",
    "computing",
    "framework",
    "well",
    "always",
    "conflicting",
    "ideas",
    "saying",
    "spark",
    "application",
    "really",
    "efficiently",
    "coded",
    "mapreduce",
    "application",
    "efficiently",
    "coded",
    "well",
    "different",
    "case",
    "however",
    "normally",
    "talk",
    "code",
    "efficiently",
    "written",
    "mapreduce",
    "spark",
    "based",
    "processing",
    "spark",
    "win",
    "battle",
    "almost",
    "100",
    "times",
    "faster",
    "mapreduce",
    "mentioned",
    "hadoop",
    "performs",
    "batch",
    "processing",
    "one",
    "paradigms",
    "map",
    "reduced",
    "programming",
    "model",
    "involves",
    "mapping",
    "reducing",
    "quite",
    "rigid",
    "performs",
    "batch",
    "processing",
    "intermittent",
    "data",
    "written",
    "sdfs",
    "written",
    "red",
    "back",
    "sdfs",
    "makes",
    "hadoop",
    "mapreduce",
    "processing",
    "slower",
    "case",
    "spark",
    "perform",
    "batch",
    "processing",
    "however",
    "lot",
    "use",
    "cases",
    "based",
    "processing",
    "take",
    "example",
    "macy",
    "take",
    "example",
    "retail",
    "giant",
    "walmart",
    "many",
    "use",
    "cases",
    "would",
    "prefer",
    "real",
    "time",
    "processing",
    "would",
    "say",
    "near",
    "real",
    "time",
    "processing",
    "say",
    "real",
    "time",
    "near",
    "real",
    "time",
    "processing",
    "data",
    "comes",
    "talking",
    "streaming",
    "kind",
    "data",
    "hadoop",
    "hadoop",
    "mapreduce",
    "obviously",
    "started",
    "written",
    "java",
    "could",
    "also",
    "write",
    "scala",
    "python",
    "however",
    "talk",
    "mapreduce",
    "lines",
    "code",
    "since",
    "written",
    "java",
    "take",
    "times",
    "execute",
    "manage",
    "dependencies",
    "right",
    "declarations",
    "create",
    "mapper",
    "reducer",
    "driver",
    "classes",
    "however",
    "compare",
    "spark",
    "lines",
    "code",
    "implemented",
    "scala",
    "scala",
    "statically",
    "typed",
    "dynamically",
    "inferred",
    "language",
    "concise",
    "benefit",
    "features",
    "functional",
    "programming",
    "language",
    "case",
    "scala",
    "whatever",
    "code",
    "written",
    "converted",
    "byte",
    "codes",
    "runs",
    "jvm",
    "hadoop",
    "supports",
    "kerberos",
    "authentication",
    "different",
    "kind",
    "authentication",
    "mechanisms",
    "kerberos",
    "one",
    "ones",
    "really",
    "get",
    "difficult",
    "manage",
    "spark",
    "supports",
    "authentication",
    "via",
    "shared",
    "secret",
    "also",
    "run",
    "yarn",
    "leveraging",
    "capability",
    "kerberos",
    "spark",
    "features",
    "really",
    "makes",
    "unique",
    "demand",
    "processing",
    "framework",
    "talk",
    "spark",
    "features",
    "one",
    "key",
    "features",
    "fast",
    "processing",
    "spark",
    "contains",
    "resilient",
    "distributed",
    "data",
    "sets",
    "rdds",
    "building",
    "blocks",
    "spark",
    "learn",
    "rdds",
    "later",
    "spark",
    "contains",
    "rdds",
    "saves",
    "huge",
    "time",
    "taken",
    "reading",
    "writing",
    "operations",
    "100",
    "times",
    "say",
    "10",
    "100",
    "times",
    "faster",
    "hadoop",
    "say",
    "memory",
    "computing",
    "would",
    "like",
    "make",
    "note",
    "difference",
    "caching",
    "memory",
    "computing",
    "think",
    "caching",
    "mainly",
    "support",
    "read",
    "ahead",
    "mechanism",
    "data",
    "benefit",
    "queries",
    "however",
    "say",
    "memory",
    "computing",
    "talking",
    "lazy",
    "valuation",
    "talking",
    "data",
    "loaded",
    "memory",
    "specific",
    "kind",
    "action",
    "invoked",
    "data",
    "stored",
    "ram",
    "say",
    "ram",
    "used",
    "processing",
    "also",
    "used",
    "storage",
    "decide",
    "whether",
    "would",
    "want",
    "ram",
    "used",
    "persistence",
    "computing",
    "access",
    "data",
    "quickly",
    "accelerate",
    "speed",
    "analytics",
    "spark",
    "quite",
    "flexible",
    "supports",
    "multiple",
    "languages",
    "already",
    "mentioned",
    "allows",
    "developers",
    "write",
    "applications",
    "java",
    "scala",
    "r",
    "python",
    "quite",
    "fault",
    "tolerance",
    "spark",
    "contains",
    "rdds",
    "could",
    "say",
    "execution",
    "logic",
    "could",
    "say",
    "temporary",
    "data",
    "sets",
    "initially",
    "data",
    "loaded",
    "data",
    "loaded",
    "rdds",
    "execution",
    "happening",
    "fault",
    "tolerant",
    "rdds",
    "distributed",
    "across",
    "multiple",
    "nodes",
    "failure",
    "one",
    "worker",
    "node",
    "cluster",
    "really",
    "affect",
    "rdds",
    "portion",
    "recomputed",
    "ensures",
    "loss",
    "data",
    "ensures",
    "data",
    "loss",
    "absolutely",
    "fault",
    "tolerant",
    "better",
    "analytics",
    "sparked",
    "rich",
    "set",
    "sql",
    "queries",
    "machine",
    "learning",
    "algorithms",
    "complex",
    "analytics",
    "supported",
    "various",
    "par",
    "components",
    "learn",
    "coming",
    "slides",
    "functionalities",
    "analytics",
    "performed",
    "better",
    "terms",
    "spark",
    "key",
    "features",
    "spark",
    "however",
    "many",
    "features",
    "related",
    "different",
    "components",
    "spark",
    "learn",
    "components",
    "spark",
    "talking",
    "spark",
    "core",
    "core",
    "component",
    "basically",
    "rdds",
    "core",
    "engine",
    "takes",
    "care",
    "processing",
    "also",
    "spark",
    "sql",
    "people",
    "would",
    "interested",
    "working",
    "structured",
    "data",
    "data",
    "structuralized",
    "would",
    "want",
    "prefer",
    "using",
    "spark",
    "sql",
    "spark",
    "sql",
    "internally",
    "components",
    "features",
    "like",
    "data",
    "frames",
    "data",
    "sets",
    "used",
    "process",
    "structured",
    "data",
    "much",
    "much",
    "faster",
    "way",
    "spark",
    "streaming",
    "important",
    "component",
    "spark",
    "allows",
    "create",
    "spark",
    "streaming",
    "applications",
    "works",
    "data",
    "streamed",
    "data",
    "constantly",
    "getting",
    "generated",
    "would",
    "also",
    "could",
    "also",
    "transform",
    "data",
    "could",
    "analyze",
    "process",
    "data",
    "comes",
    "smaller",
    "chunks",
    "sparks",
    "mlib",
    "basically",
    "set",
    "libraries",
    "allows",
    "developers",
    "data",
    "scientists",
    "build",
    "machine",
    "learning",
    "algorithms",
    "predictive",
    "analytics",
    "prescriptive",
    "descriptive",
    "analytics",
    "could",
    "build",
    "recommendation",
    "systems",
    "bigger",
    "smarter",
    "machine",
    "learning",
    "algorithms",
    "using",
    "libraries",
    "graphics",
    "think",
    "organizations",
    "like",
    "linkedin",
    "say",
    "twitter",
    "data",
    "naturally",
    "network",
    "kind",
    "flow",
    "data",
    "could",
    "represented",
    "form",
    "graphs",
    "talk",
    "graphs",
    "talking",
    "pie",
    "charts",
    "bar",
    "charts",
    "talking",
    "network",
    "related",
    "data",
    "data",
    "networked",
    "together",
    "kind",
    "relationship",
    "think",
    "facebook",
    "think",
    "linkedin",
    "one",
    "person",
    "connected",
    "person",
    "one",
    "company",
    "connected",
    "companies",
    "data",
    "represented",
    "form",
    "network",
    "graphs",
    "spark",
    "component",
    "called",
    "graphics",
    "allows",
    "graph",
    "based",
    "processing",
    "components",
    "apache",
    "spark",
    "spark",
    "core",
    "spark",
    "sql",
    "spark",
    "streaming",
    "spark",
    "mlib",
    "graphics",
    "learn",
    "components",
    "spark",
    "let",
    "learn",
    "spark",
    "core",
    "base",
    "engine",
    "used",
    "large",
    "scale",
    "parallel",
    "distributed",
    "data",
    "processing",
    "work",
    "spark",
    "least",
    "minimum",
    "would",
    "work",
    "spark",
    "core",
    "rdds",
    "building",
    "blocks",
    "spark",
    "responsible",
    "memory",
    "management",
    "fault",
    "recovery",
    "scheduling",
    "distributing",
    "monitoring",
    "jobs",
    "cluster",
    "interacting",
    "storage",
    "systems",
    "would",
    "like",
    "make",
    "key",
    "point",
    "spark",
    "storage",
    "relies",
    "storage",
    "storage",
    "could",
    "sdfs",
    "hadoop",
    "distributed",
    "file",
    "system",
    "could",
    "database",
    "like",
    "nosql",
    "database",
    "hbase",
    "could",
    "database",
    "say",
    "rdbms",
    "could",
    "connect",
    "spark",
    "fetch",
    "data",
    "extract",
    "data",
    "process",
    "analyze",
    "let",
    "learn",
    "little",
    "bit",
    "rdds",
    "resilient",
    "distributed",
    "data",
    "sets",
    "spark",
    "core",
    "base",
    "engine",
    "core",
    "engine",
    "embedded",
    "building",
    "blocks",
    "spark",
    "nothing",
    "resilient",
    "distributed",
    "data",
    "set",
    "name",
    "says",
    "resilient",
    "existing",
    "shortest",
    "period",
    "time",
    "distributed",
    "distributed",
    "across",
    "nodes",
    "data",
    "set",
    "data",
    "loaded",
    "data",
    "existing",
    "processing",
    "immutable",
    "fault",
    "tolerant",
    "distributed",
    "collection",
    "objects",
    "rdd",
    "mainly",
    "two",
    "operations",
    "performed",
    "rdd",
    "take",
    "example",
    "say",
    "want",
    "process",
    "particular",
    "file",
    "could",
    "write",
    "simple",
    "code",
    "scala",
    "would",
    "basically",
    "mean",
    "something",
    "like",
    "say",
    "val",
    "declare",
    "variable",
    "would",
    "say",
    "val",
    "x",
    "could",
    "use",
    "call",
    "spark",
    "context",
    "basically",
    "important",
    "entry",
    "point",
    "application",
    "could",
    "use",
    "method",
    "spark",
    "context",
    "example",
    "text",
    "file",
    "could",
    "point",
    "particular",
    "file",
    "method",
    "spark",
    "context",
    "spark",
    "context",
    "entry",
    "point",
    "application",
    "could",
    "give",
    "path",
    "method",
    "step",
    "evaluation",
    "say",
    "val",
    "x",
    "creating",
    "immutable",
    "variable",
    "variable",
    "assigning",
    "file",
    "step",
    "actually",
    "creates",
    "rdd",
    "resilient",
    "distributed",
    "data",
    "set",
    "imagine",
    "simple",
    "execution",
    "logic",
    "empty",
    "data",
    "set",
    "created",
    "memory",
    "node",
    "would",
    "say",
    "multiple",
    "nodes",
    "data",
    "split",
    "stored",
    "imagining",
    "yarn",
    "spark",
    "working",
    "hadoop",
    "hadoop",
    "using",
    "say",
    "two",
    "nodes",
    "distributed",
    "file",
    "system",
    "sdfs",
    "basically",
    "means",
    "file",
    "written",
    "hdfs",
    "also",
    "means",
    "file",
    "related",
    "blocks",
    "stored",
    "underlying",
    "disk",
    "machines",
    "say",
    "val",
    "x",
    "equals",
    "file",
    "using",
    "method",
    "spark",
    "context",
    "various",
    "methods",
    "like",
    "whole",
    "text",
    "files",
    "parallel",
    "eyes",
    "step",
    "create",
    "rdd",
    "imagine",
    "logical",
    "data",
    "set",
    "created",
    "memory",
    "across",
    "nodes",
    "nodes",
    "data",
    "however",
    "data",
    "loaded",
    "first",
    "rdd",
    "say",
    "first",
    "step",
    "call",
    "dag",
    "dag",
    "series",
    "steps",
    "get",
    "executed",
    "later",
    "stage",
    "later",
    "could",
    "processing",
    "could",
    "say",
    "val",
    "could",
    "something",
    "x",
    "could",
    "say",
    "x",
    "dot",
    "map",
    "would",
    "want",
    "apply",
    "function",
    "every",
    "record",
    "every",
    "element",
    "file",
    "could",
    "give",
    "logic",
    "x",
    "dot",
    "map",
    "second",
    "step",
    "creating",
    "rdd",
    "resilient",
    "distributed",
    "data",
    "set",
    "say",
    "second",
    "step",
    "dag",
    "okay",
    "external",
    "rdd",
    "one",
    "rdd",
    "created",
    "depends",
    "first",
    "rtd",
    "first",
    "rdd",
    "becomes",
    "base",
    "rdd",
    "parent",
    "rdd",
    "resultant",
    "rtd",
    "becomes",
    "child",
    "rdd",
    "go",
    "could",
    "say",
    "val",
    "zed",
    "would",
    "say",
    "okay",
    "would",
    "want",
    "filter",
    "filter",
    "could",
    "give",
    "logic",
    "might",
    "searching",
    "word",
    "searching",
    "pattern",
    "could",
    "say",
    "val",
    "zed",
    "equals",
    "dot",
    "filter",
    "creates",
    "one",
    "rdd",
    "resilient",
    "distributed",
    "data",
    "set",
    "memory",
    "say",
    "nothing",
    "one",
    "step",
    "dag",
    "tag",
    "series",
    "steps",
    "executed",
    "execution",
    "happen",
    "data",
    "get",
    "data",
    "get",
    "loaded",
    "rdds",
    "using",
    "method",
    "using",
    "transformation",
    "like",
    "map",
    "using",
    "transformation",
    "like",
    "filter",
    "flat",
    "map",
    "anything",
    "else",
    "transformations",
    "operations",
    "map",
    "filter",
    "join",
    "union",
    "many",
    "others",
    "create",
    "rdds",
    "basically",
    "means",
    "creating",
    "execution",
    "logic",
    "data",
    "evaluated",
    "operation",
    "happening",
    "right",
    "invoke",
    "action",
    "might",
    "want",
    "print",
    "result",
    "might",
    "want",
    "take",
    "elements",
    "see",
    "might",
    "want",
    "count",
    "actions",
    "actually",
    "trigger",
    "execution",
    "dag",
    "right",
    "beginning",
    "say",
    "z",
    "dot",
    "count",
    "would",
    "want",
    "count",
    "number",
    "words",
    "filtering",
    "action",
    "invoked",
    "trigger",
    "execution",
    "dag",
    "right",
    "beginning",
    "happens",
    "spark",
    "z",
    "dot",
    "count",
    "start",
    "whole",
    "execution",
    "dag",
    "right",
    "beginning",
    "z",
    "dot",
    "com",
    "second",
    "time",
    "action",
    "invoked",
    "data",
    "loaded",
    "first",
    "rtd",
    "map",
    "filter",
    "finally",
    "result",
    "core",
    "concept",
    "rdds",
    "rtd",
    "works",
    "mainly",
    "spark",
    "two",
    "kind",
    "operations",
    "one",
    "transformations",
    "one",
    "actions",
    "transformations",
    "using",
    "method",
    "spark",
    "context",
    "always",
    "always",
    "create",
    "rtd",
    "could",
    "say",
    "step",
    "tag",
    "actions",
    "something",
    "invoke",
    "execution",
    "invoke",
    "execution",
    "first",
    "rdd",
    "till",
    "last",
    "rdd",
    "get",
    "result",
    "rdds",
    "work",
    "talk",
    "components",
    "spark",
    "let",
    "learn",
    "little",
    "bit",
    "spark",
    "sql",
    "spark",
    "sql",
    "component",
    "type",
    "processing",
    "framework",
    "used",
    "structured",
    "data",
    "processing",
    "usually",
    "people",
    "might",
    "structured",
    "data",
    "stored",
    "rdbms",
    "files",
    "data",
    "structured",
    "particular",
    "delimiters",
    "pattern",
    "one",
    "wants",
    "process",
    "structured",
    "data",
    "one",
    "wants",
    "use",
    "spark",
    "processing",
    "work",
    "structured",
    "data",
    "would",
    "prefer",
    "use",
    "spark",
    "sql",
    "work",
    "different",
    "data",
    "formats",
    "say",
    "csv",
    "json",
    "even",
    "work",
    "smarter",
    "formats",
    "like",
    "avro",
    "parquet",
    "even",
    "binary",
    "files",
    "sequence",
    "files",
    "could",
    "data",
    "coming",
    "rdbms",
    "extracted",
    "using",
    "jdbc",
    "connection",
    "bottom",
    "level",
    "talk",
    "spark",
    "sql",
    "data",
    "source",
    "api",
    "basically",
    "allows",
    "get",
    "data",
    "whichever",
    "format",
    "spark",
    "sql",
    "something",
    "called",
    "data",
    "frame",
    "api",
    "data",
    "frames",
    "data",
    "frames",
    "short",
    "visualize",
    "imagine",
    "rows",
    "columns",
    "data",
    "represented",
    "form",
    "rows",
    "columns",
    "column",
    "headings",
    "data",
    "frame",
    "api",
    "allows",
    "create",
    "data",
    "frames",
    "like",
    "previous",
    "example",
    "work",
    "file",
    "want",
    "process",
    "would",
    "convert",
    "rdd",
    "using",
    "method",
    "smart",
    "context",
    "transformations",
    "similar",
    "way",
    "use",
    "data",
    "frames",
    "want",
    "use",
    "spark",
    "sql",
    "would",
    "use",
    "sparks",
    "context",
    "sql",
    "context",
    "hive",
    "context",
    "spark",
    "allows",
    "work",
    "data",
    "frames",
    "like",
    "earlier",
    "example",
    "saying",
    "val",
    "x",
    "equals",
    "sc",
    "dot",
    "text",
    "file",
    "case",
    "data",
    "frames",
    "instead",
    "sc",
    "would",
    "using",
    "say",
    "spark",
    "dot",
    "something",
    "spark",
    "context",
    "available",
    "data",
    "frames",
    "api",
    "used",
    "older",
    "versions",
    "like",
    "spark",
    "using",
    "hive",
    "context",
    "sql",
    "context",
    "working",
    "spark",
    "would",
    "saying",
    "val",
    "x",
    "equals",
    "sql",
    "context",
    "dot",
    "would",
    "using",
    "spark",
    "dot",
    "data",
    "frame",
    "api",
    "basically",
    "allows",
    "create",
    "data",
    "frames",
    "structured",
    "data",
    "also",
    "lets",
    "spark",
    "know",
    "data",
    "already",
    "particular",
    "structure",
    "follows",
    "format",
    "based",
    "sparks",
    "dag",
    "scheduler",
    "right",
    "say",
    "dag",
    "talk",
    "sequence",
    "steps",
    "spark",
    "already",
    "aware",
    "different",
    "steps",
    "involved",
    "application",
    "data",
    "frame",
    "api",
    "basically",
    "allows",
    "create",
    "data",
    "frames",
    "data",
    "data",
    "frames",
    "say",
    "talking",
    "rows",
    "columns",
    "headings",
    "data",
    "frame",
    "dsl",
    "language",
    "use",
    "spark",
    "sql",
    "hive",
    "query",
    "language",
    "options",
    "used",
    "work",
    "data",
    "frames",
    "learn",
    "data",
    "frames",
    "follow",
    "next",
    "sessions",
    "talk",
    "spark",
    "streaming",
    "interesting",
    "organizations",
    "would",
    "want",
    "work",
    "streaming",
    "data",
    "imagine",
    "store",
    "like",
    "macy",
    "would",
    "want",
    "machine",
    "learning",
    "algorithms",
    "would",
    "machine",
    "learning",
    "algorithms",
    "suppose",
    "lot",
    "customers",
    "walking",
    "store",
    "searching",
    "particular",
    "product",
    "particular",
    "item",
    "could",
    "cameras",
    "placed",
    "store",
    "already",
    "done",
    "cameras",
    "placed",
    "store",
    "keep",
    "monitoring",
    "corner",
    "store",
    "customers",
    "camera",
    "captures",
    "information",
    "information",
    "streamed",
    "processed",
    "algorithms",
    "algorithms",
    "see",
    "product",
    "series",
    "product",
    "customers",
    "might",
    "interested",
    "algorithm",
    "real",
    "time",
    "process",
    "based",
    "number",
    "customers",
    "based",
    "available",
    "product",
    "store",
    "come",
    "attractive",
    "alternative",
    "price",
    "price",
    "displayed",
    "screen",
    "probably",
    "customers",
    "would",
    "buy",
    "product",
    "processing",
    "data",
    "comes",
    "algorithms",
    "work",
    "computation",
    "give",
    "result",
    "result",
    "customers",
    "buying",
    "particular",
    "product",
    "whole",
    "essence",
    "machine",
    "learning",
    "processing",
    "really",
    "hold",
    "good",
    "customers",
    "store",
    "could",
    "relate",
    "even",
    "online",
    "shopping",
    "portal",
    "might",
    "machine",
    "learning",
    "algorithms",
    "might",
    "processing",
    "based",
    "clicks",
    "customer",
    "based",
    "clicks",
    "based",
    "customer",
    "history",
    "based",
    "customer",
    "behavior",
    "algorithms",
    "come",
    "recommendation",
    "products",
    "better",
    "altered",
    "price",
    "sale",
    "happens",
    "case",
    "would",
    "seeing",
    "essence",
    "real",
    "time",
    "processing",
    "fixed",
    "particular",
    "duration",
    "time",
    "also",
    "means",
    "something",
    "process",
    "data",
    "comes",
    "spark",
    "streaming",
    "lightweight",
    "api",
    "allows",
    "developers",
    "perform",
    "batch",
    "processing",
    "also",
    "streaming",
    "processing",
    "data",
    "provides",
    "secure",
    "reliable",
    "fast",
    "processing",
    "live",
    "data",
    "streams",
    "happens",
    "spark",
    "streaming",
    "brief",
    "input",
    "data",
    "stream",
    "data",
    "stream",
    "could",
    "file",
    "constantly",
    "getting",
    "appended",
    "could",
    "kind",
    "metrics",
    "could",
    "kind",
    "events",
    "based",
    "clicks",
    "customers",
    "based",
    "products",
    "choosing",
    "store",
    "input",
    "data",
    "stream",
    "pushed",
    "spark",
    "streaming",
    "application",
    "spark",
    "streaming",
    "application",
    "broke",
    "break",
    "content",
    "smaller",
    "streams",
    "call",
    "discriticized",
    "streams",
    "batches",
    "smaller",
    "data",
    "processing",
    "happen",
    "frames",
    "could",
    "say",
    "process",
    "file",
    "every",
    "five",
    "seconds",
    "latest",
    "data",
    "come",
    "also",
    "windows",
    "based",
    "uh",
    "options",
    "like",
    "say",
    "windows",
    "mean",
    "window",
    "past",
    "three",
    "events",
    "window",
    "past",
    "three",
    "events",
    "event",
    "five",
    "seconds",
    "batches",
    "smaller",
    "data",
    "processed",
    "spark",
    "engine",
    "process",
    "data",
    "stored",
    "used",
    "processing",
    "spark",
    "streaming",
    "talk",
    "mlib",
    "low",
    "level",
    "machine",
    "learning",
    "library",
    "simple",
    "use",
    "scalable",
    "compatible",
    "various",
    "programming",
    "languages",
    "hadoop",
    "also",
    "libraries",
    "like",
    "apache",
    "mahout",
    "used",
    "machine",
    "learning",
    "algorithms",
    "however",
    "terms",
    "spark",
    "talking",
    "machine",
    "learning",
    "algorithms",
    "built",
    "using",
    "ml",
    "labs",
    "libraries",
    "spark",
    "used",
    "processing",
    "mlib",
    "eases",
    "deployment",
    "development",
    "scalable",
    "machine",
    "learning",
    "algorithms",
    "mean",
    "think",
    "clustering",
    "techniques",
    "think",
    "classification",
    "would",
    "want",
    "classify",
    "data",
    "would",
    "want",
    "supervised",
    "unsupervised",
    "learning",
    "think",
    "collaborative",
    "filtering",
    "many",
    "data",
    "science",
    "related",
    "techniques",
    "techniques",
    "required",
    "build",
    "recommendation",
    "engines",
    "machine",
    "learning",
    "algorithms",
    "built",
    "using",
    "sparks",
    "mlip",
    "graphics",
    "spark",
    "graph",
    "computation",
    "engine",
    "mainly",
    "interested",
    "graph",
    "based",
    "processing",
    "think",
    "facebook",
    "think",
    "linkedin",
    "data",
    "stored",
    "data",
    "kind",
    "network",
    "connections",
    "could",
    "say",
    "well",
    "networked",
    "could",
    "say",
    "x",
    "connected",
    "connected",
    "z",
    "z",
    "connected",
    "x",
    "z",
    "terms",
    "graph",
    "terminologies",
    "call",
    "vertices",
    "vertex",
    "basically",
    "connected",
    "connection",
    "called",
    "edges",
    "could",
    "say",
    "friend",
    "b",
    "b",
    "vertices",
    "friend",
    "relation",
    "edge",
    "data",
    "represented",
    "form",
    "graphs",
    "would",
    "want",
    "processing",
    "way",
    "could",
    "social",
    "media",
    "could",
    "network",
    "devices",
    "could",
    "cloud",
    "platform",
    "could",
    "different",
    "applications",
    "connected",
    "particular",
    "environment",
    "data",
    "represented",
    "form",
    "graph",
    "graphics",
    "used",
    "etl",
    "extraction",
    "transformation",
    "load",
    "data",
    "analysis",
    "also",
    "interactive",
    "graph",
    "computation",
    "graphics",
    "quite",
    "powerful",
    "talk",
    "spark",
    "spark",
    "work",
    "different",
    "clustering",
    "technologies",
    "work",
    "apache",
    "mesos",
    "spark",
    "came",
    "initially",
    "prove",
    "credibility",
    "apache",
    "mesos",
    "spark",
    "work",
    "yarn",
    "usually",
    "see",
    "different",
    "working",
    "environments",
    "spark",
    "also",
    "work",
    "standalone",
    "means",
    "without",
    "hadoop",
    "spar",
    "setup",
    "master",
    "worker",
    "processes",
    "usually",
    "say",
    "technically",
    "spark",
    "uses",
    "master",
    "slave",
    "architecture",
    "consists",
    "driver",
    "program",
    "run",
    "master",
    "node",
    "also",
    "run",
    "client",
    "node",
    "depends",
    "configured",
    "application",
    "multiple",
    "executors",
    "run",
    "worker",
    "nodes",
    "master",
    "node",
    "driver",
    "program",
    "driver",
    "program",
    "internally",
    "spark",
    "context",
    "spark",
    "every",
    "spark",
    "application",
    "driver",
    "program",
    "driver",
    "program",
    "inbuilt",
    "internally",
    "used",
    "spark",
    "context",
    "basically",
    "entry",
    "point",
    "application",
    "spark",
    "functionality",
    "driver",
    "driver",
    "program",
    "interacts",
    "cluster",
    "manager",
    "say",
    "interacts",
    "cluster",
    "manager",
    "spark",
    "context",
    "entry",
    "point",
    "takes",
    "application",
    "request",
    "cluster",
    "manager",
    "said",
    "cluster",
    "manager",
    "could",
    "say",
    "apache",
    "mesos",
    "could",
    "yarn",
    "could",
    "spark",
    "standalone",
    "master",
    "cluster",
    "manager",
    "terms",
    "yarn",
    "resource",
    "manager",
    "spark",
    "application",
    "internally",
    "runs",
    "series",
    "set",
    "tasks",
    "processes",
    "driver",
    "program",
    "wherever",
    "run",
    "spark",
    "context",
    "spark",
    "context",
    "take",
    "care",
    "application",
    "execution",
    "spark",
    "context",
    "talk",
    "cluster",
    "manager",
    "cluster",
    "manager",
    "could",
    "yarn",
    "terms",
    "say",
    "cluster",
    "manager",
    "yarn",
    "would",
    "resource",
    "manager",
    "high",
    "level",
    "say",
    "job",
    "split",
    "multiple",
    "tasks",
    "tasks",
    "distributed",
    "slave",
    "nodes",
    "worker",
    "nodes",
    "whenever",
    "kind",
    "transformation",
    "use",
    "method",
    "spark",
    "context",
    "rdd",
    "created",
    "rdd",
    "distributed",
    "across",
    "multiple",
    "nodes",
    "explained",
    "earlier",
    "worker",
    "nodes",
    "slaves",
    "run",
    "different",
    "tasks",
    "spark",
    "architecture",
    "looks",
    "like",
    "learn",
    "spark",
    "architecture",
    "interaction",
    "yarn",
    "usually",
    "happens",
    "spark",
    "context",
    "interacts",
    "cluster",
    "manager",
    "terms",
    "yarn",
    "could",
    "say",
    "resource",
    "manager",
    "already",
    "know",
    "yarn",
    "would",
    "say",
    "node",
    "managers",
    "running",
    "multiple",
    "machines",
    "machine",
    "ram",
    "cpu",
    "cores",
    "allocated",
    "node",
    "manager",
    "machine",
    "data",
    "nodes",
    "running",
    "obviously",
    "hadoop",
    "related",
    "data",
    "whenever",
    "application",
    "wants",
    "process",
    "data",
    "application",
    "via",
    "spark",
    "contacts",
    "contacts",
    "cluster",
    "managers",
    "resource",
    "manager",
    "resource",
    "manager",
    "resource",
    "manager",
    "makes",
    "request",
    "resource",
    "manager",
    "makes",
    "requests",
    "node",
    "manager",
    "machines",
    "wherever",
    "relevant",
    "data",
    "resides",
    "asking",
    "containers",
    "resource",
    "manager",
    "negotiating",
    "asking",
    "containers",
    "node",
    "manager",
    "saying",
    "hey",
    "container",
    "1gb",
    "ram",
    "one",
    "cpu",
    "core",
    "container",
    "1gb",
    "ram",
    "1",
    "cpu",
    "core",
    "node",
    "manager",
    "based",
    "kind",
    "processing",
    "approve",
    "deny",
    "node",
    "manager",
    "would",
    "say",
    "fine",
    "give",
    "container",
    "container",
    "allocated",
    "approved",
    "node",
    "manager",
    "resource",
    "manager",
    "basically",
    "start",
    "extra",
    "piece",
    "code",
    "called",
    "appmaster",
    "appmaster",
    "responsible",
    "execution",
    "applications",
    "whether",
    "spark",
    "applications",
    "mapreduce",
    "application",
    "master",
    "piece",
    "code",
    "run",
    "one",
    "containers",
    "use",
    "ram",
    "cpu",
    "core",
    "use",
    "containers",
    "allocated",
    "node",
    "manager",
    "run",
    "tasks",
    "within",
    "container",
    "take",
    "care",
    "execution",
    "container",
    "combination",
    "ram",
    "cpu",
    "core",
    "within",
    "container",
    "executable",
    "process",
    "would",
    "run",
    "executor",
    "process",
    "taking",
    "care",
    "application",
    "related",
    "tasks",
    "overall",
    "spark",
    "works",
    "integration",
    "yarn",
    "let",
    "learn",
    "spark",
    "cluster",
    "managers",
    "said",
    "spark",
    "work",
    "standalone",
    "mode",
    "without",
    "hadoop",
    "default",
    "application",
    "submitted",
    "spark",
    "standalone",
    "mode",
    "cluster",
    "run",
    "fifo",
    "order",
    "application",
    "try",
    "use",
    "available",
    "nodes",
    "could",
    "spark",
    "standalone",
    "cluster",
    "basically",
    "means",
    "could",
    "multiple",
    "nodes",
    "one",
    "nodes",
    "would",
    "master",
    "process",
    "running",
    "nodes",
    "would",
    "spark",
    "worker",
    "processes",
    "running",
    "would",
    "distributed",
    "file",
    "system",
    "spark",
    "standalone",
    "rely",
    "external",
    "storage",
    "get",
    "data",
    "probably",
    "file",
    "system",
    "nodes",
    "data",
    "stored",
    "processing",
    "happen",
    "across",
    "nodes",
    "worker",
    "processes",
    "running",
    "could",
    "spark",
    "working",
    "apache",
    "mesos",
    "said",
    "apache",
    "mesos",
    "open",
    "source",
    "project",
    "manage",
    "computer",
    "clusters",
    "also",
    "run",
    "hadoop",
    "applications",
    "apache",
    "mesos",
    "introduced",
    "earlier",
    "spark",
    "came",
    "existence",
    "prove",
    "credibility",
    "apache",
    "mesos",
    "spark",
    "working",
    "hadoop",
    "yarn",
    "something",
    "widely",
    "see",
    "different",
    "working",
    "environments",
    "yarn",
    "takes",
    "care",
    "processing",
    "take",
    "care",
    "different",
    "processing",
    "frameworks",
    "also",
    "supports",
    "spark",
    "could",
    "kubernetes",
    "something",
    "making",
    "lot",
    "news",
    "today",
    "world",
    "open",
    "source",
    "system",
    "automating",
    "deployment",
    "scaling",
    "management",
    "containerized",
    "applications",
    "could",
    "multiple",
    "docker",
    "based",
    "images",
    "connecting",
    "spark",
    "also",
    "works",
    "kubernetes",
    "let",
    "look",
    "applications",
    "spark",
    "jpmorgan",
    "chase",
    "company",
    "uses",
    "spark",
    "detect",
    "fraudulent",
    "transactions",
    "analyze",
    "business",
    "spends",
    "individual",
    "suggest",
    "offers",
    "identify",
    "patterns",
    "decide",
    "much",
    "invest",
    "invest",
    "one",
    "examples",
    "banking",
    "lot",
    "banking",
    "environments",
    "using",
    "spark",
    "due",
    "processing",
    "capabilities",
    "faster",
    "processing",
    "could",
    "working",
    "fraud",
    "detection",
    "credit",
    "analysis",
    "pattern",
    "identification",
    "many",
    "use",
    "cases",
    "alibaba",
    "group",
    "uses",
    "also",
    "spark",
    "analyze",
    "large",
    "data",
    "sets",
    "data",
    "transaction",
    "details",
    "might",
    "based",
    "online",
    "stores",
    "looking",
    "browsing",
    "history",
    "form",
    "spark",
    "jobs",
    "provides",
    "recommendations",
    "users",
    "alibaba",
    "group",
    "using",
    "spark",
    "domain",
    "iq",
    "leading",
    "healthcare",
    "company",
    "uses",
    "spark",
    "analyze",
    "patients",
    "data",
    "identify",
    "possible",
    "health",
    "issues",
    "diagnose",
    "based",
    "medical",
    "history",
    "lot",
    "work",
    "happening",
    "healthcare",
    "industry",
    "processing",
    "finding",
    "lot",
    "importance",
    "faster",
    "processing",
    "required",
    "healthcare",
    "industry",
    "iqvi",
    "also",
    "using",
    "spark",
    "netflix",
    "known",
    "riot",
    "games",
    "entertainment",
    "gaming",
    "companies",
    "like",
    "netflix",
    "ride",
    "games",
    "use",
    "apache",
    "spark",
    "showcase",
    "relevant",
    "advertisements",
    "users",
    "based",
    "videos",
    "watched",
    "shared",
    "liked",
    "domains",
    "find",
    "use",
    "cases",
    "spark",
    "banking",
    "health",
    "care",
    "entertainment",
    "many",
    "using",
    "spark",
    "day",
    "day",
    "activities",
    "real",
    "time",
    "memory",
    "faster",
    "processing",
    "let",
    "discuss",
    "sparks",
    "use",
    "case",
    "let",
    "talk",
    "conviva",
    "world",
    "leading",
    "video",
    "streaming",
    "companies",
    "video",
    "streaming",
    "challenge",
    "talk",
    "youtube",
    "data",
    "could",
    "always",
    "read",
    "youtube",
    "data",
    "worth",
    "watching",
    "10",
    "years",
    "huge",
    "amount",
    "data",
    "people",
    "uploading",
    "videos",
    "companies",
    "advertisements",
    "videos",
    "streamed",
    "watched",
    "users",
    "video",
    "streaming",
    "challenge",
    "especially",
    "increasing",
    "demand",
    "high",
    "quality",
    "streaming",
    "experiences",
    "conviva",
    "collects",
    "data",
    "video",
    "streaming",
    "quality",
    "give",
    "customers",
    "visibility",
    "end",
    "user",
    "experience",
    "delivering",
    "apache",
    "spark",
    "using",
    "apache",
    "spark",
    "conviva",
    "delivers",
    "better",
    "quality",
    "service",
    "customers",
    "removing",
    "screen",
    "buffering",
    "learning",
    "detail",
    "network",
    "conditions",
    "real",
    "time",
    "information",
    "stored",
    "video",
    "player",
    "manage",
    "live",
    "video",
    "traffic",
    "coming",
    "4",
    "million",
    "video",
    "feeds",
    "every",
    "month",
    "ensure",
    "maximum",
    "retention",
    "using",
    "apache",
    "spark",
    "conviva",
    "created",
    "auto",
    "diagnostics",
    "alert",
    "automatically",
    "detects",
    "anomalies",
    "along",
    "video",
    "streaming",
    "pipeline",
    "diagnoses",
    "root",
    "cause",
    "issue",
    "really",
    "makes",
    "one",
    "leading",
    "video",
    "streaming",
    "companies",
    "based",
    "auto",
    "diagnostic",
    "alerts",
    "reduces",
    "waiting",
    "time",
    "video",
    "starts",
    "avoids",
    "buffering",
    "recovers",
    "video",
    "technical",
    "error",
    "whole",
    "goal",
    "maximize",
    "viewer",
    "engagement",
    "sparks",
    "use",
    "case",
    "conviva",
    "using",
    "spark",
    "different",
    "ways",
    "stay",
    "ahead",
    "video",
    "streaming",
    "related",
    "deliveries",
    "let",
    "quick",
    "demo",
    "setting",
    "spark",
    "windows",
    "trying",
    "sparks",
    "interactive",
    "way",
    "working",
    "first",
    "thing",
    "download",
    "spark",
    "go",
    "google",
    "type",
    "spark",
    "download",
    "click",
    "link",
    "shows",
    "spark",
    "release",
    "package",
    "type",
    "says",
    "get",
    "apache",
    "hadoop",
    "related",
    "spark",
    "choose",
    "say",
    "spark",
    "choose",
    "apache",
    "click",
    "download",
    "park",
    "bin",
    "hadoop",
    "star",
    "file",
    "click",
    "link",
    "takes",
    "mirror",
    "site",
    "click",
    "link",
    "download",
    "spark",
    "done",
    "already",
    "downloaded",
    "go",
    "downloads",
    "shows",
    "tar",
    "file",
    "unturn",
    "unzip",
    "click",
    "link",
    "already",
    "winzip",
    "allows",
    "unzip",
    "spark",
    "choose",
    "location",
    "say",
    "unzip",
    "click",
    "unzip",
    "choose",
    "local",
    "choose",
    "one",
    "folders",
    "already",
    "unzipped",
    "spar",
    "directory",
    "exists",
    "sorry",
    "let",
    "go",
    "look",
    "directory",
    "contains",
    "click",
    "c",
    "drive",
    "spark",
    "folders",
    "required",
    "us",
    "use",
    "spark",
    "look",
    "bin",
    "different",
    "applications",
    "commands",
    "use",
    "also",
    "see",
    "would",
    "added",
    "one",
    "utilities",
    "either",
    "win",
    "utils",
    "spark",
    "need",
    "hadoop",
    "planning",
    "use",
    "spark",
    "windows",
    "need",
    "desktop",
    "hadoop",
    "directory",
    "bin",
    "folder",
    "downloaded",
    "win",
    "utils",
    "executable",
    "file",
    "always",
    "search",
    "internet",
    "say",
    "download",
    "spark",
    "find",
    "link",
    "download",
    "example",
    "click",
    "search",
    "link",
    "open",
    "link",
    "different",
    "tab",
    "basically",
    "shows",
    "utils",
    "hadoop",
    "similarly",
    "search",
    "hadoop",
    "download",
    "however",
    "also",
    "work",
    "fine",
    "downloaded",
    "hadoop",
    "folder",
    "within",
    "bin",
    "need",
    "two",
    "things",
    "one",
    "spark",
    "untired",
    "particular",
    "location",
    "hadoop",
    "utils",
    "things",
    "set",
    "environment",
    "variables",
    "type",
    "envir",
    "go",
    "edit",
    "system",
    "environment",
    "variables",
    "click",
    "environment",
    "variables",
    "see",
    "added",
    "two",
    "variables",
    "click",
    "new",
    "give",
    "variable",
    "name",
    "hadoop",
    "underscore",
    "home",
    "give",
    "path",
    "hadoop",
    "directory",
    "contains",
    "bin",
    "contains",
    "also",
    "added",
    "spark",
    "home",
    "points",
    "myspark",
    "directory",
    "ready",
    "use",
    "spark",
    "windows",
    "local",
    "mode",
    "done",
    "open",
    "command",
    "prompt",
    "need",
    "go",
    "spark",
    "directory",
    "say",
    "cd",
    "spar",
    "dir",
    "slash",
    "p",
    "shows",
    "folders",
    "execute",
    "spark",
    "look",
    "bin",
    "different",
    "programs",
    "spark",
    "shell",
    "work",
    "scala",
    "interactive",
    "way",
    "working",
    "spark",
    "also",
    "use",
    "pi",
    "spark",
    "allows",
    "use",
    "python",
    "work",
    "spark",
    "also",
    "spark",
    "submit",
    "packaged",
    "application",
    "jar",
    "file",
    "submit",
    "cluster",
    "case",
    "would",
    "using",
    "spark",
    "local",
    "mode",
    "let",
    "use",
    "spark",
    "shell",
    "test",
    "say",
    "spark",
    "shell",
    "way",
    "start",
    "spark",
    "interactive",
    "way",
    "based",
    "path",
    "set",
    "spark",
    "shell",
    "able",
    "use",
    "spark",
    "says",
    "welcome",
    "spark",
    "version",
    "also",
    "shows",
    "spark",
    "context",
    "entry",
    "point",
    "application",
    "available",
    "sc",
    "wherein",
    "connecting",
    "master",
    "local",
    "application",
    "id",
    "also",
    "shows",
    "spark",
    "ui",
    "look",
    "says",
    "spark",
    "session",
    "available",
    "spark",
    "interested",
    "working",
    "spark",
    "core",
    "rdds",
    "would",
    "using",
    "spark",
    "context",
    "interested",
    "working",
    "spark",
    "sql",
    "data",
    "frames",
    "data",
    "sets",
    "using",
    "spark",
    "session",
    "let",
    "try",
    "declare",
    "variable",
    "immutable",
    "variable",
    "use",
    "spark",
    "context",
    "method",
    "spark",
    "context",
    "could",
    "tab",
    "see",
    "available",
    "options",
    "within",
    "spark",
    "context",
    "use",
    "one",
    "read",
    "file",
    "use",
    "file",
    "need",
    "point",
    "file",
    "already",
    "know",
    "spark",
    "directory",
    "file",
    "called",
    "readme",
    "dot",
    "rd",
    "method",
    "spark",
    "context",
    "enter",
    "create",
    "rdd",
    "explained",
    "rdds",
    "resilient",
    "distributed",
    "data",
    "sets",
    "evaluation",
    "happening",
    "right",
    "even",
    "file",
    "exist",
    "would",
    "still",
    "rdd",
    "created",
    "go",
    "transformations",
    "could",
    "say",
    "val",
    "could",
    "say",
    "take",
    "x",
    "would",
    "like",
    "map",
    "transformation",
    "would",
    "use",
    "uppercase",
    "inbuilt",
    "function",
    "would",
    "want",
    "convert",
    "content",
    "file",
    "uppercase",
    "hit",
    "enter",
    "creates",
    "rdd",
    "rdd",
    "basically",
    "child",
    "rdd",
    "parent",
    "rtd",
    "created",
    "text",
    "file",
    "remember",
    "evaluation",
    "happening",
    "execution",
    "happening",
    "transformations",
    "lead",
    "creation",
    "rdds",
    "invoke",
    "action",
    "see",
    "result",
    "whenever",
    "invoke",
    "action",
    "trigger",
    "execution",
    "dag",
    "starting",
    "first",
    "rdd",
    "till",
    "last",
    "transformation",
    "invoking",
    "action",
    "could",
    "say",
    "count",
    "see",
    "many",
    "lines",
    "says",
    "input",
    "path",
    "exist",
    "able",
    "find",
    "file",
    "location",
    "clearly",
    "explains",
    "evaluation",
    "happening",
    "tried",
    "read",
    "file",
    "neither",
    "map",
    "transformation",
    "invoked",
    "action",
    "tried",
    "search",
    "file",
    "location",
    "let",
    "quit",
    "check",
    "file",
    "exists",
    "quit",
    "ctrl",
    "could",
    "colon",
    "quit",
    "see",
    "messages",
    "like",
    "says",
    "unable",
    "delete",
    "temporary",
    "file",
    "known",
    "issue",
    "fix",
    "spark",
    "working",
    "fine",
    "let",
    "go",
    "check",
    "file",
    "exists",
    "go",
    "look",
    "spark",
    "directory",
    "see",
    "file",
    "exists",
    "file",
    "readme",
    "dot",
    "md",
    "however",
    "done",
    "readme",
    "dot",
    "rd",
    "let",
    "let",
    "test",
    "spark",
    "shell",
    "works",
    "fine",
    "go",
    "spark",
    "directory",
    "either",
    "could",
    "going",
    "bin",
    "directory",
    "start",
    "spark",
    "shell",
    "people",
    "prefer",
    "way",
    "wherein",
    "say",
    "bin",
    "spark",
    "shell",
    "start",
    "spark",
    "shell",
    "error",
    "slash",
    "let",
    "way",
    "start",
    "spark",
    "shell",
    "interactive",
    "way",
    "working",
    "spark",
    "using",
    "spark",
    "context",
    "spark",
    "done",
    "say",
    "val",
    "x",
    "say",
    "sc",
    "spark",
    "context",
    "already",
    "initialized",
    "start",
    "spark",
    "shell",
    "say",
    "text",
    "file",
    "would",
    "want",
    "read",
    "file",
    "readme",
    "dot",
    "md",
    "know",
    "file",
    "exists",
    "however",
    "checking",
    "file",
    "existing",
    "means",
    "evaluation",
    "happen",
    "transformation",
    "use",
    "method",
    "spark",
    "context",
    "creates",
    "rdd",
    "click",
    "created",
    "rdd",
    "string",
    "transformation",
    "x",
    "saying",
    "map",
    "say",
    "would",
    "want",
    "convert",
    "content",
    "uppercase",
    "also",
    "done",
    "finally",
    "let",
    "invoke",
    "action",
    "trigger",
    "execution",
    "tag",
    "contains",
    "rdds",
    "could",
    "say",
    "could",
    "say",
    "take",
    "10",
    "could",
    "say",
    "print",
    "ln",
    "able",
    "invoke",
    "action",
    "shows",
    "result",
    "file",
    "converted",
    "uppercase",
    "remember",
    "dot",
    "count",
    "another",
    "action",
    "trigger",
    "execution",
    "dag",
    "right",
    "beginning",
    "first",
    "rtd",
    "created",
    "transformation",
    "done",
    "show",
    "count",
    "lines",
    "exist",
    "result",
    "says",
    "105",
    "lines",
    "simple",
    "example",
    "using",
    "spark",
    "shell",
    "using",
    "spar",
    "interactive",
    "way",
    "windows",
    "machine",
    "quit",
    "always",
    "colon",
    "quit",
    "could",
    "control",
    "basically",
    "takes",
    "spark",
    "shell",
    "similarly",
    "work",
    "pi",
    "spark",
    "saying",
    "dot",
    "slash",
    "bin",
    "slash",
    "pi",
    "spark",
    "python",
    "way",
    "working",
    "spark",
    "brings",
    "python",
    "shell",
    "starts",
    "spark",
    "context",
    "available",
    "sc",
    "interested",
    "working",
    "data",
    "frames",
    "data",
    "sets",
    "using",
    "spark",
    "sql",
    "would",
    "using",
    "spark",
    "declare",
    "variable",
    "x",
    "file",
    "like",
    "using",
    "scala",
    "referring",
    "file",
    "readme",
    "dot",
    "md",
    "created",
    "rdd",
    "type",
    "x",
    "shows",
    "rdd",
    "created",
    "transformation",
    "using",
    "x",
    "dot",
    "map",
    "could",
    "say",
    "would",
    "want",
    "convert",
    "uppercase",
    "case",
    "python",
    "would",
    "normally",
    "use",
    "lambda",
    "would",
    "want",
    "kind",
    "transformation",
    "would",
    "say",
    "lambda",
    "x",
    "would",
    "want",
    "convert",
    "uppercase",
    "check",
    "rdd",
    "correctly",
    "created",
    "saying",
    "transformation",
    "say",
    "equals",
    "would",
    "want",
    "transformation",
    "x",
    "would",
    "say",
    "x",
    "dot",
    "map",
    "case",
    "python",
    "explained",
    "whenever",
    "want",
    "use",
    "anonymous",
    "function",
    "want",
    "transformation",
    "pass",
    "function",
    "transformation",
    "say",
    "lambda",
    "say",
    "line",
    "could",
    "say",
    "x",
    "whatever",
    "could",
    "say",
    "line",
    "would",
    "use",
    "inbuilt",
    "function",
    "upper",
    "convert",
    "content",
    "x",
    "uppercase",
    "remember",
    "transformations",
    "click",
    "enter",
    "sorry",
    "enter",
    "show",
    "created",
    "rdd",
    "see",
    "content",
    "say",
    "dot",
    "collect",
    "show",
    "content",
    "first",
    "lines",
    "converted",
    "uppercase",
    "clearly",
    "shows",
    "even",
    "use",
    "python",
    "work",
    "spark",
    "using",
    "pi",
    "spark",
    "quit",
    "say",
    "quit",
    "take",
    "pi",
    "spark",
    "shell",
    "work",
    "spark",
    "shell",
    "pi",
    "spark",
    "windows",
    "machine",
    "local",
    "spark",
    "setup",
    "basically",
    "spark",
    "running",
    "local",
    "mode",
    "could",
    "using",
    "interactive",
    "way",
    "working",
    "spark",
    "packaged",
    "application",
    "jar",
    "file",
    "could",
    "using",
    "spark",
    "submit",
    "submit",
    "application",
    "basically",
    "bring",
    "one",
    "ides",
    "case",
    "using",
    "eclipse",
    "get",
    "eclipse",
    "two",
    "options",
    "one",
    "writing",
    "code",
    "ide",
    "eclipse",
    "compiling",
    "code",
    "based",
    "spark",
    "related",
    "jars",
    "run",
    "application",
    "ide",
    "would",
    "interact",
    "spark",
    "one",
    "way",
    "second",
    "way",
    "use",
    "packaging",
    "tools",
    "like",
    "sbt",
    "package",
    "application",
    "jar",
    "push",
    "jar",
    "local",
    "spark",
    "hadoop",
    "base",
    "park",
    "run",
    "using",
    "spark",
    "submit",
    "simple",
    "sample",
    "application",
    "set",
    "certain",
    "things",
    "first",
    "thing",
    "downloaded",
    "eclipse",
    "people",
    "new",
    "might",
    "add",
    "scala",
    "see",
    "eclipse",
    "shows",
    "java",
    "perspective",
    "shows",
    "scalar",
    "perspective",
    "get",
    "click",
    "help",
    "say",
    "install",
    "new",
    "software",
    "say",
    "add",
    "say",
    "scala",
    "ide",
    "give",
    "link",
    "get",
    "scala",
    "ide",
    "get",
    "go",
    "browser",
    "say",
    "scala",
    "ide",
    "dot",
    "org",
    "go",
    "link",
    "scroll",
    "click",
    "stable",
    "shows",
    "different",
    "releases",
    "different",
    "ides",
    "eclipse",
    "although",
    "shows",
    "oxygen",
    "eclipse",
    "neon",
    "works",
    "fine",
    "copy",
    "link",
    "copied",
    "link",
    "give",
    "location",
    "say",
    "okay",
    "shows",
    "different",
    "options",
    "choose",
    "color",
    "ide",
    "eclipse",
    "case",
    "already",
    "installed",
    "options",
    "show",
    "however",
    "option",
    "example",
    "let",
    "choose",
    "something",
    "else",
    "say",
    "say",
    "scala",
    "search",
    "next",
    "gets",
    "activated",
    "scala",
    "id",
    "already",
    "installed",
    "case",
    "click",
    "next",
    "basically",
    "get",
    "scala",
    "search",
    "plugin",
    "also",
    "added",
    "eclipse",
    "done",
    "prompted",
    "restart",
    "eclipse",
    "scala",
    "perspective",
    "cancel",
    "already",
    "scala",
    "id",
    "dot",
    "eclipse",
    "done",
    "cancel",
    "scala",
    "id",
    "scala",
    "plugin",
    "added",
    "ide",
    "always",
    "click",
    "icon",
    "click",
    "double",
    "click",
    "scala",
    "open",
    "scala",
    "perspective",
    "first",
    "thing",
    "always",
    "click",
    "file",
    "new",
    "create",
    "scala",
    "project",
    "example",
    "could",
    "say",
    "test",
    "apps",
    "say",
    "finish",
    "added",
    "project",
    "need",
    "need",
    "make",
    "sure",
    "setup",
    "fine",
    "first",
    "thing",
    "would",
    "suggest",
    "click",
    "go",
    "build",
    "path",
    "click",
    "configure",
    "build",
    "path",
    "says",
    "color",
    "compiler",
    "would",
    "good",
    "choose",
    "project",
    "settings",
    "instead",
    "go",
    "sometimes",
    "might",
    "problems",
    "scala",
    "might",
    "able",
    "compile",
    "scala",
    "use",
    "latest",
    "bundle",
    "say",
    "apply",
    "says",
    "compiler",
    "settings",
    "changed",
    "full",
    "rebuild",
    "required",
    "changes",
    "take",
    "effect",
    "shall",
    "projects",
    "cleaned",
    "say",
    "okay",
    "say",
    "okay",
    "build",
    "workspace",
    "use",
    "scala",
    "bundle",
    "done",
    "second",
    "thing",
    "need",
    "would",
    "want",
    "write",
    "code",
    "would",
    "want",
    "code",
    "compile",
    "said",
    "want",
    "write",
    "code",
    "using",
    "ide",
    "run",
    "ide",
    "rather",
    "using",
    "build",
    "tool",
    "like",
    "meven",
    "sbt",
    "add",
    "spark",
    "related",
    "jars",
    "build",
    "path",
    "right",
    "click",
    "say",
    "build",
    "path",
    "click",
    "configure",
    "build",
    "path",
    "says",
    "libraries",
    "say",
    "add",
    "external",
    "charts",
    "needs",
    "spark",
    "related",
    "jars",
    "find",
    "remember",
    "downloaded",
    "spark",
    "even",
    "find",
    "click",
    "wherever",
    "spark",
    "click",
    "jars",
    "shows",
    "spark",
    "related",
    "jars",
    "control",
    "add",
    "build",
    "path",
    "say",
    "apply",
    "say",
    "okay",
    "done",
    "already",
    "added",
    "spark",
    "related",
    "jars",
    "might",
    "good",
    "enough",
    "write",
    "application",
    "done",
    "need",
    "basically",
    "go",
    "ahead",
    "write",
    "code",
    "click",
    "source",
    "say",
    "new",
    "say",
    "package",
    "say",
    "main",
    "dot",
    "scala",
    "click",
    "finish",
    "created",
    "main",
    "dots",
    "color",
    "would",
    "source",
    "main",
    "slash",
    "scala",
    "folders",
    "always",
    "check",
    "go",
    "c",
    "drive",
    "look",
    "users",
    "win",
    "10",
    "look",
    "workspace",
    "project",
    "see",
    "source",
    "main",
    "scala",
    "writing",
    "code",
    "right",
    "click",
    "say",
    "new",
    "create",
    "object",
    "give",
    "name",
    "say",
    "test",
    "app",
    "say",
    "finish",
    "case",
    "scala",
    "spark",
    "need",
    "create",
    "object",
    "need",
    "define",
    "main",
    "class",
    "done",
    "also",
    "need",
    "import",
    "packages",
    "need",
    "initialize",
    "spark",
    "context",
    "take",
    "example",
    "already",
    "written",
    "code",
    "look",
    "source",
    "main",
    "scala",
    "created",
    "app",
    "called",
    "first",
    "package",
    "name",
    "importing",
    "packages",
    "required",
    "us",
    "initialize",
    "path",
    "context",
    "spark",
    "configuration",
    "spark",
    "context",
    "spark",
    "conf",
    "object",
    "particular",
    "project",
    "least",
    "one",
    "object",
    "one",
    "application",
    "needs",
    "main",
    "define",
    "main",
    "saying",
    "def",
    "main",
    "args",
    "array",
    "string",
    "need",
    "basically",
    "define",
    "spark",
    "context",
    "creating",
    "configuration",
    "object",
    "say",
    "new",
    "spark",
    "conf",
    "setting",
    "application",
    "name",
    "hello",
    "spark",
    "important",
    "intend",
    "run",
    "spark",
    "application",
    "either",
    "using",
    "spark",
    "submit",
    "id",
    "need",
    "specify",
    "master",
    "local",
    "could",
    "given",
    "one",
    "thread",
    "multiple",
    "threads",
    "configuration",
    "initialize",
    "spark",
    "context",
    "pointing",
    "configuration",
    "important",
    "part",
    "see",
    "something",
    "showed",
    "command",
    "line",
    "val",
    "x",
    "pointing",
    "file",
    "project",
    "directory",
    "make",
    "sure",
    "giving",
    "windows",
    "path",
    "giving",
    "right",
    "escape",
    "characters",
    "would",
    "intend",
    "run",
    "application",
    "hadoop",
    "based",
    "cluster",
    "give",
    "path",
    "file",
    "accordingly",
    "see",
    "later",
    "val",
    "x",
    "pointing",
    "file",
    "created",
    "contains",
    "three",
    "lines",
    "sample",
    "file",
    "want",
    "test",
    "new",
    "file",
    "created",
    "variable",
    "called",
    "x",
    "would",
    "say",
    "val",
    "would",
    "use",
    "spark",
    "context",
    "method",
    "text",
    "file",
    "like",
    "interactive",
    "way",
    "pointing",
    "x",
    "also",
    "using",
    "extra",
    "features",
    "like",
    "caching",
    "cache",
    "rdd",
    "gets",
    "created",
    "create",
    "variable",
    "called",
    "counts",
    "flat",
    "map",
    "transformation",
    "would",
    "split",
    "content",
    "based",
    "space",
    "would",
    "map",
    "transformation",
    "would",
    "get",
    "every",
    "word",
    "file",
    "map",
    "number",
    "one",
    "finally",
    "reduce",
    "key",
    "operation",
    "done",
    "saves",
    "text",
    "file",
    "could",
    "collect",
    "could",
    "take",
    "could",
    "count",
    "could",
    "also",
    "save",
    "text",
    "file",
    "would",
    "want",
    "output",
    "saved",
    "location",
    "plus",
    "using",
    "java",
    "utility",
    "append",
    "random",
    "number",
    "output",
    "directory",
    "finally",
    "spark",
    "context",
    "stop",
    "application",
    "see",
    "application",
    "completely",
    "compiles",
    "already",
    "added",
    "relevant",
    "jar",
    "files",
    "build",
    "path",
    "application",
    "done",
    "need",
    "run",
    "need",
    "look",
    "run",
    "configurations",
    "click",
    "run",
    "configurations",
    "giving",
    "project",
    "name",
    "specifying",
    "main",
    "class",
    "first",
    "app",
    "straight",
    "away",
    "go",
    "environment",
    "added",
    "two",
    "variables",
    "one",
    "spark",
    "underscore",
    "local",
    "underscore",
    "ip",
    "points",
    "machine",
    "host",
    "said",
    "hadoop",
    "underscore",
    "home",
    "spark",
    "even",
    "running",
    "locally",
    "would",
    "want",
    "hadoop",
    "existing",
    "set",
    "hadoop",
    "underscore",
    "home",
    "pointing",
    "desktop",
    "hadoop",
    "directory",
    "contains",
    "bin",
    "folder",
    "bin",
    "folder",
    "contains",
    "done",
    "fine",
    "test",
    "application",
    "clicking",
    "run",
    "trigger",
    "application",
    "windows",
    "machine",
    "using",
    "ide",
    "package",
    "jar",
    "application",
    "completed",
    "created",
    "one",
    "additional",
    "directory",
    "click",
    "right",
    "click",
    "try",
    "refresh",
    "see",
    "new",
    "output",
    "created",
    "look",
    "part",
    "file",
    "shows",
    "word",
    "count",
    "simple",
    "example",
    "setting",
    "ide",
    "running",
    "applications",
    "ide",
    "local",
    "spark",
    "also",
    "see",
    "build",
    "file",
    "case",
    "would",
    "want",
    "create",
    "project",
    "would",
    "want",
    "package",
    "application",
    "jar",
    "file",
    "run",
    "cluster",
    "using",
    "spark",
    "submit",
    "case",
    "avoid",
    "adding",
    "jars",
    "build",
    "path",
    "would",
    "need",
    "would",
    "need",
    "build",
    "dot",
    "sbt",
    "file",
    "within",
    "project",
    "folder",
    "remember",
    "intend",
    "package",
    "application",
    "jar",
    "run",
    "cluster",
    "local",
    "setup",
    "would",
    "need",
    "build",
    "related",
    "charts",
    "however",
    "build",
    "related",
    "jars",
    "already",
    "cause",
    "harm",
    "let",
    "file",
    "also",
    "existing",
    "project",
    "folder",
    "run",
    "fact",
    "delete",
    "target",
    "directory",
    "anyways",
    "recreated",
    "would",
    "want",
    "also",
    "clean",
    "avoid",
    "confusion",
    "delete",
    "project",
    "folder",
    "libraries",
    "file",
    "contain",
    "basically",
    "name",
    "giving",
    "version",
    "appended",
    "jar",
    "saying",
    "scala",
    "version",
    "using",
    "saying",
    "spark",
    "version",
    "replace",
    "version",
    "intend",
    "use",
    "pointing",
    "different",
    "spark",
    "components",
    "relevant",
    "dependency",
    "related",
    "jars",
    "sbt",
    "get",
    "given",
    "spark",
    "core",
    "spark",
    "sql",
    "mlib",
    "spark",
    "streaming",
    "spark",
    "hive",
    "fetched",
    "repository",
    "file",
    "exists",
    "project",
    "folder",
    "code",
    "written",
    "build",
    "dot",
    "sbt",
    "file",
    "already",
    "existing",
    "project",
    "folder",
    "code",
    "already",
    "fine",
    "need",
    "use",
    "sbt",
    "package",
    "applications",
    "jar",
    "download",
    "sbt",
    "say",
    "download",
    "sbt",
    "windows",
    "take",
    "installing",
    "svt",
    "windows",
    "page",
    "download",
    "msi",
    "installer",
    "run",
    "installer",
    "basically",
    "install",
    "sbt",
    "machine",
    "already",
    "done",
    "build",
    "file",
    "ready",
    "code",
    "ready",
    "need",
    "use",
    "sbt",
    "command",
    "line",
    "say",
    "cd",
    "go",
    "users",
    "go",
    "win10",
    "go",
    "workspace",
    "see",
    "projects",
    "go",
    "spark",
    "apps",
    "project",
    "folder",
    "double",
    "check",
    "fine",
    "sbt",
    "already",
    "installed",
    "machine",
    "say",
    "sbt",
    "would",
    "want",
    "check",
    "always",
    "sbt",
    "version",
    "check",
    "svt",
    "command",
    "also",
    "done",
    "install",
    "sbt",
    "first",
    "type",
    "always",
    "sbt",
    "version",
    "displaying",
    "displaying",
    "svds",
    "version",
    "try",
    "fix",
    "dependencies",
    "try",
    "get",
    "relevant",
    "dependencies",
    "check",
    "sbt",
    "installed",
    "machine",
    "said",
    "code",
    "already",
    "ready",
    "file",
    "say",
    "sbt",
    "package",
    "refer",
    "look",
    "code",
    "source",
    "main",
    "scala",
    "everything",
    "fine",
    "package",
    "jar",
    "create",
    "folders",
    "within",
    "project",
    "folder",
    "click",
    "try",
    "refresh",
    "see",
    "see",
    "project",
    "created",
    "target",
    "folder",
    "created",
    "within",
    "scala",
    "jar",
    "created",
    "see",
    "sbt",
    "fine",
    "packaged",
    "code",
    "created",
    "jar",
    "file",
    "simple",
    "project",
    "used",
    "run",
    "cluster",
    "local",
    "mode",
    "let",
    "check",
    "eclipse",
    "click",
    "refresh",
    "see",
    "chart",
    "already",
    "existing",
    "basically",
    "means",
    "spark",
    "submit",
    "see",
    "already",
    "running",
    "spark",
    "submit",
    "command",
    "might",
    "show",
    "error",
    "messages",
    "see",
    "see",
    "packaging",
    "done",
    "went",
    "spark",
    "folder",
    "bin",
    "slash",
    "spark",
    "submit",
    "mentioned",
    "jar",
    "file",
    "said",
    "class",
    "main",
    "dot",
    "scala",
    "dot",
    "first",
    "app",
    "app",
    "using",
    "spark",
    "submit",
    "submit",
    "application",
    "running",
    "local",
    "mode",
    "see",
    "starts",
    "processing",
    "says",
    "created",
    "local",
    "directory",
    "starts",
    "executor",
    "localhost",
    "goes",
    "execution",
    "see",
    "could",
    "also",
    "see",
    "add",
    "jar",
    "class",
    "path",
    "finally",
    "execution",
    "error",
    "message",
    "might",
    "related",
    "file",
    "deleted",
    "problem",
    "executor",
    "basically",
    "come",
    "back",
    "check",
    "specific",
    "error",
    "error",
    "tries",
    "delete",
    "temp",
    "directory",
    "permissions",
    "done",
    "per",
    "application",
    "creating",
    "output",
    "folder",
    "park",
    "let",
    "click",
    "see",
    "refresh",
    "right",
    "see",
    "new",
    "spark",
    "output",
    "created",
    "previous",
    "sessions",
    "seen",
    "set",
    "spark",
    "windows",
    "set",
    "spark",
    "related",
    "ide",
    "run",
    "applications",
    "windows",
    "quick",
    "demo",
    "setting",
    "spark",
    "standalone",
    "cluster",
    "ubuntu",
    "machines",
    "trying",
    "spark",
    "way",
    "working",
    "need",
    "least",
    "two",
    "machines",
    "ubuntu",
    "machines",
    "set",
    "um1",
    "um2",
    "give",
    "insights",
    "machines",
    "look",
    "look",
    "etc",
    "host",
    "ip",
    "address",
    "multiple",
    "machines",
    "ping",
    "one",
    "machine",
    "machine",
    "ping",
    "um2",
    "work",
    "fine",
    "similarly",
    "also",
    "set",
    "ssh",
    "access",
    "machines",
    "machines",
    "hdu",
    "user",
    "ssh",
    "um2",
    "locks",
    "second",
    "machine",
    "without",
    "password",
    "similarly",
    "check",
    "thing",
    "second",
    "machine",
    "ssh",
    "um",
    "one",
    "works",
    "fine",
    "need",
    "two",
    "machines",
    "ping",
    "firewall",
    "disabled",
    "able",
    "ssh",
    "set",
    "standalone",
    "cluster",
    "spark",
    "first",
    "thing",
    "download",
    "spark",
    "related",
    "tar",
    "file",
    "go",
    "google",
    "type",
    "spark",
    "download",
    "interested",
    "takes",
    "spark",
    "release",
    "however",
    "shows",
    "latest",
    "release",
    "go",
    "archives",
    "basically",
    "look",
    "existing",
    "versions",
    "click",
    "download",
    "shows",
    "spark",
    "release",
    "also",
    "apache",
    "hadoop",
    "also",
    "go",
    "release",
    "archives",
    "older",
    "stable",
    "version",
    "click",
    "spark",
    "click",
    "either",
    "install",
    "something",
    "already",
    "built",
    "download",
    "one",
    "spark",
    "bin",
    "hadoop",
    "point",
    "g",
    "z",
    "click",
    "link",
    "save",
    "file",
    "already",
    "done",
    "see",
    "machine",
    "home",
    "hdu",
    "downloads",
    "different",
    "packages",
    "downloaded",
    "also",
    "notice",
    "downloaded",
    "jdk8",
    "check",
    "machines",
    "apart",
    "machines",
    "able",
    "ping",
    "able",
    "ssh",
    "also",
    "need",
    "machines",
    "java",
    "already",
    "installed",
    "done",
    "spark",
    "related",
    "package",
    "go",
    "user",
    "local",
    "directory",
    "give",
    "command",
    "sudo",
    "star",
    "xvf",
    "home",
    "sdu",
    "downloads",
    "give",
    "spark",
    "package",
    "untar",
    "spark",
    "directory",
    "create",
    "directory",
    "user",
    "local",
    "location",
    "see",
    "directory",
    "created",
    "also",
    "see",
    "spark",
    "link",
    "pointing",
    "spark",
    "would",
    "want",
    "work",
    "newer",
    "version",
    "spark",
    "could",
    "thing",
    "newer",
    "version",
    "make",
    "link",
    "pointing",
    "newer",
    "version",
    "spark",
    "create",
    "link",
    "say",
    "sudo",
    "ln",
    "minus",
    "give",
    "spark",
    "directory",
    "create",
    "link",
    "already",
    "created",
    "link",
    "spark",
    "path",
    "become",
    "done",
    "created",
    "link",
    "go",
    "bash",
    "file",
    "user",
    "carefully",
    "see",
    "given",
    "java",
    "path",
    "execute",
    "java",
    "related",
    "commands",
    "also",
    "added",
    "spark",
    "related",
    "path",
    "says",
    "user",
    "local",
    "spark",
    "case",
    "would",
    "changing",
    "spark",
    "version",
    "latest",
    "one",
    "change",
    "things",
    "bash",
    "file",
    "thing",
    "unlink",
    "existing",
    "spark",
    "link",
    "create",
    "new",
    "link",
    "newer",
    "version",
    "done",
    "machine",
    "basically",
    "using",
    "spark",
    "go",
    "user",
    "local",
    "spark",
    "different",
    "directories",
    "look",
    "bin",
    "binaries",
    "programs",
    "like",
    "pi",
    "spark",
    "spark",
    "shell",
    "spark",
    "sql",
    "spark",
    "submit",
    "see",
    "use",
    "look",
    "sbin",
    "startup",
    "scripts",
    "start",
    "history",
    "server",
    "start",
    "master",
    "worker",
    "processes",
    "look",
    "conf",
    "config",
    "directories",
    "default",
    "might",
    "see",
    "spark",
    "minus",
    "renamed",
    "dot",
    "conf",
    "slaves",
    "template",
    "renamed",
    "slaves",
    "let",
    "look",
    "go",
    "conf",
    "look",
    "spark",
    "default",
    "conf",
    "based",
    "setup",
    "intend",
    "set",
    "spark",
    "standalone",
    "cluster",
    "without",
    "hadoop",
    "would",
    "want",
    "spark",
    "standalone",
    "distributed",
    "cluster",
    "uncommented",
    "property",
    "says",
    "say",
    "spark",
    "also",
    "mention",
    "master",
    "run",
    "machine",
    "um1",
    "spark",
    "event",
    "log",
    "dot",
    "enabled",
    "true",
    "would",
    "want",
    "track",
    "events",
    "mentioned",
    "directory",
    "local",
    "directory",
    "user",
    "local",
    "spark",
    "create",
    "talks",
    "default",
    "serializer",
    "talks",
    "driver",
    "memory",
    "default",
    "5",
    "gigabyte",
    "reduced",
    "2",
    "gigabyte",
    "gigabyte",
    "based",
    "machine",
    "configuration",
    "java",
    "options",
    "intend",
    "run",
    "history",
    "server",
    "whenever",
    "spark",
    "application",
    "complete",
    "application",
    "stored",
    "history",
    "server",
    "given",
    "log",
    "directory",
    "user",
    "local",
    "spark",
    "application",
    "history",
    "also",
    "needs",
    "created",
    "spark",
    "history",
    "provider",
    "class",
    "takes",
    "care",
    "history",
    "server",
    "update",
    "interval",
    "looking",
    "applications",
    "spark",
    "default",
    "look",
    "slaves",
    "given",
    "machines",
    "would",
    "want",
    "worker",
    "processes",
    "run",
    "um1",
    "um",
    "made",
    "changes",
    "spark",
    "default",
    "conf",
    "slaves",
    "file",
    "simply",
    "scp",
    "assuming",
    "whatever",
    "done",
    "machine",
    "untarring",
    "spark",
    "directory",
    "creating",
    "link",
    "updating",
    "bash",
    "rc",
    "renaming",
    "config",
    "files",
    "steps",
    "need",
    "done",
    "second",
    "machine",
    "second",
    "machine",
    "would",
    "also",
    "prepared",
    "used",
    "spark",
    "cluster",
    "machines",
    "ping",
    "ssh",
    "machines",
    "java",
    "machines",
    "spark",
    "version",
    "downloaded",
    "basic",
    "setup",
    "done",
    "easily",
    "copy",
    "spark",
    "related",
    "config",
    "machine",
    "sdu",
    "um2",
    "give",
    "path",
    "user",
    "local",
    "spark",
    "conf",
    "way",
    "need",
    "edit",
    "config",
    "files",
    "similarly",
    "even",
    "copy",
    "slaves",
    "file",
    "need",
    "basically",
    "spark",
    "standalone",
    "cluster",
    "since",
    "updated",
    "bash",
    "rc",
    "give",
    "spark",
    "command",
    "anywhere",
    "set",
    "config",
    "files",
    "start",
    "minus",
    "dot",
    "sh",
    "based",
    "config",
    "files",
    "based",
    "directories",
    "mentioned",
    "start",
    "master",
    "process",
    "worker",
    "process",
    "machines",
    "spark",
    "standalone",
    "cluster",
    "two",
    "workers",
    "one",
    "master",
    "always",
    "go",
    "browser",
    "interface",
    "check",
    "spark",
    "ui",
    "available",
    "typing",
    "http",
    "slash",
    "master",
    "port",
    "eight",
    "zero",
    "eight",
    "zero",
    "shows",
    "spark",
    "master",
    "running",
    "two",
    "worker",
    "processes",
    "right",
    "applications",
    "running",
    "spark",
    "ui",
    "already",
    "available",
    "start",
    "using",
    "spark",
    "either",
    "spark",
    "shell",
    "pi",
    "spark",
    "even",
    "spark",
    "submit",
    "see",
    "applications",
    "getting",
    "populated",
    "additionally",
    "also",
    "start",
    "start",
    "history",
    "server",
    "saying",
    "start",
    "history",
    "server",
    "dot",
    "sh",
    "start",
    "history",
    "server",
    "done",
    "history",
    "server",
    "also",
    "running",
    "go",
    "back",
    "pull",
    "history",
    "server",
    "user",
    "interface",
    "giving",
    "port",
    "default",
    "18080",
    "spark",
    "standalone",
    "cluster",
    "one",
    "master",
    "two",
    "workers",
    "also",
    "history",
    "server",
    "running",
    "try",
    "working",
    "spark",
    "either",
    "spark",
    "shell",
    "pi",
    "spark",
    "packaged",
    "application",
    "jar",
    "using",
    "sbt",
    "mevin",
    "could",
    "also",
    "use",
    "spark",
    "submit",
    "additionally",
    "could",
    "also",
    "use",
    "spark",
    "sql",
    "work",
    "spark",
    "would",
    "say",
    "spark",
    "shell",
    "one",
    "machines",
    "machines",
    "worker",
    "processes",
    "running",
    "start",
    "spark",
    "shell",
    "machines",
    "remember",
    "spark",
    "rely",
    "storage",
    "machines",
    "file",
    "system",
    "hadoop",
    "distributed",
    "file",
    "system",
    "started",
    "spark",
    "shell",
    "shows",
    "spark",
    "context",
    "available",
    "sc",
    "spark",
    "session",
    "available",
    "spark",
    "work",
    "spark",
    "core",
    "rdds",
    "would",
    "using",
    "spark",
    "context",
    "intend",
    "work",
    "spark",
    "sql",
    "data",
    "frames",
    "data",
    "sets",
    "using",
    "spark",
    "let",
    "try",
    "quick",
    "simple",
    "example",
    "say",
    "val",
    "x",
    "sc",
    "dot",
    "text",
    "file",
    "would",
    "want",
    "point",
    "file",
    "say",
    "complete",
    "path",
    "remember",
    "giving",
    "spark",
    "command",
    "home",
    "directory",
    "say",
    "user",
    "local",
    "spark",
    "point",
    "file",
    "existing",
    "would",
    "want",
    "use",
    "processing",
    "using",
    "first",
    "step",
    "wherein",
    "using",
    "method",
    "spark",
    "context",
    "relate",
    "evaluation",
    "create",
    "rdt",
    "using",
    "scala",
    "way",
    "let",
    "create",
    "one",
    "variable",
    "transformation",
    "saying",
    "upper",
    "case",
    "would",
    "want",
    "convert",
    "content",
    "upper",
    "case",
    "transformation",
    "create",
    "rdd",
    "done",
    "invoke",
    "action",
    "basically",
    "see",
    "results",
    "could",
    "simply",
    "collect",
    "action",
    "action",
    "trigger",
    "execution",
    "dag",
    "starts",
    "farthest",
    "rtd",
    "first",
    "step",
    "file",
    "loaded",
    "rdd",
    "able",
    "see",
    "result",
    "basically",
    "execution",
    "dag",
    "time",
    "used",
    "text",
    "file",
    "method",
    "spark",
    "context",
    "2",
    "transformation",
    "invoked",
    "action",
    "becomes",
    "one",
    "job",
    "would",
    "dot",
    "count",
    "different",
    "action",
    "trigger",
    "execution",
    "dag",
    "right",
    "beginning",
    "file",
    "data",
    "loaded",
    "rdd",
    "map",
    "transformation",
    "happen",
    "count",
    "always",
    "see",
    "spark",
    "ui",
    "going",
    "refresh",
    "show",
    "application",
    "running",
    "via",
    "spark",
    "shell",
    "utilized",
    "three",
    "cores",
    "utilized",
    "memory",
    "per",
    "executor",
    "one",
    "gigabyte",
    "click",
    "application",
    "shows",
    "used",
    "workers",
    "one",
    "core",
    "one",
    "gb",
    "worker",
    "two",
    "cores",
    "memory",
    "worker",
    "click",
    "application",
    "detail",
    "ui",
    "wherein",
    "see",
    "actions",
    "invoked",
    "also",
    "see",
    "number",
    "tasks",
    "run",
    "every",
    "rdd",
    "created",
    "default",
    "two",
    "partitions",
    "partition",
    "one",
    "task",
    "well",
    "change",
    "partitions",
    "many",
    "things",
    "done",
    "learn",
    "later",
    "sessions",
    "two",
    "jobs",
    "one",
    "ended",
    "collect",
    "one",
    "ended",
    "count",
    "click",
    "collect",
    "see",
    "dag",
    "visualization",
    "tells",
    "started",
    "text",
    "file",
    "map",
    "finally",
    "collect",
    "shows",
    "everything",
    "done",
    "state",
    "0",
    "one",
    "stage",
    "click",
    "count",
    "shows",
    "dag",
    "visualization",
    "different",
    "stage",
    "different",
    "job",
    "id",
    "started",
    "text",
    "file",
    "map",
    "account",
    "would",
    "want",
    "see",
    "details",
    "always",
    "use",
    "tabs",
    "look",
    "different",
    "stages",
    "within",
    "application",
    "caching",
    "looking",
    "environment",
    "variables",
    "also",
    "see",
    "many",
    "executors",
    "used",
    "including",
    "one",
    "driver",
    "nodes",
    "executor",
    "ran",
    "many",
    "cores",
    "many",
    "tasks",
    "run",
    "shuffling",
    "involved",
    "used",
    "spark",
    "shell",
    "interactive",
    "way",
    "running",
    "application",
    "since",
    "spark",
    "ui",
    "always",
    "look",
    "applications",
    "run",
    "always",
    "drill",
    "know",
    "details",
    "since",
    "application",
    "completed",
    "refresh",
    "history",
    "server",
    "show",
    "anything",
    "started",
    "spark",
    "shell",
    "started",
    "application",
    "application",
    "still",
    "running",
    "status",
    "see",
    "running",
    "applications",
    "use",
    "spark",
    "shell",
    "standalone",
    "spark",
    "cluster",
    "colon",
    "quit",
    "takes",
    "spark",
    "shell",
    "similarly",
    "would",
    "want",
    "work",
    "pi",
    "spark",
    "python",
    "way",
    "working",
    "spark",
    "type",
    "pi",
    "spark",
    "bring",
    "python",
    "shell",
    "continue",
    "working",
    "spark",
    "go",
    "back",
    "look",
    "ui",
    "pi",
    "spark",
    "comes",
    "go",
    "since",
    "quit",
    "spark",
    "shell",
    "refresh",
    "see",
    "application",
    "yet",
    "coming",
    "history",
    "server",
    "might",
    "take",
    "time",
    "always",
    "go",
    "back",
    "look",
    "incomplete",
    "applications",
    "wait",
    "till",
    "application",
    "populated",
    "history",
    "server",
    "refresh",
    "spark",
    "ui",
    "says",
    "application",
    "completed",
    "says",
    "finished",
    "started",
    "pi",
    "spark",
    "started",
    "new",
    "application",
    "pi",
    "spark",
    "shell",
    "running",
    "status",
    "come",
    "back",
    "python",
    "way",
    "thing",
    "scala",
    "using",
    "sc",
    "dot",
    "text",
    "file",
    "point",
    "file",
    "exists",
    "user",
    "local",
    "spark",
    "read",
    "dot",
    "md",
    "file",
    "interested",
    "would",
    "created",
    "rdd",
    "confirm",
    "typing",
    "x",
    "create",
    "different",
    "variable",
    "say",
    "would",
    "want",
    "transformation",
    "map",
    "case",
    "python",
    "use",
    "anonymous",
    "functions",
    "want",
    "apply",
    "functions",
    "transformations",
    "use",
    "lambda",
    "functions",
    "say",
    "lambda",
    "line",
    "would",
    "say",
    "want",
    "line",
    "say",
    "use",
    "inbuilt",
    "uppercase",
    "upper",
    "function",
    "convert",
    "content",
    "uppercase",
    "transformation",
    "creates",
    "rtt",
    "see",
    "result",
    "dot",
    "collect",
    "bring",
    "result",
    "case",
    "says",
    "file",
    "exist",
    "typo",
    "repeat",
    "step",
    "let",
    "bring",
    "x",
    "give",
    "right",
    "name",
    "file",
    "transformation",
    "finally",
    "see",
    "result",
    "using",
    "pi",
    "spark",
    "standalone",
    "spark",
    "cluster",
    "working",
    "without",
    "hadoop",
    "two",
    "worker",
    "nodes",
    "one",
    "master",
    "node",
    "always",
    "come",
    "refresh",
    "look",
    "details",
    "application",
    "click",
    "similarly",
    "like",
    "scala",
    "application",
    "detail",
    "ui",
    "see",
    "application",
    "ended",
    "collect",
    "ran",
    "tasks",
    "look",
    "tag",
    "use",
    "spark",
    "shell",
    "pi",
    "spark",
    "thank",
    "ajay",
    "rahul",
    "tell",
    "us",
    "become",
    "big",
    "data",
    "engineer",
    "let",
    "find",
    "big",
    "data",
    "engineer",
    "big",
    "data",
    "engineer",
    "professional",
    "develops",
    "maintains",
    "tests",
    "evaluates",
    "company",
    "big",
    "data",
    "infrastructure",
    "words",
    "develop",
    "big",
    "data",
    "solutions",
    "based",
    "company",
    "requirements",
    "maintain",
    "solutions",
    "test",
    "solutions",
    "company",
    "requirements",
    "integrate",
    "solution",
    "various",
    "tools",
    "systems",
    "organization",
    "finally",
    "evaluate",
    "well",
    "solution",
    "working",
    "fulfill",
    "company",
    "requirements",
    "next",
    "let",
    "look",
    "responsibilities",
    "big",
    "data",
    "engineer",
    "need",
    "able",
    "design",
    "implement",
    "verify",
    "maintain",
    "software",
    "systems",
    "process",
    "ingesting",
    "data",
    "well",
    "processing",
    "need",
    "able",
    "build",
    "highly",
    "scalable",
    "well",
    "robust",
    "systems",
    "need",
    "able",
    "extract",
    "data",
    "one",
    "database",
    "transform",
    "well",
    "load",
    "another",
    "data",
    "store",
    "process",
    "etl",
    "extract",
    "transform",
    "load",
    "process",
    "need",
    "research",
    "well",
    "propose",
    "new",
    "ways",
    "acquire",
    "data",
    "improve",
    "overall",
    "data",
    "quality",
    "efficiency",
    "system",
    "ensure",
    "business",
    "requirements",
    "met",
    "need",
    "build",
    "suitable",
    "data",
    "architecture",
    "need",
    "able",
    "integrate",
    "several",
    "programming",
    "languages",
    "tools",
    "together",
    "generate",
    "structured",
    "solution",
    "need",
    "build",
    "models",
    "reduce",
    "overall",
    "complexity",
    "increase",
    "efficiency",
    "whole",
    "system",
    "mining",
    "data",
    "various",
    "sources",
    "finally",
    "need",
    "work",
    "well",
    "teams",
    "ones",
    "include",
    "data",
    "architects",
    "data",
    "analysts",
    "data",
    "scientists",
    "next",
    "let",
    "look",
    "skills",
    "required",
    "become",
    "big",
    "data",
    "engineer",
    "first",
    "step",
    "programming",
    "knowledge",
    "one",
    "important",
    "skills",
    "required",
    "become",
    "big",
    "data",
    "engineer",
    "experience",
    "programming",
    "languages",
    "especially",
    "experience",
    "big",
    "data",
    "solutions",
    "organizations",
    "would",
    "want",
    "create",
    "possible",
    "without",
    "experience",
    "programming",
    "languages",
    "even",
    "tell",
    "easy",
    "way",
    "get",
    "experience",
    "programming",
    "languages",
    "practice",
    "practice",
    "practice",
    "commonly",
    "used",
    "programming",
    "languages",
    "used",
    "big",
    "data",
    "engineering",
    "python",
    "java",
    "c",
    "plus",
    "plus",
    "second",
    "skill",
    "require",
    "knowledge",
    "dbms",
    "sql",
    "need",
    "know",
    "data",
    "maintained",
    "well",
    "managed",
    "database",
    "need",
    "know",
    "sql",
    "used",
    "transform",
    "well",
    "perform",
    "actions",
    "database",
    "extension",
    "know",
    "write",
    "sql",
    "queries",
    "relational",
    "database",
    "management",
    "systems",
    "commonly",
    "used",
    "database",
    "management",
    "systems",
    "big",
    "data",
    "engineering",
    "mysql",
    "rackle",
    "database",
    "microsoft",
    "sql",
    "server",
    "third",
    "skill",
    "require",
    "experience",
    "working",
    "etl",
    "warehousing",
    "tools",
    "need",
    "know",
    "construct",
    "well",
    "use",
    "data",
    "warehouse",
    "perform",
    "etl",
    "operation",
    "extract",
    "transform",
    "load",
    "operations",
    "big",
    "data",
    "engineer",
    "constantly",
    "tasked",
    "extracting",
    "unstructured",
    "data",
    "number",
    "different",
    "sources",
    "transforming",
    "meaningful",
    "information",
    "loading",
    "data",
    "storages",
    "databases",
    "data",
    "warehouses",
    "basically",
    "aggregating",
    "unstructured",
    "data",
    "multiple",
    "sources",
    "analyzing",
    "take",
    "better",
    "business",
    "decisions",
    "tools",
    "used",
    "purpose",
    "talent",
    "ibm",
    "data",
    "stage",
    "pentaho",
    "informatica",
    "next",
    "fourth",
    "skill",
    "require",
    "knowledge",
    "operating",
    "systems",
    "since",
    "big",
    "data",
    "tools",
    "unique",
    "demands",
    "root",
    "access",
    "operating",
    "system",
    "functionality",
    "well",
    "hardware",
    "strong",
    "understanding",
    "operating",
    "systems",
    "like",
    "linux",
    "unix",
    "absolutely",
    "mandatory",
    "operating",
    "systems",
    "used",
    "big",
    "data",
    "engineers",
    "unix",
    "linux",
    "solaris",
    "fifth",
    "skill",
    "require",
    "experience",
    "hadoop",
    "based",
    "analytics",
    "since",
    "hadoop",
    "one",
    "commonly",
    "used",
    "tools",
    "comes",
    "big",
    "data",
    "engineering",
    "understood",
    "need",
    "experience",
    "apache",
    "hadoop",
    "based",
    "technologies",
    "technologies",
    "like",
    "hdfs",
    "hadoop",
    "mapreduce",
    "apache",
    "ah",
    "base",
    "hive",
    "pig",
    "sixth",
    "skill",
    "require",
    "worked",
    "processing",
    "frameworks",
    "like",
    "apache",
    "spark",
    "big",
    "data",
    "engineer",
    "deal",
    "vast",
    "volumes",
    "data",
    "data",
    "need",
    "analytics",
    "engine",
    "like",
    "spark",
    "used",
    "data",
    "processing",
    "spark",
    "process",
    "live",
    "streaming",
    "data",
    "number",
    "different",
    "sources",
    "like",
    "facebook",
    "instagram",
    "twitter",
    "also",
    "perform",
    "interactive",
    "analysis",
    "data",
    "integration",
    "final",
    "skill",
    "requirement",
    "experience",
    "data",
    "mining",
    "modeling",
    "data",
    "engineer",
    "examine",
    "massive",
    "data",
    "discover",
    "patterns",
    "well",
    "new",
    "information",
    "create",
    "predictive",
    "models",
    "business",
    "make",
    "better",
    "informed",
    "decisions",
    "tools",
    "used",
    "r",
    "rapidminer",
    "weka",
    "nime",
    "let",
    "talk",
    "big",
    "data",
    "engineer",
    "salary",
    "well",
    "roles",
    "look",
    "forward",
    "average",
    "salary",
    "big",
    "data",
    "engineer",
    "united",
    "states",
    "approximately",
    "ninety",
    "thousand",
    "dollars",
    "per",
    "year",
    "ranges",
    "sixty",
    "six",
    "thousand",
    "dollars",
    "way",
    "hundred",
    "thirty",
    "thousand",
    "dollars",
    "per",
    "annum",
    "india",
    "average",
    "salary",
    "around",
    "7",
    "lakh",
    "rupees",
    "ranges",
    "4",
    "lakhs",
    "13",
    "lakhs",
    "per",
    "annum",
    "become",
    "big",
    "data",
    "engineer",
    "job",
    "roles",
    "look",
    "forward",
    "senior",
    "big",
    "data",
    "engineer",
    "business",
    "intelligence",
    "architect",
    "data",
    "architect",
    "let",
    "talk",
    "certifications",
    "big",
    "data",
    "engineer",
    "opt",
    "first",
    "cloudera",
    "ccp",
    "data",
    "engineer",
    "cloudera",
    "certified",
    "professional",
    "data",
    "engineer",
    "possesses",
    "skills",
    "develop",
    "reliable",
    "scalable",
    "data",
    "pipelines",
    "result",
    "optimized",
    "data",
    "sets",
    "variety",
    "workloads",
    "one",
    "industry",
    "demanding",
    "certification",
    "ccp",
    "evaluates",
    "recognizes",
    "candidate",
    "mastery",
    "technical",
    "skills",
    "sought",
    "employers",
    "time",
    "limit",
    "exam",
    "240",
    "minutes",
    "costs",
    "400",
    "next",
    "ibm",
    "certified",
    "data",
    "architect",
    "big",
    "data",
    "certification",
    "ibm",
    "certified",
    "big",
    "data",
    "architect",
    "understands",
    "complexity",
    "data",
    "design",
    "systems",
    "models",
    "handle",
    "different",
    "data",
    "variety",
    "including",
    "structured",
    "unstructured",
    "volume",
    "velocity",
    "veracity",
    "big",
    "data",
    "architect",
    "also",
    "able",
    "effectively",
    "address",
    "information",
    "governance",
    "security",
    "challenges",
    "associated",
    "system",
    "exam",
    "75",
    "minutes",
    "long",
    "finally",
    "google",
    "cloud",
    "certified",
    "data",
    "engineer",
    "google",
    "certified",
    "data",
    "engineer",
    "enables",
    "data",
    "driven",
    "decision",
    "making",
    "collecting",
    "transforming",
    "publishing",
    "data",
    "also",
    "able",
    "leverage",
    "deploy",
    "continuously",
    "train",
    "machine",
    "learning",
    "models",
    "length",
    "certification",
    "exam",
    "two",
    "hours",
    "registration",
    "fee",
    "200",
    "let",
    "look",
    "simply",
    "learn",
    "help",
    "become",
    "big",
    "data",
    "engineer",
    "simply",
    "learn",
    "provides",
    "big",
    "data",
    "architect",
    "masters",
    "program",
    "includes",
    "number",
    "different",
    "courses",
    "like",
    "big",
    "data",
    "hadoop",
    "spark",
    "developer",
    "apache",
    "spark",
    "scala",
    "mongodb",
    "developer",
    "administrator",
    "big",
    "data",
    "hadoop",
    "administrator",
    "much",
    "course",
    "goes",
    "50",
    "plus",
    "demand",
    "skills",
    "tools",
    "12",
    "plus",
    "real",
    "life",
    "projects",
    "possibility",
    "annual",
    "average",
    "salary",
    "19",
    "26",
    "lakh",
    "rupees",
    "per",
    "annum",
    "also",
    "help",
    "get",
    "noticed",
    "top",
    "hiring",
    "companies",
    "course",
    "also",
    "go",
    "major",
    "tools",
    "like",
    "kafka",
    "apache",
    "spark",
    "flume",
    "edge",
    "base",
    "mongodb",
    "hive",
    "pig",
    "mapreduce",
    "java",
    "scala",
    "much",
    "head",
    "get",
    "started",
    "journey",
    "get",
    "certified",
    "get",
    "thank",
    "rahul",
    "look",
    "hadoop",
    "interview",
    "questions",
    "answers",
    "instructor",
    "ajay",
    "guide",
    "us",
    "one",
    "single",
    "machine",
    "huge",
    "amount",
    "disks",
    "huge",
    "amount",
    "ram",
    "time",
    "taken",
    "read",
    "data",
    "data",
    "stored",
    "one",
    "machine",
    "would",
    "high",
    "would",
    "least",
    "fault",
    "tolerance",
    "talk",
    "sdfs",
    "data",
    "distributed",
    "sdfs",
    "stands",
    "hadoop",
    "distributed",
    "file",
    "system",
    "data",
    "distributed",
    "maintained",
    "multiple",
    "systems",
    "never",
    "one",
    "single",
    "machine",
    "also",
    "supporting",
    "reliability",
    "whatever",
    "stored",
    "hdfs",
    "say",
    "file",
    "stored",
    "depending",
    "size",
    "split",
    "blocks",
    "blocks",
    "spread",
    "across",
    "multiple",
    "nodes",
    "every",
    "block",
    "stored",
    "node",
    "replicas",
    "stored",
    "nodes",
    "replication",
    "factor",
    "depends",
    "makes",
    "sdfs",
    "reliable",
    "cases",
    "slave",
    "nodes",
    "data",
    "nodes",
    "crashing",
    "rarely",
    "data",
    "loss",
    "auto",
    "replication",
    "feature",
    "time",
    "taken",
    "read",
    "data",
    "comparatively",
    "might",
    "situations",
    "data",
    "distributed",
    "across",
    "nodes",
    "even",
    "parallel",
    "read",
    "data",
    "read",
    "might",
    "take",
    "time",
    "needs",
    "coordination",
    "multiple",
    "machines",
    "however",
    "working",
    "huge",
    "data",
    "getting",
    "stored",
    "still",
    "beneficial",
    "comparison",
    "reading",
    "single",
    "machine",
    "always",
    "think",
    "reliability",
    "auto",
    "replication",
    "feature",
    "fault",
    "tolerance",
    "data",
    "getting",
    "stored",
    "across",
    "multiple",
    "machines",
    "capability",
    "scale",
    "talk",
    "sdfs",
    "talking",
    "horizontal",
    "scalability",
    "scaling",
    "talk",
    "regular",
    "file",
    "system",
    "talking",
    "vertical",
    "scalability",
    "scaling",
    "let",
    "look",
    "specific",
    "sdfs",
    "questions",
    "sdfs",
    "fault",
    "tolerant",
    "explained",
    "previous",
    "slides",
    "sdfs",
    "fault",
    "tolerant",
    "replicates",
    "data",
    "different",
    "data",
    "nodes",
    "master",
    "node",
    "multiple",
    "slave",
    "nodes",
    "data",
    "nodes",
    "actually",
    "data",
    "getting",
    "stored",
    "also",
    "default",
    "block",
    "size",
    "128",
    "mb",
    "minimum",
    "since",
    "hadoop",
    "version",
    "2",
    "file",
    "128",
    "mb",
    "would",
    "using",
    "one",
    "logical",
    "block",
    "file",
    "size",
    "bigger",
    "128",
    "mb",
    "split",
    "blocks",
    "blocks",
    "stored",
    "across",
    "multiple",
    "machines",
    "since",
    "blocks",
    "stored",
    "across",
    "multiple",
    "machines",
    "makes",
    "fault",
    "tolerant",
    "even",
    "machines",
    "fail",
    "would",
    "still",
    "copy",
    "block",
    "existing",
    "machine",
    "two",
    "aspects",
    "one",
    "talk",
    "first",
    "rule",
    "replication",
    "basically",
    "means",
    "never",
    "two",
    "identical",
    "blocks",
    "sitting",
    "machine",
    "second",
    "rule",
    "replication",
    "terms",
    "rack",
    "awareness",
    "machines",
    "placed",
    "racks",
    "see",
    "right",
    "image",
    "never",
    "replicas",
    "placed",
    "rack",
    "even",
    "different",
    "machines",
    "fault",
    "tolerant",
    "maintain",
    "redundancy",
    "least",
    "one",
    "replica",
    "placed",
    "node",
    "rack",
    "sdfs",
    "fault",
    "tolerant",
    "let",
    "understand",
    "architecture",
    "sdfs",
    "mentioned",
    "earlier",
    "would",
    "hadoop",
    "cluster",
    "main",
    "service",
    "hdfs",
    "sdfs",
    "service",
    "would",
    "name",
    "node",
    "master",
    "process",
    "running",
    "one",
    "machines",
    "would",
    "data",
    "nodes",
    "slave",
    "machines",
    "getting",
    "stored",
    "across",
    "getting",
    "process",
    "processes",
    "running",
    "across",
    "multiple",
    "machines",
    "one",
    "processes",
    "important",
    "role",
    "play",
    "talk",
    "sdfs",
    "whatever",
    "data",
    "written",
    "hdfs",
    "data",
    "split",
    "blocks",
    "depending",
    "size",
    "blocks",
    "randomly",
    "distributed",
    "across",
    "nodes",
    "auto",
    "replication",
    "feature",
    "blocks",
    "also",
    "auto",
    "replicated",
    "across",
    "multiple",
    "machines",
    "first",
    "condition",
    "two",
    "identical",
    "blocks",
    "sit",
    "machine",
    "soon",
    "cluster",
    "comes",
    "data",
    "nodes",
    "part",
    "cluster",
    "based",
    "config",
    "files",
    "would",
    "start",
    "sending",
    "heartbeat",
    "name",
    "node",
    "would",
    "every",
    "three",
    "seconds",
    "name",
    "note",
    "name",
    "node",
    "store",
    "information",
    "ram",
    "name",
    "node",
    "starts",
    "building",
    "metadata",
    "ram",
    "metadata",
    "information",
    "data",
    "nodes",
    "available",
    "beginning",
    "data",
    "writing",
    "activity",
    "starts",
    "blocks",
    "distributed",
    "across",
    "data",
    "nodes",
    "data",
    "nodes",
    "every",
    "10",
    "seconds",
    "also",
    "send",
    "block",
    "report",
    "name",
    "node",
    "name",
    "node",
    "adding",
    "information",
    "ram",
    "metadata",
    "ram",
    "earlier",
    "data",
    "node",
    "information",
    "name",
    "node",
    "also",
    "information",
    "files",
    "files",
    "split",
    "blocks",
    "blocks",
    "stored",
    "machines",
    "file",
    "permissions",
    "name",
    "node",
    "maintaining",
    "metadata",
    "ram",
    "name",
    "node",
    "also",
    "maintaining",
    "metadata",
    "disk",
    "see",
    "red",
    "box",
    "basically",
    "information",
    "whatever",
    "information",
    "written",
    "hdfs",
    "summarize",
    "name",
    "node",
    "metadata",
    "ram",
    "metadata",
    "disk",
    "data",
    "nodes",
    "machines",
    "blocks",
    "data",
    "actually",
    "getting",
    "stored",
    "auto",
    "replication",
    "feature",
    "always",
    "existing",
    "unless",
    "disabled",
    "read",
    "write",
    "activity",
    "parallel",
    "activity",
    "however",
    "replication",
    "sequential",
    "activity",
    "mentioned",
    "talk",
    "name",
    "node",
    "master",
    "process",
    "hosting",
    "metadata",
    "disk",
    "ram",
    "talk",
    "disk",
    "basically",
    "edit",
    "log",
    "transaction",
    "log",
    "fs",
    "image",
    "file",
    "system",
    "image",
    "right",
    "time",
    "cluster",
    "started",
    "metadata",
    "disk",
    "existing",
    "gets",
    "appended",
    "every",
    "time",
    "read",
    "write",
    "operations",
    "happen",
    "hdfs",
    "metadata",
    "ram",
    "dynamically",
    "built",
    "every",
    "time",
    "cluster",
    "comes",
    "basically",
    "means",
    "cluster",
    "coming",
    "name",
    "node",
    "initial",
    "seconds",
    "minutes",
    "would",
    "safe",
    "mode",
    "basically",
    "means",
    "busy",
    "registering",
    "information",
    "data",
    "nodes",
    "name",
    "node",
    "one",
    "critical",
    "processes",
    "name",
    "node",
    "processes",
    "running",
    "able",
    "access",
    "cluster",
    "name",
    "node",
    "metadata",
    "disk",
    "important",
    "name",
    "node",
    "come",
    "maintain",
    "cluster",
    "name",
    "nodes",
    "metadata",
    "ram",
    "basically",
    "satisfying",
    "client",
    "requests",
    "look",
    "data",
    "nodes",
    "mentioned",
    "data",
    "nodes",
    "hold",
    "actual",
    "data",
    "blocks",
    "sending",
    "block",
    "reports",
    "every",
    "10",
    "seconds",
    "metadata",
    "name",
    "notes",
    "ram",
    "constantly",
    "getting",
    "updated",
    "metadata",
    "disk",
    "also",
    "constantly",
    "getting",
    "updated",
    "based",
    "kind",
    "write",
    "activity",
    "happening",
    "cluster",
    "data",
    "node",
    "storing",
    "block",
    "also",
    "help",
    "kind",
    "read",
    "activity",
    "whenever",
    "client",
    "requests",
    "whenever",
    "client",
    "application",
    "api",
    "would",
    "want",
    "read",
    "data",
    "would",
    "first",
    "talk",
    "name",
    "node",
    "name",
    "node",
    "would",
    "look",
    "metadata",
    "ram",
    "confirm",
    "client",
    "machines",
    "could",
    "reached",
    "get",
    "data",
    "client",
    "would",
    "try",
    "read",
    "data",
    "sdfs",
    "actually",
    "getting",
    "data",
    "data",
    "nodes",
    "read",
    "write",
    "requests",
    "satisfied",
    "two",
    "types",
    "metadata",
    "name",
    "node",
    "server",
    "holds",
    "mentioned",
    "earlier",
    "metadata",
    "disk",
    "important",
    "remember",
    "edit",
    "log",
    "nfs",
    "image",
    "metadata",
    "ram",
    "information",
    "data",
    "nodes",
    "files",
    "files",
    "split",
    "blocks",
    "blocks",
    "residing",
    "data",
    "nodes",
    "file",
    "permissions",
    "share",
    "good",
    "link",
    "always",
    "look",
    "detailed",
    "information",
    "metadata",
    "search",
    "sdfs",
    "metadata",
    "directories",
    "explained",
    "hortonworks",
    "however",
    "talks",
    "metadata",
    "disk",
    "name",
    "node",
    "manages",
    "details",
    "look",
    "link",
    "interested",
    "learning",
    "metadata",
    "disk",
    "coming",
    "back",
    "let",
    "look",
    "next",
    "question",
    "difference",
    "federation",
    "high",
    "availability",
    "features",
    "introduced",
    "hadoop",
    "version",
    "features",
    "horizontal",
    "scalability",
    "name",
    "node",
    "prior",
    "version",
    "2",
    "possibility",
    "could",
    "one",
    "single",
    "master",
    "basically",
    "means",
    "cluster",
    "could",
    "become",
    "unavailable",
    "name",
    "node",
    "would",
    "crash",
    "hadoop",
    "version",
    "2",
    "introduced",
    "two",
    "new",
    "features",
    "federation",
    "high",
    "availability",
    "however",
    "high",
    "availability",
    "popular",
    "one",
    "talk",
    "federation",
    "basically",
    "means",
    "number",
    "name",
    "nodes",
    "limitation",
    "number",
    "name",
    "nodes",
    "name",
    "nodes",
    "federated",
    "cluster",
    "basically",
    "means",
    "name",
    "nodes",
    "still",
    "belong",
    "cluster",
    "coordinating",
    "whenever",
    "write",
    "request",
    "comes",
    "one",
    "name",
    "node",
    "picks",
    "request",
    "guides",
    "request",
    "blocks",
    "written",
    "data",
    "nodes",
    "name",
    "node",
    "coordinate",
    "name",
    "node",
    "find",
    "block",
    "id",
    "assigned",
    "one",
    "assigned",
    "name",
    "node",
    "belong",
    "federated",
    "cluster",
    "linked",
    "via",
    "cluster",
    "id",
    "whenever",
    "application",
    "api",
    "trying",
    "talk",
    "cluster",
    "always",
    "going",
    "via",
    "cluster",
    "id",
    "one",
    "name",
    "node",
    "would",
    "pick",
    "read",
    "activity",
    "write",
    "activity",
    "processing",
    "activity",
    "name",
    "nodes",
    "sharing",
    "pool",
    "metadata",
    "name",
    "node",
    "dedicated",
    "pool",
    "remember",
    "term",
    "called",
    "namespace",
    "name",
    "service",
    "also",
    "provides",
    "high",
    "fault",
    "tolerance",
    "supposed",
    "one",
    "name",
    "node",
    "goes",
    "affect",
    "make",
    "cluster",
    "unavailable",
    "still",
    "cluster",
    "reachable",
    "name",
    "nodes",
    "running",
    "available",
    "comes",
    "heartbeats",
    "data",
    "nodes",
    "sending",
    "heart",
    "beats",
    "name",
    "nodes",
    "name",
    "nodes",
    "aware",
    "data",
    "nodes",
    "talk",
    "high",
    "availability",
    "would",
    "two",
    "name",
    "nodes",
    "would",
    "active",
    "would",
    "standby",
    "normally",
    "environment",
    "would",
    "see",
    "high",
    "availability",
    "setup",
    "zookeeper",
    "zookeeper",
    "centralized",
    "coordination",
    "service",
    "talk",
    "active",
    "standby",
    "name",
    "notes",
    "election",
    "name",
    "node",
    "made",
    "active",
    "taking",
    "care",
    "automatic",
    "failover",
    "done",
    "zookeeper",
    "high",
    "availability",
    "set",
    "without",
    "zookeeper",
    "would",
    "mean",
    "admins",
    "intervention",
    "would",
    "required",
    "make",
    "name",
    "node",
    "active",
    "standby",
    "also",
    "take",
    "care",
    "failover",
    "point",
    "time",
    "high",
    "availability",
    "active",
    "name",
    "node",
    "would",
    "taking",
    "care",
    "storing",
    "edits",
    "whatever",
    "updates",
    "happening",
    "sdfs",
    "also",
    "writing",
    "edits",
    "shared",
    "location",
    "standby",
    "name",
    "node",
    "one",
    "constantly",
    "looking",
    "latest",
    "updates",
    "applying",
    "metadata",
    "actually",
    "copy",
    "whatever",
    "active",
    "name",
    "node",
    "way",
    "standby",
    "name",
    "node",
    "always",
    "sync",
    "active",
    "name",
    "node",
    "reason",
    "active",
    "name",
    "node",
    "fails",
    "standby",
    "name",
    "node",
    "take",
    "become",
    "active",
    "remember",
    "zookeeper",
    "plays",
    "important",
    "role",
    "centralized",
    "coordination",
    "service",
    "one",
    "thing",
    "remember",
    "high",
    "availability",
    "secondary",
    "name",
    "node",
    "allowed",
    "would",
    "active",
    "name",
    "node",
    "standby",
    "name",
    "node",
    "configured",
    "separate",
    "machine",
    "access",
    "shared",
    "location",
    "shared",
    "location",
    "could",
    "nfs",
    "could",
    "quorum",
    "general",
    "nodes",
    "information",
    "refer",
    "tutorial",
    "explained",
    "sdfs",
    "high",
    "availability",
    "federation",
    "let",
    "look",
    "logical",
    "question",
    "input",
    "file",
    "350",
    "mb",
    "obviously",
    "bigger",
    "128",
    "mb",
    "many",
    "input",
    "splits",
    "would",
    "created",
    "sdfs",
    "would",
    "size",
    "input",
    "split",
    "need",
    "remember",
    "default",
    "minimum",
    "block",
    "size",
    "128",
    "mb",
    "customizable",
    "environment",
    "number",
    "larger",
    "files",
    "written",
    "average",
    "obviously",
    "go",
    "bigger",
    "block",
    "size",
    "environment",
    "lot",
    "files",
    "written",
    "files",
    "smaller",
    "size",
    "could",
    "okay",
    "128",
    "mb",
    "remember",
    "hadoop",
    "every",
    "entity",
    "directory",
    "sdfs",
    "file",
    "sdfs",
    "file",
    "multiple",
    "blocks",
    "considered",
    "objects",
    "object",
    "hadoop",
    "name",
    "nodes",
    "ram",
    "150",
    "bytes",
    "utilized",
    "block",
    "size",
    "small",
    "would",
    "number",
    "blocks",
    "would",
    "directly",
    "affect",
    "name",
    "node",
    "ram",
    "keep",
    "block",
    "size",
    "high",
    "reduce",
    "number",
    "blocks",
    "remember",
    "might",
    "affect",
    "processing",
    "processing",
    "also",
    "depends",
    "splits",
    "number",
    "splits",
    "parallel",
    "processing",
    "setting",
    "block",
    "size",
    "done",
    "consideration",
    "parallelism",
    "requirement",
    "name",
    "nodes",
    "ram",
    "available",
    "coming",
    "question",
    "file",
    "350",
    "mb",
    "would",
    "split",
    "three",
    "blocks",
    "two",
    "blocks",
    "would",
    "128",
    "mb",
    "data",
    "third",
    "block",
    "although",
    "block",
    "size",
    "would",
    "still",
    "128",
    "would",
    "94",
    "mb",
    "data",
    "would",
    "split",
    "particular",
    "file",
    "let",
    "understand",
    "rack",
    "awareness",
    "rack",
    "awareness",
    "work",
    "even",
    "racks",
    "organizations",
    "always",
    "would",
    "want",
    "place",
    "nodes",
    "machines",
    "systematic",
    "way",
    "different",
    "approaches",
    "could",
    "rack",
    "would",
    "machines",
    "running",
    "master",
    "processes",
    "intention",
    "would",
    "particular",
    "rack",
    "could",
    "higher",
    "bandwidth",
    "cooling",
    "dedicated",
    "power",
    "supply",
    "top",
    "rack",
    "switch",
    "second",
    "approach",
    "could",
    "could",
    "one",
    "master",
    "process",
    "running",
    "one",
    "machine",
    "every",
    "rack",
    "could",
    "slave",
    "processes",
    "running",
    "talk",
    "rack",
    "awareness",
    "one",
    "thing",
    "understand",
    "machines",
    "placed",
    "within",
    "racks",
    "aware",
    "hadoop",
    "follows",
    "auto",
    "replication",
    "rule",
    "replication",
    "rack",
    "aware",
    "cluster",
    "would",
    "would",
    "never",
    "replicas",
    "placed",
    "rack",
    "look",
    "block",
    "blue",
    "color",
    "never",
    "three",
    "blue",
    "boxes",
    "rack",
    "even",
    "different",
    "notes",
    "makes",
    "us",
    "makes",
    "less",
    "fault",
    "tolerant",
    "would",
    "least",
    "one",
    "copy",
    "block",
    "would",
    "stored",
    "different",
    "track",
    "different",
    "note",
    "let",
    "look",
    "basically",
    "talking",
    "replicas",
    "placed",
    "way",
    "somebody",
    "could",
    "ask",
    "question",
    "block",
    "replicas",
    "spread",
    "across",
    "three",
    "lakhs",
    "yes",
    "order",
    "make",
    "redundant",
    "increasing",
    "bandwidth",
    "requirement",
    "better",
    "approach",
    "would",
    "two",
    "blocks",
    "rack",
    "different",
    "machines",
    "one",
    "copy",
    "different",
    "track",
    "let",
    "proceed",
    "restart",
    "name",
    "node",
    "demons",
    "hadoop",
    "working",
    "apache",
    "hadoop",
    "cluster",
    "could",
    "start",
    "stop",
    "using",
    "hadoop",
    "demonscripts",
    "hadoop",
    "demons",
    "scripts",
    "would",
    "used",
    "start",
    "stop",
    "hadoop",
    "talk",
    "apache",
    "hadoop",
    "let",
    "look",
    "one",
    "particular",
    "file",
    "would",
    "like",
    "show",
    "information",
    "talks",
    "different",
    "clusters",
    "let",
    "look",
    "let",
    "look",
    "start",
    "stop",
    "file",
    "let",
    "look",
    "one",
    "gives",
    "highlight",
    "talk",
    "apache",
    "hadoop",
    "setup",
    "would",
    "done",
    "would",
    "download",
    "adobe",
    "tar",
    "file",
    "would",
    "untie",
    "edit",
    "config",
    "files",
    "would",
    "formatting",
    "start",
    "cluster",
    "said",
    "using",
    "scripts",
    "case",
    "apache",
    "hadoop",
    "could",
    "using",
    "start",
    "script",
    "internally",
    "triggers",
    "start",
    "dfs",
    "start",
    "yarn",
    "scripts",
    "start",
    "dfs",
    "internally",
    "would",
    "run",
    "hadoop",
    "demon",
    "multiple",
    "times",
    "based",
    "configs",
    "start",
    "different",
    "processes",
    "start",
    "yarn",
    "would",
    "run",
    "yarn",
    "demon",
    "script",
    "start",
    "processing",
    "related",
    "processes",
    "happens",
    "apache",
    "hadoop",
    "case",
    "cloud",
    "era",
    "hortonworks",
    "basically",
    "distribution",
    "would",
    "say",
    "multiple",
    "services",
    "would",
    "one",
    "multiple",
    "demands",
    "running",
    "across",
    "machines",
    "let",
    "take",
    "example",
    "would",
    "machine",
    "1",
    "machine",
    "2",
    "machine",
    "3",
    "processor",
    "spread",
    "across",
    "however",
    "case",
    "cloudera",
    "hortonworks",
    "cluster",
    "management",
    "solutions",
    "would",
    "never",
    "involved",
    "running",
    "script",
    "individually",
    "start",
    "stop",
    "processes",
    "fact",
    "case",
    "cloudera",
    "would",
    "clouded",
    "scm",
    "server",
    "running",
    "one",
    "machines",
    "cloudera",
    "cm",
    "agents",
    "running",
    "every",
    "machine",
    "talk",
    "hortonworks",
    "would",
    "ambari",
    "server",
    "ambari",
    "agent",
    "running",
    "agents",
    "running",
    "every",
    "machine",
    "responsible",
    "monitor",
    "processes",
    "send",
    "also",
    "heartbeat",
    "master",
    "server",
    "server",
    "one",
    "service",
    "basically",
    "give",
    "instructions",
    "agents",
    "case",
    "vendor",
    "specific",
    "distribution",
    "start",
    "stop",
    "processes",
    "automatically",
    "taken",
    "care",
    "underlying",
    "services",
    "services",
    "internally",
    "still",
    "running",
    "commands",
    "however",
    "apache",
    "hadoop",
    "manually",
    "follow",
    "start",
    "stop",
    "coming",
    "back",
    "look",
    "command",
    "related",
    "questions",
    "command",
    "help",
    "find",
    "status",
    "blocks",
    "file",
    "system",
    "health",
    "always",
    "go",
    "file",
    "system",
    "check",
    "command",
    "show",
    "files",
    "particular",
    "sdfs",
    "path",
    "show",
    "blocks",
    "also",
    "give",
    "information",
    "status",
    "replicated",
    "blocks",
    "replicated",
    "blocks",
    "misreplicated",
    "blocks",
    "default",
    "replication",
    "fsck",
    "file",
    "system",
    "check",
    "utility",
    "repair",
    "problem",
    "blocks",
    "give",
    "information",
    "blocks",
    "related",
    "files",
    "machines",
    "stored",
    "replicated",
    "per",
    "replication",
    "factor",
    "problem",
    "particular",
    "replica",
    "would",
    "happen",
    "store",
    "many",
    "small",
    "files",
    "cluster",
    "relates",
    "block",
    "information",
    "gave",
    "time",
    "back",
    "remember",
    "hadoop",
    "coded",
    "java",
    "every",
    "directory",
    "every",
    "file",
    "file",
    "related",
    "block",
    "considered",
    "object",
    "every",
    "object",
    "within",
    "hadoop",
    "cluster",
    "name",
    "node",
    "ram",
    "gets",
    "utilized",
    "number",
    "blocks",
    "would",
    "usage",
    "name",
    "nodes",
    "ram",
    "storing",
    "many",
    "small",
    "files",
    "would",
    "affect",
    "disk",
    "would",
    "directly",
    "affect",
    "name",
    "nodes",
    "ram",
    "production",
    "clusters",
    "admin",
    "guys",
    "infrastructure",
    "specialist",
    "take",
    "care",
    "everyone",
    "writing",
    "data",
    "hdfs",
    "follows",
    "quota",
    "system",
    "could",
    "controlled",
    "amount",
    "data",
    "write",
    "plus",
    "count",
    "data",
    "individual",
    "rights",
    "hdfs",
    "copy",
    "data",
    "local",
    "system",
    "onto",
    "sdfs",
    "use",
    "put",
    "command",
    "copy",
    "local",
    "given",
    "local",
    "path",
    "source",
    "destination",
    "sdfs",
    "path",
    "remember",
    "always",
    "copy",
    "local",
    "using",
    "minus",
    "f",
    "option",
    "flag",
    "option",
    "also",
    "helps",
    "writing",
    "file",
    "new",
    "file",
    "hdfs",
    "minus",
    "f",
    "chance",
    "overwriting",
    "rewriting",
    "data",
    "existing",
    "sdfs",
    "copy",
    "local",
    "minus",
    "put",
    "thing",
    "also",
    "pass",
    "argument",
    "copying",
    "control",
    "replication",
    "aspects",
    "file",
    "use",
    "dfs",
    "admin",
    "refresh",
    "nodes",
    "rm",
    "admin",
    "refresh",
    "nodes",
    "command",
    "says",
    "basically",
    "refreshing",
    "node",
    "information",
    "refresh",
    "nodes",
    "mainly",
    "used",
    "say",
    "commissioning",
    "decommissioning",
    "nodes",
    "done",
    "node",
    "added",
    "cluster",
    "node",
    "removed",
    "cluster",
    "actually",
    "informing",
    "hadoop",
    "master",
    "particular",
    "node",
    "would",
    "used",
    "storage",
    "would",
    "used",
    "processing",
    "case",
    "would",
    "done",
    "process",
    "commissioning",
    "decommissioning",
    "would",
    "giving",
    "commands",
    "refreshed",
    "nodes",
    "rm",
    "admin",
    "refresh",
    "nodes",
    "internally",
    "talk",
    "commissioning",
    "decommissioning",
    "include",
    "exclude",
    "files",
    "updated",
    "include",
    "exclude",
    "files",
    "entry",
    "machines",
    "added",
    "cluster",
    "machines",
    "removed",
    "cluster",
    "done",
    "cluster",
    "still",
    "running",
    "restart",
    "master",
    "process",
    "however",
    "use",
    "refresh",
    "commands",
    "take",
    "care",
    "commissioning",
    "decommissioning",
    "activities",
    "way",
    "change",
    "replication",
    "files",
    "sdfs",
    "already",
    "written",
    "answer",
    "course",
    "yes",
    "would",
    "want",
    "set",
    "replication",
    "factor",
    "cluster",
    "level",
    "admin",
    "access",
    "could",
    "edit",
    "sdfs",
    "hyphen",
    "site",
    "file",
    "could",
    "say",
    "hadoop",
    "hyphen",
    "site",
    "file",
    "would",
    "take",
    "care",
    "replication",
    "factor",
    "set",
    "cluster",
    "level",
    "however",
    "would",
    "want",
    "change",
    "replication",
    "data",
    "written",
    "could",
    "always",
    "use",
    "setrep",
    "command",
    "setrep",
    "command",
    "basically",
    "change",
    "replication",
    "data",
    "written",
    "could",
    "also",
    "write",
    "data",
    "different",
    "replication",
    "could",
    "use",
    "minus",
    "dfs",
    "give",
    "application",
    "factor",
    "writing",
    "data",
    "cluster",
    "hadoop",
    "let",
    "data",
    "replicated",
    "per",
    "property",
    "set",
    "config",
    "file",
    "could",
    "write",
    "data",
    "different",
    "replication",
    "could",
    "change",
    "replication",
    "data",
    "read",
    "options",
    "available",
    "takes",
    "care",
    "replication",
    "consistency",
    "hadoop",
    "cluster",
    "mean",
    "replicated",
    "blocks",
    "mentioned",
    "fsck",
    "command",
    "give",
    "information",
    "replicated",
    "blocks",
    "cluster",
    "always",
    "always",
    "name",
    "node",
    "takes",
    "care",
    "replication",
    "consistency",
    "example",
    "set",
    "replication",
    "three",
    "since",
    "know",
    "first",
    "rule",
    "replication",
    "basically",
    "means",
    "two",
    "replicas",
    "residing",
    "note",
    "would",
    "mean",
    "application",
    "three",
    "would",
    "need",
    "least",
    "three",
    "data",
    "nodes",
    "available",
    "say",
    "example",
    "cluster",
    "three",
    "nodes",
    "replication",
    "set",
    "three",
    "one",
    "point",
    "time",
    "one",
    "name",
    "node",
    "crashed",
    "happens",
    "blocks",
    "would",
    "replicated",
    "means",
    "replication",
    "factor",
    "set",
    "blocks",
    "replicated",
    "enough",
    "replicas",
    "per",
    "replication",
    "factor",
    "set",
    "problem",
    "master",
    "process",
    "name",
    "node",
    "wait",
    "time",
    "start",
    "replication",
    "data",
    "data",
    "road",
    "responding",
    "disk",
    "crashed",
    "name",
    "node",
    "get",
    "information",
    "replica",
    "name",
    "node",
    "wait",
    "time",
    "start",
    "missing",
    "blocks",
    "available",
    "nodes",
    "however",
    "name",
    "node",
    "blocks",
    "replicated",
    "situation",
    "talk",
    "replicated",
    "situation",
    "name",
    "node",
    "realizes",
    "extra",
    "copies",
    "block",
    "might",
    "case",
    "three",
    "nodes",
    "running",
    "replication",
    "three",
    "one",
    "node",
    "went",
    "due",
    "network",
    "failure",
    "issue",
    "within",
    "minutes",
    "name",
    "node",
    "data",
    "failed",
    "node",
    "back",
    "set",
    "blocks",
    "name",
    "node",
    "smart",
    "enough",
    "understand",
    "replication",
    "situation",
    "delete",
    "set",
    "blocks",
    "one",
    "nodes",
    "might",
    "node",
    "recently",
    "added",
    "might",
    "old",
    "node",
    "joined",
    "cluster",
    "node",
    "depends",
    "load",
    "particular",
    "node",
    "discussed",
    "hadoop",
    "discussed",
    "sdfs",
    "discuss",
    "mapreduce",
    "programming",
    "model",
    "say",
    "processing",
    "framework",
    "distributed",
    "cache",
    "mapreduce",
    "know",
    "talk",
    "mapreduce",
    "data",
    "processed",
    "might",
    "existing",
    "multiple",
    "nodes",
    "would",
    "mapreduce",
    "program",
    "running",
    "would",
    "basically",
    "read",
    "data",
    "underlying",
    "disks",
    "could",
    "costly",
    "operation",
    "every",
    "time",
    "data",
    "read",
    "disk",
    "distributed",
    "cache",
    "mechanism",
    "wherein",
    "data",
    "set",
    "data",
    "coming",
    "disk",
    "cached",
    "available",
    "worker",
    "nodes",
    "benefit",
    "map",
    "reduce",
    "running",
    "instead",
    "every",
    "time",
    "reading",
    "data",
    "disk",
    "would",
    "pick",
    "data",
    "distributed",
    "cache",
    "benefit",
    "mapreduce",
    "processing",
    "distributed",
    "cache",
    "set",
    "job",
    "conf",
    "specify",
    "file",
    "picked",
    "distributed",
    "cache",
    "let",
    "understand",
    "roles",
    "record",
    "reader",
    "combiner",
    "partitioner",
    "kind",
    "roles",
    "play",
    "map",
    "reduce",
    "processing",
    "paradigm",
    "map",
    "reduce",
    "operation",
    "record",
    "reader",
    "communicates",
    "input",
    "split",
    "basically",
    "converts",
    "data",
    "key",
    "value",
    "pairs",
    "key",
    "value",
    "pairs",
    "ones",
    "worked",
    "upon",
    "mapper",
    "combiner",
    "optional",
    "face",
    "like",
    "mini",
    "radius",
    "combiner",
    "class",
    "relies",
    "reducer",
    "class",
    "basically",
    "combiner",
    "would",
    "receive",
    "data",
    "map",
    "tasks",
    "would",
    "completed",
    "works",
    "based",
    "whatever",
    "reducer",
    "class",
    "mentions",
    "passes",
    "output",
    "reducer",
    "phase",
    "partitioner",
    "basically",
    "phase",
    "decides",
    "many",
    "reduced",
    "tasks",
    "would",
    "used",
    "aggregate",
    "summarize",
    "data",
    "partitioner",
    "phase",
    "would",
    "decide",
    "based",
    "number",
    "keys",
    "based",
    "number",
    "map",
    "tasks",
    "partitioner",
    "would",
    "decide",
    "one",
    "multiple",
    "reduced",
    "tasks",
    "would",
    "used",
    "take",
    "care",
    "processing",
    "either",
    "could",
    "partitioner",
    "decides",
    "many",
    "reduced",
    "tasks",
    "would",
    "run",
    "could",
    "based",
    "properties",
    "set",
    "within",
    "cluster",
    "take",
    "care",
    "number",
    "reduced",
    "tasks",
    "would",
    "used",
    "always",
    "remember",
    "partitioner",
    "decides",
    "outputs",
    "combiner",
    "sent",
    "reducer",
    "many",
    "reducers",
    "controls",
    "partitioning",
    "keys",
    "intermediate",
    "map",
    "outputs",
    "map",
    "phase",
    "whatever",
    "output",
    "generates",
    "intermediate",
    "output",
    "taken",
    "partitioner",
    "combiner",
    "partitioner",
    "sent",
    "one",
    "multiple",
    "reduced",
    "tasks",
    "one",
    "common",
    "questions",
    "might",
    "face",
    "mapreduce",
    "lower",
    "processing",
    "know",
    "mapreduce",
    "goes",
    "parallel",
    "processing",
    "know",
    "multiple",
    "map",
    "tasks",
    "running",
    "multiple",
    "nodes",
    "time",
    "also",
    "know",
    "multiple",
    "reduced",
    "tasks",
    "could",
    "running",
    "mapreduce",
    "become",
    "slower",
    "approach",
    "first",
    "mapreduce",
    "batch",
    "oriented",
    "operation",
    "mapreduce",
    "rigid",
    "strictly",
    "uses",
    "mapping",
    "reducing",
    "phases",
    "matter",
    "kind",
    "processing",
    "would",
    "want",
    "would",
    "still",
    "provide",
    "mapper",
    "function",
    "reducer",
    "function",
    "work",
    "data",
    "whenever",
    "map",
    "phase",
    "completes",
    "output",
    "map",
    "face",
    "intermittent",
    "output",
    "would",
    "written",
    "hdfs",
    "thereafter",
    "underlying",
    "disks",
    "data",
    "would",
    "shuffled",
    "sorted",
    "picked",
    "reducing",
    "phase",
    "every",
    "time",
    "data",
    "written",
    "hdfs",
    "retrieved",
    "sdfs",
    "makes",
    "mapreduce",
    "slower",
    "approach",
    "question",
    "mapreduce",
    "job",
    "possible",
    "change",
    "number",
    "mappers",
    "created",
    "default",
    "change",
    "number",
    "map",
    "tasks",
    "number",
    "map",
    "tasks",
    "depends",
    "input",
    "splits",
    "however",
    "different",
    "ways",
    "either",
    "set",
    "property",
    "number",
    "map",
    "tasks",
    "used",
    "customize",
    "code",
    "make",
    "use",
    "different",
    "format",
    "control",
    "number",
    "map",
    "tasks",
    "default",
    "number",
    "map",
    "tasks",
    "equal",
    "number",
    "splits",
    "file",
    "processing",
    "1",
    "gb",
    "file",
    "split",
    "8",
    "blocks",
    "128",
    "mb",
    "would",
    "8",
    "map",
    "tasks",
    "running",
    "cluster",
    "map",
    "tasks",
    "basically",
    "running",
    "mapper",
    "function",
    "hard",
    "coded",
    "properties",
    "map",
    "red",
    "hyphen",
    "site",
    "file",
    "specify",
    "number",
    "map",
    "tasks",
    "could",
    "control",
    "number",
    "map",
    "tasks",
    "let",
    "also",
    "talk",
    "data",
    "types",
    "prepare",
    "hadoop",
    "want",
    "get",
    "big",
    "data",
    "field",
    "start",
    "learning",
    "different",
    "data",
    "formats",
    "different",
    "data",
    "formats",
    "avro",
    "parquet",
    "sequence",
    "file",
    "binary",
    "format",
    "different",
    "formats",
    "used",
    "talk",
    "data",
    "types",
    "hadoop",
    "implementation",
    "writable",
    "writable",
    "comparable",
    "interfaces",
    "every",
    "data",
    "type",
    "java",
    "equivalent",
    "hadoop",
    "end",
    "java",
    "would",
    "intriguable",
    "hadoop",
    "float",
    "would",
    "float",
    "writable",
    "long",
    "would",
    "long",
    "writable",
    "double",
    "writable",
    "boolean",
    "writable",
    "array",
    "writable",
    "map",
    "writable",
    "object",
    "credible",
    "different",
    "data",
    "types",
    "could",
    "used",
    "within",
    "mapreduce",
    "program",
    "implementation",
    "writable",
    "writable",
    "comparable",
    "interfaces",
    "speculative",
    "execution",
    "imagine",
    "cluster",
    "huge",
    "number",
    "nodes",
    "data",
    "spread",
    "across",
    "multiple",
    "slave",
    "machines",
    "multiple",
    "nodes",
    "point",
    "time",
    "due",
    "disk",
    "degrade",
    "network",
    "issues",
    "machine",
    "heating",
    "load",
    "particular",
    "node",
    "situation",
    "data",
    "node",
    "execute",
    "task",
    "slower",
    "manner",
    "case",
    "speculative",
    "execution",
    "turned",
    "would",
    "shadow",
    "task",
    "another",
    "similar",
    "task",
    "running",
    "node",
    "processing",
    "whichever",
    "task",
    "finishes",
    "first",
    "accepted",
    "task",
    "would",
    "killed",
    "speculative",
    "execution",
    "could",
    "good",
    "working",
    "intensive",
    "workload",
    "kind",
    "environment",
    "particular",
    "node",
    "slower",
    "could",
    "benefit",
    "unoccupied",
    "node",
    "less",
    "load",
    "take",
    "care",
    "processing",
    "going",
    "understand",
    "node",
    "might",
    "slower",
    "task",
    "would",
    "scheduler",
    "maintaining",
    "knowledge",
    "resources",
    "available",
    "speculative",
    "execution",
    "property",
    "turned",
    "task",
    "running",
    "slow",
    "copy",
    "task",
    "say",
    "shadow",
    "task",
    "would",
    "run",
    "node",
    "whichever",
    "task",
    "completes",
    "first",
    "considered",
    "happens",
    "speculative",
    "execution",
    "identity",
    "mapper",
    "different",
    "chain",
    "mapper",
    "getting",
    "deeper",
    "mapreduce",
    "concepts",
    "talk",
    "mapper",
    "identity",
    "mapper",
    "default",
    "mapper",
    "chosen",
    "mapper",
    "specified",
    "mapreduce",
    "driver",
    "class",
    "every",
    "mapreduce",
    "program",
    "would",
    "map",
    "class",
    "taking",
    "care",
    "mapping",
    "phase",
    "basically",
    "mapper",
    "function",
    "would",
    "run",
    "one",
    "multiple",
    "map",
    "tasks",
    "right",
    "programming",
    "program",
    "would",
    "also",
    "reduce",
    "class",
    "would",
    "running",
    "reducer",
    "function",
    "takes",
    "care",
    "reduced",
    "tasks",
    "running",
    "multiple",
    "nodes",
    "mapper",
    "specified",
    "within",
    "driver",
    "class",
    "driver",
    "class",
    "something",
    "information",
    "flow",
    "map",
    "class",
    "reduced",
    "class",
    "input",
    "format",
    "output",
    "format",
    "job",
    "configurations",
    "identity",
    "mapper",
    "default",
    "mapper",
    "chosen",
    "mapper",
    "class",
    "mentioned",
    "driver",
    "class",
    "basically",
    "implements",
    "identity",
    "function",
    "directly",
    "writes",
    "key",
    "pairs",
    "output",
    "defined",
    "old",
    "mapreduce",
    "api",
    "particular",
    "package",
    "talk",
    "chaining",
    "mappers",
    "chain",
    "mapper",
    "basically",
    "class",
    "run",
    "multiple",
    "mappers",
    "single",
    "map",
    "task",
    "basically",
    "could",
    "say",
    "multiple",
    "map",
    "tasks",
    "would",
    "run",
    "part",
    "processing",
    "output",
    "first",
    "mapper",
    "would",
    "become",
    "input",
    "second",
    "mapper",
    "defined",
    "mentioned",
    "class",
    "package",
    "major",
    "configuration",
    "parameters",
    "required",
    "mapreduce",
    "program",
    "obviously",
    "need",
    "input",
    "location",
    "need",
    "output",
    "location",
    "input",
    "location",
    "files",
    "picked",
    "would",
    "preferably",
    "sdfs",
    "directory",
    "output",
    "location",
    "path",
    "job",
    "output",
    "would",
    "written",
    "mapreduce",
    "program",
    "also",
    "need",
    "specify",
    "input",
    "output",
    "formats",
    "specify",
    "defaults",
    "considered",
    "need",
    "also",
    "classes",
    "map",
    "reduce",
    "functions",
    "intend",
    "run",
    "code",
    "cluster",
    "need",
    "package",
    "class",
    "jar",
    "file",
    "export",
    "cluster",
    "jar",
    "file",
    "would",
    "mapper",
    "reducer",
    "driver",
    "classes",
    "important",
    "configuration",
    "parameters",
    "need",
    "consider",
    "mapreduce",
    "program",
    "difference",
    "mean",
    "map",
    "side",
    "join",
    "reduce",
    "side",
    "join",
    "map",
    "side",
    "join",
    "basically",
    "join",
    "performed",
    "mapping",
    "level",
    "mapping",
    "phase",
    "performed",
    "mapper",
    "input",
    "data",
    "worked",
    "upon",
    "divided",
    "number",
    "partitions",
    "input",
    "map",
    "form",
    "structured",
    "partition",
    "sorted",
    "order",
    "map",
    "site",
    "join",
    "understand",
    "simpler",
    "way",
    "compare",
    "rdbms",
    "concepts",
    "two",
    "tables",
    "joined",
    "always",
    "advisable",
    "give",
    "bigger",
    "table",
    "left",
    "side",
    "table",
    "first",
    "table",
    "join",
    "condition",
    "would",
    "smaller",
    "table",
    "left",
    "side",
    "bigger",
    "table",
    "right",
    "side",
    "basically",
    "means",
    "smaller",
    "table",
    "could",
    "loaded",
    "memory",
    "could",
    "used",
    "joining",
    "map",
    "side",
    "drawing",
    "similar",
    "kind",
    "mechanism",
    "input",
    "data",
    "divided",
    "number",
    "partitions",
    "talk",
    "reduced",
    "side",
    "join",
    "join",
    "performed",
    "reducer",
    "easier",
    "implement",
    "website",
    "join",
    "sorting",
    "shuffling",
    "send",
    "values",
    "send",
    "values",
    "identical",
    "keys",
    "reducer",
    "need",
    "data",
    "set",
    "structured",
    "form",
    "look",
    "map",
    "side",
    "join",
    "reduce",
    "side",
    "join",
    "joins",
    "understand",
    "mapreduce",
    "works",
    "however",
    "would",
    "suggest",
    "focus",
    "mapreduce",
    "still",
    "used",
    "processing",
    "amount",
    "map",
    "reduced",
    "base",
    "processing",
    "decreased",
    "overall",
    "across",
    "industry",
    "role",
    "output",
    "committer",
    "class",
    "map",
    "reduced",
    "job",
    "output",
    "committer",
    "name",
    "says",
    "describes",
    "commit",
    "task",
    "output",
    "mapreduce",
    "job",
    "could",
    "mentioned",
    "capacity",
    "hadoop",
    "mapreduce",
    "output",
    "committer",
    "could",
    "class",
    "extends",
    "output",
    "committer",
    "class",
    "mapreduce",
    "relies",
    "mapreduce",
    "relies",
    "output",
    "committer",
    "job",
    "set",
    "job",
    "initialization",
    "cleaning",
    "job",
    "job",
    "completion",
    "means",
    "resources",
    "used",
    "particular",
    "job",
    "setting",
    "task",
    "temporary",
    "output",
    "checking",
    "whether",
    "task",
    "needs",
    "commit",
    "committing",
    "task",
    "output",
    "discarding",
    "task",
    "important",
    "class",
    "used",
    "within",
    "mapreduce",
    "job",
    "process",
    "spilling",
    "mapreduce",
    "mean",
    "spilling",
    "basically",
    "process",
    "copying",
    "data",
    "memory",
    "buffer",
    "disk",
    "obviously",
    "buffer",
    "usage",
    "reaches",
    "certain",
    "threshold",
    "enough",
    "memory",
    "buffer",
    "memory",
    "content",
    "stored",
    "buffer",
    "memory",
    "flushed",
    "default",
    "background",
    "thread",
    "starts",
    "spilling",
    "content",
    "memory",
    "disk",
    "eighty",
    "percent",
    "buffer",
    "size",
    "filled",
    "buffer",
    "used",
    "mapreduce",
    "processing",
    "happening",
    "data",
    "data",
    "read",
    "disk",
    "loaded",
    "buffer",
    "processing",
    "happens",
    "thing",
    "also",
    "happens",
    "writing",
    "data",
    "cluster",
    "imagine",
    "100",
    "megabyte",
    "size",
    "buffer",
    "spilling",
    "start",
    "content",
    "buffer",
    "reaches",
    "80",
    "megabytes",
    "customizable",
    "set",
    "mappers",
    "reducers",
    "mapreduce",
    "job",
    "properties",
    "number",
    "mappers",
    "reducers",
    "mentioned",
    "earlier",
    "customized",
    "default",
    "number",
    "map",
    "tasks",
    "depends",
    "split",
    "number",
    "reduced",
    "tasks",
    "depends",
    "partitioning",
    "phase",
    "decides",
    "number",
    "reduced",
    "tasks",
    "would",
    "used",
    "depending",
    "keys",
    "however",
    "set",
    "properties",
    "either",
    "config",
    "files",
    "provide",
    "command",
    "line",
    "also",
    "make",
    "part",
    "code",
    "control",
    "number",
    "map",
    "tasks",
    "reduce",
    "tasks",
    "would",
    "run",
    "particular",
    "job",
    "let",
    "look",
    "one",
    "interesting",
    "question",
    "happens",
    "node",
    "running",
    "map",
    "task",
    "fails",
    "sending",
    "output",
    "reducer",
    "node",
    "running",
    "map",
    "task",
    "know",
    "could",
    "one",
    "multiple",
    "map",
    "tasks",
    "running",
    "one",
    "multiple",
    "nodes",
    "map",
    "tasks",
    "completed",
    "stages",
    "combiner",
    "reducer",
    "come",
    "existence",
    "case",
    "node",
    "crashes",
    "map",
    "task",
    "assigned",
    "whole",
    "task",
    "run",
    "node",
    "hadoop",
    "version",
    "2",
    "yarn",
    "framework",
    "temporary",
    "demon",
    "called",
    "application",
    "master",
    "application",
    "master",
    "taking",
    "care",
    "execution",
    "application",
    "particular",
    "task",
    "particular",
    "node",
    "failed",
    "due",
    "unavailability",
    "node",
    "role",
    "application",
    "master",
    "task",
    "scheduled",
    "node",
    "write",
    "output",
    "mapreduce",
    "different",
    "formats",
    "course",
    "hadoop",
    "supports",
    "various",
    "input",
    "output",
    "formats",
    "write",
    "output",
    "mapreduce",
    "different",
    "formats",
    "could",
    "default",
    "format",
    "text",
    "output",
    "format",
    "wherein",
    "records",
    "written",
    "line",
    "text",
    "could",
    "sequence",
    "file",
    "basically",
    "write",
    "sequence",
    "files",
    "binary",
    "format",
    "files",
    "output",
    "files",
    "need",
    "fed",
    "another",
    "mapreduce",
    "jobs",
    "could",
    "go",
    "map",
    "file",
    "output",
    "format",
    "write",
    "output",
    "map",
    "files",
    "could",
    "go",
    "sequence",
    "file",
    "binary",
    "output",
    "format",
    "variant",
    "sequence",
    "file",
    "input",
    "format",
    "basically",
    "writes",
    "keys",
    "values",
    "sequence",
    "file",
    "talk",
    "binary",
    "format",
    "talking",
    "readable",
    "format",
    "db",
    "output",
    "format",
    "basically",
    "used",
    "would",
    "want",
    "write",
    "data",
    "say",
    "relational",
    "databases",
    "say",
    "sql",
    "databases",
    "hbase",
    "format",
    "also",
    "sends",
    "reduce",
    "output",
    "sql",
    "table",
    "let",
    "learn",
    "little",
    "bit",
    "yarn",
    "yarn",
    "stands",
    "yet",
    "another",
    "resource",
    "negotiator",
    "processing",
    "framework",
    "benefits",
    "yan",
    "bring",
    "hadoop",
    "version",
    "2",
    "solve",
    "issues",
    "mapreduce",
    "version",
    "mapreduce",
    "version",
    "1",
    "major",
    "issues",
    "comes",
    "scalability",
    "availability",
    "sorry",
    "hadoop",
    "version",
    "1",
    "one",
    "master",
    "process",
    "processing",
    "layer",
    "job",
    "tracker",
    "job",
    "tracker",
    "listening",
    "task",
    "trackers",
    "running",
    "multiple",
    "machines",
    "job",
    "tracker",
    "responsible",
    "resource",
    "tracking",
    "job",
    "scheduling",
    "yarn",
    "still",
    "processing",
    "master",
    "called",
    "resource",
    "manager",
    "instead",
    "job",
    "tracker",
    "hadoop",
    "version",
    "2",
    "could",
    "even",
    "resource",
    "manager",
    "running",
    "high",
    "availability",
    "mode",
    "node",
    "managers",
    "would",
    "running",
    "multiple",
    "machines",
    "temporary",
    "demon",
    "called",
    "application",
    "master",
    "case",
    "hadoop",
    "version",
    "2",
    "resource",
    "manager",
    "master",
    "handling",
    "client",
    "connections",
    "taking",
    "care",
    "tracking",
    "resources",
    "jobs",
    "scheduling",
    "basically",
    "taking",
    "care",
    "execution",
    "across",
    "multiple",
    "nodes",
    "controlled",
    "application",
    "master",
    "till",
    "application",
    "completes",
    "yarn",
    "different",
    "kind",
    "resource",
    "allocations",
    "could",
    "done",
    "concept",
    "container",
    "container",
    "basically",
    "combination",
    "ram",
    "cpu",
    "cores",
    "yarn",
    "run",
    "different",
    "kind",
    "workloads",
    "mapreduce",
    "kind",
    "workload",
    "run",
    "auto",
    "version",
    "2",
    "would",
    "graph",
    "processing",
    "massive",
    "parallel",
    "processing",
    "could",
    "processing",
    "huge",
    "processing",
    "applications",
    "could",
    "run",
    "cluster",
    "based",
    "yarn",
    "talk",
    "scalability",
    "case",
    "hadoop",
    "version",
    "2",
    "cluster",
    "size",
    "10",
    "000",
    "nodes",
    "run",
    "100",
    "000",
    "concurrent",
    "tasks",
    "every",
    "application",
    "launched",
    "temporary",
    "demon",
    "called",
    "application",
    "master",
    "would",
    "10",
    "applications",
    "running",
    "would",
    "10",
    "app",
    "masters",
    "running",
    "taking",
    "care",
    "execution",
    "applications",
    "across",
    "multiple",
    "nodes",
    "compatibility",
    "hadoop",
    "version",
    "2",
    "fully",
    "compatible",
    "whatever",
    "developed",
    "per",
    "hadoop",
    "version",
    "1",
    "processing",
    "needs",
    "would",
    "taken",
    "care",
    "yarn",
    "dynamic",
    "allocation",
    "cluster",
    "resources",
    "taking",
    "care",
    "different",
    "workloads",
    "allocating",
    "resources",
    "across",
    "multiple",
    "machines",
    "using",
    "execution",
    "taken",
    "care",
    "basically",
    "means",
    "could",
    "multiple",
    "users",
    "multiple",
    "teams",
    "could",
    "open",
    "source",
    "proprietary",
    "data",
    "access",
    "engines",
    "could",
    "basically",
    "hosted",
    "using",
    "cluster",
    "yarn",
    "allocate",
    "resources",
    "application",
    "help",
    "architecture",
    "basically",
    "client",
    "application",
    "api",
    "talks",
    "resource",
    "manager",
    "resource",
    "manager",
    "mentioned",
    "managing",
    "resource",
    "allocation",
    "cluster",
    "talk",
    "resource",
    "manager",
    "internal",
    "two",
    "components",
    "one",
    "scheduler",
    "one",
    "applications",
    "manager",
    "say",
    "resource",
    "manager",
    "master",
    "tracking",
    "resources",
    "source",
    "manager",
    "one",
    "negotiating",
    "resources",
    "slave",
    "actually",
    "resource",
    "manager",
    "internal",
    "components",
    "scheduler",
    "allocates",
    "resources",
    "various",
    "running",
    "applications",
    "scheduler",
    "bothered",
    "tracking",
    "resources",
    "basically",
    "tracking",
    "applications",
    "different",
    "kind",
    "schedulers",
    "fifo",
    "first",
    "first",
    "could",
    "fair",
    "scheduler",
    "could",
    "capacity",
    "scheduler",
    "schedulers",
    "basically",
    "control",
    "resources",
    "allocated",
    "multiple",
    "applications",
    "running",
    "parallel",
    "queue",
    "mechanism",
    "scheduler",
    "schedule",
    "resources",
    "based",
    "requirements",
    "application",
    "monitoring",
    "tracking",
    "status",
    "applications",
    "applications",
    "manager",
    "one",
    "accepting",
    "job",
    "submissions",
    "monitoring",
    "restarting",
    "application",
    "masters",
    "application",
    "manager",
    "basically",
    "launching",
    "application",
    "master",
    "responsible",
    "application",
    "looks",
    "whenever",
    "job",
    "submission",
    "happens",
    "already",
    "know",
    "resource",
    "manager",
    "aware",
    "resources",
    "available",
    "every",
    "node",
    "manager",
    "every",
    "node",
    "fixed",
    "amount",
    "ram",
    "cpu",
    "cores",
    "portion",
    "resources",
    "ram",
    "cpu",
    "cores",
    "allocated",
    "node",
    "manager",
    "resource",
    "manager",
    "already",
    "aware",
    "much",
    "resources",
    "available",
    "across",
    "nodes",
    "whenever",
    "client",
    "request",
    "comes",
    "resource",
    "manager",
    "make",
    "request",
    "node",
    "manager",
    "basically",
    "request",
    "node",
    "manager",
    "hold",
    "resources",
    "processing",
    "node",
    "manager",
    "would",
    "basically",
    "approve",
    "disapprove",
    "request",
    "holding",
    "sources",
    "resources",
    "combination",
    "ram",
    "cpu",
    "cores",
    "nothing",
    "containers",
    "allocate",
    "containers",
    "different",
    "sizes",
    "within",
    "yarn",
    "hyphen",
    "site",
    "file",
    "node",
    "manager",
    "based",
    "request",
    "resource",
    "manager",
    "guarantees",
    "container",
    "would",
    "available",
    "processing",
    "resource",
    "manager",
    "starts",
    "temporary",
    "demand",
    "called",
    "application",
    "master",
    "take",
    "care",
    "execution",
    "app",
    "master",
    "launched",
    "resource",
    "manager",
    "say",
    "internal",
    "away",
    "applications",
    "manager",
    "run",
    "one",
    "containers",
    "application",
    "master",
    "also",
    "piece",
    "code",
    "run",
    "one",
    "containers",
    "containers",
    "utilized",
    "execution",
    "yarn",
    "basically",
    "taking",
    "care",
    "allocation",
    "application",
    "master",
    "managing",
    "resource",
    "needs",
    "one",
    "interacting",
    "scheduler",
    "particular",
    "node",
    "crashes",
    "responsibility",
    "app",
    "master",
    "go",
    "back",
    "master",
    "resource",
    "manager",
    "negotiate",
    "resources",
    "app",
    "master",
    "never",
    "ever",
    "negotiate",
    "resources",
    "node",
    "manager",
    "directly",
    "always",
    "talk",
    "resource",
    "manager",
    "source",
    "manager",
    "one",
    "negotiates",
    "resources",
    "container",
    "said",
    "collection",
    "resources",
    "like",
    "ram",
    "cpu",
    "network",
    "bandwidth",
    "container",
    "allocated",
    "based",
    "availability",
    "resources",
    "particular",
    "node",
    "following",
    "occupied",
    "place",
    "job",
    "tracker",
    "mapreduce",
    "resource",
    "manager",
    "resource",
    "manager",
    "name",
    "master",
    "process",
    "adobe",
    "portion",
    "would",
    "write",
    "yarn",
    "commands",
    "check",
    "status",
    "application",
    "could",
    "say",
    "yarn",
    "application",
    "minus",
    "status",
    "application",
    "id",
    "could",
    "kill",
    "also",
    "command",
    "line",
    "remember",
    "yarn",
    "ui",
    "even",
    "look",
    "applications",
    "ui",
    "even",
    "kill",
    "applications",
    "ui",
    "however",
    "knowing",
    "command",
    "line",
    "commands",
    "would",
    "useful",
    "one",
    "resource",
    "manager",
    "young",
    "base",
    "cluster",
    "yes",
    "hadoop",
    "version",
    "2",
    "allows",
    "us",
    "high",
    "availability",
    "yarn",
    "cluster",
    "active",
    "standby",
    "coordination",
    "taking",
    "care",
    "zookeeper",
    "particular",
    "time",
    "one",
    "active",
    "resource",
    "manager",
    "active",
    "resource",
    "manager",
    "fails",
    "standby",
    "resource",
    "manager",
    "comes",
    "becomes",
    "active",
    "however",
    "zookeeper",
    "playing",
    "important",
    "role",
    "remember",
    "zookeeper",
    "one",
    "coordinating",
    "server",
    "state",
    "election",
    "active",
    "standby",
    "failover",
    "different",
    "schedulers",
    "available",
    "yarn",
    "fifo",
    "scheduler",
    "first",
    "first",
    "desirable",
    "option",
    "case",
    "longer",
    "running",
    "application",
    "might",
    "block",
    "small",
    "running",
    "applications",
    "capacity",
    "scheduler",
    "basically",
    "scheduler",
    "dedicated",
    "queues",
    "created",
    "fixed",
    "amount",
    "resources",
    "multiple",
    "applications",
    "accessing",
    "cluster",
    "time",
    "would",
    "using",
    "queues",
    "resources",
    "allocated",
    "talk",
    "fair",
    "scheduler",
    "need",
    "fixed",
    "amount",
    "resources",
    "percentage",
    "could",
    "decide",
    "kind",
    "fairness",
    "followed",
    "basically",
    "means",
    "allocated",
    "20",
    "gigabytes",
    "memory",
    "however",
    "cluster",
    "100",
    "gigabytes",
    "team",
    "assigned",
    "80",
    "gigabytes",
    "memory",
    "20",
    "percent",
    "access",
    "cluster",
    "team",
    "80",
    "however",
    "team",
    "come",
    "use",
    "cluster",
    "fair",
    "scheduler",
    "go",
    "maximum",
    "100",
    "cluster",
    "find",
    "information",
    "schedulers",
    "could",
    "either",
    "look",
    "hadoop",
    "definitive",
    "guide",
    "could",
    "could",
    "go",
    "google",
    "could",
    "type",
    "example",
    "yarn",
    "scheduler",
    "let",
    "search",
    "yarn",
    "scheduler",
    "look",
    "hadoop",
    "definitive",
    "guide",
    "hadoop",
    "definitive",
    "guide",
    "beautifully",
    "explains",
    "different",
    "schedulers",
    "multiple",
    "applications",
    "run",
    "could",
    "fifo",
    "kind",
    "scheduling",
    "could",
    "capacity",
    "scheduler",
    "could",
    "fair",
    "scheduling",
    "look",
    "link",
    "good",
    "link",
    "also",
    "search",
    "yarn",
    "untangling",
    "blog",
    "four",
    "series",
    "four",
    "blocks",
    "beautifully",
    "explained",
    "yarn",
    "works",
    "resource",
    "allocation",
    "happens",
    "container",
    "runs",
    "within",
    "container",
    "scroll",
    "reading",
    "also",
    "search",
    "part",
    "two",
    "talks",
    "allocation",
    "coming",
    "back",
    "basically",
    "schedulers",
    "happens",
    "resource",
    "manager",
    "fails",
    "executing",
    "application",
    "high",
    "availability",
    "cluster",
    "high",
    "availability",
    "cluster",
    "know",
    "would",
    "two",
    "resource",
    "managers",
    "one",
    "active",
    "one",
    "standby",
    "zookeeper",
    "keeping",
    "track",
    "server",
    "states",
    "rm",
    "fails",
    "case",
    "high",
    "availability",
    "standby",
    "elected",
    "active",
    "basically",
    "resource",
    "manager",
    "standby",
    "would",
    "become",
    "active",
    "one",
    "one",
    "would",
    "instruct",
    "application",
    "master",
    "abort",
    "beginning",
    "resource",
    "manager",
    "recovers",
    "running",
    "state",
    "something",
    "called",
    "rm",
    "state",
    "store",
    "applications",
    "running",
    "status",
    "stored",
    "resource",
    "manager",
    "recovers",
    "running",
    "state",
    "looking",
    "state",
    "store",
    "taking",
    "advantage",
    "container",
    "statuses",
    "continues",
    "take",
    "care",
    "processing",
    "cluster",
    "10",
    "data",
    "nodes",
    "16",
    "gb",
    "10",
    "cores",
    "would",
    "total",
    "processing",
    "capacity",
    "cluster",
    "take",
    "minute",
    "think",
    "10",
    "data",
    "nodes",
    "16",
    "gb",
    "ram",
    "per",
    "node",
    "10",
    "cores",
    "mention",
    "answer",
    "160",
    "gb",
    "ram",
    "100",
    "cores",
    "went",
    "wrong",
    "think",
    "cluster",
    "10",
    "data",
    "nodes",
    "16",
    "gb",
    "ram",
    "10",
    "cores",
    "remember",
    "every",
    "node",
    "hadoop",
    "cluster",
    "would",
    "one",
    "multiple",
    "processors",
    "running",
    "processes",
    "would",
    "need",
    "ram",
    "machine",
    "linux",
    "file",
    "system",
    "would",
    "processes",
    "would",
    "also",
    "ram",
    "usage",
    "basically",
    "means",
    "talk",
    "10",
    "data",
    "nodes",
    "deduct",
    "least",
    "20",
    "30",
    "percent",
    "towards",
    "overheads",
    "towards",
    "cloud",
    "database",
    "services",
    "towards",
    "processes",
    "running",
    "case",
    "could",
    "say",
    "could",
    "11",
    "12",
    "gb",
    "available",
    "every",
    "machine",
    "processing",
    "say",
    "6",
    "7",
    "cores",
    "multiply",
    "10",
    "processing",
    "capacity",
    "remember",
    "thing",
    "applies",
    "disk",
    "usage",
    "also",
    "somebody",
    "asks",
    "10",
    "data",
    "node",
    "cluster",
    "machine",
    "20",
    "terabytes",
    "disks",
    "total",
    "storage",
    "capacity",
    "available",
    "sdfs",
    "answer",
    "would",
    "200",
    "consider",
    "overheads",
    "basically",
    "gives",
    "processing",
    "capacity",
    "let",
    "look",
    "one",
    "question",
    "happens",
    "requested",
    "memory",
    "cpu",
    "cores",
    "beyond",
    "goes",
    "beyond",
    "size",
    "container",
    "said",
    "configurations",
    "say",
    "particular",
    "data",
    "node",
    "100",
    "gb",
    "ram",
    "could",
    "allocate",
    "say",
    "50",
    "gb",
    "processing",
    "like",
    "100",
    "cores",
    "could",
    "say",
    "50",
    "cores",
    "processing",
    "100",
    "gb",
    "ram",
    "100",
    "cores",
    "could",
    "ideally",
    "allocate",
    "100",
    "processing",
    "ideally",
    "possible",
    "100",
    "gb",
    "ram",
    "would",
    "go",
    "50",
    "gb",
    "100",
    "cores",
    "would",
    "go",
    "50",
    "cores",
    "within",
    "ram",
    "cpu",
    "course",
    "concept",
    "containers",
    "right",
    "container",
    "combination",
    "ram",
    "cpu",
    "cores",
    "could",
    "minimum",
    "size",
    "container",
    "maximum",
    "size",
    "container",
    "point",
    "time",
    "application",
    "starts",
    "demanding",
    "memory",
    "cpu",
    "cores",
    "fit",
    "container",
    "location",
    "application",
    "fail",
    "application",
    "fail",
    "requested",
    "memory",
    "combination",
    "memory",
    "cpu",
    "cores",
    "maximum",
    "container",
    "size",
    "discuss",
    "hive",
    "peg",
    "hbase",
    "components",
    "used",
    "industry",
    "various",
    "use",
    "cases",
    "let",
    "look",
    "questions",
    "let",
    "look",
    "prepare",
    "first",
    "learn",
    "hive",
    "data",
    "warehousing",
    "package",
    "question",
    "different",
    "components",
    "hive",
    "architecture",
    "talk",
    "hive",
    "already",
    "know",
    "hive",
    "data",
    "warehousing",
    "package",
    "basically",
    "allows",
    "work",
    "structured",
    "data",
    "data",
    "structuralized",
    "normally",
    "people",
    "well",
    "versed",
    "querying",
    "basically",
    "processing",
    "data",
    "using",
    "sql",
    "queries",
    "lot",
    "people",
    "come",
    "database",
    "backgrounds",
    "would",
    "find",
    "comfortable",
    "know",
    "structured",
    "query",
    "language",
    "hive",
    "one",
    "data",
    "warehousing",
    "package",
    "resides",
    "within",
    "hadoop",
    "ecosystem",
    "uses",
    "hadoop",
    "distributed",
    "file",
    "system",
    "store",
    "data",
    "uses",
    "rdbms",
    "usually",
    "store",
    "metadata",
    "although",
    "metadata",
    "stored",
    "locally",
    "also",
    "different",
    "components",
    "hive",
    "architecture",
    "user",
    "interface",
    "user",
    "interface",
    "calls",
    "execute",
    "interface",
    "driver",
    "creates",
    "session",
    "query",
    "sends",
    "query",
    "compiler",
    "generate",
    "execution",
    "plan",
    "usually",
    "whenever",
    "hive",
    "set",
    "would",
    "metadata",
    "stored",
    "rdbms",
    "establish",
    "connection",
    "rdbms",
    "hadoop",
    "need",
    "odbc",
    "jdbc",
    "connector",
    "jar",
    "file",
    "connector",
    "jar",
    "file",
    "driver",
    "class",
    "driver",
    "class",
    "mandatory",
    "create",
    "connection",
    "hive",
    "hadoop",
    "user",
    "interface",
    "creates",
    "interface",
    "using",
    "driver",
    "metastore",
    "metastore",
    "stores",
    "metadata",
    "information",
    "object",
    "create",
    "database",
    "table",
    "indexes",
    "metadata",
    "stored",
    "metastore",
    "usually",
    "metastore",
    "stored",
    "rdbms",
    "multiple",
    "users",
    "connect",
    "hive",
    "metastore",
    "stores",
    "metadata",
    "information",
    "sends",
    "compiler",
    "execution",
    "query",
    "compiler",
    "generates",
    "execution",
    "plan",
    "dag",
    "dag",
    "stands",
    "direct",
    "acyclic",
    "graph",
    "dag",
    "stages",
    "stage",
    "either",
    "metadata",
    "operation",
    "map",
    "reduced",
    "job",
    "operation",
    "sdfs",
    "finally",
    "execution",
    "engine",
    "acts",
    "bridge",
    "hive",
    "hadoop",
    "process",
    "query",
    "execution",
    "engine",
    "communicates",
    "metastore",
    "perform",
    "operations",
    "like",
    "create",
    "drop",
    "tables",
    "four",
    "important",
    "components",
    "hive",
    "architecture",
    "difference",
    "external",
    "table",
    "manage",
    "table",
    "height",
    "various",
    "kinds",
    "table",
    "height",
    "external",
    "table",
    "manage",
    "table",
    "partition",
    "table",
    "major",
    "difference",
    "managed",
    "external",
    "table",
    "respect",
    "happens",
    "data",
    "table",
    "dropped",
    "usually",
    "whenever",
    "create",
    "table",
    "hive",
    "creates",
    "manage",
    "table",
    "could",
    "also",
    "call",
    "internal",
    "table",
    "manages",
    "data",
    "moves",
    "warehouse",
    "directory",
    "default",
    "whether",
    "create",
    "manage",
    "table",
    "external",
    "table",
    "usually",
    "data",
    "recite",
    "hive",
    "default",
    "warehouse",
    "directory",
    "could",
    "residing",
    "location",
    "chosen",
    "however",
    "talk",
    "manage",
    "table",
    "one",
    "drops",
    "manage",
    "table",
    "metadata",
    "information",
    "deleted",
    "table",
    "data",
    "also",
    "deleted",
    "sdfs",
    "talk",
    "external",
    "table",
    "created",
    "external",
    "keyword",
    "explicitly",
    "external",
    "table",
    "dropped",
    "nothing",
    "happens",
    "data",
    "resides",
    "sdfs",
    "main",
    "difference",
    "managed",
    "external",
    "table",
    "might",
    "use",
    "case",
    "somebody",
    "asks",
    "might",
    "migration",
    "kind",
    "activity",
    "interested",
    "creating",
    "lot",
    "tables",
    "using",
    "queries",
    "case",
    "could",
    "dump",
    "data",
    "sdfs",
    "could",
    "create",
    "table",
    "pointing",
    "particular",
    "directory",
    "multiple",
    "directories",
    "could",
    "testing",
    "tables",
    "would",
    "decide",
    "might",
    "need",
    "tables",
    "case",
    "would",
    "advisable",
    "create",
    "external",
    "tables",
    "even",
    "table",
    "later",
    "dropped",
    "data",
    "sdfs",
    "intact",
    "unlike",
    "manage",
    "table",
    "dropping",
    "table",
    "delete",
    "data",
    "sdfs",
    "also",
    "let",
    "learn",
    "little",
    "bit",
    "partition",
    "partition",
    "hive",
    "partitioning",
    "required",
    "hive",
    "somebody",
    "asks",
    "normally",
    "world",
    "rdbms",
    "partitioning",
    "process",
    "group",
    "similar",
    "type",
    "data",
    "together",
    "usually",
    "done",
    "basis",
    "column",
    "call",
    "partitioning",
    "key",
    "table",
    "usually",
    "one",
    "column",
    "context",
    "rdbms",
    "could",
    "used",
    "partition",
    "data",
    "avoid",
    "scanning",
    "complete",
    "table",
    "query",
    "restrict",
    "scan",
    "set",
    "data",
    "particular",
    "partition",
    "hive",
    "number",
    "partition",
    "keys",
    "partitioning",
    "provides",
    "granularity",
    "hive",
    "table",
    "reduces",
    "query",
    "latency",
    "scanning",
    "relevant",
    "partition",
    "data",
    "instead",
    "whole",
    "data",
    "set",
    "partition",
    "various",
    "levels",
    "compare",
    "rdbms",
    "hive",
    "case",
    "rdbms",
    "could",
    "one",
    "column",
    "could",
    "used",
    "partitioning",
    "could",
    "squaring",
    "specific",
    "partition",
    "case",
    "rdbms",
    "partition",
    "column",
    "usually",
    "part",
    "table",
    "definition",
    "example",
    "employee",
    "table",
    "might",
    "employee",
    "id",
    "employee",
    "name",
    "employee",
    "age",
    "employee",
    "salary",
    "four",
    "columns",
    "would",
    "decide",
    "partition",
    "table",
    "based",
    "salary",
    "column",
    "would",
    "partition",
    "feel",
    "employee",
    "table",
    "growing",
    "fast",
    "huge",
    "amount",
    "data",
    "later",
    "query",
    "table",
    "want",
    "scan",
    "complete",
    "table",
    "could",
    "split",
    "data",
    "multiple",
    "partition",
    "based",
    "salary",
    "column",
    "giving",
    "ranges",
    "hive",
    "little",
    "different",
    "hive",
    "partitioning",
    "concept",
    "static",
    "dynamic",
    "partitioning",
    "hive",
    "partition",
    "column",
    "part",
    "table",
    "definition",
    "might",
    "employee",
    "table",
    "employee",
    "id",
    "name",
    "age",
    "would",
    "table",
    "definition",
    "could",
    "partitioning",
    "done",
    "based",
    "salary",
    "column",
    "create",
    "specific",
    "folder",
    "sdfs",
    "case",
    "query",
    "data",
    "see",
    "partition",
    "column",
    "also",
    "showing",
    "partition",
    "transaction",
    "data",
    "bank",
    "example",
    "based",
    "month",
    "like",
    "chan",
    "feb",
    "etc",
    "operation",
    "regarding",
    "particular",
    "month",
    "allow",
    "us",
    "query",
    "particular",
    "folder",
    "partitioning",
    "useful",
    "hive",
    "store",
    "metadata",
    "information",
    "sdfs",
    "somebody",
    "asks",
    "know",
    "hives",
    "data",
    "stored",
    "sdfs",
    "hadoop",
    "distributed",
    "file",
    "system",
    "however",
    "metadata",
    "either",
    "stored",
    "locally",
    "mode",
    "hive",
    "would",
    "called",
    "embedded",
    "mode",
    "could",
    "hives",
    "metadata",
    "stored",
    "rdbms",
    "multiple",
    "clients",
    "initiate",
    "connection",
    "metadata",
    "important",
    "hive",
    "would",
    "stored",
    "sdfs",
    "already",
    "know",
    "sdfs",
    "read",
    "write",
    "operations",
    "time",
    "consuming",
    "distributed",
    "file",
    "system",
    "accommodate",
    "huge",
    "amount",
    "data",
    "hive",
    "stores",
    "metadata",
    "information",
    "meta",
    "store",
    "using",
    "rdbms",
    "instead",
    "sdfs",
    "allows",
    "achieve",
    "low",
    "latency",
    "faster",
    "data",
    "access",
    "somebody",
    "asks",
    "components",
    "used",
    "hive",
    "query",
    "processor",
    "usually",
    "main",
    "components",
    "parser",
    "execution",
    "engine",
    "logical",
    "plan",
    "generation",
    "optimizer",
    "type",
    "checking",
    "whenever",
    "query",
    "submitted",
    "go",
    "parser",
    "parser",
    "would",
    "check",
    "syntax",
    "would",
    "check",
    "objects",
    "queried",
    "things",
    "see",
    "query",
    "fine",
    "internally",
    "semantic",
    "analyzer",
    "also",
    "look",
    "query",
    "execution",
    "engine",
    "basically",
    "work",
    "execution",
    "part",
    "best",
    "generated",
    "execution",
    "plan",
    "could",
    "used",
    "get",
    "results",
    "query",
    "could",
    "also",
    "user",
    "defined",
    "functions",
    "user",
    "would",
    "want",
    "use",
    "normally",
    "created",
    "java",
    "java",
    "programming",
    "language",
    "basically",
    "user",
    "defined",
    "functions",
    "added",
    "class",
    "path",
    "would",
    "logical",
    "plan",
    "generation",
    "basically",
    "looks",
    "query",
    "generates",
    "logical",
    "plan",
    "best",
    "execution",
    "path",
    "would",
    "required",
    "get",
    "results",
    "internally",
    "physical",
    "plan",
    "generated",
    "looked",
    "optimizer",
    "get",
    "best",
    "path",
    "get",
    "data",
    "might",
    "also",
    "checking",
    "different",
    "operators",
    "using",
    "within",
    "query",
    "finally",
    "would",
    "also",
    "type",
    "checking",
    "important",
    "components",
    "hype",
    "somebody",
    "might",
    "ask",
    "querying",
    "data",
    "using",
    "hive",
    "different",
    "components",
    "involved",
    "could",
    "explain",
    "different",
    "components",
    "work",
    "query",
    "submitted",
    "components",
    "let",
    "look",
    "scenario",
    "based",
    "question",
    "suppose",
    "lot",
    "small",
    "csv",
    "files",
    "present",
    "sdfs",
    "directory",
    "want",
    "create",
    "single",
    "hive",
    "table",
    "files",
    "data",
    "files",
    "fields",
    "like",
    "registration",
    "number",
    "name",
    "email",
    "address",
    "needs",
    "done",
    "approach",
    "solve",
    "create",
    "single",
    "hive",
    "table",
    "lots",
    "small",
    "files",
    "without",
    "degrading",
    "performance",
    "system",
    "different",
    "approaches",
    "know",
    "lot",
    "small",
    "csv",
    "files",
    "present",
    "directory",
    "know",
    "create",
    "table",
    "hive",
    "use",
    "location",
    "parameter",
    "could",
    "say",
    "create",
    "table",
    "give",
    "table",
    "name",
    "give",
    "column",
    "data",
    "types",
    "could",
    "specify",
    "delimiters",
    "finally",
    "could",
    "say",
    "location",
    "point",
    "directory",
    "sdfs",
    "directory",
    "might",
    "directory",
    "lot",
    "csv",
    "files",
    "case",
    "avoid",
    "loading",
    "data",
    "table",
    "table",
    "point",
    "table",
    "pointing",
    "directory",
    "directly",
    "pick",
    "data",
    "one",
    "multiple",
    "files",
    "also",
    "know",
    "hive",
    "schema",
    "check",
    "read",
    "schema",
    "check",
    "write",
    "case",
    "one",
    "two",
    "files",
    "follow",
    "schema",
    "table",
    "would",
    "prevent",
    "data",
    "loading",
    "data",
    "would",
    "anyways",
    "loaded",
    "query",
    "data",
    "might",
    "show",
    "null",
    "values",
    "data",
    "loaded",
    "follow",
    "schema",
    "table",
    "one",
    "approach",
    "approach",
    "let",
    "look",
    "think",
    "sequence",
    "file",
    "format",
    "basically",
    "smart",
    "format",
    "binary",
    "format",
    "group",
    "small",
    "files",
    "together",
    "form",
    "sequence",
    "file",
    "could",
    "one",
    "smarter",
    "approach",
    "could",
    "create",
    "temporary",
    "table",
    "could",
    "say",
    "create",
    "table",
    "give",
    "table",
    "name",
    "give",
    "column",
    "names",
    "data",
    "types",
    "could",
    "specify",
    "delimiters",
    "shows",
    "row",
    "format",
    "fields",
    "terminated",
    "finally",
    "store",
    "text",
    "file",
    "load",
    "data",
    "table",
    "giving",
    "local",
    "file",
    "system",
    "path",
    "create",
    "table",
    "store",
    "data",
    "sequence",
    "file",
    "format",
    "point",
    "one",
    "storing",
    "data",
    "instead",
    "text",
    "file",
    "point",
    "three",
    "would",
    "storing",
    "data",
    "sequence",
    "file",
    "format",
    "say",
    "create",
    "table",
    "give",
    "specifications",
    "say",
    "row",
    "format",
    "delimited",
    "fields",
    "terminated",
    "comma",
    "stored",
    "sequence",
    "file",
    "move",
    "data",
    "test",
    "table",
    "test",
    "sequence",
    "file",
    "table",
    "could",
    "say",
    "insert",
    "overwrite",
    "new",
    "table",
    "select",
    "star",
    "table",
    "remember",
    "hive",
    "insert",
    "update",
    "delete",
    "however",
    "table",
    "existing",
    "insert",
    "overwrite",
    "existing",
    "table",
    "new",
    "table",
    "could",
    "one",
    "approach",
    "could",
    "lot",
    "csv",
    "files",
    "smaller",
    "files",
    "club",
    "together",
    "one",
    "big",
    "sequence",
    "file",
    "store",
    "table",
    "somebody",
    "asks",
    "write",
    "query",
    "insert",
    "new",
    "column",
    "integer",
    "data",
    "type",
    "hive",
    "table",
    "requirement",
    "might",
    "would",
    "want",
    "insert",
    "table",
    "position",
    "existing",
    "column",
    "possible",
    "alter",
    "table",
    "giving",
    "table",
    "name",
    "specifying",
    "change",
    "column",
    "giving",
    "new",
    "column",
    "data",
    "type",
    "existing",
    "column",
    "simple",
    "way",
    "wherein",
    "insert",
    "new",
    "column",
    "hive",
    "table",
    "key",
    "differences",
    "hive",
    "pig",
    "might",
    "heard",
    "hive",
    "visit",
    "data",
    "warehousing",
    "package",
    "pig",
    "scripting",
    "language",
    "used",
    "data",
    "analysis",
    "trend",
    "detection",
    "hypothesis",
    "testing",
    "data",
    "transformation",
    "many",
    "use",
    "cases",
    "compare",
    "hive",
    "pig",
    "hive",
    "uses",
    "declarative",
    "language",
    "called",
    "hiveql",
    "hive",
    "querying",
    "language",
    "similar",
    "sql",
    "reporting",
    "data",
    "analysis",
    "even",
    "data",
    "transformation",
    "data",
    "extraction",
    "pig",
    "uses",
    "high",
    "level",
    "procedural",
    "language",
    "called",
    "pig",
    "latin",
    "programming",
    "remember",
    "use",
    "mapreduce",
    "processing",
    "framework",
    "run",
    "query",
    "hive",
    "process",
    "data",
    "create",
    "submit",
    "big",
    "script",
    "trigger",
    "mapreduce",
    "job",
    "unless",
    "set",
    "local",
    "mode",
    "hive",
    "operates",
    "server",
    "side",
    "cluster",
    "basically",
    "works",
    "structured",
    "data",
    "data",
    "structuralized",
    "pig",
    "usually",
    "works",
    "operates",
    "client",
    "side",
    "cluster",
    "allows",
    "structured",
    "unstructured",
    "even",
    "could",
    "say",
    "data",
    "hive",
    "support",
    "avro",
    "file",
    "format",
    "default",
    "however",
    "done",
    "using",
    "write",
    "serializer",
    "serializer",
    "hive",
    "table",
    "related",
    "data",
    "stored",
    "avro",
    "format",
    "sequence",
    "file",
    "format",
    "part",
    "k",
    "format",
    "even",
    "text",
    "file",
    "format",
    "however",
    "working",
    "smarter",
    "formats",
    "like",
    "avro",
    "sequence",
    "file",
    "parquet",
    "might",
    "use",
    "specific",
    "serializers",
    "serializers",
    "avro",
    "package",
    "allows",
    "us",
    "use",
    "avro",
    "format",
    "pig",
    "supports",
    "avro",
    "format",
    "default",
    "hive",
    "developed",
    "facebook",
    "supports",
    "partitioning",
    "pig",
    "developed",
    "yahoo",
    "support",
    "partitioning",
    "high",
    "level",
    "differences",
    "lots",
    "lots",
    "differences",
    "remember",
    "hive",
    "data",
    "housing",
    "package",
    "pig",
    "scripting",
    "language",
    "strictly",
    "procedural",
    "flow",
    "following",
    "scripting",
    "language",
    "allows",
    "us",
    "process",
    "data",
    "let",
    "get",
    "let",
    "get",
    "deeper",
    "learn",
    "pig",
    "mentioned",
    "scripting",
    "language",
    "used",
    "data",
    "processing",
    "also",
    "uses",
    "mapreduce",
    "although",
    "even",
    "big",
    "run",
    "local",
    "mode",
    "let",
    "learn",
    "pig",
    "next",
    "section",
    "let",
    "learn",
    "questions",
    "pig",
    "scripting",
    "language",
    "extensively",
    "used",
    "data",
    "processing",
    "data",
    "analysis",
    "question",
    "apache",
    "pig",
    "different",
    "mapreduce",
    "know",
    "mapreduce",
    "programming",
    "model",
    "quite",
    "rigid",
    "comes",
    "processing",
    "data",
    "mapping",
    "reducing",
    "write",
    "huge",
    "code",
    "usually",
    "mapreduce",
    "written",
    "java",
    "also",
    "written",
    "python",
    "written",
    "scala",
    "another",
    "programming",
    "languages",
    "compare",
    "pig",
    "mapreduce",
    "pig",
    "obviously",
    "concise",
    "less",
    "lines",
    "code",
    "compared",
    "mapreduce",
    "also",
    "know",
    "pig",
    "script",
    "internally",
    "trigger",
    "mapreduce",
    "job",
    "however",
    "user",
    "need",
    "know",
    "mapreduce",
    "programming",
    "model",
    "simply",
    "write",
    "simple",
    "scripts",
    "pig",
    "automatically",
    "converted",
    "mapreduce",
    "however",
    "mapreduce",
    "lines",
    "code",
    "peak",
    "high",
    "level",
    "language",
    "easily",
    "perform",
    "join",
    "operations",
    "data",
    "processing",
    "operations",
    "mapreduce",
    "low",
    "level",
    "language",
    "perform",
    "job",
    "join",
    "operations",
    "easily",
    "join",
    "using",
    "mapreduce",
    "however",
    "really",
    "easy",
    "comparison",
    "pick",
    "said",
    "execution",
    "every",
    "pig",
    "operator",
    "converted",
    "internally",
    "mapreduce",
    "job",
    "every",
    "big",
    "script",
    "run",
    "would",
    "converted",
    "mapreduce",
    "job",
    "map",
    "reduce",
    "overall",
    "batch",
    "oriented",
    "processing",
    "takes",
    "time",
    "compile",
    "takes",
    "time",
    "execute",
    "either",
    "run",
    "mapreduce",
    "job",
    "triggered",
    "pixscript",
    "pig",
    "works",
    "versions",
    "hadoop",
    "talk",
    "mapreduce",
    "program",
    "written",
    "one",
    "hadoop",
    "version",
    "may",
    "work",
    "versions",
    "might",
    "work",
    "might",
    "depends",
    "dependencies",
    "compiler",
    "using",
    "programming",
    "language",
    "used",
    "version",
    "hadoop",
    "working",
    "main",
    "differences",
    "apache",
    "pig",
    "mapreduce",
    "different",
    "ways",
    "executing",
    "pig",
    "script",
    "could",
    "create",
    "script",
    "file",
    "store",
    "dot",
    "pic",
    "dot",
    "text",
    "could",
    "execute",
    "using",
    "pick",
    "command",
    "could",
    "bringing",
    "grunt",
    "shell",
    "pig",
    "shell",
    "usually",
    "starts",
    "mapreduce",
    "mode",
    "also",
    "bring",
    "local",
    "mode",
    "also",
    "run",
    "pig",
    "embedded",
    "embedded",
    "script",
    "programming",
    "language",
    "different",
    "ways",
    "executing",
    "big",
    "script",
    "major",
    "components",
    "pig",
    "execution",
    "environment",
    "common",
    "question",
    "interviewers",
    "would",
    "always",
    "want",
    "know",
    "different",
    "components",
    "hive",
    "different",
    "components",
    "pig",
    "even",
    "different",
    "components",
    "involved",
    "hadoop",
    "ecosystem",
    "want",
    "learn",
    "major",
    "components",
    "pig",
    "execution",
    "environment",
    "pig",
    "scripts",
    "written",
    "pig",
    "latin",
    "using",
    "operators",
    "functions",
    "submitted",
    "execution",
    "environment",
    "happens",
    "would",
    "want",
    "process",
    "data",
    "using",
    "pic",
    "parser",
    "type",
    "checking",
    "checks",
    "syntax",
    "script",
    "output",
    "parser",
    "tag",
    "direct",
    "cyclic",
    "graph",
    "block",
    "wikipedia",
    "dag",
    "dag",
    "basically",
    "sequence",
    "steps",
    "run",
    "one",
    "direction",
    "optimizer",
    "optimizer",
    "performs",
    "optimization",
    "using",
    "merge",
    "transform",
    "split",
    "etc",
    "aims",
    "reduce",
    "amount",
    "data",
    "pipeline",
    "whole",
    "purpose",
    "optimizer",
    "internal",
    "compiler",
    "pick",
    "compiler",
    "converts",
    "optimized",
    "code",
    "mapreduce",
    "job",
    "user",
    "need",
    "know",
    "mapreduce",
    "programming",
    "model",
    "works",
    "written",
    "need",
    "know",
    "running",
    "big",
    "script",
    "would",
    "internally",
    "converted",
    "mapreduce",
    "job",
    "finally",
    "execution",
    "engine",
    "mapreduce",
    "jobs",
    "submitted",
    "execution",
    "engine",
    "generate",
    "desired",
    "results",
    "major",
    "components",
    "pig",
    "execution",
    "environment",
    "let",
    "learn",
    "different",
    "complex",
    "data",
    "types",
    "pig",
    "supports",
    "various",
    "data",
    "types",
    "main",
    "ones",
    "tuple",
    "bag",
    "map",
    "tuple",
    "tuple",
    "might",
    "heard",
    "tuple",
    "ordered",
    "set",
    "fields",
    "contain",
    "different",
    "data",
    "types",
    "field",
    "array",
    "would",
    "multiple",
    "elements",
    "would",
    "types",
    "list",
    "also",
    "different",
    "types",
    "tuple",
    "collection",
    "different",
    "fields",
    "field",
    "different",
    "type",
    "could",
    "example",
    "one",
    "comma",
    "three",
    "one",
    "comma",
    "three",
    "comma",
    "string",
    "float",
    "element",
    "form",
    "tuple",
    "bag",
    "set",
    "tuples",
    "represented",
    "curly",
    "braces",
    "could",
    "also",
    "imagine",
    "like",
    "dictionary",
    "various",
    "different",
    "collection",
    "elements",
    "map",
    "map",
    "set",
    "key",
    "value",
    "pairs",
    "used",
    "represent",
    "data",
    "work",
    "big",
    "data",
    "field",
    "need",
    "know",
    "different",
    "data",
    "types",
    "supported",
    "pig",
    "supported",
    "hive",
    "supported",
    "components",
    "hadoop",
    "tuple",
    "tag",
    "map",
    "array",
    "array",
    "buffer",
    "think",
    "list",
    "think",
    "dictionaries",
    "think",
    "map",
    "key",
    "value",
    "pair",
    "different",
    "complex",
    "data",
    "types",
    "primitive",
    "data",
    "type",
    "integer",
    "character",
    "string",
    "boolean",
    "float",
    "various",
    "diagnostic",
    "operators",
    "available",
    "apache",
    "pic",
    "operators",
    "options",
    "give",
    "pic",
    "script",
    "dumb",
    "dumb",
    "operator",
    "runs",
    "pig",
    "latin",
    "scripts",
    "displays",
    "result",
    "screen",
    "either",
    "could",
    "dumb",
    "see",
    "output",
    "screen",
    "even",
    "dump",
    "could",
    "store",
    "output",
    "particular",
    "file",
    "load",
    "data",
    "using",
    "load",
    "operator",
    "pig",
    "pig",
    "also",
    "different",
    "internal",
    "storage",
    "like",
    "json",
    "loader",
    "big",
    "storage",
    "used",
    "working",
    "specific",
    "kind",
    "data",
    "could",
    "dump",
    "either",
    "processing",
    "processing",
    "dump",
    "would",
    "produce",
    "result",
    "result",
    "could",
    "stored",
    "file",
    "seen",
    "screen",
    "also",
    "describe",
    "operator",
    "used",
    "view",
    "schema",
    "relation",
    "load",
    "data",
    "view",
    "schema",
    "relation",
    "using",
    "describe",
    "operator",
    "might",
    "already",
    "know",
    "displays",
    "physical",
    "logical",
    "mapreduce",
    "execution",
    "plans",
    "normally",
    "rdbms",
    "use",
    "would",
    "like",
    "see",
    "happens",
    "behind",
    "scenes",
    "particular",
    "script",
    "query",
    "runs",
    "could",
    "load",
    "data",
    "using",
    "load",
    "operator",
    "case",
    "would",
    "want",
    "display",
    "logical",
    "physical",
    "mapreduce",
    "execution",
    "plans",
    "could",
    "use",
    "explain",
    "operator",
    "also",
    "illustrate",
    "operator",
    "gives",
    "step",
    "step",
    "execution",
    "sequence",
    "statements",
    "sometimes",
    "would",
    "want",
    "analyze",
    "script",
    "see",
    "good",
    "bad",
    "would",
    "really",
    "serve",
    "purpose",
    "could",
    "use",
    "illustrate",
    "test",
    "loading",
    "data",
    "using",
    "load",
    "operator",
    "could",
    "use",
    "illustrate",
    "operator",
    "look",
    "step",
    "step",
    "execution",
    "sequence",
    "statements",
    "would",
    "want",
    "execute",
    "different",
    "diagnostic",
    "operators",
    "available",
    "apache",
    "pic",
    "somebody",
    "asks",
    "state",
    "usage",
    "group",
    "order",
    "distinct",
    "keywords",
    "big",
    "script",
    "said",
    "pig",
    "scripting",
    "language",
    "could",
    "use",
    "various",
    "operators",
    "group",
    "basically",
    "collects",
    "various",
    "records",
    "key",
    "groups",
    "data",
    "one",
    "relations",
    "example",
    "could",
    "group",
    "data",
    "basically",
    "variable",
    "give",
    "name",
    "say",
    "group",
    "relation",
    "name",
    "age",
    "say",
    "file",
    "field",
    "various",
    "fields",
    "one",
    "field",
    "relation",
    "name",
    "could",
    "group",
    "different",
    "field",
    "order",
    "used",
    "display",
    "contents",
    "relation",
    "sorted",
    "order",
    "whether",
    "ascending",
    "descending",
    "could",
    "create",
    "variable",
    "called",
    "relation",
    "two",
    "could",
    "say",
    "order",
    "relation",
    "name",
    "one",
    "ascending",
    "descending",
    "order",
    "distinct",
    "basically",
    "removes",
    "duplicate",
    "records",
    "implemented",
    "entire",
    "records",
    "individual",
    "records",
    "would",
    "want",
    "find",
    "distinct",
    "values",
    "relation",
    "name",
    "field",
    "could",
    "use",
    "district",
    "relational",
    "operators",
    "pig",
    "various",
    "relational",
    "operators",
    "help",
    "data",
    "scientists",
    "data",
    "analysts",
    "developers",
    "analyzing",
    "data",
    "joins",
    "two",
    "tables",
    "performs",
    "group",
    "operation",
    "join",
    "table",
    "result",
    "cross",
    "used",
    "compute",
    "cross",
    "product",
    "cartesian",
    "product",
    "two",
    "relations",
    "basically",
    "iteration",
    "iterate",
    "tuples",
    "relation",
    "generating",
    "data",
    "transformation",
    "example",
    "say",
    "variable",
    "equals",
    "load",
    "file",
    "could",
    "create",
    "variable",
    "called",
    "b",
    "could",
    "say",
    "would",
    "want",
    "something",
    "say",
    "group",
    "join",
    "join",
    "two",
    "tables",
    "relation",
    "limit",
    "limit",
    "number",
    "output",
    "tuples",
    "output",
    "results",
    "split",
    "split",
    "relation",
    "two",
    "relations",
    "union",
    "get",
    "combination",
    "merge",
    "contents",
    "two",
    "relations",
    "order",
    "get",
    "sorted",
    "result",
    "relational",
    "operators",
    "extensively",
    "used",
    "pig",
    "analysis",
    "use",
    "filters",
    "apache",
    "pic",
    "say",
    "example",
    "data",
    "three",
    "fields",
    "year",
    "product",
    "quantity",
    "phone",
    "sales",
    "data",
    "filter",
    "operator",
    "could",
    "used",
    "select",
    "required",
    "values",
    "relation",
    "based",
    "condition",
    "also",
    "allows",
    "remove",
    "unwanted",
    "records",
    "data",
    "file",
    "example",
    "filter",
    "products",
    "quantity",
    "greater",
    "thousand",
    "see",
    "one",
    "row",
    "wherein",
    "multiple",
    "rows",
    "quantity",
    "greater",
    "thousand",
    "1500",
    "1700",
    "1200",
    "could",
    "create",
    "variable",
    "called",
    "would",
    "load",
    "file",
    "using",
    "pick",
    "storage",
    "explained",
    "earlier",
    "pick",
    "storage",
    "internal",
    "parameter",
    "used",
    "specify",
    "delimiters",
    "delimiter",
    "comma",
    "could",
    "say",
    "using",
    "pick",
    "storage",
    "could",
    "specify",
    "data",
    "type",
    "field",
    "integer",
    "product",
    "character",
    "array",
    "quantity",
    "integer",
    "b",
    "could",
    "say",
    "filter",
    "whatever",
    "quantity",
    "greater",
    "thousand",
    "concise",
    "simple",
    "allows",
    "us",
    "extract",
    "process",
    "data",
    "simpler",
    "way",
    "suppose",
    "file",
    "called",
    "150",
    "records",
    "hdfs",
    "file",
    "stored",
    "dfs",
    "150",
    "records",
    "consider",
    "every",
    "record",
    "one",
    "line",
    "somebody",
    "asks",
    "write",
    "pick",
    "command",
    "retrieve",
    "first",
    "10",
    "records",
    "file",
    "first",
    "load",
    "data",
    "could",
    "create",
    "variable",
    "called",
    "test",
    "underscore",
    "data",
    "would",
    "say",
    "load",
    "file",
    "using",
    "pick",
    "storage",
    "specifying",
    "delimiter",
    "comma",
    "could",
    "specify",
    "fields",
    "whatever",
    "fields",
    "file",
    "would",
    "want",
    "get",
    "10",
    "records",
    "could",
    "use",
    "limit",
    "operator",
    "could",
    "say",
    "limit",
    "test",
    "data",
    "give",
    "10",
    "records",
    "simple",
    "extract",
    "10",
    "records",
    "150",
    "records",
    "stored",
    "file",
    "sdfs",
    "learnt",
    "pig",
    "learned",
    "questions",
    "hive",
    "could",
    "always",
    "look",
    "books",
    "like",
    "programming",
    "hive",
    "programming",
    "pig",
    "look",
    "examples",
    "try",
    "examples",
    "existing",
    "hadoop",
    "setup",
    "let",
    "learn",
    "hbase",
    "nosql",
    "database",
    "edge",
    "base",
    "four",
    "dimensional",
    "database",
    "comparison",
    "rdbms",
    "usually",
    "dimensional",
    "rdbms",
    "rows",
    "columns",
    "hbase",
    "four",
    "coordinates",
    "row",
    "key",
    "always",
    "unique",
    "column",
    "family",
    "number",
    "column",
    "qualifiers",
    "number",
    "per",
    "column",
    "family",
    "version",
    "four",
    "coordinates",
    "make",
    "edge",
    "base",
    "four",
    "dimensional",
    "key",
    "value",
    "store",
    "column",
    "family",
    "store",
    "unique",
    "storing",
    "huge",
    "amount",
    "data",
    "extracting",
    "data",
    "hbase",
    "good",
    "link",
    "would",
    "suggest",
    "everyone",
    "look",
    "would",
    "want",
    "learn",
    "hbase",
    "could",
    "say",
    "hbase",
    "mapper",
    "basically",
    "brings",
    "documentation",
    "mapr",
    "specific",
    "mapper",
    "look",
    "link",
    "give",
    "detailed",
    "explanation",
    "hbase",
    "works",
    "architectural",
    "components",
    "data",
    "stored",
    "makes",
    "hbase",
    "powerful",
    "nosql",
    "database",
    "let",
    "learn",
    "important",
    "critical",
    "questions",
    "hbase",
    "might",
    "asked",
    "interviewer",
    "interview",
    "applying",
    "big",
    "data",
    "admin",
    "developer",
    "position",
    "role",
    "key",
    "components",
    "edge",
    "base",
    "said",
    "one",
    "favorite",
    "questions",
    "interviewers",
    "would",
    "want",
    "understand",
    "knowledge",
    "different",
    "components",
    "particular",
    "service",
    "hbase",
    "said",
    "nosql",
    "database",
    "comes",
    "part",
    "service",
    "cloudera",
    "hortonworks",
    "apache",
    "hadoop",
    "could",
    "also",
    "set",
    "edge",
    "base",
    "independent",
    "package",
    "key",
    "components",
    "hbase",
    "hbase",
    "region",
    "server",
    "edge",
    "base",
    "follows",
    "similar",
    "kind",
    "topology",
    "like",
    "hadoop",
    "hadoop",
    "master",
    "process",
    "name",
    "node",
    "slave",
    "processes",
    "data",
    "nodes",
    "secondary",
    "name",
    "node",
    "way",
    "hbase",
    "also",
    "master",
    "h",
    "master",
    "slave",
    "processes",
    "called",
    "region",
    "servers",
    "region",
    "servers",
    "usually",
    "data",
    "nodes",
    "however",
    "mandatory",
    "100",
    "data",
    "nodes",
    "would",
    "100",
    "region",
    "servers",
    "purely",
    "depends",
    "admin",
    "region",
    "server",
    "contain",
    "region",
    "server",
    "contains",
    "hbase",
    "tables",
    "divided",
    "horizontally",
    "regions",
    "could",
    "say",
    "group",
    "rows",
    "called",
    "regions",
    "edge",
    "base",
    "two",
    "aspects",
    "one",
    "group",
    "columns",
    "called",
    "column",
    "family",
    "one",
    "group",
    "rows",
    "called",
    "regions",
    "regions",
    "rows",
    "grouped",
    "based",
    "key",
    "values",
    "would",
    "say",
    "row",
    "keys",
    "always",
    "unique",
    "store",
    "data",
    "base",
    "would",
    "data",
    "form",
    "rows",
    "columns",
    "group",
    "rows",
    "called",
    "regions",
    "could",
    "say",
    "horizontal",
    "partitions",
    "table",
    "region",
    "server",
    "manages",
    "regions",
    "node",
    "data",
    "node",
    "running",
    "region",
    "server",
    "thousand",
    "regions",
    "runs",
    "every",
    "node",
    "decides",
    "size",
    "region",
    "region",
    "server",
    "said",
    "slave",
    "process",
    "responsible",
    "managing",
    "hbase",
    "data",
    "node",
    "region",
    "server",
    "worker",
    "node",
    "worker",
    "process",
    "data",
    "node",
    "take",
    "care",
    "read",
    "write",
    "update",
    "delete",
    "request",
    "clients",
    "talk",
    "components",
    "edge",
    "base",
    "said",
    "hp",
    "h",
    "master",
    "would",
    "always",
    "connection",
    "coming",
    "client",
    "application",
    "h",
    "master",
    "assigns",
    "regions",
    "monitors",
    "region",
    "servers",
    "assigns",
    "regions",
    "region",
    "servers",
    "load",
    "balancing",
    "without",
    "help",
    "zookeeper",
    "talk",
    "components",
    "hbase",
    "three",
    "main",
    "components",
    "zookeeper",
    "h",
    "master",
    "region",
    "server",
    "region",
    "server",
    "slave",
    "process",
    "edge",
    "master",
    "master",
    "process",
    "takes",
    "care",
    "table",
    "operations",
    "assigning",
    "regions",
    "region",
    "servers",
    "taking",
    "care",
    "read",
    "write",
    "requests",
    "come",
    "client",
    "edge",
    "master",
    "take",
    "help",
    "zookeeper",
    "centralized",
    "coordination",
    "service",
    "whenever",
    "client",
    "wants",
    "read",
    "write",
    "change",
    "schema",
    "metadata",
    "operations",
    "contact",
    "hmaster",
    "edge",
    "master",
    "internally",
    "contact",
    "zookeeper",
    "could",
    "hbase",
    "setup",
    "also",
    "high",
    "availability",
    "mode",
    "could",
    "active",
    "edge",
    "master",
    "backup",
    "etch",
    "master",
    "would",
    "zookeeper",
    "quorum",
    "way",
    "zookeeper",
    "works",
    "zookeeper",
    "centralized",
    "coordination",
    "service",
    "always",
    "run",
    "quorum",
    "processes",
    "zookeeper",
    "would",
    "always",
    "run",
    "odd",
    "number",
    "processes",
    "3",
    "5",
    "7",
    "zookeeper",
    "works",
    "concept",
    "majority",
    "consensus",
    "zookeeper",
    "centralized",
    "coordination",
    "service",
    "keeping",
    "track",
    "servers",
    "alive",
    "available",
    "also",
    "keeps",
    "track",
    "status",
    "every",
    "server",
    "zookeeper",
    "monitoring",
    "zookeeper",
    "keeps",
    "session",
    "alive",
    "particular",
    "server",
    "edge",
    "master",
    "would",
    "always",
    "check",
    "zookeeper",
    "region",
    "servers",
    "available",
    "alive",
    "regions",
    "assigned",
    "region",
    "server",
    "one",
    "end",
    "region",
    "server",
    "sending",
    "status",
    "zookeeper",
    "indicating",
    "ready",
    "kind",
    "read",
    "write",
    "operation",
    "end",
    "h",
    "master",
    "querying",
    "zookeeper",
    "check",
    "status",
    "zookeeper",
    "internally",
    "manages",
    "meta",
    "table",
    "meta",
    "table",
    "information",
    "regions",
    "residing",
    "region",
    "server",
    "row",
    "keys",
    "regions",
    "contain",
    "case",
    "read",
    "activity",
    "hmaster",
    "query",
    "zookeeper",
    "find",
    "region",
    "server",
    "contains",
    "meta",
    "table",
    "edge",
    "master",
    "gets",
    "information",
    "meta",
    "table",
    "look",
    "meta",
    "table",
    "find",
    "row",
    "keys",
    "corresponding",
    "region",
    "servers",
    "contain",
    "regions",
    "row",
    "keys",
    "would",
    "want",
    "understand",
    "rookie",
    "column",
    "families",
    "base",
    "let",
    "look",
    "would",
    "good",
    "could",
    "look",
    "excel",
    "sheet",
    "row",
    "key",
    "always",
    "unique",
    "acts",
    "primary",
    "key",
    "hbase",
    "table",
    "allows",
    "logical",
    "grouping",
    "cells",
    "make",
    "sure",
    "cells",
    "row",
    "key",
    "server",
    "said",
    "four",
    "coordinates",
    "hbase",
    "row",
    "key",
    "always",
    "unique",
    "column",
    "families",
    "nothing",
    "group",
    "columns",
    "say",
    "column",
    "families",
    "one",
    "column",
    "family",
    "number",
    "columns",
    "talk",
    "h",
    "base",
    "h",
    "base",
    "four",
    "dimensional",
    "terms",
    "edge",
    "base",
    "also",
    "called",
    "column",
    "oriented",
    "database",
    "basically",
    "means",
    "every",
    "row",
    "one",
    "column",
    "could",
    "different",
    "data",
    "type",
    "row",
    "key",
    "uniquely",
    "identifies",
    "row",
    "column",
    "families",
    "could",
    "one",
    "many",
    "depending",
    "table",
    "defined",
    "column",
    "family",
    "number",
    "columns",
    "could",
    "say",
    "every",
    "row",
    "within",
    "column",
    "family",
    "could",
    "different",
    "number",
    "columns",
    "could",
    "say",
    "row",
    "1",
    "could",
    "two",
    "columns",
    "name",
    "city",
    "within",
    "column",
    "family",
    "row",
    "2",
    "could",
    "name",
    "city",
    "age",
    "designation",
    "salary",
    "third",
    "row",
    "could",
    "thousand",
    "columns",
    "could",
    "belong",
    "one",
    "column",
    "family",
    "horizontally",
    "scalable",
    "database",
    "column",
    "family",
    "consists",
    "group",
    "columns",
    "defined",
    "table",
    "creation",
    "column",
    "family",
    "number",
    "column",
    "qualifiers",
    "separated",
    "delimiter",
    "combination",
    "row",
    "key",
    "column",
    "family",
    "column",
    "qualifier",
    "name",
    "city",
    "age",
    "value",
    "within",
    "cell",
    "makes",
    "hbase",
    "unique",
    "four",
    "dimensional",
    "database",
    "information",
    "would",
    "want",
    "learn",
    "hbase",
    "please",
    "refer",
    "link",
    "hbase",
    "mapper",
    "gives",
    "complete",
    "hbase",
    "architecture",
    "three",
    "components",
    "name",
    "node",
    "three",
    "components",
    "name",
    "node",
    "region",
    "servers",
    "zookeeper",
    "works",
    "base",
    "edge",
    "master",
    "interacts",
    "zookeeper",
    "zookeeper",
    "coordination",
    "components",
    "working",
    "together",
    "hbs",
    "take",
    "care",
    "read",
    "write",
    "coming",
    "back",
    "continuing",
    "need",
    "disable",
    "table",
    "different",
    "table",
    "operations",
    "edge",
    "base",
    "one",
    "disabling",
    "table",
    "would",
    "want",
    "check",
    "status",
    "table",
    "could",
    "check",
    "disabled",
    "giving",
    "table",
    "name",
    "order",
    "enabled",
    "table",
    "name",
    "question",
    "need",
    "disable",
    "table",
    "would",
    "want",
    "modify",
    "table",
    "kind",
    "maintenance",
    "activity",
    "case",
    "disable",
    "table",
    "modify",
    "change",
    "settings",
    "table",
    "disabled",
    "accessed",
    "scan",
    "command",
    "write",
    "code",
    "open",
    "connection",
    "base",
    "interact",
    "hbs",
    "one",
    "could",
    "either",
    "use",
    "graphical",
    "user",
    "interface",
    "hue",
    "could",
    "using",
    "command",
    "line",
    "h",
    "based",
    "shell",
    "could",
    "using",
    "hbase",
    "admin",
    "api",
    "working",
    "java",
    "say",
    "happy",
    "base",
    "working",
    "python",
    "may",
    "want",
    "open",
    "connection",
    "hbase",
    "work",
    "base",
    "programmatically",
    "case",
    "create",
    "configuration",
    "object",
    "configuration",
    "conf",
    "create",
    "configuration",
    "object",
    "use",
    "different",
    "classes",
    "like",
    "etch",
    "table",
    "interface",
    "work",
    "new",
    "table",
    "could",
    "use",
    "h",
    "column",
    "qualifier",
    "many",
    "classes",
    "available",
    "hbase",
    "admin",
    "api",
    "replication",
    "mean",
    "terms",
    "hbase",
    "edge",
    "base",
    "said",
    "works",
    "cluster",
    "way",
    "talk",
    "cluster",
    "could",
    "always",
    "set",
    "replication",
    "one",
    "edge",
    "base",
    "cluster",
    "hbase",
    "cluster",
    "replication",
    "feature",
    "edge",
    "base",
    "provides",
    "mechanism",
    "copy",
    "data",
    "clusters",
    "sync",
    "data",
    "different",
    "clusters",
    "feature",
    "used",
    "disaster",
    "recovery",
    "solution",
    "provides",
    "high",
    "availability",
    "hbase",
    "hbase",
    "cluster",
    "one",
    "one",
    "master",
    "multiple",
    "region",
    "servers",
    "running",
    "hadoop",
    "cluster",
    "could",
    "use",
    "hadoop",
    "cluster",
    "create",
    "hbase",
    "replica",
    "cluster",
    "could",
    "totally",
    "different",
    "hbase",
    "replica",
    "cluster",
    "intention",
    "things",
    "changing",
    "particular",
    "table",
    "cluster",
    "1",
    "would",
    "want",
    "replicated",
    "across",
    "different",
    "clusters",
    "could",
    "alter",
    "hbase",
    "table",
    "set",
    "replication",
    "scope",
    "replication",
    "scope",
    "0",
    "indicates",
    "table",
    "replicated",
    "set",
    "replication",
    "1",
    "basically",
    "set",
    "ah",
    "base",
    "cluster",
    "replicate",
    "hbase",
    "tables",
    "data",
    "cluster",
    "one",
    "cluster",
    "commands",
    "used",
    "enable",
    "replication",
    "replicate",
    "data",
    "table",
    "across",
    "clusters",
    "import",
    "export",
    "hbase",
    "course",
    "possible",
    "import",
    "export",
    "tables",
    "one",
    "hbase",
    "cluster",
    "hbase",
    "cluster",
    "even",
    "within",
    "cluster",
    "use",
    "hbase",
    "export",
    "utility",
    "comes",
    "particular",
    "package",
    "give",
    "table",
    "name",
    "target",
    "location",
    "export",
    "data",
    "hbase",
    "table",
    "directory",
    "sdfs",
    "could",
    "create",
    "different",
    "table",
    "would",
    "follow",
    "kind",
    "definition",
    "table",
    "exported",
    "could",
    "use",
    "import",
    "import",
    "data",
    "directory",
    "sdfs",
    "table",
    "would",
    "want",
    "learn",
    "hbase",
    "import",
    "export",
    "could",
    "look",
    "hbase",
    "import",
    "operations",
    "let",
    "search",
    "link",
    "link",
    "could",
    "learn",
    "hbase",
    "import",
    "export",
    "utilities",
    "could",
    "bulk",
    "import",
    "bulk",
    "export",
    "internally",
    "uses",
    "mapreduce",
    "could",
    "import",
    "export",
    "hbs",
    "tables",
    "moving",
    "mean",
    "compaction",
    "hbase",
    "know",
    "hbase",
    "nosql",
    "database",
    "used",
    "store",
    "huge",
    "amount",
    "data",
    "however",
    "whenever",
    "data",
    "written",
    "hbase",
    "first",
    "returned",
    "call",
    "write",
    "ahead",
    "log",
    "also",
    "mem",
    "store",
    "write",
    "cache",
    "data",
    "written",
    "wall",
    "mem",
    "store",
    "offloaded",
    "form",
    "internal",
    "hbase",
    "format",
    "file",
    "called",
    "h5",
    "usually",
    "edge",
    "files",
    "small",
    "nature",
    "also",
    "know",
    "sdfs",
    "good",
    "talk",
    "number",
    "larger",
    "files",
    "comparison",
    "large",
    "number",
    "smaller",
    "files",
    "due",
    "limitation",
    "name",
    "node",
    "memory",
    "compaction",
    "process",
    "merging",
    "hbase",
    "files",
    "smaller",
    "edge",
    "files",
    "single",
    "large",
    "file",
    "done",
    "reduce",
    "amount",
    "memory",
    "required",
    "store",
    "files",
    "number",
    "disk",
    "seeks",
    "needed",
    "could",
    "lot",
    "edge",
    "files",
    "get",
    "created",
    "data",
    "written",
    "hbase",
    "smaller",
    "files",
    "compacted",
    "major",
    "minor",
    "compaction",
    "creating",
    "one",
    "big",
    "edge",
    "file",
    "internally",
    "would",
    "written",
    "sdfs",
    "sdfs",
    "format",
    "blocks",
    "benefit",
    "compaction",
    "also",
    "feature",
    "called",
    "bloom",
    "filter",
    "bloom",
    "filter",
    "work",
    "bloom",
    "filter",
    "hbase",
    "bloom",
    "filter",
    "mechanism",
    "test",
    "whether",
    "h",
    "file",
    "contains",
    "specific",
    "row",
    "row",
    "column",
    "cell",
    "bloom",
    "filter",
    "named",
    "creator",
    "burton",
    "hovered",
    "bloom",
    "data",
    "structure",
    "predicts",
    "whether",
    "given",
    "element",
    "member",
    "set",
    "data",
    "provides",
    "index",
    "structure",
    "reduces",
    "disk",
    "reads",
    "determines",
    "probability",
    "finding",
    "row",
    "particular",
    "file",
    "one",
    "useful",
    "features",
    "edge",
    "base",
    "allows",
    "faster",
    "access",
    "avoids",
    "disk",
    "seeks",
    "hbase",
    "concept",
    "name",
    "space",
    "namespace",
    "similar",
    "elements",
    "grouped",
    "together",
    "namespace",
    "yes",
    "support",
    "name",
    "space",
    "namespace",
    "logical",
    "grouping",
    "tables",
    "analogous",
    "database",
    "rdbms",
    "create",
    "hp",
    "namespace",
    "schema",
    "rdbms",
    "database",
    "could",
    "create",
    "namespace",
    "saying",
    "create",
    "namespace",
    "giving",
    "name",
    "could",
    "also",
    "list",
    "tables",
    "within",
    "namespace",
    "could",
    "create",
    "tables",
    "within",
    "specific",
    "namespace",
    "usually",
    "done",
    "production",
    "environment",
    "cluster",
    "might",
    "cluster",
    "might",
    "different",
    "users",
    "nosql",
    "database",
    "case",
    "admin",
    "would",
    "create",
    "specific",
    "namespace",
    "specific",
    "namespace",
    "would",
    "different",
    "directories",
    "sdfs",
    "users",
    "particular",
    "business",
    "unit",
    "team",
    "work",
    "hbase",
    "objects",
    "within",
    "specific",
    "name",
    "space",
    "question",
    "important",
    "understand",
    "writes",
    "reads",
    "right",
    "ahead",
    "log",
    "wall",
    "help",
    "region",
    "server",
    "crashes",
    "said",
    "write",
    "happens",
    "happen",
    "mem",
    "store",
    "wall",
    "edit",
    "log",
    "write",
    "ahead",
    "log",
    "whenever",
    "write",
    "happens",
    "happen",
    "two",
    "places",
    "mem",
    "store",
    "right",
    "cache",
    "wall",
    "edit",
    "log",
    "data",
    "written",
    "places",
    "based",
    "limitation",
    "mem",
    "store",
    "data",
    "flushed",
    "create",
    "format",
    "file",
    "called",
    "files",
    "compacted",
    "created",
    "one",
    "bigger",
    "file",
    "stored",
    "sdfs",
    "sdfs",
    "data",
    "know",
    "stored",
    "form",
    "blocks",
    "underlying",
    "data",
    "nodes",
    "region",
    "server",
    "hosting",
    "mem",
    "store",
    "crashes",
    "region",
    "server",
    "running",
    "would",
    "data",
    "node",
    "data",
    "node",
    "crashes",
    "region",
    "server",
    "hosting",
    "mem",
    "store",
    "write",
    "cache",
    "crashes",
    "data",
    "memory",
    "data",
    "memory",
    "persisted",
    "lost",
    "hbase",
    "recover",
    "said",
    "data",
    "written",
    "wall",
    "mem",
    "store",
    "time",
    "hbase",
    "recovers",
    "writing",
    "wall",
    "write",
    "completes",
    "whenever",
    "write",
    "happens",
    "happens",
    "mem",
    "store",
    "wall",
    "time",
    "hbase",
    "cluster",
    "keeps",
    "wall",
    "record",
    "changes",
    "happen",
    "call",
    "also",
    "edit",
    "log",
    "hps",
    "goes",
    "node",
    "goes",
    "data",
    "flushed",
    "mem",
    "store",
    "edge",
    "file",
    "recovered",
    "replaying",
    "right",
    "ahead",
    "lock",
    "benefit",
    "edit",
    "log",
    "write",
    "ahead",
    "log",
    "would",
    "write",
    "hbase",
    "command",
    "list",
    "contents",
    "update",
    "column",
    "families",
    "table",
    "could",
    "scan",
    "would",
    "give",
    "complete",
    "data",
    "table",
    "specific",
    "would",
    "want",
    "look",
    "particular",
    "row",
    "could",
    "get",
    "table",
    "name",
    "give",
    "row",
    "key",
    "however",
    "could",
    "scan",
    "get",
    "complete",
    "data",
    "particular",
    "table",
    "could",
    "also",
    "describe",
    "see",
    "different",
    "column",
    "families",
    "would",
    "want",
    "alter",
    "table",
    "add",
    "new",
    "column",
    "family",
    "simple",
    "say",
    "alter",
    "give",
    "hbs",
    "table",
    "name",
    "give",
    "new",
    "column",
    "family",
    "name",
    "added",
    "table",
    "catalog",
    "tables",
    "base",
    "mentioned",
    "zookeeper",
    "knows",
    "location",
    "internal",
    "catalog",
    "table",
    "call",
    "meta",
    "table",
    "catalog",
    "tables",
    "edge",
    "base",
    "two",
    "tables",
    "one",
    "edge",
    "base",
    "meta",
    "table",
    "one",
    "hyphen",
    "rule",
    "catalog",
    "table",
    "edge",
    "base",
    "meta",
    "exists",
    "edge",
    "base",
    "table",
    "filtered",
    "hbase",
    "shells",
    "list",
    "command",
    "give",
    "list",
    "command",
    "edge",
    "base",
    "would",
    "list",
    "tables",
    "space",
    "contains",
    "meta",
    "table",
    "internal",
    "table",
    "meta",
    "table",
    "keeps",
    "list",
    "regions",
    "system",
    "location",
    "hbase",
    "meta",
    "stored",
    "zookeeper",
    "somebody",
    "wants",
    "find",
    "look",
    "particular",
    "rows",
    "need",
    "know",
    "regions",
    "contain",
    "data",
    "regions",
    "located",
    "region",
    "server",
    "get",
    "information",
    "one",
    "look",
    "meta",
    "table",
    "however",
    "looking",
    "meta",
    "table",
    "directly",
    "would",
    "giving",
    "write",
    "read",
    "operation",
    "internally",
    "uh",
    "based",
    "master",
    "queries",
    "zookeeper",
    "zookeeper",
    "information",
    "meta",
    "table",
    "exists",
    "meta",
    "table",
    "existing",
    "region",
    "server",
    "contains",
    "information",
    "row",
    "keys",
    "region",
    "servers",
    "rows",
    "found",
    "root",
    "table",
    "keeps",
    "track",
    "location",
    "meta",
    "table",
    "hot",
    "spotting",
    "edge",
    "base",
    "avoid",
    "hot",
    "spotting",
    "common",
    "problem",
    "always",
    "admin",
    "guys",
    "guys",
    "managing",
    "infrastructure",
    "would",
    "think",
    "one",
    "main",
    "idea",
    "edge",
    "base",
    "would",
    "leveraging",
    "benefit",
    "sdfs",
    "read",
    "write",
    "requests",
    "uniformly",
    "distributed",
    "across",
    "regions",
    "region",
    "servers",
    "otherwise",
    "benefit",
    "distributed",
    "cluster",
    "would",
    "data",
    "stored",
    "across",
    "region",
    "servers",
    "form",
    "regions",
    "horizontal",
    "partitions",
    "table",
    "whenever",
    "read",
    "write",
    "requests",
    "happen",
    "uniformly",
    "distributed",
    "across",
    "regions",
    "region",
    "servers",
    "hot",
    "spotting",
    "occurs",
    "given",
    "region",
    "serviced",
    "region",
    "server",
    "receives",
    "read",
    "write",
    "request",
    "basically",
    "unbalanced",
    "way",
    "read",
    "write",
    "operations",
    "hotspot",
    "avoided",
    "designing",
    "row",
    "key",
    "way",
    "data",
    "written",
    "go",
    "multiple",
    "regions",
    "across",
    "cluster",
    "could",
    "techniques",
    "salting",
    "hashing",
    "reversing",
    "key",
    "many",
    "techniques",
    "employed",
    "users",
    "hbase",
    "need",
    "make",
    "sure",
    "regions",
    "distributed",
    "across",
    "region",
    "servers",
    "spread",
    "across",
    "region",
    "servers",
    "read",
    "write",
    "request",
    "satisfied",
    "different",
    "region",
    "servers",
    "parallel",
    "rather",
    "read",
    "write",
    "requests",
    "hitting",
    "region",
    "server",
    "overloading",
    "region",
    "server",
    "may",
    "also",
    "lead",
    "crashing",
    "particular",
    "region",
    "server",
    "important",
    "questions",
    "hbase",
    "many",
    "please",
    "refer",
    "link",
    "specified",
    "discussion",
    "gives",
    "detailed",
    "explanation",
    "base",
    "works",
    "also",
    "look",
    "hp",
    "definitive",
    "guide",
    "hbase",
    "action",
    "really",
    "good",
    "books",
    "understand",
    "hbase",
    "internals",
    "works",
    "learnt",
    "hive",
    "data",
    "warehousing",
    "package",
    "learnt",
    "pig",
    "scripting",
    "scripting",
    "language",
    "allows",
    "data",
    "analysis",
    "learned",
    "questions",
    "nosql",
    "database",
    "note",
    "225",
    "nosql",
    "databases",
    "existing",
    "market",
    "would",
    "want",
    "learn",
    "know",
    "nosql",
    "databases",
    "go",
    "google",
    "type",
    "sql",
    "databases",
    "org",
    "take",
    "link",
    "nosql",
    "databases",
    "shows",
    "225",
    "nosql",
    "databases",
    "existing",
    "market",
    "different",
    "use",
    "cases",
    "used",
    "different",
    "users",
    "different",
    "features",
    "look",
    "link",
    "talk",
    "data",
    "ingestion",
    "let",
    "look",
    "data",
    "ingestion",
    "one",
    "good",
    "link",
    "would",
    "suggest",
    "look",
    "lists",
    "around",
    "18",
    "different",
    "ingestion",
    "tools",
    "talk",
    "different",
    "data",
    "ingestion",
    "tools",
    "structured",
    "data",
    "streaming",
    "data",
    "data",
    "governance",
    "data",
    "ingestion",
    "transformation",
    "look",
    "link",
    "also",
    "gives",
    "comparison",
    "different",
    "data",
    "ingestion",
    "tools",
    "let",
    "learn",
    "questions",
    "scope",
    "one",
    "data",
    "injection",
    "tools",
    "mainly",
    "used",
    "structured",
    "data",
    "could",
    "say",
    "data",
    "coming",
    "rdbms",
    "data",
    "already",
    "structured",
    "would",
    "want",
    "ingest",
    "would",
    "want",
    "store",
    "sdfs",
    "could",
    "used",
    "hive",
    "could",
    "used",
    "kind",
    "processing",
    "using",
    "mapreduce",
    "hive",
    "pig",
    "spark",
    "processing",
    "frameworks",
    "would",
    "want",
    "load",
    "data",
    "say",
    "high",
    "voltage",
    "based",
    "tables",
    "scope",
    "mainly",
    "structured",
    "data",
    "extensively",
    "used",
    "organizations",
    "migrating",
    "rdbms",
    "big",
    "data",
    "platform",
    "would",
    "interested",
    "ingesting",
    "data",
    "import",
    "export",
    "data",
    "rdbms",
    "sdfs",
    "vice",
    "versa",
    "let",
    "learn",
    "important",
    "questions",
    "scope",
    "may",
    "asked",
    "interviewer",
    "apply",
    "big",
    "data",
    "related",
    "position",
    "scoop",
    "different",
    "flu",
    "common",
    "question",
    "asked",
    "scoop",
    "mainly",
    "structured",
    "data",
    "scoop",
    "works",
    "rdbms",
    "also",
    "works",
    "sql",
    "databases",
    "import",
    "export",
    "data",
    "import",
    "data",
    "sdfs",
    "import",
    "data",
    "data",
    "warehousing",
    "package",
    "hive",
    "directly",
    "also",
    "hbase",
    "could",
    "also",
    "export",
    "data",
    "hadoop",
    "ecosystem",
    "rdbms",
    "however",
    "comes",
    "flow",
    "flow",
    "data",
    "injection",
    "tool",
    "works",
    "streaming",
    "data",
    "unstructured",
    "data",
    "data",
    "constantly",
    "getting",
    "generated",
    "example",
    "log",
    "files",
    "metrics",
    "server",
    "chat",
    "messenger",
    "interested",
    "working",
    "capturing",
    "storing",
    "streaming",
    "data",
    "storage",
    "layer",
    "sdfs",
    "hbase",
    "could",
    "using",
    "flue",
    "could",
    "tools",
    "also",
    "like",
    "kafka",
    "storm",
    "chokwa",
    "samsa",
    "nifi",
    "scoop",
    "however",
    "mainly",
    "structured",
    "data",
    "loading",
    "data",
    "scope",
    "event",
    "driven",
    "based",
    "event",
    "basically",
    "works",
    "data",
    "already",
    "stored",
    "rdbms",
    "terms",
    "flume",
    "completely",
    "event",
    "driven",
    "messages",
    "events",
    "happen",
    "data",
    "getting",
    "generated",
    "data",
    "ingested",
    "using",
    "flow",
    "scope",
    "works",
    "structured",
    "data",
    "sources",
    "various",
    "scope",
    "connectors",
    "used",
    "fetch",
    "data",
    "external",
    "data",
    "structures",
    "rdbms",
    "every",
    "rdbms",
    "mysql",
    "oracle",
    "db2",
    "microsoft",
    "sql",
    "server",
    "different",
    "connectors",
    "available",
    "flume",
    "works",
    "fetching",
    "streaming",
    "data",
    "tweets",
    "log",
    "files",
    "server",
    "metrics",
    "different",
    "sources",
    "data",
    "getting",
    "generated",
    "interested",
    "ingesting",
    "data",
    "getting",
    "generated",
    "streaming",
    "fashion",
    "would",
    "interested",
    "processing",
    "data",
    "arrives",
    "scoop",
    "import",
    "data",
    "rdbms",
    "onto",
    "sdfs",
    "also",
    "export",
    "back",
    "rdbms",
    "flume",
    "used",
    "streaming",
    "data",
    "could",
    "one",
    "one",
    "one",
    "many",
    "many",
    "one",
    "kind",
    "relation",
    "terms",
    "loom",
    "components",
    "source",
    "sync",
    "channel",
    "main",
    "difference",
    "scope",
    "flow",
    "different",
    "file",
    "formats",
    "import",
    "data",
    "using",
    "scope",
    "well",
    "lots",
    "lots",
    "formats",
    "import",
    "data",
    "scope",
    "talk",
    "scope",
    "delimited",
    "text",
    "file",
    "format",
    "default",
    "import",
    "format",
    "specified",
    "explicitly",
    "using",
    "text",
    "file",
    "argument",
    "want",
    "import",
    "data",
    "rdbms",
    "could",
    "get",
    "data",
    "sdfs",
    "using",
    "different",
    "compression",
    "schemes",
    "different",
    "formats",
    "using",
    "specific",
    "arguments",
    "could",
    "specify",
    "argument",
    "write",
    "string",
    "based",
    "representation",
    "record",
    "output",
    "files",
    "delimiters",
    "individual",
    "columns",
    "rows",
    "default",
    "format",
    "used",
    "import",
    "data",
    "using",
    "scope",
    "learn",
    "scope",
    "different",
    "arguments",
    "available",
    "click",
    "look",
    "documentation",
    "would",
    "suggest",
    "choosing",
    "one",
    "versions",
    "looking",
    "user",
    "guide",
    "search",
    "arguments",
    "look",
    "specific",
    "control",
    "arguments",
    "show",
    "import",
    "data",
    "using",
    "scope",
    "common",
    "arguments",
    "also",
    "import",
    "control",
    "arguments",
    "wherein",
    "different",
    "options",
    "like",
    "getting",
    "data",
    "avro",
    "sequence",
    "file",
    "text",
    "file",
    "parquet",
    "file",
    "different",
    "formats",
    "also",
    "get",
    "data",
    "default",
    "compression",
    "scheme",
    "gzip",
    "specify",
    "compression",
    "codec",
    "specify",
    "compression",
    "mechanism",
    "would",
    "want",
    "use",
    "importing",
    "data",
    "using",
    "scope",
    "comes",
    "default",
    "format",
    "flume",
    "could",
    "say",
    "sequence",
    "file",
    "binary",
    "format",
    "stores",
    "individual",
    "records",
    "record",
    "specific",
    "data",
    "types",
    "data",
    "types",
    "manifested",
    "java",
    "classes",
    "scope",
    "automatically",
    "generate",
    "data",
    "types",
    "scoop",
    "talk",
    "sequence",
    "file",
    "format",
    "terms",
    "scope",
    "could",
    "extracting",
    "storage",
    "data",
    "binary",
    "representation",
    "mentioned",
    "import",
    "data",
    "different",
    "formats",
    "avro",
    "parquet",
    "sequence",
    "file",
    "binary",
    "format",
    "machine",
    "readable",
    "format",
    "could",
    "also",
    "data",
    "different",
    "compression",
    "schemes",
    "let",
    "show",
    "quick",
    "examples",
    "look",
    "content",
    "could",
    "search",
    "scoop",
    "based",
    "file",
    "listed",
    "examples",
    "would",
    "want",
    "use",
    "different",
    "compression",
    "schemes",
    "examples",
    "look",
    "scoop",
    "import",
    "also",
    "giving",
    "argument",
    "scoop",
    "also",
    "triggers",
    "map",
    "reduce",
    "job",
    "would",
    "say",
    "map",
    "job",
    "run",
    "scoop",
    "import",
    "triggers",
    "map",
    "job",
    "reduce",
    "happens",
    "could",
    "specify",
    "parameter",
    "argument",
    "command",
    "line",
    "could",
    "run",
    "map",
    "job",
    "local",
    "mode",
    "save",
    "time",
    "would",
    "interact",
    "yarn",
    "run",
    "map",
    "job",
    "give",
    "connection",
    "connect",
    "whatever",
    "rdbms",
    "connecting",
    "mentioning",
    "database",
    "name",
    "give",
    "user",
    "name",
    "password",
    "give",
    "table",
    "name",
    "give",
    "target",
    "directory",
    "would",
    "create",
    "directory",
    "table",
    "name",
    "would",
    "work",
    "could",
    "say",
    "minus",
    "z",
    "get",
    "data",
    "compressed",
    "format",
    "gzip",
    "could",
    "specifying",
    "compression",
    "codec",
    "could",
    "specify",
    "compression",
    "codec",
    "would",
    "want",
    "use",
    "say",
    "snappy",
    "b",
    "lz4",
    "default",
    "could",
    "also",
    "run",
    "query",
    "giving",
    "scope",
    "import",
    "specifying",
    "query",
    "notice",
    "given",
    "table",
    "name",
    "would",
    "included",
    "query",
    "get",
    "data",
    "sequence",
    "file",
    "format",
    "binary",
    "format",
    "create",
    "huge",
    "file",
    "could",
    "also",
    "compression",
    "enabled",
    "could",
    "say",
    "output",
    "map",
    "job",
    "use",
    "compression",
    "record",
    "level",
    "data",
    "coming",
    "sequence",
    "file",
    "sequence",
    "file",
    "binary",
    "format",
    "supports",
    "compression",
    "record",
    "level",
    "block",
    "level",
    "could",
    "get",
    "data",
    "avro",
    "file",
    "data",
    "embedded",
    "schema",
    "within",
    "file",
    "parquet",
    "file",
    "also",
    "different",
    "ways",
    "set",
    "different",
    "compression",
    "schemes",
    "even",
    "get",
    "data",
    "different",
    "formats",
    "could",
    "simple",
    "scope",
    "import",
    "looking",
    "importance",
    "eval",
    "tool",
    "scope",
    "something",
    "called",
    "eval",
    "tool",
    "scoop",
    "eval",
    "tool",
    "allows",
    "users",
    "execute",
    "user",
    "defined",
    "queries",
    "respective",
    "database",
    "servers",
    "preview",
    "result",
    "console",
    "either",
    "could",
    "running",
    "straight",
    "away",
    "query",
    "import",
    "data",
    "sdfs",
    "could",
    "use",
    "scoop",
    "eval",
    "connect",
    "external",
    "rdbms",
    "specify",
    "username",
    "password",
    "could",
    "giving",
    "query",
    "see",
    "would",
    "result",
    "query",
    "intend",
    "import",
    "let",
    "learn",
    "scope",
    "imports",
    "exports",
    "data",
    "rdbms",
    "sdfs",
    "architecture",
    "rdbms",
    "know",
    "database",
    "structures",
    "tables",
    "logical",
    "internally",
    "always",
    "metadata",
    "stored",
    "scope",
    "import",
    "connects",
    "external",
    "rdbms",
    "connection",
    "uses",
    "internal",
    "connector",
    "jar",
    "file",
    "driver",
    "class",
    "something",
    "needs",
    "set",
    "admin",
    "need",
    "make",
    "sure",
    "whichever",
    "rdbms",
    "intend",
    "connect",
    "need",
    "jdbc",
    "connector",
    "particular",
    "rdbms",
    "stored",
    "within",
    "scope",
    "lib",
    "folder",
    "scope",
    "import",
    "gets",
    "metadata",
    "scoop",
    "command",
    "converts",
    "map",
    "job",
    "might",
    "one",
    "multiple",
    "map",
    "tasks",
    "depends",
    "scoop",
    "command",
    "could",
    "specifying",
    "would",
    "want",
    "import",
    "one",
    "task",
    "multiple",
    "tasks",
    "multiple",
    "map",
    "tasks",
    "run",
    "section",
    "data",
    "rdbms",
    "store",
    "sdfs",
    "high",
    "level",
    "could",
    "say",
    "scoop",
    "introspect",
    "database",
    "get",
    "gathered",
    "metadata",
    "divides",
    "input",
    "data",
    "set",
    "splits",
    "division",
    "data",
    "splits",
    "mainly",
    "happens",
    "primary",
    "key",
    "column",
    "table",
    "somebody",
    "might",
    "ask",
    "table",
    "rdbms",
    "primary",
    "key",
    "column",
    "scoop",
    "import",
    "either",
    "import",
    "using",
    "one",
    "mapper",
    "task",
    "specifying",
    "hyphen",
    "hyphen",
    "equals",
    "one",
    "would",
    "say",
    "split",
    "parameter",
    "specify",
    "numeric",
    "column",
    "rdbms",
    "import",
    "data",
    "let",
    "show",
    "quick",
    "example",
    "could",
    "look",
    "scoop",
    "command",
    "file",
    "could",
    "looking",
    "example",
    "see",
    "one",
    "specifying",
    "minus",
    "minus",
    "equals",
    "1",
    "basically",
    "means",
    "would",
    "want",
    "import",
    "data",
    "using",
    "one",
    "map",
    "task",
    "case",
    "whether",
    "table",
    "primary",
    "key",
    "column",
    "primary",
    "key",
    "column",
    "matter",
    "say",
    "minus",
    "minus",
    "ms6",
    "specifying",
    "multiple",
    "map",
    "tasks",
    "imported",
    "look",
    "primary",
    "key",
    "column",
    "table",
    "importing",
    "table",
    "primary",
    "key",
    "column",
    "could",
    "specifying",
    "split",
    "specify",
    "column",
    "data",
    "could",
    "split",
    "multiple",
    "chunks",
    "multiple",
    "map",
    "tasks",
    "could",
    "take",
    "second",
    "scenario",
    "table",
    "primary",
    "key",
    "column",
    "numeric",
    "column",
    "could",
    "split",
    "case",
    "would",
    "want",
    "use",
    "multiple",
    "mappers",
    "could",
    "still",
    "say",
    "split",
    "textual",
    "column",
    "add",
    "property",
    "allows",
    "splitting",
    "data",
    "options",
    "given",
    "scoop",
    "link",
    "going",
    "scoop",
    "imports",
    "exports",
    "data",
    "rdbms",
    "sdfs",
    "architecture",
    "said",
    "submits",
    "map",
    "job",
    "cluster",
    "basically",
    "import",
    "export",
    "exporting",
    "data",
    "sdfs",
    "case",
    "would",
    "map",
    "job",
    "would",
    "look",
    "multiple",
    "splits",
    "data",
    "existing",
    "map",
    "job",
    "would",
    "process",
    "one",
    "one",
    "table",
    "map",
    "task",
    "export",
    "rdbms",
    "suppose",
    "database",
    "sdb",
    "mysql",
    "somebody",
    "asked",
    "write",
    "command",
    "connect",
    "database",
    "import",
    "tables",
    "scoop",
    "quick",
    "example",
    "showed",
    "command",
    "file",
    "could",
    "say",
    "scope",
    "import",
    "would",
    "want",
    "connect",
    "using",
    "jdbc",
    "work",
    "jdbc",
    "connector",
    "already",
    "exists",
    "within",
    "scope",
    "lib",
    "directory",
    "admin",
    "set",
    "connect",
    "rdbms",
    "point",
    "database",
    "database",
    "name",
    "test",
    "underscore",
    "db",
    "could",
    "give",
    "user",
    "name",
    "either",
    "could",
    "give",
    "password",
    "command",
    "line",
    "say",
    "capital",
    "p",
    "could",
    "prompted",
    "password",
    "could",
    "give",
    "table",
    "name",
    "would",
    "want",
    "import",
    "could",
    "also",
    "specifying",
    "minus",
    "minus",
    "specify",
    "many",
    "map",
    "tasks",
    "want",
    "use",
    "import",
    "showed",
    "previous",
    "screen",
    "export",
    "table",
    "back",
    "rdbms",
    "need",
    "data",
    "directory",
    "hdfs",
    "example",
    "department",
    "stable",
    "retail",
    "database",
    "already",
    "imported",
    "scoop",
    "need",
    "export",
    "table",
    "back",
    "rdbms",
    "content",
    "table",
    "create",
    "new",
    "department",
    "table",
    "rdbms",
    "could",
    "create",
    "table",
    "specifying",
    "column",
    "names",
    "whether",
    "supports",
    "null",
    "primary",
    "key",
    "column",
    "always",
    "recommended",
    "scoop",
    "export",
    "connect",
    "rdbms",
    "specifying",
    "username",
    "password",
    "specify",
    "table",
    "want",
    "export",
    "data",
    "give",
    "export",
    "directory",
    "pointing",
    "directory",
    "sdfs",
    "contains",
    "data",
    "export",
    "data",
    "table",
    "seeing",
    "example",
    "could",
    "look",
    "file",
    "example",
    "import",
    "importing",
    "data",
    "directly",
    "hive",
    "scope",
    "import",
    "importing",
    "data",
    "directly",
    "hbase",
    "table",
    "query",
    "hbase",
    "table",
    "look",
    "data",
    "could",
    "also",
    "export",
    "running",
    "map",
    "job",
    "local",
    "mode",
    "connecting",
    "rdbms",
    "specifying",
    "username",
    "specifying",
    "table",
    "would",
    "want",
    "export",
    "directory",
    "sdfs",
    "kept",
    "relevant",
    "data",
    "simple",
    "example",
    "export",
    "looking",
    "role",
    "jdbc",
    "driver",
    "scoop",
    "setup",
    "said",
    "would",
    "want",
    "use",
    "scoop",
    "connect",
    "external",
    "rdbms",
    "need",
    "jdbc",
    "odbc",
    "connector",
    "jar",
    "file",
    "one",
    "admin",
    "could",
    "download",
    "jdbc",
    "connector",
    "jar",
    "file",
    "place",
    "jar",
    "file",
    "within",
    "scoop",
    "lib",
    "directory",
    "wherever",
    "scoop",
    "installed",
    "jdbc",
    "connector",
    "jar",
    "file",
    "contains",
    "driver",
    "jdbc",
    "driver",
    "standard",
    "java",
    "api",
    "used",
    "accessing",
    "different",
    "databases",
    "rdbms",
    "connector",
    "jar",
    "file",
    "much",
    "required",
    "connector",
    "jar",
    "file",
    "driver",
    "class",
    "driver",
    "class",
    "enables",
    "connection",
    "rdbms",
    "hadoop",
    "structure",
    "database",
    "vendor",
    "responsible",
    "writing",
    "implementation",
    "allow",
    "communication",
    "corresponding",
    "database",
    "need",
    "download",
    "drivers",
    "allow",
    "scoop",
    "connect",
    "external",
    "rdbms",
    "jdbc",
    "driver",
    "alone",
    "enough",
    "connect",
    "scope",
    "also",
    "need",
    "connectors",
    "interact",
    "different",
    "database",
    "connector",
    "plugable",
    "piece",
    "used",
    "fetch",
    "metadata",
    "allow",
    "scoop",
    "overcome",
    "differences",
    "sql",
    "dialects",
    "connection",
    "established",
    "normally",
    "admins",
    "would",
    "setting",
    "scope",
    "hadoop",
    "would",
    "download",
    "say",
    "mysql",
    "jdbc",
    "connector",
    "would",
    "go",
    "mysql",
    "connectors",
    "connecting",
    "mysql",
    "similarly",
    "rdbms",
    "could",
    "say",
    "going",
    "could",
    "looking",
    "previous",
    "version",
    "depending",
    "could",
    "going",
    "platform",
    "independent",
    "could",
    "downloading",
    "connected",
    "jar",
    "file",
    "enter",
    "jar",
    "file",
    "would",
    "see",
    "mysql",
    "connector",
    "jar",
    "look",
    "package",
    "within",
    "connector",
    "jar",
    "file",
    "driver",
    "class",
    "allows",
    "connection",
    "scope",
    "rdbms",
    "things",
    "done",
    "admin",
    "scope",
    "connecting",
    "external",
    "rdbms",
    "update",
    "columns",
    "already",
    "exported",
    "export",
    "put",
    "data",
    "rdbms",
    "really",
    "update",
    "columns",
    "already",
    "exported",
    "yes",
    "using",
    "update",
    "key",
    "parameter",
    "scoop",
    "export",
    "command",
    "remains",
    "thing",
    "specify",
    "table",
    "name",
    "fields",
    "terminated",
    "specific",
    "delimiter",
    "say",
    "update",
    "key",
    "column",
    "name",
    "allows",
    "us",
    "update",
    "columns",
    "already",
    "exported",
    "rdbms",
    "code",
    "gen",
    "scope",
    "commands",
    "translate",
    "mapreduce",
    "job",
    "map",
    "job",
    "code",
    "gen",
    "basically",
    "tool",
    "scope",
    "generates",
    "data",
    "access",
    "objects",
    "dao",
    "java",
    "classes",
    "encapsulate",
    "interpret",
    "imported",
    "records",
    "scoop",
    "code",
    "gen",
    "connect",
    "rdbms",
    "using",
    "username",
    "give",
    "table",
    "generate",
    "java",
    "code",
    "employee",
    "table",
    "test",
    "database",
    "code",
    "gen",
    "useful",
    "us",
    "understand",
    "data",
    "particular",
    "table",
    "finally",
    "scoop",
    "used",
    "convert",
    "data",
    "different",
    "formats",
    "think",
    "already",
    "answered",
    "right",
    "tools",
    "used",
    "purpose",
    "scoop",
    "used",
    "convert",
    "data",
    "different",
    "formats",
    "depends",
    "different",
    "arguments",
    "use",
    "import",
    "avro",
    "file",
    "parquet",
    "file",
    "binary",
    "format",
    "record",
    "block",
    "level",
    "compression",
    "interested",
    "knowing",
    "different",
    "data",
    "formats",
    "think",
    "suggest",
    "link",
    "say",
    "hadoop",
    "formats",
    "think",
    "tech",
    "maggie",
    "avro",
    "parque",
    "let",
    "see",
    "find",
    "link",
    "tech",
    "mac",
    "e",
    "yeah",
    "good",
    "link",
    "specifies",
    "talks",
    "different",
    "data",
    "formats",
    "know",
    "text",
    "file",
    "format",
    "different",
    "compression",
    "schemes",
    "data",
    "organization",
    "common",
    "formats",
    "text",
    "file",
    "structured",
    "binary",
    "sequence",
    "files",
    "compression",
    "without",
    "compression",
    "record",
    "level",
    "block",
    "level",
    "avro",
    "data",
    "file",
    "square",
    "parquet",
    "data",
    "file",
    "columnar",
    "format",
    "formats",
    "like",
    "orc",
    "rc",
    "thanks",
    "ajay",
    "reached",
    "end",
    "complete",
    "big",
    "data",
    "course",
    "hope",
    "enjoyed",
    "video",
    "like",
    "share",
    "thank",
    "watching",
    "stay",
    "tuned",
    "simply",
    "learn",
    "music"
  ],
  "keywords": [
    "know",
    "around",
    "users",
    "today",
    "generate",
    "data",
    "big",
    "think",
    "processed",
    "well",
    "like",
    "hadoop",
    "spark",
    "used",
    "video",
    "talking",
    "various",
    "components",
    "hdfs",
    "map",
    "reduce",
    "look",
    "apache",
    "sql",
    "first",
    "talk",
    "last",
    "five",
    "years",
    "never",
    "going",
    "need",
    "solution",
    "would",
    "question",
    "take",
    "couple",
    "organizations",
    "interested",
    "industry",
    "want",
    "working",
    "allow",
    "use",
    "happening",
    "start",
    "done",
    "learn",
    "always",
    "check",
    "cases",
    "systems",
    "relational",
    "databases",
    "work",
    "started",
    "value",
    "different",
    "generated",
    "example",
    "twitter",
    "facebook",
    "say",
    "google",
    "getting",
    "every",
    "business",
    "point",
    "might",
    "already",
    "analysis",
    "formats",
    "structured",
    "unstructured",
    "called",
    "main",
    "50",
    "huge",
    "amount",
    "volume",
    "portion",
    "store",
    "something",
    "organization",
    "allows",
    "process",
    "analyze",
    "also",
    "give",
    "looking",
    "let",
    "needs",
    "right",
    "second",
    "search",
    "engine",
    "send",
    "1",
    "customer",
    "queries",
    "per",
    "could",
    "lot",
    "times",
    "people",
    "loading",
    "basically",
    "connection",
    "however",
    "even",
    "means",
    "10",
    "companies",
    "collect",
    "simple",
    "go",
    "using",
    "able",
    "find",
    "list",
    "discuss",
    "later",
    "help",
    "coming",
    "back",
    "information",
    "probably",
    "earlier",
    "rdbms",
    "storing",
    "mysql",
    "server",
    "many",
    "access",
    "100",
    "percent",
    "storage",
    "processing",
    "layer",
    "one",
    "read",
    "said",
    "anything",
    "shows",
    "come",
    "understand",
    "case",
    "user",
    "whether",
    "features",
    "connect",
    "way",
    "analyzing",
    "depending",
    "particular",
    "log",
    "get",
    "examples",
    "show",
    "uses",
    "collection",
    "tell",
    "time",
    "gives",
    "us",
    "company",
    "important",
    "services",
    "care",
    "customers",
    "large",
    "complex",
    "change",
    "underlying",
    "handle",
    "kind",
    "comes",
    "given",
    "stored",
    "whatever",
    "although",
    "ways",
    "mentioned",
    "variable",
    "servers",
    "things",
    "certainly",
    "next",
    "difference",
    "call",
    "file",
    "see",
    "specific",
    "space",
    "rows",
    "similar",
    "table",
    "schema",
    "mean",
    "format",
    "easily",
    "previous",
    "finally",
    "good",
    "algorithms",
    "values",
    "task",
    "still",
    "taken",
    "obviously",
    "actually",
    "whole",
    "since",
    "number",
    "manage",
    "much",
    "query",
    "cpu",
    "based",
    "requirement",
    "distributed",
    "fault",
    "tolerant",
    "system",
    "multiple",
    "disks",
    "happen",
    "machine",
    "computing",
    "environment",
    "result",
    "works",
    "linux",
    "set",
    "files",
    "directories",
    "architecture",
    "master",
    "chunk",
    "slave",
    "machines",
    "contain",
    "metadata",
    "client",
    "api",
    "application",
    "running",
    "request",
    "internally",
    "ram",
    "disk",
    "write",
    "processes",
    "together",
    "make",
    "size",
    "block",
    "bigger",
    "split",
    "smaller",
    "across",
    "local",
    "operations",
    "cluster",
    "status",
    "form",
    "available",
    "build",
    "within",
    "broken",
    "two",
    "replication",
    "mb",
    "basic",
    "replicated",
    "default",
    "three",
    "scalable",
    "questions",
    "uh",
    "project",
    "problem",
    "object",
    "options",
    "either",
    "part",
    "download",
    "run",
    "interesting",
    "easy",
    "thing",
    "happens",
    "bring",
    "required",
    "perform",
    "compression",
    "hit",
    "open",
    "source",
    "framework",
    "applications",
    "clusters",
    "link",
    "person",
    "learning",
    "setup",
    "another",
    "distributions",
    "talks",
    "support",
    "web",
    "cloud",
    "mapper",
    "core",
    "package",
    "management",
    "intend",
    "setting",
    "distribution",
    "plan",
    "quick",
    "demo",
    "cloudera",
    "vm",
    "standalone",
    "type",
    "single",
    "node",
    "click",
    "version",
    "select",
    "details",
    "downloaded",
    "error",
    "virtual",
    "related",
    "import",
    "importing",
    "dot",
    "cores",
    "minimum",
    "little",
    "2",
    "gb",
    "sdfs",
    "ecosystem",
    "ahead",
    "added",
    "settings",
    "least",
    "better",
    "network",
    "goes",
    "fine",
    "starting",
    "roles",
    "session",
    "script",
    "scripts",
    "nodes",
    "similarly",
    "hortonworks",
    "responsible",
    "individual",
    "follow",
    "steps",
    "admin",
    "console",
    "command",
    "line",
    "interface",
    "connected",
    "terminal",
    "host",
    "name",
    "commands",
    "seconds",
    "takes",
    "dfs",
    "exists",
    "giving",
    "service",
    "tells",
    "manager",
    "root",
    "remember",
    "password",
    "says",
    "copy",
    "new",
    "id",
    "username",
    "try",
    "browser",
    "memory",
    "remove",
    "add",
    "key",
    "delete",
    "relevant",
    "depends",
    "yarn",
    "configuration",
    "hue",
    "sure",
    "load",
    "faster",
    "issues",
    "existing",
    "drop",
    "scoop",
    "scope",
    "option",
    "pointing",
    "makes",
    "zookeeper",
    "queue",
    "order",
    "logs",
    "become",
    "stuff",
    "ui",
    "yet",
    "without",
    "tools",
    "usually",
    "gets",
    "starts",
    "really",
    "four",
    "properties",
    "class",
    "level",
    "mapreduce",
    "programming",
    "model",
    "instead",
    "parallel",
    "high",
    "stores",
    "128",
    "blocks",
    "input",
    "output",
    "capacity",
    "resources",
    "history",
    "created",
    "top",
    "written",
    "language",
    "java",
    "python",
    "scala",
    "ingestion",
    "transformation",
    "mainly",
    "streaming",
    "directly",
    "hive",
    "creating",
    "tables",
    "edge",
    "hbase",
    "nosql",
    "export",
    "database",
    "records",
    "base",
    "managers",
    "edit",
    "scheduler",
    "create",
    "jobs",
    "availability",
    "pick",
    "scripting",
    "interactive",
    "graph",
    "logical",
    "provides",
    "active",
    "standby",
    "imagine",
    "config",
    "specify",
    "looks",
    "job",
    "tracker",
    "resource",
    "app",
    "trackers",
    "possible",
    "slaves",
    "everything",
    "somebody",
    "note",
    "defined",
    "path",
    "slash",
    "writing",
    "wants",
    "directory",
    "tracking",
    "rack",
    "operation",
    "activity",
    "contains",
    "ide",
    "code",
    "sample",
    "sets",
    "splits",
    "folder",
    "text",
    "home",
    "enter",
    "put",
    "meta",
    "complete",
    "hyphen",
    "tool",
    "specifying",
    "refresh",
    "bin",
    "property",
    "site",
    "string",
    "taking",
    "pig",
    "floor",
    "count",
    "total",
    "approach",
    "terms",
    "mapping",
    "reducer",
    "concept",
    "phase",
    "reducing",
    "function",
    "tasks",
    "mode",
    "reduced",
    "internal",
    "partitioner",
    "combiner",
    "binary",
    "supports",
    "record",
    "pairs",
    "lines",
    "content",
    "character",
    "sequence",
    "functions",
    "pretty",
    "classes",
    "keys",
    "program",
    "step",
    "submitted",
    "container",
    "jar",
    "pull",
    "small",
    "queues",
    "submit",
    "end",
    "minus",
    "word",
    "words",
    "test",
    "results",
    "star",
    "runs",
    "fair",
    "completed",
    "cat",
    "allocation",
    "allocated",
    "scheduling",
    "loaded",
    "real",
    "languages",
    "bit",
    "execution",
    "column",
    "update",
    "containers",
    "departments",
    "saying",
    "worker",
    "course",
    "shell",
    "location",
    "got",
    "group",
    "x",
    "simply",
    "full",
    "warehouse",
    "external",
    "equals",
    "major",
    "computer",
    "connector",
    "maybe",
    "side",
    "computers",
    "jdbc",
    "driver",
    "nice",
    "windows",
    "window",
    "underscore",
    "hql",
    "department",
    "common",
    "underneath",
    "documents",
    "filter",
    "kinds",
    "flow",
    "types",
    "codes",
    "columns",
    "c",
    "execute",
    "compiler",
    "partition",
    "integer",
    "array",
    "whenever",
    "office",
    "employee",
    "comma",
    "salary",
    "row",
    "fields",
    "join",
    "age",
    "latin",
    "action",
    "partitioning",
    "dag",
    "dump",
    "tuple",
    "field",
    "engineer",
    "h",
    "family",
    "regions",
    "region",
    "wall",
    "knowledge",
    "scan",
    "rdds",
    "frames",
    "rdd",
    "val",
    "context",
    "method",
    "avro",
    "relation",
    "pi",
    "sbt"
  ]
}