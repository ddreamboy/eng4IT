{
  "text": "have you ever been amazed by how google\nhome and amazon alexa assist in finding\ninformation when asked over voice\nor have you ever wondered how ecommerce\ncompanies send you personalized emails\nfor shopping suggestions based on the\nproducts that you bought online recently\nwell it's all because of machine\nlearning that these virtual assistants\nand product recommendations work swiftly\nin this video we're going to look at\nmachine learning roadmap for 2022\nwe will give you the step-by-step\nprocess that you need to follow to build\na successful career in machine learning\nso before we get started make sure that\nyou subscribe to the simply learn\nchannel and hit the bell icon to never\nmiss an update from simplylearn\nso what is machine learning\nmachine learning is a subset of\nartificial intelligence that uses data\nalgorithms and statistical techniques to\nbuild intelligent systems\nthese systems learn by themselves and\nimprove with experience\nthe goal of machine learning is to\ncreate computer models that can imitate\nhuman behavior machine learning has\nfound its usage in almost every business\nsector\nfrom self-driving cars medical imaging\nand diagnostics speech recognition\nfacial recognition to online fraud\ndetection all of these are possible\nbecause of machine learning\nin recent years ai and ml technologies\nhave made several breakthroughs and the\nrising demand for ai applications across\ndifferent industries has led to the\nsignificant growth of machine learning\nas for\nmarketsandmarkets.com the machine\nlearning market is expected to grow to 9\nbillion dollars by 2022 at ac agr of 44\nanother report from\nverifiedmarketresearch.com\nsuggests that the machine learning\nmarket was valued at 2.4 billion us\ndollars in 2019 and is projected to\nreach 47.29 million usc by 2027\ngrowing at a cagr of 44.9\nfrom 2020 to 2027.\nkeeping these facts and forecasts in\nmind let's look at the roadmap and\ncritical skills required to build a\ncareer in machine learning\nfirst and foremost is programming skills\nfor a machine learning engineer\nprogramming languages form the building\nblocks to develop complex machine\nlearning models\nyou need to learn at least one\nprogramming language preferably python\nor r you should be familiar with various\ncomputer science concepts such as data\nstructures including stack queues trees\nand graphs algorithms for searching and\nsorting dynamic and greedy programming\nspace and time complexity etc you also\nneed to know libraries like numpy pandas\nmatplotlib cbon d-plure tidier and\nggplot for data analysis and\nvisualization the second skill you need\nto possess is applied mathematics\nwhile solving business problems using\nmachine learning you have to use machine\nlearning algorithms\nto understand the mechanisms behind the\nalgorithms you need to have a good\nknowledge of mathematical concepts such\nas linear algebra calculus statistics\nand probability\nso mathematics and machine learning is\nnot just processing the numbers but\nunderstanding what is happening why it's\nhappening and how to obtain the best\nresults the third skill to become a\nmachine learning expert is\ndata wrangling and sql\ndata analysts and machine learning\nspecialists often work with raw data\ncollected from various sources that are\nnot fit for analysis\nit has been observed that eighty percent\nof data analysis spend too much time on\ndata wrangling\nso it's crucial for machine learning\nexperts to clean structure and enrich\nraw data into desired format and make it\nready for analysis using data wrangling\ntechniques\nsql is another crucial set that you\nshould carry\nmachine learning tasks involve using\ndata stored in the form of tables that\nare present inside relational databases\na good understanding of sql commands\nenables you to store manipulate retrieve\nand handle structured data\nthe next skill set is machine learning\nalgorithms it's incredibly essential to\ngrasp all the standard machine learning\nalgorithms\nimplementing any machine learning\ntechniques requires choosing the\nsuitable model determining the correct\nlearning method and an in-depth\nunderstanding of hyper parameter tuning\nso you should be really good with\ndifferent supervised unsupervised and\nreinforcement learning algorithms such\nas linear regression logistic regression\nsvm knn decision trees k means\nclustering etc now coming to the final\nscale we have data modeling and\nevaluation the objective of a machine\nlearning engineer is to train the best\nperforming model possible\ndepending on the problem at hand you\nwill need to choose a suitable error\nmeasure and an evaluation strategy for a\nmachine learning model\nthe most vulnerable part of a machine\nlearning candidate's resume is the\nabsence of experience working on diverse\nmachine learning projects\nwith all the essential skills acquired\nyou can now build an impressive machine\nlearning portfolio highlighting some\nexciting machine learning projects\nmachine learning engineers need a\nportfolio that showcases their expertise\nto implement machine learning techniques\nto real-world problems\nwith your resume ready you can now look\nfor the best machine learning jobs in\ntop product companies and startups\nyou can become a machine learning\nengineer a data scientist a data analyst\nor a research scientist\nsome of the top companies hiring for\nmachine learning roles are google amazon\nibm uber grammar navidia and linkedin\nmachine learning engineers are some of\nthe highest paid professionals in the\nworld\naccording to glassdoor.com the national\naverage salary for machine learning\nengineers in the united states is one\nlakh 31 000 us dollars per year in india\nyou can earn nearly 8 lakh rupees per\nannum\nthis salary may vary based on your\nexperience the industry you are applying\nfor and the company policy\nthere are immense opportunities in\nmachine learning for sectors such as\ne-commerce manufacturing logistics\nretail and healthcare\nif you are interested in a career in\nmachine learning then start your journey\nnow with simplylearn's postgraduate\nprogram in ai and machine learning that\nis in partnership with purdue university\nand in collaboration with ibm\nyou will learn all the in-demand machine\nlearning skills and work on programming\nwith hands-on examples\nthe program also offers job assistance\nto know more about the learning part\ntools covered and projects that you'll\nbe working on please click on the course\npage link in the description below\nin this video we will cover everything\nyou need to know to become an expert in\nmachine learning our experienced\ninstructors with good industry\nexperience will take you through this\ncourse\nfirst\nyou will understand the basics of\nmachine learning from a short animated\nvideo\nyou will then know the essential\napplications of machine learning you\nwill understand machine learning\nconcepts and learn why mathematics\nstatistics and linear algebra are\ncrucial then\nwe will focus on some vital machine\nlearning algorithms such as linear\nregression logistic regression decision\ntrees random forest and k nearest\nneighbors\nyou will also learn about regularization\ndimensionality reduction and principal\ncomponent analysis we will perform a\nprediction analysis on the recently held\nu.s elections as well finally you will\nstudy the machine learning roadmap for\n2021 so let's begin\nwe know humans learn from their past\nexperiences\nand machines follow instructions given\nby humans\nbut what if humans can train the\nmachines to learn from the past data and\ndo what humans can do and much faster\nwell that's called machine learning but\nit's a lot more than just learning it's\nalso about understanding and reasoning\nso today we will learn about the basics\nof machine learning\nso that's paul he loves listening to new\nsongs\nhe either likes them or dislikes them\npaul decides this on the basis of the\nsong's tempo\ngenre intensity and the gender of voice\nfor simplicity let's just use tempo and\nintensity for now so here tempo is on\nthe x-axis ranging from relaxed to fast\nwhereas intensity is on the y-axis\nranging from light to soaring we see\nthat paul likes the song with fast tempo\nand soaring intensity while he dislikes\nthe song with relaxed tempo and light\nintensity so now we know paul's choices\nlet's say paul listens to a new song\nlet's name it as song a song a has fast\ntempo and a soaring intensity so it lies\nsomewhere here looking at the data can\nyou guess whether paul will like the\nsong or not correct so paul likes this\nsong by looking at paul's past choices\nwe were able to classify the unknown\nsong very easily right let's say now\npaul listens to a new song let's label\nit as song b so song b\nlies somewhere here with medium tempo\nand medium intensity neither relaxed nor\nfast neither light nor soaring now can\nyou guess whether ball likes it or not\nnot able to guess whether paul will like\nit or dislike it are the choices unclear\ncorrect we could easily classify song a\nbut when the choice became complicated\nas in the case of song b yes and that's\nwhere machine learning comes in let's\nsee how in the same example for song b\nif we draw a circle around the song b we\nsee that there are four words for like\nwhereas one would for dislike if we go\nfor the majority votes we can say that\npaul will definitely like the song\nthat's all this was a basic machine\nlearning algorithm also it's called k\nnearest neighbors so this is just a\nsmall example in one of the many machine\nlearning algorithms quite easy right\nbelieve me it is but what happens when\nthe choices become complicated as in the\ncase of song b that's when machine\nlearning comes in it learns the data\nbuilds the prediction model and when the\nnew data point comes in it can easily\npredict for it more the data better the\nmodel higher will be the accuracy there\nare many ways in which the machine\nlearns it could be either supervised\nlearning unsupervised learning or\nreinforcement learning let's first\nquickly understand supervised learning\nsuppose your friend gives you one\nmillion coins of three different\ncurrencies say one rupee one euro and\none dirham each coin has different\nweights for example a coin of one rupee\nweighs three grams one euro weighs seven\ngrams and one dirham weighs 4 grams your\nmodel will predict the currency of the\ncoin here your weight becomes the\nfeature of coins while currency becomes\nthe label when you feed this data to the\nmachine learning model it learns which\nfeature is associated with which slave\nfor example it will learn that if a coin\nis of 3 grams it will be a 1 rupee coin\nlet's give a new coin to the machine on\nthe basis of the weight of the new coin\nyour model will predict the currency\nhence supervised learning uses labeled\ndata to train the model here the machine\nknew the features of the object and also\nthe labels associated with those\nfeatures on this note let's move to\nunsupervised learning and see the\ndifference suppose you have cricket data\nset of various players with their\nrespective scores and wickets taken when\nyou feed this data set to the machine\nthe machine identifies the pattern of\nplayer performance so it plots this data\nwith the respective wickets on the\nx-axis while runs on the y-axis while\nlooking at the data you will clearly see\nthat there are two clusters the one\ncluster are the players who scored\nhigher runs and took less wickets while\nthe other cluster is of the players who\nscored less runs but took many wickets\nso here we interpret these two clusters\nas batsmen and bowlers the important\npoint to note here is that there were no\nlabels of batsmen and bowlers hence the\nlearning with unlabeled data is\nunsupervised learning so we saw\nsupervised learning where the data was\nlabeled and the unsupervised learning\nwhere the data was unlabeled and then\nthere is reinforcement learning which is\nreward based learning or we can say that\nit works on the principle of feedback\nhere let's say you provide the system\nwith an image of a dog and ask it to\nidentify it the system identifies it as\na cat so you give a negative feedback to\nthe machine saying that it's a dog's\nimage the machine will learn from the\nfeedback and finally if it comes across\nany other image of a dog it will be able\nto classify it correctly that is\nreinforcement learning to generalize\nmachine learning model let's see a\nflowchart input is given to a machine\nlearning model which then gives the\noutput according to the algorithm\napplied if it's right we take the output\nas a final result else we provide\nfeedback to the training model and ask\nit to predict until it learns i hope\nyou've understood supervised and\nunsupervised learning so let's have a\nquick quiz you have to determine whether\nthe given scenarios uses supervised or\nunsupervised learning simple right\nscenario 1 facebook recognizes your\nfriend in a picture from an album of\ntagged photographs\nscenario 2 netflix recommends new movies\nbased on someone's past movie choices\nscenario 3 analyzing bank data for\nsuspicious transactions and flagging the\nfraud transactions think wisely and\ncomment below your answers moving on\ndon't you sometimes wonder how is\nmachine learning possible in today's era\nwell that's because today we have\nhumongous data available everybody is\nonline either making a transaction or\njust surfing the internet and that's\ngenerating a huge amount of data every\nminute and that data my friend is the\nkey to analysis\nalso the memory handling capabilities of\ncomputers have largely increased which\nhelps them to process such huge amount\nof data at hand without any delay and\nyes computers now have great\ncomputational powers so there are a lot\nof applications of machine learning out\nthere to name a few machine learning is\nused in healthcare where diagnostics are\npredicted for doctor's review the\nsentiment analysis that the tech giants\nare doing on social media is another\ninteresting application of machine\nlearning fraud detection in the finance\nsector and also to predict customer\nchurn in the e-commerce sector while\nbooking a gap you must have encountered\nsurge pricing often where it says the\nfare of your trip has been updated\ncontinue booking yes please i'm getting\nlate for office\nwell that's an interesting machine\nlearning model which is used by global\ntaxi giant uber and others where they\nhave differential pricing in real time\nbased on demand the number of cars\navailable bad weather rush r etc so they\nuse the surge pricing model to ensure\nthat those who need a cab can get one\nalso it uses predictive modeling to\npredict where the demand will be high\nwith the goal that drivers can take care\nof the demand and search pricing can be\nminimized great hey siri can you remind\nme to book a cab at 6 pm today ok i'll\nremind you thanks no problem comment\nbelow some interesting everyday examples\naround you where machines are learning\nand doing amazing jobs so that's all for\nmachine learning basics today from my\nsite machine learning has improved our\nlives in a number of wonderful ways\ntoday let's talk about some of these i'm\nrahul from simply learn and these are\nthe top 10 applications of machine\nlearning first let's talk about virtual\npersonal assistants google assistant\nalexa cortana and siri now we've all\nused one of these at least at some point\nin our lives now these help improve our\nlives in a great number of ways for\nexample you could tell them to call\nsomeone you could tell them to play some\nmusic you could tell them to even\nschedule an appointment so how do these\nthings actually work first they record\nwhatever you're saying send it over to a\nserver which is usually in a cloud\ndecode it with the help of machine\nlearning in neural networks and then\nprovide you with an output so if you've\never noticed that these systems don't\nwork very well without the internet\nthat's because the server couldn't be\ncontacted next let's talk about traffic\npredictions now say i wanted to travel\nfrom buckingham palace to lodge cricket\nground the first thing i would probably\ndo is to get on google maps so\nsearch it\nand let's put it here\nso here we have the path you should take\nto get to large cricket ground now here\nthe map is a combination of red yellow\nand blue now the blue regions signify a\nclear road that is you won't encounter\ntraffic there the yellow indicate that\nthey are slightly congested and red\nmeans they're heavily congested so let's\nlook at the map a different version of\nthe same map and here as i told you\nbefore red means heavily congested\nyellow means slow moving and blue means\nclear\nso how exactly is google able to tell\nyou that the traffic is clear slow\nmoving or heavily congested so this is\nthe help of machine learning and with\nthe help of two important measures first\nis the average time that's taken on\nspecific days at specific times on that\nroute the second one is the real time\nlocation data of vehicles from google\nmaps and with the help of sensors some\nof the other popular map services are\nbing maps maps dot me and here we go\nnext up we have social media\npersonalization so say i want to buy a\ndrone and i'm on amazon and i want to\nbuy a dji mavic pro the thing is it's\nclose to one lap so i don't want to buy\nit right now but the next time i'm on\nfacebook i'll see an advertisement for\nthe product next time i'm on youtube\ni'll see an advertisement even on\ninstagram i'll see an advertisement so\nhere with the help of machine learning\ngoogle has understood that i'm\ninterested in this particular product\nhence it's targeting me with these\nadvertisements this is also with the\nhelp of machine learning let's talk\nabout email spam filtering now this is a\nspam that's in my inbox now how does\ngmail know what spam and what's not spam\nso gmail has an entire collection of\nemails which have already been labeled\nas spam or not spam so after analyzing\nthis data gmail is able to find some\ncharacteristics like the word lottery or\nwinner from then on any new email that\ncomes to your inbox goes through a few\nspam filters to decide whether it's spam\nor not now some of the popular spam\nfilters that gmail uses is content\nfilters header filters general blacklist\nfilters and so on next we have online\nfraud detection now there are several\nways that online fraud can take place\nfor example there's identity theft where\nthey steal your identity fake accounts\nwhere these accounts only last for how\nlong the transaction takes place and\nstop existing after that and man in the\nmiddle attacks where they steal your\nmoney while the transaction is taking\nplace the feed forward neural network\nhelps determine whether a transaction is\ngenuine or fraudulent so what happens\nwith feed forward neural networks are\nthat the outputs are converted into hash\nvalues and these values become the\ninputs for the next round so for every\nreal transaction that takes place\nthere's a specific pattern a fraudulent\ntransaction would stand out because of\nthe significant changes that it would\ncause with the hash values stock market\ntrading machine learning is used\nextensively when it comes to stock\nmarket trading now you have stock market\nindices like nikai they use long\nshort-term memory neural networks now\nthese are used to classify process and\npredict datum when there are time lags\nof unknown size and duration now this is\nused to predict stock market trends\nassisted medical technology now medical\ntechnology has been innovated with the\nhelp of machine learning diagnosing\ndiseases has been easier from which we\ncan create 3d models that can predict\nwhere exactly there are lesions in the\nbrain it works just as well for brain\ntumors and ischemic stroke lesions they\ncan also be used in fetal imaging and\ncardiac analysis now some of the medical\nfields that machine learning will help\nassist in is disease identification\npersonalized treatment drug discovery\nclinical research and radiology and\nfinally we have automatic translation\nnow say you're a foreign country and you\nsee billboards and signs that you don't\nunderstand that's where automatic\ntranslation comes of help now how does\nautomatic translation actually work the\ntechnology behind it is the same as the\nsequence of sequence learning which is\nthe same thing that's used with chatbots\nhere the image recognition happens using\nconvolutional neural networks and the\ntext is identified using optical\ncharacter recognition furthermore the\nsequential sequence algorithm is also\nused to translate the text from one\nlanguage to the other hello and welcome\nto machine learning tutorial part one\nthis is part one of a machine learning\nseries put on by simply learn my name is\nrichard kirschner i'm with the simply\nlearn team that's www.simplylearn.com\nget certified get ahead what's in it for\nyou today well we'll start off with a\nbrief explanation of why machine\nlearning and what is machine learning\nand then we'll get into a few of the\ntypes of machine learning machine\nlearning algorithms linear regression\ndecision trees support vector machine\nand finally we'll do a use case where\nwe're going to classify whether recipe\nis of a cupcake or a muffin using the\nsvm or the support vector machine sounds\nlike a delicious way to explore machine\nlearning so why machine learning why do\nwe even care about having these\ncomputers come up and be able to do all\nthese new things for us well because\nmachines can now drive your car for you\nit's still very in the infant stage but\nit's just exploding as we see with\ngoogle's waymo and then uber had their\nprogram which unfortunately crashed they\nknow that this is huge this is going to\nbe the huge industry to change our whole\ntransportation infrastructure machine\nlearning is now used to detect over 50\neye diseases do you know how amazing\nthat is to have a computer that double\nchecks for the doctor for things they\nmight miss that's just huge in the\nhealth industry pretty soon they\nactually do already have that within\nsome areas where maybe not for eyes but\nfor other diseases where they're using\nthe camera on your phone to help\npre-diagnose before you go and see the\ndoctor and because the machine can now\nunlock your phone with your face i mean\nthat's just cool having it being able to\nidentify your face or your voice and be\nable to turn stuff on and off for you\ndepending on where you're at and what\nyou need talking about an ultimate\nautomation or world we live in and as we\ndig in deeper we have a nice example of\nfacebook as you can see here they have\nthe facebook post with halloween comment\nyes if you want it order here nobody\nlikes spam post on facebook that annoy\nthem into interacting with likes shares\ncomments and other actions i remember\nthe original ones were all if you don't\nclick on here you will have bad luck or\nsome kind of fear factor well this is a\nhuge thing in a social media when people\nare getting spammed and so this tactic\nknown as engagement bait takes advantage\nof facebook's news feed algorithm by\nchoosing engagement in order to get the\ngreater reach to eliminate engagement\nbait the company reviewed and\ncategorized hundreds of thousands of\nposts to train a machine learning model\nthat detects different types of\nengagement bait so in this case we have\nwe're using facebook but this is of\ncourse across all the different social\nmedia they have different tools for\nbuilding and the facebook scroll gif\nwill be replaced kind of like a virus\ncoming in there and notices that there's\na certain setup with facebook and it's\nable to replace it and they have like\nvote baiting react baiting share baiting\nthey have all these different these are\nkind of general titles but there\ncertainly are a lot of way of baiting\nyou to go in there and click on\nsomething so they fed all this this data\nwas fed into the machine and then they\nhave the new post the new post comes up\nthat takes over part of the facebook\nsetup and that's what you're looking at\nyou're looking at this new post that's\nreplaced like a virus has replaced that\nso what facebook do to eliminate this is\nthey start scanning for keywords and\nphrases like this and checks the\nclick-through rate so it starts looking\nfor people who are clicking through it\nwithout even looking at it or clicking\nthrough it is not something that\nnormally would be clicked through once\nfacebook has scanned for these keywords\nand phrases it is now able to identify\nthe spam coming in and this makes your\nlife easier so you're not getting\nspammed it's not like walking through an\nairport and a lot of countries you have\nlike hundreds of people trying to sell\nyou timeshare come join us sign up for\nthis eliminates that annoyingness so now\nyou can just enjoy your facebook and\nyour cat pictures or maybe it's your\nfamily pictures mine is family certainly\npeople like their cat pictures too\nanother good example is google's\ndeepmind project alphago a computer\nprogram that plays a board game go has\ndefeated the world's number one go\nplayer and i hope i say his name right\nkiji the ultimate go challenge game of\nthree of three was on may 27 2017 so\nit's just last year that this happened\nand what makes this so important is that\nyou know go is just is a game so it's\nnot like you're driving a car or\nsomething in our real world but they are\nusing games to learn how to get the\nmachine learning program to learn they\nwant it to learn how to learn and that\nis a huge step a lot of this is still in\nits infant stage as far as development\nas we saw what happened with the as i\nreferred to earlier the uber cars they\nlost their whole division because they\njumped ahead too fast so still an infant\nstage but boy is this like the beginning\nof just an amazing world that is\nautomated in ways we can't even imagine\nwhat tomorrow's going to look like we've\nlooked at a lot of examples of machine\nlearning so let's see if we can give a\nlittle bit more of a concrete definition\nwhat is machine learning machine\nlearning is the science of making\ncomputers learn and act like humans by\nfeeding data and information without\nbeing explicitly programmed\nwe see here we have a nice little\ndiagram where we have our ordinary\nsystem uh your computer nowadays you can\neven run a lot of the stuff on a cell\nphone because cell phones advance so\nmuch\nand then with artificial intelligence\nand machine learning it now takes the\ndata and it learns from what happened\nbefore and then it predicts what's going\nto come next and then really the biggest\npart right now in machine learning\nthat's going on is it improves on that\nhow do we find a new solution so we go\nfrom descriptive words learning about\nstuff and understanding how it fits\ntogether to predicting what it's going\nto do to post scripting coming up with a\nnew solution and when we're working on\nmachine learning there's a number of\ndifferent diagrams that people have\nposted for what steps to go through a\nlot of it might be very domain specific\nso if you're working on\nphoto identification versus language\nversus medical or physics some of these\nare switched around a little bit or new\nthings are put in they're very specific\nto the domain this is kind of a very\ngeneral diagram first you want to define\nyour objective very important to know\nwhat it is you're wanting to predict\nthen you're going to be collecting the\ndata so once you've defined an objective\nyou need to collect the data that\nmatches\nyou spend a lot of time in data science\ncollecting data and the next step\npreparing the data you've got to make\nsure that your data is clean going in\nthere's the old saying bad data in bad\nanswer out or bad data out and then once\nyou've gone through and we've cleaned\nall this stuff coming in then you're\ngoing to select the algorithm which\nalgorithm are you going to use you're\ngoing to train that algorithm in this\ncase i think we're going to be working\nwith svm the support vector machine then\nyou have to test the model does this\nmodel work is this a valid model for\nwhat we're doing and then once you've\ntested it you want to run your\nprediction you want to run your\nprediction or your choice or whatever\noutput it's going to come up with and\nthen once everything is set and you've\ndone lots of testing then you want to go\nahead and deploy the model remember i\nsaid domain specific this is very\ngeneral as far as the scope of doing\nsomething a lot of models you get\nhalfway through and you realize that\nyour data is missing something and you\nhave to go collect new data because\nyou've run a test in here someplace\nalong the line you're saying hey i'm not\nreally getting the answers i need so\nthere's a lot of things that are domain\nspecific that become part of this model\nthis is a very general model but it's a\nvery good model to start with and we do\nhave some basic divisions of what\nmachine learning does that's important\nto know for instance do you want to\npredict a category well if you're\ncategorizing thing that's classification\nfor instance whether the stock price\nwill increase or decrease so in other\nwords i'm looking for a yes no answer is\nit going up or is it going down and in\nthat case we'd actually say is it going\nup true if it's not going up it's false\nmeaning it's going down this way it's a\nyes no zero one do you want to predict a\nquantity that's regression so remember\nwe just did classification now we're\nlooking at regression these are the two\nmajor divisions and what data is doing\nfor instance predicting the age of a\nperson based on the height weight health\nand other factors so based on these\ndifferent factors you might guess how\nold a person is\nand then there are a lot of domain\nspecific things like do you want to\ndetect an anomaly that's anomaly\ndetection this is actually very popular\nright now for instance you want to\ndetect money withdrawal anomalies you\nwant to know when someone's making a\nwithdrawal that might not be their own\naccount we've actually brought this up\nbecause this is really big right now if\nyou're predicting the stock whether to\nbuy stock or not you want to be able to\nknow if what's going on in the stock\nmarket is an anomaly use a different\nprediction model because something else\nis going on you got to pull out new\ninformation in there or is this just the\nnorm i'm going to get my normal return\non my money invested so being able to\ndetect anomalies is very big in data\nscience these days\nanother question that comes up which is\non what we call untrained data is do you\nwant to discover structure in unexplored\ndata and that's called clustering for\ninstance finding groups of customers\nwith similar behavior given a large\ndatabase of customer data containing\ntheir demographics and past buying\nrecords and in this case we might notice\nthat anybody who's wearing certain set\nof shoes go shopping at certain stores\nor whatever it is they're going to make\ncertain purchases by having that\ninformation it helps us to market or\ngroup people together so that we can now\nexplore that group and find out what it\nis we want to market to them if you're\nin the marketing world and that might\nalso work in just about any arena you\nmight want to group people together\nwhether they're based on their different\nareas and investments and financial\nbackground\nwhether you're going to give them a loan\nor not before you even start looking at\nwhether they're a valid customer for the\nbank you might want to look at all these\ndifferent areas and group them together\nbased on unknown data so you're not you\ndon't know what the data is going to\ntell you but you want to cluster people\ntogether that come together\nlet's take a quick detour for\nquiz time oh my favorite so we're going\nto have a couple questions here under\nquiz time and we'll be posting the\nanswers in this part 2 of this tutorial\nso let's go ahead and take a look at\nthese quiz times questions and hopefully\nyou'll get them all right it'll get you\nthinking about how to process data and\nwhat's going on can you tell what's\nhappening in the following cases of\ncourse you're sitting there with your\ncup of coffee and you have your checkbox\nand your pen trying to figure out what's\nyour next step in your data science\nanalysis\nso the first one is grouping documents\ninto different categories based on the\ntopic and content of each document very\nbig these days\nyou know you have legal documents you\nhave maybe it's the sports group\ndocuments maybe you're analyzing\nnewspaper postings\nbut certainly having that automated is a\nhuge thing in today's world\nb identifying handwritten digits in\nimages correctly\nso we want to know whether they're\nwriting an a or capital a b c\nwhat are they writing out in their hand\ndigit their handwriting c behavior of a\nwebsite indicating that the site is not\nworking as designed\nd predicting salary of an individual\nbased on his or her years of experience\nhr hiring\nset up there so stay tuned for part two\nwe'll go ahead and answer these\nquestions when we get to the part two of\nthis tutorial or you can just simply\nwrite at the bottom and send a note to\nsimply learn and they'll follow up with\nyou on it\nback to our regular content\nnow these last few bring us into the\nnext topic which is another way of\ndividing our types of machine learning\nand that is with supervised\nunsupervised\nand reinforcement learning supervised\nlearning is a method used to enable\nmachines to classify predict objects\nproblems or situations based on labeled\ndata fed to the machine and in here you\nsee we have a jungle of data with\ncircles triangles and squares then we\nlabel them we have what's a circle\nwhat's a triangle what's a square and we\nhave our model training and it trains it\nso we know the answer very important\nwhen you're doing supervised learning\nyou already know the answer to a lot of\nyour information coming in so you have a\nhuge group of data coming in and then\nyou have a new data coming in so we've\ntrained our model the model now knows\nthe difference between a circle a square\na triangle and now that we've trained it\nwe can send in in this case a square and\na circle goes in and it predicts that\nthe top one's a square and the next\none's a circle and you can see that this\nis uh being able to predict whether\nsomeone's going to default on a loan\nbecause i was talking about banks\nearlier supervised learning on stock\nmarket whether you're going to make\nmoney or not that's always important and\nif you are looking to make a fortune in\nthe stock market keep in mind it is very\ndifficult to get all the data correct on\nthe stock market it is very it\nfluctuates and weighs you really hard to\npredict so it's quite a roller coaster\nride if you're running machine learning\non the stock market you start realizing\nyou really have to dig for new data so\nwe have supervised learning and if you\nhave supervised we should need\nunsupervised learning in unsupervised\nlearning machine learning model finds\nthe hidden pattern in an unlabeled data\nso in this case instead of telling it\nwhat the circle is and what a triangle\nis and what a square is it goes in there\nlooks at them and says for whatever\nreason it groups them together maybe\nthey'll group it by the number of\ncorners and it notices that a number of\nthem all have three corners a number of\nthem all have four corners and a number\nof them all have no corners and it's\nable to filter those through and group\nthem together we talked about that\nearlier with looking at a group of\npeople who are out shopping we want to\ngroup them together to find out what\nthey have in common and of course once\nyou understand what people have in\ncommon maybe you have one of them is a\ncustomer at your store or you have five\nof them our customer your store and they\nhave a lot in common with five others\nwho are not customers at your store how\ndo you market to those five who aren't\ncustomers at your store yet they fit the\ndemographics of who's going to shop\nthere and you'd like them to shop at\nyour store not the one next door of\ncourse this is a simplified version you\ncan see very easily the difference\nbetween a triangle and a circle which\nmight not be so easy in marketing\nreinforcement learning reinforcement\nlearning is an important type of machine\nlearning where an agent learns how to\nbehave in an environment by performing\nactions and seeing the result we have\nhere where the in this case a baby it's\nactually great that they used an infant\nfor this slide because the reinforcement\nlearning is very much in its infant\nstages but it's also probably the\nbiggest machine learning demand out\nthere right now or in the future it's\ngoing to be coming up over the next few\nyears is reinforcement learning and how\nto make that work for us and you can see\nhere where we have our action in the\naction on this one it goes into the fire\nhopefully the baby didn't it was just a\nlittle candle not a giant fire pit like\nit looks like here when the baby comes\nout and the new state is the baby is sad\nand crying because they got burned on\nthe fire and then maybe they take\nanother action the baby's called the\nagent because it's the one taking the\nactions and in this case they didn't go\ninto the fire they went a different\ndirection and now the baby's happy and\nlaughing and playing reinforcement\nlearning is very easy to understand\nbecause that's how as humans that's one\nof the ways we learn we learn whether it\nis you bring yourself on the stove don't\ndo that anymore don't touch the stove in\nthe big picture being able to have a\nmachine learning program or an ai be\nable to do this is huge\nbecause now we're starting to learn how\nto learn that's a big jump in the world\nof computer and machine learning and\nwe're going to go back and just kind of\ngo back over supervised versus\nunsupervised learning understanding this\nis huge because this is going to come up\nin any project you're working on\nwe have in supervised learning we have\nlabeled data we have direct feedback so\nsomeone's already gone in there and said\nyes it's a triangle no that's not a\ntriangle and then you predict an outcome\nso you have a nice prediction this is\nthis this new set of data is coming in\nand we know what it's going to be and\nthen with unsupervised training it's not\nlabeled so we really don't know what it\nis there's no feedback so we're not\ntelling it whether it's right or wrong\nwe're not telling it whether it's a\ntriangle or a square we're not telling\nit to go left or right all we do is\nwe're finding hidden structure in the\ndata grouping the data together to find\nout what connects to each other and then\nyou can use these together so imagine\nyou have an image and you're not sure\nwhat you're looking for so you go in and\nyou have the unstructured data find all\nthese things that are connected together\nand then somebody looks at those and\nlabels them now you can take that label\ndata and program something to predict\nwhat's in the picture so you can see how\nthey go back and forth and you can start\nconnecting all these different tools\ntogether to make a bigger picture there\nare many interesting machine learning\nalgorithms let's have a look at a few of\nthem hopefully this gives you a little\nflavor what's out there and these are\nsome of the most important ones that are\ncurrently being used we'll take a look\nat linear regression decision tree and\nthe support vector machine\nlet's start with a closer look at linear\nregression linear regression is perhaps\none of the most well-known and\nwell-understood algorithms in statistics\nand machine learning linear regression\nis a linear model for example a model\nthat assumes a linear relationship\nbetween the input variables x and the\nsingle output variable y\nand you'll see this if you remember from\nyour algebra classes y equals mx plus c\nimagine we are predicting distance\ntraveled y from speed x our linear\nregression model representation for this\nproblem would be y equals m times x plus\nc or\ndistance equals m times speed plus c\nwhere m is the coefficient and c is the\ny-intercept and we're going to look at\ntwo different variations of this first\nwe're going to start with time is\nconstant and you can see we have a\nbicyclist he's got a safety gear on\nthank goodness\nspeed equals 10 meters per second and so\nover a certain amount of time his\ndistance equals 36 kilometers we have a\nsecond bicyclist who's going twice the\nspeed or 20 meters per second and you\ncan guess if he's going twice the speed\nand time is a constant then he's going\nto go twice the distance and that's easy\nto compute 36 times 2 you get 72\nkilometers and so if you had the\nquestion of\nhow fast would somebody is going three\ntimes that speed or 30 meters per second\nis you can easily compute the distance\nin our head we can do that without\nneeding a computer but we want to do\nthis for more complicated data so it's\nkind of nice to compare the two but\nlet's just take a look at that and what\nthat looks like in a graph\nso in a linear regression model we have\nour distance to the speed and we have\nour m equals the ve slope of the line\nand we'll notice that the line has a\nplus slope and as the speed increases\ndistance also increases hence the\nvariables have a positive relationship\nand so your speed of the person which\nequals y equals mx plus c distance\ntraveled in a fixed interval of time and\nwe could very easily compute either\nfollowing the line or just knowing it's\n3 times 10 meters per second that this\nis roughly 102 kilometers distance that\nthis third bicyclist has traveled one of\nthe key\ndefinitions on here is positive\nrelationship so the slope of the line is\npositive as distance increase so does\nspeed increase let's take a look at our\nsecond example where we put distance is\na constant so we have speed equals 10\nmeters per second they have a certain\ndistance to go and it takes him 100\nseconds to travel that distance and we\nhave our second bicyclist who's still\ndoing 20 meters per second since he's\ngoing twice the speed we can guess that\nhe'll cover the distance in about half\nthe time 50 seconds and of course you\ncould probably guess on the third one\n100 divided by 30 since he's going 3\ntimes the speed you can easily guess\nthat this is 33.333\nseconds time we put that into a linear\nregression model or graph if the\ndistance is assumed to be constant let's\nsee the relationship between speed and\ntime and as time goes up the amount of\nspeed to go that same distance goes down\nso now your m equals a minus ve slope of\nthe line as the speed increases time\ndecreases hence the variable has a\nnegative relationship again there's our\ndefinition positive relationship and\nnegative relationship dependent on the\nslope of the line and with a simple\nformula like this\nand even a significant amount of data\nlet's see with the mathematical\nimplementation of linear regression and\nwe'll take this data so suppose we have\nthis data set where we have x y x equals\n1 2 3 4 5 standard series and the y\nvalue is 3\n2 2 4 3. when we take that and we go\nahead and plot these points on a graph\nyou can see there's kind of a nice\nscattering and you could probably\neyeball a line through the middle of it\nbut we're going to calculate that exact\nline for linear regression and the first\nthing we do is we come up here we have\nthe mean of x i and remember mean is\nbasically the average so we added five\nplus four plus three plus two plus one\nand divide by five and that simply comes\nout as three and then we'll do the same\nfor y we'll go ahead and add up all\nthose numbers and divide by five and we\nend up with the mean value of y of i\nequals two point eight where the x i\nreferences it's an average or means\nvalue and the y i also equals a means\nvalue of y and when we plot that you'll\nsee that we can put in the y equals 2.8\nand the x equals 3 in there on our graph\nwe kind of give it a little different\ncolor so you can sort it out with the\ndashed lines on it and it's important to\nnote that when we do the linear\nregression the linear regression model\nshould go through that dot\nnow let's find our regression equation\nto find the best fit line remember we go\nahead and take our y equals mx plus c so\nwe're looking for m and c so to find\nthis equation for our data we need to\nfind our slope of m and our coefficient\nof c\nand we have y equals mx plus c\nwhere m equals the sum of x minus x\naverage times y minus y average or y\nmeans and x means over the sum of x\nminus x means squared that's how we get\nthe slope of the value of the line and\nwe can easily do that by creating some\ncolumns here we have x y computers are\nreally good about iterating through data\nand so we can easily compute this and\nfill in a graph of data and in our graph\nyou can easily see that if we have our x\nvalue of 1 and if you remember the x i\nor the means value is 3 1 minus 3 equals\na negative 2 and 2 minus 3 equals a\nnegative 1 so on and so forth and we can\neasily fill in the column of x minus x i\ny minus y i and then from those we can\ncompute x minus x i squared and x minus\nx i times y minus y i and you can guess\nit that the next step is to go ahead and\nsum the different columns for the\nanswers we need so we get a total of 10\nfor our x minus x i squared and a total\nof 2 for x minus x i times y minus y i\nand we plug those in we get 2 10 which\nequals 0.2 so now we know the slope of\nour line equals 0.2 so we can calculate\nthe value of c that'd be the next step\nis we need to know where it crosses the\ny axis and if you remember i mentioned\nearlier that the linear regression line\nhas to pass through\nthe means value the one that we showed\nearlier we can just flip back up there\nto that graph and you can see right here\nthere's our means value which is 3\nx equals 3 and y equals 2.8 and since we\nknow that value we can simply plug that\ninto our formula y equals 0.2 x plus c\nso we plug that in we get 2.8 equals 0.2\ntimes 3 plus c and you can just solve\nfor c so now we know that our\ncoefficient equals 2.2 and once we have\nall that we can go ahead and plot our\nregression line y equals 0.2 times x\nplus 2.2 and then from this equation we\ncan compute new values so let's predict\nthe values of y using x equals 1 2 3 4 5\nand plot the points remember the 1 2 3 4\n5 was our original x values so now we're\ngoing to see what y thinks they are not\nwhat they actually are and we plug those\nin we get y of designated with y of p\nyou can see that x equals 1 equals 2.4 x\nequals 2 equals 2.6 and so on and so on\nso we have our y predicted values of\nwhat we think is going to be when we\nplug those numbers in and when we plot\nthe predicted values along with the\nactual values we can see the difference\nand this is one of the things that's\nvery important with linear aggression in\nany of these models is to understand the\nerror and so we can calculate the error\non all of our different values and you\ncan see over here we plotted x and y and\ny predict and we've drawn a little line\nso you can sort of see what the error\nlooks like there between the different\npoints so our goal is to reduce this\nerror we want to minimize that error\nvalue on our linear regression model\nminimizing the distance there are lots\nof ways to minimize the distance between\nthe line and the data points like sum of\nsquared errors sum of absolute errors\nroot mean square error etc we keep\nmoving this line through the data points\nto make sure the best fit line has the\nleast square distance between the data\npoints and the regression line so to\nrecap with a very simple linear\nregression model we first figure out the\nformula of our line through the middle\nand then we slowly adjust the line to\nminimize the error keep in mind this is\na very simple formula the math gets even\nthough the math is very much the same it\ngets much more complex as we add in\ndifferent dimensions so this is only two\ndimensions y equals mx plus c but you\ncan take that out to x\nz i j q all the different features in\nthere and they can plot a linear\nregression model on all of those using\nthe different formulas to minimize the\nerror let's go ahead and take a look at\ndecision trees a very different way to\nsolve problems in the linear regression\nmodel decision tree is a tree-shaped\nalgorithm used to determine a course of\naction each branch of a tree represents\na possible decision occurrence or\nreaction we have data which tells us if\nit is a good day to play golf and if we\nwere to open this data up in a general\nspreadsheet you can see we have the\noutlook whether it's a rainy overcast\nsunny temperature\nhot mild cool humidity windy and did i\nlike to play golf that day yes or no so\nwe're taking a census and certainly i\nwouldn't want a computer telling me when\ni should go play golf or not but you\ncould imagine if you got up in the night\nbefore you're trying to plan your day\nand it comes up and says tomorrow would\nbe a good day for golf for you in the\nmorning and not a good day in the\nafternoon or something like that this\nbecomes very beneficial and we see this\nin a lot of applications coming out now\nwhere it gives you suggestions and lets\nyou know what would be fit the match for\nyou for the next day or the next\npurchase or the next uh whatever you\nknow next meal out in this case is\ntomorrow a good day for playing golf\nbased on the weather coming in and so we\ncome up and let's uh determine if you\nshould play golf when the day is sunny\nand windy so we found out the forecast\ntomorrow is going to be sunny and windy\nand suppose we draw our tree like this\nwe're going to have our humidity and\nthen we have our normal which is if it's\nif you have a normal humidity you're\ngoing to go play golf and if the\nhumidity is really high then we look at\nthe outlook and if the outlook is sunny\novercast or rainy it's going to change\nwhat you choose to do so if you know\nthat it's a very high humidity and it's\nsunny you're probably not going to play\ngolf because you're going to be out\nthere miserable fighting off the\nmosquitoes that are out joining you to\nplay golf with you maybe if it's raining\nyou probably don't want to play in the\nrain but if it's slightly overcast you\nget just the right\nshadow that's a good day to play golf\nand be outside out on the green now in\nthis example you can probably make your\nown creep pretty easily because it's a\nvery simple set of data going in but the\nquestion is how do you know what to\nsplit where do you split your data what\nif this is much more complicated data\nwhere it's not something that you would\nparticularly understand like studying\ncancer they take about 36 measurements\nof the cancerous cells and then each one\nof those measurements represents how\nbulbous it is how extended it is how\nsharp the edges are something that as a\nhuman we would have no understanding of\nso how do we decide how to split that\ndata up and is that the right decision\ntree but since the question is going to\ncome up is this the right decision tree\nfor that we should calculate entropy and\ninformation gain two important\nvocabulary words there are the entropy\nand the information gain entropy entropy\nis a measure of randomness or impurity\nin the data set entropy should be low so\nwe want the chaos to be as low as\npossible we don't want to look at it and\nbe confused by the images or what's\ngoing on there with mixed data and the\ninformation gain it is a measure of\ndecrease in entropy after the data set\nis split also known as entropy reduction\ninformation gain should be high so we\nwant our information that we get out of\nthe split to be as high as possible\nlet's take a look at entropy from the\nmathematical side in this case we're\ngoing to denote entropy as i of p of and\nn where p is the probability that you're\ngoing to play a game of golf and n is\nthe probability where you're not going\nto play the game of golf now you don't\nreally have to memorize these formulas\nthere's a few of them out there\ndepending on what you're working with\nbut it's important to note that this is\nwhere this formula is coming from so\nwhen you see it you're not lost when\nyou're running your programming unless\nyou're building your own decision tree\ncode in the back and we simply have a\nlog squared of p over p plus n minus n\nover p plus n times the log squared of n\nof p plus n but let's break that down\nand see what actually looks like when\nwe're computing that from the computer\nscript side\nentropy of a target class of the data\nset is the whole entropy so we have\nentropy play golf and we look at this if\nwe go back to the data you can simply\ncount how many yeses and no in our\ncomplete data set for playing golf days\nin our complete set we find we have five\ndays we did play golf and nine days we\ndid not play golf and so our i equals if\nyou had those together 9 plus 5 is 14.\nand so our i equals 5 over 14 and 9 over\n14 that's our p and n values that we\nplug into that formula and you can go to\n5 over 14 equals 0.36 9 over 14 equals\n0.64 and when you do the whole equation\nyou get the minus 0.36 log root squared\nof 0.36 minus 0.64 log squared root of\n0.64 and we get a set value we get\n0.94 so we now have a full entropy value\nfor the whole set of data that we're\nworking with and we want to make that\nentropy go down and just like we\ncalculated the entropy out for the whole\nset we can also calculate entropy for\nplaying golf and the outlook is it going\nto be overcast or rainy or sunny and so\nwe look at the entropy we have p of\nsunny times e of 3 of 2 and that just\ncomes out how many sunny days yes and\nhow many sunny days no over the total\nwhich is five don't forget to put the\nwe'll divide that five out later on\nequals p overcast equals four comma zero\nplus rainy equals two comma three and\nthen when you do the whole setup we have\n5 over 14. remember i said there was a\ntotal of 5. 5 over 14 times the i of 3\nof 2 plus 4 over 14 times the 4 comma 0\nand 514 over i of 2 three and so we can\nnow compute the entropy of just the part\nit has to do with the forecast and we\nget .693 similarly we can calculate the\nentropy of other predictors like\ntemperature humidity and wind and so we\nlook at the gain outlook how much are we\ngoing to gain from this entropy play\ngolf minus entropy play golf outlook and\nwe can take the original 0.94 for the\nwhole set minus the entropy of just the\nrainy day and temperature and we end up\nwith a gain of 0.247\nso this is our information gain remember\nwe define entropy and we define an\ninformation gain the higher the\ninformation gain the lower the entropy\nthe better the information gain of the\nother three attributes can be calculated\nin the same way so we have our gain for\ntemperature equals\n0.029 we have our gain for humidity\nequals 0.152 and our gain for a windy\nday equals 0.048 and if you do a quick\ncomparison you'll see the 0.247\nis the greatest gain of information so\nthat's the split we want\nnow let's build the decision tree so we\nhave the outlook is it going to be sunny\novercast or rainy that's our first split\nbecause that gives us the most\ninformation gain and we can continue to\ngo down the tree using the different\ninformation gains with the largest\ninformation we can continue down the\nnodes of the tree where we choose the\nattribute with the largest information\ngain as the root node and then continue\nto split each subnode with the largest\ninformation gain that we can compute and\nalthough it's a little bit of a tongue\ntwister to say all that you can see that\nit's a very easy to view visual model we\nhave our outlook we split it three\ndifferent directions if the outlook is\novercast we're going to play and then we\ncan split those further down if we want\nso if the outlook is sunny but then it's\nalso windy if it's windy we're not going\nto play if it's not windy we'll play so\nwe can easily build a nice decision tree\nto guess what we would like to do\ntomorrow and give us a nice\nrecommendation for the day so we want to\nknow if it's a good day to play golf\nwhen it's sunny and windy remember the\noriginal question that came out\ntomorrow's weather report is sunny and\nwindy you can see by going down the tree\nwe go outlook sunny outlook windy we're\nnot going to play golf tomorrow so our\nlittle smartwatch pops up and says i'm\nsorry tomorrow is not a good day for\ngolf it's going to be sunny and windy\nand if you're a huge golf fan you might\ngo uh-oh\nit's not a good day to play golf we can\ngo in and watch a golf game at home so\nwe'll sit in front of the tv instead of\nbeing out playing golf in the wind\nnow that we've looked at our decision\ntree let's look at the third one of our\nalgorithms we're investigating support\nvector machine support vector machine is\na widely used classification algorithm\nthe idea of support vector machine is\nsimple the algorithm creates a\nseparation line which divides the\nclasses in the best possible manner for\nexample dog or cat disease or no disease\nsuppose we have a labeled sample data\nwhich tells height and weight of males\nand females\na new data point arrives and we want to\nknow whether it's going to be a male or\na female so we start by drawing a line\nwe draw decision lines but if we\nconsider decision line one then we will\nclassify the individual as a male and if\nwe consider decision line two then it\nwill be a female so you can see this\nperson kind of lies in the middle of the\ntwo groups it's a little confusing\ntrying to figure out which line they\nshould be under we need to know which\nline divides the classes correctly but\nhow the goal is to choose a hyperplane\nand that is one of the key words they\nuse when we talk about support vector\nmachines choose a hyperplane with the\ngreatest possible margin between the\ndecision line and the nearest point\nwithin the training set\nso you can see here we have our support\nvector we have the two nearest points to\nit and we draw a line between those two\npoints and the distance margin is the\ndistance between the hyperplane and the\nnearest data point from either set so we\nactually have a value and it should be\nequal the distance between the two\npoints that we're comparing it to when\nwe draw the hyperplanes we observe that\nline one has a maximum distance so we\nobserve that line one has a maximum\ndistance margin so we'll classify the\nnew data point correctly and our result\non this one is going to be that the new\ndata point is mel one of the reasons we\ncall it a hyperplane versus a line\nis that\na lot of times we're not looking at just\nweight and height we might be looking at\n36 different features or dimensions and\nso when we cut it with a hyperplane it's\nmore of a three-dimensional cut in the\ndata multi-dimensional it cuts the data\na certain way and each plane continues\nto cut it down until we get the best fit\nor match let's understand this with the\nhelp of an example problem statement i\nalways start with a problem statement\nwhen you're going to put some code\ntogether we're going to do some coding\nnow classifying muffin and cupcake\nrecipes using support vector machines so\nthe cupcake versus the muffin let's have\na look at our data set and we have the\ndifferent recipes here we have a muffin\nrecipe that has so much flour i'm not\nsure what measurement 55 is in but it\nhas 55 maybe it's ounces\nbut has a certain amount of flour\ncertain amount of milk sugar butter egg\nbaking powder vanilla and salt and so\nbased on these measurements we want to\nguess whether we're making a muffin or a\ncupcake and you can see in this one we\ndon't have just two features we don't\njust have height and weight as we did\nbefore between the male and female in\nhere we have a number of features\nin fact in this we're looking at eight\ndifferent features to guess whether it's\na muffin or a cupcake\nwhat's the difference between a muffin\nand a cupcake turns out muffins have\nmore flour while cupcakes have more\nbutter and sugar so basically the\ncupcake's a little bit more of a dessert\nwhere the muffins a little bit more of a\nfancy bread but how do we do that in\npython how do we code that to go through\nrecipes and figure out what the recipe\nis\nand i really just want to say cupcakes\nversus muffins like some big\nprofessional wrestling thing before we\nstart in our cupcakes versus muffins we\nare going to be working in python\nthere's many versions of python many\ndifferent editors that is one of the\nstrengths and weaknesses of python is it\njust has so much stuff attached to it\nit's one of the more popular data\nscience programming packages you can use\nin this case we're going to go ahead and\nuse anaconda and jupiter nope the\nanaconda navigator has all kinds of fun\ntools once you're into the anaconda\nnavigator you can change environments i\nactually have a number of environments\non here we'll be using python36\nenvironment so this is in python version\n3.6 although it doesn't matter too much\nwhich version you use i usually try to\nstay with the 3x because they're current\nunless you have a project that's very\nspecifically in version 2x 27 i think is\nusually what most people use in the\nversion 2. and then once we're in our\njupiter notebook editor i can go up and\ncreate a new file and we'll just jump in\nhere\nin this case we're doing svm muffin\nversus cupcake\nand then let's start with our packages\nfor\ndata analysis\nand we almost always use a couple\nthere's a few very standard packages we\nuse\nwe use\nimport import import\nnumpy\nthat's for number python they usually\ndenote it as np that's very comma\nthat's very common\nand then we're going to import pandas as\npd\nand numpy deals with number arrays\nthere's a lot of cool things you can do\nwith the numpy setup\nas far as multiplying all the values in\nan array in a numpy array data array\npandas\ni can't remember if we're using it\nactually in this data set i think we do\nas an import it makes a nice data frame\nand the difference between a data frame\nand a numpy array is that a data frame\nis more like your excel spreadsheet you\nhave columns you have indexes so you\nhave different ways of referencing it\neasily viewing it and there's additional\nfeatures you can run on a data frame and\npandas kind of sits on numpy so you need\nthem both in there\nand then finally we're working with the\nsupport vector machine so\nfrom sk learn we're going to use the sk\nlearn model\nimport svm support vector machine\nand then\nas a data scientist you should always\ntry to visualize your data some data\nobviously is too complicated or doesn't\nmake any sense to the human but if it's\npossible it's good to take a second look\nat it so you can actually see what\nyou're doing now for that we're going to\nuse two packages we're going to import\nmatplotlibrary.piplot as plt\nagain very common\nand we're going to import seaborn as sns\nand we'll go ahead and set the font\nscale in the sns right in our import\nline that's what this\nsemicolon followed by a line of data\nwe're going to set the sns and these are\ngreat because the the seaborn sits on\ntop of map plot library just like pandas\nhits on numpy so it adds a lot more\nfeatures and uses and control we're\nobviously not going to get into matplot\nlibrary and see born that be its own\ntutorial we're really just focusing on\nthe svm the support vector machine from\nsk learn and since we're in jupiter\nnotebook\nwe have to add a special line in here\nfor our matplot library and that's your\npercentage sign or ambersign matplot\nlibrary in line\nnow if you're doing this in just a\nstraight code project a lot of times i\nuse like notepad plus plus and i'll run\nit from there you don't have to have\nthat line in there because it'll just\npop up as its own window on your\ncomputer depending on how your computer\nis set up because we're running this in\nthe jupyter notebook as a browser setup\nthis tells it to display all of our\ngraphics right below on the page so\nthat's what that line is for remember\nthe first time i ran this i didn't know\nthat i had to go look that up years ago\nit was quite a headache so map plot\nlibrary inline is just because we're\nrunning this on the web setup and we can\ngo ahead and run this make sure all our\nmodules are in they're all imported\nwhich is great if you don't have an\nimport you'll need to go ahead and pip\nuse the pip or however you do it there's\na lot of other install packages out\nthere although pip is the most common\nand you have to make sure these are all\ninstalled on your python setup the next\nstep of course is we got to look at the\ndata you can't run a model for\npredicting data if you don't have actual\ndata so to do that let me go and open\nthis up and take a look and we have our\ncupcakes versus muffins\nand it's a csv file or csv meaning that\nit's comma separated variable\nand it's going to open it up in a nice\nspreadsheet for me and you can see up\nhere we have the type we have muffin\nmuffin muffin cupcake cupcake cupcake\nand then it's broken up into flour milk\nsugar butter egg baking powder vanilla\nand salt\nso we can do is we can go ahead and look\nat this data also in our python\nlet us create a variable recipes equals\nwe're going to use our pandas module\ndot read\ncsv remember is a comma separated\nvariable\nand the file name happened to be\ncupcakes versus muffins oops i got\ndouble brackets there\ndo it this way\nthere we go cupcakes versus muffins\nbecause the program i loaded or the the\nplace i saved this particular python\nprogram is in the same folder we get by\nwith just the file name but remember if\nyou're storing it in a different\nlocation you have to also put down the\nfull path on there\nand then because we're in pandas\nwe're going to go ahead and you can\nactually inline you can do this but let\nme do the full print you can just type\nin\nrecipes dot head\nin the jupiter notebook but if you're\nrunning in code in a different script\nyou need to go ahead and type out the\nwhole print recipes dot head\nand panda's nose is that's gonna do the\nfirst five lines of data and if we flip\nback on over to the spreadsheet where we\nopened up our csv file\nuh you can see where it starts on line\ntwo this one calls it zero and then\ntwo three four five six is going to\nmatch\ni'm going to close that out because we\ndon't need that anymore\nand it always starts at zero\nand these are it automatically indexes\nit since we didn't tell it to use an\nindex in here so that's the index number\nfor the left hand side and it\nautomatically took the top row\nas labels so pandas\nusing it to read a csv is just really\nslick and fast one of the reasons we\nlove our pandas\nnot just because they're cute and cuddly\nteddy bears\nand let's go ahead and plot our data\nand i'm not going to plot all of it i'm\njust going to plot the\nsugar and flower\nnow obviously\nyou can see where they get really\ncomplicated if we have tons of different\nfeatures\nand so you'll break them up and maybe\nlook at just two of them at a time to\nsee how they connect\nand to plot them we're going to go ahead\nand use seaborne\nso that's our sns\nand the command for that is sns.lm plot\nand then the two different variables i'm\ngoing to plot is flour and sugar\ndata equals recipes\nthe hue equals type\nand this is a lot of fun because it\nknows that this is pandas coming in\nso this is one of the powerful things\nabout pandas mixed with seaborne in\ndoing\ngraphing\nand then we're going to use a pallet set\none there's a lot of different sets in\nthere you can go look them up for\nseabourn or do a regular fit regular\nequals false so we're not really trying\nto fit anything\nand it's a scatter kws\na lot of these settings you look up in\nseabourn half of these you could\nprobably leave off when you run them\nsomebody played with this and found out\nthat these were the best settings for\ndoing a seaborn plot\nlet's go ahead and run that\nand because it does it in line it just\nputs it right on the page\nand you can see right here that just\nbased on sugar and flour alone\nthere's a definite split\nand we use these models because you can\nactually look at and say hey if i drew a\nline right between the middle of the\nblue dots and the red dots\nwe'd be able to do an svm and a\nhyperplane right there in the middle\nthen the next step is to\nformat or pre-process\nour data\nand we're going to break that up into\ntwo parts\nwe need a type\nlabel and remember we're going to decide\nwhether it's a muffin or a cupcake well\na computer doesn't know muffin or\ncupcake it knows 0 and 1.\nso what we're going to do is we're going\nto create a type label\nand from this we'll create a numpy array\nand p where\nand this is where we can do some logic\nwe take our recipes from our panda\nand wherever type equals muffin it's\ngoing to be 0 and then if it doesn't\nequal muffin which is cupcakes is going\nto be one so we create our type label\nthis is the answer so when we're doing\nour training model remember we have to\nhave a training data this is what we're\ngoing to train it with is that it's zero\nor one it's a muffin or it's\nnot\nand then we're going to create our\nrecipe\nfeatures\nand if you remember correctly from right\nup here the first column is type\nso we really don't need the type columns\nthat's our muffin or cupcake\nand in pandas we can easily sort that\nout\nwe take our value recipes\ndot\ncolumns that's a pandas function built\ninto pandas\nwe've got values converting them to\nvalues so it's just the column titles\ngoing across the top\nand we don't want the first one so what\nwe do is since it always starts at zero\nwe want one\ncolon till the\nend and then\nwe want to go ahead and make this a list\nand this converts it to a list of\nstrings\nand then we can go ahead and just take a\nlook and see we're looking at for the\nfeatures make sure it looks right\nlet me go ahead and run that\nand i forgot the s on recipes so we'll\ngo ahead and add the s in there and then\nrun that and we can see we have flour\nmilk sugar butter egg\nbaking powder vanilla and salt and that\nmatches what we have up here right where\nwe printed out everything but the type\nso we have our features and we have our\nlabel\nnow the recipe features is just the\ntitles of the columns and we actually\nneed\nthe ingredients\nand at this point we have a couple\noptions one we could run it over all the\ningredients\nand when you're doing this usually you\ndo but for our example we want to limit\nit so you can easily see what's going on\nbecause if we did all the ingredients\nwe have you know that's what\nseven eight different hyper planes that\nwould be built into it we only want to\nlook at once you can see what the svm is\ndoing\nand so we'll take our recipes and we'll\ndo just flour and sugar\nagain you can replace that with your\nrecipe features and do all of them but\nwe're going to do just flour and sugar\nand we're going to convert that to\nvalues\nwe don't need to make a list out of it\nbecause it's not string values these are\nactual\nvalues on there and we can go ahead and\njust print\ningredients and you can see what that\nlooks like\nand so we have just the amount of flour\nand sugar just the two sets of plots\nand just for fun let's go ahead and take\nthis over here and\ntake our recipe features\nand so if we decided to use all the\nrecipe features\nyou'll see that it makes a nice column\nof different data so it just strips out\nall the labels and everything we just\nhave just the values\nbut because we want to be able to view\nthis easily in a plot later on we'll go\nahead and take that and just do flower\nand sugar\nand we'll run that you'll see it's just\nthe two columns\nso the next step is to go ahead and fit\nour model\nwe're going to just call it model\nand it's a svm we're using a package\ncalled svc\nin this case we're going to go ahead and\nset the kernel equals linear so it's\nusing a specific set up on there and if\nwe go to the reference on their website\nfor the svm\nyou'll see that there's about there's\neight of them here\nthree of them are for regression\nthree are for classification the svc\nsupport vector classification is\nprobably one of the most commonly used\nand then there's also one for detecting\noutliers\nand another one that has to do with\nsomething a little bit more specific on\nthe model but svc and sbr are the two\nmost commonly used standing for support\nvector classifier and support vector\nregression remember regression is an\nactual value a float value or\nwhatever you're trying to work on and\nsbc is a classifier so it's a yes no\ntrue false\nbut for this we want to know zero one\nmuffin cupcake\nif we go ahead and create our model and\nonce we have our model created we're\ngonna do model.fit and this is very\ncommon especially in the sk learn all\ntheir models are followed with the fit\ncommand\nand what we put into the fit what we're\ntraining with it is we're putting in the\ningredients which in this case we\nlimited to just flour and sugar\nand the type label is it a muffin or\ncupcake\nnow in more complicated data science\nseries you'd want to split into we won't\nget into that today we split it into\ntraining data and test data and they\neven do something where they split it\ninto thirds where a third is used for\nwhether you switch between which one's\ntraining and test there's all kinds of\nthings go into that it gets very\ncomplicated when you get to the higher\nend not overly complicated just an extra\nstep which we're not going to do today\nbecause this is a very simple set of\ndata\nand let's go ahead and run this and now\nwe have our model fit\nand i got an error here so let me fix\nthat real quick\ncapital svc it turns out\ni did it lower case\nsupport vector\nclassifier there we go let's go ahead\nand run that\nand you'll see it comes up with all this\ninformation that it prints out\nautomatically\nthese are the defaults of the model\nyou notice that we change the kernel to\nlinear and there's our kernel linear on\nthe printout and there's other different\nsettings you can mess with\nwe're going to just leave that alone for\nright now for this we don't really need\nto mess with any of those\nso next we're going to dig a little bit\ninto our newly trained model\nand we're going to do this so we can\nshow you on a graph\nlet's go ahead and get the separating\nand we're going to say we're going to\nuse a w for our variable on here\nwe're going to do model\ndot coefficient underscore 0.\nso what the heck is that again we're\ndigging into the model so we've already\ngot a prediction and a train\nthis is the math behind it that we're\nlooking at right now\nand so\nthe w is going to represent two\ndifferent coefficients and if you\nremember we had y equals mx plus c\nso these coefficients are connected to\nthat\nbut in two dimensional it's a plane\nwe don't spend\ntoo much time on this because you can\nget lost in the confusion of the math so\nif you're a math whiz this is great you\ncan go through here and you'll see that\nwe have a equals minus w0 over w of one\nremember there's two different values\nthere\nand that's basically the slope that\nwe're generating\nand then we're going to build an xx what\nis xx we're going to set it up to a\nnumpy array there's our np dot line\nspace so we're creating a line\nplus the intercept well to make this\nwork we can do this as y y\nequals the slope times each value in\nthat array that's a neat thing about\nnumpy so when i do a times x x which is\na whole numpy array of values it\nmultiplies a across all of them\nand then it takes those same values and\nwe subtract the model intercept that's\nhere uh\nwhere we had mx plus c so that'd be the\nc from the formula y equals mx plus c\nand that's where all these numbers come\nfrom a little bit confusing because it's\ndigging out of these different arrays\nand then we want to do is we're going to\ntake this and we're going to go ahead\nand plot it\nso plot the parallels to separating\nhyperplane that pass through the support\nvectors\nand so we're going to create b equals a\nmodel\nsupport vectors\npulling our support vectors out there\nhere's our yy which we now know is a set\nof data and we have we're going to\ncreate yy down\nequals a times xx plus\nb1 minus a times b0\nand then model support vector b is going\nto be set that to a new value the minus\n1 set up and y y up equals a times x x\nplus b 1 minus a times b 0.\nand we go ahead and just run this to\nload these variables up if you want to\nknow understand a little bit more of\nwhat's going on you can see if we print\ny y let me just run that\nyou can see it's an array this is a line\nit's going to have in this case between\n30 and 60 so it's going to be 30\nvariables in here\nand the same thing with yy up yy down\nand we'll we'll plot those in just a\nminute on the graph so you can see what\nthose look like\njust go ahead and delete that out of\nhere and run that so it loads up the\nvariables nice clean slate\ni'm just going to copy this from before\nremember this our sns\nour seaborn plot lm plot flower sugar\nand i'll just go ahead and run that real\nquick so you can see remember what that\nlooks like it's just a straight graph on\nthere\nand then one of the new things is\nbecause seaborn sits on top of pie plot\nwe can do the pie plot for the line\ngoing through and that is simply plt dot\nplot\nand that's our xx\nand yy our two corresponding values xy\nand then somebody played with this to\nfigure out that the line width equals\ntwo in the color black would look nice\nso let's go ahead and run this whole\nthing with the pie plot on there and you\ncan see when we do this it's just doing\nflour and sugar on here\ncorresponding line between the sugar and\nthe flour and the muffin versus cupcake\nand then we generated the\nsupport vectors the yy down and yy out\nso let's take a look and see what that\nlooks like\nso we'll do our pl plot\nand again this is all against xx\nor our x value but this time we have yy\ndown\nand let's do something a little fun with\nthis\nwe can put in a k dash dash\nthat just tells it to make it a dotted\nline\nand if we're going to do the down one\nwe also want to do the up one\nso here's our yy\nup\nand when we run that it has both sets of\nline\nand so here's our support and this is\nwhat you expect you expect these two\nlines to go through the nearest data\npoint so the dashed lines go through the\nnearest muffin and the nearest cupcake\nwhen it's plotting it and then your svm\ngoes right down the middle so it gives\nit a nice split in our data and you can\nsee how easy it is to see based just on\nsugar and flour\nwhich one's a muffin or a cupcake\nlet's go ahead and create a function\nto predict\nmuffin or cupcake\ni've got my\nrecipes i pulled off the\ninternet and i want to see the\ndifference between a\nmuffin or a cupcake\nso we need a function to push that\nthrough\nand create a function with def\nand let's call it muffin or cupcake now\nremember we're just doing flour and\nsugar today we're not doing all the\ningredients\nand that actually is a pretty good split\nyou really don't need all the\ningredients you know it's flour and\nsugar\nand let's go ahead and do an if else\nstatement so if model predict\nis a flower and sugar equals zero\nso we take our model and we do it run a\npredict it's very common in sk learn we\nhave a dot predict you put the data in\nand it's going to return a value in this\ncase if it equals zero then print you're\nlooking at a muffin recipe\nelse if it's not zero that means it's\none then you're looking at a cupcake\nrecipe\nthat's pretty straightforward\nfor function or def for definition d e f\nis how you do that in python\nand of course we're going to create a\nfunction you should run something in it\nand so let's run a cupcake and we're\ngoing to send it values 50 and 20 a\nmuffin or cupcake i don't know what it\nis\nand let's run this and just see what it\ngives us\nand it says oh it's a muffin you're\nlooking at a muffin recipe\nso it very easily predicts whether we're\nlooking at a muffin or a cupcake recipe\nlet's\nplot this\nthere we go plot this on the graph so we\ncan see what that actually looks like\nand i'm just going to copy and paste it\nfrom below where we're plotting all the\npoints in there\nso this is nothing different than we did\nbefore if i run it\nyou'll see it has all the points and the\nlines on there and what we want to do is\nwe want to add another point\nand we'll do plt plot\nand if you remember correctly we did for\nour test we did 50\nand\n20. and then somebody went in here and\ndecided we'll do uh y o for yellow or\nit's kind of a oranges yellow color is\ngoing to come out marker size nine those\nare settings you can play with somebody\nelse played with them to come up with\nthe right setup so it looks good\nand you can see there it is graphed\nclearly a muffin\nin this case in cupcakes versus muffins\nthe muffin has won\nand if you'd like to do your own muffin\ncupcake\ncontender series\nyou certainly can send a note down below\nand the team at simply learn will send\nyou over the data they use for the\nmuffin and cupcake and that's true of\nany of the data we didn't actually run a\nplot on it earlier we had men versus\nwomen you can also request that\ninformation to run it on your data setup\nso you can test that out\nso to go back over our setup we went\nahead for our support vector machine\ncode we did a predict 40 parts flour 20\nparts sugar i think was different than\nthe one we did whether it's a muffin or\na cupcake hence we have built a\nclassifier using svm which is able to\nclassify if a recipe is of a cupcake or\na muffin which wraps up our cupcake\nversus muffin\ntoday in our second tutorial we're going\nto cover k-means and linear regression\nalong with going over the quiz questions\nwe had during our first tutorial\nwhat's in it for you we're going to\ncover clustering what is clustering k\nmeans clustering which is one of the\nmost common use clustering tools out\nthere including a flow chart to\nunderstand k-means clustering and how it\nfunctions and then we'll do an actual\npython live demo on clustering of cars\nbased on brands\nthen we're going to cover logistic\nregression what is logistic regression\nlogistic regression curve and sigmoid\nfunction and then we'll do another\npython code demo to classify a tumor as\nmalignant or benign based on features\nand let's start with clustering suppose\nwe have a pile of books of different\ngenres now we divide them into different\ngroups like fiction horror education and\nas we can see from this young lady she\ndefinitely is into heavy horror you can\njust tell by those eyes and the maple\ncanadian leaf on her shirt but we have\nfiction horror and education and we want\nto go ahead and divide our books up well\norganizing objects into groups based on\nsimilarity is clustering and in this\ncase as we're looking at the books we're\ntalking about clustering things with\nknown categories but you can also use it\nto explore data so you might not know\nthe categories you just know that you\nneed to divide it up in some way to\nconquer the data and to organize it\nbetter but in this case we're going to\nbe looking at clustering in specific\ncategories and let's just take a deeper\nlook at that we're going to use k means\nclustering k means clustering is\nprobably the most commonly used\nclustering tool in the machine learning\nlibrary k-means clustering is an example\nof unsupervised learning if you remember\nfrom our previous thing it is used when\nyou have unlabeled data so we don't know\nthe answer yet we have a bunch of data\nthat we want to cluster to different\ngroups define clusters in the data based\non feature similarity so we've\nintroduced a couple terms here we've\nalready talked about unsupervised\nlearning and unlabeled data so we don't\nknow the answer yet we're just going to\ngroup stuff together and see if we can\nfind an answer of how things connect\nwe've also introduced feature similarity\nfeatures being different features of the\ndata now with books we can easily see\nfiction and horror\nand history books but a lot of times\nwith data some of that information isn't\nso easy to see right when we first look\nat it and so k-means is one of those\ntools where we can start finding things\nthat connect that match with each other\nsuppose we have these data points and\nwant to assign them into a cluster now\nwhen i look at these data points i would\nprobably group them into two clusters\njust by looking at them i'd say two of\nthese group of data kind of come\ntogether\nbut in k means we pick k clusters and\nassign random centroids to clusters\nwhere the k clusters represents two\ndifferent clusters we pick k clusters\nand cyranium centroids to the clusters\nthen we compute distance from objects to\nthe centroids now we form new clusters\nbased on minimum distances\nand calculate the centroids so we figure\nout what the best distance is for the\ncentroid then we move the centroid and\nrecalculate those distances repeat\nprevious two steps iteratively till the\ncluster centroid stop changing their\npositions and become static\nrepeat previous two steps iteratively\ntill the cluster centroid stop changing\nand the positions become static once the\nclusters become static then k means\nclustering algorithm is said to be\nconverged and there's another term we\nsee throughout machine learning is\nconverged that means whatever math we're\nusing to figure out the answer has come\nto a solution or it's converged on an\nanswer so we see the flowchart to\nunderstand make a little bit more sense\nby putting it into a nice easy step by\nstep\nso we start we choose k we'll look at\nthe elbow method in just a moment we\nassign random centroids to clusters and\nsometimes you pick the centroids because\nyou might look at the data in a graph\nand say oh these are probably the\ncentral points then we compute the\ndistance from the objects to the\ncentroids\nwe take that and we form new clusters\nbased on minimum distance and calculate\ntheir centroids then we compute the\ndistance from objects to the new\ncentroids and then we go back and repeat\nthose last two steps we calculate the\ndistances so as we're doing it it brings\ninto the new centroid and then we move\nthe centroid around and we figure out\nwhat the best which objects are closest\nto each centroid so the objects can\nswitch from one centroid to the other\nthe centroids are moved around and we\ncontinue that until it is converged\nlet's see an example of this\nsuppose we have this data set of seven\nindividuals and their score on two\ntopics a and b\nso here's our subject\nin this case referring to the person\ntaking the test\nand then we have subject a where we see\nwhat they've scored on their first\nsubject and we have subject b and we can\nsee what they score on the second\nsubject now let's take two farthest\napart points as initials cluster\ncentroids remember we talked about\nselecting them randomly or we can also\njust put them in different points and\npick the furthest one apart so they move\ntogether either one works okay depending\non what kind of data you're working on\nand what you know about it\nso we took the two furthest points one\nand one and five and seven and now let's\ntake the two farthest apart points as\ninitial cluster centroids each point is\nthen assigned to the closest cluster\nwith respect to the distance from the\ncentroids so we take each one of these\npoints in there we measure that distance\nand you can see that if we measure each\nof those distances and you use the\npythagorean theorem for a triangle in\nthis case because you know the x and the\ny and you can figure out the diagonal\nline from that or you just take a ruler\nand put it on your monitor that'd be\nkind of silly but it would work if\nyou're just eyeballing it you can see\nhow they naturally come together in\ncertain areas\nnow we again calculate the centroids of\neach cluster so cluster one and then\ncluster two and we look at each\nindividual dot there's one two three\nwe're in one cluster uh the centroid\nthen moves over it becomes\n1.8 comma 2.3 so remember is that one\nand one well the very center of the data\nwe're looking at would put it at the one\npoint roughly 2 2 but 1.8 and 2.3 and\nthe second one if we wanted to make the\noverall mean vector the average vector\nof all the different distances to that\ncentroid we come up with four comma one\nand five four so we've now moved the\ncentroids we compare each individual's\ndistance to its own cluster mean and to\nthat of the opposite cluster and we find\nwe can build a nice chart on here that\nthe as we move the centroid around we\nnow have a new different kind of\nclustering of groups and using euclidean\ndistance between the points and the mean\nwe get the same formula you see these\nnew formulas coming up so we have our\nindividual dots distance to the mean\ncentroid of the cluster and distance to\nthe mean centroid of the cluster\nonly individual three is nearer to the\nmean of the opposite cluster cluster two\nthan its own cluster one and you can see\nhere in the diagram where we've kind of\ncircled that one in the middle so when\nwe've moved the cluster the centroids of\nthe clusters over one of the points\nshifted to the other cluster because\nit's closer to that group of individuals\nthus individual 3 is relocated to\ncluster 2 resulting in a new partition\nand we regenerate all those numbers of\nhow close they are to the different\nclusters\nfor the new clusters we will find the\nactual cluster centroids so now we move\nthe centroids over and you can see that\nwe've now formed two very distinct\nclusters on here on comparing the\ndistance of each individual's distance\nto its own cluster mean and to that of\nthe opposite cluster we find that the\ndata points are stable hence we have our\nfinal clusters now if you remember i\nbrought up a concept earlier caming on\nthe k-means algorithm choosing the right\nvalue of k will help in less number of\niterations and to find the appropriate\nnumber of clusters in a data set we use\nthe elbow method and within sum of\nsquares wss is defined as the sum of the\nsquared distance between each member of\nthe cluster in its centroid and so you\nsee we've done here is we have the\nnumber of clusters and as you do\nthe same k-means algorithm over the\ndifferent clusters and you calculate\nwhat that centroid looks like and you\nfind the optimal you can actually find\nthe optimal number of clusters using the\nelbow the graph is called as the elbow\nmethod and on this we guessed it two\njust by looking at the data but as you\ncan see the slope you actually just look\nfor right there where the elbow is in\nthe slope and you have a clear answer\nthat we want two different to start with\nk means equals two a lot of times people\nend up computing k-means equals two\nthree four five until they find the\nvalue which fits on the elbow joint\nsometimes you can just look at the data\nand if you're really good with that\nspecific domain remember domain i\nmentioned that last time you'll know\nthat where to pick those numbers or\nwhere to start guessing at what that k\nvalue is so let's take this and we're\ngoing to use a use case using k-means\nclustering to cluster cars into brands\nusing parameters such as horsepower\ncubic inches make year etc so we're\ngoing to use the data set cars data\nhaving information about three brands of\ncars toyota honda and nissan we'll go\nback to my favorite tool the anaconda\nnavigator with the jupiter notebook and\nlet's go ahead and flip over to our\njupiter notebook and in our jupiter\nnotebook i'm going to go ahead and just\npaste the basic\ncode that we usually start a lot of\nthese off with we're not going to go too\nmuch into this code because we've\nalready discussed numpy we've already\ndiscussed matplot library and pandas\nthey'll be being the number array pandas\nbeing the pandas data frame and map plot\nfor the graphing and don't forget since\nif you're using the jupiter notebook you\ndo need the map plot library inline so\nthat it plots everything on the screen\nif you're using a different python\neditor then you probably don't need that\nbecause it'll have a pop-up window on\nyour computer\nwe'll go ahead and run this just to load\nour libraries and our setup into here\nthe next step is of course to look at\nour data which i've already opened up in\na spreadsheet and you can see here we\nhave the miles per gallon cylinders\ncubic inches horsepower weight pounds\nhow heavy it is time it takes to get to\n60\nmy card is probably on this one at about\n80 or 90. what year it is so this is you\ncan actually see this is kind of older\ncars and then the brand toyota\nhonda nissan so the different cars are\ncoming from all the way from 1971 if we\nscroll down\nto the 80s we have between the 70s and\n80s the number of cars that they've put\nout\nand let's uh when we come back here\nwe're going to importing the data so\nwe'll go ahead and do data set equals\nand we'll use pandas to read this in and\nit's uh from a csv file remember you can\nalways post this in the comments and\nrequest the data files for these either\nin the comments here on the youtube\nvideo or go to simplylearn.com and\nrequest that the car csv i put it in the\nsame folder as the code that i've stored\nso my python code is stored in the same\nfolder so i don't have to put the full\npath if you store them in different\nfolders you do have to change this and\ndouble check your name variables and\nwe'll go ahead and run this and we've\nchosen set arbitrarily because you know\nit's a data set we're importing and\nwe've now imported our car csv into the\ndata set\nas you know you have to prep the data so\nwe're going to create the x data this is\nthe one that we're going to try to\nfigure out what's going on with and then\nthere's a number of ways to do this but\nwe'll do it in a simple loop so you can\nactually see what's going on so we'll do\nfour i\nand x dot columns so we're going to go\nthrough each of the columns\nand a lot of times it's important i'll\nmake lists of the columns and do this\nbecause i might remove certain columns\nor there might be columns that i want to\nbe processed differently but for this we\ncan go ahead and take x of i\nand we want to go fill in a and that's a\npandas command but the question is what\nare we going to fill the missing data\nwith we definitely don't want to just\nput in a number that doesn't actually\nmean something and so one of the tricks\nyou can do with this\nis we can take x of i\nand in addition to that we want to go\nahead and turn this into\nan integer because a lot of these are\nintegers so we'll go ahead and keep it\nintegers and let me add the bracket here\nand a lot of editors will do this\nthey'll think that you're closing one\nbracket make sure you get that second\nbracket in there if it's a double\nbracket that's always something that\nhappens regularly so once we have our\ninteger of x of y this is going to fill\nin any missing data with the average\nand i was so busy closing one set of\nbrackets i forgot that the mean is also\nhas brackets in there for the pandas so\nwe can see here we're going to fill in\nall the data with the average value for\nthat column so there's missing data is\nin the average of the data it does have\nthen once we've done that we'll go ahead\nand loop through it again\nand just check and see\nto make sure everything is filled in\ncorrectly and we'll print and then we\ntake x\nis null and this returns a set of the\nnull value or the how many lines are\nnull and we'll just sum that up to see\nwhat that looks like and so when i run\nthis\nand so with the x what we want to do is\nwe want to remove the last column\nbecause that had the models that's what\nwe're trying to see if we can cluster\nthese things and figure out the models\nthere is so many different ways to sort\nthe x out for one we could take the x\nand we could go data set our variable\nwe're using and use the i location one\nof the features that's in pandas\nand we could take that and then take all\nthe rows\nand all but the last column of the data\nset\nand at this time we could do values we\njust converted to values so that's one\nway to do this and if i let me just put\nthis down here and print\nx it's a capital x we chose and i run\nthis you can see it's just the values we\ncould also take out the values and\nit's not going to return anything\nbecause there's no values connected to\nit what i like to do with this\nis instead of doing the eye location\nwhich does integers more common is to\ncome in here and we have our data set\nand we're going to do data set\ndot or data set dot columns and remember\nthat lists all the columns so if i come\nin here\nlet me just mark that as red\nand i print\ndata set dot columns\nyou can see that i have my index here i\nhave my mpg cylinders everything\nincluding the brand which we don't want\nso the way to get rid of the brand would\nbe to do data columns of everything but\nthe last one minus one so now if i print\nthis you'll see the brand disappears\nand so i can actually just take\ndata set columns minus one\nand i'll put it right in here for the\ncolumns we're going to look at\nand let's un mark this\nand unmark this\nand now if i do an x dot head\ni have a new data frame and you can see\nright here we have all the different\ncolumns except for the brand at the end\nof the year\nand it turns out when you start playing\nwith the data set you're going to get an\nerror later on and it'll say cannot\nconvert string to float value and that's\nbecause if for some reason these things\nthe way they recorded them must been\nrecorded as strings so we have a neat\nfeature in here\non pandas to convert\nand it is simply convert objects\nand for this we're going to do convert\noops convert\nunderscore numeric numeric\nequals\ntrue and yes i did have to go look that\nup i don't have it memorized the convert\nnumeric in there if i'm working with a\nlot of these things i remember them but\ndepending on where i'm at what i'm doing\ni usually have to look it up and we run\nthat oops i must have missed something\nin here let me double check my spelling\nand when i double check my spilling\nyou'll see i missed the first underscore\nin the convert objects when i run this\nit now has everything converted into a\nnumeric value because that's what we're\ngoing to be working with is numeric\nvalues down here\nand the next part is that we need to go\nthrough the data and eliminate null\nvalues\nmost people when they're doing small\namounts working with small data pools\ndiscover afterwards that they have a\nnull value and they have to go back and\ndo this\nso you know be aware\nwhenever we're formatting this data\nthings are going to pop up and sometimes\nyou go backwards to fix it\nand that's fine that's just part of\nexploring the data and understanding\nwhat you have\nand i should have done this earlier but\nlet me go ahead and increase the size of\nmy window one notch\nthere we go easier to see\nso we'll do 4i in\nworking with x dot columns we'll page\nthrough all the columns and we want to\ntake\nx of i and we're going to change that\nwe're going to alter it\nand so\nwith this we want to go ahead and fill\nin\nx of i pandas has\nthe fill in a\nand that just fills in any non-existent\nmissing data\nthen we'll put my brackets up and\nthere's a lot of different ways to fill\nthis data\nif you have a really large data set some\npeople just void out that data because\nand then look at it later in a separate\nexploration of data\none of the tricks we can do is we can\ntake our column\nand we can find the means\nand the means is in other or quotation\nmarks\nso we take the columns we're going to\nfill in the non-existing one with the\nmeans the problem is that returns a\ndecimal float so some of these aren't\ndecimals\ncertainly let me be a little careful of\ndoing this but for this example we're\njust going to fill it in with the\ninteger version of this it keeps it on\npar with the other data that isn't a\ndecimal point\nand then what we also want to do is we\nwant to double check ways you can do\nthat is simply go in here\nand take our x of i\ncolumn so it's going to go through the x\nof i column it says is null\nso it's going to return any any place\nthere's a null value it actually goes\nthrough all the rows of each column is\nnull and then we want to go ahead and\nsum that so we take that and we add the\nsum value and these are all pandas so is\nnull is a panda command and so is sum\nand if we go through that we go ahead\nand run it\nand we go ahead and take and run that\nyou'll see that all the columns have\nzero null values so we've now tested and\ndouble checked and our data is nice and\nclean we have no null values everything\nis now a number value we turned it into\nnumeric\nand we've removed the last column in our\ndata\nand at this point we're actually going\nto\nstart using the elbow method to find the\noptimal number of clusters so we're now\nactually getting into the sk learn part\nuh the k means clustering on here\ni guess we'll go ahead and zoom it up\none more not so you can see what i'm\ntyping in here\nand then from sk learn we're going to or\nsk learn\ncluster\ni'm going to import\nk\nmeans\ni always forget to capitalize the k and\nthe m when i do this so it's capital k\ncapital m k means\nand we'll go and create a um\narray wcss equals when we get an empty\narray if you remember from the elbow\nmethod from our slide\nwithin the sums of squares wss is\ndefined as the sum of square distance\nbetween each member of the cluster in\nthe centroid so we're looking at that\nchange in differences as far as a\nsquared distance and we're going to run\nthis over a number of k-mean values\nin fact let's go for i in range we'll do\n11 of them\n0 11.\nand the first thing we're going to do is\nwe're going to create the actual\nblue it all lower case\nand so we're going to create this object\nfrom the k-means that we just imported\nand the variable that we want to put\ninto this is in\nclusters\nwe're going to set that equals to i\nthat's the most important one because\nwe're looking at how increasing the\nnumber of clusters\nchanges our answer there are a lot of\nsettings to the k-means\nour guys in the back did a great job\njust kind of playing with some of them\nthe most common ones that you see in a\nlot of stuff is how you init your\nk-means so we have k-means plus plus\nthis is just a tool to let the model\nitself be smart how it picks its\ncentroids to start with its initial\nincentroids we only want to iterate no\nmore than 300 times we have a max\niteration we put in there\nwe have the infinite the random state\nequals zero you really don't need to\nworry too much about these when you're\nfirst learning this as you start digging\nin deeper you start finding that these\nare shortcuts that will speed up the\nprocess\nas far as a setup but the big one that\nwe're working with is the in clusters\nequals i so we're going to literally\ntrain our k-means 11 times we're going\nto do this process 11 times\nand if you're working with big data you\nknow the first thing you do is you run a\nsmall sample of the data so you can test\nall your stuff on it and you can already\nsee the problem that if i'm going to\niterate through a terabyte of data 11\ntimes and then the k means itself is\niterating through the data multiple\ntimes that's a heck of a process\nso you got to be a little careful with\nthis a lot of times though you can find\nyour elbow using the elbow method find\nyour optimal number on a sample of data\nespecially if you're working with larger\ndata sources\nso we want to go ahead and take our\nk-means and we're just going to fit it\nif you're looking at any of the sk-learn\nvery common that you fit your model and\nif you remember correctly our variable\nwe're using is the capital x\nand once we fit this value we go back to\nthe\narray we made we want to go and just\ndepend that value on the end\nand it's not the actual fit we're\npinning in there it's when it generates\nit it generates the value you're looking\nfor is inertia so k means dot inertia\nwill pull that specific value out that\nwe need\nand let's get a visual on this we'll do\nour plt plot\nand what we're plotting here\nis first the x axis which is range 0\n11 so that will generate a nice little\nplot there and the wcss for our y-axis\nit's always nice to give our plot a\ntitle\nand let's see we'll just give it the\nelbow method for the title and let's get\nsome labels so let's go ahead and do plt\nx label\nand what we'll do we'll do number of\nclusters for that\nand plt y label\nand for that we can do oops there we go\nwcss since that's what we're doing on\nthe plot on there and finally we want to\ngo ahead and display our graph which is\nsimply plt dot oops\ndot show there we go and because we have\nit set to inline it'll appear in line\nhopefully i didn't make a type error on\nthere\nand you can see we get a very nice graph\nyou can see a very nice elbow joint\nthere at\ntwo and again right around three and\nfour and then after that there's not\nvery much now as a data scientist if i\nwas looking at this\ni would do either three or four and i'd\nactually try both of them to see what\nthe\noutput look like and they've already\ntried this in the back so we're just\ngoing to use three as a setup on here\nand let's go ahead and see what that\nlooks like when we actually use this\nto show the different kinds of cars\nand so let's go ahead and apply the\nk-means to the cars data set\nand basically we're going to copy the\ncode that we loop through up above\nwhere k means equals k means number of\nclusters and we're just going to set the\nnumber of clusters to 3.\nsince that's what we're going to look\nfor and you can do 3 and 4 on this and\ngraph them just to see how they come up\ndifferently be kind of curious to look\nat that but for this we're just going to\nset it to 3. go ahead and create our own\nvariable y\nk means for our answers\nand we're going to set that equal to\nwhoops a double equal there 2k means\nbut we're not going to do a fit we're\ngoing to do a fit predict is the setup\nyou want to use and when you're using\nuntrained models you'll see a slightly\ndifferent because usually you see fit\nand then you see just the predict but we\nwant to both fit and predict the k-means\non this and that's fit underscore\npredict and then our capital x is the\ndata we're working with\nand before we plot this data we're going\nto do a little pandas trick we're going\nto take our x value and we're going to\nset x as matrix\nso we're converting this into a nice\nrows and columns kind of set up but we\nwant the we're going to have columns\nequals none so it's just going to be a\nmatrix of data in here\nand let's go ahead and run that\na little warning you'll see these\nwarnings pop up because things are\nalways being updated so there's like\nminor changes in the versions and future\nversions\ninstead of matrix now that it's more\ncommon to set it dot values instead of\ndoing as matrix but math matrix works\njust fine for right now and you'll want\nto update that later on but let's go\nahead and dive in and plot this and see\nwhat that looks like\nand before we dive into plotting this\ndata i always like to take a look and\nsee what i am plotting so let's take a\nlook at y\nk means i'm just going to print that out\ndown here\nand we see we have an array of answers\nwe have 2 1 0 2 1 2.\nso it's clustering these different\nrows of data\nbased on the three different spaces it\nthinks it's going to be\nand then let's go ahead and print x and\nsee what we have for x and we'll see\nthat x is an array it's a matrix so we\nhave our different values in the array\nand what we're going to do it's very\nhard to plot all the different values in\nthe array so we're only going to be\nlooking at the first two or positions 0\nand one\nand if you were doing a full\npresentation in front of the board\nmeeting\nyou might actually do a little different\nthan and dig a little deeper into the\ndifferent aspects because this is all\nthe different columns we looked at but\nwhen we look at columns one and two for\nthis to make it easy\nso let's go ahead and clear this data\nout of here and let's bring up our plot\nand we're going to do a scatter plot\nhere so plt scatter\nand\nthis looks a little complicated so let's\nexplain what's going on with this\nwe're going to take the x values\nand we're only interested in y of k\nmeans equals 0 the first cluster okay\nand then we're going to take value 0 for\nthe x-axis and then we're going to do\nthe same thing here\nwe're only interested in\nk means equals 0 but we're going to take\nthis second column so we're looking at\nthe first two columns in our answer or\nin the data\nand then the guys in the back played\nwith this a little bit to make it pretty\nand they discovered that it looks good\nwith as a size equals 100 that's the\nsize of the dots\nwe're going to use red for this one and\nwhen they were looking at the data and\nwhat came out it was definitely the\ntoyota on this we're just going to go\nahead and label it toyota again that's\nsomething you really have to explore in\nhere as far as playing with those\nnumbers and see what looks good we'll go\nahead and hit enter in there and i'm\njust going to paste in the next\ntwo lines which is the next two cars\nand this is our nissa and honda and\nyou'll see with our scatter plot we're\nnow looking at where y underscore k\nmeans equals one\nand we want the zero column and y k\nmeans equals two again we're looking at\njust the first two columns zero and one\nand each of these rows then corresponds\nto nissan and honda\nand i'll go ahead and hit enter on there\nand uh finally let's take a look and put\nthe centroids on there\nagain we're going to do a scatter plot\nand on the centroids you can just pull\nthat from our k-means the model we\ncreated\ndot cluster centers\nand we're going to just do\nall of them in the first number and all\nof them and the second number which is 0\n1 because you always start with 0 and 1.\nand then they were playing with the size\nand everything to make it look good\nwe'll do a size of 300 we're going to\nmake the color yellow and we'll label\nthem it's always good to have some good\nlabels centroids\nand then we do want to do a title\nplt title\nand pop up there\nplt title cause you always want to make\nyour graphs look pretty and we'll call\nit clusters of car make\nand one of the features of the\nplot library is you can add a\nlegend it'll automatically bring in it\nsince we've already labeled the\ndifferent aspects of the legend with\ntoyota nissan and honda\nand finally we want to go ahead and show\nso we can actually see it and remember\nit's in line so if you're using a\ndifferent editor that's not the jupiter\nnotebook you'll get a pop-up of this\nand you should have a nice set of\nclusters here so we can look at this and\nwe have a clusters of honda and green\ntoyota and red nissan and purple and you\ncan see where they put the centroids to\nseparate them\nnow when we're looking at this we can\nalso plot a lot of other different data\non here as far because we only looked at\nthe first two columns this is just\ncolumn one and two or zero one as you\nlabel them in computer scripting but you\ncan see here we have nice clusters of\ncar make and we were able to pull out\nthe data and you can see how just these\ntwo columns\nform very distinct clusters of data so\nif you were exploring new data you might\ntake a look and say well what makes\nthese different\nalmost going in reverse you start\nlooking at the data and pulling apart\nthe columns to find out\nwhy is the first group set up the way it\nis maybe you're doing loans and you want\nto go well why is this group not\ndefaulting on their loans and why is the\nlast group defaulting on their loans and\nwhy is the middle group 50 defaulting on\ntheir\nbank loans and you start finding ways to\nmanipulate the data and pull out the\nanswers you want\nso now that you've seen how to use\nk-mean for clustering\nlet's move on to the next topic\nnow let's look into logistic regression\nthe logistic regression algorithm is the\nsimplest classification algorithm used\nfor binary or multi-classification\nproblems and we can see we have our\nlittle girl from canada who's into\nhorror books is back that's actually\nreally scary when you think about that\nwith those big eyes in the previous\ntutorial we learned about linear\nregression dependent and independent\nvariables so to brush up y equals mx\nplus c\nvery basic algebraic function of\ny and x the dependent variable is the\ntarget class variable we are going to\npredict\nthe independent variables x1 all the way\nup to xn are the features or attributes\nwe're going to use to predict the target\nclass\nwe know what a linear regression looks\nlike but using the graph we cannot\ndivide the outcome into categories\nit's really hard to categorize 1.5 3.6\n9.8\nfor example a linear regression graph\ncan tell us that with increase in number\nof hours studied the marks of a student\nwill increase but it will not tell us\nwhether the student will pass or not in\nsuch cases where we need the output as\ncategorical value we will use logistic\nregression and for that we're going to\nuse the sigmoid function so you can see\nhere we have our marks 0 to 100 number\nof hours studied that's going to be what\nthey're comparing it to in this example\nand we usually form a line that says y\nequals mx plus c and when we use the\nsigmoid function we have p equals 1 over\n1 plus e to the minus y\nit generates a sigmoid curve and so you\ncan see right here when you take the ln\nwhich is the natural logarithm i always\nthought it should be nl not ln that's\njust the inverse of\ne your e to the minus y and so we do\nthis we get ln of p over one minus p\nequals m times x plus c that's the\nsigmoid curve function we're looking for\nand we can zoom in on the function and\nyou'll see that the function as it\nderives goes to 1 or to 0 depending on\nwhat your x value is and the probability\nif it's greater than 0.5 the value is\nautomatically rounded off to 1\nindicating that the student will pass so\nif they're doing a certain amount of\nstudying they will probably pass then\nyou have a threshold value at the 0.5 it\nautomatically puts that right in the\nmiddle usually and your probability if\nit's less than 0.5 the value rendered\noff to zero indicating the student will\nfail so if they're not studying very\nhard they're probably going to fail this\nof course is ignoring the outliers or\nthat one student is just a natural\ngenius and doesn't need any studying to\nmemorize everything that's not me\nunfortunately i have to study hard to\nlearn new stuff\nproblem statement to classify whether a\ntumor is malignant or benign and this is\nactually one of my favorite data sets to\nplay with because it has so many\nfeatures and when you look at them you\nreally are hard to understand you can't\njust look at them and know the answer so\nit gives you a chance to kind of dive\ninto what data looks like when you\naren't able to understand the specific\ndomain of the data but i also want you\nto remind you that in the domain of\nmedicine if i told you that my\nprobability\nwas really good it classified things\nthat say 90 or 95\nand i'm classifying whether you're going\nto have a malignant or a b9 tumor i'm\nguessing that you're going to go get it\ntested anyways so you got to remember\nthe domain we're working with so why\nwould you want to do that if you know\nyou're just going to go get a biopsy\nbecause you know it's that serious this\nis like an all or nothing just\nreferencing the domain it's important it\nmight help the doctor know where to look\njust by understanding what kind of tumor\nit is so it might help them or aid them\nin something they missed from before so\nlet's go ahead and dive into the code\nand i'll come back to the domain part of\nit in just a minute so use case and\nwe're going to do our normal imports\nhere we're importing numpy\npandas seaborn the matplot library and\nwe're going to do matplot library inline\nsince i'm going to switch over to\nanaconda so let's go ahead and flip over\nthere and get this started\nso i've opened up a new window in my\nanaconda jupiter notebook\nand by the way jupiter notebook you\ndon't have to use anaconda for the\njupiter notebook i just love the\ninterface and all the tools and anaconda\nbrings so we got our import numpy as in\np\nfor our numpy number array we have our\npandas pd we're going to bring in\nseaborn to help us with our graphs as\nsns\nso many really nice tools in both\nseaborne and matplot library and we'll\ndo our\nmatplotlibrary.pipelot as plt and then\nof course we want to let it know to do\nit in line and let's go and just run\nthat so it's all set up\nand we're just going to call our data\ndata\nnot creative today\nequals pd and this happens to be in a\ncsv file\nso we'll use the pd.read underscore csv\nand i happen to name the file i renamed\nit\ndataforp2.csv you can of course write in\nthe comments below the youtube and\nrequest for the dataset itself or go to\nthe simplylearn website and we'll be\nhappy to supply that for you\nand let's just open up the data before\nwe go any further and let's just see\nwhat it looks like in a spreadsheet\nso when i pop it open in a local\nspreadsheet this is just a csv file\ncomma separated variables we have an id\nso i guess the\ncategorizes for reference or what id\nwhich test was done the diagnosis m for\nmalignant b for b9 so there's two\ndifferent options on there and that's\nwhat we're going to try to predict is\nthe m and b and test it and then we have\nlike the radius\nmean or average the texture average\nperimeter mean area mean smoothness i\ndon't know about you but unless you're a\ndoctor in the field most of stuff i mean\nyou can guess what concave means just by\nthe term concave but i really wouldn't\nknow what that means in the measurements\nthey're taking so they have all kinds of\nstuff like how smooth it is uh the\nsymmetry and these are all float values\nlet me just page through them real quick\nand you'll see there's i believe 36 if i\nremember correctly in this one\nso there's a lot of different values\nthey take all these measurements they\ntake when they go in there and they take\na look at the different growth the\ntumorous growth\nso back in our data and i put this in\nthe same folder as a code so i saved\nthis code in that folder obviously if\nyou have it in a different location you\nwant to put the full path in there and\nwe'll just do\npandas\nfirst five lines of data with the\ndata.head\nwe run that we can see that we have\npretty much what we just looked at we\nhave an id we have a diagnosis\nif we go all the way across you'll see\nall the different columns coming across\ndisplayed nicely\nfor our data\nand while we're exploring the data our\nseaborn which we referenced as sns\nmakes it very easy to go in here and do\na joint plot you'll notice that\nvery similar to because it is sitting on\ntop of the plot library so the joint\nplot does a lot of work for us and we're\njust going to look at the first two\ncolumns that we're interested in the\nradius mean and the texture mean we'll\njust look at those two columns\nand data\nequals data so that tells it which two\ncolumns we're plotting and that we're\ngoing to use the data that we pulled in\nlet's just run that\nand it generates a really nice graph on\nhere\nand there's all kinds of cool things on\nthis graph to look at i mean we have the\ntexture mean and the radius mean\nobviously the axes you can also see\nand one of the cool things on here is\nyou can also see the histogram they show\nthat for the radius mean whereas the\nmost commons radius mean come up and\nwhere the most common texture is so\nwe're looking at the tech the\non each growth its average texture and\non each radius its average uh radius on\nthere it's a little confusing because\nwe're talking about the individual\nobjects average and then we can also\nlook over here and see the the histogram\nshowing us the median or how common each\nmeasurement is\nand that's only two columns so let's dig\na little deeper into seaborn they also\nhave a heat map and if you're not\nfamiliar with heat maps a heat map just\nmeans it's in color that's all that\nmeans heat map i guess the original ones\nwere plotting heat density on something\nand so ever since then it's just called\na heat map and we're going to take our\ndata and get our corresponding numbers\nto put that into the heat map and that's\nsimply data dot c-o-r-r\nfor that that's a pandas expression\nlet's remember we're working in a pandas\ndata frame so that's one of the cool\ntools in pandas for our data and this is\npull that information into a heat map\nand see what that looks like\nand you'll see that we're now looking at\nall the different features we have our\nid we have our texture we have our area\nour compactness concave points\nand if you look down the middle of this\nchart diagonal going from the upper left\nto bottom right\nit's all white that's because when you\ncompare texture to texture they're\nidentical so they're 100 percent or in\nthis case a perfect one in their\ncorrespondence\nand you'll see that when you look at say\narea or right below it it has almost a\nblack on there when you compare it to\ntexture so these have almost no\ncorresponding data they don't really\nform a linear graph or something that\nyou can look at and say how connected\nthey are they're very scattered data\nthis is really just a really nice graph\nto get a quick look at your data doesn't\nso much change what you do but it\nchanges verifying so when you get an\nanswer or something like that or you\nstart looking at some of these\nindividual pieces you might go hey that\ndoesn't match according to showing our\nheat map this should not correlate with\neach other and if it is you're going to\nhave to start asking well why what's\ngoing on what else is coming in there\nbut it does show some really cool\ninformation on here and we can see from\nthe id\nthere's no real one feature that just\nsays if you go across the top line that\nlights up there's no one feature that\nsays hey if the area is a certain size\nthen it's going to be b9 or malignant it\nsays there's some that sort of add up\nand that's a big hint in the data that\nwe're trying to id this whether it's\nmalignant or b9 that's a big hint to us\nas data scientists to go okay\nwe can't solve this with any one feature\nit's going to be something that includes\nall the features or many of the\ndifferent features to come up with the\nsolution for it and while we're\nexploring the data let's explore one\nmore area and let's look at data dot is\nnull we want to check for null values in\nour data if you remember from earlier in\nthis tutorial we did it a little\ndifferently where we added stuff up and\nsum them up you can actually with pandas\ndo it really quickly data. is null and\nsummit and it's going to go across all\nthe columns so when i run this\nyou're going to see all the columns come\nup with no null\ndata\nso we've just just to re hash these last\nfew steps\nwe've done a lot of exploration we have\nlooked at the first two columns\nand seen how they plot with the seaborn\nwith a joint plot which shows both the\nhistogram\nand the data plotted on the xy\ncoordinates\nand obviously you can do that more in\ndetail\nwith different columns and see how they\nplot together\nand then we took and did the seaborne\nheat map the sns\ndot heat map of the data and you can see\nright here where it did a nice job\nshowing us some bright spots where stuff\ncorrelates with each other and forms a\nvery nice combination or points of\nscattering points and you can also see\nareas that don't\nand then finally we went ahead and\nchecked the data is the data null value\ndo we have any missing data in there\nvery important step because it'll crash\nlater on\nif you forget to do this step it will\nremind you when you get that nice error\ncode that says no values okay\nso not a big deal if you miss it but\nit's no fun having to go back when\nyou're you're in a huge process and\nyou've missed this step and now you're\n10 steps later and you've got to\nremember where you were pulling the data\nin\nso we need to go ahead and pull out our\nx\nand our y let me just put that down here\nand we'll set the x equal to and there's\na lot of different options here\ncertainly we could do x equals all the\ncolumns except for the first two because\nif you remember the first two is the id\nand the diagnosis so that certainly\nwould be an option\nbut we're going to do is we're actually\ngoing to focus on the worst the worst\nradius the worst texture parameter area\nsmoothness compactness and so on one of\nthe reasons to start dividing your data\nup when you're looking at this\ninformation\nis\nsometimes the data will be the same data\ncoming in so if i have two measurements\ncoming in to my model it might overweigh\nthem it might overpower the other\nmeasurements because it's measuring it's\nbasically taking that information in\ntwice\nthat's a little bit past the scope of\nthis tutorial i want you to take away\nfrom this though is that we are dividing\nthe data up into pieces and our team in\nthe back went ahead and said hey let's\njust look at the worst so i'm going to\ncreate a an array\nand you'll see this array radius worst\ntexture worse perimeter worst we've just\ntaken the worst of the worst and i'm\njust going to put that in my x so this x\nis still a pandas data frame but it's\njust those columns and our y if you\nremember correctly\nit's going to be oops hold on one second\nit's not x it's data there we go so x\nequals data and then it's a list of the\ndifferent columns the worst of the worst\nand if we're going to take that then we\nhave to have our answer for our y for\nthe stuff we know and if you remember\ncorrectly we're just going to be looking\nat\nthe diagnosis that's all we care about\nis what is it diagnosed is it b9 or\nmalignant\nand since it's a single column we can\njust do diagnosis oh i forgot to put the\nbrackets or the there we go okay so it's\njust diagnosis on there and we can also\nreal quickly do like x dot head if you\nwant to see what that looks like\nand y dot head\nand run this and you'll see\nit only does the last one i forgot about\nthat if you don't do print you can see\nthat the the y dot head is just mmm\nbecause the first ones are all malignant\nand if i run this the x dot head is just\nthe first five values of radius worst\ntexture worse parameter worst area worst\nand so on\ni'll go ahead and take that out\nso moving down\nto the next step we've built our two\ndata sets our answer and then the\nfeatures we want to look at\nin data science it's very important to\ntest your model\nso we do that by splitting the data\nand from sk learn model selection we're\ngoing to import train test split so\nwe're going to split it into two groups\nthere are so many ways to do this i\nnoticed in one of the more modern ways\nto actually split it into three groups\nand then you model each group and test\nit against the other groups so you have\nall kinds and there's reasons for that\nwhich is past the scope of this and for\nthis particular example isn't necessary\nfor this we're just going to split it\ninto two groups one to train our data\nand one to test our data and the\nsklearn\ndot model selection we have train test\nsplit you could write your own quick\ncode to do this we just randomly divide\nthe data up into two groups but they do\nit for us nicely\nand we actually can almost we can\nactually do it in one statement with\nthis where we're going to generate four\nvariables\ncapital x train capital x test so we\nhave our training data we're going to\nuse to fit the model and then we need\nsomething to test it and then we have\nour y train so we're going to train the\nanswer and then we have our test so this\nis the stuff we want to see how good it\ndid on our model\nand we'll go ahead and take our train\ntest split that we just imported\nand we're going to do x and our y are\ntwo different data that's going in for\nour split\nand then the guys in the back came up\nand wanted us to go ahead and use a test\nsize equals 0.3 that's test underscore\nsize random state it's always nice to\nkind of switch your random state around\nbut not that important\nwhat this means is that the test size is\nwe're going to take 30 percent of the\ndata and we're going to put that into\nour test variables our y test and our x\ntest and we're going to do 70 into the x\ntrain and the y train so we're going to\nuse 70 of the data to train our model\nand 30 to test it\nlet's go ahead and run that and load\nthose up\nso now we have all our stuff split up\nand all our data ready to go and now we\nget to the actual logistics part we're\nactually going to do our create our\nmodel\nso let's go ahead and bring that in from\nsklearn we're going to bring in our\nlinear model and we're going to import\nlogistic regression that's the actual\nmodel we're using and this we'll call it\nlog model\nstereo model and let's just set this\nequal to our logistic regression that we\njust imported\nso now we have a variable log model set\nto that class for us to use and with\nmost the\nmodels in the sk learn we just need to\ngo ahead and fix it fit do a fit on\nthere and we use our x train that we\nseparated out with our y\ntrain and let's go ahead and run this so\nonce we've run this we'll have a model\nthat fits this data that 70 percent of\nour training data\nand of course it prints this out that\ntells us all the different variables\nthat you can set on there there's a lot\nof different choices you can make but\nfor word though we're just going to let\nall the defaults sit we don't really\nneed to mess with those on this\nparticular example and there's nothing\nin here that really stands out as super\nimportant until you start\nfine tuning it but for what we're doing\nthe basics will work just fine and then\nlet's we need to go ahead and test out\nour model is it working so let's create\na variable y predict and this is going\nto be equal to\nour log model\nand we want to do a predict again very\nstandard format for the sk learn library\nis taking your model and doing a predict\non it and we're going to test why\npredict against the y test so we want to\nknow what the model thinks it's going to\nbe that's what our y predict is and with\nthat we want the capital x x\ntest\nso we have our train set and our test\nset and now we're going to do our y\npredict and let's go ahead and run that\nand if we print\ny\npredict\nlet me go ahead and run that\nyou'll see it comes up and it presents\nand i prints a nice array of uh b and m\nfor b9 and malignant\nfor all the different test data we put\nin there\nso that's pretty good we're not sure\nexactly how good it does but we can see\nthat it actually works and it's\nfunctional was very easy to create\nyou'll always discover with our data\nscience that as you explore this you\nspend a significant amount of time\nprepping your data\nand making sure your data coming in is\ngood\nthere's a saying\ngood data in good answers out bad data\nin\nbad answers out that's only half the\nthing that's only half of it\nselecting your models becomes the next\npart as far as how good your models are\nand then of course fine-tuning it\ndepending on what model you're using\nso we come in here we want to know how\ngood this came out so we have our y\npredict here log model dot predict x\ntest\nso for deciding how good our model is\nwe're going to go from the\nsklearn.metrics we're going to import\nclassification report and that just\nreports how good our model is doing and\nthen we're going to feed it the model\ndata let's just print this out\nand we'll take our classification report\nand we're going to put into there\nour test our actual\ndata so this is what we actually know is\ntrue\nand our prediction what our model\npredicted for that data on the test side\nand let's run that and see what that\ndoes\nso we pull that up you'll see that we\nhave\na precision for b9 and malignant b and m\nand we have a precision of 93 and 91 a\ntotal of 92 so it's kind of the average\nbetween these two 92 there's all kinds\nof different information on here your f1\nscore\nyour recall your support coming through\non this\nand for this i'll go ahead and just flip\nback to our slides that they put\ntogether for describing it and so here\nwe're going to look at the precision\nusing the classification report and you\nsee this is the same printout i had up\nabove some of the numbers might be\ndifferent because it does randomly pick\nout which data we're using so this model\nis able to predict the type of tumor\nwith 91 percent accuracy\nso we look back here let's you'll see\nwhere we have uh b9 in england it\nactually is 92 coming up here but we're\nlooking about a 92 91 precision and\nremember i reminded you about domains\nand we're talking about the domain of a\nmedical domain with a very catastrophic\noutcome you know at 91 or 92 percent\nprecision you're still going to go in\nthere and have somebody do a biopsy on\nit very different than if you're\ninvesting money and there's a 92 percent\nchance you're going to earn 10\nand 8 chance you're going to lose 8\nyou're probably going to bet the money\nbecause at that odds it's pretty good\nthat you'll make some money and in the\nlong run you do that enough you\ndefinitely will make money and also with\nthis domain i've actually seen them use\nthis to identify different forms of\ncancer that's one of the things that\nthey're starting to use these models for\nbecause then it helps the doctor know\nwhat to investigate\nso that wraps up this section we're\nfinally we're going to go in there and\nlet's discuss the answer to the quiz\nasked in machine learning tutorial part\n1.\ncan you tell what's happening in the\nfollowing cases grouping documents into\ndifferent categories based on the topic\nand content of each document this is an\nexample of clustering where k-means\nclustering can be used to group the\ndocuments by topics using bag of words\napproach so if you've gotten in there\nthat you're looking for clustering and\nhopefully you had at least one or two\nexamples like k-means that are used for\nclustering different things then give\nyourself a two thumbs up\nb identifying handwritten digits and\nimages correctly\nthis is an example of classification the\ntraditional approach to solving this\nwould be to extract digit dependent\nfeatures like curvature of different\ndigits etc and then use a classifier\nlike svm to distinguish between images\nagain if you got the fact that it's a\nclassification example give yourself a\nthumb up and if you were able to go hey\nlet's use svm or another model for this\ngive yourself those two thumbs up on it\nc behavior of a website indicating that\nthe site is not working as designed this\nis an example of anomaly detection in\nthis case the algorithm learns what is\nnormal and what is not normal usually by\nobserving the logs of the website give\nyourself a thumbs up if you got that one\nand just for a bonus can you think of\nanother example of anomaly detection one\nof the ones i use it for my own business\nis detecting anomalies in stock markets\nstock markets are very fickled and they\nbehave very radical so finding those\nerratic areas and then finding ways to\ntrack down why they're erratic was\nsomething released in social media was\nsomething released you can see where\nknowing where that anomaly is can help\nyou to figure out what the answer is to\nit in another area d predicting salary\nof an individual based on his or her\nyears of experience this is an example\nof regression this problem can be\nmathematically defined as a function\nbetween independent years of experience\nand dependent variables salary of an\nindividual and if you guessed it this\nwas a regression model give yourself a\nthumbs up and if you were able to\nremember that it was between independent\nand dependent variables and that terms\ngive yourself two thumbs up we're going\nto cover mathematics for machine\nlearning so today's agenda is going to\ncover data and its types and we're going\nto dive into linear algebra and its\nconcepts\ncalculus statistics for machine learning\nprobability for machine learning\nhands-on demos and of course throwing in\nthere in the middle is going to be your\nmatrixes and a few other things to go\nalong with all this\ndata and as types\ndata denotes the individual pieces of\nfactual information collected from\nvarious sources it is stored processed\nand later used for analysis\nand so we see here just a huge grouping\nof information a lot of tech stuff money\ndollar signs numbers\nand then you have your performing\nanalytics to drive insights and\nhopefully you have a nice share your\nshareholders gather it at the meeting\nand you're able to explain it in\nsomething they can understand\nso we talk about data types of data\nwe have in our types of data we have a\nqualitative categorical\nyou think nominal or ordinal\nand then you have your quantitative or\nnumerical which is discrete or\ncontinuous\nand let's look a little closer at those\ndata type vocabulary always people's\nfavorite is the vocabulary words okay\nnot mine\nuh but let's dive into this what we mean\nby nominal\nnominal they are used to label various\nuh label our variables without providing\nany measurable value\ncountry gender race hair color etc\nit's something that you either mark true\nor false this is a label it's on or off\neither they have a red hat on or they do\nnot\nso a lot of times when you're thinking\nnominal data\nlabels\nthink of it as a true false kind of\nsetup and we look at ordinal this is\ncategorical data with a set order or a\nscale to it\nand you can think of salary range as a\ngreat one\nmovie ratings etc\nyou see here the salary rains if you\nhave ten thousand to twenty thousand\nnumber of employees earning that rate is\na hundred and fifty twenty thousand to\nthirty thousand a hundred and so forth\nsome of the terms you'll hear is bucket\nthis is where you have 10 different\nbuckets and you want to separate it into\nsomething that makes sense into those 10\nbuckets\nand so when we start talking about\nordinal a lot of times when you get down\nto the brass bones again we're talking\ntrue false so if you're a member of the\n10 to 20 k rains\nso forth those would each be either part\nof that group or you're not but now\nwe're talking about buckets and we want\nto count how many people are in that\nbucket\nquantitative numerical data\nfalls into two classes discrete or\ncontinuous\nand so data with a final set of values\nwhich can be categorized class strength\nquestions answered correctly and runs\nhit and cricket\na lot of times when you see this you can\nthink integer\nand a very restricted integer i.e\nyou can only have 100 questions\non a test so you can it's very discreet\ni only have 100 different values that it\ncan attain\nso think usually you're talking about\nintegers but within a very small range\nthey don't have an open end or anything\nlike that\nso discrete is very solid\nsimple to count\nset number\ncontinuous on the other hand continuous\ndata can take any numerical value within\na range so water pressure weight of a\nperson etc\nusually we start thinking about float\nvalues where they can get phenomenally\nsmall in their in what they're worth\nand there's a whole series of values\nthat falls right between discrete and\ncontinuous\nyou can think of the stock market you\nhave dollar amounts it's still discrete\nbut it starts to get complicated enough\nwhen you have like you know jump in the\nstock market from\n525.33 cents to\n580.67 cents there's a lot of point\nvalues in there it'd still be called\ndiscrete but you start looking at it as\nalmost continuous because it does have\nsuch a variance in it now we talk about\nno we did we went over nominal and\nordinal\nalmost true false charts and we looked\nat quantitative and numerical data which\nwere starting to get into numbers\ndiscrete you can usually a lot of times\ndiscrete will be put into it could be\nput into true false but usually it's not\nso we want to address this stuff and the\nfirst thing you want to look at is the\nvery basic which is your algebra so\nwe're going to take a look at linear\nalgebra you can remember back when your\neuclidean geometry we have a line well\nlet's go through this we have a linear\nalgebra is the domain of mathematics\nconcerning linear equations\nand their representations in vector\nspaces and through matrixes i told you\nwe're going to talk about matrixes\nso a linear equation\nis simply\n2x plus 4y minus 3z equals 10. very\nlinear 10x plus 12.4 y equals z and now\nyou can actually solve these two\nequations by combining them\nand that's we're talking about a linear\nequation\nin the vectors we have a plus b equals c\nnow we're starting to look at a\ndirection\nand these values usually think of an x y\nz plot\nso each one is a direction and the\nactual\ndistance of like a triangle a b is c\nand then your matrix can describe all\nkinds of things\ni find matrixes\nconfuse a lot of people\nnot because they're particularly\ndifficult but because of the magnitude\nand the different things they're used\nfor\nand a matrix is a chart\nor a\nyou know think of a spreadsheet but you\nhave your rows and your columns\nand you'll see here we have a times b\nequals c\nvery important to know your counts\nso depending on how the math is being\ndone what you're using it for making\nsure you have the same rows and number\nof columns or a single number there's\nall kinds of things that play in that\nthat can make matrixes confusing\nbut really it has a lot more to do with\nwhat domain you're working in\nare you adding in\nmultiple polynomials where you have like\na x squared plus b y plus you know you\nstart to see that it can be very\nconfusing versus a very straightforward\nmatrix and let's just go a little deeper\ninto these because these are such\nprimary this is what we're here to talk\nabout is these different math uh\nmathematical computations that come up\nso we're looking at linear equations\nlet's dig deeper into that one an\nequation having a maximum order of one\nis called a linear equation\nso it's linear because when you look at\nthis we have ax plus b equals c which is\na one variable\nwe have two variable ax plus b y equals\nc a x plus b y plus z c z equals d\nand so forth but all of these\nare to the power of one you don't see x\nsquared you don't see x cubed so we're\ntalking about linear equations that's\nwhat we're talking about in their\naddition\nif you have already dived into say\nneural networks you should recognize\nthis ax plus by plus cz\nsetup plus the intercept\nwhich is basically your your neural\nnetwork each node adding up all the\ndifferent inputs and we can drill down\ninto that most common formula is your y\nequals mx plus c\nso you have your y\nequals the m which is your slope your x\nvalue plus c\nwhich is your\ny-intercept you kind of labeled it wrong\nhere\nthrew me for a loop but the the c would\nbe your y-intercept so when you set x\nequal to zero y equals c and that's\nthat's your y-intercept right there\nuh and that's they just had a reverse\nvalue of y when x equals zero\nequals the y-intercept which is c and\nyour slow gradient line which is your m\nso you get your y equals 2x plus 3.\nand there's lots of easy ways to compute\nthis this way this is why we always\nstart with the most basic one when we're\nsolving one of these problems and then\nof course the\none of the most important takeaways is\nthe slope gradient of the line\nso the slope is very important that m\nvalue\nin this case we went ahead and solved\nthis\nif you have y equals 2 x plus 3 you can\nsee how it has a nice line graph here on\nthe right\nso matrixes a matrix refers to a\nrectangular representation of an array\nof numbers arranged in columns and rows\nso we're talking m rows by n columns\nhere a11 is denotes the element of the\nfirst row in the first column similarly\na12 and it's really pronounced a11 in\nthis particular setup so it's a row one\ncolumn one a 12 is a\nrow one column two\nfirst row and second column and so on\nand there's a lot of ways to denote this\ni've seen these as like a capital letter\na smaller case a for the top row or i\nmean you can see where they can go all\nkinds of different directions as far as\nthe value\nyou just take a moment to realize\nthere's need to be some designation as\nfar as what row it's in and what column\nit's in\nand we have our basic operations we have\naddition so when you think about\naddition you have uh\ntwo matrixes of two by two and you just\nadd each individual number in that\nmatrix and then when you get to the\nbottom you have uh in this case the\nsolution is twelve ten plus two is\ntwelve five plus 3 is 8 and so on and\nthe same thing with subtraction\nnow again your counting matrix is you\nwant to check your\ndimensions of the matrix\nthe shape you'll see shape come up a lot\nin programming so we're talking about\ndimensions we're talking about the shape\nif the two shapes are equal\nthis is what happens when you add them\ntogether or subtract them\nand we have multiplication when you look\nat the multiplication you end up at the\nvery a slightly different setup going\nnow\nif we look at our last one we're\nwe're like why\nthis always gets to me when we get to\nmatrixes they don't really say why you\nmultiply matrixes\nknow my first thought is one times two\nfour times three but if you look at this\nwe get one times two plus four times\nthree\none times three plus four times five\nuh six times two plus three times three\nsix times three plus three times 5. if\nyou're looking at these matrixes\nthink of this more as an equation\nand so we have if you remember we went\nback up here for our multiple line\nequations let's just go back up a couple\nslides where we were looking at\na two variable so this is a two variable\nequation ax plus b y equals c\nand this is a way to make it very quick\nto solve these variables and that's why\nyou have the matrix and that's why you\ndo\nthe multiplication the way they do\nand this is the dot product of uh one\ntimes two\nplus four times three\none times three plus four times five\nuh six times two plus three times three\nsix times three plus three times five\nand it gives us a nice little\n14 23 21 and 33 over here which then can\nbe used and reduced down to a sample\nformula as far as solving the variables\nas you have enough inputs\nuh and then in matrix operations when\nyou're dealing with a lot of matrixes uh\nnow keep in mind multiplying matrixes is\ndifferent than finding the product of\ntwo matrixes okay so we're talking about\nmultiplication we're talking about\nsolving\nuh for equations when you're finding the\nproduct you are just finding one times\ntwo keep that in mind because that does\ncome up i've had that come up a number\nof times where i'm altering data and i\nget confused as to what i'm doing with\nit\nuh transpose flipping the matrix over is\ndiagonal comes up all the time where you\nhave you still have 12 but instead of it\nbeing 12 8 it's now 12 14\n8 21 you're just flipping the columns\nand the rows\nand then of course you can do an inverse\nchanging the signs of the values across\nthis main diagonal\nand you can see here we have the inverse\na to the minus 1 and ends up with\ninstead of 12 8 14 12 it's now minus 22\nminus 12.\nvectors uh vector just means we have\na value and a direction\nand we have down four numbers here on\nour vector\nin mathematics a one dimensional matrix\nis called a vector uh so\nif you have your x plot and you have a\nsingle value that values along the x\naxis and it's a single dimension\nif you have two dimensions you can think\nabout putting them on a graph you might\nhave x and you might have y and each\nvalue denotes a direction and then of\ncourse the actual distance is going to\nbe the hypothesis of that triangle\nand you can do that with three\ndimensionals x y and z\nand you can do it all the way to nth\ndimensions so when they talk about the k\nmeans\nuh for categorizing and how close data\nis together they will compute that based\non the pythagorean theorem so you would\ntake\nthe square of each value add them all\ntogether and find the square root and\nthat gives you a distance\nas far as where that point is where that\nvector exists or an actual point value\nand then you can compare that point\nvalue to another one it makes a very\neasy comparison versus comparing 50 or\n60 different numbers\nand that brings us up to i gene vectors\nand i gene values\nhygiene vectors the vectors that don't\nchange their span while transformation\nand i gene values the scalar values that\nare associated to the vectors\nconceptually\nyou can think of the vector as your\npicture you have a picture it's\ntwo dimensions x and y\nand so when you do those two dimensions\nand those two values or whatever that\nvalue is\nthat is that point but the values change\nwhen you skew it and so\nif we take and we have a vector a\nand that's a set value\nb is\nyour is your you have a and b which is\nyour i gene vector 2 is the i gene value\nso we're altering\nall the values by two that means we're\nmaybe we're stretching it out one\ndirection making it tall if you're doing\npicture editing\nthat's one of the places this comes in\nbut you can see when you're transforming\nuh your different information how you\ntransform it is then your hygiene value\nand you can see here vector after line\ntransit transition uh we have 3a a is\nthe hygiene vector 3 is the aging value\nso a doesn't change that's whatever we\nstarted with that's your original\npicture and 3\nis skewing it one direction and maybe\na b is being skewed another direction\nand so you have a nice tilted picture\nbecause you altered it by those by the\naging values\nso let's go ahead and pull up a demo on\nlinear algebra and to do this i'm going\nto go through my trusted anaconda into\nmy jupiter notebook\nand we'll create a new\nnotebook called linear algebra\nsince we are working in python we're\ngoing to use our numpy i always import\nthat as np or numpy array probably the\nmost popular\nmodule for doing matrixes and things in\ngiven that this is part of a series i'm\nnot going to go too much into numpy we\nare going to go ahead and create two\ndifferent variables a for a numpy array\n10 15 and b29\nwe'll go ahead and run this and you can\nsee there's our two arrays 10 15 29 and\ni went in added a space there in between\nso it's easier to read\nand since it's the last line we don't\nhave to put the print statement on it\nunless you want we can simply but we can\nsimply do a plus b so when i run this\nwe have 10 15 29 and we get 30 24 which\nis what you expect 10 plus 20 15 plus 9\nyou could almost look at this addition\nas being\njust adding up the columns on here\ncoming down and if we wanted to do it a\ndifferent way we could also do a dot t\nplus b dot t\nremember that t flips them and so if we\ndo that we now get them uh we now have\n30 24 going the other way\nwe could also do something kind of fun\nthere's a lot of different ways to do\nthis\nas far as a plus b i can also do a plus\nb\ndot t\nand you're going to see that that will\ncome out the same the 30 24 whether i\ntranspose a and b or transpose them both\nat the end\nand likewise we can very easily subtract\ntwo vectors i can go a minus b\nand we run that and we get minus 10 6.\nnow remember this is the last line in\nthis particular section that's right not\nto put the print around it\nand just like we did before\nwe can transpose either the individual\nor we can transpose the main set up and\nthen we get a -10 six 6 going the other\nway\nnow we didn't mention this in our notes\nbut you can also do a scalar\nmultiplication\nand just put down scalar so you can\nremember that\nuh what we're talking about here is i\nhave\nthis\narray here u\nand if i go a\ntimes u\nwe'll take the value 2 we'll multiply it\nby every value in here so 2 times 30 is\n60 2 times 15\nand just like we did before\nthis happens a lot because when you're\ndoing matrixes you do need to flip them\nyou get 60 30 coming this way\nso in numpy uh we have they called dot\nproduct\nand uh with this this in a two\ndimensional vectors it is equivalent of\ntwo matrix multiplication remember we\nwere talking about matrix multiplication\nuh where it is the\nwell let's walk through it\nwe'll go ahead and start by defining two\num\nnumpy arrays we'll have uh 10 20 25 6 or\nour u and our v\nand then we're going to go ahead and do\nif we take\nthe values\nand if you remember correctly\nan array like this would be 10 times 25\nplus 20 times 6.\nwe'll go ahead and\nprint that\nthere we go\nand then we'll go ahead and do the np\ndot dot\nof u comma\nv\nand we'll find when we do this we go and\nrun this\nwe're going to get\n370\n370.\nso this is a strain multiplication where\nthey use it to solve\nlinear algebra when you have multiple\nnumbers going across and so this could\nbe very complicated we could have a\nwhole string of different variables\ngoing in here but for this we get a nice\nvalue for our dot multiplication\nand we did\naddition earlier which is just your\nbasic addition\nand of course the matrix you can get\nvery complicated on these or\nin this case we'll go ahead and do\nlet's create two\ncomplex matrixes\nthis one is a matrix of\n12 10 4 6 4 31.\nwe'll just print out a so you can see\nwhat that looks like here's print\na\nwe print a out you can see that we have\na\ntwo by three\nlayer matrix for a and we can also put\ntogether\nalways kind of fun when you're playing\nwith print values\nwe could do something like this we could\ngo in here\nthere we go\nwe could print a we have it end with\nequals a\nrun and this kind of gives it a nice\nlook uh here's your matrix that's all\nthis is comma n means it just tags it on\nthe end that's all all that is doing on\nthere\nand then we can simply add in what is a\nplus b and you should already guess\nbecause this is the same as what we did\nbefore there's no difference uh we do a\nsimple vector addition we have 12 plus 2\nis 14 10 plus 8 is 18 and so on\nand just like we did the matrix addition\nwe can also do a minus b\nand do our matrix subtraction\nand we look at this we have what 12\nminus 2 is 10 10 minus 8\nwhere are we\noh there we go eight minus uh\nconfusing what i'm looking at i should\nhave reprinted out the original numbers\nuh but we can see here 12 minus 2 is of\ncourse 10 10 minus 8 is 2\n4 minus 46 is minus 42 and so forth so\nsame as the subtraction as before we\njust call it matrix subtraction it's\nidentical\nnow if you remember up here we had a\nscalar addition we're adding just one\nnumber to a matrix you can also do\nscalar multiplication\nand so simply if you have a single value\na and you have b which is your array we\ncan also do a times b\nwhen we run that\nyou can see here we have 2 times 4 is 8\n5 times 4 is 20 and so forth you're just\nmultiplying the 4 across each one of\nthese values\nand this is an interesting one that\ncomes up\na little bit of a brain teaser is uh\nmatrix and vector multiplication\nand so when we're looking at this\nuh we are\njust doing regular arrays it doesn't\nnecessarily have to be a numpy array\nwe have a which has our\narray of arrays and b which is a single\narray and so we can from here\ndo the dot\na b\nand this is going to return two values\nand the first value is that is you could\nsay it's like uh\nwe're doing the\nthis array b array\nfirst with a and then with a second one\nand so it splits it up so you have a\nmatrix of vector multiplication and you\ncan mix and match\nwhen you get into really complicated uh\nback-end stuff this becomes more common\nbecause you're now you've got layers\nupon layers of data and so you you'll\nend up with a matrix and a set of bolt\nvector matrices do you want to multiply\nnow keep in mind that if you're doing\ndata science a lot of times you're not\nlooking at this this is what's going on\nbehind the scenes so if you're in\nthe scikit looking at sk learn where\nyou're doing linear regression models\nthis is some of the math that's hidden\nbehind the scenes that's going on\nother times you might find yourself\nhaving to do part of this and manipulate\nthe data around so it fits right and\nthen you go back in and you run it\nthrough the psi kit and if we can do\nup here where we did a\nmatrix and vector multiplication we can\nalso do matrix two matrix multiplication\nand if we run this we have the two\nmatrixes\nyou can see a very complicated array\nthat of course comes out on there for\nour dot\nand just to reiterate it we have our\ntranspose matrix which is your dot t\nand so if we create a matrix a and we do\ntranspose it you can see how it flips it\nfrom\n5 10 15 20 25 30 to 5 15 25\n10 20 30\nrows and columns\nand certainly with the math this comes\nup a lot\nit also comes up a lot with xy plotting\nwhen you put in the pi plot you have one\nformat where they're looking at pairs\nand numbers and then they want all of\nx's and all y's so you know the\ntranspose is an important tool both for\nyour math and for plotting and all kinds\nof things\nanother tool that we didn't discuss uh\nis your identity matrix\nuh and this one is more definition\nuh the identity matrix um we have here\none where we just did two\nso it comes down as one zero zero one uh\none zero zero zero one zero it creates a\ndiagonal of one and what that is is when\nyou're doing your identities you could\nbe comparing\nall your different features to the\ndifferent features and how they\ncorrelate\nand of course when you have feature one\ncompared to feature one to itself it is\nalways\none\nwhere\nusually it's between zero one depending\non how well correlates so when we're\ntalking about identity matrix that's\nwhat we're talking about right here is\nthat you create this preset matrix and\nthen you might adjust these numbers\ndepending on what you're working with\nand what the domain is\nand then another thing we can do\nto kind of wrap this up we'll hit you\nwith the most complicated\npiece of this\npuzzle here is an inverse\na matrix\nand let's just go ahead and put the um\nit's a lengthy description\nlet's go and put the description this is\nstraight out of the\nthe website for\nnumpy\nso given a square matrix a here's our\nsquare matrix a which is 2 1 0 0 1 0 1 2\n1. keep in mind 3 by 3 is square it's\ngot to be equal it's going to return the\nmatrix a inverse satisfying dot a\na inverse so here's our matrix\nmultiplication\nand then of course it equals the dot\nyeah a inverse of a\nwith an identity shape of\na dot shape 0. this is just reshaping\nthe identity\nthat's a little complicated there\nso we're going to have our here's our\narray\nwe'll go ahead and run this\nand you can see what we end up with\nis we end up with an array 0.5 minus 0.5\nand so forth with our 2 1 1 going down 2\n1 0 0 1 0 1 2 1.\ngetting into a little deep on the math\nunderstanding\nwhen you need this is probably really is\nis what's really important when you're\ndoing data science\nversus\nhandwriting this out and looking up the\nmath and handwriting all the pieces out\nyou do need to know about the linear\nalgorithm inverse of a\nso if it comes up you can easily pull it\nup or at least remember where to look it\nup you took a look at the algebra side\nof it let's go ahead and take a look at\nthe calculus side of what's going on\nhere with the machine learning so\ncalculus oh my goodness and differential\nequations you got to throw that in there\nbecause that's all part of the\nbag of tricks especially when you're\ndoing large neural networks but also\ncomes up in many other areas the good\nnews is most of it's already done for\nyou in the back end\nso when it comes up you really do need\nto understand from the data science not\ndata analytics data analytics means\nyou're digging deep into\nactually solving these math equations\nand a neural network is just a giant\ndifferential equation so we talk about\ncalculus\nwe're going to go ahead\nand understand it by talking about cars\nversus time and speed\nso helps to calculate the spontaneous\nrate of change\nso suppose we plot a graph of the speed\nof a car with respect to time\nso as you can see here going down the\nhighway probably merged into the highway\nfrom an on-ramp so i had to accelerate\nso my speed went way up uh stuck in\ntraffic merged into the traffic traffic\nopens up and i accelerate again up to\nthe speed limit\nand uh maybe peter's off up there so you\ncan look at this is as\nthe speed versus time i'm getting faster\nand faster because i'm continually\naccelerating and if i hit the brakes you\ngo the other way\nso the rate of change of speed with\nrespect of time is nothing but\nacceleration how fast are we\naccelerating\nthe acceleration is the area between the\nstar point of x and the endpoint of\ndelta x\nso we can calculate a simple\nif you had x and delta x we could put a\nline there\nand that slope of the line is our\nacceleration\nnow that's pretty easy when you're doing\nlinear algebra but i don't want to know\nit just for that line and those two\npoints i want to know what across the\nwhole of what i'm working with\nthat's where we get into calculus\nso we talk about the distance between x\nand delta x it has to be the smallest\npossible near to zero in order to\napproximate the acceleration\nso the idea is instead of i mean if you\never did took a basic calculus class\nthey would draw bars down here and you\nwould divide this area up\nlet's go back up the screen you divide\nthis area of this time period up into\nmaybe 10 sections and you'd use that and\nyou could calculate the acceleration\nbetween each one of those 10 sections\nkind of thing\nand then we just keep making that space\nsmaller and smaller until delta x is\nalmost\ninfinitesimally small\nand so we get a function of a\nequals a limit as h goes to 0 of a\nfunction of a plus h minus a function of\na over h and that is you're\ncomputing the slope of the line\nwe're just computing that slope under\nsmaller and smaller and smaller samples\nand that's what calculus is calculus is\nthe integral you can see down here we\nhave our nice uh integral sign looks\nlike a giant s\nand that's what that means is that we've\ntaken this down to as small as we can\nfor that sampling\nso we're talking about calculus we're\nfinding the area under the slope is the\nmain process in the integration\nsimilar small intervals are made of the\nsmallest possible length of x plus delta\nx where delta x approaches almost an\ninfinitesimally small space\nand then it helps to find the overall\nacceleration by summing up all the links\ntogether\nso we're summing up all the\naccelerations from the beginning to the\nend\nand so here's our integral we sum of a\nof x times d of x\nequals a plus c\nthat is our basic calculus here\nso when we talk about multivariate\ncalculus\nmultivariate calculus deals with\nfunctions that have multiple variables\nand you can see here we start getting\ninto some very complicated equations\nchange in w over change of time\nequals change of w over change of z\nthe differential of z to dx differential\nof x to dt it gets pretty complicated uh\nit really translates into the\nmultivariate integration using double\nintegrals and so you have the the sum of\nthe sum of f of x of y of d of a equals\nthe sum from c to d and a to b of f of x\ny d x d y equals\nuh the sum of a to b sum of c to d of f\nx y d y d x\nunderstanding the very specifics of\neverything going on in here and actually\ndoing the math is usually calculus 1\ncalculus 2 and differential equations\nso you're talking about three\nfull-length courses to dig into\nand solve these math equations\nwhat we want to take from here is we're\ntalking about calculus uh we're talking\nabout summing of all these different\nslopes and so we're still solving a\nlinear expression we're still solving\ny equals m x plus b\nbut we're doing this for infinitesimally\nsmall x's and then we want to sum them\nup that's what this integral sign means\nthe sum of a of x d of x equals a plus c\nand when you see these very complicated\nmultivariate differentiation using the\nchain rule\nwhen we come in here and we have the\nchange of w to the change of t equals\nthe change of w dz\nand so forth that's what's going on here\nthat's what these means we're basically\nlooking for the area under the curve\nwhich really comes to\nhow is the change changing speeds going\nup how is that changing and then you end\nup with a multiple layer so if i have\nthree layers of neural networks how is\nthe third layer changing based on the\nsecond layer changing which is based on\nthe first layer changing and you get the\npicture here that now we have a very\ncomplicated\nmultivariate integration\nwith integrals\nthe good news is we can solve this\nmathematically and that's what we do\nwhen you do neural networks and reverse\npropagation\nso the nice thing is that you don't have\nto solve this on paper unless you're a\ndata analysis and you're working on the\nback end of integrating these formulas\nand building the script to actually\nbuild them so we talk about applications\nof calculus it provides us the tools to\nbuild an accurate predictive model\nso it's really behind the scenes we want\nto guess that what the change of the\nchange of the change is\nthat's a little goofy i know i just\nthrew that out there it's kind of a meta\nterm but if you can guess how things are\ngoing to change then you can guess what\nthe new numbers are\nmultivariate calculus explains the\nchange in our target variable in\nrelation to the rate of change in the\ninput variables\nso there's our multiple variables going\nin there if one variable is changing how\ndoes it affect the other variable\nand then in gradient descent calculus is\nused to find the local and global maxima\nand this is really big uh we're actually\ngoing to have a whole section here on\ngradient descent because it is\nreally i mean i talked about neural\nnetworks and how you can see how the\ndifferent layers go in there but\ngradient descent is one of the most key\nthings for trying to guess the best\nanswer to something so let's take a look\nat the code behind gradient descent\nand before we open up the code let's\njust do real quick\ngradient descent\nlet's say we have a curve like this and\nmost common\nis that this is going to represent your\nerror oops\nerror there we go error\nah hard to read there and i want to make\nthe error as low as possible\nand so what i'm looking at it is i want\nto find this line here\nwhich is the\nminimum value so we're looking for the\nminimum\nand it does that by\nuh sampling there\nand then it based on this it guesses it\nmight be someplace here and it goes hey\nthis is still going down\nit goes here and then goes back over\nhere and then goes a little bit closer\nand it's just playing a high low until\nit gets to that spot that bottom spot\nand so we want to minimize the error and\non the flip note you could also want to\nbe maximizing something you want to get\nthe best output of it\nthat's simply\nminus the value\nso if you're looking for where the peak\nis\nthis is the same as a negative\nfor where the valley is looking for that\nvalley\nthat's all that is and this is a way of\nfinding it so\nthe cool thing is um all the heavy\nlifting's done\ni actually\nended up putting together one of these a\nwhile back as when i didn't know about\nsidekick and i was just starting\nboys a long while back\nand uh\nis playing high low how do you play high\nlow not get stuck in the valleys uh\nfigure out these curves and things like\nthat well you do that and the back end\nis all the calculus and differential\nequations to calculate this out\nthe good news is you don't have to do\nthose\nso instead we're going to put together\nthe code and let's go ahead\nand see what we can do with that\nso uh guys in the back put together a\nnice little piece of code here which is\nkind of fun\nuh some things we're gonna note and this\nis this is really important stuff\nbecause when you start doing your data\nscience and digging into your machine\nlearning models\nyou're going to find\nthese things are stumbling blocks\nthe first one is current x where do we\nstart at\nkeep in mind\nyour model that you're working with is\nvery generic so whatever you use to\nminimize it the first question is where\ndo we start\nand we started at this because the\nalgorithm starts at x equals three\nso we arbitrarily picked five learning\nrate is uh how many bars to skip going\none way or the other i'm in fact i'm\ngoing to separate that a little bit\nbecause these two are really important\nif we're dealing with something like\nthis where we're talking about\nwell here's our here's the function\nwe're going to use our\ngradient of our function\n2 times x plus 5 keep it simple so\nthat's a function we're going to work\nwith so if i'm dealing with increments\nof a thousand point one is going to be a\nvery long time\nand if i'm dealing with increments of\n0.001\n0.1 is going to skip over my answer so i\nwon't get a very good answer\nand then we look at precision this tells\nus when to stop the algorithm so again\nvery specific to what you're working on\nuh if you're working with money and\nyou don't convert it into a float value\nuh you might be dealing with 0.01 which\nis a penny that might be your precision\nyou're working with\nand then of course the previous step\nsize max iterations we want something to\ncut out at a certain point usually\nthat's built into a lot of minimization\nfunctions\nand then here's our actual\nformula we're going to be working with\nand then we come in we go while previous\nstep size is greater than precision and\nit is less than max and max iters\nsay that 10 times fast\nwe're just saying if it's uh if we're if\nwe're still greater than our precision\nlevel we still got to keep digging\ndeeper\num and then we also don't want to go\npast a thou or whatever this is a\nmillion or ten thousand uh running\nthat's actually pretty high i almost\nnever do max iterations more than like\n100 or 200\nrare occasions you might go up to four\nor 500 if it's depending on the problem\nyou're working with uh so we have our\nprevious equals our current that way we\ncan track timewise\nthe current now equals the current minus\nthe rate times the formula of our\nprevious x\nso now we've generated our new version\nprevious step size equals the absolute\ncurrent previous\nso we're looking for the change in x\nerrors equals iterations plus one that's\nso we know to stop if we get too far\nand then we're just going to print the\nlocal minimum occurs at\nx on here and if we go ahead and run\nthis\nyou can see right here it gets down to\nthis point and it says hey\nlocal minimum is minus\n3.3222 for this particular series we\ncreated\nthis is created off of our formula here\nlambda x two times x plus five\nnow\nwhen i'm running this stuff uh you'll\nsee this come up a lot\nin uh with the sk learn kit\nand one of the nice reasons of breaking\nthis down the way we did\nis i could go over those top pieces uh\nthose top pieces are everything when you\nstart looking at these minimization tool\nkits in built-in code and so from um\nwe'll just do it's actually\ndocs\ndot\nscipy.org\nand we're looking at\nthe scikit\nthere we go\noptimize minimize\nyou can only minimize one value\nyou have the function that's going in\nthis function can be very complicated\nso we used a very simple function up\nhere\nit could be\nthere's all kinds of things that could\nbe on there and there's a number of\nmethods to solve this as far as how they\nshrink down\nand your x naught there's your there's\nyour start value so your function your\nstart value\num\nthere's all kinds of things that come in\nhere that you can look at which we're\nnot going to\noptimization automatically creates\nconstraints bounds\nsome of this it does automatically but\nyou really the big thing i want to point\nout here is you need to have a starting\npoint you want to start with something\nthat you already know is mostly the\nanswer\nif you don't then it's going to have a\nheck of a time trying to calculate it\nout\nor you can write your own little script\nthat does this and does a high low\nguessing and tries to find the max value\nthat brings us to\nstatistics what this is kind of all\nabout is figuring things out a lot of\nvocabulary and statistics ah so\nstatistics well i guess it's all\nrelative it's definitely not an edel\nclass\nso a bunch of stuff going on statistics\nstatistics concerns with the collection\norganization analysis interpretation\nand presentation of data\nthat is a mouthful\nso we have from end to end\nwhere does it come from is it valid what\ndoes it mean how do we organize it um\nhow do we analyze it then you gotta take\nthose analysis and interpret it into\nsomething that people can use kind of\nreduce it to\nunderstandable\nand nowadays you have to be able to\npresent it if you can't present it then\nno one else is going to understand what\nthe heck you did\nso we look at the terminologies\nthere is a lot of terminologies\ndepending on what domain you're working\nin so clearly if you're working in\na domain that deals with\nviruses and t cells and and\nhow does you know where does that come\nfrom you're studying the different\npeople then you can have a population\nif you are working with\nmechanical gear\nyou know a little bit different if\nyou're looking for the wobbling\nstatistics uh to know when to replace a\nrotor on a machine or something like\nthat\nthat can be a big deal you know we have\nthese huge fans that turn\nin our sewage processing systems and so\nthose fans they start to wobble and hum\nand do different things that the sensors\npick up at one point do you replace them\ninstead of waiting for it to break in\nwhich case it costs a lot of money\ninstead of replacing a bushing you're\nreplacing the whole fan unit\nan interesting project that came up for\nour city a while back\nso population all objects are\nmeasurements whose properties are being\nobserved\nso that's your population all the\nobjects it's easy to see it with people\nbecause we have our population in large\nbut in the case of the sewer fans we're\ntalking about the fan units that's the\npopulation of fans that we're working\nwith\nyou have a parameter a matrix that is\nused to represent a population or\ncharacteristic\nyou have your sample a subset of the\npopulation studied you don't want to do\nthem all because then you don't have a\nif you come up with a conclusion for\neveryone you don't have a way of testing\nit so you take a sample\nsometimes you don't have a choice you\ncan only take a sample of what's going\non you can't\nstudy the whole population and a\nvariable a metric of interest for each\nperson or object in a population\ntypes of sampling\nwe have a probabilistic approach\nselecting samples from a larger\npopulation using a method based on the\ntheory of probability\nand we'll go into a little bit more\ndeeper on these we have random\nsystematic stratified\nand then you have a non-probabilistic\napproach selecting samples based on the\nsubjective judgment of the researcher\nrather than random selection\nit has to do with convenience trying to\nreach a quota\nor snowball\nand they're very biased that's one of\nthe reasons you'll see this big stamp on\nthat says biased so you gotta be very\ncareful on that\nso probabilistic sampling uh when we\ntalk about a random sampling we select\nrandom size samples from each group or\ncategory so we it's as random as you can\nget we talk about systematic sampling\nwe're selecting random size samples from\neach group or category with a fixed\nperiodic interval\nuh so we kind of split it up this would\nbe like a time set up or our different\ncategories\nand you might ask your question what is\na category or a group\nif you look at i'm going to go back a\nwindow let's say we're studying\neconomics of different of an area\nwe know pretty much that based on their\nculture where they came from\nthey might need to be separated and so\nuh and when i say separated i don't mean\nseparated from their\nplace where they live i mean as far as\nthe analysis we want to look at the\ndifferent groups and make sure they're\nall represented\nso if we had like an eighty percent uh\nof a group that is uh say hispanic and\nor indian and also in that same area we\nhave twenty percent twenty percent who\nare uh let's call our expatriates they\nleft america and they're nice and\nyour caucasian group we might want to\nsample a group that is representative of\nboth uh so we're talking about\nstratified sampling and we're talking\nabout groups those are the groups we're\ntalking about and that brings us to\nstratified sampling selecting\napproximately equal size samples from\neach group or category\nthis way we can actually separate the\ncategories and give us an insight into\nthe different cultures and how that\nmight affect them in that area\nso you can see these are very very\ndifferent kind of\ndepends on what you're working with\nas far as your data and what you're\nstudying\nand so we can see here just a little bit\nmore we have selecting 25 employees from\na company of 250 employees randomly\ndon't care anything about them what\ngroups are in which office they're in\nnothing\nand we might be selecting one employee\nfrom every 50 unique employees in a\ncompany of 250 employees\nand then we have selecting one employee\nfrom every branch in the company office\nso we have all the different branches\nthere's our group or categories by the\nbranch and the category could depend on\nwhat you're studying so it has a lot of\nvariation on there\nyou see this kind of grouping and\ncategorizing is also used to generate a\nlot of misinformation\nso if you only study one group and you\nsay this is what it is\nthen everybody assumes that's what it is\nfor everybody and so you've got to be\nvery careful of that it's very unethical\nthing to kind of do\nso types of statistics we talk about\nstatistics\nwe're going to talk about descriptive\nand inferential statistics\nthere are so many different terms and\nstatistics to break it up uh so we so\nwe're talking about a particular\nsetup\nso we're talking about descriptive and\ninferential\nstatistics\nthe base of the word describe\nis pretty solid you're describing the\ndata what does it look like with\ninferential statistics we're going to\ntake that from the small population to a\nlarge population so if you're working\nwith a drug company you might look at\nthe data and say these people were\nhelped by this drug\nthey did\n80 percent better\nas far as their health or 80 percent\nbetter survival rate than the people\nwho did not have the drug so we can\ninfer that that drug will work in the\ngreater populace and will help people so\nthat's where you get your inferential so\nwe are\npredicting how it's going to affect the\ngreater population\nso descriptive statistics it is used to\ndescribe the basic features of data and\nform the basis of quantitative analysis\nof data\nso we have a measure of central\ntendencies we have your mean median and\nmode\nand then we have a measure of spread\nlike your range your interquartile range\nyour variance and your standard\ndeviation\nand we're going to look at all these a\nlittle deeper here in a second\nbut one of them you can think of is\nhow the data difference\ndifferences you know what's the max min\nrange all that stuff is your spread and\nanything that's just a single number is\nusually your central\ntendencies measure of central tendencies\nso we talk about the mean it is the\naverage of the set of values considered\nwhat is the average outcome of\nwhatever's going on\nand then your median\nseparates the higher half and the lower\nhalf of data\nso where's the center point of all your\ndifferent data points\nso your mean might have some a couple\nreally big numbers that skew it\nso that the average is much higher than\nif you took those outliers out where the\nmedian would by separating the high from\nthe low might give you a much lower\nnumber you might look at it and say oh\nthat's that's odd why is the average so\nmuch higher than the median well it's\nbecause you have some outliers or why is\nit so much lower\nand then the mode is the most frequent\nappearing value this is really\ninteresting if you're studying economics\nand how people are doing you might find\nthat the most common\nincome like in the u.s was\n1.24 000 a year\nwhere the average was closer to 80 000\nand it's like wow what a difference well\nthere's some people have a lot of money\nand so that skews that way up so the\naverage person is not making that kind\nof money and then you look at the median\nincome and you're like well the median\nincome is a little bit closer to the\naverage uh so it does create a very\ninteresting way of looking at the data\nagain these are all uh central\ntendencies single numbers you can look\nat for the whole spread of the data\nand we look at the measure of central\ntendencies the mean is the average marks\nof a students in a classroom so here we\nhave the mean sum of the marks of the\nstudents total number of students and as\nwe talked about the median\nwe have\n0 through 10\nand we take half the numbers and put\nthem on one side of the line half the\nnumbers on the other side of the line\nwe end up with five in the middle and\nthen the mode what mark was scored by\nmost of the students in a test\nin a simple case where most people\nscored like an 82 percent and got\ncertain problems wrong easy to figure\nout\nnot so easy when you have different\nareas where like you have like the um oh\nlet's go back to economy a little bit\nmore difficult to calculate if you have\na large group that scores that makes 30\n000\nand a slightly bigger group that makes\n26 000. so what do you put down for the\nmode\nuh certainly there's a number of ways to\ncalculate that and there's actually\ndifferent variations depending on what\nyou're doing\nso now we're looking at a measure of\nspread uh range what's the difference\nbetween the highest and the lowest value\nfirst thing you want to look at you know\nit's uh we had everybody in the test\nscore between 60 and 100 percent so we\ngot 100 or maybe 60 to 90 it was so hard\nthat a lot of people could not get a\nhundred percent\num you have your\ninter-quartile range quartiles divide a\nrank ordered data set into four equal\nparts\nvery common thing to do as part of all\nthe basic packages whether you're\nworking in\ndata frames with pandas whether you're\nworking in scala whether you're working\nin r\nyou'll see this come up where they have\nrange your min your max and then it'll\nhave your interquartile range how does\nit look like in each quarter of data\nvariance measures how far each number in\nthe set is from the mean and therefore\nfrom every other number in the set\nso you have like how much turbulence is\ngoing on in this data\nand then the standard deviation it is to\nmeasure the variance or the dispersion\nof a set of values from the mean\nand you'll usually see if i'm doing a\ngraph i might have the value graphed\nand then based on the the error i might\ngra graph the standard deviation in the\nerror on the graph as a background so\nyou can see how far off it is\nso standard deviation is used a lot\nso measurement of spread uh marks of a\nstudent out of a hundred we have here\nfrom 50 to 63 or 50 to 90.\nso the range maximum marks minimum marks\nwe have 90 to 45 and the spread of that\nis 45 90 minus 45. and then we have the\ninterquartile range using the same marks\nover there you can see here where the\nmedian is\nand then there's the first quarter the\nsecond quarter and the third quarter\nbased on splitting it apart by those\nvalues\nand to understand the variance and\nstandard deviation we first need to find\nout the mean\nso here's our you know calculating the\naverage there we end up approximately 66\nfor the average and then we look at that\nthe variance once we know the means we\ncan do equals the marks minus the mean\nsquared\nwhy is it squared\nbecause one you want to make sure it's\nyou don't have like if you if you're\nputting all this stuff together you end\nup with an error as far as one's\nnegative one's positive one's a little\nhigher one's a little lower\nso you always see\nthe squared value and over the total\nobservations\nand so the standard deviation equals the\nsquare root of the variance which is\napproximately 16.\nand if you were looking at\na predictable model you would be looking\nat the deviation based on the error how\nmuch error does it have\nthat's again\nreally important to know if your if your\nprediction is predicting something\nwhat's the chance of it being way off or\njust a little bit off\nnow that we've looked at the\ntools as far as some of the basics for\ndoing your statistics and we're talking\nabout\nlet's go ahead and pull up a little demo\nand show you what that looks like in\npython code so you can get some little\nhands on here for that let's go back\ninto our jupiter notebook in python now\nalmost all of this you can do in numpy\nlast time we worked in numpy this time\nwe're going to go ahead and use pandas\nand if you remember from\npandas on here\nthis is basically a data frame rows\ncolumns let's just go ahead and do a\nprint\ndf.head\nand run that\nand you can see we have the name jane\nmichael william rosie hannah sat on\ntheir salaries on here and of course\ninstead of having to do all those hand\ncalculations and add everything together\nand divide by the total\nwe can do something very simple on this\nlike use the command mean in pandas and\nso if i go ahead and do this print df\npick our column salary because we want\nto find the means of that calorie\nwe want to find the means of that column\nand we go and print this out and you can\nsee that the\naverage income on here is 71 000.\nand let's just go ahead and do this\nwe'll go ahead and put in\nmeans\nand if we're going to do that we also\nmight want to find the median\nand the median is\nvery similar\nexcept it actually is just median we're\nused to means and average is kind of\ninteresting that those are they use the\ntwo different words\nuh there can be in some computation\nslight differences but for the most part\nthe means is the average uh and then the\nmedian\noops let's put a\nmedian here\ndo you have salary that way it displays\na little better we can see the median is\n54\n000. so the halfway mark is\nsignificantly below the average\nwhy because we have somebody in here\nmakes 189 000. darn you rosie for\nthrowing off our numbers\nbut that's something you'd want to\nnotice this is this is the difference\nbetween these is huge and so is what is\nthe meaning behind that when you're\nstudying a populist and looking at\nthe different data coming in and of\ncourse we also want to find out hey\nwhat's the most uh common\nincome that people make\nin this little tiny sample and so we'll\ngo ahead and do the mode\nand you can see here with the mode uh\nit's at 50 000.\nso this is this is very telling that\nmost people are making 50 000.\nthe middle point is at 54 000. so half\nthe people are making more than that\nwhat that tells me is that if the most\ncommon income is below the median\nthen\nthere's a few there's a scale there's a\nlot of high salaries going up but\nthere's some really low salaries in\nthere\nand so this trend which is very common\nin statistic when you're analyzing\nthe economy and different people's\nincome is pretty common and the bigger\ndifference between these is also\nvery important when we're studying\nstatistics\nand when you hear someone just say hey\nthe average income was you might start\nasking questions at that point why\naren't you talking about the median\nincome why aren't you talking about the\nmode the most common income what are you\nhiding\nuh and if you're doing these analysis\nyou should be looking at these saying\nhey why are these discrepancies why are\nthese so different and of course with\nany analysis it's important to find out\nthe minimum\nand the maximum so we'll go ahead it's\njust simply\ndot min will pull up your minimum and\nthen dot max pulls up the maximum\npretty straightforward on as far as um\ntranslating it and knowing which you\nknow what the\nlowest value what your highest value is\nhere\nwhich you'll use to generate like a\nspread later on and real quick on no\nmode uh note that it puts mode zero like\ni said there's a couple different ways\nyou can compute the mode\num although the standard one's pretty\ngood we can of course do the range\nwhich is your max minus your min so now\nwe have a range of 149 000 between the\nupper end and the lower end and you\nmight want to be looking up the\nindividual values on all of these but\nturns out there is a describe\nfeature in pandas\nand so in pandas we can actually do df\nsalary describe and if we do this you\ncan see we have that there's seven uh\nsetups here's our mean\nour standard deviation which we didn't\ncompute yet which would just be a dot\nstd\nand you gotta be a little careful\nbecause when it computes it it looks for\naxes and things like that\nwe have our minimum value and here's our\nquartiles\nour maximum value and then of course the\nname salary\nso these are the these are the basic\nstatistics you can pull them up and like\njust describe\nthis is a dictionary so i could actually\ndo something like um\nin here i could actually go uh count\nand run\nand now it just prints the count\nso because this is a dictionary you can\npull any one of these values out of here\nit's kind of a quick and dirty way to\npull all the different information\nand then split it up and depending on\nwhat you need\nnow if i just walked in and gave you\nthis information\nin a meeting\nat some point you would just kind of\nfall asleep\nthat's what i would do anyway um\nso we want to go ahead and see about\ngraphing it here and we'll go ahead and\nput it into a histogram and plot that\ngraph on it\nof the salaries and let's just go ahead\nand put that in here so\nwe do our map plot inline remember\nthat's a jupiter's notebook thing\na lot of the new version of the matplot\nlibrary does it automatically\nbut just in case i always put it in\nthere import map plot library pi plot is\nplt that's my plotting\nand then we have our data frame i don't\ni guess i really don't need to respell\nthe data frame\nmaybe we could just remind ourselves\nwhat's in it so we'll go ahead and just\nprint\ndf\nthat way we still have it\nand then we have our salary df salary\nsalary.plot history title salary\ndistribution color gray\nplot ax v line salary the mean value so\nwe're going to take the mean value\ncolor violet line style dash this is\njust all making it pretty\nuh what color dashed line line width of\ntwo that kind of thing\nand the median and let's go ahead and\nrun this just so you can see what we're\ntalking about\nand so up here we are taking on our plot\num so here's the data here's our our\ndata frame print it out so you can see\nit with the salaries we'll look at the\nsalary distribution and just look at\nthis the way the is distributed um\nyou have our\nin this case we did\nlet's see we had red for the median\nwe have violet\nfor our average or mean\nand you can just see how it really\ni mean here's our outlier here's our\nperson who makes a lot of money here's\nthe\naverage and here's the median\nand so as you look at this you can say\nwow\nbased on the average it really doesn't\ntell you much about what people are\nreally taking home all it does is tell\nyou how much money is in this you know\nwhat the average salary is\nso\nsome of the things you want to take away\nin addition to this is that it's very\neasy to plot\nan axv line\nthese are these up and down lines for\nyour markers\nand as you just display the data i mean\nyou can add all kinds of things to this\nand get really complicated keeping it\nsimple is pretty straightforward i look\nat this and i can see we have a major\noutlier out here we can definitely do a\nhistogram and stuff like that\nbut you know pictures worth a thousand\nwords\nwhat you really want to make sure you\ntake away is that we can do a basic\ndescribe\nwhich pulls all this information out and\nwe can print any of the individual\ninformation from the describe\nbecause this is a dictionary\nand so if we want to go ahead and look\nup\nthe mean value we can also do describe\nmean so if you're doing a lot of\nstatistics uh being able to\nit doesn't have the print on there so\nit's only going to print the last one\nwhich happens to be the mean uh you can\nvery easily reference any one of these\nand then you can also if you're doing\nsomething a little bit more complicated\nand you don't need just the basics you\ncan come through and pull any one of the\nindividual\nreferences from the from the pandas on\nhere so now we've had a chance to\ndescribe our data\nlet's get into inferential statistics\ninferential statistics allows you to\nmake predictions or inferences from data\nand you can see here we have a nice\nlittle picture movie ratings and\num\nif we took this group of people and said\nhey how many people like the movie\ndislike it can't say and then you ask\njust a random person who comes out of\nthe movie who hasn't been in this study\nyou can infer that 55 chance of saying\nliked\n35 chance of saying disliked or a 10 or\n11 percent chance of can't say\nso that's real basics of what we're\ntalking about you're going to infer that\nthe next person is going to follow these\nstatistics\nuh so let's look at point estimation uh\nit is a process of finding an\napproximate value for a population's\nparameter like mean\nor average from random samples of the\npopulation let's take an example of\ntesting vaccines for covid19\nvaccines and flu bugs all that it's a\npretty big thing of how do you test\nthese out and make sure they're going to\nwork on the populace\na group of people are chosen from the\npopulation medical trials are performed\nresults are generalized for the whole\npopulation so here's a protected there's\nour small group up here where we've\nselected them we run medical trials on\nthem and then the results work for the\npopulation\nnice diagram with the arrows going back\nand forth and the very scary coveted\nvirus in the middle of one\nand let's take a look at the\napplications of inferential statistics\nvery central is what they call\nhypotheses testing\nand the confidence interval which go\nwith that and then as we get into\nprobability we get into our binomial\ntheorem our normal distribution in\ncentral limit theorem hypothesis testing\nhypothesis testing is used to measure\nthe plausibility of a hypothesis\nassumption by using sample data\nnow\nwhen we talk about theorems\ntheory hypothesis uh keep in mind that\nif you are in a philosophy class\ntheory\nis the same as hypothesis where theorem\nis a scientific uh statement that is\nsomething that has been proven\nalthough it is always up for debate\nbecause in science we always want to\nmake sure things are up to debate so\nhypothesis is the same as a\nphilosophical class calling a theory\nwhere theory in science is not the same\ntheory in science says this has been\nwell proven gravity is a theory\nso if you want to debate the theory of\ngravity try jumping up and down if you\nwant to have a theory about why the\neconomy is collapsing in your area\nthat is a philosophical debate\nvery important i've heard people mix\nthose up and it is a pet peeve of mine\nwhen we talk about hypotheses testing\nthe steps involved in hypotheses testing\nis first we formulate a hypothesis\nwe figure out the right test to test our\nhypothesis we execute the test and we\nmake a decision and so when you're\ntalking about hypothesis you're usually\ntrying to disprove it if you can't\ndisprove it\nand it works for all the facts then you\nmight call that a theorem at some point\nso in a use case let's consider an\nexample we have four students we're\ngiven a task to clean a room every day\nsounds like working with my kids they\ndecided to distribute the job of\ncleaning the room among themselves they\ndid so by making four chits which has\ntheir names on it and the name that gets\npicked up has to do the cleaning for\nthat day\nrob took the opportunity to make chits\nand wrote everyone's name on it so\nhere's our four people nick rob imlia\nimlia and summer\nnow rick emilia and summer are asking us\nto decide whether rob has done some\nmischief in preparing the chits i.e\nwhether rob has written his name on one\nof the chit\nfor that we will find out the\nprobability of rob getting the cleaning\njob on first day second day third day\nand so on till 12 days\nthe probability of rob getting the job\ndecreases every day i.e his turn never\ncomes up then definitely he has done\nsome mischief while making the chits\nso the probability of rob not doing work\non day one is a three out of four\nthere's a 0.75 chance that he didn't do\nwork\n2 days 3 4 times 3 4 equals 0.56\nthree days you have three-fourths\nthree-fourths three-fourths which equals\n0.42\nwhen you get to day 12 it's 0.032 which\nis less than 0.05\nremember this 0.05 that comes up a lot\nwhen we're talking about\ncertain values when we're looking at\nstatistics\nrob is cheating as he wasn't chosen for\n12 consecutive days that's a very high\nprobability when\non day 12 he still hasn't gotten the job\ncleaning the room\nso we come up to our important important\nterminologies\nwe have null hypothesis\na general statement that states that\nthere is no relationship between two\nmeasured phenomenon or no association\namong the groups\nalternative hypothesis\ncontrary to the null hypothesis it\nstates whenever something is happening a\nnew theory is preferred instead of an\nold one and so the two hypotheses go\nhand in hand uh so your null this is\nalways interesting in in talking about\ndata science and the math behind it\nit's about proving that the things have\nno correlation null hypothesis says\nthese two have zero relation to each\nother where the alternative hypothesis\nsays hey we found a relation this is\nwhat it is\nwe have p-value the p-value is a\nprobability of finding the observed or\nmore extreme results when the null\nhypothesis of a study question is true\nand the t value it is simply the\ncalculated difference represented in\nunits of standard error the greater the\nmagnitude of t the greater the evidence\nagainst the null hypothesis and you can\nlook at the t values being specific to\nthe test you're doing\nwhere the p value is derived from your t\nvalue and you're looking for what they\ncall the five percent or the 0.05\nshowing that it has a high correlation\nso digging in deeper let's assume that a\nnew drug is developed with the goal of\nlowering the blood pressure more than\nthe existing drug\nand this is a good one because the null\nvalue here isn't that you don't have any\ndrug the null value here is that it's\nbetter than the existing drug the new\ndrug doesn't lower the blood pressure\nmore than the existing drug\nnow if we get that\nthat says our null hypothesis is correct\nthere is no correlation and the new drug\nis not doing its job the alternative\nhypothesis the new drug does\nsignificantly lower the blood pressure\nmore than the existing drug\nuh yay we got a new drug out there and\nthat's our alternative hypothesis or the\nh1 or ha\nand we look at the p value results from\nthe evidence like medical trial showing\npositive results which will reject the\nnull hypothesis\nand again they're looking for\na 0.05 or 5 percent and the t value\ncomparing all the positive test results\nand finding means of different samples\nin order to test hypothesis so this is\nspecific to the test how what percentage\nof increase did they have\nand this leads us to the confidence\nintervals\na confidence interval is a range of\nvalues we are sure our true values of\nobservations lie in\nlet's say you asked a dog owner around\nyou and asked them how many cans of food\ndo you buy for your per year for your\ndog through calculations you got to know\nthat the on an average around 95 percent\nof the people bought around 200 to 300\ncans of food hence we can say that we\nhave a confidence interval of 2 300\nwhere 95 percent of our values lie in\nthat sprint data spread\nuh and this the graph really helps a lot\nso you can start seeing what you're\nlooking at here we have the 95 percent\nyou have your peak in this case it's a\nnormal distribution so you have a nice\nbell curve equal on both sides it's not\nasymmetrical and 95 percent of all the\nvalues lie within a very small range and\nthen you have your outliers the 2.5\npercent going each way\nso we touched upon hypothesis uh we're\ngoing to move into probability so you\nhave your hypothesis once you've\ngenerated your hypothesis we want to\nknow the probability of something\noccurring probability is a measure of\nthe likelihood of an event to occur any\nevent can be predicted with total\ncertainty and can only be predicted as a\nlikelihood of its occurrence so any\nevent cannot be predicted with total\ncertainty can only be predicted as a\nlikelihood of its occurrence\nuh score prediction how good you're\ngoing to do in whatever\nsport you're in weather prediction stock\nprediction\nif you've studied physics and chaos\ntheory even the location of the chair\nyou're sitting on has a probability that\nit might move three feet over\ngranted that probability is one in like\nuh i think we calculated as under one in\ntrillions upon trillions so it's\nthe better the probability the more\nlikely it's going to happen there are\nsome things that have such a low\nprobability\nthat we don't see them\nso we talk about random variable a\nrandom variable is a variable whose\npossible values are numerical outcomes\nof a random phenomena so uh we have the\ncoin tossed how many heads will occur in\nthe series of 20 coin flips probably you\nknow the on average they're 10 but you\nreally can't know because it's very\nrandom how many times a red ball is\npicked from a bag of balls if there's\nequal number of red balls and blue balls\nand green balls in there how many times\nthe sum of digits on two dice results\nare five each\nso you know there's how often you're\ngoing to roll two fives on your pair of\ndives\nso in a use case uh let's consider the\nexample of rolling two dice we have a\nrandom variable outcome equals y you can\ntake values 2 3 4 5 6 7 8 9 10 11 12.\nso we have a random variable and a\ncombination of dice\nand instead of looking at how many times\nboth dice for roll 5 let's go ahead and\nlook at total sum of five and you have\nin as far as your random variables you\ncan have a one four equals five four one\ntwo three three two\nso four of those roles can be four if\nyou look at all the different options\nyou have four of those random rolls can\nbe a five\nand if we look at the total number\nwhich happens to be 36 different options\nuh you can see that we have four out of\n36 chance every time you roll the dice\nthat you're gonna roll a total of five\nyou're gonna have an outcome of five\nand uh we'll look a little deeper as to\nwhat that means but you could think of\nthat at what point if someone never\nrolls a five or they always roll a five\ncan you say hey that person's probably\ncheating\nwe'll look a little closer at the math\nbehind that but let's just consider this\nis one of the cases is rolling two dice\nand gambling\nthere's also binomial distribution is\nthe probability of getting success or\nfailure as an outcome in an experiment\nor trial that is repeated multiple times\nand the key is is by meaning two\nbinomial so passing or failing an exam\nwinning or losing a game and getting\neither head or tails so if you ever see\nbinomial distribution it's based on a\ntrue false kind of setup you win or lose\nlet's consider a use case and let's\nconsider the game of football between\ntwo clubs\nbarcelona and dortmund the teams will\nhave to play a total of four matches and\nwe have to find out the chances of\nbarcelona winning the series so we look\nat the total games and we're looking at\nfive different games or matches\nlet's say that the winning chance for\nbarcelona is 75 or 0.75\nthat means that each game they have a 75\nchance that they're going to win that\ngame and losing chances are 25 or 0.25\nclearly 0.75 plus 0.25 equals 1. so that\naccounts for 100 of the game probability\nfor getting k wins in in matches is\ncalculated\nand we we're talking like so if you have\nfive games and you want to know if i\nplay\nhow many wins in those five games should\ni get what's a percentage on those and\nthe probability for getting k wins and\nin matches is calculated by p x equals k\nequals n c k p to the k q to the n minus\nk\nhere p is the probability of success and\nq is the probability of failure and so\nwe can do total games of n equals five\nwhere k equals zero one two three four\nfive\np which is the chance of winning is\npoint seven five q the chance of losing\nequals one minus p which equals one\nminus point o seven five which equals\npoint two five the probability that\nbarcelona will lose all of the matches\ncan then just plug in the numbers\nand we end up with a .0009765625\nso very small chance they're going to\nlose all their matches\nand we can plug in uh the value for two\nmatches probability that barcelona will\nwin at least two matches is\n0.0878 and of course we can go on to the\nprobability that barcelona will win\nthree matches the 0.26 and course four\nmatches and so on and it's always nice\nto take this information\nand let's find the accumulated discrete\nprobabilities for each of the outcomes\nwhere barcelona has won three or more\nmatches x equals three x equals 4 x\nequals 5.\nand we end up with the p equals 0.264\nplus 0.395 plus 237 which equals 0.89\nin reality\nthe probability of barcelona winning the\nseries is much higher than 0.75\nand it's always nice to\nput out a nice graph so you can actually\nsee the number of wins to the\nprobability and how that pans out\nwith our binomial case\ncontinuing in our important terminology\nlocation the location of the center of\nthe graph depends on the mean value and\nthis is some very important things so\nmuch of the data we look at and when you\nstart looking at probabilities almost\nalways has a normalized look like the\ngraph in the middle\nbut you do have left skewed where the\ndata is skewed off to the left and you\nhave more stuff happening off to the\nleft and you have right skewed data\nand so when this comes up and these\nprobabilities come up where they're\nskewed it's really important to take a\ncloser look at that\nmostly you end up with a normalized set\nof data but you got to also be aware\nthat sometimes it's a skewed data\nand then the height height of the slope\ninversely depends upon the standard\ndeviation\nso you can see down here the standard\ndeviation is really large it kind of\nsquishes it out and if the standard\ndeviation is small then most of your\ndata is going to hit right there in the\nmiddle you can have a nice peak\nand so being aware of this that you\nmight have a probability that fits\ncertain data but it has a lot of\noutliers so you're if you have a really\nhigh standard deviation\nif you're doing stock market analysis\nthis means your predictions are probably\nnot going to make you much money\nwhere if you have a very small deviation\nyou might be right on target and set to\nbecome a millionaire\nwhich leads us to the z-score z-score\ntells you how far from the mean a data\npoint is it is measured in terms of\nstandard deviations from the mean around\n68 percent of the results are found\nbetween one standard deviation\naround 95 percent of the results are\nfound between two standard deviations\nand you read the symbols of course they\nlove to throw some greek letters in\nthere we have a mu minus two sigma\nmu is just a quick way it's a kind of\nfunky u it just means the mean\nuh and then the sigma is the standard\ndeviation and that's the o with the\nlittle arrow off to the right or the\nlittle\nwagy tail going up the o with it with\nthe line on it\nso mu minus two sigma\nis your\nuh 95 percent of the results are found\nbetween two standard deviations\ncentral limit theorem\nthis goes back to the skew if you\nremember we were looking at the skew\nvalues on this previous slide have left\nskewed normalized and right skewed when\nwe're talking about it being skewed or\nnot skewed the distribution of the\nsample means will be approximately\nnormally distributed evenly distributed\nnot skewed\nif you take large random samples from\nthe population with the mean mu and the\nstandard deviation sigma with\nreplacement\nand you can see here\nof course we have our\nmu minus 2 sigma and the spread down\nhere the mean the median and the mode\nand so we're talking about very large\npopulations\nthese numbers should come together and\nyou shouldn't have a skewed value if you\ndo that's a flag that something's wrong\nthat's why this is so important to be\naware of what's going on with your data\nwhere your samples are coming from and\nthe math behind it\nand if we're going to do all this we got\nto jump into conditional probability\nthe conditional probability of an event\na is the probability that the event will\noccur\ngiven the knowledge that an event to be\nhas already occurred\nand you'll see this as bayes theorem\nb-a-y-e-s base\nuh and this is red\ni mean you have these funky looking\nlittle\np\nbrackets a b\nthis is the probability of a being true\nwhile b is already true\nand you have the probability of b being\ntrue when a is already true so p\nb of a\nprobability of a being true divided by\nthe probability of b being true\nand we talk about bayes theorem which\noccurred back in the 1800s when he\ndiscovered this this is such an\nimportant formula and it's really it's\nnot if you actually do the math you\ncould just kind of do\nx y equals j k and then you divide them\nout and you're going to see the same\nmath but it works with probabilities\nwhich makes it really nice\nand so if you have a set you might have\neight or nine different studies going on\nin different areas different people have\ndone the studies they brought them\ntogether\nif we look at today's covered virus the\nvirus spread\ncertainly the studies done in china\nversus the studies the way they're done\nin the u.s\nthat data is different in each of those\nstudies but if you can find a place\nwhere it overlaps\nwhere they're studying the same thing\ntogether you can then compute the\nchanges that you need to make in one\nstudy to make them equal and this is\nalso true if you have a study of\none group and you want to find out more\nabout it so this formula is very\npowerful and it really has to do with\nthe data collection part of the math and\ndata science and understanding where\nyour data is coming from and how you're\ngoing to combine different studies in\ndifferent groups\nand we're going to go into a use case\nlet's find out the chance of a person\ngetting lung disease due to smoking\nthis is kind of interesting the way they\nword this\nlet's say that according to medical\nreport provided by the hospital states\nthat around 10 percent of all patients\nthey treated suffered lung disease\nso we have kind of a generic medical\nreport\nthey further found out\nby a survey that 15 percent of the\npatients that visit them smoke\nso we have 10 percent that are lung\ndisease and\n15 of the patients smoke\nand finally five percent of the people\ncontinued smoke even when they had lung\ndisease uh not the brightest choice um\nbut you know it is an addiction so it\ncan be really difficult to kick and so\nwe can look at the probability of a\nprior probability of 10 people having\nlung disease\nand then probability b probability that\na patient smokes is 15 percent\nuh and the probability of b\nif b then a the probability of a patient\nsmokes even though they have lung\ndisease is five percent\nand probability of a is b probability\nthat the patient will have lung disease\nif they smoke and then when you put the\nformulas together you get a nice\nsolution here you get the probability of\na of b probability that the patient will\nhave lung disease if they smoke\nand you can just plug the numbers right\nin and we get a 3.33 percent chance\nhence there is a 3.33 chance that a\nperson who smokes will get a lung\ndisease\nso we're going to pull up a little\npython code i'm always my favorite roll\nup the sleeves\nkeep in mind we're going to be doing\nthis\nkind of like the back end way\nso that you can see what's going on and\nthen later on we're going to create\nwe'll get into another demo which shows\nyou some of the tools are already\npre-built for this\nlet's start by creating a set so we're\ngoing to create a set with curly braces\nthis means that our set has\nonly unique values so you have a list\nyou have your tuples which can never\nchange and then you have um in this case\nthe the set so four seven you can't\ncreate a four seven comma four it'll\ndelete the four outs it's only unique\nvalues\nand if you use dictionaries\nquick reminder this should look familiar\nbecause it is a dictionary we have a\nvalue and that value is assigned to or\nthat key is assigned to a value\nso you could have a key value set up as\na dictionary so it's like a dictionary\nwithout the value it's just the keys and\nthey all have to be unique\nand if we run this we have a\nset of four seven\nwe can also take a list a regular\nsetup and i'm going to go ahead and just\nthrow in another number in here four\nand run it\nand you can see here if i take my list\none two three four four\nand i convert it to a set and here it is\nmy set from list equals set my list\nthe result is one two three four so it\njust deletes that last four right out of\nthere\nand with the sets you can also go in\nthere and\nprint here is my set my set\nthree is in the set and then if you do\nthree in my set\nthat's going to be a logic function\nuh and one in my set six is not in the\nset and so forth if we run this\nwe get three is in the set true one is\nin the set false because three five\nseven is another one six is in the set\nsix is not in the set so not in my set\nyou can also use this with the list we\ncould have just used three five seven\nand it would have\nthe same response on there is three and\ni usually do if three is in but three in\nmy set is still works on just a regular\nlist\nand we'll go ahead and do a little\niteration we're going to do kind of the\ndice one remember\none two three four five six and so we're\ngoing to bring in the iteration tool and\nimport product as product\nand i'll show you what that means in\njust a second so we have our two dice we\nhave dice a\nand it's going to be a set of values\nyou can only have one value for each one\nthat's why they put it in a set and if\nyou remember from range it is up to\nseven so this is going to be one two\nthree four five six it will not include\nthe seven and the same thing for our\ndice b\nand then we're gonna do is we're gonna\ncreate a list\nwhich is the product\nof a and b so what's uh a plus b\nand if we go ahead and run this it'll\nprint that out and you'll see\nin this case when they say product\nbecause it's an iteration\ntool we're talking about creating a\ntuple of the two so we've now created a\ntuple of all possible outcomes of the\ndice where dice a is\none two three one to six and dice b is\none to six and you can see one to one\none to two one to 3 and so forth\nyou remember we had a slide on this\nearlier where we talked about\nthe different all the different outcomes\nof a dice\nwe can play around with this a little\nbit we can do n dice equals two\ndice faces one two three four five six\nuh another way of doing we did before\nand then we can create an event space\nwhere we have a set which is the product\nof the dice faces\nrepeat equals indice and we'll go and\njust run this\nand you can see here it just again puts\nit through all the different possible\nvariables we can have\nand then if we wanted to take the same\nset on here and print them all out like\nwe had before\nwe can just go through for outcome and\nevent space outcome and equals\nso the event space is creating\na sequence and as you can see here when\nwe print it out it stacks some versus\ngoing through and putting them in a nice\nline\nand we'll go ahead and do something\nlet's go print\nsince we have the end printing with a\ncomma that just means it's just gonna\nit's not gonna hit the return going down\nto the next line\nand we'll go ahead and do the length\nof our event space that'll be an\nimportant variable we're going to want\nto know in a minute\nand of course if i get carried away with\nmy typing of length we'll print it twice\nand it'll give me an error\nso we have 36 different possible\nvariations here\nand we might want to calculate something\nlike\nwhat about the multiple of 3 what if we\nwant to have\nthe probability of the multiple of 3 in\nour setup\nand so we can put together the code for\nthe outcome and event space of x y\nequals outcome\nif x plus y\nremainder 3 so we're going to divide by\n3 and look at the remainder and it\nequals 0\nthen it's a favorable outcome we're\ngoing to pop that outcome on the in\nthere\nand we'll turn it into a set so the\nfavor outcome equals a set\nnot necessary uh because we know it's\nnot going to be repeating itself but\njust in case we'll go ahead and do that\nand if we want to print out the outcome\nwe can go ahead and see what that looks\nlike and you can see here these are all\nmultiples of three one plus two is three\nfive plus four is nine which divided by\nthree is three and so forth\njust like we looked up the length of the\none before let's go ahead and print\nthe length\nof our\nf outcome\nso we can see what that looks like\nthere we go\nand of course i did forget to add the\nprint in the middle because we're\nlooping through and putting an end on\nthe on the setup on there so we're going\nto put the print in there and if i run\nthis you can see\nwe end up with 12. so we have 36 total\noptions\nwe have 12 that are multiple that add up\nto a multiple of\n3. and we can easily conv compute the\nprobability of this\nby simply taking the length\nof our favorable outcome over the length\nof the event space\nand if we print it out let me put that\nin there\nprobability\nlast line so we just type it in we end\nup with a 0.3333 chance\nthat's roughly a third\nand we want to make this look nice so\nlet's go ahead and put in another line\nthere the probability of getting the sum\nwhich is a multiple of three is\npoint three three three three\nwe can compute the same thing for five\ndice\nand if we do this for five dice and go\nahead and run it yeah you can see we\njust have a huge amount of choices\nso it just goes on and on down here and\nwe can look at\nthe length of the event space\nand we have over 7\n76\nchoices that's a lot of choices\nand if we want to ask the question like\nwe did above uh\nwhat is the sum where the sum is a\nmultiple of five but not a multiple of\nthree\nwe can go through all of these different\noptions and then\nyou can see here\nd1 d2 d3 d4 d5 equals the outcome\nand if you add these all together and\nthe\ndivision by 5 does not have a remainder\nof 0\nbut the remainder is also of a division\nby three is not equal to zero\nso the multiple of five is equal to zero\nbut the multiple three is not we can\njust append that on here and then we can\nlook at that uh favorable outcome\nwe'll go ahead and set that and we'll\njust take a look at this what's our\nlength\nof our favorable outcome\nit's always good to see what we're\nworking with and so we have 904 out of\n776\nand then of course we can just do a\nsimple division to get the probability\non here what's the probability that\nwe're going to roll\na multiple of 5 when you add them\ntogether\nbut not a multiple of three\nand so we're just going to divide those\ntwo numbers and you can see here we get\n0.11625 or 11.62 percent\nand so you can really have a nice visual\nthat this is not really complicated math\nright here on probabilities\nit's just how many options do you have\nand how many of those are you possibly\ngoing to be able to come up with with\nthe solution you're looking for\nand this leads us to a confusion matrix\na confusion matrix is a table which is\nused to describe the performance of a\nclassification model on a set of test\ndata for which the true values are known\nand so you'll see in the left we have\nthe predicted and the actual\nand we have a negative uh false negative\npositive true positive\nand then we have false positive and true\nnegative\nand you can think of this as your\npredicted model what does that mean that\nmeans if you divided your data and you\nused two-third of this to create the\nmodel\nyou might then test it against an actual\ncase for the last third to see how well\nit comes out how many times was it\ntrue positive versus a\nfalse positive it gave a false positive\nresponse\nand you can imagine in medical\nsituations this is a pretty big deal you\ndon't want to give a false positive so\nyou might adjust your model accordingly\nso you don't have a false positive say\nwith a covet virus test it'd be better\nto have a false negative and they go\nback and get retested than to have 30\npercent false positives where then the\ntest is pretty much invalid\nso in a use case like cancer prediction\nlet's consider an example where a cancer\nprediction model is put to the test for\nits accuracy and precision\nactual result of a person's medical\nreport is compared with the prediction\nmade by the machine learning model\nand so you can see here here's our\nactual predicted whether they have\ncancer or not you know cancer a big one\nyou don't want to have a\nfalse positive i mean a false negative\nin other words you don't want to have it\ntell you that you don't have cancer when\nyou do so that would be something you'd\nreally be looking for in this particular\ndomain you don't want a false negative\nand this is again you know you've\ncreated a model you have hundreds of\npeople or thousands of pieces of data\nthat come in\nthere's a real famous case study where\nthey have the imagery and all the\nmeasurements they take and there's about\n36 different measurements they take\nand then if you run the\na basic model you want to know just how\naccurate it is how many negative results\ndo you have that are either telling\npeople they have cancer that don't or\ntelling people that don't have cancer\nthat they do and then we can take these\nnumbers and we can feed them into our\naccuracy our precision and our recall\nso accuracy precision and recall\naccuracy metric to measure how\naccurately the results are predicted\nand this is your\ntotal\ntrue where you got the right results you\nadd them together the true positive the\ntrue negative\nover all the results so what percentage\nof them were accurate versus what were\nwrong\nwe talked about precision is a metric to\nmeasure how many of the correctly\npredicted cases are actually turned out\nto be positive uh so we have a precision\non\ntrue positive\nagain if you're talking about like covid\ntesting with the viruses\nyou really want this to be a high number\nyou want this true\nthat to be the center point where you\nmight have the opposite if you're\ndealing with a cancer where you want no\nfalse negatives\nso this is your metric on here precision\nis your test positive\ntrue positive plus\nfalse positive\nand then your recall how many of the\nactual positive cases we were able to\npredict quickly with our model\nuh so test positive is the test positive\nplus the\nfalse negative on there and we'll want\nto go ahead and do a demo on the naive\nbayes classifier before i get too far\ninto uh naive bayes classifier because\nwe're going to pull it from the sk learn\nor the scikit um let's go ahead kind of\nan interesting page here for classifiers\nwhen you go into the sk learn kit\nthere's a lot of ways to do\nclassification and i'll just zoom up in\nhere so you can see some of the titles\nthere's everything from the nearest\nneighbor linear\nbut we're going to be focusing on the\nnaive bayes over here\nand this is just a sample data set that\nthey put together and you can see how\nsome of these have a very different\noutput the naive bayes remember is set\nup as probably the most simplified\ncalculator or set of predictions out\nthere\nand so what we've been talking about\nwith the true false and stuff like that\nwhere there's\na then a belief that there is a\nindependent assumption between the\nfeatures where the features are very\nassumed to have some kind of connection\nuh then we can go ahead and use that for\nthe prediction and so that's what we're\nusing is a naive bayes classifier versus\nmany of the other classifiers that are\nout there\nfor this we're going to use the social\nnetwork ads it's a little data set on\nhere\nand let me go and just open that up the\nfile\nhere we go it has user id gender age\nestimated salary uh purchased\nand so we have you can see the user id\nmail 19\nestimated salary 19 000 and purchased\nzero so it's either going to make a\npurchase or not\nso look at that last one zero one we\nshould be thinking of binomials we\nshould be thinking of a simple naive\nbayes classifier kind of set up\nso if we close this out we're going to\ngo ahead and import our numpy as np\nwe're going to nice to have a good\nvisual of our data so we'll put in our\nmatplot library\nhere's our pandas our data frame\nuh and then we're going to go ahead and\nimport the data set and the data set's\ngoing to be we're going to read it from\nthe social network ads.csv then we're\ngoing to print the head just so you can\nsee it again\neven though i showed you it in the file\nand x equals the data set i location\nuh two three values and y is going to be\nthe four uh column four let me just run\nthis it's a little easier to go over\nthat\nyou can see right here we're going to be\nlooking at\n0 1 2 as age\nand estimated salary so 2 3\nand that's what i location just means\nthat we're\nlooking at the number versus a regular\nlocation a regular location you'd\nactually say age and estimated salary\nand then column four is did they make a\npurchase they purchased something\nso those are the three columns we're\ngoing to be looking at when we do this\nand we've gone ahead and imported these\nand\nimported the data so now our data set is\nall set with this information in it\nand we'll need to go ahead and split the\ndata up so we need our from the sk learn\nmodel selection we can import train test\nsplit\nthis does a nice job we can set the\nrandom state so randomly picks the data\nand we're just going to take uh 25 of\nit's going to go into the test our x\ntest and our y test\nand the 75 will go to x train and y\ntrain\nthat way once we\ncreate our model we can then have data\nto see just how accurate or how well it\nhas performed with our prediction\nthe next step in pre-processing our data\nis to go ahead and do feature scaling\nnow a lot of this is start to look\nfamiliar if you've done a number of the\nother modules and setup you should start\nnoticing that we\nbring in our data we take a look at what\nwe're working with\nwe go ahead and split it up into\ntraining and testing\nin this case we're going to go ahead and\nscale it scale it means we're putting it\nbetween a value of minus 1 and 1\nor some place in the middle ground there\nthis way if you have any huge set you\ndon't have this huge\nsetup if we go back up to here where\nsalary the salary is\n20 000 versus age 35.\nwell there's a good chance with a lot of\nthe back end math that 20 000 will skew\nthe results and the estimated salary\nwill have a higher impact than the age\ninstead of balancing them out and\nletting the calculations weigh them\nproperly\nand finally we get to actually create\nour naive bayes model\nand then we're going to go ahead and\nimport the gaussian naive bayes\nand the gaussian is is the most basic\none that's what we're looking at now it\nturns out though if you go to the sk\nlearn\nkit\nthey have a number of different ones you\ncan pull in there there's a\nbernoulli i know i've never used that\none categorical\num complement and here's our gaussian\nso there's a number of different options\nyou can look at\ngaussian when you come to the naive\nbayes is the most commonly used\nso we're talking about the naive bayes\nthat's usually what people are talking\nabout when they when they're pulling\nthis in\nand one of the nice things about the\ngaussian if you go to their website\nto sk learn the naive bayes gaussian\nthere's a lot of cool features one of\nthem is you can do partial fit on here\nthat means if you have a huge amount of\ndata you don't have to process it all i\nwant you once you can batch it into the\ngaussian nb model and there's many other\ndifferent things you can do with it as\nfar as fitting the data and how you\nmanipulate it we're just doing the\nbasics so we're going to go ahead and\ncreate our classifier we're going to\nequal the gaussian in b\nand then we're going to do a fit we're\ngoing to fit our training data and our\ntraining solution so x train y train\nand we'll go ahead and run this uh it's\ngoing to tell us that it ran the code\nright there\nand now we have our trained classifier\nmodel\nso the next step is we need to go ahead\nand run a prediction we're going to do\nour y predict equals the\nclassifier.predict\nx test so here we fit the data and now\nwe're going to go ahead and predict\nand now we get to our confusion matrix\nso from\nthe sk learn matrix metrics you can\nimport your confusion matrix\njust as saves you from doing all the\nsimple math that does it all for you\nand then we'll go ahead and create our\nconfusion metrics with the y test and\nthe y predict so we have our actual\nand we have our predicted value\nand you can see from here this is the\nchart we looked at here's predicted so\ntrue positive false positive\nfalse negative true negative\nand if we go ahead and run this there we\nhave it 65 3 7 25\nand in this particular prediction we had\n65 or predicted the truth as far as a\npurchase they're gonna make a purchase\nand we guessed three wrong\nand then we had 25 we predicted would\nnot purchase and seven of them did so\nthere's our our confusion matrix\nat this point if you were with your\nshareholders or a board meeting\nyou would start to hear some snoozing if\nthey were looking at the numbers and you\nsay hey here's my confusion\nmatrix\nso let's go ahead and visualize the\nresults\nwe're going to pull from the matplot\nlibrary colors import listed color map\nand this is actually my machine is going\nto throw an error because this is being\nbecause of the way the setup is i have a\nnewer version on here than when they put\ntogether the demo\nand we need our x set and our y set\nwhich is our x train and y train\nand then we'll create our x1 x2\nand we'll put that into a grid\nuh and we set our x set minimum stop and\nour x at max stop\nand if you come all the way over here\nwe're going to step .01 this is going to\ngive us a nice line\nis what that's doing and we're going to\nplot the contour\nplot the x limit plot the y limit\nand put the scatter plot in there let's\ngo ahead and run this uh to be honest\nwhen i'm doing these graphs there's so\nmany different ways to do that there's\nso many different ways to put this code\ntogether\nto show you what we're doing it's a lot\neasier to pull up the graph and then go\nback up and explain it\nso the first thing we want to note here\nwhen we're looking at the data\nis this is the training set\nand so we have those who didn't make a\npurchase we've drawn a nice area for\nthat\nthat's defined by the naive bayes setup\nand then we have those who did make a\npurchase the green and you can see that\nsome of the green drops fall into the\nred area and some of the red dots fall\ninto the green\nso even our training set isn't going to\nbe a hundred percent\nwe couldn't do that\nand so we're looking at our different\ndata coming down\nwe can kind of arrange our x1 x2 so we\nhave a nice plot going on and then we're\ngoing to create the\ncontour\nthat's that nice line that's drawn down\nthe middle on here with the red green\nthat's where that's what this is doing\nright here with the reshape and notice\nthat we had to\ndo the dot t if you remember from numpy\nif you did the numpy module\nyou end up with pairs you know x\nx1 x2 x1 x2 next row and so forth you\nhave to flip it so it's all one row you\nhave all your x1's and all your x2s\nso this is what we're kind of looking\nfor right here on this setup\nuh and then the scatter plot is of\ncourse um your scattered data across\nthere we're just going through all the\npoints that puts these nice little dots\non to our setup on here and we have our\nestimated salary and our h and then of\ncourse the dots are did they make a\npurchase or not\nand just a quick note this is kind of\nfunny you can see up here where it says\nx set y set equals x train y train\nwhich seems kind of a little weird to do\nthis is because this is probably\noriginally a definition\nso it was its own module that could be\ncalled over and over again\nand which is really a good way to do it\nbecause the next thing we're going to\nwant to do is do the exact same thing\nbut we're going to visualize the test\nset results\nthat way we can see what happened with\nour test group our 25 percent\nand you can see down here we have the\ntest set uh and it\nif you look at the two\ngraphs next to each other this one\nobviously has\n75 percent of the data so it's going to\nshow a lot more\nthis is only 25 of the data you can see\nthat there's a number that are kind of\non the edge as to whether they could\nguess by age and income they're going to\nmake a purchase or not\nbut that said it still is pretty clear\nit's pretty good as far as how much the\nestimate is and how good it does\nnow\ngraphs are really effective\nfor showing people what's going on but\nyou also need to have the numbers and so\nwe're going to do from sklearn we're\ngoing to import metrics\nand then we're going to print our\nmetrics classification port from the y\ntest and the y predict\nand you can see here we have precision\nprecision of 0s is 90 there's our recall\n0.96 we have an f1 score and a support\nand we have our precision the recall on\ngetting it right\nand then we can do our accuracy the\nmacro average and the weighted average\nso you can see it it pulls in\npretty good as far as how accurate it is\nyou could say it's going to be about 90\npercent is going to guess correctly\nthat that they're not going to purchase\nand we had an 89\nchance that they are going to purchase\nand then the other numbers as you get\ndown\nhave a little bit different meaning but\nit's pretty straightforward on here\nhere's our accuracy and here's our micro\naverage and the weighted average and\neverything else you might need and if\nyou forgot the exact definition of\naccuracy it is the\ntrue positive true negative over all of\nthe different setups\nprecision is your true positive over all\npositives true and false and recall is a\ntrue positive over true positive plus\nfalse negative\nand we can just real quick flip back\nthere\nso you can see those numbers on here\nhere's our precision here's our recall\nand here's our accuracy on this\nmultiple linear regression let's take a\nbrief look at what happens when you have\nmultiple inputs so in multiple linear\nregression we have well we'll start with\nthe simple linear regression where we\nhad y equals m plus x plus c and we're\ntrying to find the value of y now with\nmultiple linear regression we have\nmultiple variables coming in so instead\nof having just x we have x1 x2 x3 and\ninstead of having just one slope each\nvariable has its own slope attached to\nit as you can see here we have m1 m2 m3\nand we still just have the single\ncoefficient so when you're dealing with\nmultiple linear regression you basically\ntake your single linear regression and\nyou spread it out so you have y equals\nm1 times x1 plus m2 times x2\nso on all the way to m to the nth x to\nthe nth and then you add your\ncoefficient on there implementation of\nlinear regression now we get into my\nfavorite part let's understand how\nmultiple linear regression works by\nimplementing it in python if you\nremember before we were looking at a\ncompany and just based on its rnd trying\nto figure out his profit we're going to\nstart looking at the expenditure of the\ncompany we're going to go back to that\nwe're going to predict his profit but\ninstead of predicting it just on the r d\nwe're going to look at other factors\nlike administration costs marketing\ncosts and so on and from there we're\ngoing to see if we can figure out what\nthe profit of that company is going to\nbe to start our coding we're going to\nbegin by importing some basic libraries\nand we're going to be looking through\nthe data before we do any kind of linear\nregression we're going to take a look at\nthe data to see what we're playing with\nthen we'll go ahead and format the data\nto the format we need to be able to run\nit in the linear regression model and\nthen from there we'll go ahead and solve\nit and just see how valid our solution\nis so let's start with importing the\nbasic libraries now i'm going to be\ndoing this in anaconda jupiter notebook\na very popular ide i enjoy it's such a\nvisual to look at and so easy to use\njust any id for python will work just\nfine for this so break out your favorite\npython ide so here we are in our jupiter\nnotebook let me go ahead and paste our\nfirst piece of code in there and let's\nwalk through what libraries we're\nimporting first we're going to import\nnumpy as np and then i want you to skip\none line and look at import pandas as pd\nthese are very common tools that you\nneed with most of your linear regression\nthe numpy which stands for number python\nis usually denoted as np and you have to\nalmost have that for your sk learn\ntoolbox you always import that right off\nthe beginning pandas although you don't\nhave to have it for your sklearn\nlibraries it does such a wonderful job\nof importing data setting it up into a\ndata frame so we can manipulate it\nrather easily and it has a lot of tools\nalso in addition to that so we usually\nlike to use the pandas when we can and\ni'll show you what that looks like the\nother three lines are for us to get a\nvisual of this data and take a look at\nit so we're going to import\nmatplotlibrary.pipelot as plt and then\nseaborn as sns seaborn works with the\nmatplot library so you have to always\nimport matplot library and then seaborn\nsits on top of it and we'll take a look\nat what that looks like you could use\nany of your own plotting libraries you\nwant there's all kinds of ways to look\nat the data these are just very common\nones and the seaborne is so easy to use\nit just looks beautiful it's a nice\nrepresentation that you can actually\ntake and show somebody and the final\nline is the amber scion map plot library\ninline that is only because i'm doing an\ninline ide my interface in the anaconda\njupiter notebook requires i put that in\nthere or you're not going to see the\ngraph when it comes up let's go ahead\nand run this it's not going to be that\ninteresting so we're just setting up\nvariables in fact it's not going to do\nanything that we can see but it is\nimporting these different libraries and\nsetup the next step is load the data set\nand extract independent and dependent\nvariables now here in the slide you'll\nsee companies equals pd.read csv and it\nhas a long line there with the file at\nthe end 1000 companies dot csv you're\ngoing to have to change this to fit\nwhatever setup you have and the file\nitself you can request just go down to\nthe commentary below this video and put\na note in there and simply learn we'll\ntry to get in contact with you and\nsupply you with that file so you can try\nthis coding yourself so we're going to\nadd this code in here and we're going to\nsee that i have companies equals\npd.reader underscore csv and i've\nchanged this path to match my computer c\ncolon slash simply learn slash 1000\nunderscore companies.csv and then below\nthere we're going to set the x equals to\ncompanies under the i location and\nbecause this is companies as a pd data\nset i can use this nice notation that\nsays take every row that's what the\ncolon the first colon is comma except\nfor the last column that's what the\nsecond part is or we have a colon minus\none and we want the values set into\nthere so x is no longer a data set a\npandas data set but we can easily\nextract the data from our pandas data\nset with this notation and then y we're\ngoing to set equal to the last row well\nthe question is going to be what are we\nactually looking at so let's go ahead\nand take a look at that and we're going\nto look at the companies dot head which\nlists the first five rows of data and\ni'll open up the file in just a second\nso you can see where that's coming from\nbut let's look at the data in here as\nfar as the way the pandas sees it when i\nhit run you'll see it breaks it out into\na nice setup this is what panda is one\nof the things pandas is really good\nabout is it looks just like an excel\nspreadsheet you have your rows and\nremember when we're programming we\nalways start with zero we don't start\nwith one so it shows the first five rows\nzero one two three four and then it\nshows your different columns r and d\nspend administration marketing spend\nstate profit it even notes that the top\nare column names it was never told that\nbut pandas is able to recognize a lot of\nthings that they're not the same as the\ndata rows why don't we go ahead and open\nthis file up in a csv so you can\nactually see the raw data so here i've\nopened it up as a text editor and you\ncan see at the top we have rnd spend\ncomma administration comma marketing\nspin comma state comma profit carriage\nreturn i don't know about you but i go\ncrazy trying to read files like this\nthat's why we use the pandas you could\nalso open this up in an excel and it\nwould separate it since it is a comma\nseparated variable file but we don't\nwant to look at this one we want to look\nat something we can read rather easily\nso let's flip back and take a look at\nthat top part the first five row now as\nnice as this format is where i can see\nthe data to me it doesn't mean a whole\nlot maybe you're an expert in business\nand investments and you understand what\nuh 165\n349 dollars and 20 cents compared to the\nadministration cost of a hundred and\nthirty six thousand eight hundred ninety\nseven dollars and eighty cents so on so\non helps to create the profit of 192 261\nand 83 cents that makes no sense to me\nwhatsoever no pun intended so let's flip\nback here and take a look at our next\nset of code where we're going to graph\nit so we can get a better understanding\nof our data and what it means so at this\npoint we're going to use a single line\nof code to get a lot of information so\nwe can see where we're going with this\nlet's go ahead and paste that into our\nnotebook and see what we got going and\nso we have the visualization and again\nwe're using sns which is pandas as you\ncan see we imported the map plot library\ndot pi plot as plt which then the\nseaborn uses and we imported the seaborn\nas sns and then that final line of code\nhelps us show this in our inline coding\nwithout this it wouldn't display and you\ncould display it to a file and other\nmeans and that's the matplot library in\nline with the amber sign at the\nbeginning so here we come down to the\nsingle line of code seaborn is great\nbecause it actually recognizes the panda\ndata frame so i can just take the\ncompanies dot core for coordinates and i\ncan put that right into the seaborn and\nwhen we run this we get this beautiful\nplot and let's just take a look at what\nthis plot means if you look at this plot\non mine the colors are probably a little\nbit more purplish and blue than the\noriginal one we have the columns and the\nrows we have r and d spending we have\nadministration we have marketing\nspending and profit and if you cross\nindex any two of these since we're\ninterested in profit if you cross index\nprofit with profit it's going to show up\nif you look at the scale on the right\nway up in the dark why because those are\nthe same data they have an exact\ncorrespondence so r d spending is going\nto be the same as r d spending and the\nsame thing with administration costs\nright down the middle you get this dark\nrow or dark um diagonal row that shows\nthat this is the highest corresponding\ndata that's exactly the same and as it\nbecomes lighter there's less connections\nbetween the data so we can see with\nprofit obviously profit is the same as\nprofit and next it has a very high\ncorrelation with r d spending which we\nlooked at earlier and it has a slightly\nless connection to marketing spending\nand even less to how much money we put\ninto the administration so now that we\nhave a nice look at the data let's go\nahead and dig in and create some actual\nuseful linear regression model so that\nwe can predict values and have a better\nprofit now that we've taken a look at\nthe visualization of this data we're\ngoing to move on to the next step\ninstead of just having a pretty picture\nwe need to generate some hard data some\nhard values so let's see what that looks\nlike we're going to set up our linear\nregression model in two steps the first\none is we need to prepare some of our\ndata so it fits correctly and let's go\nahead and paste this code into our\njupiter notebook and what we're bringing\nin is we're going to bring in the sk\nlearn pre-processing where we're going\nto import the label encoder and the one\nhot encoder to use the label encoder\nwe're going to create a variable called\nlabel encoder and set it equal to\ncapital l label capital e encoder this\ncreates a class that we can reuse for\ntransferring the labels back and forth\nnow about now you should ask what labels\nare we talking about let's go take a\nlook at the data we processed before and\nsee what i'm talking about here if you\nremember when we did the companies dot\nhead and we printed the top five rows of\ndata we have our columns going across we\nhave column zero which is r and d\nspending column one which is\nadministration column two which is\nmarketing spending and column three is\nstate\nand you'll see under state we have new\nyork california florida now to do a\nlinear regression model it doesn't know\nhow to process new york it knows how to\nprocess a number so the first thing\nwe're going to do is we're going to\nchange that new york california and\nflorida and we're going to change those\nto numbers that's what this line of code\ndoes here x equals and then it has the\ncolon comma 3 in brackets the first part\nthe colon comma means that we're going\nto look at all the different rows so\nwe're going to keep them all together\nbut the only row we're going to edit is\nthe third row and in there we're going\nto take the label coder and we're going\nto fit and transform the x also the\nthird row so we're going to take that\nthird row we're going to set it equal to\na transformation and that transformation\nbasically tells it that instead of\nhaving a uh new york it has a zero or a\none or a two and then finally we need to\ndo a one hot encoder which equals one\nhot inc or categorical features equals\nthree and then we take the x and we go\nahead and do that equal to one hot\nencoder fit transform x to array this\nfinal transformation preps our data for\nus so it's completely set the way we\nneed it is just a row of numbers even\nthough it's not in here let's go ahead\nand print x and just take a look what\nthis data is doing you'll see you have\nan array of arrays and then each array\nis a row of numbers and if i go ahead\nand just do row 0 you'll see i have a\nnice organized row of numbers that the\ncomputer now understands we'll go ahead\nand take this out there because it\ndoesn't mean a whole lot to us it's just\na row of numbers\nnext on setting up our data we have\navoiding dummy variable trap this is\nvery important why because the computer\nhas automatically transformed our header\ninto the setup and it's automatically\ntransferring all these different\nvariables so when we did the encoder the\nencoder created two columns and what we\nneed to do is just have the one because\nit has both the variable and the name\nthat's what this piece of code does here\nlet's go ahead and paste this in here\nand we have x equals x colon comma one\ncolon all this is doing is removing that\none extra column we put in there when we\ndid our one hot encoder and our label\nencoding let's go ahead and run that and\nnow we get to create our linear\nregression model and let's see what that\nlooks like here and we're going to do\nthat in two steps the first step is\ngoing to be in splitting the data\nnow whenever we create a predictive\nmodel of data we always want to split it\nup so we have a training set and we have\na testing set that's very important\notherwise we'd be very unethical without\ntesting it to see how good our fit is\nand then we'll go ahead and create our\nmultiple linear regression model and\ntrain it and set it up let's go ahead\nand paste this next piece of code in\nhere and i'll go ahead and shrink it\ndown a size or two so it all fits on one\nline so from the sklearn module\nselection we're going to import train\ntest split and you'll see that we've\ncreated four completely different\nvariables we have capital x train\ncapital x test smaller case y train\nsmaller case y test that is the standard\nway that they usually reference these\nwhen we're doing different models you\nusually see that a capital x and you see\nthe train and the test and the lower\ncase y what this is is x is our data\ngoing in that's our rnd spin our\nadministration our marketing and then\nwhy which we're training is the answer\nthat's the profit because we want to\nknow the profit of an unknown entity\nthat's what we're going to shoot for in\nthis tutorial the next part train test\nsplit we take x and we take y we've\nalready created those x has the columns\nwith the data in it and y has a column\nwith profit in it and then we're going\nto set the test size equals 0.2 that\nbasically means 20 percent\nso twenty percent of the rows are going\nto be tested we're going to put them off\nto the side so since we're using a\nthousand lines of data that means that\n200 of those lines we're going to hold\noff to the side to test for later and\nthen the random state equals zero we're\ngoing to randomize which ones it picks\nto hold off to the side we'll go ahead\nand run this it's not overly exciting so\nsetting up our variables but the next\nstep is the next step we actually create\nour linear regression model now that we\ngot to the linear regression model we\nget that next piece of the puzzle let's\ngo ahead and put that code in there and\nwalk through it so here we go we're\ngoing to paste it in there and let's go\nahead and since this is a shorter line\nof code let's zoom up there so we can\nget a good look and we have from the\nsklearn dot linear underscore model\nwe're going to import linear regression\nnow i don't know if you recall from\nearlier when we were doing all the math\nlet's go ahead and flip back there and\ntake a look at that do you remember this\nwhere we had this long formula on the\nbottom and we were doing all this\nsummarization and then we also looked at\nuh setting it up with the different\nlines and then we also looked all the\nway down to multiple linear regression\nwhere we're adding all those formulas\ntogether all of that is wrapped up in\nthis one section so what's going on here\nis i'm going to create a variable called\nregressor and the regressor equals the\nlinear regression that's a linear\nregression model that has all that math\nbuilt in so we don't have to have it all\nmemorized or have to compute it\nindividually and then we do the\nregressor dot fit in this case we do x\ntrain and y train because we're using\nthe training data x being the data in\nand y being profit what we're looking at\nand this does all that math for us so\nwithin one click and one line we've\ncreated the whole linear regression\nmodel and we fit the data to the linear\nregression model and you can see that\nwhen i run the regressor it gives an\noutput linear regression it says copy x\nequals true fit intercept equals true in\njobs equal one normalize equals false\nit's just giving you some general\ninformation on what's going on with that\nregressor model now that we've created\nour linear regression model let's go\nahead and use it and if you remember we\nkept a bunch of data aside so we're\ngoing to do a y predict variable and\nwe're going to put in the x test and\nlet's see what that looks like scroll up\na little bit paste that in here\npredicting the test set results so here\nwe have y predict equals regressor dot\npredict x test going in and this gives\nus y predict now because i'm in jupiter\ninline i can just put the variable up\nthere and when i hit the run button\nit'll print that array out i could have\njust as easily done print y predict so\nif you're in a different ide that's not\nan inline setup like the jupiter\nnotebook you can do it this way print y\npredict and you'll see that for the 200\ndifferent test variables we kept off to\nthe side it's going to produce 200\nanswers this is what it says the profit\nare for those 200 predictions but let's\ndon't stop there let's keep going and\ntake a couple look we're going to take\njust a short detail here and calculating\nthe coefficients in the intercepts this\ngives us a quick flash at what's going\non behind the line we're going to take a\nshort detour here and we're going to be\ncalculating the coefficient and\nintercepts so you can see what those\nlook like what's really nice about our\nregressor we created is it already has a\ncoefficients for us we can simply just\nprint regressor dot coefficient\nunderscore when i run this you'll see\nour coefficients here and if we can do\nthe regressor coefficient we can also do\nthe regressor intercept and let's run\nthat and take a look at that this all\ncame from the multiple regression model\nand we'll flip over so you can remember\nwhere this is going into where it's\ncoming from you can see the formula down\nhere where y equals m1 times x1 plus m2\ntimes x2 and so on and so on plus c the\ncoefficient so these variables fit right\ninto this formula y equals slope 1 times\ncolumn 1 variable plus slope 2 times\ncolumn 2 variable all the way to the m\nand to the n and x to the n plus c the\ncoefficient or in this case you have\nminus 8.89 to the power of 2 etc etc\ntimes the first column and the second\ncolumn and the third column and then our\nintercept is the minus 1 0 3 0 0 9 point\nboy it gets kind of complicated when you\nlook at it this is why we don't do this\nby hand anymore this is why we have the\ncomputer to make these calculations easy\nto understand and calculate now i told\nyou that was a short detour and we're\ncoming towards the end of our script as\nyou remember from the beginning i said\nif we're going to divide this\ninformation we have to make sure it's a\nvalid model that this model works and\nunderstand how good it works so\ncalculating the r squared value that's\nwhat we're going to use to predict how\ngood our prediction is and let's take a\nlook what that looks like in code and so\nwe're going to use this from\nsklearn.metrics we're going to import r2\nscore that's the r squared value we're\nlooking at the error so in the r2 score\nwe take our y test versus our y predict\ny test is the actual values we're\ntesting that was the one that was given\nto us that we know are true the y\npredict of those 200 values is what we\nthink it was true and when we go ahead\nand run this we see we get a 0.9352\nthat's the r2 score now it's not exactly\na straight percentage so it's not saying\nit's 93 correct but you do want that in\nthe upper 90s o and higher shows that\nthis is a very valid prediction based on\nthe r2 score and if r-squared value of\n0.91 or 9-2 as we got on our model\nremember it does have a random\ngeneration involved this proves the\nmodel is a good model which means\nsuccess yay we successfully trained our\nmodel with certain predictors and\nestimated the profit of the companies\nusing linear regression let's take an\nexample and see how we can apply\nlogistic regression to predict the\nnumber that is shown in the image so\nthis is actually a live demo i will take\nyou into jupiter notebook and\nshow the code but before that let me\ntake you through a couple of slides to\nexplain what we are trying to do so\nlet's say you have an eight by eight\nimage and there the image has a number\none two three four and you need to train\nyour model to predict what this number\nis so\nhow do we do this so the first thing is\nobviously in any machine learning\nprocess you train your model so in this\ncase we are using logistic regression so\nand then we provide a training set to\ntrain the model and then we test how\naccurate our model is with the test data\nwhich means that like any machine\nlearning process we split our initial\ndata into two parts training set and\ntest set with the training set we train\nour model and then with the test set we\ntest the model then we get good accuracy\nand then we use it for for inference\nright so that is typical methodology of\ntraining testing and then deploying of\nmachine learning models so let's take a\nlook at the code and see what we are\ndoing so i will not go line by line but\njust take you through some of the blocks\nso first thing we do is import all the\nlibraries and then we basically take a\nlook at the images and see what is the\ntotal number of images we can display\nusing matplotlib some of the images are\na sample of these images and\nthen we split the data into training and\ntest as i mentioned earlier\nand we can do some exploratory analysis\nand\nthen we build our model we train our\nmodel with the training set and then we\ntest it with our test set and find out\nhow accurate our model is using the\nconfusion matrix the heat map and use\nheat map for visualizing this and i will\nshow you in the code what exactly is the\nconfusion matrix and how it can be used\nfor finding the accuracy in our example\nwe got we get an accuracy of about 0.94\nwhich is pretty good or 94 which is\npretty good all right so what is the\nconfusion matrix this is an example of a\nconfusion matrix and\nthis is used for identifying the\naccuracy of a\nclassification model or like a logistic\nregression model so the most important\npart in a confusion matrix is that first\nof all this as you can see this is a\nmatrix and the size of the matrix\ndepends on how many outputs we are\nexpecting right\nso\nthe most important part here is that the\nmodel will be most accurate when we have\nthe maximum numbers in its diagonal like\nin this case that's why it has almost 93\n94 percent because the diagonals should\nhave the maximum numbers and the others\nother than diagnose the cells other than\nthe diagonals should have very few\nnumbers so here that's what is happening\nso there is a two here there are there's\na one here but most of them are along\nthe diagonal this what does this mean\nthis means that the number that has been\nfed is zero and the number that has been\ndetected is also zero so the predicted\nvalue and the actual value are the same\nso along the diagonals that is true\nwhich means that let's let's take this\ndiagonal right if the maximum number is\nhere that means that like here in this\ncase it is 34 which means that 34 of the\nimages that have been fed or rather\nactually there are two\nmisclassifications in there so 36 images\nhave been fed which have number four and\nout of which 34 have been predicted\ncorrectly as number four and one has\nbeen predicted as number eight and\nanother one has been predicted as number\nnine so these are two misclassifications\nokay so that is the meaning of saying\nthat the maximum number should be in the\ndiagonal so if you have all of them so\nfor an ideal model which has let's say\nhundred percent accuracy everything will\nbe only in the diagonal there will be no\nnumbers other than zero in all other\ncells so that is like a hundred percent\naccurate model okay so that's uh just of\nhow to use this matrix uh how to use\nthis confusion matrix i know the name is\na little funny sounding confusion matrix\nbut actually it is not very confusing\nit's very straightforward so you are\njust plotting what has been predicted\nand what is the labeled information or\nwhat is the actual data that's also\nknown as the ground truth sometimes okay\nthese are some fancy terms that are used\nso predicted label and the actual name\nthat's all it is okay yeah so we are\nshowing a little bit more information\nhere so 38 have been predicted and here\nyou will see that all of them have been\npredicted correctly there have been 38\nzeros and the predicted value and the\nactual value is exactly the same whereas\nin this case\nright it has there are i think 37 plus\nfive yeah 42 have been fed the images 42\nimages are of digit 3 and\nthe accuracy is only 37 of them have\nbeen accurately predicted three of them\nhave been predicted as number seven and\ntwo of them have been predicted as\nnumber eight and so on and so forth okay\nall right so with that let's go into\njupiter notebook and see how the code\nlooks so this is the code in\nin jupiter notebook for logistic\nregression in this particular demo what\nwe are going to do is train our model to\nrecognize digits\nwhich are the images which have digits\nfrom let's say 0 to 5 or 0 to 9 and\nand then we will see how well it is\ntrained and whether it is able to\npredict these numbers correctly or not\nso let's get started so the first part\nis as usual we are importing some\nlibraries that are required and\nthen the last line in this block is to\nload the digits so let's go ahead and\nrun this code then\nhere we will visualize the shape of\nthese digits so we can see here if we\ntake a look this is how the shape is\n1797\nby 64. these are like 8x8 images so\nthat's that's what is reflected in this\nshape now from here onwards we are\nbasically once again importing some of\nthe libraries that are required like\nnumpy and matplot and we will take a\nlook at uh some of the sample images\nthat we have unloaded so this one for\nexample creates a figure and then we go\nahead and take a few sample images to\nsee how they look so let me run this\ncode and so that it becomes easy to\nunderstand so these are about five\nimages sample images that we are looking\nat zero one two three four so this is\nhow the image is this is how the data is\nokay and uh based on this we will\nactually train our logistic regression\nmodel and then we will test it and see\nhow well it is able to recognize so the\nway it works is the pixel information so\nas you can see here this is an 8 by 8\npixel kind of a image and\nthe each pixel whether it is activated\nor not activated that is the information\navailable for each pixel now based on\nthe pattern of this activation and\nnon-activation of the various pixels\nthis will be identified as a zero for\nexample right similarly as you can see\nso overall each of these numbers\nactually has a different pattern of the\npixel activation and that's pretty much\nthat our model needs to learn\nfor which a number what is the pattern\nof the activation of the pixels right so\nthat is what we are going to train our\nmodel okay so the first thing we need to\ndo is to split our data into training\nand test data set right so whenever we\nperform any training we split the data\ninto training and test so that the\ntraining data set is used to train the\nsystem so we pass this probably multiple\ntimes\nand then we test it with the test data\nset and the split is usually in the form\nof the and there are various ways in\nwhich you can split this data it is up\nto the individual preferences in our\ncase here we are splitting in the form\nof 23 and 77 so when we say test size as\n20 0.23 that means 23 percent of that\nentire data is used for testing and the\nremaining 77 percent is used for\ntraining so there is a readily available\nfunction which is uh called train test\nsplit so we don't have to write any\nspecial code for the splitting it will\nautomatically split the data based on\nthe proportion that we give here which\nis test size so we just give the test\nsize automatically training size will be\ndetermined and\nwe pass the data that we want to split\nand the the results will be stored in x\nunderscore train and y underscore train\nfor the training data set and what is x\nunderscore train these are these are the\nfeatures right which is like the\nindependent variable and why underscore\ntrain is the label right so\nin this case what happens is we have the\ninput value which is or the features\nvalue which is in x underscore train and\nsince this is the labeled data\nfor each of them each of the\nobservations we already have\nthe label information saying whether\nthis digit is a zero or a one or a two\nso that this this is what will be used\nfor comparison to find out whether the\nthe system is able to recognize it\ncorrectly or there is an error for each\nobservation it will compare with this\nright so this is the label so the same\nway x underscore train y underscore\ntrain is for the training data set x\nunderscore test y underscore test is for\nthe test data set okay so let me go\nahead and execute this code as well and\nthen we can go and check quickly what is\nthe how many entries are there and in\neach of these so x underscore train the\nshape is 13 83\nby 64. and y underscore train has 1383\nbecause there is nothing like the second\npart is not required here and then x\nunderscore test shape we see is 414 so\nactually there are 414 observations in\ntest and 1383 observations in train so\nthat's basically what these four lines\nof code are are saying okay then we\nimport the\nlogistic regression library and which is\na part of scikit-learn so\nwe we don't have to implement the\nlogistic regression process itself we\njust call these the function and let me\ngo ahead and execute that so that\nwe have the logistic regression library\nimported now we create an instance of\nlogistic regression right so logistic\nregr is a is an instance of logistic\nregression and then we use that for\ntraining our model so let me first\nexecute this code so these two lines so\nthe first line basically creates an\ninstance of logistic regression model\nand then the second line\nis where we are passing\nour data the training data set right\nthis is our the the predictors and uh\nthis is our target we are passing this\ndata set to train our model all right so\nonce we do this in this case the data is\nnot large but by and large the training\nis what takes usually a lot of time so\nwe spend in machine learning activities\nand machine learning projects we spend a\nlot of time for the training part of it\nokay so here the data set is relatively\nsmall so it was pretty quick so all\nright so now our model has been trained\nusing the training data set and\nwe want to see how accurate this is so\nwhat we'll do is we will test it out in\nprobably faces so let me first try out\nhow well this is working for\none image okay i will just try it out\nwith one image my the first entry in my\ntest data set and see whether it is\ncorrectly predicting or not so\nand in order to test it so for training\npurpose we use the fit method there is a\nmethod called fit which is for training\nthe model and once the training is done\nif you want to test for a particular\nvalue new input you use the predict\nmethod okay so let's run the predict\nmethod and we pass this particular image\nand\nwe see that the\nshape is or the prediction is 4. so\nlet's try a few more let me see for the\nnext 10\nseems to be fine so let me just go ahead\nand test the entire data set okay that's\nbasically what we will do so now we want\nto find out how accurately this has\nperformed so we use the score method to\nfind what is the percentages of accuracy\nand we see here that it has performed up\nto 94 percent accurate okay so that's on\nthis part now what we can also do is we\ncan um also see this accuracy\nusing what is known as a confusion\nmatrix so let us go ahead and try that\nas well\nso that we can also visualize how well\nthis model has uh done so let me execute\nthis piece of code which will basically\nimport some of the libraries that are\nrequired and\nwe we basically create a confusion\nmatrix and instance of confusion matrix\nby running confusion matrix and passing\nthese values so we have so this\nconfusion underscore matrix method takes\ntwo parameters one is the y underscore\ntest and the other is the prediction so\nwhat is the y underscore test these are\nthe labeled values which we already know\nfor the test data set and predictions\nare what the system has predicted for\nthe test data set okay so this is known\nto us\nand this is what the system has\nthe model has generated so we kind of\ncreate the confusion matrix and we will\nprint it and this is how the confusion\nmatrix looks as the name suggests it is\na matrix\nand\nthe key point out here is that the\naccuracy of the model is determined by\nhow many numbers are there in the\ndiagonal the more the numbers in the\ndiagonal the better the accuracy is\nokay and first of all the total sum of\nall the numbers in this whole matrix is\nequal to the number of observations in\nthe test data set that is the first\nthing right so if you add up all these\nnumbers that will be equal to the number\nof\nobservations in the test data set and\nthen out of that the maximum number of\nthem should be in the diagonal that\nmeans the accuracy is pretty good if the\nthe numbers in the diagonal are less and\nin all other places there are a lot of\nnumbers uh which means the accuracy is\nvery low the diagonal indicates a\ncorrect prediction that this means that\nthe actual value is same as the\npredicted value here again actual values\nsame as the predictor value and so on\nright so the moment you see a number\nhere that means the actual value is\nsomething and the predicted value is\nsomething else right similarly here the\nactual value is something and the\npredicted value is something else so\nthat is basically how we read the\nconfusion matrix now how do we find the\naccuracy you can actually add up the\ntotal values in the diagonal so it's\nlike 38 plus 44 plus 43 and so on and\ndivide that by the total number of test\nobservations that will give you the\npercentage accuracy using a confusion\nmatrix now let us visualize this\nconfusion matrix in a slightly more\nsophisticated way uh using a heat map so\nwe will create a heat map with some\nwe'll add some colors as well it's uh\nit's like a more visually\nvisually more appealing so that's the\nwhole idea so if we let me run this\npiece of code and this is how the heat\nmap looks\nand as you can see here the diagonals\nagain are all the values are here most\nof the values so which means reasonably\nthis seems to be reasonably accurate and\nyeah basically the accuracy score is 94\npercent this is calculated as i\nmentioned by adding all these numbers\ndivided by the total test value so the\ntotal number of observations in test\ndata set\nokay so this is the confusion matrix for\nlogistic regression\nall right so now that we have seen the\nconfusion matrix let's take a quick\nsample and see how well the system has\nclassified and we will take a few\nexamples of the data so if we see here\nwe we picked up randomly a few of them\nso\nthis is number four which is the actual\nvalue and also the predicted value both\nare four\nthis is an image of zero so the\npredicted value is also zero actual\nvalue is of course zero then this is the\nimage of nine\nso this has also been predicted\ncorrectly nine and actual value is nine\nand this is the image of one and again\nthis has been predicted correctly as\nlike the actual value okay so this was a\nquick demo of logistic regression how to\nuse logistic regression to identify\nimages\nneed for confusion matrixes\nclassification models have multiple\noutput categories\nmost error measures will tell us the\ntotal error in our model but we cannot\nuse it to find out individual instances\nof errors in our models\nso you have your input coming in you\nhave your classifier\nit measures the error and it says oh\n53 of these are correct\nbut we don't know which 53 are correct\nis it 53 correct uh on guessing on the\nspam is it 23 guessing on spam and\n27 guessing on what's not spam\nthis is where the confusion matrix comes\nin\nso during the classification we also\nhave to overcome the limitations of\naccuracy accuracy can be misleading for\nclassification problems\nif there is a significant class\nimbalance a model might predict the\nmajority class for all cases and have a\nhigh accuracy score\nand so you can see here we have our\nemail coming in and there's two spams\nthe classifier comes in and goes hey it\nonly catches one of those spams and it\nmisclassifies one that's not spam\nso our model predicted eight out of ten\ninstances and will have an accuracy of\n80 percent but is it classifying\ncorrectly\na confusion matrix represents a table\nlayout of the different outcomes of\nprediction\nand results of a classification problem\nand helps visualize its outcomes\nand so you see here we have our\nsimple chart predicted and actual\nthe confusion matrix\nhelps us identify the correct\npredictions of a model for different\nindividual classes as well as the errors\nso you'll see here that the values\npredicted by our classifier are along\nthe rows this is what we're going to\nguess it is or our model is guessing\nwhat this is based on its training so\nwe've already trained the model\nto guess whether it's spam or not spam\nor whatever it is you're working on\nand then the actual values of our data\nset are along the columns\nso this is the actual value that's\nsupposed to be\npeople who can speak english will be\nclassified as positives so because they\nhave a remember 001 do you speak english\nyes no and you could extend this that\nthey might have do you speak\nfrench do you speak whatever languages\nand so you might have a whole lot of\nclassifiers that you would look at each\none of these people who cannot speak\nenglish will be classified as negatives\nso there'll be a zero so you know zero\nones\nthe number of times our actual positive\nvalues are equal to predicted positive\nvalues gives us true positive tp\nthe number of times our actual negative\nvalues are equal to predictive negative\nvalues gives us true negative tn\nthe number of times our model wrongly\npredicts negative values as positives\ngives us a false positive\nfp\nand you'll see when you're working with\nthese a lot you know memorizing that is\nfalse positive you can easily figure out\nwhat that is and pretty soon you're just\nlooking at the fp or the tp depending on\nwhat you're working on and the number of\ntimes our model wrongly predicts\npositive values as negatives gives us a\nfalse negative fp\nnow i'm going to do a quick step out\nhere\nlet's say you're working in the medical\nand we're talking about cancer\ndo you really want a bunch of false\nnegatives\nyou want zero under false negative\nuh so when we look at this confusion\nmatrix if you have five percent false\npositives and five percent false\nnegatives it'd be much better to even\nhave twenty percent false positives\nbecause they go in and test it and zero\nfalse negatives\nthe same might be true if you're working\non uh say a car driving is this a safe\nplace for the car to go\nwell you really don't want any false\npositives you know yes this is safe\nright over the cliff\nso again when you're working on the\nproject or whatever it is you're working\non this chart suddenly has huge value\nwe were talking about spam email\nhow many important emails say from your\nbanking overdraft charge coming in that\nyou want to be a\na true a false negative you don't want\nit to go in the spam folder\nlikewise you want to get as much of the\nspam out of there but you don't want to\nmiss anything really important\nconfusion matrix metrics are performance\nmeasures which help us find the accuracy\nof our classifier there are four main\nmetrics\naccuracy\nprecision recall and f1 score the f1\nscore is the one i usually hear the most\nand accuracy is usually what you put on\nyour chart\nwhen you're sending in front of the\nshareholders how accurate is it people\nunderstand accuracy\num\nf1 score is a little bit more on the\nmath side and so you got to be a little\ncareful when you're quoting f1 scores in\nthe when you're sitting there with all\nthe shareholders because a lot of them\nwill just glaze over so confusion matrix\nmetrics are performance measures which\nhelp us find the accuracy of our\nclassifier there are four main metrics\naccuracy the accuracy is used to find\nthe portion of the correctly classified\nvalues it tells us how often our\nclassifier is right\nit is the sum of all true values divided\nby the total values\nand this makes sense uh\nagain it's one of those things\ni don't want to\nyou know what depends on what you're\nlooking for are you looking for\nnot to miss any spam mails are you\nlooking to drive down the road and not\nrun anybody over\nprecision\nis used to calculate the model's ability\nto classify positive values correctly it\nanswers the question when the model\npredicts a positive value how often is\nit right\nit is the true positive divided by the\ntotal number of predicted positive\nvalues again this one\ndepends on what project you're working\non whether this is what you're going to\nbe focusing on\nso recall it is used to calculate the\nmodel's ability to predict positive\nvalues\nhow often\ndoes the model actually predict the\ncorrect positive values\nit is the true positives divided by the\ntotal number of actual positive values\nand then your f1 score it is the\nharmonic mean of recall and precision it\nis useful when you need to take both\nprecision and recall into account\nconsider the following two confusion\nmatrix derived from two different\nclassifier\nto figure out which one performs better\nwe can find the confusion matrix for\nboth of them\nand you can see we're back to\ndoes it classify whether they can speak\nenglish or\nor non-speaker they speak some they\ndon't know the english language and so\nwe put these two uh confusion matrixes\nout here we can go ahead and do the math\nbehind that we can look up the accuracy\nthat's a tpn plus tn over the tf plus tn\nplus fp\nplus fn and so we get an accuracy of\n0.8125\nand we have a precision\nif you do the precision which is your tp\ntruth positive over tp plus fp\nwe get\n0.891\nand if we do the recall we'll end up\nwith the 0.825 that's your tp over tp\nplus fn\nand then of course your f1 score which\nis 2 times precision times recall over\nprecision plus recall\nand we get the 0.857\nand if we do that with another model\nlet's say we had two different models\nand we're trying to see which one we\nwant to use\nfor whatever reason\nwe might go ahead and compute the same\nthings we have our accuracy our\nprecision and our recall and our f1\nscore\nand as we're looking at this we might\nlook at the accuracy because that's\nreally what we're interested in is\nhow many people\nare we able to\nclassify as being able to speak english\ni really don't want to know\nif i'm you know i i really don't want to\nknow if they're non-speakers\ni'd rather miss 10 people speaking\nenglish instead of 15. and so you can\nsee from these charts we'd probably go\nwith the first model because it does a\nbetter job\nguessing who speaks english and has a\nhigher accuracy because in this case\nthat is what we're looking for\nso uh with that we'll go ahead and pull\nup a demo so you can see what this looks\nlike in the python\nsetup in in the actual coding for this\nwe'll go into anaconda navigator if\nyou're not familiar with anaconda it's a\nreally good tool to use as far as doing\ndisplay\nin demos\nand for quick development as a data\nscientist i just love the package\nnow if you're going to do something\nheavier lifting\nthere's some limitations with anaconda\nand with the setup but in general you\ncan do just about anything in here with\nyour python\nand for this we'll go with jupiter\nnotebook\njupiter lab is the same as jupiter\nnotebook you'll see they now have\nintegration with pi charm if you work in\npycharm\ncertainly there's a lot of other\nintegrations that\nanaconda has and we've opened up\nmy simply learn files i work on and\ncreate a new file called confusion\nmatrix demo\nand the first thing we want to note is\nthe data we're working with\nhere i've opened it up in a wordpad or\nnotepad or whatever\nyou can see it's got a row of\nheaders\ncomma separated and then all the data\ngoing down below\nand then i save this in the same file so\ni don't have to remember what path i'm\nworking on\nof course if you have your data\nseparated and you're working with a lot\nof data you probably want to put it into\na different folder or file depending on\nwhat you're doing\nand the first thing we're going to do is\ngo ahead and import our tools\nwe're going to use the pandas that's our\ndata frame\nif you haven't had a chance to work with\nthe data frame please review panda's\ndata frame and go into simply learn you\ncan pull up the pandas data frame\ntutorial on there\nand then we're going to use\nthe the scikit framework which is all\ndenoted as sklearn\nand i can just pull this in you can see\nhere's the\nscikit-learn.org\nwith the stable version that you can\nimport into your\npython and from here we're going to use\nthe train test split\nfor splitting our data we're going to do\nsome pre-processing\nwe're going to do use the logistic\nregression model that's our actual\nmachine learning model we're using and\nthen what this court this particular\nsetup is about is we're going to do the\naccuracy score the confusion matrix and\nthe classifier report\nso let me go ahead and run that and\nbring all that information in\nand just like we open the file we need\nto go ahead and load our data in here\nso we're going to go ahead and do our\npandas read csv\nand then just because we're in jupyter\nnotebook we can just put data to read\nthe data in here a lot of times we'll\nactually let me just do this i prefer to\ndo the\njust the head of the date or the top\npart\nand you can see we have age sex\ni'm not sure what cp stands for\ntest bps cholesterol\nso a lot of different measurements\nif you were in this domain you want to\nknow what all these different\nmeasurements mean\ni don't want to focus on that too much\nbecause\nwhen we're talking about data science a\nlot of times you have no idea what the\ndata means if you've ever looked up the\nbreast cancer measurement it's just a\nbunch of measurements and numbers\nunless you're a doctor you have no idea\nwhat those measurements mean\nbut if it's your specialty and your\ndomain you better know them so we're\ngoing to go ahead and create y and it's\ngoing to we're going to set it equal to\nthe target\nso here's our target value here\nand there's either 1 or 0.\nso we have a classifier if you're\ndealing with one zero true false what do\nyou have you have a classifier\nand then our x is going to be uh\neverything except for\nthe target\nso we're going to go ahead and drop the\ntarget axis equals 1. remember that's\ncolumns versus the index or rows axis\nequals 0 would would give you an error\nbut you would drop like row 2.\nand then we'll go ahead and just print\nthat out so you can see what we're\nlooking at and\nhere we have\ny data x data and you can see from the x\ndata we have the x head and we can go\nahead and just do print\nthe\ny head data\nand run that\nso this is all loading the data that\nwe've done so far if there's a confusion\nin there go back and rewind the tape and\nreview it\nand then we need to go ahead and split\nour data into our x train\nx test y train y test\nand then keep in mind you always want to\nsplit the data before we do the scalar\nand the reason is is that\nyou want the scalar on the training data\nto be set on the training data data or\nfit to it\nbut not on the test data\nthink of this as being out in the field\nyou're not it could actually alter your\nresults\nso it's always important to do make sure\nwhatever you do to the training data\nor whatever\nfit you're doing is always done on the\ntraining not on the test and then we\nwant to go ahead and scale the data now\nwe are working with a\nlinear regression model and i'll mention\nthis here in a minute when we get to the\nactual model\nuh so some sometimes you don't need to\nscale when you're working with linear\nregression models it's not going to\nchange your result as much as say a\nneural network where it has a huge\nimpact\nbut we're going to go ahead and take\nhere's our x train x test y train y test\nwe create our scalar we go ahead and\nscale\nthe scale is going to fit the x train\nand then we're going to go ahead and\ntake our x train and transform it\nand then we also need to take our x test\nand transform it based on the scale on\nhere so that our x is now between that\nnice minus 1 to one and so this is all\nuh our pre\ndata setup\nand\nhopefully\nuh all of that looks fairly familiar to\nyou if you've done a number of our other\nclasses and you're up to the setup on\nhere\nand then we want to go ahead and do is\ncreate our model and we're going to use\nthe logistic regression model\nand from the logistic regression model\nwe're going to go ahead and fit our x\ntrain and y train\nand then we'll run our predicted value\non here\nand so let's go ahead and run that and\nso now we are we actually have like our\nx test and our prediction so if you\nremember from\nour matrix we're looking for the actual\nversus the prediction and how those\ncompare\nand\nif i take us back up here you're going\nto notice that we imported the accuracy\nscore\nthe confusion matrix and the\nclassification report\nand there's of course our logistic\nregression the model we're using for\nthis\nand i did mention it's going to talk a\nlittle bit about scalar and the\nregression model\nthe scalar on a lot of your regression\nmodels your basic mass standard\nregression models and i'd have to look\nit up for the logistic regression model\nwhen you're using a standard regression\nmodel you don't need to scale the data\nit's already just built in by the way\nthe model works\nin most cases uh but if you're in a\nneural network and you're there's a lot\nof other different setups then you\nreally want to take this and fit that on\nthere\nand so we can go in and do the accuracy\nuh\nand this is if you remember correctly we\nwere looking at the accuracy\nwith the english speaking\nso this is saying our accuracy\nas to whether this person is i believe\nthis is the heart data set\nit's going to be accurate about 85\npercent of the time as far as whether\nit's going to predict the person's going\nto have a heart condition\nor the one as it comes up with the zero\none on there\nwhich would mean at this point that you\nhave an 85 percent uh being correct on\ntelling someone they're extremely high\nrisk for a heart attack kind of thing\nand so we want to go ahead and create\nour confusion matrix and we just do that\nof course the software does everything\nfor us so we'll go ahead and run this\nand you can see right here um\nhere's our 25\nprediction uh correct predictions right\nhere\nand if you remember from our slide i'll\njust bring this over says a nice visual\nwe have our true positive\nfalse positive\nuh so we had 25 which were true that it\nsaid hey this person is going to be high\nrisk at heart and we had four\nthat were still high risk that has said\nwe're false\nso out of these 25 people or out of\nthese 29 people and that makes sense\nbecause you have 0.85\nout of 29 people it was correct on 25 of\nthem\nand so uh here's our accuracy score we\nwere just looking at that our accuracy\nis your true positive and your true\nnegative over all of them so how true is\nit there's our accuracy coming up here\n0.85\nand then we have our nice matrix\ngenerated from that\nand you can see right here is a similar\nmatrix we had going for the from the\nslide and this starts to this should\nstart asking questions at this point\nso if you're in a board meeting or\nyou're working with this you really want\nto start looking at this data here and\nsaying well\nis this good enough is uh this number of\npeople and hopefully you'd have a much\nlarger data set it might is my confusion\nmatrix showing for the true positive and\nfalse positive\nis that acceptable for what we're doing\nuh and of course if you're going to put\ntogether whatever data you're putting\nout you might want to separate the\ntrue negative false positive false\nnegative true positive and you can\nsimply do that\nby doing the confusion matrix\nand then of course the ravel part lets\nyou\nset that up so you can just split that\nright up into a nice tuple and the final\nthing we want to show you here in the\ncoding\non this part\nis the confusion matrix metrics\nand so we can come in here and just use\nthe matrix equals classification report\nthe y test and the predict and then\nwe're going to take that classification\nreport and go ahead and print that out\nand you can see here it does a nice job\ngiving you your accuracy\nyour micro average your weighted average\nyou have your precision\nyour recall your f1 score and your\nsupport all in one window so you can\nstart looking at this data and saying oh\nokay\nour precision's at .83\nuh\n.87 for getting a positive and 0.83 for\nthe negative side for a zero\nand we start talking about whether this\nis a valid information or not to use\nand when we're looking at a heart attack\nprediction we're only looking at one\naspect what's the chances of this person\nhaving a heart attack or not\nyou might have something where we went\nback to the languages maybe you also\nwant to know whether they speak english\nor hindi\nor french and you can see right here\nthat we can now\ntake our confusion matrix and just\nexpand it as big as we need to depending\non how many different classifiers we're\nworking on decision tree important terms\nbefore we dive in further we need to\nlook at some basic terms we need to have\nsome definitions to go with our decision\ntree and the different parts we're going\nto be using we'll start with entropy\nentropy is a measure of randomness or\nunpredictability in the data set for\nexample we have a group of animals in\nthis picture there's four different\nkinds of animals and this data set is\nconsidered to have a high entropy you\nreally can't pick out what kind of\nanimal it is based on looking at just\nthe four animals as a big clump of\nentities so as we start splitting it\ninto subgroups we come up with our\nsecond definition which is information\ngain information gain it is a measure of\ndecrease in entropy after the data set\nis split so in this case based on the\ncolor yellow we've split one group of\nanimals on one side as true and those\nwho aren't yellow as false as we\ncontinue down the yellow side we split\nbased on the height true or false equals\nten and on the other side height is less\nthan 10 true or false and as you see as\nwe split it the entropy continues to be\nless and less and less and so our\ninformation gain is simply the entropy\ne1 from the top and how it's changed to\ne2 in the bottom and we'll look at the\ndeeper math although you really don't\nneed to know a huge amount of math when\nyou actually do the programming in\npython because they'll do it for you but\nwe'll look on the actual math of how\nthey compute entropy finally we went on\nthe different parts of our tree and they\ncall the leaf node leaf node carries the\nclassification or the decision so it's\nthe final end at the bottom the decision\nnode has two or more branches this is\nwhere we're breaking the group up into\ndifferent parts and finally you have the\nroot node the topmost decision node is\nknown as the root node\nhow does a decision tree work wonder\nwhat kind of animals all get in the\njungle today maybe you're the hunter\nwith a gun or if you're more into\nphotography you're a photographer with a\ncamera so let's look at this group of\nanimals and let's try to classify\ndifferent types of animals based on\ntheir features using a decision tree so\nthe problem statement is to classify the\ndifferent types of animals based on\ntheir features using a decision tree the\ndata set is looking quite messy and the\nentropy is high in this case so let's\nlook at a training set or a training\ndata set and we're looking at color\nwe're looking at height and then we have\nour different animals we have our\nelephants our giraffes our monkeys and\nour tigers and there of different colors\nand shapes let's see what that looks\nlike and how do we split the data we\nhave to frame the conditions that split\nthe data in such a way that the\ninformation gain is the highest note\ngain is the measure of decrease in\nentropy after splitting so the formula\nfor entropy is the sum that's what this\nsymbol looks like that looks like kind\nof like a e funky e of k where i equals\n1 to k k would represent the number of\nanimals the different animals in there\nwhere value or p\nvalue of i would be the percentage of\nthat animal times the log base two of\nthe same the percentage of that animal\nlet's try to calculate the entropy for\nthe current data set and take a look at\nwhat that looks like and don't be afraid\nof the math you don't really have to\nmemorize this math just be aware that\nit's there and this is what's going on\nin the background and so we have three\ngiraffes two tigers one monkey two\nelephants a total of eight animals\ngathered and if we plug that into the\nformula we get an entropy that equals\nthree over eight so we have three\ngiraffes a total of eight times the log\nusually they use base two on the log so\nlog base two of three over eight plus in\nthis case it says yellow fence two over\neight two elephants over total of eight\ntimes log base two two over eight plus\none monkey over total of eight log base\ntwo one over eight and plus two over\neight of the tigers log base two over\neight and if we plug that into our\ncomputer our calculator i obviously\ncan't do logs in my head we get an\nentropy equal to 0.571\nthe program will actually calculate the\nentropy of the data set similarly after\nevery split to calculate the gain now\nwe're not going to go through each set\none at a time to see what those numbers\nare we just want you to be aware that\nthis is a formula or the mathematics\nbehind it gain can be calculated by\nfinding the difference of the subsequent\nentropy values after a split now we will\ntry to choose a condition that gives us\nthe highest gain we will do that by\nsplitting the data using each condition\nand checking that the gain we get out of\nthem the condition that gives us the\nhighest gain will be used to make the\nfirst split can you guess what that\nfirst split will be just by looking at\nthis image as a human is probably pretty\neasy to split it let's see if you're\nright if you guessed the color yellow\nyou're correct let's say the condition\nthat gives us the maximum gain is yellow\nso we will split the data based on the\ncolor yellow if it's true that group of\nanimals goes to the left if it's false\nit goes to the right the entropy after\nthe splitting has to decreased\nconsiderably however we still need some\nsplitting of both the branches to attain\nan entropy value equal to zero so we\ndecide to split both the nodes using\nheight as a condition since every branch\nnow contains single label type we can\nsay that entropy in this case has\nreached the least value and here you see\nwe have the giraffes the tigers the\nmonkey and the elephants all separated\ninto their own groups this tree can now\npredict all the classes of animals\npresent in the dataset with a hundred\npercent accuracy that was easy\nuse case loan repayment prediction let's\nget into my favorite part and open up\nsome python and see what the programming\ncode in the scripting looks like in here\nwe're going to want to do a prediction\nand we start with this individual here\nwho's requesting to find out how good\nhis customers are going to be whether\nthey're going to repay their loan or not\nfor his bank and from that we want to\ngenerate a problem statement to predict\nif a customer will repay loan amount or\nnot and then we're going to be using the\ndecision tree algorithm in python let's\nsee what that looks like and let's dive\ninto the code in our first few steps of\nimplementation we're going to start by\nimporting the necessary packages that we\nneed from python and we're going to load\nup our data and take a look at what the\ndata looks like so the first thing i\nneed is i need something to edit my\npython and run it in so let's flip on\nover and here i'm using the anaconda\njupiter notebook now you can use any\npython ide you like to run it in but i\nfind the jupiter notebook is really nice\nfor doing things on the fly and let's go\nahead and just paste that code in the\nbeginning and before we start let's talk\na little bit about what we're bringing\nin and then we're going to do a couple\nthings in here we have to make a couple\nchanges as we go through this first part\nof the import the first thing we bring\nin is numpy as np that's very standard\nwhen we're dealing with mathematics\nespecially with uh very complicated\nmachine learning tools you almost always\nsee the numpy come in for your num your\nnumbers it's called number python it has\nyour mathematics in there in this case\nwe actually could take it out but\ngenerally you'll need it for most of\nyour different things you work with and\nthen we're going to use pandas as pd\nthat's also a standard the pandas is a\ndata frame setup and you can liken this\nto\ntaking your basic data and storing it in\na way that looks like an excel\nspreadsheet so as we come back to this\nwhen you see np or pd those are very\nstandard uses you'll know that that's\nthe pandas and i'll show you a little\nbit more when we explore the data in\njust a minute then we're going to need\nto split the data so i'm going to bring\nin our train test and split and this is\ncoming from the sk learn package cross\nvalidation in just a minute we're going\nto change that and we'll go over that\ntoo and then there's also the sk.tree\nimport decision tree classifier that's\nthe actual tool we're using remember i\ntold you don't be afraid of the\nmathematics it's going to be done for\nyou well the decision tree classifier\nhas all that mathematics in there for\nyou so you don't have to figure it back\nout again and then we have\nsklearn.metrics\nfor accuracy score we need to score our\nsetup that's the whole reason we're\nsplitting it between the training and\ntesting data and finally we still need\nthe sklearn import tree and that's just\nthe basic tree function is needed for\nthe decision tree classifier and finally\nwe're going to load our data down here\nand i'm going to run this and we're\ngoing to get two things on here one\nwe're going to get an error and two\nwe're going to get a warning let's see\nwhat that looks like so the first thing\nwe had is we have an error why is this\nerror here well it's looking at this it\nsays i need to read a file and when this\nwas written\nthe person who wrote it this is their\npath where they stored the file\nso let's go ahead and fix that\nand i'm going to put in here my file\npath i'm just going to call it full file\nname\nand you'll see it's on my c drive and\nthis very lengthy setup on here where i\nstored the data2.csv file\ndon't worry too much about the full path\nbecause on your computer will be\ndifferent\nthe data.2 csv file was generated by\nsimplylearn\nif you want a copy of that you can\ncomment down below and request it here\nin the youtube\nand then if i'm going to give it a name\nfull file name\ni'm going to go ahead and change it here\nto full\nfile name so let's go ahead and run it\nnow and see what happens\nand we get a warning\nwhen you're coding\nunderstanding these different warnings\nand these different errors that come up\nis probably the hardest lesson to learn\nso let's just go ahead and take a look\nat this and use this as a opportunity to\nunderstand what's going on here\nif you read the warning it says the\ncross validation is depreciated so it's\na warning on it's being removed\nand it's going to be moved in favor of\nthe model selection\nso if we go up here we have sklearn dot\ncross validation and if you research\nthis and go to the sklearn site you'll\nfind out that you can actually just swap\nit right in there with\nmodel selection\nand so when i come in here and i run it\nagain\nthat removes a warning\nwhat they've done is they've had two\ndifferent developers develop it in two\ndifferent branches\nand then they decided to keep one of\nthose and eventually get rid of the\nother one that's all that is and very\neasy and quick to fix\nbefore we go any further i went ahead\nand opened up the data\nfrom this file remember the the data\nfile we just loaded on here the data\nunderscore2.csv\nlet's talk a little bit more about that\nand see what that looks like both as a\ntext file because it's a comma separated\nvariable file and in a spreadsheet\nthis is what it looks like as a basic\ntext file you can see at the top they've\ncreated a header and it's got one two\nthree four five columns and each column\nhas data in it and let me flip this over\nbecause we're also going to look at this\nin an actual spreadsheet so you can see\nwhat that looks like and here i've\nopened it up in the open office calc\nwhich is pretty much the same as excel\nand zoomed in and you can see we've got\nour columns and our rows of data a\nlittle easier to read in here we have a\nresult yes yes no we have initial\npayment last payment credit score\nhouse number\nif we scroll way down\nwe'll see that this occupies a thousand\nand one lines of code or lines of data\nwith the first one being a column and\nthen one thousand lines of data\nnow as a programmer\nif you're looking at a small amount of\ndata i usually start by pulling it up in\ndifferent sources so i can see what i'm\nworking with\nbut in larger data you won't have that\noption it'll just be too too large so\nyou need to either bring in a small\namount that you can look at it like\nwe're doing right now or we can start\nlooking at it through the python code so\nlet's go ahead and move on and take the\nnext couple steps to explore the data\nusing python let's go ahead and see what\nit looks like in python to print the\nlength and the shape of the data so\nlet's start by printing the length of\nthe database we can use a simple lin\nfunction from python\nand when i run this\nyou'll see that it's a thousand long and\nthat's what we expected there's a\nthousand lines of data in there if you\nsubtract the column head\nthis is one of the nice things when we\ndid the uh balance data from the panda\nread csv\nyou'll see that the header is row zero\nso it automatically removes a row\nand then shows the data separate it does\na good job sorting that data out for us\nand then we can use a different function\nand let's take a look at that\nand again we're going to utilize the\ntools in panda\nand since the balance underscored data\nwas loaded as a panda data frame\nwe can do a shape on it and let's go\nahead and run the shape and see what\nthat looks like\nwhat's nice about this shape is not only\ndoes it give me the length of the data\nwe have a thousand lines it also tells\nme there's five columns so we were\nlooking at the data we had five columns\nof data and then let's take one more\nstep to explore the data using python\nand now that we've taken a look at the\nlength and the shape let's go ahead and\nuse the pandas module for head another\nbeautiful thing in the data set that we\ncan utilize so let's put that on our\nsheet here and we have print data set\nand balance data dot head and this is a\npanda's print statement of its own so it\nhas its own print feature in there and\nthen we went ahead and gave a label for\na print job here of dataset just a\nsimple print statement\nand we run that\nlet's just take a closer look at that\nlet me zoom in here\nthere we go\npandas do such a wonderful job of making\nthis a very clean\nreadable data set so you can look at the\ndata you can look at the column headers\nyou can have it when you put it as the\nhead\nit prints the first five lines of the\ndata\nand we always start with zero so we have\nfive lines we have zero one two three\nfour instead of one two three four five\nthat's a standard scripting and\nprogramming set as you want to start\nwith the zero position and that is what\nthe data head does it pulls the first\nfive rows of data puts in a nice format\nthat you can look at and view very\npowerful tool to view the data so\ninstead of having to flip and open up an\nexcel spreadsheet or open office cal or\ntrying to look at a word doc where it's\nall scrunched together and hard to read\nyou can now get a nice open view of what\nyou're working with we're working with a\nshape of a thousand long five wide so we\nhave five columns and we do the full\ndate ahead you can actually see what\nthis data looks like the initial payment\nlast payment credit scores house number\nso let's take this now that we've\nexplored the data and let's start\ndigging into the decision tree so in our\nnext step we're going to\ntrain and build our data tree and to do\nthat we need to first\nseparate the data out we're going to\nseparate into two groups so that we have\nsomething to actually train the data\nwith and then we have some data on the\nside to test it to see how good our\nmodel is remember with any of the\nmachine learning you always want to have\nsome kind of test set to weigh it\nagainst so you know how good your model\nis when you distribute it let's go ahead\nand break this code down and look at it\nin pieces\nso first we have our x and y\nwhere do x and y come from well x is\ngoing to be our data\nand y is going to be the answer or the\ntarget you can look at source and target\nin this case we're using x and y to\ndenote the data in and the data that\nwe're actually trying to guess what the\nanswer is going to be and so to separate\nit we can simply put in x equals the\nbalance of the data.values the first\nbrackets\nmeans that we're going to select all the\nlines in the database so it's all the\ndata\nand the second one says we're only going\nto look at columns one through five\nremember i always start with zero zero\nis a yes or no and that's whether the\nloan went default or not so we want to\nstart with one if we go back up here\nthat's the initial payment and it goes\nall the way through the house number\nwell if we want to look at one through\nfive we can do the same thing for y\nwhich is the answers and we're going to\nset that just equal to the zero row so\nit's just the zero row and then it's all\nrows going in there so now we've divided\nthis into two different data sets\none of them with the\ndata going in and one with the answers\nnext we need to split the data\nand here you'll see that we have it\nsplit into four different parts\nthe first one is your x training your x\ntest your y train your y test\nsimply put we have x going in where\nwe're going to train it and we have to\nknow the answer to train it with\nand then we have x test where we're\ngoing to test that data and we have to\nknow in the end what the y was supposed\nto be\nand that's where this train test split\ncomes in that we loaded earlier in the\nmodules this does it all for us and you\ncan see they set the test size equal to\n0.3 so that's roughly 30 percent will be\nused in the test and then we use a\nrandom state so it's completely random\nwhich rows it takes out of there and\nthen finally we get to actually build\nour decision tree and they've called it\nhere clf underscore entropy that's the\nactual decision tree or decision tree\nclassifier and in here they've added a\ncouple variables which we'll explore in\njust a minute and then finally we need\nto fit the data to that so we take our\nclf entropy that we created and we fit\nthe x train and since we know the\nanswers for x trade or the y train we go\nahead and put those in and let's go\nahead and run this and what most of\nthese sklearn modules do is when you set\nup the variable in this case we set the\nclf entropy called decision tree\nclassifier it automatically prints out\nwhat's in that decision tree there's a\nlot of variables you can play within\nhere and it's quite beyond the scope of\nthis\ntutorial to go through all of these and\nhow they work but we're working on\nentropy that's one of the options we've\nadded that it's completely a random\nstate of 100 so 100 percent and we have\na max depth of three now the max depth\nif you remember above when we were doing\nthe different graphs of animals means\nit's only going to go down three layers\nbefore it stops and then we have minimal\nsamples of leaves as five so it's going\nto have at least five leaves at the end\nso i'll have at least three splits i'll\nhave no more than three layers and at\nleast five end leaves with the final\nresult at the bottom now that we've\ncreated\nour decision tree classifier not only\ncreated it but trained it let's go ahead\nand apply it and see what that looks\nlike so let's go ahead and make a\nprediction and see what that looks like\nwe're going to paste our predict code in\nhere\nand before we run it let's just take a\nquick look at what it's doing here\nwe have a variable y predict that we're\ngoing to do\nand we're going to use our\nvariable clf entropy that we\ncreated and then you'll see dot predict\nand it's very common in the sk learn\nmodules\nthat their different tools have to\npredict when you're actually running a\nprediction\nin this case we're going to put our x\ntest data in here\nnow if you delivered this for use in\nactual commercial use and distributed it\nthis would be the new loans you're\nputting in here to guess\nwhether the person is going to be uh pay\nthem back or not\nin this case though we need to test out\nthe data and just see how good our\nsample is how good of our tree does at\npredicting the loan payments and finally\nsince anaconda jupiter notebook is works\nas a command line for python we can\nsimply put the y predict e in to print\nit i could just as easily have put the\nprint\nand put brackets around y predict en to\nprint it out we'll go ahead and do that\nit doesn't matter which way you do it\nand you'll see right here that runs a\nprediction this is roughly 300 in here\nremember it's 30 percent of a thousand\nso you should have about 300 answers in\nhere\nand this tells you which each one of\nthose lines of our test went in there\nand this is what our y predict came out\nso let's move on to the next step we're\ngoing to take this data and try to\nfigure out just how good a model we have\nso here we go since sklearn does all the\nheavy lifting for you and all the math\nwe have a simple line of code to let us\nknow what the accuracy is and let's go\nahead and go through that and see what\nthat means and what that looks like\nlet's go ahead and paste this in and let\nme zoom in a little bit\nthere we go\nso you have a nice full picture\nand we'll see here we're just going to\ndo a print accuracy is\nand then we do the accuracy score\nand this was something we imported\nearlier if you remember at the very\nbeginning let me just scroll up there\nreal quick so you can see where that's\ncoming from\nthat's coming from here down here from\nsklearn.metrics import accuracy score\nand you could probably run a script make\nyour own script to do this very easily\nhow accurate is it how many out of 300\ndo we get right and so we put in our y\ntest that's the one we ran the predict\non and then we put in our y predict e n\nthat's the answers we got and we're just\ngoing to multiply that by a hundred\nbecause this is just going to give us an\nanswer as a decimal and we want to see\nit as a percentage\nand let's run that and see what it looks\nlike\nand if you see here we got an accuracy\nof 93.66667\nso when we look at the number of loans\nand we look at how good our model fit we\ncan tell people it has about a 93.6\nfitting to it so just a quick recap on\nthat we now have accuracy set up on here\nand so we have created a model that uses\nthe decision tree algorithm to predict\nwhether a customer will repay the loan\nor not the accuracy of the model is\nabout 94.6 percent the bank can now use\nthis model to decide whether it should\napprove the loan request from a\nparticular customer or not and so this\ninformation is really powerful we may\nnot be able to as individuals understand\nall these numbers because they have\nthousands of numbers that come in but\nyou can see that this is a smart\ndecision for the bank to use a tool like\nthis to help them to predict how good\ntheir profit's going to be off of the\nloan balances and how many are going to\ndefault or not how does a random forest\nwork as a whole so to begin our\nrandom forest classifier let's say we\nalready have built three trees and we're\ngoing to start with the first tree that\nlooks like this just like we did in the\nexample this tree looks at the diameter\nif it's greater than or equal to 3\nit's true otherwise it's false so one\nside goes to the smaller diameter one\nside goes to larger diameter and if the\ncolor is orange it's going to go to the\nright true we're using oranges now\ninstead of lemons and if it's red it's\ngoing to go to the left false we build a\nsecond tree very similar but split\ndifferently instead of the first one\nbeing split by a diameter this one when\nthey created it if you look at that\nfirst bowl it has a lot of red objects\nso it says is the color red because\nthat's going to bring our entropy down\nthe fastest and so of course if it's\ntrue it goes to the left if it's false\nit goes to the right and then it looks\nat the shape false or true and so on and\nso on and tree three is a diameter equal\nto one and it came up with this because\nthere's a lot of cherries in this bowl\nso that would be the biggest split on\nthere is is the diameter equal to one\nthat's going to drop the entropy the\nquickest and as you can see it splits it\ninto true if it goes false and they've\nadded another category does it grow in\nthe summer and if it's false\nit goes off to the left if it's true it\ngoes off to the right let's go ahead and\nbring these three trees you can see them\nall in one image so this would be three\ncompletely different trees categorizing\na fruit and let's take a fruit now let's\ntry this\nand this fruit if you look at it we've\nblackened it out you can't see the color\non it so it's missing data remember one\nof the things we talked about earlier is\nthat a random forest works really good\nif you're missing data if you're missing\npieces so this fruit has an image but\nmaybe the person had a black and white\ncamera when they took the picture and\nwe're going to take a look at this and\nit's going to have\nto put the color in there so ignore the\ncolor down there but the diameter equals\n3 we find out it grows in the summer\nequals yes and the shape is a circle and\nif you go to the right you can look at\nwhat one of the decision trees did this\nis the third one\nis the diameter greater than equal to\nthree is the color orange well it\ndoesn't really know on this one but if\nyou look at the value it's a true and\nyou go to the right tree two classifies\nit as cherries is a color equal red is\nthe shape a circle true it is a circle\nso this would look at it and say oh\nthat's a cherry and then we go to the\nother classifier and it says is the\ndiameter equal one well that's false\ndoes it grow in the summer true so it\ngoes down and looks at as oranges so how\ndoes this random forest work the first\none says it's an orange\nthe second one said it was a cherry and\nthe third one says it's an orange\nand you can guess if you have two\noranges and one says it's a cherry\nwhen you add that all together the\nmajority of the vote says orange so the\nanswer is it's classified as an orange\neven though we didn't know the color and\nwe're missing data on it i don't know\nabout you but i'm getting tired of fruit\nso let's switch and i did promise you\nwe'd start looking at a case example and\nget into some python coding today we're\ngoing to use the case the iris flower\nanalysis\nthis is the exciting part as we roll up\nour sleeves and actually look at some\npython coding before we start the python\ncoding we need to go ahead and create a\nproblem statement wonder what species of\niris do these flowers belong to let's\ntry to predict the species of the\nflowers using machine learning in python\nlet's see how it can be done so here we\nbegin to go ahead and implement our\npython code and you'll find that the\nfirst half of our implementation is all\nabout organizing and exploring the data\ncoming in let's go ahead and take this\nfirst step which is loading the\ndifferent modules into python and let's\ngo ahead and put that in our favorite\neditor whatever your favorite editor is\nin this case i'm going to be using the\nanaconda jupiter notebook which is one\nof my favorites certainly there's\nnotepad plus plus and eclipse and dozens\nof others or just even using the python\nterminal window any of those will work\njust fine to go ahead and explore this\npython coding so here we go let's go\nahead and flip over to our jupiter\nnotebook and i've already opened up a\nnew page for python 3 code and i'm just\ngoing to paste this right in there and\nlet's take a look and see what we're\nbringing into our python the first thing\nwe're going to do is from the\nsklearn.datasets\nimport load iris now this isn't the\nactual data this is just the module that\nallows us to bring in the data the load\niris and the iris is so popular it's\nbeen around since 1936 when ronald\nfisher published a paper on it and\nthey're measuring the different parts of\nthe flower and based on those\nmeasurements predicting what kind of\nflower it is and then if we're going to\ndo a random forest classifier we need to\ngo ahead and import a random forest\nclassifier from the sk learn module so\ndot ensemble import random force\nclassifier and then we want to bring in\ntwo more modules\nand these are probably the most commonly\nused modules in python and data science\nwith any of the\nother modules that we bring in and one\nis going to be pandas we're going to\nimport pandas as pd pd is a common term\nused for pandas and pandas is basically\ncreates a data format for us where when\nyou create a pandas data frame it looks\nlike an excel spreadsheet and you'll see\nthat in a minute when we start digging\ndeeper into the code panda is just\nwonderful because it plays nice with all\nthe other modules in there and then we\nhave numpy which is our numbers python\nand the numbers python allows us to do\ndifferent mathematical sets on here\nwe'll see right off the bat we're going\nto take our np and we're going to go\nahead and seed the randomness with it\nwith 0. so np.random.seed\nis seating that is 0. this code doesn't\nactually show anything we're going to go\nahead and run it because i need to make\nsure i have all those loaded and then\nlet's take a look at the next module on\nhere the next six slides including this\none are all about exploring the data\nremember i told you half of this is\nabout looking at the data and getting it\nall set so let's go ahead and take this\ncode right here the script and let's get\nthat over into our jupyter notebook and\nhere we go we've gone ahead and run the\nimports and i'm going to paste the code\ndown here\nand let's take a look and see what's\ngoing on the first thing we're doing is\nwe're actually loading the iris data and\nif you remember up here we loaded the\nmodule that tells it how to get the iris\ndata now we're actually assigning that\ndata to the variable iris and then we're\ngoing to go ahead and use the df to\ndefine data frame\nand that's going to equal pd and if you\nremember that's pandas as pd so that's\nour pandas\nand panda data frame and then we're\nlooking at iris data\nand columns equals iris feature names\nand we're going to do the df head and\nlet's run this you can understand what's\ngoing on here\nthe first thing you want to notice is\nthat our df has created what looks like\nan excel spreadsheet\nand in this excel spreadsheet we have\nset the columns so up on the top you can\nsee the four different columns and then\nwe have the data iris.data down below\nit's a little confusing without knowing\nwhere this data is coming from so let's\nlook at the bigger picture and i'm going\nto go print i'm just going to change\nthis for a moment and we're going to\nprint all of iris and see what that\nlooks like\nso when i print all of iris i get this\nlong list of information\nand you can scroll through here and see\nall the different titles on there\nwhat's important to notice is that first\noff there's a brackets at the beginning\nso this is a python dictionary\nand in a python dictionary\nyou'll have a key\nor a label and this label pulls up\nwhatever information comes after it so\nfeature names which we actually used\nover here under columns is equal to an\narray of sepal length sepal width petal\nlength petal width these are the\ndifferent names they have for the four\ndifferent columns and if you scroll down\nfar enough you'll also see data down\nhere oh goodness it came up right\ntowards the top and data is equal to the\ndifferent data we're looking at\nnow there's a lot of other things in\nhere like target we're going to be\npulling that up in a minute and there's\nalso the names the target names which is\nfurther down and we'll show you that\nalso in a minute let's go ahead and set\nthat back to the head\nand this is one of the neat features of\npandas and panda data frames\nis when you do df.head or the\npandadataframe.head\nit'll print the first five lines of the\ndata set in there along with the headers\nif you have it in this case we have the\ncolumn header set to iris features and\nin here you'll see that we have zero one\ntwo three four in python most arrays\nalways start at zero so when you look at\nthe first five it's going to be zero one\ntwo three four not one two three four\nfive so now we've got our iris data\nimported into a data frame let's take a\nlook at the next piece of code in here\nand so in this section here\nof the code\nwe're going to take a look at the target\nand let's go ahead and get this into our\nnotebook this piece of code so we can\ndiscuss it a little bit more in detail\nso here we are in our jupyter notebook\ni'm going to put the code in here and\nbefore i run it i want to look at a\ncouple things going on so we have df\nspecies\nand this is interesting because right\nhere you'll see where i have df species\nin brackets which is uh the key code for\ncreating another column and here we have\niris.target now these are both in the\npandas setup on here so in pandas we can\ndo either one i could have just as\neasily done iris and then in brackets\ntarget depending on what i'm working on\nboth are\nacceptable\nlet's go ahead and run this code and see\nhow this changes and what we've done is\nwe've added the target from the iris\ndata set\nas another column on the end\nnow what species is this is what we're\ntrying to predict so we have our data\nwhich tells us the answer for all these\ndifferent pieces and then we've added a\ncolumn with the answer that way when we\ndo our final setup we'll have the\nability to program our neural network to\nlook for these this different data and\nknow what a setosa is or a vera color\nwhich we'll see in just a minute or\nvirginica those are the three that are\nin there and now we're going to add one\nmore column i know we're organizing all\nthis data over and over again it's kind\nof fun there's a lot of ways to organize\nit what's nice about putting everything\nonto one\ndata frame is i can then do a print out\nand it shows me exactly what i'm looking\nat and i'll show you that where you\nwhere that's different where you can\nalter that and do it slightly\ndifferently but let's go ahead and put\nthis into our script up to that now and\nhere we go we're going to put that down\nhere\nand we're going to run that\nand let's talk a little bit about what\nwe're doing now we're exploring data\nand\none of the challenges is knowing how\ngood your model is did your model work\nand to do this we need to split the data\nand we split it into two different parts\nthey usually call it the training and\nthe testing and so in here we're going\nto go ahead and put that in our database\nso you can see it clearly and we've set\nit df remember you can put brackets this\nis creating another column is train so\nwe're going to use part of it for\ntraining and this equals np remember\nthat stands for numpy.random.uniform\nso we're generating a random number\nbetween 0 and 1 and we're going to do it\nfor each of the rows that's where the\nlength df comes from so each row gets a\ngenerated number and if it's less than\n0.75 it's true and if it's greater than\n0.75 it's false this means we're going\nto take\n75\nof the data roughly because there's a\nrandomness involved and we're going to\nuse that to train it and then the other\n25 we're going to hold off to the side\nand use that to test it later on so\nlet's flip back on over and see what the\nnext step is so now that we've labeled\nour database for which is training and\nwhich is testing\nlet's go ahead and sort that into two\ndifferent variables train and test and\nlet's take this code and let's bring it\ninto our project and here we go let's\npaste it on down here and before i run\nthis let's just take a quick look at\nwhat's going on here is we have up above\nwe created remember there's our def dot\nhead which prints the first five rows\nand we've added a column is train at the\nend and so we're going to take that\nwe're going to create two variables\nwe're going to create two new data\nframes one's called train\none's called test 75 and train 25\npercent in test\nand then to sort that out\nwe're going to do that by doing df our\nmain original data frame with the iris\ndata in it\nand if df is trained equals true\nthat's going to go in the train and if\ndf is trained equals false it goes in\nthe test\nand so when i run this\nwe're going to print out the number in\neach one let's see what that looks like\nand you'll see that it puts 118 in the\ntraining module and it puts 32 in the\ntesting module which lets us know that\nthere was 150 lines of data in here so\nif you went and looked at the original\ndata you could see that there's 150\nlines and that's roughly 75 percent in\none and 25 percent for us to test our\nmodel on afterward so let's jump back to\nour code and see where this goes in the\nnext two steps\nwe want to do one more thing with our\ndata and that's make it readable to\nhumans\ni don't know about you but i hate\nlooking at zeros and ones\nso let's start with the features and\nlet's go ahead and\ntake those and make those readable to\nhumans and let's put that in our code\nlet's see here we go paste it in and\nyou'll see here we've done a couple very\nbasic things we know that the columns in\nour data frame again this is a panda\nthing the df columns\nand we know the first four of them zero\none two three that'd be the first four\nare going to be the features or the\ntitles of those columns and so when i\nrun this\nyou'll see down here that it creates an\nindex sepa length sepa width pedal\nlength and pedal width and this should\nbe familiar because if you look up here\nhere's our column titles going across\nand here's the first four\none thing i want you to notice here is\nthat when you're in a command line\nwhether it's jupiter notebook or you're\nrunning command line in the terminal\nwindow\nif you just put the name of it it'll\nprint it out this is the same as doing\nprint\nfeatures\nand the shorthand is you just put\nfeatures in here\nif you're actually writing a code\nand saving the script and running it by\nremote you really need to put the print\nin there but for this when i run it\nyou'll see it gives me the same thing\nbut for this we want to go ahead and\nwe'll just leave it as features because\nit doesn't really matter and this is one\nof the fun thing about jupiter notebooks\nis i'm just building the code as we go\nand then we need to go ahead and create\nthe labels for the other part so let's\ntake a look and see what that\nfor our final step in prepping our data\nbefore we actually start running the\ntraining and the testing is we're going\nto go ahead and convert the species on\nhere into something the computer\nunderstands so let's put this code into\nour script and see where that takes us\nall right here we go we've set y equal\nto pd dot factorize\ntrain species of zero\nso let's break this down just a little\nbit we have our pandas right here pd\nfactorize what's factorized doing i'm\ngoing to come back to that in just a\nsecond\nlet's look at what train species is\nand why we're looking at the group 0 on\nthere\nand let's go up here and here is our\nspecies\nremember this on that we created this\nwhole column here for species\nand then it has setosa setosa setosa\nsetosa and if you scroll down enough\nyou'd also see virginica and vera color\nwe need to convert that into something\nthe computer understands zeros and ones\nso the train species\nof zero because this is in the format of\na of an array of arrays so you have to\nhave the zero on the end\nand then species is just that column\nfactorize goes in there and looks at the\nfact that there's only three of them so\nwhen i run this you'll see that y\ngenerates an array that's equal to in\nthis case it's the training set and it's\nzeros ones and twos representing the\nthree different kinds of flowers we have\nso now we have something the computer\nunderstands and we have a nice table\nthat we can read and understand and now\nfinally we get to actually start doing\nthe predicting so here we go\nuh we have\ntwo lines of code oh my goodness that\nwas a lot of work to get to two lines of\ncode but there is a lot in these two\nlines of code so let's take a look and\nsee what's going on here and put this\ninto our full script that we're running\nand let's paste this in here and let's\ntake a look and see what this is we have\nwe're creating a variable clf and we're\ngoing to set this equal to the random\nforest classifier and we're passing two\nvariables in here and there's a lot of\nvariables you can play with as far as\nthese two are concerned they're very\nstandard\nin jobs all that does is to prioritize\nit not something to really worry about\nusually when you're doing this on your\nown computer you do in jobs equals two\nif you're working in a larger or big\ndata and you need to prioritize it\ndifferently this is what that number\ndoes is it changes your priorities and\nhow it's going to run across the system\nand things like that and then the random\nstate is just how it starts\nzero's fine for here\nbut let's go ahead and run this\nwe also have clf dot fit train features\ncomma y and before we run it let's talk\nabout this a little bit more clf\ndot fit so we're fitting we're training\nit we are actually creating our random\nforest classifier right here this is a\ncode that does everything and we're\ngoing to take our training set remember\nwe kept our test off to the side and\nwe're going to take our training set\nwith the features and then we're going\nto go ahead and put that in and here's\nour target the y\nso the y is 0 1 and 2 that we just\ncreated and the features is the actual\ndata going in that we put into the\ntraining set\nlet's go ahead and run that\nand this is kind of an interesting thing\nbecause it printed out the random force\nclassifier\nand everything around it and so when\nyou're running this in your terminal\nwindow or in a script like this\nthis automatically treats us like just\nlike when we were up here and i typed in\ny and i printed out y instead of print y\nthis does the same thing it treats this\nas a variable and prints it out but if\nyou're actually running your code that\nwouldn't be the case and what is printed\nout is it shows us all the different\nvariables we can change and if we go\ndown here you can actually see in jobs\nequals two\nyou can see the random state equals zero\nthose are the two that we sent in there\nyou would really have to dig deep to\nfind out all these the different\nmeanings of all these different settings\non here some of them are\nself-explanatory if you kind of think\nabout it a little bit like max features\nas auto so all the features that we're\nputting in there is just going to\nautomatically take all four of them\nwhatever we send it it'll take some of\nthem might have so many features because\nyou're processing words there might be\nlike 1.4 million features in there\nbecause you're doing legal documents and\nthat's how many different words are in\nthere at that point you probably want to\nlimit the maximum features that you're\ngoing to process and leaf nodes that's\nthe end nodes remember we have the fruit\nand we're talking about the leaf nodes\nlike i said there's a lot in this we're\nlooking at a lot of stuff here so you\nmight have in this case there's probably\nonly think three leaf nodes maybe four\nyou might have thousands of leaf nodes\nat which point you do need to put a cap\non that and say okay it can only go so\nfar and then we're going to use all of\nour resources on processing this and\nthat really is what most of these are\nabout is limiting the process and making\nsure we don't overwhelm a system and\nthere's some other settings in here\nagain we're not going to go over all of\nthem warm start equals false warm start\nis if you're programming it one piece at\na time externally since we're not we're\nnot going to have like we're not going\nto continually train this particular\nlearning tree and again like i said\nthere's a lot of things in here that\nyou'll want to look up more detail from\nthe sk learn\nand if you're digging in deep and\nrunning a major project on here for\ntoday though all we need to do is fit\nour train our features and our target y\nso now we have our training model what's\nnext if we're going to create a model\nwe now need to test it remember we set\naside the test feature test group 25 of\nthe data so let's go ahead and take this\ncode and let's put it into our script\nand see what that looks like okay here\nwe go\nand we're going to run this\nand it's going to come out with a bunch\nof zeros ones and twos which represents\nthe three type of flowers the setosa the\nvirginica and the versacolor and what\nwe're putting into our predict is the\ntest features\nand i always kind of like to know what\nit is i am looking at\nso real quick we're going to do test\nfeatures and remember features is an\narray\nof sepal length simple width pedal\nlength pedal width so when we put it in\nthis way it actually loads all these\ndifferent columns that we loaded into\nfeatures so if we did just features let\nme just do features in here seeing what\nfeatures looks like\nthis is just playing with the with\npandas data frames you'll see that it's\nan index so when you put an index in\nlike this\ninto\ntest features into test\nit then takes those columns and creates\na panda data frames from those columns\nand in this case\nwe're going to go ahead and put those\ninto our predict\nso we're going to put each one of these\nlines of data\nthe 5.0 3.4 1.5 0.2 and we're going to\nput those in and we're going to predict\nwhat our new\nforest classifier is going to come up\nwith and this is what it predicts it\npredicts 0 0 0 1 2 one one two two two\nand and uh again this is the\nflower type setosa virginica and\nversacolor so now that we've taken our\ntest features\nlet's explore that let's see exactly\nwhat that data means to us so the first\nthing we can do with our predicts is we\ncan actually generate a different\nprediction model when i say different\nwe're going to view it differently it's\nnot that the data itself is different so\nlet's take this next piece of code and\nput it into our script\nso we're pasting it in here and you'll\nsee that we're doing uh predict and\nwe've added underscore proba for\nprobability so there's our clf.predict\nprobability so we're running it just\nlike we ran it up here but this time\nwith this we're going to get a slightly\ndifferent result and we're only going to\nlook at the first 10.\nso you'll see down here instead of\nlooking at all of them\nwhich was what 27 you'll see right down\nhere\nthat this generates a much larger field\non the probability and let's take a look\nand see what that looks like\nand what that means\nso when we do the predict underscore\nprabha\nfor probability it generates three\nnumbers so we had three leaf nodes at\nthe end and if you remember from all the\ntheory we did\nthis is the predictors the first one is\npredicting a one\nfor setosa it predicts a 0 for virginica\nand it predicts a 0 for versa color and\nso on and so on and so on and let's um\nyou know what i'm going to change this\njust a little bit let's look at 10\nto 20 just because we\ncan we start to get a little different\nof data and you'll see right down here\nit gets to this one this line right here\nand this line has 0 0.5 0.5\nand so if we're going to vote and we\nhave two equal votes it's gonna go with\nthe first one so it says uh setosa gets\nzero votes virginica gets point five\nvotes versacolor gets point five votes\nbut let's just go with the virginica\nsince these two are equal and so on and\nso on down the list you can see how they\nvary on here so now we've looked at both\nhow to do a basic predict of the\nfeatures and we've looked at the predict\nprobability\nlet's see what's next on here so now we\nwant to go ahead and start mapping names\nfor the plants we want to attach names\nso that it makes a little more sense for\nus and this we're going to do in these\nnext two steps we're going to start by\nsetting up our predictions and mapping\nthem to the name so let's see what that\nlooks like and let's go ahead and paste\nthat code in here and run it and this\ngoes along with the next piece of code\nso we'll skip through this quickly and\nthen come back to a little bit so here's\niris\ndot target names\nand uh if you remember correctly this\nwas the the names that we've been\ntalking about this whole time the setosa\nvirginica versus color and then we're\ngoing to go ahead and do the prediction\nagain we've run we could have just set a\nvariable equal to this instead of\nre-running it each time but we're going\nto run it again clf dot predict test\nfeatures remember that returns the zeros\nthe ones and the twos and then we're\ngoing to set that equal to predictions\nso this time we're actually putting it\nin a variable and when i run this\nit distributes it it comes out as an\narray and the array is setosa satosa\nsatosa satosa setosa we're only looking\nat the first five we could actually do\nlet's do the first 25 just so we can see\na little bit more on there and you'll\nsee that it starts mapping it to all the\ndifferent flower types the versa color\nand the virginica in there and let's see\nhow this goes with the next one so let's\ntake a look at the top part of our\nspecies in here and we'll take this code\nand put it in our script\nand let's put that down here and paste\nit there we go and we'll go ahead and\nrun it\nand let's talk about both these sections\nof code here\nand how they go together\nthe first one is our predictions and i\nwent ahead and did predictions through\n25 let's just do five\nand so we have setosa satoshi satosa\nsatoshi that's what we're predicting\nfrom our test model\nand then we come down here we look at\ntest species i remember i could have\njust done test dot species dot head and\nyou'll see it says setosa satosa setosa\nsetosa and they match\nso the first one is what our\nforest is doing\nand the second one is what the actual\ndata is now is we need to combine these\nso that we can understand what that\nmeans we need to know how good our\nforest is how good it is at predicting\nthe features so that's where we come up\nto the next step which is lots of fun\nwe're going to use a single line of code\nto combine our predictions and our\nactuals so we have a nice chart to look\nat and let's go ahead and put that in\nour script in our jupiter notebook here\nlet's see let's go ahead and paste that\nin and then i'm gonna because i'm on the\njupiter notebook i can do a control\nminus so you can see the whole line\nthere\nthere we go resize it\nand let's take a look and see what's\ngoing on here we're gonna create in\npandas remember pd stands for pandas and\nwe're doing a cross tab this function\ntakes two sets of data\nand creates a chart out of them so when\ni run it you'll get a nice chart down\nhere\nand we have the predicted species\nso across the top you'll see the setosa\nversus color virginica and the actual\nspecies setosa versacolor virginica and\nso the way to read this chart and let's\ngo ahead and take a look on how to read\nthis chart here when you read this chart\nyou have setosa where they meet you have\nversus color where they meet and you\nhave virginica where they meet and\nthey're meeting where the actual and the\npredicted agree\nso this is the number of accurate\npredictions so in this case it equals\n30. if you had 13 plus 5 plus 12 you get\n30. and then we notice here where it\nsays virginica but it was supposed to be\nversacolor this is inaccurate so now we\nhave two two inaccurate predictions and\n30 accurate predictions so we'll say\nthat the model accuracy is 93 that's\njust 30 divided by 32 and if we multiply\nit by a hundred we can say that it is 93\npercent accurate so we have a 93 percent\naccuracy with our model i did want to\nadd one more quick thing in here on our\nscripting before we wrap it up so let's\nflip back on over to my script in here\nwe're going to take this line of code\nfrom up above i don't know if you\nremember it but predix equals the iris\ndot target underscore names\nso we're going to map it to the names\nand we're going to run the prediction\nand we read it on test features but you\nknow we're not just testing it we want\nto actually deploy it so at this point i\nwould go ahead and change this\nand this is an array of arrays this is\nreally important when you're running\nthese to know that\nso you need the double brackets and i\ncould actually create data maybe let's\njust do two flowers so maybe i'm\nprocessing more data coming in and we'll\nput two flowers in here\nand then i actually want to see what the\nanswer are is so let's go ahead and type\nin preds and print that out and when i\nrun this\nyou'll see that i've now predicted two\nflowers that maybe i measured in my\nfront yard as versacolor and versacolor\nnot surprising since i put the same data\nin for each one\nthis would be the actual\nend product going out to be used on data\nthat you don't know the answer for\nso that's going to conclude our\nscripting part of this today we're going\nto cover the k nearest neighbors let's\nrefer to as k n n and k n n is really a\nfundamental place to start in the\nmachine learning it's the basis of a lot\nof other things and just the logic\nbehind it is easy to understand and\nincorporated in other forms of machine\nlearning so today what's in it for you\nwhy do we need k n n\nwhat is k n\nhow do we choose the factor k\nwhen do we use k n n\nhow does k n algorithm work and then\nwe'll dive in to my favorite part the\nuse case predict whether a person will\nhave diabetes or not that is a very\ncommon and popular used data set as far\nas testing out models and learning how\nto use the different models in machine\nlearning by now we all know machine\nlearning models make predictions by\nlearning from the past data available so\nwe have our input values our machine\nlearning model builds on those inputs of\nwhat we already know and then we use\nthat to create a predicted output\nis that a dog little kid looking over\nthere watching the black cat cross their\npath no dear you can differentiate\nbetween a cat and a dog based on their\ncharacteristics\ncats\ncats have sharp claws uses to climb\nsmaller lengths of ears meows and purrs\ndoesn't love to play around dogs they\nhave dull claws bigger length of ears\nbarks loves to run around you usually\ndon't see a cat running around people\nalthough i do have a cat that does that\nwhere dogs do and we can look at these\nwe can say we can evaluate the sharpness\nof the claws how sharper their claws\nand we can evaluate the length of the\nears and we can usually sort out cats\nfrom dogs based on even those two\ncharacteristics\nnow tell me if it is a cat or a dog not\na question usually little kids know cats\nand dogs by now\nunless they live a place where there's\nnot many cats or dogs so if we look at\nthe sharpness of the claws the length of\nthe ears and we can see that the cat has\nsmaller ears and sharper claws than the\nother animals its features are more like\ncats it must be a cat\nsharp claws length of ears and it goes\nin the cat group because knn is based on\nfeature similarity we can do\nclassification using knn classifier so\nwe have our input value the picture of\nthe black cat it goes into our trained\nmodel and it predicts that this is a cat\ncoming out so what is knn what is the k\nn algorithm\nk nearest neighbors is what that stands\nfor it's one of the simplest supervised\nmachine learning algorithms mostly used\nfor classification so we want to know is\nthis a dog or it's not a dog is it a cat\nor not a cat it classifies a data point\nbased on how its neighbors are\nclassified knn stores all available\ncases and classifies new cases based on\na similarity measure and here we gone\nfrom cats and dogs right into wine\nanother favorite of mine k n stores all\navailable cases and classifies new cases\nbased on a similarity measure and here\nyou see we have a measurement of sulfur\ndioxide versus the chloride level and\nthen the different wines they've tested\nand where they fall on that graph based\non how much sulfur dioxide and how much\nchloride k and k n is a perimeter that\nrefers to the number of nearest\nneighbors to include in the majority of\nthe voting process and so if we add a\nnew glass of wine there red or white we\nwant to know what the neighbors are in\nthis case we're going to put k equals 5.\nwe'll talk about k in just a minute a\ndata point is classified by the majority\nof votes from its five nearest neighbors\nhere the unknown point would be\nclassified as red since four out of five\nneighbors are red so how do we choose k\nhow do we know k equals 5 i mean that's\nwhat's the value we put in there so\nwe're going to talk about it how do we\nchoose a factor k k n algorithm is based\non feature similarity choosing the right\nvalue of k\nis a process called parameter tuning and\nis important for better accuracy so at k\nequals three we can classify we have a\nquestion mark in the middle as either a\nas a square or not is it a square or is\nit in this case a triangle and so if we\nset k equals to three we're going to\nlook at the three nearest neighbors\nwe're going to say this is a square and\nif we put k equals to 7 we classify as a\ntriangle depending on what the other\ndata is around and you can see as the k\nchanges depending on where that point is\nthat drastically changes your answer and\nwe jump here we go how do we choose the\nfactor of k you'll find this in all\nmachine learning choosing these factors\nthat's the face you get he's like oh my\ngosh did i choose the right k did i set\nit right my values in whatever machine\nlearning tool you're looking at so that\nyou don't have a huge bias in one\ndirection or the other and in terms of k\nn n the number of k if you choose it too\nlow the bias is based on it's just two\nnoises it's right next to a couple\nthings and it's going to pick those\nthings and you might get a skewed answer\nand if your k is too big then it's going\nto take forever to process so you're\ngoing to run into processing issues and\nresource issues so what we do the most\ncommon use and there's other options for\nchoosing k is to use the square root of\nn so it is a total number of values you\nhave you take the square root of it in\nmost cases you also if it's an even\nnumber so if you're using uh like this\ncase squares and triangles if it's even\nyou want to make your k value odd that\nhelps it select better so in other words\nyou're not going to have a balance\nbetween two different factors that are\nequal so usually take the square root of\nn and if it's even you add one to it or\nsubtract one from it and that's where\nyou get the k value from that is the\nmost common use and it's pretty solid it\nworks very well when do we use knn we\ncan use knn when data is labeled so you\nneed a label on it we know we have a\ngroup of pictures with dogs dogs cats\ncats data is noise free and so you can\nsee here when we have a class and we\nhave like underweight 140 23 hello kitty\nnormal that's pretty confusing we have a\nhigh variety of data coming in so it's\nvery noisy and that would cause an issue\ndata set is small so we're usually\nworking with smaller data sets where you\nmight get into a gig of data if it's\nreally clean doesn't have a lot of noise\nbecause k n is a lazy learner i.e it\ndoesn't learn a discriminative function\nfrom the training set so it's very lazy\nso if you have very complicated data and\nyou have a large amount of it you're not\ngoing to use the knn but it's really\ngreat to get a place to start even with\nlarge data you can sort out a small\nsample and get an idea of what that\nlooks like using the knn and also just\nusing for smaller data sets k n works\nreally good how does a k n algorithm\nwork consider a data set having two\nvariables height in centimeters and\nweight in kilograms and each point is\nclassified as normal or underweight so\nwe see right here we have two variables\nyou know true false they're either\nnormal or they're not they're\nunderweight on the basis of the given\ndata we have to classify the below set\nas normal or underweight using knn so if\nwe have new data coming in this says 57\nkilograms and 177 centimeters is that\ngoing to be normal or underweight to\nfind the nearest neighbors we'll\ncalculate the euclidean distance\naccording to the euclidean distance\nformula the distance between two points\nin the plane with the coordinates x y\nand a b is given by distance d equals\nthe square root of x minus a squared\nplus y minus b squared and you can\nremember that from the two edges of a\ntriangle we're computing the third edge\nsince we know\nthe x side and the y side let's\ncalculate it to understand clearly so we\nhave our unknown point and we placed it\nthere in red and we have our other\npoints where the data is scattered\naround the distance d1 is the square\nroot of 170 minus 167 squared plus 57\nminus 51 squared which is about 6.7 and\ndistance 2 is about 13. and distance 3\nis about 13.4 similarly we will\ncalculate the euclidean distance of\nunknown data point from all the points\nin the data set and because we're\ndealing with small amount of data that's\nnot that hard to do it's actually pretty\nquick for a computer and it's not a\nreally complicated mass you can just see\nhow close is the data based on the\neuclidean distance hence we have\ncalculated the euclidean distance of\nunknown data point from all the points\nas shown where x1 and y1 equal 57 and\n170 whose class we have to classify so\nnow we're looking at that we're saying\nwell here's the euclidean distance who's\ngoing to be their closest neighbors now\nlet's calculate the nearest neighbor at\nk equals three and we can see the three\nclosest neighbors puts them at normal\nand that's pretty self-evident when you\nlook at this graph it's pretty easy to\nsay okay what we're just voting normal\nnormal normal three votes for normal\nthis is going to be a normal weight so\nmajority of neighbors are pointing\ntowards normal hence as per k n\nalgorithm the class of 57 170 should be\nnormal so a recap of knn positive\ninteger k is specified along with a new\nsample we select the k entries in our\ndatabase which are closest to the new\nsample we find the most common\nclassification of these entries this is\nthe classification we give to the new\nsample so as you can see it's pretty\nstraightforward we're just looking for\nthe closest things that match what we\ngot so let's take a look and see what\nthat looks like in a use case in python\nso let's dive into the predict diabetes\nuse case so use case predict diabetes\nthe objective predict whether a person\nwill be diagnosed with diabetes or not\nwe have a data set of 768 people who\nwere or were not diagnosed with diabetes\nand let's go ahead and open that file\nand just take a look at that data and\nthis is in a simple spreadsheet format\nthe data itself is comma separated very\ncommon set of data and it's also a very\ncommon way to get the data and you can\nsee here we have columns a through i\nthat's what one two three four five six\nseven eight\neight columns with a particular\nattribute and then the ninth column\nwhich is the outcome is whether they\nhave diabetes as a data scientist the\nfirst thing you should be looking at is\ninsulin well you know if someone has\ninsulin they have diabetes that's why\nthey're taking it and that could cause\nissue in some of the machine learning\npackages but for a very basic setup this\nworks fine for doing the knn and the\nnext thing you notice is it didn't take\nvery much to open it up i can scroll\ndown to the bottom of the data there's\n768.\nit's pretty much a small data set you\nknow at 769 i can easily fit this into\nmy ram on my computer i can look at it i\ncan manipulate it and it's not going to\nreally tax just a regular desktop\ncomputer you don't even need an\nenterprise version to run a lot of this\nso let's start with importing all the\ntools we need and before that of course\nwe need to discuss what ide i'm using\ncertainly you can use any particular\neditor for python but i like to use for\ndoing uh very basic visual stuff the\nanaconda which is great for doing demos\nwith the jupiter notebook and just a\nquick view of the anaconda navigator\nwhich is the new release out there which\nis really nice you can see under home i\ncan choose my application we're going to\nbe using python36 i have a couple\ndifferent versions on this particular\nmachine if i go under environments i can\ncreate a unique environment for each one\nwhich is nice and there's even a little\nbutton there where i can install\ndifferent packages so if i click on that\nbutton and open the terminal i can use a\nsimple pip install to install different\npackages i'm working with let's go ahead\nand go back under home and we're going\nto launch our notebook and i've already\nyou know kind of like\nthe old cooking shows i've already\nprepared a lot of my stuff so we don't\nhave to wait for it to launch because it\ntakes a few minutes for it to open up a\nbrowser window in this case it's going\nto open up chrome because that's my\ndefault that i use and since the script\nis pre-done you'll see i have a number\nof windows open up at the top the one\nwe're working in and since we're working\non the k n predict whether a person will\nhave diabetes or not let's go and put\nthat title in there\nand i'm also going to go up here and\nclick on cell actually we want to go\nahead and first insert a cell below and\nthen i'm going to go back up to the top\ncell and i'm going to change the cell\ntype to markdown that means this is not\ngoing to run as python it's a markdown\nlanguage so if i run this first one it\ncomes up in nice big letters which is\nkind of nice remind us what we're\nworking on and by now you should be\nfamiliar with doing all of our imports\nwe're going to import the pandas as pd\nimport numpy is np pandas is the pandas\ndata frame and numpy is a number array\nvery powerful tools to use in here so we\nhave our imports so we've brought in our\npandas our numpy our two general python\ntools and then you can see over here we\nhave our trained test split by now you\nshould be familiar with splitting the\ndata\nwe want to split part of it for training\nour thing and then training our\nparticular model and then we want to go\nahead and test the remaining data to see\nhow good it is pre-processing a standard\nscalar preprocessor so we don't have a\nbias of really large numbers remember in\nthe data we had like number of\npregnancies isn't going to get very\nlarge where the amount of insulin they\ntake can get up to 256 so 256 versus 6\nthat will skew results so we want to go\nahead and change that so they're all\nuniform between minus 1 and 1. and then\nthe actual tool this is the k neighbors\nclassifier we're going to use\nand finally the last three are three\ntools to test all about testing our\nmodel how good is it we just put down\ntest on there and we have our confusion\nmatrix our f1 score and our accuracy so\nwe have our two general python modules\nwe're importing and then we have our six\nmodules specific from the sk learn setup\nand then we do need to go ahead and run\nthis so these are actually imported\nthere we go and then move on to the next\nstep and so in this set we're going to\ngo ahead and load the database we're\ngoing to use pandas remember pandas is\npd and we'll take a look at the data in\npython we looked at it in a simple\nspreadsheet but usually i like to also\npull it up so we can see what we're\ndoing so here's our data set equals\npd.read csv that's a pandas command and\nthe diabetes folder i just put in the\nsame folder where my ipython script is\nif you put in a different folder you\nneed the full length on there we can\nalso do a quick links of the data set\nthat is a simple python command len for\nlength we might even let's go ahead and\nprint that we'll go print and if you do\nit on its own line link that data set\nthe jupyter notebook it'll automatically\nprint it but when you're in most of your\ndifferent setups you want to do the\nprint in front of there and then we want\nto take a look at the actual data set\nand since we're in pandas we can simply\ndo data set head and again let's go\nahead and add the print in there\nif you put a bunch of these in a row you\nknow the data set one head data set two\nhead it only prints out the last one so\ni use i always like to keep the print\nstatement in there but because most\nprojects only use one data frame pandas\ndata frame doing it this way doesn't\nreally matter the other way works just\nfine and you can see when we hit the run\nbutton we have the 768 lines which we\nknew and we have our pregnancies it's\nautomatically given a label on the left\nremember the head only shows the first\nfive lines so we have zero through four\nand just a quick look at the data you\ncan see it matches what we looked at\nbefore we have pregnancy glucose blood\npressure all the way to age and then the\noutcome on the end and we're going to do\na couple things in this next step we're\ngoing to create a list of columns where\nwe can't have zero there's no such thing\nas zero skin thickness or zero blood\npressure zero glucose uh any of those\nyou'd be dead so not a really good\nfactor if they don't if they have a zero\nin there because they didn't have the\ndata and we'll take a look at that\nbecause we're going to start replacing\nthat information with a couple of\ndifferent things and let's see what that\nlooks like so first we create a nice\nlist as you can see we have the values\ntalked about glucose blood pressure skin\nthickness and this is a nice way when\nyou're working with columns is to list\nthe columns you need to do some kind of\ntransformation on a very common thing to\ndo and then for this particular setup we\ncertainly could use the there's some\npanda tools that will do a lot of this\nwhere we can replace the n a but we're\ngoing to go ahead and do it as a data\nset column equals dataset column.replace\nthis is this is still pandas you can do\na direct there's also one that's that\nyou look for your nan a lot of different\noptions in here but the nan in numpy nan\nis what that stands for is none it\ndoesn't exist so the first thing we're\ndoing here is we're replacing the zero\nwith a numpy none there's no data there\nthat's what that says that's what this\nis saying right here so put the zero in\nand we're going to play zeros with no\ndata so if it's a zero that means the\nperson's well hopefully not dead hope\nthey just didn't get the data the next\nthing we want to do is we're going to\ncreate the mean which is the integer\nfrom the data set from the column dot\nmean where we skip n a's we can do that\nthat is a pandas command there the skip\nn a so we're going to figure out the\nmean of that data set and then we're\ngoing to take that data set column and\nwe're going to replace all the n p n a n\nwith the means why did we do that and we\ncould have actually just taken this step\nand gone right down here and just\nreplaced zero and skip anything were\nexcept you could actually there's a way\nto skip zeros and then just replace all\nthe zeros but in this case we want to go\nahead and do it this way so you could\nsee that we're switching this to a\nnon-existent value and we're going to\ncreate the mean well this is the average\nperson so if we don't know what it is if\nthey did not get the data and the data\nis missing one of the tricks is you\nreplace it with the average what is the\nmost common data for that this way you\ncan still use the rest of those values\nto do your computation and it kind of\njust brings that particular value of\nthose missing values out of the equation\nlet's go ahead and take this and we'll\ngo ahead and run it doesn't actually do\nanything so we're still preparing our\ndata if you want to see what that looks\nlike we don't have anything in the first\nfew lines so it's not going to show up\nbut we certainly could look at a row\nlet's do that let's go into our data set\nwith printed data set\nand let's pick in this case let's just\ndo\nglucose and if i run this this is going\nto print all the different glucose\nlevels going down and we thankfully\ndon't see anything in here that looks\nlike missing data at least on the ones\nit shows you can see it skipped a bunch\nin the middle that's what it does if you\nhave too many lines in jupiter notebook\nit'll skip a few and and go on to the\nnext in a data set let me go and remove\nthis and we'll just zero out that\nand of course before we do any\nprocessing before proceeding any further\nwe need to split the data set into our\ntrain and testing data that way we have\nsomething to train it with and something\nto test it on and you're going to notice\nwe did a little something here with the\npandas database code there we go my\ndrawing tool we've added in this right\nhere\nof the data set and what this says is\nthat the first one in pandas this is\nfrom the pd pandas it's going to say\nwithin the data set we want to look at\nthe eye location and it is all rows\nthat's what that says so we're going to\nkeep all the rows but we're only looking\nat 0 column 0 to 8. remember column\nthis is actually 0 to 7 it doesn't\ninclude the last one and then we go down\nhere to y which is our answer and we\nwant just the last one just column eight\nand you can do it this way with this\nparticular notation and then if you\nremember we imported the train test\nsplit that's part of the sk learn right\nthere and we simply put in our x and our\ny\nwe're going to do random state equals\nzero you don't have to necessarily seat\nit that's a seed number i think the\ndefault is one when you seed it i'd have\nto look that up and then the test size\ntest size is 0.2 that simply means we're\ngoing to take 20 percent of the data and\nput it aside so that we can test it\nlater that's all that is and again we're\ngoing to run it not very exciting so far\nwe haven't had any printout other than\nto look at the data but that is a lot of\nthis is prepping this data once you prep\nit the actual lines of code are quick\nand easy and we're almost there with the\nactual writing of our knn we need to go\nahead and do a scale the data if you\nremember correctly we're fitting the\ndata in a standard scalar which means\ninstead of the data being from you know\nfive to 303 in one column and the next\ncolumn is one to six we're going to set\nthat all so that all the data is between\nminus one and one that's what that\nstandard scalar does keeps it\nstandardized and we only want to fit the\nscalar with the training set but we want\nto make sure the testing set is the x\ntest going in is also transformed so\nit's processing it the same so here we\ngo with our standard scalar we're going\nto call it sc underscore x for the\nscalar and we're going to import the\nstandard scalar into this variable and\nthen our x train equals sc underscore x\ndot fit transform so we're creating the\nscalar on the x train variable and then\nour x test we're also going to transform\nit so\nwe've trained and transformed the x\ntrain and then the x test\nisn't part of that training it isn't\npart of that of training the transformer\nit just gets transformed that's all it\ndoes and again we're going to and run\nthis and if you look at this we've now\ngone through these steps all three of\nthem we've taken care of replacing our\nzeros for key\ncolumns that shouldn't be zero and we\nreplace that with the means of those\ncolumns that way that they fit right in\nwith our data models we've come down\nhere we split the data so now we have\nour test data and our training data and\nthen we've taken and we scaled the data\nso all of our data going in no no we\ndon't tr we don't train the y part the y\ntrain and y test that never has to be\ntrained it's only the data going in\nthat's what we want to train in there\nthen define the model using k neighbors\nclassifier and fit the train data in the\nmodel so we do all that data prep and\nyou can see down here we're only going\nto have a couple lines of code where\nwe're actually building our model and\ntraining it that's one of the cool\nthings about python and how far we've\ncome it's such an exciting time to be in\nmachine learning because there's so many\nautomated tools let's see before we do\nthis let's do a quick length of and\nlet's do y we want let's just do length\nof y\nand we get 768 and if we import math we\ndo math dot square root let's do y\ntrain there we go it's actually supposed\nto be x train\nbefore we do this let's go ahead and do\nimport math and do math square root\nlength of y test and when i run that we\nget 12.409\ni want to show you where this number\ncomes from we're about to use 12 is an\neven number so if you know if you're\never voting on things remember the\nneighbors all vote don't want to have an\neven number of neighbors voting so we\nwant to do something odd and let's just\ntake one away we'll make it 11. let me\ndelete this out of here that's one of\nthe reasons i love jupiter notebook is\nyou can flip around and do all kinds of\nthings on the fly so we'll go ahead and\nput in our classifier we're creating our\nclassifier now and it's going to be the\nk neighbors classifier n neighbors equal\n11. remember we did 12 minus 1 for 11 so\nwe have an odd number of neighbors p\nequals 2 because we're looking for is it\nare they diabetic or not and we're using\nthe euclidean metric there are other\nmeans of measuring the distance you\ncould do like square square means values\nall kinds of measure this but the\neuclidean is the most common one and it\nworks quite well it's important to\nevaluate the model let's use the\nconfusion matrix to do that and we're\ngoing to use the confusion matrix\nwonderful tool and then we'll jump into\nthe f1 score\nand finally accuracy score which is\nprobably the most commonly used quoted\nnumber when you go into a meeting or\nsomething like that so let's go ahead\nand paste that in there and we'll set\nthe cm equal to confusion matrix why\ntest y predict so those are the two\nvalues we're going to put in there and\nlet me go ahead and run that and print\nit out and the way you interpret this is\nyou have the y predicted which would be\nyour title up here you could do let's\njust do p r e d\npredicted across the top\nand actual going down actually it's\nalways hard to to write in here actual\nthat means that this column here down\nthe middle that's the important column\nand it means that our prediction said 94\nand prediction in the actual agreed on\n94 and 32. this number here\nthe 13 and the 15 those are what was\nwrong so you could have like three\ndifferent if you're looking at this\nacross three different variables instead\nof just two you'd end up with the third\nrow down here in the column going down\nthe middle so in the first case we have\nthe the and i believe the zero has a 94\npeople who don't have diabetes the\nprediction said that 13 of those people\ndid have diabetes and were at high risk\nand the 32 that had diabetes it had\ncorrect\nbut our prediction said another 15 out\nof that 15 it classified as incorrect so\nyou can see where that classification\ncomes in and how that works on the\nconfusion matrix then we're going to go\nahead and print the f1 score let me just\nrun that and you see we get a 0.69 in\nour f1 score\nthe f1 takes into account both sides of\nthe balance of false positives where if\nwe go ahead and just do the accuracy\naccount and that's what most people\nthink of is it looks at just how many we\ngot right out of how many we got wrong\nso a lot of people when you're a data\nscientist and you're talking to other\ndata scientists they're going to ask you\nwhat the f1 score the f score is if\nyou're talking to the general public or\nthe decision makers in the business\nthey're going to ask what the accuracy\nis and the accuracy is always better\nthan the f1 score but the f1 score is\nmore telling it lets us know that\nthere's more false positives than we\nwould like on here but 82 percent not\ntoo bad for a quick flash look at\npeople's different statistics in running\nan sk learn and running the k n n the k\nnearest neighbor on it so we have\ncreated a model using knn which can\npredict whether a person will have\ndiabetes or not or at the very least\nwhether they should go get a checkup and\nhave their glucose checked regularly or\nnot the print accuracy score we got the\n0.818 was pretty close to what we got\nand we can pretty much round that off\nand just say we have an accuracy of 80\ntells us it is a pretty fair fit in the\nmodel so far so clear but a question\nshould be coming up we have our sample\ndata set but instead of looking like\nthis what if it looked like this where\nwe have two sets of data but one of them\noccurs in the middle of another set you\ncan see here where we have the blue and\nthe yellow and then blue again on the\nother side of our data line in this data\nset we can't use a hyperplane so when\nyou see data like this it's necessary to\nmove away from a 1d view of the data to\na two-dimensional view of the data and\nfor the transformation we use what's\ncalled a kernel function the kernel\nfunction will take the 1d input and\ntransfer it to a two-dimensional output\nas you can see in this picture here the\n1d when transferred to a 2-dimensional\nmakes it very easy to draw a line\nbetween the two data sets what if we\nmake it even more complicated how do we\nperform an svm for this type of data set\nhere you can see we have a two\ndimensional data set where the data is\nin the middle surrounded by the green\ndata on the outside in this case we're\ngoing to segregate the two classes we\nhave our sample data set and if you draw\na line through it's obviously not an\noptimal hyperplane in there so to do\nthat we need to transfer the 2d to a 3d\narray and when you translate it into a\nthree-dimensional array using the kernel\nyou can see where you can place a\nhyperplane right through it and easily\nsplit the data before we start looking\nat a programming example and dive into\nthe script let's look at the advantage\nof the support vector machine we'll\nstart with high dimensional input space\nor sometimes referred to as the curse of\ndimensionality we looked at earlier one\ndimension two dimension three dimension\nwhen you get to a thousand dimensions a\nlot of problems start occurring with\nmost algorithms that have to be adjusted\nfor the svm automatically does that in\nhigh dimensional space one of the high\ndimensional space one high dimensional\nspace that we work on is sparse document\nvectors this is where we tokenize the\nwords and documents so we can run our\nmachine learning algorithms over them\ni've seen ones get as high as 2.4\nmillion different tokens that's a lot of\nvectors to look at and finally we have\nregularization parameter the realization\nparameter or lambda is a parameter that\nhelps figure out whether we're going to\nhave a bias or overfitting of the data\nwhether it's going to be overfitted to a\nvery specific instance or is going to be\nbiased to a high or low value with the\nsvm it naturally avoids the overfitting\nand bias problems that we see in many\nother algorithms these three advantages\nof the support vector machine make it a\nvery powerful tool to add to your\nrepertoire of machine learning tools now\nwe did promise you a used case study\nwe're actually going to dive into some\npython programming and so we're going to\ngo into a problem statement and start\noff with the zoo so in the zoo example\nwe have family members going to the zoo\nwe have the young child going dead is\nthat a group of crocodiles or alligators\nwell that's hard to differentiate and\nzoos are a great place to start looking\nat science and understanding how things\nwork especially as a young child and so\nwe can see the parents sitting here\nthinking well what is the difference\nbetween a crocodile and an alligator\nwell one crocodiles are larger in size\nalligators are smaller in size snout\nwidth the crocodiles have a narrow snout\nand alligators have a wider snout and of\ncourse in the modern day and age the\nfather is sitting here thinking how can\ni turn this into a lesson for my son and\nhe goes let a support vector machine\nsegregate the two groups i don't know if\nmy dad ever told me that but that would\nbe funny now in this example we're not\ngoing to use actual measurements and\ndata we're just using that for imagery\nand that's very common in a lot of\nmachine learning algorithms and setting\nthem up but let's roll up our sleeves\nand we'll talk about that more in just a\nmoment as we break into our python\nscript\nso here we arrive in our actual coding\nand i'm going to move this into a python\neditor in just a moment but let's talk a\nlittle bit about what we're going to\ncover first we're going to cover in the\ncode the setup how to actually create\nour svm and you're going to find that\nthere's only two lines of code that\nactually create it and the rest of it is\ndone so quick and fast that it's all\nhere in the first page and we'll show\nyou what that looks like as far as our\ndata because we're going to create some\ndata i talked about creating data just a\nminute ago and so we'll get into the\ncreating data here and you'll see this\nnice correction of our two blobs and\nwe'll go through that in just a second\nand then the second part is we're going\nto take this and we're going to bump it\nup a notch we're going to show you what\nit looks like behind the scenes but\nlet's start with actually creating our\nsetup i like to use the anaconda jupiter\nnotebook because it's very easy to use\nbut you can use any of your favorite\npython editors or setups and go in there\nbut let's go ahead and switch over there\nand see what that looks like so here we\nare in the anaconda\npython notebook or anaconda jupiter\nnotebook with python we're using python\n3. i believe this is 3.5 but it should\nbe work in any of your 3x versions and\nyou'd have to look at the sklearn and\nmake sure if you're using a 2x version\nan earlier version let's go and put our\ncode in there and one of the things i\nlike about the jupiter notebook is i go\nup to view and i'm going to go ahead and\ntoggle the line numbers on to make it a\nlittle bit easier to talk about and we\ncan even increase the size because this\nis edited in this case i'm using google\nchrome explorer and that's how it opens\nup for the editor although anyone any\nlike i said any editor will work now the\nfirst step is going to be our imports\nand we're going to import four different\nparts the first two i want you to look\nat are line one and line two are numpy\nas np and matplot library dot pi plot as\nplt now these are very standardized\nimports when you're doing work the first\none is the numbers python we need that\nbecause part of the platform we're using\nuses that for the numpy array and i'll\ntalk about that in a minute so you can\nunderstand why we want to use a numpy\narray versus the standard python array\nand normally it's pretty standard setup\nto use np for numpy the map plot library\nis how we're going to view our data so\nthis says you do need the np for the sk\nlearn module but the map plot library is\npurely for our use for visualization and\nso you really don't need that for the\nsvm but we're gonna put it there so you\nhave a nice visual aid and we can show\nyou what it looks like that's really\nimportant at the end when you finish\neverything so you have a nice display\nfor everybody to look at and then\nfinally we're gonna i'm gonna jump one\nahead to line number four that's the\nsklearn.dataset\nimport make blobs and i told you that we\nwere going to make up data and this is a\ntool that's in the sk learning to make\nup data i personally don't want to go to\nthe zoo get in trouble for jumping over\nthe fence and probably get eaten by the\ncrocodiles or alligators as i work on\nmeasuring their snouts and width and\nlength\ninstead we're just going to make up some\ndata and that's what that make blobs is\nit's a wonderful tool if you're ready to\ntest your setup and you're not sure\nabout what data you're going to put in\nthere you can create this blob and it\nmakes it real easy to use and finally we\nhave our actual svm the sklearn import\nsvm on line three so that covers all our\nimports we're going to create remember i\nused to make blobs to create data and\nwe're going to create a capital x and a\nlowercase y equals make blobs in samples\nequals 40. so we're going to make 40\nlines of data it's going to have two\ncenters with a random state equals 20.\nso each\neach group is going to have 20 different\npieces of data in it and the way that\nlooks is that we'll have under x\nan x y plane so i have two numbers under\nx and y will be 0 1 that's the two\ndifferent centers so we have yes or no\nin this case alligator or crocodile\nthat's what that represents and then i\ntold you that the actual sk learner the\nsvm is in two lines of code and we see\nit right here with clf equals svm dot\nsvc kernel equals linear and i set sql\nto 1 although in this example since we\nare not regularizing the data because we\nwant to be very clear and easy to see i\nwent ahead you can set it to a thousand\na lot of times when you're not doing\nthat but for this thing linear because\nit's a very simple linear example we\nonly have the two dimensions and it'll\nbe a nice linear hyperplane will be a\nnice linear line instead of a full plane\nso we're not dealing with a huge amount\nof data and then all we have to do is do\nclf dot fit x comma y and that's it clf\nhas been created and then we're going to\ngo ahead and display it and i'm going to\ntalk about this display here in just a\nsecond but let me go ahead and run this\ncode and this is what we've done is\nwe've created two blobs you'll see the\nblue on the side and then kind of an\norangish on the other side that's our\ntwo sets of data they represent one\nrepresents crocodiles and one represents\nalligators and then we have our\nmeasurements in this case we have like\nthe width and length of the snout and i\ndid say i was going to come up here and\ntalk just a little bit about our plot\nand you'll see plt that's what we\nimported we're going to do a scatter\nplot that means we're just putting dots\non there and then look at this notation\ni have the capital x and then in\nbrackets i have a colon comma 0. that's\nfrom numpy if you did that in a regular\narray you'll get an error in a python\narray you have to have that in a numpy\narray it turns out that our make blobs\nreturns a numpy array and this notation\nis great because what it means is the\nfirst part is the colon means we're\ngoing to do all the rows that's all the\ndata in our blob we created under\ncapital x and then the second part has a\ncomma 0. we're only going to take the\nfirst value and then if you notice we do\nthe same thing but we're going to take\nthe second value remember we always\nstart with zero and then one so we have\ncolumn zero and column one and you can\nlook at this as our x y plots the first\none is the x plot and the second one is\nthe y plot so the first one is on the\nbottom 0 2 4 6 8 and 10 and then the\nsecond one x of the one is the 4 5 6 7 8\n9 10 going up the left hand side s\nequals 30 is just the size of the dots\nwe can see them it says real tiny dots\nand then the c map equals plt.cm.paired\nand you'll also see the c equals y\nthat's the color we're using two colors\nzero one and that's why we get the nice\nblue and the two different colors for\nthe alligator and the crocodile now you\ncan see here that we did this the actual\nfit was done in two lines of code a lot\nof times will be a third line where we\nregularize the data we set it between\nlike minus one and one and we reshape it\nbut for this it's not necessary and it's\nalso kind of nice because you can\nactually see what's going on and then if\nwe wanted to we wanted to actually run a\nprediction let's take a look and see\nwhat that looks like and to predict some\nnew data and we'll show this again as we\nget towards the end of digging in deep\nyou can simply assign your new data in\nthis case i am giving it a width and\nlength 3 4 and a width and length 5 6.\nand note that i put the data as a set of\nbrackets and then i have the brackets\ninside and the reason i do that is\nbecause when we're looking at data it's\ndesigned to process a large amount of\ndata coming in we don't want to just\nprocess one line at a time and so in\nthis case i'm processing two lines and\nthen i'm just going to print and you'll\nsee clf dot predict new data so the clf\nand the dot predict part is going to\ngive us an answer and let's see what\nthat looks like and you'll see 0 1. so\npredicted the first one the 3 4 is going\nto be on the one side and the 5 6 is\ngoing to be on the other side so one\ncame out as an alligator and one came\nout as a crocodile now that's pretty\nshort explanation for the setup but\nreally we want to dug in and see what\nit's going on behind the scenes and\nlet's see what that looks like\nso the next step is to dig in deep and\nfind out what's going on behind the\nscenes and also put that in a nice\npretty graph\nwe're going to spend more work on this\nand we did actually generating the\noriginal model and you'll see here that\nwe go through a few steps and i'll move\nthis over to our editor in just a second\nwe come in we create our original data\nit's exactly identical to the first part\nand i'll explain why we redid that and\nshow you how not to redo that and then\nwe're going to go in there and add in\nthose lines we're going to see what\nthose lines look like and how to set\nthose up and finally we're going to plot\nall that on here and show it and you'll\nget a nice graph with the what we saw\nearlier when we were going through the\ntheory behind this where it shows the\nsupport vectors and the hyperplane and\nthose are done where you can see the\nsupport vectors as the dashed lines and\nthe solid line which is the hyperplane\nlet's get that into our jupiter notebook\nbefore i scroll down to a new line i\nwant you to notice line 13 has plot show\nand we're going to talk about that here\nin just a second but let's scroll down\nto a new line down here and i'm going to\npaste that code in and you'll see that\nthe plot show has moved down below let's\nscroll up a little bit and if you look\nat the top here of our new section one\ntwo three\nand four is the same code we had before\nand let's go back up here and take a\nlook at that we're going to fit the\nvalues on our svm and then we're going\nto plot scatter it and then we're going\nto do a plot show so you should be\nasking why are we redoing the same code\nwell when you do the plot show that\nblanks out what's in the plot so once\ni've done this plot show i have to\nreload that data now we could do this\nsimply by removing it up here re-running\nit and then coming down here and then we\nwouldn't have to rerun these first four\nlines of code now in this it doesn't\nmatter too much and you'll see the plot\nshow was down here and then removed\nright there on line five i'll go ahead\nand just delete that out of there\nbecause we don't want to blank out our\nscreen we want to move on to the next\nsetup so we can go ahead and just skip\nthe first four lines because we did that\nbefore and let's take a look at the ax\nequals plt.gca\nnow right now we're actually spending a\nlot of time just graphing that's all\nwe're doing here okay so this is how we\ndisplay a nice graph with our results in\nour data ax is very standard used\nvariable when you talk about plt and\nit's just setting it to that axis the\nlast axis in the plt it can get very\nconfusing if you're working with many\ndifferent layers of data on the same\ngraph and this makes it very easy to\nreference the ax so this reference is\nlooking at the plt that we created and\nwe already mapped out our two blobs on\nand then we want to know the limits so\nwe want to know how big the graph is we\ncan find out the x limit and the y limit\nsimply with the get x limit and get y\nlimit commands which is part of our met\nplot library and then we're going to\ncreate a grid and you'll see down here\nwe have we've set the variable xx equal\nto np.line space x limit 0 x limit 1\ncomma 30 and we've done the same thing\nfor the y space and then we're going to\ngo in here and we create a mesh grid and\nthis is a numpy command so we're back to\nour numbers python let's go through what\nthese numpy commands mean with the line\nspace in the mesh grid we've taken x x\nsmall x x equals np line space and we\nhave our x limit 0 and our x limit 1 and\nwe're going to create 30 points on it\nand we're going to do the same thing for\nthe y axis now this has nothing to do\nwith our evaluation it's all we're doing\nis we're creating a grid of data and so\nwe're creating a set of points between 0\nand the x limit we're creating 30 points\nand the same thing with the y and then\nthe mesh grid loops those all together\nso it forms a nice grid so if we were\ngoing to do this say between the limit 0\nand 10 and do 10 points we would have a\n0 0 1 1 0 1 0 2 0 3 0 4 to 10 and so on\nyou can just imagine a point at each\ncorner one of those boxes and the mesh\ngrid combines them all so we take the yy\nand the xx we created and creates the\nfull grid and we've set that grid into\nthe yy coordinates and the x coordinates\nnow remember we're working with numbi\nand python we like to separate those we\nlike to have instead of it being x comma\n1 you know x comma y\nand then x2 comma y2 and this in the\nnext set of data it would be a column of\nx's and a column of y's and that's what\nwe have here is we have a column of y's\nand we put it as a capital yy and a\ncolumn of x's capital x with all those\ndifferent points being listed and\nfinally we get down to the numpy v stack\njust as we created those in the mesh\ngrid we're now going to put them all\ninto one array x y array now that we've\ncreated the stack of data points we're\ngoing to do something interesting here\nwe're going to create a value z and the\nz equals the clf that's our that's our\nsupport vector machine we created and\nwe've already trained and we have a dot\ndecision function we're going to put the\nx y in there so here we have all this\ndata we're going to put that x y in\nthere that data and we're going to\nreshape it and you'll see that we have\nthe x x dot shape in here this literally\ntakes the xx resets it up connected to\nthe y and the z value lets us know\nwhether it is the left hand side is\ngoing to generate three different values\nthe z value does and it'll tell us\nwhether that data is a support vector to\nthe left the hyperplane in the middle or\nthe support vector to the right so it\ngenerates three different values for\neach of those points and those points\nhave been reshaped so they're right on a\nline on those three different lines so\nwe've set all of our data up we've\nlabeled it to three different areas and\nwe've reshaped it and we've just taken\n30 points in each direction if you do\nthe math you have 30 times 30 so it's\n900 points of data and we separated\nbetween the three lines and reshaped it\nto fit those three lines we can then go\nback to our map plot library we've\ncreated the ax and we're going to create\na contour and you'll see here we have\ncontour capital xx capital yy these have\nbeen reshaped to fit those lines z is\nthe labels so now we have the three\ndifferent points with the labels in\nthere and we can set the colors equals k\nand i told you we had three different\nlabels but we have three levels of data\nthe alphas just makes it kind of\nsee-through so it's only 0.5 of the\nvalue in there so when we graph it the\ndata will show up from behind it\nwherever the lines go and finally the\nline styles this is where we set the two\nsupport vectors to be dash dash lines\nand then a single one is just a straight\nline that's what all that setup does and\nthen finally we take our ax dot scatter\nwe're going to go ahead and plot the\nsupport vectors but we've programmed it\nin there so that they look nice like the\ndash dash line and the dashed line on\nthat grid and you can see here when we\ndo the clf dot support vectors we are\nlooking at column zero and column one\nand then again we have the s equals one\nhundred so we're gonna make them larger\nand the line width equals one face\ncolors equals none let's take a look and\nsee what that looks like when we show it\nand you can see we get down to our end\nresult it creates a really nice graph we\nhave our two support vectors and dashed\nlines and they have the near data so you\ncan see those two points or in this case\nthe four points where those lines nicely\ncleave the data and then you have your\nhyper plane down the middle which is as\nfar from the two different points as\npossible creating the maximum distance\nso you can see that we have our nice\noutput for the size of the body and the\nwidth of the snout and we've easily\nseparated the two groups of crocodile\nand alligator congratulations you've\ndone it we've made it of course these\nare pretend data for our crocodiles and\nalligators but this hands-on example\nwill help you to encounter any support\nvector machine projects in the future\nand you can see how easy they are to set\nup and look at in depth regularization\nin machine learning\nso our agenda on this one is fitting the\ndata\nunderstanding linear regression\nbias and variance\nwhat is overfitting\nwhat is underfitting and those are like\nthe biggest things right now in data\nscience is overfitting and underfitting\nwhat does that mean\nand what is regularization and then\nwe'll do a quick hands-on demo to take a\nlook at this\nso fitting the data let's start with\nfitting the data and we talk about\nwhat is data fitting it's a process of\nplotting a series of data points and\ndrawing the best fit line to understand\nthe relationship between the variables\nand this is what we called data fitting\nand you can see here we have a couple of\nlines we've drawn on this graph we're\ngoing to go in a little deeper on there\nso we might have in this case just the\ntwo dimensions we have an efficiency of\nthe car and we have the distance\ntraveled in 1000 kilometers\nand so what is data fitting well it's a\nlinear relationship and a linear\nrelationship\nvery specifically linear means line uh\nthe line used to represent the\nrelationship is a straight line that\npasses through the data points and the\nvariables have linear relationship\nlinear regression\nso let's start with uh how linear\nregression works a linear regression\nfinds a line that best fits the data\npoint and gives a relationship between\nthe two variables\nand so you can see here we have the\nefficiency of the car\nversus the distance traveled and you can\nsee this nice straight line drawn\nthrough there\nand when you talk about multiple\nvariables all you're doing is putting\nthis instead of a line it now becomes a\nplane\nit gets a little more complicated with\nmultiple variables but they all come\ndown to this linear kind of drawing a\nline through your data and finding what\nfits the the data the best\nand so we can consider an example uh\nlet's say that we want to find the\nrelationship between the temperature\noutside versus the sales of ice cream\nand so we start looking at that we're\nlooking at the how many ice cream cones\nwe're selling or how much money we sold\nin ice cream and we're looking at how\nwarm it is outside which would hopefully\ndraw a lot of people into the ice cream\nstore\nand suppose we have two lines we're\ngoing to draw l1 and l2 and we're going\nto kind of guess which one we think is\nthe best fit\nand which claim to describe the\nrelationship between the variables\nand so first we find the square of the\ndistance between the line l1\nand each data point and add them all and\nfind the mean distance\nand i want you to think about that when\nwe\nsquare something if it's a negative or\npositive number it no longer matters\nbecause a minus 2 squared is 4 2 squared\nis 4. so we're removing what side of the\nline is on and we're just looking for\nthe error in this case the mean distance\nof each of the little dotted lines you\nsee here\nthis way of calculating the square of\nthe distance adding them and then taking\nthe mean is called mean square error or\nloss function\nand we talk about loss how far off are\nwe that's what we're really talking\nabout what did we miss when we have a\npositive distance and a negative\ndistance and of course when we square it\nit is neither it just becomes a positive\nerror\nand so we take the mean\nsquare error and a lot of times you'll\nsee it referred to as mse\nif i look in the code and i'm going\nthrough my python code and i see mse i\nknow that's a mean squared error\nand we take all the dotted lines and we\ncalculate this error we add them all\ntogether and then we average it or find\nthe means\nand in this case\nthey ran a demo on this and it was uh\n1127.27\nfor our l1 line\nnow we find the loss function for line\nl2 in a similar fashion and we get the\nmean square error to be 6397\nand it's computed the same way so maybe\nyou put this line just way outside the\ndata range and this is the error you get\nby analyzing our results we find that\nthe loss function or the mean square\nerror is less for l1 than l2 hence l1 is\nthe best fit line\nthis process describes a lot of machine\nlearning processes it was we're going to\nkeep guessing and get as close as we can\nto find the right answer we have to have\nsome way to invite to calculate this and\nfigure out which one's the best and the\nmean square error is one of the better\nfits to doing for doing this and most\ncommonly used\nwe really want to talk about bias and\nvariance very important terms to know in\nmachine learning\nand with linear regression\nso bias\nbias occurs when an algorithm has\nlimited flexibility to learn from data\nvariance defines the algorithm's\nsensitivity to specifics sets of data\nlet's start with bias and variance you\ncan see here we have the two different\nsetups\nbias you can think is very generalized\nwhere variance is very specific\nand so we talk about bias such models\npay very little attention to the\ntraining data and over simplify the\nmodel\ntherefore the validation error or\nprediction error and training error\nfollow similar trends\nand uh with bias if you over simplify it\nso much you're going to miss\nyour\nlocal\nif if you have like a really\ngood fit you're going to miss it you're\ngoing to just kind of\nguess what the average is and that's\nwhat your answer is going to be\nwith variance a model with a high\nvariance pays a lot of attention to\ntraining data and does not generalize\ntherefore the validation error or\nprediction error are far apart from each\nother\nsuch models always lead to a high error\non training and test data as a bias does\nwhere variants such models usually\nperform very well on training data but\nhave high error rates on test data\nand i want you to think about this\nwhen we're talking about a bias\nthe error is going to be high both when\nyou're training it and you're testing it\nwhy because we're just kind of\ngetting an average we're not really\nfitting it close\nwith variance we're fitting it so close\nthat the test data does really good it's\ngoing to nail it every time if you're\ndoing categorical testing that's a car\nthat's a truck that's a bicycle\nbut with variants\nsuddenly a\ntruck has to have certain features\nand it might have to be red\nbecause you had so many red pictures so\nif it has if it's an 18 wheeler it has\nto be red if it's blue then it has to be\na bicycle\nthat's the kind of variance we're\ntalking about where it picks up on\nsomething and it cannot get the right\nanswer unless it gets a very specific\ndata\nand we see that so that as you're\ntesting it your models\nand you programmed it you got to look\nfor how i trained it what is coming out\nand if it's not if it's not looking good\non either bias or if it's not looking\ngood on the training\nor on the test data\nthen your bias then your bias in your\ndata\nif it really looks good on the training\ndata then that's going to be your\nvariance you've over fitted the data\nand those are very important things to\nknow when you are building your models\nin regression of any kind or any kind of\nsetup for predicting\nso in dark games if all the data found a\nparticular pointer this can be\nconsidered as a biased throw and the\nplayer aims for the particular score\nfor variance if all the darts fall on\ndifferent pointers and no two darts fall\non the same pointer then this can be\nconsidered as a varied throw and the\nplayer aims for various scores\nagain the bias sums everything up in one\npoint kind of averages it together where\nthe variance really looks for the\nindividual\npredictions coming out\nso let's go ahead and talk about\noverfitting\nwhen we talk about overfitting it's a\nscenario where the machine learning\nmodel tries to learn from the details\nalong with the noise and the data tries\nto fit each data point on the curve\nyou can see that\num\nif you plug in your coordinates you're\njust going to get the whatever it's\nfitted every point on the data stream\nthere's no average there's no\ntwo points that might have that you know\ny might have two different answers\nbecause\nif the wind blows a certain way um in\nthe efficiency of your car maybe you\nhave a headwind so your car might alter\nhow efficient it is as it goes and so\nthere's going to be this variance on\nhere and this says no you can't have any\nvariance with you know the this is it's\ngoing to be exactly this it can't be any\nyou can't be the same speed or the same\ncar and have a slightly different\nefficiency\nso as the model has very less\nflexibility it fails to predict new data\npoints and thus the model rejects every\nnew data point during the prediction\nso you'll get like a really high error\non here\nand so uh reasons for overfitting\ndata used for training is not cleaned\nand contains noise garbage values in it\nyou can spend so much time cleaning your\ndata\nand it's so important it's so important\nthat if you have if you have some kind\nof\nsomething wrong with the data coming in\nit needs to be addressed whether it's a\nsource of the data maybe they use in\nmedical different measuring tools\nso you now have to adjust for data that\ncame in from hospital a versus hospital\nb or even off of machine a and machine b\nis testing something and those those\nnumbers are coming in wrong\nthe model has a high variance\nagain wind is a good example i was\ntalking about that with the car\nyou may have a hundred tests but because\nthe wind's blowing it's all over the\nplace\nsize of training data used is not enough\nso a small amount of data is going to\nalso cause this problem you only have a\nfew points and you try to plot\neverything\nthe model is too complex\nthis comes up a lot\nwe put too many pieces together and how\nthey interact can't even be tracked\nand so you have to go back back break it\nup and find out actually what correlates\nand what doesn't\nso\nwhat is underfitting a scenario where\nmachine learning models\ncan either learn\nthe relationship between the data points\nnor predict\nor classify a new data point and you can\nsee here we have our efficiency of our\ncar and our line drawn and it's just\ngoing to be way off for both the\ntraining and the predicting data\nas the model doesn't fully learn the\npatterns it accepts every new data point\nduring the prediction\nso instead of looking for a general\npattern we just kind of accept\neverything\ndata used for training is not cleaned\nand contains noise garbage and values\nagain under fitting and overfitting same\nissue\nyou got to clean your data\nthe model has a high bias\nwe've seen this in all kinds of things\nfrom\n[Music]\nuh\nthe mod the most common is the driving\ncars to facial identification or\nwhatever it is the model itself when\nthey build it might have a bias towards\none thing and this would be an\nunderfitted model would have that bias\nbecause it's averaged it out so if you\nhave five people from india and 10\npeople from\nafrica and 20 people from the us you\ncreated a bias\nbecause it's looking at the 20 people\nand you only have a small amount of data\nto work with\nsize of training data used is not enough\nthat goes with the size i was just\ntalking about\nso we have a model with a high bias we\nhave size of training data used is not\nenough the model is too simple\nagain this is one straight line through\nall the data when it needs a slight\nshift to it for other reasons\nso what is a good fit\nuh a linear curve that best fits the\ndata is neither overfitting or\nunderfitting models but is just right\nand of course we have the nice examples\nhere where we have overfitting\nlines going up and down every point is\ntrying to be include gluted underfitting\nthe line really is off from where the\ndata is and then a good fit is got to\nget rid of that minimize that\nerror coming through regularization is\ntaking the guesswork out you're looking\nat this graph and you're going oh which\none is that really over fit or is that\nunder fit that's pretty hard to tell\nso we talk about regularization\nregularization techniques are used to\ncalibrate the linear regression models\nand to minimize the adjusted loss\nfunction and prevent overfitting or\nunderfitting\nso what that means uh in this case we're\ngoing to go ahead and take a look at a\ncouple different things\nwe're going to look at regularization\nwhich we'll start with a linear model\nwe'll look at the ridge regularization\nand the lasso regularization\nand these models are just like just like\nwe did the\nmlp the multi-layered positron in the sk\nlearn module you could bring in the\nridge module and you can bring in the\nlasso module\nso when we talk about\nridge regression it modifies the\noverfitted or underfitted models by\nadding the penalty equivalent to the sum\nof the squares of the magnitude of the\ncoefficients\nand so we have a cost function equals\nloss equals\nlambda times the sum of w\nsquared or the absolute value of w\ndepending on how you're doing it now\nremember we talked about error whether\nwe either square it or we absolute value\nit\nbecause that removes a plus or minus\nsign on there and there's reasons to do\nit either way but it is more common to\nsquare the value\nand then we have our um\nin this case the lambda is going to be\nthe penalty for the errors we've thrown\nin a greek character for you just to\nconfuse everybody\nand w is the slope of the curve of the\nline\nso uh we're going to look at this and\nwe're going to draw a line this is going\nto be like a linear regression model so\nif you had in sklearn you could import\njust a standard linear regression model\nit would plot this line across whatever\ndata we're working on\nand we look at this of course we're just\nextrapolating this i know they use some\nspecific data but you don't want to get\ninto the actual domain\nand so for a linear regression line\nlet's consider two points that are on\nthe line\nand we'll go ahead and have a loss\nequals zero\nuh considering the two points on the\nline we'll go ahead and do lambda equals\none we'll set our w is going to be one\npoint four\nthen the cost function equals zero plus\none times one point four squared which\nequals 1.96\nso\nreally don't get caught up too much in\nthe math on this other than\nunderstanding\nthat this is something that's very easy\nfor a computer to calculate and if you\never see the loss plus the plus the\nlambda times w the sum of w squared\nand then let's say we have a ridge\nregression line and it does this we go\nahead and plot it and we do the\ncalculations on the data and for the\nridge regression let's assume a loss\nequals 0.3 squared plus 0.2 squared\nequals 0.13 so when they put all the\ncalculations through\nof the two points we end up with the\n0.0.62\nso we've now had a linear regression\nmodel we now had a ridge regression\nmodel\nand the ridge regression model plots a\nlittle differently than the standard\nlinear regression model\nand comparing the two models with all\nthe data points we can see that the\nridge regression line fits the model\nmore accurately than the linear\nregression line\nand i find this true on a lot of data i\nwork with i'll end up using either the\nridge regression model or the lasso mars\nregression model for fitting especially\ndealing with a lot of like stock markets\ndaily\nsetup they come out slightly better you\nget a slightly better\nfit\nand so we have our lasso we just talked\nabout lasso coming in here\nand the cost function equals\ninstead of doing a squared we're just\ngoing to do the absolute value and so if\nyou remember this is where ridge\nregression changes where's my ridge\nregression model\nwe're squaring the value here\nand if you look at this we're not\nsquaring the value we're just finding\nthe absolute value on here and so the\nloss of this of the squared individuals\nand here is our\nlambda symbol again penalty for errors\nand w equals the slope of the curve\nand comparing the two models with all\nthe data points we can see that the\nlasso regression line fits the model\nmore accurately than the linear\nregression line\nand this is like i said i use these two\nmodels a lot uh the ridge and this is\nimportant this is\nthis is kind of the meat of the matter\nhow do you know which one to use some of\nit is you just do it a bunch of times\nand then you figure it out\nridge regularization is useful when we\nhave many variables with relatively\nsmaller data samples\nthe model does not encourage convergence\ntowards zero but is likely to make them\ncloser to zero and prevent overfitting\nthe lasser regularization model is\npreferred when we are fitting a linear\nmodel with fewer variables\nso in the la in the iris thing we had\nfour or five variables as you measure\nthe different leaf pieces uh you might\nbe doing the measurements on the cancer\nproject which has 36 different variables\nso as we get down to the iris with four\nvariables\nlasso lar will probably uh work pretty\ngood where you might use the ridge\nregularization with more model with if\nyou have something significant larger\nand it encourages the coefficients of\nthe variables to go towards zero because\nof the shape of the constraint which is\nan absolute value\nand with any of this we want to go ahead\nand\ndo a demo and lasso and ridge regression\nso let's take a look and see what that\nlooks like in our code and bring up our\njupiter notebook\nwe'll start with our imports pandas is\npd import numpy is np import matplot\nlibrary as plt\nsklearn we're going to import our data\nsets it's kind of more generic\nwe usually just import one data set\ninstead of all of them but you know\nquick and dirty when you're putting some\nof these together\nwe have our sklearn model selection\nwe're going to import our train test\nsplit for splitting our data up\nand then we'll bring in our linear\nregression model\nand we'll go ahead and run these just to\nload them up\nand then load our data set we're just\ntalking about that\nyou could just\nhave imported the load boston and boston\ndata set in there instead of loading all\nthe data sets\nand then once we've loaded our data set\nwe want to go ahead and take a look at\nthat data and see what we got here\nlet me just go and pop that down there\nand\ngo and run it\nand so we've gone ahead and taken our uh\nboston uh data we're gonna look we put\nit into our pandas data frame\num the boston data set and then the\nboston columns we want to see what's\ngoing on with them\nwe have our target\nwe have the house price\netc and so our x equals boston\neye location\nnow remember in pandas the new updates\nto pandas they want eye location if\nyou're going to pull data we used to be\nable to leave this off\nbut it does something different it\ncreates a slice versus a direct\nsetup so make sure using that eye\nlocation and the i the output so this is\nall just bringing our data together\nand we can see here if we do we print\nthe boston\npanda's head\nwe can see here all of our different\naspects we're looking for\nand if you're following the x and the y\nthe x is\neverything\nexcept for the last column\nwhere y is uh all the it's that's what\nthis means all the rows except for the\nlast column and then y is all the rows\nbut just the last column so y is our\nhouse price\nand the x is the\ncrimsian industry chas knox and all\nthese other different\nstatistics they've collected for house\nsales in boston\nthere we go oops control\nso we'll go ahead and split our data x\ntrain and our x\ntest y train y test equals the train\ntest split which we imported\nand we have our boston you could have\neasily used the x and y on here as\nopposed to boston\neye location\nand we'll create our test size we're\ngoing to take 25 of the data and put it\nin as a test\nand then we'll go ahead and run this\nneed an extra drink there\nuh so we have our train and test and\nthen of course the print the train data\nshape\ni love doing this kind of thing whenever\ni'm working with this data print out the\nshape\nmake sure everything looks correct uh so\nthat we have 127 by 13 and 127 by one\n379 by 13 they should match and if\nthey're if the the data sets are not\nquite matching\nthen you know something's wrong and\nyou're going to get all those errors i\ndon't know how many times i've gone\nthrough here and\nit's dropped a row on one of them and\nnot on the other or something weird has\nhappened when i'm cleaning the data\nthis is pretty straightforward and\nsimple because the data comes in a nice\npre-package is all clean for you\nso let's go ahead and apply apply the\nmultiple linear regression model\nand we'll call this lreg reg linear\nregression and we're going to go ahead\nand fit that linear regression model to\nx train and y train\nthen we'll generate the prediction on\nthe test set\nso here's our l reg y predict\nwith our x test going into the\nprediction\nand let's calculate that mean square\nerror mse i told you you'll see mse used\na lot\npeople use it in variables and things\nlike that it's pretty common\nand we get our mean squared error equals\nthis is just the basic formula we've\nalready been talking about what's the\ndifference squared\nand then we look for the average of that\nwe'll go ahead and just run this\nand you can see when we get through the\nend of this we have our mean square\nerror on test\nwe have our total and then we have each\ncolumn coming down\nand at this point unless you really know\nthe data you're working with it's not\ngoing to mean a whole lot so if it's in\nyour domain you might be know what\nyou're looking at when you see these\nkinds of numbers coming up\nbut if it's not it's just a bunch of\nnumbers and that's okay\nat least that's okay for this demo uh\nand then we're gonna go ahead and plot\nthese so we can see what's going on\nand this is always kind of fun it's\nalways nice to have a nice visual of\nwhat you're looking at and you can see\nhere\nwhen we plot the coefficient scores on\nhere and we\nthe guys in the back did a great job\nputting some pretty colors together\nmaking it look nice and setting up the\ncolumns\nyou can see here\nuh your nox has like just a huge\ncoefficient\nwhen i look at a table like this i look\nfor what has very little\ndifferent coefficients they're not using\na huge change and what has huge changes\nand that flags you for all kinds of\nthings as you're working with the data\nbut it depends so much on the domain\nyou're working with\nthese are great things though as just a\nquick look to see what's going on with\nyour data and what you're looking for\nand of course once we look at this now\nour motive is to reduce the coefficient\nscore so now we want to take these and\nand\nbring them down as much as we can\nand for that we're going to work with\nthe ridge regression on here\nso let's start by going we're going to\nimport our ridge model the red\nregression from the sk learn library or\nthe scikit\nand we're going to go ahead and train\nthe model so here's our ridge r equals\nalpha equals 1. and i mentioned that\nearlier\nwhen i work with the ridge model\nyou'll see alpha equals one if you set\nalpha equal to zero that's a standard\nlinear regression model so you have\nalpha equals one two three four and you\nusually use one two or three four and a\nstandard integer on there and we'll go\nahead and fit the ridge model on there\nwith our x train and our y train data\ngenerate a prediction for that\nfor our x test\nand we'll calculate the mean square\nerror\njust like we did before this should all\nlook familiar\nand we'll go ahead and print that out\nand we'll look at the ridge coefficients\nfor our data and see what that looks\nlike\nnow\nif i jump up and down between these two\nyou'll get a headache\n[Laughter]\nyou'll still see the knox value let's\njust look at the knocks because that was\nthe biggest value it's a minus nine here\nand if we go back up here the nox value\nis a minus 18. so right off the bat i'm\nseeing a huge change\nin the biggest coefficient there\nuh so if we're going to do that nice\nsetup we want to go ahead and just print\nit and see what that looks like\nhere we go and we've\nset up our plot subplots and again the\nteam put together some nice colors so it\nmakes it look good\nwe're doing an x bar based on the\ncolumns\nand our\nl regress coefficients color equals\ncolor x spine bottom and so forth\nso just put together a nice little graph\nand you're starting to see\none this when you compare this if you\nput on the same graph as this one up\nhere this is up here minus 18\nthis is at minus nine\nand so this graph is half the size of\nthe graph above the same thing with\nthese values here\nthey might look the same but they're\nactually all\nalmost half the value on here\nand then finally you can do the same\nthing for the lasso regression\nthis would all look\nvery similar as far as what we worked on\nbefore\nand i'm just going to print that on here\nand run it\nand again let's go up to\nknox look where nox is it's all the way\ndown to zero\nand if we look at our next biggest\ncoefficient it's minus 0.8 and really\nhere's our\n22.73\nlet me go up here\num\n16.7\nand we go up here and we look at the\nsame number\nuh 16.69\nand so we look at this uh if i was\nrunning this and doing uh working on a\nproject with this i would look at these\nnumbers i start with the 16.69\ncome down here and compare it to\nuh\n16.78 6 9 is better than 7 8.\nso from the very beginning we might\nstart looking at this first model for\noverall\npredicting\nbut there's other factors involved we\nmight know that uh the nox value is\ncentral and the other ones aren't quite\nas good and so we might start looking at\njust certain\nsetups like what is our what is this\nparticular coefficient because it might\nhave a certain meaning to us and so\nforth and so you look at all those\ndifferent\nitems in there again but the bottom\ndollar is our first model did better\nthan our other two models our mean\nsquare error on the test set\ncontinues to\ncome down on this\ndimensionality reduction dimensionality\nreduction refers to the technique that\nreduces the number of input variables in\na data set\nand so you can see on the table on the\nright shows the orders made at an\nautomobile parts retailer\nthe retailer sells different automobile\nparts from different companies and you\ncan see we have company b-packs iso max\nand they have the item the tire the axle\nan order id a price number and a\nquantity in order to predict the future\ncells\nwe find out that using correlation\nanalysis\nthat we just need three attributes\ntherefore we have reduced the number of\nattributes from five to three\nand clearly we don't really care about\nthe part number i don't think the part\nnumber would have an effect on how many\ntires are bought\nand even the store who's buying them\nprobably does not have an effect on that\nin this case that's what they've\nactually done is remove those and we\njust have the item the tire the price\nand the quantity one of the things you\nshould be taking away from this is in\nthe scheme of things\nwe are in the descriptive phase we're\ndescribing the data\nand we're pre-processing the data what\ncan we do to clean it up\nwhy dimensionality reduction\nwell number one less dimensions for a\ngiven data set means less computation or\ntraining time\nthat can be really important if you're\ntrying a number of different models\nand you're re-running them over and over\nagain and even if you have seven\ngigabytes of data that can start taking\ndays to go through all those different\nmodels\nso this is huge this is probably the\nhugest part as far as reducing\nour data set\nredundancy is removed after removing\nsimilar entries from the data set\nagain pre-processing some of our models\nlike a neural network if you put in two\nof the same data it might give them a\nhigher weight than they would if it was\njust once we want to get rid of that\nredundancy\nit also increases the processing time if\nyou have multiple\ndata coming in\nspace required to store the data is\nreduced\nso if we're committing this into a big\ndata\npool we might not send the company that\nbought it why would we want to store two\nwhole extra columns when we added into\nthat pool of data\nmakes the data easy for plotting in 2d\nand 3d plots this is my favorite part\nvery important you're in your\nshareholder meeting\nyou want to be able to give them a\nreally good\nclear\nand simplified version you want to\nreduce it down to something people can\ntake in\nit helps to find out the most\nsignificant features and skip the rest\nwhich also comes in in post scribing uh\nleads to better human interpretation\nthat kind of goes with number four it\nmakes data easy for plotting you have a\nbetter interpretation when we're looking\nat it principal component analysis\nso what is it\nprincipal component analysis is a\ntechnique for reducing the\ndimensionality of data sets increasing\ninterpretability but at the same time\nminimizing information loss so we take\nsome very complex data set with lots of\nvariables we run it through the pca\nwe reduce the variables we end up with a\nreduced variable setup\nthis is very confusing to look at\nbecause\nif you look at the end result\nwe have the different colors all lined\nup so what we're going to take a look at\nis let's say we have a picture here\nlet's say you are asked to take a\npicture of some toddlers and you are\ndeciding which angle would be the best\nto take the picture from so if we come\nup here we look at this we say okay this\nis you know one angle\nwe get the back of a lot of heads not\nmany faces\nso we'll do it from here we might get\nthe one person up front smiling a lot of\nthe people in the class are missing so\nwe have a huge amount off to the right\nof blank space maybe from up here again\nwe have the back of someone's head\nand it turns out that the best angle to\nclick the picture from might be this\nbottom left angle you look at it you say\nhey that makes sense it's a good\nconfiguration of all the people in the\npicture now when we're talking about\ndata it's not\nyou really can't do it by what you think\nis going to be the best we need to have\nsome kind of mathematical\nformula so it's consistent and so it\nmakes sense in the back end\none of the projects i worked on many\nyears ago\nhas something similar to the iris if\nyou've ever done the iris data sets\nprobably one of the most common ones out\nthere where they have the flower\nand they're measuring the stamen\nin the petals and they have width and\nthey have length of the petal\ninstead of putting through the width and\nthe length of the petal we could just as\neasily do the\nwidth-to-length ratio we can divide the\nwidth by the length and you get a single\nnumber where you had two\nthat's the kind of idea that's going on\ninto this in pre-processing and looking\nat what we can do to bring the data down\nthe very simplified example on my\niris\npedal example\nwhen we look at the similarity in pca we\nfind the best picture or projection of\nthe data points\nand so we look down at from one angle\nwe've drawn a line down there\nwe can see these data points based on in\nthis case just two variables now keep in\nmind we're usually talking about 36 40\nvariables almost all of your\nbusiness models usually have about 26 to\n27 different variables they're looking\nat\nsame thing with like a bank loan model\nwe're talking 26 to 36 different\nvariables they're looking at that are\ngoing in\nso we want to do is we want to find the\nbest view in this case we're just\nlooking at the x y\nwe look down at it and we have our\nsecond\nidea pc2 and again we're looking at the\nx i this x y this time from a different\ndirection\nhere for our ease we can consider that\nwe get two principal components namely\npc1 and pc2\ncomparing both the principal components\nwe find the data points are sufficiently\nspaced in pc1\nso if you look at what we got here we\nhave pc1 you can see along the line how\nthe data points are spaced versus the\nspacing in pc2 and that's what they're\ncoming up with what is going to give us\nthe best look for these data points when\nwe combine them and we're looking at\nthem from just a single angle\nwhereas in pc2 they are less spaced\nwhich makes the observation and further\ncalculations much more difficult\ntherefore we accept the pc1 and not the\npc2 as the data points are more spaced\nnow obviously the back end calculations\nare a little bit more complicated when\nwe get into the math of how they decide\nwhat is more valuable\nthis gives you an idea though that when\nwe're talking about this we're talking\nabout the perspective\nwhich would help in understanding how\npca analysis works\nwe want to go ahead and do is dive into\nthe important terminologies under pca\nand important terminologies are views\nthe perspective through which data\npoints are observed\nand so you'll hear that if someone's\ntalking about a pca presentation and\nthey're not taking the time to reduce it\nto something that the average person\nshareholders can understand you might\nhear them refer to it as the different\nviews what view are we taking\ndimension number of columns in a data\nset are called the dimensions of that\ndata set\nand we talked about you'll hear features\ndimensions\ni was talking about features there's\nusually when you're running a business\nyou're talking 25 26 27 different\nfeatures minimal and then you have the\nprincipal component new variables that\nare constructed as linear combinations\nor mixtures of the initial variables\nprincipal component is very important\nit's a combination if you remember my\nflower example it would be the\nwidth over the length of the petal as\nopposed to putting both width and length\nin you just put in the ratio instead\nwhich is a single number versus two\nseparate numbers projections\nthe perpendicular distance between the\nprincipal component and the data points\nand that goes to that line we had\nearlier it's that right angle line of\nwhere those point all those points fall\nonto the line\nimportant properties important\nproperties number of principal\ncomponents is always less than or equal\nto the number of attributes\nthat just makes common sense you're not\ngoing to do\n10 principal properties with only three\nfeatures\nyou're trying to reduce them so it's\njust kind of goofy but it is important\nto remember that people will throw\nweird code out there and just randomly\ndo stuff with instead of really thinking\nit through principle components are\northogonal\nand this is what we're talking about\nthat right angle from the line\nwhen we do pc1 we're looking at how\nthose points fall on to that line\nsame thing with pc2 we want to make sure\nthat pc1 does not equal pc2 we don't\nwant to have the same two principal\npoints\nwhen we do two points\nthe priority of principal components\ndecreases as their numbers\nincrease\nthis is important to understand\nif you're going to create\none principle\ncomponent\neverything is summarized into that one\ncomponent as we go to two components the\npriority\nhow much it holds value decreases as we\ngo down so if you have five different\npoints each one of those points is going\nto have less value than just the one\npoint which has everything summarized in\nit\nhow pca works\ni said there was more in the back end we\ntalk about the math this is what we're\ntalking about is how does it actually\nwork\nso now we have understanding that you're\nlooking at a perspective\nnow we want to see how that math side\nworks pca performs the following\noperations in order to evaluate the\nprincipal components for a given data\nset\nfirst we start with the standardization\nthen we have a covariance matrix\ncomputation\nand we use that to generate our i gene\nvectors and i gene values\nwhich is the feature vector and if you\nremember the i gene vector is like a\ntranslation for\nmoving the data\nfrom x equals 1 to x equals 2 or\nwhatever altering it and the i gene\nvalue is the final value that we\ngenerate\nwhen we talk about standardization the\nmain aim of this step is to standardize\nthe range of the attributes so that each\none of them lie within similar\nboundaries\nthis process involves removal of the\nmean from the variable values and\nscaling the data with respect to the\nstandard deviation\nand you can see here we have z equals\nthe variable values minus the mean\nover the standard deviation\nthe covariance matrix computation\ncovariance matrix is used to express the\ncorrelation between any two or more\nattributes in multi-dimensional data set\nthe covariance matrix has the entries as\nthe variance and the covariance of the\ntribute values the variance is denoted\nby var and the covariance is denoted by\ncov\non the right we can see the covariance\nmatrix for two attributes and their\nvalues\nwhen we do a hands-on and look at the\ncode we'll do a display of this so you\ncan see what we're talking about and\nwhat that looks like\nfor now you can just notice that this is\na matrix that we're generating with the\nvariance and then the covariance of x to\ny\non the right side we can see the\ncovariance table for more than two\nattributes in a multi-dimensional data\nset\nthis is what i was talking about we\nusually are looking at not just one\nfeature two features\nwe're usually looking at 25 30 features\ngoing on\nand so if we do i was set up like this\nwe should see all those different\nfeatures as the different variables\ncovariance matrix tells us how the two\nor more variables are related positive\ncovariance indicate that the value of\none variable is directly proportional to\nthe other variable\nnegative covariance indicate that the\nvalue of one variable is inversely\nproportional to the other variable that\nis always important to note whenever\nwe're doing any of these matrixes that\nwe're going to be looking at that\npositive and negative whether it's\ninverted or not\nand then we have the iogene values and\nthe i gene vectors\niogene values and hygiene vectors are\nthe mathematical value\nthat are extracted from the covariance\ntable\nthey are responsible for the generation\nof a new set of variables from the old\nset of variables which further lead to\nthe construction of the principal\ncomponents\nigen vectors do not change directions\nafter linear transformation\ni gene values are the scalars or the\nmagnitude of the i gene vectors\nand again this is just chain\ntransforming that data so we're going to\nchange\nthe vector b\nto the b prime as denoted b on the chart\nand so when we have like multiple\nvariables how do we calculate that new\nvariable\nand then we have feature vectors feature\nvectors is simply a matrix that has igen\nvectors of the components that we decide\nto keep as the columns\nhere we decide whether we must keep or\ndiscard the less significant principal\ncomponents that we have generated in the\nabove steps\nthis becomes really important as we\nstart looking at\nthe back end of this and we'll do this\nin the demo\nbut one of the more important steps to\nunderstand\nand so we have the pca example consider\nmatrix x within rows or observations and\nk columns or variables\nnow for this matrix we would construct a\nvariable space with as many dimensions\nas the variable\nbut for our simplicity let's consider\nthis three dimensions for now\nnow each observation row of the matrix x\nis placed in the k dimensional variable\nspace such that the rows in the data\ntable form a swarm of points in this\nspace\nnow we find the mean of all the\nobservations and then place it along the\ndata points on the plot\nthe first principal component is a line\nthat best accounts for the shape of the\npoint swarm it represents the maximum\nvariance direction in the data\neach observation may be projected onto\nthis line in order to get a coordinate\nvalue along the pc one this value is\nknown as a score\nusually only one principal component is\ninsufficient to model the systematic\nvariation for a data set thus a second\nprincipal axis is created\nthe second principle component is\noriented such that it reflects the\nsecond largest source of variation in\nthe data while being orthogonal to pc1\npc2 also passes through the average\npoint\nlet's go ahead and pull this up and just\nsee what that means\ninside our python scripting\ni'm going to use the anaconda navigator\nand i will be in python 3.6\nfor this example\ni believe there's even like a 3.9 out\ni tend to stay in 3.6 because a lot of\nthe models i use especially with the\nneural networks are stable in three six\nand then we open up our\njupiter i'm in chrome and go ahead and\ncreate a new python three\nand for ease of use our team in the back\nwas nice enough to put this together for\nme\nand we'll go and start with the\nlibraries the first thing i like to do\nwhenever i'm looking at\nany new setup\nwell you know what let's do let's do the\nlibraries first we're going to do our\nbasic libraries which is matplot library\nthe plt from the matplot library pandas\nour data frame\npd numpy our numbers array np\nseaborn for graphing sns that goes with\nthe plot that actually sits on matplot\nlibrary so the seaborn sits on there\nand then we have our amber sign because\nwe're in jupiter notebook map plot\nlibrary in line the newer version\nactually doesn't require that but i put\nit in there either anyway just because\ni'm so used to it\nand then we want to go ahead and take a\nlook at the data\nand in this case we're going to pull in\ncertainly you can have lots of fun with\ndifferent data but we're going to use\nthe cancer data set\nand one of the reasons a cancer data set\nis it has like 36 35 different features\nso it's kind of fun to use that as our\nbase for this and we'll go ahead and run\nthis and look at our keys\nand the first thing we notice in our\nkeys for the cancer data set\nis we have our data we have our target\nour frame target names description\nfeature names and file name\nso\nwhat we're looking for in all this\nis\nlet's take a look at the description\nlet's go in here and pull up the\ndescription on here\ni'm not going to spend a huge amount of\ntime on the description um\nbecause this is we don't want to get\ninto a medical domain we want to focus\non our pca setup\nwhat's important is you start looking at\nwhat the different attributes are what\nthey mean\nif you were in the medical field you'd\nwant to note all these different things\nwhether what they're measuring where\nit's coming from\nyou can actually see the actual\ndifferent\nmeasurements they're taking\nno missing attributes\nwe page all the way to the bottom and\nyou're going to have your data in this\ncase our target\nand if you dig deep enough to the target\nlet's actually do this\nlet's go ahead and print target names\nreal quick here i always like to just\ntake a look and see what's on the other\nend of this\ntarget\nnames\nrun that\nyeah so the target name is is it\nmalignant or is it b9\nso in other words is this\ndangerous growth or is it something we\ndon't have to worry about that's the\nbottom line with the cancer in this case\nand then we can go ahead and load our\ndata\nand you know let me go up uh just a\nnotch here for easy of reading\nit's hard to get that just right that's\nall you have to do\nso let's go ahead and look at our data\nuh our we're going to use our pandas\nand we're going to go ahead and do our\ndata frame it's going to equal cancer\ndata\ncolumns equals cancer feature equals\nfeature names so remember up here we\nalready loaded the the\nnames up of our of the features in there\nwhat is going to come out of this let me\njust see if we can get to that\nit's at the top of target names\nthat's just this list of names here in\nthe setup\nand we can go ahead and run this code\nand i'll print the head and you can see\nhere we have the mean radius the mean\ntexture mean perimeter\ni don't know about you this is a\nwonderful data set if you're playing\nwith it because\nlike many of the data that most of the\ndata that comes in half the time we\ndon't even know we're looking at\nwe're just handed a bunch of stuff as a\ndata scientist going what the heck is\nthis\nand so this is a good place to start\nbecause this has a number of different\nfeatures in there we have no idea what\nthese feature means or where they come\nfrom we want to just look at the data\nand figure that out\nand now we actually are getting into the\npca side of it as we've noticed before\nis difficult to visualize high\ndimensional data\nwe can use pca to find the first two\nprincipal components and visualize the\ndata this new two-dimensional space with\na single scatter plot\nbefore we do this we need to go ahead\nand scale our data\nnow\ni haven't run this to see if you really\nhave to scale the data on this\nbut as just a general\nrun time i almost do that as the first\nstep of any modeling even if it's\npremodeling as we're doing here\nin neural networks that is so important\nwith pca visualization it's already\ngoing to scale it when we do the means\nand deviation inside the pca\nbut just in case it's always good to\nscale it\nand then we're going to take our pca\nwith the site kit learn uses very\nsimilar process to other pre-processing\nfunctions that come with scikit-learn\nwe instantiate a pca object find the\nprincipal components using the fit\nmethod then apply the rotation and\ndimensionality reduction by calling\ntransform we can also specify how many\ncomponents we want to\nkeep when creating the pca object\nand so the code for this\noops getting a little bit ahead\nlet me go and run this code\nuh so the code for this\nis\nfrom sklearn decomposition import pca\npca equals pca in components equals two\nand that's really important to note that\nbecause we're only going to want to look\nat two components\ni would never go over four components\nespecially if you're going to demo this\nwith somebody else if you're showing\nthis to the shareholders\nthe whole idea is to reduce it to\nsomething people can see\nand then the pca fit we're going to is\ngoing to take the scaled data that we\ngenerated up here\nand then you can see we've created our\npca model with in components equals two\nnow whenever i use a new tool\ni like to go in there and actually see\nwhat i'm using so let's go to the scikit\nwebpage for the pca\nand you can see in here here's our call\nstatement it describes what all the\ndifferent setups you have on there\nprobably the biggest one to look at\nwould be\nwell the biggest one is your components\nhow many components do you want\nwhich you have to put in there pretty\nmuch\nand then you also might look at the svd\nsolver it's on auto right now but you\ncan override that and do different\nthings with it it does a pretty good job\nas it is\nand if we\ngo down all the way down to\nthere we go\nto our methods\nif you notice we have fit\nwe have fit transform\nnowhere in here is predict because this\nis not used for prediction\nit's used to look at the data again\nwe're in the describe setup\nwe're fitting the data we're taking a\nlook at it we've already looked at our\nminimum maximum we've already looked at\nwhat's in each quarter we've done a full\ndescription of the data this is part of\ndescribing the data\nthat's the biggest thing i take away\nwhen i come zooming in here and of\ncourse i have examples of it down here\nif you forget\nand the biggest one of course is the\nnumber of components\nand then i mean the rest you can play\nwith\nthe actual solver whether you're doing a\nfull or randomize there's different\nthings it does pretty good on the auto\nand now we can transform this data to\nits first two principal components\nand so we have our xpca\nwe're going to set that equal to pca\ntransform scaled data\nso there we go there's our first\ntransformation\nand let's just go ahead and print the\nscaled data shape and the xpca data\nshape\nand the reason we want to do this is\njust to show us\nwhat's going on here we've taken 30\nfeatures i think i said 36 or something\nlike that but it's 30\nand we've compressed it down to two\nfeatures and we decided we wanted two\nfeatures and that's where this comes\nfrom\nwe spawn two features\nso let's go ahead and\nplot these and take a look and see\nwhat's going on\nand\nwe're just going to use our plt figure\nwe'll set the figure size on here here's\nour scatter plot\nxpca\nx underscore pca of\nof one these are two different\nperceptions we're using uh and then\nyou'll see right here c for color cancer\nequals target\nand so remember we have zero we have one\nand if i remember correctly zero was\nmalignant one was b9\nso everything in the zero column is\ngoing to be one color and the other\ncolor is going to be one and then we're\ngoing to use the plasma map just kind of\ntelling you what color it is add some\nlabels first principle component second\nprincipal component and we'll go ahead\nand run this\nand you can see here instead of having a\nchart one of those heat maps with 30\ndifferent\ncolumns in it\nwe can look at this and say hey this one\nactually did a pretty good job\nof separating the data\nand a couple things when i'm looking at\nthis that i notice is first\nwe have a very clear area where it's\nclumped together\nwhere it's going to be b9\nand we have a huge area it's still\nclumped together more spread out where\nit's going to be malignant or i think i\nhad that backwards\nand then in the middle\nbecause we're dealing with something\nin this particular case cancer we would\ntry to separate i would be exploring how\nto separate this middle group out\nin other words there's an area where\neverything overlaps\nand we're not going to have a clear\nresult on it\njust because those are the people you\nwant to go in there and have extra tests\nor treat it differently\nversus going in and saying just cutting\ninto the can into the cancer so the body\nabsorbs it and it dissipates versus uh\nactively going in there removing it\ntesting it going through chemo and all\nthe different things that's a big\ndifference you know as far as what's\ngoing to happen here in that middle line\nwhere the overlap is going to be huge\nthat's domain specific going back to the\ndata\nwe can see here\nclearly by using these two components we\ncan easily separate these two classes\nso the next step is what does that mean\ninterpreting the components\nunfortunately with this great power of\ndimensionality reduction comes the cost\nof not being able to easily understand\nwhat these components represent\ni don't know what principle component\none licks work represents or second\nprinciple\nthe components correspond to\ncombinations of original features the\ncomponents themselves are stored as an\nattribute of the filtered pca object and\nso we talk look at that we can go ahead\nand do look at the pca components this\nis in our model we built we've trained\nit we can run that and you can see\nhere's the actual components uh it's the\ntwo components have each have their own\narray\nand within the array you can see the\nwhat the scores are using\nand these actually give weight to what\nfeatures are doing what\nso in this numpy matrix array each row\nrepresents a principal component and\neach column relates back to the original\nfeatures\nwhat's really neat about this is we can\nnow go in reverse\nand drop this onto a heat map\nand start seeing\nwhat this means and so let me go ahead\nand just put this down upside down down\nhere\nwe'll go ahead and put this in here\nwe're going to use our\ndf comp data frame and we do our pca\ncomponents\nand i want you to notice how easy this\nis uh\nwe're going to set our columns equal to\ncancer feature names\nthat just makes it really easy\nand we're dumping it into a data frame\nwhat's neat about a data frame is when\nwe get to seaborn it will pull that data\nframe apart and\nand set it up for us when we want and so\nwe're just going to do the cn the\nseaborn heat map\nof our data frame composition and we'll\nuse the plasma coloring\nand it creates a nice little\ncolor graph here\nyou can see we have the mean radius and\nall the different features along the\nbottom\non the right we have a scale so we can\nsee we have the dark colors all the way\nto the really light colors which are\nwhat's really shining there this is like\nthe primary stuff we want to look at\nso this heat map and the color bar\nbasically represent the correlation\nbetween the various features and the\nprincipal component itself\nso you know very powerful map to look at\nand then you can go in here and we might\nnotice that the mean radius\nlook how how on the bottom of the map it\nis\non some of this\nso you have some interesting\ncorrelations here that change the\nvariations on that and what means what\nthis is more when you get to a post\nscribe you can also use this to try to\nguess as what these things mean and what\nyou want to change to get a better\nresult in this session\nwe will briefly understand what\ncoronavirus really is and the various\nsymptoms of coronavirus\nthen we look at the global impact of\ncoronavirus in terms of the total cases\nthe deaths reported the fatality rate\nand the tests carried out so far by\ndifferent countries after that we will\nperform an analysis using svm and\npolynomial regression in python to\npredict the number of upcoming cases\nfrom the 28th of april to the 17th of\nmay\nthe data we have taken is from the 22nd\nof january to the 27th of april then we\nwill analyze the model using charts and\ngraphs and finally we will discuss the\nvarious safety measures you can take to\nensure you are safe and not attacked by\ncoronavirus\nso what is coronavirus coronavirus or\ncovad19 is an infectious disease caused\nby a newly discovered coronavirus that\nis believed to have emerged from a\nseafood market in wuhan china during\ndecember 2019.\nit is zoonotic\nso it's a disease that can be\ntransmitted from animals to people or\nmore specifically a disease that\nnormally exists in animals but that can\ninfect humans too\nthe virus that causes covid19 is mainly\ntransmitted through droplets generated\nwhen an infected person coughs sneezes\nor exhales\nthese droplets are too heavy to hang in\nthe air and quickly fall on floors or\nsurfaces\nit appears that symptoms show up in\npeople within 14 days of exposure to the\nvirus\nwith that now let's look at the various\nsymptoms of covid19\na patient with coronavirus\ncan show generic symptoms such as cough\nfever shortness of breath and muscle\npain\nthey can also have a sore throat\nheadache and loss of taste or smell\nthey are also expected to have middle\neast respiratory syndrome or mars\nand severe acute respiratory syndrome or\nsars\nmiddle east respiratory syndrome is a\nviral respiratory illness caused by a\ncoronavirus\nit is a contagious sometimes fatal\nrespiratory illness\nit often spreads through close contact\nwith an infected person\nsars is contagious and sometimes fatal\nrespiratory illness caused by a\ncoronavirus\nit appeared in 2009 in china\nit spread worldwide within a few months\nalthough it was quickly contained sars\nis a virus transmitted through droplets\nthat enters the air when someone with\nthe disease coughs sneezes or talks\nnow let's look at the impact of\ncoronavirus worldwide\nthe maps and the charts that i am going\nto show you now have been taken from an\norganization called our world in data\nthey largely focus on problems that the\nworld faces such as poverty different\ndiseases hunger climate change\nexistential crisis and inequality\ntheir main goal is to research and use\ndata to make progress against the\nworld's largest problems\nthe map that you see on your screens\nshows the total number of confirmed\ncoveted 19 cases till the 26th of april\nbelow you can see the color scale\nranging from 0 to 1 million\nthe countries with the least number of\ncases are marked in light orange color\nthe countries with cases between 5000 to\n10 000 cases have been depicted using\norange color\nthe countries with cases above 1 lakh\nare shown using red color\nwhile those above 5 lakhs have a dark\nred color\nyou can see from the map\nafrican nations have fewer cases\ncompared to nations in asia europe and\namerica\nchina has close to 84 000 cases\nindia has around 26 500 cases as of\napril 26th\niran has nearly 89 000 cases\nnow if you look at australia it has\ncases of around 6700\nin north america the united states has\nthe highest number of cases which is\nactually the highest throughout the\nworld\nin south america brazil has the maximum\nnumber of cases\nthe next map shows the confirmed cases\nof countries on the asian continent\nchina iran turkey india and saudi arabia\nhave the highest number of cases\nthen we have the map of europe\nin europe countries such as italy france\ngermany the united kingdom spain and\nrussia have the highest number of cases\nmoving ahead you can see a graph of\ncertain selected countries and the total\nconfirmed cases\ni have filtered out the total world\ncases than countries like the united\nstates spain italy germany france china\nand india\nyou will learn how to create a similar\ngraph when we will do our prediction\nanalysis\nlet's now have a look at the total\nnumber of daily confirmed cases\nso on the 26th of april the united\nstates had around 48 500 cases russia\nhad around 6000 cases brazil had nearly\n5 500 cases followed by the united\nkingdom with close to 5000 cases\nthe number of confirmed cases is lower\nthan the number of total cases\nthe reason being\nlimited testing in different countries\nacross the world\nnow looking at the total debts reported\nacross the world on this map the united\nstates has the highest number with over\n53 000 deaths so far followed by italy\nfrance the united kingdom and germany\nnext you can see a graph of a few\ncountries and their total death cases\ntill the 26th of april\nyou can see the united states spain\nitaly france china and india here\nwe will see how to build a similar graph\nin our demo\nup next on your screens is the map of\ndifferent countries and their fatality\nrate\nfatality rate is actually the ratio\nbetween the confirmed deaths and\nconfirmed cases\nfrance has the highest fatality rate\nwith over 18.22\nfollowed by the united kingdom at 13.69\npercent and italy at 13.51\nmoving ahead the next map shows the\ntotal number of covet 19 test numbers\nthe united states has conducted over 5\nmillion tests so far followed by russia\nwith over 2.8 million tests germany\nturkey canada and india have been\nconducting coveted 19 tests in large\nnumbers to deal with the situation\nnow each country is dealing with this\npandemic within its own capacity to make\nsure the situation doesn't worsen most\naffected countries have gone for a\nlockdown\nbut does lockdown work well the research\nsays yes\non the screens you can see the number of\ndays some countries have enforced a\npartial or full lockdown to curb the\nspread of coronavirus\nthis information has been taken from one\nof india's leading media groups india\ntoday data intelligence unit\nthe orange color represents full\nlockdown while the yellow color depicts\npartial lockdown\ncountries like australia india belgium\nthe united kingdom china the united\nstates france spain and others already\nhave announced lockdown for more than 40\ndays\nto make sure the lockdowns don't extend\nfurther and the situation improves\npeople are being advised to stay at home\nand avoid public gatherings\nnow let's look at where india stands\nwith its battle for corona virus\nas per the press information bureau of\nindia or pib\nhere are the figures and the situation\nreport till 5 pm on the 26th of april\nthe states that have been affected the\nmost are maharashtra gujarat delhi\nmadhya pradesh uttar pradesh and tamil\nnadu\nmaharashtra has over 7600 cases followed\nby gujarat with over 3000 cases\nmaharashtra also has the highest number\nof deaths reported which stands at 323\nnext is gujarat with 133 followed by\nmadhya pradesh at 99 deaths so far\nthere are still 312 cases yet to be\nassigned to the states which are under\nthe process of contact tracing\nyou can see on the top there are more\nthan 27 000 confirmed cases in india out\nof which 20 1777 are active cases\nmore than 6000 people have recovered\nfrom corona virus while the death tally\nstands at 872\nthe government is doing its best to\ncontrol the situation please follow the\nrules and guidelines issued by the local\nand central authorities this will help\nus fight the situation together\neffectively\nnow coming to the most important part of\nthis session which you guys have been\nwaiting for a while\nthe coronavirus outbreak prediction\nanalysis\nwe will analyze the outbreak of\ncoronavirus in the coming days across\nvarious countries worldwide visualize\nthem using charts and graphs and predict\nthe number of upcoming cases for the\nnext 20 days using polynomial regression\nand support vector machines model in\npython\nour data has information from the 22nd\nof january to the 28th of april\nthe data sets we will be using are taken\nfrom the repository operated by the\njohns hopkins university center of\nsystem science and engineering\nlet me show you where you can locate the\ndata sets\nso here is the github repository where\nyou can find the data sets and a few\nother resources\nit is being updated on a regular basis\ni would encourage you to go through this\nlink once\nnow\nlet me take you to the jupiter notebook\nwhere i have already implemented the\ncourse i'll run through it and explain\neach cell of code to make you understand\nwhat it does\nso this is my jupiter notebook\nfirst\nwe will import all the libraries\nnecessary for this analysis\nwe are importing the numpy and pandas\nthat are used for numerical computations\ndata manipulation and data analysis\nthen\ni'm importing the matplotlib library for\ncreating data visualizations\nnext i'm also going to import random\nmath and time libraries\nafter that from the sklearn library i am\nimporting the linear regression model as\nwell as certain functions such as strain\ntest split and polynomial features to\nbuild our polynomial regression\nsvr will help us create the support\nvector machines model\nwe will also use sql one matrix to\nimport functions such as\nmean squared error mean absolute error\nand so on\nthis we will use for calculating the\naccuracy of our model\nlet me now run this cell by hitting\nshift plus enter\nso you can see we have successfully run\nthe first cell\nnow\nwe will load our datasets\nwe'll be using three datasets for this\ndemo\nthe data sets are present in a github\nrepository maintained by johns hopkins\nuniversity to import the data sets i\nhave used the pandas library and read\nunderscore csv function\ni provided the url location of the files\nfollowed by the file name and the\nextension of the file which is\ndot csv\ni have loaded these three datasets to\nthree variable names\nnow let's run the cells to import all\nthe three data sets\nso first i have imported the confirmed\ncases data set\nnow to display the data set i am using\nthe dot head function to see the top\nfive rows from the data\nif i scroll down\nyou can see our confirmed underscore\ncases data set\nwe are displaying the first five rows\nfrom the data set\nit has information like province of the\nstate country or the region the latitude\nthe longitude\nand the date value starting from 22nd of\njanuary to the 27th of\napril\nnow let me go ahead\nand import the debts underscore reported\ndata set\nand now to display the data set i have\nused the dotted function\nso this is how\nthe depth data set looks like\nlet me now go ahead\nand import the recovered underscore\ncases data set\nand now to display the head of the data\nset\nso here is how the recovered\ndata set looks like or the recoveries\ndata set looks like\nfinally i load the data set which is all\nthe latest updates from across the globe\nregarding confirmed cases deaths\nrecovered and active cases\nlet's see the head of this latest data\nset\nas you can see it has information about\nthe province or the state\ncountry or the region\nwhen it was last updated\nthe latitude and the longitude of the\nlocation\nand it has data regarding the confirmed\ncases the deaths reported\nthe recovered cases the active cases and\nso on\nafter that i am extracting all the\ncolumn names from the confirmed cases\ndata frame using the dot keys function\nif i run this\nbelow you can see we have\nall the column names\nnext\nwe are extracting only the date columns\nthat have information of confirmed cases\ndeath cases and recovered cases using\nthe dot loc function\nwe are storing it in respective\nvariables\nthe first parameter that is colon\ntells us we need all the rows from the\ndata\nwhile the second parameter tells us we\nneed the data from the fourth column\ntill the last column\nwhich are the date values\nlet me run it\nlet's check the confirmed variable and\nsee what it has\nas you can see it has the date columns\nfrom the 22nd of jan till the 27th of\napril\nin the next cell of code i am basically\ncreating multiple empty lists to find\nthe total cases worldwide the total\nnumber of debts so far the mortality\nrate the recovered cases and the total\nactive cases\ni also want to find out the confirmed\ncases debts and recoveries for few\ncountries such as china italy united\nstates spain france germany uk russia\nand india\nso i have created empty lists for these\ncountries as well\nwe will see how to append values to this\nempty list in the next step\nlet me just run it\nnow using a for loop i am finding the\ntotal sum of confirmed cases death cases\nand recoveries made so far\nthese values i am appending to the empty\nlist world cases total debts\ntotal recovered and total active cases\nthe total active cases are calculated as\nthe confirmed cases\nminus the death cases minus the\nrecovered cases\nnow to calculate the mortality rate i am\nusing a simple formula which is the sum\nof the total debts divided by the total\nconfirmed cases\nthe recovery rate is also calculated by\ndividing the total recovered cases with\nthe total confirmed cases\nthen i am appending the values of\nconfirmed cases for each country using\nthe append function and selecting\ndifferent countries for which we had\ncreated an empty list\nsimilarly i am also appending the values\nfor debts and recoveries made for each\nof these countries\nlet's run it\nnow you can check the world cases each\nday\nso\nthese are the world cases that have come\nup each day\nand you can see the number has reached\nalmost\n30 lakhs now\nthen you can see the total death so far\nglobally\nso these are the deaths\nreported each day across the globe\nyou can see\nthere have been around\nmore than 2 lakhs casualties so far\nlet me collapse it\nnext i'm printing out the total sum of\nconfirmed cases\nso you can see it stands at around 30\nlakhs 41 764.\nsimilarly let me print out the total\ndeath sum\nso it's around 2 lakhs 11 167\nas of 27th of april\nnow also check the recovered sum\nit's around 8 lakhs 93 967.\nyou can also have a look at the cases\nbased on individual countries\nso here you can see the cases that have\ncome up so far in the united states\nwhich is u.s underscore cases\nso united states has nearly 9 lakhs 88\n1997 cases\nlet me collapse it\nnow let's check for india as well\nso in india the total cases\nhave reached 29\n451.\nlet me collapse it further\nyou can also have a look at the\nrecoveries for each country here i am\nprinting out the recoveries made in\nitaly\nnext\ni want to analyze the data for\nunderstanding the daily increase in\ncases across the world the daily\ncasualties and the daily recovery cases\nfor that\ni am using a user defined function to\ntrack the daily increase\nlet me run it\nokay\nnow to find the total worldwide increase\nin cases i am calling the above created\nuser defined function and assigning it\nto world daily increase variable\nsimilarly i am using the same method to\nfind the daily increase in cases for\ndifferent countries\nlet me show you the daily increase in\nconfirmed cases for spain\nso yesterday that is 27th of april speed\nhad\n2793 cases\nnow let's check for germany also\nas you can see\nyesterday germany had around 988 cases\nin the next cell i am finding out the\ndaily death cases across the world as\nwell as for a few countries\nlet me run it\nlet's check the daily deaths that have\nbeen reported in china\nso these are the\nvalues of daily deaths in china\nyou can see the numbers have\nsignificantly reduced\nalso let's check for the united kingdom\nas well\nso on 27th of april united kingdom had\n363 deaths reported\nmoving ahead i am also calculating the\ndaily recovery cases across the world\nand for some countries\nnow let's go ahead and check the data\nfrom india in terms of the daily\nrecovery cases\nso these are the values\nand if i scroll down you can see\nyesterday that is 27th of april india\nhad 614 recovered cases\nalso let's have a look at the world\nrecovery cases\nhere you can see on 27th of april\nnearly 28 234 people have recovered\nacross the globe\nwith that\nthe next step is to visualize our data\nso first i am finding out the list of\nunique countries using the list function\nand passing the latest data variable to\nfilter the different uni countries\nso here you can see the list of\ndifferent\ncountries\nnow the cell that you see on your\nscreens\nwe are using these lines of code to find\nthe country's confirmed cases\nthe death cases active cases recovery\ncases and mortality rate\nwe are finding the sum of the cases and\nappending it to empty list that we have\ncreated\nwe are also removing the countries that\nhave no cases at all and finally we are\nsorting the countries by the number of\nconfirmed cases\nso let me run it\nin this cell\nyou will be visualizing the data using a\ngradient styled color map to see the\ncountry name the number of confirmed\ncases and the number of deaths the\nnumber of recoveries and so on\ni have taken blue as my gradient\nlet me run it\nas you can see\nwe have an interesting table that looks\nreally attractive\non the left you can see the country\nnames\nthe table has been sorted in descending\norder of confirmed cases\nyou can see the dark blue color depicts\nthe countries with the highest number of\ncases\nin this case it's united states\nthen we have spain italy and france\nwe also have germany and united kingdom\nyou can also see the number of deaths\nand recoveries in the middle of this\ntable\nand to the right you can see the number\nof active cases and the mortality rate\nyou can see from the visual\nitaly\nhas the highest mortality rate\nfollowed by the united kingdom\nsimilarly\ni am finding out the list of unique\nprovinces using the list function and\npassing the latest data variable to\nfilter out different provinces\nin the next cell\nwe are finding the confirmed cases the\ndeath cases active cases recovery cases\nand mortality rate for each province of\na particular country\nwe are also finding the sum of the cases\nand appending it to empty list that we\nhave created\nwe are also removing the provinces that\nhave no cases at all\nand finally we are sorting the provinces\nby the number of confirmed cases\nso let's run it\nin this cell\nwe'll be visualizing the data of\ndifferent provinces using a\ngradient-styled color map to see the\nprovince or the state names the number\nof confirmed cases the number of deaths\nnumber of recoveries and so on\ni have taken red as my gradient\nlet's run it\nthere you go\nwe have our visual here on the screen\nnew york state has the highest number of\nconfirmed cases\nand the number of deaths as well\nthe hubei province in china\nhas had the highest recoveries which is\naround\n63 604\nand to the extreme right you can see the\nmortality rate\nthe next cell of code deals with\nhandling missing values or nan values\nit is important to handle such values\notherwise your data would be noisy and\nmessy and it will lead to inappropriate\nresults\nnow\ni am plotting a horizontal bar graph to\ncompare the number of cases in the\nunited states and outside of united\nstates\nthe outside united states cases have\nbeen calculated by subtracting the\nunited states confirmed cases\nfrom the country confirmed cases\nworldwide\ni have used plt dot bar h function to\nplot my horizontal bars\nwe have also given a title to this plot\nlet me run it\nso this is my bar graph\nthe blue bar represents the united\nstates confirmed cases\nand the red bar represents the cases\noutside of united states\nnow if you want to print the actual\nvalues of the cases in the united states\nand outside of united states you can do\nthat too\nas you can see\nthere are nearly 19 lakhs 58 592 cases\noutside of united states in united\nstates there are nearly\n9 lakhs\n3854 cases\nand the total cases stand at\n28 lakhs 96\n746 cases\nthen i want to show only 10 countries\nwith the most confirmed cases\nthe rest i want to group it into the\nothers category\nso using two empty lists i am going to\nfind out the top 10 countries with the\nhighest number of cases the remaining i\nam categorizing under others\nnow to show these countries i have\ndefined a function\nthat will plot my horizontal bar graph\nlet me run it\nnow using plot underscore bar underscore\ngraphs function we will create the bar\ngraph\ni have used the visual underscore unique\nunderscore countries and visual\nunderscore confirmed underscore cases\nvariables and given a title to the graph\nso here is our horizontal bar graph\nyou can see the united states has the\nhighest number of cases\nthe next is spain\nthe united kingdom is at the 6th\nposition\nrussia is at the 10th place\nand on the top you can see the others\ncategory as well\nnow we will create a pie chart to plot\nthe same graph\nfor that i have created another function\ncalled plot underscore pi underscore\ncharts\nlet me just run it\nnow using plot underscore pi underscore\ncharts function name we will create the\npie chart\ni've again used visual underscore unique\nunderscore countries and visual\nunderscore confirmed underscore cases\nvariables and given a title to the graph\nthere you go\nwe have our pie chart on the screen\nand on the right you can see the legend\nwhich shows the different country names\nthe maximum area in the pie chart has\nbeen occupied by united states\nthen we have the others category\nfollowed by\nspain italy and france\nand follow the same procedure to show 10\nprovinces with the most confirmed cases\nthe rest i will group into the others\ncategory for that i will be using\ntwo empty lists\nvisual underscore unique underscore\nprovinces and visual underscore\nconfirmed underscore cases 2\nlet's plot the graph using plot\nunderscore bar underscore graphs\nfunction\nso here\nyou can see the bar graph\nnew york province or the new york state\nhas the highest number of cases\nthen we have new jersey\nfollowed by uber provisions in china and\nmassachusetts\nand on the top you can see the others\ncategory\nthe next cell of code\nlooks pretty complicated\nit is essentially trying to plot a pie\nchart for different countries along with\nits several states and provinces\nhere we want to show only the top 10\nstates with the highest number of cases\nlet me run it\nnow\ni want to show the pie chart for the\nunited states and its top 10 states\nlet me do that\nas you can see we have our pie chart and\non the right you can see we have the\nlegend for 10 states\nso the maximum area in the pie chart has\nbeen occupied by\nnew york and then\nwe have the other\nstates\nnext is new jersey followed by\nmassachusetts and california\nlet me also check the confirmed cases\nfor different states in france\ni'll change this to\nfrance\nso here is our pie chart\nwhich has\n10 different states in france\nwith that let's now come to building our\nmodel using polynomial regression and\nsupport vector machines\nin this step i am converting all the\ndates and the cases in the form of a\nnumpy array using the np dot array\nfunction\nlet's run it\nsince our prediction will be for the\nnext 20 days so i have created a\nvariable called days underscore future\nand have assigned a value of 20\nthen i'm adding the last 20 days to the\ntotal number of days we have\nnow let me show you the values of future\nunderscore forecast\nso here\nis a list of values basically is the\nnumber of total days\nnext\nwe are converting all the integers into\ndate time values for better\nvisualization\nnow it's time to split our data into\ntraining and testing sets\nfor that i am using the train underscore\ntest underscore split function\ni have taken days since 22nd of jan and\nthe world cases as my parameters\ni'll be using 75 percent of\ndata for training the model and 25\npercent for testing the model\nwe will now transform our data for\npolynomial regression\nfor that i will use the polynomial\nfeatures function and fit underscore\ntransform method to transform our\ntraining testing and future forecast\ndata\nnext i will build the polynomial\nregression model using the linear\nregression function\nand i have used the linear underscore\nmodel dot fit function to fit the\ntraining data\nafter that\ni have used the predict function to\npredict the test data set values\nfinally i am printing the mean absolute\nerror value and the mean squared error\nvalue as well\nso you can see the results here\nlet's now plot the graph between the\ntest data set and the values from the\npredicted polynomial regression model\nso below you can see the graph here\nthe blue line represents the test data\nand the red line represents our\npolynomial regression predictions\nnow let's start building our model using\na support vector machines algorithm\nsvm uses different parameters to build a\nmodel these parameters are kernel c\ngamma epsilon shrinking and svm\nunderscore grid\nkernel specifies the kernel type to be\nused in the algorithm\nit must be one of linear poly rbf\nsigmoid precomputed or callable\nif nothing is given rbf will be used by\ndefault\nc is a regularization parameter\ngamma is the kernel coefficient of rbf\npoly and sigmoid\nepsilon specifies the epsilon tube\nwithin which no penalty is associated in\nthe training loss function\nshrinking takes boolean values true or\nfalse\nsvm grid has all the values parameters\npassed to it\nhere we are creating the support vector\nregressors and then using the fit\nfunction to fit our training data set\nfinally we are predicting the values for\nfuture data as well\nlet me now print the mae and the msc\nvalues and display the graph of the\nprediction\nas you can see\nthe blue line represents the test data\nand the red line represents the svm\npredictions\nnow let's create some more\nvisualizations to understand the model\nand our data better\nhere i am plotting the number of\ncoronavirus cases over time\ni have used the adjusted dates and the\nworld cases to plot the graph\ni've also assigned the x labels and the\ny labels\nlet me run it\nas you can see from the graph\nthe total number of cases\nhave reached close to\n30 lakhs now\nnext in this graph we are showing the\ntotal number of coronavirus deaths over\ntime\ni've also used the x label and the y\nlabel\nyou can see the total number of deaths\nhave reached over\n2 lakhs\nsimilarly in the next graph\nwe can see the total number of recovered\ncases which are more than 8 lakhs\nthen\nwe are also looking at the total number\nof active cases across the globe\nso here is the graph for that\nyou can see the number of coronavirus\nactive cases over\ntime am now creating a bar graph\nwith adjusted dates and the world daily\nincrease as my variables\nas you can see on some of the days the\ncases have gone very high\nreaching 100 000 in a day\nthe next graph we are plotting to show\nthe world daily increase in confirmed\ndeath cases\nso here is the\nbar graph for that\nyou can see there are certain days that\nhave reported\nmore than 10 000 deaths in a day\nthe next graph shows the world daily\nincrease in confirmed recovery cases\nso here is the graph for that\nmoving ahead i'll plot the graphs to\nshow the number of coronavirus cases\nover time\nlet's now plot the actual graph\ni have taken adjusted underscore dates\nworld cases linear underscore trade\nvariables as my parameter\nlet me run it\nyou can see this is our graph\nthe blue line represents the confirmed\nwe will check for the svm model as well\nso below is the graph\nagain the\nnow\nlet me print out the predicted values\nfrom the polynomial regression model as\na data frame\nso you can see\nwe have our predicted\nconfirmed cases for the next 20 days\nthat is from the 28th of april to the\n17th of may\nlikewise\nlet me also print out the predicted\nvalues for the svm model as a data frame\nso here are\nour predicted values from the svm model\nin the next graph we are plotting the\ndepths and the recoveries in the same\ngraph\nso here is our line graph\nyou can see clearly the number of\nrecoveries are way higher than the\nnumber of deaths which is actually a\ngood sign\nmoving forward\nthe next graph shows a line chart with\nthe number of coronavirus deaths versus\nthe number of coronavirus recoveries\nso here is the graph\nthe x-axis has the number of recoveries\nand the y-axis has the number of\ntotal deaths\nnow this cell of code looks pretty huge\nwe want to plot the total number of\nconfirmed cases the daily increase in\nconfirmed cases the increase in debts\nand the increase in recoveries for\ndifferent countries across the globe\nlet me run this cell\nso using the country underscore plot\nfunction that was defined above we will\nbe plotting the data for various\ncountries\nlet me start with china first\nso you can see there are four graphs in\ntotal if i scroll down the first you see\nhere is china confirmed cases\nnext is the china daily increase in\nconfirmed cases\nso you can see in one of the days it had\na huge spike\nin the increase in confirmed cases then\nwe have the china daily increase in\ndebts\nand finally we have the daily increase\nin recoveries\nnow let's check for italy\nlet me scroll down you can see the same\nplots for italy as well the first we\nhave total\nconfirmed cases in italy\nwhich is nearly around\n2 lakhs\nthen we have the daily increase in\nconfirmed cases\nnext we have the italy\ndaily increase in deaths\nand finally we have the increase in\nrecoveries\nlet's have a look at how the graphs for\nindia looks like\nyou see the total well\nlet's also see for the united states\nso here you can see all the four graphs\nfor the united states\nspain\nfrance germany and and they have been\ndepicted using different colors\non the top left you can see the legend\nnow this graph looks similar to the one\nthat we saw in our slides\nnext i'm plotting the total coronavirus\ndepth\nso here is our graph\nfinally let us look at the total\ncoronavirus cases in these countries\nthe last three graphs look similar to\nwhat we have used in the slides from our\nworld in data\nnow it's my responsibility to call out\nall the safety precautions that you\nshould take to make sure you are safe\nfrom being attacked by coronavirus\nfirst and foremost do not panic\nduring these unprecedented times if you\nfeel you are showing symptoms of\ncoronavirus do not wait please consult a\ndoctor immediately\nstay at home to ensure you are not\nexposed to the virus outside\nmaintain social distancing with people\nif you are talking to someone outside\nand if someone is sick\ndo not forget to wash your hands\nregularly using soaps sanitizers and\nother disinfectants\nalways remember to wear a mask while you\nare going outside\nmake sure to practice respiratory\nhygiene such as\ncover your mouth while coughing and try\neating healthy food\nall these precautions will help you\navoid coronavirus and make sure you are\nsafe and healthy\nnow moving on to the final section let\nme tell you how simply learn can help\nyou start your career in machine\nlearning\nso let me take you to our website first\nokay so i am on the chrome browser\nlet me search for simplylearn.com\nand here\nunder what you want to learn let me type\nmachine learning it will show me the\nrelevant courses that simply learn\noffers\nin the machine learning category\nso you can see here there are multiple\ncourses\nfirst let me open the\nfirst link\nand let's open the second link as well\nso\nso this is the post graduate program in\nai and machine learning which is in\ncollaboration with purdue university and\nibm\nif i scroll down\nyou can see the key features of this\ncourse\nso we'll get purdue alumni association\nmembership\nindustry recognized ibm certificates\nenrollment to simply launch job assist\nthere's 25 plus hands-on projects on gpu\nenabled labs you have\n450 plus hours of applied learning\ncapstone projects in three domains and\nmuch more\nand here you can see on the right this\nis the certificate that you will get\nafter completing this program\nand you will also get\ncertificates recognized by ibm\nand another key feature of this course\nis you can enroll to simply learn job\nassist program so you will get im jobs\ncrew membership for six months resume\nassistance and career monitoring design\nto interview preparation and career\naffairs\nif i scroll further here you can see the\nlearning path\nand you will learn about python for data\nscience\nthis machine learning\ndeep learning with tensorflow and carers\nthat's advanced deep learning and\ncomputer vision you will also learn\nabout natural language processing that\nis nlp and speech recognition\nwe have reinforcement learning\nand you also have the opportunity to\nselect a few electives so we have ibm\nwatson for chatbots machine learning\nwith r there's git and github training\nand two others\nhere you can see\nthe skills that will be covered so you\nwill learn about statistics which is a\ncore component of machine learning we\nlearn about python supervised learning\nthere's dance computer vision tensorflow\nthere's reinforcement learning speech\nrecognition you will also learn about\nnumpy pandas and other libraries\nand here you can see these are some of\nthe tools that will be covered in this\ncourse\nif i scroll further now this is the\nimportant section you can see the\nindustry projects that you will get to\nwork on\nso this is\non twitter there's one on zumato one on\nuber\nand mercedes benz as well\nthey are our course advisors\nso please go ahead and enroll to this\npostgraduate program in ai machine\nlearning if you want to kick start your\ncareer in\nmachine learning\nnow the next course we have is machine\nlearning certification course\nif i scroll down you can see the details\nof this course\nso you will gain expertise with 25 plus\nhands-on exercises you will get to work\non four real-life industry based\nprojects with integrated labs there will\nbe dedicated mentoring sessions from\nindustry experts 44 hours of instructed\nled training with certification\nhere on the right you can see the skills\nthat will be covered\nso we learn about time series modeling\nlinear and logistic regression we will\nalso learn about support vector machines\ngames clustering knife base decision\ntrees\nrandom forest\nand you learn the concepts of bagging\nand boosting and deep learning\nfundamentals as well\nhere you can see the\ncourse content\nand if i scroll further now these are\nthe projects that you will get to work\non\nand finally you will receive this\ncertificate once you complete the course\nthank you all for watching this full\ncourse video tutorial on machine\nlearning with python i hope you liked it\nif you have any questions then please\nput them in the comments section our\nteam will help you solve your queries\nthanks again stay tuned for more from\nsimply loan\nhi there if you like this video\nsubscribe to the simply learn youtube\nchannel and click here to watch similar\nvideos to nerd up and get certified\nclick here\nyou\n",
  "words": [
    "ever",
    "amazed",
    "google",
    "home",
    "amazon",
    "alexa",
    "assist",
    "finding",
    "information",
    "asked",
    "voice",
    "ever",
    "wondered",
    "ecommerce",
    "companies",
    "send",
    "personalized",
    "emails",
    "shopping",
    "suggestions",
    "based",
    "products",
    "bought",
    "online",
    "recently",
    "well",
    "machine",
    "learning",
    "virtual",
    "assistants",
    "product",
    "recommendations",
    "work",
    "swiftly",
    "video",
    "going",
    "look",
    "machine",
    "learning",
    "roadmap",
    "2022",
    "give",
    "process",
    "need",
    "follow",
    "build",
    "successful",
    "career",
    "machine",
    "learning",
    "get",
    "started",
    "make",
    "sure",
    "subscribe",
    "simply",
    "learn",
    "channel",
    "hit",
    "bell",
    "icon",
    "never",
    "miss",
    "update",
    "simplylearn",
    "machine",
    "learning",
    "machine",
    "learning",
    "subset",
    "artificial",
    "intelligence",
    "uses",
    "data",
    "algorithms",
    "statistical",
    "techniques",
    "build",
    "intelligent",
    "systems",
    "systems",
    "learn",
    "improve",
    "experience",
    "goal",
    "machine",
    "learning",
    "create",
    "computer",
    "models",
    "imitate",
    "human",
    "behavior",
    "machine",
    "learning",
    "found",
    "usage",
    "almost",
    "every",
    "business",
    "sector",
    "cars",
    "medical",
    "imaging",
    "diagnostics",
    "speech",
    "recognition",
    "facial",
    "recognition",
    "online",
    "fraud",
    "detection",
    "possible",
    "machine",
    "learning",
    "recent",
    "years",
    "ai",
    "ml",
    "technologies",
    "made",
    "several",
    "breakthroughs",
    "rising",
    "demand",
    "ai",
    "applications",
    "across",
    "different",
    "industries",
    "led",
    "significant",
    "growth",
    "machine",
    "learning",
    "machine",
    "learning",
    "market",
    "expected",
    "grow",
    "9",
    "billion",
    "dollars",
    "2022",
    "ac",
    "agr",
    "44",
    "another",
    "report",
    "suggests",
    "machine",
    "learning",
    "market",
    "valued",
    "billion",
    "us",
    "dollars",
    "2019",
    "projected",
    "reach",
    "million",
    "usc",
    "2027",
    "growing",
    "cagr",
    "2020",
    "keeping",
    "facts",
    "forecasts",
    "mind",
    "let",
    "look",
    "roadmap",
    "critical",
    "skills",
    "required",
    "build",
    "career",
    "machine",
    "learning",
    "first",
    "foremost",
    "programming",
    "skills",
    "machine",
    "learning",
    "engineer",
    "programming",
    "languages",
    "form",
    "building",
    "blocks",
    "develop",
    "complex",
    "machine",
    "learning",
    "models",
    "need",
    "learn",
    "least",
    "one",
    "programming",
    "language",
    "preferably",
    "python",
    "r",
    "familiar",
    "various",
    "computer",
    "science",
    "concepts",
    "data",
    "structures",
    "including",
    "stack",
    "queues",
    "trees",
    "graphs",
    "algorithms",
    "searching",
    "sorting",
    "dynamic",
    "greedy",
    "programming",
    "space",
    "time",
    "complexity",
    "etc",
    "also",
    "need",
    "know",
    "libraries",
    "like",
    "numpy",
    "pandas",
    "matplotlib",
    "cbon",
    "tidier",
    "ggplot",
    "data",
    "analysis",
    "visualization",
    "second",
    "skill",
    "need",
    "possess",
    "applied",
    "mathematics",
    "solving",
    "business",
    "problems",
    "using",
    "machine",
    "learning",
    "use",
    "machine",
    "learning",
    "algorithms",
    "understand",
    "mechanisms",
    "behind",
    "algorithms",
    "need",
    "good",
    "knowledge",
    "mathematical",
    "concepts",
    "linear",
    "algebra",
    "calculus",
    "statistics",
    "probability",
    "mathematics",
    "machine",
    "learning",
    "processing",
    "numbers",
    "understanding",
    "happening",
    "happening",
    "obtain",
    "best",
    "results",
    "third",
    "skill",
    "become",
    "machine",
    "learning",
    "expert",
    "data",
    "wrangling",
    "sql",
    "data",
    "analysts",
    "machine",
    "learning",
    "specialists",
    "often",
    "work",
    "raw",
    "data",
    "collected",
    "various",
    "sources",
    "fit",
    "analysis",
    "observed",
    "eighty",
    "percent",
    "data",
    "analysis",
    "spend",
    "much",
    "time",
    "data",
    "wrangling",
    "crucial",
    "machine",
    "learning",
    "experts",
    "clean",
    "structure",
    "enrich",
    "raw",
    "data",
    "desired",
    "format",
    "make",
    "ready",
    "analysis",
    "using",
    "data",
    "wrangling",
    "techniques",
    "sql",
    "another",
    "crucial",
    "set",
    "carry",
    "machine",
    "learning",
    "tasks",
    "involve",
    "using",
    "data",
    "stored",
    "form",
    "tables",
    "present",
    "inside",
    "relational",
    "databases",
    "good",
    "understanding",
    "sql",
    "commands",
    "enables",
    "store",
    "manipulate",
    "retrieve",
    "handle",
    "structured",
    "data",
    "next",
    "skill",
    "set",
    "machine",
    "learning",
    "algorithms",
    "incredibly",
    "essential",
    "grasp",
    "standard",
    "machine",
    "learning",
    "algorithms",
    "implementing",
    "machine",
    "learning",
    "techniques",
    "requires",
    "choosing",
    "suitable",
    "model",
    "determining",
    "correct",
    "learning",
    "method",
    "understanding",
    "hyper",
    "parameter",
    "tuning",
    "really",
    "good",
    "different",
    "supervised",
    "unsupervised",
    "reinforcement",
    "learning",
    "algorithms",
    "linear",
    "regression",
    "logistic",
    "regression",
    "svm",
    "knn",
    "decision",
    "trees",
    "k",
    "means",
    "clustering",
    "etc",
    "coming",
    "final",
    "scale",
    "data",
    "modeling",
    "evaluation",
    "objective",
    "machine",
    "learning",
    "engineer",
    "train",
    "best",
    "performing",
    "model",
    "possible",
    "depending",
    "problem",
    "hand",
    "need",
    "choose",
    "suitable",
    "error",
    "measure",
    "evaluation",
    "strategy",
    "machine",
    "learning",
    "model",
    "vulnerable",
    "part",
    "machine",
    "learning",
    "candidate",
    "resume",
    "absence",
    "experience",
    "working",
    "diverse",
    "machine",
    "learning",
    "projects",
    "essential",
    "skills",
    "acquired",
    "build",
    "impressive",
    "machine",
    "learning",
    "portfolio",
    "highlighting",
    "exciting",
    "machine",
    "learning",
    "projects",
    "machine",
    "learning",
    "engineers",
    "need",
    "portfolio",
    "showcases",
    "expertise",
    "implement",
    "machine",
    "learning",
    "techniques",
    "problems",
    "resume",
    "ready",
    "look",
    "best",
    "machine",
    "learning",
    "jobs",
    "top",
    "product",
    "companies",
    "startups",
    "become",
    "machine",
    "learning",
    "engineer",
    "data",
    "scientist",
    "data",
    "analyst",
    "research",
    "scientist",
    "top",
    "companies",
    "hiring",
    "machine",
    "learning",
    "roles",
    "google",
    "amazon",
    "ibm",
    "uber",
    "grammar",
    "navidia",
    "linkedin",
    "machine",
    "learning",
    "engineers",
    "highest",
    "paid",
    "professionals",
    "world",
    "according",
    "national",
    "average",
    "salary",
    "machine",
    "learning",
    "engineers",
    "united",
    "states",
    "one",
    "lakh",
    "31",
    "000",
    "us",
    "dollars",
    "per",
    "year",
    "india",
    "earn",
    "nearly",
    "8",
    "lakh",
    "rupees",
    "per",
    "annum",
    "salary",
    "may",
    "vary",
    "based",
    "experience",
    "industry",
    "applying",
    "company",
    "policy",
    "immense",
    "opportunities",
    "machine",
    "learning",
    "sectors",
    "manufacturing",
    "logistics",
    "retail",
    "healthcare",
    "interested",
    "career",
    "machine",
    "learning",
    "start",
    "journey",
    "simplylearn",
    "postgraduate",
    "program",
    "ai",
    "machine",
    "learning",
    "partnership",
    "purdue",
    "university",
    "collaboration",
    "ibm",
    "learn",
    "machine",
    "learning",
    "skills",
    "work",
    "programming",
    "examples",
    "program",
    "also",
    "offers",
    "job",
    "assistance",
    "know",
    "learning",
    "part",
    "tools",
    "covered",
    "projects",
    "working",
    "please",
    "click",
    "course",
    "page",
    "link",
    "description",
    "video",
    "cover",
    "everything",
    "need",
    "know",
    "become",
    "expert",
    "machine",
    "learning",
    "experienced",
    "instructors",
    "good",
    "industry",
    "experience",
    "take",
    "course",
    "first",
    "understand",
    "basics",
    "machine",
    "learning",
    "short",
    "animated",
    "video",
    "know",
    "essential",
    "applications",
    "machine",
    "learning",
    "understand",
    "machine",
    "learning",
    "concepts",
    "learn",
    "mathematics",
    "statistics",
    "linear",
    "algebra",
    "crucial",
    "focus",
    "vital",
    "machine",
    "learning",
    "algorithms",
    "linear",
    "regression",
    "logistic",
    "regression",
    "decision",
    "trees",
    "random",
    "forest",
    "k",
    "nearest",
    "neighbors",
    "also",
    "learn",
    "regularization",
    "dimensionality",
    "reduction",
    "principal",
    "component",
    "analysis",
    "perform",
    "prediction",
    "analysis",
    "recently",
    "held",
    "elections",
    "well",
    "finally",
    "study",
    "machine",
    "learning",
    "roadmap",
    "2021",
    "let",
    "begin",
    "know",
    "humans",
    "learn",
    "past",
    "experiences",
    "machines",
    "follow",
    "instructions",
    "given",
    "humans",
    "humans",
    "train",
    "machines",
    "learn",
    "past",
    "data",
    "humans",
    "much",
    "faster",
    "well",
    "called",
    "machine",
    "learning",
    "lot",
    "learning",
    "also",
    "understanding",
    "reasoning",
    "today",
    "learn",
    "basics",
    "machine",
    "learning",
    "paul",
    "loves",
    "listening",
    "new",
    "songs",
    "either",
    "likes",
    "dislikes",
    "paul",
    "decides",
    "basis",
    "song",
    "tempo",
    "genre",
    "intensity",
    "gender",
    "voice",
    "simplicity",
    "let",
    "use",
    "tempo",
    "intensity",
    "tempo",
    "ranging",
    "relaxed",
    "fast",
    "whereas",
    "intensity",
    "ranging",
    "light",
    "soaring",
    "see",
    "paul",
    "likes",
    "song",
    "fast",
    "tempo",
    "soaring",
    "intensity",
    "dislikes",
    "song",
    "relaxed",
    "tempo",
    "light",
    "intensity",
    "know",
    "paul",
    "choices",
    "let",
    "say",
    "paul",
    "listens",
    "new",
    "song",
    "let",
    "name",
    "song",
    "song",
    "fast",
    "tempo",
    "soaring",
    "intensity",
    "lies",
    "somewhere",
    "looking",
    "data",
    "guess",
    "whether",
    "paul",
    "like",
    "song",
    "correct",
    "paul",
    "likes",
    "song",
    "looking",
    "paul",
    "past",
    "choices",
    "able",
    "classify",
    "unknown",
    "song",
    "easily",
    "right",
    "let",
    "say",
    "paul",
    "listens",
    "new",
    "song",
    "let",
    "label",
    "song",
    "b",
    "song",
    "b",
    "lies",
    "somewhere",
    "medium",
    "tempo",
    "medium",
    "intensity",
    "neither",
    "relaxed",
    "fast",
    "neither",
    "light",
    "soaring",
    "guess",
    "whether",
    "ball",
    "likes",
    "able",
    "guess",
    "whether",
    "paul",
    "like",
    "dislike",
    "choices",
    "unclear",
    "correct",
    "could",
    "easily",
    "classify",
    "song",
    "choice",
    "became",
    "complicated",
    "case",
    "song",
    "b",
    "yes",
    "machine",
    "learning",
    "comes",
    "let",
    "see",
    "example",
    "song",
    "b",
    "draw",
    "circle",
    "around",
    "song",
    "b",
    "see",
    "four",
    "words",
    "like",
    "whereas",
    "one",
    "would",
    "dislike",
    "go",
    "majority",
    "votes",
    "say",
    "paul",
    "definitely",
    "like",
    "song",
    "basic",
    "machine",
    "learning",
    "algorithm",
    "also",
    "called",
    "k",
    "nearest",
    "neighbors",
    "small",
    "example",
    "one",
    "many",
    "machine",
    "learning",
    "algorithms",
    "quite",
    "easy",
    "right",
    "believe",
    "happens",
    "choices",
    "become",
    "complicated",
    "case",
    "song",
    "b",
    "machine",
    "learning",
    "comes",
    "learns",
    "data",
    "builds",
    "prediction",
    "model",
    "new",
    "data",
    "point",
    "comes",
    "easily",
    "predict",
    "data",
    "better",
    "model",
    "higher",
    "accuracy",
    "many",
    "ways",
    "machine",
    "learns",
    "could",
    "either",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "reinforcement",
    "learning",
    "let",
    "first",
    "quickly",
    "understand",
    "supervised",
    "learning",
    "suppose",
    "friend",
    "gives",
    "one",
    "million",
    "coins",
    "three",
    "different",
    "currencies",
    "say",
    "one",
    "rupee",
    "one",
    "euro",
    "one",
    "dirham",
    "coin",
    "different",
    "weights",
    "example",
    "coin",
    "one",
    "rupee",
    "weighs",
    "three",
    "grams",
    "one",
    "euro",
    "weighs",
    "seven",
    "grams",
    "one",
    "dirham",
    "weighs",
    "4",
    "grams",
    "model",
    "predict",
    "currency",
    "coin",
    "weight",
    "becomes",
    "feature",
    "coins",
    "currency",
    "becomes",
    "label",
    "feed",
    "data",
    "machine",
    "learning",
    "model",
    "learns",
    "feature",
    "associated",
    "slave",
    "example",
    "learn",
    "coin",
    "3",
    "grams",
    "1",
    "rupee",
    "coin",
    "let",
    "give",
    "new",
    "coin",
    "machine",
    "basis",
    "weight",
    "new",
    "coin",
    "model",
    "predict",
    "currency",
    "hence",
    "supervised",
    "learning",
    "uses",
    "labeled",
    "data",
    "train",
    "model",
    "machine",
    "knew",
    "features",
    "object",
    "also",
    "labels",
    "associated",
    "features",
    "note",
    "let",
    "move",
    "unsupervised",
    "learning",
    "see",
    "difference",
    "suppose",
    "cricket",
    "data",
    "set",
    "various",
    "players",
    "respective",
    "scores",
    "wickets",
    "taken",
    "feed",
    "data",
    "set",
    "machine",
    "machine",
    "identifies",
    "pattern",
    "player",
    "performance",
    "plots",
    "data",
    "respective",
    "wickets",
    "runs",
    "looking",
    "data",
    "clearly",
    "see",
    "two",
    "clusters",
    "one",
    "cluster",
    "players",
    "scored",
    "higher",
    "runs",
    "took",
    "less",
    "wickets",
    "cluster",
    "players",
    "scored",
    "less",
    "runs",
    "took",
    "many",
    "wickets",
    "interpret",
    "two",
    "clusters",
    "batsmen",
    "bowlers",
    "important",
    "point",
    "note",
    "labels",
    "batsmen",
    "bowlers",
    "hence",
    "learning",
    "unlabeled",
    "data",
    "unsupervised",
    "learning",
    "saw",
    "supervised",
    "learning",
    "data",
    "labeled",
    "unsupervised",
    "learning",
    "data",
    "unlabeled",
    "reinforcement",
    "learning",
    "reward",
    "based",
    "learning",
    "say",
    "works",
    "principle",
    "feedback",
    "let",
    "say",
    "provide",
    "system",
    "image",
    "dog",
    "ask",
    "identify",
    "system",
    "identifies",
    "cat",
    "give",
    "negative",
    "feedback",
    "machine",
    "saying",
    "dog",
    "image",
    "machine",
    "learn",
    "feedback",
    "finally",
    "comes",
    "across",
    "image",
    "dog",
    "able",
    "classify",
    "correctly",
    "reinforcement",
    "learning",
    "generalize",
    "machine",
    "learning",
    "model",
    "let",
    "see",
    "flowchart",
    "input",
    "given",
    "machine",
    "learning",
    "model",
    "gives",
    "output",
    "according",
    "algorithm",
    "applied",
    "right",
    "take",
    "output",
    "final",
    "result",
    "else",
    "provide",
    "feedback",
    "training",
    "model",
    "ask",
    "predict",
    "learns",
    "hope",
    "understood",
    "supervised",
    "unsupervised",
    "learning",
    "let",
    "quick",
    "quiz",
    "determine",
    "whether",
    "given",
    "scenarios",
    "uses",
    "supervised",
    "unsupervised",
    "learning",
    "simple",
    "right",
    "scenario",
    "1",
    "facebook",
    "recognizes",
    "friend",
    "picture",
    "album",
    "tagged",
    "photographs",
    "scenario",
    "2",
    "netflix",
    "recommends",
    "new",
    "movies",
    "based",
    "someone",
    "past",
    "movie",
    "choices",
    "scenario",
    "3",
    "analyzing",
    "bank",
    "data",
    "suspicious",
    "transactions",
    "flagging",
    "fraud",
    "transactions",
    "think",
    "wisely",
    "comment",
    "answers",
    "moving",
    "sometimes",
    "wonder",
    "machine",
    "learning",
    "possible",
    "today",
    "era",
    "well",
    "today",
    "humongous",
    "data",
    "available",
    "everybody",
    "online",
    "either",
    "making",
    "transaction",
    "surfing",
    "internet",
    "generating",
    "huge",
    "amount",
    "data",
    "every",
    "minute",
    "data",
    "friend",
    "key",
    "analysis",
    "also",
    "memory",
    "handling",
    "capabilities",
    "computers",
    "largely",
    "increased",
    "helps",
    "process",
    "huge",
    "amount",
    "data",
    "hand",
    "without",
    "delay",
    "yes",
    "computers",
    "great",
    "computational",
    "powers",
    "lot",
    "applications",
    "machine",
    "learning",
    "name",
    "machine",
    "learning",
    "used",
    "healthcare",
    "diagnostics",
    "predicted",
    "doctor",
    "review",
    "sentiment",
    "analysis",
    "tech",
    "giants",
    "social",
    "media",
    "another",
    "interesting",
    "application",
    "machine",
    "learning",
    "fraud",
    "detection",
    "finance",
    "sector",
    "also",
    "predict",
    "customer",
    "churn",
    "sector",
    "booking",
    "gap",
    "must",
    "encountered",
    "surge",
    "pricing",
    "often",
    "says",
    "fare",
    "trip",
    "updated",
    "continue",
    "booking",
    "yes",
    "please",
    "getting",
    "late",
    "office",
    "well",
    "interesting",
    "machine",
    "learning",
    "model",
    "used",
    "global",
    "taxi",
    "giant",
    "uber",
    "others",
    "differential",
    "pricing",
    "real",
    "time",
    "based",
    "demand",
    "number",
    "cars",
    "available",
    "bad",
    "weather",
    "rush",
    "r",
    "etc",
    "use",
    "surge",
    "pricing",
    "model",
    "ensure",
    "need",
    "cab",
    "get",
    "one",
    "also",
    "uses",
    "predictive",
    "modeling",
    "predict",
    "demand",
    "high",
    "goal",
    "drivers",
    "take",
    "care",
    "demand",
    "search",
    "pricing",
    "minimized",
    "great",
    "hey",
    "siri",
    "remind",
    "book",
    "cab",
    "6",
    "pm",
    "today",
    "ok",
    "remind",
    "thanks",
    "problem",
    "comment",
    "interesting",
    "everyday",
    "examples",
    "around",
    "machines",
    "learning",
    "amazing",
    "jobs",
    "machine",
    "learning",
    "basics",
    "today",
    "site",
    "machine",
    "learning",
    "improved",
    "lives",
    "number",
    "wonderful",
    "ways",
    "today",
    "let",
    "talk",
    "rahul",
    "simply",
    "learn",
    "top",
    "10",
    "applications",
    "machine",
    "learning",
    "first",
    "let",
    "talk",
    "virtual",
    "personal",
    "assistants",
    "google",
    "assistant",
    "alexa",
    "cortana",
    "siri",
    "used",
    "one",
    "least",
    "point",
    "lives",
    "help",
    "improve",
    "lives",
    "great",
    "number",
    "ways",
    "example",
    "could",
    "tell",
    "call",
    "someone",
    "could",
    "tell",
    "play",
    "music",
    "could",
    "tell",
    "even",
    "schedule",
    "appointment",
    "things",
    "actually",
    "work",
    "first",
    "record",
    "whatever",
    "saying",
    "send",
    "server",
    "usually",
    "cloud",
    "decode",
    "help",
    "machine",
    "learning",
    "neural",
    "networks",
    "provide",
    "output",
    "ever",
    "noticed",
    "systems",
    "work",
    "well",
    "without",
    "internet",
    "server",
    "could",
    "contacted",
    "next",
    "let",
    "talk",
    "traffic",
    "predictions",
    "say",
    "wanted",
    "travel",
    "buckingham",
    "palace",
    "lodge",
    "cricket",
    "ground",
    "first",
    "thing",
    "would",
    "probably",
    "get",
    "google",
    "maps",
    "search",
    "let",
    "put",
    "path",
    "take",
    "get",
    "large",
    "cricket",
    "ground",
    "map",
    "combination",
    "red",
    "yellow",
    "blue",
    "blue",
    "regions",
    "signify",
    "clear",
    "road",
    "wo",
    "encounter",
    "traffic",
    "yellow",
    "indicate",
    "slightly",
    "congested",
    "red",
    "means",
    "heavily",
    "congested",
    "let",
    "look",
    "map",
    "different",
    "version",
    "map",
    "told",
    "red",
    "means",
    "heavily",
    "congested",
    "yellow",
    "means",
    "slow",
    "moving",
    "blue",
    "means",
    "clear",
    "exactly",
    "google",
    "able",
    "tell",
    "traffic",
    "clear",
    "slow",
    "moving",
    "heavily",
    "congested",
    "help",
    "machine",
    "learning",
    "help",
    "two",
    "important",
    "measures",
    "first",
    "average",
    "time",
    "taken",
    "specific",
    "days",
    "specific",
    "times",
    "route",
    "second",
    "one",
    "real",
    "time",
    "location",
    "data",
    "vehicles",
    "google",
    "maps",
    "help",
    "sensors",
    "popular",
    "map",
    "services",
    "bing",
    "maps",
    "maps",
    "dot",
    "go",
    "next",
    "social",
    "media",
    "personalization",
    "say",
    "want",
    "buy",
    "drone",
    "amazon",
    "want",
    "buy",
    "dji",
    "mavic",
    "pro",
    "thing",
    "close",
    "one",
    "lap",
    "want",
    "buy",
    "right",
    "next",
    "time",
    "facebook",
    "see",
    "advertisement",
    "product",
    "next",
    "time",
    "youtube",
    "see",
    "advertisement",
    "even",
    "instagram",
    "see",
    "advertisement",
    "help",
    "machine",
    "learning",
    "google",
    "understood",
    "interested",
    "particular",
    "product",
    "hence",
    "targeting",
    "advertisements",
    "also",
    "help",
    "machine",
    "learning",
    "let",
    "talk",
    "email",
    "spam",
    "filtering",
    "spam",
    "inbox",
    "gmail",
    "know",
    "spam",
    "spam",
    "gmail",
    "entire",
    "collection",
    "emails",
    "already",
    "labeled",
    "spam",
    "spam",
    "analyzing",
    "data",
    "gmail",
    "able",
    "find",
    "characteristics",
    "like",
    "word",
    "lottery",
    "winner",
    "new",
    "email",
    "comes",
    "inbox",
    "goes",
    "spam",
    "filters",
    "decide",
    "whether",
    "spam",
    "popular",
    "spam",
    "filters",
    "gmail",
    "uses",
    "content",
    "filters",
    "header",
    "filters",
    "general",
    "blacklist",
    "filters",
    "next",
    "online",
    "fraud",
    "detection",
    "several",
    "ways",
    "online",
    "fraud",
    "take",
    "place",
    "example",
    "identity",
    "theft",
    "steal",
    "identity",
    "fake",
    "accounts",
    "accounts",
    "last",
    "long",
    "transaction",
    "takes",
    "place",
    "stop",
    "existing",
    "man",
    "middle",
    "attacks",
    "steal",
    "money",
    "transaction",
    "taking",
    "place",
    "feed",
    "forward",
    "neural",
    "network",
    "helps",
    "determine",
    "whether",
    "transaction",
    "genuine",
    "fraudulent",
    "happens",
    "feed",
    "forward",
    "neural",
    "networks",
    "outputs",
    "converted",
    "hash",
    "values",
    "values",
    "become",
    "inputs",
    "next",
    "round",
    "every",
    "real",
    "transaction",
    "takes",
    "place",
    "specific",
    "pattern",
    "fraudulent",
    "transaction",
    "would",
    "stand",
    "significant",
    "changes",
    "would",
    "cause",
    "hash",
    "values",
    "stock",
    "market",
    "trading",
    "machine",
    "learning",
    "used",
    "extensively",
    "comes",
    "stock",
    "market",
    "trading",
    "stock",
    "market",
    "indices",
    "like",
    "nikai",
    "use",
    "long",
    "memory",
    "neural",
    "networks",
    "used",
    "classify",
    "process",
    "predict",
    "datum",
    "time",
    "lags",
    "unknown",
    "size",
    "duration",
    "used",
    "predict",
    "stock",
    "market",
    "trends",
    "assisted",
    "medical",
    "technology",
    "medical",
    "technology",
    "innovated",
    "help",
    "machine",
    "learning",
    "diagnosing",
    "diseases",
    "easier",
    "create",
    "3d",
    "models",
    "predict",
    "exactly",
    "lesions",
    "brain",
    "works",
    "well",
    "brain",
    "tumors",
    "ischemic",
    "stroke",
    "lesions",
    "also",
    "used",
    "fetal",
    "imaging",
    "cardiac",
    "analysis",
    "medical",
    "fields",
    "machine",
    "learning",
    "help",
    "assist",
    "disease",
    "identification",
    "personalized",
    "treatment",
    "drug",
    "discovery",
    "clinical",
    "research",
    "radiology",
    "finally",
    "automatic",
    "translation",
    "say",
    "foreign",
    "country",
    "see",
    "billboards",
    "signs",
    "understand",
    "automatic",
    "translation",
    "comes",
    "help",
    "automatic",
    "translation",
    "actually",
    "work",
    "technology",
    "behind",
    "sequence",
    "sequence",
    "learning",
    "thing",
    "used",
    "chatbots",
    "image",
    "recognition",
    "happens",
    "using",
    "convolutional",
    "neural",
    "networks",
    "text",
    "identified",
    "using",
    "optical",
    "character",
    "recognition",
    "furthermore",
    "sequential",
    "sequence",
    "algorithm",
    "also",
    "used",
    "translate",
    "text",
    "one",
    "language",
    "hello",
    "welcome",
    "machine",
    "learning",
    "tutorial",
    "part",
    "one",
    "part",
    "one",
    "machine",
    "learning",
    "series",
    "put",
    "simply",
    "learn",
    "name",
    "richard",
    "kirschner",
    "simply",
    "learn",
    "team",
    "get",
    "certified",
    "get",
    "ahead",
    "today",
    "well",
    "start",
    "brief",
    "explanation",
    "machine",
    "learning",
    "machine",
    "learning",
    "get",
    "types",
    "machine",
    "learning",
    "machine",
    "learning",
    "algorithms",
    "linear",
    "regression",
    "decision",
    "trees",
    "support",
    "vector",
    "machine",
    "finally",
    "use",
    "case",
    "going",
    "classify",
    "whether",
    "recipe",
    "cupcake",
    "muffin",
    "using",
    "svm",
    "support",
    "vector",
    "machine",
    "sounds",
    "like",
    "delicious",
    "way",
    "explore",
    "machine",
    "learning",
    "machine",
    "learning",
    "even",
    "care",
    "computers",
    "come",
    "able",
    "new",
    "things",
    "us",
    "well",
    "machines",
    "drive",
    "car",
    "still",
    "infant",
    "stage",
    "exploding",
    "see",
    "google",
    "waymo",
    "uber",
    "program",
    "unfortunately",
    "crashed",
    "know",
    "huge",
    "going",
    "huge",
    "industry",
    "change",
    "whole",
    "transportation",
    "infrastructure",
    "machine",
    "learning",
    "used",
    "detect",
    "50",
    "eye",
    "diseases",
    "know",
    "amazing",
    "computer",
    "double",
    "checks",
    "doctor",
    "things",
    "might",
    "miss",
    "huge",
    "health",
    "industry",
    "pretty",
    "soon",
    "actually",
    "already",
    "within",
    "areas",
    "maybe",
    "eyes",
    "diseases",
    "using",
    "camera",
    "phone",
    "help",
    "go",
    "see",
    "doctor",
    "machine",
    "unlock",
    "phone",
    "face",
    "mean",
    "cool",
    "able",
    "identify",
    "face",
    "voice",
    "able",
    "turn",
    "stuff",
    "depending",
    "need",
    "talking",
    "ultimate",
    "automation",
    "world",
    "live",
    "dig",
    "deeper",
    "nice",
    "example",
    "facebook",
    "see",
    "facebook",
    "post",
    "halloween",
    "comment",
    "yes",
    "want",
    "order",
    "nobody",
    "likes",
    "spam",
    "post",
    "facebook",
    "annoy",
    "interacting",
    "likes",
    "shares",
    "comments",
    "actions",
    "remember",
    "original",
    "ones",
    "click",
    "bad",
    "luck",
    "kind",
    "fear",
    "factor",
    "well",
    "huge",
    "thing",
    "social",
    "media",
    "people",
    "getting",
    "spammed",
    "tactic",
    "known",
    "engagement",
    "bait",
    "takes",
    "advantage",
    "facebook",
    "news",
    "feed",
    "algorithm",
    "choosing",
    "engagement",
    "order",
    "get",
    "greater",
    "reach",
    "eliminate",
    "engagement",
    "bait",
    "company",
    "reviewed",
    "categorized",
    "hundreds",
    "thousands",
    "posts",
    "train",
    "machine",
    "learning",
    "model",
    "detects",
    "different",
    "types",
    "engagement",
    "bait",
    "case",
    "using",
    "facebook",
    "course",
    "across",
    "different",
    "social",
    "media",
    "different",
    "tools",
    "building",
    "facebook",
    "scroll",
    "gif",
    "replaced",
    "kind",
    "like",
    "virus",
    "coming",
    "notices",
    "certain",
    "setup",
    "facebook",
    "able",
    "replace",
    "like",
    "vote",
    "baiting",
    "react",
    "baiting",
    "share",
    "baiting",
    "different",
    "kind",
    "general",
    "titles",
    "certainly",
    "lot",
    "way",
    "baiting",
    "go",
    "click",
    "something",
    "fed",
    "data",
    "fed",
    "machine",
    "new",
    "post",
    "new",
    "post",
    "comes",
    "takes",
    "part",
    "facebook",
    "setup",
    "looking",
    "looking",
    "new",
    "post",
    "replaced",
    "like",
    "virus",
    "replaced",
    "facebook",
    "eliminate",
    "start",
    "scanning",
    "keywords",
    "phrases",
    "like",
    "checks",
    "rate",
    "starts",
    "looking",
    "people",
    "clicking",
    "without",
    "even",
    "looking",
    "clicking",
    "something",
    "normally",
    "would",
    "clicked",
    "facebook",
    "scanned",
    "keywords",
    "phrases",
    "able",
    "identify",
    "spam",
    "coming",
    "makes",
    "life",
    "easier",
    "getting",
    "spammed",
    "like",
    "walking",
    "airport",
    "lot",
    "countries",
    "like",
    "hundreds",
    "people",
    "trying",
    "sell",
    "timeshare",
    "come",
    "join",
    "us",
    "sign",
    "eliminates",
    "annoyingness",
    "enjoy",
    "facebook",
    "cat",
    "pictures",
    "maybe",
    "family",
    "pictures",
    "mine",
    "family",
    "certainly",
    "people",
    "like",
    "cat",
    "pictures",
    "another",
    "good",
    "example",
    "google",
    "deepmind",
    "project",
    "alphago",
    "computer",
    "program",
    "plays",
    "board",
    "game",
    "go",
    "defeated",
    "world",
    "number",
    "one",
    "go",
    "player",
    "hope",
    "say",
    "name",
    "right",
    "kiji",
    "ultimate",
    "go",
    "challenge",
    "game",
    "three",
    "three",
    "may",
    "27",
    "2017",
    "last",
    "year",
    "happened",
    "makes",
    "important",
    "know",
    "go",
    "game",
    "like",
    "driving",
    "car",
    "something",
    "real",
    "world",
    "using",
    "games",
    "learn",
    "get",
    "machine",
    "learning",
    "program",
    "learn",
    "want",
    "learn",
    "learn",
    "huge",
    "step",
    "lot",
    "still",
    "infant",
    "stage",
    "far",
    "development",
    "saw",
    "happened",
    "referred",
    "earlier",
    "uber",
    "cars",
    "lost",
    "whole",
    "division",
    "jumped",
    "ahead",
    "fast",
    "still",
    "infant",
    "stage",
    "boy",
    "like",
    "beginning",
    "amazing",
    "world",
    "automated",
    "ways",
    "ca",
    "even",
    "imagine",
    "tomorrow",
    "going",
    "look",
    "like",
    "looked",
    "lot",
    "examples",
    "machine",
    "learning",
    "let",
    "see",
    "give",
    "little",
    "bit",
    "concrete",
    "definition",
    "machine",
    "learning",
    "machine",
    "learning",
    "science",
    "making",
    "computers",
    "learn",
    "act",
    "like",
    "humans",
    "feeding",
    "data",
    "information",
    "without",
    "explicitly",
    "programmed",
    "see",
    "nice",
    "little",
    "diagram",
    "ordinary",
    "system",
    "uh",
    "computer",
    "nowadays",
    "even",
    "run",
    "lot",
    "stuff",
    "cell",
    "phone",
    "cell",
    "phones",
    "advance",
    "much",
    "artificial",
    "intelligence",
    "machine",
    "learning",
    "takes",
    "data",
    "learns",
    "happened",
    "predicts",
    "going",
    "come",
    "next",
    "really",
    "biggest",
    "part",
    "right",
    "machine",
    "learning",
    "going",
    "improves",
    "find",
    "new",
    "solution",
    "go",
    "descriptive",
    "words",
    "learning",
    "stuff",
    "understanding",
    "fits",
    "together",
    "predicting",
    "going",
    "post",
    "scripting",
    "coming",
    "new",
    "solution",
    "working",
    "machine",
    "learning",
    "number",
    "different",
    "diagrams",
    "people",
    "posted",
    "steps",
    "go",
    "lot",
    "might",
    "domain",
    "specific",
    "working",
    "photo",
    "identification",
    "versus",
    "language",
    "versus",
    "medical",
    "physics",
    "switched",
    "around",
    "little",
    "bit",
    "new",
    "things",
    "put",
    "specific",
    "domain",
    "kind",
    "general",
    "diagram",
    "first",
    "want",
    "define",
    "objective",
    "important",
    "know",
    "wanting",
    "predict",
    "going",
    "collecting",
    "data",
    "defined",
    "objective",
    "need",
    "collect",
    "data",
    "matches",
    "spend",
    "lot",
    "time",
    "data",
    "science",
    "collecting",
    "data",
    "next",
    "step",
    "preparing",
    "data",
    "got",
    "make",
    "sure",
    "data",
    "clean",
    "going",
    "old",
    "saying",
    "bad",
    "data",
    "bad",
    "answer",
    "bad",
    "data",
    "gone",
    "cleaned",
    "stuff",
    "coming",
    "going",
    "select",
    "algorithm",
    "algorithm",
    "going",
    "use",
    "going",
    "train",
    "algorithm",
    "case",
    "think",
    "going",
    "working",
    "svm",
    "support",
    "vector",
    "machine",
    "test",
    "model",
    "model",
    "work",
    "valid",
    "model",
    "tested",
    "want",
    "run",
    "prediction",
    "want",
    "run",
    "prediction",
    "choice",
    "whatever",
    "output",
    "going",
    "come",
    "everything",
    "set",
    "done",
    "lots",
    "testing",
    "want",
    "go",
    "ahead",
    "deploy",
    "model",
    "remember",
    "said",
    "domain",
    "specific",
    "general",
    "far",
    "scope",
    "something",
    "lot",
    "models",
    "get",
    "halfway",
    "realize",
    "data",
    "missing",
    "something",
    "go",
    "collect",
    "new",
    "data",
    "run",
    "test",
    "someplace",
    "along",
    "line",
    "saying",
    "hey",
    "really",
    "getting",
    "answers",
    "need",
    "lot",
    "things",
    "domain",
    "specific",
    "become",
    "part",
    "model",
    "general",
    "model",
    "good",
    "model",
    "start",
    "basic",
    "divisions",
    "machine",
    "learning",
    "important",
    "know",
    "instance",
    "want",
    "predict",
    "category",
    "well",
    "categorizing",
    "thing",
    "classification",
    "instance",
    "whether",
    "stock",
    "price",
    "increase",
    "decrease",
    "words",
    "looking",
    "yes",
    "answer",
    "going",
    "going",
    "case",
    "actually",
    "say",
    "going",
    "true",
    "going",
    "false",
    "meaning",
    "going",
    "way",
    "yes",
    "zero",
    "one",
    "want",
    "predict",
    "quantity",
    "regression",
    "remember",
    "classification",
    "looking",
    "regression",
    "two",
    "major",
    "divisions",
    "data",
    "instance",
    "predicting",
    "age",
    "person",
    "based",
    "height",
    "weight",
    "health",
    "factors",
    "based",
    "different",
    "factors",
    "might",
    "guess",
    "old",
    "person",
    "lot",
    "domain",
    "specific",
    "things",
    "like",
    "want",
    "detect",
    "anomaly",
    "anomaly",
    "detection",
    "actually",
    "popular",
    "right",
    "instance",
    "want",
    "detect",
    "money",
    "withdrawal",
    "anomalies",
    "want",
    "know",
    "someone",
    "making",
    "withdrawal",
    "might",
    "account",
    "actually",
    "brought",
    "really",
    "big",
    "right",
    "predicting",
    "stock",
    "whether",
    "buy",
    "stock",
    "want",
    "able",
    "know",
    "going",
    "stock",
    "market",
    "anomaly",
    "use",
    "different",
    "prediction",
    "model",
    "something",
    "else",
    "going",
    "got",
    "pull",
    "new",
    "information",
    "norm",
    "going",
    "get",
    "normal",
    "return",
    "money",
    "invested",
    "able",
    "detect",
    "anomalies",
    "big",
    "data",
    "science",
    "days",
    "another",
    "question",
    "comes",
    "call",
    "untrained",
    "data",
    "want",
    "discover",
    "structure",
    "unexplored",
    "data",
    "called",
    "clustering",
    "instance",
    "finding",
    "groups",
    "customers",
    "similar",
    "behavior",
    "given",
    "large",
    "database",
    "customer",
    "data",
    "containing",
    "demographics",
    "past",
    "buying",
    "records",
    "case",
    "might",
    "notice",
    "anybody",
    "wearing",
    "certain",
    "set",
    "shoes",
    "go",
    "shopping",
    "certain",
    "stores",
    "whatever",
    "going",
    "make",
    "certain",
    "purchases",
    "information",
    "helps",
    "us",
    "market",
    "group",
    "people",
    "together",
    "explore",
    "group",
    "find",
    "want",
    "market",
    "marketing",
    "world",
    "might",
    "also",
    "work",
    "arena",
    "might",
    "want",
    "group",
    "people",
    "together",
    "whether",
    "based",
    "different",
    "areas",
    "investments",
    "financial",
    "background",
    "whether",
    "going",
    "give",
    "loan",
    "even",
    "start",
    "looking",
    "whether",
    "valid",
    "customer",
    "bank",
    "might",
    "want",
    "look",
    "different",
    "areas",
    "group",
    "together",
    "based",
    "unknown",
    "data",
    "know",
    "data",
    "going",
    "tell",
    "want",
    "cluster",
    "people",
    "together",
    "come",
    "together",
    "let",
    "take",
    "quick",
    "detour",
    "quiz",
    "time",
    "oh",
    "favorite",
    "going",
    "couple",
    "questions",
    "quiz",
    "time",
    "posting",
    "answers",
    "part",
    "2",
    "tutorial",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "quiz",
    "times",
    "questions",
    "hopefully",
    "get",
    "right",
    "get",
    "thinking",
    "process",
    "data",
    "going",
    "tell",
    "happening",
    "following",
    "cases",
    "course",
    "sitting",
    "cup",
    "coffee",
    "checkbox",
    "pen",
    "trying",
    "figure",
    "next",
    "step",
    "data",
    "science",
    "analysis",
    "first",
    "one",
    "grouping",
    "documents",
    "different",
    "categories",
    "based",
    "topic",
    "content",
    "document",
    "big",
    "days",
    "know",
    "legal",
    "documents",
    "maybe",
    "sports",
    "group",
    "documents",
    "maybe",
    "analyzing",
    "newspaper",
    "postings",
    "certainly",
    "automated",
    "huge",
    "thing",
    "today",
    "world",
    "b",
    "identifying",
    "handwritten",
    "digits",
    "images",
    "correctly",
    "want",
    "know",
    "whether",
    "writing",
    "capital",
    "b",
    "c",
    "writing",
    "hand",
    "digit",
    "handwriting",
    "c",
    "behavior",
    "website",
    "indicating",
    "site",
    "working",
    "designed",
    "predicting",
    "salary",
    "individual",
    "based",
    "years",
    "experience",
    "hr",
    "hiring",
    "set",
    "stay",
    "tuned",
    "part",
    "two",
    "go",
    "ahead",
    "answer",
    "questions",
    "get",
    "part",
    "two",
    "tutorial",
    "simply",
    "write",
    "bottom",
    "send",
    "note",
    "simply",
    "learn",
    "follow",
    "back",
    "regular",
    "content",
    "last",
    "bring",
    "us",
    "next",
    "topic",
    "another",
    "way",
    "dividing",
    "types",
    "machine",
    "learning",
    "supervised",
    "unsupervised",
    "reinforcement",
    "learning",
    "supervised",
    "learning",
    "method",
    "used",
    "enable",
    "machines",
    "classify",
    "predict",
    "objects",
    "problems",
    "situations",
    "based",
    "labeled",
    "data",
    "fed",
    "machine",
    "see",
    "jungle",
    "data",
    "circles",
    "triangles",
    "squares",
    "label",
    "circle",
    "triangle",
    "square",
    "model",
    "training",
    "trains",
    "know",
    "answer",
    "important",
    "supervised",
    "learning",
    "already",
    "know",
    "answer",
    "lot",
    "information",
    "coming",
    "huge",
    "group",
    "data",
    "coming",
    "new",
    "data",
    "coming",
    "trained",
    "model",
    "model",
    "knows",
    "difference",
    "circle",
    "square",
    "triangle",
    "trained",
    "send",
    "case",
    "square",
    "circle",
    "goes",
    "predicts",
    "top",
    "one",
    "square",
    "next",
    "one",
    "circle",
    "see",
    "uh",
    "able",
    "predict",
    "whether",
    "someone",
    "going",
    "default",
    "loan",
    "talking",
    "banks",
    "earlier",
    "supervised",
    "learning",
    "stock",
    "market",
    "whether",
    "going",
    "make",
    "money",
    "always",
    "important",
    "looking",
    "make",
    "fortune",
    "stock",
    "market",
    "keep",
    "mind",
    "difficult",
    "get",
    "data",
    "correct",
    "stock",
    "market",
    "fluctuates",
    "weighs",
    "really",
    "hard",
    "predict",
    "quite",
    "roller",
    "coaster",
    "ride",
    "running",
    "machine",
    "learning",
    "stock",
    "market",
    "start",
    "realizing",
    "really",
    "dig",
    "new",
    "data",
    "supervised",
    "learning",
    "supervised",
    "need",
    "unsupervised",
    "learning",
    "unsupervised",
    "learning",
    "machine",
    "learning",
    "model",
    "finds",
    "hidden",
    "pattern",
    "unlabeled",
    "data",
    "case",
    "instead",
    "telling",
    "circle",
    "triangle",
    "square",
    "goes",
    "looks",
    "says",
    "whatever",
    "reason",
    "groups",
    "together",
    "maybe",
    "group",
    "number",
    "corners",
    "notices",
    "number",
    "three",
    "corners",
    "number",
    "four",
    "corners",
    "number",
    "corners",
    "able",
    "filter",
    "group",
    "together",
    "talked",
    "earlier",
    "looking",
    "group",
    "people",
    "shopping",
    "want",
    "group",
    "together",
    "find",
    "common",
    "course",
    "understand",
    "people",
    "common",
    "maybe",
    "one",
    "customer",
    "store",
    "five",
    "customer",
    "store",
    "lot",
    "common",
    "five",
    "others",
    "customers",
    "store",
    "market",
    "five",
    "customers",
    "store",
    "yet",
    "fit",
    "demographics",
    "going",
    "shop",
    "like",
    "shop",
    "store",
    "one",
    "next",
    "door",
    "course",
    "simplified",
    "version",
    "see",
    "easily",
    "difference",
    "triangle",
    "circle",
    "might",
    "easy",
    "marketing",
    "reinforcement",
    "learning",
    "reinforcement",
    "learning",
    "important",
    "type",
    "machine",
    "learning",
    "agent",
    "learns",
    "behave",
    "environment",
    "performing",
    "actions",
    "seeing",
    "result",
    "case",
    "baby",
    "actually",
    "great",
    "used",
    "infant",
    "slide",
    "reinforcement",
    "learning",
    "much",
    "infant",
    "stages",
    "also",
    "probably",
    "biggest",
    "machine",
    "learning",
    "demand",
    "right",
    "future",
    "going",
    "coming",
    "next",
    "years",
    "reinforcement",
    "learning",
    "make",
    "work",
    "us",
    "see",
    "action",
    "action",
    "one",
    "goes",
    "fire",
    "hopefully",
    "baby",
    "little",
    "candle",
    "giant",
    "fire",
    "pit",
    "like",
    "looks",
    "like",
    "baby",
    "comes",
    "new",
    "state",
    "baby",
    "sad",
    "crying",
    "got",
    "burned",
    "fire",
    "maybe",
    "take",
    "another",
    "action",
    "baby",
    "called",
    "agent",
    "one",
    "taking",
    "actions",
    "case",
    "go",
    "fire",
    "went",
    "different",
    "direction",
    "baby",
    "happy",
    "laughing",
    "playing",
    "reinforcement",
    "learning",
    "easy",
    "understand",
    "humans",
    "one",
    "ways",
    "learn",
    "learn",
    "whether",
    "bring",
    "stove",
    "anymore",
    "touch",
    "stove",
    "big",
    "picture",
    "able",
    "machine",
    "learning",
    "program",
    "ai",
    "able",
    "huge",
    "starting",
    "learn",
    "learn",
    "big",
    "jump",
    "world",
    "computer",
    "machine",
    "learning",
    "going",
    "go",
    "back",
    "kind",
    "go",
    "back",
    "supervised",
    "versus",
    "unsupervised",
    "learning",
    "understanding",
    "huge",
    "going",
    "come",
    "project",
    "working",
    "supervised",
    "learning",
    "labeled",
    "data",
    "direct",
    "feedback",
    "someone",
    "already",
    "gone",
    "said",
    "yes",
    "triangle",
    "triangle",
    "predict",
    "outcome",
    "nice",
    "prediction",
    "new",
    "set",
    "data",
    "coming",
    "know",
    "going",
    "unsupervised",
    "training",
    "labeled",
    "really",
    "know",
    "feedback",
    "telling",
    "whether",
    "right",
    "wrong",
    "telling",
    "whether",
    "triangle",
    "square",
    "telling",
    "go",
    "left",
    "right",
    "finding",
    "hidden",
    "structure",
    "data",
    "grouping",
    "data",
    "together",
    "find",
    "connects",
    "use",
    "together",
    "imagine",
    "image",
    "sure",
    "looking",
    "go",
    "unstructured",
    "data",
    "find",
    "things",
    "connected",
    "together",
    "somebody",
    "looks",
    "labels",
    "take",
    "label",
    "data",
    "program",
    "something",
    "predict",
    "picture",
    "see",
    "go",
    "back",
    "forth",
    "start",
    "connecting",
    "different",
    "tools",
    "together",
    "make",
    "bigger",
    "picture",
    "many",
    "interesting",
    "machine",
    "learning",
    "algorithms",
    "let",
    "look",
    "hopefully",
    "gives",
    "little",
    "flavor",
    "important",
    "ones",
    "currently",
    "used",
    "take",
    "look",
    "linear",
    "regression",
    "decision",
    "tree",
    "support",
    "vector",
    "machine",
    "let",
    "start",
    "closer",
    "look",
    "linear",
    "regression",
    "linear",
    "regression",
    "perhaps",
    "one",
    "algorithms",
    "statistics",
    "machine",
    "learning",
    "linear",
    "regression",
    "linear",
    "model",
    "example",
    "model",
    "assumes",
    "linear",
    "relationship",
    "input",
    "variables",
    "x",
    "single",
    "output",
    "variable",
    "see",
    "remember",
    "algebra",
    "classes",
    "equals",
    "mx",
    "plus",
    "c",
    "imagine",
    "predicting",
    "distance",
    "traveled",
    "speed",
    "x",
    "linear",
    "regression",
    "model",
    "representation",
    "problem",
    "would",
    "equals",
    "times",
    "x",
    "plus",
    "c",
    "distance",
    "equals",
    "times",
    "speed",
    "plus",
    "c",
    "coefficient",
    "c",
    "going",
    "look",
    "two",
    "different",
    "variations",
    "first",
    "going",
    "start",
    "time",
    "constant",
    "see",
    "bicyclist",
    "got",
    "safety",
    "gear",
    "thank",
    "goodness",
    "speed",
    "equals",
    "10",
    "meters",
    "per",
    "second",
    "certain",
    "amount",
    "time",
    "distance",
    "equals",
    "36",
    "kilometers",
    "second",
    "bicyclist",
    "going",
    "twice",
    "speed",
    "20",
    "meters",
    "per",
    "second",
    "guess",
    "going",
    "twice",
    "speed",
    "time",
    "constant",
    "going",
    "go",
    "twice",
    "distance",
    "easy",
    "compute",
    "36",
    "times",
    "2",
    "get",
    "72",
    "kilometers",
    "question",
    "fast",
    "would",
    "somebody",
    "going",
    "three",
    "times",
    "speed",
    "30",
    "meters",
    "per",
    "second",
    "easily",
    "compute",
    "distance",
    "head",
    "without",
    "needing",
    "computer",
    "want",
    "complicated",
    "data",
    "kind",
    "nice",
    "compare",
    "two",
    "let",
    "take",
    "look",
    "looks",
    "like",
    "graph",
    "linear",
    "regression",
    "model",
    "distance",
    "speed",
    "equals",
    "slope",
    "line",
    "notice",
    "line",
    "plus",
    "slope",
    "speed",
    "increases",
    "distance",
    "also",
    "increases",
    "hence",
    "variables",
    "positive",
    "relationship",
    "speed",
    "person",
    "equals",
    "equals",
    "mx",
    "plus",
    "c",
    "distance",
    "traveled",
    "fixed",
    "interval",
    "time",
    "could",
    "easily",
    "compute",
    "either",
    "following",
    "line",
    "knowing",
    "3",
    "times",
    "10",
    "meters",
    "per",
    "second",
    "roughly",
    "102",
    "kilometers",
    "distance",
    "third",
    "bicyclist",
    "traveled",
    "one",
    "key",
    "definitions",
    "positive",
    "relationship",
    "slope",
    "line",
    "positive",
    "distance",
    "increase",
    "speed",
    "increase",
    "let",
    "take",
    "look",
    "second",
    "example",
    "put",
    "distance",
    "constant",
    "speed",
    "equals",
    "10",
    "meters",
    "per",
    "second",
    "certain",
    "distance",
    "go",
    "takes",
    "100",
    "seconds",
    "travel",
    "distance",
    "second",
    "bicyclist",
    "still",
    "20",
    "meters",
    "per",
    "second",
    "since",
    "going",
    "twice",
    "speed",
    "guess",
    "cover",
    "distance",
    "half",
    "time",
    "50",
    "seconds",
    "course",
    "could",
    "probably",
    "guess",
    "third",
    "one",
    "100",
    "divided",
    "30",
    "since",
    "going",
    "3",
    "times",
    "speed",
    "easily",
    "guess",
    "seconds",
    "time",
    "put",
    "linear",
    "regression",
    "model",
    "graph",
    "distance",
    "assumed",
    "constant",
    "let",
    "see",
    "relationship",
    "speed",
    "time",
    "time",
    "goes",
    "amount",
    "speed",
    "go",
    "distance",
    "goes",
    "equals",
    "minus",
    "slope",
    "line",
    "speed",
    "increases",
    "time",
    "decreases",
    "hence",
    "variable",
    "negative",
    "relationship",
    "definition",
    "positive",
    "relationship",
    "negative",
    "relationship",
    "dependent",
    "slope",
    "line",
    "simple",
    "formula",
    "like",
    "even",
    "significant",
    "amount",
    "data",
    "let",
    "see",
    "mathematical",
    "implementation",
    "linear",
    "regression",
    "take",
    "data",
    "suppose",
    "data",
    "set",
    "x",
    "x",
    "equals",
    "1",
    "2",
    "3",
    "4",
    "5",
    "standard",
    "series",
    "value",
    "3",
    "2",
    "2",
    "4",
    "take",
    "go",
    "ahead",
    "plot",
    "points",
    "graph",
    "see",
    "kind",
    "nice",
    "scattering",
    "could",
    "probably",
    "eyeball",
    "line",
    "middle",
    "going",
    "calculate",
    "exact",
    "line",
    "linear",
    "regression",
    "first",
    "thing",
    "come",
    "mean",
    "x",
    "remember",
    "mean",
    "basically",
    "average",
    "added",
    "five",
    "plus",
    "four",
    "plus",
    "three",
    "plus",
    "two",
    "plus",
    "one",
    "divide",
    "five",
    "simply",
    "comes",
    "three",
    "go",
    "ahead",
    "add",
    "numbers",
    "divide",
    "five",
    "end",
    "mean",
    "value",
    "equals",
    "two",
    "point",
    "eight",
    "x",
    "references",
    "average",
    "means",
    "value",
    "also",
    "equals",
    "means",
    "value",
    "plot",
    "see",
    "put",
    "equals",
    "x",
    "equals",
    "3",
    "graph",
    "kind",
    "give",
    "little",
    "different",
    "color",
    "sort",
    "dashed",
    "lines",
    "important",
    "note",
    "linear",
    "regression",
    "linear",
    "regression",
    "model",
    "go",
    "dot",
    "let",
    "find",
    "regression",
    "equation",
    "find",
    "best",
    "fit",
    "line",
    "remember",
    "go",
    "ahead",
    "take",
    "equals",
    "mx",
    "plus",
    "c",
    "looking",
    "c",
    "find",
    "equation",
    "data",
    "need",
    "find",
    "slope",
    "coefficient",
    "c",
    "equals",
    "mx",
    "plus",
    "c",
    "equals",
    "sum",
    "x",
    "minus",
    "x",
    "average",
    "times",
    "minus",
    "average",
    "means",
    "x",
    "means",
    "sum",
    "x",
    "minus",
    "x",
    "means",
    "squared",
    "get",
    "slope",
    "value",
    "line",
    "easily",
    "creating",
    "columns",
    "x",
    "computers",
    "really",
    "good",
    "iterating",
    "data",
    "easily",
    "compute",
    "fill",
    "graph",
    "data",
    "graph",
    "easily",
    "see",
    "x",
    "value",
    "1",
    "remember",
    "x",
    "means",
    "value",
    "3",
    "1",
    "minus",
    "3",
    "equals",
    "negative",
    "2",
    "2",
    "minus",
    "3",
    "equals",
    "negative",
    "1",
    "forth",
    "easily",
    "fill",
    "column",
    "x",
    "minus",
    "x",
    "minus",
    "compute",
    "x",
    "minus",
    "x",
    "squared",
    "x",
    "minus",
    "x",
    "times",
    "minus",
    "guess",
    "next",
    "step",
    "go",
    "ahead",
    "sum",
    "different",
    "columns",
    "answers",
    "need",
    "get",
    "total",
    "10",
    "x",
    "minus",
    "x",
    "squared",
    "total",
    "2",
    "x",
    "minus",
    "x",
    "times",
    "minus",
    "plug",
    "get",
    "2",
    "10",
    "equals",
    "know",
    "slope",
    "line",
    "equals",
    "calculate",
    "value",
    "c",
    "next",
    "step",
    "need",
    "know",
    "crosses",
    "axis",
    "remember",
    "mentioned",
    "earlier",
    "linear",
    "regression",
    "line",
    "pass",
    "means",
    "value",
    "one",
    "showed",
    "earlier",
    "flip",
    "back",
    "graph",
    "see",
    "right",
    "means",
    "value",
    "3",
    "x",
    "equals",
    "3",
    "equals",
    "since",
    "know",
    "value",
    "simply",
    "plug",
    "formula",
    "equals",
    "x",
    "plus",
    "c",
    "plug",
    "get",
    "equals",
    "times",
    "3",
    "plus",
    "c",
    "solve",
    "c",
    "know",
    "coefficient",
    "equals",
    "go",
    "ahead",
    "plot",
    "regression",
    "line",
    "equals",
    "times",
    "x",
    "plus",
    "equation",
    "compute",
    "new",
    "values",
    "let",
    "predict",
    "values",
    "using",
    "x",
    "equals",
    "1",
    "2",
    "3",
    "4",
    "5",
    "plot",
    "points",
    "remember",
    "1",
    "2",
    "3",
    "4",
    "5",
    "original",
    "x",
    "values",
    "going",
    "see",
    "thinks",
    "actually",
    "plug",
    "get",
    "designated",
    "p",
    "see",
    "x",
    "equals",
    "1",
    "equals",
    "x",
    "equals",
    "2",
    "equals",
    "predicted",
    "values",
    "think",
    "going",
    "plug",
    "numbers",
    "plot",
    "predicted",
    "values",
    "along",
    "actual",
    "values",
    "see",
    "difference",
    "one",
    "things",
    "important",
    "linear",
    "aggression",
    "models",
    "understand",
    "error",
    "calculate",
    "error",
    "different",
    "values",
    "see",
    "plotted",
    "x",
    "predict",
    "drawn",
    "little",
    "line",
    "sort",
    "see",
    "error",
    "looks",
    "like",
    "different",
    "points",
    "goal",
    "reduce",
    "error",
    "want",
    "minimize",
    "error",
    "value",
    "linear",
    "regression",
    "model",
    "minimizing",
    "distance",
    "lots",
    "ways",
    "minimize",
    "distance",
    "line",
    "data",
    "points",
    "like",
    "sum",
    "squared",
    "errors",
    "sum",
    "absolute",
    "errors",
    "root",
    "mean",
    "square",
    "error",
    "etc",
    "keep",
    "moving",
    "line",
    "data",
    "points",
    "make",
    "sure",
    "best",
    "fit",
    "line",
    "least",
    "square",
    "distance",
    "data",
    "points",
    "regression",
    "line",
    "recap",
    "simple",
    "linear",
    "regression",
    "model",
    "first",
    "figure",
    "formula",
    "line",
    "middle",
    "slowly",
    "adjust",
    "line",
    "minimize",
    "error",
    "keep",
    "mind",
    "simple",
    "formula",
    "math",
    "gets",
    "even",
    "though",
    "math",
    "much",
    "gets",
    "much",
    "complex",
    "add",
    "different",
    "dimensions",
    "two",
    "dimensions",
    "equals",
    "mx",
    "plus",
    "c",
    "take",
    "x",
    "z",
    "j",
    "q",
    "different",
    "features",
    "plot",
    "linear",
    "regression",
    "model",
    "using",
    "different",
    "formulas",
    "minimize",
    "error",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "decision",
    "trees",
    "different",
    "way",
    "solve",
    "problems",
    "linear",
    "regression",
    "model",
    "decision",
    "tree",
    "algorithm",
    "used",
    "determine",
    "course",
    "action",
    "branch",
    "tree",
    "represents",
    "possible",
    "decision",
    "occurrence",
    "reaction",
    "data",
    "tells",
    "us",
    "good",
    "day",
    "play",
    "golf",
    "open",
    "data",
    "general",
    "spreadsheet",
    "see",
    "outlook",
    "whether",
    "rainy",
    "overcast",
    "sunny",
    "temperature",
    "hot",
    "mild",
    "cool",
    "humidity",
    "windy",
    "like",
    "play",
    "golf",
    "day",
    "yes",
    "taking",
    "census",
    "certainly",
    "would",
    "want",
    "computer",
    "telling",
    "go",
    "play",
    "golf",
    "could",
    "imagine",
    "got",
    "night",
    "trying",
    "plan",
    "day",
    "comes",
    "says",
    "tomorrow",
    "would",
    "good",
    "day",
    "golf",
    "morning",
    "good",
    "day",
    "afternoon",
    "something",
    "like",
    "becomes",
    "beneficial",
    "see",
    "lot",
    "applications",
    "coming",
    "gives",
    "suggestions",
    "lets",
    "know",
    "would",
    "fit",
    "match",
    "next",
    "day",
    "next",
    "purchase",
    "next",
    "uh",
    "whatever",
    "know",
    "next",
    "meal",
    "case",
    "tomorrow",
    "good",
    "day",
    "playing",
    "golf",
    "based",
    "weather",
    "coming",
    "come",
    "let",
    "uh",
    "determine",
    "play",
    "golf",
    "day",
    "sunny",
    "windy",
    "found",
    "forecast",
    "tomorrow",
    "going",
    "sunny",
    "windy",
    "suppose",
    "draw",
    "tree",
    "like",
    "going",
    "humidity",
    "normal",
    "normal",
    "humidity",
    "going",
    "go",
    "play",
    "golf",
    "humidity",
    "really",
    "high",
    "look",
    "outlook",
    "outlook",
    "sunny",
    "overcast",
    "rainy",
    "going",
    "change",
    "choose",
    "know",
    "high",
    "humidity",
    "sunny",
    "probably",
    "going",
    "play",
    "golf",
    "going",
    "miserable",
    "fighting",
    "mosquitoes",
    "joining",
    "play",
    "golf",
    "maybe",
    "raining",
    "probably",
    "want",
    "play",
    "rain",
    "slightly",
    "overcast",
    "get",
    "right",
    "shadow",
    "good",
    "day",
    "play",
    "golf",
    "outside",
    "green",
    "example",
    "probably",
    "make",
    "creep",
    "pretty",
    "easily",
    "simple",
    "set",
    "data",
    "going",
    "question",
    "know",
    "split",
    "split",
    "data",
    "much",
    "complicated",
    "data",
    "something",
    "would",
    "particularly",
    "understand",
    "like",
    "studying",
    "cancer",
    "take",
    "36",
    "measurements",
    "cancerous",
    "cells",
    "one",
    "measurements",
    "represents",
    "bulbous",
    "extended",
    "sharp",
    "edges",
    "something",
    "human",
    "would",
    "understanding",
    "decide",
    "split",
    "data",
    "right",
    "decision",
    "tree",
    "since",
    "question",
    "going",
    "come",
    "right",
    "decision",
    "tree",
    "calculate",
    "entropy",
    "information",
    "gain",
    "two",
    "important",
    "vocabulary",
    "words",
    "entropy",
    "information",
    "gain",
    "entropy",
    "entropy",
    "measure",
    "randomness",
    "impurity",
    "data",
    "set",
    "entropy",
    "low",
    "want",
    "chaos",
    "low",
    "possible",
    "want",
    "look",
    "confused",
    "images",
    "going",
    "mixed",
    "data",
    "information",
    "gain",
    "measure",
    "decrease",
    "entropy",
    "data",
    "set",
    "split",
    "also",
    "known",
    "entropy",
    "reduction",
    "information",
    "gain",
    "high",
    "want",
    "information",
    "get",
    "split",
    "high",
    "possible",
    "let",
    "take",
    "look",
    "entropy",
    "mathematical",
    "side",
    "case",
    "going",
    "denote",
    "entropy",
    "p",
    "n",
    "p",
    "probability",
    "going",
    "play",
    "game",
    "golf",
    "n",
    "probability",
    "going",
    "play",
    "game",
    "golf",
    "really",
    "memorize",
    "formulas",
    "depending",
    "working",
    "important",
    "note",
    "formula",
    "coming",
    "see",
    "lost",
    "running",
    "programming",
    "unless",
    "building",
    "decision",
    "tree",
    "code",
    "back",
    "simply",
    "log",
    "squared",
    "p",
    "p",
    "plus",
    "n",
    "minus",
    "n",
    "p",
    "plus",
    "n",
    "times",
    "log",
    "squared",
    "n",
    "p",
    "plus",
    "n",
    "let",
    "break",
    "see",
    "actually",
    "looks",
    "like",
    "computing",
    "computer",
    "script",
    "side",
    "entropy",
    "target",
    "class",
    "data",
    "set",
    "whole",
    "entropy",
    "entropy",
    "play",
    "golf",
    "look",
    "go",
    "back",
    "data",
    "simply",
    "count",
    "many",
    "yeses",
    "complete",
    "data",
    "set",
    "playing",
    "golf",
    "days",
    "complete",
    "set",
    "find",
    "five",
    "days",
    "play",
    "golf",
    "nine",
    "days",
    "play",
    "golf",
    "equals",
    "together",
    "9",
    "plus",
    "5",
    "equals",
    "5",
    "14",
    "9",
    "14",
    "p",
    "n",
    "values",
    "plug",
    "formula",
    "go",
    "5",
    "14",
    "equals",
    "9",
    "14",
    "equals",
    "whole",
    "equation",
    "get",
    "minus",
    "log",
    "root",
    "squared",
    "minus",
    "log",
    "squared",
    "root",
    "get",
    "set",
    "value",
    "get",
    "full",
    "entropy",
    "value",
    "whole",
    "set",
    "data",
    "working",
    "want",
    "make",
    "entropy",
    "go",
    "like",
    "calculated",
    "entropy",
    "whole",
    "set",
    "also",
    "calculate",
    "entropy",
    "playing",
    "golf",
    "outlook",
    "going",
    "overcast",
    "rainy",
    "sunny",
    "look",
    "entropy",
    "p",
    "sunny",
    "times",
    "e",
    "3",
    "2",
    "comes",
    "many",
    "sunny",
    "days",
    "yes",
    "many",
    "sunny",
    "days",
    "total",
    "five",
    "forget",
    "put",
    "divide",
    "five",
    "later",
    "equals",
    "p",
    "overcast",
    "equals",
    "four",
    "comma",
    "zero",
    "plus",
    "rainy",
    "equals",
    "two",
    "comma",
    "three",
    "whole",
    "setup",
    "5",
    "remember",
    "said",
    "total",
    "5",
    "5",
    "14",
    "times",
    "3",
    "2",
    "plus",
    "4",
    "14",
    "times",
    "4",
    "comma",
    "0",
    "514",
    "2",
    "three",
    "compute",
    "entropy",
    "part",
    "forecast",
    "get",
    "similarly",
    "calculate",
    "entropy",
    "predictors",
    "like",
    "temperature",
    "humidity",
    "wind",
    "look",
    "gain",
    "outlook",
    "much",
    "going",
    "gain",
    "entropy",
    "play",
    "golf",
    "minus",
    "entropy",
    "play",
    "golf",
    "outlook",
    "take",
    "original",
    "whole",
    "set",
    "minus",
    "entropy",
    "rainy",
    "day",
    "temperature",
    "end",
    "gain",
    "information",
    "gain",
    "remember",
    "define",
    "entropy",
    "define",
    "information",
    "gain",
    "higher",
    "information",
    "gain",
    "lower",
    "entropy",
    "better",
    "information",
    "gain",
    "three",
    "attributes",
    "calculated",
    "way",
    "gain",
    "temperature",
    "equals",
    "gain",
    "humidity",
    "equals",
    "gain",
    "windy",
    "day",
    "equals",
    "quick",
    "comparison",
    "see",
    "greatest",
    "gain",
    "information",
    "split",
    "want",
    "let",
    "build",
    "decision",
    "tree",
    "outlook",
    "going",
    "sunny",
    "overcast",
    "rainy",
    "first",
    "split",
    "gives",
    "us",
    "information",
    "gain",
    "continue",
    "go",
    "tree",
    "using",
    "different",
    "information",
    "gains",
    "largest",
    "information",
    "continue",
    "nodes",
    "tree",
    "choose",
    "attribute",
    "largest",
    "information",
    "gain",
    "root",
    "node",
    "continue",
    "split",
    "subnode",
    "largest",
    "information",
    "gain",
    "compute",
    "although",
    "little",
    "bit",
    "tongue",
    "twister",
    "say",
    "see",
    "easy",
    "view",
    "visual",
    "model",
    "outlook",
    "split",
    "three",
    "different",
    "directions",
    "outlook",
    "overcast",
    "going",
    "play",
    "split",
    "want",
    "outlook",
    "sunny",
    "also",
    "windy",
    "windy",
    "going",
    "play",
    "windy",
    "play",
    "easily",
    "build",
    "nice",
    "decision",
    "tree",
    "guess",
    "would",
    "like",
    "tomorrow",
    "give",
    "us",
    "nice",
    "recommendation",
    "day",
    "want",
    "know",
    "good",
    "day",
    "play",
    "golf",
    "sunny",
    "windy",
    "remember",
    "original",
    "question",
    "came",
    "tomorrow",
    "weather",
    "report",
    "sunny",
    "windy",
    "see",
    "going",
    "tree",
    "go",
    "outlook",
    "sunny",
    "outlook",
    "windy",
    "going",
    "play",
    "golf",
    "tomorrow",
    "little",
    "smartwatch",
    "pops",
    "says",
    "sorry",
    "tomorrow",
    "good",
    "day",
    "golf",
    "going",
    "sunny",
    "windy",
    "huge",
    "golf",
    "fan",
    "might",
    "go",
    "good",
    "day",
    "play",
    "golf",
    "go",
    "watch",
    "golf",
    "game",
    "home",
    "sit",
    "front",
    "tv",
    "instead",
    "playing",
    "golf",
    "wind",
    "looked",
    "decision",
    "tree",
    "let",
    "look",
    "third",
    "one",
    "algorithms",
    "investigating",
    "support",
    "vector",
    "machine",
    "support",
    "vector",
    "machine",
    "widely",
    "used",
    "classification",
    "algorithm",
    "idea",
    "support",
    "vector",
    "machine",
    "simple",
    "algorithm",
    "creates",
    "separation",
    "line",
    "divides",
    "classes",
    "best",
    "possible",
    "manner",
    "example",
    "dog",
    "cat",
    "disease",
    "disease",
    "suppose",
    "labeled",
    "sample",
    "data",
    "tells",
    "height",
    "weight",
    "males",
    "females",
    "new",
    "data",
    "point",
    "arrives",
    "want",
    "know",
    "whether",
    "going",
    "male",
    "female",
    "start",
    "drawing",
    "line",
    "draw",
    "decision",
    "lines",
    "consider",
    "decision",
    "line",
    "one",
    "classify",
    "individual",
    "male",
    "consider",
    "decision",
    "line",
    "two",
    "female",
    "see",
    "person",
    "kind",
    "lies",
    "middle",
    "two",
    "groups",
    "little",
    "confusing",
    "trying",
    "figure",
    "line",
    "need",
    "know",
    "line",
    "divides",
    "classes",
    "correctly",
    "goal",
    "choose",
    "hyperplane",
    "one",
    "key",
    "words",
    "use",
    "talk",
    "support",
    "vector",
    "machines",
    "choose",
    "hyperplane",
    "greatest",
    "possible",
    "margin",
    "decision",
    "line",
    "nearest",
    "point",
    "within",
    "training",
    "set",
    "see",
    "support",
    "vector",
    "two",
    "nearest",
    "points",
    "draw",
    "line",
    "two",
    "points",
    "distance",
    "margin",
    "distance",
    "hyperplane",
    "nearest",
    "data",
    "point",
    "either",
    "set",
    "actually",
    "value",
    "equal",
    "distance",
    "two",
    "points",
    "comparing",
    "draw",
    "hyperplanes",
    "observe",
    "line",
    "one",
    "maximum",
    "distance",
    "observe",
    "line",
    "one",
    "maximum",
    "distance",
    "margin",
    "classify",
    "new",
    "data",
    "point",
    "correctly",
    "result",
    "one",
    "going",
    "new",
    "data",
    "point",
    "mel",
    "one",
    "reasons",
    "call",
    "hyperplane",
    "versus",
    "line",
    "lot",
    "times",
    "looking",
    "weight",
    "height",
    "might",
    "looking",
    "36",
    "different",
    "features",
    "dimensions",
    "cut",
    "hyperplane",
    "cut",
    "data",
    "cuts",
    "data",
    "certain",
    "way",
    "plane",
    "continues",
    "cut",
    "get",
    "best",
    "fit",
    "match",
    "let",
    "understand",
    "help",
    "example",
    "problem",
    "statement",
    "always",
    "start",
    "problem",
    "statement",
    "going",
    "put",
    "code",
    "together",
    "going",
    "coding",
    "classifying",
    "muffin",
    "cupcake",
    "recipes",
    "using",
    "support",
    "vector",
    "machines",
    "cupcake",
    "versus",
    "muffin",
    "let",
    "look",
    "data",
    "set",
    "different",
    "recipes",
    "muffin",
    "recipe",
    "much",
    "flour",
    "sure",
    "measurement",
    "55",
    "55",
    "maybe",
    "ounces",
    "certain",
    "amount",
    "flour",
    "certain",
    "amount",
    "milk",
    "sugar",
    "butter",
    "egg",
    "baking",
    "powder",
    "vanilla",
    "salt",
    "based",
    "measurements",
    "want",
    "guess",
    "whether",
    "making",
    "muffin",
    "cupcake",
    "see",
    "one",
    "two",
    "features",
    "height",
    "weight",
    "male",
    "female",
    "number",
    "features",
    "fact",
    "looking",
    "eight",
    "different",
    "features",
    "guess",
    "whether",
    "muffin",
    "cupcake",
    "difference",
    "muffin",
    "cupcake",
    "turns",
    "muffins",
    "flour",
    "cupcakes",
    "butter",
    "sugar",
    "basically",
    "cupcake",
    "little",
    "bit",
    "dessert",
    "muffins",
    "little",
    "bit",
    "fancy",
    "bread",
    "python",
    "code",
    "go",
    "recipes",
    "figure",
    "recipe",
    "really",
    "want",
    "say",
    "cupcakes",
    "versus",
    "muffins",
    "like",
    "big",
    "professional",
    "wrestling",
    "thing",
    "start",
    "cupcakes",
    "versus",
    "muffins",
    "going",
    "working",
    "python",
    "many",
    "versions",
    "python",
    "many",
    "different",
    "editors",
    "one",
    "strengths",
    "weaknesses",
    "python",
    "much",
    "stuff",
    "attached",
    "one",
    "popular",
    "data",
    "science",
    "programming",
    "packages",
    "use",
    "case",
    "going",
    "go",
    "ahead",
    "use",
    "anaconda",
    "jupiter",
    "nope",
    "anaconda",
    "navigator",
    "kinds",
    "fun",
    "tools",
    "anaconda",
    "navigator",
    "change",
    "environments",
    "actually",
    "number",
    "environments",
    "using",
    "python36",
    "environment",
    "python",
    "version",
    "although",
    "matter",
    "much",
    "version",
    "use",
    "usually",
    "try",
    "stay",
    "3x",
    "current",
    "unless",
    "project",
    "specifically",
    "version",
    "2x",
    "27",
    "think",
    "usually",
    "people",
    "use",
    "version",
    "jupiter",
    "notebook",
    "editor",
    "go",
    "create",
    "new",
    "file",
    "jump",
    "case",
    "svm",
    "muffin",
    "versus",
    "cupcake",
    "let",
    "start",
    "packages",
    "data",
    "analysis",
    "almost",
    "always",
    "use",
    "couple",
    "standard",
    "packages",
    "use",
    "use",
    "import",
    "import",
    "import",
    "numpy",
    "number",
    "python",
    "usually",
    "denote",
    "np",
    "comma",
    "common",
    "going",
    "import",
    "pandas",
    "pd",
    "numpy",
    "deals",
    "number",
    "arrays",
    "lot",
    "cool",
    "things",
    "numpy",
    "setup",
    "far",
    "multiplying",
    "values",
    "array",
    "numpy",
    "array",
    "data",
    "array",
    "pandas",
    "ca",
    "remember",
    "using",
    "actually",
    "data",
    "set",
    "think",
    "import",
    "makes",
    "nice",
    "data",
    "frame",
    "difference",
    "data",
    "frame",
    "numpy",
    "array",
    "data",
    "frame",
    "like",
    "excel",
    "spreadsheet",
    "columns",
    "indexes",
    "different",
    "ways",
    "referencing",
    "easily",
    "viewing",
    "additional",
    "features",
    "run",
    "data",
    "frame",
    "pandas",
    "kind",
    "sits",
    "numpy",
    "need",
    "finally",
    "working",
    "support",
    "vector",
    "machine",
    "sk",
    "learn",
    "going",
    "use",
    "sk",
    "learn",
    "model",
    "import",
    "svm",
    "support",
    "vector",
    "machine",
    "data",
    "scientist",
    "always",
    "try",
    "visualize",
    "data",
    "data",
    "obviously",
    "complicated",
    "make",
    "sense",
    "human",
    "possible",
    "good",
    "take",
    "second",
    "look",
    "actually",
    "see",
    "going",
    "use",
    "two",
    "packages",
    "going",
    "import",
    "plt",
    "common",
    "going",
    "import",
    "seaborn",
    "sns",
    "go",
    "ahead",
    "set",
    "font",
    "scale",
    "sns",
    "right",
    "import",
    "line",
    "semicolon",
    "followed",
    "line",
    "data",
    "going",
    "set",
    "sns",
    "great",
    "seaborn",
    "sits",
    "top",
    "map",
    "plot",
    "library",
    "like",
    "pandas",
    "hits",
    "numpy",
    "adds",
    "lot",
    "features",
    "uses",
    "control",
    "obviously",
    "going",
    "get",
    "matplot",
    "library",
    "see",
    "born",
    "tutorial",
    "really",
    "focusing",
    "svm",
    "support",
    "vector",
    "machine",
    "sk",
    "learn",
    "since",
    "jupiter",
    "notebook",
    "add",
    "special",
    "line",
    "matplot",
    "library",
    "percentage",
    "sign",
    "ambersign",
    "matplot",
    "library",
    "line",
    "straight",
    "code",
    "project",
    "lot",
    "times",
    "use",
    "like",
    "notepad",
    "plus",
    "plus",
    "run",
    "line",
    "pop",
    "window",
    "computer",
    "depending",
    "computer",
    "set",
    "running",
    "jupyter",
    "notebook",
    "browser",
    "setup",
    "tells",
    "display",
    "graphics",
    "right",
    "page",
    "line",
    "remember",
    "first",
    "time",
    "ran",
    "know",
    "go",
    "look",
    "years",
    "ago",
    "quite",
    "headache",
    "map",
    "plot",
    "library",
    "inline",
    "running",
    "web",
    "setup",
    "go",
    "ahead",
    "run",
    "make",
    "sure",
    "modules",
    "imported",
    "great",
    "import",
    "need",
    "go",
    "ahead",
    "pip",
    "use",
    "pip",
    "however",
    "lot",
    "install",
    "packages",
    "although",
    "pip",
    "common",
    "make",
    "sure",
    "installed",
    "python",
    "setup",
    "next",
    "step",
    "course",
    "got",
    "look",
    "data",
    "ca",
    "run",
    "model",
    "predicting",
    "data",
    "actual",
    "data",
    "let",
    "go",
    "open",
    "take",
    "look",
    "cupcakes",
    "versus",
    "muffins",
    "csv",
    "file",
    "csv",
    "meaning",
    "comma",
    "separated",
    "variable",
    "going",
    "open",
    "nice",
    "spreadsheet",
    "see",
    "type",
    "muffin",
    "muffin",
    "muffin",
    "cupcake",
    "cupcake",
    "cupcake",
    "broken",
    "flour",
    "milk",
    "sugar",
    "butter",
    "egg",
    "baking",
    "powder",
    "vanilla",
    "salt",
    "go",
    "ahead",
    "look",
    "data",
    "also",
    "python",
    "let",
    "us",
    "create",
    "variable",
    "recipes",
    "equals",
    "going",
    "use",
    "pandas",
    "module",
    "dot",
    "read",
    "csv",
    "remember",
    "comma",
    "separated",
    "variable",
    "file",
    "name",
    "happened",
    "cupcakes",
    "versus",
    "muffins",
    "oops",
    "got",
    "double",
    "brackets",
    "way",
    "go",
    "cupcakes",
    "versus",
    "muffins",
    "program",
    "loaded",
    "place",
    "saved",
    "particular",
    "python",
    "program",
    "folder",
    "get",
    "file",
    "name",
    "remember",
    "storing",
    "different",
    "location",
    "also",
    "put",
    "full",
    "path",
    "pandas",
    "going",
    "go",
    "ahead",
    "actually",
    "inline",
    "let",
    "full",
    "print",
    "type",
    "recipes",
    "dot",
    "head",
    "jupiter",
    "notebook",
    "running",
    "code",
    "different",
    "script",
    "need",
    "go",
    "ahead",
    "type",
    "whole",
    "print",
    "recipes",
    "dot",
    "head",
    "panda",
    "nose",
    "gon",
    "na",
    "first",
    "five",
    "lines",
    "data",
    "flip",
    "back",
    "spreadsheet",
    "opened",
    "csv",
    "file",
    "uh",
    "see",
    "starts",
    "line",
    "two",
    "one",
    "calls",
    "zero",
    "two",
    "three",
    "four",
    "five",
    "six",
    "going",
    "match",
    "going",
    "close",
    "need",
    "anymore",
    "always",
    "starts",
    "zero",
    "automatically",
    "indexes",
    "since",
    "tell",
    "use",
    "index",
    "index",
    "number",
    "left",
    "hand",
    "side",
    "automatically",
    "took",
    "top",
    "row",
    "labels",
    "pandas",
    "using",
    "read",
    "csv",
    "really",
    "slick",
    "fast",
    "one",
    "reasons",
    "love",
    "pandas",
    "cute",
    "cuddly",
    "teddy",
    "bears",
    "let",
    "go",
    "ahead",
    "plot",
    "data",
    "going",
    "plot",
    "going",
    "plot",
    "sugar",
    "flower",
    "obviously",
    "see",
    "get",
    "really",
    "complicated",
    "tons",
    "different",
    "features",
    "break",
    "maybe",
    "look",
    "two",
    "time",
    "see",
    "connect",
    "plot",
    "going",
    "go",
    "ahead",
    "use",
    "seaborne",
    "sns",
    "command",
    "plot",
    "two",
    "different",
    "variables",
    "going",
    "plot",
    "flour",
    "sugar",
    "data",
    "equals",
    "recipes",
    "hue",
    "equals",
    "type",
    "lot",
    "fun",
    "knows",
    "pandas",
    "coming",
    "one",
    "powerful",
    "things",
    "pandas",
    "mixed",
    "seaborne",
    "graphing",
    "going",
    "use",
    "pallet",
    "set",
    "one",
    "lot",
    "different",
    "sets",
    "go",
    "look",
    "seabourn",
    "regular",
    "fit",
    "regular",
    "equals",
    "false",
    "really",
    "trying",
    "fit",
    "anything",
    "scatter",
    "kws",
    "lot",
    "settings",
    "look",
    "seabourn",
    "half",
    "could",
    "probably",
    "leave",
    "run",
    "somebody",
    "played",
    "found",
    "best",
    "settings",
    "seaborn",
    "plot",
    "let",
    "go",
    "ahead",
    "run",
    "line",
    "puts",
    "right",
    "page",
    "see",
    "right",
    "based",
    "sugar",
    "flour",
    "alone",
    "definite",
    "split",
    "use",
    "models",
    "actually",
    "look",
    "say",
    "hey",
    "drew",
    "line",
    "right",
    "middle",
    "blue",
    "dots",
    "red",
    "dots",
    "able",
    "svm",
    "hyperplane",
    "right",
    "middle",
    "next",
    "step",
    "format",
    "data",
    "going",
    "break",
    "two",
    "parts",
    "need",
    "type",
    "label",
    "remember",
    "going",
    "decide",
    "whether",
    "muffin",
    "cupcake",
    "well",
    "computer",
    "know",
    "muffin",
    "cupcake",
    "knows",
    "0",
    "going",
    "going",
    "create",
    "type",
    "label",
    "create",
    "numpy",
    "array",
    "p",
    "logic",
    "take",
    "recipes",
    "panda",
    "wherever",
    "type",
    "equals",
    "muffin",
    "going",
    "0",
    "equal",
    "muffin",
    "cupcakes",
    "going",
    "one",
    "create",
    "type",
    "label",
    "answer",
    "training",
    "model",
    "remember",
    "training",
    "data",
    "going",
    "train",
    "zero",
    "one",
    "muffin",
    "going",
    "create",
    "recipe",
    "features",
    "remember",
    "correctly",
    "right",
    "first",
    "column",
    "type",
    "really",
    "need",
    "type",
    "columns",
    "muffin",
    "cupcake",
    "pandas",
    "easily",
    "sort",
    "take",
    "value",
    "recipes",
    "dot",
    "columns",
    "pandas",
    "function",
    "built",
    "pandas",
    "got",
    "values",
    "converting",
    "values",
    "column",
    "titles",
    "going",
    "across",
    "top",
    "want",
    "first",
    "one",
    "since",
    "always",
    "starts",
    "zero",
    "want",
    "one",
    "colon",
    "till",
    "end",
    "want",
    "go",
    "ahead",
    "make",
    "list",
    "converts",
    "list",
    "strings",
    "go",
    "ahead",
    "take",
    "look",
    "see",
    "looking",
    "features",
    "make",
    "sure",
    "looks",
    "right",
    "let",
    "go",
    "ahead",
    "run",
    "forgot",
    "recipes",
    "go",
    "ahead",
    "add",
    "run",
    "see",
    "flour",
    "milk",
    "sugar",
    "butter",
    "egg",
    "baking",
    "powder",
    "vanilla",
    "salt",
    "matches",
    "right",
    "printed",
    "everything",
    "type",
    "features",
    "label",
    "recipe",
    "features",
    "titles",
    "columns",
    "actually",
    "need",
    "ingredients",
    "point",
    "couple",
    "options",
    "one",
    "could",
    "run",
    "ingredients",
    "usually",
    "example",
    "want",
    "limit",
    "easily",
    "see",
    "going",
    "ingredients",
    "know",
    "seven",
    "eight",
    "different",
    "hyper",
    "planes",
    "would",
    "built",
    "want",
    "look",
    "see",
    "svm",
    "take",
    "recipes",
    "flour",
    "sugar",
    "replace",
    "recipe",
    "features",
    "going",
    "flour",
    "sugar",
    "going",
    "convert",
    "values",
    "need",
    "make",
    "list",
    "string",
    "values",
    "actual",
    "values",
    "go",
    "ahead",
    "print",
    "ingredients",
    "see",
    "looks",
    "like",
    "amount",
    "flour",
    "sugar",
    "two",
    "sets",
    "plots",
    "fun",
    "let",
    "go",
    "ahead",
    "take",
    "take",
    "recipe",
    "features",
    "decided",
    "use",
    "recipe",
    "features",
    "see",
    "makes",
    "nice",
    "column",
    "different",
    "data",
    "strips",
    "labels",
    "everything",
    "values",
    "want",
    "able",
    "view",
    "easily",
    "plot",
    "later",
    "go",
    "ahead",
    "take",
    "flower",
    "sugar",
    "run",
    "see",
    "two",
    "columns",
    "next",
    "step",
    "go",
    "ahead",
    "fit",
    "model",
    "going",
    "call",
    "model",
    "svm",
    "using",
    "package",
    "called",
    "svc",
    "case",
    "going",
    "go",
    "ahead",
    "set",
    "kernel",
    "equals",
    "linear",
    "using",
    "specific",
    "set",
    "go",
    "reference",
    "website",
    "svm",
    "see",
    "eight",
    "three",
    "regression",
    "three",
    "classification",
    "svc",
    "support",
    "vector",
    "classification",
    "probably",
    "one",
    "commonly",
    "used",
    "also",
    "one",
    "detecting",
    "outliers",
    "another",
    "one",
    "something",
    "little",
    "bit",
    "specific",
    "model",
    "svc",
    "sbr",
    "two",
    "commonly",
    "used",
    "standing",
    "support",
    "vector",
    "classifier",
    "support",
    "vector",
    "regression",
    "remember",
    "regression",
    "actual",
    "value",
    "float",
    "value",
    "whatever",
    "trying",
    "work",
    "sbc",
    "classifier",
    "yes",
    "true",
    "false",
    "want",
    "know",
    "zero",
    "one",
    "muffin",
    "cupcake",
    "go",
    "ahead",
    "create",
    "model",
    "model",
    "created",
    "gon",
    "na",
    "common",
    "especially",
    "sk",
    "learn",
    "models",
    "followed",
    "fit",
    "command",
    "put",
    "fit",
    "training",
    "putting",
    "ingredients",
    "case",
    "limited",
    "flour",
    "sugar",
    "type",
    "label",
    "muffin",
    "cupcake",
    "complicated",
    "data",
    "science",
    "series",
    "want",
    "split",
    "wo",
    "get",
    "today",
    "split",
    "training",
    "data",
    "test",
    "data",
    "even",
    "something",
    "split",
    "thirds",
    "third",
    "used",
    "whether",
    "switch",
    "one",
    "training",
    "test",
    "kinds",
    "things",
    "go",
    "gets",
    "complicated",
    "get",
    "higher",
    "end",
    "overly",
    "complicated",
    "extra",
    "step",
    "going",
    "today",
    "simple",
    "set",
    "data",
    "let",
    "go",
    "ahead",
    "run",
    "model",
    "fit",
    "got",
    "error",
    "let",
    "fix",
    "real",
    "quick",
    "capital",
    "svc",
    "turns",
    "lower",
    "case",
    "support",
    "vector",
    "classifier",
    "go",
    "let",
    "go",
    "ahead",
    "run",
    "see",
    "comes",
    "information",
    "prints",
    "automatically",
    "defaults",
    "model",
    "notice",
    "change",
    "kernel",
    "linear",
    "kernel",
    "linear",
    "printout",
    "different",
    "settings",
    "mess",
    "going",
    "leave",
    "alone",
    "right",
    "really",
    "need",
    "mess",
    "next",
    "going",
    "dig",
    "little",
    "bit",
    "newly",
    "trained",
    "model",
    "going",
    "show",
    "graph",
    "let",
    "go",
    "ahead",
    "get",
    "separating",
    "going",
    "say",
    "going",
    "use",
    "w",
    "variable",
    "going",
    "model",
    "dot",
    "coefficient",
    "underscore",
    "heck",
    "digging",
    "model",
    "already",
    "got",
    "prediction",
    "train",
    "math",
    "behind",
    "looking",
    "right",
    "w",
    "going",
    "represent",
    "two",
    "different",
    "coefficients",
    "remember",
    "equals",
    "mx",
    "plus",
    "c",
    "coefficients",
    "connected",
    "two",
    "dimensional",
    "plane",
    "spend",
    "much",
    "time",
    "get",
    "lost",
    "confusion",
    "math",
    "math",
    "whiz",
    "great",
    "go",
    "see",
    "equals",
    "minus",
    "w0",
    "w",
    "one",
    "remember",
    "two",
    "different",
    "values",
    "basically",
    "slope",
    "generating",
    "going",
    "build",
    "xx",
    "xx",
    "going",
    "set",
    "numpy",
    "array",
    "np",
    "dot",
    "line",
    "space",
    "creating",
    "line",
    "plus",
    "intercept",
    "well",
    "make",
    "work",
    "equals",
    "slope",
    "times",
    "value",
    "array",
    "neat",
    "thing",
    "numpy",
    "times",
    "x",
    "x",
    "whole",
    "numpy",
    "array",
    "values",
    "multiplies",
    "across",
    "takes",
    "values",
    "subtract",
    "model",
    "intercept",
    "uh",
    "mx",
    "plus",
    "c",
    "c",
    "formula",
    "equals",
    "mx",
    "plus",
    "c",
    "numbers",
    "come",
    "little",
    "bit",
    "confusing",
    "digging",
    "different",
    "arrays",
    "want",
    "going",
    "take",
    "going",
    "go",
    "ahead",
    "plot",
    "plot",
    "parallels",
    "separating",
    "hyperplane",
    "pass",
    "support",
    "vectors",
    "going",
    "create",
    "b",
    "equals",
    "model",
    "support",
    "vectors",
    "pulling",
    "support",
    "vectors",
    "yy",
    "know",
    "set",
    "data",
    "going",
    "create",
    "yy",
    "equals",
    "times",
    "xx",
    "plus",
    "b1",
    "minus",
    "times",
    "b0",
    "model",
    "support",
    "vector",
    "b",
    "going",
    "set",
    "new",
    "value",
    "minus",
    "1",
    "set",
    "equals",
    "times",
    "x",
    "x",
    "plus",
    "b",
    "1",
    "minus",
    "times",
    "b",
    "go",
    "ahead",
    "run",
    "load",
    "variables",
    "want",
    "know",
    "understand",
    "little",
    "bit",
    "going",
    "see",
    "print",
    "let",
    "run",
    "see",
    "array",
    "line",
    "going",
    "case",
    "30",
    "60",
    "going",
    "30",
    "variables",
    "thing",
    "yy",
    "yy",
    "plot",
    "minute",
    "graph",
    "see",
    "look",
    "like",
    "go",
    "ahead",
    "delete",
    "run",
    "loads",
    "variables",
    "nice",
    "clean",
    "slate",
    "going",
    "copy",
    "remember",
    "sns",
    "seaborn",
    "plot",
    "lm",
    "plot",
    "flower",
    "sugar",
    "go",
    "ahead",
    "run",
    "real",
    "quick",
    "see",
    "remember",
    "looks",
    "like",
    "straight",
    "graph",
    "one",
    "new",
    "things",
    "seaborn",
    "sits",
    "top",
    "pie",
    "plot",
    "pie",
    "plot",
    "line",
    "going",
    "simply",
    "plt",
    "dot",
    "plot",
    "xx",
    "yy",
    "two",
    "corresponding",
    "values",
    "xy",
    "somebody",
    "played",
    "figure",
    "line",
    "width",
    "equals",
    "two",
    "color",
    "black",
    "would",
    "look",
    "nice",
    "let",
    "go",
    "ahead",
    "run",
    "whole",
    "thing",
    "pie",
    "plot",
    "see",
    "flour",
    "sugar",
    "corresponding",
    "line",
    "sugar",
    "flour",
    "muffin",
    "versus",
    "cupcake",
    "generated",
    "support",
    "vectors",
    "yy",
    "yy",
    "let",
    "take",
    "look",
    "see",
    "looks",
    "like",
    "pl",
    "plot",
    "xx",
    "x",
    "value",
    "time",
    "yy",
    "let",
    "something",
    "little",
    "fun",
    "put",
    "k",
    "dash",
    "dash",
    "tells",
    "make",
    "dotted",
    "line",
    "going",
    "one",
    "also",
    "want",
    "one",
    "yy",
    "run",
    "sets",
    "line",
    "support",
    "expect",
    "expect",
    "two",
    "lines",
    "go",
    "nearest",
    "data",
    "point",
    "dashed",
    "lines",
    "go",
    "nearest",
    "muffin",
    "nearest",
    "cupcake",
    "plotting",
    "svm",
    "goes",
    "right",
    "middle",
    "gives",
    "nice",
    "split",
    "data",
    "see",
    "easy",
    "see",
    "based",
    "sugar",
    "flour",
    "one",
    "muffin",
    "cupcake",
    "let",
    "go",
    "ahead",
    "create",
    "function",
    "predict",
    "muffin",
    "cupcake",
    "got",
    "recipes",
    "pulled",
    "internet",
    "want",
    "see",
    "difference",
    "muffin",
    "cupcake",
    "need",
    "function",
    "push",
    "create",
    "function",
    "def",
    "let",
    "call",
    "muffin",
    "cupcake",
    "remember",
    "flour",
    "sugar",
    "today",
    "ingredients",
    "actually",
    "pretty",
    "good",
    "split",
    "really",
    "need",
    "ingredients",
    "know",
    "flour",
    "sugar",
    "let",
    "go",
    "ahead",
    "else",
    "statement",
    "model",
    "predict",
    "flower",
    "sugar",
    "equals",
    "zero",
    "take",
    "model",
    "run",
    "predict",
    "common",
    "sk",
    "learn",
    "dot",
    "predict",
    "put",
    "data",
    "going",
    "return",
    "value",
    "case",
    "equals",
    "zero",
    "print",
    "looking",
    "muffin",
    "recipe",
    "else",
    "zero",
    "means",
    "one",
    "looking",
    "cupcake",
    "recipe",
    "pretty",
    "straightforward",
    "function",
    "def",
    "definition",
    "e",
    "f",
    "python",
    "course",
    "going",
    "create",
    "function",
    "run",
    "something",
    "let",
    "run",
    "cupcake",
    "going",
    "send",
    "values",
    "50",
    "20",
    "muffin",
    "cupcake",
    "know",
    "let",
    "run",
    "see",
    "gives",
    "us",
    "says",
    "oh",
    "muffin",
    "looking",
    "muffin",
    "recipe",
    "easily",
    "predicts",
    "whether",
    "looking",
    "muffin",
    "cupcake",
    "recipe",
    "let",
    "plot",
    "go",
    "plot",
    "graph",
    "see",
    "actually",
    "looks",
    "like",
    "going",
    "copy",
    "paste",
    "plotting",
    "points",
    "nothing",
    "different",
    "run",
    "see",
    "points",
    "lines",
    "want",
    "want",
    "add",
    "another",
    "point",
    "plt",
    "plot",
    "remember",
    "correctly",
    "test",
    "50",
    "somebody",
    "went",
    "decided",
    "uh",
    "yellow",
    "kind",
    "oranges",
    "yellow",
    "color",
    "going",
    "come",
    "marker",
    "size",
    "nine",
    "settings",
    "play",
    "somebody",
    "else",
    "played",
    "come",
    "right",
    "setup",
    "looks",
    "good",
    "see",
    "graphed",
    "clearly",
    "muffin",
    "case",
    "cupcakes",
    "versus",
    "muffins",
    "muffin",
    "like",
    "muffin",
    "cupcake",
    "contender",
    "series",
    "certainly",
    "send",
    "note",
    "team",
    "simply",
    "learn",
    "send",
    "data",
    "use",
    "muffin",
    "cupcake",
    "true",
    "data",
    "actually",
    "run",
    "plot",
    "earlier",
    "men",
    "versus",
    "women",
    "also",
    "request",
    "information",
    "run",
    "data",
    "setup",
    "test",
    "go",
    "back",
    "setup",
    "went",
    "ahead",
    "support",
    "vector",
    "machine",
    "code",
    "predict",
    "40",
    "parts",
    "flour",
    "20",
    "parts",
    "sugar",
    "think",
    "different",
    "one",
    "whether",
    "muffin",
    "cupcake",
    "hence",
    "built",
    "classifier",
    "using",
    "svm",
    "able",
    "classify",
    "recipe",
    "cupcake",
    "muffin",
    "wraps",
    "cupcake",
    "versus",
    "muffin",
    "today",
    "second",
    "tutorial",
    "going",
    "cover",
    "linear",
    "regression",
    "along",
    "going",
    "quiz",
    "questions",
    "first",
    "tutorial",
    "going",
    "cover",
    "clustering",
    "clustering",
    "k",
    "means",
    "clustering",
    "one",
    "common",
    "use",
    "clustering",
    "tools",
    "including",
    "flow",
    "chart",
    "understand",
    "clustering",
    "functions",
    "actual",
    "python",
    "live",
    "demo",
    "clustering",
    "cars",
    "based",
    "brands",
    "going",
    "cover",
    "logistic",
    "regression",
    "logistic",
    "regression",
    "logistic",
    "regression",
    "curve",
    "sigmoid",
    "function",
    "another",
    "python",
    "code",
    "demo",
    "classify",
    "tumor",
    "malignant",
    "benign",
    "based",
    "features",
    "let",
    "start",
    "clustering",
    "suppose",
    "pile",
    "books",
    "different",
    "genres",
    "divide",
    "different",
    "groups",
    "like",
    "fiction",
    "horror",
    "education",
    "see",
    "young",
    "lady",
    "definitely",
    "heavy",
    "horror",
    "tell",
    "eyes",
    "maple",
    "canadian",
    "leaf",
    "shirt",
    "fiction",
    "horror",
    "education",
    "want",
    "go",
    "ahead",
    "divide",
    "books",
    "well",
    "organizing",
    "objects",
    "groups",
    "based",
    "similarity",
    "clustering",
    "case",
    "looking",
    "books",
    "talking",
    "clustering",
    "things",
    "known",
    "categories",
    "also",
    "use",
    "explore",
    "data",
    "might",
    "know",
    "categories",
    "know",
    "need",
    "divide",
    "way",
    "conquer",
    "data",
    "organize",
    "better",
    "case",
    "going",
    "looking",
    "clustering",
    "specific",
    "categories",
    "let",
    "take",
    "deeper",
    "look",
    "going",
    "use",
    "k",
    "means",
    "clustering",
    "k",
    "means",
    "clustering",
    "probably",
    "commonly",
    "used",
    "clustering",
    "tool",
    "machine",
    "learning",
    "library",
    "clustering",
    "example",
    "unsupervised",
    "learning",
    "remember",
    "previous",
    "thing",
    "used",
    "unlabeled",
    "data",
    "know",
    "answer",
    "yet",
    "bunch",
    "data",
    "want",
    "cluster",
    "different",
    "groups",
    "define",
    "clusters",
    "data",
    "based",
    "feature",
    "similarity",
    "introduced",
    "couple",
    "terms",
    "already",
    "talked",
    "unsupervised",
    "learning",
    "unlabeled",
    "data",
    "know",
    "answer",
    "yet",
    "going",
    "group",
    "stuff",
    "together",
    "see",
    "find",
    "answer",
    "things",
    "connect",
    "also",
    "introduced",
    "feature",
    "similarity",
    "features",
    "different",
    "features",
    "data",
    "books",
    "easily",
    "see",
    "fiction",
    "horror",
    "history",
    "books",
    "lot",
    "times",
    "data",
    "information",
    "easy",
    "see",
    "right",
    "first",
    "look",
    "one",
    "tools",
    "start",
    "finding",
    "things",
    "connect",
    "match",
    "suppose",
    "data",
    "points",
    "want",
    "assign",
    "cluster",
    "look",
    "data",
    "points",
    "would",
    "probably",
    "group",
    "two",
    "clusters",
    "looking",
    "say",
    "two",
    "group",
    "data",
    "kind",
    "come",
    "together",
    "k",
    "means",
    "pick",
    "k",
    "clusters",
    "assign",
    "random",
    "centroids",
    "clusters",
    "k",
    "clusters",
    "represents",
    "two",
    "different",
    "clusters",
    "pick",
    "k",
    "clusters",
    "cyranium",
    "centroids",
    "clusters",
    "compute",
    "distance",
    "objects",
    "centroids",
    "form",
    "new",
    "clusters",
    "based",
    "minimum",
    "distances",
    "calculate",
    "centroids",
    "figure",
    "best",
    "distance",
    "centroid",
    "move",
    "centroid",
    "recalculate",
    "distances",
    "repeat",
    "previous",
    "two",
    "steps",
    "iteratively",
    "till",
    "cluster",
    "centroid",
    "stop",
    "changing",
    "positions",
    "become",
    "static",
    "repeat",
    "previous",
    "two",
    "steps",
    "iteratively",
    "till",
    "cluster",
    "centroid",
    "stop",
    "changing",
    "positions",
    "become",
    "static",
    "clusters",
    "become",
    "static",
    "k",
    "means",
    "clustering",
    "algorithm",
    "said",
    "converged",
    "another",
    "term",
    "see",
    "throughout",
    "machine",
    "learning",
    "converged",
    "means",
    "whatever",
    "math",
    "using",
    "figure",
    "answer",
    "come",
    "solution",
    "converged",
    "answer",
    "see",
    "flowchart",
    "understand",
    "make",
    "little",
    "bit",
    "sense",
    "putting",
    "nice",
    "easy",
    "step",
    "step",
    "start",
    "choose",
    "k",
    "look",
    "elbow",
    "method",
    "moment",
    "assign",
    "random",
    "centroids",
    "clusters",
    "sometimes",
    "pick",
    "centroids",
    "might",
    "look",
    "data",
    "graph",
    "say",
    "oh",
    "probably",
    "central",
    "points",
    "compute",
    "distance",
    "objects",
    "centroids",
    "take",
    "form",
    "new",
    "clusters",
    "based",
    "minimum",
    "distance",
    "calculate",
    "centroids",
    "compute",
    "distance",
    "objects",
    "new",
    "centroids",
    "go",
    "back",
    "repeat",
    "last",
    "two",
    "steps",
    "calculate",
    "distances",
    "brings",
    "new",
    "centroid",
    "move",
    "centroid",
    "around",
    "figure",
    "best",
    "objects",
    "closest",
    "centroid",
    "objects",
    "switch",
    "one",
    "centroid",
    "centroids",
    "moved",
    "around",
    "continue",
    "converged",
    "let",
    "see",
    "example",
    "suppose",
    "data",
    "set",
    "seven",
    "individuals",
    "score",
    "two",
    "topics",
    "b",
    "subject",
    "case",
    "referring",
    "person",
    "taking",
    "test",
    "subject",
    "see",
    "scored",
    "first",
    "subject",
    "subject",
    "b",
    "see",
    "score",
    "second",
    "subject",
    "let",
    "take",
    "two",
    "farthest",
    "apart",
    "points",
    "initials",
    "cluster",
    "centroids",
    "remember",
    "talked",
    "selecting",
    "randomly",
    "also",
    "put",
    "different",
    "points",
    "pick",
    "furthest",
    "one",
    "apart",
    "move",
    "together",
    "either",
    "one",
    "works",
    "okay",
    "depending",
    "kind",
    "data",
    "working",
    "know",
    "took",
    "two",
    "furthest",
    "points",
    "one",
    "one",
    "five",
    "seven",
    "let",
    "take",
    "two",
    "farthest",
    "apart",
    "points",
    "initial",
    "cluster",
    "centroids",
    "point",
    "assigned",
    "closest",
    "cluster",
    "respect",
    "distance",
    "centroids",
    "take",
    "one",
    "points",
    "measure",
    "distance",
    "see",
    "measure",
    "distances",
    "use",
    "pythagorean",
    "theorem",
    "triangle",
    "case",
    "know",
    "x",
    "figure",
    "diagonal",
    "line",
    "take",
    "ruler",
    "put",
    "monitor",
    "kind",
    "silly",
    "would",
    "work",
    "eyeballing",
    "see",
    "naturally",
    "come",
    "together",
    "certain",
    "areas",
    "calculate",
    "centroids",
    "cluster",
    "cluster",
    "one",
    "cluster",
    "two",
    "look",
    "individual",
    "dot",
    "one",
    "two",
    "three",
    "one",
    "cluster",
    "uh",
    "centroid",
    "moves",
    "becomes",
    "comma",
    "remember",
    "one",
    "one",
    "well",
    "center",
    "data",
    "looking",
    "would",
    "put",
    "one",
    "point",
    "roughly",
    "2",
    "2",
    "second",
    "one",
    "wanted",
    "make",
    "overall",
    "mean",
    "vector",
    "average",
    "vector",
    "different",
    "distances",
    "centroid",
    "come",
    "four",
    "comma",
    "one",
    "five",
    "four",
    "moved",
    "centroids",
    "compare",
    "individual",
    "distance",
    "cluster",
    "mean",
    "opposite",
    "cluster",
    "find",
    "build",
    "nice",
    "chart",
    "move",
    "centroid",
    "around",
    "new",
    "different",
    "kind",
    "clustering",
    "groups",
    "using",
    "euclidean",
    "distance",
    "points",
    "mean",
    "get",
    "formula",
    "see",
    "new",
    "formulas",
    "coming",
    "individual",
    "dots",
    "distance",
    "mean",
    "centroid",
    "cluster",
    "distance",
    "mean",
    "centroid",
    "cluster",
    "individual",
    "three",
    "nearer",
    "mean",
    "opposite",
    "cluster",
    "cluster",
    "two",
    "cluster",
    "one",
    "see",
    "diagram",
    "kind",
    "circled",
    "one",
    "middle",
    "moved",
    "cluster",
    "centroids",
    "clusters",
    "one",
    "points",
    "shifted",
    "cluster",
    "closer",
    "group",
    "individuals",
    "thus",
    "individual",
    "3",
    "relocated",
    "cluster",
    "2",
    "resulting",
    "new",
    "partition",
    "regenerate",
    "numbers",
    "close",
    "different",
    "clusters",
    "new",
    "clusters",
    "find",
    "actual",
    "cluster",
    "centroids",
    "move",
    "centroids",
    "see",
    "formed",
    "two",
    "distinct",
    "clusters",
    "comparing",
    "distance",
    "individual",
    "distance",
    "cluster",
    "mean",
    "opposite",
    "cluster",
    "find",
    "data",
    "points",
    "stable",
    "hence",
    "final",
    "clusters",
    "remember",
    "brought",
    "concept",
    "earlier",
    "caming",
    "algorithm",
    "choosing",
    "right",
    "value",
    "k",
    "help",
    "less",
    "number",
    "iterations",
    "find",
    "appropriate",
    "number",
    "clusters",
    "data",
    "set",
    "use",
    "elbow",
    "method",
    "within",
    "sum",
    "squares",
    "wss",
    "defined",
    "sum",
    "squared",
    "distance",
    "member",
    "cluster",
    "centroid",
    "see",
    "done",
    "number",
    "clusters",
    "algorithm",
    "different",
    "clusters",
    "calculate",
    "centroid",
    "looks",
    "like",
    "find",
    "optimal",
    "actually",
    "find",
    "optimal",
    "number",
    "clusters",
    "using",
    "elbow",
    "graph",
    "called",
    "elbow",
    "method",
    "guessed",
    "two",
    "looking",
    "data",
    "see",
    "slope",
    "actually",
    "look",
    "right",
    "elbow",
    "slope",
    "clear",
    "answer",
    "want",
    "two",
    "different",
    "start",
    "k",
    "means",
    "equals",
    "two",
    "lot",
    "times",
    "people",
    "end",
    "computing",
    "equals",
    "two",
    "three",
    "four",
    "five",
    "find",
    "value",
    "fits",
    "elbow",
    "joint",
    "sometimes",
    "look",
    "data",
    "really",
    "good",
    "specific",
    "domain",
    "remember",
    "domain",
    "mentioned",
    "last",
    "time",
    "know",
    "pick",
    "numbers",
    "start",
    "guessing",
    "k",
    "value",
    "let",
    "take",
    "going",
    "use",
    "use",
    "case",
    "using",
    "clustering",
    "cluster",
    "cars",
    "brands",
    "using",
    "parameters",
    "horsepower",
    "cubic",
    "inches",
    "make",
    "year",
    "etc",
    "going",
    "use",
    "data",
    "set",
    "cars",
    "data",
    "information",
    "three",
    "brands",
    "cars",
    "toyota",
    "honda",
    "nissan",
    "go",
    "back",
    "favorite",
    "tool",
    "anaconda",
    "navigator",
    "jupiter",
    "notebook",
    "let",
    "go",
    "ahead",
    "flip",
    "jupiter",
    "notebook",
    "jupiter",
    "notebook",
    "going",
    "go",
    "ahead",
    "paste",
    "basic",
    "code",
    "usually",
    "start",
    "lot",
    "going",
    "go",
    "much",
    "code",
    "already",
    "discussed",
    "numpy",
    "already",
    "discussed",
    "matplot",
    "library",
    "pandas",
    "number",
    "array",
    "pandas",
    "pandas",
    "data",
    "frame",
    "map",
    "plot",
    "graphing",
    "forget",
    "since",
    "using",
    "jupiter",
    "notebook",
    "need",
    "map",
    "plot",
    "library",
    "inline",
    "plots",
    "everything",
    "screen",
    "using",
    "different",
    "python",
    "editor",
    "probably",
    "need",
    "window",
    "computer",
    "go",
    "ahead",
    "run",
    "load",
    "libraries",
    "setup",
    "next",
    "step",
    "course",
    "look",
    "data",
    "already",
    "opened",
    "spreadsheet",
    "see",
    "miles",
    "per",
    "gallon",
    "cylinders",
    "cubic",
    "inches",
    "horsepower",
    "weight",
    "pounds",
    "heavy",
    "time",
    "takes",
    "get",
    "60",
    "card",
    "probably",
    "one",
    "80",
    "year",
    "actually",
    "see",
    "kind",
    "older",
    "cars",
    "brand",
    "toyota",
    "honda",
    "nissan",
    "different",
    "cars",
    "coming",
    "way",
    "1971",
    "scroll",
    "80s",
    "70s",
    "80s",
    "number",
    "cars",
    "put",
    "let",
    "uh",
    "come",
    "back",
    "going",
    "importing",
    "data",
    "go",
    "ahead",
    "data",
    "set",
    "equals",
    "use",
    "pandas",
    "read",
    "uh",
    "csv",
    "file",
    "remember",
    "always",
    "post",
    "comments",
    "request",
    "data",
    "files",
    "either",
    "comments",
    "youtube",
    "video",
    "go",
    "request",
    "car",
    "csv",
    "put",
    "folder",
    "code",
    "stored",
    "python",
    "code",
    "stored",
    "folder",
    "put",
    "full",
    "path",
    "store",
    "different",
    "folders",
    "change",
    "double",
    "check",
    "name",
    "variables",
    "go",
    "ahead",
    "run",
    "chosen",
    "set",
    "arbitrarily",
    "know",
    "data",
    "set",
    "importing",
    "imported",
    "car",
    "csv",
    "data",
    "set",
    "know",
    "prep",
    "data",
    "going",
    "create",
    "x",
    "data",
    "one",
    "going",
    "try",
    "figure",
    "going",
    "number",
    "ways",
    "simple",
    "loop",
    "actually",
    "see",
    "going",
    "four",
    "x",
    "dot",
    "columns",
    "going",
    "go",
    "columns",
    "lot",
    "times",
    "important",
    "make",
    "lists",
    "columns",
    "might",
    "remove",
    "certain",
    "columns",
    "might",
    "columns",
    "want",
    "processed",
    "differently",
    "go",
    "ahead",
    "take",
    "x",
    "want",
    "go",
    "fill",
    "pandas",
    "command",
    "question",
    "going",
    "fill",
    "missing",
    "data",
    "definitely",
    "want",
    "put",
    "number",
    "actually",
    "mean",
    "something",
    "one",
    "tricks",
    "take",
    "x",
    "addition",
    "want",
    "go",
    "ahead",
    "turn",
    "integer",
    "lot",
    "integers",
    "go",
    "ahead",
    "keep",
    "integers",
    "let",
    "add",
    "bracket",
    "lot",
    "editors",
    "think",
    "closing",
    "one",
    "bracket",
    "make",
    "sure",
    "get",
    "second",
    "bracket",
    "double",
    "bracket",
    "always",
    "something",
    "happens",
    "regularly",
    "integer",
    "x",
    "going",
    "fill",
    "missing",
    "data",
    "average",
    "busy",
    "closing",
    "one",
    "set",
    "brackets",
    "forgot",
    "mean",
    "also",
    "brackets",
    "pandas",
    "see",
    "going",
    "fill",
    "data",
    "average",
    "value",
    "column",
    "missing",
    "data",
    "average",
    "data",
    "done",
    "go",
    "ahead",
    "loop",
    "check",
    "see",
    "make",
    "sure",
    "everything",
    "filled",
    "correctly",
    "print",
    "take",
    "x",
    "null",
    "returns",
    "set",
    "null",
    "value",
    "many",
    "lines",
    "null",
    "sum",
    "see",
    "looks",
    "like",
    "run",
    "x",
    "want",
    "want",
    "remove",
    "last",
    "column",
    "models",
    "trying",
    "see",
    "cluster",
    "things",
    "figure",
    "models",
    "many",
    "different",
    "ways",
    "sort",
    "x",
    "one",
    "could",
    "take",
    "x",
    "could",
    "go",
    "data",
    "set",
    "variable",
    "using",
    "use",
    "location",
    "one",
    "features",
    "pandas",
    "could",
    "take",
    "take",
    "rows",
    "last",
    "column",
    "data",
    "set",
    "time",
    "could",
    "values",
    "converted",
    "values",
    "one",
    "way",
    "let",
    "put",
    "print",
    "x",
    "capital",
    "x",
    "chose",
    "run",
    "see",
    "values",
    "could",
    "also",
    "take",
    "values",
    "going",
    "return",
    "anything",
    "values",
    "connected",
    "like",
    "instead",
    "eye",
    "location",
    "integers",
    "common",
    "come",
    "data",
    "set",
    "going",
    "data",
    "set",
    "dot",
    "data",
    "set",
    "dot",
    "columns",
    "remember",
    "lists",
    "columns",
    "come",
    "let",
    "mark",
    "red",
    "print",
    "data",
    "set",
    "dot",
    "columns",
    "see",
    "index",
    "mpg",
    "cylinders",
    "everything",
    "including",
    "brand",
    "want",
    "way",
    "get",
    "rid",
    "brand",
    "would",
    "data",
    "columns",
    "everything",
    "last",
    "one",
    "minus",
    "one",
    "print",
    "see",
    "brand",
    "disappears",
    "actually",
    "take",
    "data",
    "set",
    "columns",
    "minus",
    "one",
    "put",
    "right",
    "columns",
    "going",
    "look",
    "let",
    "un",
    "mark",
    "unmark",
    "x",
    "dot",
    "head",
    "new",
    "data",
    "frame",
    "see",
    "right",
    "different",
    "columns",
    "except",
    "brand",
    "end",
    "year",
    "turns",
    "start",
    "playing",
    "data",
    "set",
    "going",
    "get",
    "error",
    "later",
    "say",
    "convert",
    "string",
    "float",
    "value",
    "reason",
    "things",
    "way",
    "recorded",
    "must",
    "recorded",
    "strings",
    "neat",
    "feature",
    "pandas",
    "convert",
    "simply",
    "convert",
    "objects",
    "going",
    "convert",
    "oops",
    "convert",
    "underscore",
    "numeric",
    "numeric",
    "equals",
    "true",
    "yes",
    "go",
    "look",
    "memorized",
    "convert",
    "numeric",
    "working",
    "lot",
    "things",
    "remember",
    "depending",
    "usually",
    "look",
    "run",
    "oops",
    "must",
    "missed",
    "something",
    "let",
    "double",
    "check",
    "spelling",
    "double",
    "check",
    "spilling",
    "see",
    "missed",
    "first",
    "underscore",
    "convert",
    "objects",
    "run",
    "everything",
    "converted",
    "numeric",
    "value",
    "going",
    "working",
    "numeric",
    "values",
    "next",
    "part",
    "need",
    "go",
    "data",
    "eliminate",
    "null",
    "values",
    "people",
    "small",
    "amounts",
    "working",
    "small",
    "data",
    "pools",
    "discover",
    "afterwards",
    "null",
    "value",
    "go",
    "back",
    "know",
    "aware",
    "whenever",
    "formatting",
    "data",
    "things",
    "going",
    "pop",
    "sometimes",
    "go",
    "backwards",
    "fix",
    "fine",
    "part",
    "exploring",
    "data",
    "understanding",
    "done",
    "earlier",
    "let",
    "go",
    "ahead",
    "increase",
    "size",
    "window",
    "one",
    "notch",
    "go",
    "easier",
    "see",
    "4i",
    "working",
    "x",
    "dot",
    "columns",
    "page",
    "columns",
    "want",
    "take",
    "x",
    "going",
    "change",
    "going",
    "alter",
    "want",
    "go",
    "ahead",
    "fill",
    "x",
    "pandas",
    "fill",
    "fills",
    "missing",
    "data",
    "put",
    "brackets",
    "lot",
    "different",
    "ways",
    "fill",
    "data",
    "really",
    "large",
    "data",
    "set",
    "people",
    "void",
    "data",
    "look",
    "later",
    "separate",
    "exploration",
    "data",
    "one",
    "tricks",
    "take",
    "column",
    "find",
    "means",
    "means",
    "quotation",
    "marks",
    "take",
    "columns",
    "going",
    "fill",
    "one",
    "means",
    "problem",
    "returns",
    "decimal",
    "float",
    "decimals",
    "certainly",
    "let",
    "little",
    "careful",
    "example",
    "going",
    "fill",
    "integer",
    "version",
    "keeps",
    "par",
    "data",
    "decimal",
    "point",
    "also",
    "want",
    "want",
    "double",
    "check",
    "ways",
    "simply",
    "go",
    "take",
    "x",
    "column",
    "going",
    "go",
    "x",
    "column",
    "says",
    "null",
    "going",
    "return",
    "place",
    "null",
    "value",
    "actually",
    "goes",
    "rows",
    "column",
    "null",
    "want",
    "go",
    "ahead",
    "sum",
    "take",
    "add",
    "sum",
    "value",
    "pandas",
    "null",
    "panda",
    "command",
    "sum",
    "go",
    "go",
    "ahead",
    "run",
    "go",
    "ahead",
    "take",
    "run",
    "see",
    "columns",
    "zero",
    "null",
    "values",
    "tested",
    "double",
    "checked",
    "data",
    "nice",
    "clean",
    "null",
    "values",
    "everything",
    "number",
    "value",
    "turned",
    "numeric",
    "removed",
    "last",
    "column",
    "data",
    "point",
    "actually",
    "going",
    "start",
    "using",
    "elbow",
    "method",
    "find",
    "optimal",
    "number",
    "clusters",
    "actually",
    "getting",
    "sk",
    "learn",
    "part",
    "uh",
    "k",
    "means",
    "clustering",
    "guess",
    "go",
    "ahead",
    "zoom",
    "one",
    "see",
    "typing",
    "sk",
    "learn",
    "going",
    "sk",
    "learn",
    "cluster",
    "going",
    "import",
    "k",
    "means",
    "always",
    "forget",
    "capitalize",
    "k",
    "capital",
    "k",
    "capital",
    "k",
    "means",
    "go",
    "create",
    "um",
    "array",
    "wcss",
    "equals",
    "get",
    "empty",
    "array",
    "remember",
    "elbow",
    "method",
    "slide",
    "within",
    "sums",
    "squares",
    "wss",
    "defined",
    "sum",
    "square",
    "distance",
    "member",
    "cluster",
    "centroid",
    "looking",
    "change",
    "differences",
    "far",
    "squared",
    "distance",
    "going",
    "run",
    "number",
    "values",
    "fact",
    "let",
    "go",
    "range",
    "11",
    "0",
    "first",
    "thing",
    "going",
    "going",
    "create",
    "actual",
    "blue",
    "lower",
    "case",
    "going",
    "create",
    "object",
    "imported",
    "variable",
    "want",
    "put",
    "clusters",
    "going",
    "set",
    "equals",
    "important",
    "one",
    "looking",
    "increasing",
    "number",
    "clusters",
    "changes",
    "answer",
    "lot",
    "settings",
    "guys",
    "back",
    "great",
    "job",
    "kind",
    "playing",
    "common",
    "ones",
    "see",
    "lot",
    "stuff",
    "init",
    "plus",
    "plus",
    "tool",
    "let",
    "model",
    "smart",
    "picks",
    "centroids",
    "start",
    "initial",
    "incentroids",
    "want",
    "iterate",
    "300",
    "times",
    "max",
    "iteration",
    "put",
    "infinite",
    "random",
    "state",
    "equals",
    "zero",
    "really",
    "need",
    "worry",
    "much",
    "first",
    "learning",
    "start",
    "digging",
    "deeper",
    "start",
    "finding",
    "shortcuts",
    "speed",
    "process",
    "far",
    "setup",
    "big",
    "one",
    "working",
    "clusters",
    "equals",
    "going",
    "literally",
    "train",
    "11",
    "times",
    "going",
    "process",
    "11",
    "times",
    "working",
    "big",
    "data",
    "know",
    "first",
    "thing",
    "run",
    "small",
    "sample",
    "data",
    "test",
    "stuff",
    "already",
    "see",
    "problem",
    "going",
    "iterate",
    "terabyte",
    "data",
    "11",
    "times",
    "k",
    "means",
    "iterating",
    "data",
    "multiple",
    "times",
    "heck",
    "process",
    "got",
    "little",
    "careful",
    "lot",
    "times",
    "though",
    "find",
    "elbow",
    "using",
    "elbow",
    "method",
    "find",
    "optimal",
    "number",
    "sample",
    "data",
    "especially",
    "working",
    "larger",
    "data",
    "sources",
    "want",
    "go",
    "ahead",
    "take",
    "going",
    "fit",
    "looking",
    "common",
    "fit",
    "model",
    "remember",
    "correctly",
    "variable",
    "using",
    "capital",
    "x",
    "fit",
    "value",
    "go",
    "back",
    "array",
    "made",
    "want",
    "go",
    "depend",
    "value",
    "end",
    "actual",
    "fit",
    "pinning",
    "generates",
    "generates",
    "value",
    "looking",
    "inertia",
    "k",
    "means",
    "dot",
    "inertia",
    "pull",
    "specific",
    "value",
    "need",
    "let",
    "get",
    "visual",
    "plt",
    "plot",
    "plotting",
    "first",
    "x",
    "axis",
    "range",
    "0",
    "11",
    "generate",
    "nice",
    "little",
    "plot",
    "wcss",
    "always",
    "nice",
    "give",
    "plot",
    "title",
    "let",
    "see",
    "give",
    "elbow",
    "method",
    "title",
    "let",
    "get",
    "labels",
    "let",
    "go",
    "ahead",
    "plt",
    "x",
    "label",
    "number",
    "clusters",
    "plt",
    "label",
    "oops",
    "go",
    "wcss",
    "since",
    "plot",
    "finally",
    "want",
    "go",
    "ahead",
    "display",
    "graph",
    "simply",
    "plt",
    "dot",
    "oops",
    "dot",
    "show",
    "go",
    "set",
    "inline",
    "appear",
    "line",
    "hopefully",
    "make",
    "type",
    "error",
    "see",
    "get",
    "nice",
    "graph",
    "see",
    "nice",
    "elbow",
    "joint",
    "two",
    "right",
    "around",
    "three",
    "four",
    "much",
    "data",
    "scientist",
    "looking",
    "would",
    "either",
    "three",
    "four",
    "actually",
    "try",
    "see",
    "output",
    "look",
    "like",
    "already",
    "tried",
    "back",
    "going",
    "use",
    "three",
    "setup",
    "let",
    "go",
    "ahead",
    "see",
    "looks",
    "like",
    "actually",
    "use",
    "show",
    "different",
    "kinds",
    "cars",
    "let",
    "go",
    "ahead",
    "apply",
    "cars",
    "data",
    "set",
    "basically",
    "going",
    "copy",
    "code",
    "loop",
    "k",
    "means",
    "equals",
    "k",
    "means",
    "number",
    "clusters",
    "going",
    "set",
    "number",
    "clusters",
    "since",
    "going",
    "look",
    "3",
    "4",
    "graph",
    "see",
    "come",
    "differently",
    "kind",
    "curious",
    "look",
    "going",
    "set",
    "go",
    "ahead",
    "create",
    "variable",
    "k",
    "means",
    "answers",
    "going",
    "set",
    "equal",
    "whoops",
    "double",
    "equal",
    "2k",
    "means",
    "going",
    "fit",
    "going",
    "fit",
    "predict",
    "setup",
    "want",
    "use",
    "using",
    "untrained",
    "models",
    "see",
    "slightly",
    "different",
    "usually",
    "see",
    "fit",
    "see",
    "predict",
    "want",
    "fit",
    "predict",
    "fit",
    "underscore",
    "predict",
    "capital",
    "x",
    "data",
    "working",
    "plot",
    "data",
    "going",
    "little",
    "pandas",
    "trick",
    "going",
    "take",
    "x",
    "value",
    "going",
    "set",
    "x",
    "matrix",
    "converting",
    "nice",
    "rows",
    "columns",
    "kind",
    "set",
    "want",
    "going",
    "columns",
    "equals",
    "none",
    "going",
    "matrix",
    "data",
    "let",
    "go",
    "ahead",
    "run",
    "little",
    "warning",
    "see",
    "warnings",
    "pop",
    "things",
    "always",
    "updated",
    "like",
    "minor",
    "changes",
    "versions",
    "future",
    "versions",
    "instead",
    "matrix",
    "common",
    "set",
    "dot",
    "values",
    "instead",
    "matrix",
    "math",
    "matrix",
    "works",
    "fine",
    "right",
    "want",
    "update",
    "later",
    "let",
    "go",
    "ahead",
    "dive",
    "plot",
    "see",
    "looks",
    "like",
    "dive",
    "plotting",
    "data",
    "always",
    "like",
    "take",
    "look",
    "see",
    "plotting",
    "let",
    "take",
    "look",
    "k",
    "means",
    "going",
    "print",
    "see",
    "array",
    "answers",
    "2",
    "1",
    "0",
    "2",
    "1",
    "clustering",
    "different",
    "rows",
    "data",
    "based",
    "three",
    "different",
    "spaces",
    "thinks",
    "going",
    "let",
    "go",
    "ahead",
    "print",
    "x",
    "see",
    "x",
    "see",
    "x",
    "array",
    "matrix",
    "different",
    "values",
    "array",
    "going",
    "hard",
    "plot",
    "different",
    "values",
    "array",
    "going",
    "looking",
    "first",
    "two",
    "positions",
    "0",
    "one",
    "full",
    "presentation",
    "front",
    "board",
    "meeting",
    "might",
    "actually",
    "little",
    "different",
    "dig",
    "little",
    "deeper",
    "different",
    "aspects",
    "different",
    "columns",
    "looked",
    "look",
    "columns",
    "one",
    "two",
    "make",
    "easy",
    "let",
    "go",
    "ahead",
    "clear",
    "data",
    "let",
    "bring",
    "plot",
    "going",
    "scatter",
    "plot",
    "plt",
    "scatter",
    "looks",
    "little",
    "complicated",
    "let",
    "explain",
    "going",
    "going",
    "take",
    "x",
    "values",
    "interested",
    "k",
    "means",
    "equals",
    "0",
    "first",
    "cluster",
    "okay",
    "going",
    "take",
    "value",
    "0",
    "going",
    "thing",
    "interested",
    "k",
    "means",
    "equals",
    "0",
    "going",
    "take",
    "second",
    "column",
    "looking",
    "first",
    "two",
    "columns",
    "answer",
    "data",
    "guys",
    "back",
    "played",
    "little",
    "bit",
    "make",
    "pretty",
    "discovered",
    "looks",
    "good",
    "size",
    "equals",
    "100",
    "size",
    "dots",
    "going",
    "use",
    "red",
    "one",
    "looking",
    "data",
    "came",
    "definitely",
    "toyota",
    "going",
    "go",
    "ahead",
    "label",
    "toyota",
    "something",
    "really",
    "explore",
    "far",
    "playing",
    "numbers",
    "see",
    "looks",
    "good",
    "go",
    "ahead",
    "hit",
    "enter",
    "going",
    "paste",
    "next",
    "two",
    "lines",
    "next",
    "two",
    "cars",
    "nissa",
    "honda",
    "see",
    "scatter",
    "plot",
    "looking",
    "underscore",
    "k",
    "means",
    "equals",
    "one",
    "want",
    "zero",
    "column",
    "k",
    "means",
    "equals",
    "two",
    "looking",
    "first",
    "two",
    "columns",
    "zero",
    "one",
    "rows",
    "corresponds",
    "nissan",
    "honda",
    "go",
    "ahead",
    "hit",
    "enter",
    "uh",
    "finally",
    "let",
    "take",
    "look",
    "put",
    "centroids",
    "going",
    "scatter",
    "plot",
    "centroids",
    "pull",
    "model",
    "created",
    "dot",
    "cluster",
    "centers",
    "going",
    "first",
    "number",
    "second",
    "number",
    "0",
    "1",
    "always",
    "start",
    "0",
    "playing",
    "size",
    "everything",
    "make",
    "look",
    "good",
    "size",
    "300",
    "going",
    "make",
    "color",
    "yellow",
    "label",
    "always",
    "good",
    "good",
    "labels",
    "centroids",
    "want",
    "title",
    "plt",
    "title",
    "pop",
    "plt",
    "title",
    "cause",
    "always",
    "want",
    "make",
    "graphs",
    "look",
    "pretty",
    "call",
    "clusters",
    "car",
    "make",
    "one",
    "features",
    "plot",
    "library",
    "add",
    "legend",
    "automatically",
    "bring",
    "since",
    "already",
    "labeled",
    "different",
    "aspects",
    "legend",
    "toyota",
    "nissan",
    "honda",
    "finally",
    "want",
    "go",
    "ahead",
    "show",
    "actually",
    "see",
    "remember",
    "line",
    "using",
    "different",
    "editor",
    "jupiter",
    "notebook",
    "get",
    "nice",
    "set",
    "clusters",
    "look",
    "clusters",
    "honda",
    "green",
    "toyota",
    "red",
    "nissan",
    "purple",
    "see",
    "put",
    "centroids",
    "separate",
    "looking",
    "also",
    "plot",
    "lot",
    "different",
    "data",
    "far",
    "looked",
    "first",
    "two",
    "columns",
    "column",
    "one",
    "two",
    "zero",
    "one",
    "label",
    "computer",
    "scripting",
    "see",
    "nice",
    "clusters",
    "car",
    "make",
    "able",
    "pull",
    "data",
    "see",
    "two",
    "columns",
    "form",
    "distinct",
    "clusters",
    "data",
    "exploring",
    "new",
    "data",
    "might",
    "take",
    "look",
    "say",
    "well",
    "makes",
    "different",
    "almost",
    "going",
    "reverse",
    "start",
    "looking",
    "data",
    "pulling",
    "apart",
    "columns",
    "find",
    "first",
    "group",
    "set",
    "way",
    "maybe",
    "loans",
    "want",
    "go",
    "well",
    "group",
    "defaulting",
    "loans",
    "last",
    "group",
    "defaulting",
    "loans",
    "middle",
    "group",
    "50",
    "defaulting",
    "bank",
    "loans",
    "start",
    "finding",
    "ways",
    "manipulate",
    "data",
    "pull",
    "answers",
    "want",
    "seen",
    "use",
    "clustering",
    "let",
    "move",
    "next",
    "topic",
    "let",
    "look",
    "logistic",
    "regression",
    "logistic",
    "regression",
    "algorithm",
    "simplest",
    "classification",
    "algorithm",
    "used",
    "binary",
    "problems",
    "see",
    "little",
    "girl",
    "canada",
    "horror",
    "books",
    "back",
    "actually",
    "really",
    "scary",
    "think",
    "big",
    "eyes",
    "previous",
    "tutorial",
    "learned",
    "linear",
    "regression",
    "dependent",
    "independent",
    "variables",
    "brush",
    "equals",
    "mx",
    "plus",
    "c",
    "basic",
    "algebraic",
    "function",
    "x",
    "dependent",
    "variable",
    "target",
    "class",
    "variable",
    "going",
    "predict",
    "independent",
    "variables",
    "x1",
    "way",
    "xn",
    "features",
    "attributes",
    "going",
    "use",
    "predict",
    "target",
    "class",
    "know",
    "linear",
    "regression",
    "looks",
    "like",
    "using",
    "graph",
    "divide",
    "outcome",
    "categories",
    "really",
    "hard",
    "categorize",
    "example",
    "linear",
    "regression",
    "graph",
    "tell",
    "us",
    "increase",
    "number",
    "hours",
    "studied",
    "marks",
    "student",
    "increase",
    "tell",
    "us",
    "whether",
    "student",
    "pass",
    "cases",
    "need",
    "output",
    "categorical",
    "value",
    "use",
    "logistic",
    "regression",
    "going",
    "use",
    "sigmoid",
    "function",
    "see",
    "marks",
    "0",
    "100",
    "number",
    "hours",
    "studied",
    "going",
    "comparing",
    "example",
    "usually",
    "form",
    "line",
    "says",
    "equals",
    "mx",
    "plus",
    "c",
    "use",
    "sigmoid",
    "function",
    "p",
    "equals",
    "1",
    "1",
    "plus",
    "e",
    "minus",
    "generates",
    "sigmoid",
    "curve",
    "see",
    "right",
    "take",
    "ln",
    "natural",
    "logarithm",
    "always",
    "thought",
    "nl",
    "ln",
    "inverse",
    "e",
    "e",
    "minus",
    "get",
    "ln",
    "p",
    "one",
    "minus",
    "p",
    "equals",
    "times",
    "x",
    "plus",
    "c",
    "sigmoid",
    "curve",
    "function",
    "looking",
    "zoom",
    "function",
    "see",
    "function",
    "derives",
    "goes",
    "1",
    "0",
    "depending",
    "x",
    "value",
    "probability",
    "greater",
    "value",
    "automatically",
    "rounded",
    "1",
    "indicating",
    "student",
    "pass",
    "certain",
    "amount",
    "studying",
    "probably",
    "pass",
    "threshold",
    "value",
    "automatically",
    "puts",
    "right",
    "middle",
    "usually",
    "probability",
    "less",
    "value",
    "rendered",
    "zero",
    "indicating",
    "student",
    "fail",
    "studying",
    "hard",
    "probably",
    "going",
    "fail",
    "course",
    "ignoring",
    "outliers",
    "one",
    "student",
    "natural",
    "genius",
    "need",
    "studying",
    "memorize",
    "everything",
    "unfortunately",
    "study",
    "hard",
    "learn",
    "new",
    "stuff",
    "problem",
    "statement",
    "classify",
    "whether",
    "tumor",
    "malignant",
    "benign",
    "actually",
    "one",
    "favorite",
    "data",
    "sets",
    "play",
    "many",
    "features",
    "look",
    "really",
    "hard",
    "understand",
    "ca",
    "look",
    "know",
    "answer",
    "gives",
    "chance",
    "kind",
    "dive",
    "data",
    "looks",
    "like",
    "able",
    "understand",
    "specific",
    "domain",
    "data",
    "also",
    "want",
    "remind",
    "domain",
    "medicine",
    "told",
    "probability",
    "really",
    "good",
    "classified",
    "things",
    "say",
    "90",
    "95",
    "classifying",
    "whether",
    "going",
    "malignant",
    "b9",
    "tumor",
    "guessing",
    "going",
    "go",
    "get",
    "tested",
    "anyways",
    "got",
    "remember",
    "domain",
    "working",
    "would",
    "want",
    "know",
    "going",
    "go",
    "get",
    "biopsy",
    "know",
    "serious",
    "like",
    "nothing",
    "referencing",
    "domain",
    "important",
    "might",
    "help",
    "doctor",
    "know",
    "look",
    "understanding",
    "kind",
    "tumor",
    "might",
    "help",
    "aid",
    "something",
    "missed",
    "let",
    "go",
    "ahead",
    "dive",
    "code",
    "come",
    "back",
    "domain",
    "part",
    "minute",
    "use",
    "case",
    "going",
    "normal",
    "imports",
    "importing",
    "numpy",
    "pandas",
    "seaborn",
    "matplot",
    "library",
    "going",
    "matplot",
    "library",
    "inline",
    "since",
    "going",
    "switch",
    "anaconda",
    "let",
    "go",
    "ahead",
    "flip",
    "get",
    "started",
    "opened",
    "new",
    "window",
    "anaconda",
    "jupiter",
    "notebook",
    "way",
    "jupiter",
    "notebook",
    "use",
    "anaconda",
    "jupiter",
    "notebook",
    "love",
    "interface",
    "tools",
    "anaconda",
    "brings",
    "got",
    "import",
    "numpy",
    "p",
    "numpy",
    "number",
    "array",
    "pandas",
    "pd",
    "going",
    "bring",
    "seaborn",
    "help",
    "us",
    "graphs",
    "sns",
    "many",
    "really",
    "nice",
    "tools",
    "seaborne",
    "matplot",
    "library",
    "plt",
    "course",
    "want",
    "let",
    "know",
    "line",
    "let",
    "go",
    "run",
    "set",
    "going",
    "call",
    "data",
    "data",
    "creative",
    "today",
    "equals",
    "pd",
    "happens",
    "csv",
    "file",
    "use",
    "underscore",
    "csv",
    "happen",
    "name",
    "file",
    "renamed",
    "course",
    "write",
    "comments",
    "youtube",
    "request",
    "dataset",
    "go",
    "simplylearn",
    "website",
    "happy",
    "supply",
    "let",
    "open",
    "data",
    "go",
    "let",
    "see",
    "looks",
    "like",
    "spreadsheet",
    "pop",
    "open",
    "local",
    "spreadsheet",
    "csv",
    "file",
    "comma",
    "separated",
    "variables",
    "id",
    "guess",
    "categorizes",
    "reference",
    "id",
    "test",
    "done",
    "diagnosis",
    "malignant",
    "b",
    "b9",
    "two",
    "different",
    "options",
    "going",
    "try",
    "predict",
    "b",
    "test",
    "like",
    "radius",
    "mean",
    "average",
    "texture",
    "average",
    "perimeter",
    "mean",
    "area",
    "mean",
    "smoothness",
    "know",
    "unless",
    "doctor",
    "field",
    "stuff",
    "mean",
    "guess",
    "concave",
    "means",
    "term",
    "concave",
    "really",
    "would",
    "know",
    "means",
    "measurements",
    "taking",
    "kinds",
    "stuff",
    "like",
    "smooth",
    "uh",
    "symmetry",
    "float",
    "values",
    "let",
    "page",
    "real",
    "quick",
    "see",
    "believe",
    "36",
    "remember",
    "correctly",
    "one",
    "lot",
    "different",
    "values",
    "take",
    "measurements",
    "take",
    "go",
    "take",
    "look",
    "different",
    "growth",
    "tumorous",
    "growth",
    "back",
    "data",
    "put",
    "folder",
    "code",
    "saved",
    "code",
    "folder",
    "obviously",
    "different",
    "location",
    "want",
    "put",
    "full",
    "path",
    "pandas",
    "first",
    "five",
    "lines",
    "data",
    "run",
    "see",
    "pretty",
    "much",
    "looked",
    "id",
    "diagnosis",
    "go",
    "way",
    "across",
    "see",
    "different",
    "columns",
    "coming",
    "across",
    "displayed",
    "nicely",
    "data",
    "exploring",
    "data",
    "seaborn",
    "referenced",
    "sns",
    "makes",
    "easy",
    "go",
    "joint",
    "plot",
    "notice",
    "similar",
    "sitting",
    "top",
    "plot",
    "library",
    "joint",
    "plot",
    "lot",
    "work",
    "us",
    "going",
    "look",
    "first",
    "two",
    "columns",
    "interested",
    "radius",
    "mean",
    "texture",
    "mean",
    "look",
    "two",
    "columns",
    "data",
    "equals",
    "data",
    "tells",
    "two",
    "columns",
    "plotting",
    "going",
    "use",
    "data",
    "pulled",
    "let",
    "run",
    "generates",
    "really",
    "nice",
    "graph",
    "kinds",
    "cool",
    "things",
    "graph",
    "look",
    "mean",
    "texture",
    "mean",
    "radius",
    "mean",
    "obviously",
    "axes",
    "also",
    "see",
    "one",
    "cool",
    "things",
    "also",
    "see",
    "histogram",
    "show",
    "radius",
    "mean",
    "whereas",
    "commons",
    "radius",
    "mean",
    "come",
    "common",
    "texture",
    "looking",
    "tech",
    "growth",
    "average",
    "texture",
    "radius",
    "average",
    "uh",
    "radius",
    "little",
    "confusing",
    "talking",
    "individual",
    "objects",
    "average",
    "also",
    "look",
    "see",
    "histogram",
    "showing",
    "us",
    "median",
    "common",
    "measurement",
    "two",
    "columns",
    "let",
    "dig",
    "little",
    "deeper",
    "seaborn",
    "also",
    "heat",
    "map",
    "familiar",
    "heat",
    "maps",
    "heat",
    "map",
    "means",
    "color",
    "means",
    "heat",
    "map",
    "guess",
    "original",
    "ones",
    "plotting",
    "heat",
    "density",
    "something",
    "ever",
    "since",
    "called",
    "heat",
    "map",
    "going",
    "take",
    "data",
    "get",
    "corresponding",
    "numbers",
    "put",
    "heat",
    "map",
    "simply",
    "data",
    "dot",
    "pandas",
    "expression",
    "let",
    "remember",
    "working",
    "pandas",
    "data",
    "frame",
    "one",
    "cool",
    "tools",
    "pandas",
    "data",
    "pull",
    "information",
    "heat",
    "map",
    "see",
    "looks",
    "like",
    "see",
    "looking",
    "different",
    "features",
    "id",
    "texture",
    "area",
    "compactness",
    "concave",
    "points",
    "look",
    "middle",
    "chart",
    "diagonal",
    "going",
    "upper",
    "left",
    "bottom",
    "right",
    "white",
    "compare",
    "texture",
    "texture",
    "identical",
    "100",
    "percent",
    "case",
    "perfect",
    "one",
    "correspondence",
    "see",
    "look",
    "say",
    "area",
    "right",
    "almost",
    "black",
    "compare",
    "texture",
    "almost",
    "corresponding",
    "data",
    "really",
    "form",
    "linear",
    "graph",
    "something",
    "look",
    "say",
    "connected",
    "scattered",
    "data",
    "really",
    "really",
    "nice",
    "graph",
    "get",
    "quick",
    "look",
    "data",
    "much",
    "change",
    "changes",
    "verifying",
    "get",
    "answer",
    "something",
    "like",
    "start",
    "looking",
    "individual",
    "pieces",
    "might",
    "go",
    "hey",
    "match",
    "according",
    "showing",
    "heat",
    "map",
    "correlate",
    "going",
    "start",
    "asking",
    "well",
    "going",
    "else",
    "coming",
    "show",
    "really",
    "cool",
    "information",
    "see",
    "id",
    "real",
    "one",
    "feature",
    "says",
    "go",
    "across",
    "top",
    "line",
    "lights",
    "one",
    "feature",
    "says",
    "hey",
    "area",
    "certain",
    "size",
    "going",
    "b9",
    "malignant",
    "says",
    "sort",
    "add",
    "big",
    "hint",
    "data",
    "trying",
    "id",
    "whether",
    "malignant",
    "b9",
    "big",
    "hint",
    "us",
    "data",
    "scientists",
    "go",
    "okay",
    "ca",
    "solve",
    "one",
    "feature",
    "going",
    "something",
    "includes",
    "features",
    "many",
    "different",
    "features",
    "come",
    "solution",
    "exploring",
    "data",
    "let",
    "explore",
    "one",
    "area",
    "let",
    "look",
    "data",
    "dot",
    "null",
    "want",
    "check",
    "null",
    "values",
    "data",
    "remember",
    "earlier",
    "tutorial",
    "little",
    "differently",
    "added",
    "stuff",
    "sum",
    "actually",
    "pandas",
    "really",
    "quickly",
    "data",
    "null",
    "summit",
    "going",
    "go",
    "across",
    "columns",
    "run",
    "going",
    "see",
    "columns",
    "come",
    "null",
    "data",
    "hash",
    "last",
    "steps",
    "done",
    "lot",
    "exploration",
    "looked",
    "first",
    "two",
    "columns",
    "seen",
    "plot",
    "seaborn",
    "joint",
    "plot",
    "shows",
    "histogram",
    "data",
    "plotted",
    "xy",
    "coordinates",
    "obviously",
    "detail",
    "different",
    "columns",
    "see",
    "plot",
    "together",
    "took",
    "seaborne",
    "heat",
    "map",
    "sns",
    "dot",
    "heat",
    "map",
    "data",
    "see",
    "right",
    "nice",
    "job",
    "showing",
    "us",
    "bright",
    "spots",
    "stuff",
    "correlates",
    "forms",
    "nice",
    "combination",
    "points",
    "scattering",
    "points",
    "also",
    "see",
    "areas",
    "finally",
    "went",
    "ahead",
    "checked",
    "data",
    "data",
    "null",
    "value",
    "missing",
    "data",
    "important",
    "step",
    "crash",
    "later",
    "forget",
    "step",
    "remind",
    "get",
    "nice",
    "error",
    "code",
    "says",
    "values",
    "okay",
    "big",
    "deal",
    "miss",
    "fun",
    "go",
    "back",
    "huge",
    "process",
    "missed",
    "step",
    "10",
    "steps",
    "later",
    "got",
    "remember",
    "pulling",
    "data",
    "need",
    "go",
    "ahead",
    "pull",
    "x",
    "let",
    "put",
    "set",
    "x",
    "equal",
    "lot",
    "different",
    "options",
    "certainly",
    "could",
    "x",
    "equals",
    "columns",
    "except",
    "first",
    "two",
    "remember",
    "first",
    "two",
    "id",
    "diagnosis",
    "certainly",
    "would",
    "option",
    "going",
    "actually",
    "going",
    "focus",
    "worst",
    "worst",
    "radius",
    "worst",
    "texture",
    "parameter",
    "area",
    "smoothness",
    "compactness",
    "one",
    "reasons",
    "start",
    "dividing",
    "data",
    "looking",
    "information",
    "sometimes",
    "data",
    "data",
    "coming",
    "two",
    "measurements",
    "coming",
    "model",
    "might",
    "overweigh",
    "might",
    "overpower",
    "measurements",
    "measuring",
    "basically",
    "taking",
    "information",
    "twice",
    "little",
    "bit",
    "past",
    "scope",
    "tutorial",
    "want",
    "take",
    "away",
    "though",
    "dividing",
    "data",
    "pieces",
    "team",
    "back",
    "went",
    "ahead",
    "said",
    "hey",
    "let",
    "look",
    "worst",
    "going",
    "create",
    "array",
    "see",
    "array",
    "radius",
    "worst",
    "texture",
    "worse",
    "perimeter",
    "worst",
    "taken",
    "worst",
    "worst",
    "going",
    "put",
    "x",
    "x",
    "still",
    "pandas",
    "data",
    "frame",
    "columns",
    "remember",
    "correctly",
    "going",
    "oops",
    "hold",
    "one",
    "second",
    "x",
    "data",
    "go",
    "x",
    "equals",
    "data",
    "list",
    "different",
    "columns",
    "worst",
    "worst",
    "going",
    "take",
    "answer",
    "stuff",
    "know",
    "remember",
    "correctly",
    "going",
    "looking",
    "diagnosis",
    "care",
    "diagnosed",
    "b9",
    "malignant",
    "since",
    "single",
    "column",
    "diagnosis",
    "oh",
    "forgot",
    "put",
    "brackets",
    "go",
    "okay",
    "diagnosis",
    "also",
    "real",
    "quickly",
    "like",
    "x",
    "dot",
    "head",
    "want",
    "see",
    "looks",
    "like",
    "dot",
    "head",
    "run",
    "see",
    "last",
    "one",
    "forgot",
    "print",
    "see",
    "dot",
    "head",
    "mmm",
    "first",
    "ones",
    "malignant",
    "run",
    "x",
    "dot",
    "head",
    "first",
    "five",
    "values",
    "radius",
    "worst",
    "texture",
    "worse",
    "parameter",
    "worst",
    "area",
    "worst",
    "go",
    "ahead",
    "take",
    "moving",
    "next",
    "step",
    "built",
    "two",
    "data",
    "sets",
    "answer",
    "features",
    "want",
    "look",
    "data",
    "science",
    "important",
    "test",
    "model",
    "splitting",
    "data",
    "sk",
    "learn",
    "model",
    "selection",
    "going",
    "import",
    "train",
    "test",
    "split",
    "going",
    "split",
    "two",
    "groups",
    "many",
    "ways",
    "noticed",
    "one",
    "modern",
    "ways",
    "actually",
    "split",
    "three",
    "groups",
    "model",
    "group",
    "test",
    "groups",
    "kinds",
    "reasons",
    "past",
    "scope",
    "particular",
    "example",
    "necessary",
    "going",
    "split",
    "two",
    "groups",
    "one",
    "train",
    "data",
    "one",
    "test",
    "data",
    "sklearn",
    "dot",
    "model",
    "selection",
    "train",
    "test",
    "split",
    "could",
    "write",
    "quick",
    "code",
    "randomly",
    "divide",
    "data",
    "two",
    "groups",
    "us",
    "nicely",
    "actually",
    "almost",
    "actually",
    "one",
    "statement",
    "going",
    "generate",
    "four",
    "variables",
    "capital",
    "x",
    "train",
    "capital",
    "x",
    "test",
    "training",
    "data",
    "going",
    "use",
    "fit",
    "model",
    "need",
    "something",
    "test",
    "train",
    "going",
    "train",
    "answer",
    "test",
    "stuff",
    "want",
    "see",
    "good",
    "model",
    "go",
    "ahead",
    "take",
    "train",
    "test",
    "split",
    "imported",
    "going",
    "x",
    "two",
    "different",
    "data",
    "going",
    "split",
    "guys",
    "back",
    "came",
    "wanted",
    "us",
    "go",
    "ahead",
    "use",
    "test",
    "size",
    "equals",
    "test",
    "underscore",
    "size",
    "random",
    "state",
    "always",
    "nice",
    "kind",
    "switch",
    "random",
    "state",
    "around",
    "important",
    "means",
    "test",
    "size",
    "going",
    "take",
    "30",
    "percent",
    "data",
    "going",
    "put",
    "test",
    "variables",
    "test",
    "x",
    "test",
    "going",
    "70",
    "x",
    "train",
    "train",
    "going",
    "use",
    "70",
    "data",
    "train",
    "model",
    "30",
    "test",
    "let",
    "go",
    "ahead",
    "run",
    "load",
    "stuff",
    "split",
    "data",
    "ready",
    "go",
    "get",
    "actual",
    "logistics",
    "part",
    "actually",
    "going",
    "create",
    "model",
    "let",
    "go",
    "ahead",
    "bring",
    "sklearn",
    "going",
    "bring",
    "linear",
    "model",
    "going",
    "import",
    "logistic",
    "regression",
    "actual",
    "model",
    "using",
    "call",
    "log",
    "model",
    "stereo",
    "model",
    "let",
    "set",
    "equal",
    "logistic",
    "regression",
    "imported",
    "variable",
    "log",
    "model",
    "set",
    "class",
    "us",
    "use",
    "models",
    "sk",
    "learn",
    "need",
    "go",
    "ahead",
    "fix",
    "fit",
    "fit",
    "use",
    "x",
    "train",
    "separated",
    "train",
    "let",
    "go",
    "ahead",
    "run",
    "run",
    "model",
    "fits",
    "data",
    "70",
    "percent",
    "training",
    "data",
    "course",
    "prints",
    "tells",
    "us",
    "different",
    "variables",
    "set",
    "lot",
    "different",
    "choices",
    "make",
    "word",
    "though",
    "going",
    "let",
    "defaults",
    "sit",
    "really",
    "need",
    "mess",
    "particular",
    "example",
    "nothing",
    "really",
    "stands",
    "super",
    "important",
    "start",
    "fine",
    "tuning",
    "basics",
    "work",
    "fine",
    "let",
    "need",
    "go",
    "ahead",
    "test",
    "model",
    "working",
    "let",
    "create",
    "variable",
    "predict",
    "going",
    "equal",
    "log",
    "model",
    "want",
    "predict",
    "standard",
    "format",
    "sk",
    "learn",
    "library",
    "taking",
    "model",
    "predict",
    "going",
    "test",
    "predict",
    "test",
    "want",
    "know",
    "model",
    "thinks",
    "going",
    "predict",
    "want",
    "capital",
    "x",
    "x",
    "test",
    "train",
    "set",
    "test",
    "set",
    "going",
    "predict",
    "let",
    "go",
    "ahead",
    "run",
    "print",
    "predict",
    "let",
    "go",
    "ahead",
    "run",
    "see",
    "comes",
    "presents",
    "prints",
    "nice",
    "array",
    "uh",
    "b",
    "b9",
    "malignant",
    "different",
    "test",
    "data",
    "put",
    "pretty",
    "good",
    "sure",
    "exactly",
    "good",
    "see",
    "actually",
    "works",
    "functional",
    "easy",
    "create",
    "always",
    "discover",
    "data",
    "science",
    "explore",
    "spend",
    "significant",
    "amount",
    "time",
    "prepping",
    "data",
    "making",
    "sure",
    "data",
    "coming",
    "good",
    "saying",
    "good",
    "data",
    "good",
    "answers",
    "bad",
    "data",
    "bad",
    "answers",
    "half",
    "thing",
    "half",
    "selecting",
    "models",
    "becomes",
    "next",
    "part",
    "far",
    "good",
    "models",
    "course",
    "depending",
    "model",
    "using",
    "come",
    "want",
    "know",
    "good",
    "came",
    "predict",
    "log",
    "model",
    "dot",
    "predict",
    "x",
    "test",
    "deciding",
    "good",
    "model",
    "going",
    "go",
    "going",
    "import",
    "classification",
    "report",
    "reports",
    "good",
    "model",
    "going",
    "feed",
    "model",
    "data",
    "let",
    "print",
    "take",
    "classification",
    "report",
    "going",
    "put",
    "test",
    "actual",
    "data",
    "actually",
    "know",
    "true",
    "prediction",
    "model",
    "predicted",
    "data",
    "test",
    "side",
    "let",
    "run",
    "see",
    "pull",
    "see",
    "precision",
    "b9",
    "malignant",
    "b",
    "precision",
    "93",
    "91",
    "total",
    "92",
    "kind",
    "average",
    "two",
    "92",
    "kinds",
    "different",
    "information",
    "f1",
    "score",
    "recall",
    "support",
    "coming",
    "go",
    "ahead",
    "flip",
    "back",
    "slides",
    "put",
    "together",
    "describing",
    "going",
    "look",
    "precision",
    "using",
    "classification",
    "report",
    "see",
    "printout",
    "numbers",
    "might",
    "different",
    "randomly",
    "pick",
    "data",
    "using",
    "model",
    "able",
    "predict",
    "type",
    "tumor",
    "91",
    "percent",
    "accuracy",
    "look",
    "back",
    "let",
    "see",
    "uh",
    "b9",
    "england",
    "actually",
    "92",
    "coming",
    "looking",
    "92",
    "91",
    "precision",
    "remember",
    "reminded",
    "domains",
    "talking",
    "domain",
    "medical",
    "domain",
    "catastrophic",
    "outcome",
    "know",
    "91",
    "92",
    "percent",
    "precision",
    "still",
    "going",
    "go",
    "somebody",
    "biopsy",
    "different",
    "investing",
    "money",
    "92",
    "percent",
    "chance",
    "going",
    "earn",
    "10",
    "8",
    "chance",
    "going",
    "lose",
    "8",
    "probably",
    "going",
    "bet",
    "money",
    "odds",
    "pretty",
    "good",
    "make",
    "money",
    "long",
    "run",
    "enough",
    "definitely",
    "make",
    "money",
    "also",
    "domain",
    "actually",
    "seen",
    "use",
    "identify",
    "different",
    "forms",
    "cancer",
    "one",
    "things",
    "starting",
    "use",
    "models",
    "helps",
    "doctor",
    "know",
    "investigate",
    "wraps",
    "section",
    "finally",
    "going",
    "go",
    "let",
    "discuss",
    "answer",
    "quiz",
    "asked",
    "machine",
    "learning",
    "tutorial",
    "part",
    "tell",
    "happening",
    "following",
    "cases",
    "grouping",
    "documents",
    "different",
    "categories",
    "based",
    "topic",
    "content",
    "document",
    "example",
    "clustering",
    "clustering",
    "used",
    "group",
    "documents",
    "topics",
    "using",
    "bag",
    "words",
    "approach",
    "gotten",
    "looking",
    "clustering",
    "hopefully",
    "least",
    "one",
    "two",
    "examples",
    "like",
    "used",
    "clustering",
    "different",
    "things",
    "give",
    "two",
    "thumbs",
    "b",
    "identifying",
    "handwritten",
    "digits",
    "images",
    "correctly",
    "example",
    "classification",
    "traditional",
    "approach",
    "solving",
    "would",
    "extract",
    "digit",
    "dependent",
    "features",
    "like",
    "curvature",
    "different",
    "digits",
    "etc",
    "use",
    "classifier",
    "like",
    "svm",
    "distinguish",
    "images",
    "got",
    "fact",
    "classification",
    "example",
    "give",
    "thumb",
    "able",
    "go",
    "hey",
    "let",
    "use",
    "svm",
    "another",
    "model",
    "give",
    "two",
    "thumbs",
    "c",
    "behavior",
    "website",
    "indicating",
    "site",
    "working",
    "designed",
    "example",
    "anomaly",
    "detection",
    "case",
    "algorithm",
    "learns",
    "normal",
    "normal",
    "usually",
    "observing",
    "logs",
    "website",
    "give",
    "thumbs",
    "got",
    "one",
    "bonus",
    "think",
    "another",
    "example",
    "anomaly",
    "detection",
    "one",
    "ones",
    "use",
    "business",
    "detecting",
    "anomalies",
    "stock",
    "markets",
    "stock",
    "markets",
    "fickled",
    "behave",
    "radical",
    "finding",
    "erratic",
    "areas",
    "finding",
    "ways",
    "track",
    "erratic",
    "something",
    "released",
    "social",
    "media",
    "something",
    "released",
    "see",
    "knowing",
    "anomaly",
    "help",
    "figure",
    "answer",
    "another",
    "area",
    "predicting",
    "salary",
    "individual",
    "based",
    "years",
    "experience",
    "example",
    "regression",
    "problem",
    "mathematically",
    "defined",
    "function",
    "independent",
    "years",
    "experience",
    "dependent",
    "variables",
    "salary",
    "individual",
    "guessed",
    "regression",
    "model",
    "give",
    "thumbs",
    "able",
    "remember",
    "independent",
    "dependent",
    "variables",
    "terms",
    "give",
    "two",
    "thumbs",
    "going",
    "cover",
    "mathematics",
    "machine",
    "learning",
    "today",
    "agenda",
    "going",
    "cover",
    "data",
    "types",
    "going",
    "dive",
    "linear",
    "algebra",
    "concepts",
    "calculus",
    "statistics",
    "machine",
    "learning",
    "probability",
    "machine",
    "learning",
    "demos",
    "course",
    "throwing",
    "middle",
    "going",
    "matrixes",
    "things",
    "go",
    "along",
    "data",
    "types",
    "data",
    "denotes",
    "individual",
    "pieces",
    "factual",
    "information",
    "collected",
    "various",
    "sources",
    "stored",
    "processed",
    "later",
    "used",
    "analysis",
    "see",
    "huge",
    "grouping",
    "information",
    "lot",
    "tech",
    "stuff",
    "money",
    "dollar",
    "signs",
    "numbers",
    "performing",
    "analytics",
    "drive",
    "insights",
    "hopefully",
    "nice",
    "share",
    "shareholders",
    "gather",
    "meeting",
    "able",
    "explain",
    "something",
    "understand",
    "talk",
    "data",
    "types",
    "data",
    "types",
    "data",
    "qualitative",
    "categorical",
    "think",
    "nominal",
    "ordinal",
    "quantitative",
    "numerical",
    "discrete",
    "continuous",
    "let",
    "look",
    "little",
    "closer",
    "data",
    "type",
    "vocabulary",
    "always",
    "people",
    "favorite",
    "vocabulary",
    "words",
    "okay",
    "mine",
    "uh",
    "let",
    "dive",
    "mean",
    "nominal",
    "nominal",
    "used",
    "label",
    "various",
    "uh",
    "label",
    "variables",
    "without",
    "providing",
    "measurable",
    "value",
    "country",
    "gender",
    "race",
    "hair",
    "color",
    "etc",
    "something",
    "either",
    "mark",
    "true",
    "false",
    "label",
    "either",
    "red",
    "hat",
    "lot",
    "times",
    "thinking",
    "nominal",
    "data",
    "labels",
    "think",
    "true",
    "false",
    "kind",
    "setup",
    "look",
    "ordinal",
    "categorical",
    "data",
    "set",
    "order",
    "scale",
    "think",
    "salary",
    "range",
    "great",
    "one",
    "movie",
    "ratings",
    "etc",
    "see",
    "salary",
    "rains",
    "ten",
    "thousand",
    "twenty",
    "thousand",
    "number",
    "employees",
    "earning",
    "rate",
    "hundred",
    "fifty",
    "twenty",
    "thousand",
    "thirty",
    "thousand",
    "hundred",
    "forth",
    "terms",
    "hear",
    "bucket",
    "10",
    "different",
    "buckets",
    "want",
    "separate",
    "something",
    "makes",
    "sense",
    "10",
    "buckets",
    "start",
    "talking",
    "ordinal",
    "lot",
    "times",
    "get",
    "brass",
    "bones",
    "talking",
    "true",
    "false",
    "member",
    "10",
    "20",
    "k",
    "rains",
    "forth",
    "would",
    "either",
    "part",
    "group",
    "talking",
    "buckets",
    "want",
    "count",
    "many",
    "people",
    "bucket",
    "quantitative",
    "numerical",
    "data",
    "falls",
    "two",
    "classes",
    "discrete",
    "continuous",
    "data",
    "final",
    "set",
    "values",
    "categorized",
    "class",
    "strength",
    "questions",
    "answered",
    "correctly",
    "runs",
    "hit",
    "cricket",
    "lot",
    "times",
    "see",
    "think",
    "integer",
    "restricted",
    "integer",
    "100",
    "questions",
    "test",
    "discreet",
    "100",
    "different",
    "values",
    "attain",
    "think",
    "usually",
    "talking",
    "integers",
    "within",
    "small",
    "range",
    "open",
    "end",
    "anything",
    "like",
    "discrete",
    "solid",
    "simple",
    "count",
    "set",
    "number",
    "continuous",
    "hand",
    "continuous",
    "data",
    "take",
    "numerical",
    "value",
    "within",
    "range",
    "water",
    "pressure",
    "weight",
    "person",
    "etc",
    "usually",
    "start",
    "thinking",
    "float",
    "values",
    "get",
    "phenomenally",
    "small",
    "worth",
    "whole",
    "series",
    "values",
    "falls",
    "right",
    "discrete",
    "continuous",
    "think",
    "stock",
    "market",
    "dollar",
    "amounts",
    "still",
    "discrete",
    "starts",
    "get",
    "complicated",
    "enough",
    "like",
    "know",
    "jump",
    "stock",
    "market",
    "cents",
    "cents",
    "lot",
    "point",
    "values",
    "still",
    "called",
    "discrete",
    "start",
    "looking",
    "almost",
    "continuous",
    "variance",
    "talk",
    "went",
    "nominal",
    "ordinal",
    "almost",
    "true",
    "false",
    "charts",
    "looked",
    "quantitative",
    "numerical",
    "data",
    "starting",
    "get",
    "numbers",
    "discrete",
    "usually",
    "lot",
    "times",
    "discrete",
    "put",
    "could",
    "put",
    "true",
    "false",
    "usually",
    "want",
    "address",
    "stuff",
    "first",
    "thing",
    "want",
    "look",
    "basic",
    "algebra",
    "going",
    "take",
    "look",
    "linear",
    "algebra",
    "remember",
    "back",
    "euclidean",
    "geometry",
    "line",
    "well",
    "let",
    "go",
    "linear",
    "algebra",
    "domain",
    "mathematics",
    "concerning",
    "linear",
    "equations",
    "representations",
    "vector",
    "spaces",
    "matrixes",
    "told",
    "going",
    "talk",
    "matrixes",
    "linear",
    "equation",
    "simply",
    "2x",
    "plus",
    "4y",
    "minus",
    "3z",
    "equals",
    "linear",
    "10x",
    "plus",
    "equals",
    "z",
    "actually",
    "solve",
    "two",
    "equations",
    "combining",
    "talking",
    "linear",
    "equation",
    "vectors",
    "plus",
    "b",
    "equals",
    "c",
    "starting",
    "look",
    "direction",
    "values",
    "usually",
    "think",
    "x",
    "z",
    "plot",
    "one",
    "direction",
    "actual",
    "distance",
    "like",
    "triangle",
    "b",
    "c",
    "matrix",
    "describe",
    "kinds",
    "things",
    "find",
    "matrixes",
    "confuse",
    "lot",
    "people",
    "particularly",
    "difficult",
    "magnitude",
    "different",
    "things",
    "used",
    "matrix",
    "chart",
    "know",
    "think",
    "spreadsheet",
    "rows",
    "columns",
    "see",
    "times",
    "b",
    "equals",
    "c",
    "important",
    "know",
    "counts",
    "depending",
    "math",
    "done",
    "using",
    "making",
    "sure",
    "rows",
    "number",
    "columns",
    "single",
    "number",
    "kinds",
    "things",
    "play",
    "make",
    "matrixes",
    "confusing",
    "really",
    "lot",
    "domain",
    "working",
    "adding",
    "multiple",
    "polynomials",
    "like",
    "x",
    "squared",
    "plus",
    "b",
    "plus",
    "know",
    "start",
    "see",
    "confusing",
    "versus",
    "straightforward",
    "matrix",
    "let",
    "go",
    "little",
    "deeper",
    "primary",
    "talk",
    "different",
    "math",
    "uh",
    "mathematical",
    "computations",
    "come",
    "looking",
    "linear",
    "equations",
    "let",
    "dig",
    "deeper",
    "one",
    "equation",
    "maximum",
    "order",
    "one",
    "called",
    "linear",
    "equation",
    "linear",
    "look",
    "ax",
    "plus",
    "b",
    "equals",
    "c",
    "one",
    "variable",
    "two",
    "variable",
    "ax",
    "plus",
    "b",
    "equals",
    "c",
    "x",
    "plus",
    "b",
    "plus",
    "z",
    "c",
    "z",
    "equals",
    "forth",
    "power",
    "one",
    "see",
    "x",
    "squared",
    "see",
    "x",
    "cubed",
    "talking",
    "linear",
    "equations",
    "talking",
    "addition",
    "already",
    "dived",
    "say",
    "neural",
    "networks",
    "recognize",
    "ax",
    "plus",
    "plus",
    "cz",
    "setup",
    "plus",
    "intercept",
    "basically",
    "neural",
    "network",
    "node",
    "adding",
    "different",
    "inputs",
    "drill",
    "common",
    "formula",
    "equals",
    "mx",
    "plus",
    "c",
    "equals",
    "slope",
    "x",
    "value",
    "plus",
    "c",
    "kind",
    "labeled",
    "wrong",
    "threw",
    "loop",
    "c",
    "would",
    "set",
    "x",
    "equal",
    "zero",
    "equals",
    "c",
    "right",
    "uh",
    "reverse",
    "value",
    "x",
    "equals",
    "zero",
    "equals",
    "c",
    "slow",
    "gradient",
    "line",
    "get",
    "equals",
    "2x",
    "plus",
    "lots",
    "easy",
    "ways",
    "compute",
    "way",
    "always",
    "start",
    "basic",
    "one",
    "solving",
    "one",
    "problems",
    "course",
    "one",
    "important",
    "takeaways",
    "slope",
    "gradient",
    "line",
    "slope",
    "important",
    "value",
    "case",
    "went",
    "ahead",
    "solved",
    "equals",
    "2",
    "x",
    "plus",
    "3",
    "see",
    "nice",
    "line",
    "graph",
    "right",
    "matrixes",
    "matrix",
    "refers",
    "rectangular",
    "representation",
    "array",
    "numbers",
    "arranged",
    "columns",
    "rows",
    "talking",
    "rows",
    "n",
    "columns",
    "a11",
    "denotes",
    "element",
    "first",
    "row",
    "first",
    "column",
    "similarly",
    "a12",
    "really",
    "pronounced",
    "a11",
    "particular",
    "setup",
    "row",
    "one",
    "column",
    "one",
    "12",
    "row",
    "one",
    "column",
    "two",
    "first",
    "row",
    "second",
    "column",
    "lot",
    "ways",
    "denote",
    "seen",
    "like",
    "capital",
    "letter",
    "smaller",
    "case",
    "top",
    "row",
    "mean",
    "see",
    "go",
    "kinds",
    "different",
    "directions",
    "far",
    "value",
    "take",
    "moment",
    "realize",
    "need",
    "designation",
    "far",
    "row",
    "column",
    "basic",
    "operations",
    "addition",
    "think",
    "addition",
    "uh",
    "two",
    "matrixes",
    "two",
    "two",
    "add",
    "individual",
    "number",
    "matrix",
    "get",
    "bottom",
    "uh",
    "case",
    "solution",
    "twelve",
    "ten",
    "plus",
    "two",
    "twelve",
    "five",
    "plus",
    "3",
    "8",
    "thing",
    "subtraction",
    "counting",
    "matrix",
    "want",
    "check",
    "dimensions",
    "matrix",
    "shape",
    "see",
    "shape",
    "come",
    "lot",
    "programming",
    "talking",
    "dimensions",
    "talking",
    "shape",
    "two",
    "shapes",
    "equal",
    "happens",
    "add",
    "together",
    "subtract",
    "multiplication",
    "look",
    "multiplication",
    "end",
    "slightly",
    "different",
    "setup",
    "going",
    "look",
    "last",
    "one",
    "like",
    "always",
    "gets",
    "get",
    "matrixes",
    "really",
    "say",
    "multiply",
    "matrixes",
    "know",
    "first",
    "thought",
    "one",
    "times",
    "two",
    "four",
    "times",
    "three",
    "look",
    "get",
    "one",
    "times",
    "two",
    "plus",
    "four",
    "times",
    "three",
    "one",
    "times",
    "three",
    "plus",
    "four",
    "times",
    "five",
    "uh",
    "six",
    "times",
    "two",
    "plus",
    "three",
    "times",
    "three",
    "six",
    "times",
    "three",
    "plus",
    "three",
    "times",
    "looking",
    "matrixes",
    "think",
    "equation",
    "remember",
    "went",
    "back",
    "multiple",
    "line",
    "equations",
    "let",
    "go",
    "back",
    "couple",
    "slides",
    "looking",
    "two",
    "variable",
    "two",
    "variable",
    "equation",
    "ax",
    "plus",
    "b",
    "equals",
    "c",
    "way",
    "make",
    "quick",
    "solve",
    "variables",
    "matrix",
    "multiplication",
    "way",
    "dot",
    "product",
    "uh",
    "one",
    "times",
    "two",
    "plus",
    "four",
    "times",
    "three",
    "one",
    "times",
    "three",
    "plus",
    "four",
    "times",
    "five",
    "uh",
    "six",
    "times",
    "two",
    "plus",
    "three",
    "times",
    "three",
    "six",
    "times",
    "three",
    "plus",
    "three",
    "times",
    "five",
    "gives",
    "us",
    "nice",
    "little",
    "14",
    "23",
    "21",
    "33",
    "used",
    "reduced",
    "sample",
    "formula",
    "far",
    "solving",
    "variables",
    "enough",
    "inputs",
    "uh",
    "matrix",
    "operations",
    "dealing",
    "lot",
    "matrixes",
    "uh",
    "keep",
    "mind",
    "multiplying",
    "matrixes",
    "different",
    "finding",
    "product",
    "two",
    "matrixes",
    "okay",
    "talking",
    "multiplication",
    "talking",
    "solving",
    "uh",
    "equations",
    "finding",
    "product",
    "finding",
    "one",
    "times",
    "two",
    "keep",
    "mind",
    "come",
    "come",
    "number",
    "times",
    "altering",
    "data",
    "get",
    "confused",
    "uh",
    "transpose",
    "flipping",
    "matrix",
    "diagonal",
    "comes",
    "time",
    "still",
    "12",
    "instead",
    "12",
    "8",
    "12",
    "14",
    "8",
    "21",
    "flipping",
    "columns",
    "rows",
    "course",
    "inverse",
    "changing",
    "signs",
    "values",
    "across",
    "main",
    "diagonal",
    "see",
    "inverse",
    "minus",
    "1",
    "ends",
    "instead",
    "12",
    "8",
    "14",
    "12",
    "minus",
    "22",
    "minus",
    "vectors",
    "uh",
    "vector",
    "means",
    "value",
    "direction",
    "four",
    "numbers",
    "vector",
    "mathematics",
    "one",
    "dimensional",
    "matrix",
    "called",
    "vector",
    "uh",
    "x",
    "plot",
    "single",
    "value",
    "values",
    "along",
    "x",
    "axis",
    "single",
    "dimension",
    "two",
    "dimensions",
    "think",
    "putting",
    "graph",
    "might",
    "x",
    "might",
    "value",
    "denotes",
    "direction",
    "course",
    "actual",
    "distance",
    "going",
    "hypothesis",
    "triangle",
    "three",
    "dimensionals",
    "x",
    "z",
    "way",
    "nth",
    "dimensions",
    "talk",
    "k",
    "means",
    "uh",
    "categorizing",
    "close",
    "data",
    "together",
    "compute",
    "based",
    "pythagorean",
    "theorem",
    "would",
    "take",
    "square",
    "value",
    "add",
    "together",
    "find",
    "square",
    "root",
    "gives",
    "distance",
    "far",
    "point",
    "vector",
    "exists",
    "actual",
    "point",
    "value",
    "compare",
    "point",
    "value",
    "another",
    "one",
    "makes",
    "easy",
    "comparison",
    "versus",
    "comparing",
    "50",
    "60",
    "different",
    "numbers",
    "brings",
    "us",
    "gene",
    "vectors",
    "gene",
    "values",
    "hygiene",
    "vectors",
    "vectors",
    "change",
    "span",
    "transformation",
    "gene",
    "values",
    "scalar",
    "values",
    "associated",
    "vectors",
    "conceptually",
    "think",
    "vector",
    "picture",
    "picture",
    "two",
    "dimensions",
    "x",
    "two",
    "dimensions",
    "two",
    "values",
    "whatever",
    "value",
    "point",
    "values",
    "change",
    "skew",
    "take",
    "vector",
    "set",
    "value",
    "b",
    "b",
    "gene",
    "vector",
    "2",
    "gene",
    "value",
    "altering",
    "values",
    "two",
    "means",
    "maybe",
    "stretching",
    "one",
    "direction",
    "making",
    "tall",
    "picture",
    "editing",
    "one",
    "places",
    "comes",
    "see",
    "transforming",
    "uh",
    "different",
    "information",
    "transform",
    "hygiene",
    "value",
    "see",
    "vector",
    "line",
    "transit",
    "transition",
    "uh",
    "3a",
    "hygiene",
    "vector",
    "3",
    "aging",
    "value",
    "change",
    "whatever",
    "started",
    "original",
    "picture",
    "3",
    "skewing",
    "one",
    "direction",
    "maybe",
    "b",
    "skewed",
    "another",
    "direction",
    "nice",
    "tilted",
    "picture",
    "altered",
    "aging",
    "values",
    "let",
    "go",
    "ahead",
    "pull",
    "demo",
    "linear",
    "algebra",
    "going",
    "go",
    "trusted",
    "anaconda",
    "jupiter",
    "notebook",
    "create",
    "new",
    "notebook",
    "called",
    "linear",
    "algebra",
    "since",
    "working",
    "python",
    "going",
    "use",
    "numpy",
    "always",
    "import",
    "np",
    "numpy",
    "array",
    "probably",
    "popular",
    "module",
    "matrixes",
    "things",
    "given",
    "part",
    "series",
    "going",
    "go",
    "much",
    "numpy",
    "going",
    "go",
    "ahead",
    "create",
    "two",
    "different",
    "variables",
    "numpy",
    "array",
    "10",
    "15",
    "b29",
    "go",
    "ahead",
    "run",
    "see",
    "two",
    "arrays",
    "10",
    "15",
    "29",
    "went",
    "added",
    "space",
    "easier",
    "read",
    "since",
    "last",
    "line",
    "put",
    "print",
    "statement",
    "unless",
    "want",
    "simply",
    "simply",
    "plus",
    "b",
    "run",
    "10",
    "15",
    "29",
    "get",
    "30",
    "24",
    "expect",
    "10",
    "plus",
    "20",
    "15",
    "plus",
    "9",
    "could",
    "almost",
    "look",
    "addition",
    "adding",
    "columns",
    "coming",
    "wanted",
    "different",
    "way",
    "could",
    "also",
    "dot",
    "plus",
    "b",
    "dot",
    "remember",
    "flips",
    "get",
    "uh",
    "30",
    "24",
    "going",
    "way",
    "could",
    "also",
    "something",
    "kind",
    "fun",
    "lot",
    "different",
    "ways",
    "far",
    "plus",
    "b",
    "also",
    "plus",
    "b",
    "dot",
    "going",
    "see",
    "come",
    "30",
    "24",
    "whether",
    "transpose",
    "b",
    "transpose",
    "end",
    "likewise",
    "easily",
    "subtract",
    "two",
    "vectors",
    "go",
    "minus",
    "b",
    "run",
    "get",
    "minus",
    "10",
    "remember",
    "last",
    "line",
    "particular",
    "section",
    "right",
    "put",
    "print",
    "around",
    "like",
    "transpose",
    "either",
    "individual",
    "transpose",
    "main",
    "set",
    "get",
    "six",
    "6",
    "going",
    "way",
    "mention",
    "notes",
    "also",
    "scalar",
    "multiplication",
    "put",
    "scalar",
    "remember",
    "uh",
    "talking",
    "array",
    "u",
    "go",
    "times",
    "u",
    "take",
    "value",
    "2",
    "multiply",
    "every",
    "value",
    "2",
    "times",
    "30",
    "60",
    "2",
    "times",
    "15",
    "like",
    "happens",
    "lot",
    "matrixes",
    "need",
    "flip",
    "get",
    "60",
    "30",
    "coming",
    "way",
    "numpy",
    "uh",
    "called",
    "dot",
    "product",
    "uh",
    "two",
    "dimensional",
    "vectors",
    "equivalent",
    "two",
    "matrix",
    "multiplication",
    "remember",
    "talking",
    "matrix",
    "multiplication",
    "uh",
    "well",
    "let",
    "walk",
    "go",
    "ahead",
    "start",
    "defining",
    "two",
    "um",
    "numpy",
    "arrays",
    "uh",
    "10",
    "20",
    "25",
    "6",
    "u",
    "v",
    "going",
    "go",
    "ahead",
    "take",
    "values",
    "remember",
    "correctly",
    "array",
    "like",
    "would",
    "10",
    "times",
    "25",
    "plus",
    "20",
    "times",
    "go",
    "ahead",
    "print",
    "go",
    "go",
    "ahead",
    "np",
    "dot",
    "dot",
    "u",
    "comma",
    "v",
    "find",
    "go",
    "run",
    "going",
    "get",
    "370",
    "strain",
    "multiplication",
    "use",
    "solve",
    "linear",
    "algebra",
    "multiple",
    "numbers",
    "going",
    "across",
    "could",
    "complicated",
    "could",
    "whole",
    "string",
    "different",
    "variables",
    "going",
    "get",
    "nice",
    "value",
    "dot",
    "multiplication",
    "addition",
    "earlier",
    "basic",
    "addition",
    "course",
    "matrix",
    "get",
    "complicated",
    "case",
    "go",
    "ahead",
    "let",
    "create",
    "two",
    "complex",
    "matrixes",
    "one",
    "matrix",
    "12",
    "10",
    "4",
    "6",
    "4",
    "print",
    "see",
    "looks",
    "like",
    "print",
    "print",
    "see",
    "two",
    "three",
    "layer",
    "matrix",
    "also",
    "put",
    "together",
    "always",
    "kind",
    "fun",
    "playing",
    "print",
    "values",
    "could",
    "something",
    "like",
    "could",
    "go",
    "go",
    "could",
    "print",
    "end",
    "equals",
    "run",
    "kind",
    "gives",
    "nice",
    "look",
    "uh",
    "matrix",
    "comma",
    "n",
    "means",
    "tags",
    "end",
    "simply",
    "add",
    "plus",
    "b",
    "already",
    "guess",
    "difference",
    "uh",
    "simple",
    "vector",
    "addition",
    "12",
    "plus",
    "2",
    "14",
    "10",
    "plus",
    "8",
    "18",
    "like",
    "matrix",
    "addition",
    "also",
    "minus",
    "b",
    "matrix",
    "subtraction",
    "look",
    "12",
    "minus",
    "2",
    "10",
    "10",
    "minus",
    "8",
    "oh",
    "go",
    "eight",
    "minus",
    "uh",
    "confusing",
    "looking",
    "reprinted",
    "original",
    "numbers",
    "uh",
    "see",
    "12",
    "minus",
    "2",
    "course",
    "10",
    "10",
    "minus",
    "8",
    "2",
    "4",
    "minus",
    "46",
    "minus",
    "42",
    "forth",
    "subtraction",
    "call",
    "matrix",
    "subtraction",
    "identical",
    "remember",
    "scalar",
    "addition",
    "adding",
    "one",
    "number",
    "matrix",
    "also",
    "scalar",
    "multiplication",
    "simply",
    "single",
    "value",
    "b",
    "array",
    "also",
    "times",
    "b",
    "run",
    "see",
    "2",
    "times",
    "4",
    "8",
    "5",
    "times",
    "4",
    "20",
    "forth",
    "multiplying",
    "4",
    "across",
    "one",
    "values",
    "interesting",
    "one",
    "comes",
    "little",
    "bit",
    "brain",
    "teaser",
    "uh",
    "matrix",
    "vector",
    "multiplication",
    "looking",
    "uh",
    "regular",
    "arrays",
    "necessarily",
    "numpy",
    "array",
    "array",
    "arrays",
    "b",
    "single",
    "array",
    "dot",
    "b",
    "going",
    "return",
    "two",
    "values",
    "first",
    "value",
    "could",
    "say",
    "like",
    "uh",
    "array",
    "b",
    "array",
    "first",
    "second",
    "one",
    "splits",
    "matrix",
    "vector",
    "multiplication",
    "mix",
    "match",
    "get",
    "really",
    "complicated",
    "uh",
    "stuff",
    "becomes",
    "common",
    "got",
    "layers",
    "upon",
    "layers",
    "data",
    "end",
    "matrix",
    "set",
    "bolt",
    "vector",
    "matrices",
    "want",
    "multiply",
    "keep",
    "mind",
    "data",
    "science",
    "lot",
    "times",
    "looking",
    "going",
    "behind",
    "scenes",
    "scikit",
    "looking",
    "sk",
    "learn",
    "linear",
    "regression",
    "models",
    "math",
    "hidden",
    "behind",
    "scenes",
    "going",
    "times",
    "might",
    "find",
    "part",
    "manipulate",
    "data",
    "around",
    "fits",
    "right",
    "go",
    "back",
    "run",
    "psi",
    "kit",
    "matrix",
    "vector",
    "multiplication",
    "also",
    "matrix",
    "two",
    "matrix",
    "multiplication",
    "run",
    "two",
    "matrixes",
    "see",
    "complicated",
    "array",
    "course",
    "comes",
    "dot",
    "reiterate",
    "transpose",
    "matrix",
    "dot",
    "create",
    "matrix",
    "transpose",
    "see",
    "flips",
    "5",
    "10",
    "15",
    "20",
    "25",
    "30",
    "5",
    "15",
    "25",
    "10",
    "20",
    "30",
    "rows",
    "columns",
    "certainly",
    "math",
    "comes",
    "lot",
    "also",
    "comes",
    "lot",
    "xy",
    "plotting",
    "put",
    "pi",
    "plot",
    "one",
    "format",
    "looking",
    "pairs",
    "numbers",
    "want",
    "x",
    "know",
    "transpose",
    "important",
    "tool",
    "math",
    "plotting",
    "kinds",
    "things",
    "another",
    "tool",
    "discuss",
    "uh",
    "identity",
    "matrix",
    "uh",
    "one",
    "definition",
    "uh",
    "identity",
    "matrix",
    "um",
    "one",
    "two",
    "comes",
    "one",
    "zero",
    "zero",
    "one",
    "uh",
    "one",
    "zero",
    "zero",
    "zero",
    "one",
    "zero",
    "creates",
    "diagonal",
    "one",
    "identities",
    "could",
    "comparing",
    "different",
    "features",
    "different",
    "features",
    "correlate",
    "course",
    "feature",
    "one",
    "compared",
    "feature",
    "one",
    "always",
    "one",
    "usually",
    "zero",
    "one",
    "depending",
    "well",
    "correlates",
    "talking",
    "identity",
    "matrix",
    "talking",
    "right",
    "create",
    "preset",
    "matrix",
    "might",
    "adjust",
    "numbers",
    "depending",
    "working",
    "domain",
    "another",
    "thing",
    "kind",
    "wrap",
    "hit",
    "complicated",
    "piece",
    "puzzle",
    "inverse",
    "matrix",
    "let",
    "go",
    "ahead",
    "put",
    "um",
    "lengthy",
    "description",
    "let",
    "go",
    "put",
    "description",
    "straight",
    "website",
    "numpy",
    "given",
    "square",
    "matrix",
    "square",
    "matrix",
    "2",
    "1",
    "0",
    "0",
    "1",
    "0",
    "1",
    "2",
    "keep",
    "mind",
    "3",
    "3",
    "square",
    "got",
    "equal",
    "going",
    "return",
    "matrix",
    "inverse",
    "satisfying",
    "dot",
    "inverse",
    "matrix",
    "multiplication",
    "course",
    "equals",
    "dot",
    "yeah",
    "inverse",
    "identity",
    "shape",
    "dot",
    "shape",
    "reshaping",
    "identity",
    "little",
    "complicated",
    "going",
    "array",
    "go",
    "ahead",
    "run",
    "see",
    "end",
    "end",
    "array",
    "minus",
    "forth",
    "2",
    "1",
    "1",
    "going",
    "2",
    "1",
    "0",
    "0",
    "1",
    "0",
    "1",
    "2",
    "getting",
    "little",
    "deep",
    "math",
    "understanding",
    "need",
    "probably",
    "really",
    "really",
    "important",
    "data",
    "science",
    "versus",
    "handwriting",
    "looking",
    "math",
    "handwriting",
    "pieces",
    "need",
    "know",
    "linear",
    "algorithm",
    "inverse",
    "comes",
    "easily",
    "pull",
    "least",
    "remember",
    "look",
    "took",
    "look",
    "algebra",
    "side",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "calculus",
    "side",
    "going",
    "machine",
    "learning",
    "calculus",
    "oh",
    "goodness",
    "differential",
    "equations",
    "got",
    "throw",
    "part",
    "bag",
    "tricks",
    "especially",
    "large",
    "neural",
    "networks",
    "also",
    "comes",
    "many",
    "areas",
    "good",
    "news",
    "already",
    "done",
    "back",
    "end",
    "comes",
    "really",
    "need",
    "understand",
    "data",
    "science",
    "data",
    "analytics",
    "data",
    "analytics",
    "means",
    "digging",
    "deep",
    "actually",
    "solving",
    "math",
    "equations",
    "neural",
    "network",
    "giant",
    "differential",
    "equation",
    "talk",
    "calculus",
    "going",
    "go",
    "ahead",
    "understand",
    "talking",
    "cars",
    "versus",
    "time",
    "speed",
    "helps",
    "calculate",
    "spontaneous",
    "rate",
    "change",
    "suppose",
    "plot",
    "graph",
    "speed",
    "car",
    "respect",
    "time",
    "see",
    "going",
    "highway",
    "probably",
    "merged",
    "highway",
    "accelerate",
    "speed",
    "went",
    "way",
    "uh",
    "stuck",
    "traffic",
    "merged",
    "traffic",
    "traffic",
    "opens",
    "accelerate",
    "speed",
    "limit",
    "uh",
    "maybe",
    "peter",
    "look",
    "speed",
    "versus",
    "time",
    "getting",
    "faster",
    "faster",
    "continually",
    "accelerating",
    "hit",
    "brakes",
    "go",
    "way",
    "rate",
    "change",
    "speed",
    "respect",
    "time",
    "nothing",
    "acceleration",
    "fast",
    "accelerating",
    "acceleration",
    "area",
    "star",
    "point",
    "x",
    "endpoint",
    "delta",
    "x",
    "calculate",
    "simple",
    "x",
    "delta",
    "x",
    "could",
    "put",
    "line",
    "slope",
    "line",
    "acceleration",
    "pretty",
    "easy",
    "linear",
    "algebra",
    "want",
    "know",
    "line",
    "two",
    "points",
    "want",
    "know",
    "across",
    "whole",
    "working",
    "get",
    "calculus",
    "talk",
    "distance",
    "x",
    "delta",
    "x",
    "smallest",
    "possible",
    "near",
    "zero",
    "order",
    "approximate",
    "acceleration",
    "idea",
    "instead",
    "mean",
    "ever",
    "took",
    "basic",
    "calculus",
    "class",
    "would",
    "draw",
    "bars",
    "would",
    "divide",
    "area",
    "let",
    "go",
    "back",
    "screen",
    "divide",
    "area",
    "time",
    "period",
    "maybe",
    "10",
    "sections",
    "use",
    "could",
    "calculate",
    "acceleration",
    "one",
    "10",
    "sections",
    "kind",
    "thing",
    "keep",
    "making",
    "space",
    "smaller",
    "smaller",
    "delta",
    "x",
    "almost",
    "infinitesimally",
    "small",
    "get",
    "function",
    "equals",
    "limit",
    "h",
    "goes",
    "0",
    "function",
    "plus",
    "h",
    "minus",
    "function",
    "h",
    "computing",
    "slope",
    "line",
    "computing",
    "slope",
    "smaller",
    "smaller",
    "smaller",
    "samples",
    "calculus",
    "calculus",
    "integral",
    "see",
    "nice",
    "uh",
    "integral",
    "sign",
    "looks",
    "like",
    "giant",
    "means",
    "taken",
    "small",
    "sampling",
    "talking",
    "calculus",
    "finding",
    "area",
    "slope",
    "main",
    "process",
    "integration",
    "similar",
    "small",
    "intervals",
    "made",
    "smallest",
    "possible",
    "length",
    "x",
    "plus",
    "delta",
    "x",
    "delta",
    "x",
    "approaches",
    "almost",
    "infinitesimally",
    "small",
    "space",
    "helps",
    "find",
    "overall",
    "acceleration",
    "summing",
    "links",
    "together",
    "summing",
    "accelerations",
    "beginning",
    "end",
    "integral",
    "sum",
    "x",
    "times",
    "x",
    "equals",
    "plus",
    "c",
    "basic",
    "calculus",
    "talk",
    "multivariate",
    "calculus",
    "multivariate",
    "calculus",
    "deals",
    "functions",
    "multiple",
    "variables",
    "see",
    "start",
    "getting",
    "complicated",
    "equations",
    "change",
    "w",
    "change",
    "time",
    "equals",
    "change",
    "w",
    "change",
    "z",
    "differential",
    "z",
    "dx",
    "differential",
    "x",
    "dt",
    "gets",
    "pretty",
    "complicated",
    "uh",
    "really",
    "translates",
    "multivariate",
    "integration",
    "using",
    "double",
    "integrals",
    "sum",
    "sum",
    "f",
    "x",
    "equals",
    "sum",
    "c",
    "b",
    "f",
    "x",
    "x",
    "equals",
    "uh",
    "sum",
    "b",
    "sum",
    "c",
    "f",
    "x",
    "x",
    "understanding",
    "specifics",
    "everything",
    "going",
    "actually",
    "math",
    "usually",
    "calculus",
    "1",
    "calculus",
    "2",
    "differential",
    "equations",
    "talking",
    "three",
    "courses",
    "dig",
    "solve",
    "math",
    "equations",
    "want",
    "take",
    "talking",
    "calculus",
    "uh",
    "talking",
    "summing",
    "different",
    "slopes",
    "still",
    "solving",
    "linear",
    "expression",
    "still",
    "solving",
    "equals",
    "x",
    "plus",
    "b",
    "infinitesimally",
    "small",
    "x",
    "want",
    "sum",
    "integral",
    "sign",
    "means",
    "sum",
    "x",
    "x",
    "equals",
    "plus",
    "c",
    "see",
    "complicated",
    "multivariate",
    "differentiation",
    "using",
    "chain",
    "rule",
    "come",
    "change",
    "w",
    "change",
    "equals",
    "change",
    "w",
    "dz",
    "forth",
    "going",
    "means",
    "basically",
    "looking",
    "area",
    "curve",
    "really",
    "comes",
    "change",
    "changing",
    "speeds",
    "going",
    "changing",
    "end",
    "multiple",
    "layer",
    "three",
    "layers",
    "neural",
    "networks",
    "third",
    "layer",
    "changing",
    "based",
    "second",
    "layer",
    "changing",
    "based",
    "first",
    "layer",
    "changing",
    "get",
    "picture",
    "complicated",
    "multivariate",
    "integration",
    "integrals",
    "good",
    "news",
    "solve",
    "mathematically",
    "neural",
    "networks",
    "reverse",
    "propagation",
    "nice",
    "thing",
    "solve",
    "paper",
    "unless",
    "data",
    "analysis",
    "working",
    "back",
    "end",
    "integrating",
    "formulas",
    "building",
    "script",
    "actually",
    "build",
    "talk",
    "applications",
    "calculus",
    "provides",
    "us",
    "tools",
    "build",
    "accurate",
    "predictive",
    "model",
    "really",
    "behind",
    "scenes",
    "want",
    "guess",
    "change",
    "change",
    "change",
    "little",
    "goofy",
    "know",
    "threw",
    "kind",
    "meta",
    "term",
    "guess",
    "things",
    "going",
    "change",
    "guess",
    "new",
    "numbers",
    "multivariate",
    "calculus",
    "explains",
    "change",
    "target",
    "variable",
    "relation",
    "rate",
    "change",
    "input",
    "variables",
    "multiple",
    "variables",
    "going",
    "one",
    "variable",
    "changing",
    "affect",
    "variable",
    "gradient",
    "descent",
    "calculus",
    "used",
    "find",
    "local",
    "global",
    "maxima",
    "really",
    "big",
    "uh",
    "actually",
    "going",
    "whole",
    "section",
    "gradient",
    "descent",
    "really",
    "mean",
    "talked",
    "neural",
    "networks",
    "see",
    "different",
    "layers",
    "go",
    "gradient",
    "descent",
    "one",
    "key",
    "things",
    "trying",
    "guess",
    "best",
    "answer",
    "something",
    "let",
    "take",
    "look",
    "code",
    "behind",
    "gradient",
    "descent",
    "open",
    "code",
    "let",
    "real",
    "quick",
    "gradient",
    "descent",
    "let",
    "say",
    "curve",
    "like",
    "common",
    "going",
    "represent",
    "error",
    "oops",
    "error",
    "go",
    "error",
    "ah",
    "hard",
    "read",
    "want",
    "make",
    "error",
    "low",
    "possible",
    "looking",
    "want",
    "find",
    "line",
    "minimum",
    "value",
    "looking",
    "minimum",
    "uh",
    "sampling",
    "based",
    "guesses",
    "might",
    "someplace",
    "goes",
    "hey",
    "still",
    "going",
    "goes",
    "goes",
    "back",
    "goes",
    "little",
    "bit",
    "closer",
    "playing",
    "high",
    "low",
    "gets",
    "spot",
    "bottom",
    "spot",
    "want",
    "minimize",
    "error",
    "flip",
    "note",
    "could",
    "also",
    "want",
    "maximizing",
    "something",
    "want",
    "get",
    "best",
    "output",
    "simply",
    "minus",
    "value",
    "looking",
    "peak",
    "negative",
    "valley",
    "looking",
    "valley",
    "way",
    "finding",
    "cool",
    "thing",
    "um",
    "heavy",
    "lifting",
    "done",
    "actually",
    "ended",
    "putting",
    "together",
    "one",
    "back",
    "know",
    "sidekick",
    "starting",
    "boys",
    "long",
    "back",
    "uh",
    "playing",
    "high",
    "low",
    "play",
    "high",
    "low",
    "get",
    "stuck",
    "valleys",
    "uh",
    "figure",
    "curves",
    "things",
    "like",
    "well",
    "back",
    "end",
    "calculus",
    "differential",
    "equations",
    "calculate",
    "good",
    "news",
    "instead",
    "going",
    "put",
    "together",
    "code",
    "let",
    "go",
    "ahead",
    "see",
    "uh",
    "guys",
    "back",
    "put",
    "together",
    "nice",
    "little",
    "piece",
    "code",
    "kind",
    "fun",
    "uh",
    "things",
    "gon",
    "na",
    "note",
    "really",
    "important",
    "stuff",
    "start",
    "data",
    "science",
    "digging",
    "machine",
    "learning",
    "models",
    "going",
    "find",
    "things",
    "stumbling",
    "blocks",
    "first",
    "one",
    "current",
    "x",
    "start",
    "keep",
    "mind",
    "model",
    "working",
    "generic",
    "whatever",
    "use",
    "minimize",
    "first",
    "question",
    "start",
    "started",
    "algorithm",
    "starts",
    "x",
    "equals",
    "three",
    "arbitrarily",
    "picked",
    "five",
    "learning",
    "rate",
    "uh",
    "many",
    "bars",
    "skip",
    "going",
    "one",
    "way",
    "fact",
    "going",
    "separate",
    "little",
    "bit",
    "two",
    "really",
    "important",
    "dealing",
    "something",
    "like",
    "talking",
    "well",
    "function",
    "going",
    "use",
    "gradient",
    "function",
    "2",
    "times",
    "x",
    "plus",
    "5",
    "keep",
    "simple",
    "function",
    "going",
    "work",
    "dealing",
    "increments",
    "thousand",
    "point",
    "one",
    "going",
    "long",
    "time",
    "dealing",
    "increments",
    "going",
    "skip",
    "answer",
    "wo",
    "get",
    "good",
    "answer",
    "look",
    "precision",
    "tells",
    "us",
    "stop",
    "algorithm",
    "specific",
    "working",
    "uh",
    "working",
    "money",
    "convert",
    "float",
    "value",
    "uh",
    "might",
    "dealing",
    "penny",
    "might",
    "precision",
    "working",
    "course",
    "previous",
    "step",
    "size",
    "max",
    "iterations",
    "want",
    "something",
    "cut",
    "certain",
    "point",
    "usually",
    "built",
    "lot",
    "minimization",
    "functions",
    "actual",
    "formula",
    "going",
    "working",
    "come",
    "go",
    "previous",
    "step",
    "size",
    "greater",
    "precision",
    "less",
    "max",
    "max",
    "iters",
    "say",
    "10",
    "times",
    "fast",
    "saying",
    "uh",
    "still",
    "greater",
    "precision",
    "level",
    "still",
    "got",
    "keep",
    "digging",
    "deeper",
    "um",
    "also",
    "want",
    "go",
    "past",
    "thou",
    "whatever",
    "million",
    "ten",
    "thousand",
    "uh",
    "running",
    "actually",
    "pretty",
    "high",
    "almost",
    "never",
    "max",
    "iterations",
    "like",
    "100",
    "200",
    "rare",
    "occasions",
    "might",
    "go",
    "four",
    "500",
    "depending",
    "problem",
    "working",
    "uh",
    "previous",
    "equals",
    "current",
    "way",
    "track",
    "timewise",
    "current",
    "equals",
    "current",
    "minus",
    "rate",
    "times",
    "formula",
    "previous",
    "x",
    "generated",
    "new",
    "version",
    "previous",
    "step",
    "size",
    "equals",
    "absolute",
    "current",
    "previous",
    "looking",
    "change",
    "x",
    "errors",
    "equals",
    "iterations",
    "plus",
    "one",
    "know",
    "stop",
    "get",
    "far",
    "going",
    "print",
    "local",
    "minimum",
    "occurs",
    "x",
    "go",
    "ahead",
    "run",
    "see",
    "right",
    "gets",
    "point",
    "says",
    "hey",
    "local",
    "minimum",
    "minus",
    "particular",
    "series",
    "created",
    "created",
    "formula",
    "lambda",
    "x",
    "two",
    "times",
    "x",
    "plus",
    "five",
    "running",
    "stuff",
    "uh",
    "see",
    "come",
    "lot",
    "uh",
    "sk",
    "learn",
    "kit",
    "one",
    "nice",
    "reasons",
    "breaking",
    "way",
    "could",
    "go",
    "top",
    "pieces",
    "uh",
    "top",
    "pieces",
    "everything",
    "start",
    "looking",
    "minimization",
    "tool",
    "kits",
    "code",
    "um",
    "actually",
    "docs",
    "dot",
    "looking",
    "scikit",
    "go",
    "optimize",
    "minimize",
    "minimize",
    "one",
    "value",
    "function",
    "going",
    "function",
    "complicated",
    "used",
    "simple",
    "function",
    "could",
    "kinds",
    "things",
    "could",
    "number",
    "methods",
    "solve",
    "far",
    "shrink",
    "x",
    "naught",
    "start",
    "value",
    "function",
    "start",
    "value",
    "um",
    "kinds",
    "things",
    "come",
    "look",
    "going",
    "optimization",
    "automatically",
    "creates",
    "constraints",
    "bounds",
    "automatically",
    "really",
    "big",
    "thing",
    "want",
    "point",
    "need",
    "starting",
    "point",
    "want",
    "start",
    "something",
    "already",
    "know",
    "mostly",
    "answer",
    "going",
    "heck",
    "time",
    "trying",
    "calculate",
    "write",
    "little",
    "script",
    "high",
    "low",
    "guessing",
    "tries",
    "find",
    "max",
    "value",
    "brings",
    "us",
    "statistics",
    "kind",
    "figuring",
    "things",
    "lot",
    "vocabulary",
    "statistics",
    "ah",
    "statistics",
    "well",
    "guess",
    "relative",
    "definitely",
    "edel",
    "class",
    "bunch",
    "stuff",
    "going",
    "statistics",
    "statistics",
    "concerns",
    "collection",
    "organization",
    "analysis",
    "interpretation",
    "presentation",
    "data",
    "mouthful",
    "end",
    "end",
    "come",
    "valid",
    "mean",
    "organize",
    "um",
    "analyze",
    "got",
    "ta",
    "take",
    "analysis",
    "interpret",
    "something",
    "people",
    "use",
    "kind",
    "reduce",
    "understandable",
    "nowadays",
    "able",
    "present",
    "ca",
    "present",
    "one",
    "else",
    "going",
    "understand",
    "heck",
    "look",
    "terminologies",
    "lot",
    "terminologies",
    "depending",
    "domain",
    "working",
    "clearly",
    "working",
    "domain",
    "deals",
    "viruses",
    "cells",
    "know",
    "come",
    "studying",
    "different",
    "people",
    "population",
    "working",
    "mechanical",
    "gear",
    "know",
    "little",
    "bit",
    "different",
    "looking",
    "wobbling",
    "statistics",
    "uh",
    "know",
    "replace",
    "rotor",
    "machine",
    "something",
    "like",
    "big",
    "deal",
    "know",
    "huge",
    "fans",
    "turn",
    "sewage",
    "processing",
    "systems",
    "fans",
    "start",
    "wobble",
    "hum",
    "different",
    "things",
    "sensors",
    "pick",
    "one",
    "point",
    "replace",
    "instead",
    "waiting",
    "break",
    "case",
    "costs",
    "lot",
    "money",
    "instead",
    "replacing",
    "bushing",
    "replacing",
    "whole",
    "fan",
    "unit",
    "interesting",
    "project",
    "came",
    "city",
    "back",
    "population",
    "objects",
    "measurements",
    "whose",
    "properties",
    "observed",
    "population",
    "objects",
    "easy",
    "see",
    "people",
    "population",
    "large",
    "case",
    "sewer",
    "fans",
    "talking",
    "fan",
    "units",
    "population",
    "fans",
    "working",
    "parameter",
    "matrix",
    "used",
    "represent",
    "population",
    "characteristic",
    "sample",
    "subset",
    "population",
    "studied",
    "want",
    "come",
    "conclusion",
    "everyone",
    "way",
    "testing",
    "take",
    "sample",
    "sometimes",
    "choice",
    "take",
    "sample",
    "going",
    "ca",
    "study",
    "whole",
    "population",
    "variable",
    "metric",
    "interest",
    "person",
    "object",
    "population",
    "types",
    "sampling",
    "probabilistic",
    "approach",
    "selecting",
    "samples",
    "larger",
    "population",
    "using",
    "method",
    "based",
    "theory",
    "probability",
    "go",
    "little",
    "bit",
    "deeper",
    "random",
    "systematic",
    "stratified",
    "approach",
    "selecting",
    "samples",
    "based",
    "subjective",
    "judgment",
    "researcher",
    "rather",
    "random",
    "selection",
    "convenience",
    "trying",
    "reach",
    "quota",
    "snowball",
    "biased",
    "one",
    "reasons",
    "see",
    "big",
    "stamp",
    "says",
    "biased",
    "got",
    "ta",
    "careful",
    "probabilistic",
    "sampling",
    "uh",
    "talk",
    "random",
    "sampling",
    "select",
    "random",
    "size",
    "samples",
    "group",
    "category",
    "random",
    "get",
    "talk",
    "systematic",
    "sampling",
    "selecting",
    "random",
    "size",
    "samples",
    "group",
    "category",
    "fixed",
    "periodic",
    "interval",
    "uh",
    "kind",
    "split",
    "would",
    "like",
    "time",
    "set",
    "different",
    "categories",
    "might",
    "ask",
    "question",
    "category",
    "group",
    "look",
    "going",
    "go",
    "back",
    "window",
    "let",
    "say",
    "studying",
    "economics",
    "different",
    "area",
    "know",
    "pretty",
    "much",
    "based",
    "culture",
    "came",
    "might",
    "need",
    "separated",
    "uh",
    "say",
    "separated",
    "mean",
    "separated",
    "place",
    "live",
    "mean",
    "far",
    "analysis",
    "want",
    "look",
    "different",
    "groups",
    "make",
    "sure",
    "represented",
    "like",
    "eighty",
    "percent",
    "uh",
    "group",
    "uh",
    "say",
    "hispanic",
    "indian",
    "also",
    "area",
    "twenty",
    "percent",
    "twenty",
    "percent",
    "uh",
    "let",
    "call",
    "expatriates",
    "left",
    "america",
    "nice",
    "caucasian",
    "group",
    "might",
    "want",
    "sample",
    "group",
    "representative",
    "uh",
    "talking",
    "stratified",
    "sampling",
    "talking",
    "groups",
    "groups",
    "talking",
    "brings",
    "us",
    "stratified",
    "sampling",
    "selecting",
    "approximately",
    "equal",
    "size",
    "samples",
    "group",
    "category",
    "way",
    "actually",
    "separate",
    "categories",
    "give",
    "us",
    "insight",
    "different",
    "cultures",
    "might",
    "affect",
    "area",
    "see",
    "different",
    "kind",
    "depends",
    "working",
    "far",
    "data",
    "studying",
    "see",
    "little",
    "bit",
    "selecting",
    "25",
    "employees",
    "company",
    "250",
    "employees",
    "randomly",
    "care",
    "anything",
    "groups",
    "office",
    "nothing",
    "might",
    "selecting",
    "one",
    "employee",
    "every",
    "50",
    "unique",
    "employees",
    "company",
    "250",
    "employees",
    "selecting",
    "one",
    "employee",
    "every",
    "branch",
    "company",
    "office",
    "different",
    "branches",
    "group",
    "categories",
    "branch",
    "category",
    "could",
    "depend",
    "studying",
    "lot",
    "variation",
    "see",
    "kind",
    "grouping",
    "categorizing",
    "also",
    "used",
    "generate",
    "lot",
    "misinformation",
    "study",
    "one",
    "group",
    "say",
    "everybody",
    "assumes",
    "everybody",
    "got",
    "careful",
    "unethical",
    "thing",
    "kind",
    "types",
    "statistics",
    "talk",
    "statistics",
    "going",
    "talk",
    "descriptive",
    "inferential",
    "statistics",
    "many",
    "different",
    "terms",
    "statistics",
    "break",
    "uh",
    "talking",
    "particular",
    "setup",
    "talking",
    "descriptive",
    "inferential",
    "statistics",
    "base",
    "word",
    "describe",
    "pretty",
    "solid",
    "describing",
    "data",
    "look",
    "like",
    "inferential",
    "statistics",
    "going",
    "take",
    "small",
    "population",
    "large",
    "population",
    "working",
    "drug",
    "company",
    "might",
    "look",
    "data",
    "say",
    "people",
    "helped",
    "drug",
    "80",
    "percent",
    "better",
    "far",
    "health",
    "80",
    "percent",
    "better",
    "survival",
    "rate",
    "people",
    "drug",
    "infer",
    "drug",
    "work",
    "greater",
    "populace",
    "help",
    "people",
    "get",
    "inferential",
    "predicting",
    "going",
    "affect",
    "greater",
    "population",
    "descriptive",
    "statistics",
    "used",
    "describe",
    "basic",
    "features",
    "data",
    "form",
    "basis",
    "quantitative",
    "analysis",
    "data",
    "measure",
    "central",
    "tendencies",
    "mean",
    "median",
    "mode",
    "measure",
    "spread",
    "like",
    "range",
    "interquartile",
    "range",
    "variance",
    "standard",
    "deviation",
    "going",
    "look",
    "little",
    "deeper",
    "second",
    "one",
    "think",
    "data",
    "difference",
    "differences",
    "know",
    "max",
    "min",
    "range",
    "stuff",
    "spread",
    "anything",
    "single",
    "number",
    "usually",
    "central",
    "tendencies",
    "measure",
    "central",
    "tendencies",
    "talk",
    "mean",
    "average",
    "set",
    "values",
    "considered",
    "average",
    "outcome",
    "whatever",
    "going",
    "median",
    "separates",
    "higher",
    "half",
    "lower",
    "half",
    "data",
    "center",
    "point",
    "different",
    "data",
    "points",
    "mean",
    "might",
    "couple",
    "really",
    "big",
    "numbers",
    "skew",
    "average",
    "much",
    "higher",
    "took",
    "outliers",
    "median",
    "would",
    "separating",
    "high",
    "low",
    "might",
    "give",
    "much",
    "lower",
    "number",
    "might",
    "look",
    "say",
    "oh",
    "odd",
    "average",
    "much",
    "higher",
    "median",
    "well",
    "outliers",
    "much",
    "lower",
    "mode",
    "frequent",
    "appearing",
    "value",
    "really",
    "interesting",
    "studying",
    "economics",
    "people",
    "might",
    "find",
    "common",
    "income",
    "like",
    "000",
    "year",
    "average",
    "closer",
    "80",
    "000",
    "like",
    "wow",
    "difference",
    "well",
    "people",
    "lot",
    "money",
    "skews",
    "way",
    "average",
    "person",
    "making",
    "kind",
    "money",
    "look",
    "median",
    "income",
    "like",
    "well",
    "median",
    "income",
    "little",
    "bit",
    "closer",
    "average",
    "uh",
    "create",
    "interesting",
    "way",
    "looking",
    "data",
    "uh",
    "central",
    "tendencies",
    "single",
    "numbers",
    "look",
    "whole",
    "spread",
    "data",
    "look",
    "measure",
    "central",
    "tendencies",
    "mean",
    "average",
    "marks",
    "students",
    "classroom",
    "mean",
    "sum",
    "marks",
    "students",
    "total",
    "number",
    "students",
    "talked",
    "median",
    "0",
    "10",
    "take",
    "half",
    "numbers",
    "put",
    "one",
    "side",
    "line",
    "half",
    "numbers",
    "side",
    "line",
    "end",
    "five",
    "middle",
    "mode",
    "mark",
    "scored",
    "students",
    "test",
    "simple",
    "case",
    "people",
    "scored",
    "like",
    "82",
    "percent",
    "got",
    "certain",
    "problems",
    "wrong",
    "easy",
    "figure",
    "easy",
    "different",
    "areas",
    "like",
    "like",
    "um",
    "oh",
    "let",
    "go",
    "back",
    "economy",
    "little",
    "bit",
    "difficult",
    "calculate",
    "large",
    "group",
    "scores",
    "makes",
    "30",
    "000",
    "slightly",
    "bigger",
    "group",
    "makes",
    "26",
    "put",
    "mode",
    "uh",
    "certainly",
    "number",
    "ways",
    "calculate",
    "actually",
    "different",
    "variations",
    "depending",
    "looking",
    "measure",
    "spread",
    "uh",
    "range",
    "difference",
    "highest",
    "lowest",
    "value",
    "first",
    "thing",
    "want",
    "look",
    "know",
    "uh",
    "everybody",
    "test",
    "score",
    "60",
    "100",
    "percent",
    "got",
    "100",
    "maybe",
    "60",
    "90",
    "hard",
    "lot",
    "people",
    "could",
    "get",
    "hundred",
    "percent",
    "um",
    "range",
    "quartiles",
    "divide",
    "rank",
    "ordered",
    "data",
    "set",
    "four",
    "equal",
    "parts",
    "common",
    "thing",
    "part",
    "basic",
    "packages",
    "whether",
    "working",
    "data",
    "frames",
    "pandas",
    "whether",
    "working",
    "scala",
    "whether",
    "working",
    "r",
    "see",
    "come",
    "range",
    "min",
    "max",
    "interquartile",
    "range",
    "look",
    "like",
    "quarter",
    "data",
    "variance",
    "measures",
    "far",
    "number",
    "set",
    "mean",
    "therefore",
    "every",
    "number",
    "set",
    "like",
    "much",
    "turbulence",
    "going",
    "data",
    "standard",
    "deviation",
    "measure",
    "variance",
    "dispersion",
    "set",
    "values",
    "mean",
    "usually",
    "see",
    "graph",
    "might",
    "value",
    "graphed",
    "based",
    "error",
    "might",
    "gra",
    "graph",
    "standard",
    "deviation",
    "error",
    "graph",
    "background",
    "see",
    "far",
    "standard",
    "deviation",
    "used",
    "lot",
    "measurement",
    "spread",
    "uh",
    "marks",
    "student",
    "hundred",
    "50",
    "63",
    "50",
    "range",
    "maximum",
    "marks",
    "minimum",
    "marks",
    "90",
    "45",
    "spread",
    "45",
    "90",
    "minus",
    "interquartile",
    "range",
    "using",
    "marks",
    "see",
    "median",
    "first",
    "quarter",
    "second",
    "quarter",
    "third",
    "quarter",
    "based",
    "splitting",
    "apart",
    "values",
    "understand",
    "variance",
    "standard",
    "deviation",
    "first",
    "need",
    "find",
    "mean",
    "know",
    "calculating",
    "average",
    "end",
    "approximately",
    "66",
    "average",
    "look",
    "variance",
    "know",
    "means",
    "equals",
    "marks",
    "minus",
    "mean",
    "squared",
    "squared",
    "one",
    "want",
    "make",
    "sure",
    "like",
    "putting",
    "stuff",
    "together",
    "end",
    "error",
    "far",
    "one",
    "negative",
    "one",
    "positive",
    "one",
    "little",
    "higher",
    "one",
    "little",
    "lower",
    "always",
    "see",
    "squared",
    "value",
    "total",
    "observations",
    "standard",
    "deviation",
    "equals",
    "square",
    "root",
    "variance",
    "approximately",
    "looking",
    "predictable",
    "model",
    "would",
    "looking",
    "deviation",
    "based",
    "error",
    "much",
    "error",
    "really",
    "important",
    "know",
    "prediction",
    "predicting",
    "something",
    "chance",
    "way",
    "little",
    "bit",
    "looked",
    "tools",
    "far",
    "basics",
    "statistics",
    "talking",
    "let",
    "go",
    "ahead",
    "pull",
    "little",
    "demo",
    "show",
    "looks",
    "like",
    "python",
    "code",
    "get",
    "little",
    "hands",
    "let",
    "go",
    "back",
    "jupiter",
    "notebook",
    "python",
    "almost",
    "numpy",
    "last",
    "time",
    "worked",
    "numpy",
    "time",
    "going",
    "go",
    "ahead",
    "use",
    "pandas",
    "remember",
    "pandas",
    "basically",
    "data",
    "frame",
    "rows",
    "columns",
    "let",
    "go",
    "ahead",
    "print",
    "run",
    "see",
    "name",
    "jane",
    "michael",
    "william",
    "rosie",
    "hannah",
    "sat",
    "salaries",
    "course",
    "instead",
    "hand",
    "calculations",
    "add",
    "everything",
    "together",
    "divide",
    "total",
    "something",
    "simple",
    "like",
    "use",
    "command",
    "mean",
    "pandas",
    "go",
    "ahead",
    "print",
    "df",
    "pick",
    "column",
    "salary",
    "want",
    "find",
    "means",
    "calorie",
    "want",
    "find",
    "means",
    "column",
    "go",
    "print",
    "see",
    "average",
    "income",
    "71",
    "let",
    "go",
    "ahead",
    "go",
    "ahead",
    "put",
    "means",
    "going",
    "also",
    "might",
    "want",
    "find",
    "median",
    "median",
    "similar",
    "except",
    "actually",
    "median",
    "used",
    "means",
    "average",
    "kind",
    "interesting",
    "use",
    "two",
    "different",
    "words",
    "uh",
    "computation",
    "slight",
    "differences",
    "part",
    "means",
    "average",
    "uh",
    "median",
    "oops",
    "let",
    "put",
    "median",
    "salary",
    "way",
    "displays",
    "little",
    "better",
    "see",
    "median",
    "54",
    "halfway",
    "mark",
    "significantly",
    "average",
    "somebody",
    "makes",
    "189",
    "darn",
    "rosie",
    "throwing",
    "numbers",
    "something",
    "want",
    "notice",
    "difference",
    "huge",
    "meaning",
    "behind",
    "studying",
    "populist",
    "looking",
    "different",
    "data",
    "coming",
    "course",
    "also",
    "want",
    "find",
    "hey",
    "uh",
    "common",
    "income",
    "people",
    "make",
    "little",
    "tiny",
    "sample",
    "go",
    "ahead",
    "mode",
    "see",
    "mode",
    "uh",
    "50",
    "telling",
    "people",
    "making",
    "50",
    "middle",
    "point",
    "54",
    "half",
    "people",
    "making",
    "tells",
    "common",
    "income",
    "median",
    "scale",
    "lot",
    "high",
    "salaries",
    "going",
    "really",
    "low",
    "salaries",
    "trend",
    "common",
    "statistic",
    "analyzing",
    "economy",
    "different",
    "people",
    "income",
    "pretty",
    "common",
    "bigger",
    "difference",
    "also",
    "important",
    "studying",
    "statistics",
    "hear",
    "someone",
    "say",
    "hey",
    "average",
    "income",
    "might",
    "start",
    "asking",
    "questions",
    "point",
    "talking",
    "median",
    "income",
    "talking",
    "mode",
    "common",
    "income",
    "hiding",
    "uh",
    "analysis",
    "looking",
    "saying",
    "hey",
    "discrepancies",
    "different",
    "course",
    "analysis",
    "important",
    "find",
    "minimum",
    "maximum",
    "go",
    "ahead",
    "simply",
    "dot",
    "min",
    "pull",
    "minimum",
    "dot",
    "max",
    "pulls",
    "maximum",
    "pretty",
    "straightforward",
    "far",
    "um",
    "translating",
    "knowing",
    "know",
    "lowest",
    "value",
    "highest",
    "value",
    "use",
    "generate",
    "like",
    "spread",
    "later",
    "real",
    "quick",
    "mode",
    "uh",
    "note",
    "puts",
    "mode",
    "zero",
    "like",
    "said",
    "couple",
    "different",
    "ways",
    "compute",
    "mode",
    "um",
    "although",
    "standard",
    "one",
    "pretty",
    "good",
    "course",
    "range",
    "max",
    "minus",
    "min",
    "range",
    "149",
    "000",
    "upper",
    "end",
    "lower",
    "end",
    "might",
    "want",
    "looking",
    "individual",
    "values",
    "turns",
    "describe",
    "feature",
    "pandas",
    "pandas",
    "actually",
    "df",
    "salary",
    "describe",
    "see",
    "seven",
    "uh",
    "setups",
    "mean",
    "standard",
    "deviation",
    "compute",
    "yet",
    "would",
    "dot",
    "std",
    "got",
    "ta",
    "little",
    "careful",
    "computes",
    "looks",
    "axes",
    "things",
    "like",
    "minimum",
    "value",
    "quartiles",
    "maximum",
    "value",
    "course",
    "name",
    "salary",
    "basic",
    "statistics",
    "pull",
    "like",
    "describe",
    "dictionary",
    "could",
    "actually",
    "something",
    "like",
    "um",
    "could",
    "actually",
    "go",
    "uh",
    "count",
    "run",
    "prints",
    "count",
    "dictionary",
    "pull",
    "one",
    "values",
    "kind",
    "quick",
    "dirty",
    "way",
    "pull",
    "different",
    "information",
    "split",
    "depending",
    "need",
    "walked",
    "gave",
    "information",
    "meeting",
    "point",
    "would",
    "kind",
    "fall",
    "asleep",
    "would",
    "anyway",
    "um",
    "want",
    "go",
    "ahead",
    "see",
    "graphing",
    "go",
    "ahead",
    "put",
    "histogram",
    "plot",
    "graph",
    "salaries",
    "let",
    "go",
    "ahead",
    "put",
    "map",
    "plot",
    "inline",
    "remember",
    "jupiter",
    "notebook",
    "thing",
    "lot",
    "new",
    "version",
    "matplot",
    "library",
    "automatically",
    "case",
    "always",
    "put",
    "import",
    "map",
    "plot",
    "library",
    "pi",
    "plot",
    "plt",
    "plotting",
    "data",
    "frame",
    "guess",
    "really",
    "need",
    "respell",
    "data",
    "frame",
    "maybe",
    "could",
    "remind",
    "go",
    "ahead",
    "print",
    "df",
    "way",
    "still",
    "salary",
    "df",
    "salary",
    "history",
    "title",
    "salary",
    "distribution",
    "color",
    "gray",
    "plot",
    "ax",
    "v",
    "line",
    "salary",
    "mean",
    "value",
    "going",
    "take",
    "mean",
    "value",
    "color",
    "violet",
    "line",
    "style",
    "dash",
    "making",
    "pretty",
    "uh",
    "color",
    "dashed",
    "line",
    "line",
    "width",
    "two",
    "kind",
    "thing",
    "median",
    "let",
    "go",
    "ahead",
    "run",
    "see",
    "talking",
    "taking",
    "plot",
    "um",
    "data",
    "data",
    "frame",
    "print",
    "see",
    "salaries",
    "look",
    "salary",
    "distribution",
    "look",
    "way",
    "distributed",
    "um",
    "case",
    "let",
    "see",
    "red",
    "median",
    "violet",
    "average",
    "mean",
    "see",
    "really",
    "mean",
    "outlier",
    "person",
    "makes",
    "lot",
    "money",
    "average",
    "median",
    "look",
    "say",
    "wow",
    "based",
    "average",
    "really",
    "tell",
    "much",
    "people",
    "really",
    "taking",
    "home",
    "tell",
    "much",
    "money",
    "know",
    "average",
    "salary",
    "things",
    "want",
    "take",
    "away",
    "addition",
    "easy",
    "plot",
    "axv",
    "line",
    "lines",
    "markers",
    "display",
    "data",
    "mean",
    "add",
    "kinds",
    "things",
    "get",
    "really",
    "complicated",
    "keeping",
    "simple",
    "pretty",
    "straightforward",
    "look",
    "see",
    "major",
    "outlier",
    "definitely",
    "histogram",
    "stuff",
    "like",
    "know",
    "pictures",
    "worth",
    "thousand",
    "words",
    "really",
    "want",
    "make",
    "sure",
    "take",
    "away",
    "basic",
    "describe",
    "pulls",
    "information",
    "print",
    "individual",
    "information",
    "describe",
    "dictionary",
    "want",
    "go",
    "ahead",
    "look",
    "mean",
    "value",
    "also",
    "describe",
    "mean",
    "lot",
    "statistics",
    "uh",
    "able",
    "print",
    "going",
    "print",
    "last",
    "one",
    "happens",
    "mean",
    "uh",
    "easily",
    "reference",
    "one",
    "also",
    "something",
    "little",
    "bit",
    "complicated",
    "need",
    "basics",
    "come",
    "pull",
    "one",
    "individual",
    "references",
    "pandas",
    "chance",
    "describe",
    "data",
    "let",
    "get",
    "inferential",
    "statistics",
    "inferential",
    "statistics",
    "allows",
    "make",
    "predictions",
    "inferences",
    "data",
    "see",
    "nice",
    "little",
    "picture",
    "movie",
    "ratings",
    "um",
    "took",
    "group",
    "people",
    "said",
    "hey",
    "many",
    "people",
    "like",
    "movie",
    "dislike",
    "ca",
    "say",
    "ask",
    "random",
    "person",
    "comes",
    "movie",
    "study",
    "infer",
    "55",
    "chance",
    "saying",
    "liked",
    "35",
    "chance",
    "saying",
    "disliked",
    "10",
    "11",
    "percent",
    "chance",
    "ca",
    "say",
    "real",
    "basics",
    "talking",
    "going",
    "infer",
    "next",
    "person",
    "going",
    "follow",
    "statistics",
    "uh",
    "let",
    "look",
    "point",
    "estimation",
    "uh",
    "process",
    "finding",
    "approximate",
    "value",
    "population",
    "parameter",
    "like",
    "mean",
    "average",
    "random",
    "samples",
    "population",
    "let",
    "take",
    "example",
    "testing",
    "vaccines",
    "covid19",
    "vaccines",
    "flu",
    "bugs",
    "pretty",
    "big",
    "thing",
    "test",
    "make",
    "sure",
    "going",
    "work",
    "populace",
    "group",
    "people",
    "chosen",
    "population",
    "medical",
    "trials",
    "performed",
    "results",
    "generalized",
    "whole",
    "population",
    "protected",
    "small",
    "group",
    "selected",
    "run",
    "medical",
    "trials",
    "results",
    "work",
    "population",
    "nice",
    "diagram",
    "arrows",
    "going",
    "back",
    "forth",
    "scary",
    "coveted",
    "virus",
    "middle",
    "one",
    "let",
    "take",
    "look",
    "applications",
    "inferential",
    "statistics",
    "central",
    "call",
    "hypotheses",
    "testing",
    "confidence",
    "interval",
    "go",
    "get",
    "probability",
    "get",
    "binomial",
    "theorem",
    "normal",
    "distribution",
    "central",
    "limit",
    "theorem",
    "hypothesis",
    "testing",
    "hypothesis",
    "testing",
    "used",
    "measure",
    "plausibility",
    "hypothesis",
    "assumption",
    "using",
    "sample",
    "data",
    "talk",
    "theorems",
    "theory",
    "hypothesis",
    "uh",
    "keep",
    "mind",
    "philosophy",
    "class",
    "theory",
    "hypothesis",
    "theorem",
    "scientific",
    "uh",
    "statement",
    "something",
    "proven",
    "although",
    "always",
    "debate",
    "science",
    "always",
    "want",
    "make",
    "sure",
    "things",
    "debate",
    "hypothesis",
    "philosophical",
    "class",
    "calling",
    "theory",
    "theory",
    "science",
    "theory",
    "science",
    "says",
    "well",
    "proven",
    "gravity",
    "theory",
    "want",
    "debate",
    "theory",
    "gravity",
    "try",
    "jumping",
    "want",
    "theory",
    "economy",
    "collapsing",
    "area",
    "philosophical",
    "debate",
    "important",
    "heard",
    "people",
    "mix",
    "pet",
    "peeve",
    "mine",
    "talk",
    "hypotheses",
    "testing",
    "steps",
    "involved",
    "hypotheses",
    "testing",
    "first",
    "formulate",
    "hypothesis",
    "figure",
    "right",
    "test",
    "test",
    "hypothesis",
    "execute",
    "test",
    "make",
    "decision",
    "talking",
    "hypothesis",
    "usually",
    "trying",
    "disprove",
    "ca",
    "disprove",
    "works",
    "facts",
    "might",
    "call",
    "theorem",
    "point",
    "use",
    "case",
    "let",
    "consider",
    "example",
    "four",
    "students",
    "given",
    "task",
    "clean",
    "room",
    "every",
    "day",
    "sounds",
    "like",
    "working",
    "kids",
    "decided",
    "distribute",
    "job",
    "cleaning",
    "room",
    "among",
    "making",
    "four",
    "chits",
    "names",
    "name",
    "gets",
    "picked",
    "cleaning",
    "day",
    "rob",
    "took",
    "opportunity",
    "make",
    "chits",
    "wrote",
    "everyone",
    "name",
    "four",
    "people",
    "nick",
    "rob",
    "imlia",
    "imlia",
    "summer",
    "rick",
    "emilia",
    "summer",
    "asking",
    "us",
    "decide",
    "whether",
    "rob",
    "done",
    "mischief",
    "preparing",
    "chits",
    "whether",
    "rob",
    "written",
    "name",
    "one",
    "chit",
    "find",
    "probability",
    "rob",
    "getting",
    "cleaning",
    "job",
    "first",
    "day",
    "second",
    "day",
    "third",
    "day",
    "till",
    "12",
    "days",
    "probability",
    "rob",
    "getting",
    "job",
    "decreases",
    "every",
    "day",
    "turn",
    "never",
    "comes",
    "definitely",
    "done",
    "mischief",
    "making",
    "chits",
    "probability",
    "rob",
    "work",
    "day",
    "one",
    "three",
    "four",
    "chance",
    "work",
    "2",
    "days",
    "3",
    "4",
    "times",
    "3",
    "4",
    "equals",
    "three",
    "days",
    "equals",
    "get",
    "day",
    "12",
    "less",
    "remember",
    "comes",
    "lot",
    "talking",
    "certain",
    "values",
    "looking",
    "statistics",
    "rob",
    "cheating",
    "chosen",
    "12",
    "consecutive",
    "days",
    "high",
    "probability",
    "day",
    "12",
    "still",
    "gotten",
    "job",
    "cleaning",
    "room",
    "come",
    "important",
    "important",
    "terminologies",
    "null",
    "hypothesis",
    "general",
    "statement",
    "states",
    "relationship",
    "two",
    "measured",
    "phenomenon",
    "association",
    "among",
    "groups",
    "alternative",
    "hypothesis",
    "contrary",
    "null",
    "hypothesis",
    "states",
    "whenever",
    "something",
    "happening",
    "new",
    "theory",
    "preferred",
    "instead",
    "old",
    "one",
    "two",
    "hypotheses",
    "go",
    "hand",
    "hand",
    "uh",
    "null",
    "always",
    "interesting",
    "talking",
    "data",
    "science",
    "math",
    "behind",
    "proving",
    "things",
    "correlation",
    "null",
    "hypothesis",
    "says",
    "two",
    "zero",
    "relation",
    "alternative",
    "hypothesis",
    "says",
    "hey",
    "found",
    "relation",
    "probability",
    "finding",
    "observed",
    "extreme",
    "results",
    "null",
    "hypothesis",
    "study",
    "question",
    "true",
    "value",
    "simply",
    "calculated",
    "difference",
    "represented",
    "units",
    "standard",
    "error",
    "greater",
    "magnitude",
    "greater",
    "evidence",
    "null",
    "hypothesis",
    "look",
    "values",
    "specific",
    "test",
    "p",
    "value",
    "derived",
    "value",
    "looking",
    "call",
    "five",
    "percent",
    "showing",
    "high",
    "correlation",
    "digging",
    "deeper",
    "let",
    "assume",
    "new",
    "drug",
    "developed",
    "goal",
    "lowering",
    "blood",
    "pressure",
    "existing",
    "drug",
    "good",
    "one",
    "null",
    "value",
    "drug",
    "null",
    "value",
    "better",
    "existing",
    "drug",
    "new",
    "drug",
    "lower",
    "blood",
    "pressure",
    "existing",
    "drug",
    "get",
    "says",
    "null",
    "hypothesis",
    "correct",
    "correlation",
    "new",
    "drug",
    "job",
    "alternative",
    "hypothesis",
    "new",
    "drug",
    "significantly",
    "lower",
    "blood",
    "pressure",
    "existing",
    "drug",
    "uh",
    "yay",
    "got",
    "new",
    "drug",
    "alternative",
    "hypothesis",
    "h1",
    "ha",
    "look",
    "p",
    "value",
    "results",
    "evidence",
    "like",
    "medical",
    "trial",
    "showing",
    "positive",
    "results",
    "reject",
    "null",
    "hypothesis",
    "looking",
    "5",
    "percent",
    "value",
    "comparing",
    "positive",
    "test",
    "results",
    "finding",
    "means",
    "different",
    "samples",
    "order",
    "test",
    "hypothesis",
    "specific",
    "test",
    "percentage",
    "increase",
    "leads",
    "us",
    "confidence",
    "intervals",
    "confidence",
    "interval",
    "range",
    "values",
    "sure",
    "true",
    "values",
    "observations",
    "lie",
    "let",
    "say",
    "asked",
    "dog",
    "owner",
    "around",
    "asked",
    "many",
    "cans",
    "food",
    "buy",
    "per",
    "year",
    "dog",
    "calculations",
    "got",
    "know",
    "average",
    "around",
    "95",
    "percent",
    "people",
    "bought",
    "around",
    "200",
    "300",
    "cans",
    "food",
    "hence",
    "say",
    "confidence",
    "interval",
    "2",
    "300",
    "95",
    "percent",
    "values",
    "lie",
    "sprint",
    "data",
    "spread",
    "uh",
    "graph",
    "really",
    "helps",
    "lot",
    "start",
    "seeing",
    "looking",
    "95",
    "percent",
    "peak",
    "case",
    "normal",
    "distribution",
    "nice",
    "bell",
    "curve",
    "equal",
    "sides",
    "asymmetrical",
    "95",
    "percent",
    "values",
    "lie",
    "within",
    "small",
    "range",
    "outliers",
    "percent",
    "going",
    "way",
    "touched",
    "upon",
    "hypothesis",
    "uh",
    "going",
    "move",
    "probability",
    "hypothesis",
    "generated",
    "hypothesis",
    "want",
    "know",
    "probability",
    "something",
    "occurring",
    "probability",
    "measure",
    "likelihood",
    "event",
    "occur",
    "event",
    "predicted",
    "total",
    "certainty",
    "predicted",
    "likelihood",
    "occurrence",
    "event",
    "predicted",
    "total",
    "certainty",
    "predicted",
    "likelihood",
    "occurrence",
    "uh",
    "score",
    "prediction",
    "good",
    "going",
    "whatever",
    "sport",
    "weather",
    "prediction",
    "stock",
    "prediction",
    "studied",
    "physics",
    "chaos",
    "theory",
    "even",
    "location",
    "chair",
    "sitting",
    "probability",
    "might",
    "move",
    "three",
    "feet",
    "granted",
    "probability",
    "one",
    "like",
    "uh",
    "think",
    "calculated",
    "one",
    "trillions",
    "upon",
    "trillions",
    "better",
    "probability",
    "likely",
    "going",
    "happen",
    "things",
    "low",
    "probability",
    "see",
    "talk",
    "random",
    "variable",
    "random",
    "variable",
    "variable",
    "whose",
    "possible",
    "values",
    "numerical",
    "outcomes",
    "random",
    "phenomena",
    "uh",
    "coin",
    "tossed",
    "many",
    "heads",
    "occur",
    "series",
    "20",
    "coin",
    "flips",
    "probably",
    "know",
    "average",
    "10",
    "really",
    "ca",
    "know",
    "random",
    "many",
    "times",
    "red",
    "ball",
    "picked",
    "bag",
    "balls",
    "equal",
    "number",
    "red",
    "balls",
    "blue",
    "balls",
    "green",
    "balls",
    "many",
    "times",
    "sum",
    "digits",
    "two",
    "dice",
    "results",
    "five",
    "know",
    "often",
    "going",
    "roll",
    "two",
    "fives",
    "pair",
    "dives",
    "use",
    "case",
    "uh",
    "let",
    "consider",
    "example",
    "rolling",
    "two",
    "dice",
    "random",
    "variable",
    "outcome",
    "equals",
    "take",
    "values",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "10",
    "11",
    "random",
    "variable",
    "combination",
    "dice",
    "instead",
    "looking",
    "many",
    "times",
    "dice",
    "roll",
    "5",
    "let",
    "go",
    "ahead",
    "look",
    "total",
    "sum",
    "five",
    "far",
    "random",
    "variables",
    "one",
    "four",
    "equals",
    "five",
    "four",
    "one",
    "two",
    "three",
    "three",
    "two",
    "four",
    "roles",
    "four",
    "look",
    "different",
    "options",
    "four",
    "random",
    "rolls",
    "five",
    "look",
    "total",
    "number",
    "happens",
    "36",
    "different",
    "options",
    "uh",
    "see",
    "four",
    "36",
    "chance",
    "every",
    "time",
    "roll",
    "dice",
    "gon",
    "na",
    "roll",
    "total",
    "five",
    "gon",
    "na",
    "outcome",
    "five",
    "uh",
    "look",
    "little",
    "deeper",
    "means",
    "could",
    "think",
    "point",
    "someone",
    "never",
    "rolls",
    "five",
    "always",
    "roll",
    "five",
    "say",
    "hey",
    "person",
    "probably",
    "cheating",
    "look",
    "little",
    "closer",
    "math",
    "behind",
    "let",
    "consider",
    "one",
    "cases",
    "rolling",
    "two",
    "dice",
    "gambling",
    "also",
    "binomial",
    "distribution",
    "probability",
    "getting",
    "success",
    "failure",
    "outcome",
    "experiment",
    "trial",
    "repeated",
    "multiple",
    "times",
    "key",
    "meaning",
    "two",
    "binomial",
    "passing",
    "failing",
    "exam",
    "winning",
    "losing",
    "game",
    "getting",
    "either",
    "head",
    "tails",
    "ever",
    "see",
    "binomial",
    "distribution",
    "based",
    "true",
    "false",
    "kind",
    "setup",
    "win",
    "lose",
    "let",
    "consider",
    "use",
    "case",
    "let",
    "consider",
    "game",
    "football",
    "two",
    "clubs",
    "barcelona",
    "dortmund",
    "teams",
    "play",
    "total",
    "four",
    "matches",
    "find",
    "chances",
    "barcelona",
    "winning",
    "series",
    "look",
    "total",
    "games",
    "looking",
    "five",
    "different",
    "games",
    "matches",
    "let",
    "say",
    "winning",
    "chance",
    "barcelona",
    "75",
    "means",
    "game",
    "75",
    "chance",
    "going",
    "win",
    "game",
    "losing",
    "chances",
    "25",
    "clearly",
    "plus",
    "equals",
    "accounts",
    "100",
    "game",
    "probability",
    "getting",
    "k",
    "wins",
    "matches",
    "calculated",
    "talking",
    "like",
    "five",
    "games",
    "want",
    "know",
    "play",
    "many",
    "wins",
    "five",
    "games",
    "get",
    "percentage",
    "probability",
    "getting",
    "k",
    "wins",
    "matches",
    "calculated",
    "p",
    "x",
    "equals",
    "k",
    "equals",
    "n",
    "c",
    "k",
    "p",
    "k",
    "q",
    "n",
    "minus",
    "k",
    "p",
    "probability",
    "success",
    "q",
    "probability",
    "failure",
    "total",
    "games",
    "n",
    "equals",
    "five",
    "k",
    "equals",
    "zero",
    "one",
    "two",
    "three",
    "four",
    "five",
    "p",
    "chance",
    "winning",
    "point",
    "seven",
    "five",
    "q",
    "chance",
    "losing",
    "equals",
    "one",
    "minus",
    "p",
    "equals",
    "one",
    "minus",
    "point",
    "seven",
    "five",
    "equals",
    "point",
    "two",
    "five",
    "probability",
    "barcelona",
    "lose",
    "matches",
    "plug",
    "numbers",
    "end",
    "small",
    "chance",
    "going",
    "lose",
    "matches",
    "plug",
    "uh",
    "value",
    "two",
    "matches",
    "probability",
    "barcelona",
    "win",
    "least",
    "two",
    "matches",
    "course",
    "go",
    "probability",
    "barcelona",
    "win",
    "three",
    "matches",
    "course",
    "four",
    "matches",
    "always",
    "nice",
    "take",
    "information",
    "let",
    "find",
    "accumulated",
    "discrete",
    "probabilities",
    "outcomes",
    "barcelona",
    "three",
    "matches",
    "x",
    "equals",
    "three",
    "x",
    "equals",
    "4",
    "x",
    "equals",
    "end",
    "p",
    "equals",
    "plus",
    "plus",
    "237",
    "equals",
    "reality",
    "probability",
    "barcelona",
    "winning",
    "series",
    "much",
    "higher",
    "always",
    "nice",
    "put",
    "nice",
    "graph",
    "actually",
    "see",
    "number",
    "wins",
    "probability",
    "pans",
    "binomial",
    "case",
    "continuing",
    "important",
    "terminology",
    "location",
    "location",
    "center",
    "graph",
    "depends",
    "mean",
    "value",
    "important",
    "things",
    "much",
    "data",
    "look",
    "start",
    "looking",
    "probabilities",
    "almost",
    "always",
    "normalized",
    "look",
    "like",
    "graph",
    "middle",
    "left",
    "skewed",
    "data",
    "skewed",
    "left",
    "stuff",
    "happening",
    "left",
    "right",
    "skewed",
    "data",
    "comes",
    "probabilities",
    "come",
    "skewed",
    "really",
    "important",
    "take",
    "closer",
    "look",
    "mostly",
    "end",
    "normalized",
    "set",
    "data",
    "got",
    "also",
    "aware",
    "sometimes",
    "skewed",
    "data",
    "height",
    "height",
    "slope",
    "inversely",
    "depends",
    "upon",
    "standard",
    "deviation",
    "see",
    "standard",
    "deviation",
    "really",
    "large",
    "kind",
    "squishes",
    "standard",
    "deviation",
    "small",
    "data",
    "going",
    "hit",
    "right",
    "middle",
    "nice",
    "peak",
    "aware",
    "might",
    "probability",
    "fits",
    "certain",
    "data",
    "lot",
    "outliers",
    "really",
    "high",
    "standard",
    "deviation",
    "stock",
    "market",
    "analysis",
    "means",
    "predictions",
    "probably",
    "going",
    "make",
    "much",
    "money",
    "small",
    "deviation",
    "might",
    "right",
    "target",
    "set",
    "become",
    "millionaire",
    "leads",
    "us",
    "tells",
    "far",
    "mean",
    "data",
    "point",
    "measured",
    "terms",
    "standard",
    "deviations",
    "mean",
    "around",
    "68",
    "percent",
    "results",
    "found",
    "one",
    "standard",
    "deviation",
    "around",
    "95",
    "percent",
    "results",
    "found",
    "two",
    "standard",
    "deviations",
    "read",
    "symbols",
    "course",
    "love",
    "throw",
    "greek",
    "letters",
    "mu",
    "minus",
    "two",
    "sigma",
    "mu",
    "quick",
    "way",
    "kind",
    "funky",
    "u",
    "means",
    "mean",
    "uh",
    "sigma",
    "standard",
    "deviation",
    "little",
    "arrow",
    "right",
    "little",
    "wagy",
    "tail",
    "going",
    "line",
    "mu",
    "minus",
    "two",
    "sigma",
    "uh",
    "95",
    "percent",
    "results",
    "found",
    "two",
    "standard",
    "deviations",
    "central",
    "limit",
    "theorem",
    "goes",
    "back",
    "skew",
    "remember",
    "looking",
    "skew",
    "values",
    "previous",
    "slide",
    "left",
    "skewed",
    "normalized",
    "right",
    "skewed",
    "talking",
    "skewed",
    "skewed",
    "distribution",
    "sample",
    "means",
    "approximately",
    "normally",
    "distributed",
    "evenly",
    "distributed",
    "skewed",
    "take",
    "large",
    "random",
    "samples",
    "population",
    "mean",
    "mu",
    "standard",
    "deviation",
    "sigma",
    "replacement",
    "see",
    "course",
    "mu",
    "minus",
    "2",
    "sigma",
    "spread",
    "mean",
    "median",
    "mode",
    "talking",
    "large",
    "populations",
    "numbers",
    "come",
    "together",
    "skewed",
    "value",
    "flag",
    "something",
    "wrong",
    "important",
    "aware",
    "going",
    "data",
    "samples",
    "coming",
    "math",
    "behind",
    "going",
    "got",
    "jump",
    "conditional",
    "probability",
    "conditional",
    "probability",
    "event",
    "probability",
    "event",
    "occur",
    "given",
    "knowledge",
    "event",
    "already",
    "occurred",
    "see",
    "bayes",
    "theorem",
    "base",
    "uh",
    "red",
    "mean",
    "funky",
    "looking",
    "little",
    "p",
    "brackets",
    "b",
    "probability",
    "true",
    "b",
    "already",
    "true",
    "probability",
    "b",
    "true",
    "already",
    "true",
    "p",
    "b",
    "probability",
    "true",
    "divided",
    "probability",
    "b",
    "true",
    "talk",
    "bayes",
    "theorem",
    "occurred",
    "back",
    "1800s",
    "discovered",
    "important",
    "formula",
    "really",
    "actually",
    "math",
    "could",
    "kind",
    "x",
    "equals",
    "j",
    "k",
    "divide",
    "going",
    "see",
    "math",
    "works",
    "probabilities",
    "makes",
    "really",
    "nice",
    "set",
    "might",
    "eight",
    "nine",
    "different",
    "studies",
    "going",
    "different",
    "areas",
    "different",
    "people",
    "done",
    "studies",
    "brought",
    "together",
    "look",
    "today",
    "covered",
    "virus",
    "virus",
    "spread",
    "certainly",
    "studies",
    "done",
    "china",
    "versus",
    "studies",
    "way",
    "done",
    "data",
    "different",
    "studies",
    "find",
    "place",
    "overlaps",
    "studying",
    "thing",
    "together",
    "compute",
    "changes",
    "need",
    "make",
    "one",
    "study",
    "make",
    "equal",
    "also",
    "true",
    "study",
    "one",
    "group",
    "want",
    "find",
    "formula",
    "powerful",
    "really",
    "data",
    "collection",
    "part",
    "math",
    "data",
    "science",
    "understanding",
    "data",
    "coming",
    "going",
    "combine",
    "different",
    "studies",
    "different",
    "groups",
    "going",
    "go",
    "use",
    "case",
    "let",
    "find",
    "chance",
    "person",
    "getting",
    "lung",
    "disease",
    "due",
    "smoking",
    "kind",
    "interesting",
    "way",
    "word",
    "let",
    "say",
    "according",
    "medical",
    "report",
    "provided",
    "hospital",
    "states",
    "around",
    "10",
    "percent",
    "patients",
    "treated",
    "suffered",
    "lung",
    "disease",
    "kind",
    "generic",
    "medical",
    "report",
    "found",
    "survey",
    "15",
    "percent",
    "patients",
    "visit",
    "smoke",
    "10",
    "percent",
    "lung",
    "disease",
    "15",
    "patients",
    "smoke",
    "finally",
    "five",
    "percent",
    "people",
    "continued",
    "smoke",
    "even",
    "lung",
    "disease",
    "uh",
    "brightest",
    "choice",
    "um",
    "know",
    "addiction",
    "really",
    "difficult",
    "kick",
    "look",
    "probability",
    "prior",
    "probability",
    "10",
    "people",
    "lung",
    "disease",
    "probability",
    "b",
    "probability",
    "patient",
    "smokes",
    "15",
    "percent",
    "uh",
    "probability",
    "b",
    "b",
    "probability",
    "patient",
    "smokes",
    "even",
    "though",
    "lung",
    "disease",
    "five",
    "percent",
    "probability",
    "b",
    "probability",
    "patient",
    "lung",
    "disease",
    "smoke",
    "put",
    "formulas",
    "together",
    "get",
    "nice",
    "solution",
    "get",
    "probability",
    "b",
    "probability",
    "patient",
    "lung",
    "disease",
    "smoke",
    "plug",
    "numbers",
    "right",
    "get",
    "percent",
    "chance",
    "hence",
    "chance",
    "person",
    "smokes",
    "get",
    "lung",
    "disease",
    "going",
    "pull",
    "little",
    "python",
    "code",
    "always",
    "favorite",
    "roll",
    "sleeves",
    "keep",
    "mind",
    "going",
    "kind",
    "like",
    "back",
    "end",
    "way",
    "see",
    "going",
    "later",
    "going",
    "create",
    "get",
    "another",
    "demo",
    "shows",
    "tools",
    "already",
    "let",
    "start",
    "creating",
    "set",
    "going",
    "create",
    "set",
    "curly",
    "braces",
    "means",
    "set",
    "unique",
    "values",
    "list",
    "tuples",
    "never",
    "change",
    "um",
    "case",
    "set",
    "four",
    "seven",
    "ca",
    "create",
    "four",
    "seven",
    "comma",
    "four",
    "delete",
    "four",
    "outs",
    "unique",
    "values",
    "use",
    "dictionaries",
    "quick",
    "reminder",
    "look",
    "familiar",
    "dictionary",
    "value",
    "value",
    "assigned",
    "key",
    "assigned",
    "value",
    "could",
    "key",
    "value",
    "set",
    "dictionary",
    "like",
    "dictionary",
    "without",
    "value",
    "keys",
    "unique",
    "run",
    "set",
    "four",
    "seven",
    "also",
    "take",
    "list",
    "regular",
    "setup",
    "going",
    "go",
    "ahead",
    "throw",
    "another",
    "number",
    "four",
    "run",
    "see",
    "take",
    "list",
    "one",
    "two",
    "three",
    "four",
    "four",
    "convert",
    "set",
    "set",
    "list",
    "equals",
    "set",
    "list",
    "result",
    "one",
    "two",
    "three",
    "four",
    "deletes",
    "last",
    "four",
    "right",
    "sets",
    "also",
    "go",
    "print",
    "set",
    "set",
    "three",
    "set",
    "three",
    "set",
    "going",
    "logic",
    "function",
    "uh",
    "one",
    "set",
    "six",
    "set",
    "forth",
    "run",
    "get",
    "three",
    "set",
    "true",
    "one",
    "set",
    "false",
    "three",
    "five",
    "seven",
    "another",
    "one",
    "six",
    "set",
    "six",
    "set",
    "set",
    "also",
    "use",
    "list",
    "could",
    "used",
    "three",
    "five",
    "seven",
    "would",
    "response",
    "three",
    "usually",
    "three",
    "three",
    "set",
    "still",
    "works",
    "regular",
    "list",
    "go",
    "ahead",
    "little",
    "iteration",
    "going",
    "kind",
    "dice",
    "one",
    "remember",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "going",
    "bring",
    "iteration",
    "tool",
    "import",
    "product",
    "product",
    "show",
    "means",
    "second",
    "two",
    "dice",
    "dice",
    "going",
    "set",
    "values",
    "one",
    "value",
    "one",
    "put",
    "set",
    "remember",
    "range",
    "seven",
    "going",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "include",
    "seven",
    "thing",
    "dice",
    "b",
    "gon",
    "na",
    "gon",
    "na",
    "create",
    "list",
    "product",
    "b",
    "uh",
    "plus",
    "b",
    "go",
    "ahead",
    "run",
    "print",
    "see",
    "case",
    "say",
    "product",
    "iteration",
    "tool",
    "talking",
    "creating",
    "tuple",
    "two",
    "created",
    "tuple",
    "possible",
    "outcomes",
    "dice",
    "dice",
    "one",
    "two",
    "three",
    "one",
    "six",
    "dice",
    "b",
    "one",
    "six",
    "see",
    "one",
    "one",
    "one",
    "two",
    "one",
    "3",
    "forth",
    "remember",
    "slide",
    "earlier",
    "talked",
    "different",
    "different",
    "outcomes",
    "dice",
    "play",
    "around",
    "little",
    "bit",
    "n",
    "dice",
    "equals",
    "two",
    "dice",
    "faces",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "uh",
    "another",
    "way",
    "create",
    "event",
    "space",
    "set",
    "product",
    "dice",
    "faces",
    "repeat",
    "equals",
    "indice",
    "go",
    "run",
    "see",
    "puts",
    "different",
    "possible",
    "variables",
    "wanted",
    "take",
    "set",
    "print",
    "like",
    "go",
    "outcome",
    "event",
    "space",
    "outcome",
    "equals",
    "event",
    "space",
    "creating",
    "sequence",
    "see",
    "print",
    "stacks",
    "versus",
    "going",
    "putting",
    "nice",
    "line",
    "go",
    "ahead",
    "something",
    "let",
    "go",
    "print",
    "since",
    "end",
    "printing",
    "comma",
    "means",
    "gon",
    "na",
    "gon",
    "na",
    "hit",
    "return",
    "going",
    "next",
    "line",
    "go",
    "ahead",
    "length",
    "event",
    "space",
    "important",
    "variable",
    "going",
    "want",
    "know",
    "minute",
    "course",
    "get",
    "carried",
    "away",
    "typing",
    "length",
    "print",
    "twice",
    "give",
    "error",
    "36",
    "different",
    "possible",
    "variations",
    "might",
    "want",
    "calculate",
    "something",
    "like",
    "multiple",
    "3",
    "want",
    "probability",
    "multiple",
    "3",
    "setup",
    "put",
    "together",
    "code",
    "outcome",
    "event",
    "space",
    "x",
    "equals",
    "outcome",
    "x",
    "plus",
    "remainder",
    "3",
    "going",
    "divide",
    "3",
    "look",
    "remainder",
    "equals",
    "0",
    "favorable",
    "outcome",
    "going",
    "pop",
    "outcome",
    "turn",
    "set",
    "favor",
    "outcome",
    "equals",
    "set",
    "necessary",
    "uh",
    "know",
    "going",
    "repeating",
    "case",
    "go",
    "ahead",
    "want",
    "print",
    "outcome",
    "go",
    "ahead",
    "see",
    "looks",
    "like",
    "see",
    "multiples",
    "three",
    "one",
    "plus",
    "two",
    "three",
    "five",
    "plus",
    "four",
    "nine",
    "divided",
    "three",
    "three",
    "forth",
    "like",
    "looked",
    "length",
    "one",
    "let",
    "go",
    "ahead",
    "print",
    "length",
    "f",
    "outcome",
    "see",
    "looks",
    "like",
    "go",
    "course",
    "forget",
    "add",
    "print",
    "middle",
    "looping",
    "putting",
    "end",
    "setup",
    "going",
    "put",
    "print",
    "run",
    "see",
    "end",
    "36",
    "total",
    "options",
    "12",
    "multiple",
    "add",
    "multiple",
    "easily",
    "conv",
    "compute",
    "probability",
    "simply",
    "taking",
    "length",
    "favorable",
    "outcome",
    "length",
    "event",
    "space",
    "print",
    "let",
    "put",
    "probability",
    "last",
    "line",
    "type",
    "end",
    "chance",
    "roughly",
    "third",
    "want",
    "make",
    "look",
    "nice",
    "let",
    "go",
    "ahead",
    "put",
    "another",
    "line",
    "probability",
    "getting",
    "sum",
    "multiple",
    "three",
    "point",
    "three",
    "three",
    "three",
    "three",
    "compute",
    "thing",
    "five",
    "dice",
    "five",
    "dice",
    "go",
    "ahead",
    "run",
    "yeah",
    "see",
    "huge",
    "amount",
    "choices",
    "goes",
    "look",
    "length",
    "event",
    "space",
    "7",
    "76",
    "choices",
    "lot",
    "choices",
    "want",
    "ask",
    "question",
    "like",
    "uh",
    "sum",
    "sum",
    "multiple",
    "five",
    "multiple",
    "three",
    "go",
    "different",
    "options",
    "see",
    "d1",
    "d2",
    "d3",
    "d4",
    "d5",
    "equals",
    "outcome",
    "add",
    "together",
    "division",
    "5",
    "remainder",
    "0",
    "remainder",
    "also",
    "division",
    "three",
    "equal",
    "zero",
    "multiple",
    "five",
    "equal",
    "zero",
    "multiple",
    "three",
    "append",
    "look",
    "uh",
    "favorable",
    "outcome",
    "go",
    "ahead",
    "set",
    "take",
    "look",
    "length",
    "favorable",
    "outcome",
    "always",
    "good",
    "see",
    "working",
    "904",
    "776",
    "course",
    "simple",
    "division",
    "get",
    "probability",
    "probability",
    "going",
    "roll",
    "multiple",
    "5",
    "add",
    "together",
    "multiple",
    "three",
    "going",
    "divide",
    "two",
    "numbers",
    "see",
    "get",
    "percent",
    "really",
    "nice",
    "visual",
    "really",
    "complicated",
    "math",
    "right",
    "probabilities",
    "many",
    "options",
    "many",
    "possibly",
    "going",
    "able",
    "come",
    "solution",
    "looking",
    "leads",
    "us",
    "confusion",
    "matrix",
    "confusion",
    "matrix",
    "table",
    "used",
    "describe",
    "performance",
    "classification",
    "model",
    "set",
    "test",
    "data",
    "true",
    "values",
    "known",
    "see",
    "left",
    "predicted",
    "actual",
    "negative",
    "uh",
    "false",
    "negative",
    "positive",
    "true",
    "positive",
    "false",
    "positive",
    "true",
    "negative",
    "think",
    "predicted",
    "model",
    "mean",
    "means",
    "divided",
    "data",
    "used",
    "create",
    "model",
    "might",
    "test",
    "actual",
    "case",
    "last",
    "third",
    "see",
    "well",
    "comes",
    "many",
    "times",
    "true",
    "positive",
    "versus",
    "false",
    "positive",
    "gave",
    "false",
    "positive",
    "response",
    "imagine",
    "medical",
    "situations",
    "pretty",
    "big",
    "deal",
    "want",
    "give",
    "false",
    "positive",
    "might",
    "adjust",
    "model",
    "accordingly",
    "false",
    "positive",
    "say",
    "covet",
    "virus",
    "test",
    "better",
    "false",
    "negative",
    "go",
    "back",
    "get",
    "retested",
    "30",
    "percent",
    "false",
    "positives",
    "test",
    "pretty",
    "much",
    "invalid",
    "use",
    "case",
    "like",
    "cancer",
    "prediction",
    "let",
    "consider",
    "example",
    "cancer",
    "prediction",
    "model",
    "put",
    "test",
    "accuracy",
    "precision",
    "actual",
    "result",
    "person",
    "medical",
    "report",
    "compared",
    "prediction",
    "made",
    "machine",
    "learning",
    "model",
    "see",
    "actual",
    "predicted",
    "whether",
    "cancer",
    "know",
    "cancer",
    "big",
    "one",
    "want",
    "false",
    "positive",
    "mean",
    "false",
    "negative",
    "words",
    "want",
    "tell",
    "cancer",
    "would",
    "something",
    "really",
    "looking",
    "particular",
    "domain",
    "want",
    "false",
    "negative",
    "know",
    "created",
    "model",
    "hundreds",
    "people",
    "thousands",
    "pieces",
    "data",
    "come",
    "real",
    "famous",
    "case",
    "study",
    "imagery",
    "measurements",
    "take",
    "36",
    "different",
    "measurements",
    "take",
    "run",
    "basic",
    "model",
    "want",
    "know",
    "accurate",
    "many",
    "negative",
    "results",
    "either",
    "telling",
    "people",
    "cancer",
    "telling",
    "people",
    "cancer",
    "take",
    "numbers",
    "feed",
    "accuracy",
    "precision",
    "recall",
    "accuracy",
    "precision",
    "recall",
    "accuracy",
    "metric",
    "measure",
    "accurately",
    "results",
    "predicted",
    "total",
    "true",
    "got",
    "right",
    "results",
    "add",
    "together",
    "true",
    "positive",
    "true",
    "negative",
    "results",
    "percentage",
    "accurate",
    "versus",
    "wrong",
    "talked",
    "precision",
    "metric",
    "measure",
    "many",
    "correctly",
    "predicted",
    "cases",
    "actually",
    "turned",
    "positive",
    "uh",
    "precision",
    "true",
    "positive",
    "talking",
    "like",
    "covid",
    "testing",
    "viruses",
    "really",
    "want",
    "high",
    "number",
    "want",
    "true",
    "center",
    "point",
    "might",
    "opposite",
    "dealing",
    "cancer",
    "want",
    "false",
    "negatives",
    "metric",
    "precision",
    "test",
    "positive",
    "true",
    "positive",
    "plus",
    "false",
    "positive",
    "recall",
    "many",
    "actual",
    "positive",
    "cases",
    "able",
    "predict",
    "quickly",
    "model",
    "uh",
    "test",
    "positive",
    "test",
    "positive",
    "plus",
    "false",
    "negative",
    "want",
    "go",
    "ahead",
    "demo",
    "naive",
    "bayes",
    "classifier",
    "get",
    "far",
    "uh",
    "naive",
    "bayes",
    "classifier",
    "going",
    "pull",
    "sk",
    "learn",
    "scikit",
    "um",
    "let",
    "go",
    "ahead",
    "kind",
    "interesting",
    "page",
    "classifiers",
    "go",
    "sk",
    "learn",
    "kit",
    "lot",
    "ways",
    "classification",
    "zoom",
    "see",
    "titles",
    "everything",
    "nearest",
    "neighbor",
    "linear",
    "going",
    "focusing",
    "naive",
    "bayes",
    "sample",
    "data",
    "set",
    "put",
    "together",
    "see",
    "different",
    "output",
    "naive",
    "bayes",
    "remember",
    "set",
    "probably",
    "simplified",
    "calculator",
    "set",
    "predictions",
    "talking",
    "true",
    "false",
    "stuff",
    "like",
    "belief",
    "independent",
    "assumption",
    "features",
    "features",
    "assumed",
    "kind",
    "connection",
    "uh",
    "go",
    "ahead",
    "use",
    "prediction",
    "using",
    "naive",
    "bayes",
    "classifier",
    "versus",
    "many",
    "classifiers",
    "going",
    "use",
    "social",
    "network",
    "ads",
    "little",
    "data",
    "set",
    "let",
    "go",
    "open",
    "file",
    "go",
    "user",
    "id",
    "gender",
    "age",
    "estimated",
    "salary",
    "uh",
    "purchased",
    "see",
    "user",
    "id",
    "mail",
    "19",
    "estimated",
    "salary",
    "19",
    "000",
    "purchased",
    "zero",
    "either",
    "going",
    "make",
    "purchase",
    "look",
    "last",
    "one",
    "zero",
    "one",
    "thinking",
    "binomials",
    "thinking",
    "simple",
    "naive",
    "bayes",
    "classifier",
    "kind",
    "set",
    "close",
    "going",
    "go",
    "ahead",
    "import",
    "numpy",
    "np",
    "going",
    "nice",
    "good",
    "visual",
    "data",
    "put",
    "matplot",
    "library",
    "pandas",
    "data",
    "frame",
    "uh",
    "going",
    "go",
    "ahead",
    "import",
    "data",
    "set",
    "data",
    "set",
    "going",
    "going",
    "read",
    "social",
    "network",
    "going",
    "print",
    "head",
    "see",
    "even",
    "though",
    "showed",
    "file",
    "x",
    "equals",
    "data",
    "set",
    "location",
    "uh",
    "two",
    "three",
    "values",
    "going",
    "four",
    "uh",
    "column",
    "four",
    "let",
    "run",
    "little",
    "easier",
    "go",
    "see",
    "right",
    "going",
    "looking",
    "0",
    "1",
    "2",
    "age",
    "estimated",
    "salary",
    "2",
    "3",
    "location",
    "means",
    "looking",
    "number",
    "versus",
    "regular",
    "location",
    "regular",
    "location",
    "actually",
    "say",
    "age",
    "estimated",
    "salary",
    "column",
    "four",
    "make",
    "purchase",
    "purchased",
    "something",
    "three",
    "columns",
    "going",
    "looking",
    "gone",
    "ahead",
    "imported",
    "imported",
    "data",
    "data",
    "set",
    "set",
    "information",
    "need",
    "go",
    "ahead",
    "split",
    "data",
    "need",
    "sk",
    "learn",
    "model",
    "selection",
    "import",
    "train",
    "test",
    "split",
    "nice",
    "job",
    "set",
    "random",
    "state",
    "randomly",
    "picks",
    "data",
    "going",
    "take",
    "uh",
    "25",
    "going",
    "go",
    "test",
    "x",
    "test",
    "test",
    "75",
    "go",
    "x",
    "train",
    "train",
    "way",
    "create",
    "model",
    "data",
    "see",
    "accurate",
    "well",
    "performed",
    "prediction",
    "next",
    "step",
    "data",
    "go",
    "ahead",
    "feature",
    "scaling",
    "lot",
    "start",
    "look",
    "familiar",
    "done",
    "number",
    "modules",
    "setup",
    "start",
    "noticing",
    "bring",
    "data",
    "take",
    "look",
    "working",
    "go",
    "ahead",
    "split",
    "training",
    "testing",
    "case",
    "going",
    "go",
    "ahead",
    "scale",
    "scale",
    "means",
    "putting",
    "value",
    "minus",
    "1",
    "1",
    "place",
    "middle",
    "ground",
    "way",
    "huge",
    "set",
    "huge",
    "setup",
    "go",
    "back",
    "salary",
    "salary",
    "20",
    "000",
    "versus",
    "age",
    "well",
    "good",
    "chance",
    "lot",
    "back",
    "end",
    "math",
    "20",
    "000",
    "skew",
    "results",
    "estimated",
    "salary",
    "higher",
    "impact",
    "age",
    "instead",
    "balancing",
    "letting",
    "calculations",
    "weigh",
    "properly",
    "finally",
    "get",
    "actually",
    "create",
    "naive",
    "bayes",
    "model",
    "going",
    "go",
    "ahead",
    "import",
    "gaussian",
    "naive",
    "bayes",
    "gaussian",
    "basic",
    "one",
    "looking",
    "turns",
    "though",
    "go",
    "sk",
    "learn",
    "kit",
    "number",
    "different",
    "ones",
    "pull",
    "bernoulli",
    "know",
    "never",
    "used",
    "one",
    "categorical",
    "um",
    "complement",
    "gaussian",
    "number",
    "different",
    "options",
    "look",
    "gaussian",
    "come",
    "naive",
    "bayes",
    "commonly",
    "used",
    "talking",
    "naive",
    "bayes",
    "usually",
    "people",
    "talking",
    "pulling",
    "one",
    "nice",
    "things",
    "gaussian",
    "go",
    "website",
    "sk",
    "learn",
    "naive",
    "bayes",
    "gaussian",
    "lot",
    "cool",
    "features",
    "one",
    "partial",
    "fit",
    "means",
    "huge",
    "amount",
    "data",
    "process",
    "want",
    "batch",
    "gaussian",
    "nb",
    "model",
    "many",
    "different",
    "things",
    "far",
    "fitting",
    "data",
    "manipulate",
    "basics",
    "going",
    "go",
    "ahead",
    "create",
    "classifier",
    "going",
    "equal",
    "gaussian",
    "b",
    "going",
    "fit",
    "going",
    "fit",
    "training",
    "data",
    "training",
    "solution",
    "x",
    "train",
    "train",
    "go",
    "ahead",
    "run",
    "uh",
    "going",
    "tell",
    "us",
    "ran",
    "code",
    "right",
    "trained",
    "classifier",
    "model",
    "next",
    "step",
    "need",
    "go",
    "ahead",
    "run",
    "prediction",
    "going",
    "predict",
    "equals",
    "x",
    "test",
    "fit",
    "data",
    "going",
    "go",
    "ahead",
    "predict",
    "get",
    "confusion",
    "matrix",
    "sk",
    "learn",
    "matrix",
    "metrics",
    "import",
    "confusion",
    "matrix",
    "saves",
    "simple",
    "math",
    "go",
    "ahead",
    "create",
    "confusion",
    "metrics",
    "test",
    "predict",
    "actual",
    "predicted",
    "value",
    "see",
    "chart",
    "looked",
    "predicted",
    "true",
    "positive",
    "false",
    "positive",
    "false",
    "negative",
    "true",
    "negative",
    "go",
    "ahead",
    "run",
    "65",
    "3",
    "7",
    "25",
    "particular",
    "prediction",
    "65",
    "predicted",
    "truth",
    "far",
    "purchase",
    "gon",
    "na",
    "make",
    "purchase",
    "guessed",
    "three",
    "wrong",
    "25",
    "predicted",
    "would",
    "purchase",
    "seven",
    "confusion",
    "matrix",
    "point",
    "shareholders",
    "board",
    "meeting",
    "would",
    "start",
    "hear",
    "snoozing",
    "looking",
    "numbers",
    "say",
    "hey",
    "confusion",
    "matrix",
    "let",
    "go",
    "ahead",
    "visualize",
    "results",
    "going",
    "pull",
    "matplot",
    "library",
    "colors",
    "import",
    "listed",
    "color",
    "map",
    "actually",
    "machine",
    "going",
    "throw",
    "error",
    "way",
    "setup",
    "newer",
    "version",
    "put",
    "together",
    "demo",
    "need",
    "x",
    "set",
    "set",
    "x",
    "train",
    "train",
    "create",
    "x1",
    "x2",
    "put",
    "grid",
    "uh",
    "set",
    "x",
    "set",
    "minimum",
    "stop",
    "x",
    "max",
    "stop",
    "come",
    "way",
    "going",
    "step",
    "going",
    "give",
    "us",
    "nice",
    "line",
    "going",
    "plot",
    "contour",
    "plot",
    "x",
    "limit",
    "plot",
    "limit",
    "put",
    "scatter",
    "plot",
    "let",
    "go",
    "ahead",
    "run",
    "uh",
    "honest",
    "graphs",
    "many",
    "different",
    "ways",
    "many",
    "different",
    "ways",
    "put",
    "code",
    "together",
    "show",
    "lot",
    "easier",
    "pull",
    "graph",
    "go",
    "back",
    "explain",
    "first",
    "thing",
    "want",
    "note",
    "looking",
    "data",
    "training",
    "set",
    "make",
    "purchase",
    "drawn",
    "nice",
    "area",
    "defined",
    "naive",
    "bayes",
    "setup",
    "make",
    "purchase",
    "green",
    "see",
    "green",
    "drops",
    "fall",
    "red",
    "area",
    "red",
    "dots",
    "fall",
    "green",
    "even",
    "training",
    "set",
    "going",
    "hundred",
    "percent",
    "could",
    "looking",
    "different",
    "data",
    "coming",
    "kind",
    "arrange",
    "x1",
    "x2",
    "nice",
    "plot",
    "going",
    "going",
    "create",
    "contour",
    "nice",
    "line",
    "drawn",
    "middle",
    "red",
    "green",
    "right",
    "reshape",
    "notice",
    "dot",
    "remember",
    "numpy",
    "numpy",
    "module",
    "end",
    "pairs",
    "know",
    "x",
    "x1",
    "x2",
    "x1",
    "x2",
    "next",
    "row",
    "forth",
    "flip",
    "one",
    "row",
    "x1",
    "x2s",
    "kind",
    "looking",
    "right",
    "setup",
    "uh",
    "scatter",
    "plot",
    "course",
    "um",
    "scattered",
    "data",
    "across",
    "going",
    "points",
    "puts",
    "nice",
    "little",
    "dots",
    "setup",
    "estimated",
    "salary",
    "h",
    "course",
    "dots",
    "make",
    "purchase",
    "quick",
    "note",
    "kind",
    "funny",
    "see",
    "says",
    "x",
    "set",
    "set",
    "equals",
    "x",
    "train",
    "train",
    "seems",
    "kind",
    "little",
    "weird",
    "probably",
    "originally",
    "definition",
    "module",
    "could",
    "called",
    "really",
    "good",
    "way",
    "next",
    "thing",
    "going",
    "want",
    "exact",
    "thing",
    "going",
    "visualize",
    "test",
    "set",
    "results",
    "way",
    "see",
    "happened",
    "test",
    "group",
    "25",
    "percent",
    "see",
    "test",
    "set",
    "uh",
    "look",
    "two",
    "graphs",
    "next",
    "one",
    "obviously",
    "75",
    "percent",
    "data",
    "going",
    "show",
    "lot",
    "25",
    "data",
    "see",
    "number",
    "kind",
    "edge",
    "whether",
    "could",
    "guess",
    "age",
    "income",
    "going",
    "make",
    "purchase",
    "said",
    "still",
    "pretty",
    "clear",
    "pretty",
    "good",
    "far",
    "much",
    "estimate",
    "good",
    "graphs",
    "really",
    "effective",
    "showing",
    "people",
    "going",
    "also",
    "need",
    "numbers",
    "going",
    "sklearn",
    "going",
    "import",
    "metrics",
    "going",
    "print",
    "metrics",
    "classification",
    "port",
    "test",
    "predict",
    "see",
    "precision",
    "precision",
    "0s",
    "90",
    "recall",
    "f1",
    "score",
    "support",
    "precision",
    "recall",
    "getting",
    "right",
    "accuracy",
    "macro",
    "average",
    "weighted",
    "average",
    "see",
    "pulls",
    "pretty",
    "good",
    "far",
    "accurate",
    "could",
    "say",
    "going",
    "90",
    "percent",
    "going",
    "guess",
    "correctly",
    "going",
    "purchase",
    "89",
    "chance",
    "going",
    "purchase",
    "numbers",
    "get",
    "little",
    "bit",
    "different",
    "meaning",
    "pretty",
    "straightforward",
    "accuracy",
    "micro",
    "average",
    "weighted",
    "average",
    "everything",
    "else",
    "might",
    "need",
    "forgot",
    "exact",
    "definition",
    "accuracy",
    "true",
    "positive",
    "true",
    "negative",
    "different",
    "setups",
    "precision",
    "true",
    "positive",
    "positives",
    "true",
    "false",
    "recall",
    "true",
    "positive",
    "true",
    "positive",
    "plus",
    "false",
    "negative",
    "real",
    "quick",
    "flip",
    "back",
    "see",
    "numbers",
    "precision",
    "recall",
    "accuracy",
    "multiple",
    "linear",
    "regression",
    "let",
    "take",
    "brief",
    "look",
    "happens",
    "multiple",
    "inputs",
    "multiple",
    "linear",
    "regression",
    "well",
    "start",
    "simple",
    "linear",
    "regression",
    "equals",
    "plus",
    "x",
    "plus",
    "c",
    "trying",
    "find",
    "value",
    "multiple",
    "linear",
    "regression",
    "multiple",
    "variables",
    "coming",
    "instead",
    "x",
    "x1",
    "x2",
    "x3",
    "instead",
    "one",
    "slope",
    "variable",
    "slope",
    "attached",
    "see",
    "m1",
    "m2",
    "m3",
    "still",
    "single",
    "coefficient",
    "dealing",
    "multiple",
    "linear",
    "regression",
    "basically",
    "take",
    "single",
    "linear",
    "regression",
    "spread",
    "equals",
    "m1",
    "times",
    "x1",
    "plus",
    "m2",
    "times",
    "x2",
    "way",
    "nth",
    "x",
    "nth",
    "add",
    "coefficient",
    "implementation",
    "linear",
    "regression",
    "get",
    "favorite",
    "part",
    "let",
    "understand",
    "multiple",
    "linear",
    "regression",
    "works",
    "implementing",
    "python",
    "remember",
    "looking",
    "company",
    "based",
    "rnd",
    "trying",
    "figure",
    "profit",
    "going",
    "start",
    "looking",
    "expenditure",
    "company",
    "going",
    "go",
    "back",
    "going",
    "predict",
    "profit",
    "instead",
    "predicting",
    "r",
    "going",
    "look",
    "factors",
    "like",
    "administration",
    "costs",
    "marketing",
    "costs",
    "going",
    "see",
    "figure",
    "profit",
    "company",
    "going",
    "start",
    "coding",
    "going",
    "begin",
    "importing",
    "basic",
    "libraries",
    "going",
    "looking",
    "data",
    "kind",
    "linear",
    "regression",
    "going",
    "take",
    "look",
    "data",
    "see",
    "playing",
    "go",
    "ahead",
    "format",
    "data",
    "format",
    "need",
    "able",
    "run",
    "linear",
    "regression",
    "model",
    "go",
    "ahead",
    "solve",
    "see",
    "valid",
    "solution",
    "let",
    "start",
    "importing",
    "basic",
    "libraries",
    "going",
    "anaconda",
    "jupiter",
    "notebook",
    "popular",
    "ide",
    "enjoy",
    "visual",
    "look",
    "easy",
    "use",
    "id",
    "python",
    "work",
    "fine",
    "break",
    "favorite",
    "python",
    "ide",
    "jupiter",
    "notebook",
    "let",
    "go",
    "ahead",
    "paste",
    "first",
    "piece",
    "code",
    "let",
    "walk",
    "libraries",
    "importing",
    "first",
    "going",
    "import",
    "numpy",
    "np",
    "want",
    "skip",
    "one",
    "line",
    "look",
    "import",
    "pandas",
    "pd",
    "common",
    "tools",
    "need",
    "linear",
    "regression",
    "numpy",
    "stands",
    "number",
    "python",
    "usually",
    "denoted",
    "np",
    "almost",
    "sk",
    "learn",
    "toolbox",
    "always",
    "import",
    "right",
    "beginning",
    "pandas",
    "although",
    "sklearn",
    "libraries",
    "wonderful",
    "job",
    "importing",
    "data",
    "setting",
    "data",
    "frame",
    "manipulate",
    "rather",
    "easily",
    "lot",
    "tools",
    "also",
    "addition",
    "usually",
    "like",
    "use",
    "pandas",
    "show",
    "looks",
    "like",
    "three",
    "lines",
    "us",
    "get",
    "visual",
    "data",
    "take",
    "look",
    "going",
    "import",
    "plt",
    "seaborn",
    "sns",
    "seaborn",
    "works",
    "matplot",
    "library",
    "always",
    "import",
    "matplot",
    "library",
    "seaborn",
    "sits",
    "top",
    "take",
    "look",
    "looks",
    "like",
    "could",
    "use",
    "plotting",
    "libraries",
    "want",
    "kinds",
    "ways",
    "look",
    "data",
    "common",
    "ones",
    "seaborne",
    "easy",
    "use",
    "looks",
    "beautiful",
    "nice",
    "representation",
    "actually",
    "take",
    "show",
    "somebody",
    "final",
    "line",
    "amber",
    "scion",
    "map",
    "plot",
    "library",
    "inline",
    "inline",
    "ide",
    "interface",
    "anaconda",
    "jupiter",
    "notebook",
    "requires",
    "put",
    "going",
    "see",
    "graph",
    "comes",
    "let",
    "go",
    "ahead",
    "run",
    "going",
    "interesting",
    "setting",
    "variables",
    "fact",
    "going",
    "anything",
    "see",
    "importing",
    "different",
    "libraries",
    "setup",
    "next",
    "step",
    "load",
    "data",
    "set",
    "extract",
    "independent",
    "dependent",
    "variables",
    "slide",
    "see",
    "companies",
    "equals",
    "csv",
    "long",
    "line",
    "file",
    "end",
    "1000",
    "companies",
    "dot",
    "csv",
    "going",
    "change",
    "fit",
    "whatever",
    "setup",
    "file",
    "request",
    "go",
    "commentary",
    "video",
    "put",
    "note",
    "simply",
    "learn",
    "try",
    "get",
    "contact",
    "supply",
    "file",
    "try",
    "coding",
    "going",
    "add",
    "code",
    "going",
    "see",
    "companies",
    "equals",
    "underscore",
    "csv",
    "changed",
    "path",
    "match",
    "computer",
    "c",
    "colon",
    "slash",
    "simply",
    "learn",
    "slash",
    "1000",
    "underscore",
    "going",
    "set",
    "x",
    "equals",
    "companies",
    "location",
    "companies",
    "pd",
    "data",
    "set",
    "use",
    "nice",
    "notation",
    "says",
    "take",
    "every",
    "row",
    "colon",
    "first",
    "colon",
    "comma",
    "except",
    "last",
    "column",
    "second",
    "part",
    "colon",
    "minus",
    "one",
    "want",
    "values",
    "set",
    "x",
    "longer",
    "data",
    "set",
    "pandas",
    "data",
    "set",
    "easily",
    "extract",
    "data",
    "pandas",
    "data",
    "set",
    "notation",
    "going",
    "set",
    "equal",
    "last",
    "row",
    "well",
    "question",
    "going",
    "actually",
    "looking",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "going",
    "look",
    "companies",
    "dot",
    "head",
    "lists",
    "first",
    "five",
    "rows",
    "data",
    "open",
    "file",
    "second",
    "see",
    "coming",
    "let",
    "look",
    "data",
    "far",
    "way",
    "pandas",
    "sees",
    "hit",
    "run",
    "see",
    "breaks",
    "nice",
    "setup",
    "panda",
    "one",
    "things",
    "pandas",
    "really",
    "good",
    "looks",
    "like",
    "excel",
    "spreadsheet",
    "rows",
    "remember",
    "programming",
    "always",
    "start",
    "zero",
    "start",
    "one",
    "shows",
    "first",
    "five",
    "rows",
    "zero",
    "one",
    "two",
    "three",
    "four",
    "shows",
    "different",
    "columns",
    "r",
    "spend",
    "administration",
    "marketing",
    "spend",
    "state",
    "profit",
    "even",
    "notes",
    "top",
    "column",
    "names",
    "never",
    "told",
    "pandas",
    "able",
    "recognize",
    "lot",
    "things",
    "data",
    "rows",
    "go",
    "ahead",
    "open",
    "file",
    "csv",
    "actually",
    "see",
    "raw",
    "data",
    "opened",
    "text",
    "editor",
    "see",
    "top",
    "rnd",
    "spend",
    "comma",
    "administration",
    "comma",
    "marketing",
    "spin",
    "comma",
    "state",
    "comma",
    "profit",
    "carriage",
    "return",
    "know",
    "go",
    "crazy",
    "trying",
    "read",
    "files",
    "like",
    "use",
    "pandas",
    "could",
    "also",
    "open",
    "excel",
    "would",
    "separate",
    "since",
    "comma",
    "separated",
    "variable",
    "file",
    "want",
    "look",
    "one",
    "want",
    "look",
    "something",
    "read",
    "rather",
    "easily",
    "let",
    "flip",
    "back",
    "take",
    "look",
    "top",
    "part",
    "first",
    "five",
    "row",
    "nice",
    "format",
    "see",
    "data",
    "mean",
    "whole",
    "lot",
    "maybe",
    "expert",
    "business",
    "investments",
    "understand",
    "uh",
    "165",
    "349",
    "dollars",
    "20",
    "cents",
    "compared",
    "administration",
    "cost",
    "hundred",
    "thirty",
    "six",
    "thousand",
    "eight",
    "hundred",
    "ninety",
    "seven",
    "dollars",
    "eighty",
    "cents",
    "helps",
    "create",
    "profit",
    "192",
    "261",
    "83",
    "cents",
    "makes",
    "sense",
    "whatsoever",
    "pun",
    "intended",
    "let",
    "flip",
    "back",
    "take",
    "look",
    "next",
    "set",
    "code",
    "going",
    "graph",
    "get",
    "better",
    "understanding",
    "data",
    "means",
    "point",
    "going",
    "use",
    "single",
    "line",
    "code",
    "get",
    "lot",
    "information",
    "see",
    "going",
    "let",
    "go",
    "ahead",
    "paste",
    "notebook",
    "see",
    "got",
    "going",
    "visualization",
    "using",
    "sns",
    "pandas",
    "see",
    "imported",
    "map",
    "plot",
    "library",
    "dot",
    "pi",
    "plot",
    "plt",
    "seaborn",
    "uses",
    "imported",
    "seaborn",
    "sns",
    "final",
    "line",
    "code",
    "helps",
    "us",
    "show",
    "inline",
    "coding",
    "without",
    "would",
    "display",
    "could",
    "display",
    "file",
    "means",
    "matplot",
    "library",
    "line",
    "amber",
    "sign",
    "beginning",
    "come",
    "single",
    "line",
    "code",
    "seaborn",
    "great",
    "actually",
    "recognizes",
    "panda",
    "data",
    "frame",
    "take",
    "companies",
    "dot",
    "core",
    "coordinates",
    "put",
    "right",
    "seaborn",
    "run",
    "get",
    "beautiful",
    "plot",
    "let",
    "take",
    "look",
    "plot",
    "means",
    "look",
    "plot",
    "mine",
    "colors",
    "probably",
    "little",
    "bit",
    "purplish",
    "blue",
    "original",
    "one",
    "columns",
    "rows",
    "r",
    "spending",
    "administration",
    "marketing",
    "spending",
    "profit",
    "cross",
    "index",
    "two",
    "since",
    "interested",
    "profit",
    "cross",
    "index",
    "profit",
    "profit",
    "going",
    "show",
    "look",
    "scale",
    "right",
    "way",
    "dark",
    "data",
    "exact",
    "correspondence",
    "r",
    "spending",
    "going",
    "r",
    "spending",
    "thing",
    "administration",
    "costs",
    "right",
    "middle",
    "get",
    "dark",
    "row",
    "dark",
    "um",
    "diagonal",
    "row",
    "shows",
    "highest",
    "corresponding",
    "data",
    "exactly",
    "becomes",
    "lighter",
    "less",
    "connections",
    "data",
    "see",
    "profit",
    "obviously",
    "profit",
    "profit",
    "next",
    "high",
    "correlation",
    "r",
    "spending",
    "looked",
    "earlier",
    "slightly",
    "less",
    "connection",
    "marketing",
    "spending",
    "even",
    "less",
    "much",
    "money",
    "put",
    "administration",
    "nice",
    "look",
    "data",
    "let",
    "go",
    "ahead",
    "dig",
    "create",
    "actual",
    "useful",
    "linear",
    "regression",
    "model",
    "predict",
    "values",
    "better",
    "profit",
    "taken",
    "look",
    "visualization",
    "data",
    "going",
    "move",
    "next",
    "step",
    "instead",
    "pretty",
    "picture",
    "need",
    "generate",
    "hard",
    "data",
    "hard",
    "values",
    "let",
    "see",
    "looks",
    "like",
    "going",
    "set",
    "linear",
    "regression",
    "model",
    "two",
    "steps",
    "first",
    "one",
    "need",
    "prepare",
    "data",
    "fits",
    "correctly",
    "let",
    "go",
    "ahead",
    "paste",
    "code",
    "jupiter",
    "notebook",
    "bringing",
    "going",
    "bring",
    "sk",
    "learn",
    "going",
    "import",
    "label",
    "encoder",
    "one",
    "hot",
    "encoder",
    "use",
    "label",
    "encoder",
    "going",
    "create",
    "variable",
    "called",
    "label",
    "encoder",
    "set",
    "equal",
    "capital",
    "l",
    "label",
    "capital",
    "e",
    "encoder",
    "creates",
    "class",
    "reuse",
    "transferring",
    "labels",
    "back",
    "forth",
    "ask",
    "labels",
    "talking",
    "let",
    "go",
    "take",
    "look",
    "data",
    "processed",
    "see",
    "talking",
    "remember",
    "companies",
    "dot",
    "head",
    "printed",
    "top",
    "five",
    "rows",
    "data",
    "columns",
    "going",
    "across",
    "column",
    "zero",
    "r",
    "spending",
    "column",
    "one",
    "administration",
    "column",
    "two",
    "marketing",
    "spending",
    "column",
    "three",
    "state",
    "see",
    "state",
    "new",
    "york",
    "california",
    "florida",
    "linear",
    "regression",
    "model",
    "know",
    "process",
    "new",
    "york",
    "knows",
    "process",
    "number",
    "first",
    "thing",
    "going",
    "going",
    "change",
    "new",
    "york",
    "california",
    "florida",
    "going",
    "change",
    "numbers",
    "line",
    "code",
    "x",
    "equals",
    "colon",
    "comma",
    "3",
    "brackets",
    "first",
    "part",
    "colon",
    "comma",
    "means",
    "going",
    "look",
    "different",
    "rows",
    "going",
    "keep",
    "together",
    "row",
    "going",
    "edit",
    "third",
    "row",
    "going",
    "take",
    "label",
    "coder",
    "going",
    "fit",
    "transform",
    "x",
    "also",
    "third",
    "row",
    "going",
    "take",
    "third",
    "row",
    "going",
    "set",
    "equal",
    "transformation",
    "transformation",
    "basically",
    "tells",
    "instead",
    "uh",
    "new",
    "york",
    "zero",
    "one",
    "two",
    "finally",
    "need",
    "one",
    "hot",
    "encoder",
    "equals",
    "one",
    "hot",
    "inc",
    "categorical",
    "features",
    "equals",
    "three",
    "take",
    "x",
    "go",
    "ahead",
    "equal",
    "one",
    "hot",
    "encoder",
    "fit",
    "transform",
    "x",
    "array",
    "final",
    "transformation",
    "preps",
    "data",
    "us",
    "completely",
    "set",
    "way",
    "need",
    "row",
    "numbers",
    "even",
    "though",
    "let",
    "go",
    "ahead",
    "print",
    "x",
    "take",
    "look",
    "data",
    "see",
    "array",
    "arrays",
    "array",
    "row",
    "numbers",
    "go",
    "ahead",
    "row",
    "0",
    "see",
    "nice",
    "organized",
    "row",
    "numbers",
    "computer",
    "understands",
    "go",
    "ahead",
    "take",
    "mean",
    "whole",
    "lot",
    "us",
    "row",
    "numbers",
    "next",
    "setting",
    "data",
    "avoiding",
    "dummy",
    "variable",
    "trap",
    "important",
    "computer",
    "automatically",
    "transformed",
    "header",
    "setup",
    "automatically",
    "transferring",
    "different",
    "variables",
    "encoder",
    "encoder",
    "created",
    "two",
    "columns",
    "need",
    "one",
    "variable",
    "name",
    "piece",
    "code",
    "let",
    "go",
    "ahead",
    "paste",
    "x",
    "equals",
    "x",
    "colon",
    "comma",
    "one",
    "colon",
    "removing",
    "one",
    "extra",
    "column",
    "put",
    "one",
    "hot",
    "encoder",
    "label",
    "encoding",
    "let",
    "go",
    "ahead",
    "run",
    "get",
    "create",
    "linear",
    "regression",
    "model",
    "let",
    "see",
    "looks",
    "like",
    "going",
    "two",
    "steps",
    "first",
    "step",
    "going",
    "splitting",
    "data",
    "whenever",
    "create",
    "predictive",
    "model",
    "data",
    "always",
    "want",
    "split",
    "training",
    "set",
    "testing",
    "set",
    "important",
    "otherwise",
    "unethical",
    "without",
    "testing",
    "see",
    "good",
    "fit",
    "go",
    "ahead",
    "create",
    "multiple",
    "linear",
    "regression",
    "model",
    "train",
    "set",
    "let",
    "go",
    "ahead",
    "paste",
    "next",
    "piece",
    "code",
    "go",
    "ahead",
    "shrink",
    "size",
    "two",
    "fits",
    "one",
    "line",
    "sklearn",
    "module",
    "selection",
    "going",
    "import",
    "train",
    "test",
    "split",
    "see",
    "created",
    "four",
    "completely",
    "different",
    "variables",
    "capital",
    "x",
    "train",
    "capital",
    "x",
    "test",
    "smaller",
    "case",
    "train",
    "smaller",
    "case",
    "test",
    "standard",
    "way",
    "usually",
    "reference",
    "different",
    "models",
    "usually",
    "see",
    "capital",
    "x",
    "see",
    "train",
    "test",
    "lower",
    "case",
    "x",
    "data",
    "going",
    "rnd",
    "spin",
    "administration",
    "marketing",
    "training",
    "answer",
    "profit",
    "want",
    "know",
    "profit",
    "unknown",
    "entity",
    "going",
    "shoot",
    "tutorial",
    "next",
    "part",
    "train",
    "test",
    "split",
    "take",
    "x",
    "take",
    "already",
    "created",
    "x",
    "columns",
    "data",
    "column",
    "profit",
    "going",
    "set",
    "test",
    "size",
    "equals",
    "basically",
    "means",
    "20",
    "percent",
    "twenty",
    "percent",
    "rows",
    "going",
    "tested",
    "going",
    "put",
    "side",
    "since",
    "using",
    "thousand",
    "lines",
    "data",
    "means",
    "200",
    "lines",
    "going",
    "hold",
    "side",
    "test",
    "later",
    "random",
    "state",
    "equals",
    "zero",
    "going",
    "randomize",
    "ones",
    "picks",
    "hold",
    "side",
    "go",
    "ahead",
    "run",
    "overly",
    "exciting",
    "setting",
    "variables",
    "next",
    "step",
    "next",
    "step",
    "actually",
    "create",
    "linear",
    "regression",
    "model",
    "got",
    "linear",
    "regression",
    "model",
    "get",
    "next",
    "piece",
    "puzzle",
    "let",
    "go",
    "ahead",
    "put",
    "code",
    "walk",
    "go",
    "going",
    "paste",
    "let",
    "go",
    "ahead",
    "since",
    "shorter",
    "line",
    "code",
    "let",
    "zoom",
    "get",
    "good",
    "look",
    "sklearn",
    "dot",
    "linear",
    "underscore",
    "model",
    "going",
    "import",
    "linear",
    "regression",
    "know",
    "recall",
    "earlier",
    "math",
    "let",
    "go",
    "ahead",
    "flip",
    "back",
    "take",
    "look",
    "remember",
    "long",
    "formula",
    "bottom",
    "summarization",
    "also",
    "looked",
    "uh",
    "setting",
    "different",
    "lines",
    "also",
    "looked",
    "way",
    "multiple",
    "linear",
    "regression",
    "adding",
    "formulas",
    "together",
    "wrapped",
    "one",
    "section",
    "going",
    "going",
    "create",
    "variable",
    "called",
    "regressor",
    "regressor",
    "equals",
    "linear",
    "regression",
    "linear",
    "regression",
    "model",
    "math",
    "built",
    "memorized",
    "compute",
    "individually",
    "regressor",
    "dot",
    "fit",
    "case",
    "x",
    "train",
    "train",
    "using",
    "training",
    "data",
    "x",
    "data",
    "profit",
    "looking",
    "math",
    "us",
    "within",
    "one",
    "click",
    "one",
    "line",
    "created",
    "whole",
    "linear",
    "regression",
    "model",
    "fit",
    "data",
    "linear",
    "regression",
    "model",
    "see",
    "run",
    "regressor",
    "gives",
    "output",
    "linear",
    "regression",
    "says",
    "copy",
    "x",
    "equals",
    "true",
    "fit",
    "intercept",
    "equals",
    "true",
    "jobs",
    "equal",
    "one",
    "normalize",
    "equals",
    "false",
    "giving",
    "general",
    "information",
    "going",
    "regressor",
    "model",
    "created",
    "linear",
    "regression",
    "model",
    "let",
    "go",
    "ahead",
    "use",
    "remember",
    "kept",
    "bunch",
    "data",
    "aside",
    "going",
    "predict",
    "variable",
    "going",
    "put",
    "x",
    "test",
    "let",
    "see",
    "looks",
    "like",
    "scroll",
    "little",
    "bit",
    "paste",
    "predicting",
    "test",
    "set",
    "results",
    "predict",
    "equals",
    "regressor",
    "dot",
    "predict",
    "x",
    "test",
    "going",
    "gives",
    "us",
    "predict",
    "jupiter",
    "inline",
    "put",
    "variable",
    "hit",
    "run",
    "button",
    "print",
    "array",
    "could",
    "easily",
    "done",
    "print",
    "predict",
    "different",
    "ide",
    "inline",
    "setup",
    "like",
    "jupiter",
    "notebook",
    "way",
    "print",
    "predict",
    "see",
    "200",
    "different",
    "test",
    "variables",
    "kept",
    "side",
    "going",
    "produce",
    "200",
    "answers",
    "says",
    "profit",
    "200",
    "predictions",
    "let",
    "stop",
    "let",
    "keep",
    "going",
    "take",
    "couple",
    "look",
    "going",
    "take",
    "short",
    "detail",
    "calculating",
    "coefficients",
    "intercepts",
    "gives",
    "us",
    "quick",
    "flash",
    "going",
    "behind",
    "line",
    "going",
    "take",
    "short",
    "detour",
    "going",
    "calculating",
    "coefficient",
    "intercepts",
    "see",
    "look",
    "like",
    "really",
    "nice",
    "regressor",
    "created",
    "already",
    "coefficients",
    "us",
    "simply",
    "print",
    "regressor",
    "dot",
    "coefficient",
    "underscore",
    "run",
    "see",
    "coefficients",
    "regressor",
    "coefficient",
    "also",
    "regressor",
    "intercept",
    "let",
    "run",
    "take",
    "look",
    "came",
    "multiple",
    "regression",
    "model",
    "flip",
    "remember",
    "going",
    "coming",
    "see",
    "formula",
    "equals",
    "m1",
    "times",
    "x1",
    "plus",
    "m2",
    "times",
    "x2",
    "plus",
    "c",
    "coefficient",
    "variables",
    "fit",
    "right",
    "formula",
    "equals",
    "slope",
    "1",
    "times",
    "column",
    "1",
    "variable",
    "plus",
    "slope",
    "2",
    "times",
    "column",
    "2",
    "variable",
    "way",
    "n",
    "x",
    "n",
    "plus",
    "c",
    "coefficient",
    "case",
    "minus",
    "power",
    "2",
    "etc",
    "etc",
    "times",
    "first",
    "column",
    "second",
    "column",
    "third",
    "column",
    "intercept",
    "minus",
    "1",
    "0",
    "3",
    "0",
    "0",
    "9",
    "point",
    "boy",
    "gets",
    "kind",
    "complicated",
    "look",
    "hand",
    "anymore",
    "computer",
    "make",
    "calculations",
    "easy",
    "understand",
    "calculate",
    "told",
    "short",
    "detour",
    "coming",
    "towards",
    "end",
    "script",
    "remember",
    "beginning",
    "said",
    "going",
    "divide",
    "information",
    "make",
    "sure",
    "valid",
    "model",
    "model",
    "works",
    "understand",
    "good",
    "works",
    "calculating",
    "r",
    "squared",
    "value",
    "going",
    "use",
    "predict",
    "good",
    "prediction",
    "let",
    "take",
    "look",
    "looks",
    "like",
    "code",
    "going",
    "use",
    "going",
    "import",
    "r2",
    "score",
    "r",
    "squared",
    "value",
    "looking",
    "error",
    "r2",
    "score",
    "take",
    "test",
    "versus",
    "predict",
    "test",
    "actual",
    "values",
    "testing",
    "one",
    "given",
    "us",
    "know",
    "true",
    "predict",
    "200",
    "values",
    "think",
    "true",
    "go",
    "ahead",
    "run",
    "see",
    "get",
    "r2",
    "score",
    "exactly",
    "straight",
    "percentage",
    "saying",
    "93",
    "correct",
    "want",
    "upper",
    "90s",
    "higher",
    "shows",
    "valid",
    "prediction",
    "based",
    "r2",
    "score",
    "value",
    "got",
    "model",
    "remember",
    "random",
    "generation",
    "involved",
    "proves",
    "model",
    "good",
    "model",
    "means",
    "success",
    "yay",
    "successfully",
    "trained",
    "model",
    "certain",
    "predictors",
    "estimated",
    "profit",
    "companies",
    "using",
    "linear",
    "regression",
    "let",
    "take",
    "example",
    "see",
    "apply",
    "logistic",
    "regression",
    "predict",
    "number",
    "shown",
    "image",
    "actually",
    "live",
    "demo",
    "take",
    "jupiter",
    "notebook",
    "show",
    "code",
    "let",
    "take",
    "couple",
    "slides",
    "explain",
    "trying",
    "let",
    "say",
    "eight",
    "eight",
    "image",
    "image",
    "number",
    "one",
    "two",
    "three",
    "four",
    "need",
    "train",
    "model",
    "predict",
    "number",
    "first",
    "thing",
    "obviously",
    "machine",
    "learning",
    "process",
    "train",
    "model",
    "case",
    "using",
    "logistic",
    "regression",
    "provide",
    "training",
    "set",
    "train",
    "model",
    "test",
    "accurate",
    "model",
    "test",
    "data",
    "means",
    "like",
    "machine",
    "learning",
    "process",
    "split",
    "initial",
    "data",
    "two",
    "parts",
    "training",
    "set",
    "test",
    "set",
    "training",
    "set",
    "train",
    "model",
    "test",
    "set",
    "test",
    "model",
    "get",
    "good",
    "accuracy",
    "use",
    "inference",
    "right",
    "typical",
    "methodology",
    "training",
    "testing",
    "deploying",
    "machine",
    "learning",
    "models",
    "let",
    "take",
    "look",
    "code",
    "see",
    "go",
    "line",
    "line",
    "take",
    "blocks",
    "first",
    "thing",
    "import",
    "libraries",
    "basically",
    "take",
    "look",
    "images",
    "see",
    "total",
    "number",
    "images",
    "display",
    "using",
    "matplotlib",
    "images",
    "sample",
    "images",
    "split",
    "data",
    "training",
    "test",
    "mentioned",
    "earlier",
    "exploratory",
    "analysis",
    "build",
    "model",
    "train",
    "model",
    "training",
    "set",
    "test",
    "test",
    "set",
    "find",
    "accurate",
    "model",
    "using",
    "confusion",
    "matrix",
    "heat",
    "map",
    "use",
    "heat",
    "map",
    "visualizing",
    "show",
    "code",
    "exactly",
    "confusion",
    "matrix",
    "used",
    "finding",
    "accuracy",
    "example",
    "got",
    "get",
    "accuracy",
    "pretty",
    "good",
    "94",
    "pretty",
    "good",
    "right",
    "confusion",
    "matrix",
    "example",
    "confusion",
    "matrix",
    "used",
    "identifying",
    "accuracy",
    "classification",
    "model",
    "like",
    "logistic",
    "regression",
    "model",
    "important",
    "part",
    "confusion",
    "matrix",
    "first",
    "see",
    "matrix",
    "size",
    "matrix",
    "depends",
    "many",
    "outputs",
    "expecting",
    "right",
    "important",
    "part",
    "model",
    "accurate",
    "maximum",
    "numbers",
    "diagonal",
    "like",
    "case",
    "almost",
    "93",
    "94",
    "percent",
    "diagonals",
    "maximum",
    "numbers",
    "others",
    "diagnose",
    "cells",
    "diagonals",
    "numbers",
    "happening",
    "two",
    "one",
    "along",
    "diagonal",
    "mean",
    "means",
    "number",
    "fed",
    "zero",
    "number",
    "detected",
    "also",
    "zero",
    "predicted",
    "value",
    "actual",
    "value",
    "along",
    "diagonals",
    "true",
    "means",
    "let",
    "let",
    "take",
    "diagonal",
    "right",
    "maximum",
    "number",
    "means",
    "like",
    "case",
    "34",
    "means",
    "34",
    "images",
    "fed",
    "rather",
    "actually",
    "two",
    "misclassifications",
    "36",
    "images",
    "fed",
    "number",
    "four",
    "34",
    "predicted",
    "correctly",
    "number",
    "four",
    "one",
    "predicted",
    "number",
    "eight",
    "another",
    "one",
    "predicted",
    "number",
    "nine",
    "two",
    "misclassifications",
    "okay",
    "meaning",
    "saying",
    "maximum",
    "number",
    "diagonal",
    "ideal",
    "model",
    "let",
    "say",
    "hundred",
    "percent",
    "accuracy",
    "everything",
    "diagonal",
    "numbers",
    "zero",
    "cells",
    "like",
    "hundred",
    "percent",
    "accurate",
    "model",
    "okay",
    "uh",
    "use",
    "matrix",
    "uh",
    "use",
    "confusion",
    "matrix",
    "know",
    "name",
    "little",
    "funny",
    "sounding",
    "confusion",
    "matrix",
    "actually",
    "confusing",
    "straightforward",
    "plotting",
    "predicted",
    "labeled",
    "information",
    "actual",
    "data",
    "also",
    "known",
    "ground",
    "truth",
    "sometimes",
    "okay",
    "fancy",
    "terms",
    "used",
    "predicted",
    "label",
    "actual",
    "name",
    "okay",
    "yeah",
    "showing",
    "little",
    "bit",
    "information",
    "38",
    "predicted",
    "see",
    "predicted",
    "correctly",
    "38",
    "zeros",
    "predicted",
    "value",
    "actual",
    "value",
    "exactly",
    "whereas",
    "case",
    "right",
    "think",
    "37",
    "plus",
    "five",
    "yeah",
    "42",
    "fed",
    "images",
    "42",
    "images",
    "digit",
    "3",
    "accuracy",
    "37",
    "accurately",
    "predicted",
    "three",
    "predicted",
    "number",
    "seven",
    "two",
    "predicted",
    "number",
    "eight",
    "forth",
    "okay",
    "right",
    "let",
    "go",
    "jupiter",
    "notebook",
    "see",
    "code",
    "looks",
    "code",
    "jupiter",
    "notebook",
    "logistic",
    "regression",
    "particular",
    "demo",
    "going",
    "train",
    "model",
    "recognize",
    "digits",
    "images",
    "digits",
    "let",
    "say",
    "0",
    "5",
    "0",
    "9",
    "see",
    "well",
    "trained",
    "whether",
    "able",
    "predict",
    "numbers",
    "correctly",
    "let",
    "get",
    "started",
    "first",
    "part",
    "usual",
    "importing",
    "libraries",
    "required",
    "last",
    "line",
    "block",
    "load",
    "digits",
    "let",
    "go",
    "ahead",
    "run",
    "code",
    "visualize",
    "shape",
    "digits",
    "see",
    "take",
    "look",
    "shape",
    "1797",
    "like",
    "8x8",
    "images",
    "reflected",
    "shape",
    "onwards",
    "basically",
    "importing",
    "libraries",
    "required",
    "like",
    "numpy",
    "matplot",
    "take",
    "look",
    "uh",
    "sample",
    "images",
    "unloaded",
    "one",
    "example",
    "creates",
    "figure",
    "go",
    "ahead",
    "take",
    "sample",
    "images",
    "see",
    "look",
    "let",
    "run",
    "code",
    "becomes",
    "easy",
    "understand",
    "five",
    "images",
    "sample",
    "images",
    "looking",
    "zero",
    "one",
    "two",
    "three",
    "four",
    "image",
    "data",
    "okay",
    "uh",
    "based",
    "actually",
    "train",
    "logistic",
    "regression",
    "model",
    "test",
    "see",
    "well",
    "able",
    "recognize",
    "way",
    "works",
    "pixel",
    "information",
    "see",
    "8",
    "8",
    "pixel",
    "kind",
    "image",
    "pixel",
    "whether",
    "activated",
    "activated",
    "information",
    "available",
    "pixel",
    "based",
    "pattern",
    "activation",
    "various",
    "pixels",
    "identified",
    "zero",
    "example",
    "right",
    "similarly",
    "see",
    "overall",
    "numbers",
    "actually",
    "different",
    "pattern",
    "pixel",
    "activation",
    "pretty",
    "much",
    "model",
    "needs",
    "learn",
    "number",
    "pattern",
    "activation",
    "pixels",
    "right",
    "going",
    "train",
    "model",
    "okay",
    "first",
    "thing",
    "need",
    "split",
    "data",
    "training",
    "test",
    "data",
    "set",
    "right",
    "whenever",
    "perform",
    "training",
    "split",
    "data",
    "training",
    "test",
    "training",
    "data",
    "set",
    "used",
    "train",
    "system",
    "pass",
    "probably",
    "multiple",
    "times",
    "test",
    "test",
    "data",
    "set",
    "split",
    "usually",
    "form",
    "various",
    "ways",
    "split",
    "data",
    "individual",
    "preferences",
    "case",
    "splitting",
    "form",
    "23",
    "77",
    "say",
    "test",
    "size",
    "20",
    "means",
    "23",
    "percent",
    "entire",
    "data",
    "used",
    "testing",
    "remaining",
    "77",
    "percent",
    "used",
    "training",
    "readily",
    "available",
    "function",
    "uh",
    "called",
    "train",
    "test",
    "split",
    "write",
    "special",
    "code",
    "splitting",
    "automatically",
    "split",
    "data",
    "based",
    "proportion",
    "give",
    "test",
    "size",
    "give",
    "test",
    "size",
    "automatically",
    "training",
    "size",
    "determined",
    "pass",
    "data",
    "want",
    "split",
    "results",
    "stored",
    "x",
    "underscore",
    "train",
    "underscore",
    "train",
    "training",
    "data",
    "set",
    "x",
    "underscore",
    "train",
    "features",
    "right",
    "like",
    "independent",
    "variable",
    "underscore",
    "train",
    "label",
    "right",
    "case",
    "happens",
    "input",
    "value",
    "features",
    "value",
    "x",
    "underscore",
    "train",
    "since",
    "labeled",
    "data",
    "observations",
    "already",
    "label",
    "information",
    "saying",
    "whether",
    "digit",
    "zero",
    "one",
    "two",
    "used",
    "comparison",
    "find",
    "whether",
    "system",
    "able",
    "recognize",
    "correctly",
    "error",
    "observation",
    "compare",
    "right",
    "label",
    "way",
    "x",
    "underscore",
    "train",
    "underscore",
    "train",
    "training",
    "data",
    "set",
    "x",
    "underscore",
    "test",
    "underscore",
    "test",
    "test",
    "data",
    "set",
    "okay",
    "let",
    "go",
    "ahead",
    "execute",
    "code",
    "well",
    "go",
    "check",
    "quickly",
    "many",
    "entries",
    "x",
    "underscore",
    "train",
    "shape",
    "13",
    "83",
    "underscore",
    "train",
    "1383",
    "nothing",
    "like",
    "second",
    "part",
    "required",
    "x",
    "underscore",
    "test",
    "shape",
    "see",
    "414",
    "actually",
    "414",
    "observations",
    "test",
    "1383",
    "observations",
    "train",
    "basically",
    "four",
    "lines",
    "code",
    "saying",
    "okay",
    "import",
    "logistic",
    "regression",
    "library",
    "part",
    "implement",
    "logistic",
    "regression",
    "process",
    "call",
    "function",
    "let",
    "go",
    "ahead",
    "execute",
    "logistic",
    "regression",
    "library",
    "imported",
    "create",
    "instance",
    "logistic",
    "regression",
    "right",
    "logistic",
    "regr",
    "instance",
    "logistic",
    "regression",
    "use",
    "training",
    "model",
    "let",
    "first",
    "execute",
    "code",
    "two",
    "lines",
    "first",
    "line",
    "basically",
    "creates",
    "instance",
    "logistic",
    "regression",
    "model",
    "second",
    "line",
    "passing",
    "data",
    "training",
    "data",
    "set",
    "right",
    "predictors",
    "uh",
    "target",
    "passing",
    "data",
    "set",
    "train",
    "model",
    "right",
    "case",
    "data",
    "large",
    "large",
    "training",
    "takes",
    "usually",
    "lot",
    "time",
    "spend",
    "machine",
    "learning",
    "activities",
    "machine",
    "learning",
    "projects",
    "spend",
    "lot",
    "time",
    "training",
    "part",
    "okay",
    "data",
    "set",
    "relatively",
    "small",
    "pretty",
    "quick",
    "right",
    "model",
    "trained",
    "using",
    "training",
    "data",
    "set",
    "want",
    "see",
    "accurate",
    "test",
    "probably",
    "faces",
    "let",
    "first",
    "try",
    "well",
    "working",
    "one",
    "image",
    "okay",
    "try",
    "one",
    "image",
    "first",
    "entry",
    "test",
    "data",
    "set",
    "see",
    "whether",
    "correctly",
    "predicting",
    "order",
    "test",
    "training",
    "purpose",
    "use",
    "fit",
    "method",
    "method",
    "called",
    "fit",
    "training",
    "model",
    "training",
    "done",
    "want",
    "test",
    "particular",
    "value",
    "new",
    "input",
    "use",
    "predict",
    "method",
    "okay",
    "let",
    "run",
    "predict",
    "method",
    "pass",
    "particular",
    "image",
    "see",
    "shape",
    "prediction",
    "let",
    "try",
    "let",
    "see",
    "next",
    "10",
    "seems",
    "fine",
    "let",
    "go",
    "ahead",
    "test",
    "entire",
    "data",
    "set",
    "okay",
    "basically",
    "want",
    "find",
    "accurately",
    "performed",
    "use",
    "score",
    "method",
    "find",
    "percentages",
    "accuracy",
    "see",
    "performed",
    "94",
    "percent",
    "accurate",
    "okay",
    "part",
    "also",
    "um",
    "also",
    "see",
    "accuracy",
    "using",
    "known",
    "confusion",
    "matrix",
    "let",
    "us",
    "go",
    "ahead",
    "try",
    "well",
    "also",
    "visualize",
    "well",
    "model",
    "uh",
    "done",
    "let",
    "execute",
    "piece",
    "code",
    "basically",
    "import",
    "libraries",
    "required",
    "basically",
    "create",
    "confusion",
    "matrix",
    "instance",
    "confusion",
    "matrix",
    "running",
    "confusion",
    "matrix",
    "passing",
    "values",
    "confusion",
    "underscore",
    "matrix",
    "method",
    "takes",
    "two",
    "parameters",
    "one",
    "underscore",
    "test",
    "prediction",
    "underscore",
    "test",
    "labeled",
    "values",
    "already",
    "know",
    "test",
    "data",
    "set",
    "predictions",
    "system",
    "predicted",
    "test",
    "data",
    "set",
    "okay",
    "known",
    "us",
    "system",
    "model",
    "generated",
    "kind",
    "create",
    "confusion",
    "matrix",
    "print",
    "confusion",
    "matrix",
    "looks",
    "name",
    "suggests",
    "matrix",
    "key",
    "point",
    "accuracy",
    "model",
    "determined",
    "many",
    "numbers",
    "diagonal",
    "numbers",
    "diagonal",
    "better",
    "accuracy",
    "okay",
    "first",
    "total",
    "sum",
    "numbers",
    "whole",
    "matrix",
    "equal",
    "number",
    "observations",
    "test",
    "data",
    "set",
    "first",
    "thing",
    "right",
    "add",
    "numbers",
    "equal",
    "number",
    "observations",
    "test",
    "data",
    "set",
    "maximum",
    "number",
    "diagonal",
    "means",
    "accuracy",
    "pretty",
    "good",
    "numbers",
    "diagonal",
    "less",
    "places",
    "lot",
    "numbers",
    "uh",
    "means",
    "accuracy",
    "low",
    "diagonal",
    "indicates",
    "correct",
    "prediction",
    "means",
    "actual",
    "value",
    "predicted",
    "value",
    "actual",
    "values",
    "predictor",
    "value",
    "right",
    "moment",
    "see",
    "number",
    "means",
    "actual",
    "value",
    "something",
    "predicted",
    "value",
    "something",
    "else",
    "right",
    "similarly",
    "actual",
    "value",
    "something",
    "predicted",
    "value",
    "something",
    "else",
    "basically",
    "read",
    "confusion",
    "matrix",
    "find",
    "accuracy",
    "actually",
    "add",
    "total",
    "values",
    "diagonal",
    "like",
    "38",
    "plus",
    "44",
    "plus",
    "43",
    "divide",
    "total",
    "number",
    "test",
    "observations",
    "give",
    "percentage",
    "accuracy",
    "using",
    "confusion",
    "matrix",
    "let",
    "us",
    "visualize",
    "confusion",
    "matrix",
    "slightly",
    "sophisticated",
    "way",
    "uh",
    "using",
    "heat",
    "map",
    "create",
    "heat",
    "map",
    "add",
    "colors",
    "well",
    "uh",
    "like",
    "visually",
    "visually",
    "appealing",
    "whole",
    "idea",
    "let",
    "run",
    "piece",
    "code",
    "heat",
    "map",
    "looks",
    "see",
    "diagonals",
    "values",
    "values",
    "means",
    "reasonably",
    "seems",
    "reasonably",
    "accurate",
    "yeah",
    "basically",
    "accuracy",
    "score",
    "94",
    "percent",
    "calculated",
    "mentioned",
    "adding",
    "numbers",
    "divided",
    "total",
    "test",
    "value",
    "total",
    "number",
    "observations",
    "test",
    "data",
    "set",
    "okay",
    "confusion",
    "matrix",
    "logistic",
    "regression",
    "right",
    "seen",
    "confusion",
    "matrix",
    "let",
    "take",
    "quick",
    "sample",
    "see",
    "well",
    "system",
    "classified",
    "take",
    "examples",
    "data",
    "see",
    "picked",
    "randomly",
    "number",
    "four",
    "actual",
    "value",
    "also",
    "predicted",
    "value",
    "four",
    "image",
    "zero",
    "predicted",
    "value",
    "also",
    "zero",
    "actual",
    "value",
    "course",
    "zero",
    "image",
    "nine",
    "also",
    "predicted",
    "correctly",
    "nine",
    "actual",
    "value",
    "nine",
    "image",
    "one",
    "predicted",
    "correctly",
    "like",
    "actual",
    "value",
    "okay",
    "quick",
    "demo",
    "logistic",
    "regression",
    "use",
    "logistic",
    "regression",
    "identify",
    "images",
    "need",
    "confusion",
    "matrixes",
    "classification",
    "models",
    "multiple",
    "output",
    "categories",
    "error",
    "measures",
    "tell",
    "us",
    "total",
    "error",
    "model",
    "use",
    "find",
    "individual",
    "instances",
    "errors",
    "models",
    "input",
    "coming",
    "classifier",
    "measures",
    "error",
    "says",
    "oh",
    "53",
    "correct",
    "know",
    "53",
    "correct",
    "53",
    "correct",
    "uh",
    "guessing",
    "spam",
    "23",
    "guessing",
    "spam",
    "27",
    "guessing",
    "spam",
    "confusion",
    "matrix",
    "comes",
    "classification",
    "also",
    "overcome",
    "limitations",
    "accuracy",
    "accuracy",
    "misleading",
    "classification",
    "problems",
    "significant",
    "class",
    "imbalance",
    "model",
    "might",
    "predict",
    "majority",
    "class",
    "cases",
    "high",
    "accuracy",
    "score",
    "see",
    "email",
    "coming",
    "two",
    "spams",
    "classifier",
    "comes",
    "goes",
    "hey",
    "catches",
    "one",
    "spams",
    "misclassifies",
    "one",
    "spam",
    "model",
    "predicted",
    "eight",
    "ten",
    "instances",
    "accuracy",
    "80",
    "percent",
    "classifying",
    "correctly",
    "confusion",
    "matrix",
    "represents",
    "table",
    "layout",
    "different",
    "outcomes",
    "prediction",
    "results",
    "classification",
    "problem",
    "helps",
    "visualize",
    "outcomes",
    "see",
    "simple",
    "chart",
    "predicted",
    "actual",
    "confusion",
    "matrix",
    "helps",
    "us",
    "identify",
    "correct",
    "predictions",
    "model",
    "different",
    "individual",
    "classes",
    "well",
    "errors",
    "see",
    "values",
    "predicted",
    "classifier",
    "along",
    "rows",
    "going",
    "guess",
    "model",
    "guessing",
    "based",
    "training",
    "already",
    "trained",
    "model",
    "guess",
    "whether",
    "spam",
    "spam",
    "whatever",
    "working",
    "actual",
    "values",
    "data",
    "set",
    "along",
    "columns",
    "actual",
    "value",
    "supposed",
    "people",
    "speak",
    "english",
    "classified",
    "positives",
    "remember",
    "001",
    "speak",
    "english",
    "yes",
    "could",
    "extend",
    "might",
    "speak",
    "french",
    "speak",
    "whatever",
    "languages",
    "might",
    "whole",
    "lot",
    "classifiers",
    "would",
    "look",
    "one",
    "people",
    "speak",
    "english",
    "classified",
    "negatives",
    "zero",
    "know",
    "zero",
    "ones",
    "number",
    "times",
    "actual",
    "positive",
    "values",
    "equal",
    "predicted",
    "positive",
    "values",
    "gives",
    "us",
    "true",
    "positive",
    "tp",
    "number",
    "times",
    "actual",
    "negative",
    "values",
    "equal",
    "predictive",
    "negative",
    "values",
    "gives",
    "us",
    "true",
    "negative",
    "tn",
    "number",
    "times",
    "model",
    "wrongly",
    "predicts",
    "negative",
    "values",
    "positives",
    "gives",
    "us",
    "false",
    "positive",
    "fp",
    "see",
    "working",
    "lot",
    "know",
    "memorizing",
    "false",
    "positive",
    "easily",
    "figure",
    "pretty",
    "soon",
    "looking",
    "fp",
    "tp",
    "depending",
    "working",
    "number",
    "times",
    "model",
    "wrongly",
    "predicts",
    "positive",
    "values",
    "negatives",
    "gives",
    "us",
    "false",
    "negative",
    "fp",
    "going",
    "quick",
    "step",
    "let",
    "say",
    "working",
    "medical",
    "talking",
    "cancer",
    "really",
    "want",
    "bunch",
    "false",
    "negatives",
    "want",
    "zero",
    "false",
    "negative",
    "uh",
    "look",
    "confusion",
    "matrix",
    "five",
    "percent",
    "false",
    "positives",
    "five",
    "percent",
    "false",
    "negatives",
    "much",
    "better",
    "even",
    "twenty",
    "percent",
    "false",
    "positives",
    "go",
    "test",
    "zero",
    "false",
    "negatives",
    "might",
    "true",
    "working",
    "uh",
    "say",
    "car",
    "driving",
    "safe",
    "place",
    "car",
    "go",
    "well",
    "really",
    "want",
    "false",
    "positives",
    "know",
    "yes",
    "safe",
    "right",
    "cliff",
    "working",
    "project",
    "whatever",
    "working",
    "chart",
    "suddenly",
    "huge",
    "value",
    "talking",
    "spam",
    "email",
    "many",
    "important",
    "emails",
    "say",
    "banking",
    "overdraft",
    "charge",
    "coming",
    "want",
    "true",
    "false",
    "negative",
    "want",
    "go",
    "spam",
    "folder",
    "likewise",
    "want",
    "get",
    "much",
    "spam",
    "want",
    "miss",
    "anything",
    "really",
    "important",
    "confusion",
    "matrix",
    "metrics",
    "performance",
    "measures",
    "help",
    "us",
    "find",
    "accuracy",
    "classifier",
    "four",
    "main",
    "metrics",
    "accuracy",
    "precision",
    "recall",
    "f1",
    "score",
    "f1",
    "score",
    "one",
    "usually",
    "hear",
    "accuracy",
    "usually",
    "put",
    "chart",
    "sending",
    "front",
    "shareholders",
    "accurate",
    "people",
    "understand",
    "accuracy",
    "um",
    "f1",
    "score",
    "little",
    "bit",
    "math",
    "side",
    "got",
    "little",
    "careful",
    "quoting",
    "f1",
    "scores",
    "sitting",
    "shareholders",
    "lot",
    "glaze",
    "confusion",
    "matrix",
    "metrics",
    "performance",
    "measures",
    "help",
    "us",
    "find",
    "accuracy",
    "classifier",
    "four",
    "main",
    "metrics",
    "accuracy",
    "accuracy",
    "used",
    "find",
    "portion",
    "correctly",
    "classified",
    "values",
    "tells",
    "us",
    "often",
    "classifier",
    "right",
    "sum",
    "true",
    "values",
    "divided",
    "total",
    "values",
    "makes",
    "sense",
    "uh",
    "one",
    "things",
    "want",
    "know",
    "depends",
    "looking",
    "looking",
    "miss",
    "spam",
    "mails",
    "looking",
    "drive",
    "road",
    "run",
    "anybody",
    "precision",
    "used",
    "calculate",
    "model",
    "ability",
    "classify",
    "positive",
    "values",
    "correctly",
    "answers",
    "question",
    "model",
    "predicts",
    "positive",
    "value",
    "often",
    "right",
    "true",
    "positive",
    "divided",
    "total",
    "number",
    "predicted",
    "positive",
    "values",
    "one",
    "depends",
    "project",
    "working",
    "whether",
    "going",
    "focusing",
    "recall",
    "used",
    "calculate",
    "model",
    "ability",
    "predict",
    "positive",
    "values",
    "often",
    "model",
    "actually",
    "predict",
    "correct",
    "positive",
    "values",
    "true",
    "positives",
    "divided",
    "total",
    "number",
    "actual",
    "positive",
    "values",
    "f1",
    "score",
    "harmonic",
    "mean",
    "recall",
    "precision",
    "useful",
    "need",
    "take",
    "precision",
    "recall",
    "account",
    "consider",
    "following",
    "two",
    "confusion",
    "matrix",
    "derived",
    "two",
    "different",
    "classifier",
    "figure",
    "one",
    "performs",
    "better",
    "find",
    "confusion",
    "matrix",
    "see",
    "back",
    "classify",
    "whether",
    "speak",
    "english",
    "speak",
    "know",
    "english",
    "language",
    "put",
    "two",
    "uh",
    "confusion",
    "matrixes",
    "go",
    "ahead",
    "math",
    "behind",
    "look",
    "accuracy",
    "tpn",
    "plus",
    "tn",
    "tf",
    "plus",
    "tn",
    "plus",
    "fp",
    "plus",
    "fn",
    "get",
    "accuracy",
    "precision",
    "precision",
    "tp",
    "truth",
    "positive",
    "tp",
    "plus",
    "fp",
    "get",
    "recall",
    "end",
    "tp",
    "tp",
    "plus",
    "fn",
    "course",
    "f1",
    "score",
    "2",
    "times",
    "precision",
    "times",
    "recall",
    "precision",
    "plus",
    "recall",
    "get",
    "another",
    "model",
    "let",
    "say",
    "two",
    "different",
    "models",
    "trying",
    "see",
    "one",
    "want",
    "use",
    "whatever",
    "reason",
    "might",
    "go",
    "ahead",
    "compute",
    "things",
    "accuracy",
    "precision",
    "recall",
    "f1",
    "score",
    "looking",
    "might",
    "look",
    "accuracy",
    "really",
    "interested",
    "many",
    "people",
    "able",
    "classify",
    "able",
    "speak",
    "english",
    "really",
    "want",
    "know",
    "know",
    "really",
    "want",
    "know",
    "rather",
    "miss",
    "10",
    "people",
    "speaking",
    "english",
    "instead",
    "see",
    "charts",
    "probably",
    "go",
    "first",
    "model",
    "better",
    "job",
    "guessing",
    "speaks",
    "english",
    "higher",
    "accuracy",
    "case",
    "looking",
    "uh",
    "go",
    "ahead",
    "pull",
    "demo",
    "see",
    "looks",
    "like",
    "python",
    "setup",
    "actual",
    "coding",
    "go",
    "anaconda",
    "navigator",
    "familiar",
    "anaconda",
    "really",
    "good",
    "tool",
    "use",
    "far",
    "display",
    "demos",
    "quick",
    "development",
    "data",
    "scientist",
    "love",
    "package",
    "going",
    "something",
    "heavier",
    "lifting",
    "limitations",
    "anaconda",
    "setup",
    "general",
    "anything",
    "python",
    "go",
    "jupiter",
    "notebook",
    "jupiter",
    "lab",
    "jupiter",
    "notebook",
    "see",
    "integration",
    "pi",
    "charm",
    "work",
    "pycharm",
    "certainly",
    "lot",
    "integrations",
    "anaconda",
    "opened",
    "simply",
    "learn",
    "files",
    "work",
    "create",
    "new",
    "file",
    "called",
    "confusion",
    "matrix",
    "demo",
    "first",
    "thing",
    "want",
    "note",
    "data",
    "working",
    "opened",
    "wordpad",
    "notepad",
    "whatever",
    "see",
    "got",
    "row",
    "headers",
    "comma",
    "separated",
    "data",
    "going",
    "save",
    "file",
    "remember",
    "path",
    "working",
    "course",
    "data",
    "separated",
    "working",
    "lot",
    "data",
    "probably",
    "want",
    "put",
    "different",
    "folder",
    "file",
    "depending",
    "first",
    "thing",
    "going",
    "go",
    "ahead",
    "import",
    "tools",
    "going",
    "use",
    "pandas",
    "data",
    "frame",
    "chance",
    "work",
    "data",
    "frame",
    "please",
    "review",
    "panda",
    "data",
    "frame",
    "go",
    "simply",
    "learn",
    "pull",
    "pandas",
    "data",
    "frame",
    "tutorial",
    "going",
    "use",
    "scikit",
    "framework",
    "denoted",
    "sklearn",
    "pull",
    "see",
    "stable",
    "version",
    "import",
    "python",
    "going",
    "use",
    "train",
    "test",
    "split",
    "splitting",
    "data",
    "going",
    "going",
    "use",
    "logistic",
    "regression",
    "model",
    "actual",
    "machine",
    "learning",
    "model",
    "using",
    "court",
    "particular",
    "setup",
    "going",
    "accuracy",
    "score",
    "confusion",
    "matrix",
    "classifier",
    "report",
    "let",
    "go",
    "ahead",
    "run",
    "bring",
    "information",
    "like",
    "open",
    "file",
    "need",
    "go",
    "ahead",
    "load",
    "data",
    "going",
    "go",
    "ahead",
    "pandas",
    "read",
    "csv",
    "jupyter",
    "notebook",
    "put",
    "data",
    "read",
    "data",
    "lot",
    "times",
    "actually",
    "let",
    "prefer",
    "head",
    "date",
    "top",
    "part",
    "see",
    "age",
    "sex",
    "sure",
    "cp",
    "stands",
    "test",
    "bps",
    "cholesterol",
    "lot",
    "different",
    "measurements",
    "domain",
    "want",
    "know",
    "different",
    "measurements",
    "mean",
    "want",
    "focus",
    "much",
    "talking",
    "data",
    "science",
    "lot",
    "times",
    "idea",
    "data",
    "means",
    "ever",
    "looked",
    "breast",
    "cancer",
    "measurement",
    "bunch",
    "measurements",
    "numbers",
    "unless",
    "doctor",
    "idea",
    "measurements",
    "mean",
    "specialty",
    "domain",
    "better",
    "know",
    "going",
    "go",
    "ahead",
    "create",
    "going",
    "going",
    "set",
    "equal",
    "target",
    "target",
    "value",
    "either",
    "1",
    "classifier",
    "dealing",
    "one",
    "zero",
    "true",
    "false",
    "classifier",
    "x",
    "going",
    "uh",
    "everything",
    "except",
    "target",
    "going",
    "go",
    "ahead",
    "drop",
    "target",
    "axis",
    "equals",
    "remember",
    "columns",
    "versus",
    "index",
    "rows",
    "axis",
    "equals",
    "0",
    "would",
    "would",
    "give",
    "error",
    "would",
    "drop",
    "like",
    "row",
    "go",
    "ahead",
    "print",
    "see",
    "looking",
    "data",
    "x",
    "data",
    "see",
    "x",
    "data",
    "x",
    "head",
    "go",
    "ahead",
    "print",
    "head",
    "data",
    "run",
    "loading",
    "data",
    "done",
    "far",
    "confusion",
    "go",
    "back",
    "rewind",
    "tape",
    "review",
    "need",
    "go",
    "ahead",
    "split",
    "data",
    "x",
    "train",
    "x",
    "test",
    "train",
    "test",
    "keep",
    "mind",
    "always",
    "want",
    "split",
    "data",
    "scalar",
    "reason",
    "want",
    "scalar",
    "training",
    "data",
    "set",
    "training",
    "data",
    "data",
    "fit",
    "test",
    "data",
    "think",
    "field",
    "could",
    "actually",
    "alter",
    "results",
    "always",
    "important",
    "make",
    "sure",
    "whatever",
    "training",
    "data",
    "whatever",
    "fit",
    "always",
    "done",
    "training",
    "test",
    "want",
    "go",
    "ahead",
    "scale",
    "data",
    "working",
    "linear",
    "regression",
    "model",
    "mention",
    "minute",
    "get",
    "actual",
    "model",
    "uh",
    "sometimes",
    "need",
    "scale",
    "working",
    "linear",
    "regression",
    "models",
    "going",
    "change",
    "result",
    "much",
    "say",
    "neural",
    "network",
    "huge",
    "impact",
    "going",
    "go",
    "ahead",
    "take",
    "x",
    "train",
    "x",
    "test",
    "train",
    "test",
    "create",
    "scalar",
    "go",
    "ahead",
    "scale",
    "scale",
    "going",
    "fit",
    "x",
    "train",
    "going",
    "go",
    "ahead",
    "take",
    "x",
    "train",
    "transform",
    "also",
    "need",
    "take",
    "x",
    "test",
    "transform",
    "based",
    "scale",
    "x",
    "nice",
    "minus",
    "1",
    "one",
    "uh",
    "pre",
    "data",
    "setup",
    "hopefully",
    "uh",
    "looks",
    "fairly",
    "familiar",
    "done",
    "number",
    "classes",
    "setup",
    "want",
    "go",
    "ahead",
    "create",
    "model",
    "going",
    "use",
    "logistic",
    "regression",
    "model",
    "logistic",
    "regression",
    "model",
    "going",
    "go",
    "ahead",
    "fit",
    "x",
    "train",
    "train",
    "run",
    "predicted",
    "value",
    "let",
    "go",
    "ahead",
    "run",
    "actually",
    "like",
    "x",
    "test",
    "prediction",
    "remember",
    "matrix",
    "looking",
    "actual",
    "versus",
    "prediction",
    "compare",
    "take",
    "us",
    "back",
    "going",
    "notice",
    "imported",
    "accuracy",
    "score",
    "confusion",
    "matrix",
    "classification",
    "report",
    "course",
    "logistic",
    "regression",
    "model",
    "using",
    "mention",
    "going",
    "talk",
    "little",
    "bit",
    "scalar",
    "regression",
    "model",
    "scalar",
    "lot",
    "regression",
    "models",
    "basic",
    "mass",
    "standard",
    "regression",
    "models",
    "look",
    "logistic",
    "regression",
    "model",
    "using",
    "standard",
    "regression",
    "model",
    "need",
    "scale",
    "data",
    "already",
    "built",
    "way",
    "model",
    "works",
    "cases",
    "uh",
    "neural",
    "network",
    "lot",
    "different",
    "setups",
    "really",
    "want",
    "take",
    "fit",
    "go",
    "accuracy",
    "uh",
    "remember",
    "correctly",
    "looking",
    "accuracy",
    "english",
    "speaking",
    "saying",
    "accuracy",
    "whether",
    "person",
    "believe",
    "heart",
    "data",
    "set",
    "going",
    "accurate",
    "85",
    "percent",
    "time",
    "far",
    "whether",
    "going",
    "predict",
    "person",
    "going",
    "heart",
    "condition",
    "one",
    "comes",
    "zero",
    "one",
    "would",
    "mean",
    "point",
    "85",
    "percent",
    "uh",
    "correct",
    "telling",
    "someone",
    "extremely",
    "high",
    "risk",
    "heart",
    "attack",
    "kind",
    "thing",
    "want",
    "go",
    "ahead",
    "create",
    "confusion",
    "matrix",
    "course",
    "software",
    "everything",
    "us",
    "go",
    "ahead",
    "run",
    "see",
    "right",
    "um",
    "25",
    "prediction",
    "uh",
    "correct",
    "predictions",
    "right",
    "remember",
    "slide",
    "bring",
    "says",
    "nice",
    "visual",
    "true",
    "positive",
    "false",
    "positive",
    "uh",
    "25",
    "true",
    "said",
    "hey",
    "person",
    "going",
    "high",
    "risk",
    "heart",
    "four",
    "still",
    "high",
    "risk",
    "said",
    "false",
    "25",
    "people",
    "29",
    "people",
    "makes",
    "sense",
    "29",
    "people",
    "correct",
    "25",
    "uh",
    "accuracy",
    "score",
    "looking",
    "accuracy",
    "true",
    "positive",
    "true",
    "negative",
    "true",
    "accuracy",
    "coming",
    "nice",
    "matrix",
    "generated",
    "see",
    "right",
    "similar",
    "matrix",
    "going",
    "slide",
    "starts",
    "start",
    "asking",
    "questions",
    "point",
    "board",
    "meeting",
    "working",
    "really",
    "want",
    "start",
    "looking",
    "data",
    "saying",
    "well",
    "good",
    "enough",
    "uh",
    "number",
    "people",
    "hopefully",
    "much",
    "larger",
    "data",
    "set",
    "might",
    "confusion",
    "matrix",
    "showing",
    "true",
    "positive",
    "false",
    "positive",
    "acceptable",
    "uh",
    "course",
    "going",
    "put",
    "together",
    "whatever",
    "data",
    "putting",
    "might",
    "want",
    "separate",
    "true",
    "negative",
    "false",
    "positive",
    "false",
    "negative",
    "true",
    "positive",
    "simply",
    "confusion",
    "matrix",
    "course",
    "ravel",
    "part",
    "lets",
    "set",
    "split",
    "right",
    "nice",
    "tuple",
    "final",
    "thing",
    "want",
    "show",
    "coding",
    "part",
    "confusion",
    "matrix",
    "metrics",
    "come",
    "use",
    "matrix",
    "equals",
    "classification",
    "report",
    "test",
    "predict",
    "going",
    "take",
    "classification",
    "report",
    "go",
    "ahead",
    "print",
    "see",
    "nice",
    "job",
    "giving",
    "accuracy",
    "micro",
    "average",
    "weighted",
    "average",
    "precision",
    "recall",
    "f1",
    "score",
    "support",
    "one",
    "window",
    "start",
    "looking",
    "data",
    "saying",
    "oh",
    "okay",
    "precision",
    "uh",
    "getting",
    "positive",
    "negative",
    "side",
    "zero",
    "start",
    "talking",
    "whether",
    "valid",
    "information",
    "use",
    "looking",
    "heart",
    "attack",
    "prediction",
    "looking",
    "one",
    "aspect",
    "chances",
    "person",
    "heart",
    "attack",
    "might",
    "something",
    "went",
    "back",
    "languages",
    "maybe",
    "also",
    "want",
    "know",
    "whether",
    "speak",
    "english",
    "hindi",
    "french",
    "see",
    "right",
    "take",
    "confusion",
    "matrix",
    "expand",
    "big",
    "need",
    "depending",
    "many",
    "different",
    "classifiers",
    "working",
    "decision",
    "tree",
    "important",
    "terms",
    "dive",
    "need",
    "look",
    "basic",
    "terms",
    "need",
    "definitions",
    "go",
    "decision",
    "tree",
    "different",
    "parts",
    "going",
    "using",
    "start",
    "entropy",
    "entropy",
    "measure",
    "randomness",
    "unpredictability",
    "data",
    "set",
    "example",
    "group",
    "animals",
    "picture",
    "four",
    "different",
    "kinds",
    "animals",
    "data",
    "set",
    "considered",
    "high",
    "entropy",
    "really",
    "ca",
    "pick",
    "kind",
    "animal",
    "based",
    "looking",
    "four",
    "animals",
    "big",
    "clump",
    "entities",
    "start",
    "splitting",
    "subgroups",
    "come",
    "second",
    "definition",
    "information",
    "gain",
    "information",
    "gain",
    "measure",
    "decrease",
    "entropy",
    "data",
    "set",
    "split",
    "case",
    "based",
    "color",
    "yellow",
    "split",
    "one",
    "group",
    "animals",
    "one",
    "side",
    "true",
    "yellow",
    "false",
    "continue",
    "yellow",
    "side",
    "split",
    "based",
    "height",
    "true",
    "false",
    "equals",
    "ten",
    "side",
    "height",
    "less",
    "10",
    "true",
    "false",
    "see",
    "split",
    "entropy",
    "continues",
    "less",
    "less",
    "less",
    "information",
    "gain",
    "simply",
    "entropy",
    "e1",
    "top",
    "changed",
    "e2",
    "bottom",
    "look",
    "deeper",
    "math",
    "although",
    "really",
    "need",
    "know",
    "huge",
    "amount",
    "math",
    "actually",
    "programming",
    "python",
    "look",
    "actual",
    "math",
    "compute",
    "entropy",
    "finally",
    "went",
    "different",
    "parts",
    "tree",
    "call",
    "leaf",
    "node",
    "leaf",
    "node",
    "carries",
    "classification",
    "decision",
    "final",
    "end",
    "bottom",
    "decision",
    "node",
    "two",
    "branches",
    "breaking",
    "group",
    "different",
    "parts",
    "finally",
    "root",
    "node",
    "topmost",
    "decision",
    "node",
    "known",
    "root",
    "node",
    "decision",
    "tree",
    "work",
    "wonder",
    "kind",
    "animals",
    "get",
    "jungle",
    "today",
    "maybe",
    "hunter",
    "gun",
    "photography",
    "photographer",
    "camera",
    "let",
    "look",
    "group",
    "animals",
    "let",
    "try",
    "classify",
    "different",
    "types",
    "animals",
    "based",
    "features",
    "using",
    "decision",
    "tree",
    "problem",
    "statement",
    "classify",
    "different",
    "types",
    "animals",
    "based",
    "features",
    "using",
    "decision",
    "tree",
    "data",
    "set",
    "looking",
    "quite",
    "messy",
    "entropy",
    "high",
    "case",
    "let",
    "look",
    "training",
    "set",
    "training",
    "data",
    "set",
    "looking",
    "color",
    "looking",
    "height",
    "different",
    "animals",
    "elephants",
    "giraffes",
    "monkeys",
    "tigers",
    "different",
    "colors",
    "shapes",
    "let",
    "see",
    "looks",
    "like",
    "split",
    "data",
    "frame",
    "conditions",
    "split",
    "data",
    "way",
    "information",
    "gain",
    "highest",
    "note",
    "gain",
    "measure",
    "decrease",
    "entropy",
    "splitting",
    "formula",
    "entropy",
    "sum",
    "symbol",
    "looks",
    "like",
    "looks",
    "like",
    "kind",
    "like",
    "e",
    "funky",
    "e",
    "k",
    "equals",
    "1",
    "k",
    "k",
    "would",
    "represent",
    "number",
    "animals",
    "different",
    "animals",
    "value",
    "p",
    "value",
    "would",
    "percentage",
    "animal",
    "times",
    "log",
    "base",
    "two",
    "percentage",
    "animal",
    "let",
    "try",
    "calculate",
    "entropy",
    "current",
    "data",
    "set",
    "take",
    "look",
    "looks",
    "like",
    "afraid",
    "math",
    "really",
    "memorize",
    "math",
    "aware",
    "going",
    "background",
    "three",
    "giraffes",
    "two",
    "tigers",
    "one",
    "monkey",
    "two",
    "elephants",
    "total",
    "eight",
    "animals",
    "gathered",
    "plug",
    "formula",
    "get",
    "entropy",
    "equals",
    "three",
    "eight",
    "three",
    "giraffes",
    "total",
    "eight",
    "times",
    "log",
    "usually",
    "use",
    "base",
    "two",
    "log",
    "log",
    "base",
    "two",
    "three",
    "eight",
    "plus",
    "case",
    "says",
    "yellow",
    "fence",
    "two",
    "eight",
    "two",
    "elephants",
    "total",
    "eight",
    "times",
    "log",
    "base",
    "two",
    "two",
    "eight",
    "plus",
    "one",
    "monkey",
    "total",
    "eight",
    "log",
    "base",
    "two",
    "one",
    "eight",
    "plus",
    "two",
    "eight",
    "tigers",
    "log",
    "base",
    "two",
    "eight",
    "plug",
    "computer",
    "calculator",
    "obviously",
    "ca",
    "logs",
    "head",
    "get",
    "entropy",
    "equal",
    "program",
    "actually",
    "calculate",
    "entropy",
    "data",
    "set",
    "similarly",
    "every",
    "split",
    "calculate",
    "gain",
    "going",
    "go",
    "set",
    "one",
    "time",
    "see",
    "numbers",
    "want",
    "aware",
    "formula",
    "mathematics",
    "behind",
    "gain",
    "calculated",
    "finding",
    "difference",
    "subsequent",
    "entropy",
    "values",
    "split",
    "try",
    "choose",
    "condition",
    "gives",
    "us",
    "highest",
    "gain",
    "splitting",
    "data",
    "using",
    "condition",
    "checking",
    "gain",
    "get",
    "condition",
    "gives",
    "us",
    "highest",
    "gain",
    "used",
    "make",
    "first",
    "split",
    "guess",
    "first",
    "split",
    "looking",
    "image",
    "human",
    "probably",
    "pretty",
    "easy",
    "split",
    "let",
    "see",
    "right",
    "guessed",
    "color",
    "yellow",
    "correct",
    "let",
    "say",
    "condition",
    "gives",
    "us",
    "maximum",
    "gain",
    "yellow",
    "split",
    "data",
    "based",
    "color",
    "yellow",
    "true",
    "group",
    "animals",
    "goes",
    "left",
    "false",
    "goes",
    "right",
    "entropy",
    "splitting",
    "decreased",
    "considerably",
    "however",
    "still",
    "need",
    "splitting",
    "branches",
    "attain",
    "entropy",
    "value",
    "equal",
    "zero",
    "decide",
    "split",
    "nodes",
    "using",
    "height",
    "condition",
    "since",
    "every",
    "branch",
    "contains",
    "single",
    "label",
    "type",
    "say",
    "entropy",
    "case",
    "reached",
    "least",
    "value",
    "see",
    "giraffes",
    "tigers",
    "monkey",
    "elephants",
    "separated",
    "groups",
    "tree",
    "predict",
    "classes",
    "animals",
    "present",
    "dataset",
    "hundred",
    "percent",
    "accuracy",
    "easy",
    "use",
    "case",
    "loan",
    "repayment",
    "prediction",
    "let",
    "get",
    "favorite",
    "part",
    "open",
    "python",
    "see",
    "programming",
    "code",
    "scripting",
    "looks",
    "like",
    "going",
    "want",
    "prediction",
    "start",
    "individual",
    "requesting",
    "find",
    "good",
    "customers",
    "going",
    "whether",
    "going",
    "repay",
    "loan",
    "bank",
    "want",
    "generate",
    "problem",
    "statement",
    "predict",
    "customer",
    "repay",
    "loan",
    "amount",
    "going",
    "using",
    "decision",
    "tree",
    "algorithm",
    "python",
    "let",
    "see",
    "looks",
    "like",
    "let",
    "dive",
    "code",
    "first",
    "steps",
    "implementation",
    "going",
    "start",
    "importing",
    "necessary",
    "packages",
    "need",
    "python",
    "going",
    "load",
    "data",
    "take",
    "look",
    "data",
    "looks",
    "like",
    "first",
    "thing",
    "need",
    "need",
    "something",
    "edit",
    "python",
    "run",
    "let",
    "flip",
    "using",
    "anaconda",
    "jupiter",
    "notebook",
    "use",
    "python",
    "ide",
    "like",
    "run",
    "find",
    "jupiter",
    "notebook",
    "really",
    "nice",
    "things",
    "fly",
    "let",
    "go",
    "ahead",
    "paste",
    "code",
    "beginning",
    "start",
    "let",
    "talk",
    "little",
    "bit",
    "bringing",
    "going",
    "couple",
    "things",
    "make",
    "couple",
    "changes",
    "go",
    "first",
    "part",
    "import",
    "first",
    "thing",
    "bring",
    "numpy",
    "np",
    "standard",
    "dealing",
    "mathematics",
    "especially",
    "uh",
    "complicated",
    "machine",
    "learning",
    "tools",
    "almost",
    "always",
    "see",
    "numpy",
    "come",
    "num",
    "numbers",
    "called",
    "number",
    "python",
    "mathematics",
    "case",
    "actually",
    "could",
    "take",
    "generally",
    "need",
    "different",
    "things",
    "work",
    "going",
    "use",
    "pandas",
    "pd",
    "also",
    "standard",
    "pandas",
    "data",
    "frame",
    "setup",
    "liken",
    "taking",
    "basic",
    "data",
    "storing",
    "way",
    "looks",
    "like",
    "excel",
    "spreadsheet",
    "come",
    "back",
    "see",
    "np",
    "pd",
    "standard",
    "uses",
    "know",
    "pandas",
    "show",
    "little",
    "bit",
    "explore",
    "data",
    "minute",
    "going",
    "need",
    "split",
    "data",
    "going",
    "bring",
    "train",
    "test",
    "split",
    "coming",
    "sk",
    "learn",
    "package",
    "cross",
    "validation",
    "minute",
    "going",
    "change",
    "go",
    "also",
    "import",
    "decision",
    "tree",
    "classifier",
    "actual",
    "tool",
    "using",
    "remember",
    "told",
    "afraid",
    "mathematics",
    "going",
    "done",
    "well",
    "decision",
    "tree",
    "classifier",
    "mathematics",
    "figure",
    "back",
    "accuracy",
    "score",
    "need",
    "score",
    "setup",
    "whole",
    "reason",
    "splitting",
    "training",
    "testing",
    "data",
    "finally",
    "still",
    "need",
    "sklearn",
    "import",
    "tree",
    "basic",
    "tree",
    "function",
    "needed",
    "decision",
    "tree",
    "classifier",
    "finally",
    "going",
    "load",
    "data",
    "going",
    "run",
    "going",
    "get",
    "two",
    "things",
    "one",
    "going",
    "get",
    "error",
    "two",
    "going",
    "get",
    "warning",
    "let",
    "see",
    "looks",
    "like",
    "first",
    "thing",
    "error",
    "error",
    "well",
    "looking",
    "says",
    "need",
    "read",
    "file",
    "written",
    "person",
    "wrote",
    "path",
    "stored",
    "file",
    "let",
    "go",
    "ahead",
    "fix",
    "going",
    "put",
    "file",
    "path",
    "going",
    "call",
    "full",
    "file",
    "name",
    "see",
    "c",
    "drive",
    "lengthy",
    "setup",
    "stored",
    "file",
    "worry",
    "much",
    "full",
    "path",
    "computer",
    "different",
    "csv",
    "file",
    "generated",
    "simplylearn",
    "want",
    "copy",
    "comment",
    "request",
    "youtube",
    "going",
    "give",
    "name",
    "full",
    "file",
    "name",
    "going",
    "go",
    "ahead",
    "change",
    "full",
    "file",
    "name",
    "let",
    "go",
    "ahead",
    "run",
    "see",
    "happens",
    "get",
    "warning",
    "coding",
    "understanding",
    "different",
    "warnings",
    "different",
    "errors",
    "come",
    "probably",
    "hardest",
    "lesson",
    "learn",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "use",
    "opportunity",
    "understand",
    "going",
    "read",
    "warning",
    "says",
    "cross",
    "validation",
    "depreciated",
    "warning",
    "removed",
    "going",
    "moved",
    "favor",
    "model",
    "selection",
    "go",
    "sklearn",
    "dot",
    "cross",
    "validation",
    "research",
    "go",
    "sklearn",
    "site",
    "find",
    "actually",
    "swap",
    "right",
    "model",
    "selection",
    "come",
    "run",
    "removes",
    "warning",
    "done",
    "two",
    "different",
    "developers",
    "develop",
    "two",
    "different",
    "branches",
    "decided",
    "keep",
    "one",
    "eventually",
    "get",
    "rid",
    "one",
    "easy",
    "quick",
    "fix",
    "go",
    "went",
    "ahead",
    "opened",
    "data",
    "file",
    "remember",
    "data",
    "file",
    "loaded",
    "data",
    "let",
    "talk",
    "little",
    "bit",
    "see",
    "looks",
    "like",
    "text",
    "file",
    "comma",
    "separated",
    "variable",
    "file",
    "spreadsheet",
    "looks",
    "like",
    "basic",
    "text",
    "file",
    "see",
    "top",
    "created",
    "header",
    "got",
    "one",
    "two",
    "three",
    "four",
    "five",
    "columns",
    "column",
    "data",
    "let",
    "flip",
    "also",
    "going",
    "look",
    "actual",
    "spreadsheet",
    "see",
    "looks",
    "like",
    "opened",
    "open",
    "office",
    "calc",
    "pretty",
    "much",
    "excel",
    "zoomed",
    "see",
    "got",
    "columns",
    "rows",
    "data",
    "little",
    "easier",
    "read",
    "result",
    "yes",
    "yes",
    "initial",
    "payment",
    "last",
    "payment",
    "credit",
    "score",
    "house",
    "number",
    "scroll",
    "way",
    "see",
    "occupies",
    "thousand",
    "one",
    "lines",
    "code",
    "lines",
    "data",
    "first",
    "one",
    "column",
    "one",
    "thousand",
    "lines",
    "data",
    "programmer",
    "looking",
    "small",
    "amount",
    "data",
    "usually",
    "start",
    "pulling",
    "different",
    "sources",
    "see",
    "working",
    "larger",
    "data",
    "wo",
    "option",
    "large",
    "need",
    "either",
    "bring",
    "small",
    "amount",
    "look",
    "like",
    "right",
    "start",
    "looking",
    "python",
    "code",
    "let",
    "go",
    "ahead",
    "move",
    "take",
    "next",
    "couple",
    "steps",
    "explore",
    "data",
    "using",
    "python",
    "let",
    "go",
    "ahead",
    "see",
    "looks",
    "like",
    "python",
    "print",
    "length",
    "shape",
    "data",
    "let",
    "start",
    "printing",
    "length",
    "database",
    "use",
    "simple",
    "lin",
    "function",
    "python",
    "run",
    "see",
    "thousand",
    "long",
    "expected",
    "thousand",
    "lines",
    "data",
    "subtract",
    "column",
    "head",
    "one",
    "nice",
    "things",
    "uh",
    "balance",
    "data",
    "panda",
    "read",
    "csv",
    "see",
    "header",
    "row",
    "zero",
    "automatically",
    "removes",
    "row",
    "shows",
    "data",
    "separate",
    "good",
    "job",
    "sorting",
    "data",
    "us",
    "use",
    "different",
    "function",
    "let",
    "take",
    "look",
    "going",
    "utilize",
    "tools",
    "panda",
    "since",
    "balance",
    "underscored",
    "data",
    "loaded",
    "panda",
    "data",
    "frame",
    "shape",
    "let",
    "go",
    "ahead",
    "run",
    "shape",
    "see",
    "looks",
    "like",
    "nice",
    "shape",
    "give",
    "length",
    "data",
    "thousand",
    "lines",
    "also",
    "tells",
    "five",
    "columns",
    "looking",
    "data",
    "five",
    "columns",
    "data",
    "let",
    "take",
    "one",
    "step",
    "explore",
    "data",
    "using",
    "python",
    "taken",
    "look",
    "length",
    "shape",
    "let",
    "go",
    "ahead",
    "use",
    "pandas",
    "module",
    "head",
    "another",
    "beautiful",
    "thing",
    "data",
    "set",
    "utilize",
    "let",
    "put",
    "sheet",
    "print",
    "data",
    "set",
    "balance",
    "data",
    "dot",
    "head",
    "panda",
    "print",
    "statement",
    "print",
    "feature",
    "went",
    "ahead",
    "gave",
    "label",
    "print",
    "job",
    "dataset",
    "simple",
    "print",
    "statement",
    "run",
    "let",
    "take",
    "closer",
    "look",
    "let",
    "zoom",
    "go",
    "pandas",
    "wonderful",
    "job",
    "making",
    "clean",
    "readable",
    "data",
    "set",
    "look",
    "data",
    "look",
    "column",
    "headers",
    "put",
    "head",
    "prints",
    "first",
    "five",
    "lines",
    "data",
    "always",
    "start",
    "zero",
    "five",
    "lines",
    "zero",
    "one",
    "two",
    "three",
    "four",
    "instead",
    "one",
    "two",
    "three",
    "four",
    "five",
    "standard",
    "scripting",
    "programming",
    "set",
    "want",
    "start",
    "zero",
    "position",
    "data",
    "head",
    "pulls",
    "first",
    "five",
    "rows",
    "data",
    "puts",
    "nice",
    "format",
    "look",
    "view",
    "powerful",
    "tool",
    "view",
    "data",
    "instead",
    "flip",
    "open",
    "excel",
    "spreadsheet",
    "open",
    "office",
    "cal",
    "trying",
    "look",
    "word",
    "doc",
    "scrunched",
    "together",
    "hard",
    "read",
    "get",
    "nice",
    "open",
    "view",
    "working",
    "working",
    "shape",
    "thousand",
    "long",
    "five",
    "wide",
    "five",
    "columns",
    "full",
    "date",
    "ahead",
    "actually",
    "see",
    "data",
    "looks",
    "like",
    "initial",
    "payment",
    "last",
    "payment",
    "credit",
    "scores",
    "house",
    "number",
    "let",
    "take",
    "explored",
    "data",
    "let",
    "start",
    "digging",
    "decision",
    "tree",
    "next",
    "step",
    "going",
    "train",
    "build",
    "data",
    "tree",
    "need",
    "first",
    "separate",
    "data",
    "going",
    "separate",
    "two",
    "groups",
    "something",
    "actually",
    "train",
    "data",
    "data",
    "side",
    "test",
    "see",
    "good",
    "model",
    "remember",
    "machine",
    "learning",
    "always",
    "want",
    "kind",
    "test",
    "set",
    "weigh",
    "know",
    "good",
    "model",
    "distribute",
    "let",
    "go",
    "ahead",
    "break",
    "code",
    "look",
    "pieces",
    "first",
    "x",
    "x",
    "come",
    "well",
    "x",
    "going",
    "data",
    "going",
    "answer",
    "target",
    "look",
    "source",
    "target",
    "case",
    "using",
    "x",
    "denote",
    "data",
    "data",
    "actually",
    "trying",
    "guess",
    "answer",
    "going",
    "separate",
    "simply",
    "put",
    "x",
    "equals",
    "balance",
    "first",
    "brackets",
    "means",
    "going",
    "select",
    "lines",
    "database",
    "data",
    "second",
    "one",
    "says",
    "going",
    "look",
    "columns",
    "one",
    "five",
    "remember",
    "always",
    "start",
    "zero",
    "zero",
    "yes",
    "whether",
    "loan",
    "went",
    "default",
    "want",
    "start",
    "one",
    "go",
    "back",
    "initial",
    "payment",
    "goes",
    "way",
    "house",
    "number",
    "well",
    "want",
    "look",
    "one",
    "five",
    "thing",
    "answers",
    "going",
    "set",
    "equal",
    "zero",
    "row",
    "zero",
    "row",
    "rows",
    "going",
    "divided",
    "two",
    "different",
    "data",
    "sets",
    "one",
    "data",
    "going",
    "one",
    "answers",
    "next",
    "need",
    "split",
    "data",
    "see",
    "split",
    "four",
    "different",
    "parts",
    "first",
    "one",
    "x",
    "training",
    "x",
    "test",
    "train",
    "test",
    "simply",
    "put",
    "x",
    "going",
    "going",
    "train",
    "know",
    "answer",
    "train",
    "x",
    "test",
    "going",
    "test",
    "data",
    "know",
    "end",
    "supposed",
    "train",
    "test",
    "split",
    "comes",
    "loaded",
    "earlier",
    "modules",
    "us",
    "see",
    "set",
    "test",
    "size",
    "equal",
    "roughly",
    "30",
    "percent",
    "used",
    "test",
    "use",
    "random",
    "state",
    "completely",
    "random",
    "rows",
    "takes",
    "finally",
    "get",
    "actually",
    "build",
    "decision",
    "tree",
    "called",
    "clf",
    "underscore",
    "entropy",
    "actual",
    "decision",
    "tree",
    "decision",
    "tree",
    "classifier",
    "added",
    "couple",
    "variables",
    "explore",
    "minute",
    "finally",
    "need",
    "fit",
    "data",
    "take",
    "clf",
    "entropy",
    "created",
    "fit",
    "x",
    "train",
    "since",
    "know",
    "answers",
    "x",
    "trade",
    "train",
    "go",
    "ahead",
    "put",
    "let",
    "go",
    "ahead",
    "run",
    "sklearn",
    "modules",
    "set",
    "variable",
    "case",
    "set",
    "clf",
    "entropy",
    "called",
    "decision",
    "tree",
    "classifier",
    "automatically",
    "prints",
    "decision",
    "tree",
    "lot",
    "variables",
    "play",
    "within",
    "quite",
    "beyond",
    "scope",
    "tutorial",
    "go",
    "work",
    "working",
    "entropy",
    "one",
    "options",
    "added",
    "completely",
    "random",
    "state",
    "100",
    "100",
    "percent",
    "max",
    "depth",
    "three",
    "max",
    "depth",
    "remember",
    "different",
    "graphs",
    "animals",
    "means",
    "going",
    "go",
    "three",
    "layers",
    "stops",
    "minimal",
    "samples",
    "leaves",
    "five",
    "going",
    "least",
    "five",
    "leaves",
    "end",
    "least",
    "three",
    "splits",
    "three",
    "layers",
    "least",
    "five",
    "end",
    "leaves",
    "final",
    "result",
    "bottom",
    "created",
    "decision",
    "tree",
    "classifier",
    "created",
    "trained",
    "let",
    "go",
    "ahead",
    "apply",
    "see",
    "looks",
    "like",
    "let",
    "go",
    "ahead",
    "make",
    "prediction",
    "see",
    "looks",
    "like",
    "going",
    "paste",
    "predict",
    "code",
    "run",
    "let",
    "take",
    "quick",
    "look",
    "variable",
    "predict",
    "going",
    "going",
    "use",
    "variable",
    "clf",
    "entropy",
    "created",
    "see",
    "dot",
    "predict",
    "common",
    "sk",
    "learn",
    "modules",
    "different",
    "tools",
    "predict",
    "actually",
    "running",
    "prediction",
    "case",
    "going",
    "put",
    "x",
    "test",
    "data",
    "delivered",
    "use",
    "actual",
    "commercial",
    "use",
    "distributed",
    "would",
    "new",
    "loans",
    "putting",
    "guess",
    "whether",
    "person",
    "going",
    "uh",
    "pay",
    "back",
    "case",
    "though",
    "need",
    "test",
    "data",
    "see",
    "good",
    "sample",
    "good",
    "tree",
    "predicting",
    "loan",
    "payments",
    "finally",
    "since",
    "anaconda",
    "jupiter",
    "notebook",
    "works",
    "command",
    "line",
    "python",
    "simply",
    "put",
    "predict",
    "e",
    "print",
    "could",
    "easily",
    "put",
    "print",
    "put",
    "brackets",
    "around",
    "predict",
    "en",
    "print",
    "go",
    "ahead",
    "matter",
    "way",
    "see",
    "right",
    "runs",
    "prediction",
    "roughly",
    "300",
    "remember",
    "30",
    "percent",
    "thousand",
    "300",
    "answers",
    "tells",
    "one",
    "lines",
    "test",
    "went",
    "predict",
    "came",
    "let",
    "move",
    "next",
    "step",
    "going",
    "take",
    "data",
    "try",
    "figure",
    "good",
    "model",
    "go",
    "since",
    "sklearn",
    "heavy",
    "lifting",
    "math",
    "simple",
    "line",
    "code",
    "let",
    "us",
    "know",
    "accuracy",
    "let",
    "go",
    "ahead",
    "go",
    "see",
    "means",
    "looks",
    "like",
    "let",
    "go",
    "ahead",
    "paste",
    "let",
    "zoom",
    "little",
    "bit",
    "go",
    "nice",
    "full",
    "picture",
    "see",
    "going",
    "print",
    "accuracy",
    "accuracy",
    "score",
    "something",
    "imported",
    "earlier",
    "remember",
    "beginning",
    "let",
    "scroll",
    "real",
    "quick",
    "see",
    "coming",
    "coming",
    "import",
    "accuracy",
    "score",
    "could",
    "probably",
    "run",
    "script",
    "make",
    "script",
    "easily",
    "accurate",
    "many",
    "300",
    "get",
    "right",
    "put",
    "test",
    "one",
    "ran",
    "predict",
    "put",
    "predict",
    "e",
    "n",
    "answers",
    "got",
    "going",
    "multiply",
    "hundred",
    "going",
    "give",
    "us",
    "answer",
    "decimal",
    "want",
    "see",
    "percentage",
    "let",
    "run",
    "see",
    "looks",
    "like",
    "see",
    "got",
    "accuracy",
    "look",
    "number",
    "loans",
    "look",
    "good",
    "model",
    "fit",
    "tell",
    "people",
    "fitting",
    "quick",
    "recap",
    "accuracy",
    "set",
    "created",
    "model",
    "uses",
    "decision",
    "tree",
    "algorithm",
    "predict",
    "whether",
    "customer",
    "repay",
    "loan",
    "accuracy",
    "model",
    "percent",
    "bank",
    "use",
    "model",
    "decide",
    "whether",
    "approve",
    "loan",
    "request",
    "particular",
    "customer",
    "information",
    "really",
    "powerful",
    "may",
    "able",
    "individuals",
    "understand",
    "numbers",
    "thousands",
    "numbers",
    "come",
    "see",
    "smart",
    "decision",
    "bank",
    "use",
    "tool",
    "like",
    "help",
    "predict",
    "good",
    "profit",
    "going",
    "loan",
    "balances",
    "many",
    "going",
    "default",
    "random",
    "forest",
    "work",
    "whole",
    "begin",
    "random",
    "forest",
    "classifier",
    "let",
    "say",
    "already",
    "built",
    "three",
    "trees",
    "going",
    "start",
    "first",
    "tree",
    "looks",
    "like",
    "like",
    "example",
    "tree",
    "looks",
    "diameter",
    "greater",
    "equal",
    "3",
    "true",
    "otherwise",
    "false",
    "one",
    "side",
    "goes",
    "smaller",
    "diameter",
    "one",
    "side",
    "goes",
    "larger",
    "diameter",
    "color",
    "orange",
    "going",
    "go",
    "right",
    "true",
    "using",
    "oranges",
    "instead",
    "lemons",
    "red",
    "going",
    "go",
    "left",
    "false",
    "build",
    "second",
    "tree",
    "similar",
    "split",
    "differently",
    "instead",
    "first",
    "one",
    "split",
    "diameter",
    "one",
    "created",
    "look",
    "first",
    "bowl",
    "lot",
    "red",
    "objects",
    "says",
    "color",
    "red",
    "going",
    "bring",
    "entropy",
    "fastest",
    "course",
    "true",
    "goes",
    "left",
    "false",
    "goes",
    "right",
    "looks",
    "shape",
    "false",
    "true",
    "tree",
    "three",
    "diameter",
    "equal",
    "one",
    "came",
    "lot",
    "cherries",
    "bowl",
    "would",
    "biggest",
    "split",
    "diameter",
    "equal",
    "one",
    "going",
    "drop",
    "entropy",
    "quickest",
    "see",
    "splits",
    "true",
    "goes",
    "false",
    "added",
    "another",
    "category",
    "grow",
    "summer",
    "false",
    "goes",
    "left",
    "true",
    "goes",
    "right",
    "let",
    "go",
    "ahead",
    "bring",
    "three",
    "trees",
    "see",
    "one",
    "image",
    "would",
    "three",
    "completely",
    "different",
    "trees",
    "categorizing",
    "fruit",
    "let",
    "take",
    "fruit",
    "let",
    "try",
    "fruit",
    "look",
    "blackened",
    "ca",
    "see",
    "color",
    "missing",
    "data",
    "remember",
    "one",
    "things",
    "talked",
    "earlier",
    "random",
    "forest",
    "works",
    "really",
    "good",
    "missing",
    "data",
    "missing",
    "pieces",
    "fruit",
    "image",
    "maybe",
    "person",
    "black",
    "white",
    "camera",
    "took",
    "picture",
    "going",
    "take",
    "look",
    "going",
    "put",
    "color",
    "ignore",
    "color",
    "diameter",
    "equals",
    "3",
    "find",
    "grows",
    "summer",
    "equals",
    "yes",
    "shape",
    "circle",
    "go",
    "right",
    "look",
    "one",
    "decision",
    "trees",
    "third",
    "one",
    "diameter",
    "greater",
    "equal",
    "three",
    "color",
    "orange",
    "well",
    "really",
    "know",
    "one",
    "look",
    "value",
    "true",
    "go",
    "right",
    "tree",
    "two",
    "classifies",
    "cherries",
    "color",
    "equal",
    "red",
    "shape",
    "circle",
    "true",
    "circle",
    "would",
    "look",
    "say",
    "oh",
    "cherry",
    "go",
    "classifier",
    "says",
    "diameter",
    "equal",
    "one",
    "well",
    "false",
    "grow",
    "summer",
    "true",
    "goes",
    "looks",
    "oranges",
    "random",
    "forest",
    "work",
    "first",
    "one",
    "says",
    "orange",
    "second",
    "one",
    "said",
    "cherry",
    "third",
    "one",
    "says",
    "orange",
    "guess",
    "two",
    "oranges",
    "one",
    "says",
    "cherry",
    "add",
    "together",
    "majority",
    "vote",
    "says",
    "orange",
    "answer",
    "classified",
    "orange",
    "even",
    "though",
    "know",
    "color",
    "missing",
    "data",
    "know",
    "getting",
    "tired",
    "fruit",
    "let",
    "switch",
    "promise",
    "start",
    "looking",
    "case",
    "example",
    "get",
    "python",
    "coding",
    "today",
    "going",
    "use",
    "case",
    "iris",
    "flower",
    "analysis",
    "exciting",
    "part",
    "roll",
    "sleeves",
    "actually",
    "look",
    "python",
    "coding",
    "start",
    "python",
    "coding",
    "need",
    "go",
    "ahead",
    "create",
    "problem",
    "statement",
    "wonder",
    "species",
    "iris",
    "flowers",
    "belong",
    "let",
    "try",
    "predict",
    "species",
    "flowers",
    "using",
    "machine",
    "learning",
    "python",
    "let",
    "see",
    "done",
    "begin",
    "go",
    "ahead",
    "implement",
    "python",
    "code",
    "find",
    "first",
    "half",
    "implementation",
    "organizing",
    "exploring",
    "data",
    "coming",
    "let",
    "go",
    "ahead",
    "take",
    "first",
    "step",
    "loading",
    "different",
    "modules",
    "python",
    "let",
    "go",
    "ahead",
    "put",
    "favorite",
    "editor",
    "whatever",
    "favorite",
    "editor",
    "case",
    "going",
    "using",
    "anaconda",
    "jupiter",
    "notebook",
    "one",
    "favorites",
    "certainly",
    "notepad",
    "plus",
    "plus",
    "eclipse",
    "dozens",
    "others",
    "even",
    "using",
    "python",
    "terminal",
    "window",
    "work",
    "fine",
    "go",
    "ahead",
    "explore",
    "python",
    "coding",
    "go",
    "let",
    "go",
    "ahead",
    "flip",
    "jupiter",
    "notebook",
    "already",
    "opened",
    "new",
    "page",
    "python",
    "3",
    "code",
    "going",
    "paste",
    "right",
    "let",
    "take",
    "look",
    "see",
    "bringing",
    "python",
    "first",
    "thing",
    "going",
    "import",
    "load",
    "iris",
    "actual",
    "data",
    "module",
    "allows",
    "us",
    "bring",
    "data",
    "load",
    "iris",
    "iris",
    "popular",
    "around",
    "since",
    "1936",
    "ronald",
    "fisher",
    "published",
    "paper",
    "measuring",
    "different",
    "parts",
    "flower",
    "based",
    "measurements",
    "predicting",
    "kind",
    "flower",
    "going",
    "random",
    "forest",
    "classifier",
    "need",
    "go",
    "ahead",
    "import",
    "random",
    "forest",
    "classifier",
    "sk",
    "learn",
    "module",
    "dot",
    "ensemble",
    "import",
    "random",
    "force",
    "classifier",
    "want",
    "bring",
    "two",
    "modules",
    "probably",
    "commonly",
    "used",
    "modules",
    "python",
    "data",
    "science",
    "modules",
    "bring",
    "one",
    "going",
    "pandas",
    "going",
    "import",
    "pandas",
    "pd",
    "pd",
    "common",
    "term",
    "used",
    "pandas",
    "pandas",
    "basically",
    "creates",
    "data",
    "format",
    "us",
    "create",
    "pandas",
    "data",
    "frame",
    "looks",
    "like",
    "excel",
    "spreadsheet",
    "see",
    "minute",
    "start",
    "digging",
    "deeper",
    "code",
    "panda",
    "wonderful",
    "plays",
    "nice",
    "modules",
    "numpy",
    "numbers",
    "python",
    "numbers",
    "python",
    "allows",
    "us",
    "different",
    "mathematical",
    "sets",
    "see",
    "right",
    "bat",
    "going",
    "take",
    "np",
    "going",
    "go",
    "ahead",
    "seed",
    "randomness",
    "seating",
    "code",
    "actually",
    "show",
    "anything",
    "going",
    "go",
    "ahead",
    "run",
    "need",
    "make",
    "sure",
    "loaded",
    "let",
    "take",
    "look",
    "next",
    "module",
    "next",
    "six",
    "slides",
    "including",
    "one",
    "exploring",
    "data",
    "remember",
    "told",
    "half",
    "looking",
    "data",
    "getting",
    "set",
    "let",
    "go",
    "ahead",
    "take",
    "code",
    "right",
    "script",
    "let",
    "get",
    "jupyter",
    "notebook",
    "go",
    "gone",
    "ahead",
    "run",
    "imports",
    "going",
    "paste",
    "code",
    "let",
    "take",
    "look",
    "see",
    "going",
    "first",
    "thing",
    "actually",
    "loading",
    "iris",
    "data",
    "remember",
    "loaded",
    "module",
    "tells",
    "get",
    "iris",
    "data",
    "actually",
    "assigning",
    "data",
    "variable",
    "iris",
    "going",
    "go",
    "ahead",
    "use",
    "df",
    "define",
    "data",
    "frame",
    "going",
    "equal",
    "pd",
    "remember",
    "pandas",
    "pd",
    "pandas",
    "panda",
    "data",
    "frame",
    "looking",
    "iris",
    "data",
    "columns",
    "equals",
    "iris",
    "feature",
    "names",
    "going",
    "df",
    "head",
    "let",
    "run",
    "understand",
    "going",
    "first",
    "thing",
    "want",
    "notice",
    "df",
    "created",
    "looks",
    "like",
    "excel",
    "spreadsheet",
    "excel",
    "spreadsheet",
    "set",
    "columns",
    "top",
    "see",
    "four",
    "different",
    "columns",
    "data",
    "little",
    "confusing",
    "without",
    "knowing",
    "data",
    "coming",
    "let",
    "look",
    "bigger",
    "picture",
    "going",
    "go",
    "print",
    "going",
    "change",
    "moment",
    "going",
    "print",
    "iris",
    "see",
    "looks",
    "like",
    "print",
    "iris",
    "get",
    "long",
    "list",
    "information",
    "scroll",
    "see",
    "different",
    "titles",
    "important",
    "notice",
    "first",
    "brackets",
    "beginning",
    "python",
    "dictionary",
    "python",
    "dictionary",
    "key",
    "label",
    "label",
    "pulls",
    "whatever",
    "information",
    "comes",
    "feature",
    "names",
    "actually",
    "used",
    "columns",
    "equal",
    "array",
    "sepal",
    "length",
    "sepal",
    "width",
    "petal",
    "length",
    "petal",
    "width",
    "different",
    "names",
    "four",
    "different",
    "columns",
    "scroll",
    "far",
    "enough",
    "also",
    "see",
    "data",
    "oh",
    "goodness",
    "came",
    "right",
    "towards",
    "top",
    "data",
    "equal",
    "different",
    "data",
    "looking",
    "lot",
    "things",
    "like",
    "target",
    "going",
    "pulling",
    "minute",
    "also",
    "names",
    "target",
    "names",
    "show",
    "also",
    "minute",
    "let",
    "go",
    "ahead",
    "set",
    "back",
    "head",
    "one",
    "neat",
    "features",
    "pandas",
    "panda",
    "data",
    "frames",
    "print",
    "first",
    "five",
    "lines",
    "data",
    "set",
    "along",
    "headers",
    "case",
    "column",
    "header",
    "set",
    "iris",
    "features",
    "see",
    "zero",
    "one",
    "two",
    "three",
    "four",
    "python",
    "arrays",
    "always",
    "start",
    "zero",
    "look",
    "first",
    "five",
    "going",
    "zero",
    "one",
    "two",
    "three",
    "four",
    "one",
    "two",
    "three",
    "four",
    "five",
    "got",
    "iris",
    "data",
    "imported",
    "data",
    "frame",
    "let",
    "take",
    "look",
    "next",
    "piece",
    "code",
    "section",
    "code",
    "going",
    "take",
    "look",
    "target",
    "let",
    "go",
    "ahead",
    "get",
    "notebook",
    "piece",
    "code",
    "discuss",
    "little",
    "bit",
    "detail",
    "jupyter",
    "notebook",
    "going",
    "put",
    "code",
    "run",
    "want",
    "look",
    "couple",
    "things",
    "going",
    "df",
    "species",
    "interesting",
    "right",
    "see",
    "df",
    "species",
    "brackets",
    "uh",
    "key",
    "code",
    "creating",
    "another",
    "column",
    "pandas",
    "setup",
    "pandas",
    "either",
    "one",
    "could",
    "easily",
    "done",
    "iris",
    "brackets",
    "target",
    "depending",
    "working",
    "acceptable",
    "let",
    "go",
    "ahead",
    "run",
    "code",
    "see",
    "changes",
    "done",
    "added",
    "target",
    "iris",
    "data",
    "set",
    "another",
    "column",
    "end",
    "species",
    "trying",
    "predict",
    "data",
    "tells",
    "us",
    "answer",
    "different",
    "pieces",
    "added",
    "column",
    "answer",
    "way",
    "final",
    "setup",
    "ability",
    "program",
    "neural",
    "network",
    "look",
    "different",
    "data",
    "know",
    "setosa",
    "vera",
    "color",
    "see",
    "minute",
    "virginica",
    "three",
    "going",
    "add",
    "one",
    "column",
    "know",
    "organizing",
    "data",
    "kind",
    "fun",
    "lot",
    "ways",
    "organize",
    "nice",
    "putting",
    "everything",
    "onto",
    "one",
    "data",
    "frame",
    "print",
    "shows",
    "exactly",
    "looking",
    "show",
    "different",
    "alter",
    "slightly",
    "differently",
    "let",
    "go",
    "ahead",
    "put",
    "script",
    "go",
    "going",
    "put",
    "going",
    "run",
    "let",
    "talk",
    "little",
    "bit",
    "exploring",
    "data",
    "one",
    "challenges",
    "knowing",
    "good",
    "model",
    "model",
    "work",
    "need",
    "split",
    "data",
    "split",
    "two",
    "different",
    "parts",
    "usually",
    "call",
    "training",
    "testing",
    "going",
    "go",
    "ahead",
    "put",
    "database",
    "see",
    "clearly",
    "set",
    "df",
    "remember",
    "put",
    "brackets",
    "creating",
    "another",
    "column",
    "train",
    "going",
    "use",
    "part",
    "training",
    "equals",
    "np",
    "remember",
    "stands",
    "generating",
    "random",
    "number",
    "0",
    "1",
    "going",
    "rows",
    "length",
    "df",
    "comes",
    "row",
    "gets",
    "generated",
    "number",
    "less",
    "true",
    "greater",
    "false",
    "means",
    "going",
    "take",
    "75",
    "data",
    "roughly",
    "randomness",
    "involved",
    "going",
    "use",
    "train",
    "25",
    "going",
    "hold",
    "side",
    "use",
    "test",
    "later",
    "let",
    "flip",
    "back",
    "see",
    "next",
    "step",
    "labeled",
    "database",
    "training",
    "testing",
    "let",
    "go",
    "ahead",
    "sort",
    "two",
    "different",
    "variables",
    "train",
    "test",
    "let",
    "take",
    "code",
    "let",
    "bring",
    "project",
    "go",
    "let",
    "paste",
    "run",
    "let",
    "take",
    "quick",
    "look",
    "going",
    "created",
    "remember",
    "def",
    "dot",
    "head",
    "prints",
    "first",
    "five",
    "rows",
    "added",
    "column",
    "train",
    "end",
    "going",
    "take",
    "going",
    "create",
    "two",
    "variables",
    "going",
    "create",
    "two",
    "new",
    "data",
    "frames",
    "one",
    "called",
    "train",
    "one",
    "called",
    "test",
    "75",
    "train",
    "25",
    "percent",
    "test",
    "sort",
    "going",
    "df",
    "main",
    "original",
    "data",
    "frame",
    "iris",
    "data",
    "df",
    "trained",
    "equals",
    "true",
    "going",
    "go",
    "train",
    "df",
    "trained",
    "equals",
    "false",
    "goes",
    "test",
    "run",
    "going",
    "print",
    "number",
    "one",
    "let",
    "see",
    "looks",
    "like",
    "see",
    "puts",
    "118",
    "training",
    "module",
    "puts",
    "32",
    "testing",
    "module",
    "lets",
    "us",
    "know",
    "150",
    "lines",
    "data",
    "went",
    "looked",
    "original",
    "data",
    "could",
    "see",
    "150",
    "lines",
    "roughly",
    "75",
    "percent",
    "one",
    "25",
    "percent",
    "us",
    "test",
    "model",
    "afterward",
    "let",
    "jump",
    "back",
    "code",
    "see",
    "goes",
    "next",
    "two",
    "steps",
    "want",
    "one",
    "thing",
    "data",
    "make",
    "readable",
    "humans",
    "know",
    "hate",
    "looking",
    "zeros",
    "ones",
    "let",
    "start",
    "features",
    "let",
    "go",
    "ahead",
    "take",
    "make",
    "readable",
    "humans",
    "let",
    "put",
    "code",
    "let",
    "see",
    "go",
    "paste",
    "see",
    "done",
    "couple",
    "basic",
    "things",
    "know",
    "columns",
    "data",
    "frame",
    "panda",
    "thing",
    "df",
    "columns",
    "know",
    "first",
    "four",
    "zero",
    "one",
    "two",
    "three",
    "first",
    "four",
    "going",
    "features",
    "titles",
    "columns",
    "run",
    "see",
    "creates",
    "index",
    "sepa",
    "length",
    "sepa",
    "width",
    "pedal",
    "length",
    "pedal",
    "width",
    "familiar",
    "look",
    "column",
    "titles",
    "going",
    "across",
    "first",
    "four",
    "one",
    "thing",
    "want",
    "notice",
    "command",
    "line",
    "whether",
    "jupiter",
    "notebook",
    "running",
    "command",
    "line",
    "terminal",
    "window",
    "put",
    "name",
    "print",
    "print",
    "features",
    "shorthand",
    "put",
    "features",
    "actually",
    "writing",
    "code",
    "saving",
    "script",
    "running",
    "remote",
    "really",
    "need",
    "put",
    "print",
    "run",
    "see",
    "gives",
    "thing",
    "want",
    "go",
    "ahead",
    "leave",
    "features",
    "really",
    "matter",
    "one",
    "fun",
    "thing",
    "jupiter",
    "notebooks",
    "building",
    "code",
    "go",
    "need",
    "go",
    "ahead",
    "create",
    "labels",
    "part",
    "let",
    "take",
    "look",
    "see",
    "final",
    "step",
    "prepping",
    "data",
    "actually",
    "start",
    "running",
    "training",
    "testing",
    "going",
    "go",
    "ahead",
    "convert",
    "species",
    "something",
    "computer",
    "understands",
    "let",
    "put",
    "code",
    "script",
    "see",
    "takes",
    "us",
    "right",
    "go",
    "set",
    "equal",
    "pd",
    "dot",
    "factorize",
    "train",
    "species",
    "zero",
    "let",
    "break",
    "little",
    "bit",
    "pandas",
    "right",
    "pd",
    "factorize",
    "factorized",
    "going",
    "come",
    "back",
    "second",
    "let",
    "look",
    "train",
    "species",
    "looking",
    "group",
    "0",
    "let",
    "go",
    "species",
    "remember",
    "created",
    "whole",
    "column",
    "species",
    "setosa",
    "setosa",
    "setosa",
    "setosa",
    "scroll",
    "enough",
    "also",
    "see",
    "virginica",
    "vera",
    "color",
    "need",
    "convert",
    "something",
    "computer",
    "understands",
    "zeros",
    "ones",
    "train",
    "species",
    "zero",
    "format",
    "array",
    "arrays",
    "zero",
    "end",
    "species",
    "column",
    "factorize",
    "goes",
    "looks",
    "fact",
    "three",
    "run",
    "see",
    "generates",
    "array",
    "equal",
    "case",
    "training",
    "set",
    "zeros",
    "ones",
    "twos",
    "representing",
    "three",
    "different",
    "kinds",
    "flowers",
    "something",
    "computer",
    "understands",
    "nice",
    "table",
    "read",
    "understand",
    "finally",
    "get",
    "actually",
    "start",
    "predicting",
    "go",
    "uh",
    "two",
    "lines",
    "code",
    "oh",
    "goodness",
    "lot",
    "work",
    "get",
    "two",
    "lines",
    "code",
    "lot",
    "two",
    "lines",
    "code",
    "let",
    "take",
    "look",
    "see",
    "going",
    "put",
    "full",
    "script",
    "running",
    "let",
    "paste",
    "let",
    "take",
    "look",
    "see",
    "creating",
    "variable",
    "clf",
    "going",
    "set",
    "equal",
    "random",
    "forest",
    "classifier",
    "passing",
    "two",
    "variables",
    "lot",
    "variables",
    "play",
    "far",
    "two",
    "concerned",
    "standard",
    "jobs",
    "prioritize",
    "something",
    "really",
    "worry",
    "usually",
    "computer",
    "jobs",
    "equals",
    "two",
    "working",
    "larger",
    "big",
    "data",
    "need",
    "prioritize",
    "differently",
    "number",
    "changes",
    "priorities",
    "going",
    "run",
    "across",
    "system",
    "things",
    "like",
    "random",
    "state",
    "starts",
    "zero",
    "fine",
    "let",
    "go",
    "ahead",
    "run",
    "also",
    "clf",
    "dot",
    "fit",
    "train",
    "features",
    "comma",
    "run",
    "let",
    "talk",
    "little",
    "bit",
    "clf",
    "dot",
    "fit",
    "fitting",
    "training",
    "actually",
    "creating",
    "random",
    "forest",
    "classifier",
    "right",
    "code",
    "everything",
    "going",
    "take",
    "training",
    "set",
    "remember",
    "kept",
    "test",
    "side",
    "going",
    "take",
    "training",
    "set",
    "features",
    "going",
    "go",
    "ahead",
    "put",
    "target",
    "0",
    "1",
    "2",
    "created",
    "features",
    "actual",
    "data",
    "going",
    "put",
    "training",
    "set",
    "let",
    "go",
    "ahead",
    "run",
    "kind",
    "interesting",
    "thing",
    "printed",
    "random",
    "force",
    "classifier",
    "everything",
    "around",
    "running",
    "terminal",
    "window",
    "script",
    "like",
    "automatically",
    "treats",
    "us",
    "like",
    "like",
    "typed",
    "printed",
    "instead",
    "print",
    "thing",
    "treats",
    "variable",
    "prints",
    "actually",
    "running",
    "code",
    "would",
    "case",
    "printed",
    "shows",
    "us",
    "different",
    "variables",
    "change",
    "go",
    "actually",
    "see",
    "jobs",
    "equals",
    "two",
    "see",
    "random",
    "state",
    "equals",
    "zero",
    "two",
    "sent",
    "would",
    "really",
    "dig",
    "deep",
    "find",
    "different",
    "meanings",
    "different",
    "settings",
    "kind",
    "think",
    "little",
    "bit",
    "like",
    "max",
    "features",
    "auto",
    "features",
    "putting",
    "going",
    "automatically",
    "take",
    "four",
    "whatever",
    "send",
    "take",
    "might",
    "many",
    "features",
    "processing",
    "words",
    "might",
    "like",
    "million",
    "features",
    "legal",
    "documents",
    "many",
    "different",
    "words",
    "point",
    "probably",
    "want",
    "limit",
    "maximum",
    "features",
    "going",
    "process",
    "leaf",
    "nodes",
    "end",
    "nodes",
    "remember",
    "fruit",
    "talking",
    "leaf",
    "nodes",
    "like",
    "said",
    "lot",
    "looking",
    "lot",
    "stuff",
    "might",
    "case",
    "probably",
    "think",
    "three",
    "leaf",
    "nodes",
    "maybe",
    "four",
    "might",
    "thousands",
    "leaf",
    "nodes",
    "point",
    "need",
    "put",
    "cap",
    "say",
    "okay",
    "go",
    "far",
    "going",
    "use",
    "resources",
    "processing",
    "really",
    "limiting",
    "process",
    "making",
    "sure",
    "overwhelm",
    "system",
    "settings",
    "going",
    "go",
    "warm",
    "start",
    "equals",
    "false",
    "warm",
    "start",
    "programming",
    "one",
    "piece",
    "time",
    "externally",
    "since",
    "going",
    "like",
    "going",
    "continually",
    "train",
    "particular",
    "learning",
    "tree",
    "like",
    "said",
    "lot",
    "things",
    "want",
    "look",
    "detail",
    "sk",
    "learn",
    "digging",
    "deep",
    "running",
    "major",
    "project",
    "today",
    "though",
    "need",
    "fit",
    "train",
    "features",
    "target",
    "training",
    "model",
    "next",
    "going",
    "create",
    "model",
    "need",
    "test",
    "remember",
    "set",
    "aside",
    "test",
    "feature",
    "test",
    "group",
    "25",
    "data",
    "let",
    "go",
    "ahead",
    "take",
    "code",
    "let",
    "put",
    "script",
    "see",
    "looks",
    "like",
    "okay",
    "go",
    "going",
    "run",
    "going",
    "come",
    "bunch",
    "zeros",
    "ones",
    "twos",
    "represents",
    "three",
    "type",
    "flowers",
    "setosa",
    "virginica",
    "versacolor",
    "putting",
    "predict",
    "test",
    "features",
    "always",
    "kind",
    "like",
    "know",
    "looking",
    "real",
    "quick",
    "going",
    "test",
    "features",
    "remember",
    "features",
    "array",
    "sepal",
    "length",
    "simple",
    "width",
    "pedal",
    "length",
    "pedal",
    "width",
    "put",
    "way",
    "actually",
    "loads",
    "different",
    "columns",
    "loaded",
    "features",
    "features",
    "let",
    "features",
    "seeing",
    "features",
    "looks",
    "like",
    "playing",
    "pandas",
    "data",
    "frames",
    "see",
    "index",
    "put",
    "index",
    "like",
    "test",
    "features",
    "test",
    "takes",
    "columns",
    "creates",
    "panda",
    "data",
    "frames",
    "columns",
    "case",
    "going",
    "go",
    "ahead",
    "put",
    "predict",
    "going",
    "put",
    "one",
    "lines",
    "data",
    "going",
    "put",
    "going",
    "predict",
    "new",
    "forest",
    "classifier",
    "going",
    "come",
    "predicts",
    "predicts",
    "0",
    "0",
    "0",
    "1",
    "2",
    "one",
    "one",
    "two",
    "two",
    "two",
    "uh",
    "flower",
    "type",
    "setosa",
    "virginica",
    "versacolor",
    "taken",
    "test",
    "features",
    "let",
    "explore",
    "let",
    "see",
    "exactly",
    "data",
    "means",
    "us",
    "first",
    "thing",
    "predicts",
    "actually",
    "generate",
    "different",
    "prediction",
    "model",
    "say",
    "different",
    "going",
    "view",
    "differently",
    "data",
    "different",
    "let",
    "take",
    "next",
    "piece",
    "code",
    "put",
    "script",
    "pasting",
    "see",
    "uh",
    "predict",
    "added",
    "underscore",
    "proba",
    "probability",
    "probability",
    "running",
    "like",
    "ran",
    "time",
    "going",
    "get",
    "slightly",
    "different",
    "result",
    "going",
    "look",
    "first",
    "see",
    "instead",
    "looking",
    "27",
    "see",
    "right",
    "generates",
    "much",
    "larger",
    "field",
    "probability",
    "let",
    "take",
    "look",
    "see",
    "looks",
    "like",
    "means",
    "predict",
    "underscore",
    "prabha",
    "probability",
    "generates",
    "three",
    "numbers",
    "three",
    "leaf",
    "nodes",
    "end",
    "remember",
    "theory",
    "predictors",
    "first",
    "one",
    "predicting",
    "one",
    "setosa",
    "predicts",
    "0",
    "virginica",
    "predicts",
    "0",
    "versa",
    "color",
    "let",
    "um",
    "know",
    "going",
    "change",
    "little",
    "bit",
    "let",
    "look",
    "10",
    "20",
    "start",
    "get",
    "little",
    "different",
    "data",
    "see",
    "right",
    "gets",
    "one",
    "line",
    "right",
    "line",
    "0",
    "going",
    "vote",
    "two",
    "equal",
    "votes",
    "gon",
    "na",
    "go",
    "first",
    "one",
    "says",
    "uh",
    "setosa",
    "gets",
    "zero",
    "votes",
    "virginica",
    "gets",
    "point",
    "five",
    "votes",
    "versacolor",
    "gets",
    "point",
    "five",
    "votes",
    "let",
    "go",
    "virginica",
    "since",
    "two",
    "equal",
    "list",
    "see",
    "vary",
    "looked",
    "basic",
    "predict",
    "features",
    "looked",
    "predict",
    "probability",
    "let",
    "see",
    "next",
    "want",
    "go",
    "ahead",
    "start",
    "mapping",
    "names",
    "plants",
    "want",
    "attach",
    "names",
    "makes",
    "little",
    "sense",
    "us",
    "going",
    "next",
    "two",
    "steps",
    "going",
    "start",
    "setting",
    "predictions",
    "mapping",
    "name",
    "let",
    "see",
    "looks",
    "like",
    "let",
    "go",
    "ahead",
    "paste",
    "code",
    "run",
    "goes",
    "along",
    "next",
    "piece",
    "code",
    "skip",
    "quickly",
    "come",
    "back",
    "little",
    "bit",
    "iris",
    "dot",
    "target",
    "names",
    "uh",
    "remember",
    "correctly",
    "names",
    "talking",
    "whole",
    "time",
    "setosa",
    "virginica",
    "versus",
    "color",
    "going",
    "go",
    "ahead",
    "prediction",
    "run",
    "could",
    "set",
    "variable",
    "equal",
    "instead",
    "time",
    "going",
    "run",
    "clf",
    "dot",
    "predict",
    "test",
    "features",
    "remember",
    "returns",
    "zeros",
    "ones",
    "twos",
    "going",
    "set",
    "equal",
    "predictions",
    "time",
    "actually",
    "putting",
    "variable",
    "run",
    "distributes",
    "comes",
    "array",
    "array",
    "setosa",
    "satosa",
    "satosa",
    "satosa",
    "setosa",
    "looking",
    "first",
    "five",
    "could",
    "actually",
    "let",
    "first",
    "25",
    "see",
    "little",
    "bit",
    "see",
    "starts",
    "mapping",
    "different",
    "flower",
    "types",
    "versa",
    "color",
    "virginica",
    "let",
    "see",
    "goes",
    "next",
    "one",
    "let",
    "take",
    "look",
    "top",
    "part",
    "species",
    "take",
    "code",
    "put",
    "script",
    "let",
    "put",
    "paste",
    "go",
    "go",
    "ahead",
    "run",
    "let",
    "talk",
    "sections",
    "code",
    "go",
    "together",
    "first",
    "one",
    "predictions",
    "went",
    "ahead",
    "predictions",
    "25",
    "let",
    "five",
    "setosa",
    "satoshi",
    "satosa",
    "satoshi",
    "predicting",
    "test",
    "model",
    "come",
    "look",
    "test",
    "species",
    "remember",
    "could",
    "done",
    "test",
    "dot",
    "species",
    "dot",
    "head",
    "see",
    "says",
    "setosa",
    "satosa",
    "setosa",
    "setosa",
    "match",
    "first",
    "one",
    "forest",
    "second",
    "one",
    "actual",
    "data",
    "need",
    "combine",
    "understand",
    "means",
    "need",
    "know",
    "good",
    "forest",
    "good",
    "predicting",
    "features",
    "come",
    "next",
    "step",
    "lots",
    "fun",
    "going",
    "use",
    "single",
    "line",
    "code",
    "combine",
    "predictions",
    "actuals",
    "nice",
    "chart",
    "look",
    "let",
    "go",
    "ahead",
    "put",
    "script",
    "jupiter",
    "notebook",
    "let",
    "see",
    "let",
    "go",
    "ahead",
    "paste",
    "gon",
    "na",
    "jupiter",
    "notebook",
    "control",
    "minus",
    "see",
    "whole",
    "line",
    "go",
    "resize",
    "let",
    "take",
    "look",
    "see",
    "going",
    "gon",
    "na",
    "create",
    "pandas",
    "remember",
    "pd",
    "stands",
    "pandas",
    "cross",
    "tab",
    "function",
    "takes",
    "two",
    "sets",
    "data",
    "creates",
    "chart",
    "run",
    "get",
    "nice",
    "chart",
    "predicted",
    "species",
    "across",
    "top",
    "see",
    "setosa",
    "versus",
    "color",
    "virginica",
    "actual",
    "species",
    "setosa",
    "versacolor",
    "virginica",
    "way",
    "read",
    "chart",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "read",
    "chart",
    "read",
    "chart",
    "setosa",
    "meet",
    "versus",
    "color",
    "meet",
    "virginica",
    "meet",
    "meeting",
    "actual",
    "predicted",
    "agree",
    "number",
    "accurate",
    "predictions",
    "case",
    "equals",
    "13",
    "plus",
    "5",
    "plus",
    "12",
    "get",
    "notice",
    "says",
    "virginica",
    "supposed",
    "versacolor",
    "inaccurate",
    "two",
    "two",
    "inaccurate",
    "predictions",
    "30",
    "accurate",
    "predictions",
    "say",
    "model",
    "accuracy",
    "93",
    "30",
    "divided",
    "32",
    "multiply",
    "hundred",
    "say",
    "93",
    "percent",
    "accurate",
    "93",
    "percent",
    "accuracy",
    "model",
    "want",
    "add",
    "one",
    "quick",
    "thing",
    "scripting",
    "wrap",
    "let",
    "flip",
    "back",
    "script",
    "going",
    "take",
    "line",
    "code",
    "know",
    "remember",
    "predix",
    "equals",
    "iris",
    "dot",
    "target",
    "underscore",
    "names",
    "going",
    "map",
    "names",
    "going",
    "run",
    "prediction",
    "read",
    "test",
    "features",
    "know",
    "testing",
    "want",
    "actually",
    "deploy",
    "point",
    "would",
    "go",
    "ahead",
    "change",
    "array",
    "arrays",
    "really",
    "important",
    "running",
    "know",
    "need",
    "double",
    "brackets",
    "could",
    "actually",
    "create",
    "data",
    "maybe",
    "let",
    "two",
    "flowers",
    "maybe",
    "processing",
    "data",
    "coming",
    "put",
    "two",
    "flowers",
    "actually",
    "want",
    "see",
    "answer",
    "let",
    "go",
    "ahead",
    "type",
    "preds",
    "print",
    "run",
    "see",
    "predicted",
    "two",
    "flowers",
    "maybe",
    "measured",
    "front",
    "yard",
    "versacolor",
    "versacolor",
    "surprising",
    "since",
    "put",
    "data",
    "one",
    "would",
    "actual",
    "end",
    "product",
    "going",
    "used",
    "data",
    "know",
    "answer",
    "going",
    "conclude",
    "scripting",
    "part",
    "today",
    "going",
    "cover",
    "k",
    "nearest",
    "neighbors",
    "let",
    "refer",
    "k",
    "n",
    "n",
    "k",
    "n",
    "n",
    "really",
    "fundamental",
    "place",
    "start",
    "machine",
    "learning",
    "basis",
    "lot",
    "things",
    "logic",
    "behind",
    "easy",
    "understand",
    "incorporated",
    "forms",
    "machine",
    "learning",
    "today",
    "need",
    "k",
    "n",
    "n",
    "k",
    "n",
    "choose",
    "factor",
    "k",
    "use",
    "k",
    "n",
    "n",
    "k",
    "n",
    "algorithm",
    "work",
    "dive",
    "favorite",
    "part",
    "use",
    "case",
    "predict",
    "whether",
    "person",
    "diabetes",
    "common",
    "popular",
    "used",
    "data",
    "set",
    "far",
    "testing",
    "models",
    "learning",
    "use",
    "different",
    "models",
    "machine",
    "learning",
    "know",
    "machine",
    "learning",
    "models",
    "make",
    "predictions",
    "learning",
    "past",
    "data",
    "available",
    "input",
    "values",
    "machine",
    "learning",
    "model",
    "builds",
    "inputs",
    "already",
    "know",
    "use",
    "create",
    "predicted",
    "output",
    "dog",
    "little",
    "kid",
    "looking",
    "watching",
    "black",
    "cat",
    "cross",
    "path",
    "dear",
    "differentiate",
    "cat",
    "dog",
    "based",
    "characteristics",
    "cats",
    "cats",
    "sharp",
    "claws",
    "uses",
    "climb",
    "smaller",
    "lengths",
    "ears",
    "meows",
    "purrs",
    "love",
    "play",
    "around",
    "dogs",
    "dull",
    "claws",
    "bigger",
    "length",
    "ears",
    "barks",
    "loves",
    "run",
    "around",
    "usually",
    "see",
    "cat",
    "running",
    "around",
    "people",
    "although",
    "cat",
    "dogs",
    "look",
    "say",
    "evaluate",
    "sharpness",
    "claws",
    "sharper",
    "claws",
    "evaluate",
    "length",
    "ears",
    "usually",
    "sort",
    "cats",
    "dogs",
    "based",
    "even",
    "two",
    "characteristics",
    "tell",
    "cat",
    "dog",
    "question",
    "usually",
    "little",
    "kids",
    "know",
    "cats",
    "dogs",
    "unless",
    "live",
    "place",
    "many",
    "cats",
    "dogs",
    "look",
    "sharpness",
    "claws",
    "length",
    "ears",
    "see",
    "cat",
    "smaller",
    "ears",
    "sharper",
    "claws",
    "animals",
    "features",
    "like",
    "cats",
    "must",
    "cat",
    "sharp",
    "claws",
    "length",
    "ears",
    "goes",
    "cat",
    "group",
    "knn",
    "based",
    "feature",
    "similarity",
    "classification",
    "using",
    "knn",
    "classifier",
    "input",
    "value",
    "picture",
    "black",
    "cat",
    "goes",
    "trained",
    "model",
    "predicts",
    "cat",
    "coming",
    "knn",
    "k",
    "n",
    "algorithm",
    "k",
    "nearest",
    "neighbors",
    "stands",
    "one",
    "simplest",
    "supervised",
    "machine",
    "learning",
    "algorithms",
    "mostly",
    "used",
    "classification",
    "want",
    "know",
    "dog",
    "dog",
    "cat",
    "cat",
    "classifies",
    "data",
    "point",
    "based",
    "neighbors",
    "classified",
    "knn",
    "stores",
    "available",
    "cases",
    "classifies",
    "new",
    "cases",
    "based",
    "similarity",
    "measure",
    "gone",
    "cats",
    "dogs",
    "right",
    "wine",
    "another",
    "favorite",
    "mine",
    "k",
    "n",
    "stores",
    "available",
    "cases",
    "classifies",
    "new",
    "cases",
    "based",
    "similarity",
    "measure",
    "see",
    "measurement",
    "sulfur",
    "dioxide",
    "versus",
    "chloride",
    "level",
    "different",
    "wines",
    "tested",
    "fall",
    "graph",
    "based",
    "much",
    "sulfur",
    "dioxide",
    "much",
    "chloride",
    "k",
    "k",
    "n",
    "perimeter",
    "refers",
    "number",
    "nearest",
    "neighbors",
    "include",
    "majority",
    "voting",
    "process",
    "add",
    "new",
    "glass",
    "wine",
    "red",
    "white",
    "want",
    "know",
    "neighbors",
    "case",
    "going",
    "put",
    "k",
    "equals",
    "talk",
    "k",
    "minute",
    "data",
    "point",
    "classified",
    "majority",
    "votes",
    "five",
    "nearest",
    "neighbors",
    "unknown",
    "point",
    "would",
    "classified",
    "red",
    "since",
    "four",
    "five",
    "neighbors",
    "red",
    "choose",
    "k",
    "know",
    "k",
    "equals",
    "5",
    "mean",
    "value",
    "put",
    "going",
    "talk",
    "choose",
    "factor",
    "k",
    "k",
    "n",
    "algorithm",
    "based",
    "feature",
    "similarity",
    "choosing",
    "right",
    "value",
    "k",
    "process",
    "called",
    "parameter",
    "tuning",
    "important",
    "better",
    "accuracy",
    "k",
    "equals",
    "three",
    "classify",
    "question",
    "mark",
    "middle",
    "either",
    "square",
    "square",
    "case",
    "triangle",
    "set",
    "k",
    "equals",
    "three",
    "going",
    "look",
    "three",
    "nearest",
    "neighbors",
    "going",
    "say",
    "square",
    "put",
    "k",
    "equals",
    "7",
    "classify",
    "triangle",
    "depending",
    "data",
    "around",
    "see",
    "k",
    "changes",
    "depending",
    "point",
    "drastically",
    "changes",
    "answer",
    "jump",
    "go",
    "choose",
    "factor",
    "k",
    "find",
    "machine",
    "learning",
    "choosing",
    "factors",
    "face",
    "get",
    "like",
    "oh",
    "gosh",
    "choose",
    "right",
    "k",
    "set",
    "right",
    "values",
    "whatever",
    "machine",
    "learning",
    "tool",
    "looking",
    "huge",
    "bias",
    "one",
    "direction",
    "terms",
    "k",
    "n",
    "n",
    "number",
    "k",
    "choose",
    "low",
    "bias",
    "based",
    "two",
    "noises",
    "right",
    "next",
    "couple",
    "things",
    "going",
    "pick",
    "things",
    "might",
    "get",
    "skewed",
    "answer",
    "k",
    "big",
    "going",
    "take",
    "forever",
    "process",
    "going",
    "run",
    "processing",
    "issues",
    "resource",
    "issues",
    "common",
    "use",
    "options",
    "choosing",
    "k",
    "use",
    "square",
    "root",
    "n",
    "total",
    "number",
    "values",
    "take",
    "square",
    "root",
    "cases",
    "also",
    "even",
    "number",
    "using",
    "uh",
    "like",
    "case",
    "squares",
    "triangles",
    "even",
    "want",
    "make",
    "k",
    "value",
    "odd",
    "helps",
    "select",
    "better",
    "words",
    "going",
    "balance",
    "two",
    "different",
    "factors",
    "equal",
    "usually",
    "take",
    "square",
    "root",
    "n",
    "even",
    "add",
    "one",
    "subtract",
    "one",
    "get",
    "k",
    "value",
    "common",
    "use",
    "pretty",
    "solid",
    "works",
    "well",
    "use",
    "knn",
    "use",
    "knn",
    "data",
    "labeled",
    "need",
    "label",
    "know",
    "group",
    "pictures",
    "dogs",
    "dogs",
    "cats",
    "cats",
    "data",
    "noise",
    "free",
    "see",
    "class",
    "like",
    "underweight",
    "140",
    "23",
    "hello",
    "kitty",
    "normal",
    "pretty",
    "confusing",
    "high",
    "variety",
    "data",
    "coming",
    "noisy",
    "would",
    "cause",
    "issue",
    "data",
    "set",
    "small",
    "usually",
    "working",
    "smaller",
    "data",
    "sets",
    "might",
    "get",
    "gig",
    "data",
    "really",
    "clean",
    "lot",
    "noise",
    "k",
    "n",
    "lazy",
    "learner",
    "learn",
    "discriminative",
    "function",
    "training",
    "set",
    "lazy",
    "complicated",
    "data",
    "large",
    "amount",
    "going",
    "use",
    "knn",
    "really",
    "great",
    "get",
    "place",
    "start",
    "even",
    "large",
    "data",
    "sort",
    "small",
    "sample",
    "get",
    "idea",
    "looks",
    "like",
    "using",
    "knn",
    "also",
    "using",
    "smaller",
    "data",
    "sets",
    "k",
    "n",
    "works",
    "really",
    "good",
    "k",
    "n",
    "algorithm",
    "work",
    "consider",
    "data",
    "set",
    "two",
    "variables",
    "height",
    "centimeters",
    "weight",
    "kilograms",
    "point",
    "classified",
    "normal",
    "underweight",
    "see",
    "right",
    "two",
    "variables",
    "know",
    "true",
    "false",
    "either",
    "normal",
    "underweight",
    "basis",
    "given",
    "data",
    "classify",
    "set",
    "normal",
    "underweight",
    "using",
    "knn",
    "new",
    "data",
    "coming",
    "says",
    "57",
    "kilograms",
    "177",
    "centimeters",
    "going",
    "normal",
    "underweight",
    "find",
    "nearest",
    "neighbors",
    "calculate",
    "euclidean",
    "distance",
    "according",
    "euclidean",
    "distance",
    "formula",
    "distance",
    "two",
    "points",
    "plane",
    "coordinates",
    "x",
    "b",
    "given",
    "distance",
    "equals",
    "square",
    "root",
    "x",
    "minus",
    "squared",
    "plus",
    "minus",
    "b",
    "squared",
    "remember",
    "two",
    "edges",
    "triangle",
    "computing",
    "third",
    "edge",
    "since",
    "know",
    "x",
    "side",
    "side",
    "let",
    "calculate",
    "understand",
    "clearly",
    "unknown",
    "point",
    "placed",
    "red",
    "points",
    "data",
    "scattered",
    "around",
    "distance",
    "d1",
    "square",
    "root",
    "170",
    "minus",
    "167",
    "squared",
    "plus",
    "57",
    "minus",
    "51",
    "squared",
    "distance",
    "2",
    "distance",
    "3",
    "similarly",
    "calculate",
    "euclidean",
    "distance",
    "unknown",
    "data",
    "point",
    "points",
    "data",
    "set",
    "dealing",
    "small",
    "amount",
    "data",
    "hard",
    "actually",
    "pretty",
    "quick",
    "computer",
    "really",
    "complicated",
    "mass",
    "see",
    "close",
    "data",
    "based",
    "euclidean",
    "distance",
    "hence",
    "calculated",
    "euclidean",
    "distance",
    "unknown",
    "data",
    "point",
    "points",
    "shown",
    "x1",
    "y1",
    "equal",
    "57",
    "170",
    "whose",
    "class",
    "classify",
    "looking",
    "saying",
    "well",
    "euclidean",
    "distance",
    "going",
    "closest",
    "neighbors",
    "let",
    "calculate",
    "nearest",
    "neighbor",
    "k",
    "equals",
    "three",
    "see",
    "three",
    "closest",
    "neighbors",
    "puts",
    "normal",
    "pretty",
    "look",
    "graph",
    "pretty",
    "easy",
    "say",
    "okay",
    "voting",
    "normal",
    "normal",
    "normal",
    "three",
    "votes",
    "normal",
    "going",
    "normal",
    "weight",
    "majority",
    "neighbors",
    "pointing",
    "towards",
    "normal",
    "hence",
    "per",
    "k",
    "n",
    "algorithm",
    "class",
    "57",
    "170",
    "normal",
    "recap",
    "knn",
    "positive",
    "integer",
    "k",
    "specified",
    "along",
    "new",
    "sample",
    "select",
    "k",
    "entries",
    "database",
    "closest",
    "new",
    "sample",
    "find",
    "common",
    "classification",
    "entries",
    "classification",
    "give",
    "new",
    "sample",
    "see",
    "pretty",
    "straightforward",
    "looking",
    "closest",
    "things",
    "match",
    "got",
    "let",
    "take",
    "look",
    "see",
    "looks",
    "like",
    "use",
    "case",
    "python",
    "let",
    "dive",
    "predict",
    "diabetes",
    "use",
    "case",
    "use",
    "case",
    "predict",
    "diabetes",
    "objective",
    "predict",
    "whether",
    "person",
    "diagnosed",
    "diabetes",
    "data",
    "set",
    "768",
    "people",
    "diagnosed",
    "diabetes",
    "let",
    "go",
    "ahead",
    "open",
    "file",
    "take",
    "look",
    "data",
    "simple",
    "spreadsheet",
    "format",
    "data",
    "comma",
    "separated",
    "common",
    "set",
    "data",
    "also",
    "common",
    "way",
    "get",
    "data",
    "see",
    "columns",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "eight",
    "columns",
    "particular",
    "attribute",
    "ninth",
    "column",
    "outcome",
    "whether",
    "diabetes",
    "data",
    "scientist",
    "first",
    "thing",
    "looking",
    "insulin",
    "well",
    "know",
    "someone",
    "insulin",
    "diabetes",
    "taking",
    "could",
    "cause",
    "issue",
    "machine",
    "learning",
    "packages",
    "basic",
    "setup",
    "works",
    "fine",
    "knn",
    "next",
    "thing",
    "notice",
    "take",
    "much",
    "open",
    "scroll",
    "bottom",
    "data",
    "pretty",
    "much",
    "small",
    "data",
    "set",
    "know",
    "769",
    "easily",
    "fit",
    "ram",
    "computer",
    "look",
    "manipulate",
    "going",
    "really",
    "tax",
    "regular",
    "desktop",
    "computer",
    "even",
    "need",
    "enterprise",
    "version",
    "run",
    "lot",
    "let",
    "start",
    "importing",
    "tools",
    "need",
    "course",
    "need",
    "discuss",
    "ide",
    "using",
    "certainly",
    "use",
    "particular",
    "editor",
    "python",
    "like",
    "use",
    "uh",
    "basic",
    "visual",
    "stuff",
    "anaconda",
    "great",
    "demos",
    "jupiter",
    "notebook",
    "quick",
    "view",
    "anaconda",
    "navigator",
    "new",
    "release",
    "really",
    "nice",
    "see",
    "home",
    "choose",
    "application",
    "going",
    "using",
    "python36",
    "couple",
    "different",
    "versions",
    "particular",
    "machine",
    "go",
    "environments",
    "create",
    "unique",
    "environment",
    "one",
    "nice",
    "even",
    "little",
    "button",
    "install",
    "different",
    "packages",
    "click",
    "button",
    "open",
    "terminal",
    "use",
    "simple",
    "pip",
    "install",
    "install",
    "different",
    "packages",
    "working",
    "let",
    "go",
    "ahead",
    "go",
    "back",
    "home",
    "going",
    "launch",
    "notebook",
    "already",
    "know",
    "kind",
    "like",
    "old",
    "cooking",
    "shows",
    "already",
    "prepared",
    "lot",
    "stuff",
    "wait",
    "launch",
    "takes",
    "minutes",
    "open",
    "browser",
    "window",
    "case",
    "going",
    "open",
    "chrome",
    "default",
    "use",
    "since",
    "script",
    "see",
    "number",
    "windows",
    "open",
    "top",
    "one",
    "working",
    "since",
    "working",
    "k",
    "n",
    "predict",
    "whether",
    "person",
    "diabetes",
    "let",
    "go",
    "put",
    "title",
    "also",
    "going",
    "go",
    "click",
    "cell",
    "actually",
    "want",
    "go",
    "ahead",
    "first",
    "insert",
    "cell",
    "going",
    "go",
    "back",
    "top",
    "cell",
    "going",
    "change",
    "cell",
    "type",
    "markdown",
    "means",
    "going",
    "run",
    "python",
    "markdown",
    "language",
    "run",
    "first",
    "one",
    "comes",
    "nice",
    "big",
    "letters",
    "kind",
    "nice",
    "remind",
    "us",
    "working",
    "familiar",
    "imports",
    "going",
    "import",
    "pandas",
    "pd",
    "import",
    "numpy",
    "np",
    "pandas",
    "pandas",
    "data",
    "frame",
    "numpy",
    "number",
    "array",
    "powerful",
    "tools",
    "use",
    "imports",
    "brought",
    "pandas",
    "numpy",
    "two",
    "general",
    "python",
    "tools",
    "see",
    "trained",
    "test",
    "split",
    "familiar",
    "splitting",
    "data",
    "want",
    "split",
    "part",
    "training",
    "thing",
    "training",
    "particular",
    "model",
    "want",
    "go",
    "ahead",
    "test",
    "remaining",
    "data",
    "see",
    "good",
    "standard",
    "scalar",
    "preprocessor",
    "bias",
    "really",
    "large",
    "numbers",
    "remember",
    "data",
    "like",
    "number",
    "pregnancies",
    "going",
    "get",
    "large",
    "amount",
    "insulin",
    "take",
    "get",
    "256",
    "256",
    "versus",
    "6",
    "skew",
    "results",
    "want",
    "go",
    "ahead",
    "change",
    "uniform",
    "minus",
    "1",
    "actual",
    "tool",
    "k",
    "neighbors",
    "classifier",
    "going",
    "use",
    "finally",
    "last",
    "three",
    "three",
    "tools",
    "test",
    "testing",
    "model",
    "good",
    "put",
    "test",
    "confusion",
    "matrix",
    "f1",
    "score",
    "accuracy",
    "two",
    "general",
    "python",
    "modules",
    "importing",
    "six",
    "modules",
    "specific",
    "sk",
    "learn",
    "setup",
    "need",
    "go",
    "ahead",
    "run",
    "actually",
    "imported",
    "go",
    "move",
    "next",
    "step",
    "set",
    "going",
    "go",
    "ahead",
    "load",
    "database",
    "going",
    "use",
    "pandas",
    "remember",
    "pandas",
    "pd",
    "take",
    "look",
    "data",
    "python",
    "looked",
    "simple",
    "spreadsheet",
    "usually",
    "like",
    "also",
    "pull",
    "see",
    "data",
    "set",
    "equals",
    "csv",
    "pandas",
    "command",
    "diabetes",
    "folder",
    "put",
    "folder",
    "ipython",
    "script",
    "put",
    "different",
    "folder",
    "need",
    "full",
    "length",
    "also",
    "quick",
    "links",
    "data",
    "set",
    "simple",
    "python",
    "command",
    "len",
    "length",
    "might",
    "even",
    "let",
    "go",
    "ahead",
    "print",
    "go",
    "print",
    "line",
    "link",
    "data",
    "set",
    "jupyter",
    "notebook",
    "automatically",
    "print",
    "different",
    "setups",
    "want",
    "print",
    "front",
    "want",
    "take",
    "look",
    "actual",
    "data",
    "set",
    "since",
    "pandas",
    "simply",
    "data",
    "set",
    "head",
    "let",
    "go",
    "ahead",
    "add",
    "print",
    "put",
    "bunch",
    "row",
    "know",
    "data",
    "set",
    "one",
    "head",
    "data",
    "set",
    "two",
    "head",
    "prints",
    "last",
    "one",
    "use",
    "always",
    "like",
    "keep",
    "print",
    "statement",
    "projects",
    "use",
    "one",
    "data",
    "frame",
    "pandas",
    "data",
    "frame",
    "way",
    "really",
    "matter",
    "way",
    "works",
    "fine",
    "see",
    "hit",
    "run",
    "button",
    "768",
    "lines",
    "knew",
    "pregnancies",
    "automatically",
    "given",
    "label",
    "left",
    "remember",
    "head",
    "shows",
    "first",
    "five",
    "lines",
    "zero",
    "four",
    "quick",
    "look",
    "data",
    "see",
    "matches",
    "looked",
    "pregnancy",
    "glucose",
    "blood",
    "pressure",
    "way",
    "age",
    "outcome",
    "end",
    "going",
    "couple",
    "things",
    "next",
    "step",
    "going",
    "create",
    "list",
    "columns",
    "ca",
    "zero",
    "thing",
    "zero",
    "skin",
    "thickness",
    "zero",
    "blood",
    "pressure",
    "zero",
    "glucose",
    "uh",
    "dead",
    "really",
    "good",
    "factor",
    "zero",
    "data",
    "take",
    "look",
    "going",
    "start",
    "replacing",
    "information",
    "couple",
    "different",
    "things",
    "let",
    "see",
    "looks",
    "like",
    "first",
    "create",
    "nice",
    "list",
    "see",
    "values",
    "talked",
    "glucose",
    "blood",
    "pressure",
    "skin",
    "thickness",
    "nice",
    "way",
    "working",
    "columns",
    "list",
    "columns",
    "need",
    "kind",
    "transformation",
    "common",
    "thing",
    "particular",
    "setup",
    "certainly",
    "could",
    "use",
    "panda",
    "tools",
    "lot",
    "replace",
    "n",
    "going",
    "go",
    "ahead",
    "data",
    "set",
    "column",
    "equals",
    "dataset",
    "still",
    "pandas",
    "direct",
    "also",
    "one",
    "look",
    "nan",
    "lot",
    "different",
    "options",
    "nan",
    "numpy",
    "nan",
    "stands",
    "none",
    "exist",
    "first",
    "thing",
    "replacing",
    "zero",
    "numpy",
    "none",
    "data",
    "says",
    "saying",
    "right",
    "put",
    "zero",
    "going",
    "play",
    "zeros",
    "data",
    "zero",
    "means",
    "person",
    "well",
    "hopefully",
    "dead",
    "hope",
    "get",
    "data",
    "next",
    "thing",
    "want",
    "going",
    "create",
    "mean",
    "integer",
    "data",
    "set",
    "column",
    "dot",
    "mean",
    "skip",
    "n",
    "pandas",
    "command",
    "skip",
    "n",
    "going",
    "figure",
    "mean",
    "data",
    "set",
    "going",
    "take",
    "data",
    "set",
    "column",
    "going",
    "replace",
    "n",
    "p",
    "n",
    "n",
    "means",
    "could",
    "actually",
    "taken",
    "step",
    "gone",
    "right",
    "replaced",
    "zero",
    "skip",
    "anything",
    "except",
    "could",
    "actually",
    "way",
    "skip",
    "zeros",
    "replace",
    "zeros",
    "case",
    "want",
    "go",
    "ahead",
    "way",
    "could",
    "see",
    "switching",
    "value",
    "going",
    "create",
    "mean",
    "well",
    "average",
    "person",
    "know",
    "get",
    "data",
    "data",
    "missing",
    "one",
    "tricks",
    "replace",
    "average",
    "common",
    "data",
    "way",
    "still",
    "use",
    "rest",
    "values",
    "computation",
    "kind",
    "brings",
    "particular",
    "value",
    "missing",
    "values",
    "equation",
    "let",
    "go",
    "ahead",
    "take",
    "go",
    "ahead",
    "run",
    "actually",
    "anything",
    "still",
    "preparing",
    "data",
    "want",
    "see",
    "looks",
    "like",
    "anything",
    "first",
    "lines",
    "going",
    "show",
    "certainly",
    "could",
    "look",
    "row",
    "let",
    "let",
    "go",
    "data",
    "set",
    "printed",
    "data",
    "set",
    "let",
    "pick",
    "case",
    "let",
    "glucose",
    "run",
    "going",
    "print",
    "different",
    "glucose",
    "levels",
    "going",
    "thankfully",
    "see",
    "anything",
    "looks",
    "like",
    "missing",
    "data",
    "least",
    "ones",
    "shows",
    "see",
    "skipped",
    "bunch",
    "middle",
    "many",
    "lines",
    "jupiter",
    "notebook",
    "skip",
    "go",
    "next",
    "data",
    "set",
    "let",
    "go",
    "remove",
    "zero",
    "course",
    "processing",
    "proceeding",
    "need",
    "split",
    "data",
    "set",
    "train",
    "testing",
    "data",
    "way",
    "something",
    "train",
    "something",
    "test",
    "going",
    "notice",
    "little",
    "something",
    "pandas",
    "database",
    "code",
    "go",
    "drawing",
    "tool",
    "added",
    "right",
    "data",
    "set",
    "says",
    "first",
    "one",
    "pandas",
    "pd",
    "pandas",
    "going",
    "say",
    "within",
    "data",
    "set",
    "want",
    "look",
    "eye",
    "location",
    "rows",
    "says",
    "going",
    "keep",
    "rows",
    "looking",
    "0",
    "column",
    "0",
    "remember",
    "column",
    "actually",
    "0",
    "7",
    "include",
    "last",
    "one",
    "go",
    "answer",
    "want",
    "last",
    "one",
    "column",
    "eight",
    "way",
    "particular",
    "notation",
    "remember",
    "imported",
    "train",
    "test",
    "split",
    "part",
    "sk",
    "learn",
    "right",
    "simply",
    "put",
    "x",
    "going",
    "random",
    "state",
    "equals",
    "zero",
    "necessarily",
    "seat",
    "seed",
    "number",
    "think",
    "default",
    "one",
    "seed",
    "look",
    "test",
    "size",
    "test",
    "size",
    "simply",
    "means",
    "going",
    "take",
    "20",
    "percent",
    "data",
    "put",
    "aside",
    "test",
    "later",
    "going",
    "run",
    "exciting",
    "far",
    "printout",
    "look",
    "data",
    "lot",
    "prepping",
    "data",
    "prep",
    "actual",
    "lines",
    "code",
    "quick",
    "easy",
    "almost",
    "actual",
    "writing",
    "knn",
    "need",
    "go",
    "ahead",
    "scale",
    "data",
    "remember",
    "correctly",
    "fitting",
    "data",
    "standard",
    "scalar",
    "means",
    "instead",
    "data",
    "know",
    "five",
    "303",
    "one",
    "column",
    "next",
    "column",
    "one",
    "six",
    "going",
    "set",
    "data",
    "minus",
    "one",
    "one",
    "standard",
    "scalar",
    "keeps",
    "standardized",
    "want",
    "fit",
    "scalar",
    "training",
    "set",
    "want",
    "make",
    "sure",
    "testing",
    "set",
    "x",
    "test",
    "going",
    "also",
    "transformed",
    "processing",
    "go",
    "standard",
    "scalar",
    "going",
    "call",
    "sc",
    "underscore",
    "x",
    "scalar",
    "going",
    "import",
    "standard",
    "scalar",
    "variable",
    "x",
    "train",
    "equals",
    "sc",
    "underscore",
    "x",
    "dot",
    "fit",
    "transform",
    "creating",
    "scalar",
    "x",
    "train",
    "variable",
    "x",
    "test",
    "also",
    "going",
    "transform",
    "trained",
    "transformed",
    "x",
    "train",
    "x",
    "test",
    "part",
    "training",
    "part",
    "training",
    "transformer",
    "gets",
    "transformed",
    "going",
    "run",
    "look",
    "gone",
    "steps",
    "three",
    "taken",
    "care",
    "replacing",
    "zeros",
    "key",
    "columns",
    "zero",
    "replace",
    "means",
    "columns",
    "way",
    "fit",
    "right",
    "data",
    "models",
    "come",
    "split",
    "data",
    "test",
    "data",
    "training",
    "data",
    "taken",
    "scaled",
    "data",
    "data",
    "going",
    "tr",
    "train",
    "part",
    "train",
    "test",
    "never",
    "trained",
    "data",
    "going",
    "want",
    "train",
    "define",
    "model",
    "using",
    "k",
    "neighbors",
    "classifier",
    "fit",
    "train",
    "data",
    "model",
    "data",
    "prep",
    "see",
    "going",
    "couple",
    "lines",
    "code",
    "actually",
    "building",
    "model",
    "training",
    "one",
    "cool",
    "things",
    "python",
    "far",
    "come",
    "exciting",
    "time",
    "machine",
    "learning",
    "many",
    "automated",
    "tools",
    "let",
    "see",
    "let",
    "quick",
    "length",
    "let",
    "want",
    "let",
    "length",
    "get",
    "768",
    "import",
    "math",
    "math",
    "dot",
    "square",
    "root",
    "let",
    "train",
    "go",
    "actually",
    "supposed",
    "x",
    "train",
    "let",
    "go",
    "ahead",
    "import",
    "math",
    "math",
    "square",
    "root",
    "length",
    "test",
    "run",
    "get",
    "want",
    "show",
    "number",
    "comes",
    "use",
    "12",
    "even",
    "number",
    "know",
    "ever",
    "voting",
    "things",
    "remember",
    "neighbors",
    "vote",
    "want",
    "even",
    "number",
    "neighbors",
    "voting",
    "want",
    "something",
    "odd",
    "let",
    "take",
    "one",
    "away",
    "make",
    "let",
    "delete",
    "one",
    "reasons",
    "love",
    "jupiter",
    "notebook",
    "flip",
    "around",
    "kinds",
    "things",
    "fly",
    "go",
    "ahead",
    "put",
    "classifier",
    "creating",
    "classifier",
    "going",
    "k",
    "neighbors",
    "classifier",
    "n",
    "neighbors",
    "equal",
    "remember",
    "12",
    "minus",
    "1",
    "11",
    "odd",
    "number",
    "neighbors",
    "p",
    "equals",
    "2",
    "looking",
    "diabetic",
    "using",
    "euclidean",
    "metric",
    "means",
    "measuring",
    "distance",
    "could",
    "like",
    "square",
    "square",
    "means",
    "values",
    "kinds",
    "measure",
    "euclidean",
    "common",
    "one",
    "works",
    "quite",
    "well",
    "important",
    "evaluate",
    "model",
    "let",
    "use",
    "confusion",
    "matrix",
    "going",
    "use",
    "confusion",
    "matrix",
    "wonderful",
    "tool",
    "jump",
    "f1",
    "score",
    "finally",
    "accuracy",
    "score",
    "probably",
    "commonly",
    "used",
    "quoted",
    "number",
    "go",
    "meeting",
    "something",
    "like",
    "let",
    "go",
    "ahead",
    "paste",
    "set",
    "cm",
    "equal",
    "confusion",
    "matrix",
    "test",
    "predict",
    "two",
    "values",
    "going",
    "put",
    "let",
    "go",
    "ahead",
    "run",
    "print",
    "way",
    "interpret",
    "predicted",
    "would",
    "title",
    "could",
    "let",
    "p",
    "r",
    "e",
    "predicted",
    "across",
    "top",
    "actual",
    "going",
    "actually",
    "always",
    "hard",
    "write",
    "actual",
    "means",
    "column",
    "middle",
    "important",
    "column",
    "means",
    "prediction",
    "said",
    "94",
    "prediction",
    "actual",
    "agreed",
    "94",
    "number",
    "13",
    "15",
    "wrong",
    "could",
    "like",
    "three",
    "different",
    "looking",
    "across",
    "three",
    "different",
    "variables",
    "instead",
    "two",
    "end",
    "third",
    "row",
    "column",
    "going",
    "middle",
    "first",
    "case",
    "believe",
    "zero",
    "94",
    "people",
    "diabetes",
    "prediction",
    "said",
    "13",
    "people",
    "diabetes",
    "high",
    "risk",
    "32",
    "diabetes",
    "correct",
    "prediction",
    "said",
    "another",
    "15",
    "15",
    "classified",
    "incorrect",
    "see",
    "classification",
    "comes",
    "works",
    "confusion",
    "matrix",
    "going",
    "go",
    "ahead",
    "print",
    "f1",
    "score",
    "let",
    "run",
    "see",
    "get",
    "f1",
    "score",
    "f1",
    "takes",
    "account",
    "sides",
    "balance",
    "false",
    "positives",
    "go",
    "ahead",
    "accuracy",
    "account",
    "people",
    "think",
    "looks",
    "many",
    "got",
    "right",
    "many",
    "got",
    "wrong",
    "lot",
    "people",
    "data",
    "scientist",
    "talking",
    "data",
    "scientists",
    "going",
    "ask",
    "f1",
    "score",
    "f",
    "score",
    "talking",
    "general",
    "public",
    "decision",
    "makers",
    "business",
    "going",
    "ask",
    "accuracy",
    "accuracy",
    "always",
    "better",
    "f1",
    "score",
    "f1",
    "score",
    "telling",
    "lets",
    "us",
    "know",
    "false",
    "positives",
    "would",
    "like",
    "82",
    "percent",
    "bad",
    "quick",
    "flash",
    "look",
    "people",
    "different",
    "statistics",
    "running",
    "sk",
    "learn",
    "running",
    "k",
    "n",
    "n",
    "k",
    "nearest",
    "neighbor",
    "created",
    "model",
    "using",
    "knn",
    "predict",
    "whether",
    "person",
    "diabetes",
    "least",
    "whether",
    "go",
    "get",
    "checkup",
    "glucose",
    "checked",
    "regularly",
    "print",
    "accuracy",
    "score",
    "got",
    "pretty",
    "close",
    "got",
    "pretty",
    "much",
    "round",
    "say",
    "accuracy",
    "80",
    "tells",
    "us",
    "pretty",
    "fair",
    "fit",
    "model",
    "far",
    "clear",
    "question",
    "coming",
    "sample",
    "data",
    "set",
    "instead",
    "looking",
    "like",
    "looked",
    "like",
    "two",
    "sets",
    "data",
    "one",
    "occurs",
    "middle",
    "another",
    "set",
    "see",
    "blue",
    "yellow",
    "blue",
    "side",
    "data",
    "line",
    "data",
    "set",
    "ca",
    "use",
    "hyperplane",
    "see",
    "data",
    "like",
    "necessary",
    "move",
    "away",
    "1d",
    "view",
    "data",
    "view",
    "data",
    "transformation",
    "use",
    "called",
    "kernel",
    "function",
    "kernel",
    "function",
    "take",
    "1d",
    "input",
    "transfer",
    "output",
    "see",
    "picture",
    "1d",
    "transferred",
    "makes",
    "easy",
    "draw",
    "line",
    "two",
    "data",
    "sets",
    "make",
    "even",
    "complicated",
    "perform",
    "svm",
    "type",
    "data",
    "set",
    "see",
    "two",
    "dimensional",
    "data",
    "set",
    "data",
    "middle",
    "surrounded",
    "green",
    "data",
    "outside",
    "case",
    "going",
    "segregate",
    "two",
    "classes",
    "sample",
    "data",
    "set",
    "draw",
    "line",
    "obviously",
    "optimal",
    "hyperplane",
    "need",
    "transfer",
    "2d",
    "3d",
    "array",
    "translate",
    "array",
    "using",
    "kernel",
    "see",
    "place",
    "hyperplane",
    "right",
    "easily",
    "split",
    "data",
    "start",
    "looking",
    "programming",
    "example",
    "dive",
    "script",
    "let",
    "look",
    "advantage",
    "support",
    "vector",
    "machine",
    "start",
    "high",
    "dimensional",
    "input",
    "space",
    "sometimes",
    "referred",
    "curse",
    "dimensionality",
    "looked",
    "earlier",
    "one",
    "dimension",
    "two",
    "dimension",
    "three",
    "dimension",
    "get",
    "thousand",
    "dimensions",
    "lot",
    "problems",
    "start",
    "occurring",
    "algorithms",
    "adjusted",
    "svm",
    "automatically",
    "high",
    "dimensional",
    "space",
    "one",
    "high",
    "dimensional",
    "space",
    "one",
    "high",
    "dimensional",
    "space",
    "work",
    "sparse",
    "document",
    "vectors",
    "tokenize",
    "words",
    "documents",
    "run",
    "machine",
    "learning",
    "algorithms",
    "seen",
    "ones",
    "get",
    "high",
    "million",
    "different",
    "tokens",
    "lot",
    "vectors",
    "look",
    "finally",
    "regularization",
    "parameter",
    "realization",
    "parameter",
    "lambda",
    "parameter",
    "helps",
    "figure",
    "whether",
    "going",
    "bias",
    "overfitting",
    "data",
    "whether",
    "going",
    "overfitted",
    "specific",
    "instance",
    "going",
    "biased",
    "high",
    "low",
    "value",
    "svm",
    "naturally",
    "avoids",
    "overfitting",
    "bias",
    "problems",
    "see",
    "many",
    "algorithms",
    "three",
    "advantages",
    "support",
    "vector",
    "machine",
    "make",
    "powerful",
    "tool",
    "add",
    "repertoire",
    "machine",
    "learning",
    "tools",
    "promise",
    "used",
    "case",
    "study",
    "actually",
    "going",
    "dive",
    "python",
    "programming",
    "going",
    "go",
    "problem",
    "statement",
    "start",
    "zoo",
    "zoo",
    "example",
    "family",
    "members",
    "going",
    "zoo",
    "young",
    "child",
    "going",
    "dead",
    "group",
    "crocodiles",
    "alligators",
    "well",
    "hard",
    "differentiate",
    "zoos",
    "great",
    "place",
    "start",
    "looking",
    "science",
    "understanding",
    "things",
    "work",
    "especially",
    "young",
    "child",
    "see",
    "parents",
    "sitting",
    "thinking",
    "well",
    "difference",
    "crocodile",
    "alligator",
    "well",
    "one",
    "crocodiles",
    "larger",
    "size",
    "alligators",
    "smaller",
    "size",
    "snout",
    "width",
    "crocodiles",
    "narrow",
    "snout",
    "alligators",
    "wider",
    "snout",
    "course",
    "modern",
    "day",
    "age",
    "father",
    "sitting",
    "thinking",
    "turn",
    "lesson",
    "son",
    "goes",
    "let",
    "support",
    "vector",
    "machine",
    "segregate",
    "two",
    "groups",
    "know",
    "dad",
    "ever",
    "told",
    "would",
    "funny",
    "example",
    "going",
    "use",
    "actual",
    "measurements",
    "data",
    "using",
    "imagery",
    "common",
    "lot",
    "machine",
    "learning",
    "algorithms",
    "setting",
    "let",
    "roll",
    "sleeves",
    "talk",
    "moment",
    "break",
    "python",
    "script",
    "arrive",
    "actual",
    "coding",
    "going",
    "move",
    "python",
    "editor",
    "moment",
    "let",
    "talk",
    "little",
    "bit",
    "going",
    "cover",
    "first",
    "going",
    "cover",
    "code",
    "setup",
    "actually",
    "create",
    "svm",
    "going",
    "find",
    "two",
    "lines",
    "code",
    "actually",
    "create",
    "rest",
    "done",
    "quick",
    "fast",
    "first",
    "page",
    "show",
    "looks",
    "like",
    "far",
    "data",
    "going",
    "create",
    "data",
    "talked",
    "creating",
    "data",
    "minute",
    "ago",
    "get",
    "creating",
    "data",
    "see",
    "nice",
    "correction",
    "two",
    "blobs",
    "go",
    "second",
    "second",
    "part",
    "going",
    "take",
    "going",
    "bump",
    "notch",
    "going",
    "show",
    "looks",
    "like",
    "behind",
    "scenes",
    "let",
    "start",
    "actually",
    "creating",
    "setup",
    "like",
    "use",
    "anaconda",
    "jupiter",
    "notebook",
    "easy",
    "use",
    "use",
    "favorite",
    "python",
    "editors",
    "setups",
    "go",
    "let",
    "go",
    "ahead",
    "switch",
    "see",
    "looks",
    "like",
    "anaconda",
    "python",
    "notebook",
    "anaconda",
    "jupiter",
    "notebook",
    "python",
    "using",
    "python",
    "believe",
    "work",
    "3x",
    "versions",
    "look",
    "sklearn",
    "make",
    "sure",
    "using",
    "2x",
    "version",
    "earlier",
    "version",
    "let",
    "go",
    "put",
    "code",
    "one",
    "things",
    "like",
    "jupiter",
    "notebook",
    "go",
    "view",
    "going",
    "go",
    "ahead",
    "toggle",
    "line",
    "numbers",
    "make",
    "little",
    "bit",
    "easier",
    "talk",
    "even",
    "increase",
    "size",
    "edited",
    "case",
    "using",
    "google",
    "chrome",
    "explorer",
    "opens",
    "editor",
    "although",
    "anyone",
    "like",
    "said",
    "editor",
    "work",
    "first",
    "step",
    "going",
    "imports",
    "going",
    "import",
    "four",
    "different",
    "parts",
    "first",
    "two",
    "want",
    "look",
    "line",
    "one",
    "line",
    "two",
    "numpy",
    "np",
    "matplot",
    "library",
    "dot",
    "pi",
    "plot",
    "plt",
    "standardized",
    "imports",
    "work",
    "first",
    "one",
    "numbers",
    "python",
    "need",
    "part",
    "platform",
    "using",
    "uses",
    "numpy",
    "array",
    "talk",
    "minute",
    "understand",
    "want",
    "use",
    "numpy",
    "array",
    "versus",
    "standard",
    "python",
    "array",
    "normally",
    "pretty",
    "standard",
    "setup",
    "use",
    "np",
    "numpy",
    "map",
    "plot",
    "library",
    "going",
    "view",
    "data",
    "says",
    "need",
    "np",
    "sk",
    "learn",
    "module",
    "map",
    "plot",
    "library",
    "purely",
    "use",
    "visualization",
    "really",
    "need",
    "svm",
    "gon",
    "na",
    "put",
    "nice",
    "visual",
    "aid",
    "show",
    "looks",
    "like",
    "really",
    "important",
    "end",
    "finish",
    "everything",
    "nice",
    "display",
    "everybody",
    "look",
    "finally",
    "gon",
    "na",
    "gon",
    "na",
    "jump",
    "one",
    "ahead",
    "line",
    "number",
    "four",
    "import",
    "make",
    "blobs",
    "told",
    "going",
    "make",
    "data",
    "tool",
    "sk",
    "learning",
    "make",
    "data",
    "personally",
    "want",
    "go",
    "zoo",
    "get",
    "trouble",
    "jumping",
    "fence",
    "probably",
    "get",
    "eaten",
    "crocodiles",
    "alligators",
    "work",
    "measuring",
    "snouts",
    "width",
    "length",
    "instead",
    "going",
    "make",
    "data",
    "make",
    "blobs",
    "wonderful",
    "tool",
    "ready",
    "test",
    "setup",
    "sure",
    "data",
    "going",
    "put",
    "create",
    "blob",
    "makes",
    "real",
    "easy",
    "use",
    "finally",
    "actual",
    "svm",
    "sklearn",
    "import",
    "svm",
    "line",
    "three",
    "covers",
    "imports",
    "going",
    "create",
    "remember",
    "used",
    "make",
    "blobs",
    "create",
    "data",
    "going",
    "create",
    "capital",
    "x",
    "lowercase",
    "equals",
    "make",
    "blobs",
    "samples",
    "equals",
    "going",
    "make",
    "40",
    "lines",
    "data",
    "going",
    "two",
    "centers",
    "random",
    "state",
    "equals",
    "group",
    "going",
    "20",
    "different",
    "pieces",
    "data",
    "way",
    "looks",
    "x",
    "x",
    "plane",
    "two",
    "numbers",
    "x",
    "0",
    "1",
    "two",
    "different",
    "centers",
    "yes",
    "case",
    "alligator",
    "crocodile",
    "represents",
    "told",
    "actual",
    "sk",
    "learner",
    "svm",
    "two",
    "lines",
    "code",
    "see",
    "right",
    "clf",
    "equals",
    "svm",
    "dot",
    "svc",
    "kernel",
    "equals",
    "linear",
    "set",
    "sql",
    "1",
    "although",
    "example",
    "since",
    "regularizing",
    "data",
    "want",
    "clear",
    "easy",
    "see",
    "went",
    "ahead",
    "set",
    "thousand",
    "lot",
    "times",
    "thing",
    "linear",
    "simple",
    "linear",
    "example",
    "two",
    "dimensions",
    "nice",
    "linear",
    "hyperplane",
    "nice",
    "linear",
    "line",
    "instead",
    "full",
    "plane",
    "dealing",
    "huge",
    "amount",
    "data",
    "clf",
    "dot",
    "fit",
    "x",
    "comma",
    "clf",
    "created",
    "going",
    "go",
    "ahead",
    "display",
    "going",
    "talk",
    "display",
    "second",
    "let",
    "go",
    "ahead",
    "run",
    "code",
    "done",
    "created",
    "two",
    "blobs",
    "see",
    "blue",
    "side",
    "kind",
    "orangish",
    "side",
    "two",
    "sets",
    "data",
    "represent",
    "one",
    "represents",
    "crocodiles",
    "one",
    "represents",
    "alligators",
    "measurements",
    "case",
    "like",
    "width",
    "length",
    "snout",
    "say",
    "going",
    "come",
    "talk",
    "little",
    "bit",
    "plot",
    "see",
    "plt",
    "imported",
    "going",
    "scatter",
    "plot",
    "means",
    "putting",
    "dots",
    "look",
    "notation",
    "capital",
    "x",
    "brackets",
    "colon",
    "comma",
    "numpy",
    "regular",
    "array",
    "get",
    "error",
    "python",
    "array",
    "numpy",
    "array",
    "turns",
    "make",
    "blobs",
    "returns",
    "numpy",
    "array",
    "notation",
    "great",
    "means",
    "first",
    "part",
    "colon",
    "means",
    "going",
    "rows",
    "data",
    "blob",
    "created",
    "capital",
    "x",
    "second",
    "part",
    "comma",
    "going",
    "take",
    "first",
    "value",
    "notice",
    "thing",
    "going",
    "take",
    "second",
    "value",
    "remember",
    "always",
    "start",
    "zero",
    "one",
    "column",
    "zero",
    "column",
    "one",
    "look",
    "x",
    "plots",
    "first",
    "one",
    "x",
    "plot",
    "second",
    "one",
    "plot",
    "first",
    "one",
    "bottom",
    "0",
    "2",
    "4",
    "6",
    "8",
    "10",
    "second",
    "one",
    "x",
    "one",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "10",
    "going",
    "left",
    "hand",
    "side",
    "equals",
    "30",
    "size",
    "dots",
    "see",
    "says",
    "real",
    "tiny",
    "dots",
    "c",
    "map",
    "equals",
    "also",
    "see",
    "c",
    "equals",
    "color",
    "using",
    "two",
    "colors",
    "zero",
    "one",
    "get",
    "nice",
    "blue",
    "two",
    "different",
    "colors",
    "alligator",
    "crocodile",
    "see",
    "actual",
    "fit",
    "done",
    "two",
    "lines",
    "code",
    "lot",
    "times",
    "third",
    "line",
    "regularize",
    "data",
    "set",
    "like",
    "minus",
    "one",
    "one",
    "reshape",
    "necessary",
    "also",
    "kind",
    "nice",
    "actually",
    "see",
    "going",
    "wanted",
    "wanted",
    "actually",
    "run",
    "prediction",
    "let",
    "take",
    "look",
    "see",
    "looks",
    "like",
    "predict",
    "new",
    "data",
    "show",
    "get",
    "towards",
    "end",
    "digging",
    "deep",
    "simply",
    "assign",
    "new",
    "data",
    "case",
    "giving",
    "width",
    "length",
    "3",
    "4",
    "width",
    "length",
    "5",
    "note",
    "put",
    "data",
    "set",
    "brackets",
    "brackets",
    "inside",
    "reason",
    "looking",
    "data",
    "designed",
    "process",
    "large",
    "amount",
    "data",
    "coming",
    "want",
    "process",
    "one",
    "line",
    "time",
    "case",
    "processing",
    "two",
    "lines",
    "going",
    "print",
    "see",
    "clf",
    "dot",
    "predict",
    "new",
    "data",
    "clf",
    "dot",
    "predict",
    "part",
    "going",
    "give",
    "us",
    "answer",
    "let",
    "see",
    "looks",
    "like",
    "see",
    "0",
    "predicted",
    "first",
    "one",
    "3",
    "4",
    "going",
    "one",
    "side",
    "5",
    "6",
    "going",
    "side",
    "one",
    "came",
    "alligator",
    "one",
    "came",
    "crocodile",
    "pretty",
    "short",
    "explanation",
    "setup",
    "really",
    "want",
    "dug",
    "see",
    "going",
    "behind",
    "scenes",
    "let",
    "see",
    "looks",
    "like",
    "next",
    "step",
    "dig",
    "deep",
    "find",
    "going",
    "behind",
    "scenes",
    "also",
    "put",
    "nice",
    "pretty",
    "graph",
    "going",
    "spend",
    "work",
    "actually",
    "generating",
    "original",
    "model",
    "see",
    "go",
    "steps",
    "move",
    "editor",
    "second",
    "come",
    "create",
    "original",
    "data",
    "exactly",
    "identical",
    "first",
    "part",
    "explain",
    "redid",
    "show",
    "redo",
    "going",
    "go",
    "add",
    "lines",
    "going",
    "see",
    "lines",
    "look",
    "like",
    "set",
    "finally",
    "going",
    "plot",
    "show",
    "get",
    "nice",
    "graph",
    "saw",
    "earlier",
    "going",
    "theory",
    "behind",
    "shows",
    "support",
    "vectors",
    "hyperplane",
    "done",
    "see",
    "support",
    "vectors",
    "dashed",
    "lines",
    "solid",
    "line",
    "hyperplane",
    "let",
    "get",
    "jupiter",
    "notebook",
    "scroll",
    "new",
    "line",
    "want",
    "notice",
    "line",
    "13",
    "plot",
    "show",
    "going",
    "talk",
    "second",
    "let",
    "scroll",
    "new",
    "line",
    "going",
    "paste",
    "code",
    "see",
    "plot",
    "show",
    "moved",
    "let",
    "scroll",
    "little",
    "bit",
    "look",
    "top",
    "new",
    "section",
    "one",
    "two",
    "three",
    "four",
    "code",
    "let",
    "go",
    "back",
    "take",
    "look",
    "going",
    "fit",
    "values",
    "svm",
    "going",
    "plot",
    "scatter",
    "going",
    "plot",
    "show",
    "asking",
    "redoing",
    "code",
    "well",
    "plot",
    "show",
    "blanks",
    "plot",
    "done",
    "plot",
    "show",
    "reload",
    "data",
    "could",
    "simply",
    "removing",
    "coming",
    "would",
    "rerun",
    "first",
    "four",
    "lines",
    "code",
    "matter",
    "much",
    "see",
    "plot",
    "show",
    "removed",
    "right",
    "line",
    "five",
    "go",
    "ahead",
    "delete",
    "want",
    "blank",
    "screen",
    "want",
    "move",
    "next",
    "setup",
    "go",
    "ahead",
    "skip",
    "first",
    "four",
    "lines",
    "let",
    "take",
    "look",
    "ax",
    "equals",
    "right",
    "actually",
    "spending",
    "lot",
    "time",
    "graphing",
    "okay",
    "display",
    "nice",
    "graph",
    "results",
    "data",
    "ax",
    "standard",
    "used",
    "variable",
    "talk",
    "plt",
    "setting",
    "axis",
    "last",
    "axis",
    "plt",
    "get",
    "confusing",
    "working",
    "many",
    "different",
    "layers",
    "data",
    "graph",
    "makes",
    "easy",
    "reference",
    "ax",
    "reference",
    "looking",
    "plt",
    "created",
    "already",
    "mapped",
    "two",
    "blobs",
    "want",
    "know",
    "limits",
    "want",
    "know",
    "big",
    "graph",
    "find",
    "x",
    "limit",
    "limit",
    "simply",
    "get",
    "x",
    "limit",
    "get",
    "limit",
    "commands",
    "part",
    "met",
    "plot",
    "library",
    "going",
    "create",
    "grid",
    "see",
    "set",
    "variable",
    "xx",
    "equal",
    "space",
    "x",
    "limit",
    "0",
    "x",
    "limit",
    "1",
    "comma",
    "30",
    "done",
    "thing",
    "space",
    "going",
    "go",
    "create",
    "mesh",
    "grid",
    "numpy",
    "command",
    "back",
    "numbers",
    "python",
    "let",
    "go",
    "numpy",
    "commands",
    "mean",
    "line",
    "space",
    "mesh",
    "grid",
    "taken",
    "x",
    "x",
    "small",
    "x",
    "x",
    "equals",
    "np",
    "line",
    "space",
    "x",
    "limit",
    "0",
    "x",
    "limit",
    "1",
    "going",
    "create",
    "30",
    "points",
    "going",
    "thing",
    "axis",
    "nothing",
    "evaluation",
    "creating",
    "grid",
    "data",
    "creating",
    "set",
    "points",
    "0",
    "x",
    "limit",
    "creating",
    "30",
    "points",
    "thing",
    "mesh",
    "grid",
    "loops",
    "together",
    "forms",
    "nice",
    "grid",
    "going",
    "say",
    "limit",
    "0",
    "10",
    "10",
    "points",
    "would",
    "0",
    "0",
    "1",
    "1",
    "0",
    "1",
    "0",
    "2",
    "0",
    "3",
    "0",
    "4",
    "10",
    "imagine",
    "point",
    "corner",
    "one",
    "boxes",
    "mesh",
    "grid",
    "combines",
    "take",
    "yy",
    "xx",
    "created",
    "creates",
    "full",
    "grid",
    "set",
    "grid",
    "yy",
    "coordinates",
    "x",
    "coordinates",
    "remember",
    "working",
    "numbi",
    "python",
    "like",
    "separate",
    "like",
    "instead",
    "x",
    "comma",
    "1",
    "know",
    "x",
    "comma",
    "x2",
    "comma",
    "y2",
    "next",
    "set",
    "data",
    "would",
    "column",
    "x",
    "column",
    "column",
    "put",
    "capital",
    "yy",
    "column",
    "x",
    "capital",
    "x",
    "different",
    "points",
    "listed",
    "finally",
    "get",
    "numpy",
    "v",
    "stack",
    "created",
    "mesh",
    "grid",
    "going",
    "put",
    "one",
    "array",
    "x",
    "array",
    "created",
    "stack",
    "data",
    "points",
    "going",
    "something",
    "interesting",
    "going",
    "create",
    "value",
    "z",
    "z",
    "equals",
    "clf",
    "support",
    "vector",
    "machine",
    "created",
    "already",
    "trained",
    "dot",
    "decision",
    "function",
    "going",
    "put",
    "x",
    "data",
    "going",
    "put",
    "x",
    "data",
    "going",
    "reshape",
    "see",
    "x",
    "x",
    "dot",
    "shape",
    "literally",
    "takes",
    "xx",
    "resets",
    "connected",
    "z",
    "value",
    "lets",
    "us",
    "know",
    "whether",
    "left",
    "hand",
    "side",
    "going",
    "generate",
    "three",
    "different",
    "values",
    "z",
    "value",
    "tell",
    "us",
    "whether",
    "data",
    "support",
    "vector",
    "left",
    "hyperplane",
    "middle",
    "support",
    "vector",
    "right",
    "generates",
    "three",
    "different",
    "values",
    "points",
    "points",
    "reshaped",
    "right",
    "line",
    "three",
    "different",
    "lines",
    "set",
    "data",
    "labeled",
    "three",
    "different",
    "areas",
    "reshaped",
    "taken",
    "30",
    "points",
    "direction",
    "math",
    "30",
    "times",
    "30",
    "900",
    "points",
    "data",
    "separated",
    "three",
    "lines",
    "reshaped",
    "fit",
    "three",
    "lines",
    "go",
    "back",
    "map",
    "plot",
    "library",
    "created",
    "ax",
    "going",
    "create",
    "contour",
    "see",
    "contour",
    "capital",
    "xx",
    "capital",
    "yy",
    "reshaped",
    "fit",
    "lines",
    "z",
    "labels",
    "three",
    "different",
    "points",
    "labels",
    "set",
    "colors",
    "equals",
    "k",
    "told",
    "three",
    "different",
    "labels",
    "three",
    "levels",
    "data",
    "alphas",
    "makes",
    "kind",
    "value",
    "graph",
    "data",
    "show",
    "behind",
    "wherever",
    "lines",
    "go",
    "finally",
    "line",
    "styles",
    "set",
    "two",
    "support",
    "vectors",
    "dash",
    "dash",
    "lines",
    "single",
    "one",
    "straight",
    "line",
    "setup",
    "finally",
    "take",
    "ax",
    "dot",
    "scatter",
    "going",
    "go",
    "ahead",
    "plot",
    "support",
    "vectors",
    "programmed",
    "look",
    "nice",
    "like",
    "dash",
    "dash",
    "line",
    "dashed",
    "line",
    "grid",
    "see",
    "clf",
    "dot",
    "support",
    "vectors",
    "looking",
    "column",
    "zero",
    "column",
    "one",
    "equals",
    "one",
    "hundred",
    "gon",
    "na",
    "make",
    "larger",
    "line",
    "width",
    "equals",
    "one",
    "face",
    "colors",
    "equals",
    "none",
    "let",
    "take",
    "look",
    "see",
    "looks",
    "like",
    "show",
    "see",
    "get",
    "end",
    "result",
    "creates",
    "really",
    "nice",
    "graph",
    "two",
    "support",
    "vectors",
    "dashed",
    "lines",
    "near",
    "data",
    "see",
    "two",
    "points",
    "case",
    "four",
    "points",
    "lines",
    "nicely",
    "cleave",
    "data",
    "hyper",
    "plane",
    "middle",
    "far",
    "two",
    "different",
    "points",
    "possible",
    "creating",
    "maximum",
    "distance",
    "see",
    "nice",
    "output",
    "size",
    "body",
    "width",
    "snout",
    "easily",
    "separated",
    "two",
    "groups",
    "crocodile",
    "alligator",
    "congratulations",
    "done",
    "made",
    "course",
    "pretend",
    "data",
    "crocodiles",
    "alligators",
    "example",
    "help",
    "encounter",
    "support",
    "vector",
    "machine",
    "projects",
    "future",
    "see",
    "easy",
    "set",
    "look",
    "depth",
    "regularization",
    "machine",
    "learning",
    "agenda",
    "one",
    "fitting",
    "data",
    "understanding",
    "linear",
    "regression",
    "bias",
    "variance",
    "overfitting",
    "underfitting",
    "like",
    "biggest",
    "things",
    "right",
    "data",
    "science",
    "overfitting",
    "underfitting",
    "mean",
    "regularization",
    "quick",
    "demo",
    "take",
    "look",
    "fitting",
    "data",
    "let",
    "start",
    "fitting",
    "data",
    "talk",
    "data",
    "fitting",
    "process",
    "plotting",
    "series",
    "data",
    "points",
    "drawing",
    "best",
    "fit",
    "line",
    "understand",
    "relationship",
    "variables",
    "called",
    "data",
    "fitting",
    "see",
    "couple",
    "lines",
    "drawn",
    "graph",
    "going",
    "go",
    "little",
    "deeper",
    "might",
    "case",
    "two",
    "dimensions",
    "efficiency",
    "car",
    "distance",
    "traveled",
    "1000",
    "kilometers",
    "data",
    "fitting",
    "well",
    "linear",
    "relationship",
    "linear",
    "relationship",
    "specifically",
    "linear",
    "means",
    "line",
    "uh",
    "line",
    "used",
    "represent",
    "relationship",
    "straight",
    "line",
    "passes",
    "data",
    "points",
    "variables",
    "linear",
    "relationship",
    "linear",
    "regression",
    "let",
    "start",
    "uh",
    "linear",
    "regression",
    "works",
    "linear",
    "regression",
    "finds",
    "line",
    "best",
    "fits",
    "data",
    "point",
    "gives",
    "relationship",
    "two",
    "variables",
    "see",
    "efficiency",
    "car",
    "versus",
    "distance",
    "traveled",
    "see",
    "nice",
    "straight",
    "line",
    "drawn",
    "talk",
    "multiple",
    "variables",
    "putting",
    "instead",
    "line",
    "becomes",
    "plane",
    "gets",
    "little",
    "complicated",
    "multiple",
    "variables",
    "come",
    "linear",
    "kind",
    "drawing",
    "line",
    "data",
    "finding",
    "fits",
    "data",
    "best",
    "consider",
    "example",
    "uh",
    "let",
    "say",
    "want",
    "find",
    "relationship",
    "temperature",
    "outside",
    "versus",
    "sales",
    "ice",
    "cream",
    "start",
    "looking",
    "looking",
    "many",
    "ice",
    "cream",
    "cones",
    "selling",
    "much",
    "money",
    "sold",
    "ice",
    "cream",
    "looking",
    "warm",
    "outside",
    "would",
    "hopefully",
    "draw",
    "lot",
    "people",
    "ice",
    "cream",
    "store",
    "suppose",
    "two",
    "lines",
    "going",
    "draw",
    "l1",
    "l2",
    "going",
    "kind",
    "guess",
    "one",
    "think",
    "best",
    "fit",
    "claim",
    "describe",
    "relationship",
    "variables",
    "first",
    "find",
    "square",
    "distance",
    "line",
    "l1",
    "data",
    "point",
    "add",
    "find",
    "mean",
    "distance",
    "want",
    "think",
    "square",
    "something",
    "negative",
    "positive",
    "number",
    "longer",
    "matters",
    "minus",
    "2",
    "squared",
    "4",
    "2",
    "squared",
    "removing",
    "side",
    "line",
    "looking",
    "error",
    "case",
    "mean",
    "distance",
    "little",
    "dotted",
    "lines",
    "see",
    "way",
    "calculating",
    "square",
    "distance",
    "adding",
    "taking",
    "mean",
    "called",
    "mean",
    "square",
    "error",
    "loss",
    "function",
    "talk",
    "loss",
    "far",
    "really",
    "talking",
    "miss",
    "positive",
    "distance",
    "negative",
    "distance",
    "course",
    "square",
    "neither",
    "becomes",
    "positive",
    "error",
    "take",
    "mean",
    "square",
    "error",
    "lot",
    "times",
    "see",
    "referred",
    "mse",
    "look",
    "code",
    "going",
    "python",
    "code",
    "see",
    "mse",
    "know",
    "mean",
    "squared",
    "error",
    "take",
    "dotted",
    "lines",
    "calculate",
    "error",
    "add",
    "together",
    "average",
    "find",
    "means",
    "case",
    "ran",
    "demo",
    "uh",
    "l1",
    "line",
    "find",
    "loss",
    "function",
    "line",
    "l2",
    "similar",
    "fashion",
    "get",
    "mean",
    "square",
    "error",
    "6397",
    "computed",
    "way",
    "maybe",
    "put",
    "line",
    "way",
    "outside",
    "data",
    "range",
    "error",
    "get",
    "analyzing",
    "results",
    "find",
    "loss",
    "function",
    "mean",
    "square",
    "error",
    "less",
    "l1",
    "l2",
    "hence",
    "l1",
    "best",
    "fit",
    "line",
    "process",
    "describes",
    "lot",
    "machine",
    "learning",
    "processes",
    "going",
    "keep",
    "guessing",
    "get",
    "close",
    "find",
    "right",
    "answer",
    "way",
    "invite",
    "calculate",
    "figure",
    "one",
    "best",
    "mean",
    "square",
    "error",
    "one",
    "better",
    "fits",
    "commonly",
    "used",
    "really",
    "want",
    "talk",
    "bias",
    "variance",
    "important",
    "terms",
    "know",
    "machine",
    "learning",
    "linear",
    "regression",
    "bias",
    "bias",
    "occurs",
    "algorithm",
    "limited",
    "flexibility",
    "learn",
    "data",
    "variance",
    "defines",
    "algorithm",
    "sensitivity",
    "specifics",
    "sets",
    "data",
    "let",
    "start",
    "bias",
    "variance",
    "see",
    "two",
    "different",
    "setups",
    "bias",
    "think",
    "generalized",
    "variance",
    "specific",
    "talk",
    "bias",
    "models",
    "pay",
    "little",
    "attention",
    "training",
    "data",
    "simplify",
    "model",
    "therefore",
    "validation",
    "error",
    "prediction",
    "error",
    "training",
    "error",
    "follow",
    "similar",
    "trends",
    "uh",
    "bias",
    "simplify",
    "much",
    "going",
    "miss",
    "local",
    "like",
    "really",
    "good",
    "fit",
    "going",
    "miss",
    "going",
    "kind",
    "guess",
    "average",
    "answer",
    "going",
    "variance",
    "model",
    "high",
    "variance",
    "pays",
    "lot",
    "attention",
    "training",
    "data",
    "generalize",
    "therefore",
    "validation",
    "error",
    "prediction",
    "error",
    "far",
    "apart",
    "models",
    "always",
    "lead",
    "high",
    "error",
    "training",
    "test",
    "data",
    "bias",
    "variants",
    "models",
    "usually",
    "perform",
    "well",
    "training",
    "data",
    "high",
    "error",
    "rates",
    "test",
    "data",
    "want",
    "think",
    "talking",
    "bias",
    "error",
    "going",
    "high",
    "training",
    "testing",
    "kind",
    "getting",
    "average",
    "really",
    "fitting",
    "close",
    "variance",
    "fitting",
    "close",
    "test",
    "data",
    "really",
    "good",
    "going",
    "nail",
    "every",
    "time",
    "categorical",
    "testing",
    "car",
    "truck",
    "bicycle",
    "variants",
    "suddenly",
    "truck",
    "certain",
    "features",
    "might",
    "red",
    "many",
    "red",
    "pictures",
    "18",
    "wheeler",
    "red",
    "blue",
    "bicycle",
    "kind",
    "variance",
    "talking",
    "picks",
    "something",
    "get",
    "right",
    "answer",
    "unless",
    "gets",
    "specific",
    "data",
    "see",
    "testing",
    "models",
    "programmed",
    "got",
    "look",
    "trained",
    "coming",
    "looking",
    "good",
    "either",
    "bias",
    "looking",
    "good",
    "training",
    "test",
    "data",
    "bias",
    "bias",
    "data",
    "really",
    "looks",
    "good",
    "training",
    "data",
    "going",
    "variance",
    "fitted",
    "data",
    "important",
    "things",
    "know",
    "building",
    "models",
    "regression",
    "kind",
    "kind",
    "setup",
    "predicting",
    "dark",
    "games",
    "data",
    "found",
    "particular",
    "pointer",
    "considered",
    "biased",
    "throw",
    "player",
    "aims",
    "particular",
    "score",
    "variance",
    "darts",
    "fall",
    "different",
    "pointers",
    "two",
    "darts",
    "fall",
    "pointer",
    "considered",
    "varied",
    "throw",
    "player",
    "aims",
    "various",
    "scores",
    "bias",
    "sums",
    "everything",
    "one",
    "point",
    "kind",
    "averages",
    "together",
    "variance",
    "really",
    "looks",
    "individual",
    "predictions",
    "coming",
    "let",
    "go",
    "ahead",
    "talk",
    "overfitting",
    "talk",
    "overfitting",
    "scenario",
    "machine",
    "learning",
    "model",
    "tries",
    "learn",
    "details",
    "along",
    "noise",
    "data",
    "tries",
    "fit",
    "data",
    "point",
    "curve",
    "see",
    "um",
    "plug",
    "coordinates",
    "going",
    "get",
    "whatever",
    "fitted",
    "every",
    "point",
    "data",
    "stream",
    "average",
    "two",
    "points",
    "might",
    "know",
    "might",
    "two",
    "different",
    "answers",
    "wind",
    "blows",
    "certain",
    "way",
    "um",
    "efficiency",
    "car",
    "maybe",
    "headwind",
    "car",
    "might",
    "alter",
    "efficient",
    "goes",
    "going",
    "variance",
    "says",
    "ca",
    "variance",
    "know",
    "going",
    "exactly",
    "ca",
    "ca",
    "speed",
    "car",
    "slightly",
    "different",
    "efficiency",
    "model",
    "less",
    "flexibility",
    "fails",
    "predict",
    "new",
    "data",
    "points",
    "thus",
    "model",
    "rejects",
    "every",
    "new",
    "data",
    "point",
    "prediction",
    "get",
    "like",
    "really",
    "high",
    "error",
    "uh",
    "reasons",
    "overfitting",
    "data",
    "used",
    "training",
    "cleaned",
    "contains",
    "noise",
    "garbage",
    "values",
    "spend",
    "much",
    "time",
    "cleaning",
    "data",
    "important",
    "important",
    "kind",
    "something",
    "wrong",
    "data",
    "coming",
    "needs",
    "addressed",
    "whether",
    "source",
    "data",
    "maybe",
    "use",
    "medical",
    "different",
    "measuring",
    "tools",
    "adjust",
    "data",
    "came",
    "hospital",
    "versus",
    "hospital",
    "b",
    "even",
    "machine",
    "machine",
    "b",
    "testing",
    "something",
    "numbers",
    "coming",
    "wrong",
    "model",
    "high",
    "variance",
    "wind",
    "good",
    "example",
    "talking",
    "car",
    "may",
    "hundred",
    "tests",
    "wind",
    "blowing",
    "place",
    "size",
    "training",
    "data",
    "used",
    "enough",
    "small",
    "amount",
    "data",
    "going",
    "also",
    "cause",
    "problem",
    "points",
    "try",
    "plot",
    "everything",
    "model",
    "complex",
    "comes",
    "lot",
    "put",
    "many",
    "pieces",
    "together",
    "interact",
    "ca",
    "even",
    "tracked",
    "go",
    "back",
    "back",
    "break",
    "find",
    "actually",
    "correlates",
    "underfitting",
    "scenario",
    "machine",
    "learning",
    "models",
    "either",
    "learn",
    "relationship",
    "data",
    "points",
    "predict",
    "classify",
    "new",
    "data",
    "point",
    "see",
    "efficiency",
    "car",
    "line",
    "drawn",
    "going",
    "way",
    "training",
    "predicting",
    "data",
    "model",
    "fully",
    "learn",
    "patterns",
    "accepts",
    "every",
    "new",
    "data",
    "point",
    "prediction",
    "instead",
    "looking",
    "general",
    "pattern",
    "kind",
    "accept",
    "everything",
    "data",
    "used",
    "training",
    "cleaned",
    "contains",
    "noise",
    "garbage",
    "values",
    "fitting",
    "overfitting",
    "issue",
    "got",
    "clean",
    "data",
    "model",
    "high",
    "bias",
    "seen",
    "kinds",
    "things",
    "music",
    "uh",
    "mod",
    "common",
    "driving",
    "cars",
    "facial",
    "identification",
    "whatever",
    "model",
    "build",
    "might",
    "bias",
    "towards",
    "one",
    "thing",
    "would",
    "underfitted",
    "model",
    "would",
    "bias",
    "averaged",
    "five",
    "people",
    "india",
    "10",
    "people",
    "africa",
    "20",
    "people",
    "us",
    "created",
    "bias",
    "looking",
    "20",
    "people",
    "small",
    "amount",
    "data",
    "work",
    "size",
    "training",
    "data",
    "used",
    "enough",
    "goes",
    "size",
    "talking",
    "model",
    "high",
    "bias",
    "size",
    "training",
    "data",
    "used",
    "enough",
    "model",
    "simple",
    "one",
    "straight",
    "line",
    "data",
    "needs",
    "slight",
    "shift",
    "reasons",
    "good",
    "fit",
    "uh",
    "linear",
    "curve",
    "best",
    "fits",
    "data",
    "neither",
    "overfitting",
    "underfitting",
    "models",
    "right",
    "course",
    "nice",
    "examples",
    "overfitting",
    "lines",
    "going",
    "every",
    "point",
    "trying",
    "include",
    "gluted",
    "underfitting",
    "line",
    "really",
    "data",
    "good",
    "fit",
    "got",
    "get",
    "rid",
    "minimize",
    "error",
    "coming",
    "regularization",
    "taking",
    "guesswork",
    "looking",
    "graph",
    "going",
    "oh",
    "one",
    "really",
    "fit",
    "fit",
    "pretty",
    "hard",
    "tell",
    "talk",
    "regularization",
    "regularization",
    "techniques",
    "used",
    "calibrate",
    "linear",
    "regression",
    "models",
    "minimize",
    "adjusted",
    "loss",
    "function",
    "prevent",
    "overfitting",
    "underfitting",
    "means",
    "uh",
    "case",
    "going",
    "go",
    "ahead",
    "take",
    "look",
    "couple",
    "different",
    "things",
    "going",
    "look",
    "regularization",
    "start",
    "linear",
    "model",
    "look",
    "ridge",
    "regularization",
    "lasso",
    "regularization",
    "models",
    "like",
    "like",
    "mlp",
    "positron",
    "sk",
    "learn",
    "module",
    "could",
    "bring",
    "ridge",
    "module",
    "bring",
    "lasso",
    "module",
    "talk",
    "ridge",
    "regression",
    "modifies",
    "overfitted",
    "underfitted",
    "models",
    "adding",
    "penalty",
    "equivalent",
    "sum",
    "squares",
    "magnitude",
    "coefficients",
    "cost",
    "function",
    "equals",
    "loss",
    "equals",
    "lambda",
    "times",
    "sum",
    "w",
    "squared",
    "absolute",
    "value",
    "w",
    "depending",
    "remember",
    "talked",
    "error",
    "whether",
    "either",
    "square",
    "absolute",
    "value",
    "removes",
    "plus",
    "minus",
    "sign",
    "reasons",
    "either",
    "way",
    "common",
    "square",
    "value",
    "um",
    "case",
    "lambda",
    "going",
    "penalty",
    "errors",
    "thrown",
    "greek",
    "character",
    "confuse",
    "everybody",
    "w",
    "slope",
    "curve",
    "line",
    "uh",
    "going",
    "look",
    "going",
    "draw",
    "line",
    "going",
    "like",
    "linear",
    "regression",
    "model",
    "sklearn",
    "could",
    "import",
    "standard",
    "linear",
    "regression",
    "model",
    "would",
    "plot",
    "line",
    "across",
    "whatever",
    "data",
    "working",
    "look",
    "course",
    "extrapolating",
    "know",
    "use",
    "specific",
    "data",
    "want",
    "get",
    "actual",
    "domain",
    "linear",
    "regression",
    "line",
    "let",
    "consider",
    "two",
    "points",
    "line",
    "go",
    "ahead",
    "loss",
    "equals",
    "zero",
    "uh",
    "considering",
    "two",
    "points",
    "line",
    "go",
    "ahead",
    "lambda",
    "equals",
    "one",
    "set",
    "w",
    "going",
    "one",
    "point",
    "four",
    "cost",
    "function",
    "equals",
    "zero",
    "plus",
    "one",
    "times",
    "one",
    "point",
    "four",
    "squared",
    "equals",
    "really",
    "get",
    "caught",
    "much",
    "math",
    "understanding",
    "something",
    "easy",
    "computer",
    "calculate",
    "ever",
    "see",
    "loss",
    "plus",
    "plus",
    "lambda",
    "times",
    "w",
    "sum",
    "w",
    "squared",
    "let",
    "say",
    "ridge",
    "regression",
    "line",
    "go",
    "ahead",
    "plot",
    "calculations",
    "data",
    "ridge",
    "regression",
    "let",
    "assume",
    "loss",
    "equals",
    "squared",
    "plus",
    "squared",
    "equals",
    "put",
    "calculations",
    "two",
    "points",
    "end",
    "linear",
    "regression",
    "model",
    "ridge",
    "regression",
    "model",
    "ridge",
    "regression",
    "model",
    "plots",
    "little",
    "differently",
    "standard",
    "linear",
    "regression",
    "model",
    "comparing",
    "two",
    "models",
    "data",
    "points",
    "see",
    "ridge",
    "regression",
    "line",
    "fits",
    "model",
    "accurately",
    "linear",
    "regression",
    "line",
    "find",
    "true",
    "lot",
    "data",
    "work",
    "end",
    "using",
    "either",
    "ridge",
    "regression",
    "model",
    "lasso",
    "mars",
    "regression",
    "model",
    "fitting",
    "especially",
    "dealing",
    "lot",
    "like",
    "stock",
    "markets",
    "daily",
    "setup",
    "come",
    "slightly",
    "better",
    "get",
    "slightly",
    "better",
    "fit",
    "lasso",
    "talked",
    "lasso",
    "coming",
    "cost",
    "function",
    "equals",
    "instead",
    "squared",
    "going",
    "absolute",
    "value",
    "remember",
    "ridge",
    "regression",
    "changes",
    "ridge",
    "regression",
    "model",
    "squaring",
    "value",
    "look",
    "squaring",
    "value",
    "finding",
    "absolute",
    "value",
    "loss",
    "squared",
    "individuals",
    "lambda",
    "symbol",
    "penalty",
    "errors",
    "w",
    "equals",
    "slope",
    "curve",
    "comparing",
    "two",
    "models",
    "data",
    "points",
    "see",
    "lasso",
    "regression",
    "line",
    "fits",
    "model",
    "accurately",
    "linear",
    "regression",
    "line",
    "like",
    "said",
    "use",
    "two",
    "models",
    "lot",
    "uh",
    "ridge",
    "important",
    "kind",
    "meat",
    "matter",
    "know",
    "one",
    "use",
    "bunch",
    "times",
    "figure",
    "ridge",
    "regularization",
    "useful",
    "many",
    "variables",
    "relatively",
    "smaller",
    "data",
    "samples",
    "model",
    "encourage",
    "convergence",
    "towards",
    "zero",
    "likely",
    "make",
    "closer",
    "zero",
    "prevent",
    "overfitting",
    "lasser",
    "regularization",
    "model",
    "preferred",
    "fitting",
    "linear",
    "model",
    "fewer",
    "variables",
    "la",
    "iris",
    "thing",
    "four",
    "five",
    "variables",
    "measure",
    "different",
    "leaf",
    "pieces",
    "uh",
    "might",
    "measurements",
    "cancer",
    "project",
    "36",
    "different",
    "variables",
    "get",
    "iris",
    "four",
    "variables",
    "lasso",
    "lar",
    "probably",
    "uh",
    "work",
    "pretty",
    "good",
    "might",
    "use",
    "ridge",
    "regularization",
    "model",
    "something",
    "significant",
    "larger",
    "encourages",
    "coefficients",
    "variables",
    "go",
    "towards",
    "zero",
    "shape",
    "constraint",
    "absolute",
    "value",
    "want",
    "go",
    "ahead",
    "demo",
    "lasso",
    "ridge",
    "regression",
    "let",
    "take",
    "look",
    "see",
    "looks",
    "like",
    "code",
    "bring",
    "jupiter",
    "notebook",
    "start",
    "imports",
    "pandas",
    "pd",
    "import",
    "numpy",
    "np",
    "import",
    "matplot",
    "library",
    "plt",
    "sklearn",
    "going",
    "import",
    "data",
    "sets",
    "kind",
    "generic",
    "usually",
    "import",
    "one",
    "data",
    "set",
    "instead",
    "know",
    "quick",
    "dirty",
    "putting",
    "together",
    "sklearn",
    "model",
    "selection",
    "going",
    "import",
    "train",
    "test",
    "split",
    "splitting",
    "data",
    "bring",
    "linear",
    "regression",
    "model",
    "go",
    "ahead",
    "run",
    "load",
    "load",
    "data",
    "set",
    "talking",
    "could",
    "imported",
    "load",
    "boston",
    "boston",
    "data",
    "set",
    "instead",
    "loading",
    "data",
    "sets",
    "loaded",
    "data",
    "set",
    "want",
    "go",
    "ahead",
    "take",
    "look",
    "data",
    "see",
    "got",
    "let",
    "go",
    "pop",
    "go",
    "run",
    "gone",
    "ahead",
    "taken",
    "uh",
    "boston",
    "uh",
    "data",
    "gon",
    "na",
    "look",
    "put",
    "pandas",
    "data",
    "frame",
    "um",
    "boston",
    "data",
    "set",
    "boston",
    "columns",
    "want",
    "see",
    "going",
    "target",
    "house",
    "price",
    "etc",
    "x",
    "equals",
    "boston",
    "eye",
    "location",
    "remember",
    "pandas",
    "new",
    "updates",
    "pandas",
    "want",
    "eye",
    "location",
    "going",
    "pull",
    "data",
    "used",
    "able",
    "leave",
    "something",
    "different",
    "creates",
    "slice",
    "versus",
    "direct",
    "setup",
    "make",
    "sure",
    "using",
    "eye",
    "location",
    "output",
    "bringing",
    "data",
    "together",
    "see",
    "print",
    "boston",
    "panda",
    "head",
    "see",
    "different",
    "aspects",
    "looking",
    "following",
    "x",
    "x",
    "everything",
    "except",
    "last",
    "column",
    "uh",
    "means",
    "rows",
    "except",
    "last",
    "column",
    "rows",
    "last",
    "column",
    "house",
    "price",
    "x",
    "crimsian",
    "industry",
    "chas",
    "knox",
    "different",
    "statistics",
    "collected",
    "house",
    "sales",
    "boston",
    "go",
    "oops",
    "control",
    "go",
    "ahead",
    "split",
    "data",
    "x",
    "train",
    "x",
    "test",
    "train",
    "test",
    "equals",
    "train",
    "test",
    "split",
    "imported",
    "boston",
    "could",
    "easily",
    "used",
    "x",
    "opposed",
    "boston",
    "eye",
    "location",
    "create",
    "test",
    "size",
    "going",
    "take",
    "25",
    "data",
    "put",
    "test",
    "go",
    "ahead",
    "run",
    "need",
    "extra",
    "drink",
    "uh",
    "train",
    "test",
    "course",
    "print",
    "train",
    "data",
    "shape",
    "love",
    "kind",
    "thing",
    "whenever",
    "working",
    "data",
    "print",
    "shape",
    "make",
    "sure",
    "everything",
    "looks",
    "correct",
    "uh",
    "127",
    "13",
    "127",
    "one",
    "379",
    "13",
    "match",
    "data",
    "sets",
    "quite",
    "matching",
    "know",
    "something",
    "wrong",
    "going",
    "get",
    "errors",
    "know",
    "many",
    "times",
    "gone",
    "dropped",
    "row",
    "one",
    "something",
    "weird",
    "happened",
    "cleaning",
    "data",
    "pretty",
    "straightforward",
    "simple",
    "data",
    "comes",
    "nice",
    "clean",
    "let",
    "go",
    "ahead",
    "apply",
    "apply",
    "multiple",
    "linear",
    "regression",
    "model",
    "call",
    "lreg",
    "reg",
    "linear",
    "regression",
    "going",
    "go",
    "ahead",
    "fit",
    "linear",
    "regression",
    "model",
    "x",
    "train",
    "train",
    "generate",
    "prediction",
    "test",
    "set",
    "l",
    "reg",
    "predict",
    "x",
    "test",
    "going",
    "prediction",
    "let",
    "calculate",
    "mean",
    "square",
    "error",
    "mse",
    "told",
    "see",
    "mse",
    "used",
    "lot",
    "people",
    "use",
    "variables",
    "things",
    "like",
    "pretty",
    "common",
    "get",
    "mean",
    "squared",
    "error",
    "equals",
    "basic",
    "formula",
    "already",
    "talking",
    "difference",
    "squared",
    "look",
    "average",
    "go",
    "ahead",
    "run",
    "see",
    "get",
    "end",
    "mean",
    "square",
    "error",
    "test",
    "total",
    "column",
    "coming",
    "point",
    "unless",
    "really",
    "know",
    "data",
    "working",
    "going",
    "mean",
    "whole",
    "lot",
    "domain",
    "might",
    "know",
    "looking",
    "see",
    "kinds",
    "numbers",
    "coming",
    "bunch",
    "numbers",
    "okay",
    "least",
    "okay",
    "demo",
    "uh",
    "gon",
    "na",
    "go",
    "ahead",
    "plot",
    "see",
    "going",
    "always",
    "kind",
    "fun",
    "always",
    "nice",
    "nice",
    "visual",
    "looking",
    "see",
    "plot",
    "coefficient",
    "scores",
    "guys",
    "back",
    "great",
    "job",
    "putting",
    "pretty",
    "colors",
    "together",
    "making",
    "look",
    "nice",
    "setting",
    "columns",
    "see",
    "uh",
    "nox",
    "like",
    "huge",
    "coefficient",
    "look",
    "table",
    "like",
    "look",
    "little",
    "different",
    "coefficients",
    "using",
    "huge",
    "change",
    "huge",
    "changes",
    "flags",
    "kinds",
    "things",
    "working",
    "data",
    "depends",
    "much",
    "domain",
    "working",
    "great",
    "things",
    "though",
    "quick",
    "look",
    "see",
    "going",
    "data",
    "looking",
    "course",
    "look",
    "motive",
    "reduce",
    "coefficient",
    "score",
    "want",
    "take",
    "bring",
    "much",
    "going",
    "work",
    "ridge",
    "regression",
    "let",
    "start",
    "going",
    "going",
    "import",
    "ridge",
    "model",
    "red",
    "regression",
    "sk",
    "learn",
    "library",
    "scikit",
    "going",
    "go",
    "ahead",
    "train",
    "model",
    "ridge",
    "r",
    "equals",
    "alpha",
    "equals",
    "mentioned",
    "earlier",
    "work",
    "ridge",
    "model",
    "see",
    "alpha",
    "equals",
    "one",
    "set",
    "alpha",
    "equal",
    "zero",
    "standard",
    "linear",
    "regression",
    "model",
    "alpha",
    "equals",
    "one",
    "two",
    "three",
    "four",
    "usually",
    "use",
    "one",
    "two",
    "three",
    "four",
    "standard",
    "integer",
    "go",
    "ahead",
    "fit",
    "ridge",
    "model",
    "x",
    "train",
    "train",
    "data",
    "generate",
    "prediction",
    "x",
    "test",
    "calculate",
    "mean",
    "square",
    "error",
    "like",
    "look",
    "familiar",
    "go",
    "ahead",
    "print",
    "look",
    "ridge",
    "coefficients",
    "data",
    "see",
    "looks",
    "like",
    "jump",
    "two",
    "get",
    "headache",
    "laughter",
    "still",
    "see",
    "knox",
    "value",
    "let",
    "look",
    "knocks",
    "biggest",
    "value",
    "minus",
    "nine",
    "go",
    "back",
    "nox",
    "value",
    "minus",
    "right",
    "bat",
    "seeing",
    "huge",
    "change",
    "biggest",
    "coefficient",
    "uh",
    "going",
    "nice",
    "setup",
    "want",
    "go",
    "ahead",
    "print",
    "see",
    "looks",
    "like",
    "go",
    "set",
    "plot",
    "subplots",
    "team",
    "put",
    "together",
    "nice",
    "colors",
    "makes",
    "look",
    "good",
    "x",
    "bar",
    "based",
    "columns",
    "l",
    "regress",
    "coefficients",
    "color",
    "equals",
    "color",
    "x",
    "spine",
    "bottom",
    "forth",
    "put",
    "together",
    "nice",
    "little",
    "graph",
    "starting",
    "see",
    "one",
    "compare",
    "put",
    "graph",
    "one",
    "minus",
    "18",
    "minus",
    "nine",
    "graph",
    "half",
    "size",
    "graph",
    "thing",
    "values",
    "might",
    "look",
    "actually",
    "almost",
    "half",
    "value",
    "finally",
    "thing",
    "lasso",
    "regression",
    "would",
    "look",
    "similar",
    "far",
    "worked",
    "going",
    "print",
    "run",
    "let",
    "go",
    "knox",
    "look",
    "nox",
    "way",
    "zero",
    "look",
    "next",
    "biggest",
    "coefficient",
    "minus",
    "really",
    "let",
    "go",
    "um",
    "go",
    "look",
    "number",
    "uh",
    "look",
    "uh",
    "running",
    "uh",
    "working",
    "project",
    "would",
    "look",
    "numbers",
    "start",
    "come",
    "compare",
    "uh",
    "6",
    "9",
    "better",
    "7",
    "beginning",
    "might",
    "start",
    "looking",
    "first",
    "model",
    "overall",
    "predicting",
    "factors",
    "involved",
    "might",
    "know",
    "uh",
    "nox",
    "value",
    "central",
    "ones",
    "quite",
    "good",
    "might",
    "start",
    "looking",
    "certain",
    "setups",
    "like",
    "particular",
    "coefficient",
    "might",
    "certain",
    "meaning",
    "us",
    "forth",
    "look",
    "different",
    "items",
    "bottom",
    "dollar",
    "first",
    "model",
    "better",
    "two",
    "models",
    "mean",
    "square",
    "error",
    "test",
    "set",
    "continues",
    "come",
    "dimensionality",
    "reduction",
    "dimensionality",
    "reduction",
    "refers",
    "technique",
    "reduces",
    "number",
    "input",
    "variables",
    "data",
    "set",
    "see",
    "table",
    "right",
    "shows",
    "orders",
    "made",
    "automobile",
    "parts",
    "retailer",
    "retailer",
    "sells",
    "different",
    "automobile",
    "parts",
    "different",
    "companies",
    "see",
    "company",
    "iso",
    "max",
    "item",
    "tire",
    "axle",
    "order",
    "id",
    "price",
    "number",
    "quantity",
    "order",
    "predict",
    "future",
    "cells",
    "find",
    "using",
    "correlation",
    "analysis",
    "need",
    "three",
    "attributes",
    "therefore",
    "reduced",
    "number",
    "attributes",
    "five",
    "three",
    "clearly",
    "really",
    "care",
    "part",
    "number",
    "think",
    "part",
    "number",
    "would",
    "effect",
    "many",
    "tires",
    "bought",
    "even",
    "store",
    "buying",
    "probably",
    "effect",
    "case",
    "actually",
    "done",
    "remove",
    "item",
    "tire",
    "price",
    "quantity",
    "one",
    "things",
    "taking",
    "away",
    "scheme",
    "things",
    "descriptive",
    "phase",
    "describing",
    "data",
    "data",
    "clean",
    "dimensionality",
    "reduction",
    "well",
    "number",
    "one",
    "less",
    "dimensions",
    "given",
    "data",
    "set",
    "means",
    "less",
    "computation",
    "training",
    "time",
    "really",
    "important",
    "trying",
    "number",
    "different",
    "models",
    "even",
    "seven",
    "gigabytes",
    "data",
    "start",
    "taking",
    "days",
    "go",
    "different",
    "models",
    "huge",
    "probably",
    "hugest",
    "part",
    "far",
    "reducing",
    "data",
    "set",
    "redundancy",
    "removed",
    "removing",
    "similar",
    "entries",
    "data",
    "set",
    "models",
    "like",
    "neural",
    "network",
    "put",
    "two",
    "data",
    "might",
    "give",
    "higher",
    "weight",
    "would",
    "want",
    "get",
    "rid",
    "redundancy",
    "also",
    "increases",
    "processing",
    "time",
    "multiple",
    "data",
    "coming",
    "space",
    "required",
    "store",
    "data",
    "reduced",
    "committing",
    "big",
    "data",
    "pool",
    "might",
    "send",
    "company",
    "bought",
    "would",
    "want",
    "store",
    "two",
    "whole",
    "extra",
    "columns",
    "added",
    "pool",
    "data",
    "makes",
    "data",
    "easy",
    "plotting",
    "2d",
    "3d",
    "plots",
    "favorite",
    "part",
    "important",
    "shareholder",
    "meeting",
    "want",
    "able",
    "give",
    "really",
    "good",
    "clear",
    "simplified",
    "version",
    "want",
    "reduce",
    "something",
    "people",
    "take",
    "helps",
    "find",
    "significant",
    "features",
    "skip",
    "rest",
    "also",
    "comes",
    "post",
    "scribing",
    "uh",
    "leads",
    "better",
    "human",
    "interpretation",
    "kind",
    "goes",
    "number",
    "four",
    "makes",
    "data",
    "easy",
    "plotting",
    "better",
    "interpretation",
    "looking",
    "principal",
    "component",
    "analysis",
    "principal",
    "component",
    "analysis",
    "technique",
    "reducing",
    "dimensionality",
    "data",
    "sets",
    "increasing",
    "interpretability",
    "time",
    "minimizing",
    "information",
    "loss",
    "take",
    "complex",
    "data",
    "set",
    "lots",
    "variables",
    "run",
    "pca",
    "reduce",
    "variables",
    "end",
    "reduced",
    "variable",
    "setup",
    "confusing",
    "look",
    "look",
    "end",
    "result",
    "different",
    "colors",
    "lined",
    "going",
    "take",
    "look",
    "let",
    "say",
    "picture",
    "let",
    "say",
    "asked",
    "take",
    "picture",
    "toddlers",
    "deciding",
    "angle",
    "would",
    "best",
    "take",
    "picture",
    "come",
    "look",
    "say",
    "okay",
    "know",
    "one",
    "angle",
    "get",
    "back",
    "lot",
    "heads",
    "many",
    "faces",
    "might",
    "get",
    "one",
    "person",
    "front",
    "smiling",
    "lot",
    "people",
    "class",
    "missing",
    "huge",
    "amount",
    "right",
    "blank",
    "space",
    "maybe",
    "back",
    "someone",
    "head",
    "turns",
    "best",
    "angle",
    "click",
    "picture",
    "might",
    "bottom",
    "left",
    "angle",
    "look",
    "say",
    "hey",
    "makes",
    "sense",
    "good",
    "configuration",
    "people",
    "picture",
    "talking",
    "data",
    "really",
    "ca",
    "think",
    "going",
    "best",
    "need",
    "kind",
    "mathematical",
    "formula",
    "consistent",
    "makes",
    "sense",
    "back",
    "end",
    "one",
    "projects",
    "worked",
    "many",
    "years",
    "ago",
    "something",
    "similar",
    "iris",
    "ever",
    "done",
    "iris",
    "data",
    "sets",
    "probably",
    "one",
    "common",
    "ones",
    "flower",
    "measuring",
    "stamen",
    "petals",
    "width",
    "length",
    "petal",
    "instead",
    "putting",
    "width",
    "length",
    "petal",
    "could",
    "easily",
    "ratio",
    "divide",
    "width",
    "length",
    "get",
    "single",
    "number",
    "two",
    "kind",
    "idea",
    "going",
    "looking",
    "bring",
    "data",
    "simplified",
    "example",
    "iris",
    "pedal",
    "example",
    "look",
    "similarity",
    "pca",
    "find",
    "best",
    "picture",
    "projection",
    "data",
    "points",
    "look",
    "one",
    "angle",
    "drawn",
    "line",
    "see",
    "data",
    "points",
    "based",
    "case",
    "two",
    "variables",
    "keep",
    "mind",
    "usually",
    "talking",
    "36",
    "40",
    "variables",
    "almost",
    "business",
    "models",
    "usually",
    "26",
    "27",
    "different",
    "variables",
    "looking",
    "thing",
    "like",
    "bank",
    "loan",
    "model",
    "talking",
    "26",
    "36",
    "different",
    "variables",
    "looking",
    "going",
    "want",
    "want",
    "find",
    "best",
    "view",
    "case",
    "looking",
    "x",
    "look",
    "second",
    "idea",
    "pc2",
    "looking",
    "x",
    "x",
    "time",
    "different",
    "direction",
    "ease",
    "consider",
    "get",
    "two",
    "principal",
    "components",
    "namely",
    "pc1",
    "pc2",
    "comparing",
    "principal",
    "components",
    "find",
    "data",
    "points",
    "sufficiently",
    "spaced",
    "pc1",
    "look",
    "got",
    "pc1",
    "see",
    "along",
    "line",
    "data",
    "points",
    "spaced",
    "versus",
    "spacing",
    "pc2",
    "coming",
    "going",
    "give",
    "us",
    "best",
    "look",
    "data",
    "points",
    "combine",
    "looking",
    "single",
    "angle",
    "whereas",
    "pc2",
    "less",
    "spaced",
    "makes",
    "observation",
    "calculations",
    "much",
    "difficult",
    "therefore",
    "accept",
    "pc1",
    "pc2",
    "data",
    "points",
    "spaced",
    "obviously",
    "back",
    "end",
    "calculations",
    "little",
    "bit",
    "complicated",
    "get",
    "math",
    "decide",
    "valuable",
    "gives",
    "idea",
    "though",
    "talking",
    "talking",
    "perspective",
    "would",
    "help",
    "understanding",
    "pca",
    "analysis",
    "works",
    "want",
    "go",
    "ahead",
    "dive",
    "important",
    "terminologies",
    "pca",
    "important",
    "terminologies",
    "views",
    "perspective",
    "data",
    "points",
    "observed",
    "hear",
    "someone",
    "talking",
    "pca",
    "presentation",
    "taking",
    "time",
    "reduce",
    "something",
    "average",
    "person",
    "shareholders",
    "understand",
    "might",
    "hear",
    "refer",
    "different",
    "views",
    "view",
    "taking",
    "dimension",
    "number",
    "columns",
    "data",
    "set",
    "called",
    "dimensions",
    "data",
    "set",
    "talked",
    "hear",
    "features",
    "dimensions",
    "talking",
    "features",
    "usually",
    "running",
    "business",
    "talking",
    "25",
    "26",
    "27",
    "different",
    "features",
    "minimal",
    "principal",
    "component",
    "new",
    "variables",
    "constructed",
    "linear",
    "combinations",
    "mixtures",
    "initial",
    "variables",
    "principal",
    "component",
    "important",
    "combination",
    "remember",
    "flower",
    "example",
    "would",
    "width",
    "length",
    "petal",
    "opposed",
    "putting",
    "width",
    "length",
    "put",
    "ratio",
    "instead",
    "single",
    "number",
    "versus",
    "two",
    "separate",
    "numbers",
    "projections",
    "perpendicular",
    "distance",
    "principal",
    "component",
    "data",
    "points",
    "goes",
    "line",
    "earlier",
    "right",
    "angle",
    "line",
    "point",
    "points",
    "fall",
    "onto",
    "line",
    "important",
    "properties",
    "important",
    "properties",
    "number",
    "principal",
    "components",
    "always",
    "less",
    "equal",
    "number",
    "attributes",
    "makes",
    "common",
    "sense",
    "going",
    "10",
    "principal",
    "properties",
    "three",
    "features",
    "trying",
    "reduce",
    "kind",
    "goofy",
    "important",
    "remember",
    "people",
    "throw",
    "weird",
    "code",
    "randomly",
    "stuff",
    "instead",
    "really",
    "thinking",
    "principle",
    "components",
    "orthogonal",
    "talking",
    "right",
    "angle",
    "line",
    "pc1",
    "looking",
    "points",
    "fall",
    "line",
    "thing",
    "pc2",
    "want",
    "make",
    "sure",
    "pc1",
    "equal",
    "pc2",
    "want",
    "two",
    "principal",
    "points",
    "two",
    "points",
    "priority",
    "principal",
    "components",
    "decreases",
    "numbers",
    "increase",
    "important",
    "understand",
    "going",
    "create",
    "one",
    "principle",
    "component",
    "everything",
    "summarized",
    "one",
    "component",
    "go",
    "two",
    "components",
    "priority",
    "much",
    "holds",
    "value",
    "decreases",
    "go",
    "five",
    "different",
    "points",
    "one",
    "points",
    "going",
    "less",
    "value",
    "one",
    "point",
    "everything",
    "summarized",
    "pca",
    "works",
    "said",
    "back",
    "end",
    "talk",
    "math",
    "talking",
    "actually",
    "work",
    "understanding",
    "looking",
    "perspective",
    "want",
    "see",
    "math",
    "side",
    "works",
    "pca",
    "performs",
    "following",
    "operations",
    "order",
    "evaluate",
    "principal",
    "components",
    "given",
    "data",
    "set",
    "first",
    "start",
    "standardization",
    "covariance",
    "matrix",
    "computation",
    "use",
    "generate",
    "gene",
    "vectors",
    "gene",
    "values",
    "feature",
    "vector",
    "remember",
    "gene",
    "vector",
    "like",
    "translation",
    "moving",
    "data",
    "x",
    "equals",
    "1",
    "x",
    "equals",
    "2",
    "whatever",
    "altering",
    "gene",
    "value",
    "final",
    "value",
    "generate",
    "talk",
    "standardization",
    "main",
    "aim",
    "step",
    "standardize",
    "range",
    "attributes",
    "one",
    "lie",
    "within",
    "similar",
    "boundaries",
    "process",
    "involves",
    "removal",
    "mean",
    "variable",
    "values",
    "scaling",
    "data",
    "respect",
    "standard",
    "deviation",
    "see",
    "z",
    "equals",
    "variable",
    "values",
    "minus",
    "mean",
    "standard",
    "deviation",
    "covariance",
    "matrix",
    "computation",
    "covariance",
    "matrix",
    "used",
    "express",
    "correlation",
    "two",
    "attributes",
    "data",
    "set",
    "covariance",
    "matrix",
    "entries",
    "variance",
    "covariance",
    "tribute",
    "values",
    "variance",
    "denoted",
    "var",
    "covariance",
    "denoted",
    "cov",
    "right",
    "see",
    "covariance",
    "matrix",
    "two",
    "attributes",
    "values",
    "look",
    "code",
    "display",
    "see",
    "talking",
    "looks",
    "like",
    "notice",
    "matrix",
    "generating",
    "variance",
    "covariance",
    "x",
    "right",
    "side",
    "see",
    "covariance",
    "table",
    "two",
    "attributes",
    "data",
    "set",
    "talking",
    "usually",
    "looking",
    "one",
    "feature",
    "two",
    "features",
    "usually",
    "looking",
    "25",
    "30",
    "features",
    "going",
    "set",
    "like",
    "see",
    "different",
    "features",
    "different",
    "variables",
    "covariance",
    "matrix",
    "tells",
    "us",
    "two",
    "variables",
    "related",
    "positive",
    "covariance",
    "indicate",
    "value",
    "one",
    "variable",
    "directly",
    "proportional",
    "variable",
    "negative",
    "covariance",
    "indicate",
    "value",
    "one",
    "variable",
    "inversely",
    "proportional",
    "variable",
    "always",
    "important",
    "note",
    "whenever",
    "matrixes",
    "going",
    "looking",
    "positive",
    "negative",
    "whether",
    "inverted",
    "iogene",
    "values",
    "gene",
    "vectors",
    "iogene",
    "values",
    "hygiene",
    "vectors",
    "mathematical",
    "value",
    "extracted",
    "covariance",
    "table",
    "responsible",
    "generation",
    "new",
    "set",
    "variables",
    "old",
    "set",
    "variables",
    "lead",
    "construction",
    "principal",
    "components",
    "igen",
    "vectors",
    "change",
    "directions",
    "linear",
    "transformation",
    "gene",
    "values",
    "scalars",
    "magnitude",
    "gene",
    "vectors",
    "chain",
    "transforming",
    "data",
    "going",
    "change",
    "vector",
    "b",
    "b",
    "prime",
    "denoted",
    "b",
    "chart",
    "like",
    "multiple",
    "variables",
    "calculate",
    "new",
    "variable",
    "feature",
    "vectors",
    "feature",
    "vectors",
    "simply",
    "matrix",
    "igen",
    "vectors",
    "components",
    "decide",
    "keep",
    "columns",
    "decide",
    "whether",
    "must",
    "keep",
    "discard",
    "less",
    "significant",
    "principal",
    "components",
    "generated",
    "steps",
    "becomes",
    "really",
    "important",
    "start",
    "looking",
    "back",
    "end",
    "demo",
    "one",
    "important",
    "steps",
    "understand",
    "pca",
    "example",
    "consider",
    "matrix",
    "x",
    "within",
    "rows",
    "observations",
    "k",
    "columns",
    "variables",
    "matrix",
    "would",
    "construct",
    "variable",
    "space",
    "many",
    "dimensions",
    "variable",
    "simplicity",
    "let",
    "consider",
    "three",
    "dimensions",
    "observation",
    "row",
    "matrix",
    "x",
    "placed",
    "k",
    "dimensional",
    "variable",
    "space",
    "rows",
    "data",
    "table",
    "form",
    "swarm",
    "points",
    "space",
    "find",
    "mean",
    "observations",
    "place",
    "along",
    "data",
    "points",
    "plot",
    "first",
    "principal",
    "component",
    "line",
    "best",
    "accounts",
    "shape",
    "point",
    "swarm",
    "represents",
    "maximum",
    "variance",
    "direction",
    "data",
    "observation",
    "may",
    "projected",
    "onto",
    "line",
    "order",
    "get",
    "coordinate",
    "value",
    "along",
    "pc",
    "one",
    "value",
    "known",
    "score",
    "usually",
    "one",
    "principal",
    "component",
    "insufficient",
    "model",
    "systematic",
    "variation",
    "data",
    "set",
    "thus",
    "second",
    "principal",
    "axis",
    "created",
    "second",
    "principle",
    "component",
    "oriented",
    "reflects",
    "second",
    "largest",
    "source",
    "variation",
    "data",
    "orthogonal",
    "pc1",
    "pc2",
    "also",
    "passes",
    "average",
    "point",
    "let",
    "go",
    "ahead",
    "pull",
    "see",
    "means",
    "inside",
    "python",
    "scripting",
    "going",
    "use",
    "anaconda",
    "navigator",
    "python",
    "example",
    "believe",
    "even",
    "like",
    "tend",
    "stay",
    "lot",
    "models",
    "use",
    "especially",
    "neural",
    "networks",
    "stable",
    "three",
    "six",
    "open",
    "jupiter",
    "chrome",
    "go",
    "ahead",
    "create",
    "new",
    "python",
    "three",
    "ease",
    "use",
    "team",
    "back",
    "nice",
    "enough",
    "put",
    "together",
    "go",
    "start",
    "libraries",
    "first",
    "thing",
    "like",
    "whenever",
    "looking",
    "new",
    "setup",
    "well",
    "know",
    "let",
    "let",
    "libraries",
    "first",
    "going",
    "basic",
    "libraries",
    "matplot",
    "library",
    "plt",
    "matplot",
    "library",
    "pandas",
    "data",
    "frame",
    "pd",
    "numpy",
    "numbers",
    "array",
    "np",
    "seaborn",
    "graphing",
    "sns",
    "goes",
    "plot",
    "actually",
    "sits",
    "matplot",
    "library",
    "seaborn",
    "sits",
    "amber",
    "sign",
    "jupiter",
    "notebook",
    "map",
    "plot",
    "library",
    "line",
    "newer",
    "version",
    "actually",
    "require",
    "put",
    "either",
    "anyway",
    "used",
    "want",
    "go",
    "ahead",
    "take",
    "look",
    "data",
    "case",
    "going",
    "pull",
    "certainly",
    "lots",
    "fun",
    "different",
    "data",
    "going",
    "use",
    "cancer",
    "data",
    "set",
    "one",
    "reasons",
    "cancer",
    "data",
    "set",
    "like",
    "36",
    "35",
    "different",
    "features",
    "kind",
    "fun",
    "use",
    "base",
    "go",
    "ahead",
    "run",
    "look",
    "keys",
    "first",
    "thing",
    "notice",
    "keys",
    "cancer",
    "data",
    "set",
    "data",
    "target",
    "frame",
    "target",
    "names",
    "description",
    "feature",
    "names",
    "file",
    "name",
    "looking",
    "let",
    "take",
    "look",
    "description",
    "let",
    "go",
    "pull",
    "description",
    "going",
    "spend",
    "huge",
    "amount",
    "time",
    "description",
    "um",
    "want",
    "get",
    "medical",
    "domain",
    "want",
    "focus",
    "pca",
    "setup",
    "important",
    "start",
    "looking",
    "different",
    "attributes",
    "mean",
    "medical",
    "field",
    "want",
    "note",
    "different",
    "things",
    "whether",
    "measuring",
    "coming",
    "actually",
    "see",
    "actual",
    "different",
    "measurements",
    "taking",
    "missing",
    "attributes",
    "page",
    "way",
    "bottom",
    "going",
    "data",
    "case",
    "target",
    "dig",
    "deep",
    "enough",
    "target",
    "let",
    "actually",
    "let",
    "go",
    "ahead",
    "print",
    "target",
    "names",
    "real",
    "quick",
    "always",
    "like",
    "take",
    "look",
    "see",
    "end",
    "target",
    "names",
    "run",
    "yeah",
    "target",
    "name",
    "malignant",
    "b9",
    "words",
    "dangerous",
    "growth",
    "something",
    "worry",
    "bottom",
    "line",
    "cancer",
    "case",
    "go",
    "ahead",
    "load",
    "data",
    "know",
    "let",
    "go",
    "uh",
    "notch",
    "easy",
    "reading",
    "hard",
    "get",
    "right",
    "let",
    "go",
    "ahead",
    "look",
    "data",
    "uh",
    "going",
    "use",
    "pandas",
    "going",
    "go",
    "ahead",
    "data",
    "frame",
    "going",
    "equal",
    "cancer",
    "data",
    "columns",
    "equals",
    "cancer",
    "feature",
    "equals",
    "feature",
    "names",
    "remember",
    "already",
    "loaded",
    "names",
    "features",
    "going",
    "come",
    "let",
    "see",
    "get",
    "top",
    "target",
    "names",
    "list",
    "names",
    "setup",
    "go",
    "ahead",
    "run",
    "code",
    "print",
    "head",
    "see",
    "mean",
    "radius",
    "mean",
    "texture",
    "mean",
    "perimeter",
    "know",
    "wonderful",
    "data",
    "set",
    "playing",
    "like",
    "many",
    "data",
    "data",
    "comes",
    "half",
    "time",
    "even",
    "know",
    "looking",
    "handed",
    "bunch",
    "stuff",
    "data",
    "scientist",
    "going",
    "heck",
    "good",
    "place",
    "start",
    "number",
    "different",
    "features",
    "idea",
    "feature",
    "means",
    "come",
    "want",
    "look",
    "data",
    "figure",
    "actually",
    "getting",
    "pca",
    "side",
    "noticed",
    "difficult",
    "visualize",
    "high",
    "dimensional",
    "data",
    "use",
    "pca",
    "find",
    "first",
    "two",
    "principal",
    "components",
    "visualize",
    "data",
    "new",
    "space",
    "single",
    "scatter",
    "plot",
    "need",
    "go",
    "ahead",
    "scale",
    "data",
    "run",
    "see",
    "really",
    "scale",
    "data",
    "general",
    "run",
    "time",
    "almost",
    "first",
    "step",
    "modeling",
    "even",
    "premodeling",
    "neural",
    "networks",
    "important",
    "pca",
    "visualization",
    "already",
    "going",
    "scale",
    "means",
    "deviation",
    "inside",
    "pca",
    "case",
    "always",
    "good",
    "scale",
    "going",
    "take",
    "pca",
    "site",
    "kit",
    "learn",
    "uses",
    "similar",
    "process",
    "functions",
    "come",
    "instantiate",
    "pca",
    "object",
    "find",
    "principal",
    "components",
    "using",
    "fit",
    "method",
    "apply",
    "rotation",
    "dimensionality",
    "reduction",
    "calling",
    "transform",
    "also",
    "specify",
    "many",
    "components",
    "want",
    "keep",
    "creating",
    "pca",
    "object",
    "code",
    "oops",
    "getting",
    "little",
    "bit",
    "ahead",
    "let",
    "go",
    "run",
    "code",
    "uh",
    "code",
    "sklearn",
    "decomposition",
    "import",
    "pca",
    "pca",
    "equals",
    "pca",
    "components",
    "equals",
    "two",
    "really",
    "important",
    "note",
    "going",
    "want",
    "look",
    "two",
    "components",
    "would",
    "never",
    "go",
    "four",
    "components",
    "especially",
    "going",
    "demo",
    "somebody",
    "else",
    "showing",
    "shareholders",
    "whole",
    "idea",
    "reduce",
    "something",
    "people",
    "see",
    "pca",
    "fit",
    "going",
    "going",
    "take",
    "scaled",
    "data",
    "generated",
    "see",
    "created",
    "pca",
    "model",
    "components",
    "equals",
    "two",
    "whenever",
    "use",
    "new",
    "tool",
    "like",
    "go",
    "actually",
    "see",
    "using",
    "let",
    "go",
    "scikit",
    "webpage",
    "pca",
    "see",
    "call",
    "statement",
    "describes",
    "different",
    "setups",
    "probably",
    "biggest",
    "one",
    "look",
    "would",
    "well",
    "biggest",
    "one",
    "components",
    "many",
    "components",
    "want",
    "put",
    "pretty",
    "much",
    "also",
    "might",
    "look",
    "svd",
    "solver",
    "auto",
    "right",
    "override",
    "different",
    "things",
    "pretty",
    "good",
    "job",
    "go",
    "way",
    "go",
    "methods",
    "notice",
    "fit",
    "fit",
    "transform",
    "nowhere",
    "predict",
    "used",
    "prediction",
    "used",
    "look",
    "data",
    "describe",
    "setup",
    "fitting",
    "data",
    "taking",
    "look",
    "already",
    "looked",
    "minimum",
    "maximum",
    "already",
    "looked",
    "quarter",
    "done",
    "full",
    "description",
    "data",
    "part",
    "describing",
    "data",
    "biggest",
    "thing",
    "take",
    "away",
    "come",
    "zooming",
    "course",
    "examples",
    "forget",
    "biggest",
    "one",
    "course",
    "number",
    "components",
    "mean",
    "rest",
    "play",
    "actual",
    "solver",
    "whether",
    "full",
    "randomize",
    "different",
    "things",
    "pretty",
    "good",
    "auto",
    "transform",
    "data",
    "first",
    "two",
    "principal",
    "components",
    "xpca",
    "going",
    "set",
    "equal",
    "pca",
    "transform",
    "scaled",
    "data",
    "go",
    "first",
    "transformation",
    "let",
    "go",
    "ahead",
    "print",
    "scaled",
    "data",
    "shape",
    "xpca",
    "data",
    "shape",
    "reason",
    "want",
    "show",
    "us",
    "going",
    "taken",
    "30",
    "features",
    "think",
    "said",
    "36",
    "something",
    "like",
    "30",
    "compressed",
    "two",
    "features",
    "decided",
    "wanted",
    "two",
    "features",
    "comes",
    "spawn",
    "two",
    "features",
    "let",
    "go",
    "ahead",
    "plot",
    "take",
    "look",
    "see",
    "going",
    "going",
    "use",
    "plt",
    "figure",
    "set",
    "figure",
    "size",
    "scatter",
    "plot",
    "xpca",
    "x",
    "underscore",
    "pca",
    "one",
    "two",
    "different",
    "perceptions",
    "using",
    "uh",
    "see",
    "right",
    "c",
    "color",
    "cancer",
    "equals",
    "target",
    "remember",
    "zero",
    "one",
    "remember",
    "correctly",
    "zero",
    "malignant",
    "one",
    "b9",
    "everything",
    "zero",
    "column",
    "going",
    "one",
    "color",
    "color",
    "going",
    "one",
    "going",
    "use",
    "plasma",
    "map",
    "kind",
    "telling",
    "color",
    "add",
    "labels",
    "first",
    "principle",
    "component",
    "second",
    "principal",
    "component",
    "go",
    "ahead",
    "run",
    "see",
    "instead",
    "chart",
    "one",
    "heat",
    "maps",
    "30",
    "different",
    "columns",
    "look",
    "say",
    "hey",
    "one",
    "actually",
    "pretty",
    "good",
    "job",
    "separating",
    "data",
    "couple",
    "things",
    "looking",
    "notice",
    "first",
    "clear",
    "area",
    "clumped",
    "together",
    "going",
    "b9",
    "huge",
    "area",
    "still",
    "clumped",
    "together",
    "spread",
    "going",
    "malignant",
    "think",
    "backwards",
    "middle",
    "dealing",
    "something",
    "particular",
    "case",
    "cancer",
    "would",
    "try",
    "separate",
    "would",
    "exploring",
    "separate",
    "middle",
    "group",
    "words",
    "area",
    "everything",
    "overlaps",
    "going",
    "clear",
    "result",
    "people",
    "want",
    "go",
    "extra",
    "tests",
    "treat",
    "differently",
    "versus",
    "going",
    "saying",
    "cutting",
    "cancer",
    "body",
    "absorbs",
    "dissipates",
    "versus",
    "uh",
    "actively",
    "going",
    "removing",
    "testing",
    "going",
    "chemo",
    "different",
    "things",
    "big",
    "difference",
    "know",
    "far",
    "going",
    "happen",
    "middle",
    "line",
    "overlap",
    "going",
    "huge",
    "domain",
    "specific",
    "going",
    "back",
    "data",
    "see",
    "clearly",
    "using",
    "two",
    "components",
    "easily",
    "separate",
    "two",
    "classes",
    "next",
    "step",
    "mean",
    "interpreting",
    "components",
    "unfortunately",
    "great",
    "power",
    "dimensionality",
    "reduction",
    "comes",
    "cost",
    "able",
    "easily",
    "understand",
    "components",
    "represent",
    "know",
    "principle",
    "component",
    "one",
    "licks",
    "work",
    "represents",
    "second",
    "principle",
    "components",
    "correspond",
    "combinations",
    "original",
    "features",
    "components",
    "stored",
    "attribute",
    "filtered",
    "pca",
    "object",
    "talk",
    "look",
    "go",
    "ahead",
    "look",
    "pca",
    "components",
    "model",
    "built",
    "trained",
    "run",
    "see",
    "actual",
    "components",
    "uh",
    "two",
    "components",
    "array",
    "within",
    "array",
    "see",
    "scores",
    "using",
    "actually",
    "give",
    "weight",
    "features",
    "numpy",
    "matrix",
    "array",
    "row",
    "represents",
    "principal",
    "component",
    "column",
    "relates",
    "back",
    "original",
    "features",
    "really",
    "neat",
    "go",
    "reverse",
    "drop",
    "onto",
    "heat",
    "map",
    "start",
    "seeing",
    "means",
    "let",
    "go",
    "ahead",
    "put",
    "upside",
    "go",
    "ahead",
    "put",
    "going",
    "use",
    "df",
    "comp",
    "data",
    "frame",
    "pca",
    "components",
    "want",
    "notice",
    "easy",
    "uh",
    "going",
    "set",
    "columns",
    "equal",
    "cancer",
    "feature",
    "names",
    "makes",
    "really",
    "easy",
    "dumping",
    "data",
    "frame",
    "neat",
    "data",
    "frame",
    "get",
    "seaborn",
    "pull",
    "data",
    "frame",
    "apart",
    "set",
    "us",
    "want",
    "going",
    "cn",
    "seaborn",
    "heat",
    "map",
    "data",
    "frame",
    "composition",
    "use",
    "plasma",
    "coloring",
    "creates",
    "nice",
    "little",
    "color",
    "graph",
    "see",
    "mean",
    "radius",
    "different",
    "features",
    "along",
    "bottom",
    "right",
    "scale",
    "see",
    "dark",
    "colors",
    "way",
    "really",
    "light",
    "colors",
    "really",
    "shining",
    "like",
    "primary",
    "stuff",
    "want",
    "look",
    "heat",
    "map",
    "color",
    "bar",
    "basically",
    "represent",
    "correlation",
    "various",
    "features",
    "principal",
    "component",
    "know",
    "powerful",
    "map",
    "look",
    "go",
    "might",
    "notice",
    "mean",
    "radius",
    "look",
    "bottom",
    "map",
    "interesting",
    "correlations",
    "change",
    "variations",
    "means",
    "get",
    "post",
    "scribe",
    "also",
    "use",
    "try",
    "guess",
    "things",
    "mean",
    "want",
    "change",
    "get",
    "better",
    "result",
    "session",
    "briefly",
    "understand",
    "coronavirus",
    "really",
    "various",
    "symptoms",
    "coronavirus",
    "look",
    "global",
    "impact",
    "coronavirus",
    "terms",
    "total",
    "cases",
    "deaths",
    "reported",
    "fatality",
    "rate",
    "tests",
    "carried",
    "far",
    "different",
    "countries",
    "perform",
    "analysis",
    "using",
    "svm",
    "polynomial",
    "regression",
    "python",
    "predict",
    "number",
    "upcoming",
    "cases",
    "28th",
    "april",
    "17th",
    "may",
    "data",
    "taken",
    "22nd",
    "january",
    "27th",
    "april",
    "analyze",
    "model",
    "using",
    "charts",
    "graphs",
    "finally",
    "discuss",
    "various",
    "safety",
    "measures",
    "take",
    "ensure",
    "safe",
    "attacked",
    "coronavirus",
    "coronavirus",
    "coronavirus",
    "covad19",
    "infectious",
    "disease",
    "caused",
    "newly",
    "discovered",
    "coronavirus",
    "believed",
    "emerged",
    "seafood",
    "market",
    "wuhan",
    "china",
    "december",
    "zoonotic",
    "disease",
    "transmitted",
    "animals",
    "people",
    "specifically",
    "disease",
    "normally",
    "exists",
    "animals",
    "infect",
    "humans",
    "virus",
    "causes",
    "covid19",
    "mainly",
    "transmitted",
    "droplets",
    "generated",
    "infected",
    "person",
    "coughs",
    "sneezes",
    "exhales",
    "droplets",
    "heavy",
    "hang",
    "air",
    "quickly",
    "fall",
    "floors",
    "surfaces",
    "appears",
    "symptoms",
    "show",
    "people",
    "within",
    "14",
    "days",
    "exposure",
    "virus",
    "let",
    "look",
    "various",
    "symptoms",
    "covid19",
    "patient",
    "coronavirus",
    "show",
    "generic",
    "symptoms",
    "cough",
    "fever",
    "shortness",
    "breath",
    "muscle",
    "pain",
    "also",
    "sore",
    "throat",
    "headache",
    "loss",
    "taste",
    "smell",
    "also",
    "expected",
    "middle",
    "east",
    "respiratory",
    "syndrome",
    "mars",
    "severe",
    "acute",
    "respiratory",
    "syndrome",
    "sars",
    "middle",
    "east",
    "respiratory",
    "syndrome",
    "viral",
    "respiratory",
    "illness",
    "caused",
    "coronavirus",
    "contagious",
    "sometimes",
    "fatal",
    "respiratory",
    "illness",
    "often",
    "spreads",
    "close",
    "contact",
    "infected",
    "person",
    "sars",
    "contagious",
    "sometimes",
    "fatal",
    "respiratory",
    "illness",
    "caused",
    "coronavirus",
    "appeared",
    "2009",
    "china",
    "spread",
    "worldwide",
    "within",
    "months",
    "although",
    "quickly",
    "contained",
    "sars",
    "virus",
    "transmitted",
    "droplets",
    "enters",
    "air",
    "someone",
    "disease",
    "coughs",
    "sneezes",
    "talks",
    "let",
    "look",
    "impact",
    "coronavirus",
    "worldwide",
    "maps",
    "charts",
    "going",
    "show",
    "taken",
    "organization",
    "called",
    "world",
    "data",
    "largely",
    "focus",
    "problems",
    "world",
    "faces",
    "poverty",
    "different",
    "diseases",
    "hunger",
    "climate",
    "change",
    "existential",
    "crisis",
    "inequality",
    "main",
    "goal",
    "research",
    "use",
    "data",
    "make",
    "progress",
    "world",
    "largest",
    "problems",
    "map",
    "see",
    "screens",
    "shows",
    "total",
    "number",
    "confirmed",
    "coveted",
    "19",
    "cases",
    "till",
    "26th",
    "april",
    "see",
    "color",
    "scale",
    "ranging",
    "0",
    "1",
    "million",
    "countries",
    "least",
    "number",
    "cases",
    "marked",
    "light",
    "orange",
    "color",
    "countries",
    "cases",
    "5000",
    "10",
    "000",
    "cases",
    "depicted",
    "using",
    "orange",
    "color",
    "countries",
    "cases",
    "1",
    "lakh",
    "shown",
    "using",
    "red",
    "color",
    "5",
    "lakhs",
    "dark",
    "red",
    "color",
    "see",
    "map",
    "african",
    "nations",
    "fewer",
    "cases",
    "compared",
    "nations",
    "asia",
    "europe",
    "america",
    "china",
    "close",
    "84",
    "000",
    "cases",
    "india",
    "around",
    "26",
    "500",
    "cases",
    "april",
    "26th",
    "iran",
    "nearly",
    "89",
    "000",
    "cases",
    "look",
    "australia",
    "cases",
    "around",
    "6700",
    "north",
    "america",
    "united",
    "states",
    "highest",
    "number",
    "cases",
    "actually",
    "highest",
    "throughout",
    "world",
    "south",
    "america",
    "brazil",
    "maximum",
    "number",
    "cases",
    "next",
    "map",
    "shows",
    "confirmed",
    "cases",
    "countries",
    "asian",
    "continent",
    "china",
    "iran",
    "turkey",
    "india",
    "saudi",
    "arabia",
    "highest",
    "number",
    "cases",
    "map",
    "europe",
    "europe",
    "countries",
    "italy",
    "france",
    "germany",
    "united",
    "kingdom",
    "spain",
    "russia",
    "highest",
    "number",
    "cases",
    "moving",
    "ahead",
    "see",
    "graph",
    "certain",
    "selected",
    "countries",
    "total",
    "confirmed",
    "cases",
    "filtered",
    "total",
    "world",
    "cases",
    "countries",
    "like",
    "united",
    "states",
    "spain",
    "italy",
    "germany",
    "france",
    "china",
    "india",
    "learn",
    "create",
    "similar",
    "graph",
    "prediction",
    "analysis",
    "let",
    "look",
    "total",
    "number",
    "daily",
    "confirmed",
    "cases",
    "26th",
    "april",
    "united",
    "states",
    "around",
    "48",
    "500",
    "cases",
    "russia",
    "around",
    "6000",
    "cases",
    "brazil",
    "nearly",
    "5",
    "500",
    "cases",
    "followed",
    "united",
    "kingdom",
    "close",
    "5000",
    "cases",
    "number",
    "confirmed",
    "cases",
    "lower",
    "number",
    "total",
    "cases",
    "reason",
    "limited",
    "testing",
    "different",
    "countries",
    "across",
    "world",
    "looking",
    "total",
    "debts",
    "reported",
    "across",
    "world",
    "map",
    "united",
    "states",
    "highest",
    "number",
    "53",
    "000",
    "deaths",
    "far",
    "followed",
    "italy",
    "france",
    "united",
    "kingdom",
    "germany",
    "next",
    "see",
    "graph",
    "countries",
    "total",
    "death",
    "cases",
    "till",
    "26th",
    "april",
    "see",
    "united",
    "states",
    "spain",
    "italy",
    "france",
    "china",
    "india",
    "see",
    "build",
    "similar",
    "graph",
    "demo",
    "next",
    "screens",
    "map",
    "different",
    "countries",
    "fatality",
    "rate",
    "fatality",
    "rate",
    "actually",
    "ratio",
    "confirmed",
    "deaths",
    "confirmed",
    "cases",
    "france",
    "highest",
    "fatality",
    "rate",
    "followed",
    "united",
    "kingdom",
    "percent",
    "italy",
    "moving",
    "ahead",
    "next",
    "map",
    "shows",
    "total",
    "number",
    "covet",
    "19",
    "test",
    "numbers",
    "united",
    "states",
    "conducted",
    "5",
    "million",
    "tests",
    "far",
    "followed",
    "russia",
    "million",
    "tests",
    "germany",
    "turkey",
    "canada",
    "india",
    "conducting",
    "coveted",
    "19",
    "tests",
    "large",
    "numbers",
    "deal",
    "situation",
    "country",
    "dealing",
    "pandemic",
    "within",
    "capacity",
    "make",
    "sure",
    "situation",
    "worsen",
    "affected",
    "countries",
    "gone",
    "lockdown",
    "lockdown",
    "work",
    "well",
    "research",
    "says",
    "yes",
    "screens",
    "see",
    "number",
    "days",
    "countries",
    "enforced",
    "partial",
    "full",
    "lockdown",
    "curb",
    "spread",
    "coronavirus",
    "information",
    "taken",
    "one",
    "india",
    "leading",
    "media",
    "groups",
    "india",
    "today",
    "data",
    "intelligence",
    "unit",
    "orange",
    "color",
    "represents",
    "full",
    "lockdown",
    "yellow",
    "color",
    "depicts",
    "partial",
    "lockdown",
    "countries",
    "like",
    "australia",
    "india",
    "belgium",
    "united",
    "kingdom",
    "china",
    "united",
    "states",
    "france",
    "spain",
    "others",
    "already",
    "announced",
    "lockdown",
    "40",
    "days",
    "make",
    "sure",
    "lockdowns",
    "extend",
    "situation",
    "improves",
    "people",
    "advised",
    "stay",
    "home",
    "avoid",
    "public",
    "gatherings",
    "let",
    "look",
    "india",
    "stands",
    "battle",
    "corona",
    "virus",
    "per",
    "press",
    "information",
    "bureau",
    "india",
    "pib",
    "figures",
    "situation",
    "report",
    "till",
    "5",
    "pm",
    "26th",
    "april",
    "states",
    "affected",
    "maharashtra",
    "gujarat",
    "delhi",
    "madhya",
    "pradesh",
    "uttar",
    "pradesh",
    "tamil",
    "nadu",
    "maharashtra",
    "7600",
    "cases",
    "followed",
    "gujarat",
    "3000",
    "cases",
    "maharashtra",
    "also",
    "highest",
    "number",
    "deaths",
    "reported",
    "stands",
    "323",
    "next",
    "gujarat",
    "133",
    "followed",
    "madhya",
    "pradesh",
    "99",
    "deaths",
    "far",
    "still",
    "312",
    "cases",
    "yet",
    "assigned",
    "states",
    "process",
    "contact",
    "tracing",
    "see",
    "top",
    "27",
    "000",
    "confirmed",
    "cases",
    "india",
    "20",
    "1777",
    "active",
    "cases",
    "6000",
    "people",
    "recovered",
    "corona",
    "virus",
    "death",
    "tally",
    "stands",
    "872",
    "government",
    "best",
    "control",
    "situation",
    "please",
    "follow",
    "rules",
    "guidelines",
    "issued",
    "local",
    "central",
    "authorities",
    "help",
    "us",
    "fight",
    "situation",
    "together",
    "effectively",
    "coming",
    "important",
    "part",
    "session",
    "guys",
    "waiting",
    "coronavirus",
    "outbreak",
    "prediction",
    "analysis",
    "analyze",
    "outbreak",
    "coronavirus",
    "coming",
    "days",
    "across",
    "various",
    "countries",
    "worldwide",
    "visualize",
    "using",
    "charts",
    "graphs",
    "predict",
    "number",
    "upcoming",
    "cases",
    "next",
    "20",
    "days",
    "using",
    "polynomial",
    "regression",
    "support",
    "vector",
    "machines",
    "model",
    "python",
    "data",
    "information",
    "22nd",
    "january",
    "28th",
    "april",
    "data",
    "sets",
    "using",
    "taken",
    "repository",
    "operated",
    "johns",
    "hopkins",
    "university",
    "center",
    "system",
    "science",
    "engineering",
    "let",
    "show",
    "locate",
    "data",
    "sets",
    "github",
    "repository",
    "find",
    "data",
    "sets",
    "resources",
    "updated",
    "regular",
    "basis",
    "would",
    "encourage",
    "go",
    "link",
    "let",
    "take",
    "jupiter",
    "notebook",
    "already",
    "implemented",
    "course",
    "run",
    "explain",
    "cell",
    "code",
    "make",
    "understand",
    "jupiter",
    "notebook",
    "first",
    "import",
    "libraries",
    "necessary",
    "analysis",
    "importing",
    "numpy",
    "pandas",
    "used",
    "numerical",
    "computations",
    "data",
    "manipulation",
    "data",
    "analysis",
    "importing",
    "matplotlib",
    "library",
    "creating",
    "data",
    "visualizations",
    "next",
    "also",
    "going",
    "import",
    "random",
    "math",
    "time",
    "libraries",
    "sklearn",
    "library",
    "importing",
    "linear",
    "regression",
    "model",
    "well",
    "certain",
    "functions",
    "strain",
    "test",
    "split",
    "polynomial",
    "features",
    "build",
    "polynomial",
    "regression",
    "svr",
    "help",
    "us",
    "create",
    "support",
    "vector",
    "machines",
    "model",
    "also",
    "use",
    "sql",
    "one",
    "matrix",
    "import",
    "functions",
    "mean",
    "squared",
    "error",
    "mean",
    "absolute",
    "error",
    "use",
    "calculating",
    "accuracy",
    "model",
    "let",
    "run",
    "cell",
    "hitting",
    "shift",
    "plus",
    "enter",
    "see",
    "successfully",
    "run",
    "first",
    "cell",
    "load",
    "datasets",
    "using",
    "three",
    "datasets",
    "demo",
    "data",
    "sets",
    "present",
    "github",
    "repository",
    "maintained",
    "johns",
    "hopkins",
    "university",
    "import",
    "data",
    "sets",
    "used",
    "pandas",
    "library",
    "read",
    "underscore",
    "csv",
    "function",
    "provided",
    "url",
    "location",
    "files",
    "followed",
    "file",
    "name",
    "extension",
    "file",
    "dot",
    "csv",
    "loaded",
    "three",
    "datasets",
    "three",
    "variable",
    "names",
    "let",
    "run",
    "cells",
    "import",
    "three",
    "data",
    "sets",
    "first",
    "imported",
    "confirmed",
    "cases",
    "data",
    "set",
    "display",
    "data",
    "set",
    "using",
    "dot",
    "head",
    "function",
    "see",
    "top",
    "five",
    "rows",
    "data",
    "scroll",
    "see",
    "confirmed",
    "underscore",
    "cases",
    "data",
    "set",
    "displaying",
    "first",
    "five",
    "rows",
    "data",
    "set",
    "information",
    "like",
    "province",
    "state",
    "country",
    "region",
    "latitude",
    "longitude",
    "date",
    "value",
    "starting",
    "22nd",
    "january",
    "27th",
    "april",
    "let",
    "go",
    "ahead",
    "import",
    "debts",
    "underscore",
    "reported",
    "data",
    "set",
    "display",
    "data",
    "set",
    "used",
    "dotted",
    "function",
    "depth",
    "data",
    "set",
    "looks",
    "like",
    "let",
    "go",
    "ahead",
    "import",
    "recovered",
    "underscore",
    "cases",
    "data",
    "set",
    "display",
    "head",
    "data",
    "set",
    "recovered",
    "data",
    "set",
    "looks",
    "like",
    "recoveries",
    "data",
    "set",
    "looks",
    "like",
    "finally",
    "load",
    "data",
    "set",
    "latest",
    "updates",
    "across",
    "globe",
    "regarding",
    "confirmed",
    "cases",
    "deaths",
    "recovered",
    "active",
    "cases",
    "let",
    "see",
    "head",
    "latest",
    "data",
    "set",
    "see",
    "information",
    "province",
    "state",
    "country",
    "region",
    "last",
    "updated",
    "latitude",
    "longitude",
    "location",
    "data",
    "regarding",
    "confirmed",
    "cases",
    "deaths",
    "reported",
    "recovered",
    "cases",
    "active",
    "cases",
    "extracting",
    "column",
    "names",
    "confirmed",
    "cases",
    "data",
    "frame",
    "using",
    "dot",
    "keys",
    "function",
    "run",
    "see",
    "column",
    "names",
    "next",
    "extracting",
    "date",
    "columns",
    "information",
    "confirmed",
    "cases",
    "death",
    "cases",
    "recovered",
    "cases",
    "using",
    "dot",
    "loc",
    "function",
    "storing",
    "respective",
    "variables",
    "first",
    "parameter",
    "colon",
    "tells",
    "us",
    "need",
    "rows",
    "data",
    "second",
    "parameter",
    "tells",
    "us",
    "need",
    "data",
    "fourth",
    "column",
    "till",
    "last",
    "column",
    "date",
    "values",
    "let",
    "run",
    "let",
    "check",
    "confirmed",
    "variable",
    "see",
    "see",
    "date",
    "columns",
    "22nd",
    "jan",
    "till",
    "27th",
    "april",
    "next",
    "cell",
    "code",
    "basically",
    "creating",
    "multiple",
    "empty",
    "lists",
    "find",
    "total",
    "cases",
    "worldwide",
    "total",
    "number",
    "debts",
    "far",
    "mortality",
    "rate",
    "recovered",
    "cases",
    "total",
    "active",
    "cases",
    "also",
    "want",
    "find",
    "confirmed",
    "cases",
    "debts",
    "recoveries",
    "countries",
    "china",
    "italy",
    "united",
    "states",
    "spain",
    "france",
    "germany",
    "uk",
    "russia",
    "india",
    "created",
    "empty",
    "lists",
    "countries",
    "well",
    "see",
    "append",
    "values",
    "empty",
    "list",
    "next",
    "step",
    "let",
    "run",
    "using",
    "loop",
    "finding",
    "total",
    "sum",
    "confirmed",
    "cases",
    "death",
    "cases",
    "recoveries",
    "made",
    "far",
    "values",
    "appending",
    "empty",
    "list",
    "world",
    "cases",
    "total",
    "debts",
    "total",
    "recovered",
    "total",
    "active",
    "cases",
    "total",
    "active",
    "cases",
    "calculated",
    "confirmed",
    "cases",
    "minus",
    "death",
    "cases",
    "minus",
    "recovered",
    "cases",
    "calculate",
    "mortality",
    "rate",
    "using",
    "simple",
    "formula",
    "sum",
    "total",
    "debts",
    "divided",
    "total",
    "confirmed",
    "cases",
    "recovery",
    "rate",
    "also",
    "calculated",
    "dividing",
    "total",
    "recovered",
    "cases",
    "total",
    "confirmed",
    "cases",
    "appending",
    "values",
    "confirmed",
    "cases",
    "country",
    "using",
    "append",
    "function",
    "selecting",
    "different",
    "countries",
    "created",
    "empty",
    "list",
    "similarly",
    "also",
    "appending",
    "values",
    "debts",
    "recoveries",
    "made",
    "countries",
    "let",
    "run",
    "check",
    "world",
    "cases",
    "day",
    "world",
    "cases",
    "come",
    "day",
    "see",
    "number",
    "reached",
    "almost",
    "30",
    "lakhs",
    "see",
    "total",
    "death",
    "far",
    "globally",
    "deaths",
    "reported",
    "day",
    "across",
    "globe",
    "see",
    "around",
    "2",
    "lakhs",
    "casualties",
    "far",
    "let",
    "collapse",
    "next",
    "printing",
    "total",
    "sum",
    "confirmed",
    "cases",
    "see",
    "stands",
    "around",
    "30",
    "lakhs",
    "41",
    "similarly",
    "let",
    "print",
    "total",
    "death",
    "sum",
    "around",
    "2",
    "lakhs",
    "11",
    "167",
    "27th",
    "april",
    "also",
    "check",
    "recovered",
    "sum",
    "around",
    "8",
    "lakhs",
    "93",
    "also",
    "look",
    "cases",
    "based",
    "individual",
    "countries",
    "see",
    "cases",
    "come",
    "far",
    "united",
    "states",
    "underscore",
    "cases",
    "united",
    "states",
    "nearly",
    "9",
    "lakhs",
    "88",
    "1997",
    "cases",
    "let",
    "collapse",
    "let",
    "check",
    "india",
    "well",
    "india",
    "total",
    "cases",
    "reached",
    "29",
    "let",
    "collapse",
    "also",
    "look",
    "recoveries",
    "country",
    "printing",
    "recoveries",
    "made",
    "italy",
    "next",
    "want",
    "analyze",
    "data",
    "understanding",
    "daily",
    "increase",
    "cases",
    "across",
    "world",
    "daily",
    "casualties",
    "daily",
    "recovery",
    "cases",
    "using",
    "user",
    "defined",
    "function",
    "track",
    "daily",
    "increase",
    "let",
    "run",
    "okay",
    "find",
    "total",
    "worldwide",
    "increase",
    "cases",
    "calling",
    "created",
    "user",
    "defined",
    "function",
    "assigning",
    "world",
    "daily",
    "increase",
    "variable",
    "similarly",
    "using",
    "method",
    "find",
    "daily",
    "increase",
    "cases",
    "different",
    "countries",
    "let",
    "show",
    "daily",
    "increase",
    "confirmed",
    "cases",
    "spain",
    "yesterday",
    "27th",
    "april",
    "speed",
    "2793",
    "cases",
    "let",
    "check",
    "germany",
    "also",
    "see",
    "yesterday",
    "germany",
    "around",
    "988",
    "cases",
    "next",
    "cell",
    "finding",
    "daily",
    "death",
    "cases",
    "across",
    "world",
    "well",
    "countries",
    "let",
    "run",
    "let",
    "check",
    "daily",
    "deaths",
    "reported",
    "china",
    "values",
    "daily",
    "deaths",
    "china",
    "see",
    "numbers",
    "significantly",
    "reduced",
    "also",
    "let",
    "check",
    "united",
    "kingdom",
    "well",
    "27th",
    "april",
    "united",
    "kingdom",
    "363",
    "deaths",
    "reported",
    "moving",
    "ahead",
    "also",
    "calculating",
    "daily",
    "recovery",
    "cases",
    "across",
    "world",
    "countries",
    "let",
    "go",
    "ahead",
    "check",
    "data",
    "india",
    "terms",
    "daily",
    "recovery",
    "cases",
    "values",
    "scroll",
    "see",
    "yesterday",
    "27th",
    "april",
    "india",
    "614",
    "recovered",
    "cases",
    "also",
    "let",
    "look",
    "world",
    "recovery",
    "cases",
    "see",
    "27th",
    "april",
    "nearly",
    "28",
    "234",
    "people",
    "recovered",
    "across",
    "globe",
    "next",
    "step",
    "visualize",
    "data",
    "first",
    "finding",
    "list",
    "unique",
    "countries",
    "using",
    "list",
    "function",
    "passing",
    "latest",
    "data",
    "variable",
    "filter",
    "different",
    "uni",
    "countries",
    "see",
    "list",
    "different",
    "countries",
    "cell",
    "see",
    "screens",
    "using",
    "lines",
    "code",
    "find",
    "country",
    "confirmed",
    "cases",
    "death",
    "cases",
    "active",
    "cases",
    "recovery",
    "cases",
    "mortality",
    "rate",
    "finding",
    "sum",
    "cases",
    "appending",
    "empty",
    "list",
    "created",
    "also",
    "removing",
    "countries",
    "cases",
    "finally",
    "sorting",
    "countries",
    "number",
    "confirmed",
    "cases",
    "let",
    "run",
    "cell",
    "visualizing",
    "data",
    "using",
    "gradient",
    "styled",
    "color",
    "map",
    "see",
    "country",
    "name",
    "number",
    "confirmed",
    "cases",
    "number",
    "deaths",
    "number",
    "recoveries",
    "taken",
    "blue",
    "gradient",
    "let",
    "run",
    "see",
    "interesting",
    "table",
    "looks",
    "really",
    "attractive",
    "left",
    "see",
    "country",
    "names",
    "table",
    "sorted",
    "descending",
    "order",
    "confirmed",
    "cases",
    "see",
    "dark",
    "blue",
    "color",
    "depicts",
    "countries",
    "highest",
    "number",
    "cases",
    "case",
    "united",
    "states",
    "spain",
    "italy",
    "france",
    "also",
    "germany",
    "united",
    "kingdom",
    "also",
    "see",
    "number",
    "deaths",
    "recoveries",
    "middle",
    "table",
    "right",
    "see",
    "number",
    "active",
    "cases",
    "mortality",
    "rate",
    "see",
    "visual",
    "italy",
    "highest",
    "mortality",
    "rate",
    "followed",
    "united",
    "kingdom",
    "similarly",
    "finding",
    "list",
    "unique",
    "provinces",
    "using",
    "list",
    "function",
    "passing",
    "latest",
    "data",
    "variable",
    "filter",
    "different",
    "provinces",
    "next",
    "cell",
    "finding",
    "confirmed",
    "cases",
    "death",
    "cases",
    "active",
    "cases",
    "recovery",
    "cases",
    "mortality",
    "rate",
    "province",
    "particular",
    "country",
    "also",
    "finding",
    "sum",
    "cases",
    "appending",
    "empty",
    "list",
    "created",
    "also",
    "removing",
    "provinces",
    "cases",
    "finally",
    "sorting",
    "provinces",
    "number",
    "confirmed",
    "cases",
    "let",
    "run",
    "cell",
    "visualizing",
    "data",
    "different",
    "provinces",
    "using",
    "color",
    "map",
    "see",
    "province",
    "state",
    "names",
    "number",
    "confirmed",
    "cases",
    "number",
    "deaths",
    "number",
    "recoveries",
    "taken",
    "red",
    "gradient",
    "let",
    "run",
    "go",
    "visual",
    "screen",
    "new",
    "york",
    "state",
    "highest",
    "number",
    "confirmed",
    "cases",
    "number",
    "deaths",
    "well",
    "hubei",
    "province",
    "china",
    "highest",
    "recoveries",
    "around",
    "63",
    "604",
    "extreme",
    "right",
    "see",
    "mortality",
    "rate",
    "next",
    "cell",
    "code",
    "deals",
    "handling",
    "missing",
    "values",
    "nan",
    "values",
    "important",
    "handle",
    "values",
    "otherwise",
    "data",
    "would",
    "noisy",
    "messy",
    "lead",
    "inappropriate",
    "results",
    "plotting",
    "horizontal",
    "bar",
    "graph",
    "compare",
    "number",
    "cases",
    "united",
    "states",
    "outside",
    "united",
    "states",
    "outside",
    "united",
    "states",
    "cases",
    "calculated",
    "subtracting",
    "united",
    "states",
    "confirmed",
    "cases",
    "country",
    "confirmed",
    "cases",
    "worldwide",
    "used",
    "plt",
    "dot",
    "bar",
    "h",
    "function",
    "plot",
    "horizontal",
    "bars",
    "also",
    "given",
    "title",
    "plot",
    "let",
    "run",
    "bar",
    "graph",
    "blue",
    "bar",
    "represents",
    "united",
    "states",
    "confirmed",
    "cases",
    "red",
    "bar",
    "represents",
    "cases",
    "outside",
    "united",
    "states",
    "want",
    "print",
    "actual",
    "values",
    "cases",
    "united",
    "states",
    "outside",
    "united",
    "states",
    "see",
    "nearly",
    "19",
    "lakhs",
    "58",
    "592",
    "cases",
    "outside",
    "united",
    "states",
    "united",
    "states",
    "nearly",
    "9",
    "lakhs",
    "3854",
    "cases",
    "total",
    "cases",
    "stand",
    "28",
    "lakhs",
    "96",
    "746",
    "cases",
    "want",
    "show",
    "10",
    "countries",
    "confirmed",
    "cases",
    "rest",
    "want",
    "group",
    "others",
    "category",
    "using",
    "two",
    "empty",
    "lists",
    "going",
    "find",
    "top",
    "10",
    "countries",
    "highest",
    "number",
    "cases",
    "remaining",
    "categorizing",
    "others",
    "show",
    "countries",
    "defined",
    "function",
    "plot",
    "horizontal",
    "bar",
    "graph",
    "let",
    "run",
    "using",
    "plot",
    "underscore",
    "bar",
    "underscore",
    "graphs",
    "function",
    "create",
    "bar",
    "graph",
    "used",
    "visual",
    "underscore",
    "unique",
    "underscore",
    "countries",
    "visual",
    "underscore",
    "confirmed",
    "underscore",
    "cases",
    "variables",
    "given",
    "title",
    "graph",
    "horizontal",
    "bar",
    "graph",
    "see",
    "united",
    "states",
    "highest",
    "number",
    "cases",
    "next",
    "spain",
    "united",
    "kingdom",
    "6th",
    "position",
    "russia",
    "10th",
    "place",
    "top",
    "see",
    "others",
    "category",
    "well",
    "create",
    "pie",
    "chart",
    "plot",
    "graph",
    "created",
    "another",
    "function",
    "called",
    "plot",
    "underscore",
    "pi",
    "underscore",
    "charts",
    "let",
    "run",
    "using",
    "plot",
    "underscore",
    "pi",
    "underscore",
    "charts",
    "function",
    "name",
    "create",
    "pie",
    "chart",
    "used",
    "visual",
    "underscore",
    "unique",
    "underscore",
    "countries",
    "visual",
    "underscore",
    "confirmed",
    "underscore",
    "cases",
    "variables",
    "given",
    "title",
    "graph",
    "go",
    "pie",
    "chart",
    "screen",
    "right",
    "see",
    "legend",
    "shows",
    "different",
    "country",
    "names",
    "maximum",
    "area",
    "pie",
    "chart",
    "occupied",
    "united",
    "states",
    "others",
    "category",
    "followed",
    "spain",
    "italy",
    "france",
    "follow",
    "procedure",
    "show",
    "10",
    "provinces",
    "confirmed",
    "cases",
    "rest",
    "group",
    "others",
    "category",
    "using",
    "two",
    "empty",
    "lists",
    "visual",
    "underscore",
    "unique",
    "underscore",
    "provinces",
    "visual",
    "underscore",
    "confirmed",
    "underscore",
    "cases",
    "2",
    "let",
    "plot",
    "graph",
    "using",
    "plot",
    "underscore",
    "bar",
    "underscore",
    "graphs",
    "function",
    "see",
    "bar",
    "graph",
    "new",
    "york",
    "province",
    "new",
    "york",
    "state",
    "highest",
    "number",
    "cases",
    "new",
    "jersey",
    "followed",
    "uber",
    "provisions",
    "china",
    "massachusetts",
    "top",
    "see",
    "others",
    "category",
    "next",
    "cell",
    "code",
    "looks",
    "pretty",
    "complicated",
    "essentially",
    "trying",
    "plot",
    "pie",
    "chart",
    "different",
    "countries",
    "along",
    "several",
    "states",
    "provinces",
    "want",
    "show",
    "top",
    "10",
    "states",
    "highest",
    "number",
    "cases",
    "let",
    "run",
    "want",
    "show",
    "pie",
    "chart",
    "united",
    "states",
    "top",
    "10",
    "states",
    "let",
    "see",
    "pie",
    "chart",
    "right",
    "see",
    "legend",
    "10",
    "states",
    "maximum",
    "area",
    "pie",
    "chart",
    "occupied",
    "new",
    "york",
    "states",
    "next",
    "new",
    "jersey",
    "followed",
    "massachusetts",
    "california",
    "let",
    "also",
    "check",
    "confirmed",
    "cases",
    "different",
    "states",
    "france",
    "change",
    "france",
    "pie",
    "chart",
    "10",
    "different",
    "states",
    "france",
    "let",
    "come",
    "building",
    "model",
    "using",
    "polynomial",
    "regression",
    "support",
    "vector",
    "machines",
    "step",
    "converting",
    "dates",
    "cases",
    "form",
    "numpy",
    "array",
    "using",
    "np",
    "dot",
    "array",
    "function",
    "let",
    "run",
    "since",
    "prediction",
    "next",
    "20",
    "days",
    "created",
    "variable",
    "called",
    "days",
    "underscore",
    "future",
    "assigned",
    "value",
    "20",
    "adding",
    "last",
    "20",
    "days",
    "total",
    "number",
    "days",
    "let",
    "show",
    "values",
    "future",
    "underscore",
    "forecast",
    "list",
    "values",
    "basically",
    "number",
    "total",
    "days",
    "next",
    "converting",
    "integers",
    "date",
    "time",
    "values",
    "better",
    "visualization",
    "time",
    "split",
    "data",
    "training",
    "testing",
    "sets",
    "using",
    "train",
    "underscore",
    "test",
    "underscore",
    "split",
    "function",
    "taken",
    "days",
    "since",
    "22nd",
    "jan",
    "world",
    "cases",
    "parameters",
    "using",
    "75",
    "percent",
    "data",
    "training",
    "model",
    "25",
    "percent",
    "testing",
    "model",
    "transform",
    "data",
    "polynomial",
    "regression",
    "use",
    "polynomial",
    "features",
    "function",
    "fit",
    "underscore",
    "transform",
    "method",
    "transform",
    "training",
    "testing",
    "future",
    "forecast",
    "data",
    "next",
    "build",
    "polynomial",
    "regression",
    "model",
    "using",
    "linear",
    "regression",
    "function",
    "used",
    "linear",
    "underscore",
    "model",
    "dot",
    "fit",
    "function",
    "fit",
    "training",
    "data",
    "used",
    "predict",
    "function",
    "predict",
    "test",
    "data",
    "set",
    "values",
    "finally",
    "printing",
    "mean",
    "absolute",
    "error",
    "value",
    "mean",
    "squared",
    "error",
    "value",
    "well",
    "see",
    "results",
    "let",
    "plot",
    "graph",
    "test",
    "data",
    "set",
    "values",
    "predicted",
    "polynomial",
    "regression",
    "model",
    "see",
    "graph",
    "blue",
    "line",
    "represents",
    "test",
    "data",
    "red",
    "line",
    "represents",
    "polynomial",
    "regression",
    "predictions",
    "let",
    "start",
    "building",
    "model",
    "using",
    "support",
    "vector",
    "machines",
    "algorithm",
    "svm",
    "uses",
    "different",
    "parameters",
    "build",
    "model",
    "parameters",
    "kernel",
    "c",
    "gamma",
    "epsilon",
    "shrinking",
    "svm",
    "underscore",
    "grid",
    "kernel",
    "specifies",
    "kernel",
    "type",
    "used",
    "algorithm",
    "must",
    "one",
    "linear",
    "poly",
    "rbf",
    "sigmoid",
    "precomputed",
    "callable",
    "nothing",
    "given",
    "rbf",
    "used",
    "default",
    "c",
    "regularization",
    "parameter",
    "gamma",
    "kernel",
    "coefficient",
    "rbf",
    "poly",
    "sigmoid",
    "epsilon",
    "specifies",
    "epsilon",
    "tube",
    "within",
    "penalty",
    "associated",
    "training",
    "loss",
    "function",
    "shrinking",
    "takes",
    "boolean",
    "values",
    "true",
    "false",
    "svm",
    "grid",
    "values",
    "parameters",
    "passed",
    "creating",
    "support",
    "vector",
    "regressors",
    "using",
    "fit",
    "function",
    "fit",
    "training",
    "data",
    "set",
    "finally",
    "predicting",
    "values",
    "future",
    "data",
    "well",
    "let",
    "print",
    "mae",
    "msc",
    "values",
    "display",
    "graph",
    "prediction",
    "see",
    "blue",
    "line",
    "represents",
    "test",
    "data",
    "red",
    "line",
    "represents",
    "svm",
    "predictions",
    "let",
    "create",
    "visualizations",
    "understand",
    "model",
    "data",
    "better",
    "plotting",
    "number",
    "coronavirus",
    "cases",
    "time",
    "used",
    "adjusted",
    "dates",
    "world",
    "cases",
    "plot",
    "graph",
    "also",
    "assigned",
    "x",
    "labels",
    "labels",
    "let",
    "run",
    "see",
    "graph",
    "total",
    "number",
    "cases",
    "reached",
    "close",
    "30",
    "lakhs",
    "next",
    "graph",
    "showing",
    "total",
    "number",
    "coronavirus",
    "deaths",
    "time",
    "also",
    "used",
    "x",
    "label",
    "label",
    "see",
    "total",
    "number",
    "deaths",
    "reached",
    "2",
    "lakhs",
    "similarly",
    "next",
    "graph",
    "see",
    "total",
    "number",
    "recovered",
    "cases",
    "8",
    "lakhs",
    "also",
    "looking",
    "total",
    "number",
    "active",
    "cases",
    "across",
    "globe",
    "graph",
    "see",
    "number",
    "coronavirus",
    "active",
    "cases",
    "time",
    "creating",
    "bar",
    "graph",
    "adjusted",
    "dates",
    "world",
    "daily",
    "increase",
    "variables",
    "see",
    "days",
    "cases",
    "gone",
    "high",
    "reaching",
    "100",
    "000",
    "day",
    "next",
    "graph",
    "plotting",
    "show",
    "world",
    "daily",
    "increase",
    "confirmed",
    "death",
    "cases",
    "bar",
    "graph",
    "see",
    "certain",
    "days",
    "reported",
    "10",
    "000",
    "deaths",
    "day",
    "next",
    "graph",
    "shows",
    "world",
    "daily",
    "increase",
    "confirmed",
    "recovery",
    "cases",
    "graph",
    "moving",
    "ahead",
    "plot",
    "graphs",
    "show",
    "number",
    "coronavirus",
    "cases",
    "time",
    "let",
    "plot",
    "actual",
    "graph",
    "taken",
    "adjusted",
    "underscore",
    "dates",
    "world",
    "cases",
    "linear",
    "underscore",
    "trade",
    "variables",
    "parameter",
    "let",
    "run",
    "see",
    "graph",
    "blue",
    "line",
    "represents",
    "confirmed",
    "check",
    "svm",
    "model",
    "well",
    "graph",
    "let",
    "print",
    "predicted",
    "values",
    "polynomial",
    "regression",
    "model",
    "data",
    "frame",
    "see",
    "predicted",
    "confirmed",
    "cases",
    "next",
    "20",
    "days",
    "28th",
    "april",
    "17th",
    "may",
    "likewise",
    "let",
    "also",
    "print",
    "predicted",
    "values",
    "svm",
    "model",
    "data",
    "frame",
    "predicted",
    "values",
    "svm",
    "model",
    "next",
    "graph",
    "plotting",
    "depths",
    "recoveries",
    "graph",
    "line",
    "graph",
    "see",
    "clearly",
    "number",
    "recoveries",
    "way",
    "higher",
    "number",
    "deaths",
    "actually",
    "good",
    "sign",
    "moving",
    "forward",
    "next",
    "graph",
    "shows",
    "line",
    "chart",
    "number",
    "coronavirus",
    "deaths",
    "versus",
    "number",
    "coronavirus",
    "recoveries",
    "graph",
    "number",
    "recoveries",
    "number",
    "total",
    "deaths",
    "cell",
    "code",
    "looks",
    "pretty",
    "huge",
    "want",
    "plot",
    "total",
    "number",
    "confirmed",
    "cases",
    "daily",
    "increase",
    "confirmed",
    "cases",
    "increase",
    "debts",
    "increase",
    "recoveries",
    "different",
    "countries",
    "across",
    "globe",
    "let",
    "run",
    "cell",
    "using",
    "country",
    "underscore",
    "plot",
    "function",
    "defined",
    "plotting",
    "data",
    "various",
    "countries",
    "let",
    "start",
    "china",
    "first",
    "see",
    "four",
    "graphs",
    "total",
    "scroll",
    "first",
    "see",
    "china",
    "confirmed",
    "cases",
    "next",
    "china",
    "daily",
    "increase",
    "confirmed",
    "cases",
    "see",
    "one",
    "days",
    "huge",
    "spike",
    "increase",
    "confirmed",
    "cases",
    "china",
    "daily",
    "increase",
    "debts",
    "finally",
    "daily",
    "increase",
    "recoveries",
    "let",
    "check",
    "italy",
    "let",
    "scroll",
    "see",
    "plots",
    "italy",
    "well",
    "first",
    "total",
    "confirmed",
    "cases",
    "italy",
    "nearly",
    "around",
    "2",
    "lakhs",
    "daily",
    "increase",
    "confirmed",
    "cases",
    "next",
    "italy",
    "daily",
    "increase",
    "deaths",
    "finally",
    "increase",
    "recoveries",
    "let",
    "look",
    "graphs",
    "india",
    "looks",
    "like",
    "see",
    "total",
    "well",
    "let",
    "also",
    "see",
    "united",
    "states",
    "see",
    "four",
    "graphs",
    "united",
    "states",
    "spain",
    "france",
    "germany",
    "depicted",
    "using",
    "different",
    "colors",
    "top",
    "left",
    "see",
    "legend",
    "graph",
    "looks",
    "similar",
    "one",
    "saw",
    "slides",
    "next",
    "plotting",
    "total",
    "coronavirus",
    "depth",
    "graph",
    "finally",
    "let",
    "us",
    "look",
    "total",
    "coronavirus",
    "cases",
    "countries",
    "last",
    "three",
    "graphs",
    "look",
    "similar",
    "used",
    "slides",
    "world",
    "data",
    "responsibility",
    "call",
    "safety",
    "precautions",
    "take",
    "make",
    "sure",
    "safe",
    "attacked",
    "coronavirus",
    "first",
    "foremost",
    "panic",
    "unprecedented",
    "times",
    "feel",
    "showing",
    "symptoms",
    "coronavirus",
    "wait",
    "please",
    "consult",
    "doctor",
    "immediately",
    "stay",
    "home",
    "ensure",
    "exposed",
    "virus",
    "outside",
    "maintain",
    "social",
    "distancing",
    "people",
    "talking",
    "someone",
    "outside",
    "someone",
    "sick",
    "forget",
    "wash",
    "hands",
    "regularly",
    "using",
    "soaps",
    "sanitizers",
    "disinfectants",
    "always",
    "remember",
    "wear",
    "mask",
    "going",
    "outside",
    "make",
    "sure",
    "practice",
    "respiratory",
    "hygiene",
    "cover",
    "mouth",
    "coughing",
    "try",
    "eating",
    "healthy",
    "food",
    "precautions",
    "help",
    "avoid",
    "coronavirus",
    "make",
    "sure",
    "safe",
    "healthy",
    "moving",
    "final",
    "section",
    "let",
    "tell",
    "simply",
    "learn",
    "help",
    "start",
    "career",
    "machine",
    "learning",
    "let",
    "take",
    "website",
    "first",
    "okay",
    "chrome",
    "browser",
    "let",
    "search",
    "want",
    "learn",
    "let",
    "type",
    "machine",
    "learning",
    "show",
    "relevant",
    "courses",
    "simply",
    "learn",
    "offers",
    "machine",
    "learning",
    "category",
    "see",
    "multiple",
    "courses",
    "first",
    "let",
    "open",
    "first",
    "link",
    "let",
    "open",
    "second",
    "link",
    "well",
    "post",
    "graduate",
    "program",
    "ai",
    "machine",
    "learning",
    "collaboration",
    "purdue",
    "university",
    "ibm",
    "scroll",
    "see",
    "key",
    "features",
    "course",
    "get",
    "purdue",
    "alumni",
    "association",
    "membership",
    "industry",
    "recognized",
    "ibm",
    "certificates",
    "enrollment",
    "simply",
    "launch",
    "job",
    "assist",
    "25",
    "plus",
    "projects",
    "gpu",
    "enabled",
    "labs",
    "450",
    "plus",
    "hours",
    "applied",
    "learning",
    "capstone",
    "projects",
    "three",
    "domains",
    "much",
    "see",
    "right",
    "certificate",
    "get",
    "completing",
    "program",
    "also",
    "get",
    "certificates",
    "recognized",
    "ibm",
    "another",
    "key",
    "feature",
    "course",
    "enroll",
    "simply",
    "learn",
    "job",
    "assist",
    "program",
    "get",
    "im",
    "jobs",
    "crew",
    "membership",
    "six",
    "months",
    "resume",
    "assistance",
    "career",
    "monitoring",
    "design",
    "interview",
    "preparation",
    "career",
    "affairs",
    "scroll",
    "see",
    "learning",
    "path",
    "learn",
    "python",
    "data",
    "science",
    "machine",
    "learning",
    "deep",
    "learning",
    "tensorflow",
    "carers",
    "advanced",
    "deep",
    "learning",
    "computer",
    "vision",
    "also",
    "learn",
    "natural",
    "language",
    "processing",
    "nlp",
    "speech",
    "recognition",
    "reinforcement",
    "learning",
    "also",
    "opportunity",
    "select",
    "electives",
    "ibm",
    "watson",
    "chatbots",
    "machine",
    "learning",
    "r",
    "git",
    "github",
    "training",
    "two",
    "others",
    "see",
    "skills",
    "covered",
    "learn",
    "statistics",
    "core",
    "component",
    "machine",
    "learning",
    "learn",
    "python",
    "supervised",
    "learning",
    "dance",
    "computer",
    "vision",
    "tensorflow",
    "reinforcement",
    "learning",
    "speech",
    "recognition",
    "also",
    "learn",
    "numpy",
    "pandas",
    "libraries",
    "see",
    "tools",
    "covered",
    "course",
    "scroll",
    "important",
    "section",
    "see",
    "industry",
    "projects",
    "get",
    "work",
    "twitter",
    "one",
    "zumato",
    "one",
    "uber",
    "mercedes",
    "benz",
    "well",
    "course",
    "advisors",
    "please",
    "go",
    "ahead",
    "enroll",
    "postgraduate",
    "program",
    "ai",
    "machine",
    "learning",
    "want",
    "kick",
    "start",
    "career",
    "machine",
    "learning",
    "next",
    "course",
    "machine",
    "learning",
    "certification",
    "course",
    "scroll",
    "see",
    "details",
    "course",
    "gain",
    "expertise",
    "25",
    "plus",
    "exercises",
    "get",
    "work",
    "four",
    "industry",
    "based",
    "projects",
    "integrated",
    "labs",
    "dedicated",
    "mentoring",
    "sessions",
    "industry",
    "experts",
    "44",
    "hours",
    "instructed",
    "led",
    "training",
    "certification",
    "right",
    "see",
    "skills",
    "covered",
    "learn",
    "time",
    "series",
    "modeling",
    "linear",
    "logistic",
    "regression",
    "also",
    "learn",
    "support",
    "vector",
    "machines",
    "games",
    "clustering",
    "knife",
    "base",
    "decision",
    "trees",
    "random",
    "forest",
    "learn",
    "concepts",
    "bagging",
    "boosting",
    "deep",
    "learning",
    "fundamentals",
    "well",
    "see",
    "course",
    "content",
    "scroll",
    "projects",
    "get",
    "work",
    "finally",
    "receive",
    "certificate",
    "complete",
    "course",
    "thank",
    "watching",
    "full",
    "course",
    "video",
    "tutorial",
    "machine",
    "learning",
    "python",
    "hope",
    "liked",
    "questions",
    "please",
    "put",
    "comments",
    "section",
    "team",
    "help",
    "solve",
    "queries",
    "thanks",
    "stay",
    "tuned",
    "simply",
    "loan",
    "hi",
    "like",
    "video",
    "subscribe",
    "simply",
    "learn",
    "youtube",
    "channel",
    "click",
    "watch",
    "similar",
    "videos",
    "nerd",
    "get",
    "certified",
    "click"
  ],
  "keywords": [
    "ever",
    "finding",
    "information",
    "companies",
    "based",
    "well",
    "machine",
    "learning",
    "product",
    "work",
    "going",
    "look",
    "give",
    "process",
    "need",
    "build",
    "get",
    "make",
    "sure",
    "simply",
    "learn",
    "hit",
    "uses",
    "data",
    "algorithms",
    "create",
    "computer",
    "models",
    "almost",
    "every",
    "cars",
    "medical",
    "possible",
    "across",
    "different",
    "market",
    "9",
    "another",
    "report",
    "us",
    "mind",
    "let",
    "first",
    "programming",
    "form",
    "least",
    "one",
    "python",
    "r",
    "various",
    "science",
    "graphs",
    "space",
    "time",
    "etc",
    "also",
    "know",
    "libraries",
    "like",
    "numpy",
    "pandas",
    "analysis",
    "second",
    "mathematics",
    "problems",
    "using",
    "use",
    "understand",
    "behind",
    "good",
    "linear",
    "algebra",
    "calculus",
    "statistics",
    "probability",
    "processing",
    "numbers",
    "understanding",
    "best",
    "results",
    "third",
    "fit",
    "percent",
    "spend",
    "much",
    "format",
    "set",
    "store",
    "next",
    "standard",
    "model",
    "correct",
    "method",
    "parameter",
    "really",
    "supervised",
    "unsupervised",
    "reinforcement",
    "regression",
    "logistic",
    "svm",
    "knn",
    "decision",
    "k",
    "means",
    "clustering",
    "coming",
    "final",
    "scale",
    "train",
    "depending",
    "problem",
    "hand",
    "choose",
    "error",
    "measure",
    "part",
    "working",
    "projects",
    "top",
    "highest",
    "world",
    "average",
    "salary",
    "united",
    "states",
    "000",
    "per",
    "india",
    "8",
    "company",
    "start",
    "program",
    "job",
    "tools",
    "course",
    "cover",
    "everything",
    "take",
    "random",
    "forest",
    "nearest",
    "neighbors",
    "regularization",
    "principal",
    "component",
    "prediction",
    "finally",
    "machines",
    "given",
    "called",
    "lot",
    "today",
    "paul",
    "new",
    "either",
    "song",
    "see",
    "say",
    "name",
    "looking",
    "guess",
    "whether",
    "able",
    "classify",
    "easily",
    "right",
    "label",
    "b",
    "could",
    "complicated",
    "case",
    "yes",
    "comes",
    "example",
    "draw",
    "around",
    "four",
    "words",
    "would",
    "go",
    "basic",
    "algorithm",
    "small",
    "many",
    "easy",
    "happens",
    "point",
    "predict",
    "better",
    "higher",
    "accuracy",
    "ways",
    "gives",
    "three",
    "seven",
    "4",
    "weight",
    "becomes",
    "feature",
    "3",
    "1",
    "hence",
    "labeled",
    "features",
    "labels",
    "note",
    "move",
    "difference",
    "taken",
    "two",
    "clusters",
    "cluster",
    "took",
    "less",
    "important",
    "works",
    "system",
    "image",
    "dog",
    "cat",
    "negative",
    "saying",
    "correctly",
    "input",
    "output",
    "result",
    "else",
    "training",
    "quick",
    "simple",
    "facebook",
    "picture",
    "2",
    "someone",
    "think",
    "answers",
    "moving",
    "sometimes",
    "making",
    "huge",
    "amount",
    "minute",
    "key",
    "helps",
    "great",
    "used",
    "predicted",
    "interesting",
    "says",
    "getting",
    "others",
    "real",
    "number",
    "high",
    "hey",
    "talk",
    "10",
    "help",
    "tell",
    "call",
    "play",
    "even",
    "things",
    "actually",
    "whatever",
    "usually",
    "neural",
    "networks",
    "predictions",
    "thing",
    "probably",
    "put",
    "path",
    "large",
    "map",
    "red",
    "yellow",
    "blue",
    "clear",
    "slightly",
    "version",
    "told",
    "exactly",
    "specific",
    "days",
    "times",
    "location",
    "dot",
    "want",
    "close",
    "particular",
    "spam",
    "already",
    "find",
    "goes",
    "general",
    "place",
    "last",
    "takes",
    "middle",
    "money",
    "taking",
    "values",
    "changes",
    "stock",
    "size",
    "disease",
    "drug",
    "country",
    "tutorial",
    "series",
    "ahead",
    "types",
    "support",
    "vector",
    "recipe",
    "cupcake",
    "muffin",
    "way",
    "explore",
    "come",
    "car",
    "still",
    "change",
    "whole",
    "50",
    "double",
    "might",
    "pretty",
    "within",
    "maybe",
    "mean",
    "stuff",
    "talking",
    "dig",
    "deeper",
    "nice",
    "order",
    "remember",
    "original",
    "ones",
    "kind",
    "people",
    "greater",
    "scroll",
    "virus",
    "certain",
    "setup",
    "certainly",
    "something",
    "rate",
    "makes",
    "countries",
    "trying",
    "project",
    "game",
    "step",
    "far",
    "earlier",
    "ca",
    "looked",
    "little",
    "bit",
    "uh",
    "run",
    "cell",
    "predicts",
    "biggest",
    "fits",
    "together",
    "predicting",
    "steps",
    "domain",
    "versus",
    "matches",
    "got",
    "answer",
    "gone",
    "test",
    "done",
    "testing",
    "said",
    "missing",
    "along",
    "line",
    "category",
    "classification",
    "increase",
    "true",
    "false",
    "zero",
    "person",
    "height",
    "big",
    "pull",
    "normal",
    "question",
    "groups",
    "similar",
    "notice",
    "group",
    "loan",
    "oh",
    "favorite",
    "couple",
    "cases",
    "figure",
    "images",
    "capital",
    "c",
    "individual",
    "bottom",
    "back",
    "regular",
    "bring",
    "objects",
    "triangle",
    "square",
    "trained",
    "always",
    "keep",
    "hard",
    "running",
    "instead",
    "telling",
    "looks",
    "talked",
    "common",
    "five",
    "type",
    "state",
    "went",
    "direction",
    "playing",
    "outcome",
    "wrong",
    "left",
    "forth",
    "tree",
    "relationship",
    "variables",
    "x",
    "single",
    "variable",
    "equals",
    "mx",
    "plus",
    "distance",
    "speed",
    "coefficient",
    "36",
    "20",
    "compute",
    "30",
    "head",
    "graph",
    "slope",
    "positive",
    "100",
    "since",
    "half",
    "divided",
    "minus",
    "formula",
    "5",
    "value",
    "plot",
    "points",
    "calculate",
    "basically",
    "added",
    "divide",
    "add",
    "end",
    "eight",
    "color",
    "lines",
    "equation",
    "sum",
    "squared",
    "creating",
    "columns",
    "fill",
    "column",
    "total",
    "plug",
    "flip",
    "solve",
    "p",
    "actual",
    "root",
    "math",
    "gets",
    "though",
    "dimensions",
    "z",
    "represents",
    "tells",
    "day",
    "golf",
    "open",
    "spreadsheet",
    "outlook",
    "sunny",
    "windy",
    "purchase",
    "outside",
    "split",
    "studying",
    "cancer",
    "measurements",
    "entropy",
    "gain",
    "low",
    "side",
    "n",
    "code",
    "log",
    "script",
    "target",
    "class",
    "14",
    "full",
    "calculated",
    "e",
    "later",
    "comma",
    "0",
    "similarly",
    "lower",
    "attributes",
    "although",
    "view",
    "visual",
    "came",
    "idea",
    "creates",
    "sample",
    "consider",
    "confusing",
    "hyperplane",
    "equal",
    "maximum",
    "reasons",
    "statement",
    "coding",
    "recipes",
    "flour",
    "sugar",
    "anaconda",
    "jupiter",
    "kinds",
    "fun",
    "try",
    "notebook",
    "editor",
    "file",
    "import",
    "np",
    "pd",
    "array",
    "frame",
    "sk",
    "visualize",
    "obviously",
    "plt",
    "seaborn",
    "sns",
    "followed",
    "library",
    "matplot",
    "display",
    "inline",
    "modules",
    "imported",
    "csv",
    "separated",
    "module",
    "read",
    "brackets",
    "print",
    "panda",
    "gon",
    "na",
    "six",
    "automatically",
    "row",
    "flower",
    "command",
    "sets",
    "anything",
    "scatter",
    "parts",
    "function",
    "colon",
    "list",
    "options",
    "limit",
    "convert",
    "kernel",
    "classifier",
    "created",
    "putting",
    "show",
    "w",
    "underscore",
    "digging",
    "confusion",
    "vectors",
    "yy",
    "load",
    "pie",
    "width",
    "plotting",
    "paste",
    "chart",
    "demo",
    "malignant",
    "tool",
    "previous",
    "bunch",
    "terms",
    "pick",
    "centroids",
    "minimum",
    "centroid",
    "elbow",
    "central",
    "score",
    "okay",
    "diagonal",
    "importing",
    "check",
    "addition",
    "null",
    "rows",
    "separate",
    "um",
    "range",
    "max",
    "multiple",
    "generate",
    "title",
    "matrix",
    "dive",
    "chance",
    "classified",
    "b9",
    "id",
    "radius",
    "texture",
    "area",
    "showing",
    "median",
    "heat",
    "pieces",
    "shows",
    "worst",
    "splitting",
    "sklearn",
    "stands",
    "precision",
    "f1",
    "recall",
    "enough",
    "matrixes",
    "thousand",
    "hundred",
    "variance",
    "equations",
    "describe",
    "gradient",
    "12",
    "smaller",
    "shape",
    "multiplication",
    "dealing",
    "hypothesis",
    "gene",
    "scalar",
    "transform",
    "skewed",
    "15",
    "25",
    "piece",
    "samples",
    "length",
    "accurate",
    "skip",
    "population",
    "theory",
    "mode",
    "spread",
    "deviation",
    "income",
    "observations",
    "df",
    "names",
    "event",
    "dice",
    "bayes",
    "china",
    "table",
    "naive",
    "fitting",
    "colors",
    "grid",
    "profit",
    "animals",
    "clf",
    "iris",
    "species",
    "setosa",
    "virginica",
    "diabetes",
    "bias",
    "overfitting",
    "loss",
    "ridge",
    "daily",
    "bar",
    "pca",
    "components",
    "covariance",
    "coronavirus",
    "deaths",
    "polynomial",
    "april",
    "confirmed",
    "lakhs",
    "italy",
    "france",
    "death",
    "active",
    "recovered",
    "recoveries"
  ]
}