{
  "text": "welcome back so we're talking about\nneural networks which is a very powerful\nexpressive machine learning architecture\nto learn arbitrary input/output\nfunctions given that you have enough\ntraining data so now I want to walk\nthrough a little bit of the architecture\nand kind of how you build neural\nnetworks what they're made of things\nlike that so the basic building block of\na neural network is a neuron which is\nthis little functional unit kind of an\ninput/output node or neuron and to be\nmathematically precise you have kind of\nan input signal u that goes into this\nnode and it does some mathematical\noperation on you to give you some output\nY and this could be something like just\nmultiplying it by a constant or adding a\nconstant or it can be more more\nsophisticated so often people use\nsigmoidal function z' these are called\nactivation functions where if u is small\nthe output is just 0 and if u as large\nregardless of how large it is\nthe output might be 1 and then there's\nsome smooth activation function from 0\nto 1 this is a sigmoid 'el or a\nhyperbolic tangent activation function\nvery very common also neural inspired so\na lot of the neurons in your brain kind\nof have these sigmoid activation\nfunctions so this is really really\ncommonly used increasingly now people\nuse this kind of this rel u linear kind\nof it's it's 0 up until a point and then\nit grows linearly with you this is a\nvery useful useful building blocks that\npeople are using in modern machine\nlearning architecture and I could give a\nwhole lecture on all of these different\nactivation functions there's tons of\nthem some are better for something some\nare better for others the rel U is very\ncommon so is the sigmoid 'el so you take\nthat that neuron that unit and you start\nto stack it either in series or in\nparallel or both so I can put two\nneurons next to each other and do a more\ncomplicated function this function this\nfunction followed by this function I can\nhave middle layers I can\nfor example do three different functions\nfrom this output and then I can add all\nof those up or multiply them or do some\nother function downstream and so I can\nbuild up this kind of complexity in\nwhat's known as an artificial neural\nnetwork so here I built up this neural\nnetwork it's a network with nodes and\nedges describing the topology of how\nit's connected but it's artificial\nso neural networks you know you have\nneural networks in your head this is an\nartificial one built up out of these\nbuilding blocks and the different nodes\ncan have different activation functions\nyou they can add up linear combinations\nof their the previous layer and things\nlike that okay and then if you keep\nstacking more and more and more of these\nlayers so each layer is doing some kind\nof sequential processing if you start to\nadd a lot of these then you have what's\nknown as a deep neural network and this\nis the basis of deep learning today okay\nand so there are a ton of things you can\ndo with these neural network\narchitectures I'm just going to show you\na few here so this is this is kind of\nthe neural networks zoom my colleague\nNathan cuts made this image it was\ninspired by the Asimov Institute's\nneural network zoom this is in our\ntextbook data driven science and\nengineering and this just gives you an\nidea of some of the the massive variety\nof neural network architectures that you\ncan design and so each color for the\ndifferent nodes means different types of\nnodes different types of computations\nthat are happening there's also the\ndifferent topology of whether or not\ninformation is getting compressed in a\nbottleneck and expanded or in all of\nthese different architectures and this\nis only a few of kind of the the\narchitectures out there in neural\nnetwork so really there there are tons\nof different and every every year there\nare dozens of new architectures being\nproposed to solve different problems so\nit really is a zoo there's a ton of a\nton of different things you can do and\nwe're increasingly learning through\ntrial and error and study and analysis\nand lots of hard work by many\nresearchers which architectures are good\nfor which problems and so again this is\njust just a few of them\nso a couple of key ones I want to point\nout convolutional neural networks CN NS\nare really really important these are\nused a lot in image recognition the\nbasic idea is if I have a big image this\narray here I don't know if you can see\nbut there's a smiley face I took this\nfrom Wikipedia what a con the new\nconvolutional neural network does is it\nhas these convolutional layers that\nbasically take a mask and slide it\nacross the image doing local\ncomputations in local patches and so\nwhat you might be able to do is pull out\nedges or features and you would run that\nthrough a convolutional layer and pull\nout these these edges and then you might\nrun those through another convolution at\nlayer and another convolutional layer\nand you stack these convolutional layers\nand then you do some processing on them\nand so convolutional neural network CN\nNS are really important for image\nrecognition anything where there is a\ntranslation invariant so you know if I\nhave a picture of a cat the cat could be\nover here or it could be over here CN NS\ncan start to pull out kind of this\ntranslational invariance that exists in\nimages recurrent neural networks are\nreally useful for audio and temporal\nsignal signals that vary in time like if\nyou look at the the acoustic signature\nof speech in time and what these do I\ndidn't actually draw it but what you can\ndo is you can add these kind of feedback\nloops so you can have the neurons\nfeeding back to themselves or I can have\ndifferent layers kind of feeding back\nand so this allows you to have this kind\nof temporal feedback where there's some\nmemory in the system it's not just a\nfeed-forward Network which is what I\nshowed you before where all the\ninformation just flows from left to\nright here you have this kind of\nfeedback network this recurrent neural\nnetwork that gives you this memory so LS\nTMS long short term memory networks are\nreally useful for audio processing and\ndynamical systems anything that varies\nin time because they have these kind of\nfeedback signals which allow you to have\nkind of memory so there's information\nthat it's just living here kind of\ngetting recirculated through this\nnetwork so that's pretty cool very very\ninteresting architecture another one\nthat I really liked a lot\nand I use a lot is the auto-encoder so\nhere I'm showing you kind of what I\nthink of as a shallow linear\nauto-encoder meaning that my nodes are\ndoing linear combinations just taking\nlinear combinations of the input layer\nand what the auto encoder is trying to\ndo is take a high dimensional signal\ncompress it down to some latent space\nthis is Z variable that's all the\ninformation I need to keep track of in a\nway that that Z information can be\nrelisted back to the high dimensional\nimage so that X hat is approximately\nequal to X okay so this is kind of a\ncompression decompression or an encoder\ndecoder architecture where you can take\nbig data high dimensional data and\nfigure out what is the kind of the late\nin space what are the degrees of freedom\nthat matter and so researchers showed a\nlong time ago that you could reconstruct\nthe famous principal component analysis\nwhich has been around for a hundred\nyears in this neural network\narchitecture this shallow linear auto\nencoder network and since then\nresearchers have massively generalized\nthis too deep autoencoders where now the\nthe nodes can have nonlinear activation\nfunctions and I can have many many many\nmany layers and get better compression\nbetter kind of extraction of the\nessential features of my high\ndimensional data in this late in space\nis Eve and that's one of the reasons I\nlike these models is because there's\nsome interpretability that you get when\nyou do this kind of information\nbottleneck this compression down to this\nlatent space because there might be few\nenough variables here that I can\nactually analyze and try to understand\nwhat these mean with respect to the data\nso I think these are kind of just some\nneat architectures you have\nconvolutional neural networks recurrent\nneural networks you have these auto\nencoder networks and there's many many\nmore kind of as much as you can imagine\nyou can build a network to do it and\nmaybe that's the last thing I want to\npoint out is that designing and\nimplementing these architectures is\nbecoming extremely simple because of the\nexplosion of open source software put\nout by Google and Facebook and others so\nyou have Tenzer flow and pi torch and\ncaris which are these incredibly\npowerful environments where you can\ndesign\nneural network architectures and then\ntrain them with with training data to\nbuild these very very powerful and\nexpressive models so it's a really neat\nand also increasingly easy to design and\nuse okay thank you\n",
  "words": [
    "welcome",
    "back",
    "talking",
    "neural",
    "networks",
    "powerful",
    "expressive",
    "machine",
    "learning",
    "architecture",
    "learn",
    "arbitrary",
    "functions",
    "given",
    "enough",
    "training",
    "data",
    "want",
    "walk",
    "little",
    "bit",
    "architecture",
    "kind",
    "build",
    "neural",
    "networks",
    "made",
    "things",
    "like",
    "basic",
    "building",
    "block",
    "neural",
    "network",
    "neuron",
    "little",
    "functional",
    "unit",
    "kind",
    "node",
    "neuron",
    "mathematically",
    "precise",
    "kind",
    "input",
    "signal",
    "u",
    "goes",
    "node",
    "mathematical",
    "operation",
    "give",
    "output",
    "could",
    "something",
    "like",
    "multiplying",
    "constant",
    "adding",
    "constant",
    "sophisticated",
    "often",
    "people",
    "use",
    "sigmoidal",
    "function",
    "z",
    "called",
    "activation",
    "functions",
    "u",
    "small",
    "output",
    "0",
    "u",
    "large",
    "regardless",
    "large",
    "output",
    "might",
    "1",
    "smooth",
    "activation",
    "function",
    "0",
    "1",
    "sigmoid",
    "hyperbolic",
    "tangent",
    "activation",
    "function",
    "common",
    "also",
    "neural",
    "inspired",
    "lot",
    "neurons",
    "brain",
    "kind",
    "sigmoid",
    "activation",
    "functions",
    "really",
    "really",
    "commonly",
    "used",
    "increasingly",
    "people",
    "use",
    "kind",
    "rel",
    "u",
    "linear",
    "kind",
    "0",
    "point",
    "grows",
    "linearly",
    "useful",
    "useful",
    "building",
    "blocks",
    "people",
    "using",
    "modern",
    "machine",
    "learning",
    "architecture",
    "could",
    "give",
    "whole",
    "lecture",
    "different",
    "activation",
    "functions",
    "tons",
    "better",
    "something",
    "better",
    "others",
    "rel",
    "u",
    "common",
    "sigmoid",
    "take",
    "neuron",
    "unit",
    "start",
    "stack",
    "either",
    "series",
    "parallel",
    "put",
    "two",
    "neurons",
    "next",
    "complicated",
    "function",
    "function",
    "function",
    "followed",
    "function",
    "middle",
    "layers",
    "example",
    "three",
    "different",
    "functions",
    "output",
    "add",
    "multiply",
    "function",
    "downstream",
    "build",
    "kind",
    "complexity",
    "known",
    "artificial",
    "neural",
    "network",
    "built",
    "neural",
    "network",
    "network",
    "nodes",
    "edges",
    "describing",
    "topology",
    "connected",
    "artificial",
    "neural",
    "networks",
    "know",
    "neural",
    "networks",
    "head",
    "artificial",
    "one",
    "built",
    "building",
    "blocks",
    "different",
    "nodes",
    "different",
    "activation",
    "functions",
    "add",
    "linear",
    "combinations",
    "previous",
    "layer",
    "things",
    "like",
    "okay",
    "keep",
    "stacking",
    "layers",
    "layer",
    "kind",
    "sequential",
    "processing",
    "start",
    "add",
    "lot",
    "known",
    "deep",
    "neural",
    "network",
    "basis",
    "deep",
    "learning",
    "today",
    "okay",
    "ton",
    "things",
    "neural",
    "network",
    "architectures",
    "going",
    "show",
    "kind",
    "neural",
    "networks",
    "zoom",
    "colleague",
    "nathan",
    "cuts",
    "made",
    "image",
    "inspired",
    "asimov",
    "institute",
    "neural",
    "network",
    "zoom",
    "textbook",
    "data",
    "driven",
    "science",
    "engineering",
    "gives",
    "idea",
    "massive",
    "variety",
    "neural",
    "network",
    "architectures",
    "design",
    "color",
    "different",
    "nodes",
    "means",
    "different",
    "types",
    "nodes",
    "different",
    "types",
    "computations",
    "happening",
    "also",
    "different",
    "topology",
    "whether",
    "information",
    "getting",
    "compressed",
    "bottleneck",
    "expanded",
    "different",
    "architectures",
    "kind",
    "architectures",
    "neural",
    "network",
    "really",
    "tons",
    "different",
    "every",
    "every",
    "year",
    "dozens",
    "new",
    "architectures",
    "proposed",
    "solve",
    "different",
    "problems",
    "really",
    "zoo",
    "ton",
    "ton",
    "different",
    "things",
    "increasingly",
    "learning",
    "trial",
    "error",
    "study",
    "analysis",
    "lots",
    "hard",
    "work",
    "many",
    "researchers",
    "architectures",
    "good",
    "problems",
    "couple",
    "key",
    "ones",
    "want",
    "point",
    "convolutional",
    "neural",
    "networks",
    "cn",
    "ns",
    "really",
    "really",
    "important",
    "used",
    "lot",
    "image",
    "recognition",
    "basic",
    "idea",
    "big",
    "image",
    "array",
    "know",
    "see",
    "smiley",
    "face",
    "took",
    "wikipedia",
    "con",
    "new",
    "convolutional",
    "neural",
    "network",
    "convolutional",
    "layers",
    "basically",
    "take",
    "mask",
    "slide",
    "across",
    "image",
    "local",
    "computations",
    "local",
    "patches",
    "might",
    "able",
    "pull",
    "edges",
    "features",
    "would",
    "run",
    "convolutional",
    "layer",
    "pull",
    "edges",
    "might",
    "run",
    "another",
    "convolution",
    "layer",
    "another",
    "convolutional",
    "layer",
    "stack",
    "convolutional",
    "layers",
    "processing",
    "convolutional",
    "neural",
    "network",
    "cn",
    "ns",
    "really",
    "important",
    "image",
    "recognition",
    "anything",
    "translation",
    "invariant",
    "know",
    "picture",
    "cat",
    "cat",
    "could",
    "could",
    "cn",
    "ns",
    "start",
    "pull",
    "kind",
    "translational",
    "invariance",
    "exists",
    "images",
    "recurrent",
    "neural",
    "networks",
    "really",
    "useful",
    "audio",
    "temporal",
    "signal",
    "signals",
    "vary",
    "time",
    "like",
    "look",
    "acoustic",
    "signature",
    "speech",
    "time",
    "actually",
    "draw",
    "add",
    "kind",
    "feedback",
    "loops",
    "neurons",
    "feeding",
    "back",
    "different",
    "layers",
    "kind",
    "feeding",
    "back",
    "allows",
    "kind",
    "temporal",
    "feedback",
    "memory",
    "system",
    "network",
    "showed",
    "information",
    "flows",
    "left",
    "right",
    "kind",
    "feedback",
    "network",
    "recurrent",
    "neural",
    "network",
    "gives",
    "memory",
    "ls",
    "tms",
    "long",
    "short",
    "term",
    "memory",
    "networks",
    "really",
    "useful",
    "audio",
    "processing",
    "dynamical",
    "systems",
    "anything",
    "varies",
    "time",
    "kind",
    "feedback",
    "signals",
    "allow",
    "kind",
    "memory",
    "information",
    "living",
    "kind",
    "getting",
    "recirculated",
    "network",
    "pretty",
    "cool",
    "interesting",
    "architecture",
    "another",
    "one",
    "really",
    "liked",
    "lot",
    "use",
    "lot",
    "showing",
    "kind",
    "think",
    "shallow",
    "linear",
    "meaning",
    "nodes",
    "linear",
    "combinations",
    "taking",
    "linear",
    "combinations",
    "input",
    "layer",
    "auto",
    "encoder",
    "trying",
    "take",
    "high",
    "dimensional",
    "signal",
    "compress",
    "latent",
    "space",
    "z",
    "variable",
    "information",
    "need",
    "keep",
    "track",
    "way",
    "z",
    "information",
    "relisted",
    "back",
    "high",
    "dimensional",
    "image",
    "x",
    "hat",
    "approximately",
    "equal",
    "x",
    "okay",
    "kind",
    "compression",
    "decompression",
    "encoder",
    "decoder",
    "architecture",
    "take",
    "big",
    "data",
    "high",
    "dimensional",
    "data",
    "figure",
    "kind",
    "late",
    "space",
    "degrees",
    "freedom",
    "matter",
    "researchers",
    "showed",
    "long",
    "time",
    "ago",
    "could",
    "reconstruct",
    "famous",
    "principal",
    "component",
    "analysis",
    "around",
    "hundred",
    "years",
    "neural",
    "network",
    "architecture",
    "shallow",
    "linear",
    "auto",
    "encoder",
    "network",
    "since",
    "researchers",
    "massively",
    "generalized",
    "deep",
    "autoencoders",
    "nodes",
    "nonlinear",
    "activation",
    "functions",
    "many",
    "many",
    "many",
    "many",
    "layers",
    "get",
    "better",
    "compression",
    "better",
    "kind",
    "extraction",
    "essential",
    "features",
    "high",
    "dimensional",
    "data",
    "late",
    "space",
    "eve",
    "one",
    "reasons",
    "like",
    "models",
    "interpretability",
    "get",
    "kind",
    "information",
    "bottleneck",
    "compression",
    "latent",
    "space",
    "might",
    "enough",
    "variables",
    "actually",
    "analyze",
    "try",
    "understand",
    "mean",
    "respect",
    "data",
    "think",
    "kind",
    "neat",
    "architectures",
    "convolutional",
    "neural",
    "networks",
    "recurrent",
    "neural",
    "networks",
    "auto",
    "encoder",
    "networks",
    "many",
    "many",
    "kind",
    "much",
    "imagine",
    "build",
    "network",
    "maybe",
    "last",
    "thing",
    "want",
    "point",
    "designing",
    "implementing",
    "architectures",
    "becoming",
    "extremely",
    "simple",
    "explosion",
    "open",
    "source",
    "software",
    "put",
    "google",
    "facebook",
    "others",
    "tenzer",
    "flow",
    "pi",
    "torch",
    "caris",
    "incredibly",
    "powerful",
    "environments",
    "design",
    "neural",
    "network",
    "architectures",
    "train",
    "training",
    "data",
    "build",
    "powerful",
    "expressive",
    "models",
    "really",
    "neat",
    "also",
    "increasingly",
    "easy",
    "design",
    "use",
    "okay",
    "thank"
  ],
  "keywords": [
    "back",
    "neural",
    "networks",
    "powerful",
    "learning",
    "architecture",
    "functions",
    "data",
    "want",
    "kind",
    "build",
    "things",
    "like",
    "building",
    "network",
    "neuron",
    "signal",
    "u",
    "output",
    "could",
    "people",
    "use",
    "function",
    "z",
    "activation",
    "0",
    "might",
    "sigmoid",
    "also",
    "lot",
    "neurons",
    "really",
    "increasingly",
    "linear",
    "point",
    "useful",
    "different",
    "better",
    "take",
    "start",
    "layers",
    "add",
    "artificial",
    "nodes",
    "edges",
    "know",
    "one",
    "combinations",
    "layer",
    "okay",
    "processing",
    "deep",
    "ton",
    "architectures",
    "image",
    "design",
    "information",
    "many",
    "researchers",
    "convolutional",
    "cn",
    "ns",
    "pull",
    "another",
    "recurrent",
    "time",
    "feedback",
    "memory",
    "auto",
    "encoder",
    "high",
    "dimensional",
    "space",
    "compression"
  ]
}