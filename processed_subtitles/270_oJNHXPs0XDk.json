{
  "text": "welcome back so we're talking about\nneural networks which is a very powerful\nexpressive machine learning architecture\nto learn arbitrary input/output\nfunctions given that you have enough\ntraining data so now I want to walk\nthrough a little bit of the architecture\nand kind of how you build neural\nnetworks what they're made of things\nlike that so the basic building block of\na neural network is a neuron which is\nthis little functional unit kind of an\ninput/output node or neuron and to be\nmathematically precise you have kind of\nan input signal u that goes into this\nnode and it does some mathematical\noperation on you to give you some output\nY and this could be something like just\nmultiplying it by a constant or adding a\nconstant or it can be more more\nsophisticated so often people use\nsigmoidal function z' these are called\nactivation functions where if u is small\nthe output is just 0 and if u as large\nregardless of how large it is\nthe output might be 1 and then there's\nsome smooth activation function from 0\nto 1 this is a sigmoid 'el or a\nhyperbolic tangent activation function\nvery very common also neural inspired so\na lot of the neurons in your brain kind\nof have these sigmoid activation\nfunctions so this is really really\ncommonly used increasingly now people\nuse this kind of this rel u linear kind\nof it's it's 0 up until a point and then\nit grows linearly with you this is a\nvery useful useful building blocks that\npeople are using in modern machine\nlearning architecture and I could give a\nwhole lecture on all of these different\nactivation functions there's tons of\nthem some are better for something some\nare better for others the rel U is very\ncommon so is the sigmoid ' so you take\nthat that neuron that unit and you start\nto stack it either in series or in\nparallel or both so I can put two\nneurons next to each other and do a more\ncomplicated function this function this\nfunction followed by this function I can\nhave middle layers I can\nfor example do three different functions\nfrom this output and then I can add all\nof those up or multiply them or do some\nother function downstream and so I can\nbuild up this kind of complexity in\nwhat's known as an artificial neural\nnetwork so here I built up this neural\nnetwork it's a network with nodes and\nedges describing the topology of how\nit's connected but it's artificial\nso neural networks you know you have\nneural networks in your head this is an\nartificial one built up out of these\nbuilding blocks and the different nodes\ncan have different activation functions\nyou they can add up linear combinations\nof their the previous layer and things\nlike that and then if you keep\nstacking more and more and more of these\nlayers so each layer is doing some kind\nof sequential processing if you start to\nadd a lot of these then you have what's\nknown as a deep neural network and this\nis the basis of deep learning today okay\n and so there are a ton of things you can\ndo with these neural network\narchitectures I'm just going to show you\na few here so this is this is kind of\nthe neural networks zoom my colleague\nNathan cuts made this image it was\ninspired by the Asimov Institute's\nneural network zoom this is in our\ntextbook data driven science and\nengineering and this just gives you an\nidea of some of the the massive variety\nof neural network architectures that you\ncan design and so each color for the\ndifferent nodes means different types of\nnodes different types of computations\nthat are happening there's also the\ndifferent topology of whether or not\ninformation is getting compressed in a\nbottleneck and expanded or in all of\nthese different architectures and this\nis only a few of kind of the the\narchitectures out there in neural\nnetwork so really there there are tons\nof different and every every year there\nare dozens of new architectures being\nproposed to solve different problems so\nit really is a zoo there's a ton of a\nton of different things you can do and\nwe're increasingly learning through\ntrial and error and study and analysis\nand lots of hard work by many\nresearchers which architectures are good\nfor which problems and so again this is\njust just a few of them\nso a couple of key ones I want to point\nout convolutional neural networks CN NS\nare really really important these are\nused a lot in image recognition the\nbasic idea is if I have a big image this\narray here I don't know if you can see\nbut there's a smiley face I took this\nfrom Wikipedia what a con the new\nconvolutional neural network does is it\nhas these convolutional layers that\nbasically take a mask and slide it\nacross the image doing local\ncomputations in local patches and so\nwhat you might be able to do is pull out\nedges or features and you would run that\nthrough a convolutional layer and pull\nout these these edges and then you might\nrun those through another convolution at\nlayer and another convolutional layer\nand you stack these convolutional layers\nand then you do some processing on them\nand so convolutional neural network CN\nNS are really important for image\nrecognition anything where there is a\ntranslation invariant so you know if I\nhave a picture of a cat the cat could be\nover here or it could be over here CN NS\ncan start to pull out kind of this\ntranslational invariance that exists in\nimages recurrent neural networks are\nreally useful for audio and temporal\nsignal signals that vary in time like if\nyou look at the the acoustic signature\nof speech in time and what these do I\ndidn't actually draw it but what you can\ndo is you can add these kind of feedback\nloops so you can have the neurons\nfeeding back to themselves or I can have\ndifferent layers kind of feeding back\n and so this allows you to have this kind\nof temporal feedback where there's some\nmemory in the system it's not just a\nfeed-forward Network which is what I\nshowed you before where all the\ninformation just flows from left to\nright here you have this kind of\nfeedback network this recurrent neural\nnetwork that gives you this memory so LS\nTMS long short term memory networks are\nreally useful for audio processing and\ndynamical systems anything that varies\nin time because they have these kind of\nfeedback signals which allow you to have\nkind of memory so there's information\nthat it's just living here kind of\ngetting recirculated through this\nnetwork so that's pretty cool very very\ninteresting architecture another one\nthat I really liked a lot\nand I use a lot is the auto-encoder so\nhere I'm showing you kind of what I\nthink of as a shallow linear\nauto-encoder meaning that my nodes are\ndoing linear combinations just taking\nlinear combinations of the input layer\nand what the auto encoder is trying to\ndo is take a high dimensional signal\ncompress it down to some latent space\nthis is Z variable that's all the\ninformation I need to keep track of in a\nway that that Z information can be\nrelisted back to the high dimensional\nimage so that X hat is approximately\nequal to X so this is kind of a\ncompression decompression or an encoder\ndecoder architecture where you can take\nbig data high dimensional data and\nfigure out what is the kind of the late\nin space what are the degrees of freedom\nthat matter and so researchers showed a\nlong time ago that you could reconstruct\nthe famous principal component analysis\nwhich has been around for a hundred\nyears in this neural network\narchitecture this shallow linear auto\nencoder network and since then\nresearchers have massively generalized\nthis too deep autoencoders where now the\nthe nodes can have nonlinear activation\nfunctions and I can have many many many\nmany layers and get better compression\nbetter kind of extraction of the\nessential features of my high\ndimensional data in this late in space\nis Eve and that's one of the reasons I\nlike these models is because there's\nsome interpretability that you get when\nyou do this kind of information\nbottleneck this compression down to this\nlatent space because there might be few\nenough variables here that I can\nactually analyze and try to understand\nwhat these mean with respect to the data\n so I think these are kind of just some\nneat architectures you have\nconvolutional neural networks recurrent\nneural networks you have these auto\nencoder networks and there's many many\nmore kind of as much as you can imagine\nyou can build a network to do it and\nmaybe that's the last thing I want to\npoint out is that designing and\nimplementing these architectures is\nbecoming extremely simple because of the\nexplosion of open source software put\nout by Google and Facebook and others so\nyou have Tenzer flow and pi torch and\ncaris which are these incredibly\npowerful environments where you can\ndesign\nneural network architectures and then\ntrain them with with training data to\nbuild these very very powerful and\nexpressive models so it's a really neat\nand also increasingly easy to design and\nuse okay thank you\n\n",
  "sentences": [
    "welcome back so we're talking about\nneural networks which is a very powerful\nexpressive machine learning architecture\nto learn arbitrary input/output\nfunctions given that you have enough\ntraining data so now I want to walk\nthrough a little bit of the architecture\nand kind of how you build neural\nnetworks what they're made of things\nlike that so the basic building block of\na neural network is a neuron which is\nthis little functional unit kind of an\ninput/output node or neuron and to be\nmathematically precise you have kind of\nan input signal u that goes into this\nnode and it does some mathematical\noperation on you to give you some output\nY and this could be something like just\nmultiplying it by a constant or adding a\nconstant or it can be more more\nsophisticated so often people use\nsigmoidal function z' these are called\nactivation functions where if u is small\nthe output is just 0 and if u as large\nregardless of how large it is\nthe output might be 1 and then there's\nsome smooth activation function from 0\nto 1 this is a sigmoid 'el or a\nhyperbolic tangent activation function\nvery very common also neural inspired so\na lot of the neurons in your brain kind\nof have these sigmoid activation\nfunctions so this is really really\ncommonly used increasingly now people\nuse this kind of this rel u linear kind\nof it's it's 0 up until a point and then\nit grows linearly with you this is a\nvery useful useful building blocks that\npeople are using in modern machine\nlearning architecture and I could give a\nwhole lecture on all of these different\nactivation functions there's tons of\nthem some are better for something some\nare better for others the rel U is very\ncommon so is the sigmoid ' so you take\nthat that neuron that unit and you start\nto stack it either in series or in\nparallel or both so I can put two\nneurons next to each other and do a more\ncomplicated function this function this\nfunction followed by this function I can\nhave middle layers I can\nfor example do three different functions\nfrom this output and then I can add all\nof those up or multiply them or do some\nother function downstream and so I can\nbuild up this kind of complexity in\nwhat's known as an artificial neural\nnetwork so here I built up this neural\nnetwork it's a network with nodes and\nedges describing the topology of how\nit's connected but it's artificial\nso neural networks you know you have\nneural networks in your head this is an\nartificial one built up out of these\nbuilding blocks and the different nodes\ncan have different activation functions\nyou they can add up linear combinations\nof their the previous layer and things\nlike that and then if you keep\nstacking more and more and more of these\nlayers so each layer is doing some kind\nof sequential processing if you start to\nadd a lot of these then you have what's\nknown as a deep neural network and this\nis the basis of deep learning today okay\n and so there are a ton of things you can\ndo with these neural network\narchitectures I'm just going to show you\na few here so this is this is kind of\nthe neural networks zoom my colleague\nNathan cuts made this image it was\ninspired by the Asimov Institute's\nneural network zoom this is in our\ntextbook data driven science and\nengineering and this just gives you an\nidea of some of the the massive variety\nof neural network architectures that you\ncan design and so each color for the\ndifferent nodes means different types of\nnodes different types of computations\nthat are happening there's also the\ndifferent topology of whether or not\ninformation is getting compressed in a\nbottleneck and expanded or in all of\nthese different architectures and this\nis only a few of kind of the the\narchitectures out there in neural\nnetwork so really there there are tons\nof different and every every year there\nare dozens of new architectures being\nproposed to solve different problems so\nit really is a zoo there's a ton of a\nton of different things you can do and\nwe're increasingly learning through\ntrial and error and study and analysis\nand lots of hard work by many\nresearchers which architectures are good\nfor which problems and so again this is\njust just a few of them\nso a couple of key ones I want to point\nout convolutional neural networks CN NS\nare really really important these are\nused a lot in image recognition the\nbasic idea is if I have a big image this\narray here I don't know if you can see\nbut there's a smiley face I took this\nfrom Wikipedia what a con the new\nconvolutional neural network does is it\nhas these convolutional layers that\nbasically take a mask and slide it\nacross the image doing local\ncomputations in local patches and so\nwhat you might be able to do is pull out\nedges or features and you would run that\nthrough a convolutional layer and pull\nout these these edges and then you might\nrun those through another convolution at\nlayer and another convolutional layer\nand you stack these convolutional layers\nand then you do some processing on them\nand so convolutional neural network CN\nNS are really important for image\nrecognition anything where there is a\ntranslation invariant so you know if I\nhave a picture of a cat the cat could be\nover here or it could be over here CN NS\ncan start to pull out kind of this\ntranslational invariance that exists in\nimages recurrent neural networks are\nreally useful for audio and temporal\nsignal signals that vary in time like if\nyou look at the the acoustic signature\nof speech in time and what these do I\ndidn't actually draw it but what you can\ndo is you can add these kind of feedback\nloops so you can have the neurons\nfeeding back to themselves or I can have\ndifferent layers kind of feeding back\n and so this allows you to have this kind\nof temporal feedback where there's some\nmemory in the system it's not just a\nfeed-forward Network which is what I\nshowed you before where all the\ninformation just flows from left to\nright here you have this kind of\nfeedback network this recurrent neural\nnetwork that gives you this memory so LS\nTMS long short term memory networks are\nreally useful for audio processing and\ndynamical systems anything that varies\nin time because they have these kind of\nfeedback signals which allow you to have\nkind of memory so there's information\nthat it's just living here kind of\ngetting recirculated through this\nnetwork so that's pretty cool very very\ninteresting architecture another one\nthat I really liked a lot\nand I use a lot is the auto-encoder so\nhere I'm showing you kind of what I\nthink of as a shallow linear\nauto-encoder meaning that my nodes are\ndoing linear combinations just taking\nlinear combinations of the input layer\nand what the auto encoder is trying to\ndo is take a high dimensional signal\ncompress it down to some latent space\nthis is Z variable that's all the\ninformation I need to keep track of in a\nway that that Z information can be\nrelisted back to the high dimensional\nimage so that X hat is approximately\nequal to X so this is kind of a\ncompression decompression or an encoder\ndecoder architecture where you can take\nbig data high dimensional data and\nfigure out what is the kind of the late\nin space what are the degrees of freedom\nthat matter and so researchers showed a\nlong time ago that you could reconstruct\nthe famous principal component analysis\nwhich has been around for a hundred\nyears in this neural network\narchitecture this shallow linear auto\nencoder network and since then\nresearchers have massively generalized\nthis too deep autoencoders where now the\nthe nodes can have nonlinear activation\nfunctions and I can have many many many\nmany layers and get better compression\nbetter kind of extraction of the\nessential features of my high\ndimensional data in this late in space\nis Eve and that's one of the reasons I\nlike these models is because there's\nsome interpretability that you get when\nyou do this kind of information\nbottleneck this compression down to this\nlatent space because there might be few\nenough variables here that I can\nactually analyze and try to understand\nwhat these mean with respect to the data\n so I think these are kind of just some\nneat architectures you have\nconvolutional neural networks recurrent\nneural networks you have these auto\nencoder networks and there's many many\nmore kind of as much as you can imagine\nyou can build a network to do it and\nmaybe that's the last thing I want to\npoint out is that designing and\nimplementing these architectures is\nbecoming extremely simple because of the\nexplosion of open source software put\nout by Google and Facebook and others so\nyou have Tenzer flow and pi torch and\ncaris which are these incredibly\npowerful environments where you can\ndesign\nneural network architectures and then\ntrain them with with training data to\nbuild these very very powerful and\nexpressive models so it's a really neat\nand also increasingly easy to design and\nuse okay thank you"
  ],
  "paragraphs": [
    "welcome back so we're talking about",
    "neural networks which is a very powerful",
    "expressive machine learning architecture",
    "to learn arbitrary input/output",
    "functions given that you have enough",
    "training data so now I want to walk",
    "through a little bit of the architecture",
    "and kind of how you build neural",
    "networks what they're made of things",
    "like that so the basic building block of",
    "a neural network is a neuron which is",
    "this little functional unit kind of an",
    "input/output node or neuron and to be",
    "mathematically precise you have kind of",
    "an input signal u that goes into this",
    "node and it does some mathematical",
    "operation on you to give you some output",
    "Y and this could be something like just",
    "multiplying it by a constant or adding a",
    "constant or it can be more more",
    "sophisticated so often people use",
    "sigmoidal function z' these are called",
    "activation functions where if u is small",
    "the output is just 0 and if u as large",
    "regardless of how large it is",
    "the output might be 1 and then there's",
    "some smooth activation function from 0",
    "to 1 this is a sigmoid 'el or a",
    "hyperbolic tangent activation function",
    "very very common also neural inspired so",
    "a lot of the neurons in your brain kind",
    "of have these sigmoid activation",
    "functions so this is really really",
    "commonly used increasingly now people",
    "use this kind of this rel u linear kind",
    "of it's it's 0 up until a point and then",
    "it grows linearly with you this is a",
    "very useful useful building blocks that",
    "people are using in modern machine",
    "learning architecture and I could give a",
    "whole lecture on all of these different",
    "activation functions there's tons of",
    "them some are better for something some",
    "are better for others the rel U is very",
    "common so is the sigmoid ' so you take",
    "that that neuron that unit and you start",
    "to stack it either in series or in",
    "parallel or both so I can put two",
    "neurons next to each other and do a more",
    "complicated function this function this",
    "function followed by this function I can",
    "have middle layers I can",
    "for example do three different functions",
    "from this output and then I can add all",
    "of those up or multiply them or do some",
    "other function downstream and so I can",
    "build up this kind of complexity in",
    "what's known as an artificial neural",
    "network so here I built up this neural",
    "network it's a network with nodes and",
    "edges describing the topology of how",
    "it's connected but it's artificial",
    "so neural networks you know you have",
    "neural networks in your head this is an",
    "artificial one built up out of these",
    "building blocks and the different nodes",
    "can have different activation functions",
    "you they can add up linear combinations",
    "of their the previous layer and things",
    "like that and then if you keep",
    "stacking more and more and more of these",
    "layers so each layer is doing some kind",
    "of sequential processing if you start to",
    "add a lot of these then you have what's",
    "known as a deep neural network and this",
    "is the basis of deep learning today okay",
    "and so there are a ton of things you can",
    "do with these neural network",
    "architectures I'm just going to show you",
    "a few here so this is this is kind of",
    "the neural networks zoom my colleague",
    "Nathan cuts made this image it was",
    "inspired by the Asimov Institute's",
    "neural network zoom this is in our",
    "textbook data driven science and",
    "engineering and this just gives you an",
    "idea of some of the the massive variety",
    "of neural network architectures that you",
    "can design and so each color for the",
    "different nodes means different types of",
    "nodes different types of computations",
    "that are happening there's also the",
    "different topology of whether or not",
    "information is getting compressed in a",
    "bottleneck and expanded or in all of",
    "these different architectures and this",
    "is only a few of kind of the the",
    "architectures out there in neural",
    "network so really there there are tons",
    "of different and every every year there",
    "are dozens of new architectures being",
    "proposed to solve different problems so",
    "it really is a zoo there's a ton of a",
    "ton of different things you can do and",
    "we're increasingly learning through",
    "trial and error and study and analysis",
    "and lots of hard work by many",
    "researchers which architectures are good",
    "for which problems and so again this is",
    "just just a few of them",
    "so a couple of key ones I want to point",
    "out convolutional neural networks CN NS",
    "are really really important these are",
    "used a lot in image recognition the",
    "basic idea is if I have a big image this",
    "array here I don't know if you can see",
    "but there's a smiley face I took this",
    "from Wikipedia what a con the new",
    "convolutional neural network does is it",
    "has these convolutional layers that",
    "basically take a mask and slide it",
    "across the image doing local",
    "computations in local patches and so",
    "what you might be able to do is pull out",
    "edges or features and you would run that",
    "through a convolutional layer and pull",
    "out these these edges and then you might",
    "run those through another convolution at",
    "layer and another convolutional layer",
    "and you stack these convolutional layers",
    "and then you do some processing on them",
    "and so convolutional neural network CN",
    "NS are really important for image",
    "recognition anything where there is a",
    "translation invariant so you know if I",
    "have a picture of a cat the cat could be",
    "over here or it could be over here CN NS",
    "can start to pull out kind of this",
    "translational invariance that exists in",
    "images recurrent neural networks are",
    "really useful for audio and temporal",
    "signal signals that vary in time like if",
    "you look at the the acoustic signature",
    "of speech in time and what these do I",
    "didn't actually draw it but what you can",
    "do is you can add these kind of feedback",
    "loops so you can have the neurons",
    "feeding back to themselves or I can have",
    "different layers kind of feeding back",
    "and so this allows you to have this kind",
    "of temporal feedback where there's some",
    "memory in the system it's not just a",
    "feed-forward Network which is what I",
    "showed you before where all the",
    "information just flows from left to",
    "right here you have this kind of",
    "feedback network this recurrent neural",
    "network that gives you this memory so LS",
    "TMS long short term memory networks are",
    "really useful for audio processing and",
    "dynamical systems anything that varies",
    "in time because they have these kind of",
    "feedback signals which allow you to have",
    "kind of memory so there's information",
    "that it's just living here kind of",
    "getting recirculated through this",
    "network so that's pretty cool very very",
    "interesting architecture another one",
    "that I really liked a lot",
    "and I use a lot is the auto-encoder so",
    "here I'm showing you kind of what I",
    "think of as a shallow linear",
    "auto-encoder meaning that my nodes are",
    "doing linear combinations just taking",
    "linear combinations of the input layer",
    "and what the auto encoder is trying to",
    "do is take a high dimensional signal",
    "compress it down to some latent space",
    "this is Z variable that's all the",
    "information I need to keep track of in a",
    "way that that Z information can be",
    "relisted back to the high dimensional",
    "image so that X hat is approximately",
    "equal to X so this is kind of a",
    "compression decompression or an encoder",
    "decoder architecture where you can take",
    "big data high dimensional data and",
    "figure out what is the kind of the late",
    "in space what are the degrees of freedom",
    "that matter and so researchers showed a",
    "long time ago that you could reconstruct",
    "the famous principal component analysis",
    "which has been around for a hundred",
    "years in this neural network",
    "architecture this shallow linear auto",
    "encoder network and since then",
    "researchers have massively generalized",
    "this too deep autoencoders where now the",
    "the nodes can have nonlinear activation",
    "functions and I can have many many many",
    "many layers and get better compression",
    "better kind of extraction of the",
    "essential features of my high",
    "dimensional data in this late in space",
    "is Eve and that's one of the reasons I",
    "like these models is because there's",
    "some interpretability that you get when",
    "you do this kind of information",
    "bottleneck this compression down to this",
    "latent space because there might be few",
    "enough variables here that I can",
    "actually analyze and try to understand",
    "what these mean with respect to the data",
    "so I think these are kind of just some",
    "neat architectures you have",
    "convolutional neural networks recurrent",
    "neural networks you have these auto",
    "encoder networks and there's many many",
    "more kind of as much as you can imagine",
    "you can build a network to do it and",
    "maybe that's the last thing I want to",
    "point out is that designing and",
    "implementing these architectures is",
    "becoming extremely simple because of the",
    "explosion of open source software put",
    "out by Google and Facebook and others so",
    "you have Tenzer flow and pi torch and",
    "caris which are these incredibly",
    "powerful environments where you can",
    "design",
    "neural network architectures and then",
    "train them with with training data to",
    "build these very very powerful and",
    "expressive models so it's a really neat",
    "and also increasingly easy to design and",
    "use okay thank you"
  ],
  "keywords": [
    "recirculated",
    "hat",
    "recurrent",
    "nodes",
    "multiplying",
    "anything",
    "brain",
    "temporal",
    "patches",
    "equal",
    "variables",
    "researchers",
    "reconstruct",
    "operation",
    "build",
    "allows",
    "shallow",
    "commonly",
    "linear",
    "arbitrary",
    "caris",
    "multiply",
    "flows",
    "neurons",
    "machine",
    "mask",
    "hundred",
    "years",
    "modern",
    "ago",
    "signals",
    "variable",
    "encoder",
    "neural",
    "built",
    "relisted",
    "learning",
    "features",
    "give",
    "interpretability",
    "hard",
    "training",
    "simple",
    "given",
    "varies",
    "need",
    "draw",
    "made",
    "lots",
    "slide",
    "gives",
    "asimov",
    "two",
    "cat",
    "thing",
    "array",
    "lot",
    "ton",
    "invariance",
    "data",
    "u",
    "1",
    "couple",
    "precise",
    "zoom",
    "error",
    "run",
    "think",
    "mathematical",
    "key",
    "one",
    "basis",
    "example",
    "open",
    "describing",
    "point",
    "whether",
    "autoencoders",
    "talking",
    "middle",
    "liked",
    "memory",
    "sequential",
    "reasons",
    "walk",
    "powerful",
    "unit",
    "wikipedia",
    "many",
    "rel",
    "functions",
    "grows",
    "block",
    "approximately",
    "neat",
    "show",
    "space",
    "today",
    "sophisticated",
    "compression",
    "software",
    "translational",
    "freedom",
    "happening",
    "something",
    "big",
    "essential",
    "institute",
    "architecture",
    "topology",
    "take",
    "using",
    "time",
    "eve",
    "really",
    "add",
    "science",
    "types",
    "extremely",
    "blocks",
    "right",
    "tenzer",
    "edges",
    "tangent",
    "system",
    "auto",
    "layer",
    "stack",
    "previous",
    "combinations",
    "three",
    "signature",
    "known",
    "google",
    "track",
    "vary",
    "sigmoid",
    "lecture",
    "neuron",
    "look",
    "respect",
    "goes",
    "things",
    "models",
    "deep",
    "incredibly",
    "parallel",
    "local",
    "driven",
    "also",
    "trial",
    "cool",
    "downstream",
    "stacking",
    "problems",
    "pretty",
    "series",
    "see",
    "next",
    "textbook",
    "want",
    "either",
    "flow",
    "good",
    "small",
    "idea",
    "invariant",
    "took",
    "face",
    "x",
    "put",
    "enough",
    "environments",
    "convolution",
    "systems",
    "famous",
    "nonlinear",
    "understand",
    "late",
    "connected",
    "computations",
    "loops",
    "tms",
    "way",
    "get",
    "design",
    "back",
    "image",
    "generalized",
    "ls",
    "important",
    "analyze",
    "thank",
    "mean",
    "much",
    "dozens",
    "feedback",
    "increasingly",
    "people",
    "bottleneck",
    "network",
    "head",
    "designing",
    "decompression",
    "easy",
    "dimensional",
    "try",
    "smiley",
    "year",
    "zoo",
    "extraction",
    "component",
    "another",
    "signal",
    "recognition",
    "could",
    "ones",
    "learn",
    "speech",
    "latent",
    "output",
    "keep",
    "figure",
    "degrees",
    "train",
    "study",
    "since",
    "able",
    "short",
    "actually",
    "across",
    "better",
    "new",
    "called",
    "proposed",
    "audio",
    "massively",
    "followed",
    "linearly",
    "functional",
    "expressive",
    "networks",
    "welcome",
    "use",
    "know",
    "function",
    "basic",
    "con",
    "matter",
    "becoming",
    "processing",
    "z",
    "complexity",
    "okay",
    "input",
    "constant",
    "convolutional",
    "inspired",
    "activation",
    "interesting",
    "whole",
    "complicated",
    "principal",
    "artificial",
    "around",
    "analysis",
    "source",
    "used",
    "last",
    "like",
    "high",
    "acoustic",
    "sigmoidal",
    "common",
    "work",
    "would",
    "explosion",
    "tons",
    "images",
    "nathan",
    "term",
    "variety",
    "every",
    "long",
    "0",
    "means",
    "translation",
    "cuts",
    "cn",
    "node",
    "mathematically",
    "compress",
    "allow",
    "dynamical",
    "massive",
    "others",
    "pull",
    "meaning",
    "colleague",
    "architectures",
    "going",
    "color",
    "imagine",
    "basically",
    "implementing",
    "showing",
    "compressed",
    "layers",
    "taking",
    "might",
    "maybe",
    "solve",
    "trying",
    "feeding",
    "start",
    "building",
    "large",
    "picture",
    "engineering",
    "adding",
    "regardless",
    "hyperbolic",
    "getting",
    "left",
    "facebook",
    "kind",
    "different",
    "often",
    "showed",
    "exists",
    "expanded",
    "pi",
    "little",
    "information",
    "bit",
    "torch",
    "living",
    "ns",
    "smooth",
    "useful",
    "decoder"
  ]
}