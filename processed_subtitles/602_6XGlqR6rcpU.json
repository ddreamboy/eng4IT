{
  "text": "[Music]\nhey everyone welcome back today we'll be\ntalking about dimensionality reduction\npretty quick video\nso let's just start with what is\ndimensionality reduction so let's say\nyou have some training data\nwhich is n rows by p columns so you have\nn\nobservations and p features p predictors\nthe goal of dimensionality reduction is\nexactly what it sounds like\nis to get the number of dimensions\nnumber of features number of predictors\nnumber of columns down\nsignificantly so we're basically trying\nto go from n by p\nmatrix of training data to some n by k\nmatrix of training data where k\nis supposed to be much smaller than p\nwhy would we want to do this so a couple\nreasons that come up are storage\nobviously if you're going in the realm\nof big data\nbig data takes a lot of storage on\nwhatever machine you're running it on so\nit would be nice\nif we could compact this data into a\nsmaller space\nand hopefully not lose too much\ninformation along the way\nanother consideration is the model\ntraining time so obviously if you have a\nlarger amount of data\nyour model might take a lot longer to\ntrain because it might have to consider\nmany many features\nif you have less features then your\nmodel will probably take less time to\ntrain\nand finally maybe a lesser one but also\none that's often overlooked is\ninterpretability\nthis is not going to be true in all the\ndimensionality reduction techniques as\nwe'll see one of the techniques does\nhave this and one does not but\nunder certain situations this actually\nhelps you isolate\nsome of the features or columns that you\ncare the most about\nand you can throw away the ones that are\nnot very important so it could also be\nhelpful for that but it's very very\nimportant to note in this video\nthat when you do dimensionality\nreduction however you do it\nyou're always going to be losing some\ndata and this is just intuitive i don't\nthink it requires a formal proof you\ncould do a formal proof but basically if\nyou're going from\nthis much stuff and you're trying to\npress it into this much space\nyou literally just have less numbers and\nso you can't capture\nall of the original dynamics in the\noriginal data set\nand so the real goal of dimensionality\nreduction is not purely just\ngiving me less data it's to compact my\ndata but lose\nas little of the valuable information as\npossible so keep as much of the\nimportant stuff\nand throw away the stuff that's\nirrelevant and so let's talk about two\nmethods to do dimensionality reduction\nthere are many out there but i want to\ngo over two intro methods that you'll\nprobably learn early on\nduring your studies in machine learning\nso the first one here is called pca or\nprincipal components analysis now this\nis not a video where i'm going to go\ninto all the depth of pca i have videos\non that which i'll link in the\ndescription below\nbut just to remind you here's a very\ncommon picture you see during pca\nyou might have some two-dimensional data\nx1 x2 coordinates and all these green\nx's are your data points\nyou can see they more or less line up\nalong this red line\nand so we project them into a lower\ndimensional space in this case we're\nprojecting\nour two-dimensional data into a\none-dimensional linear space\nso you notice when you project these\ngreen x's onto this red line\nyou're still going to be losing some\ndata because the projections are going\nto be basically smooshing them into this\none-dimensional space but because they\nalready line up with the line pretty\nwell you're losing a very small amount\nof data\nand so that's what's going on here\nyou're projecting your data into a\nk-dimensional space where k is less than\nthe original number of dimensions\nnow the pros of this is it's efficient\nso if you go through the entire process\nof pca\nit's basically built to compress your\ndata in such a way that you're going to\nbe retaining as much of the variation\nin the original data set as possible\nthat's the entire goal of pca\nand so it's no surprise that it's doing\nthat in an efficient way\nbut by being very efficient you actually\nlose a lot in terms of interpretability\nfor example to go a little bit deep into\npca for a second you take the covariance\nmatrix of your data you find the\neigenvectors of that you project your\ndata onto the eigenvectors of your\ncovariance matrix\nobviously kind of a complex process and\nso you lose a little bit of what's\nactually going on what does my transform\ndata represent anymore because you're\nkind of\nnow having to think about your transform\ndata as coordinates in this new\ndimension\nthis new basis basically new coordinate\nspace and so it's a little bit tricky\nit's not impossible but it's a little\ntricky to\nmap that back to your original features\nso it's important to note these pros and\ncons\nnow the other major method i've seen in\nintro machine learning courses to do\ndimensionality reduction is lasso so\nwe've probably seen this formulation\nmany many times but this is just a\nregularization\nof ordinary least squares and we\ntypically have this in two flavors we\nhave lasso and ridge\nand today is the lasso formulation so\nwe're taking the true labels minus the\npredicted labels we're taking the\nl2 normal bat and then this extra term\nwe tack on is for regularization and\nwhen we have l1 norm\nas we do there this is called lasso\nregularization\nand the key feature of lasso\nregularization as we know\nis that it's going to send many of your\nbeta i's to exactly zero\nand so we can actually use this for\nfeature selection because we can run\nthis\nso let's say we fix some kind of lambda\nwhich is a parameter\nand then we can just pick the variables\nwhose beta i's are not sent to exactly\nzero\nand in some sense the model considers\nthese as important or predictive\nof this target variable y so\nthe pros here are that this is\ninterpretable notice we're not\nprojecting our data into any dimensional\nspace we're simply just eliminating\ncolumns that are not helpful in\npredicting this target variable y\nand so at the end of the day we have a\nsmaller set of columns but the key\nobservation is that those columns are\ncoming from the original data set itself\nwe're not transforming the features so\nyou can just say that oh this column was\nimportant this column got thrown away\neasy to interpret and the other kind of\nbonus is that it takes the response into\naccount\nnotice that y is explicitly taken into\naccount here which is not actually true\nfor pca and of course the con here is\nthat it's not going to be as efficient\nas pca because\nagain pca was designed to compress your\ndata in the most efficient way possible\nthis is simply just marking columns for\ndeletion so it's not going to be as\nefficient in compressing your data\nso i hope this very quick kind of run\nthrough of dimensionality reduction\ntechniques was helpful if it was please\nlike and subscribe for more videos just\nlike this\ni'll catch you next time\n",
  "words": [
    "music",
    "hey",
    "everyone",
    "welcome",
    "back",
    "today",
    "talking",
    "dimensionality",
    "reduction",
    "pretty",
    "quick",
    "video",
    "let",
    "start",
    "dimensionality",
    "reduction",
    "let",
    "say",
    "training",
    "data",
    "n",
    "rows",
    "p",
    "columns",
    "n",
    "observations",
    "p",
    "features",
    "p",
    "predictors",
    "goal",
    "dimensionality",
    "reduction",
    "exactly",
    "sounds",
    "like",
    "get",
    "number",
    "dimensions",
    "number",
    "features",
    "number",
    "predictors",
    "number",
    "columns",
    "significantly",
    "basically",
    "trying",
    "go",
    "n",
    "p",
    "matrix",
    "training",
    "data",
    "n",
    "k",
    "matrix",
    "training",
    "data",
    "k",
    "supposed",
    "much",
    "smaller",
    "p",
    "would",
    "want",
    "couple",
    "reasons",
    "come",
    "storage",
    "obviously",
    "going",
    "realm",
    "big",
    "data",
    "big",
    "data",
    "takes",
    "lot",
    "storage",
    "whatever",
    "machine",
    "running",
    "would",
    "nice",
    "could",
    "compact",
    "data",
    "smaller",
    "space",
    "hopefully",
    "lose",
    "much",
    "information",
    "along",
    "way",
    "another",
    "consideration",
    "model",
    "training",
    "time",
    "obviously",
    "larger",
    "amount",
    "data",
    "model",
    "might",
    "take",
    "lot",
    "longer",
    "train",
    "might",
    "consider",
    "many",
    "many",
    "features",
    "less",
    "features",
    "model",
    "probably",
    "take",
    "less",
    "time",
    "train",
    "finally",
    "maybe",
    "lesser",
    "one",
    "also",
    "one",
    "often",
    "overlooked",
    "interpretability",
    "going",
    "true",
    "dimensionality",
    "reduction",
    "techniques",
    "see",
    "one",
    "techniques",
    "one",
    "certain",
    "situations",
    "actually",
    "helps",
    "isolate",
    "features",
    "columns",
    "care",
    "throw",
    "away",
    "ones",
    "important",
    "could",
    "also",
    "helpful",
    "important",
    "note",
    "video",
    "dimensionality",
    "reduction",
    "however",
    "always",
    "going",
    "losing",
    "data",
    "intuitive",
    "think",
    "requires",
    "formal",
    "proof",
    "could",
    "formal",
    "proof",
    "basically",
    "going",
    "much",
    "stuff",
    "trying",
    "press",
    "much",
    "space",
    "literally",
    "less",
    "numbers",
    "ca",
    "capture",
    "original",
    "dynamics",
    "original",
    "data",
    "set",
    "real",
    "goal",
    "dimensionality",
    "reduction",
    "purely",
    "giving",
    "less",
    "data",
    "compact",
    "data",
    "lose",
    "little",
    "valuable",
    "information",
    "possible",
    "keep",
    "much",
    "important",
    "stuff",
    "throw",
    "away",
    "stuff",
    "irrelevant",
    "let",
    "talk",
    "two",
    "methods",
    "dimensionality",
    "reduction",
    "many",
    "want",
    "go",
    "two",
    "intro",
    "methods",
    "probably",
    "learn",
    "early",
    "studies",
    "machine",
    "learning",
    "first",
    "one",
    "called",
    "pca",
    "principal",
    "components",
    "analysis",
    "video",
    "going",
    "go",
    "depth",
    "pca",
    "videos",
    "link",
    "description",
    "remind",
    "common",
    "picture",
    "see",
    "pca",
    "might",
    "data",
    "x1",
    "x2",
    "coordinates",
    "green",
    "x",
    "data",
    "points",
    "see",
    "less",
    "line",
    "along",
    "red",
    "line",
    "project",
    "lower",
    "dimensional",
    "space",
    "case",
    "projecting",
    "data",
    "linear",
    "space",
    "notice",
    "project",
    "green",
    "x",
    "onto",
    "red",
    "line",
    "still",
    "going",
    "losing",
    "data",
    "projections",
    "going",
    "basically",
    "smooshing",
    "space",
    "already",
    "line",
    "line",
    "pretty",
    "well",
    "losing",
    "small",
    "amount",
    "data",
    "going",
    "projecting",
    "data",
    "space",
    "k",
    "less",
    "original",
    "number",
    "dimensions",
    "pros",
    "efficient",
    "go",
    "entire",
    "process",
    "pca",
    "basically",
    "built",
    "compress",
    "data",
    "way",
    "going",
    "retaining",
    "much",
    "variation",
    "original",
    "data",
    "set",
    "possible",
    "entire",
    "goal",
    "pca",
    "surprise",
    "efficient",
    "way",
    "efficient",
    "actually",
    "lose",
    "lot",
    "terms",
    "interpretability",
    "example",
    "go",
    "little",
    "bit",
    "deep",
    "pca",
    "second",
    "take",
    "covariance",
    "matrix",
    "data",
    "find",
    "eigenvectors",
    "project",
    "data",
    "onto",
    "eigenvectors",
    "covariance",
    "matrix",
    "obviously",
    "kind",
    "complex",
    "process",
    "lose",
    "little",
    "bit",
    "actually",
    "going",
    "transform",
    "data",
    "represent",
    "anymore",
    "kind",
    "think",
    "transform",
    "data",
    "coordinates",
    "new",
    "dimension",
    "new",
    "basis",
    "basically",
    "new",
    "coordinate",
    "space",
    "little",
    "bit",
    "tricky",
    "impossible",
    "little",
    "tricky",
    "map",
    "back",
    "original",
    "features",
    "important",
    "note",
    "pros",
    "cons",
    "major",
    "method",
    "seen",
    "intro",
    "machine",
    "learning",
    "courses",
    "dimensionality",
    "reduction",
    "lasso",
    "probably",
    "seen",
    "formulation",
    "many",
    "many",
    "times",
    "regularization",
    "ordinary",
    "least",
    "squares",
    "typically",
    "two",
    "flavors",
    "lasso",
    "ridge",
    "today",
    "lasso",
    "formulation",
    "taking",
    "true",
    "labels",
    "minus",
    "predicted",
    "labels",
    "taking",
    "l2",
    "normal",
    "bat",
    "extra",
    "term",
    "tack",
    "regularization",
    "l1",
    "norm",
    "called",
    "lasso",
    "regularization",
    "key",
    "feature",
    "lasso",
    "regularization",
    "know",
    "going",
    "send",
    "many",
    "beta",
    "exactly",
    "zero",
    "actually",
    "use",
    "feature",
    "selection",
    "run",
    "let",
    "say",
    "fix",
    "kind",
    "lambda",
    "parameter",
    "pick",
    "variables",
    "whose",
    "beta",
    "sent",
    "exactly",
    "zero",
    "sense",
    "model",
    "considers",
    "important",
    "predictive",
    "target",
    "variable",
    "pros",
    "interpretable",
    "notice",
    "projecting",
    "data",
    "dimensional",
    "space",
    "simply",
    "eliminating",
    "columns",
    "helpful",
    "predicting",
    "target",
    "variable",
    "end",
    "day",
    "smaller",
    "set",
    "columns",
    "key",
    "observation",
    "columns",
    "coming",
    "original",
    "data",
    "set",
    "transforming",
    "features",
    "say",
    "oh",
    "column",
    "important",
    "column",
    "got",
    "thrown",
    "away",
    "easy",
    "interpret",
    "kind",
    "bonus",
    "takes",
    "response",
    "account",
    "notice",
    "explicitly",
    "taken",
    "account",
    "actually",
    "true",
    "pca",
    "course",
    "con",
    "going",
    "efficient",
    "pca",
    "pca",
    "designed",
    "compress",
    "data",
    "efficient",
    "way",
    "possible",
    "simply",
    "marking",
    "columns",
    "deletion",
    "going",
    "efficient",
    "compressing",
    "data",
    "hope",
    "quick",
    "kind",
    "run",
    "dimensionality",
    "reduction",
    "techniques",
    "helpful",
    "please",
    "like",
    "subscribe",
    "videos",
    "like",
    "catch",
    "next",
    "time"
  ],
  "keywords": [
    "dimensionality",
    "reduction",
    "video",
    "let",
    "say",
    "training",
    "data",
    "n",
    "p",
    "columns",
    "features",
    "goal",
    "exactly",
    "like",
    "number",
    "basically",
    "go",
    "matrix",
    "k",
    "much",
    "smaller",
    "obviously",
    "going",
    "lot",
    "machine",
    "could",
    "space",
    "lose",
    "way",
    "model",
    "time",
    "might",
    "take",
    "many",
    "less",
    "probably",
    "one",
    "true",
    "techniques",
    "see",
    "actually",
    "away",
    "important",
    "helpful",
    "losing",
    "stuff",
    "original",
    "set",
    "little",
    "possible",
    "two",
    "pca",
    "line",
    "project",
    "projecting",
    "notice",
    "pros",
    "efficient",
    "bit",
    "kind",
    "new",
    "lasso",
    "regularization"
  ]
}