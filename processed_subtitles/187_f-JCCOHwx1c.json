{
  "text": "hello on my name is Krishna and welcome\nto my youtube channel so I guys I think\nyou like this new setup and probably\nI'll be recording all my videos like\nthis and upcoming videos itself and in\nthis particular video we are going to\nimplement the sequence to sequence\nlearning with the help of machine\nlearning language translation and I'll\nbe showing you how we can take up the\ndata set how we can do all the things as\nwe go ahead and for this please remember\nguys you need to be very very good at\nthe basics that is you need to know RN n\nyou need to know unless key M RN n you\nneed to know bi-directional RNA and this\nparticular video is more to show you or\nmake you understand how a code is\nactually implemented to do language\ntranslation so I am going to refer this\nparticular block guys amazing blog it\nhas been written and it is provided to\neveryone you can just read and\nunderstand this whole thing if you have\na basics right ok and again as I said\nthat we are doing machine learning the\nlanguage translation so far first of all\nfrom where we are getting the data set\njust click on this ok in this particular\nscenario again this is blog you know\nthey have actually done English to\nFrench translation so English to French\ndata set is already present over here\nwhy they have actually taken that\nbecause then there is a huge data set\nfor this you know they are more than 100\nK records with respect to different\ndifferent English and French words so we\nare going to use this and probably I'll\nalso run this whole thing and show it to\nyou now let me just go to the\narchitecture make you understand what\nare things we have to actually do for\nthat I'm just going to quickly rub this\nyou know so wrap this all so that I'll\nbe able to able to explain you\neverything now fine let's go ahead and\ntry to understand this remember guys if\nyou have seen my encoders and decoders\nthe video or sequence to sequence\nlearning their two main important\ncomponents one is the encoder so this is\nbasically my encoder ok the another one\nis basically called a decoder okay now\nwe have seen that in encoder we just\nhave to provide our words\nthe processing will actually happen\nthere will be a context vector that will\nbe W that will be created once this\ncontext vector is actually created then\nit is passed to my decoder layer and\nwith the help of that we are actually\ncreating our output one important thing\nto load over here is that in our\nencoders we do not consider the output\nover here\nwe skip those output we don't require\nthis output right because we are\nactually more focused on getting the\ncontext vector okay so what we are going\nto do is that I am going to make you\nunderstand how that code is actually\nimplemented if you just copy and paste\nthat code it will work but we still need\nto understand how we can design this\nencoder to decoders right now one more\nthing that you need to consider over\nhere is that this as you know that we\nare passing character like a B C right\nyou are passing all this particular\ncharacters and this is specifically\ncharacter to character translation here\nnow when I pass this particular\ncharacter we also need to take care that\nhow it is actually passed it should be\npassed in the form of vectors and in\nthis particular example we are going to\nconvert that into or actually send these\ncharacters in the terms of 100\npresentation one art representation\nbasically means one suppose you have 73\ncharacters\nokay and in English specifically in this\nparticular example in English there are\n73 unique characters now if I want to\nrepresent a with some vector\nrepresentation I will basically have 73\nfeatures and based on that I suppose\nthat a wherever it at whichever location\nit is present I'll keep it as one\nremaining all the characters will be 0\nthat is how is basically represented so\nsimilarly we are going to do in this\nparticular way and remember guys please\nmake sure that you have your basic\nconcepts of LS T M R and N and you know\nhow to implement all those things so it\nwill be pretty much important that will\nbe also easy for me for me to make you\nunderstand ok now what we are going to\ndo first of all okay remember we are\ngoing to make sentences into 3 lumpiness\none is the character input encoder input\ndata decoder input data and decoder\ntarget data now what does this basically\nmean that is we obviously need to\nprovide this input data and as you know\nthat in L esteem we need to provide the\ninput it has to be provided in the three\nright so this is basically my decoder\ninput right decoder input and this is\nall my decoded target data decoder input\nis basically nothing but the vectors or\nthe context patterns that is been\ngenerated by the encoders so we are\ngoing to use that okay then we are going\nto the next step over here you can see\nthat they have given on what 3d array of\nshape basically number of tails is like\nhow many number of sentences that I have\nwhat is the maximum sentence length and\nwhat is the number of English characters\nso if you really want to give in the\nform of one order presentation we\nbasically have to consider this this is\nbasically that the total number of\ncharacters that is actually present\nbecause we are going to provide this\ninto a one heart of acceleration of the\nEnglish sentence so let me just go back\nand this is how I did a set looks like\nthat is you can see this is my English\nword this is my French word this is my\nEnglish word this is my French word this\nis my English word this is my furniture\nso like this you have huge amount of\ndata set we need to remove this this\nthis is not required so for that we will\nbe doing some kind of processing Oh\ncurrently in my system guys we I have\ntensorflow 2.0 so whenever you have\ntensorflow 2.0 that basically is chaos\nand integrated within them\nif you don't have tensorflow greater\nthan 2.0 at that time you just have to\nremove this particular code that is\ntensorflow from tensorflow just remove\nit and you can just write from chaos\ndrop models import model from Cara's not\nlayer input input a list intensity now\nwe are going to initialize some of the\nvariables like bath size we are going to\ninitialize epochs we are going to\ninitialize some latent dimension this is\nbasically the feature dimensions or\ndimension it is case that we are going\nto consider how the number of samples\nshould really not be able to take it as\nten thousand okay here we are just\nspecifying some values itself so that we\nwill be training on that bunch of data\nnow the first step after this here I\nhave actually provided my dataset path\nin this particular step what we are\ndoing is that we are reading the input\nand the target data input data is\nbasically all my English sentence target\ndata is basically all my output sentence\nand then with respect to each and every\ninput sentence we are we are we are\nreading all the characters all the\nunique\ncharacters that is what we are doing so\nhere you can see there is something like\ninput characters and target characters\nso input target input characters and\ntarget characters will give you all the\nEnglish sentence so suck up all the\nEnglish words so if I go and rent out\ninput characters so these are all my\nEnglish words you can see over here okay\nguys I think this is a simple for loop I\nI think you will be able to understand\nokay so input characters and target\ncharacters before input and target\ncharacters you can see that first of all\nyou have opened that particular file\nthen you are traversing to each and\nevery lines you are doing the split with\nrespect to data input text and target\ntext input text if you want to really\nsee this input text so here is all my\nEnglish input text you can see over here\nsimilarly if you want to go and see the\ntarget text you'll be able to see all\nthe targets test and this remember the\n/n is put because we need to indicate\nthat which is the end of that specific\nsentence because then only we'll be able\nto understand that how we would be\ntraining our neural networks how we will\nbe training our ell STM lens you know\nhow we will come to know that okay this\nis the end of the sentence with respect\nto that so this target text input text\nwe have got and similarly we have got\ninput characters and concrete characters\ntarget characters if you see guys this\nbasically indicates that all the French\ncharacters that I have in my dataset\nokay so target characters you can see\nthese are all the French characters that\nare present okay and if you really want\nto see the length it will be probably\nsomewhere around I guess 90 I just seen\nthis okay 90 okay guys so next step is\nactually to take all this particular\ninput characters target characters and\nthis included tokens like how many the\nlength of the input characters you can\nsee we are printing it you know I'm\nactually finding out what is the maximum\nsequence length and what is the decoder\nsequences all these rings are there so\nwe have the number of samples basically\nsay that how many number of sentences I\nhave number of unique input tokens\nbasically indicates that how many unique\ncharacters I have in my English language\nover there based on the data set that I\nhave\n93\nbasically the target characters the\nlength of the target characters what is\nthe maximum length for input centered so\nbasically I have all my English\nsentences from that the maximum length\nof the English sentence is somewhere\naround 16 words\nsimilarly the maximum sequence length\nfor the output which is basically my\nFrench the words is basically 59 okay\nnow the next step I'm assigning each and\nevery token so token basically means\nlike 0 1 2 3 those kind of indexes to\neach and every characters based on the\nsorted list and this is what that\nspecific code will do okay so if I\nexecute this and if I show you what is\nthe input token index so input token\nindex what it will do how many English\ncharacters are present based on that in\nthe sorted order it will start putting\nup indexes so here you can see that for\nthe space it has put 0 for this it has\nput 1 for dollar it has put 2 3 4 like\nthis so total number of 70s and last\ncharacter it is given as 17 decks or\ntotal number of characters over here is\nbasically 71 similarly if I go and see\nfor the target target token index you\nwill be able to see that yes I'm having\nsomewhere around 92 okay now the next\nstep is basically to show you how we can\ndo one Hart representation by using\nnumba initially for the encoder input\ndata I am going to use NP dot 0 so I am\ngoing to put all zeroes values based on\nthe dimensions that I have given over\nhere so remember the first dimension\nagain if I go to go in this particular\nblock the first dimension it says I have\nto put number of pairs number of times\nbasically means how many total number of\nsentences are there I am going to put\nthat the second parameter is basically\nwhat is the maximum English seat\nsentence length that also I know that\nright then what is the number of English\ncharacters that also I know that right\nso here I am going to put as Mac Mac's\ninclude a sequence length the total\nlength of the input sentences and total\nnumber of encoded tokens so that will\nactually give me like how many number of\nEnglish sentence words are actually\npresent right so here is those values\nthis is basically my sequence length so\nyou know you can see that my sequence\nlength max equals length for English is\nsomewhere on 16\nthe unique input tokens that is 71 that\nis why abused number of input tokens\nover here right similarly for the\ndecoder input also have to do in that\nparticular way and for the target data\nalso I have to do in that particular way\nalways remember this will be same in\ndecoder input data instead of using\nEnglish I will be using French over here\nso max decoder sequence length max\nnumber of decoder tokens everything over\nhere will be basically taken as the\nFrench directors so what will be the\nFrench character length if I go and see\nover here right so this is the decoder\nsequence length this is basically of the\nFrench that we are trying to see it is\nsomewhere around 59\nright how many number of unique tokens\nare actually present in the French\ncharacters they are 93 so I am going to\nuse that similarly in this decoder\ntarget it also I am going to use the\ninformation of the decoder or the French\ncharacter itself\nnow this particular code will actually\nhelp us to do the whatnot representation\nit is simple gas suppose I have the\nwords let me just quickly show show it\nin front of you okay\nsuppose I am just going to wrap this\nokay\nsuppose suppose I have some words okay I\nhave some words like the blue o D right\nsuppose I just have these four words\nnow suppose are in my first sentence I\nhave my blue word so W will be given the\nvalue as 1 and remaining all will be\ngiven as the value is 0 this is what is\none hot representation basically says\nand that is what I am going to do over\nhere right but remember you're the\ntarget length is completely different\nfrom English it is somewhere around or\nhow much we saw if I go and see in the\ntop for English it is somewhere around\n71 unique tokens are there but the\nsentence length is somewhere around 16\nright\nso considering the 16 length and\nconsidering the 71 unique tokens\nwherever that particular token is\npresent that will become 1 and remaining\nall will become 0 so that is what this\nwhole code is doing if that is what this\nwhole code is doing actually you have to\nunderstand this particular code guys\njust try to free that you know so here\nyou can see for T comma characters in\nenumerate input underscore text what is\ninput and that's good text okay I'll\njust show you what is input underscore\nnext you can see that okay first of all\nit is traversing inside this in so input\nunderscore text is one one sentences\nlike this okay one one syntax is like\nthis now when I see this this is\nbasically replacing wherever that\nparticular character is there that is\ngetting replaced by one okay guys I'll\nnot deep dive more into the code just\ntry to understand this okay just try to\ngo line by line and try to understand it\nit will be always beneficial to you okay\nso that is how in and but remember this\nparticular whole code is actually being\nthe one hut representation okay so after\nyou do this what we are going to do is\nthat then we will start creating our\nlsdm layer now lsdm layer initially will\nask for the encoder input shape so the\nencoder input shape I am giving it as\nnumber of included tokens then in lsdm\nare taken what is my little late and I\nmention I had initialized over here the\nleading dimension is 256 this is just\nlike our timestamps that we are going to\nconsider and then we are going to take a\nreturn state as true one very very\nimportant thing is happening over here\njust understand if I go to my WordPad\nremember I am NOT taking all these\nparticular outputs okay I I don't want\nthis output because that is not how\nencoder and decoder actually work\nright this is not how it actually works\nso what we I am doing actually me see\nyou me so first of all I will go and\ninitialize this I have initialized my\nHTM layer now this encoder when it is\ntaking the encoder input it gives us\nthree values one is the encoder output\nwhich we have to skip we don't require\nit what is the state underscore H which\nis basically Magadan sense and one is\nthis cell state okay now if I go back\nover here guys I will be requiring the\nstudents and information whatever the\noutput I am getting it over here with\nrespect to the complete time stamps as I\nam going hi okay so I don't what I don't\nwant this out\nI don't require any of this output so\nwhat I am doing over here you can see\nthat I am just taking this state H and\nstate C and I am putting inside my\nencoder state variable okay encoder\nstate beautiful now similarly what I do\nwith respect to the decoder inputs now\nin decoder in course I know what is my\ndecoder number of decoded tokens I\nabsolutely know that again I have\ncreated a latent dimension created all\nthe lists iam like that okay but here\nnow we will be much more concerned in\ngetting the output itself I don't have\nto worry about the other things okay so\nnow here I am focusing on getting the\ndecoder outputs okay now once I get the\ndecoder outputs the decoder output\nbasically means this information this\ninformation I want to get this\ninformation and this is the layer of\nthat I am actually creating in my second\nstep right in the first layer I took\nthis okay\nI want it this value this value and this\noutput value I have taken that I have\nstored it in a variable and now from the\ndecoder listing I am just interested in\nthe decoder output right then I'm using\na dense layer and finally I am actually\ngetting all the outputs with respect to\nthat okay now this is what is with\nrespect to my encoder inputs and my\ndecoder inputs and my decoder outputs\nnow I will be taking all this particular\nvalue and considering in creating our\nmodel so if I give this values inside my\nlist you can see my encoder inputs and\ndecoder inputs this both will be almost\nsame but just with timestamp the +1 and\nthen finally I am getting my decoder\nafter that I am compiling I am fitting\nthe data with some validation split with\nsome hundred each box you can see that\nand probably this will be able to give\nyou around now\n87 percent accuracy remember guys this\nwill also work in your local laptop and\nit will work in proper like within 15\nminutes it will be able to do the\nhundred imposter since this is a\ncharacter to character implementation\nokay now this was how my model got\ntrained now one assignment I really want\nto give it to you guys is that please\ntry to understand this code you know\nwhere I am actually generating the\nsentences okay you can see that this is\nbasically generating the sentence\nso just try to understand this and\nprobably I know it will be little bit\ndifficult but I want you to read the\nblocks and understand this how we are\ndoing the sampling and how we are\ngenerating okay\nI have explained you till here like how\nthe model is actually generated this is\npretty much simple refer the blocks in\nthe reason why I am Telling You is that\nunderstand and probably if you don't\nknow about lsdm don't go over here first\nof all then only you will not be able to\nunderstand because in the next model\nwhich I am going to discuss which is\nabout attention models there again we\nneed to make some architecture changes\nyou know instead of using this encoder\nover here right I will be using this\nwill be getting changed to\nbi-directional LSD okay so yes guys this\nwas all about this particular video used\nto let me know whether you have any\nqueries but I will be very very happy to\nhelp you out but please give a try give\na try to understand this code which is\nactually changing your ten sentences or\nwhich is running your course this is\nwhat I want you want to explore this is\nhardly around 15 to 20 lines of code but\nI really want to make make you\nunderstand a thing if you have\nunderstood this the other part will be\npretty much easy so yes guys this is all\nabout this particular video in my next\nvideo deep learning will be coming with\nattention models and we'll discuss about\nattention mall as or how it actually was\nso yes guys this was all about this\nparticularly de sel in the next video I\nhave a great day thank you wonder bye\n",
  "words": [
    "hello",
    "name",
    "krishna",
    "welcome",
    "youtube",
    "channel",
    "guys",
    "think",
    "like",
    "new",
    "setup",
    "probably",
    "recording",
    "videos",
    "like",
    "upcoming",
    "videos",
    "particular",
    "video",
    "going",
    "implement",
    "sequence",
    "sequence",
    "learning",
    "help",
    "machine",
    "learning",
    "language",
    "translation",
    "showing",
    "take",
    "data",
    "set",
    "things",
    "go",
    "ahead",
    "please",
    "remember",
    "guys",
    "need",
    "good",
    "basics",
    "need",
    "know",
    "rn",
    "n",
    "need",
    "know",
    "unless",
    "key",
    "rn",
    "n",
    "need",
    "know",
    "rna",
    "particular",
    "video",
    "show",
    "make",
    "understand",
    "code",
    "actually",
    "implemented",
    "language",
    "translation",
    "going",
    "refer",
    "particular",
    "block",
    "guys",
    "amazing",
    "blog",
    "written",
    "provided",
    "everyone",
    "read",
    "understand",
    "whole",
    "thing",
    "basics",
    "right",
    "ok",
    "said",
    "machine",
    "learning",
    "language",
    "translation",
    "far",
    "first",
    "getting",
    "data",
    "set",
    "click",
    "ok",
    "particular",
    "scenario",
    "blog",
    "know",
    "actually",
    "done",
    "english",
    "french",
    "translation",
    "english",
    "french",
    "data",
    "set",
    "already",
    "present",
    "actually",
    "taken",
    "huge",
    "data",
    "set",
    "know",
    "100",
    "k",
    "records",
    "respect",
    "different",
    "different",
    "english",
    "french",
    "words",
    "going",
    "use",
    "probably",
    "also",
    "run",
    "whole",
    "thing",
    "show",
    "let",
    "go",
    "architecture",
    "make",
    "understand",
    "things",
    "actually",
    "going",
    "quickly",
    "rub",
    "know",
    "wrap",
    "able",
    "able",
    "explain",
    "everything",
    "fine",
    "let",
    "go",
    "ahead",
    "try",
    "understand",
    "remember",
    "guys",
    "seen",
    "encoders",
    "decoders",
    "video",
    "sequence",
    "sequence",
    "learning",
    "two",
    "main",
    "important",
    "components",
    "one",
    "encoder",
    "basically",
    "encoder",
    "ok",
    "another",
    "one",
    "basically",
    "called",
    "decoder",
    "okay",
    "seen",
    "encoder",
    "provide",
    "words",
    "processing",
    "actually",
    "happen",
    "context",
    "vector",
    "w",
    "created",
    "context",
    "vector",
    "actually",
    "created",
    "passed",
    "decoder",
    "layer",
    "help",
    "actually",
    "creating",
    "output",
    "one",
    "important",
    "thing",
    "load",
    "encoders",
    "consider",
    "output",
    "skip",
    "output",
    "require",
    "output",
    "right",
    "actually",
    "focused",
    "getting",
    "context",
    "vector",
    "okay",
    "going",
    "going",
    "make",
    "understand",
    "code",
    "actually",
    "implemented",
    "copy",
    "paste",
    "code",
    "work",
    "still",
    "need",
    "understand",
    "design",
    "encoder",
    "decoders",
    "right",
    "one",
    "thing",
    "need",
    "consider",
    "know",
    "passing",
    "character",
    "like",
    "b",
    "c",
    "right",
    "passing",
    "particular",
    "characters",
    "specifically",
    "character",
    "character",
    "translation",
    "pass",
    "particular",
    "character",
    "also",
    "need",
    "take",
    "care",
    "actually",
    "passed",
    "passed",
    "form",
    "vectors",
    "particular",
    "example",
    "going",
    "convert",
    "actually",
    "send",
    "characters",
    "terms",
    "100",
    "presentation",
    "one",
    "art",
    "representation",
    "basically",
    "means",
    "one",
    "suppose",
    "73",
    "characters",
    "okay",
    "english",
    "specifically",
    "particular",
    "example",
    "english",
    "73",
    "unique",
    "characters",
    "want",
    "represent",
    "vector",
    "representation",
    "basically",
    "73",
    "features",
    "based",
    "suppose",
    "wherever",
    "whichever",
    "location",
    "present",
    "keep",
    "one",
    "remaining",
    "characters",
    "0",
    "basically",
    "represented",
    "similarly",
    "going",
    "particular",
    "way",
    "remember",
    "guys",
    "please",
    "make",
    "sure",
    "basic",
    "concepts",
    "ls",
    "r",
    "n",
    "know",
    "implement",
    "things",
    "pretty",
    "much",
    "important",
    "also",
    "easy",
    "make",
    "understand",
    "ok",
    "going",
    "first",
    "okay",
    "remember",
    "going",
    "make",
    "sentences",
    "3",
    "lumpiness",
    "one",
    "character",
    "input",
    "encoder",
    "input",
    "data",
    "decoder",
    "input",
    "data",
    "decoder",
    "target",
    "data",
    "basically",
    "mean",
    "obviously",
    "need",
    "provide",
    "input",
    "data",
    "know",
    "l",
    "esteem",
    "need",
    "provide",
    "input",
    "provided",
    "three",
    "right",
    "basically",
    "decoder",
    "input",
    "right",
    "decoder",
    "input",
    "decoded",
    "target",
    "data",
    "decoder",
    "input",
    "basically",
    "nothing",
    "vectors",
    "context",
    "patterns",
    "generated",
    "encoders",
    "going",
    "use",
    "okay",
    "going",
    "next",
    "step",
    "see",
    "given",
    "3d",
    "array",
    "shape",
    "basically",
    "number",
    "tails",
    "like",
    "many",
    "number",
    "sentences",
    "maximum",
    "sentence",
    "length",
    "number",
    "english",
    "characters",
    "really",
    "want",
    "give",
    "form",
    "one",
    "order",
    "presentation",
    "basically",
    "consider",
    "basically",
    "total",
    "number",
    "characters",
    "actually",
    "present",
    "going",
    "provide",
    "one",
    "heart",
    "acceleration",
    "english",
    "sentence",
    "let",
    "go",
    "back",
    "set",
    "looks",
    "like",
    "see",
    "english",
    "word",
    "french",
    "word",
    "english",
    "word",
    "french",
    "word",
    "english",
    "word",
    "furniture",
    "like",
    "huge",
    "amount",
    "data",
    "set",
    "need",
    "remove",
    "required",
    "kind",
    "processing",
    "oh",
    "currently",
    "system",
    "guys",
    "tensorflow",
    "whenever",
    "tensorflow",
    "basically",
    "chaos",
    "integrated",
    "within",
    "tensorflow",
    "greater",
    "time",
    "remove",
    "particular",
    "code",
    "tensorflow",
    "tensorflow",
    "remove",
    "write",
    "chaos",
    "drop",
    "models",
    "import",
    "model",
    "cara",
    "layer",
    "input",
    "input",
    "list",
    "intensity",
    "going",
    "initialize",
    "variables",
    "like",
    "bath",
    "size",
    "going",
    "initialize",
    "epochs",
    "going",
    "initialize",
    "latent",
    "dimension",
    "basically",
    "feature",
    "dimensions",
    "dimension",
    "case",
    "going",
    "consider",
    "number",
    "samples",
    "really",
    "able",
    "take",
    "ten",
    "thousand",
    "okay",
    "specifying",
    "values",
    "training",
    "bunch",
    "data",
    "first",
    "step",
    "actually",
    "provided",
    "dataset",
    "path",
    "particular",
    "step",
    "reading",
    "input",
    "target",
    "data",
    "input",
    "data",
    "basically",
    "english",
    "sentence",
    "target",
    "data",
    "basically",
    "output",
    "sentence",
    "respect",
    "every",
    "input",
    "sentence",
    "reading",
    "characters",
    "unique",
    "characters",
    "see",
    "something",
    "like",
    "input",
    "characters",
    "target",
    "characters",
    "input",
    "target",
    "input",
    "characters",
    "target",
    "characters",
    "give",
    "english",
    "sentence",
    "suck",
    "english",
    "words",
    "go",
    "rent",
    "input",
    "characters",
    "english",
    "words",
    "see",
    "okay",
    "guys",
    "think",
    "simple",
    "loop",
    "think",
    "able",
    "understand",
    "okay",
    "input",
    "characters",
    "target",
    "characters",
    "input",
    "target",
    "characters",
    "see",
    "first",
    "opened",
    "particular",
    "file",
    "traversing",
    "every",
    "lines",
    "split",
    "respect",
    "data",
    "input",
    "text",
    "target",
    "text",
    "input",
    "text",
    "want",
    "really",
    "see",
    "input",
    "text",
    "english",
    "input",
    "text",
    "see",
    "similarly",
    "want",
    "go",
    "see",
    "target",
    "text",
    "able",
    "see",
    "targets",
    "test",
    "remember",
    "put",
    "need",
    "indicate",
    "end",
    "specific",
    "sentence",
    "able",
    "understand",
    "would",
    "training",
    "neural",
    "networks",
    "training",
    "ell",
    "stm",
    "lens",
    "know",
    "come",
    "know",
    "okay",
    "end",
    "sentence",
    "respect",
    "target",
    "text",
    "input",
    "text",
    "got",
    "similarly",
    "got",
    "input",
    "characters",
    "concrete",
    "characters",
    "target",
    "characters",
    "see",
    "guys",
    "basically",
    "indicates",
    "french",
    "characters",
    "dataset",
    "okay",
    "target",
    "characters",
    "see",
    "french",
    "characters",
    "present",
    "okay",
    "really",
    "want",
    "see",
    "length",
    "probably",
    "somewhere",
    "around",
    "guess",
    "90",
    "seen",
    "okay",
    "90",
    "okay",
    "guys",
    "next",
    "step",
    "actually",
    "take",
    "particular",
    "input",
    "characters",
    "target",
    "characters",
    "included",
    "tokens",
    "like",
    "many",
    "length",
    "input",
    "characters",
    "see",
    "printing",
    "know",
    "actually",
    "finding",
    "maximum",
    "sequence",
    "length",
    "decoder",
    "sequences",
    "rings",
    "number",
    "samples",
    "basically",
    "say",
    "many",
    "number",
    "sentences",
    "number",
    "unique",
    "input",
    "tokens",
    "basically",
    "indicates",
    "many",
    "unique",
    "characters",
    "english",
    "language",
    "based",
    "data",
    "set",
    "93",
    "basically",
    "target",
    "characters",
    "length",
    "target",
    "characters",
    "maximum",
    "length",
    "input",
    "centered",
    "basically",
    "english",
    "sentences",
    "maximum",
    "length",
    "english",
    "sentence",
    "somewhere",
    "around",
    "16",
    "words",
    "similarly",
    "maximum",
    "sequence",
    "length",
    "output",
    "basically",
    "french",
    "words",
    "basically",
    "59",
    "okay",
    "next",
    "step",
    "assigning",
    "every",
    "token",
    "token",
    "basically",
    "means",
    "like",
    "0",
    "1",
    "2",
    "3",
    "kind",
    "indexes",
    "every",
    "characters",
    "based",
    "sorted",
    "list",
    "specific",
    "code",
    "okay",
    "execute",
    "show",
    "input",
    "token",
    "index",
    "input",
    "token",
    "index",
    "many",
    "english",
    "characters",
    "present",
    "based",
    "sorted",
    "order",
    "start",
    "putting",
    "indexes",
    "see",
    "space",
    "put",
    "0",
    "put",
    "1",
    "dollar",
    "put",
    "2",
    "3",
    "4",
    "like",
    "total",
    "number",
    "70s",
    "last",
    "character",
    "given",
    "17",
    "decks",
    "total",
    "number",
    "characters",
    "basically",
    "71",
    "similarly",
    "go",
    "see",
    "target",
    "target",
    "token",
    "index",
    "able",
    "see",
    "yes",
    "somewhere",
    "around",
    "92",
    "okay",
    "next",
    "step",
    "basically",
    "show",
    "one",
    "hart",
    "representation",
    "using",
    "numba",
    "initially",
    "encoder",
    "input",
    "data",
    "going",
    "use",
    "np",
    "dot",
    "0",
    "going",
    "put",
    "zeroes",
    "values",
    "based",
    "dimensions",
    "given",
    "remember",
    "first",
    "dimension",
    "go",
    "go",
    "particular",
    "block",
    "first",
    "dimension",
    "says",
    "put",
    "number",
    "pairs",
    "number",
    "times",
    "basically",
    "means",
    "many",
    "total",
    "number",
    "sentences",
    "going",
    "put",
    "second",
    "parameter",
    "basically",
    "maximum",
    "english",
    "seat",
    "sentence",
    "length",
    "also",
    "know",
    "right",
    "number",
    "english",
    "characters",
    "also",
    "know",
    "right",
    "going",
    "put",
    "mac",
    "mac",
    "include",
    "sequence",
    "length",
    "total",
    "length",
    "input",
    "sentences",
    "total",
    "number",
    "encoded",
    "tokens",
    "actually",
    "give",
    "like",
    "many",
    "number",
    "english",
    "sentence",
    "words",
    "actually",
    "present",
    "right",
    "values",
    "basically",
    "sequence",
    "length",
    "know",
    "see",
    "sequence",
    "length",
    "max",
    "equals",
    "length",
    "english",
    "somewhere",
    "16",
    "unique",
    "input",
    "tokens",
    "71",
    "abused",
    "number",
    "input",
    "tokens",
    "right",
    "similarly",
    "decoder",
    "input",
    "also",
    "particular",
    "way",
    "target",
    "data",
    "also",
    "particular",
    "way",
    "always",
    "remember",
    "decoder",
    "input",
    "data",
    "instead",
    "using",
    "english",
    "using",
    "french",
    "max",
    "decoder",
    "sequence",
    "length",
    "max",
    "number",
    "decoder",
    "tokens",
    "everything",
    "basically",
    "taken",
    "french",
    "directors",
    "french",
    "character",
    "length",
    "go",
    "see",
    "right",
    "decoder",
    "sequence",
    "length",
    "basically",
    "french",
    "trying",
    "see",
    "somewhere",
    "around",
    "59",
    "right",
    "many",
    "number",
    "unique",
    "tokens",
    "actually",
    "present",
    "french",
    "characters",
    "93",
    "going",
    "use",
    "similarly",
    "decoder",
    "target",
    "also",
    "going",
    "use",
    "information",
    "decoder",
    "french",
    "character",
    "particular",
    "code",
    "actually",
    "help",
    "us",
    "whatnot",
    "representation",
    "simple",
    "gas",
    "suppose",
    "words",
    "let",
    "quickly",
    "show",
    "show",
    "front",
    "okay",
    "suppose",
    "going",
    "wrap",
    "okay",
    "suppose",
    "suppose",
    "words",
    "okay",
    "words",
    "like",
    "blue",
    "right",
    "suppose",
    "four",
    "words",
    "suppose",
    "first",
    "sentence",
    "blue",
    "word",
    "w",
    "given",
    "value",
    "1",
    "remaining",
    "given",
    "value",
    "0",
    "one",
    "hot",
    "representation",
    "basically",
    "says",
    "going",
    "right",
    "remember",
    "target",
    "length",
    "completely",
    "different",
    "english",
    "somewhere",
    "around",
    "much",
    "saw",
    "go",
    "see",
    "top",
    "english",
    "somewhere",
    "around",
    "71",
    "unique",
    "tokens",
    "sentence",
    "length",
    "somewhere",
    "around",
    "16",
    "right",
    "considering",
    "16",
    "length",
    "considering",
    "71",
    "unique",
    "tokens",
    "wherever",
    "particular",
    "token",
    "present",
    "become",
    "1",
    "remaining",
    "become",
    "0",
    "whole",
    "code",
    "whole",
    "code",
    "actually",
    "understand",
    "particular",
    "code",
    "guys",
    "try",
    "free",
    "know",
    "see",
    "comma",
    "characters",
    "enumerate",
    "input",
    "underscore",
    "text",
    "input",
    "good",
    "text",
    "okay",
    "show",
    "input",
    "underscore",
    "next",
    "see",
    "okay",
    "first",
    "traversing",
    "inside",
    "input",
    "underscore",
    "text",
    "one",
    "one",
    "sentences",
    "like",
    "okay",
    "one",
    "one",
    "syntax",
    "like",
    "see",
    "basically",
    "replacing",
    "wherever",
    "particular",
    "character",
    "getting",
    "replaced",
    "one",
    "okay",
    "guys",
    "deep",
    "dive",
    "code",
    "try",
    "understand",
    "okay",
    "try",
    "go",
    "line",
    "line",
    "try",
    "understand",
    "always",
    "beneficial",
    "okay",
    "remember",
    "particular",
    "whole",
    "code",
    "actually",
    "one",
    "hut",
    "representation",
    "okay",
    "going",
    "start",
    "creating",
    "lsdm",
    "layer",
    "lsdm",
    "layer",
    "initially",
    "ask",
    "encoder",
    "input",
    "shape",
    "encoder",
    "input",
    "shape",
    "giving",
    "number",
    "included",
    "tokens",
    "lsdm",
    "taken",
    "little",
    "late",
    "mention",
    "initialized",
    "leading",
    "dimension",
    "256",
    "like",
    "timestamps",
    "going",
    "consider",
    "going",
    "take",
    "return",
    "state",
    "true",
    "one",
    "important",
    "thing",
    "happening",
    "understand",
    "go",
    "wordpad",
    "remember",
    "taking",
    "particular",
    "outputs",
    "okay",
    "want",
    "output",
    "encoder",
    "decoder",
    "actually",
    "work",
    "right",
    "actually",
    "works",
    "actually",
    "see",
    "first",
    "go",
    "initialize",
    "initialized",
    "htm",
    "layer",
    "encoder",
    "taking",
    "encoder",
    "input",
    "gives",
    "us",
    "three",
    "values",
    "one",
    "encoder",
    "output",
    "skip",
    "require",
    "state",
    "underscore",
    "h",
    "basically",
    "magadan",
    "sense",
    "one",
    "cell",
    "state",
    "okay",
    "go",
    "back",
    "guys",
    "requiring",
    "students",
    "information",
    "whatever",
    "output",
    "getting",
    "respect",
    "complete",
    "time",
    "stamps",
    "going",
    "hi",
    "okay",
    "want",
    "require",
    "output",
    "see",
    "taking",
    "state",
    "h",
    "state",
    "c",
    "putting",
    "inside",
    "encoder",
    "state",
    "variable",
    "okay",
    "encoder",
    "state",
    "beautiful",
    "similarly",
    "respect",
    "decoder",
    "inputs",
    "decoder",
    "course",
    "know",
    "decoder",
    "number",
    "decoded",
    "tokens",
    "absolutely",
    "know",
    "created",
    "latent",
    "dimension",
    "created",
    "lists",
    "iam",
    "like",
    "okay",
    "much",
    "concerned",
    "getting",
    "output",
    "worry",
    "things",
    "okay",
    "focusing",
    "getting",
    "decoder",
    "outputs",
    "okay",
    "get",
    "decoder",
    "outputs",
    "decoder",
    "output",
    "basically",
    "means",
    "information",
    "information",
    "want",
    "get",
    "information",
    "layer",
    "actually",
    "creating",
    "second",
    "step",
    "right",
    "first",
    "layer",
    "took",
    "okay",
    "want",
    "value",
    "value",
    "output",
    "value",
    "taken",
    "stored",
    "variable",
    "decoder",
    "listing",
    "interested",
    "decoder",
    "output",
    "right",
    "using",
    "dense",
    "layer",
    "finally",
    "actually",
    "getting",
    "outputs",
    "respect",
    "okay",
    "respect",
    "encoder",
    "inputs",
    "decoder",
    "inputs",
    "decoder",
    "outputs",
    "taking",
    "particular",
    "value",
    "considering",
    "creating",
    "model",
    "give",
    "values",
    "inside",
    "list",
    "see",
    "encoder",
    "inputs",
    "decoder",
    "inputs",
    "almost",
    "timestamp",
    "finally",
    "getting",
    "decoder",
    "compiling",
    "fitting",
    "data",
    "validation",
    "split",
    "hundred",
    "box",
    "see",
    "probably",
    "able",
    "give",
    "around",
    "87",
    "percent",
    "accuracy",
    "remember",
    "guys",
    "also",
    "work",
    "local",
    "laptop",
    "work",
    "proper",
    "like",
    "within",
    "15",
    "minutes",
    "able",
    "hundred",
    "imposter",
    "since",
    "character",
    "character",
    "implementation",
    "okay",
    "model",
    "got",
    "trained",
    "one",
    "assignment",
    "really",
    "want",
    "give",
    "guys",
    "please",
    "try",
    "understand",
    "code",
    "know",
    "actually",
    "generating",
    "sentences",
    "okay",
    "see",
    "basically",
    "generating",
    "sentence",
    "try",
    "understand",
    "probably",
    "know",
    "little",
    "bit",
    "difficult",
    "want",
    "read",
    "blocks",
    "understand",
    "sampling",
    "generating",
    "okay",
    "explained",
    "till",
    "like",
    "model",
    "actually",
    "generated",
    "pretty",
    "much",
    "simple",
    "refer",
    "blocks",
    "reason",
    "telling",
    "understand",
    "probably",
    "know",
    "lsdm",
    "go",
    "first",
    "able",
    "understand",
    "next",
    "model",
    "going",
    "discuss",
    "attention",
    "models",
    "need",
    "make",
    "architecture",
    "changes",
    "know",
    "instead",
    "using",
    "encoder",
    "right",
    "using",
    "getting",
    "changed",
    "lsd",
    "okay",
    "yes",
    "guys",
    "particular",
    "video",
    "used",
    "let",
    "know",
    "whether",
    "queries",
    "happy",
    "help",
    "please",
    "give",
    "try",
    "give",
    "try",
    "understand",
    "code",
    "actually",
    "changing",
    "ten",
    "sentences",
    "running",
    "course",
    "want",
    "want",
    "explore",
    "hardly",
    "around",
    "15",
    "20",
    "lines",
    "code",
    "really",
    "want",
    "make",
    "make",
    "understand",
    "thing",
    "understood",
    "part",
    "pretty",
    "much",
    "easy",
    "yes",
    "guys",
    "particular",
    "video",
    "next",
    "video",
    "deep",
    "learning",
    "coming",
    "attention",
    "models",
    "discuss",
    "attention",
    "mall",
    "actually",
    "yes",
    "guys",
    "particularly",
    "de",
    "sel",
    "next",
    "video",
    "great",
    "day",
    "thank",
    "wonder",
    "bye"
  ],
  "keywords": [
    "guys",
    "like",
    "probably",
    "particular",
    "video",
    "going",
    "sequence",
    "learning",
    "help",
    "language",
    "translation",
    "take",
    "data",
    "set",
    "things",
    "go",
    "please",
    "remember",
    "need",
    "know",
    "show",
    "make",
    "understand",
    "code",
    "actually",
    "whole",
    "thing",
    "right",
    "ok",
    "first",
    "getting",
    "english",
    "french",
    "present",
    "taken",
    "respect",
    "words",
    "use",
    "also",
    "let",
    "able",
    "try",
    "important",
    "one",
    "encoder",
    "basically",
    "decoder",
    "okay",
    "provide",
    "context",
    "vector",
    "created",
    "layer",
    "creating",
    "output",
    "consider",
    "work",
    "character",
    "characters",
    "representation",
    "means",
    "suppose",
    "unique",
    "want",
    "based",
    "0",
    "similarly",
    "much",
    "sentences",
    "input",
    "target",
    "next",
    "step",
    "see",
    "given",
    "number",
    "many",
    "maximum",
    "sentence",
    "length",
    "really",
    "give",
    "total",
    "word",
    "tensorflow",
    "model",
    "initialize",
    "dimension",
    "values",
    "every",
    "text",
    "put",
    "somewhere",
    "around",
    "tokens",
    "16",
    "token",
    "1",
    "71",
    "yes",
    "using",
    "information",
    "value",
    "underscore",
    "lsdm",
    "state",
    "taking",
    "outputs",
    "inputs"
  ]
}