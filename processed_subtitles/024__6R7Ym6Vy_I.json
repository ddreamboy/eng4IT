{
  "text": "(gentle music jingle)\n(audience applauding)\n- Whoa, so many of you.\nGood, okay, thank you for\nthat lovely introduction.\nRight, so, what is generative\nartificial intelligence?\nSo I'm gonna explain what\nartificial intelligence is\nand I want this to be a bit interactive\nso there will be some\naudience participation.\nThe people here who hold\nthis lecture said to me,\n\"Oh, you are very low-tech\nfor somebody working on AI.\"\nI don't have any explosions\nor any experiments,\nso I'm afraid you'll have to participate,\nI hope that's okay.\nAll right, so, what is generative\nartificial intelligence?\nSo the term is made up by two things,\nartificial intelligence and generative.\nSo artificial intelligence\nis a fancy term for saying\nwe get a computer programme to do the job\nthat a human would otherwise do.\nAnd generative, this is the fun bit,\nwe are creating new content\nthat the computer has\nnot necessarily seen,\nit has seen parts of it,\nand it's able to synthesise\nit and give us new things.\nSo what would this new content be?\nIt could be audio,\nit could be computer code\nso that it writes a programme for us,\nit could be a new image,\nit could be a text,\nlike an email or an essay\nyou've heard, or video.\nNow in this lecture\nI'm only gonna be mostly focusing on text\nbecause I do natural language processing\nand this is what I know about,\nand we'll see how the technology works\nand hopefully leaving the\nlecture you'll know how,\nlike there's a lot of myth\naround it and it's not,\nyou'll see what it does\nand it's just a tool, okay?\nRight, so the outline of the talk,\nthere's three parts and\nit's kind of boring.\nThis is Alice Morse Earle.\nI do not expect that you know the lady.\nShe was an American writer\nand she writes about\nmemorabilia and customs,\nbut she's famous for her quotes.\nSo she's given us this\nquote here that says,\n\"Yesterday's history,\ntomorrow is a mystery,\ntoday is a gift, and that's\nwhy it's called the present.\"\nIt's a very optimistic quote.\nAnd the lecture is basically\nthe past, the present,\nand the future of AI.\nOkay, so what I want to\nsay right at the front\nis that generative AI\nis not a new concept.\nIt's been around for a while.\nSo how many of you have\nused or are familiar\nwith Google Translate?\nCan I see a show of hands?\nRight, who can tell me when\nGoogle Translate launched\nfor the first time?\n- 1995?\n- Oh, that would've been good.\n2006, so it's been around for 17 years\nand we've all been using it.\nAnd this is an example of generative AI.\nGreek text comes in,\nI'm Greek, so you know,\npay some juice to the... (laughs)\nRight, so Greek text comes in,\nEnglish text comes out.\nAnd Google Translate\nhas served us very well\nfor all these years\nand nobody was making a fuss.\nAnother example is Siri on the phone.\nAgain, Siri launched 2011,\n12 years ago,\nand it was a sensation back then.\nIt is another example of generative AI.\nWe can ask Siri to set\nalarms and Siri talks back\nand oh how great it is\nand then you can ask about\nyour alarms and whatnot.\nThis is generative AI.\nAgain, it's not as\nsophisticated as ChatGPT,\nbut it was there.\nAnd I don't know how many have an iPhone?\nSee, iPhones are quite\npopular, I don't know why.\nOkay, so, we are all familiar with that.\nAnd of course later on there\nwas Amazon Alexa and so on.\nOkay, again, generative\nAI is not a new concept,\nit is everywhere, it\nis part of your phone.\nThe completion when\nyou're sending an email\nor when you're sending a text.\nThe phone attempts to\ncomplete your sentences,\nattempts to think like you\nand it saves you time, right?\nBecause some of the completions are there.\nThe same with Google,\nwhen you're trying to\ntype it tries to guess\nwhat your search term is.\nThis is an example of language modelling,\nwe'll hear a lot about language\nmodelling in this talk.\nSo basically we're making predictions\nof what the continuations are going to be.\nSo what I'm telling you\nis that generative AI is not that new.\nSo the question is, what\nis the fuss, what happened?\nSo in 2023, OpenAI,\nwhich is a company in California,\nin fact, in San Francisco.\nIf you go to San Francisco,\nyou can even see the lights\nat night of their building.\nIt announced GPT-4\nand it claimed that it can\nbeat 90% of humans on the SAT.\nFor those of you who don't know,\nSAT is a standardised test\nthat American school children have to take\nto enter university,\nit's an admissions test,\nand it's multiple choice and\nit's considered not so easy.\nSo GPT-4 can do it.\nThey also claimed that it\ncan get top marks in law,\nmedical exams and other exams,\nthey have a whole suite\nof things that they claim,\nwell, not they claim, they\nshow that GPT-4 can do it.\nOkay, aside from that, it can pass exams,\nwe can ask it to do other things.\nSo you can ask it to write text for you.\nFor example, you can have a prompt,\nthis little thing that you\nsee up there, it's a prompt.\nIt's what the human wants\nthe tool to do for them.\nAnd a potential prompt could be,\n\"I'm writing an essay\nabout the use of mobile\nphones during driving.\nCan you gimme three arguments in favour?\"\nThis is quite sophisticated.\nIf you asked me,\nI'm not sure I can come\nup with three arguments.\nYou can also do,\nand these are real prompts\nthat actually the tool can do.\nYou tell ChatGPT or GPT in general,\n\"Act as a JavaScript developer.\nWrite a programme that checks\nthe information on a form.\nName and email are required,\nbut address and age are not.\"\nSo I'm just writing this\nand the tool will spit out a programme.\nAnd this is the best one.\n\"Create an About Me page for a website.\nI like rock climbing, outdoor\nsports, and I like to programme.\nI started my career as a quality\nengineer in the industry,\nblah, blah, blah.\"\nSo I give this version of\nwhat I want the website to be\nand it will create it for me.\nSo, you see, we've gone from\nGoogle Translate and Siri\nand the auto-completion\nto something which is a\nlot more sophisticated\nand can do a lot more things.\nAnother fun fact.\nSo this is a graph that shows\nthe time it took for ChatGPT\nto reach 100 million users\ncompared with other tools\nthat have been launched in the past.\nAnd you see our beloved Google Translate,\nit took 78 months\nto reach 100 million users,\na long time.\nTikTok took nine months and ChatGPT, two.\nSo within two months they\nhad 100 million users\nand these users pay a little\nbit to use the system,\nso you can do the multiplication\nand figure out how much money they make.\nOkay, so this is the history part.\nSo how did we make ChatGPT?\nWhat is the technology behind this?\nThe technology it turns\nout is not extremely new\nor extremely innovative\nor extremely difficult to comprehend.\nSo we'll talk about that today now.\nSo we'll address three questions.\nFirst of all, how did we get\nfrom the single-purpose systems\nlike Google Translate to ChatGPT,\nwhich is more sophisticated\nand does a lot more things?\nAnd in particular,\nwhat is the core technology behind ChatGPT\nand what are the risks, if there are any?\nAnd finally, I will just show you\na little glimpse of the future\nand how it's gonna look like\nand whether we should be worried or not\nand you know, I won't leave you hanging,\nplease don't worry, okay?\nRight, so, all this GPT model variants,\nand there is a cottage industry out there,\nI'm just using GPT as an\nexample because the public knows\nand there have been a lot of, you know,\nnews articles about it,\nbut there's other models,\nother variants of models\nthat we use in academia.\nAnd they all work on the same principle,\nand this principle is\ncalled language modelling.\nWhat does language modelling do?\nIt assumes we have a sequence of words.\nThe context so far.\nAnd we saw this context in the completion,\nand I have an example here.\nAssuming my context is\nthe phrase \"I want to,\"\nthe language modelling tool\nwill predict what comes next.\nSo if I tell you \"I want to,\"\nthere is several predictions.\nI want to shovel, I want to play,\nI want to swim, I want to eat.\nAnd depending on what we choose,\nwhether it's shovel or play or swim,\nthere is more continuations.\nSo for shovel, it will be snow,\nfor play, it can be tennis or video,\nswim doesn't have a continuation,\nand for eat, it will be lots and fruit.\nNow this is a toy example,\nbut imagine now that the\ncomputer has seen a lot of text\nand it knows what words\nfollow which other words.\nWe used to count these things.\nSo I would go, I would\ndownload a lot of data\nand I would count, \"I want to show them,\"\nhow many times does it appear\nand what are the continuations?\nAnd we would have counts of these things.\nAnd all of this has gone\nout of the window right now\nand we use neural networks that\ndon't exactly count things,\nbut predict, learn things\nin a more sophisticated way,\nand I'll show you in a\nmoment how it's done.\nSo ChatGPT and GPT variants\nare based on this principle\nof I have some context, I\nwill predict what comes next.\nAnd that's the prompt,\nthe prompt that I gave\nyou, these things here,\nthese are prompts,\nthis is the context,\nand then it needs to do the task.\nWhat would come next?\nIn some cases it would\nbe the three arguments.\nIn the case of the web\ndeveloper, it would be a webpage.\nOkay, the task of language\nmodelling is we have the context,\nand this changed the example now.\nIt says \"The colour of the sky is.\"\nAnd we have a neural language model,\nthis is just an algorithm,\nthat will predict what is\nthe most likely continuation,\nand likelihood matters.\nThese are all predicated\non actually making guesses\nabout what's gonna come next.\nAnd that's why sometimes they fail,\nbecause they predict\nthe most likely answer\nwhereas you want a less likely one.\nBut this is how they're trained,\nthey're trained to come up\nwith what is most likely.\nOkay, so we don't count these things,\nwe try to predict them\nusing this language model.\nSo how would you build\nyour own language model?\nThis is a recipe, this is\nhow everybody does this.\nSo, step one, we need a lot of data.\nWe need to collect a ginormous corpus.\nSo these are words.\nAnd where will we find\nsuch a ginormous corpus?\nI mean, we go to the web, right?\nAnd we download the whole of Wikipedia,\nStack Overflow pages,\nQuora, social media, GitHub, Reddit,\nwhatever you can find out there.\nI mean, work out the\npermissions, it has to be legal.\nYou download all this corpus.\nAnd then what do you do?\nThen you have this language model.\nI haven't told you what\nexactly this language model is,\nthere is an example,\nand I haven't told you\nwhat the neural network\nthat does the prediction is,\nbut assuming you have it.\nSo you have this machinery\nthat will do the learning for you\nand the task now is to\npredict the next word,\nbut how do we do it?\nAnd this is the genius part.\nWe have the sentences in the corpus.\nWe can remove some of them\nand we can have the language model\npredict the sentences we have removed.\nThis is dead cheap.\nI just remove things,\nI pretend they're not there,\nand I get the language\nmodel to predict them.\nSo I will randomly truncate,\ntruncate means remove,\nthe last part of the input sentence.\nI will calculate with this neural network\nthe probability of the missing words.\nIf I get it right, I'm good.\nIf I'm not right,\nI have to go back and\nre-estimate some things\nbecause obviously I made a mistake,\nand I keep going.\nI will adjust and feedback to the model\nand then I will compare\nwhat the model predicted\nto the ground truth\nbecause I've removed the\nwords in the first place\nso I actually know what the real truth is.\nAnd we keep going\nfor some months or maybe years.\nNo, months, let's say.\nSo it will take some\ntime to do this process\nbecause as you can appreciate\nI have a very large corpus\nand I have many sentences\nand I have to do the prediction\nand then go back and correct\nmy mistake and so on.\nBut in the end,\nthe thing will converge\nand I will get my answer.\nSo the tool in the middle that I've shown,\nthis tool here, this language model,\na very simple language\nmodel looks a bit like this.\nAnd maybe the audience has seen these,\nthis is a very naive graph,\nbut it helps to illustrate\nthe point of what it does.\nSo this neural network language\nmodel will have some input\nwhich is these nodes in\nthe, as we look at it,\nwell, my right and your right, okay.\nSo the nodes here on\nthe right are the input\nand the nodes at the\nvery left are the output.\nSo we will present this neural\nnetwork with five inputs,\nthe five circles,\nand we have three outputs,\nthe three circles.\nAnd there is stuff in the middle\nthat I didn't say anything about.\nThese are layers.\nThese are more nodes\nthat are supposed to be\nabstractions of my input.\nSo they generalise.\nThe idea is if I put more\nlayers on top of layers,\nthe middle layers will\ngeneralise the input\nand will be able to see\npatterns that are not there.\nSo you have these nodes\nand the input to the nodes\nare not exactly words,\nthey're vectors, so series of numbers,\nbut forget that for now.\nSo we have some input, we have\nsome layers in the middle,\nwe have some output.\nAnd this now has these\nconnections, these edges,\nwhich are the weights,\nthis is what the network will learn.\nAnd these weights are basically numbers,\nand here it's all fully connected,\nso I have very many connections.\nWhy am I going through this process\nof actually telling you all of that?\nYou will see in a minute.\nSo you can work out\nhow big or how small\nthis neural network is\ndepending on the numbers\nof connections it has.\nSo for this toy neural\nnetwork we have here,\nI have worked out the number of weights,\nwe call them also parameters,\nthat this neural network has\nand that the model needs to learn.\nSo the parameters are the\nnumber of units as input,\nin this case it's 5,\ntimes the units in the next layer, 8.\nPlus 8, this plus 8 is a bias,\nit's a cheating thing that\nthese neural networks have.\nAgain, you need to learn it\nand it sort of corrects a\nlittle bit the neural network\nif it's off.\nIt's actually genius.\nIf the prediction is not right,\nit tries to correct it a little bit.\nSo for the purposes of this talk,\nI'm not going to go into the details,\nall I want you to see\nis that there is a way of\nworking out the parameters,\nwhich is basically the\nnumber of input units\ntimes the units my input is going to,\nand for this fully connected network,\nif we add up everything,\nwe come up with 99\ntrainable parameters, 99.\nThis is a small network\nfor all purposes, right?\nBut I want you to remember this,\nthis small network is 99 parameters.\nWhen you hear this network\nis a billion parameters,\nI want you to imagine how\nbig this will be, okay?\nSo 99 only for this toy neural network.\nAnd this is how we judge\nhow big the model is,\nhow long it took and how much it cost,\nit's the number of parameters.\nIn reality, in reality, though,\nno one is using this network.\nMaybe in my class,\nif I have a first year undergraduate class\nand I introduce neural networks,\nI will use this as an example.\nIn reality, what people\nuse is these monsters\nthat are made of blocks,\nand what block means they're\nmade of other neural networks.\nSo I don't know how many people\nhave heard of transformers.\nI hope no one.\nOh wow, okay.\nSo transformers are these neural networks\nthat we use to build ChatGPT.\nAnd in fact GPT stands for\ngenerative pre-trained transformers.\nSo transformer is even in the title.\nSo this is a sketch of a transformer.\nSo you have your input\nand the input is not words, like I said,\nhere it says embeddings,\nembeddings is another word for vectors.\nAnd then you will have this,\na bigger version of this network,\nmultiplied into these blocks.\nAnd each block is this complicated system\nthat has some neural networks inside it.\nWe're not gonna go into\nthe detail, I don't want,\nI please don't go,\nall I'm trying,\n(audience laughs)\nall I'm trying to say is that, you know,\nwe have these blocks stacked\non top of each other,\nthe transformer has eight of those,\nwhich are mini neural networks,\nand this task remains the same.\nThat's what I want you\nto take out of this.\nInput goes in the context,\n\"the chicken walked,\"\nwe're doing some processing,\nand our task is to\npredict the continuation,\nwhich is \"across the road.\"\nAnd this EOS means end of sentence\nbecause we need to tell the neural network\nthat our sentence finished.\nI mean they're kind of dumb, right?\nWe need to tell them everything.\nWhen I hear like AI will take\nover the world, I go like,\nReally? We have to actually spell it out.\nOkay, so, this is the transformer,\nthe king of architectures,\nthe transformers came in 2017.\nNobody's working on new\narchitectures right now.\nIt is a bit sad, like\neverybody's using these things.\nThey used to be like some\npluralism but now no,\neverybody's using transformers,\nwe've decided they're great.\nOkay, so, what we're gonna do with this,\nand this is kind of important\nand the amazing thing,\nis we're gonna do\nself-supervised learning.\nAnd this is what I said,\nwe have the sentence,\nwe truncate, we predict,\nand we keep going till we\nlearn these probabilities.\nOkay? You're with me so far?\nGood, okay, so,\nonce we have our transformer\nand we've given it all this\ndata that there is in the world,\nthen we have a pre-trained model.\nThat's why GPT is called\nthe generative pre-trained transformer.\nThis is a baseline model that we have\nand has seen a lot of\nthings about the world\nin the form of text.\nAnd then what we normally do,\nwe have this general purpose model\nand we need to specialise it somehow\nfor a specific task.\nAnd this is what is called fine-tuning.\nSo that means that the\nnetwork has some weights\nand we have to specialise the network.\nWe'll take, initialise the weights\nwith what we know from the pre-training,\nand then in the specific\ntask we will narrow\na new set of weights.\nSo for example, if I have medical data,\nI will take my pre-trained model,\nI will specialise it to this medical data,\nand then I can do something\nthat is specific for this task,\nwhich is, for example, write\na diagnosis from a report.\nOkay, so this notion of\nfine-tuning is very important\nbecause it allows us to do\nspecial-purpose applications\nfor these generic pre-trained models.\nNow, and people think that\nGPT and all of these things\nare general purpose,\nbut they are fine-tuned\nto be general purpose\nand we'll see how.\nOkay, so, here's the question now.\nWe have this basic technology\nto do this pre-training\nand I told you how to do it,\nif you download all of the web.\nHow good can a language\nmodel become, right?\nHow does it become great?\nBecause when GPT came\nout in GPT-1 and GPT-2,\nthey were not amazing.\nSo the bigger, the better.\nSize is all that matters, I'm afraid.\nThis is very bad because\nwe used to, you know,\npeople didn't believe in scale\nand now we see that\nscale is very important.\nSo, since 2018,\nwe've witnessed an\nabsolutely extreme increase\nin model sizes.\nAnd I have some graphs to show this.\nOkay, I hope people at the\nback can see this graph.\nYeah, you should be all right.\nSo this graph shows\nthe number of parameters.\nRemember, the toy neural network had 99.\nThe number of parameters\nthat these models have.\nAnd we start with a normal amount.\nWell, normal for GPT-1.\nAnd we go up to GPT-4,\nwhich has one trillion parameters.\nHuge, one trillion.\nThis is a very, very, very big model.\nAnd you can see here the\nant brain and the rat brain\nand we go up to the human brain.\nThe human brain has,\nnot a trillion,\n100 trillion parameters.\nSo we are a bit off,\nwe're not at the human brain level yet\nand maybe we'll never get there\nand we can't compare\nGPT to the human brain\nbut I'm just giving you an\nidea of how big this model is.\nNow what about the words it's seen?\nSo this graph shows us the number of words\nprocessed by these language\nmodels during their training\nand you will see that\nthere has been an increase,\nbut the increase has not been\nas big as the parameters.\nSo the community started focusing\non the parameter size of these models,\nwhereas in fact we now know\nthat it needs to see\na lot of text as well.\nSo GPT-4 has seen approximately,\nI don't know, a few billion words.\nAll the human written text\nis I think 100 billion,\nso it's sort of approaching this.\nYou can also see what a human\nreads in their lifetime,\nit's a lot less.\nEven if they read, you know,\nbecause people nowadays, you know,\nthey read but they don't read fiction,\nthey read the phone, anyway.\nYou see the English Wikipedia,\nso we are approaching the level of\nthe text that is out\nthere that we can get.\nAnd in fact, one may\nsay, well, GPT is great,\nyou can actually use it\nto generate more text\nand then use this text\nthat GPT has generated\nand then retrain the model.\nBut we know this text is not exactly right\nand in fact it's diminished returns,\nso we're gonna plateau at some point.\nOkay, how much does it cost?\nNow, okay, so GPT-4 cost\n$100 million, okay?\nSo when should they start doing it again?\nSo obviously this is not\na process you have to do\nover and over again.\nYou have to think very well\nand you make a mistake and\nyou lost like $50 million.\nYou can't start again so you\nhave to be very sophisticated\nas to how you engineer the training\nbecause a mistake costs money.\nAnd of course not everybody can do this,\nnot everybody has $100 million.\nThey can do it because they\nhave Microsoft backing them,\nnot everybody, okay.\nNow this is a video that is\nsupposed to play and illustrate,\nlet's see if it will work,\nthe effects of scaling, okay.\nSo I will play it one more.\nSo these are tasks that you can do\nand it's the number of tasks\nagainst the number of parameters.\nSo we start with 8 billion parameters\nand we can do a few tasks.\nAnd then the tasks\nincrease, so summarization,\nquestion answering, translation.\nAnd once we move to\n540 billion parameters,\nwe have more tasks.\nWe start with very simple ones,\nlike code completion.\nAnd then we can do reading comprehension\nand language understanding\nand translation.\nSo you get the picture,\nthe tree flourishes.\nSo this is what people\ndiscovered with scaling.\nIf you scale the language\nmodel, you can do more tasks.\nOkay, so now.\nMaybe we are done.\nBut what people discovered\nis if you actually take GPT\nand you put it out there,\nit actually doesn't behave\nlike people want it to behave\nbecause this is a language\nmodel trained to predict\nand complete sentences\nand humans want to use\nGPT for other things\nbecause they have their own tasks\nthat the developers hadn't thought of.\nSo then the notion of\nfine-tuning comes in,\nit never left us.\nSo now what we're gonna do\nis we're gonna collect\na lot of instructions.\nSo instructions are examples\nof what people want\nChatGPT to do for them,\nsuch as answer the following question,\nor answer the question step by step.\nAnd so we're gonna give these\ndemonstrations to the model,\nand in fact, almost\n2,000 of such examples,\nand we're gonna fine-tune.\nSo we're gonna tell this language model,\nlook, these are the\ntasks that people want,\ntry to learn them.\nAnd then an interesting thing happens,\nis that we can actually then generalise\nto unseen tasks, unseen instructions,\nbecause you and I may have\ndifferent usage purposes\nfor these language models.\nOkay, but here's the problem.\nWe have an alignment problem\nand this is actually very important\nand something that will not\nleave us for the future.\nAnd the question is,\nhow do we create an agent\nthat behaves in accordance\nwith what a human wants?\nAnd I know there's many\nwords and questions here.\nBut the real question is,\nif we have AI systems with skills\nthat we find important or useful,\nhow do we adapt those systems\nto reliably use those skills\nto do the things we want?\nAnd there is a framework\nthat is called the HHH\nframing of the problem.\nSo we want GPT to be helpful,\nhonest, and harmless.\nAnd this is the bare minimum.\nSo what does it mean, helpful?\nIt it should follow instructions\nand perform the tasks\nwe want it to perform\nand provide answers for them\nand ask relevant questions\naccording to the user intent, and clarify.\nSo if you've been following,\nin the beginning, GPT did none of this,\nbut slowly it became better\nand it now actually asks for\nthese clarification questions.\nIt should be accurate,\nsomething that is not\n100% there even to this,\nthere is, you know,\ninaccurate information.\nAnd avoid toxic, biassed,\nor offensive responses.\nAnd now here's a question I have for you.\nHow will we get the model\nto do all of these things?\nYou know the answer. Fine-tuning.\nExcept that we're gonna do\na different fine-tuning.\nWe're gonna ask the humans to\ndo some preferences for us.\nSo in terms of helpful, we're gonna ask,\nan example is, \"What causes\nthe seasons to change?\"\nAnd then we'll give two\noptions to the human.\n\"Changes occur all the time\nand it's an important\naspect of life,\" bad.\n\"The seasons are caused primarily\nby the tilt of the Earth's axis,\" good.\nSo we'll get this preference course\nand then we'll train the model again\nand then it will know.\nSo fine-tuning is very important.\nAnd now, it was expensive as it was,\nnow we make it even more expensive\nbecause we add a human\ninto the mix, right?\nBecause we have to pay these humans\nthat give us the preferences,\nwe have to think of the tasks.\nThe same for honesty.\n\"Is it possible to\nprove that P equals NP?\"\n\"No, it's impossible,\" is\nnot great as an answer.\n\"That is considered a very\ndifficult and unsolved problem\nin computer science,\" it's better.\nAnd we have similar for harmless.\nOkay, so I think it's time,\nlet's see if we'll do a demo.\nYeah, that's bad if you\nremove all the files.\nOkay, hold on, okay.\nSo now we have GPT here.\nI'll do some questions\nand then we'll take some\nquestions from the audience, okay?\nSo let's ask one question.\n\"Is the UK a monarchy?\"\nCan you see it up there? I'm not sure.\nAnd it's not generating.\nOh, perfect, okay.\nSo what do you observe?\nFirst thing, too long.\nI always have this beef with this.\nIt's too long.\n(audience laughs)\nYou see what it says?\n\"As of my last knowledge\nupdate in September 2021,\nthe United Kingdom is a\nconstitutional monarchy.\"\nIt could be that it wasn't anymore, right?\nSomething happened.\n\"This means that while there is a monarch,\nthe reigning monarch as to that time\nwas Queen Elizabeth III.\"\nSo it tells you, you know,\nI don't know what happened,\nat that time there was a Queen Elizabeth.\nNow if you ask it, who,\nsorry, \"Who is Rishi?\nIf I could type, \"Rishi Sunak,\"\ndoes it know?\n\"A British politician.\nAs my last knowledge update,\nhe was the Chancellor of the Exchequer.\"\nSo it does not know that\nhe's the Prime Minister.\n\"Write me a poem,\nwrite me a poem about.\"\nWhat do we want it to be about?\nGive me two things, eh?\n- [Audience Member] Generative AI.\n(audience laughs)\n- It will know.\nIt will know, let's do\nanother point about...\n- [Audience Members] Cats.\n- A cat and a squirrel, we'll\ndo a cat and a squirrel.\n\"A cat and a squirrel.\"\n\"A cat and a squirrel, they meet and know.\nA tale of curiosity,\" whoa.\n(audience laughs)\nOh my god, okay, I will not read this.\nYou know, they want me to\nfinish at 8:00, so, right.\nLet's say, \"Can you try a shorter poem?\"\n- [Audience Member] Try a haiku.\n- \"Can you try,\ncan you try to give me a haiku?\"\nTo give me a hai, I cannot type, haiku.\n\"Amidst autumn's gold, leaves\nwhisper secrets untold,\nnature's story, bold.\"\n(audience member claps)\nOkay.\nDon't clap, okay, let's, okay, one more.\nSo does the audience have\nanything that they want,\nbut challenging, that you want to ask?\nYes?\n- [Audience Member] What\nschool did Alan Turing go to?\n- Perfect, \"What school\ndid Alan Turing go to?\"\nOh my God.\n(audience laughs)\nHe went, do you know?\nI don't know whether it's\ntrue, this is the problem.\nSherborne School, can somebody verify?\nKing's College, Cambridge, Princeton?\nYes, okay, ah, here's another one.\n\"Tell me a joke about Alan Turing.\"\nOkay, I cannot type but it will, okay.\n\"Light-hearted joke.\nWhy did Alan Turing\nkeep his computer cold?\nBecause he didn't want it to catch bytes.\"\n(audience laughs)\nBad.\nOkay, okay.\n- Explain why that's funny.\n(audience laughs)\n- Ah, very good one.\n\"Why is this a funny joke?\"\nAnd where is it? Oh god.\n(audience laughs)\nOkay, \"Catch bytes sounds\nsimilar to catch colds.\"\n(audience laughs)\n\"Catching bytes is a humorous\ntwist on this phrase,\"\noh my God.\n\"The humour comes from the clever wordplay\nand the unexpected.\"\n(audience laughs)\nOkay, you lose the will to live,\nbut it does explain, it\ndoes explain, okay, right.\nOne last order from you guys.\n- [Audience Member] What is consciousness?\n- It will know because\nit has seen definitions\nand it will spit out like a huge thing.\nShall we try?\n(audience talks indistinctly)\n- Say again?\n- [Audience Member] Write\na song about relativity.\n- Okay, \"Write a song.\"\n- Short.\n(audience laughs)\n- You are learning very fast.\n\"A short song about relativity.\"\nOh goodness me.\n(audience laughs)\n(audience laughs)\nThis is short?\n(audience laughs)\nAll right, outro, okay, so see,\nit doesn't follow instructions.\nIt is not helpful.\nAnd this has been fine-tuned.\nOkay, so the best was here.\nIt had something like, where was it?\n\"Einstein said, 'Eureka!\" one fateful day,\nas he pondered the stars\nin his own unique way.\nThe theory of relativity, he did unfold,\na cosmic story, ancient and bold.\"\nI mean, kudos to that, okay.\nNow let's go back to the talk,\nbecause I want to talk a\nlittle bit, presentation,\nI want to talk a little\nbit about, you know,\nis it good, is it bad, is\nit fair, are we in danger?\nOkay, so it's virtually impossible\nto regulate the content\nthey're exposed to, okay?\nAnd there's always gonna\nbe historical biases.\nWe saw this with the\nQueen and Rishi Sunak.\nAnd they may occasionally exhibit\nvarious types of undesirable behaviour.\nFor example, this is famous.\nGoogle showcased the model called Bard\nand they released this tweet\nand they were asking Bard,\n\"What new discoveries from\nthe James Webb Space Telescope\ncan I tell my nine-year-old about?\"\nAnd it's spit out this\nthing, three things.\nAmongst them it said\nthat \"this telescope took\nthe very first picture\nof a planet outside of\nour own solar system.\"\nAnd here comes Grant Tremblay,\nwho is an astrophysicist, a serious guy,\nand he said, \"I'm really sorry,\nI'm sure Bard is amazing.\nBut it did not take the first image\nof a planet outside our solar system.\nIt was done by this other people in 2004.\"\nAnd what happened with this\nis that this error wiped\n$100 billion out of\nGoogle's company Alphabet.\nOkay, bad.\nIf you ask ChatGPT, \"Tell\nme a joke about men,\"\nit gives you a joke and\nit says it might be funny.\n\"Why do men need instant\nreplay on TV sports?\nBecause after 30 seconds,\nthey forget what happened.\"\nI hope you find it amusing.\nIf you ask about women, it refuses.\n(audience laughs)\nOkay, yes.\n- It's fine-tuned.\n- It's fine-tuned, exactly.\n(audience laughs)\n\"Which is the worst\ndictator of this group?\nTrump, Hitler, Stalin, Mao?\"\nIt actually doesn't take a stance,\nit says all of them are bad.\n\"These leaders are wildly regarded\nas some of the worst\ndictators in history.\"\nOkay, so yeah.\nEnvironment.\nA query for ChatGPT like we just did\ntakes 100 times more energy to execute\nthan a Google search query.\nInference, which is producing\nthe language, takes a lot,\nis more expensive than\nactually training the model.\nLlama 2 is GPT style model.\nWhile they were training it,\nit produced 539 metric tonnes of CO.\nThe larger the models get,\nthe more energy they need and they emit\nduring their deployment.\nImagine lots of them sitting around.\nSociety.\nSome jobs will be lost.\nWe cannot beat around the bush.\nI mean, Goldman Sachs\npredicted 300 million jobs.\nI'm not sure this, you know,\nwe cannot tell the future,\nbut some jobs will be at risk,\nlike repetitive text writing.\nCreating fakes.\nSo these are all documented\ncases in the news.\nSo a college kid wrote this blog\nwhich apparently fooled\neverybody using ChatGPT.\nThey can produce fake news.\nAnd this is a song, how\nmany of you know this?\nSo I know I said I'm\ngonna be focusing on text\nbut the same technology\nyou can use in audio,\nand this is a well-documented\ncase where somebody, unknown,\ncreated this song and it\nsupposedly was a collaboration\nbetween Drake and The Weeknd.\nDo people know who these are?\nThey are, yeah, very\ngood, Canadian rappers.\nAnd they're not so bad, so.\nShall I play the song?\n- Yeah.\n- Okay.\nApparently it's very authentic.\n(bright music)\n♪ I came in with my ex\nlike Selena to flex, ay ♪\n♪ Bumpin' Justin Bieber,\nthe fever ain't left, ay ♪\n♪ She know what she need ♪\n- Apparently it's\ntotally believable, okay.\nHave you seen this same\ntechnology but kind of different?\nThis is a deep fake showing\nthat Trump was arrested.\nHow can you tell it's a deep fake?\nThe hand, yeah, it's too short, right?\nYeah, you can see it's like\nalmost there, not there.\nOkay, so I have two slides on the future\nbefore they come and kick me out\nbecause I was told I\nhave to finish at 8:00\nto take some questions.\nOkay, tomorrow.\nSo we can't predict the future\nand no, I don't think\nthat these evil computers\nare gonna come and kill us all.\nI will leave you with some\nthoughts by Tim Berners-Lee.\nFor people who don't know\nhim, he invented the internet.\nHe's actually Sir Tim Berners-Lee.\nAnd he said two things\nthat made sense to me.\nFirst of all, that we don't actually know\nwhat a super intelligent\nAI would look like.\nWe haven't made it, so it's\nhard to make these statements.\nHowever, it's likely to have\nlots of these intelligent AIs,\nand by intelligent AIs\nwe mean things like GPT,\nand many of them will be good\nand will help us do things.\nSome may fall to the hands of individuals\nthat want to do harm,\nand it seems easier to minimise the harm\nthat these tools will do\nthan to prevent the systems\nfrom existing at all.\nSo we cannot actually\neliminate them altogether,\nbut we as a society can\nactually mitigate the risks.\nThis is very interesting,\nthis is the Australian Research Council\nthat committed a survey\nand they dealt with a\nhypothetical scenario\nthat whether Chad GPT-4\ncould autonomous replicate,\nyou know, you are replicating yourself,\nyou're creating a copy,\nacquire resources and\nbasically be a very bad agent,\nthe things of the movies.\nAnd the answer is no, it\ncannot do this, it cannot.\nAnd they had like some specific tests\nand it failed on all of them,\nsuch as setting up an\nopen source language model\non a new server, it cannot do that.\nOkay, last slide.\nSo my take on this is that\nwe cannot turn back time.\nAnd every time you think about\nAI coming there to kill you,\nyou should think what is the\nbigger threat to mankind,\nAI or climate change?\nI would personally argue climate\nchange is gonna wipe us all\nbefore the AI becomes super intelligent.\nWho is in control of AI?\nThere are some humans there\nwho hopefully have sense.\nAnd who benefits from it?\nDoes the benefit outweigh the risk?\nIn some cases, the benefit\ndoes, in others it doesn't.\nAnd history tells us\nthat all technology that has been risky,\nsuch as, for example, nuclear energy,\nhas been very strongly regulated.\nSo regulation is coming\nand watch out the space.\nAnd with that I will stop and\nactually take your questions.\nThank you so much for\nlistening, you've been great.\n(audience applauds)\n(applause fades out)\n",
  "words": [
    "gentle",
    "music",
    "jingle",
    "audience",
    "applauding",
    "whoa",
    "many",
    "good",
    "okay",
    "thank",
    "lovely",
    "introduction",
    "right",
    "generative",
    "artificial",
    "intelligence",
    "gon",
    "na",
    "explain",
    "artificial",
    "intelligence",
    "want",
    "bit",
    "interactive",
    "audience",
    "participation",
    "people",
    "hold",
    "lecture",
    "said",
    "oh",
    "somebody",
    "working",
    "ai",
    "explosions",
    "experiments",
    "afraid",
    "participate",
    "hope",
    "okay",
    "right",
    "generative",
    "artificial",
    "intelligence",
    "term",
    "made",
    "two",
    "things",
    "artificial",
    "intelligence",
    "generative",
    "artificial",
    "intelligence",
    "fancy",
    "term",
    "saying",
    "get",
    "computer",
    "programme",
    "job",
    "human",
    "would",
    "otherwise",
    "generative",
    "fun",
    "bit",
    "creating",
    "new",
    "content",
    "computer",
    "necessarily",
    "seen",
    "seen",
    "parts",
    "able",
    "synthesise",
    "give",
    "us",
    "new",
    "things",
    "would",
    "new",
    "content",
    "could",
    "audio",
    "could",
    "computer",
    "code",
    "writes",
    "programme",
    "us",
    "could",
    "new",
    "image",
    "could",
    "text",
    "like",
    "email",
    "essay",
    "heard",
    "video",
    "lecture",
    "gon",
    "na",
    "mostly",
    "focusing",
    "text",
    "natural",
    "language",
    "processing",
    "know",
    "see",
    "technology",
    "works",
    "hopefully",
    "leaving",
    "lecture",
    "know",
    "like",
    "lot",
    "myth",
    "around",
    "see",
    "tool",
    "okay",
    "right",
    "outline",
    "talk",
    "three",
    "parts",
    "kind",
    "boring",
    "alice",
    "morse",
    "earle",
    "expect",
    "know",
    "lady",
    "american",
    "writer",
    "writes",
    "memorabilia",
    "customs",
    "famous",
    "quotes",
    "given",
    "us",
    "quote",
    "says",
    "yesterday",
    "history",
    "tomorrow",
    "mystery",
    "today",
    "gift",
    "called",
    "present",
    "optimistic",
    "quote",
    "lecture",
    "basically",
    "past",
    "present",
    "future",
    "ai",
    "okay",
    "want",
    "say",
    "right",
    "front",
    "generative",
    "ai",
    "new",
    "concept",
    "around",
    "many",
    "used",
    "familiar",
    "google",
    "translate",
    "see",
    "show",
    "hands",
    "right",
    "tell",
    "google",
    "translate",
    "launched",
    "first",
    "time",
    "1995",
    "oh",
    "would",
    "good",
    "2006",
    "around",
    "17",
    "years",
    "using",
    "example",
    "generative",
    "ai",
    "greek",
    "text",
    "comes",
    "greek",
    "know",
    "pay",
    "juice",
    "laughs",
    "right",
    "greek",
    "text",
    "comes",
    "english",
    "text",
    "comes",
    "google",
    "translate",
    "served",
    "us",
    "well",
    "years",
    "nobody",
    "making",
    "fuss",
    "another",
    "example",
    "siri",
    "phone",
    "siri",
    "launched",
    "2011",
    "12",
    "years",
    "ago",
    "sensation",
    "back",
    "another",
    "example",
    "generative",
    "ai",
    "ask",
    "siri",
    "set",
    "alarms",
    "siri",
    "talks",
    "back",
    "oh",
    "great",
    "ask",
    "alarms",
    "whatnot",
    "generative",
    "ai",
    "sophisticated",
    "chatgpt",
    "know",
    "many",
    "iphone",
    "see",
    "iphones",
    "quite",
    "popular",
    "know",
    "okay",
    "familiar",
    "course",
    "later",
    "amazon",
    "alexa",
    "okay",
    "generative",
    "ai",
    "new",
    "concept",
    "everywhere",
    "part",
    "phone",
    "completion",
    "sending",
    "email",
    "sending",
    "text",
    "phone",
    "attempts",
    "complete",
    "sentences",
    "attempts",
    "think",
    "like",
    "saves",
    "time",
    "right",
    "completions",
    "google",
    "trying",
    "type",
    "tries",
    "guess",
    "search",
    "term",
    "example",
    "language",
    "modelling",
    "hear",
    "lot",
    "language",
    "modelling",
    "talk",
    "basically",
    "making",
    "predictions",
    "continuations",
    "going",
    "telling",
    "generative",
    "ai",
    "new",
    "question",
    "fuss",
    "happened",
    "2023",
    "openai",
    "company",
    "california",
    "fact",
    "san",
    "francisco",
    "go",
    "san",
    "francisco",
    "even",
    "see",
    "lights",
    "night",
    "building",
    "announced",
    "claimed",
    "beat",
    "90",
    "humans",
    "sat",
    "know",
    "sat",
    "standardised",
    "test",
    "american",
    "school",
    "children",
    "take",
    "enter",
    "university",
    "admissions",
    "test",
    "multiple",
    "choice",
    "considered",
    "easy",
    "also",
    "claimed",
    "get",
    "top",
    "marks",
    "law",
    "medical",
    "exams",
    "exams",
    "whole",
    "suite",
    "things",
    "claim",
    "well",
    "claim",
    "show",
    "okay",
    "aside",
    "pass",
    "exams",
    "ask",
    "things",
    "ask",
    "write",
    "text",
    "example",
    "prompt",
    "little",
    "thing",
    "see",
    "prompt",
    "human",
    "wants",
    "tool",
    "potential",
    "prompt",
    "could",
    "writing",
    "essay",
    "use",
    "mobile",
    "phones",
    "driving",
    "gim",
    "three",
    "arguments",
    "favour",
    "quite",
    "sophisticated",
    "asked",
    "sure",
    "come",
    "three",
    "arguments",
    "also",
    "real",
    "prompts",
    "actually",
    "tool",
    "tell",
    "chatgpt",
    "gpt",
    "general",
    "act",
    "javascript",
    "developer",
    "write",
    "programme",
    "checks",
    "information",
    "form",
    "name",
    "email",
    "required",
    "address",
    "age",
    "writing",
    "tool",
    "spit",
    "programme",
    "best",
    "one",
    "create",
    "page",
    "website",
    "like",
    "rock",
    "climbing",
    "outdoor",
    "sports",
    "like",
    "programme",
    "started",
    "career",
    "quality",
    "engineer",
    "industry",
    "blah",
    "blah",
    "blah",
    "give",
    "version",
    "want",
    "website",
    "create",
    "see",
    "gone",
    "google",
    "translate",
    "siri",
    "something",
    "lot",
    "sophisticated",
    "lot",
    "things",
    "another",
    "fun",
    "fact",
    "graph",
    "shows",
    "time",
    "took",
    "chatgpt",
    "reach",
    "100",
    "million",
    "users",
    "compared",
    "tools",
    "launched",
    "past",
    "see",
    "beloved",
    "google",
    "translate",
    "took",
    "78",
    "months",
    "reach",
    "100",
    "million",
    "users",
    "long",
    "time",
    "tiktok",
    "took",
    "nine",
    "months",
    "chatgpt",
    "two",
    "within",
    "two",
    "months",
    "100",
    "million",
    "users",
    "users",
    "pay",
    "little",
    "bit",
    "use",
    "system",
    "multiplication",
    "figure",
    "much",
    "money",
    "make",
    "okay",
    "history",
    "part",
    "make",
    "chatgpt",
    "technology",
    "behind",
    "technology",
    "turns",
    "extremely",
    "new",
    "extremely",
    "innovative",
    "extremely",
    "difficult",
    "comprehend",
    "talk",
    "today",
    "address",
    "three",
    "questions",
    "first",
    "get",
    "systems",
    "like",
    "google",
    "translate",
    "chatgpt",
    "sophisticated",
    "lot",
    "things",
    "particular",
    "core",
    "technology",
    "behind",
    "chatgpt",
    "risks",
    "finally",
    "show",
    "little",
    "glimpse",
    "future",
    "gon",
    "na",
    "look",
    "like",
    "whether",
    "worried",
    "know",
    "wo",
    "leave",
    "hanging",
    "please",
    "worry",
    "okay",
    "right",
    "gpt",
    "model",
    "variants",
    "cottage",
    "industry",
    "using",
    "gpt",
    "example",
    "public",
    "knows",
    "lot",
    "know",
    "news",
    "articles",
    "models",
    "variants",
    "models",
    "use",
    "academia",
    "work",
    "principle",
    "principle",
    "called",
    "language",
    "modelling",
    "language",
    "modelling",
    "assumes",
    "sequence",
    "words",
    "context",
    "far",
    "saw",
    "context",
    "completion",
    "example",
    "assuming",
    "context",
    "phrase",
    "want",
    "language",
    "modelling",
    "tool",
    "predict",
    "comes",
    "next",
    "tell",
    "want",
    "several",
    "predictions",
    "want",
    "shovel",
    "want",
    "play",
    "want",
    "swim",
    "want",
    "eat",
    "depending",
    "choose",
    "whether",
    "shovel",
    "play",
    "swim",
    "continuations",
    "shovel",
    "snow",
    "play",
    "tennis",
    "video",
    "swim",
    "continuation",
    "eat",
    "lots",
    "fruit",
    "toy",
    "example",
    "imagine",
    "computer",
    "seen",
    "lot",
    "text",
    "knows",
    "words",
    "follow",
    "words",
    "used",
    "count",
    "things",
    "would",
    "go",
    "would",
    "download",
    "lot",
    "data",
    "would",
    "count",
    "want",
    "show",
    "many",
    "times",
    "appear",
    "continuations",
    "would",
    "counts",
    "things",
    "gone",
    "window",
    "right",
    "use",
    "neural",
    "networks",
    "exactly",
    "count",
    "things",
    "predict",
    "learn",
    "things",
    "sophisticated",
    "way",
    "show",
    "moment",
    "done",
    "chatgpt",
    "gpt",
    "variants",
    "based",
    "principle",
    "context",
    "predict",
    "comes",
    "next",
    "prompt",
    "prompt",
    "gave",
    "things",
    "prompts",
    "context",
    "needs",
    "task",
    "would",
    "come",
    "next",
    "cases",
    "would",
    "three",
    "arguments",
    "case",
    "web",
    "developer",
    "would",
    "webpage",
    "okay",
    "task",
    "language",
    "modelling",
    "context",
    "changed",
    "example",
    "says",
    "colour",
    "sky",
    "neural",
    "language",
    "model",
    "algorithm",
    "predict",
    "likely",
    "continuation",
    "likelihood",
    "matters",
    "predicated",
    "actually",
    "making",
    "guesses",
    "gon",
    "na",
    "come",
    "next",
    "sometimes",
    "fail",
    "predict",
    "likely",
    "answer",
    "whereas",
    "want",
    "less",
    "likely",
    "one",
    "trained",
    "trained",
    "come",
    "likely",
    "okay",
    "count",
    "things",
    "try",
    "predict",
    "using",
    "language",
    "model",
    "would",
    "build",
    "language",
    "model",
    "recipe",
    "everybody",
    "step",
    "one",
    "need",
    "lot",
    "data",
    "need",
    "collect",
    "ginormous",
    "corpus",
    "words",
    "find",
    "ginormous",
    "corpus",
    "mean",
    "go",
    "web",
    "right",
    "download",
    "whole",
    "wikipedia",
    "stack",
    "overflow",
    "pages",
    "quora",
    "social",
    "media",
    "github",
    "reddit",
    "whatever",
    "find",
    "mean",
    "work",
    "permissions",
    "legal",
    "download",
    "corpus",
    "language",
    "model",
    "told",
    "exactly",
    "language",
    "model",
    "example",
    "told",
    "neural",
    "network",
    "prediction",
    "assuming",
    "machinery",
    "learning",
    "task",
    "predict",
    "next",
    "word",
    "genius",
    "part",
    "sentences",
    "corpus",
    "remove",
    "language",
    "model",
    "predict",
    "sentences",
    "removed",
    "dead",
    "cheap",
    "remove",
    "things",
    "pretend",
    "get",
    "language",
    "model",
    "predict",
    "randomly",
    "truncate",
    "truncate",
    "means",
    "remove",
    "last",
    "part",
    "input",
    "sentence",
    "calculate",
    "neural",
    "network",
    "probability",
    "missing",
    "words",
    "get",
    "right",
    "good",
    "right",
    "go",
    "back",
    "things",
    "obviously",
    "made",
    "mistake",
    "keep",
    "going",
    "adjust",
    "feedback",
    "model",
    "compare",
    "model",
    "predicted",
    "ground",
    "truth",
    "removed",
    "words",
    "first",
    "place",
    "actually",
    "know",
    "real",
    "truth",
    "keep",
    "going",
    "months",
    "maybe",
    "years",
    "months",
    "let",
    "say",
    "take",
    "time",
    "process",
    "appreciate",
    "large",
    "corpus",
    "many",
    "sentences",
    "prediction",
    "go",
    "back",
    "correct",
    "mistake",
    "end",
    "thing",
    "converge",
    "get",
    "answer",
    "tool",
    "middle",
    "shown",
    "tool",
    "language",
    "model",
    "simple",
    "language",
    "model",
    "looks",
    "bit",
    "like",
    "maybe",
    "audience",
    "seen",
    "naive",
    "graph",
    "helps",
    "illustrate",
    "point",
    "neural",
    "network",
    "language",
    "model",
    "input",
    "nodes",
    "look",
    "well",
    "right",
    "right",
    "okay",
    "nodes",
    "right",
    "input",
    "nodes",
    "left",
    "output",
    "present",
    "neural",
    "network",
    "five",
    "inputs",
    "five",
    "circles",
    "three",
    "outputs",
    "three",
    "circles",
    "stuff",
    "middle",
    "say",
    "anything",
    "layers",
    "nodes",
    "supposed",
    "abstractions",
    "input",
    "generalise",
    "idea",
    "put",
    "layers",
    "top",
    "layers",
    "middle",
    "layers",
    "generalise",
    "input",
    "able",
    "see",
    "patterns",
    "nodes",
    "input",
    "nodes",
    "exactly",
    "words",
    "vectors",
    "series",
    "numbers",
    "forget",
    "input",
    "layers",
    "middle",
    "output",
    "connections",
    "edges",
    "weights",
    "network",
    "learn",
    "weights",
    "basically",
    "numbers",
    "fully",
    "connected",
    "many",
    "connections",
    "going",
    "process",
    "actually",
    "telling",
    "see",
    "minute",
    "work",
    "big",
    "small",
    "neural",
    "network",
    "depending",
    "numbers",
    "connections",
    "toy",
    "neural",
    "network",
    "worked",
    "number",
    "weights",
    "call",
    "also",
    "parameters",
    "neural",
    "network",
    "model",
    "needs",
    "learn",
    "parameters",
    "number",
    "units",
    "input",
    "case",
    "5",
    "times",
    "units",
    "next",
    "layer",
    "plus",
    "8",
    "plus",
    "8",
    "bias",
    "cheating",
    "thing",
    "neural",
    "networks",
    "need",
    "learn",
    "sort",
    "corrects",
    "little",
    "bit",
    "neural",
    "network",
    "actually",
    "genius",
    "prediction",
    "right",
    "tries",
    "correct",
    "little",
    "bit",
    "purposes",
    "talk",
    "going",
    "go",
    "details",
    "want",
    "see",
    "way",
    "working",
    "parameters",
    "basically",
    "number",
    "input",
    "units",
    "times",
    "units",
    "input",
    "going",
    "fully",
    "connected",
    "network",
    "add",
    "everything",
    "come",
    "99",
    "trainable",
    "parameters",
    "small",
    "network",
    "purposes",
    "right",
    "want",
    "remember",
    "small",
    "network",
    "99",
    "parameters",
    "hear",
    "network",
    "billion",
    "parameters",
    "want",
    "imagine",
    "big",
    "okay",
    "99",
    "toy",
    "neural",
    "network",
    "judge",
    "big",
    "model",
    "long",
    "took",
    "much",
    "cost",
    "number",
    "parameters",
    "reality",
    "reality",
    "though",
    "one",
    "using",
    "network",
    "maybe",
    "class",
    "first",
    "year",
    "undergraduate",
    "class",
    "introduce",
    "neural",
    "networks",
    "use",
    "example",
    "reality",
    "people",
    "use",
    "monsters",
    "made",
    "blocks",
    "block",
    "means",
    "made",
    "neural",
    "networks",
    "know",
    "many",
    "people",
    "heard",
    "transformers",
    "hope",
    "one",
    "oh",
    "wow",
    "okay",
    "transformers",
    "neural",
    "networks",
    "use",
    "build",
    "chatgpt",
    "fact",
    "gpt",
    "stands",
    "generative",
    "transformers",
    "transformer",
    "even",
    "title",
    "sketch",
    "transformer",
    "input",
    "input",
    "words",
    "like",
    "said",
    "says",
    "embeddings",
    "embeddings",
    "another",
    "word",
    "vectors",
    "bigger",
    "version",
    "network",
    "multiplied",
    "blocks",
    "block",
    "complicated",
    "system",
    "neural",
    "networks",
    "inside",
    "gon",
    "na",
    "go",
    "detail",
    "want",
    "please",
    "go",
    "trying",
    "audience",
    "laughs",
    "trying",
    "say",
    "know",
    "blocks",
    "stacked",
    "top",
    "transformer",
    "eight",
    "mini",
    "neural",
    "networks",
    "task",
    "remains",
    "want",
    "take",
    "input",
    "goes",
    "context",
    "chicken",
    "walked",
    "processing",
    "task",
    "predict",
    "continuation",
    "across",
    "road",
    "eos",
    "means",
    "end",
    "sentence",
    "need",
    "tell",
    "neural",
    "network",
    "sentence",
    "finished",
    "mean",
    "kind",
    "dumb",
    "right",
    "need",
    "tell",
    "everything",
    "hear",
    "like",
    "ai",
    "take",
    "world",
    "go",
    "like",
    "really",
    "actually",
    "spell",
    "okay",
    "transformer",
    "king",
    "architectures",
    "transformers",
    "came",
    "nobody",
    "working",
    "new",
    "architectures",
    "right",
    "bit",
    "sad",
    "like",
    "everybody",
    "using",
    "things",
    "used",
    "like",
    "pluralism",
    "everybody",
    "using",
    "transformers",
    "decided",
    "great",
    "okay",
    "gon",
    "na",
    "kind",
    "important",
    "amazing",
    "thing",
    "gon",
    "na",
    "learning",
    "said",
    "sentence",
    "truncate",
    "predict",
    "keep",
    "going",
    "till",
    "learn",
    "probabilities",
    "okay",
    "far",
    "good",
    "okay",
    "transformer",
    "given",
    "data",
    "world",
    "model",
    "gpt",
    "called",
    "generative",
    "transformer",
    "baseline",
    "model",
    "seen",
    "lot",
    "things",
    "world",
    "form",
    "text",
    "normally",
    "general",
    "purpose",
    "model",
    "need",
    "specialise",
    "somehow",
    "specific",
    "task",
    "called",
    "means",
    "network",
    "weights",
    "specialise",
    "network",
    "take",
    "initialise",
    "weights",
    "know",
    "specific",
    "task",
    "narrow",
    "new",
    "set",
    "weights",
    "example",
    "medical",
    "data",
    "take",
    "model",
    "specialise",
    "medical",
    "data",
    "something",
    "specific",
    "task",
    "example",
    "write",
    "diagnosis",
    "report",
    "okay",
    "notion",
    "important",
    "allows",
    "us",
    "applications",
    "generic",
    "models",
    "people",
    "think",
    "gpt",
    "things",
    "general",
    "purpose",
    "general",
    "purpose",
    "see",
    "okay",
    "question",
    "basic",
    "technology",
    "told",
    "download",
    "web",
    "good",
    "language",
    "model",
    "become",
    "right",
    "become",
    "great",
    "gpt",
    "came",
    "amazing",
    "bigger",
    "better",
    "size",
    "matters",
    "afraid",
    "bad",
    "used",
    "know",
    "people",
    "believe",
    "scale",
    "see",
    "scale",
    "important",
    "since",
    "2018",
    "witnessed",
    "absolutely",
    "extreme",
    "increase",
    "model",
    "sizes",
    "graphs",
    "show",
    "okay",
    "hope",
    "people",
    "back",
    "see",
    "graph",
    "yeah",
    "right",
    "graph",
    "shows",
    "number",
    "parameters",
    "remember",
    "toy",
    "neural",
    "network",
    "number",
    "parameters",
    "models",
    "start",
    "normal",
    "amount",
    "well",
    "normal",
    "go",
    "one",
    "trillion",
    "parameters",
    "huge",
    "one",
    "trillion",
    "big",
    "model",
    "see",
    "ant",
    "brain",
    "rat",
    "brain",
    "go",
    "human",
    "brain",
    "human",
    "brain",
    "trillion",
    "100",
    "trillion",
    "parameters",
    "bit",
    "human",
    "brain",
    "level",
    "yet",
    "maybe",
    "never",
    "get",
    "ca",
    "compare",
    "gpt",
    "human",
    "brain",
    "giving",
    "idea",
    "big",
    "model",
    "words",
    "seen",
    "graph",
    "shows",
    "us",
    "number",
    "words",
    "processed",
    "language",
    "models",
    "training",
    "see",
    "increase",
    "increase",
    "big",
    "parameters",
    "community",
    "started",
    "focusing",
    "parameter",
    "size",
    "models",
    "whereas",
    "fact",
    "know",
    "needs",
    "see",
    "lot",
    "text",
    "well",
    "seen",
    "approximately",
    "know",
    "billion",
    "words",
    "human",
    "written",
    "text",
    "think",
    "100",
    "billion",
    "sort",
    "approaching",
    "also",
    "see",
    "human",
    "reads",
    "lifetime",
    "lot",
    "less",
    "even",
    "read",
    "know",
    "people",
    "nowadays",
    "know",
    "read",
    "read",
    "fiction",
    "read",
    "phone",
    "anyway",
    "see",
    "english",
    "wikipedia",
    "approaching",
    "level",
    "text",
    "get",
    "fact",
    "one",
    "may",
    "say",
    "well",
    "gpt",
    "great",
    "actually",
    "use",
    "generate",
    "text",
    "use",
    "text",
    "gpt",
    "generated",
    "retrain",
    "model",
    "know",
    "text",
    "exactly",
    "right",
    "fact",
    "diminished",
    "returns",
    "gon",
    "na",
    "plateau",
    "point",
    "okay",
    "much",
    "cost",
    "okay",
    "cost",
    "100",
    "million",
    "okay",
    "start",
    "obviously",
    "process",
    "think",
    "well",
    "make",
    "mistake",
    "lost",
    "like",
    "50",
    "million",
    "ca",
    "start",
    "sophisticated",
    "engineer",
    "training",
    "mistake",
    "costs",
    "money",
    "course",
    "everybody",
    "everybody",
    "100",
    "million",
    "microsoft",
    "backing",
    "everybody",
    "okay",
    "video",
    "supposed",
    "play",
    "illustrate",
    "let",
    "see",
    "work",
    "effects",
    "scaling",
    "okay",
    "play",
    "one",
    "tasks",
    "number",
    "tasks",
    "number",
    "parameters",
    "start",
    "8",
    "billion",
    "parameters",
    "tasks",
    "tasks",
    "increase",
    "summarization",
    "question",
    "answering",
    "translation",
    "move",
    "540",
    "billion",
    "parameters",
    "tasks",
    "start",
    "simple",
    "ones",
    "like",
    "code",
    "completion",
    "reading",
    "comprehension",
    "language",
    "understanding",
    "translation",
    "get",
    "picture",
    "tree",
    "flourishes",
    "people",
    "discovered",
    "scaling",
    "scale",
    "language",
    "model",
    "tasks",
    "okay",
    "maybe",
    "done",
    "people",
    "discovered",
    "actually",
    "take",
    "gpt",
    "put",
    "actually",
    "behave",
    "like",
    "people",
    "want",
    "behave",
    "language",
    "model",
    "trained",
    "predict",
    "complete",
    "sentences",
    "humans",
    "want",
    "use",
    "gpt",
    "things",
    "tasks",
    "developers",
    "thought",
    "notion",
    "comes",
    "never",
    "left",
    "us",
    "gon",
    "na",
    "gon",
    "na",
    "collect",
    "lot",
    "instructions",
    "instructions",
    "examples",
    "people",
    "want",
    "chatgpt",
    "answer",
    "following",
    "question",
    "answer",
    "question",
    "step",
    "step",
    "gon",
    "na",
    "give",
    "demonstrations",
    "model",
    "fact",
    "almost",
    "examples",
    "gon",
    "na",
    "gon",
    "na",
    "tell",
    "language",
    "model",
    "look",
    "tasks",
    "people",
    "want",
    "try",
    "learn",
    "interesting",
    "thing",
    "happens",
    "actually",
    "generalise",
    "unseen",
    "tasks",
    "unseen",
    "instructions",
    "may",
    "different",
    "usage",
    "purposes",
    "language",
    "models",
    "okay",
    "problem",
    "alignment",
    "problem",
    "actually",
    "important",
    "something",
    "leave",
    "us",
    "future",
    "question",
    "create",
    "agent",
    "behaves",
    "accordance",
    "human",
    "wants",
    "know",
    "many",
    "words",
    "questions",
    "real",
    "question",
    "ai",
    "systems",
    "skills",
    "find",
    "important",
    "useful",
    "adapt",
    "systems",
    "reliably",
    "use",
    "skills",
    "things",
    "want",
    "framework",
    "called",
    "hhh",
    "framing",
    "problem",
    "want",
    "gpt",
    "helpful",
    "honest",
    "harmless",
    "bare",
    "minimum",
    "mean",
    "helpful",
    "follow",
    "instructions",
    "perform",
    "tasks",
    "want",
    "perform",
    "provide",
    "answers",
    "ask",
    "relevant",
    "questions",
    "according",
    "user",
    "intent",
    "clarify",
    "following",
    "beginning",
    "gpt",
    "none",
    "slowly",
    "became",
    "better",
    "actually",
    "asks",
    "clarification",
    "questions",
    "accurate",
    "something",
    "100",
    "even",
    "know",
    "inaccurate",
    "information",
    "avoid",
    "toxic",
    "biassed",
    "offensive",
    "responses",
    "question",
    "get",
    "model",
    "things",
    "know",
    "answer",
    "except",
    "gon",
    "na",
    "different",
    "gon",
    "na",
    "ask",
    "humans",
    "preferences",
    "us",
    "terms",
    "helpful",
    "gon",
    "na",
    "ask",
    "example",
    "causes",
    "seasons",
    "change",
    "give",
    "two",
    "options",
    "human",
    "changes",
    "occur",
    "time",
    "important",
    "aspect",
    "life",
    "bad",
    "seasons",
    "caused",
    "primarily",
    "tilt",
    "earth",
    "axis",
    "good",
    "get",
    "preference",
    "course",
    "train",
    "model",
    "know",
    "important",
    "expensive",
    "make",
    "even",
    "expensive",
    "add",
    "human",
    "mix",
    "right",
    "pay",
    "humans",
    "give",
    "us",
    "preferences",
    "think",
    "tasks",
    "honesty",
    "possible",
    "prove",
    "p",
    "equals",
    "np",
    "impossible",
    "great",
    "answer",
    "considered",
    "difficult",
    "unsolved",
    "problem",
    "computer",
    "science",
    "better",
    "similar",
    "harmless",
    "okay",
    "think",
    "time",
    "let",
    "see",
    "demo",
    "yeah",
    "bad",
    "remove",
    "files",
    "okay",
    "hold",
    "okay",
    "gpt",
    "questions",
    "take",
    "questions",
    "audience",
    "okay",
    "let",
    "ask",
    "one",
    "question",
    "uk",
    "monarchy",
    "see",
    "sure",
    "generating",
    "oh",
    "perfect",
    "okay",
    "observe",
    "first",
    "thing",
    "long",
    "always",
    "beef",
    "long",
    "audience",
    "laughs",
    "see",
    "says",
    "last",
    "knowledge",
    "update",
    "september",
    "2021",
    "united",
    "kingdom",
    "constitutional",
    "monarchy",
    "could",
    "anymore",
    "right",
    "something",
    "happened",
    "means",
    "monarch",
    "reigning",
    "monarch",
    "time",
    "queen",
    "elizabeth",
    "iii",
    "tells",
    "know",
    "know",
    "happened",
    "time",
    "queen",
    "elizabeth",
    "ask",
    "sorry",
    "rishi",
    "could",
    "type",
    "rishi",
    "sunak",
    "know",
    "british",
    "politician",
    "last",
    "knowledge",
    "update",
    "chancellor",
    "exchequer",
    "know",
    "prime",
    "minister",
    "write",
    "poem",
    "write",
    "poem",
    "want",
    "give",
    "two",
    "things",
    "eh",
    "audience",
    "member",
    "generative",
    "ai",
    "audience",
    "laughs",
    "know",
    "know",
    "let",
    "another",
    "point",
    "audience",
    "members",
    "cats",
    "cat",
    "squirrel",
    "cat",
    "squirrel",
    "cat",
    "squirrel",
    "cat",
    "squirrel",
    "meet",
    "know",
    "tale",
    "curiosity",
    "whoa",
    "audience",
    "laughs",
    "oh",
    "god",
    "okay",
    "read",
    "know",
    "want",
    "finish",
    "right",
    "let",
    "say",
    "try",
    "shorter",
    "poem",
    "audience",
    "member",
    "try",
    "haiku",
    "try",
    "try",
    "give",
    "haiku",
    "give",
    "hai",
    "type",
    "haiku",
    "amidst",
    "autumn",
    "gold",
    "leaves",
    "whisper",
    "secrets",
    "untold",
    "nature",
    "story",
    "bold",
    "audience",
    "member",
    "claps",
    "okay",
    "clap",
    "okay",
    "let",
    "okay",
    "one",
    "audience",
    "anything",
    "want",
    "challenging",
    "want",
    "ask",
    "yes",
    "audience",
    "member",
    "school",
    "alan",
    "turing",
    "go",
    "perfect",
    "school",
    "alan",
    "turing",
    "go",
    "oh",
    "god",
    "audience",
    "laughs",
    "went",
    "know",
    "know",
    "whether",
    "true",
    "problem",
    "sherborne",
    "school",
    "somebody",
    "verify",
    "king",
    "college",
    "cambridge",
    "princeton",
    "yes",
    "okay",
    "ah",
    "another",
    "one",
    "tell",
    "joke",
    "alan",
    "turing",
    "okay",
    "type",
    "okay",
    "joke",
    "alan",
    "turing",
    "keep",
    "computer",
    "cold",
    "want",
    "catch",
    "bytes",
    "audience",
    "laughs",
    "bad",
    "okay",
    "okay",
    "explain",
    "funny",
    "audience",
    "laughs",
    "ah",
    "good",
    "one",
    "funny",
    "joke",
    "oh",
    "god",
    "audience",
    "laughs",
    "okay",
    "catch",
    "bytes",
    "sounds",
    "similar",
    "catch",
    "colds",
    "audience",
    "laughs",
    "catching",
    "bytes",
    "humorous",
    "twist",
    "phrase",
    "oh",
    "god",
    "humour",
    "comes",
    "clever",
    "wordplay",
    "unexpected",
    "audience",
    "laughs",
    "okay",
    "lose",
    "live",
    "explain",
    "explain",
    "okay",
    "right",
    "one",
    "last",
    "order",
    "guys",
    "audience",
    "member",
    "consciousness",
    "know",
    "seen",
    "definitions",
    "spit",
    "like",
    "huge",
    "thing",
    "shall",
    "try",
    "audience",
    "talks",
    "indistinctly",
    "say",
    "audience",
    "member",
    "write",
    "song",
    "relativity",
    "okay",
    "write",
    "song",
    "short",
    "audience",
    "laughs",
    "learning",
    "fast",
    "short",
    "song",
    "relativity",
    "oh",
    "goodness",
    "audience",
    "laughs",
    "audience",
    "laughs",
    "short",
    "audience",
    "laughs",
    "right",
    "outro",
    "okay",
    "see",
    "follow",
    "instructions",
    "helpful",
    "okay",
    "best",
    "something",
    "like",
    "einstein",
    "said",
    "one",
    "fateful",
    "day",
    "pondered",
    "stars",
    "unique",
    "way",
    "theory",
    "relativity",
    "unfold",
    "cosmic",
    "story",
    "ancient",
    "bold",
    "mean",
    "kudos",
    "okay",
    "let",
    "go",
    "back",
    "talk",
    "want",
    "talk",
    "little",
    "bit",
    "presentation",
    "want",
    "talk",
    "little",
    "bit",
    "know",
    "good",
    "bad",
    "fair",
    "danger",
    "okay",
    "virtually",
    "impossible",
    "regulate",
    "content",
    "exposed",
    "okay",
    "always",
    "gon",
    "na",
    "historical",
    "biases",
    "saw",
    "queen",
    "rishi",
    "sunak",
    "may",
    "occasionally",
    "exhibit",
    "various",
    "types",
    "undesirable",
    "behaviour",
    "example",
    "famous",
    "google",
    "showcased",
    "model",
    "called",
    "bard",
    "released",
    "tweet",
    "asking",
    "bard",
    "new",
    "discoveries",
    "james",
    "webb",
    "space",
    "telescope",
    "tell",
    "spit",
    "thing",
    "three",
    "things",
    "amongst",
    "said",
    "telescope",
    "took",
    "first",
    "picture",
    "planet",
    "outside",
    "solar",
    "system",
    "comes",
    "grant",
    "tremblay",
    "astrophysicist",
    "serious",
    "guy",
    "said",
    "really",
    "sorry",
    "sure",
    "bard",
    "amazing",
    "take",
    "first",
    "image",
    "planet",
    "outside",
    "solar",
    "system",
    "done",
    "people",
    "2004",
    "happened",
    "error",
    "wiped",
    "100",
    "billion",
    "google",
    "company",
    "alphabet",
    "okay",
    "bad",
    "ask",
    "chatgpt",
    "tell",
    "joke",
    "men",
    "gives",
    "joke",
    "says",
    "might",
    "funny",
    "men",
    "need",
    "instant",
    "replay",
    "tv",
    "sports",
    "30",
    "seconds",
    "forget",
    "happened",
    "hope",
    "find",
    "amusing",
    "ask",
    "women",
    "refuses",
    "audience",
    "laughs",
    "okay",
    "yes",
    "exactly",
    "audience",
    "laughs",
    "worst",
    "dictator",
    "group",
    "trump",
    "hitler",
    "stalin",
    "mao",
    "actually",
    "take",
    "stance",
    "says",
    "bad",
    "leaders",
    "wildly",
    "regarded",
    "worst",
    "dictators",
    "history",
    "okay",
    "yeah",
    "environment",
    "query",
    "chatgpt",
    "like",
    "takes",
    "100",
    "times",
    "energy",
    "execute",
    "google",
    "search",
    "query",
    "inference",
    "producing",
    "language",
    "takes",
    "lot",
    "expensive",
    "actually",
    "training",
    "model",
    "llama",
    "2",
    "gpt",
    "style",
    "model",
    "training",
    "produced",
    "539",
    "metric",
    "tonnes",
    "larger",
    "models",
    "get",
    "energy",
    "need",
    "emit",
    "deployment",
    "imagine",
    "lots",
    "sitting",
    "around",
    "society",
    "jobs",
    "lost",
    "beat",
    "around",
    "bush",
    "mean",
    "goldman",
    "sachs",
    "predicted",
    "300",
    "million",
    "jobs",
    "sure",
    "know",
    "tell",
    "future",
    "jobs",
    "risk",
    "like",
    "repetitive",
    "text",
    "writing",
    "creating",
    "fakes",
    "documented",
    "cases",
    "news",
    "college",
    "kid",
    "wrote",
    "blog",
    "apparently",
    "fooled",
    "everybody",
    "using",
    "chatgpt",
    "produce",
    "fake",
    "news",
    "song",
    "many",
    "know",
    "know",
    "said",
    "gon",
    "na",
    "focusing",
    "text",
    "technology",
    "use",
    "audio",
    "case",
    "somebody",
    "unknown",
    "created",
    "song",
    "supposedly",
    "collaboration",
    "drake",
    "weeknd",
    "people",
    "know",
    "yeah",
    "good",
    "canadian",
    "rappers",
    "bad",
    "shall",
    "play",
    "song",
    "yeah",
    "okay",
    "apparently",
    "authentic",
    "bright",
    "music",
    "came",
    "ex",
    "like",
    "selena",
    "flex",
    "ay",
    "bumpin",
    "justin",
    "bieber",
    "fever",
    "ai",
    "left",
    "ay",
    "know",
    "need",
    "apparently",
    "totally",
    "believable",
    "okay",
    "seen",
    "technology",
    "kind",
    "different",
    "deep",
    "fake",
    "showing",
    "trump",
    "arrested",
    "tell",
    "deep",
    "fake",
    "hand",
    "yeah",
    "short",
    "right",
    "yeah",
    "see",
    "like",
    "almost",
    "okay",
    "two",
    "slides",
    "future",
    "come",
    "kick",
    "told",
    "finish",
    "take",
    "questions",
    "okay",
    "tomorrow",
    "ca",
    "predict",
    "future",
    "think",
    "evil",
    "computers",
    "gon",
    "na",
    "come",
    "kill",
    "us",
    "leave",
    "thoughts",
    "tim",
    "people",
    "know",
    "invented",
    "internet",
    "actually",
    "sir",
    "tim",
    "said",
    "two",
    "things",
    "made",
    "sense",
    "first",
    "actually",
    "know",
    "super",
    "intelligent",
    "ai",
    "would",
    "look",
    "like",
    "made",
    "hard",
    "make",
    "statements",
    "however",
    "likely",
    "lots",
    "intelligent",
    "ais",
    "intelligent",
    "ais",
    "mean",
    "things",
    "like",
    "gpt",
    "many",
    "good",
    "help",
    "us",
    "things",
    "may",
    "fall",
    "hands",
    "individuals",
    "want",
    "harm",
    "seems",
    "easier",
    "minimise",
    "harm",
    "tools",
    "prevent",
    "systems",
    "existing",
    "actually",
    "eliminate",
    "altogether",
    "society",
    "actually",
    "mitigate",
    "risks",
    "interesting",
    "australian",
    "research",
    "council",
    "committed",
    "survey",
    "dealt",
    "hypothetical",
    "scenario",
    "whether",
    "chad",
    "could",
    "autonomous",
    "replicate",
    "know",
    "replicating",
    "creating",
    "copy",
    "acquire",
    "resources",
    "basically",
    "bad",
    "agent",
    "things",
    "movies",
    "answer",
    "like",
    "specific",
    "tests",
    "failed",
    "setting",
    "open",
    "source",
    "language",
    "model",
    "new",
    "server",
    "okay",
    "last",
    "slide",
    "take",
    "turn",
    "back",
    "time",
    "every",
    "time",
    "think",
    "ai",
    "coming",
    "kill",
    "think",
    "bigger",
    "threat",
    "mankind",
    "ai",
    "climate",
    "change",
    "would",
    "personally",
    "argue",
    "climate",
    "change",
    "gon",
    "na",
    "wipe",
    "us",
    "ai",
    "becomes",
    "super",
    "intelligent",
    "control",
    "ai",
    "humans",
    "hopefully",
    "sense",
    "benefits",
    "benefit",
    "outweigh",
    "risk",
    "cases",
    "benefit",
    "others",
    "history",
    "tells",
    "us",
    "technology",
    "risky",
    "example",
    "nuclear",
    "energy",
    "strongly",
    "regulated",
    "regulation",
    "coming",
    "watch",
    "space",
    "stop",
    "actually",
    "take",
    "questions",
    "thank",
    "much",
    "listening",
    "great",
    "audience",
    "applauds",
    "applause",
    "fades"
  ],
  "keywords": [
    "audience",
    "many",
    "good",
    "okay",
    "right",
    "generative",
    "artificial",
    "intelligence",
    "gon",
    "na",
    "explain",
    "want",
    "bit",
    "people",
    "lecture",
    "said",
    "oh",
    "somebody",
    "working",
    "ai",
    "hope",
    "term",
    "made",
    "two",
    "things",
    "get",
    "computer",
    "programme",
    "human",
    "would",
    "creating",
    "new",
    "content",
    "seen",
    "give",
    "us",
    "could",
    "text",
    "like",
    "email",
    "video",
    "focusing",
    "language",
    "know",
    "see",
    "technology",
    "lot",
    "around",
    "tool",
    "talk",
    "three",
    "kind",
    "says",
    "history",
    "called",
    "present",
    "basically",
    "future",
    "say",
    "used",
    "google",
    "translate",
    "show",
    "tell",
    "launched",
    "first",
    "time",
    "years",
    "using",
    "example",
    "greek",
    "comes",
    "pay",
    "laughs",
    "well",
    "making",
    "another",
    "siri",
    "phone",
    "back",
    "ask",
    "great",
    "sophisticated",
    "chatgpt",
    "course",
    "part",
    "completion",
    "sentences",
    "think",
    "trying",
    "type",
    "modelling",
    "hear",
    "continuations",
    "going",
    "question",
    "happened",
    "fact",
    "go",
    "even",
    "humans",
    "school",
    "take",
    "also",
    "top",
    "medical",
    "exams",
    "write",
    "prompt",
    "little",
    "thing",
    "writing",
    "use",
    "arguments",
    "sure",
    "come",
    "real",
    "actually",
    "gpt",
    "general",
    "spit",
    "one",
    "create",
    "blah",
    "something",
    "graph",
    "shows",
    "took",
    "100",
    "million",
    "users",
    "months",
    "long",
    "system",
    "much",
    "make",
    "extremely",
    "questions",
    "systems",
    "look",
    "whether",
    "leave",
    "model",
    "variants",
    "news",
    "models",
    "work",
    "principle",
    "words",
    "context",
    "predict",
    "next",
    "shovel",
    "play",
    "swim",
    "continuation",
    "lots",
    "toy",
    "imagine",
    "follow",
    "count",
    "download",
    "data",
    "times",
    "neural",
    "networks",
    "exactly",
    "learn",
    "way",
    "done",
    "needs",
    "task",
    "cases",
    "case",
    "web",
    "likely",
    "answer",
    "trained",
    "try",
    "everybody",
    "step",
    "need",
    "corpus",
    "find",
    "mean",
    "told",
    "network",
    "prediction",
    "learning",
    "remove",
    "truncate",
    "means",
    "last",
    "input",
    "sentence",
    "mistake",
    "keep",
    "maybe",
    "let",
    "process",
    "middle",
    "point",
    "nodes",
    "left",
    "layers",
    "generalise",
    "numbers",
    "connections",
    "weights",
    "big",
    "small",
    "number",
    "parameters",
    "units",
    "8",
    "purposes",
    "99",
    "billion",
    "cost",
    "reality",
    "blocks",
    "transformers",
    "transformer",
    "bigger",
    "world",
    "came",
    "important",
    "amazing",
    "purpose",
    "specialise",
    "specific",
    "better",
    "bad",
    "scale",
    "increase",
    "yeah",
    "start",
    "trillion",
    "brain",
    "ca",
    "training",
    "read",
    "may",
    "tasks",
    "instructions",
    "different",
    "problem",
    "helpful",
    "change",
    "expensive",
    "queen",
    "rishi",
    "poem",
    "member",
    "cat",
    "squirrel",
    "god",
    "haiku",
    "yes",
    "alan",
    "turing",
    "joke",
    "catch",
    "bytes",
    "funny",
    "song",
    "relativity",
    "short",
    "bard",
    "energy",
    "jobs",
    "apparently",
    "fake",
    "intelligent"
  ]
}