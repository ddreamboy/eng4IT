{
  "text": "This comprehensive course will teach you the foundations of machine learning and deep learning\nusing PyTorch. PyTorch is a machine learning framework written in Python. You'll learn machine\nlearning by writing PyTorch code. So when in doubt, run the provided code and experiment.\nYour teacher for this course is Daniel Bourke. Daniel is a machine learning engineer and popular\ncourse creator. So enjoy the course and don't watch the whole thing in one sitting.\nHello, welcome to the video. It's quite a big one. But if you've come here to learn machine\nlearning and deep learning and PyTorch code, well, you're in the right place. Now, this video and\ntutorial is focused for beginners who have got about three to six months of Python coding experience.\nSo we're going to cover a whole bunch of important machine learning concepts\nby writing PyTorch code. Now, if you get stuck, you can leave a comment below or post on the course\nGitHub discussions page. And on GitHub is where you'll be able to find all the materials that we cover,\nas well as on learn pytorch.io. There's an online readable book version of this course there.\nBut if you finish this video and you find that, hey, I would still like to learn more PyTorch.\nI mean, you can't really cover all the PyTorch in a day that video titles just apply on words of\nthe length of video. That's an aside. There is five more chapters available at learn pytorch.io,\ncovering everything from transfer learning to model deployment to experiment tracking.\nAnd all the videos to go with those are available at zero to mastery.io. But that's enough for me.\nHaving machine learning and I'll see you inside.\nHello, my name is Daniel and welcome to the deep learning with\nPyTorch course. Now, that was too good not to watch twice. Welcome to the deep learning with\ncools at fire PyTorch course. So this is very exciting. Are you going to see that animation\nquite a bit because, I mean, it's fun and PyTorch's symbol is a flame because of torch.\nBut let's get into it. So naturally, if you've come to this course, you might have already\nresearched what is deep learning, but we're going to cover it quite briefly.\nAnd just in the sense of how much you need to know for this course, because we're going to be\nmore focused on, rather than just definitions, we're going to be focused on getting practical\nand seeing things happen. So if we define what machine learning is, because as we'll see in a\nsecond, deep learning is a subset of machine learning. Machine learning is turning things\ndata, which can be almost anything, images, text, tables of numbers, video, audio files,\nalmost anything can be classified as data into numbers. So computers love numbers,\nand then finding patterns in those numbers. Now, how do we find those patterns? Well,\nthe computer does this part specifically a machine learning algorithm or a deep learning\nalgorithm of things that we're going to be building in this course. How? Code and math. Now,\nthis course is code focused. I want to stress that before you get into it. We're focused on\nwriting code. Now, behind the scenes, that code is going to trigger some math to find patterns in\nthose numbers. If you would like to deep dive into the math behind the code, I'm going to be\nlinking extra resources for that. However, we're going to be getting hands on and writing lots of\ncode to do lots of this. And so if we keep going to break things down a little bit more,\nmachine learning versus deep learning, if we have this giant bubble here of artificial\nintelligence, you might have seen something similar like this on the internet. I've just\ncopied that and put it into pretty colors for this course. So you've got this overarching\nbig bubble of the topic of artificial intelligence, which you could define as, again, almost anything\nyou want. Then typically, there's a subset within artificial intelligence, which is known as machine\nlearning, which is quite a broad topic. And then within machine learning, you have another topic\ncalled deep learning. And so that's what we're going to be focused on working with PyTorch,\nwriting deep learning code. But again, you could use PyTorch for a lot of different machine\nlearning things. And truth be told, I kind of use these two terms interchangeably. Yes, ML is the\nbroader topic and deep learning is a bit more nuanced. But again, if you want to form your\nown definitions of these, I'd highly encourage you to do so. This course is more focused on,\nrather than defining what things are, is seeing how they work. So this is what we're focused on doing.\nJust to break things down, if you're familiar with the fundamentals of machine learning,\nyou probably understand this paradigm, but we're going to just rehash on it anyway. So if we\nconsider traditional programming, let's say you'd like to write a computer program that's enabled to,\nor has the ability to reproduce your grandmother's favorite or famous roast chicken dish. And so we\nmight have some inputs here, which are some beautiful vegetables, a chicken that you've raised on the\nfarm. You might write down some rules. This could be your program, cut the vegetables, season the\nchicken, preheat the oven, cook the chicken for 30 minutes and add vegetables. Now, it might not\nbe this simple, or it might actually be because your Sicilian grandmother is a great cook. So she's\nput things into an art now and can just do it step by step. And then those inputs combined with\nthose rules makes this beautiful roast chicken dish. So that's traditional programming. Now,\na machine learning algorithm typically takes some inputs and some desired outputs and then\nfigures out the rules. So the patterns between the inputs and the outputs. So where in traditional\nprogram, we had to hand write all of these rules, the ideal machine learning algorithm will figure\nout this bridge between our inputs and our idealized output. Now, in the machine learning sense, this\nis typically described as supervised learning, because you will have some kind of input with\nsome kind of output, also known as features, and also known as labels. And the machine learning\nalgorithm's job is to figure out the relationships between the inputs or the features and the outputs\nor the label. So if we wanted to write a machine learning algorithm to figure out our Sicilian\ngrandmother's famous roast chicken dish, we would probably gather a bunch of inputs of ingredients\nsuch as these delicious vegetables and chicken, and then have a whole bunch of outputs of the\nfinished product and see if our algorithm can figure out what we should do to go from these\ninputs to output. So that's almost enough to cover of the difference between traditional programming\nand machine learning as far as definitions go. We're going to get hands on encoding these sort\nof algorithms throughout the course. For now, let's go to the next video and ask the question,\nwhy use machine learning or deep learning? And actually, before we get there, I'd like you to\nthink about that. So going back to what we just saw, the paradigm between traditional programming\nand machine learning, why would you want to use machine learning algorithms rather than\ntraditional programming? So if you had to write all these rules, could that get cumbersome?\nSo have a think about it and we'll cover it in the next video.\nWelcome back. So in the last video, we covered briefly the difference between\ntraditional programming and machine learning. And again, I don't want to spend too much time\non definitions. I'd rather you see this in practice. And I left you with the question,\nwhy would you want to use machine learning or deep learning? Well, let's think of a good reason.\nWhy not? I mean, if we had to write all those handwritten rules to reproduce Alsace and grandmother's\nroast chicken dish all the time, that would be quite cumbersome, right? Well, let's draw a line\non that. Why not? What's a better reason? And kind of what we just said, right? For a complex\nproblem, can you think of all the rules? So let's imagine we're trying to build a self-driving car.\nNow, if you've learned to drive, you've probably done so in maybe 20 hours, 100 hours. But now,\nI'll give you a task of writing down every single rule about driving. How do you back out of your\ndriveway? How do you turn left and go down the street? How do you park a reverse park? How do\nyou stop at an intersection? How do you know how fast to go somewhere? So we just listed half a\ndozen rules. But you could probably go a fair few more. You might get into the thousands.\nAnd so for a complex problem, such as driving, can you think of all the rules? Well, probably not.\nSo that's where machine learning and deep learning come in to help. And so this is a beautiful comment\nI like to share with you on one of my YouTube videos is my 2020 machine learning roadmap.\nAnd this is from Yashawing. I'm probably going to mispronounce this if I even try to.\nBut Yashawing says, I think you can use ML. So ML is machine learning. I'm going to use that\na lot throughout the course, by the way. ML is machine learning, just so you know.\nFor literally anything, as long as you can convert it into numbers, ah, that's what we said before,\nmachine learning is turning something into computer readable numbers. And then programming it to find\npatterns, except with a machine learning algorithm, typically we write the algorithm and it finds\nthe patterns, not us. And so literally it could be anything, any input or output from the universe.\nThat's pretty darn cool about machine learning, right? But should you always use it just because\nit could be used for anything? Well, I'd like to also introduce you to Google's number one rule\nof machine learning. Now, if you can build a simple rule based system such as the step of five\nrules that we had to map the ingredients to our Sicilian grandmothers roast chicken dish,\nif you can write just five steps to do that, that's going to work every time, well, you should\nprobably do that. So if you can build a simple rule based system that doesn't require machine\nlearning, do that. And of course, maybe it's not so very simple, but maybe you can just write some\nrules to solve the problem that you're working on. And this is from a wise software engineer,\nwhich is, I kind of hinted at it before, rule one of Google's machine learning handbook. Now,\nI'm going to highly recommend you read through that, but we're not going to go through that in\nthis video. So check that out. You can Google that otherwise the links will be where you get links.\nSo just keep that in mind, although machine learning is very powerful and very fun and very\nexcited, it doesn't mean that you should always use it. I know this is quite the thing to be saying\nat the start of a deep learning machine learning course, but I just want you to keep in mind,\nsimple rule based systems are still good. Machine learning isn't a solve all for everything.\nNow, let's have a look at what deep learning is good for, but I'm going to leave you on a\nclip hammock because we're going to check this out in the next video. See you soon.\nIn the last video, we familiarized ourselves with Google's number one rule of machine learning,\nwhich is basically if you don't need it, don't use it. And with that in mind,\nwhat should we actually be looking to use machine learning or deep learning for?\nWell, problems with long lists of rules. So when the traditional approach fails to\nremember the traditional approach is you have some sort of data input, you write a list of rules for\nthat data to be manipulated in some way, shape, or form, and then you have the outputs that you\nknow. But if you have a long, long list of rules, like the rules of driving a car, which could be\nhundreds, could be thousands, could be millions, who knows, that's where machine learning and\ndeep learning may help. And it kind of is at the moment in the world of self-driving cars,\nmachine learning and deep learning are the state of the art approach.\nContinually changing environments. So whatever the benefits of deep learning is that it can\nkeep learning if it needs to. And so it can adapt and learn to new scenarios. So if you update the\ndata that your model was trained on, it can adjust to new different kinds of data in the future.\nSo similarly to if you are driving a car, you might know your own neighborhood very well.\nBut then when you go to somewhere you haven't been before, sure you can draw on the foundations\nof what you know, but you're going to have to adapt. How fast should you go? Where should you\nstop? Where should you park? These kinds of things. So with problems with long lists of rules,\nor continually changing environments, or if you had a large, large data set. And so this is where\ndeep learning is flourishing in the world of technology. So let's give an example. One of my\nfavorites is the food 101 data set, which you can search for online, which is images of 101\ndifferent kinds of foods. Now we briefly looked at what a rule list might look like for cooking\nyour grandmother's famous Sicilian roast chicken dish. But can you imagine if you wanted to build\nan app that could take photos of different food, how long your list of rules would be to differentiate\n101 different foods? It'd be so long. You need rule sets for every single one. Let's just take\none food, for example. How do you write a program to tell what a banana looks like? I mean you'd\nhave to code what a banana looks like, but not only a banana, what everything that isn't a banana\nlooks like. So keep this in mind. What deep learning is good for? Problems with long lists of rules,\ncontinually changing environments, or discovering insights within large collections of data.\nNow, what deep learning is not good for? And I'm going to write typically here because,\nagain, this is problem specific. Deep learning is quite powerful these days and things might\nchange in the future. So keep an open mind, if there's anything about this course, it's not for\nme to tell you exactly what's what. It's for me to spark a curiosity into you to figure out what's\nwhat, or even better yet, what's not what. So when you need explainability, as we'll see,\nthe patterns learned by a deep learning model, which is lots of numbers, called weights and biases,\nwe'll have a look at that later on, are typically uninterpretable by a human. So some of the times\ndeep learning models can have a million, 10 million, 100 million, a billion, some models are getting\ninto the trillions of parameters. When I say parameters, I mean numbers or patterns in data.\nRemember, machine learning is turning things into numbers and then writing a machine learning model\nto find patterns in those numbers. So sometimes those patterns themselves can be lists of numbers\nthat are in the millions. And so can you imagine looking at a list of numbers that has a million\ndifferent things going on? That's going to be quite hard. I find it hard to understand\nthree or four numbers, let alone a million. And when the traditional approach is a better option,\nagain, this is Google's rule number one of machine learning. If you can do what you need to do with\na simple rule based system, well, maybe you don't need to use machine learning or deep learning.\nAgain, I'm going to use the deep learning machine learning terms interchangeably.\nI'm not too concerned with definitions. You can form your own definitions, but just so you know,\nfrom my perspective, ML and deep learning are quite similar. When arrows are unacceptable.\nSo since the outputs of a deep learning model aren't always predictable, we'll see that deep\nlearning models are probabilistic. That means they're when they predict something, they're making a\nprobabilistic bet on it. Whereas in a rule based system, you kind of know what the outputs are\ngoing to be every single time. So if you can't have errors based on probabilistic errors,\nwell, then you probably shouldn't use deep learning and you'd like to go back to a simple rule based\nsystem. And then finally, when you don't have much data, so deep learning models usually require a\nfairly large amount of data to produce great results. However, there's a caveat here, you know,\nat the start, I said typically, we're going to see some techniques of how to get great results\nwithout huge amounts of data. And again, I wrote typically here because there are techniques,\nyou can just research deep learning explainability. You're going to find a whole bunch of stuff.\nYou can look up examples of when machine learning versus deep learning. And then when arrows are\nunacceptable, again, there are ways to make your model reproducible. So it predicts you know what's\ngoing to come out. So we do a lot of testing to verify this as well. And so what's next? Ah,\nwe've got machine learning versus deep learning, and we're going to have a look at some different\nproblem spaces in a second, and mainly breaking down in terms of what kind of data you have.\nNot going to do this now prevent this video from getting too long. We'll cover all these\ncolorful beautiful pictures in the next video. Welcome back. So in the last video, we covered a\nfew things of what deep learning is good for and what deep learning is typically not good for.\nSo let's dive in to a little more of a comparison of machine learning versus deep learning. Again,\nI'm going to be using these terms quite interchangeably. But there are some specific things that\ntypically you want traditional style of machine learning techniques versus deep learning. However,\nthis is constantly changing. So again, I'm not talking in absolutes here. I'm more just talking\nin general. And I'll leave it to you to use your own curiosity to research the specific\ndifferences between these two. But typically, for machine learning, like the traditional style of\nalgorithms, although they are still machine learning algorithms, which is kind of a little\nbit confusing where deep learning and machine learning differ is you want to use traditional\nmachine learning algorithms on structured data. So if you have tables of numbers, this is what I\nmean by structured rows and columns, structured data. And possibly one of the best algorithms\nfor this type of data is a gradient boosted machine, such as xg boost. This is an algorithm\nthat you'll see in a lot of data science competitions, and also used in production settings. When I\nsay production settings, I mean, applications that you may interact with on the internet,\nor use on a day to day. So that's production. xg boost is typically the favorite algorithm for\nthese kinds of situations. So again, if you have structured data, you might look into xg boost\nrather than building a deep learning algorithm. But again, the rules aren't set in stone. That's\nwhere deep learning and machine learning is kind of an art kind of a science is that sometimes\nxg boost is the best for structured data, but there might be exceptions to the rule. But for deep\nlearning, it is typically better for unstructured data. And what I mean by that is data that's kind\nof all over the place. It's not in your nice, standardized rows and columns. So say you had\nnatural language such as this tweet by this person, whose name is quite similar to mine,\nand has the same Twitter account as me. Oh, maybe I wrote that. How do I learn machine learning? What\nyou need to hear? Learn Python, learn math, start probability, software engineering, build.\nWhat you need to do? Google it, go down the rabbit hole, resurfacing six to nine months,\nand ring assess. I like that. Or if you had a whole bunch of texts such as the definition for\ndeep learning on Wikipedia, again, this is the reason why I'm not covering as many definitions\nin this course is because look how simple you can look these things up. Wikipedia is going to\nbe able to define deep learning far better than what I can. I'm more focused on just getting involved\nin working hands on with this stuff than defining what it is. And then we have\nimages. If we wanted to build a burger, take a photo app thing, you would work with image data,\nwhich doesn't really have much of a structure. Although we'll see that there are ways for deep\nlearning that we can turn this kind of data to have some sort of structure through the beauty\nof a tensor. And then we might have audio files such as if you were talking to your voice assistant.\nI'm not going to say one because a whole bunch of my devices might go crazy if I say the name of\nmy voice assistant, which rhymes with I'm not even going to say that out loud. And so typically,\nfor unstructured data, you'll want to use a neural network of some kind. So structured data,\ngradient boosted machine, or a random forest, or a tree based algorithm, such as extra boost,\nand unstructured data, neural networks. So let's keep going. Let's have a look at some of the\ncommon algorithms that you might use for structured data, machine learning versus unstructured data,\ndeep learning. So random forest is one of my favorites, gradient boosted models,\nnative base nearest neighbor, support vector machine, SVM, and then many more. But since\nthe advent of deep learning, these are often referred to as shallow algorithms. So deep learning,\nwhy is it called deep learning? Well, as we'll see is that it can have many different layers\nof algorithm, you might have an input layer, 100 layers in the middle, and then an output layer.\nBut we'll get hands on with this later on. And so common algorithms for deep learning and neural\nnetworks, fully connected neural network, convolutional neural network, recurrent neural network,\ntransformers have taken over over the past couple years, and of course, many more. And the beautiful\nthing about deep learning and neural networks is is almost as many problems that it can be applied\nto is as many different ways that you can construct them. So this is why I'm putting all these\ndot points on the page. And I can understand if you haven't had much experience of machine\nlearning or deep learning, this can be a whole bunch of information overload. But good news is\nwhat we're going to be focused on building with PyTorch is neural networks, fully connected neural\nnetworks and convolutional neural networks, the foundation of deep learning. But the excellent\nthing is, the exciting thing is, is that if we learn these foundational building blocks,\nwe can get into these other styles of things here. And again, part art, part science of machine\nlearning and deep learning is depending on how you represent your problem, depending on what your\nproblem is, many of the algorithms here and here can be used for both. So I know I've just kind of\nbedazzled you and saying that, Oh, well, you kind of use these ones for deep learning, you kind of\nuse these ones for machine learning. But depending on what your problem is, you can also use both.\nSo that's a little bit of confusion to machine learning. But that's a fun part about it too,\nis use your curiosity to figure out what's best for whatever you're working on. And with all this\ntalk about neural networks, how about in the next video, we cover what are neural networks. Now,\nI'd like you to Google this before we watch the next video, because it's going to be hundreds of\ndefinitions of what they are. And I'd like you to start forming your own definition of what a\nneural network is. I'll see you in the next video. Welcome back. In the last video, I left you with\nthe cliffhanger of a question. What are neural networks? And I gave you the challenge of\nGoogling that, but you might have already done that by the time you've got here.\nLet's just do that together. If I type in what are neural networks, I've already done this.\nWhat are neural networks? Explain neural networks, neural network definition. There are hundreds\nof definitions of things like this online neural network in five minutes. Three blue one brown.\nI'd highly recommend that channel series on neural networks. That's going to be in the\nextracurricular stat quest is also amazing. So there's hundreds of different definitions on here,\nand you can read 10 of them, five of them, three of them, make your own definition.\nBut for the sake of this course, here's how I'm going to find neural networks.\nSo we have some data of whatever it is. We might have images of food. We might have\ntweets or natural language, and we might have speech. So these are some examples of inputs\nfor unstructured data, because they're not rows and columns. So these are the input data that\nwe have. And then how do we use them with a neural network? Well, before data can be used in a neural\nnetwork, it needs to be turned into numbers, because humans, we like looking at images of Raman and\nspaghetti. We know that that's Raman. We know that that's spaghetti after we've seen it one or two\ntimes. And we like reading good tweets, and we like listening to amazing music or hearing our\nfriend talk on the phone in audio file. However, before a computer understands what's going on\nin these inputs, it needs to turn them into numbers. So this is what I call a numerical\nencoding or a representation. And this numerical encoding, these square brackets indicate that\nit's part of a matrix or a tensor, which we're going to get very hands on with throughout this\ncourse. So we have our inputs, we've turned it into numbers, and then we pass it through a neural\nnetwork. And now this is a graphic for a neural network. However, the graphics for neural networks,\nas we'll see, can get quite involved. But they all represent the same fundamentals. So if we go to\nthis one, for example, we have an input layer, then we have multiple hidden layers. However,\nyou define this, you can design these and how you want. Then we have an output layer. So our\ninputs will go in some kind of data. The hidden layers will perform mathematical operations on the\ninput. So the numbers, and then we'll have an output. Oh, there's three blue one brown neural\nnetworks from the ground up. Great video. Highly recommend you check that out. But then if we come\nback to this, so we've got our inputs, we've turned it into numbers. And we've got our neural\nnetworks that we put the input in. This is typically the input layer, hidden layer. This can be as\nmany different layers as you want, as many different, each of these little dots is called a node.\nThere's a lot of information here, but we're going to get hands-on with seeing what this looks\nlike. And then we have some kind of output. Now, which neural network should you use? Well,\nyou can choose the appropriate neural network for your problem, which could involve you\nhand coding each one of these steps. Or you could find one that has worked on problems similar to\nyour own, such as for images, you might use a CNN, which is a convolutional neural network.\nFor natural language, you might use a transformer. For speech, you might also use a transformer.\nBut fundamentally, they all follow the same principle of inputs, manipulation, outputs.\nAnd so the neural network will learn a representation on its own. We want to find what it learns.\nSo it's going to manipulate these patterns in some way, shape, or form. And when I say\nlearns representation, I'm going to also refer to it as learns patterns in the data.\nA lot of people refer to it as features. A feature may be the fact that the word do comes out to how,\nusually, in across a whole bunch of different languages. A feature can be almost anything you\nwant. And again, we don't define this. The neural network learns these representations,\npatterns, features, also called weights on its own. And then where do we go from there? Well,\nwe've got some sort of numbers, numerical encoding turned our data into numbers. Our neural network\nhas learned a representation that it thinks best represents the patterns in our data.\nAnd then it outputs those representation outputs, which we can use. And often you'll\nhear this referred to as features or weight matrix or weight tensor.\nLearned representation is also another common one. There's a lot of different terms for these\nthings. And then it will output. We can convert these outputs into human understandable outputs.\nSo if we were to look at these, this could be, again, I said representations or patterns that\nare neural network learns can be millions of numbers. This is only nine. So imagine if these\nwere millions of different numbers, I can barely understand the nine numbers that is going on here.\nSo we need a way to convert these into human understandable terms. So for this example,\nwe might have some input data, which are images of food. And then we want our neural network to\nlearn the representations between an image of ramen and an image of spaghetti.\nAnd then eventually we'll take those patterns that it's learned and we'll convert them into\nwhether it thinks that this is an image of ramen or spaghetti. Or in the case of this tweet,\nis this a tweet for a natural disaster or not a natural disaster? So our neural network has,\nwell, we've written code to turn this into numbers. Pass it through our neural network. Our neural\nnetwork has learned some kind of patterns. And then we ideally want it to represent this tweet\nas not a disaster. And then we can write code to do each of these steps here. And the same thing\nfor these inputs going as speech, turning into something that you might say to your smart speaker,\nwhich I'm not going to say because a whole bunch of my devices might go off. And so let's cover\nthe anatomy of neural networks. We've hinted at this a little bit already. But this is like\nneural network anatomy 101. Again, this is highly customizable what this thing actually is. We're\ngoing to see it in PyTorch code later on. But the data goes into the input layer. And in this case,\nthe number of units slash neurons slash nodes is two hidden layers. You can have, I put a s here\nbecause you can have one hidden layer, or the deep in deep learning comes from having lots of\nlayers. So this is only showing four layers. You might have, well, this is three layers as well.\nIt might be very deep neural networks such as ResNet 152. This is 152 different layers.\nSo again, you can, or this is 34, because this is only ResNet 34. But ResNet 152 has 152 different\nlayers. So that's a common computer vision or a popular computer vision algorithm, by the way.\nLots of terms we're throwing out here. But with time, you'll start to become familiar with them.\nSo hidden layers can be almost as many as you want. We've only got pictured one here. And in this\ncase, there's three hidden units slash neurons. And then we have an output layer. So the outputs\nlearned representation or prediction probabilities from here, depending on how we set it up, which\nagain, we will see what these are later on. And in this case, it has one hidden unit. So two input,\nthree, one output, you can customize the number of these, you can customize how many layers there\nare, you can customize what goes into here, you can customize what goes out of there. So now,\nif we talk about the overall architecture, which is describing all of the layers combined. So that's,\nwhen you hear neural network architecture, it talks about the input, the hidden layers,\nwhich may be more than one, and the output layer. So that's a terminology for overall architecture.\nNow, I say patterns is an arbitrary term. You can hear embedding embedding might come from hidden\nlayers, weights, feature representation, feature vectors, all referring to similar things. So,\nagain, how do we turn our data into some numerical form, build a neural network to figure out patterns\nto output some desired output that we want. And now to get more technical, each layer is usually a\ncombination of linear, so straight lines, and nonlinear, non-straight functions. So what I mean by that\nis a linear function is a straight line, a nonlinear function is a non-straight line.\nIf I asked you to draw whatever you want with unlimited straight lines and not straight lines,\nso you can use straight lines or curved lines, what kind of patterns could you draw?\nAt a fundamental level, that is basically what a neural network is doing. It's using a combination\nof linear, straight lines, and not straight lines to draw patterns in our data. We'll see what\nthis looks like later on. Now, from the next video, let's dive in briefly to different kinds of\nlearning. So we've looked at what a neural network is, the overall algorithm, but there are also\ndifferent paradigms of how a neural network learns. I'll see you in the next video.\nWelcome back. We've discussed a brief overview of an anatomy of what a neural network is,\nbut let's now discuss some learning paradigms. So the first one is supervised learning,\nand then we have unsupervised and self-supervised learning, and transfer learning. Now supervised\nlearning is when you have data and labels, such as in the example we gave at the start, which was\nhow you would build a neural network or a machine learning algorithm to figure out the rules to\ncook your Sicilian grandmother's famous roast chicken dish. So in the case of supervised learning,\nyou'd have a lot of data, so inputs, such as raw ingredients as vegetables and chicken,\nand a lot of examples of what that inputs should ideally look like. Or in the case of discerning\nphotos between a cat and a dog, you might have a thousand photos of a cat and a thousand photos\nof a dog that you know which photos are cat and which photos are dog, and you pass those photos\nto a machine learning algorithm to discern. So in that case, you have data, the photos, and the\nlabels, aka cat and dog, for each of those photos. So that's supervised learning, data and labels.\nUnsupervised and self-supervised learning is you just have the data itself.\nYou don't have any labels. So in the case of cat and dog photos, you only have the photos.\nYou don't have the labels of cat and dog. So in the case of self-supervised learning,\nyou could get a machine learning algorithm to learn an inherent representation of what,\nand when I say representation, I mean patterns and numbers, I mean weights, I mean features,\na whole bunch of different names describing the same thing. You could get a self-supervised\nlearning algorithm to figure out the fundamental patterns between a dog and a cat image, but\nit wouldn't necessarily know the difference between the two.\nThat's where you could come in later and go show me the patterns you've learned,\nand it might show you the patterns and you could go, okay, the patterns that look like this,\na dog and the patterns that look like that, a cat. So self-supervised and unsupervised learning\nlearn solely on the data itself. And then finally, transfer learning is a very, very\nimportant paradigm in deep learning. It's taking the patterns that one model has learned\nof a data set and transferring it to another model, such in the case of if we were trying to\nbuild a supervised learning algorithm for discerning between cat and dog photos.\nWe might start with a model that has already learned patterns and images\nand transfer those foundational patterns to our own model so that our model gets a head start.\nThis is transfer learning is a very, very powerful technique, but as for this course,\nwe're going to be writing code to focus on these two supervised learning and transfer learning,\nwhich are two of the most common paradigms or common types of learning in machine learning\nand deep learning. However, this style of code though can be adapted across different learning\nparadigms. Now, I just want to let you know there is one that I haven't mentioned here,\nwhich is kind of in its own bucket, and that is reinforcement learning. So I'll leave this\nas an extension if you wanted to look it up. But essentially, this is a good one.\nThat's a good photo, actually. So shout out to Katie Nuggets. The whole idea of reinforcement\nlearning is that you have some kind of environment and an agent that does actions in that environment,\nand you give rewards and observations back to that agent. So say, for example,\nyou wanted to teach your dog to urinate outside. Well, you would reward its actions of urinating\noutside and possibly not reward its actions of urinating all over your couch. So reinforcement\nlearning is again, it's kind of in its own paradigm. This picture has a good explanation\nbetween unsupervised learning, supervised learning to separate two different things,\nand then reinforcement learning is kind of like that. But again, I will let you research the\ndifferent learning paradigms a little bit more in your own time. As I said, we're going to be\nfocused on writing code to do supervised learning and transfer learning, specifically pytorch code.\nNow with that covered, let's get a few examples of what is deep learning actually used for. And\nbefore we get into the next video, I'm going to issue you a challenge to search this question\nyourself and come up with some of your own ideas for what deep learning is currently used for.\nSo give that a shot and I'll see you in the next video. How'd you go? Did you do some research?\nDid you find out what deep learning is actually used for? I bet you found a treasure trail of\nthings. And hey, I mean, if you're reading this course, chances are that you probably already\nknow some use cases for deep learning. You're like, Daniel, hurry up and get to the code. Well,\nwe're going to get there, don't you worry? But let's have a look at some things that deep\nlearning can be used for. But before, I just want to remind you of this comment. This is from\nYasha Sway on the 2020 machine learning roadmap video. I think you can use ML and remember,\nML is machine learning. And remember, deep learning is a part of ML for literally anything as long\nas you can convert it into numbers and program it to find patterns. Literally, it could be anything,\nany input or output from the universe. So that's a beautiful thing about machine learning is that\nif you can encode it something into numbers, chances are you can build a machine learning\nalgorithm to find patterns in those numbers. Will it work? Well, again, that's the reason machine\nlearning and deep learning is part art, part science. A scientist would love to know that their\nexperiments would work. But an artist is kind of excited about the fact that, I don't know,\nthis might work, it might not. And so that's something to keep in mind. Along with the rule\nnumber one of machine learning is if you don't need it, you don't use it. But if you do use it,\nit can be used for almost anything. And let's get a little bit specific and find out some deep\nlearning use cases. And I've put some up there for a reason because there are lots. These are just\nsome that I interact with in my day to day life, such as recommendation, we've got a programming\nvideo, we've got a programming podcast, we got some jujitsu videos, we've got some RuneScape\nvideos, a soundtrack from my favorite movie. Have you noticed, whenever you go to YouTube,\nyou don't really search for things anymore. Well, sometimes you might, but the recommendation\npage is pretty darn good. That's all powered by deep learning. And in the last 10 years,\nhave you noticed that translation has got pretty good too? Well, that's powered by deep learning\nas well. Now, I don't have much hands on experience with this. I did use it when I was in Japan.\nI speak a very little amount of Japanese and even smaller amount of Mandarin. But if I wanted to\ntranslate deep learning as epic to Spanish, it might come out as el aprendise, profando es ebiko.\nNow, all of the native Spanish speakers watching this video can laugh at me because that was a very\nAustralian version of saying deep learning is epic in Spanish. But that's so cool. All the Google\nTranslate is now powered by deep learning. And the beautiful thing, if I couldn't say it myself,\nI could click this speaker and it would say it for me. So that speech recognition that's powered\nby deep learning. So if you were to ask your voice assistant who's the biggest big dog of them all,\nof course, they're going to say you, which is what I've set up, my voice assistant to say.\nThat's part of speech recognition. And in computer vision, oh, look at this. You see this? Where is\nthis photo from? This photo is from this person driving this car. Did a hit and run on my car,\nat the front of my house, my apartment building, my car was parked on the street, this car, the\ntrailer came off, ran into the back of my car, basically destroyed it, and then they drove off.\nHowever, my next door neighbors security camera picked up on this car. Now, I became a detective\nfor a week, and I thought, hmm, if there was a computer vision algorithm built into that camera,\nit could have detected when the car hit. I mean, it took a lot of searching to find it,\nit turns out the car hit about 3.30am in the morning. So it's pitch black. And of course,\nwe didn't get the license plate. So this person is out there somewhere in the world after doing\na hit and run. So if you're watching this video, just remember computer vision might catch you one\nday. So this is called object detection, where you would place a box around the area where the\npixels most represent the object that you're looking for. So for computer vision, we could\ntrain an object detector to capture cars that drive past a certain camera. And then if someone\ndoes a hit and run on you, you could capture it. And then fingers crossed, it's not too dark\nthat you can read the license plate and go, hey, excuse me, please, this person has hit my car\nand wrecked it. So that's a very close to home story of where computer vision could be used.\nAnd then finally, natural language processing. Have you noticed as well, your spam detector on\nyour email inbox is pretty darn good? Well, some are powered by deep learning, some not,\nit's hard to tell these days what is powered by deep learning, what isn't. But natural language\nprocessing is the process of looking at natural language text. So unstructured text. So whatever\nyou'd write an email in a story in a Wikipedia document and deciding or getting your algorithm\nto find patterns in that. So for this example, you would find that this email is not spam.\nThis deep learning course is incredible. I can't wait to use what I've learned. Thank you so much.\nAnd by the way, that is my real email. So if you want to email me, you can. And then this is spam.\nHey, Daniel, congratulations, you win a lot of money. Wow, I really like that a lot of money.\nBut somebody said, I don't think that this is real. So that would probably go to my spam inbox.\nNow, with that being said, if we wanted to put these problems in a little bit more of a\nclassification, this is known as sequence to sequence because you put one sequence in\nand get one sequence out. Same as this, you have a sequence of audio waves and you get some\ntext out. So sequence to sequence, sec to sec. This is classification slash regression. In this\ncase, the regression is predicting a number. That's what a regression problem is. You would predict\nthe coordinates of where these box corners should be. So say this should be at however many pixels\nin from the X angle and however many pixels down from the Y angle, that's that corner.\nAnd then you would draw in between the corners. And then the classification part would go,\nHey, this is that car that did a hit and run on us. And in this case, this is classification.\nClassification is predicting whether something is one thing or another, or perhaps more than one\nthing or another in the class of multi class classification. So this email is not spam. That's\na class and this email is spam. So that's also a class. So I think we've only got one direction\nto go now that we've sort of laid the foundation for the course. And that is\nWell, let's start talking about PyTorch. I'll see you in the next video.\nWell, let's now cover some of the foundations of\nPyTorch. But first, you might be asking, what is PyTorch? Well, of course, we could just go to\nour friend, the internet, and look up PyTorch.org. This is the homepage for PyTorch.\nThis course is not a replacement for everything on this homepage. This should be your ground truth\nfor everything PyTorch. So you can get started. You've got a big ecosystem. You've got a way to\nset up on your local computer. You've got resources. You've got docs. PyTorch. You've got the GitHub.\nYou've got search. You've got blog, everything here. This website should be the place you're\nvisiting most throughout this course as we're writing PyTorch code. You're coming here.\nYou're reading about it. You're checking things out. You're looking at examples.\nBut for the sake of this course, let's break PyTorch down. Oh, there's a little flame animation\nI just forgot about. What is PyTorch? I didn't sync up the animations. That's all right. So\nPyTorch is the most popular research deep learning framework. I'll get to that in a second.\nIt allows you to write fast deep learning code in Python. If you know Python, it's a very user-friendly\nprogramming language. PyTorch allows us to write state-of-the-art deep learning code\naccelerated by GPUs with Python. It enables you access to many pre-built deep learning models\nfrom Torch Hub, which is a website that has lots of, if you remember, I said transfer learning is\na way that we can use other deep learning models to power our own. Torch Hub is a resource for that.\nSame as Torch Vision.Models. We'll be looking at this throughout the course.\nIt provides an ecosystem for the whole stack of machine learning. From pre-processing data,\ngetting your data into tenses, what if you started with some images? How do you represent them as\nnumbers? Then you can build models such as neural networks to model that data. Then you can even\ndeploy your model in your application slash cloud, well, deploy your PyTorch model. Application slash\ncloud will be depending on what sort of application slash cloud that you're using, but generally it\nwill run some kind of PyTorch model. And it was originally designed and used in-house by Facebook\nslash meta. I'm pretty sure Facebook have renamed themselves meta now, but it is now open source\nand used by companies such as Tesla, Microsoft and OpenAI. And when I say it is the most popular\ndeep learning research framework, don't take my word for it. Let's have a look at papers with code\ndot com slash trends. If you're not sure what papers with code is, it is a website that tracks\nthe latest and greatest machine learning papers and whether or not they have code. So we have some\nother languages here, other deep learning frameworks, PyTorch, TensorFlow, Jax is another one, MXNet,\npaddle paddle, the original torch. So PyTorch is an evolution of torch written in Python,\nCAF2, Mindspore. But if we look at this, when is this? Last date is December 2021. We have,\noh, this is going to move every time I move it. No. So I'll highlight PyTorch at 58% there.\nSo by far and large, the most popular research machine learning framework used to write the code\nfor state of the art machine learning algorithms. So this is browse state of the art papers with\ncode.com amazing website. We have semantic segmentation, image classification, object detection, image\ngeneration, computer vision, natural language processing, medical, I'll let you explore this.\nIt's one of my favorite resources for staying up to date on the field. But as you see, out of the\n65,000 papers with code that this website is tracked, 58% of them are implemented with PyTorch.\nHow cool is that? And this is what we're learning. So let's jump into there. Why PyTorch? Well,\nother than the reasons that we just spoke about, it's a research favorite. This is highlighting.\nThere we go. So there we go. I've highlighted it here. PyTorch, 58%, nearly 2,500 repos. If\nyou're not sure what a repo is, a repo is a place where you store all of your code online.\nAnd generally, if a paper gets published in machine learning, if it's fantastic research,\nit will come with code, code that you can access and use for your own applications or your own\nresearch. Again, why PyTorch? Well, this is a tweet from Francois Chale, who's the author of\nKeras, which is another popular deep learning framework. But with tools like Colab, we're going\nto see what Colab is in a second, Keras and TensorFlow. I've added in here and PyTorch.\nVirtually anyone can solve in a day with no initial investment problems that would have\nrequired an engineering team working for a quarter and $20,000 in hardware in 2014. So this is just\nto highlight how good the space of deep learning and machine learning tooling has become. Colab,\nKeras and TensorFlow are all fantastic. And now PyTorch is added to this list. If you want to\ncheck that out, there's Francois Chale on Twitter. Very, very prominent voice in the machine learning\nfield. Why PyTorch? If you want some more reasons, well, have a look at this. Look at all the\nplaces that are using PyTorch. It's just coming up everywhere. We've got Andre Kapathi here,\nwho's the director of AI at Tesla. So if we go, we could search this, PyTorch\nat Tesla. We've got a YouTube talk there, Andre Kapathi, director of AI at Tesla.\nAnd so Tesla are using PyTorch for the computer vision models of autopilot. So if we go to videos\nor maybe images, does it come up there? Things like this, a car detecting what's going on in the scene.\nOf course, there'll be some other code for planning, but I'll let you research that.\nWhen we come back here, OpenAI, which is one of the biggest open artificial intelligence\nresearch firms, open in the sense that they publish a lot of their research methodologies,\nhowever, recently there's been some debate about that. But if you go to openai.com,\nlet's just say that they're one of the biggest AI research entities in the world,\nand they've standardized on PyTorch. So they've got a great blog, they've got great research,\nand now they've got OpenAI API, which is, you can use their API to access some of the models\nthat they've trained. Presumably with PyTorch, because this blog post from January 2020 says\nthat OpenAI is now standardized across PyTorch. There's a repo called the incredible PyTorch,\nwhich collects a whole bunch of different projects that are built on top of PyTorch.\nThat's the beauty of PyTorch is that you can build on top of it, you can build with it\nAI for AG, for agriculture. PyTorch has been used. Let's have a look. PyTorch in agriculture.\nThere we go. Agricultural robots use PyTorch. This is a medium article.\nIt's everywhere. So if we go down here, this is using object detection. Beautiful.\nObject detection to detect what kind of weeds should be sprayed with fertilizer. This is just\none of many different things, so PyTorch on a big tractor like this. It can be used almost\nanywhere. If we come back, PyTorch builds the future of AI and machine learning at Facebook,\nso Facebook, which is also MetaAI, a little bit confusing, even though it says MetaAI,\nit's on AI.facebook.com. That may change by the time you watch this. They use PyTorch in-house\nfor all of their machine learning applications. Microsoft is huge in the PyTorch game.\nIt's absolutely everywhere. So if that's not enough reason to use PyTorch,\nwell, then maybe you're in the wrong course. So you've seen enough reasons of why to use PyTorch.\nI'm going to give you one more. That is that it helps you run your code, your machine learning code\naccelerated on a GPU. We've covered this briefly, but what is a GPU slash a TPU,\nbecause this is more of a newer chip these days. A GPU is a graphics processing unit,\nwhich is essentially very fast at crunching numbers. Originally designed for video games,\nif you've ever designed or played a video game, you know that the graphics are quite intense,\nespecially these days. And so to render those graphics, you need to do a lot of numerical calculations.\nAnd so the beautiful thing about PyTorch is that it enables you to leverage a GPU through an\ninterface called CUDA, which is a lot of words I'm going to throw out you here, a lot of acronyms\nin the deep learning space, CUDA. Let's just search CUDA. CUDA toolkit. So CUDA is a parallel\ncomputing platform and application programming interface, which is an API that allows software\nto use certain types of graphics processing units for general purpose computing. That's what\nwe want. So PyTorch leverages CUDA to enable you to run your machine learning code on NVIDIA\nGPUs. Now, there is also an ability to run your PyTorch code on TPUs, which is a tensor processing\nunit. However, GPUs are far more popular when running various types of PyTorch code. So we're\ngoing to focus on running our PyTorch code on the GPU. And to just give you a quick example,\nPyTorch on TPU, let's see that. Getting started with PyTorch on cloud TPUs, there's plenty of\nguys for that. But as I said, GPUs are going to be far more common in practice. So that's what\nwe're going to focus on. And with that said, we've said tensor processing unit. Now, the reason\nwhy these are called tensor processing units is because machine learning and deep learning\ndeals a lot with tensors. And so in the next video, let's answer the question, what is a tensor?\nBut before I go through and answer that from my perspective, I'd like you to research this\nquestion. So open up Google or your favorite search engine and type in what is a tensor and\nsee what you find. I'll see you in the next video. Welcome back. In the last video, I left you on\nthe cliffhanger question of what is a tensor? And I also issued you the challenge to research\nwhat is a tensor. Because as I said, this course isn't all about telling you exactly what things\nare. It's more so sparking a curiosity in you so that you can stumble upon the answers to these\nthings yourself. But let's have a look. What is a tensor? Now, if you remember this graphic,\nthere's a lot going on here. But this is our neural network. We have some kind of input,\nsome kind of numerical encoding. Now, we start with this data. In our case, it's unstructured data\nbecause we have some images here, some text here, and some audio file here. Now, these necessarily\ndon't go in all at the same time. This image could just focus on a neural network specifically\nfor images. This text could focus on a neural network specifically for text. And this sound bite\nor speech could focus on a neural network specifically for speech. However, the field is sort of also\nmoving towards building neural networks that are capable of handling all three types of inputs.\nFor now, we're going to start small and then build up the algorithms that we're going to focus on\nare neural networks that focus on one type of data. But the premise is still the same. You have\nsome kind of input. You have to numerically encode it in some form, pass it to a neural network\nto learn representations or patterns within that numerical encoding, output some form of\nrepresentation. And then we can convert that representation into things that humans understand.\nAnd you might have already seen these, and I might have already referenced the fact that\nthese are tensors. So when the question comes up, what are tensors? A tensor could be almost\nanything. It could be almost any representation of numbers. We're going to get very hands on with\ntensors. And that's actually the fundamental building block of PyTorch aside from neural network\ncomponents is the torch dot tensor. We're going to see that very shortly. But this is a very\nimportant takeaway is that you have some sort of input data. You're going to numerically encode\nthat data, turn it into a tensor of some kind. Whatever that kind is will depend on the problem\nyou're working with. Then you're going to pass it to a neural network, which will perform mathematical\noperations on that tensor. Now, a lot of those mathematical operations are taken care of by\nPyTorch behind the scenes. So we'll be writing code to execute some kind of mathematical\noperations on these tensors. And then the neural network that we create, or the one that's already\nbeen created, but we just use for our problem, we'll output another tensor, similar to the input,\nbut has been manipulated in a certain way that we've sort of programmed it to. And then we can take\nthis output tensor and change it into something that a human can understand. So to remove a lot\nof the text around it, make it a little bit more clearer. If we were focusing on building an image\nclassification model, so we want to classify whether this was a photo of Raman or spaghetti,\nwe would have images as input. We would turn those images into numbers, which are represented\nby a tensor. We would pass that tensor of numbers to a neural network, or there might be lots of\ntensors here. We might have 10,000 images. We might have a million images. Or in some cases,\nif you're Google or Facebook, you might be working with 300 million or a billion images at a time.\nThe principle still stands that you encode your data in some form of numerical representation,\nwhich is a tensor, pass that tensor, or lots of tensors to a neural network. The neural network\nperforms mathematical operations on those tensors, outputs a tensor, we convert that tensor into\nsomething that we can understand as humans. And so with that being said, we've covered a lot of\nthe fundamentals. What is machine learning? What is deep learning? What is neural network? Well,\nwe've touched the surface of these things. You can get as deep as you like. We've covered\nwhy use PyTorch. What is PyTorch? Now, the fundamental building block of deep learning\nis tensors. We've covered that. Let's get a bit more specific in the next video\nof what we're going to cover code-wise in this first module. I'm so excited we're going to start\ncodes in. I'll see you in the next video. Now it's time to get specific about what we're going to\ncover code-wise in this fundamentals module. But I just want to reiterate the fact that\ngoing back to the last video where I challenge you to look up what is a tensor, here's exactly\nwhat I would do. I would come to Google. I would type in the question, what is a tensor? There we go.\nWhat is a tensor in PyTorch? It knows Google knows that using that deep learning data that we want\nto know what a tensor is in PyTorch. But a tensor is a very general thing. It's not\nassociated with just PyTorch. Now we've got tensor on Wikipedia. We've got tensor. This is probably\nmy favorite video on what is a tensor. By Dan Flesch. Flesch, I'm probably saying that wrong,\nbut good first name. This is going to be your extra curriculum for this video and the previous\nvideo is to watch this on what is a tensor. Now you might be saying, well, what gives? I've come to\nthis course to learn PyTorch and all this guy's doing, all you're doing, Daniel, is just Googling\nthings when a question comes up. Why don't you just tell me what it is? Well, if I was to tell you\neverything about deep learning and machine learning and PyTorch and what it is and what it's not,\nthat course would be far too long. I'm doing this on purpose. I'm searching questions like this on\npurpose because that's exactly what I do day to day as a machine learning engineer. I write code\nlike we're about to do. And then if I don't know something, I literally go to whatever search engine\nI'm using, Google most of the time, and type in whatever error I'm getting or PyTorch, what is\na tensor, something like that. So I want to not only tell you that it's okay to search questions\nlike that, but it's encouraged. So just keep that in mind as we go through the whole course,\nyou're going to see me do it a lot. Let's get into what we're going to cover. Here we go.\nNow, this tweet is from Elon Musk. And so I've decided, you know what, let's base the whole\ncourse on this tweet. We have learning MLDL from university, you have a little bit of a small brain.\nOnline courses, well, like this one, that brain's starting to explode and you get some little fireworks\nfrom YouTube. Oh, you're watching this on YouTube. Look at that shiny brain from articles. My goodness.\nLucky that this course comes in article format. If you go to learn pytorch.io, all of the course\nmaterials are in online book format. So we're going to get into this fundamental section very\nshortly. But if you want a reference, the course materials are built off this book. And by the\ntime you watch this, there's going to be more chapters here. So we're covering all the bases\nhere. And then finally, from memes, you would ascend to some godlike creature. I think that's\nhovering underwater. So that is the best way to learn machine learning. So this is what we're\ngoing to start with MLDL from university online courses, YouTube from articles from memes. No,\nno, no, no. But kind of here is what we're going to cover broadly. So now in this module,\nwe are going to cover the pytorch basics and fundamentals, mainly dealing with tensors and\ntensor operations. Remember, a neural network is all about input tensors, performing operations on\nthose tensors, creating output operations. Later, we're going to be focused on pre-processing data,\ngetting it into tensors, so turning data from raw form, images, whatever, into a numerical\nencoding, which is a tensor. Then we're going to look at building and using pre-trained deep\nlearning models, specifically neural networks. We're going to fit a model to the data. So we're\ngoing to show our model or write code for our model to learn patterns in the data that we've\npre-processed. We're going to see how we can make predictions with our model, because that's\nwhat deep learning and machine learning is all about, right, using patterns from the past to\npredict the future. And then we're going to evaluate our model's predictions. We're going to learn\nhow to save and load our models. For example, if you wanted to export your model from where we're\nworking to an application or something like that. And then finally, we're going to see how we can\nuse a trained model to make predictions on our own data on custom data, which is very fun. And\nhow? Well, you can see that the scientist has faded out a little bit, but that's not really that true.\nWe're going to do it like cooks, not chemists. So chemists are quite precise. Everything has to be\nexactly how it is. But cooks are more like, oh, you know what, a little bit of salt, a little bit of\nbutter. Does it taste good? Okay, well, then we're on. But machine learning is a little bit of both.\nIt's a little bit of science, a little bit of art. That's how we're going to do it. But\nI like the idea of this being a machine learning cooking show. So welcome to cooking with machine\nlearning, cooking with PyTorch with Daniel. And finally, we've got a workflow here, which we have\na PyTorch workflow, which is one of many. We're going to kind of use this throughout the entire\ncourse is step one, we're going to get our data ready. Step two, we're going to build a\npick a pre trained model to suit whatever problem we're working on. Step two point one,\npick a loss function and optimizer. Don't worry about what they are. We're going to cover them\nsoon. Step two point two, build a training loop. Now this is kind of all part of the parcel of\nstep two, hence why we've got two point one and two point two. You'll see what that means later on.\nNumber three, we're going to fit the model to the data and make a prediction. So say we're working\non image classification for Raman or spaghetti. How do we build a neural network or put our\nimages through that neural network to get some sort of idea of what's in an image? We'll see\nhow to do that. Well, the value weight our model to see if it's predicting BS or it's actually\ngoing all right. Number five, we're going to improve through experimentation. That's another\nbig thing that you'll notice throughout machine learning throughout this course is that it's\nvery experimental part art, part science. Number six, save and reload your trained model. Again,\nI put these with numerical order, but they can kind of be mixed and matched depending on where\nyou are in the journey. But numerical order is just easy to understand for now. Now we've got\none more video, maybe another one before we get into code. But in the next video, I'm going to\ncover some very, very important points on how you should approach this course. I'll see you there.\nNow you might be asking, how should I approach this course? You might not be asking, but we're\ngoing to answer it anyway. How to approach this course? This is how I would recommend approaching\nthis course. So I'm a machine learning engineer day to day and learning machine learning to\ncoding machine learning, a kind of two different things. I remember when I first learned it was\nkind of, you learned a lot of theory rather than writing code. So not to take away from the theory\nof being important, this course is going to be focusing on writing machine learning specifically\nPyTorch code. So the number one step to approaching this course is to code along. Now because this\ncourse is focused on purely writing code, I will be linking extracurricular resources for you to\nlearn more about what's going on behind the scenes of the code. My idea of teaching is that if we\ncan code together, write some code, see how it's working, that's going to spark your curiosity to\nfigure out what's going on behind the scenes. So motto number one is if and out, run the code,\nwrite it, run the code, see what happens. Number two, I love that. Explore an experiment again.\nApproach this with the idea, the mind of a scientist and a chef or science and art. Experiment,\nexperiment, experiment. Try things with rigor like a scientist would, and then just try things\nfor the fun of it like a chef would. Number three, visualize what you don't understand. I can't\nemphasize this one enough. We have three models so far. If and out, run the code, you're going to\nhear me say this a lot. Experiment, experiment, experiment. And number three, visualize, visualize,\nvisualize. Why is this? Well, because we've spoken about machine learning and deep learning\ndeals with a lot of data, a lot of numbers. And so I find it that if I visualize some numbers in\nwhatever form that isn't just numbers all over a page, I tend to understand it better.\nAnd there are some great extracurricular resources that I'm going to link that also turn what we're\ndoing. So writing code into fantastic visualizations. Number four, ask questions, including the dumb\nquestions. Really, there's no such thing as a dumb question. Everyone is just on a different\npart of their learning journey. And in fact, if you do have a quote unquote dumb question,\nit turns out that a lot of people probably have that one as well. So be sure to ask questions.\nI'm going to link a resource in a minute of where you can ask those questions, but\nplease, please, please ask questions, not only to the community, but to Google to the internet\nto wherever you can, or just yourself. Ask questions of the code and write code to figure\nout the answer to those questions. Number five, do the exercises. There are some\ngreat exercises that I've created for each of the modules. If we go, have we got the book version\nof the course up here? We do. Within all of these chapters here, down the bottom is going to be\nexercises and extra curriculum. So we've got some exercises. I'm not going to jump into them,\nbut I would highly recommend don't just follow along with the course and code after I code.\nPlease, please, please give the exercises a go because that's going to stretch your knowledge.\nWe're going to have a lot of practice writing code together, doing all of this stuff here.\nBut then the exercises are going to give you a chance to practice what you've learned.\nAnd then of course, extra curriculum. Well, hey, if you want to learn more, there's plenty of\nopportunities to do so there. And then finally, number six, share your work. I can't emphasize\nenough how much writing about learning deep learning or sharing my work through GitHub or\ndifferent code resources or with the community has helped with my learning. So if you learn\nsomething cool about PyTorch, I'd love to see it. Link it to me somehow in the Discord chat\nor on GitHub or whatever. There'll be links of where you can find me. I'd love to see it. Please\ndo share your work. It's a great way to not only learn something because when you share it, when\nyou write about it, it's like, how would someone else understand it? But it's also a great way to\nhelp others learn too. And so we said how to approach this course. Now, let's go how not to\napproach this course. I would love for you to avoid overthinking the process. And this is your brain,\nand this is your brain on fire. So avoid having your brain on fire. That's not a good place to be.\nWe are working with PyTorch, so it's going to be quite hot. Just playing on words with the name\ntorch. But avoid your brain catching on fire. And avoid saying, I can't learn,\nI've said this to myself lots of times, and then I've practiced it and it turns out I can\nactually learn those things. So let's just draw a red line on there. Oh, I think a red line.\nYeah, there we go. Nice and thick red line. We'll get that out there. It doesn't really make sense\nnow that this says avoid and crossed out. But don't say I can't learn and prevent your brain from\ncatching on fire. Finally, we've got one more video that I'm going to cover before this one\ngets too long of the resources for the course before we get into coding. I'll see you there.\nNow, there are some fundamental resources that I would like you to be aware of before we\ngo any further in this course. These are going to be paramount to what we're working with.\nSo for this course, there are three things. There is the GitHub repo. So if we click this link,\nI've got a pinned on my browser. So you might want to do the same while you're going through\nthe course. But this is Mr. D. Burks in my GitHub slash PyTorch deep learning. It is still a work\nin progress at the time of recording this video. But by the time you go through it, it won't look\ntoo much different, but there just be more materials. You'll have materials outline,\nsection, what does it cover? As you can see, some more are coming soon at the time of recording\nthis. So these will probably be done by the time you watch this exercise in extra curriculum.\nThere'll be links here. Basically, everything you need for the course will be in the GitHub repo.\nAnd then if we come back, also on the GitHub repo, the same repo. So Mr. D. Burks slash PyTorch\ndeep learning. If you click on discussions, this is going to be the Q and A. This is just the same\nlink here, the Q and A for the course. So if you have a question here, you can click new discussion,\nyou can go Q and A, and then type in video, and then the title PyTorch Fundamentals, and then go\nin here. Or you could type in your error as well. What is N-DIM for a tensor? And then in here,\nyou can type in some stuff here. Hello. I'm having trouble on video X, Y, Z. Put in the name of the\nvideo. So that way I can, or someone else can help you out. And then code, you can go three\nback ticks, write Python, and then you can go import torch, torch dot rand n, which is going to\ncreate a tensor. We're going to see this in a second. Yeah, yeah, yeah. And then if you post that\nquestion, the formatting of the code is very helpful that we can understand what's going on,\nand what's going on here. So this is basically the outline of how I would ask a question video.\nThis is going on. What is such and such for whatever's going on? Hello. This is what I'm having\ntrouble with. Here's the code, and here's what's happening. You could even include the error message,\nand then you can just click start discussion, and then someone, either myself or someone else from\nthe course will be able to help out there. And the beautiful thing about this is that it's all in\none place. You can start to search it. There's nothing here yet because the course isn't out yet,\nbut as you go through it, there will probably be more and more stuff here. Then if you have any\nissues with the code that you think needs fixed, you can also open a new issue there. I'll let you\nread more into what's going on. I've just got some issues here already about the fact that I\nneed to record videos for the course. I need to create some stuff. But if you think there's\nsomething that could be improved, make an issue. If you have a question about the course,\nask a discussion. And then if we come back to the keynote, we have one more resource. So that\nwas the course materials all live in the GitHub. The course Q&A is on the course\nGitHub's discussions tab, and then the course online book. Now, this is a work of art.\nThis is quite beautiful. It is some code to automatically turn all of the materials from the\nGitHub. So if we come into here code, if we click on notebook zero zero, this is going to sometimes\nif you've ever worked with Jupiter notebooks on GitHub, they can take a while to load.\nSo all of the materials here automatically get converted into this book. So the beautiful\nthing about the book is that it's got different headings here. It's all readable. It's all online.\nIt's going to have all the images there. And you can also search some stuff here,\nPyTorch training steps, creating a training loop in PyTorch. Beautiful. We're going to see\nthis later on. So they're the three big materials that you need to be aware of, the three big resources\nfor this specific course materials on GitHub course Q&A course online book, which is\nlearn pytorch.io, simple URL to remember, all the materials will be there. And then\nspecifically for PyTorch or things PyTorch, the PyTorch website and the PyTorch forums.\nSo if you have a question that's not course related, but more PyTorch related, I'd highly\nrecommend you go to the PyTorch forums, which is available at discuss.pytorch.org. We've got a link\nthere. Then the PyTorch website, PyTorch.org, this is going to be your home ground for everything\nPyTorch of course. We have the documentation here. And as I said, this course is not a replacement\nfor getting familiar with the PyTorch documentation. This, the course actually is built off all of\nthe PyTorch documentation. It's just organized in a slightly different way. So there's plenty of\namazing resources here on everything to do with PyTorch. This is your home ground. And you're\ngoing to see me referring to this a lot throughout the course. So just keep these in mind, course\nmaterials on GitHub, course discussions, learnpytorch.io. This is all for the course. And all things\nPyTorch specific, so not necessarily this course, but just PyTorch in general, the PyTorch website\nand the PyTorch forums. With that all being said, we've come so far. We've covered a lot already,\nbut guess what time it is? Let's write some code. I'll see you in the next video.\nWe've covered enough of the fundamentals so far. Well, from a theory point of view,\nlet's get into coding. So I'm going to go over to Google Chrome. I'm going to introduce you to\nthe tool. One of the main tools we're going to be using for the entire course. And that is Google\nColab. So the way I would suggest following along with this course is remember, one of the major\nones is to code along. So we're going to go to colab.research.google. I've got a typo here.\nClassic. You're going to see me do lots of typos throughout this course. Colab.research.google.com.\nThis is going to load up Google Colab. Now, you can follow along with what I'm going to do,\nbut if you'd like to find out how to use Google Colab from a top-down perspective,\nyou can go through some of these. I'd probably recommend going through overview of\nCollaboratory Features. But essentially, what Google Colab is going to enable us to do is\ncreate a new notebook. And this is how we're going to practice writing PyTorch code.\nSo if you refer to the reference document of learnpytorch.io, these are actually\nColab notebooks just in book format, so online book format. So these are the basis materials\nfor what the course is going to be. There's going to be more here, but every new module,\nwe're going to start a new notebook. And I'm going to just zoom in here.\nSo this one, the first module is going to be zero, zero, because Python code starts at zero,\nzero. And we're going to call this PyTorch Fundamentals. I'm going to call mine video,\njust so we know that this is the notebook that I wrote through the video. And what this is going\nto do is if we click Connect, it's going to give us a space to write Python code. So here we can go\nprint. Hello, I'm excited to learn PyTorch. And then if we hit shift and enter, it comes out like\nthat. But another beautiful benefit of Google Colab are PS. I'm using the pro version, which\ncosts about $10 a month or so. That price may be different depending on where you're from.\nThe reason I'm doing that is because I use Colab all the time. However, you do not have to use\nthe paid version for this course. Google Colab comes with a free version, which you'll be able\nto use to complete this course. If you see it worthwhile, I find the pro version is worthwhile.\nAnother benefit of Google Colab is if we go here, we can go to runtime. Let me just show you that\nagain. Runtime, change runtime type, hardware accelerator. And we can choose to run our code\non an accelerator here. Now we've got GPU and TPU. We're going to be focused on using\nGPU. If you'd like to look into TPU, I'll leave that to you. But we can click GPU, click save.\nAnd now our code, if we write it in such a way, will run on the GPU. Now we're going to see this\nlater on code that runs on the GPU is a lot faster in terms of compute time, especially for deep\nlearning. So if we write here in a video SMI, we now have access to a GPU. In my case, I have a\nTesla P100. It's quite a good GPU. You tend to get the better GPUs. If you pay for Google Colab,\nif you don't pay for it, you get the free version, you get a free GPU. It just won't be as fast as\nthe GPUs you typically get with the paid version. So just keep that in mind. A whole bunch of stuff\nthat we can do here. I'm not going to go through it all because there's too much. But we've covered\nbasically what we need to cover. So if we just come up here, I'm going to write a text cell. So\noo dot pytorch fundamentals. And I'm going to link in here resource notebook. Now you can come\nto learn pytorch.io and all the notebooks are going to be in sync. So 00, we can put this in here.\nResource notebook is there. That's what this notebook is going to be based off. This one here.\nAnd then if you have a question about what's going on in this notebook,\nyou can come to the course GitHub. And then we go back, back. This is where you can see what's\ngoing on. This is pytorch deep learning projects as you can see what's happening. At the moment,\nI've got pytorch course creation because I'm in the middle of creating it. But if you have a question,\nyou can come to Mr. D Burke slash pytorch deep learning slash discussions, which is this tab here,\nand then ask a question by clicking new discussion. So any discussions related to this notebook,\nyou can ask it there. And I'm going to turn this right now. This is a code cell.\nCoLab is basically comprised of code and text cells. I'm going to turn this into a text cell\nby pressing command mm, shift and enter. Now we have a text cell. And then if we wanted another\ncode cell, we could go like that text code text code, yada, yada, yada. But I'm going to delete this.\nAnd to finish off this video, we're going to import pytorch. So we're going to import torch.\nAnd then we're going to print torch dot dot version. So that's another beautiful thing about Google\nColab is that it comes with pytorch pre installed and a lot of other common Python data science\npackages, such as we could also go import pandas as PD, import NumPy as MP import mapplot lib\nlib dot pyplot as PLT. This is Google Colab is by far the easiest way to get started with this\ncourse. You can run things locally. If you'd like to do that, I'd refer to you to pytorch deep\nlearning is going to be set up dot MD, getting set up to code pytorch. We've just gone through\nnumber one setting up with Google Colab. There is also another option for getting started locally.\nRight now, this document's a work in progress, but it'll be finished by the time you watch this\nvideo. This is not a replacement, though, for the pytorch documentation for getting set up\nlocally. So if you'd like to run locally on your machine, rather than going on Google Colab,\nplease refer to this documentation or set up dot MD here. But if you'd like to get started\nas soon as possible, I'd highly recommend you using Google Colab. In fact, the entire course\nis going to be able to be run through Google Colab. So let's finish off this video, make sure\nwe've got pytorch ready to go. And of course, some fundamental data science packages here.\nWonderful. This means that we have pytorch 1.10.0. So if your version number is far greater than this,\nmaybe you're watching this video a couple of years in the future, and pytorch is up to 2.11,\nmaybe some of the code in this notebook won't work. But 1.10.0 should be more than enough for\nwhat we're going to do. And plus Q111, CU111, stands for CUDA version 11.1, I believe. And what\nthat would mean is if we came in here, and we wanted to install it on Linux, which is what\nColab runs on, there's Mac and Windows as well. We've got CUDA. Yeah. So right now, as of recording\nthis video, the latest pytorch build is 1.10.2. So you'll need at least pytorch 1.10 to complete\nthis course and CUDA 11.3. So that's CUDA toolkit. If you remember, CUDA toolkit is NVIDIA's\nprogramming. There we go. NVIDIA developer. CUDA is what enables us to run our pytorch code on\nNVIDIA GPUs, which we have access to in Google Colab. Beautiful. So we're set up ready to write code.\nLet's get started in the next video writing some pytorch code. This is so exciting. I'll see you\nthere. So we've got set up. We've got access to pytorch. We've got a Google Colab instance running\nhere. We've got a GPU because we've gone up to runtime, change runtime type, hardware accelerator.\nYou won't necessarily need a GPU for this entire notebook, but I just wanted to show you how to\nget access to a GPU because we're going to be using them later on. So let's get rid of this.\nAnd one last thing, how I'd recommend going through this course is in a split window fashion.\nSo for example, you might have the video where I'm talking right now and writing code on the\nleft side, and then you might have another window over the other side with your own Colab\nwindow. And you can go new notebook, call it whatever you want, my notebook. You could call it very\nsimilar to what we're writing here. And then if I write code over on this side, on this video,\nyou can't copy it, of course, but you'll write the same code here and then go on and go on and\ngo on. And if you get stuck, of course, you have the reference notebook and you have an\nopportunity to ask a question here. So with that being said, let's get started. The first thing\nwe're going to have a look at in PyTorch is an introduction to tenses. So tenses are the main\nbuilding block of deep learning in general, or data. And so you may have watched the video,\nwhat is a tensor? For the sake of this course, tenses are a way to represent data, especially\nmulti dimensional data, numeric data that is, but that numeric data represents something else.\nSo let's go in here, creating tenses. So the first kind of tensor we're going to create is\nactually called a scalar. I know I'm going to throw a lot of different names of things at you,\nbut it's important that you're aware of such nomenclature. Even though in PyTorch, almost\neverything is referred to as a tensor, there are different kinds of tenses. And just to\nexemplify the fact that we're using a reference notebook, if we go up here, we can see we have\nimporting PyTorch. We've done that. Now we're up to introduction to tenses. We've got creating\ntenses, and we've got scalar, etc, etc, etc. So this is what we're going to be working through.\nLet's do it together. So scalar, the way to, oops, what have I done there? The way to create a\ntensor in PyTorch, we're going to call this scalar equals torch dot tensor. And we're going to fill\nit with the number seven. And then if we press or retype in scalar, what do we get back? Seven,\nwonderful. And it's got the tensor data type here. So how would we find out about what torch dot\ntensor actually is? Well, let me show you how I would. We go to torch dot tensor. There we go.\nWe've got the documentation. So this is possibly the most common class in PyTorch other than\none we're going to see later on that you'll use, which is torch dot nn. Basically, everything in\nPyTorch works off torch dot tensor. And if you'd like to learn more, you can read through here.\nIn fact, I would encourage you to read through this documentation for at least 10 minutes\nafter you finish some videos here. So with that being said, I'm going to link that in here.\nSo PyTorch tensors are created using torch dot tensor. And then we've got that link there.\nOops, typos got law Daniel. Come on. They're better than this. No, I'm kidding. There's going to be\ntypos got law through the whole course. Okay. Now, what are some attributes of a scalar? So\nsome details about scalars. Let's find out how many dimensions there are. Oh, and by the way,\nthis warning, perfect timing. Google Colab will give you some warnings here, depending on whether\nyou're using a GPU or not. Now, the reason being is because Google Colab provides GPUs to you and\nI for free. However, GPUs aren't free for Google to provide. So if we're not using a GPU, we can\nsave some resources, allow someone else to use a GPU by going to none. And of course, we can\nalways switch this back. So I'm going to turn my GPU off so that someone else out there,\nI'm not using the GPU at the moment, they can use it. So what you're also going to see is if\nyour Google Colab instance ever restarts up here, we're going to have to rerun these cells. So if\nyou stop coding for a while, go have a break and then come back and you start your notebook again,\nthat's one downside of Google Colab is that it resets after a few hours. How many hours? I don't\nknow exactly. The reset time is longer if you have the pro subscription, but because it's a free\nservice and the way Google calculate usage and all that sort of stuff, I can't give a conclusive\nevidence or conclusive answer on how long until it resets. But just know, if you come back, you might\nhave to rerun some of your cells and you can do that with shift and enter. So a scalar has no\ndimensions. All right, it's just a single number. But then we move on to the next thing. Or actually,\nif we wanted to get this number out of a tensor type, we can use scalar dot item, this is going\nto give it back as just a regular Python integer. Wonderful, there we go, the number seven back,\nget tensor back as Python int. Now, the next thing that we have is a vector. So let's write\nin here vector, which again is going to be created with torch dot tensor. But you will also hear\nthe word vector used a lot too. Now, what is the deal? Oops, seven dot seven. Google Colab's auto\ncomplete is a bit funny. It doesn't always do the thing you want it to. So if we see a vector,\nwe've got two numbers here. And then if we really wanted to find out what is a vector.\nSo a vector usually has magnitude and direction. So what we're going to see later on is, there we\ngo, magnitude, how far it's going and which way it's going. And then if we plotted it, we've got,\nyeah, a vector equals the magnitude would be the length here and the direction would be where it's\npointing. And oh, here we go, scalar vector matrix tensor. This is what we're working on as well.\nSo the thing about vectors, how they differ with scalars is how I just remember them is\nrather than magnitude and direction is a vector typically has more than one number.\nSo if we go vector and dim, how many dimensions does it have?\nIt has one dimension, which is kind of confusing. But when we see tensors with more than one\ndimension, it'll make sense. And another way that I remember how many dimensions something\nhas is by the number of square brackets. So let's check out something else. Maybe we go vector\ndot shape shape is two. So the difference between dimension. So dimension is like number of square\nbrackets. And when I say, even though there's two here, I mean number of pairs of closing square\nbrackets. So there's one pair of closing square brackets here. But the shape of the vector is two.\nSo we have two by one elements. So that means a total of two elements. Now if we wanted to step\nthings up a notch, let's create a matrix. So this is another term you're going to hear.\nAnd you might be wondering why I'm capitalizing matrix. Well, I'll explain that in the second\nmatrix equals torch dot tensor. And we're going to put two square brackets here. You might be\nthinking, what could the two square brackets mean? Or actually, that's a little bit of a challenge.\nIf one pair of square brackets had an endem of one, what will the endem be number of dimensions\nof two square brackets? So let's create this matrix. Beautiful. So we've got another tensor here.\nAgain, as I said, these things have different names, like the traditional name of scalar,\nvector matrix, but they're all still a torch dot tensor. That's a little bit confusing,\nbut the thing you should remember in PyTorch is basically anytime you encode data into numbers,\nit's of a tensor data type. And so now how many n number of dimensions do you think a matrix has?\nIt has two. So there we go. We have two square brackets. So if we wanted to get matrix,\nlet's index on the zeroth axis. Let's see what happens there. Ah, so we get seven and eight.\nAnd then we get off the first dimension. Ah, nine and 10. So this is where the square brackets,\nthe pairings come into play. We've got two square bracket pairings on the outside here.\nSo we have an endem of two. Now, if we get the shape of the matrix, what do you think the shape will be?\nAh, two by two. So we've got two numbers here by two. So we have a total of four elements in there.\nSo we're covering a fair bit of ground here, nice and quick, but that's going to be the\nteaching style of this course is we're going to get quite hands on and writing a lot of code and\njust interacting with it rather than continually going back over and discussing what's going on\nhere. The best way to find out what's happening within a matrix is to write more code that's similar\nto these matrices here. But let's not stop at matrix. Let's upgrade to a tensor now. So I might\nput this in capitals as well. And I haven't explained what the capitals mean yet, but we'll see that\nin a second. So let's go torch dot tensor. And what we're going to do is this time,\nwe've done one square bracket pairing. We've done two square bracket pairings. Let's do three\nsquare bracket pairings and just get a little bit adventurous. All right. And so you might be thinking\nat the moment, this is quite tedious. I'm just going to write a bunch of random numbers here. One,\ntwo, three, three, six, nine, two, five, four. Now you might be thinking, Daniel, you've said\ntensors could have millions of numbers. If we had to write them all by hand, that would be\nquite tedious. And yes, you're completely right. The fact is, though, that most of the time,\nyou won't be crafting tensors by hand. PyTorch will do a lot of that behind the scenes. However,\nit's important to know that these are the fundamental building blocks of the models\nand the deep learning neural networks that we're going to be building. So tensor capitals as well,\nwe have three square brackets. So, or three square bracket pairings. I'm just going to refer to three\nsquare brackets at the very start because they're going to be paired down here. How many n dim or\nnumber of dimensions do you think our tensor will have? Three, wonderful. And what do you think the\nshape of our tensor is? We have three elements here. We have three elements here, three elements\nhere. And we have one, two, three. So maybe our tensor has a shape of one by three by three.\nHmm. What does that mean? Well, we've got three by one, two, three. That's the second square\nbracket there by one. Ah, so that's the first dimension there or the zeroth dimension because\nwe remember PyTorch is zero indexed. We have, well, let's just instead of talking about it,\nlet's just get on the zeroth axis and see what happens with the zeroth dimension. There we go.\nOkay. So there's, this is the far left one, zero, which is very confusing because we've got a one\nhere, but so we've got, oops, don't mean that. What this is saying is we've got one three by three\nshape tensor. So very outer bracket matches up with this number one here. And then this three\nmatches up with the next one here, which is one, two, three. And then this three matches up with\nthis one, one, two, three. Now, if you'd like to see this with a pretty picture, we can see it here.\nSo dim zero lines up. So the blue bracket, the very outer one, lines up with the one. Then dim\nequals one, this one here, the middle bracket, lines up with the middle dimension here. And then\ndim equals two, the very inner lines up with these three here. So again, this is going to take a lot\nof practice. It's taken me a lot of practice to understand the dimensions of tensors. But\nto practice, I would like you to write out your own tensor of, you can put however many square\nbrackets you want. And then just interact with the end dim shape and indexing, just as I've done\nhere, but you can put any combination of numbers inside this tensor. That's a little bit of practice\nbefore the next video. So give that a shot and then we'll move on to the next topic.\nI'll see you there. Welcome back. In the last video, we covered the basic building blocks of data\nrepresentation in deep learning, which is the tensor, or in PyTorch, specifically torch.tensor.\nBut within that, we had to look at what a scalar is. We had to look at what a vector is. We had to\nlook at a matrix. We had to look at what a tensor is. And I issued you the challenge to get as\ncreative as you like with creating your own tensor. So I hope you gave that a shot because as you'll\nsee throughout the course and your deep learning journey, a tensor can represent or can be of almost\nany shape and size and have almost any combination of numbers within it. And so this is very important\nto be able to interact with different tensors to be able to understand what the different names of\nthings are. So when you hear matrix, you go, oh, maybe that's a two dimensional tensor. When you\nhear a vector, maybe that's a one dimensional tensor. When you hear a tensor, that could be any\namount of dimensions. And just for reference for that, if we come back to the course reference,\nwe've got a scalar. What is it? A single number, number of dimensions, zero. We've got a vector,\na number with direction, number of dimensions, one, a matrix, a tensor. And now here's another little\ntidbit of the nomenclature of things, the naming of things. Typically, you'll see a variable name\nfor a scalar or a vector as a lowercase. So a vector, you might have a lowercase y storing that\ndata. But for a matrix or a tensor, you'll often see an uppercase letter or variable in Python in\nour case, because we're writing code. And so I am not exactly sure why this is, but this is just\nwhat you're going to see in machine learning and deep learning code and research papers\nacross the board. This is a typical nomenclature. Scalars and vectors, lowercase, matrix and tensors,\nuppercase, that's where that naming comes from. And that's why I've given the tensor uppercase here.\nNow, with that being said, let's jump in to another very important concept with tensors.\nAnd that is random tensors. Why random tensors? I'm just writing this in a code cell now.\nI could go here. This is a comment in Python, random tensors. But we'll get rid of that. We could\njust start another text cell here. And then three hashes is going to give us a heading, random tensors\nthere. Or I could turn this again into a markdown cell with command mm when I'm using Google Colab.\nSo random tensors. Let's write down here. Why random tensors? So we've done the tedious thing\nof creating our own tensors with some numbers that we've defined, whatever these are. Again,\nyou could define these as almost anything. But random tensors is a big part in pytorch because\nlet's write this down. Random tensors are important because the way many neural networks learn is\nthat they start with tensors full of random numbers and then adjust those random numbers\nto better represent the data. So seriously, this is one of the big concepts of neural networks.\nI'm going to write in code here, which is this is what the tick is for. Start with random numbers.\nLook at data, update random numbers. Look at data, update random numbers. That is the crux\nof neural networks. So let's create a random tensor with pytorch. Remember how I said that\npytorch is going to create tensors for you behind the scenes? Well, this is one of the ways that\nit does so. So we create a random tensor and we give it a size of random tensor of size or shape.\nPytorch use these independently. So size, shape, they mean the different versions of the same thing.\nSo random tensor equals torch dot rand. And we're going to type in here three, four. And the beautiful\nthing about Google Colab as well is that if we wait long enough, it's going to pop up with the doc\nstring of what's going on. I personally find this a little hard to read in Google Colab,\nbecause you see you can keep going down there. You might be able to read that. But what can we do?\nWell, we can go to torch dot rand. Then we go to the documentation. Beautiful. Now there's a whole\nbunch of stuff here that you're more than welcome to read. We're not going to go through all that.\nWe're just going to see what happens hands on. So we'll copy that in here. And write this in notes,\ntorch random tensors. Done. Just going to make some code cells down here. So I've got some space.\nI can get this a bit up here. Let's see what our random tensor looks like. There we go. Beautiful\nof size three, four. So we've got three or four elements here. And then we've got three deep\nhere. So again, there's the two pairs. So what do you think the number of dimensions will be\nfor random tensor? And dim. Two beautiful. And so we have some random numbers here. Now the\nbeautiful thing about pie torch again is that it's going to do a lot of this behind the scenes. So\nif we wanted to create a size of 10 10, in some cases, we won't want one dimension here. And then\nit's going to go 10 10. And then if we check the number of dimensions, how many do you think it\nwill be now three? Why is that? Because we've got one 10 10. And then if we wanted to create 10 10 10.\nWhat's the number of dimensions going to be? It's not going to change. Why is that?\nWe haven't run that cell yet, but we've got a lot of numbers here.\nWe can find out what 10 times 10 times 10 is. And I know we can do that in our heads, but\nthe beauty of collab is we've got a calculator right here. 10 times 10 times 10. We've got a\nthousand elements in there. But sometimes tenses can be hundreds of thousands of elements or\nmillions of elements. But pie torch is going to take care of a lot of this behind the scenes. So\nlet's clean up a bit of space here. This is a random tensor. Random numbers beautiful of now\nit's got two dimensions because we've got three by four. And if we put another one in the front\nthere, we're going to have how many dimensions three dimensions there. But again, this number\nof dimensions could be any number. And what's inside here could be any number. Let's get rid of that.\nAnd let's get a bit specific because right now this is just a random tensor of whatever dimension.\nHow about we create a random tensor with similar shape to an image tensor. So a lot of the time\nwhen we turn images, image size tensor, when we turn images into tenses, they're going to have,\nlet me just write it in code for you first, size equals a height, a width, and a number of color\nchannels. And so in this case, it's going to be height with color channels. And the color channels\nare red, green, blue. And so let's create a random image tensor. Let's view the size of it or the\nshape. And then random image size tensor will view the end dim. Beautiful. Okay, so we've got\ntorch size, the same size two, two, four, two, four, three, height, width, color channels. And we've got\nthree dimensions, one, four, height, width, color channels. Let's go and see an example of this. This\nis the PyTorch Fundamentals notebook. If we go up to here, so say we wanted to encode this image\nof my dad eating pizza with thumbs up of a square image of two, two, four by two, two, four.\nThis is an input. And if we wanted to encode this into tensor format, well, one of the ways of\nrepresenting an image tensor, very common ways is to split it into color channels because with\nred, green, and blue, you can create almost any color you want. And then we have a tensor\nrepresentation. So sometimes you're going to see color channels come first. We can switch this\naround and our code quite easily by going color channels here. But you'll also see color channels\ncome at the end. I know I'm saying a lot that we kind of haven't covered yet. The main takeaway\nfrom here is that almost any data can be represented as a tensor. And one of the common ways to represent\nimages is in the format color channels, height, width, and how these values are will depend on\nwhat's in the image. But we've done this in a random way. So the takeaway from this video is\nthat PyTorch enables you to create tensors quite easily with the random method. However, it is\ngoing to do a lot of this creating tensors for you behind the scenes and why a random tensor is so\nvaluable because neural networks start with random numbers, look at data such as image tensors,\nand then adjust those random numbers to better represent that data. And they repeat those steps\nonwards and onwards and onwards. Let's finish this video here. I'm going to challenge for you\njust to create your own random tensor of whatever size and shape you want. So you could have 5, 10,\n10 here and see what that looks like. And then we'll keep coding in the next video.\nI hope you took on the challenge of creating random tensor of your own size. And just a little\ntidbit here. You might have seen me in the previous video. I didn't use the size parameter. But in\nthis case, I did here, you can go either way. So if we go torch dot rand size equals, we put in a\ntuple here of three three, we've got that tensor there three three. But then also if we don't put\nthe size in there, it's the default. So it's going to create a very similar tensor. So whether you\nhave this size or not, it's going to have quite a similar output depending on the shape that you\nput in there. But now let's get started to another kind of tensor that you might see zeros and ones.\nSo say you wanted to create a tensor, but that wasn't just full of random numbers,\nyou wanted to create a tensor of all zeros. This is helpful for if you're creating some form of\nmask. Now, we haven't covered what a mask is. But essentially, if we create a tensor of all zeros,\nwhat happens when you multiply a number by zero? All zeros. So if we wanted to multiply\nthese two together, let's do zeros times random tensor.\nThere we go, all zeros. So maybe if you're working with this random tensor and you wanted to mask\nout, say all of the numbers in this column for some reason, you could create a tensor of zeros in\nthat column, multiply it by your target tensor, and you would zero all those numbers. That's telling\nyour model, hey, ignore all of the numbers that are in here because I've zeroed them out. And then\nif you wanted to create a tensor of all ones, create a tensor of all ones, we can go ones equals\ntorch dot ones, size equals three, four. And then if we have a look, there's another parameter I\nhaven't showed you yet, but this is another important one is the D type. So the default data type,\nso that's what D type stands for, is torch dot float. We've actually been using torch dot float\nthe whole time, because that's whenever you create a tensor with pytorch, we're using a pytorch\nmethod, unless you explicitly define what the data type is, we'll see that later on, defining\nwhat the data type is, it starts off as torch float 32. So these are float numbers. So that\nis how you create zeros and ones zeros is probably I've seen more common than ones in use, but just\nkeep these in mind, you might come across them. There are lots of different methods to creating\ntensors. And truth be told, like random is probably one of the most common, but you might see zeros\nand ones out in the field. So now we've covered that. Let's move on into the next video, where\nwe're going to create a range. So have a go at creating a tensor full of zeros and whatever size\nyou want, and a tensor full of ones and whatever size you want. And I'll see you in the next video.\nWelcome back. I hope you took on the challenge of creating a torch tensor of zeros of your\nown size and ones of your own size. But now let's investigate how we might create a range of\ntensors and tensors like. So these are two other very common methods of creating tensors.\nSo let's start by creating a range. So we'll first use torch dot range, because depending on\nwhen you're watching this video, torch dot range may be still in play or it may be deprecated.\nIf we write in torch dot range right now with the pie torch version that I'm using, which is\ntorch dot version, which is torch or pie torch 1.10 point zero torch range is deprecated and\nwill be removed in a future release. So just keep that in mind. If you come across some code that's\nusing torch dot range, maybe out of whack. So the way to get around that is to fix that is to use\na range instead. And if we just write in torch dot a range, we've got tensors of zero to nine,\nbecause it of course starts at zero index. If we wanted one to 10, we could go like this.\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10. And we can go zero, or we go 1, 2, 10, equals torch a range.\nWonderful. And we can also define the step. So let's let's type in some start and where can we\nfind the documentation on a range? Sometimes in Google Colab, you can press shift tab,\nbut I find that it doesn't always work for me. Yeah, you could hover over it, but we can also just\ngo torch a range and look for the documentation torch a range. So we've got start and step. Let's\nsee what all of these three do. Maybe we start at zero, and maybe we want it to go to a thousand,\nand then we want a step of what should our step be? What's a fun number? 77. So it's not one to 10\nanymore, but here we go. We've got start at zero, 77 plus 77 plus 77, all the way up to it finishes\nat a thousand. So if we wanted to take it back to one to 10, we can go up here. 110, and the default\nstep is going to be one. Oops, we needed the end to be that it's going to finish at end minus one.\nThere we go. Beautiful. Now we can also create tensors like. So creating tensors like. So tensors\nlike is say you had a particular shape of a tensor you wanted to replicate somewhere else, but you\ndidn't want to explicitly define what that shape should be. So what's the shape of one to 10?\nOne to 10. Now if we wanted to create a tensor full of zeros that had the same shape as this,\nwe can use tensor like or zeros like. So 10 zeros, zeros equals, I'm not even sure if I'm\nspelling zeros right then, zeros. Well, I might have a typo spelling zeros here, but you get what\nI'm saying is torch zeros. Oh, torch spell it like that. That's why I'm spelling it like that.\nZeros like one to 10. And then the input is going to be one to 10. And we have a look at 10 zeros.\nMy goodness, this is taking quite the while to run. This is troubleshooting on the fly.\nIf something's happening like this, you can try to stop. If something was happening like that,\nyou can click run and then stop. Well, it's running so fast that I can't click stop. If you do also\nrun into trouble, you can go runtime, restart runtime. We might just do that now just to show you.\nRestart and run all is going to restart the compute engine behind the collab notebook.\nAnd run all the cells to where we are. So let's just see that we restart and run runtime. If you're\ngetting errors, sometimes this helps. There is no set in stone way to troubleshoot errors. It's\nguess and check with this. So there we go. We've created 10 zeros, which is torch zeros like\nour one to 10 tensor. So we've got zeros in the same shape as one to 10. So if you'd like to create\ntensors, use torch arrange and get deprecated message. Use torch arrange instead for creating\na range of tensors with a start and end in a step. And then if you wanted to create tensors\nor a tensor like something else, you want to look for the like method. And then you put an input,\nwhich is another tensor. And then it'll create a similar tensor with whatever this method here\nis like in that fashion or in the same shape as your input. So with that being said,\ngive that a try, create a range of tensors, and then try to replicate that range shape that you've\nmade with zeros. I'll see you in the next video. Welcome back. Let's now get into a very important\ntopic of tensor data types. So we've briefly hinted on this before. And I said that let's create\na tensor to begin with float 32 tensor. And we're going to go float 32 tensor equals torch\ndot tensor. And let's just put in the numbers three, six, nine. If you've ever played need for\nspeed underground, you'll know where three, six, nine comes from. And then we're going to go\nD type equals, let's just put none and see what happens, hey, float 32 tensor. Oh, what is the\ndata type? float 32, tensor dot D type. float 32, even though we put none, this is because\nthe default data type in pytorch, even if it's specified as none is going to come out as float 32.\nWhat if we wanted to change that to something else? Well, let's type in here float 16.\nAnd now we've got float 32 tensor. This variable name is a lie now because it's a float 16 tensor.\nSo we'll leave that as none. Let's go there. There's another parameter when creating tensors.\nIt's very important, which is device. So we'll see what that is later on. And then there's a\nfinal one, which is also very important, which is requires grad equals false. Now this could be\ntrue, of course, we're going to set this as false. So these are three of the most important parameters\nwhen you're creating tensors. Now, again, you won't necessarily always have to enter these when\nyou're creating tensors, because pytorch does a lot of tensor creation behind the scenes for you.\nSo let's just write out what these are. Data type is what data type is the tensor, e.g. float 32,\nor float 16. Now, if you'd like to look at what data types are available for pytorch tensors,\nwe can go torch tensor and write up the top unless the documentation changes. We have data types.\nIt's so important that data types is the first thing that comes up when you're creating a tensor.\nSo we have 32-bit floating point, 64-bit floating point, 16, 16, 32-bit complex. Now,\nthe most common ones that you will likely interact with are 32-bit floating point and 16-bit floating\npoint. Now, what does this mean? What do these numbers actually mean? Well, they have to do with\nprecision in computing. So let's look up that. Precision in computing. Precision computer science.\nSo in computer science, the precision of a numerical quantity, we're dealing with numbers, right?\nAs a measure of the detail in which the quantity is expressed. This is usually measured in bits,\nbut sometimes in decimal digits. It is related to precision in mathematics, which describes the\nnumber of digits that are used to express a value. So, for us, precision is the numerical quantity,\nis a measure of the detail, how much detail in which the quantity is expressed. So, I'm not going\nto dive into the background of computer science and how computers represent numbers. The important\ntakeaway for you from this will be that single precision floating point is usually called float\n32, which means, yeah, a number contains 32 bits in computer memory. So if you imagine, if we have\na tensor that is using 32 bit floating point, the computer memory stores the number as 32 bits.\nOr if it has 16 bit floating point, it stores it as 16 bits or 16 numbers representing or 16.\nI'm not sure if a bit equates to a single number in computer memory. But what this means is that\na 32 bit tensor is single precision. This is half precision. Now, this means that it's the default\nof 32, float 32, torch dot float 32, as we've seen in code, which means it's going to take up\na certain amount of space in computer memory. Now, you might be thinking, why would I do anything\nother than the default? Well, if you'd like to sacrifice some detail in how your number is\nrepresented. So instead of 32 bits, it's represented by 16 bits, you can calculate faster on numbers\nthat take up less memory. So that is the main differentiator between 32 bit and 16 bit. But if\nyou need more precision, you might go up to 64 bit. So just keep that in mind as you go forward.\nSingle precision is 32. Half precision is 16. What do these numbers represent? They represent\nhow much detail a single number is stored in memory. That was a lot to take in. But we're talking\nabout 10 to data types. I'm spending a lot of time here, because I'm going to put a note here,\nnote, tensor data types is one of the three big issues with pytorch and deep learning or\nnot not issues, they're going to be errors that you run into and deep learning. Three big\nerrors, you'll run into with pytorch and deep learning. So one is tensors, not right data type.\nTwo tensors, not right shape. We've seen a few shapes of four and three tensors, not on the right\ndevice. And so in this case, if we had a tensor that was float 16 and we were trying to do computations\nwith a tensor that was float 32, we might run into some errors. And so that's the tensors not\nbeing in the right data type. So it's important to know about the D type parameter here. And then\ntensors not being the right shape. Well, that's once we get onto matrix multiplication, we'll see\nthat if one tensor is a certain shape and another tensor is another shape and those shapes don't\nline up, we're going to run into shape errors. And this is a perfect segue to the device.\nDevice equals none. By default, this is going to be CPU. This is why we are using Google Colab\nbecause it enables us to have access to, oh, we don't want to restart, enables us to have access\nto a GPU. As I've said before, a GPU enables us. So we could change this to CUDA. That would be,\nwe'll see how to write device agnostic code later on. But this device, if you try to do\noperations between two tensors that are not on the same device. So for example, you have one tensor\nthat lives on a GPU for fast computing, and you have another tensor that lives on a CPU and you\ntry to do something with them, while pytorch is going to throw you an error. And then finally,\nthis last requirement is grad is if you want pytorch to track the gradients, we haven't covered\nwhat that is of a tensor when it goes through certain numerical calculations. This is a bit of\na bombardment, but I thought I'd throw these in as important parameters to be aware of since\nwe're discussing data type. And really, it would be reminiscent of me to discuss data type without\ndiscussing not the right shape or not the right device. So with that being said, let's write down\nhere what device is your tensor on, and whether or not to track gradients with this tensor's\noperations. So we have a float 32 tensor. Now, how might we change the tensor data type of this?\nLet's create float 16 tensor. And we saw that we could explicitly write in float 16 tensor.\nOr we can just type in here, float 16 tensor equals float 32 tensor dot type. And we're going to type\nin torch dot float 16, why float 16, because well, that's how we define float 16, or we could use\nhalf. So the same thing, these things are the same, let's just do half, or float 16 is more\nexplicit for me. And then let's check out float 16 tensor. Beautiful, we've converted our float\n32 tensor into float 16. So that is one of the ways that you'll be able to tackle the tensors\nnot in the right data type issue that you run into. And just a little note on the precision\nand computing, if you'd like to read more on that, I'm going to link this in here. And this is all\nabout how computers store numbers. So precision in computing. There we go. I'll just get rid of that.\nWonderful. So give that a try, create some tensors, research, or go to the documentation of torch\ndot tensor and see if you can find out a little bit more about D type device and requires grad,\nand create some tensors of different data types. Play around with whatever the ones you want here,\nand see if you can run into some errors, maybe try to multiply two tensors together. So if you go\nfloat 16 tensor times float 32 tensor, give that a try and see what happens. I'll see you in the next\nvideo. Welcome back. In the last video, we covered a little bit about tensor data types,\nas well as some of the most common parameters you'll see past to the torch dot tensor method.\nAnd so I should do the challenge at the end of the last video to create some of your own tensors\nof different data types, and then to see what happens when you multiply a float 16 tensor by a\nfloat 32 tensor. Oh, it works. And but you've like Daniel, you said that you're going to have tensors\nnot the right data type. Well, this is another kind of gotcha or caveat of pie torch and deep\nlearning in general, is that sometimes you'll find that even if you think something may error\nbecause these two tensors are different data types, it actually results in no error. But then\nsometimes you'll have other operations that you do, especially training large neural networks,\nwhere you'll get data type issues. The important thing is to just be aware of the fact that some\noperations will run an error when your tensors are not in the right data type. So let's try another\ntype. Maybe we try a 32 bit integer. So torch dot in 32. And we try to multiply that by a float.\nWonder what will happen then? So let's go into 32 in 32 tensor equals torch dot tensor. And we'll\njust make it three. Notice that there's no floats there or no dot points to make it a float.\nThree, six, nine and D type can be torch in 32. And then in 32 tensor, what does this look like?\nTypo, of course, one of many in 32 tensor. So now let's go float 32 tensor and see what happens.\nCan we get pie torch to throw an error in 32 tensor?\nHuh, it worked as well. Or maybe we go into 64. What happens here?\nStill works. Now, see, this is again one of the confusing parts of doing tensor operations.\nWhat if we do a long tensor? Torch to long. Is this going to still work?\nAh, torch has no attribute called long. That's not a data type issue.\nI think it's long tensor. Long tensor. Does this work? D type must be torch D type.\nTorch long tensor. I could have sworn that this was torch dot tensor.\nOh, there we go. Torch dot long tensor. That's another word for 64 bit.\nSo what is this saying? CPU tensor. Okay, let's see. This is some troubleshooting on the fly here.\nThen we multiply it. This is a float 32 times a long. It works. Okay, so it's actually a bit\nmore robust than what I thought it was. But just keep this in mind when we're training models,\nwe're probably going to run into some errors at some point of our tensor's not being the\nright data type. And if pie torch throws us an error saying your tensors are in the wrong data\ntype, well, at least we know now how to change that data type or how to set the data type if we\nneed to. And so with that being said, let's just formalize what we've been doing a fair bit already.\nAnd that's getting information from tensors. So the three big things that we'll want to get\nfrom our tensors in line with the three big errors that we're going to face in neural networks and\ndeep lining is let's copy these down. Just going to get this, copy this down below. So if we want\nto get some information from tensors, how do we check the shape? How do we check the data type?\nHow do we check the device? Let's write that down. So to get information from this, to get\nD type or let's write data type from a tensor can use tensor dot D type. And let's go here to get\nshape from a tensor can use tensor dot shape. And to get device from a tensor, which devices it on\nCPU or GPU can use tensor dot device. Let's see these three in action. So if we run into one of\nthe three big problems in deep learning and neural networks in general, especially with PyTorch,\ntensor's not the right data type, tensor's not the right shape or tensor's not on the right device.\nLet's create a tensor and try these three out. We've got some tensor equals torch dot\nrand and we'll create it a three four. Let's have a look at what it looks like.\nThere we go. Random numbers of shape three and four. Now let's find out some details about it.\nFind out details about some tensor. So print or print some tensor.\nAnd oops, didn't want that print. And let's format it or make an F string of shape of tensor.\nOh, let's do data type first. We'll follow that order.\nData type of tensor. And we're going to go, how do we do this? Some tensor dot what?\nDot d type. Beautiful. And then we're going to print tensors not in the right shape. So let's go\nshape of tensor equals some tensor dot shape. Oh, I went a bit too fast, but we could also use\nsize. Let's just confirm that actually. We'll code that out together. From my experience,\nsome tensor dot size, and some tensor dot shape result in the same thing. Is that true? Oh, function.\nOh, that's what it is. Some tensor dot size is a function, not an attribute.\nThere we go. Which one should you use? For me, I'm probably more used to using shape. You may come\nacross dot size as well, but just realize that they do quite the same thing except one's a function\nand one's an attribute. An attribute is written dot shape without the curly brackets. A function\nor a method is with the brackets at the end. So that's the difference between these are attributes\nhere. D type size. We're going to change this to shape. Tensor attributes. This is what we're\ngetting. I should probably write that down. This is tensor attributes. That's the formal name for\nthese things. And then finally, what else do we want? Tensors, what device are we looking for?\nLet's get rid of this, get rid of this. And then print f device tensor is on. By default,\nour tensor is on the CPU. So some tensor dot device. There we go. So now we've got our tensor\nhere, some tensor. The data type is a torch float 32 because we didn't change it to anything else.\nAnd torch float 32 is the default. The shape is three four, which makes a lot of sense because\nwe passed in three four here. And the device tensor is on is the CPU, which is, of course,\nthe default, unless we explicitly say to put it on another device, all of the tensors that we\ncreate will default to being on the CPU, rather than the GPU. And we'll see later on how to put\ntensors and other things in torch onto a GPU. But with that being said, give it a shot,\ncreate your own tensor, get some information from that tensor, and see if you can change\nthese around. So see if you could create a random tensor, but instead of float 32, it's a float 16.\nAnd then probably another extracurricular, we haven't covered this yet. But see how to change\nthe device a pytorch tensor is on. Give that a crack. And I'll see you in the next video.\nWelcome back. So in the last video, we had a look at a few tensor attributes, namely the data\ntype of a tensor, the shape of a tensor, and the device that a tensor lives on. And I alluded to\nthe fact that these will help resolve three of the most common issues in building neural networks,\ndeep learning models, specifically with pytorch. So tensor has not been the right data type,\ntensor has not been the right shape, and tensor has not been on the right device. So now let's\nget into manipulating tensors. And what I mean by that, so let's just write here the title,\nmanipulating tensors. And this is going to be tensor operations. So when we're building neural\nnetworks, neural networks are comprised of lots of mathematical functions that pytorch code is going\nto run behind the scenes for us. So let's go here, tensor operations include addition,\nsubtraction, and these are the regular addition, subtraction, multiplication. There's two types\nof multiplication in that you'll typically see referenced in deep learning and neural networks,\ndivision, and matrix multiplication. And these, the ones here, so addition, subtraction,\nmultiplication, division, your typical operations that you're probably familiar with matrix multiplication.\nThe only different one here is matrix multiplication. We're going to have a look at that in a minute.\nBut to find patterns in numbers of a data set, a neural network will combine these functions\nin some way, shape or form. So it takes a tensor full of random numbers, performs some kind of\ncombination of addition, subtraction, multiplication, division, matrix multiplication. It doesn't have\nto be all of these. It could be any combination of these to manipulate these numbers in some way\nto represent a data set. So that's how a neural network learns is it will just comprise these\nfunctions, look at some data to adjust the numbers of a random tensor, and then go from there. But\nwith that being said, let's look at a few of these. So we'll begin with addition. First thing we need\nto do is create a tensor. And to add something to a tensor, we'll just go torch tensor. Let's go one,\ntwo, three, add something to a tensor is tensor plus, we can use plus as the addition operator,\njust like in Python, tensor plus 10 is going to be tensor 11, 12, 13, tensor plus 100 is going to be\nas you'd expect plus 100. Let's leave that as plus 10 and add 10 to it. And so you might be\nable to guess how we would multiply it by 10. So let's go multiply tensor by 10. We can go tensor,\nstar, which are my keyboard shift eight, 10. We get 10, 10, 10. And because we didn't reassign it,\nour tensor is still 123. So if we go, if we reassign it here, tensor equals tensor by 10,\nand then check out tensor, we've now got 10 2030. And the same thing here, we'll have 10 2030. But\nthen if we go back from the top, if we delete this reassignment, oh, what do we get there, tensor\nby 10. Oh, what's happened here? Oh, because we've got, yeah, okay, I see, tensor by 10, tensor,\nstill 123. What should we try now? How about subtract subtract 10 equals tensor minus 10.\nAnd you can also use, well, there we go, one minus 10, eight minus 10, three minus 10.\nYou can also use like torch has inbuilt functions or pytorch. So try out pytorch\ninbuilt functions. So torch dot mall is short for multiply. We can pass in our tensor here,\nand we can add in 10. That's going to multiply each element of tensor by 10. So just taking\nthe original tensor that we created, which is 123. And performing the same thing as this,\nI would recommend where you can use the operators from Python. If for some reason, you see torch\ndot mall, maybe there's a reason for that. But generally, these are more understandable if you\njust use the operators, if you need to do a straight up multiplication, straight up addition, or straight\nup subtraction, because torch also has torch dot add, torch dot add, is it torch dot add? It might\nbe torch dot add. I'm not sure. Oh, there we go. Yeah, torch dot add. So as I alluded to before,\nthere's two different types of multiplication that you'll hear about element wise and matrix\nmultiplication. We're going to cover matrix multiplication in the next video. As a challenge,\nthough, I would like you to search what is matrix multiplication. And I think the first website that\ncomes up, matrix multiplication, Wikipedia, yeah, math is fun. It has a great guide. So before we\nget into matrix multiplication, jump into math is fun to have a look at matrix multiplying,\nand have a think about how we might be able to replicate that in pie torch. Even if you're not\nsure, just have a think about it. I'll see you in the next video. Welcome back. In the last video,\nwe discussed some basic tensor operations, such as addition, subtraction, multiplication,\nelement wise, division, and matrix multiplication. But we didn't actually go through what matrix\nmultiplication is. So now let's start on that more particularly discussing the difference between\nelement wise and matrix multiplication. So we'll come down here, let's write another heading,\nmatrix multiplication. So there's two ways, or two main ways. Yeah, let's write that two main\nways of performing multiplication in neural networks and deep learning. So one is the simple\nversion, which is what we've seen, which is element wise multiplication. And number two is matrix\nmultiplication. So matrix multiplication is actually possibly the most common tensor operation you\nwill find inside neural networks. And in the last video, I issued the extra curriculum of having a\nlook at the math is fun dot com page for how to multiply matrices. So the first example they go\nthrough is element wise multiplication, which just means multiplying each element by a specific\nnumber. In this case, we have two times four equals eight, two times zero equals zero, two times one\nequals two, two times negative nine equals negative 18. But then if we move on to matrix\nmultiplication, which is multiplying a matrix by another matrix, we need to do the dot product.\nSo that's something that you'll also hear matrix multiplication referred to as the dot product.\nSo these two are used interchangeably matrix multiplication or dot product. And if we just\nlook up the symbol for dot product, you'll find that it's just a dot. There we go, a heavy dot,\nimages. There we go, a dot B. So this is vector a dot product B. A few different options there,\nbut let's look at what it looks like in pytorch code. But first, there's a little bit of a\ndifference here. So how did we get from multiplying this matrix here of one, two, three, four, five,\nsix, times seven, eight, nine, 10, 11, 12? How did we get 58 there? Well, we start by going,\nthis is the difference between element wise and dot product, by the way, one times seven.\nWe'll record that down there. So that's seven. And then two times nine. So this is first row,\nfirst column, two times nine is 18. And then three times 11 is 33. And if we add those up,\nseven plus 18, plus 33, we get 58. And then if we were to do that for each other element that's\nthroughout these two matrices, we end up with something like this. So that's what I'd encourage\nyou to go through step by step and reproduce this a good challenge would be to reproduce this by\nhand with pytorch code. But now let's go back and write some pytorch code to do both of these. So\nI just want to link here as well, more information on multiplying matrices. So I'm going to turn\nthis into markdown. Let's first see element wise, element wise multiplication. We're going to start\nwith just a rudimentary example. So if we have our tensor, what is it at the moment? It's 123.\nAnd then if we multiply that by itself, we get 149. But let's print something out so it looks a bit\nprettier than that. So print, I'm going to turn this into a string. And then we do that. So if we\nprint tensor times tensor, element wise multiplication is going to give us print equals. And then\nlet's do in here tensor times tensor. We go like that. Wonderful. So we get one times one\nequals one, two times two equals four, three times three equals nine. Now for matrix multiplication,\npytorch stores matrix multiplication, similar to torch dot mall in the torch dot mat mall space,\nwhich stands for matrix multiplication. So let's just test it out. Let's just true the exact\nsame thing that we did here, instead of element wise, we'll do matrix multiplication on our 123\ntensor. What happens here? Oh my goodness, 14. Now why did we get 14 instead of 149? Can you guess\nhow we got to 14 or think about how we got to 14 from these numbers? So if we recall back,\nwe saw that for we're only multiplying two smaller tensors, by the way, 123. This example is with\na larger one, but the same principle applies across different sizes of tensors or matrices.\nAnd when I say matrix multiplication, you can also do matrix multiplication between tensors.\nAnd in our case, we're using vectors just to add to the confusion. But what is the difference\nhere between element wise and dot product? Well, we've got one main addition. And that is addition.\nSo if we were to code this out by hand, matrix multiplication by hand, we'd have recall that\nthe elements of our tensor are 123. So if we wanted to matrix multiply that by itself,\nwe'd have one times one, which is the equivalent of doing one times seven in this visual example.\nAnd then we'd have plus, it's going to be two times two, two times two. What does that give us?\nPlus three times three. What does it give us? Three times three. That gives us 14. So that's how\nwe got to that number there. Now we could do this with a for loop. So let's have a gaze at when I\nsay gaze, it means have a look. That's a Australian colloquialism for having a look. But I want to\nshow you the time difference in it might not actually be that big a difference if we do it by hand\nversus using something like matmore. And that's another thing to note is that if PyTorch has a\nmethod already implemented, chances are it's a fast calculating version of that method. So I know\nfor basic operators, I said it's usually best to just use this straight up basic operator.\nBut for something like matrix multiplication or other advanced operators instead of the basic\noperators, you probably want to use the torch version rather than writing a for loop, which is\nwhat we're about to do. So let's go value equals zero. This is matrix multiplication by hand. So\nfor I in range, len tensor, so for each element in the length of our tensor, which is 123, we want to\nupdate our value to be plus equal, which is doing this plus reassignment here. The ith element in\neach tensor times the ith element. So times itself. And then how long is this going to take?\nLet's now return the value. We should get 14, print 14. There we go. So 1.9 milliseconds on\nwhatever CPU that Google collab is using behind the scenes. But now if we time it and use the torch\nmethod torch dot matmore, it was tensor dot sensor. And again, we're using a very small tensor. So\nokay, there we go. It actually showed how much quicker it is, even with such a small tensor.\nSo this is 1.9 milliseconds. This is 252 microseconds. So this is 10 times slower using a for loop,\nthen pie torches vectorized version. I'll let you look into that if you want to find out what\nvectorization means. It's just a type of programming that rather than writing for loops, because as\nyou could imagine, if this tensor was, let's say, had a million elements instead of just three,\nif you have to loop through each of those elements one by one, that's going to be quite cumbersome.\nSo a lot of pie torches functions behind the scenes implement optimized functions to perform\nmathematical operations, such as matrix multiplication, like the one we did by hand,\nin a far faster manner, as we can see here. And that's only with a tensor of three elements.\nSo you can imagine the speedups on something like a tensor with a million elements.\nBut with that being said, that is the crux of matrix multiplication. For a little bit more,\nI encourage you to read through this documentation here by mathisfun.com. Otherwise,\nlet's look at a couple of rules that we have to satisfy for larger versions of matrix multiplication.\nBecause right now, we've done it with a simple tensor, only 123. Let's step things up a notch\nin the next video. Welcome back. In the last video, we were introduced to matrix multiplication,\nwhich although we haven't seen it yet, is one of the most common operations in neural networks.\nAnd we saw that you should always try to use torches implementation of certain operations,\nexcept if they're basic operations, like plus multiplication and whatnot,\nbecause chances are it's a lot faster version than if you would do things by hand. And also,\nit's a lot less code. Like compared to this, this is pretty verbose code compared to just a matrix\nmultiply these two tensors. But there's something that we didn't allude to in the last video.\nThere's a couple of rules that need to be satisfied when performing matrix multiplication.\nIt worked for us because we have a rather simple tensor. But once you start to build larger tensors,\nyou might run into one of the most common errors in deep learning. I'm going to write this down\nactually here. This is one to be very familiar with. One of the most common errors in deep\nlearning, we've already alluded to this as well, is shape errors. So let's jump back to this in a\nminute. I just want to write up here. So there are two rules that performing or two main rules\nthat performing matrix multiplication needs to satisfy. Otherwise, we're going to get an error.\nSo number one is the inner dimensions must match. Let's see what this means.\nSo if we want to have two tensors of shape, three by two, and then we're going to use the at symbol.\nNow, we might be asking why the at symbol. Well, the at symbol is another, is a like an operator\nsymbol for matrix multiplication. So I just want to give you an example. If we go tensor at\nat stands for matrix multiplication, we get tensor 14, which is exactly the same as what we got there.\nShould you use at or should you use mat mall? I would personally recommend to use mat mall.\nIt's a little bit clearer at sometimes can get confusing because it's not as common as seeing\nsomething like mat mall. So we'll get rid of that, but I'm just using it up here for brevity.\nAnd then we're going to go three, two. Now, this won't work. We'll see why in a second.\nBut if we go two, three, at, and then we have three, two, this will work. Or, and then if we go\nthe reverse, say threes on the outside, twos here. And then we have twos on the inside and threes\non the outside, this will work. Now, why is this? Well, this is the rule number one. The inner\ndimensions must match. So the inner dimensions are what I mean by this is let's create torch\nround or create of size 32. And then we'll get its shape. So we have, so if we created a tensor\nlike this, three, two, and then if we created another tensor, well, let me just show you straight\nup torch dot mat mall torch dot ran to watch this won't work. We'll get an error. There we go. So\nthis is one of the most common errors that you're going to face in deep learning is that matrix\none and matrix two shapes cannot be multiplied because it doesn't satisfy rule number one.\nThe inner dimensions must match. And so what I mean by inner dimensions is this dimension multiplied\nby this dimension. So say we were trying to multiply three, two by three, two, these are the inner\ndimensions. Now this will work because why the inner dimensions match. Two, three by three, two,\ntwo, three by three, two. Now notice how the inner dimensions, inner, inner match. Let's see what\ncomes out here. Look at that. And now this is where rule two comes into play. Two. The resulting\nmatrix has the shape of the outer dimensions. So we've just seen this one two, three at three, two,\nwhich is at remember is matrix multiply. So we have a matrix of shape, two, three,\nmatrix multiply a matrix of three, two, the inner dimensions match. So it works. The resulting shape\nis what? Two, two. Just as we've seen here, we've got a shape of two, two. Now what if we did\nthe reverse? What if we did this one that also will work? Three on the outside. What do you think\nis going to happen here? In fact, I encourage you to pause the video and give it a go. So this\nis going to result in a three three matrix. But don't take my word for it. Let's have a look. Three,\nput two on the inside and we'll put two on the inside here and then three on the outside. What\ndoes it give us? Oh, look at that. A three three. One, two, three. One, two, three. Now what if we\nwere to change this? Two and two. This can be almost any number you want. Let's change them both\nto 10. What's going to happen? Will this work? What's the resulting shape going to be? So the\ninner dimensions match? What's rule number two? The resulting matrix has the shape of the outer\ndimension. So what do you think is going to be the shape of this resulting matrix multiplication?\nWell, let's have a look. It's still three three. Wow. Now what if we go 10? 10 on the outside\nand 10 and 10 on the inside? What do we get? Well, we get, I'm not going to count all of those,\nbut if we just go shape, we get 10 by 10. Because these are the two main rules of matrix multiplication\nis if you're running into an error that the matrix multiplication can't work. So let's say this was\n10 and this was seven. Watch what's going to happen? We can't multiply them because the inner\ndimensions do not match. We don't have 10 and 10. We have 10 and seven. But then when we change\nthis so that they match, we get 10 and 10. Beautiful. So now let's create a little bit more of a\nspecific example. We'll create two tenses. We'll come down. Actually, to prevent this video from\nbeing too long, I've got an error in the word error. That's funny. We'll go on with one of those\ncommon errors in deep learning shape errors. We've just seen it, but I'm going to get a little bit\nmore specific with that shape error in the next video. Before we do that, have a look at matrix\nmultiplication. There's a website, my other favorite website. I told you I've got two. This is my\nother one. Matrix multiplication dot XYZ. This is your challenge before the next video. Put in\nsome random numbers here, whatever you want, two, 10, five, six, seven, eight, whatever you want. Change\nthese around a bit, three, four. Well, that's a five, not a four. And then multiply and just watch\nwhat happens. That's all I'd like you to do. Just watch what happens and we're going to replicate\nsomething like this in PyTorch code in the next video. I'll see you there.\nWelcome back. In the last video, we discussed a little bit more about matrix multiplication,\nbut we're not done there. We looked at two of the main rules of matrix multiplication,\nand we saw a few errors of what happens if those rules aren't satisfied, particularly if the\ninner dimensions don't match. So this is what I've been alluding to as one of the most common\nerrors in deep learning, and that is shape errors. Because neural networks are comprised of lots of\nmatrix multiplication operations, if you have some sort of tensor shape error somewhere\nin your neural network, chances are you're going to get a shape error. So now let's investigate\nhow we can deal with those. So let's create some tenses, shapes for matrix multiplication.\nAnd I also showed you the website, sorry, matrix multiplication dot xyz. I hope you had a go at\ntyping in some numbers here and visualizing what happens, because we're going to reproduce\nsomething very similar to what happens here, but with PyTorch code. Shapes for matrix multiplication,\nwe have tensor a, let's create this as torch dot tensor. We're going to create a tensor with\njust the elements one, two, all the way up to, let's just go to six, hey, that'll be enough. Six,\nwonderful. And then tensor b can be equal to a torch tensor\nof where we're going to go for this one. Let's go seven, 10, this will be a little bit confusing\nthis one, but then we'll go eight, 11, and this will go up to 12, nine, 12. So it's the same\nsort of sequence as what's going on here, but they've been swapped around. So we've got the\nvertical axis here, instead of one, two, three, four, this is just seven, eight, nine, 10, 11, 12.\nBut let's now try and perform a matrix multiplication. How do we do that?\nTorch dot mat mall for matrix multiplication. PS torch also has torch dot mm, which stands\nfor matrix multiplication, which is a short version. So I'll just write down here so that you know\ntensor a, tensor b. I'm going to write torch dot mm is the same as torch dot mat mall. It's an alias\nfor writing less code. This is literally how common matrix multiplications are in PyTorch\nis that they've made torch dot mm as an alias for mat mall. So you have to type four less characters\nusing torch dot mm instead of mat mall. But I like to write mat mall because it's a little bit\nlike it explains what it does a little bit more than mm. So what do you think's going to happen\nhere? It's okay if you're not sure. But what you could probably do to find out is check the\nshapes of these. Does this operation matrix multiplication satisfy the rules that we just\ndiscussed? Especially this one. This is the main one. The inner dimensions must match.\nWell, let's have a look, hey? Oh, no, mat one and mat two shapes cannot be multiplied.\nThree by two and three by two. This is very similar to what we went through in the last video.\nBut now we've got some actual numbers there. Let's check the shape.\nOh, torch size three two. Torch size three two now. In the last video we created a random tensor\nand we could adjust the shape on the fly. But these tensors already exist. How might we adjust\nthe shape of these? Well, now I'm going to introduce you to another very common operation or tensor\nmanipulation that you'll see. And that is the transpose. To fix our tensor shape issues,\nwe can manipulate the shape of one of our tensors using a transpose. And so, all right here,\nwe're going to see this anyway, but I'm going to define it in words. A transpose switches the\naxes or dimensions of a given tensor. So let's see this in action. If we go, and the way to do it,\nis you can go tensor b dot t. Let's see what happens. Let's look at the original tensor b as well.\nSo dot t stands for transpose. And that's a little bit hard to read, so we might do these on\ndifferent lines, tensor b. We'll get rid of that. So you see what's happened here. Instead of\ntensor b, this is the original one. We might put the original on top. Instead of the original one\nhaving seven, eight, nine, 10, 11, 12 down the vertical, the transpose has transposed it to seven,\neight, nine across the horizontal and 10, 11, 12 down here. Now, if we get the shape of this,\ntensor b dot shape, let's have a look at that. Let's have a look at the original shape, tensor b dot\nshape. What's happened? Oh, no, we've still got three, two. Oh, that's what I've missed out here.\nI've got a typo. Excuse me. I thought I was, you think code that you've written is working,\nbut then you realize you've got something as small as just a dot t missing, and it throws off your\nwhole train of thought. So you're seeing these arrows on the fly here. Now, tensor b is this,\nbut its shape is torch dot size three, two. And if we try to matrix multiply three, two, and three,\ntwo, tensor a and tensor b, we get an error. Why? Because the inner dimensions do not match.\nBut if we perform a transpose on tensor b, we switch the dimensions around. So now,\nwe perform a transpose with tensor b dot t, t's for transpose. We have, this is the important\npoint as well. We still have the same elements. It's just that they've been rearranged. They've\nbeen transposed. So now, tensor b still has the same information encoded, but rearranged.\nSo now we have torch size two, three. And so when we try to matrix multiply these,\nwe satisfy the first criteria. And now look at the output of the matrix multiplication of tensor a\nand tensor b dot t transposed is three, three. And that is because of the second rule of matrix\nmultiplication. The resulting matrix has the shape of the outer dimensions. So we've got three,\ntwo matrix multiply two, three results in a shape of three, three. So let's predify some of this,\nand we'll print out what's going on here. Just so we know, we can step through it,\nbecause right now we've just got codal over the place a bit. Let's see here, the matrix\nmultiplication operation works when tensor b is transposed. And in a second, I'm going to\nshow you what this looks like visually. But right now we've done it with pytorch code,\nwhich might be a little confusing. And that's perfectly fine. Matrix multiplication takes a\nlittle while and a little practice. So original shapes is going to be tensor a dot shape. Let's\nsee what this is. And tensor b equals tensor b dot shape. But the reason why we're spending so\nmuch time on this is because as you'll see, as you get more and more into neural networks and\ndeep learning, the matrix multiplication operation is one of the most or if not the most common.\nSame shape as above, because we haven't changed tensor a shape, we've only changed tensor b shape,\nor we've transposed it. And then in tensor b dot transpose equals, we want tensor b dot\nt dot shape. Wonderful. And then if we print, let's just print out, oops,\nprint, I spelled the wrong word there, print. We want, what are we multiplying here? This is\none of the ways, remember our motto of visualize, visualize, visualize, well, this is how I visualize,\nvisualize, visualize things, shape, let's do the at symbol for brevity, tensor, and let's get b dot\nt dot shape. We'll put down our little rule here, inner dimensions must match. And then print,\nlet's get the output output, I'll put that on a new line. The output is going to equal\ntorch dot, or our outputs already here, but we're going to rewrite it for a little bit of practice,\ntensor a, tensor b dot t. And then we can go print output. And then finally, print, let's get it on a\nnew line as well, the output shape, a fair bit going on here. But we're going to step through it,\nand it's going to help us understand a little bit about what's going on. That's the data visualizes\nmotto. There we go. Okay, so the original shapes are what torch size three two, and torch size three\ntwo, the new shapes tensor a stays the same, we haven't changed tensor a, and then we have tensor\nb dot t is torch size two three, then we multiply a three by two by a two by three. So the inner\ndimensions must match, which is correct, they do match two and two. Then we have an output of tensor\nat 27, 30, 33, 61, 68, 75, etc. And the output shape is what the output shape is the outer\ndimensions three three. Now, of course, you could rearrange this maybe transpose tensor a instead of\ntensor b, have a play around with it. See if you can create some more errors trying to multiply these\ntwo, and see what happens if you transpose tensor a instead of tensor b, that's my challenge. But\nbefore we finish this video, how about we just recreate what we've done here with this cool website\nmatrix multiplication. So what did we have? We had tensor a, which is one to six, let's recreate\nthis, remove that, this is going to be one, two, three, four, five, six, and then we want to increase\nthis, and this is going to be seven, eight, nine, 10, 11, 12. Is that the right way of doing things?\nSo this is already transposed, just to let you know. So this is the equivalent of tensor b\non the right here, tensor b dot t. So let me just show you, if we go tensor b dot transpose,\nwhich original version was that, but we're just passing in the transpose version to our matrix\nmultiplication website. And then if we click multiply, this is what's happening behind the\nscenes with our pytorch code of matmore. We have one times seven plus two times 10. Did you see\nthat little flippy thing that it did? That's where the 27 comes from. And then if we come down here,\nwhat's our first element? 27 when we matrix multiply them. Then if we do the same thing,\nthe next step, we get 30 and 61, from a combination of these numbers, do it again,\n33, 68, 95, from a combination of these numbers, again, and again, and finally we end up with\nexactly what we have here. So that's a little bit of practice for you to go through is to create\nsome of your own tensors can be almost whatever you want. And then try to matrix multiply them\nwith different shapes. See what happens when you transpose and what different values you get.\nAnd if you'd like to visualize it, you could write out something like this. That really\nhelps me understand matrix multiplication. And then if you really want to visualize it,\nyou can go through this website and recreate your target tensors in something like this.\nI'm not sure how long you can go. But yeah, that should be enough to get started.\nSo give that a try and I'll see you in the next video.\nWelcome back. In the last few videos, we've covered one of the most fundamental operations\nin neural networks. And that is matrix multiplication. But now it's time to move on.\nAnd let's cover tensor aggregation. And what I mean by that is finding the min, max, mean,\nsum, et cetera, tensor aggregation of certain tensor values. So for whatever reason, you may\nwant to find the minimum value of a tensor, the maximum value, the mean, the sum, what's going on\nthere. So let's have a look at some few PyTorch methods that are in built to do all of these.\nAnd again, if you're finding one of these values, it's called tensor aggregation because you're\ngoing from what's typically a large amount of numbers to a small amount of numbers. So the min\nof this tensor would be 27. So you're turning it from nine elements to one element, hence\naggregation. So let's create a tensor, create a tensor, x equals torch dot, let's use a range.\nWe'll create maybe a zero to 100 with a step of 10. Sounds good to me. And we can find the min\nby going, can we do torch dot min? Maybe we can. Or we could also go\nx dot min.\nAnd then we can do the same, find the max torch dot max and x dot max. Now how do you think we\nmight get the average? So let's try it out. Or find the mean, find the mean torch dot mean\nx. Oops, we don't have an x. Is this going to work? What's happened? Mean input data type\nshould be either floating point or complex D types got long instead. Ha ha. Finally,\nI knew the error would show its face eventually. Remember how I said it right up here that\nwe've covered a fair bit already. But right up here, some of the most common errors that\nyou're going to run into is tensor is not the right data type, not the right shape. We've seen\nthat with matrix multiplication, not the right device. We haven't seen that yet. But not the\nright data type. This is one of those times. So it turns out that the tensor that we created,\nx is of the data type, x dot D type.\nIn 64, which is long. So if we go to, let's look up torch tensor.\nThis is where they're getting long from. We've seen long before is N64. Where's that or long?\nYeah. So long tenter. That's what it's saying. And it turns out that the torch mean function\ncan't work on tensors with data type long. So what can we do here? Well, we can change\nthe data type of x. So let's go torch mean x type and change it to float 32. Or before we do that,\nif we go to torch dot mean, is this going to tell us that it needs a D type? Oh, D type.\nOne option on the desired data type. Does it have float 32? It doesn't tell us. Ah, so this is\nanother one of those little hidden things that you're going to come across. And you only really\ncome across this by writing code is that sometimes the documentation doesn't really tell you explicitly\nwhat D type the input should be, the input tensor. However, we find out that with this error message\nthat it should either be a floating point or a complex D type, not along. So we can convert it\nto torch float 32. So all we've done is gone x type as type float 32. Let's see what happens here.\n45 beautiful. And then the same thing, if we went, can we do x dot mean? Is that going to work as well?\nOh, same thing. So if we go x dot type torch dot float 32, get the mean of that. There we go.\nSo that is, I knew it would come up eventually. A beautiful example of finding the right data\ntype. Let me just put a note here. Note the torch dot mean function requires a tensor of float 32.\nSo so far, we've seen two of the major errors in PyTorch is data type and shape issues. What's\nanother one that we said? Oh, some. So find the sum. Find the sum we want x dot sum or maybe we\njust do torch dot sum first. Keep it in line with what's going on above and x dot sum.\nWhich one of these should you use like torch dot something x or x dot sum? Personally,\nI prefer torch dot max, but you'll also probably see me at points right this. It really depends\non what's going on. I would say pick whichever style you prefer. And because behind the scenes,\nthey're calling the same methodology. Picture whichever style you prefer and stick with that\nthroughout your code. For now, let's leave it at that tensor aggregation. There's some\nfinding min max mean sum. In the next video, we're going to look at finding the positional\nmin and max, which is also known as arg max and arg min or vice versa. So actually, that's a\nlittle bit of a challenge for the next video is see how you can find out what the positional\nmin and max is of this. And what I mean by that is which index does the max value occur at and\nwhich index of this tensor does the min occur at? You'll probably want to look into the methods\narg min torch dot arg min for that one and torch dot arg max for that. But we'll cover that in the\nnext video. I'll see you there. Welcome back. In the last video, we learned all about tensor\naggregation. And we found the min the max the mean and the sum. And we also ran into one of the most\ncommon issues in pie torch and deep learning and neural networks in general. And that was wrong\ndata types. And so we solved that issue by converting because some functions such as torch dot mean\nrequire a specific type of data type as input. And we created our tensor here, which was of by\ndefault torch in 64. However, torch dot mean requires torch dot float 32. We saw that in an error.\nWe fix that by changing the type of the inputs. I also issued you the challenge of finding\nfinding the positional min and max. And you might have found that you can use the\narg min for the minimum. Let's remind ourselves of what x is x. So this means at tensor index of\ntensor x. If we find the argument, that is the minimum value, which is zero. So at index zero,\nwe get the value zero. So that's at zero there. Zero there. This is an index value. So this is\nwhat arg min stands for find the position in tensor that has the minimum value with arg min.\nAnd then returns index position of target tensor\nwhere the minimum value occurs. Now, let's just change x to start from one,\njust so there we go. So the arg min is still position zero, position zero. So this is an index\nvalue. And then if we index on x at the zeroth index, we get one. So the minimum value in\nx is one. And then the maximum, you might guess, is find the position in tensor that has the maximum\nvalue with arg max. And it's going to be the same thing, except it'll be the maximum, which is,\nwhich position index nine. So if we go zero, one, two, three, four, five, six, seven, eight,\nnine. And then if we index on x for the ninth element, we get 91 beautiful. Now these two are\nuseful for if yes, you want to define the minimum of a tensor, you can just use min. But if you\nsometimes you don't want the actual minimum value, you just want to know where it appears,\nparticularly with the arg max value. This is helpful for when we use the soft max activation\nfunction later on. Now we haven't covered that yet. So I'm not going to allude too much to it.\nBut just remember to find the positional min and max, you can use arg min and arg max.\nSo that's all we need to cover with that. Let's keep going in the next video. I'll see you then.\nWelcome back. So we've covered a fair bit of ground. And just to let you know, I took a little break\nafter going through all of these. And I'd just like to show you how I get back to where I'm at,\nbecause if we tried to just write x here and press shift and enter, because our collab\nwas disconnected, it's now connecting because as soon as you press any button in collab, it's\ngoing to reconnect. It's going to try to connect, initialize, and then x is probably not going to\nbe stored in memory anymore. So there we go. Name x is not defined. That's because the collab\nstate gets reset if you take a break for a couple of hours. This is to ensure Google can keep\nproviding resources for free. And it deletes everything to ensure that there's no compute\nresources that are being wasted. So to get back to here, I'm just going to go restart and run all.\nYou don't necessarily have to restart the notebook. You could also go, do we have run all? Yeah,\nwe could do run before. That'll run every cell before this. We could run after we could run the\nselection, which is this cell here. I'm going to click run all, which is just going to go through\nevery single cell that we've coded above and run them all. However, it will also stop at the errors\nwhere I've left in on purpose. So remember when we ran into a shape error? Well, because this error,\nwe didn't fix it. I left it there on purpose so that we could keep seeing a shape error. It's\ngoing to stop at this cell. So we're going to have to run every cell after the error cell.\nSo see how it's going to run these now. They run fine. And then we get right back to where we were,\nwhich was X. So that's just a little tidbit of how I get back into coding. Let's now cover reshaping,\nstacking, squeezing, and unsqueezing. You might be thinking, squeezing and unsqueezing. What are\nyou talking about, Daniel? Well, it's all to do with tenses. And you're like, are we going to\nsqueeze our tenses? Give them a hug. Are we going to let them go by unsqueezing them?\nWell, let's quickly define what these are. So reshaping is we saw before one of the most common\nerrors in machine learning and deep learning is shape mismatches with matrices because they\nhave to satisfy certain rules. So reshape reshapes an input tensor to a defined shape.\nNow, we're just defining these things in words right now, but we're going to see it in code in\njust a minute. There's also view, which is return a view of an input tensor of certain shape,\nbut keep the same memory as the original tensor. So we'll see what view is in a second.\nReshaping and view are quite similar, but a view always shares the same memory as the original\ntensor. It just shows you the same tensor, but from a different perspective, a different shape.\nAnd then we have stacking, which is combine multiple tensors on top of each other. This is a V stack\nfor vertical stack or side by side. H stack. Let's see what different types of torch stacks there are.\nAgain, this is how I research different things. If I wanted to learn something new, I would search\ntorch something stack concatenate a sequence of tensors along a new dimension. Okay. So maybe we\nnot H stack or V stack, we can just define what dimension we'd like to combine them on.\nI wonder if there is a torch V stack. Torch V stack. Oh, there it is. And is there a torch H stack for\nhorizontal stack? There is a H stack. Beautiful. So we'll focus on just the plain stack. If you\nwant to have a look at V stack, it'll be quite similar to what we're going to do with stack\nand same with H stack. Again, this is just words for now. We're going to see the code in a minute.\nSo there's also squeeze, which removes all one dimensions. I'm going to put one in code,\ndimensions from a tensor. We'll see what that looks like. And then there's unsqueeze,\nwhich adds a one dimension to our target tensor. And then finally, there's permute, which is return\na view of the input with dimensions permuted. So swapped in a certain way. So a fair few methods\nhere. But essentially the crust of all of these, the main point of all of these is to manipulate\nour tensors in some way to change their shape or change their dimension. Because again, one of the\nnumber one issues in machine learning and deep learning is tensor shape issues. So let's start\noff by creating a tensor and have a look at each of these. Let's create a tensor. And then we're\ngoing to just import torch. We don't have to, but this will just enable us to run the notebook\ndirectly from this cell if we wanted to, instead of having to run everything above here. So let's\ncreate another X torch dot a range because range is deprecated. I'm just going to add a few code\ncells here so that I can scroll and that's in the middle of the screen there. Beautiful. So let's\njust make it between one and 10 nice and simple. And then let's have a look at X and X dot shape.\nWhat does this give us? Okay, beautiful. So we've got the numbers from one to nine. Our tensor is\nof shape torch size nine. Let's start with reshape. So how about we add an extra dimension. So then\nwe have X reshaped equals X dot reshape. Now a key thing to keep in mind about the reshape\nis that the dimensions have to be compatible with the original dimensions. So we're going to\nchange the shape of our original tensor with a reshape. And we try to change it into the shape\none seven. Does that work with the number nine? Well, let's find out, hey, let's check out X reshaped.\nAnd then we'll look at X reshaped dot shape. What's this going to do? Oh, why do we get an error there?\nWell, it's telling us here, this is what pie torch is actually really good at is giving us\nerrors for what's going wrong. We have one seven is invalid for input size of nine.\nWell, why is that? Well, we're trying to squeeze nine elements into a tensor of one\ntimes seven into seven elements. But if we change this to nine, what do we get? Ah, so do you notice\nwhat just happened here? We just added a single dimension. See the single square bracket with\nthe extra shape here. What if we wanted to add two? Can we do that? No, we can't. Why is that?\nWell, because two nine is invalid for input size nine, because two times nine is what?\n18. So we're trying to double the amount of elements without having double the amount of elements.\nSo if we change this back to one, what happens if we change these around nine one? What does this\ndo? Oh, a little bit different there. So now instead of adding one on the first dimension or\nthe zeroth dimension, because Python is zero indexed, we added it on the first dimension,\nwhich is giving us a square bracket here if we go back. So we add it to the outside here,\nbecause we've put the one there. And then if we wanted to add it on the inside,\nwe put the one on the outside there. So then we've got the torch size nine one. Now, let's try\nchange the view, change the view. So just to reiterate, the reshape has to be compatible\nwith the original size. So how about we change this to one to 10? So we have a size of 10,\nand then we can go five, two, what happens there? Oh, it's compatible because five times two equals\n10. And then what's another way we could do this? How about we make it up to 12? So we've got 12\nelements, and then we can go three, four, a code cells taking a little while run here.\nThen we'll go back to nine, just so we've got the original there.\nWhoops, they're going to be incompatible. Oh, so this is another thing. This is good.\nWe're getting some errors on the fly here. Sometimes you'll get saved failed with Google\nCoLab, and automatic saving failed. What you can do to fix this is just either keep coding,\nkeep running some cells, and CoLab will fix itself in the background, or restart the notebook,\nclose it, and open again. So we've got size nine, or size eight, sorry, incompatible.\nBut this is good. You're seeing the errors that come up on the fly, rather than me sort of just\ntelling you what the errors are, you're seeing them as they come up for me. I'm trying to live\ncode this, and this is what's going to happen when you start to use Google CoLab, and subsequently\nother forms of Jupyter Notebooks. But now let's get into the view, so we can go z equals,\nlet's change the view of x. View will change it to one nine, and then we'll go z, and then z dot shape.\nAh, we get the same thing here. So view is quite similar to reshape. Remember, though, that a\nview shares the memory with the original tensor. So z is just a different view of x. So z shares\nthe same memory as what x does. So let's exemplify this. So changing z changes x, because a view of\na tensor shares the same memory as the original input. So let's just change z, change the first\nelement by using indexing here. So we're targeting one, we'll set this to equal five, and then we'll\nsee what z and x equal. Yeah, so see, we've got z, the first one here, we change the first element,\nthe zero element to five. And the same thing happens with x, we change the first element of z.\nSo because z is a view of x, the first element of x changes as well. But let's keep going. How\nabout we stack some tenses on top of each other? And we'll see what the stack function does in\ntorch. So stack tenses on top of each other. And I'll just see if I press command S to save,\nmaybe we'll get this fixed. Or maybe it just will fix itself. Oh, notebook is saved.\nUnless you've made some extensive changes that you're worried about losing, you could just\ndownload this notebook, so file download, and upload it to collab. But usually if you click yes,\nit sort of resolves itself. Yeah, there we go. All changes saved. So that's beautiful\ntroubleshooting on the fly. I like that. So x stack, let's stack some tenses together,\nequals torch stack. Let's go x x x, because if we look at what the doc string of stack is,\nwill we get this in collab? Or we just go to the documentations? Yeah. So list, it takes a list of\ntenses and concatenates a sequence of tenses along a new dimension. And we define the dimension,\nthe dimension by default is zero. That's a little bit hard to read for me. So tenses,\ndim equals zero. If we come into here, the default dimension is zero. Let's see what happens when\nwe play around with the dimension here. So we've got four x's. And the first one, we'll just do it\nby default, x stack. Okay, wonderful. So they're stacked vertically. Let's see what happens if we\nchange this to one. Oh, they rearranged a little and stack like that. What happens if we change it\nto two? Does it have a dimension to? Oh, we can't do that. Well, that's because the original shape\nof x is incompatible with using dimension two. So the only real way to get used to what happens\nhere by stacking them on top of each other is to play around with the different values for the\ndimension. So dim zero, dim one, they look a little bit different there. Now they're on top of each\nother. And so the first zero index is now the zeroth tensor. And then same with two being there,\nthree and so on. But we'll leave it at the default. And there's also v stack and h stack. I'll leave\nthat to you to to practice those. But I think from memory v stack is using dimension equals zero.\nOr h stack is like using dimension equals one. I may have those back the front. You can correct me\nif I'm wrong there. Now let's move on. We're going to now have a look at squeeze and unsqueeze.\nSo actually, I'm going to get you to practice this. So see if you can look up torch squeeze\nand torch unsqueeze. And see if you can try them out. We've created a tensor here. We've used\nreshape and view and we've used stack. The usage of squeeze and unsqueeze is quite similar. So give\nthat a go. And to prevent this video from getting too long, we'll do them together in the next video.\nWelcome back. In the last video, I issued the challenge of trying out torch dot squeeze,\nwhich removes all single dimensions from a target tensor. And how would you try that out? Well,\nhere's what I would have done. I'd go to torch dot squeeze and see what happens. Open up the\ndocumentation. Squeeze input dimension returns a tensor with all the dimensions of input size\none removed. And does it have some demonstrations? Yes, it does. Wow. Okay. So you could copy this in\nstraight into a notebook, copy it here. But what I'd actually encourage you to do quite often is\nif you're looking up a new torch method you haven't used, code all of the example by hand. And then\njust practice what the inputs and outputs look like. So x is the input here. Check the size of x,\nsqueeze x, well, set the squeeze of x to y, check the size of y. So let's replicate something\nsimilar to this. We'll go into here, we'll look at x reshaped and we'll remind ourselves of x reshaped\ndot shape. And then how about we see what x reshaped dot squeeze looks like. Okay. What happened here?\nWell, we started with two square brackets. And we started with a shape of one nine\nand removes all single dimensions from a target tensor. And now if we call the squeeze method on\nx reshaped, we only have one square bracket here. So what do you think the shape of x reshaped dot\nsqueeze is going to be? We'll check the shape here. It's just nine. So that's the squeeze method,\nremoves all single dimensions. If we had one one nine, it would remove all of the ones. So it would\njust end up being nine as well. Now, let's write some print statements so we can have a little\npretty output. So previous tensor, this is what I like to do. This is a form of visualize, visualize,\nvisualize. If I'm trying to get my head around something, I print out each successive change\nto see what's happening. That way, I can go, Oh, okay. So that's what it was there. And then I\ncalled that line of code there. Yes, it's a bit tedious. But you do this half a dozen times, a\nfair few times. I mean, I still do it a lot of the time, even though I've written thousands of lines\nof machine learning code. But it starts to become instinct after a while, you start to go, Oh, okay,\nI've got a dimension mismatch on my tensors. So I need to squeeze them before I put them into a\ncertain function. For a little while, but with practice, just like riding a bike, right? But that\ntry saying is like when you first start, you're all wobbly all over the place having to look up\nthe documentation, not that there's much documentation for riding a bike, you just kind of keep trying.\nBut that's the style of coding. I'd like you to adopt is to just try it first. Then if you're stuck,\ngo to the documentation, look something up, print it out like this, what we're doing,\nquite cumbersome. But this is going to give us a good explanation for what's happening. Here's our\nprevious tensor x reshaped. And then if we look at the shape of x reshaped, it's one nine. And then\nif we call the squeeze method, which removes all single dimensions from a target tensor,\nwe have the new tensor, which is has one square bracket removed. And the new shape is all single\ndimensions removed. So it's still the original values, but just a different dimension. Now,\nlet's do the same as what we've done here with unsqueeze. So we've given our tensors a hug and\nsqueezed out all the single dimensions of them. Now we're going to unsqueeze them. We're going to\ntake a step back and let them grow a bit. So torch unsqueeze adds a single dimension\nto a target tensor at a specific dim dimension. Now that's another thing to note in PyTorch whenever\nit says dim, that's dimension as in this is a zeroth dimension, first dimension. And if there\nwas more here, we'd go two, three, four, five, six, et cetera. Because why tensors can have\nunlimited dimensions. So let's go previous target can be excused. So we'll get this squeezed version\nof our tensor, which is x squeezed up here. And then we'll go print. The previous shape\nis going to be x squeezed dot shape. And then we're going to add an extra dimension with unsqueeze.\nThere we go, x unsqueezed equals x squeezed. So our tensor before that we remove the single\ndimension. And we're going to put in unsqueeze, dim, we'll do it on the zeroth dimension. And I\nwant you to have a think about what this is going to output even before we run the code.\nJust think about, because we've added an extra dimension on the zeroth dimension,\nwhat's the new shape of the unsqueeze tensor going to be? So we're going to go x unsqueezed.\nAnd then we're going to go print, we'll get our new tensor shape, which is going to be x unsqueezed\ndot shape. All right, let's have a look. There we go. So there's our previous tensor,\nwhich is the squeezed version, just as a single dimension here. And then we have our new tensor,\nwhich with the unsqueeze method on dimension zero, we've added a square bracket on the zeroth\ndimension, which is this one here. Now what do you think's going to happen if I change this to one?\nWhere's the single dimension going to be added? Let's have a look. Ah, so instead of adding the\nsingle dimension on the zeroth dimension, we've added it on the first dimension here. It's quite\nconfusing because Python is zero index. So I kind of want to my brain's telling me to say first,\nbut it's really the zeroth index here or the zeroth dimension. Now let's change this back to\nzero. But that's just another way of exploring things. Every time there's like a parameter that\nwe have here, dim equals something like that could be shape, could be size, whatever, try\nchanging the values. That's what I'd encourage you to do. And even write some print code like\nwe've done here. Now there's one more we want to try out. And that's permute. So torch dot permute\nrearranges the dimensions of a target tensor in a specified order. So if we wanted to check out,\nlet's get rid of some of these extra tabs. Torch dot permute. Let's have a look. This one took me\na little bit of practice to get used to. Because again, working with zeroth dimensions, even though\nit seems like the first one. So returns a view. Okay. So we know that a view shares the memory of\nthe original input tensor with its dimensions permuted. So permuted for me, I didn't really know\nwhat that word meant. I just have mapped in my own memory that permute means rearrange dimensions.\nSo the example here is we start with a random tensor, we check the size, and then we'd have\ntorch permute. We're going to swap the order of the dimensions. So the second dimension is first,\nthe zeroth dimension is in the middle, and the first dimension is here. So these are dimension\nvalues. So if we have torch random two, three, five, two, zero, one has changed this one to be\nover here. And then zero, one is two, three, and now two, three there. So let's try something similar\nto this. So one of the common places you'll be using permute, or you might see permute being\nused is with images. So there's a data specific data format. We've kind of seen a little bit\nbefore, not too much. Original equals torch dot rand size equals. So an image tensor,\nwe go height width color channels on the end. So I'll just write this down. So this is height\nwidth color channels. Remember, much of, and I'm going to spell color Australian style,\nmuch of deep learning is turning your data into numerical representations. And this is quite common\nnumerical representation of image data. You have a tensor dimension for the height, a tensor dimension\nfor the width, and a tensor dimension for the color channels, which is red, green, and blue,\nbecause a certain number of red, green, and blue creates almost any color. Now, if we want to\npermute this, so permute the original tensor to rearrange the axis or dimension, axis or dimension,\nare kind of used in the same light for tensors or dim order. So let's switch the color channels\nto be the first or the zeroth dimension. So instead of height width color channels,\nit'll be color channels height width. How would we do that with permute? Let's give it a shot.\nX permuted equals X original dot permute. And we're going to take the second dimension,\nbecause this takes a series of dims here. So the second dimension is color channels. Remember,\nzero, one, two. So two, we want two first, then we want the height, which is a zero. And then we\nwant the width, which is one. And now let's do this shifts, axis, zero to one, one to two,\nand two to zero. So this is the order as well. This two maps to zero. This zero maps to the first\nindex. This one maps to this index. But that's enough talk about it. Let's see what it looks like.\nSo print, previous shape, X original dot shape. And then we go here, print new shape. This will\nbe the permuted version. We want X permuted dot shape. Let's see what this looks like. Wonderful.\nThat's exactly what we wanted. So you see, let's just write a little note here. Now this is\ncolor channels, height, width. So the same data is going to be in both of these tenses. So X\noriginal X permuted, it's just viewed from a different point of view. Because remember, a\npermute is a view. And what did we discuss? A view shares the same memory as the original tensor.\nSo X permuted will share the same place in memory as X original, even though it's from a different\nshape. So a little challenge before we move on to the next video for you, or before you move\non to the next video, try change one of the values in X original. Have a look at X original.\nAnd see if that same value, it could be, let's get one of this zero, zero, get all of the dimensions\nhere, zero. See what that is? Or can we get a single value maybe? Oops. Oh, no, we'll need a zero\nhere, getting some practice on indexing here. Oh, zero, zero, zero. There we go. Okay, so maybe\nwe set that to some value, whatever you choose, and see if that changes in X permuted. So give\nthat a shot, and I'll see you in the next video. Welcome back. In the last video, we covered\nsqueezing, unsqueezing, and permuting, which I'm not going to lie, these concepts are quite a\nlot to take in, but just so you're aware of them. Remember, what are they working towards? They're\nhelping us fix shape and dimension issues with our tensors, which is one of the most common\nissues in deep learning and neural networks. And I usually do the little challenge of changing a\nvalue of X original to highlight the fact that permute returns a different view of the original\ntensor. And a view in PyTorch shares memory with that original tensor. So if we change the value\nat zero, zero, zero of X original to, in my case, 728218, it happens the same value gets copied across\nto X permuted. So with that being said, we looked at selecting data from tensors here, and this is\nusing a technique called indexing. So let's just rehash that, because this is another thing that\ncan be a little bit of a hurdle when first working with multi dimensional tensors. So let's see how\nwe can select data from tensors with indexing. So if you've ever done indexing, indexing,\nwith PyTorch is similar to indexing with NumPy. If you've ever worked with NumPy,\nand you've done indexing, selecting data from arrays, NumPy uses an array as its main data type,\nPyTorch uses tensors. It's very similar. So let's again start by creating a tensor.\nAnd again, I'm just going to add a few code cells here, so I can make my screen right in the middle.\nNow we're going to import torch. Again, we don't need to import torch all the time,\njust so you can run the notebook from here later on. X equals torch dot. Let's create a range again,\njust nice and simple. This is how I like to work out the fundamentals too, is just create the small\nrange, reshape it, and the reshape has to be compatible with the original dimension. So we go\none, three, three, and why is this because torch a range is going to return us nine values, because\nit's from the start here to the end minus one, and then one times three times three is what is\nnine. So let's have a look x x dot shape. Beautiful. So we have one, two, three, four, five, six,\nseven, eight, nine of size one. So we have this is the outer bracket here, which is going to contain\nall of this. And then we have three, which is this one here, one, two, three. And then we have three,\nwhich is one, two, three. Now let's work with this. Let's index on our new tensor. So let's see what\nhappens when we get x zero, this is going to index on the first bracket. So we get this one here. So\nwe've indexed on the first dimension here, the zero dimension on this one here, which is why we get\nwhat's inside here. And then let's try again, let's index on the middle bracket. So dimension\none. So we got to go x, and then zero, and then zero. Let's see what happens there. Now is this the\nsame as going x zero, zero? It is, there we go. So it depends on what you want to use. Sometimes\nI prefer to go like this. So I know that I'm getting the first bracket, and then the zeroth\nversion of that first bracket. So then we have these three values here. Now what do you think\nwhat's going to happen if we index on third dimension or the second dimension here? Well,\nlet's find out. So let's index on the most in our bracket, which is last dimension.\nSo we have x zero, zero, zero. What numbers is going to give us back of x zero,\non the zero dimension gives us back this middle tensor. And then if x zero, zero gives us back\nthe zeroth index of the middle tensor. If we go x zero, zero, zero is going to give us the zeroth\ntensor, the zeroth index, and the zeroth element. A lot to take in there. But what we've done is\nwe've just broken it down step by step. We've got this first zero targets this outer bracket\nand returns us all of this. And then zero, zero targets this first because of this first zero,\nand then the zero here targets this. And then if we go zero, zero, zero, we target this,\nthen we target this, and then we get this back because we are getting the zeroth index here.\nSo if we change this to one, what do we get back? Two. And if we change these all to one,\nwhat will we get? This is a bit of trivia here, or a challenge. So we're going one, one, one.\nLet's see what happens. Oh, no, did you catch that before I ran the code? I did that one quite\nquickly. We have index one is out of bounds. Why is that? Well, because this dimension is only one\nhere. So we can only index on the zero. That's where it gets a little bit confusing because this\nsays one, but because it's only got zero dimension, we can only index on the zero if to mention. But\nwhat if we do 011? What does that give us? Five. Beautiful. So I'd like to issue you the challenge\nof how about getting number nine? How would you get number nine? So rearrange this code to get\nnumber nine. That's your challenge. Now, I just want to show you as well, is you can use,\nyou can also use, you might see this, the semicolon to select all of a target dimension. So let's say\nwe wanted to get all of the zeroth dimension, but the zero element from that. We can get 123.\nAnd then let's say we want to say get all values of the zeroth and first dimensions,\nbut only index one of the second dimension. Oh, that was a mouthful. But get all values of\nzeroth and first dimensions, but only index one of second dimension. So let's break this\ndown step by step. We want all values of zeroth and first dimensions, but only index one of the\nsecond dimension. We press enter, shift enter, 258. So what did we get there? 258. Okay. So we've\ngot all elements of the zeroth and first dimension, but then so which will return us this thing here.\nBut then we only want 258, which is the first element here of the second dimension, which is\nthis three there. So quite confusing. But with some practice, you can figure out how to select\nalmost any numbers you want from any kind of tensor that you have. So now let's try again,\nget all values of the zero dimension, but only the one index value of the first and second\ndimension. So what might this look like? Let's break it down again. So we come down here x,\nand we're going to go all values of the zero dimension because zero comes first. And then we\nwant only the one index value of the first and only the one index value of the second.\nWhat is this going to give us five? Oh, we selected the middle tensor. So really,\nthis line of code is exactly the same as this line of code here, except we've got the square\nbrackets on the outside here, because we've got this semicolon there. So if we change this to a zero,\nwe remove that. But because we've got the semicolon there, we've selected all the\ndimensions. So we get back the square bracket there, something to keep in mind. Finally,\nlet's just go one more. So get index zero of zero and first dimension, and all values of second\ndimension. So x zero, zero. So zero, the index of zero and first dimension, zero, zero,\nand all values of the second dimension. What have we just done here? We've got tensor one,\ntwo, three, lovely. This code again is equivalent to what we've done up here. This has a semicolon\non the end. But what this line explicitly says without the semicolon is, hey, give us all the\nvalues on the remaining dimension there. So my challenge for you is to take this tensor that we\nhave got here and index on it to return nine. So I'll write down here, index on x to return nine.\nSo if you have a look at x, as well as index on x to return three, six, nine. So these values\nhere. So give those both a go and I'll see you in the next video. Welcome back. How'd you go?\nDid you give the challenge ago? I finished the last video with issuing the challenge to index on\nx to return nine and index on x to return three, six, nine. Now here's what I came up with. Again,\nthere's a few different ways that you could approach both of these. But this is just what\nI've found. So because x is one, three, three of size, well, that's his dimensions. If we want to\nselect nine, we need zero, which is this first outer bracket to get all of these elements. And\nthen we need two to select this bottom one here. And then we need this final two to select the\nsecond dimension of this bottom one here. And then for three, six, nine, we need all of the\nelements in the first dimension, all of the in the zeroth dimension, all of the elements in the\nfirst dimension. And then we get two, which is this three, six, nine set up here. So that's how I\nwould practice indexing, start with whatever shape tensor you like, create it something like this,\nand then see how you can write different indexing to select whatever number you pick.\nSo now let's move on to the next part, which is PyTorch tensors and NumPy. So NumPy is a\npopular scientific, very popular. PyTorch actually requires NumPy when you install PyTorch. Popular\nscientific Python numerical computing library, that's a bit of a mouthful. And because of this,\nPyTorch has functionality to interact with it. So quite often, you might start off with,\nlet's change this into Markdown, you might start off with your data, because it's numerical format,\nyou might start off with data in NumPy, NumPy array, want in PyTorch tensor. Because your\ndata might be represented by NumPy because it started in NumPy, but say you want to do\nsome deep learning on it and you want to leverage PyTorch's deep learning capabilities,\nwell, you might want to change your data from NumPy to a PyTorch tensor. And PyTorch has a\nmethod to do this, which is torch from NumPy, which will take in an ND array, which is NumPy's\nmain data type, and change it into a torch tensor. We'll see this in a second. And then if you want\nto go from PyTorch tensor to NumPy because you want to use some sort of NumPy method,\nwell, the method to do this is torch dot tensor, and you can call dot NumPy on it. But this is all\njust talking about in words, let's see it in action. So NumPy array to tensor. Let's try this out\nfirst. So we'll import torch so we can run this cell on its own, and then import NumPy as np,\nthe common naming convention for NumPy, we're going to create an array in NumPy. And we're\ngoing to just put one to eight, a range. And then we're going to go tensor equals torch from NumPy\nbecause we want to go from NumPy array to a torch tensor. So we use from NumPy, and then we pass\nin array, and then we have array and tensor. Wonderful. So there's our NumPy array, and our torch\ntensor with the same data. But what you might notice here is that the D type for the tensor is\ntorch dot float 64. Now why is this? It's because NumPy's default data type. Oh, D type\nis float 64. Whereas tensor, what have we discussed before? What's pytorch's default data type?\nfloat 64. Well, that's not pytorch's default data type. If we were to create torch, a range,\n1.0 to 8.0, by default, pytorch is going to create it in\nfloat 32. So just be aware of that. If you are going from NumPy to pytorch, the default NumPy\ndata type is float 64. And pytorch reflects that data type when you use the from NumPy method.\nI wonder if there's a D type. Can we go D type equals torch dot float 32? Takes no keyword.\nOkay. But how could we change the data type here? Well, we could go type torch float 32.\nYeah, that will give us a tensor D type of float 32 instead of float 64. Beautiful. I'll just keep\nthat there so you know, warning when converting from NumPy pytorch, pytorch reflects NumPy's\ndefault data type of float 64, unless specified. Otherwise, because what have we discussed,\nwhen you're trying to perform certain calculations, you might run into a data type issue. So you might\nneed to convert the type from float 64 to float 32. Now, let's see what happens. What do you think\nwill happen if we change the array? We change the value of an array. Well, let's find out.\nSo change the value of array. The question is, what will this do to tensor? Because we've used\nthe from NumPy method, do you think if we change the array, the tensor will change? So let's try\nthis array equals array plus one. So we're just adding one to every value in the array. Now,\nwhat is the array and the tensor going to look like? Uh huh. So array, we only change the first\nvalue there. Oh, sorry, we change every value because we have one to seven. Now it's two, three,\nfour, five, six, seven, eight. We change the value from the array. It doesn't change the\nvalue of the tensor. So that's just something to keep in mind. If you use from NumPy, we get\na new tensor in memory here. So the original, the new tensor doesn't change if you change the\noriginal array. So now let's go from tensor to NumPy. If you wanted to go back to NumPy,\ntensor to NumPy array. So we'll start with a tensor. We could use the one we have right now,\nbut we're going to create another one, but we'll create one of ones just for fun.\nOne rhymes with fun. NumPy tensor equals. How do we go to NumPy? Well, we have\ntorch dot tensor dot NumPy. So we just simply call NumPy on here. And then we have tensor\nand NumPy tensor. What data type do you think the NumPy tensor is going to have?\nBecause we've returned it to NumPy. Pi torches, default data type is\nFlight 32. So if we change that to NumPy, what's going to be the D type of the NumPy tensor?\nNumPy tensor dot D type. It reflects the original D type of what you set the tensor as. So just\nkeep that in mind. If you're going between PyTorch and NumPy, default data type of NumPy is\nfloat 64, whereas the default data type of PyTorch is float 32. So that may cause some errors if\nyou're doing different kinds of calculations. Now, what do you think is going to happen if we\nwent from our tensor to an array, if we change the tensor, change the tensor, what happens to\nNumPy tensor? So we get tensor equals tensor plus one. And then we go NumPy tensor.\nOh, we'll get tensor as well. So our tensor is now all twos because we added one to the ones.\nBut our NumPy tensor remains the same. Remains unchanged. So this means they don't share memory.\nSo that's how we go in between PyTorch and NumPy. If you'd like to look up more, I'd encourage\nyou to go PyTorch and NumPy. So warm up NumPy, beginner. There's a fair few tutorials here on\nPyTorch because NumPy is so prevalent, they work pretty well together. So have a look at that.\nThere's a lot going on there. There's a few more links, I'd encourage you to check out,\nbut we've covered some of the main ones that you'll see in practice. With that being said,\nlet's now jump into the next video where we're going to have a look at the concept of reproducibility.\nIf you'd like to look that up, I'd encourage you to search PyTorch's reproducibility and see\nwhat you can find. Otherwise, I'll see you in the next video. Welcome back. It's now time for us\nto cover the topic of reproducibility. If I could even spell it, that would be fantastic.\nReproducibility. Trying to take the random out of random. So we've touched upon the concept of\nneural networks harnessing the power of randomness. And what I mean by that is we haven't actually\nbuilt our own neural network yet, but we will be doing that. And we've created tenses full of random\nvalues. And so in short, how our neural network learns is start with random numbers, perform tensor\noperations, update random numbers to try and make them better representations of the data. Again,\nagain, again, again, again. However, if you're trying to do reproducible experiments, sometimes\nyou don't want so much randomness. And what I mean by this is if we were creating random tensors,\nfrom what we've seen so far is that every time we create a random tensor, let's create one here,\ntorch dot rand, and we'll create it of three three. Every time we run this cell, it gives us new numbers.\nSo 7 7 5 2. There we go. Rand again. Right. So we get a whole bunch of random numbers here.\nEvery single time. But what if you were trying to share this notebook with a friend,\nso say you went up share and you clicked the share link and you sent that to someone and you're like,\nhey, try out this machine learning experiment I did. And you wanted a little less randomness\nbecause neural networks start with random numbers. How might you do that? Well, let's\nthis write down to reduce the randomness in neural networks. And pytorch comes the concept of a\nrandom seed. So we're going to see this in action. But essentially, let's write this down,\nessentially what the random seed does is flavor the randomness. So because of how computers work,\nthey're actually not true randomness. And actually, there's arguments against this,\nand it's quite a big debate in the computer science topic, whatnot, but I am not a computer\nscientist, I am a machine learning engineer. So computers are fundamentally deterministic.\nIt means they run the same steps over and over again. So what the randomness we're doing here\nis referred to as pseudo randomness or generated randomness. And the random seed,\nwhich is what you see a lot in machine learning experiments, flavors that randomness. So let's\nsee it in practice. And at the end of this video, I'll give you two resources that I'd recommend\nto learn a little bit more about the concept of pseudo randomness and reproducibility in pytorch.\nLet's start by importing torch so you could start this notebook right from here. Create two random\ntensors. We'll just call this random tensor a equals torch dot rand and we'll go three four\nand we'll go random tensor b equals torch dot rand same size three four. And then if we have a\nlook at let's go print random tensor a print random tensor b. And then let's print to see if\nthey're equal anywhere random tensor a equals equals equals random tensor b. Now what do you\nthink this is going to do? If we have a look at one equals one, what does it return? True.\nSo this is comparison operator to compare two different tensors. We're creating two random\ntensors here. We're going to have a look at them. We'd expect them to be full of random values.\nDo you think any of the values in each of these random tensors is going to be equal to each other?\nWell, there is a chance that they are, but it's highly unlikely. I'll be quite surprised if they are.\nOh, again, my connection might be a little bit. Oh, there we go. Beautiful. So we have tensor a\ntensor of three four with random numbers. And we have tensor b of three four with random numbers.\nSo if we were, if I was to share this notebook with my friend or my colleague or even you,\nif you ran this cell, you are going to get random numbers as well. And you have every chance of\nreplicating one of these numbers. But again, it's highly unlikely. So again, I'm getting that\nautomatic save failed. You might get that if your internet connection is dropping out, maybe that's\nsomething going on with my internet connection. But again, as we've seen, usually this resolves\nitself. If you try a few times, I'll just keep coding. If it really doesn't resolve itself,\nyou can go file is a download notebook or save a copy and drive download. You can download the\nnotebook, save it to your local machine, re upload it to upload notebook and start again in another\nGoogle Colab instance. But there we go. It fixed itself. Wonderful troubleshooting on the fly.\nSo the way we make these reproducible is through the concept of a random seed. So let's have a\nlook at that. Let's make some random, but reproducible tenses. So import torch. And we're going to\nset the random seed by going torch dot manual seed random. Oh, we don't have random set yet.\nI'm going to set my random seed. You set the random seed to some numerical value. 42 is a common\none. You might see zero. You might see one, two, three, four. Essentially, you can set it to whatever\nyou want. And each of these, you can think of 77, 100, as different flavors of randomness. So\nI like to use 42, because it's the answer to the universe. And then we go random seed. And now\nlet's create some random tenses. Random tensor C with the flavor of our random seed. Three,\nfour. And then we're going to go torch tensor D equals torch dot rand three, four. Now, let's\nsee what happens. We'll print out random tensor C. And we'll print out random tensor D. And then\nwe'll print out to see if they're equal anywhere. Random tensor C equals random tensor D. So let's\nfind out what happens. Huh, what gives? Well, we've got randomness. We set the random seed. We're\ntelling pytorch a flavor our randomness with 42 torch manual seed. Hmm, let's try set the manual\nseed each time we call a random method. We go there. Ah, much better. So now we've got some\nflavored randomness. So a thing to keep in mind is that if you want to use the torch manual seed,\ngenerally it only works for one block of code if you're using a notebook. So that's just\nsomething to keep in mind. If you're creating random tensors, one after the other, we're using\nassignment like this, you should use torch dot manual seed every time you want to call the rand\nmethod or some sort of randomness. However, if we're using other torch processes, usually what\nyou might see is torch manual seed is set right at the start of a cell. And then a whole bunch\nof code is done down here. But because we're calling subsequent methods here, we have to reset\nthe random seed. Otherwise, if we don't do this, we comment this line, it's going to flavor the\nrandomness of torch random tensor C with torch manual seed. But then random tensor D is just\ngoing to have no flavor. It's not going to use a random seed. So we reset it there. Wonderful.\nSo I wonder, does this have a seed method? Let's go torch dot rand. Does this have seed?\nSometimes they have a seed method. Seed, no, it doesn't. Okay, that's all right.\nThe more you learn, but there's documentation for torch dot rand. And I said that I was going to\nlink at the end of this video. So the manual seed is a way to, or the random seed, but in\ntorch, it's called a manual seed is a way to flavor the randomness. So these numbers, as you see,\nare still quite random. But the random seed just makes them reproducible. So if I was to share this\nwith you, if you had to run this block of code, ideally, you're going to get the same numerical\noutput here. So with that being said, I'd like to refer to you to the pie torch reproducibility\ndocument, because we've only quite scratched the surface of this of reproducibility. We've covered\none of the main ones. But this is a great document on how to go through reproducibility in pie torch.\nSo this is your extra curriculum for this, even if you don't understand what's going on in a lot\nof the code here, just be aware of reproducibility, because it's an important topic in machine\nlearning and deep learning. So I'll put this here, extra resources for reproducibility.\nAs we go pie torch randomness, we'll change this into markdown. And then finally, the concept\nof a random seed is Wikipedia random seed. So random seeds quite a universal concept,\nnot just for pie torch, there's a random seed and NumPy as well. So if you'd like to see what\nthis means, yeah, initialize a pseudo random number generator. So that's a big word, pseudo random\nnumber generator. But if you'd like to learn about more random number generation computing,\nand what a random seed does is I'd refer to you to check out this documentation here.\nWhoo, far out, we have covered a lot. But there's a couple more topics you should really be aware\nof to finish off the pie torch fundamentals. You got this. I'll see you in the next video.\nWelcome back. Now, let's talk about the important concept of running tenses or pie\ntorch objects. So running tenses and pie torch objects on GPUs and making faster computations.\nSo we've discussed that GPUs, let me just scroll down a little bit here, GPUs equal faster\ncomputation on numbers. Thanks to CUDA plus NVIDIA hardware plus pie torch working behind the\nscenes to make everything hunky dory. Good. That's what hunky dory means, by the way,\nif you never heard that before. So let's have a look at how we do this. Now, we first need to\ntalk about, let's go here one getting a GPU. There's a few different ways we've seen one before.\nNumber one easiest is to use what we're using right now. Use Google Colab for a free GPU.\nBut there's also Google Colab Pro. And I think there might even be, let's look up Google Colab\nPro. Choose the best that's right for you. I use Google Colab Pro because I use it almost every day.\nSo yeah, I pay for Colab Pro. You can use Colab for free, which is might be what you're using.\nThere's also Colab Pro Plus, which has a lot more advantages as well. But Colab Pro is giving me\nfaster GPUs, so access to faster GPUs, which means you spend less time waiting while your code is running.\nMore memory, longer run time, so it'll last a bit longer if you leave it running idle.\nAnd then Colab Pro again is a step up from that. I personally haven't had a need yet to use\nGoogle Colab Pro Plus. You can complete this whole course on the free tier as well. But as you start\nto code more, as you start to run bigger models, as you start to want to compute more, you might\nwant to look into something like Google Colab Pro. Or let's go here. Options to upgrade as well.\nAnd then another way is use your own GPU. Now this takes a little bit of setup and requires\nthe investment of purchasing a GPU. There's lots of options. So one of my favorite posts for\ngetting a GPU is, yeah, the best GPUs for deep learning in 2020, or something like this.\nWhat do we got? Deep learning? Tim Detmos. This is, yeah, which GPUs to get for deep learning?\nNow, I believe at the time of this video, I think it's been updated since this date. So don't take\nmy word for it. But this is a fantastic blog post for figuring out what GPUs see this post\nfor what option to get. And then number three is use cloud computing. So such as\nGCP, which is Google Cloud Platform AWS, which is Amazon Web Services or Azure.\nThese services, which is Azure is by Microsoft, allow you to rent computers on the cloud and access\nthem. So the first option using Google Colab, which is what we're using is by far the easiest\nand free. So there's big advantages there. However, the downside is that you have to use a website\nhere, Google Colab, you can't run it locally. You don't get the benefit of using cloud computing,\nbut my personal workflow is I run basically all of my small scale experiments and things like\nlearning new stuff in Google Colab. And then if I want to upgrade things, run video experiments,\nI have my own dedicated deep learning PC, which I have built with a big powerful GPU. And then\nalso I use cloud computing if necessary. So that's my workflow. Start with Google Colab.\nAnd then these two, if I need to do some larger experiments. But because this is the beginning\nof course, we can just stick with Google Colab for the time being. But I thought I'd make you aware\nof these other two options. And if you'd like to set up a GPU, so four, two, three, PyTorch plus\nGPU drivers, which is CUDA takes a little bit of setting up to do this, refer to PyTorch\nsetup documentation. So if we go to pytorch.org, they have some great setup guides here,\nget started. And we have start locally. This is if you want to run on your local machine,\nsuch as a Linux setup. This is what I have Linux CUDA 11.3. It's going to give you a\nconda install command to use conda. And then if you want to use cloud partners, which is Alibaba\nCloud, Amazon Web Services, Google Cloud Platform, this is where you'll want to go. So I'll just link\nthis in here. But for this course, we're going to be focusing on using Google Colab. So now,\nlet's see how we might get a GPU in Google Colab. And we've already covered this, but I'm going to\nrecover it just so you know. We're going to change the runtime type. You can go in any notebook and\ndo this, runtime type, hardware accelerator, we can select GPU, click save. Now this is going to\nrestart our runtime and connect us to our runtime, aka a Google compute instance with a GPU. And so\nnow if we run NVIDIA SMI, I have a Tesla P100 GPU. So let's look at this Tesla P100\nGPU. Do we have an image? Yeah, so this is the GPU that I've got running, not the Tesla car,\nthe GPU. So this is quite a powerful GPU. That is because I have upgraded to Colab Pro. Now,\nif you're not using Colab Pro, you might get something like a Tesla K80, which is a slightly\nless powerful GPU than a Tesla P100, but still a GPU nonetheless and will still work faster than\njust running PyTorch code on the pure CPU, which is the default in Google Colab and the default\nin PyTorch. And so now we can also check to see if we have GPU access with PyTorch. So let's go\nhere. This is number two now. Check for GPU access with PyTorch. So this is a little command that's\ngoing to allow us or tell us if PyTorch, just having the GPU here, this is by the way, another\nthing that Colab has a good setup with, is that all the connections between PyTorch and the NVIDIA\nGPU are set up for us. Whereas when you set it up on your own GPU or using cloud computing,\nthere are a few steps you have to go through, which we're not going to cover in this course.\nI'd highly recommend you go through the getting started locally set up if you want to do that,\nto connect PyTorch to your own GPU. So let's check for the GPU access with PyTorch.\nThis is another advantage of using Google Colab. Almost zero set up to get started. So import\ntorch and then we're going to go torch dot cuda dot is available. And remember, cuda is\nNVIDIA's programming interface that allows us to use GPUs for numerical computing. There we go,\nbeautiful. So big advantage of Google Colab is we get access to a free GPU. In my case, I'm paying\nfor the faster GPU, but in your case, you're more than welcome to use the free version.\nAll that means it'll be slightly slower than a faster GPU here. And we now have access to GPUs\nwith PyTorch. So there is one more thing known as device agnostic code. So set up device agnostic\ncode. Now, this is an important concept in PyTorch because wherever you run PyTorch, you might not\nalways have access to a GPU. But if there was access to a GPU, you'd like it to use it if it's\navailable. So one of the ways that this is done in PyTorch is to set the device variable. Now,\nreally, you could set this to any variable you want, but you're going to see it used as device\nquite often. So cuda if torch dot cuda is available. Else CPU. So all this is going to say, and we'll\nsee where we use the device variable later on is set the device to use cuda if it's available. So\nit is so true. If it's not available, if we don't have access to a GPU that PyTorch can use,\njust default to the CPU. So with that being said, there's one more thing. You can also count the\nnumber of GPUs. So this won't really apply to us for now because we're just going to stick with\nusing one GPU. But as you upgrade your PyTorch experiments and machine learning experiments,\nyou might have access to more than one GPU. So you can also count the devices here.\nWe have access to one GPU, which is this here. So the reason why you might want to count the number\nof devices is because if you're running huge models on large data sets, you might want to run one\nmodel on a certain GPU, another model on another GPU, and so on and so on. But final thing before\nwe finish this video is if we go PyTorch device agnostic code, cuda semantics, there's a little\nsection in here called best practices. This is basically what we just covered there is setting\nthe device argument. Now this is using the arg pass, but so yeah, there we go. args.device,\ntorch.device, cuda, args.device, torch.device, CPU. So this is one way to set it from the Python\narguments when you're running scripts, but we're using the version of running it through a notebook.\nSo check this out. I'll just link this here, device agnostic code. It's okay if you're not sure\nof what's going on here. We're going to cover it a little bit more later on throughout the course,\nbut right here for PyTorch, since it's capable of running compute on the GPU or CPU,\nit's best practice to set up device agnostic code, e.g. run on GPU if available,\nelse default to CPU. So check out the best practices for using cuda, which is namely setting up\ndevice agnostic code. And let's in the next video, see what I mean about setting our PyTorch tensors\nand objects to the target device. Welcome back. In the last video, we checked out a few different\noptions for getting a GPU, and then getting PyTorch to run on the GPU. And for now we're using\nGoogle Colab, which is the easiest way to get set up because it gives us free access to a GPU,\nfaster ones if you set up with Colab Pro, and it comes with PyTorch automatically set up to\nuse the GPU if it's available. So now let's see how we can actually use the GPU. So to do so,\nwe'll look at putting tensors and models on the GPU. So the reason we want our tensors slash models\non the GPU is because using GPU results in faster computations. And if we're getting our machine\nlearning models to find patterns and numbers, GPUs are great at doing numerical calculations.\nAnd the numerical calculations we're going to be doing are tensor operations like we saw above.\nSo the tensor operations, well, we've covered a lot. Somewhere here, tensor operations,\nthere we go, manipulating tensor operations. So if we can run these computations faster,\nwe can discover patterns in our data faster, we can do more experiments, and we can work towards\nfinding the best possible model for whatever problem that we're working on. So let's see,\nwe'll create a tensor, as usual, create a tensor. Now the default is on the CPU.\nSo tensor equals torch dot tensor. And we'll just make it a nice simple one, one, two, three.\nAnd let's write here, tensor not on GPU will print out tensor. And this is where we can use,\nwe saw this parameter before device. Can we pass it in here? Device equals CPU.\nLet's see what this comes out with. There we go. So if we print it out, tensor 123 is on the CPU.\nBut even if we got rid of that device parameter, by default, it's going to be on the CPU. Wonderful.\nSo now PyTorch makes it quite easy to move things to, and I'm saying to for a reason,\nto the GPU, or to, even better, the target device. So if the GPU is available, we use CUDA.\nIf it's not, it uses CPU. This is why we set up the device variable. So let's see,\nmove tensor to GPU. If available,\ntensor on GPU equals tensor dot two device. Now let's have a look at this, tensor on GPU.\nSo this is going to shift the tensor that we created up here to the target device.\nWonderful. Look at that. So now our tensor 123 is on device CUDA zero. Now this is the index of\nthe GPU that we're using, because we only have one, it's going to be at index zero. So later on,\nwhen you start to do bigger experiments and work with multiple GPUs, you might have different tensors\nthat are stored on different GPUs. But for now, we're just sticking with one GPU, keeping it nice\nand simple. And so you might have a case where you want to move, oh, actually, the reason why we\nset up device agnostic code is again, this code would work if we run this, regardless if we had,\nso it won't error out. But regardless if we had a GPU or not, this code will work. So whatever device\nwe have access to, whether it's only a CPU or whether it's a GPU, this tensor will move to whatever\ntarget device. But since we have a GPU available, it goes there. You'll see this a lot. This two\nmethod moves tensors and it can be also used for models. We're going to see that later on. So just\nkeep two device in mind. And then you might want to, for some computations, such as using NumPy,\nNumPy only works with the CPU. So you might want to move tensors back to the CPU, moving tensors back\nto the CPU. So can you guess how we might do that? It's okay if you don't know. We haven't covered a\nlot of things, but I'm going to challenge you anyway, because that's the fun part of thinking\nabout something. So let's see how we can do it. Let's write down if tensor is on GPU, can't transform\nit to NumPy. So let's see what happens if we take our tensor on the GPU and try to go NumPy.\nWhat happens? Well, we get an error. So this is another huge error. Remember the top three\nerrors in deep learning or pytorch? There's lots of them, but number one, shape errors,\nnumber two, data type issues. And with pytorch, number three is device issues. So can't convert\nCUDA zero device type tensor to NumPy. So NumPy doesn't work with the GPU. Use tensor dot CPU\nto copy the tensor to host memory first. So if we call tensor dot CPU, it's going to bring our\ntarget tensor back to the CPU. And then we should be able to use it with NumPy. So\nto fix the GPU tensor with NumPy issue, we can first set it to the CPU. So tensor back on CPU\nequals tensor on GPU dot CPU. We're just taking what this said here. That's a beautiful thing\nabout pytorch is very helpful error messages. And then we're going to go NumPy.\nAnd then if we go tensor back on CPU, is this going to work? Let's have a look. Oh, of course,\nit's not because I typed it wrong. And I've typed it again twice. Third time, third time's a charm.\nThere we go. Okay, so that works because we've put it back to the CPU first before calling NumPy.\nAnd then if we refer back to our tensor on the GPU, because we've reassociated this, again,\nwe've got typos galore classic, because we've reassigned tensor back on CPU, our tensor on\nGPU remains unchanged. So that's the four main things about working with pytorch on the GPU.\nThere are a few more tidbits such as multiple GPUs, but now you've got the fundamentals. We're\ngoing to stick with using one GPU. And if you'd like to later on once you've learned a bit more\nresearch into multiple GPUs, well, as you might have guessed, pytorch has functionality for that too.\nSo have a go at getting access to a GPU using colab, check to see if it's available, set up device\nagnostic code, create a few dummy tensors and just set them to different devices, see what happens\nif you change the device parameter, run a few errors by trying to do some NumPy calculations\nwith tensors on the GPU, and then bring those tensors on the GPU back to NumPy and see what happens\nthere. So I think we've covered, I think we've reached the end of the fundamentals. We've covered\na fair bit. Introduction to tensors, the minmax, a whole bunch of stuff inside the introduction\nto tensors, finding the positional minmax, reshaping, indexing, working with tensors and NumPy,\nreproducibility, using a GPU and moving stuff back to the GPU far out. Now you're probably wondering,\nDaniel, we've covered a whole bunch. What should I do to practice all this? Well, I'm glad you asked.\nLet's cover that in the next video. Welcome back. And you should be very proud of your\nself right now. We've been through a lot, but we've covered a whole bunch of PyTorch fundamentals.\nThese are going to be the building blocks that we use throughout the rest of the course.\nBut before moving on to the next section, I'd encourage you to try out what you've learned\nthrough the exercises and extra curriculum. Now, I've set up a few exercises here based off\neverything that we've covered. If you go into learn pytorch.io, go to the section that we're\ncurrently on. This is going to be the case for every section, by the way. So just keep this in mind,\nis we're working on PyTorch fundamentals. Now, if you go to the PyTorch fundamentals notebook,\nthis is going to refresh, but that if you scroll down to the table of contents at the bottom of\neach one is going to be some exercises and extra curriculum. So these exercises here,\nsuch as documentation reading, because a lot you've seen me refer to the PyTorch documentation\nfor almost everything we've covered a lot, but it's important to become familiar with that.\nSo exercise number one is read some of the documentation. Exercise number two is create a\nrandom tensor with shape, seven, seven. Three, perform a matrix multiplication on the tensor from two\nwith another random tensor. So these exercises are all based off what we've covered here.\nSo I'd encourage you to reference what we've covered in whichever notebook you choose,\ncould be this learn pytorch.io, could be going back through the one we've just coded together\nin the video. So I'm going to link this here, exercises, see exercises for this notebook here.\nSo then how should you approach these exercises? So one way would be to just read them here,\nand then in collab we'll go file new notebook, wait for the notebook to load. Then you could call this\nzero zero pytorch exercises or something like that, and then you could start off by importing\ntorch, and then away you go. For me, I'd probably set this up on one side of the screen, this one\nup on the other side of the screen, and then I just have the exercises here. So number one,\nI'm not going to really write much code for that, but you could have documentation reading here.\nAnd then so this encourages you to read through torch.tensor and go through there\nfor 10 minutes or so. And then for the other ones, we've got create a random tensor with shape\nseven seven. So we just comment that out. So torch, round seven seven, and there we go.\nSome are as easy as that. Some are a little bit more complex. As we go throughout the course,\nthese exercises are going to get a little bit more in depth as we've learned more.\nBut if you'd like an exercise template, you can come back to the GitHub. This is the home for all\nof the course materials. You can go into extras and then exercises. I've created templates for\neach of the exercises. So pytorch fundamentals exercises. If you open this up, this is a template\nfor all of the exercises. So you see there, create a random tensor with shape seven seven.\nThese are all just headings. And if you'd like to open this in CoLab and work on it,\nhow can you do that? Well, you can copy this link here. Come to Google CoLab. We'll go file,\nopen notebook, GitHub. You can type in the link there. Click search. What's this going to do?\nBoom. Pytorch fundamentals exercises. So now you can go through all of the exercises. This\nwill be the same for every module on the course and test your knowledge. Now it is open book. You\ncan use the notebook here, the ones that we've coded together. But I would encourage you to try\nto do these things on your own first. If you get stuck, you can always reference back. And then\nif you'd like to see an example solutions, you can go back to the extras. There's a solutions folder\nas well. And that's where the solutions live. So the fundamental exercise solutions. But again,\nI would encourage you to try these out, at least give them a go before having a look at the solutions.\nSo just keep that in mind at the end of every module, there's exercises and extra curriculum.\nThe exercises will be code based. The extra curriculum is usually like reading based.\nSo spend one hour going through the Pytorch basics tutorial. I recommend the quick start\nin tensor sections. And then finally to learn more on how a tensor can represent data,\nwatch the video what's a tensor which we referred to throughout this. But massive effort on finishing\nthe Pytorch fundamentals section. I'll see you in the next section. Friends, welcome back to\nthe Pytorch workflow module. Now let's have a look at what we're going to get into.\nSo this is a Pytorch workflow. And I say a because it's one of many. When you get into\ndeep learning machine learning, you'll find that there's a fair few ways to do things. But here's\nthe rough outline of what we're going to do. We're going to get our data ready and turn it into\ntensors because remember a tensor can represent almost any kind of data. We're going to pick or\nbuild or pick a pre-trained model. We'll pick a loss function and optimize it. Don't worry if\nyou don't know what they are. We're going to cover this. We're going to build a training loop,\nfit the model to make a prediction. So fit the model to the data that we have. We'll learn how\nto evaluate our models. We'll see how we can improve through experimentation and we'll save\nand reload our trained model. So if you wanted to export your model from a notebook and use it\nsomewhere else, this is what you want to be doing. And so where can you get help? Probably the most\nimportant thing is to follow along with the code. We'll be coding all of this together.\nRemember model number one. If and out, run the code. Try it for yourself. That's how I learn best.\nIs I write code? I try it. I get it wrong. I try again and keep going until I get it right.\nRead the doc string because that's going to show you some documentation about the functions that\nwe're using. So on a Mac, you can use shift command and space in Google Colab or if you're on a Windows\nPC, it might be control here. If you're still stuck, try searching for it. You'll probably come\nacross resources such as stack overflow or the PyTorch documentation. We've already seen this\na whole bunch and we're probably going to see it a lot more throughout this entire course actually\nbecause that's going to be the ground truth of everything PyTorch. Try again. And finally,\nif you're still stuck, ask a question. So the best place to ask a question will be\nat the PyTorch deep learning slash discussions tab. And then if we go to GitHub,\nthat's just under here. So Mr. Deeburg PyTorch deep learning. This is all the course materials.\nWe see here, this is your ground truth for the entire course. And then if you have a question,\ngo to the discussions tab, new discussion, you can ask a question there. And don't forget to\nplease put the video and the code that you're trying to run. That way we can reference\nwhat's going on and help you out there. And also, don't forget, there is the book version of the\ncourse. So learn pytorch.io. By the time you watch this video, it'll probably have all the chapters\nhere. But here's what we're working through. This is what the videos are based on. All of this,\nwe're going to go through all of this. How fun is that? But this is just reference material.\nSo you can read this at your own time. We're going to focus on coding together. And speaking of coding.\nLet's code. I'll see you over at Google Colab.\nOh, right. Well, let's get hands on with some code. I'm going to come over to colab.research.google.com.\nYou may already have that bookmark. And I'm going to start a new notebook. So we're going to do\neverything from scratch here. We'll let this load up. I'm just going to zoom in a little bit.\nBeautiful. And now I'm going to title this 01 pytorch workflow. And I'm going to put the video\nending on here so that you know that this notebook's from the video. Why is that? Because in the\ncourse resources, we have the original notebook here, which is what this video notebook is going\nto be based off. You can refer to this notebook as reference for what we're going to go through.\nIt's got a lot of pictures and beautiful text annotations. We're going to be focused on the\ncode in the videos. And then of course, you've got the book version of the notebook as well,\nwhich is just a different formatted version of this exact same notebook. So I'm going to link\nboth of these up here. So let's write in here, pytorch workflow. And let's explore an example,\npytorch end to end workflow. And then I'm going to put the resources. So ground truth notebook.\nWe go here. And I'm also going to put the book version.\nBook version of notebook. And finally, ask a question, which will be where at the discussions\npage. Then we'll go there. Beautiful. Let's turn this into markdown. So let's get started. Let's\njust jump right in and start what we're covering. So this is the trend I want to start getting\ntowards is rather than spending a whole bunch of time going through keynotes and slides,\nI'd rather we just code together. And then we explain different things as they need to be\nexplained because that's what you're going to be doing if you end up writing a lot of pytorch is\nyou're going to be writing code and then looking things up as you go. So I'll get out of these\nextra tabs. I don't think we need them. Just these two will be the most important. So what we're\ncovering, let's create a little dictionary so we can check this if we wanted to later on.\nSo referring to our pytorch workflows, at least the example one that we're going to go through,\nwhich is just here. So we're going to go through all six of these steps, maybe a little bit of\neach one, but just to see it going from this to this, that's what we're really focused on. And then\nwe're going to go through through rest the course like really dig deep into all of these. So what\nwe're covering number one is data preparing and loading. Number two is we're going to see how we\ncan build a machine learning model in pytorch or a deep learning model. And then we're going\nto see how we're going to fit our model to the data. So this is called training. So fit is another\nword. As I said in machine learning, there's a lot of different names for similar things,\nkind of confusing, but you'll pick it up with time. So we're going to once we've trained a model,\nwe're going to see how we can make predictions and evaluate those predictions,\nevaluating a model. If you make predictions, it's often referred to as inference. I typically\nsay making predictions, but inference is another very common term. And then we're going to look\nat how we can save and load a model. And then we're going to put it all together. So a little bit\ndifferent from the visual version we have of the pytorch workflow. So if we go back to here,\nI might zoom in a little. There we go. So we're going to focus on this one later on,\nimprove through experimentation. We're just going to focus on the getting data ready,\nbuilding a model, fitting the model, evaluating model, save and reload. So we'll see this one more,\nlike in depth later on, but I'll hint at different things that you can do\nfor this while we're working through this workflow. And so let's put that in here.\nAnd then if we wanted to refer to this later, we can just go what we're covering.\nOh, this is going to connect, of course. Beautiful. So we can refer to this later on,\nif we wanted to. And we're going to start by import torch. We're going to get pytorch ready\nto go import nn. So I'll write a note here. And then we haven't seen this one before, but\nwe're going to see a few things that we haven't seen, but that's okay. We'll explain it as we go.\nSo nn contains all of pytorch's building blocks for neural networks.\nAnd how would we learn more about torch nn? Well, if we just go torch.nn, here's how I'd\nlearn about it, pytorch documentation. Beautiful. Look at all these. These are the basic building\nblocks for graphs. Now, when you see the word graph, it's referring to a computational graph,\nwhich is in the case of neural networks, let's look up a photo of a neural network.\nImages, this is a graph. So if you start from here, you're going to go towards the right.\nThere's going to be many different pictures. So yeah, this is a good one. Input layer. You have\na hidden layer, hidden layer to output layer. So torch and n comprises of a whole bunch of\ndifferent layers. So you can see layers, layers, layers. And each one of these, you can see input\nlayer, hidden layer one, hidden layer two. So it's our job as data scientists and machine\nlearning engineers to combine these torch dot nn building blocks to build things such as these.\nNow, it might not be exactly like this, but that's the beauty of pytorch is that you can\ncombine these in almost any different way to build any kind of neural network you can imagine.\nAnd so let's keep going. That's torch nn. We're going to get hands on with it,\nrather than just talk about it. And we're going to need map plot lib because what's our other\nmotto? Our data explorers motto is visualize, visualize, visualize. And let's check our pytorch\nversion. Pytorch version torch dot version. So this is just to show you you'll need\nat least this version. So 1.10 plus CUDA 111. That means that we've got CU stands for CUDA.\nThat means we've got access to CUDA. We don't have a GPU on this runtime yet,\nbecause we haven't gone to GPU. We might do that later.\nSo if you have a version that's lower than this, say 1.8, 0.0, you'll want pytorch 1.10 at least.\nIf you have a version higher than this, your code should still work. But that's about enough\nfor this video. We've got our workflow ready to set up our notebook, our video notebook.\nWe've got the resources. We've got what we're covering. We've got our dependencies.\nLet's in the next one get started on one data, preparing and loading.\nI'll see you in the next video.\nLet's now get on to the first step of our pytorch workflow. And that is data, preparing and loading.\nNow, I want to stress data can be almost anything in machine learning.\nI mean, you could have an Excel spreadsheet, which is rows and columns,\nnice and formatted data. You could have images of any kind. You could have videos. I mean,\nYouTube has lots of data. You could have audio like songs or podcasts. You could have even DNA\nthese days. Patents and DNA are starting to get discovered by machine learning. And then, of course,\nyou could have text like what we're writing here. And so what we're going to be focusing on\nthroughout this entire course is the fact that machine learning is a game of two parts.\nSo one, get data into a numerical representation to build a model to learn patterns in that\nnumerical representation. Of course, there's more around it. Yes, yes, yes. I understand you can\nget as complex as you like, but these are the main two concepts. And machine learning, when I say\nmachine learning, saying goes for deep learning, you need some kind of, oh, number a call. Number\na call. I like that word, number a call representation. Then you want to build a model to learn patterns\nin that numerical representation. And if you want, I've got a nice pretty picture that describes that\nmachine learning a game of two parts. Let's refer to our data. Remember, data can be almost\nanything. These are our inputs. So the first step that we want to do is create some form\nof numerical encoding in the form of tenses to represent these inputs, how this looks will be\ndependent on the data, depending on the numerical encoding you choose to use. Then we're going to\nbuild some sort of neural network to learn a representation, which is also referred to as\npatterns features or weights within that numerical encoding. It's going to output that\nrepresentation. And then we want to do something without representation, such as in the case of\nthis, we're doing image recognition, image classification, is it a photo of Raman or spaghetti?\nIs this tweet spam or not spam? Is this audio file saying what it says here? I'm not going to say\nthis because my audio assistant that's also named to this word here is close by and I don't want it\nto go off. So this is our game of two parts. One here is convert our data into a numerical\nrepresentation. And two here is build a model or use a pre trained model to find patterns in\nthat numerical representation. And so we've got a little stationary picture here, turn data into\nnumbers, part two, build a model to learn patterns in numbers. So with that being said,\nnow let's create some data to showcase this. So to showcase this, let's create some known\ndata using the linear regression formula. Now, if you're not sure what linear regression is,\nor the formula is, let's have a look linear regression formula. This is how I'd find it.\nOkay, we have some fancy Greek letters here. But essentially, we have y equals a function of x\nand b plus epsilon. Okay. Well, there we go. A linear regression line has the equation in the\nform of y equals a plus bx. Oh, I like this one better. This is nice and simple. We're going to\nstart from as simple as possible and work up from there. So y equals a plus bx, where x is the\nexplanatory variable, and y is the dependent variable. The slope of the line is b. And the\nslope is also known as the gradient. And a is the intercept. Okay, the value of when y\nwhen x equals zero. Now, this is just text on a page. This is formula on a page. You know how I\nlike to learn things? Let's code it out. So let's write it here. We'll use a linear regression formula\nto make a straight line with known parameters. I'm going to write this down because parameter\nis a common word that you're going to hear in machine learning as well. So a parameter is\nsomething that a model learns. So for our data set, if machine learning is a game of two parts,\nwe're going to start with this. Number one is going to be done for us, because we're going to\nstart with a known representation, a known data set. And then we want our model to learn that\nrepresentation. This is all just talk, Daniel, let's get into coding. Yes, you're right. You're\nright. Let's do it. So create known parameters. So I'm going to use a little bit different\nnames to what that Google definition did. So weight is going to be 0.7 and bias is going to be 0.3.\nNow weight and bias are another common two terms that you're going to hear in neural networks.\nSo just keep that in mind. But for us, this is going to be the equivalent of our weight will be B\nand our bias will be A. But forget about this for the time being. Let's just focus on the code.\nSo we know these numbers. But we want to build a model that is able to estimate these numbers.\nHow? By looking at different examples. So let's create some data here. We're going to create a\nrange of numbers. Start equals zero and equals one. We're going to create some numbers between\nzero and one. And they're going to have a gap. So the step the gap is going to be 0.02.\nNow we're going to create an X variable. Why is X a capital here?\nWell, it's because typically X in machine learning you'll find is a matrix or a tensor.\nAnd if we remember back to the fundamentals, a capital represents a matrix or a tensor\nand a lowercase represents a vector. But now case it's going to be a little confusing because\nX is a vector. But later on, X will start to be a tensor and a matrix. So for now,\nwe'll just keep the capital, not capital notation.\nWe're going to create the formula here, which is remember how I said our weight is in this case,\nthe B and our bias is the A. So we've got the same formula here. Y equals weight times X plus\nbias. Now let's have a look at these different numbers. So we'll view the first 10 of X and we'll\nview the first 10 of Y. We'll have a look at the length of X and we'll have a look at the length of\nY. Wonderful. So we've got some values here. We've got 50 numbers of each. This is a little\nconfusing. Let's just view the first 10 of X and Y first. And then we can have a look at the\nlength here. So what we're going to be doing is building a model to learn some values,\nto look at the X values here and learn what the associated Y value is and the relationship\nbetween those. Of course, we know what the relationship is between X and Y because we've\ncoded this formula here. But you won't always know that in the wild. That is the whole premise of\nmachine learning. This is our ideal output and this is our input. The whole premise of machine\nlearning is to learn a representation of the input and how it maps to the output. So here are our\ninput numbers and these are our output numbers. And we know that the parameters of the weight and\nbias are 0.7 and 0.3. We could have set these to whatever we want, by the way. I just like the\nnumber 7 and 3. You could set these to 0.9, whatever, whatever. The premise would be the same.\nSo, oh, and what I've just done here, I kind of just coded this without talking.\nBut I just did torch a range and it starts at 0 and it ends at 1 and the step is 0.02. So there\nwe go, 000 by 0.02, 04. And I've unsqueezed it. So what does unsqueezed do? Removes the extra\ndimensions. Oh, sorry, ads are extra dimension. You're getting confused here. So if we remove that,\nwe get no extra square bracket. But if we add unsqueeze, you'll see that we need this later on\nfor when we're doing models. Wonderful. So let's just leave it at that. That's enough for this\nvideo, we've got some data to work with. Don't worry if this is a little bit confusing for now,\nwe're going to keep coding on and see what we can do to build a model to infer patterns in this\ndata. But right now, I want you to have a think, this is tensor data, but it's just numbers on a\npage. What might be a better way to hint, this is a hint by the way, visualize it. What's our\ndata explorer's motto? Let's have a look at that in the next video. Welcome back. In the last\nvideo, we created some numbers on a page using the linear regression formula with some known\nparameters. Now, there's a lot going on here, but that's all right. We're going to keep building\nupon what we've done and learn by doing. So in this video, we're going to cover one of the most\nimportant concepts in machine learning in general. So splitting data into training and test sets.\nOne of the most important concepts in machine learning in general. Now, I know I've said this\nalready a few times. One of the most important concepts, but truly, this is possibly, in terms\nof data, this is probably the number one thing that you need to be aware of. And if you've come\nfrom a little bit of a machine learning background, you probably well and truly know all about this.\nBut we're going to recover it anyway. So let's jump in to some pretty pictures. Oh, look at that\none speaking of pretty pictures. But that's not what we're focused on now. We're looking at the\nthree data sets. And I've written down here possibly the most important concept in machine\nlearning, because it definitely is from a data perspective. So the course materials,\nimagine you're at university. So this is going to be the training set. And then you have the\npractice exam, which is the validation set. Then you have the final exam, which is the test set.\nAnd the goal of all of this is for generalization. So let's step back. So say you're trying to learn\nsomething at university or through this course, you might have all of the materials, which is your\ntraining set. So this is where our model learns patterns from. And then to practice what you've\ndone, you might have a practice exam. So the mid semester exam or something like that. Now,\nlet's just see if you're learning the course materials well. So in the case of our model,\nwe might tune our model on this plastic exam. So we might find that on the validation set,\nour model doesn't do too well. And we adjusted a bit, and then we retrain it, and then it does\nbetter. Before finally, at the end of semester, the most important exam is your final exam. And\nthis is to see if you've gone through the entire course materials, and you've learned some things.\nNow you can adapt to unseen material. And that's a big point here. We're going to see this in\npractice is that when the model learns something on the course materials, it never sees the validation\nset or the test set. So say we started with 100 data points, you might use 70 of those data points\nfor the training material. You might use 15% of those data points, so 15 for the practice.\nAnd you might use 15 for the final exam. So this final exam is just like if you're at university\nlearning something is to see if, hey, have you learned any skills from this material at all?\nAre you ready to go into the wild into the quote unquote real world? And so this final exam is to\ntest your model's generalization, because it's never seen this data is, let's define generalization\nis the ability for a machine learning model or a deep learning model to perform well on data it\nhasn't seen before, because that's our whole goal, right? We want to build a machine learning model\non some training data that we can deploy in our application or production setting. And then\nmore data comes in that it hasn't seen before. And it can make decisions based on that new data\nbecause of the patterns it's learned in the training set. So just keep this in mind,\nthree data sets training validation test. And if we jump in to the learn pytorch book,\nwe've got split data. So we're going to create three sets. Or in our case, we're only going to\ncreate two or training in a test. Why is that? Because you don't always need a validation set.\nThere is often a use case for a validation set. But the main two that are always used is the training\nset and the testing set. And how much should you split? Well, usually for the training set,\nyou'll have 60 to 80% of your data. If you do create a validation set, you'll have somewhere\nbetween 10 and 20. And if you do create a testing set, it's a similar split to the validation set,\nyou'll have between 10 and 20%. So training, always testing always validation often, but\nnot always. So with that being said, I'll let you refer to those materials if you want. But now\nlet's create a training and test set with our data. So we saw before that we have 50 points,\nwe have X and Y, we have one to one ratio. So one value of X relates to one value of Y.\nAnd we know that the split now for the training set is 60 to 80%. And the test set is 10 to 20%.\nSo let's go with the upper bounds of each of these, 80% and 20%, which is a very common split,\nactually 80, 20. So let's go create a train test split. And we're going to go train split.\nWe'll create a number here so we can see how much. So we want an integer of 0.8, which is 80%\nof the length of X. What does that give us? Train split should be about 40 samples. Wonderful.\nSo we're going to create 40 samples of X and 40 samples of Y. Our model will train on those 40\nsamples to predict what? The other 10 samples. So let's see this in practice. So X train,\nY train equals X. And we're going to use indexing to get all of the samples up until the train\nsplit. That's what this colon does here. So hey, X up until the train split, Y up until the train\nsplit, and then for the testing. Oh, thanks for that. Auto correct cola, but didn't actually need that\none. X test. Y test equals X. And then we're going to get everything from the train split onwards.\nSo the index onwards, that's what this notation means here. And Y from the train split onwards as\nwell. Now, there are many different ways to create a train and test split. Ours is quite simple here,\nbut that's because we're working with quite a simple data set. One of the most popular methods\nthat I like is scikit learns train test split. We're going to see this one later on. It adds a\nlittle bit of randomness into splitting your data. But that's for another video, just to make you\naware of it. So let's go length X train. We should have 40 training samples to\nhow many testing samples length X test and length Y test. Wonderful 40 40 10 10 because we have\ntraining features, training labels, testing features, testing labels. So essentially what we've\ncreated here is now a training set. We've split our data. Training set could also be referred to\nas training split yet another example of where machine learning has different names for different\nthings. So set split same thing training split test split. This is what we've created. Remember,\nthe validation set is used often, but not always because our data set is quite simple. We're just\nsticking with the necessities training and test. But keep this in mind. One of your biggest,\nbiggest, biggest hurdles in machine learning will be creating proper training and test sets. So\nit's a very important concept. With that being said, I did issue the challenge in the last video\nto visualize these numbers on a page. We haven't done that in this video. So let's move towards\nthat next. I'd like you to think of how could you make these more visual? Right. These are just\nnumbers on a page right now. Maybe that plot lib can help. Let's find out. Hey, hey, hey, welcome\nback. In the last video, we split our data into training and test sets. And now later on,\nwe're going to be building a model to learn patterns in the training data to relate to the\ntesting data. But as I said, right now, our data is just numbers on a page. It's kind of\nhard to understand. You might be able to understand this, but I prefer to get visual. So let's write\nthis down. How might we better visualize our data? And I'm put a capital here. So we're grammatically\ncorrect. And this is where the data Explorers motto comes in. Visualize, visualize, visualize.\nHa ha. Right. So if ever you don't understand a concept, one of the best ways to start\nunderstanding it more for me is to visualize it. So let's write a function to do just that.\nWe're going to call this plot predictions. We'll see why we call it this later on. That's the\nbenefit of making these videos is that I've got a plan for the future. Although it might seem\nlike I'm winging it, there is a little bit of behind the scenes happening here. So we'll have\nthe train data, which is our X train. And then we'll have the train labels, which is our Y train.\nAnd we'll also have the test data. Yeah, that's a good idea. X test. And we'll also have the test\nlabels, equals Y test. Excuse me. I was looking at too many X's there. And then the predictions.\nAnd we'll set this to none, because we don't have any predictions yet. But as you might have guessed,\nwe might have some later on. So we'll put a little doc string here, so that we're being nice and\nPythonic. So plots training data, test data, and compares predictions. Nice and simple.\nNothing too outlandish. And then we're going to create a figure. This is where map plot lib comes\nin. Plot figure. And we'll go fig size equals 10, seven, which is my favorite hand in poker.\nAnd we'll plot the training data in blue also happens to be a good dimension for a map plot.\nPlot dot scatter. Train data. Creating a scatter plot here. We'll see what it does in a second.\nColor. We're going to give this a color of B for blue. That's what C stands for in map plot lib\nscatter. We'll go size equals four and label equals training data. Now, where could you find\ninformation about this scatter function here? We've got command shift space. Is that going to\ngive us a little bit of a doc string? Or sometimes if command not space is not working,\nyou can also hover over this bracket. I think you can even hover over this.\nThere we go. But this is a little hard for me to read. Like it's there, but it's got a lot going\non. X, Y, S, C, C map. I just like to go map plot lib scatter. There we go. We've got a whole\nbunch of information there. A little bit easier to read for me here. And then you can see some\nexamples. Beautiful. So now let's jump back into here. So in our function plot predictions,\nwe've taken some training data, test data. We've got the training data plotting in blue. What\ncolor should we use for the testing data? How about green? I like that idea. Plot.scatter.\nTest data. Green's my favorite color. What's your favorite color? C equals G. You might be\nable to just plot it in your favorite color here. Just remember though, it'll be a little bit\ndifferent from the videos. And then we're going to call this testing data. So just the exact same\nline is above, but with a different set of data. Now, let's check if there are predictions. So\nare there predictions? So if predictions is not none, let's plot the predictions, plot the\npredictions, if they exist. So plot scatter test data. And why are we plotting the test data?\nRemember, what is our scatter function? Let's go back up to here. It takes in x and y. So\nour predictions are going to be compared to the testing data labels. So that's the whole\ngame that we're playing here. We're going to train our model on the training data.\nAnd then to evaluate it, we're going to get our model to predict the y values\nas with the input of x test. And then to evaluate our model, we compare how good our models\npredictions are. In other words, predictions versus the actual values of the test data set.\nBut we're going to see this in practice. Rather than just talk about it. So let's do our predictions\nin red. And label equals predictions. Wonderful. So let's also show the legend, because, I mean,\nwe're legends. So we could just put in a mirror here. Now I'm kidding. Legend is going to show\nour labels on the map plot. So prop equals size and prop stands for properties. Well,\nit may or may not. I just like to think it does. That's how I remember it. So we have a beautiful\nfunction here to plot our data. Should we try it out? Remember, we've got hard coded inputs here,\nso we don't actually need to input anything to our function. We've got our train and test data\nready to go. If in doubt, run the code, let's check it out. Did we make a mistake in our plot\npredictions function? You might have caught it. Hey, there we go. Beautiful. So because we don't\nhave any predictions, we get no red dots. But this is what we're trying to do. We've got a simple\nstraight line. You can't get a much more simple data set than that. So we've got our training data\nin blue, and we've got our testing data in green. So the whole idea of what we're going to be doing\nwith our machine learning model is we don't actually really need to build a machine learning\nmodel for this. We could do other things, but machine learning is fun. So we're going to take\nin the blue dots. There's quite a pattern here, right? This is the relationship we have an x value\nhere, and we have a y value. So we're going to build a model to try and learn the pattern\nof these blue dots, so that if we fed our model, our model, the x values of the green dots,\ncould it predict the appropriate y values for that? Because remember, these are the test data set.\nSo pass our model x test to predict y test. So blue dots as input, green dots as the ideal output.\nThis is the ideal output, a perfect model would have red dots over the top of the green dots. So\nthat's what we will try to work towards. Now, we know the relationship between x and y.\nHow do we know that? Well, we set that up above here. This is our weight and bias.\nWe created that line y equals weight times x plus bias, which is the simple version of the\nlinear regression formula. So mx plus c, you might have heard that in high school algebra,\nso gradient plus intercept. That's what we've got. With that being said,\nlet's move on to the next video and build a model. Well, this is exciting. I'll see you there.\nWelcome back. In the last video, we saw how to get visual with our data. We followed the data\nexplorer's motto of visualize, visualize, visualize. And we've got an idea of the training data that\nwe're working with and the testing data that we're trying to build a model to learn the patterns\nin the training data, essentially this upwards trend here, to be able to predict the testing data.\nSo I just want to give you another heads up. I took a little break after the recording last\nvideo. And so now my colab notebook has disconnected. So I'm going to click reconnect.\nAnd my variables here may not work. So this is what might happen on your end. If you take a break\nfrom using Google Colab and come back, if I try to run this function, they might have been saved,\nit looks like they have. But if not, you can go restart and run all. This is typically one of the\nmost helpful troubleshooting steps of using Google Colab. If a cell, say down here isn't working,\nyou can always rerun the cells above. And that may help with a lower cell here, such as if this\nfunction wasn't instantiated because this cell wasn't run, and we couldn't run this cell here,\nwhich calls this function here, we just have to rerun this cell above so that we can run this one.\nBut now let's get into building our first PyTorch model. We're going to jump straight into the code.\nSo our first PyTorch model. Now this is very exciting.\nLet's do it. So we'll turn this into Markdown. Now we're going to create a linear regression model.\nSo look at linear regression formula again, we're going to create a model that's essentially going\nto run this computation. So we need to create a model that has a parameter for A, a parameter for B,\nand in our case it's going to be weight and bias, and a way to do this forward computation.\nWhat I mean by that, we're going to see with code. So let's do it. We'll do it with pure PyTorch.\nSo create a linear regression model class. Now if you're not experienced with using Python classes,\nI'm going to be using them throughout the course, and I'm going to call this one linear regression\nmodel. If you haven't dealt with Python classes before, that's okay. I'm going to be explaining\nwhat we're doing as we're doing it. But if you'd like a deeper dive, I'd recommend you to real Python\nclasses. OOP in Python three. That's a good rhyming. So I'm just going to link this here.\nBecause we're going to be building classes throughout the course,\nI'd recommend getting familiar with OOP, which is object oriented programming, a little bit of a\nmouthful, hence the OOP in Python. To do so, you can use the following resource from real Python.\nBut when I'm going to go through that now, I'd rather just code it out and talk it out while we\ndo it. So we've got a class here. Now the first thing you might notice is that the class inherits\nfrom nn.module. And you might be wondering, well, what's nn.module? Well, let's write down here,\nalmost everything in PyTorch inherits from nn.module. So you can imagine nn.module as the\nLego building bricks of PyTorch model. And so nn.module has a lot of helpful inbuilt things that's\ngoing to help us build our PyTorch models. And of course, how could you learn more about it?\nWell, you could go nn.module, PyTorch. Module. Here we go. Base class for all neural network\nmodules. Wonderful. Your models should also subclass this class. So that's what we're building. We're\nbuilding our own PyTorch model. And so the documentation here says that your models should\nalso subclass this class. And another thing with PyTorch, this is what makes it, it might seem very\nconfusing when you first begin. But modules can contain other modules. So what I mean by being a\nLego brick is that you can stack these modules on top of each other and make progressively more\ncomplex neural networks as you go. But we'll leave that for later on. For now, we're going to start\nwith something nice and simple. And let's clean up our web browser. So we're going to create a\nconstructor here, which is with the init function. It's going to take self as a parameter. If you're\nnot sure of what's going on here, just follow along with the code for now. And I'd encourage you\nto read this documentation here after the video. So then we have super dot init. I know when I\nfirst started learning this, I was like, why do we have to write a knit twice? And then what's\nsuper and all that jazz. But just for now, just take this as being some required Python syntax.\nAnd then we have self dot weights. So that means we're going to create a weights parameter. We'll\nsee why we do this in a second. And to create that parameter, we're going to use nn dot parameter.\nAnd just a quick reminder that we imported nn from torch before. And if you remember,\nnn is the building block layer for neural networks. And within nn, so nn stands for neural network\nis module. So we've got nn dot parameter. Now, we're going to start with random parameters.\nSo torch dot rand n. One, we're going to talk through each of these in a second. So I'm also\ngoing to put requires, requires grad equals true. We haven't touched any of these, but that's okay.\nD type equals torch dot float. So let's see what nn parameter tells us. What do we have here?\nA kind of tensor that is to be considered a module parameter. So we've just created a module\nusing nn module. Parameters are torch tensor subclasses. So this is a tensor in itself\nthat have a very special property when used with modules. When they're assigned as a module\nattribute, they are automatically added to the list of its parameters. And we'll appear e g\nin module dot parameters iterator. Oh, we're going to see that later on. Assigning a tensor\ndoesn't have such effect. So we're creating a parameter here. Now requires grad. What does that\nmean? Well, let's just rather than just try to read the doc string collab, let's look it up.\nnn dot parameter. What does it say requires grad optional. If the parameter requires gradient.\nHmm. What does requires gradient mean? Well, let's come back to that in a second. And then\nfor now, I just want you to think about it. D type equals torch dot float. Now,\nthe data type here torch dot float is, as we've discussed before, is the default\nfor pytorch to watch dot float. This could also be torch dot float 32. So we're just going to\nleave it as torch float 32, because pytorch likes to work with flight 32. Now, do we have\nthis by default? We do. So we don't necessarily have to set requires grad equals true. So just\nkeep that in mind. So now we've created a parameter for the weights. We also have to create a parameter\nfor the bias. Let's finish creating this. And then we'll write the code, then we'll talk about it.\nSo rand n. Now requires grad equals true. And d type equals torch dot float. There we go.\nAnd now we're going to write a forward method. So forward method to define the computation\nin the model. So let's go def forward, which self takes in a parameter x, which is data,\nwhich X is expected to be of type torch tensor. And it returns a torch dot tensor. And then we go\nhere. And so we say X, we don't necessarily need this comment. I'm just going to write it anyway.\nX is the input data. So in our case, it might be the training data. And then from here, we want\nit to return self dot weights times X plus self dot bias. Now, where have we seen this before?\nWell, this is the linear regression formula. Now, let's take a step back into how we created our data.\nAnd then we'll go back through and talk a little bit more about what's going on here.\nSo if we go back up to our data, where did we create that? We created it here. So you see how\nwe've created known parameters, weight and bias. And then we created our y variable, our target,\nusing the linear regression formula, wait times X plus bias, and X were a range of numbers.\nSo what we've done with our linear regression model that we've created from scratch,\nif we go down here, we've created a parameter, weights. This could just be weight, if we wanted to.\nWe've created a parameter here. So when we created our data, we knew what the parameters weight and\nbias were. The whole goal of our model is to start with random numbers. So these are going to be\nrandom parameters. And to look at the data, which in our case will be the training samples,\nand update those random numbers to represent the pattern here. So ideally, our model, if it's\nlearning correctly, will take our weight, which is going to be a random value, and our bias,\nwhich is going to be a random value. And it will run it through this forward calculation,\nwhich is the same formula that we use to create our data. And it will adjust the weight and bias\nto represent as close as possible, if not perfect, the known parameters. So that's the premise of\nmachine learning. And how does it do this? Through an algorithm called gradient descent. So I'm just\ngoing to write this down because we've talked a lot about this, but I'd like to just tie it together\nhere. So what our model does, so start with random values, weight and bias, look at training data,\nand adjust the random values to better represent the, or get closer to the ideal values. So the\nweight and bias values we use to create the data. So that's what it's going to do. It's going to\nstart with random values, and then continually look at our training data to see if it can adjust\nthose random values to be what would represent this straight line here. Now, how does it do so?\nHow does it do so? Through two main algorithms. So one is gradient descent, and two is back\npropagation. So I'm going to leave it here for the time being, but we're going to continue talking\nabout this gradient descent is why we have requires grad equals true. And so what this is going to\ndo is when we run computations using this model here, pytorch is going to keep track of the gradients\nof our weights parameter and our bias parameter. And then it's going to update them through a\ncombination of gradient descent and back propagation. Now, I'm going to leave this as extracurricular\nfor you to look through and gradient descent and back propagation. I'm going to add some\nresources here. There will also be plenty of resources in the pytorch workflow fundamentals\nbook chapter on how these algorithms work behind the scenes. We're going to be focused on the code,\nthe pytorch code, to trigger these algorithms behind the scenes. So pytorch, lucky for us,\nhas implemented gradient descent and back propagation for us. So we're writing the higher level code\nhere to trigger these two algorithms. So in the next video, we're going to step through this a\nlittle bit more, and then further discuss some of the most useful and required modules of pytorch,\nparticularly an N and a couple of others. So let's leave it there, and I'll see you in the next video.\nWelcome back. In the last video, we covered a whole bunch in creating our first pytorch model\nthat inherits from nn.module. We talked about object oriented programming and how a lot of\npytorch uses object oriented programming. I can't say that. I might just say OOP for now.\nWhat I've done since last video, though, is I've added two resources here for gradient descent\nand back propagation. These are two of my favorite videos on YouTube by the channel three blue\none brown. So this is on gradient descent. I would highly recommend watching this entire series,\nby the way. So that's your extra curriculum for this video, in particular, and for this course overall\nis to go through these two videos. Even if you're not sure entirely what's happening,\nyou will gain an intuition for the code that we're going to be writing with pytorch.\nSo just keep that in mind as we go forward, a lot of what pytorch is doing behind the scenes for us\nis taking care of these two algorithms for us. And we also created two parameters here in our model\nwhere we've instantiated them as random values. So one parameter for each of the ones that we use,\nthe weight and bias for our data set. And now I want you to keep in mind that we're working\nwith a simple data set here. So we've created our known parameters. But in a data set that you\nhaven't created by yourself, you've maybe gathered that from the internet, such as images,\nyou won't be necessarily defining these parameters. Instead, another module from nn will define the\nparameters for you. And we'll work out what those parameters should end up being. But since we're\nworking with a simple data set, we can define our two parameters that we're trying to estimate.\nThis is a key point here is that our model is going to start with random values. That's the\nannotation I've added here. Start with a random weight value using torch random. And then we've\ntold it that it can update via gradient descent. So pytorch is going to track the gradients of\nthis parameter for us. And then we've told it that the D type we want is float 32. We don't\nnecessarily need these two set explicitly, because a lot of the time the default in pytorch is to\nset these two requires grad equals true and d type equals torch dot float. Does that for us\nbehind the scenes? But just to keep things as fundamental and as straightforward as possible,\nwe've set all of this explicitly. So let's jump into the keynote. I'd just like to explain\nwhat's going on one more time in a visual sense. So here's the exact code that we've\njust written. I've just copied it from here. And I've just made it a little bit more colorful.\nBut here's what's going on. So when you build a model in pytorch, it subclasses the nn.modgable\nclass. This contains all the building blocks for neural networks. So our class of model, subclasses\nnn.modgable. Now, inside the constructor, we initialize the model parameters. Now, as we'll see,\nlater on with bigger models, we won't necessarily always explicitly create the weights and biases.\nWe might initialize whole layers. Now, this is a concept we haven't touched on yet, but\nwe might initialize a list of layers or whatever we need. So basically, what happens in here is that\nwe create whatever variables that we need for our model to use. And so these could be different\nlayers from torch.nn, single parameters, which is what we've done in our case, hard coded values,\nor even functions. Now, we've explicitly set requires grad equals true for our model parameters.\nSo this, in turn, means that pytorch behind the scenes will track all of the gradients\nfor these parameters here for use with torch.auto grad. So torch.auto grad module of pytorch is what\nimplements gradient descent. Now, a lot of this will happen behind the scenes for when we write\nour pytorch training code. So if you'd like to know what's happening behind the scenes,\nI'd highly recommend you checking out these two videos, hence is why I've linked them here.\nOh, and for many pytorch.nn modules requires grad is true is set by default.\nFinally, we've got a forward method. Now, any subclass of nn.modgable, which is what we've done,\nrequires a forward method. Now, we can see this in the documentation. If we go torch\ndot nn.modgable.\nClick on module. Do we have forward?\nYeah, there we go. So forward, we've got a lot of things built into an nn.modgable.\nSo you see here, this is a subclass of an nn.modgable. And then we have forward.\nSo forward is what defines the computation performed at every call. So if we were\nto call linear regression model and put some data through it, the forward method is the\noperation that this module does that this model does. And in our case, our forward method is\nthe linear regression function. So keep this in mind, any subclass of nn.modgable needs to\noverride the forward method. So you need to define a forward method if you're going to subclass\nnn.modgable. We'll see this very hands on. But for now, I believe that's enough coverage of what\nwe've done. If you have any questions, remember, you can ask it in the discussions. We've got a\nfair bit going on here. But I think we've broken it down a fair bit. The next step is for us to,\nI know I mentioned this in a previous video is to cover some PyTorch model building essentials.\nBut we're going to cover a few more of them. We've seen some already. But the next way to really\nstart to understand what's going on is to check the contents of our model, train one, and make\nsome predictions with it. So let's get hands on with that in the next few videos. I'll see you there.\nWelcome back. In the last couple of videos, we stepped through creating our first PyTorch model.\nAnd it looks like there's a fair bit going on here. But some of the main takeaways is that almost\nevery model in PyTorch inherits from nn.modgable. And if you are going to inherit from nn.modgable,\nyou should override the forward method to define what computation is happening in your model.\nAnd for later on, when our model is learning things, in other words, updating its weights and\nbias values from random values to values that better fit the data, it's going to do so via\ngradient descent and back propagation. And so these two videos are some extra curriculum\nfor what's happening behind the scenes. But we haven't actually written any code yet to trigger\nthese two. So I'll refer back to these when we actually do write code to do that. For now,\nwe've just got a model that defines some forward computation. But speaking of models, let's have\na look at a couple of PyTorch model building essentials. So we're not going to write too much\ncode for this video, and it's going to be relatively short. But I just want to introduce you to some\nof the main classes that you're going to be interacting with in PyTorch. And we've seen\nsome of these already. So one of the first is torch.nn. So contains all of the building blocks\nfor computational graphs. Computational graphs is another word for neural networks.\nWell, actually computational graphs is quite general. I'll just write here, a neural network\ncan be considered a computational graph. So then we have torch.nn.parameter. We've seen this.\nSo what parameters should our model try and learn? And then we can write here often a PyTorch\nlayer from torch.nn will set these for us. And then we've got torch.nn.module, which is\nwhat we've seen here. And so torch.nn.module is the base class for all neural network modules.\nIf you subclass it, you should overwrite forward, which is what we've done here. We've created our\nown forward method. So what else should we cover here? We're going to see these later\non, but I'm going to put it here, torch.optim. This is where the optimizers in PyTorch live.\nThey will help with gradient descent. So optimizer, an optimizer is, as we've said before,\nthat our model starts with random values. And it looks at training data and adjusts the random\nvalues to better represent the ideal values. The optimizer contains algorithm that's going to\noptimize these values, instead of being random, to being values that better represent our data.\nSo those algorithms live in torch.optim. And then one more for now, I'll link to extra resources.\nAnd we're going to cover them as we go. That's how I like to do things, cover them as we need them.\nSo all nn.module. So this is the forward method. I'm just going to explicitly say here that all\nnn.module subclasses require you to overwrite forward. This method defines what happens\nin the forward computation. So in our case, if we were to pass some data to our linear regression\nmodel, the forward method would take that data and perform this computation here.\nAnd as your models get bigger and bigger, ours is quite straightforward here.\nThis forward computation can be as simple or as complex as you like, depending on what you'd\nlike your model to do. And so I've got a nice and fancy slide here, which basically reiterates\nwhat we've just discussed. PyTorch is central neural network building modules.\nSo the module torch.nn, torch.nn.module, torch.optim, torch.utils.dataset. We haven't actually talked\nabout this yet. And I believe there's one more data loader. We're going to see these two later on.\nBut these are very helpful when you've got a bit more of a complicated data set. In our case,\nwe've got just 50 integers for our data set. We've got a simple straight line. But when we need\nto create more complex data sets, we're going to use these. So this will help us build models.\nThis will help us optimize our models parameters. And this will help us load data. And if you'd\nlike more, one of my favorite resources is the PyTorch cheat sheet. Again, we're referring\nback to the documentation. See, all of this documentation, right? As I said, this course is\nnot a replacement for the documentation. It's just my interpretation of how one should best\nbecome familiar with PyTorch. So we've got imports, the general import torch from torch.utils.dataset\ndata loader. Oh, did you look at that? We've got that mentioned here, data, data set data loader.\nAnd torch, script and jit, neural network API. I want an X. I'll let you go through here.\nWe're covering some of the most fundamental ones here. But there's, of course, PyTorch is\nquite a big library. So some extra curricula for this video would be to go through this for\nfive to 10 minutes and just read. You don't have to understand them all. We're going to start to\nget more familiar with all of these. We're not all of them because, I mean, that would require\nmaking videos for the whole documentation. But a lot of these through writing them via code.\nSo that's enough for this video. I'll link this PyTorch cheat sheet in the video here.\nAnd in the next video, how about we, we haven't actually checked out what happens if we do\ncreate an instance of our linear regression model. I think we should do that. I'll see you there.\nWelcome back. In the last video, we covered some of the PyTorch model building essentials. And look,\nI linked a cheat sheet here. There's a lot going on. There's a lot of text going on in the page.\nOf course, the reference material for here is in the Learn PyTorch book. PyTorch model building\nessentials under 0.1, which is the notebook we're working on here. But I couldn't help myself.\nI wanted to add some color to this. So before we inspect our model, let's just add a little bit\nof color to our text on the page. We go to whoa. Here's our workflow. This is what we're covering\nin this video, right? Or in this module, 0.1. But to get data ready, here are some of the most\nimportant PyTorch modules. Torchvision.transforms. We'll see that when we cover computer vision later\non. Torch.utils.data.data set. So that's if we want to create a data set that's a little bit\nmore complicated than because our data set is so simple, we haven't used either of these\ndata set creator or data loader. And if we go build a picker model, well, we can use torch.nn.\nWe've seen that one. We've seen torch.nn.module. So in our case, we're building a model. But if we\nwanted a pre-trained model, well, there's some computer vision models that have already been\nbuilt for us in torchvision.models. Now torchvision stands for PyTorch's computer vision\nmodule. So we haven't covered that either. But this is just a spoiler for what's coming on\nlater on. Then if the optimizer, if we wanted to optimize our model's parameters to better\nrepresent a data set, we can go to torch.optim. Then if we wanted to evaluate the model,\nwell, we've got torch metrics for that. We haven't seen that, but we're going to be\nhands-on with all of these later on. Then if we wanted to improve through experimentation,\nwe've got torch.utils.tensorboard. Hmm. What's this? But again, if you want more,\nthere's some at the PyTorch cheat sheet. But now this is just adding a little bit of color\nand a little bit of code to our PyTorch workflow. And with that being said, let's get a little bit\ndeeper into what we've built, which is our first PyTorch model. So checking the contents of our\nPyTorch model. So now we've created a model. Let's see what's inside. You might already be able\nto guess this by the fact of what we've created in the constructor here in the init function.\nSo what do you think we have inside our model? And how do you think we'd look in that? Now,\nof course, these are questions you might not have the answer to because you've just, you're like,\nDaniel, I'm just starting to learn PyTorch. I don't know these, but I'm asking you just to start\nthinking about these different things, you know? So we can check out our model parameters or what's\ninside our model using, wait for it, dot parameters. Oh, don't you love it when things are nice and\nsimple? Well, let's check it out. Hey, well, first things we're going to do is let's create a random\nseed. Now, why are we creating a random seed? Well, because recall, we're creating these parameters\nwith random values. And if we were to create them with outer random seed, we would get different\nvalues every time. So for the sake of the educational sense, for the sake of this video,\nwe're going to create a manual seed here, torch dot manual seed. I'm going to use 42 or maybe 43,\nI could use 43 now 42 because I love 42. It's the answer to the universe. And we're going to create\nan instance of the model that we created. So this is a subclass of an end up module.\nSo let's do it. Model zero, because it's going to be the zeroth model, the first model that\nwe've ever created in this whole course, how amazing linear regression model, which is what\nour class is called. So we can just call it like that. That's all I'm doing, just calling this class.\nAnd so let's just see what happens there. And then if we go model zero, what does it give us? Oh,\nlinear regression. Okay, it doesn't give us much. But we want to find out what's going on in here.\nSo check out the parameters. So model zero dot parameters. What do we get from this? Oh, a generator.\nWell, let's turn this into a list that'll be better to look at. There we go. Oh, how exciting is that?\nSo parameter containing. Look at the values tensor requires grad equals true parameter containing\nwonderful. So these are our model parameters. So why are they the values that they are? Well,\nit's because we've used torch rand n. Let's see what happens if we go, let's just create torch dot\nrand n one, what happens? We get a value like that. And now if we run this again,\nwe get the same values. But if we run this again, so keep this in one two, three, four,\nfive, actually, that's, wow, that's pretty cool that we got a random value that was all in order,\nfour in a row. Can we do it twice in a row? Probably not. Oh, we get it the same one. Now,\nwhy is that? Oh, we get a different one. Did we just get the same one twice? Oh, my gosh,\nwe got the same value twice in a row. You saw that. You saw that. That's incredible. Now,\nthe reason why we get this is because this one is different every time because there's no random\nseed. Watch if we put the random seed here, torch dot manual seed, 42, 3, 3, 6, 7, what happens?\n3, 3, 6, 7, what happens? 3, 3, 6, 7. Okay. And what if we commented out the random seed\nhere, initialized our model, different values, two, three, five, two, three, four, five, it must\nlike that value. Oh, my goodness. Let me know if you get that value, right? So if we keep going,\nwe get different values every single time. Why is this? Why are we getting different values\nevery single time? You might be, Daniel, you sound like a broken record, but I'm trying to\nreally drive home the fact that we initialize our models with random parameters. So this is the\nessence of what our machine learning models and deep learning models are going to do. Start with\nrandom values, weights and bias. Maybe we've only got two parameters here, but the future models\nthat we build might have thousands. And so of course, we're not going to do them all by hand.\nWe'll see how we do that later on. But for now, we start with random values. And our ideal model\nwill look at the training data and adjust these random values. But just so that we can get\nreproducible results, I'll get rid of this cell. I've set the random seed here. So you should be\ngetting similar values to this. If you're not, because there's maybe some sort of pytorch update\nand how the random seeds calculated, you might get slightly different values. But for now,\nwe'll use torch.manualc.42. And I want you to just be aware of this can be a little bit confusing.\nIf you just do the list of parameters, for me, I understand it better if I list the name parameters.\nSo the way we do that is with model zero, and we call state dict on it. This is going to give us\nour dictionary of the parameters of our model. So as you can see here, we've got weights,\nand we've got bias, and they are random values. So where did weights and bias come from? Well,\nof course, they came from here, weights, bias. But of course, as well up here,\nwe've got known parameters. So now our whole goal is what? Our whole goal is to build code,\nor write code, that is going to allow our model to look at these blue dots here,\nand adjust this weight and bias value to be weights as close as possible to weight and bias.\nNow, how do we go from here and here to here and here? Well, we're going to see that in future\nvideos, but the closer we get these values to these two, the better we're going to be able to\npredict and model our data. Now, this principle, I cannot stress enough, is the fundamental\nentire foundation, the fundamental foundation. Well, good description, Daniel. The entire\nfoundation of deep learning, we start with some random values, and we use gradient descent and\nback propagation, plus whatever data that we're working with to move these random values as close\nas possible to the ideal values. And in most cases, you won't know what the ideal values are.\nBut in our simple case, we already know what the ideal values are.\nSo just keep that in mind going forward. The premise of deep learning is to start with random\nvalues and make them more representative closer to the ideal values. With that being said,\nlet's try and make some predictions with our model as it is. I mean, it's got random values.\nHow do you think the predictions will go? So I think in the next video, we'll make some predictions\non this test data and see what they look like. I'll see you there. Welcome back. In the last\nvideo, we checked out the internals of our first PyTorch model. And we found out that because we're\ncreating our model with torch dot or the parameters of our model, with torch dot rand, they begin as\nrandom variables. And we also discussed the entire premise of deep learning is to start with random\nnumbers and slowly progress those towards more ideal numbers, slightly less random numbers based\non the data. So let's see, before we start to improve these numbers, let's see what their predictive\npower is like right now. Now you might be able to guess how well these random numbers will be\nable to predict on our data. You're not sure what that predicting means? Let's have a look. So making\npredictions using torch dot inference mode, something we haven't seen. But as always, we're going to\ndiscuss it while we use it. So to check our models predictive power, let's see how well\nit predicts Y test based on X test. Because remember again, another premise of a machine\nlearning model is to take some features as input and make some predictions close to some sort of\nlabels. So when we pass data through our model, it's going to run it through the forward method.\nSo here's where it's a little bit confusing. We defined a forward method and it takes X as input.\nNow I've done a little X, but we're going to pass it in a large X as its input. But the reason why I've\ndone a little X is because oftentimes in pytorch code, you're going to find all over the internet\nis that X is quite common, commonly used in the forward method here, like this as the input data.\nSo I've just left it there because that's what you're going to find quite often.\nSo let's test it out. We haven't discussed what inference mode does yet, but we will make predictions\nwith model. So with torch dot inference mode, let's use it. And then we will discuss what's going\non. Y threads equals a model zero X test. So that's all we're doing. We're passing the X test data\nthrough our model. Now, when we pass this X test in here, let's remind ourselves of what X test is.\nX test 10 variables here. And we're trying to our ideal model will predict the exact values of Y test.\nSo this is what our model will do if it's a perfect model. It will take these X test values as input,\nand it will return these Y test values as output. That's an ideal model. So the predictions are the\nexact same as the test data set. How do you think our model will go considering it's starting with\nrandom values as its parameters? Well, let's find out, hey. So what can that Y threads?\nOh, what's happened here? Not implemented error. Ah, this is an error I get quite often in Google\nColab when I'm creating a high-torch model. Now, it usually happens. I'm glad we've stumbled upon\nthis. And I think I know the fix. But if not, we might see a little bit of troubleshooting in this\nvideo is that when we create this, if you see this not implemented error, right, it's saying that\nthe Ford method. Here we go. Ford implemented. There we go. It's a little bit of a rabbit hole\nthis not implemented area. I've come across it a fair few times and it took me a while to figure\nout that for some reason the spacing. So in Python, you know how you have space space and that defines\na function space space. There's another thing there and another line there. For some reason,\nif you look at this line in my notebook, and by the way, if you don't have these lines or if you\ndon't have these numbers, you can go into tools, settings, editor, and then you can define them here.\nSo show line numbers, show notation guides, all that sort of jazz there. You can customize what's\ngoing on. But I just have these two on because I've run into this error a fair few times. And so\nit's because that this Ford method is not in line with this bracket here. So we need to highlight\nthis and click shift tab, move it over. So now you see that it's in line here. And then if we run\nthis, won't change any output there. See, that's the hidden gotcha. Is that when we ran this before,\nit found no error. But then when we run it down here, it works. So just keep that in mind. I'm\nreally glad we stumbled upon that because indentation errors, not implemented errors,\none of the most common errors you'll find in PyTorch, or in, well, when you're writing PyTorch\ncode in Google CoLab, I'm not sure why, but it just happens. So these are our models predictions\nso far by running the test data through our models Ford method that we defined. And so if\nwe look at Y test, are these close? Oh my gosh, they are shocking. So why don't we visualize them?\nPlot predictions. And we're going to put in predictions equals Y threads.\nLet's have a look. Oh my goodness. All the way over here. Remember how we discussed before\nthat an ideal model will have, what, red dots on top of the green dots because our ideal model\nwill be perfectly predicting the test data. So right now, because our model is initialized with\nrandom parameters, it's basically making random predictions. So they're extremely far from where\nour ideal predictions are, is that we'll have some training data. And our model predictions,\nwhen we first create our model will be quite bad. But we want to write some code that will\nhopefully move these red dots closer to these green dots. I'm going to see how we can do that in\nlater videos. But we did one thing up here, which we haven't discussed, which is with torch dot\ninference mode. Now this is a context manager, which is what happens when we're making predictions.\nSo making predictions, another word for predictions is inference torch uses inference. So I'll try\nto use that a bit more, but I like to use predictions as well. We could also just go\nY preds equals model zero dot X test. And we're going to get quite a similar output.\nRight. But I've put on inference mode because I want to start making that a habit for later on,\nwhen we make predictions, put on inference mode. Now why do this? You might notice something different.\nWhat's the difference here between the outputs? Y preds equals model. There's no inference mode\nhere, no context manager. Do you notice that there's a grad function here? And we don't need to go\ninto discussing what exactly this is doing here. But do you notice that this one is lacking that\ngrad function? So do you remember how behind the scenes I said that pie torch does a few things\nwith requires grad equals true, it keeps track of the gradients of different parameters so that\nthey can be used in gradient descent and back propagation. Now what inference mode does is it\nturns off that gradient tracking. So it essentially removes all of the, because when we're doing\ninference, we're not doing training. So we don't need to keep track of the gradient. So we don't\nneed to do to keep track of how we should update our models. So inference mode disables all of the\nuseful things that are available during training. What's the benefit of this? Well, it means that\npie torch behind the scenes is keeping track of less data. So in turn, it will, with our small\ndata set, it probably won't be too dramatic. But with a larger data set, it means that your\npredictions will potentially be a lot faster because a whole bunch of numbers aren't being\nkept track of or a whole bunch of things that you don't need during prediction mode or inference\nmode. That's why it's called inference mode. I'm not being saved to memory. If you'd like to\nlearn more about this, you go pie torch inference mode Twitter. I just remember to search for Twitter\nbecause they did a big tweet storm about it. Here we go. So oh, this is another thing that we can\ncover. I'm going to copy this in here. But there's also a blog post about what's going on behind\nthe scenes. Long story short, it makes your code faster. Want to make your inference code and pie\ntorch run faster? Here's a quick thread on doing exactly that. And that's what we're doing. So\nI'm going to write down here. See more on inference mode here.\nAnd I just want to highlight something as well is that they referenced torch no grad with the\ntorch inference mode context manager. Inference mode is fairly new in pie torch. So you might\nsee a lot of code existing pie torch code with torch dot no grad. You can use this as well.\nWhy? Preds equals model zero. And this will do much of the same as what inference mode is doing.\nBut inference mode has a few things that are advantages over no grad, which are discussed in\nthis thread here. But if we do this, we get very similar output to what we got before.\nGrad function. But as you'll read in here and in the pie torch documentation, inference mode is\nthe favored way of doing inference for now. I just wanted to highlight this. So you can also do\nsomething similar with torch dot no grad. However, inference mode is preferred. Alrighty. So I'm\njust going to comment this out. So we just have one thing going on there. The main takeaway\nfrom this video is that when we're making predictions, we use the context manager torch\ndot inference mode. And right now, because our models variables or internal parameters are\nrandomly initialized, our models predictions are as good as random. So they're actually not too far\noff where our values are. At least the red dots aren't like scattered all over here. But in the\nupcoming videos, we're going to be writing some pie torch training code to move these values\ncloser to the green dots by looking at the training data here. So with that being said,\nI'll see you in the next video. Friends, welcome back. In the last video, we saw that our model\nperforms pretty poorly. Like, ideally, these red dots should be in line with these green dots.\nAnd we know that because why? Well, it's because our model is initialized with random parameters.\nAnd I just want to put a little note here. You don't necessarily have to initialize your model\nwith random parameters. You could initialize it with this could be zero. Yeah, these two values,\nweights can bias could be zero and you could go from there. Or you could also use the parameters\nfrom another model. But we're going to see that later on. That's something called transfer learning.\nThat's just a little spoiler for what's to come. And so we've also discussed that an ideal model\nwill replicate these known parameters. So in other words, start with random unknown parameters,\nthese two values here. And then we want to write some code for our model to move towards estimating\nthe ideal parameters here. Now, I just want to be explicit here and write down some intuition\nbefore we jump into the training code. But this is very exciting. We're about to get into\ntraining our very first machine learning model. So what's right here, the whole idea of training\nis for a model to move from some unknown parameters, these may be random to some known parameters.\nOr in other words, from a poor representation, representation of the data to a better representation\nof the data. And so in our case, would you say that our models representation of the green dots\nhere with this red dots, is that a good representation? Or is that a poor representation?\nI mean, I don't know about you, but I would say that to me, this is a fairly poor representation.\nAnd one way to measure the representation between your models outputs, in our case, the red dots,\nthe predictions, and the testing data, is to use a loss function. So I'm going to write\nthis down here. This is what we're moving towards. We're moving towards training, but we need a\nway to measure how poorly our models predictions are doing. So one way to measure how poor or how\nwrong your models predictions are, is to use a loss function. And so if we go pytorch loss\nfunctions, we're going to see that pytorch has a fair few loss functions built in. But the essence\nof all of them is quite similar. So just wait for this to load my internet's going a little bit\nslow today, but that's okay. We're not in a rush here. We're learning something fun.\nIf I search here for loss, loss functions, here we go. So yeah, this is torch in N. These are the\nbasic building blocks for graphs, whole bunch of good stuff in here, including loss functions.\nBeautiful. And this is another thing to note as well, another one of those scenarios where\nthere's more words for the same thing. You might also see a loss function referred to as a criterion.\nThere's another word called cost function. So I might just write this down so you're aware of it.\nYeah, cost function versus loss function. And maybe some formal definitions about what all of these\nare. Maybe they're used in different fields. But in the case of we're focused on machine learning,\nright? So I'm just going to go note, loss function may also be called cost function or criterion in\ndifferent areas. For our case, we're going to refer to it as a loss function. And let's\njust formally define a loss function here, because we're going to go through a fair few steps in\nthe upcoming videos. So this is a warning, nothing we can't handle. But I want to put some formal\ndefinitions on things. We're going to see them in practice. That's what I prefer to do,\nrather than just sit here defining stuff. This lecture has already had enough text on the page.\nSo hurry up and get into coding Daniel. A loss function is a function to measure how wrong your\nmodels predictions are to the ideal outputs. So lower is better. So ideally, think of a measurement,\nhow could we measure the difference between the red dots and the green dots? One of the\nsimplest ways to do so would be just measure the distance here, right? So if we go, let's just\nestimate this is 035 to 0.8. They're abouts. So what's the difference there? About 0.45.\nThen we could do the same again for all of these other dots, and then maybe take the average of that.\nNow, if you've worked with loss functions before, you might have realized that I've just\nreproduced mean absolute error. But we're going to get to that in a minute. So we need a loss\nfunction. I'm going to write down another little dot point here. This is just setting up intuition.\nThings we need to train. We need a loss function. This is PyTorch. And this is machine learning\nin general, actually. But we're focused on PyTorch. We need an optimizer. What does the optimizer do?\nTakes into account the loss of a model and adjusts the model's parameters. So the parameters recall\nour weight and bias values. Weight and biases. We can check those or bias. We can check those by\ngoing model dot parameter or parameters. But I also like, oh, that's going to give us a generator,\nisn't it? Why do we not define the model yet? What do we call our model? Oh, model zero. Excuse me.\nI forgot where. I'm going to build a lot of models in this course. So we're giving them numbers.\nModeled up parameters. Yeah, we've got a generator. So we'll turn that into a list.\nBut model zero, if we want to get them labeled, we want state dict here.\nThere we go. So our weight is this value. That's a random value we've set. And there's the bias.\nAnd now we've only got two parameters for our model. So it's quite simple. However, the principles\nthat we're learning here are going to be the same principles, taking a loss function,\ntrying to minimize it, so getting it to lower. So the ideal model will predict exactly what our\ntest data is. And an optimizer will take into account the loss and will adjust a model's parameter.\nAnd our case weights and bias to be, let's finish this definition takes into account the\nloss of a model and adjust the model's parameters, e.g. weight and bias, in our case, to improve the\nloss function. And specifically, for PyTorch, we need a training loop and a testing loop.\nNow, this is what we're going to work towards building throughout the next couple of videos.\nWe're going to focus on these two first, the loss function and optimizer. There's the formal\ndefinition of those. You're going to find many different definitions. That's how I'm going to\nfind them. Loss function measures how wrong your model's predictions are, lower is better,\noptimizer takes into account the loss of your model. So how wrong it is, and starts to move\nthese two values into a way that improves where these red dots end up. But these, again, these\nprinciples of a loss function and an optimizer can be for models with two parameters or models\nwith millions of parameters, can be for computer vision models, or could be for simple models like\nours that predict the dots on a straight line. So with that being said, let's jump into the next\nvideo. We'll start to look a little deeper into loss function, row problem, and an optimizer.\nI'll see you there. Welcome back. We're in the exciting streak of videos coming up here. I mean,\nthe whole course is fun. Trust me. But this is really exciting because training your first machine\nlearning model seems a little bit like magic, but it's even more fun when you're writing the code\nyourself what's going on behind the scenes. So we discussed that the whole concept of training\nis from going unknown parameters, random parameters, such as what we've got so far\nto parameters that better represent the data. And we spoke of the concept of a loss function.\nWe want to minimize the loss function. That is the whole idea of a training loop in PyTorch,\nor an optimization loop in PyTorch. And an optimizer is one of those ways that can\nnudge the parameters of our model. In our case, weights or bias towards values rather than just\nbeing random values like they are now towards values that lower the loss function. And if we\nlower the loss function, what does a loss function do? It measures how wrong our models\npredictions are compared to the ideal outputs. So if we lower that, well, hopefully we move\nthese red dots towards the green dots. And so as you might have guessed, PyTorch has some built\nin functionality for implementing loss functions and optimizers. And by the way, what we're covering\nso far is in the train model section of the PyTorch workflow fundamentals, I've got a little\nnice table here, which describes a loss function. What does it do? Where does it live in PyTorch?\nCommon values, we're going to see some of these hands on. If you'd like to read about it,\nof course, you have the book version of the course here. So loss functions in PyTorch,\nI'm just in docstorch.nn. Look at this. Look at all these loss functions. There's far too many\nfor us to go through all in one hit. So we're just going to focus on some of the most common ones.\nLook at that. We've got about what's our 15 loss functions, something like that? Well, truth be\ntold is that which one should use? You're not really going to know unless you start to work hands\non with different problems. And so in our case, we're going to be looking at L1 loss. And this is\nan again, once more another instance where different machine learning libraries have different names\nfor the same thing, this is mean absolute error, which we kind of discussed in the last video,\nwhich is if we took the distance from this red dot to this green dot and say at 0.4, they're about\n0.4, 0.4, and then took the mean, well, we've got the mean absolute error. But in PyTorch,\nthey call it L1 loss, which is a little bit confusing because then we go to MSE loss,\nwhich is mean squared error, which is L2. So naming conventions just takes a little bit of getting\nused to this is a warning for you. So let's have a look at the L1 loss function. Again,\nI'm just making you aware of where the other loss functions are. We'll do with some binary\ncross entropy loss later in the course. And maybe even is that categorical cross entropy?\nWe'll see that later on. But all the others will be problem specific. For now, a couple of loss\nfunctions like this, L1 loss, MSE loss, we use for regression problems. So that's predicting a number.\nCross entropy loss is a loss that you use with classification problems. But we'll see those hands\non later on. Let's have a look at L1 loss. So L1 loss creates a criterion. As I said, you might\nhear the word criterion used in PyTorch for a loss function. I typically call them loss functions.\nThe literature typically calls it loss functions. That measures the mean absolute error. There we\ngo. L1 loss is the mean absolute error between each element in the input X and target Y. Now,\nyour extracurricular measure might have guessed is to read through the documentation for the\ndifferent loss functions, especially L1 loss. But for the sake of this video, let's just implement\nit for ourselves. Oh, and if you want a little bit of a graphic, I've got one here. This is where\nwe're up to, by the way, picking a loss function optimizer for step two. This is a fun part, right?\nWe're getting into training a model. So we've got mean absolute error. Here's that graph we've\nseen before. Oh, look at this. Okay. So we've got the difference here. I've actually measured\nthis before in the past. So I kind of knew what it was. Mean absolute error is if we repeat for\nall samples in our set that we're working with. And if we take the absolute difference between\nthese two dots, well, then we take the mean, we've got mean absolute error. So MAE loss equals\ntorch mean we could write it out. That's the beauty of pine torch, right? We could write this out.\nOr we could use the torch and N version, which is recommended. So let's jump in. There's a colorful\nslide describing what we're about to do. So let's go set up a loss function. And then we're also\ngoing to put in here, set up an optimizer. So let's call it loss FN equals NN dot L1 loss.\nSimple as that. And then if we have a look at what's our loss function, what does this say?\nOh my goodness. My internet is going quite slow today.\nIt's raining outside. So there might be some delays somewhere. But that's right. Gives us a\nchance to sit here and be mindful about what we're doing. Look at that. Okay. Loss function.\nL1 loss. Beautiful. So we've got a loss function. Our objective for training a machine learning\nmodel will be two. Let's go back. Look at the colorful graphic will be to minimize these\ndistances here. And in turn, minimize the overall value of MAE. That is our goal.\nIf our red dots line up with our green dots, we will have a loss value of zero, the ideal point\nfor a model to be. And so let's go here. We now need an optimizer. As we discussed before,\nthe optimizer takes into account the loss of a model. So these two work in tandem.\nThat's why I've put them as similar steps if we go back a few slides.\nSo this is why I put these as 2.1. Often picking a loss function and optimizer and pytorch\ncome as part of the same package because they work together. The optimizer's objective is to\ngive the model values. So parameters like a weight and a bias that minimize the loss function.\nThey work in tandem. And so let's see what an optimizer optimizes. Where might that be?\nWhat if we search here? I typically don't use this search because I prefer just using Google\nsearch. But does this give us optimizer? Hey, there we go. So again, pytorch has torch.optim\nwhich is where the optimizers are. Torch.optim. Let me put this link in here.\nThis is another bit of your extracurricular. If you want to read more about different optimizers\nin pytorch, as you might have guessed, they have a few. Torch.optim is a package implementing\nvarious optimization algorithms. Most commonly used methods are already supported and the interface\nis general enough so that more sophisticated ones can also be easily integrated into the future.\nSo if we have a look at what algorithms exist here, again, we're going to throw a lot of names\nat you. But in the literature, a lot of them that have made it into here are already good working\nalgorithms. So it's a matter of picking whichever one's best for your problem. How do you find that\nout? Well, SGD, stochastic gradient descent, is possibly the most popular. However, there are\nsome iterations on SGD, such as Adam, which is another one that's really popular. So again,\nthis is one of those other machine learning is part art, part science is trial and error of\nfiguring out what works best for your problem for us. We're going to start with SGD because\nit's the most popular. And if you were paying attention to a previous video, you might have\nseen that I said, look up gradient descent, wherever we got this gradient descent. There we go.\nSo this is one of the main algorithms that improves our models. So gradient descent and back\npropagation. So if we have a look at this stochastic gradient descent, bit of a tongue twister,\nis random gradient descent. So that's what stochastic means. So basically, our model\nimproves by taking random numbers, let's go down here, here, and randomly adjusting them\nso that they minimize the loss. And once how optimizer, that's right here, once how optimizer\ntorch dot opt in, let's implement SGD, SGD stochastic gradient descent. We're going to write this here,\nstochastic gradient descent. It starts by randomly adjusting these values. And once it's found\nsome random values or random steps that have minimized the loss value, we're going to see\nthis in action later on, it's going to continue adjusting them in that direction. So say it says,\noh, weights, if I increase the weights, it reduces the loss. So it's going to keep increasing the\nweights until the weights no longer reduce the loss. Maybe it gets to a point at say 0.65.\nIf you increase the weights anymore, the loss is going to go up. So the optimizer is like,\nwell, I'm going to stop there. And then for the bias, the same thing happens. If it decreases the\nbias and finds that the loss increases, well, it's going to go, well, I'm going to try increasing\nthe bias instead. So again, one last summary of what's going on here, a loss function measures\nhow wrong our model is. And the optimizer adjust our model parameters, no matter whether there's\ntwo parameters or millions of them to reduce the loss. There are a couple of things that\nan optimizer needs to take in. It needs to take in as an argument, params. So this is if we go to\nSGD, I'm just going to link this as well. SGD, there's the formula of what SGD does. I look at this\nand I go, hmm, there's a lot going on here. And take me a while to understand that. So I like to\nsee it in code. So we need params. This is short for what parameters should I optimize as an optimizer.\nAnd then we also need an LR, which stands for, I'm going to write this in a comment, LR equals\nlearning rate, possibly the most, oh, I didn't even type rate, did I possibly the most important\nhyper parameter you can set? So let me just remind you, I'm throwing lots of words out here, but I'm\nkind of like trying to write notes about what we're doing. Again, we're going to see these in action\nin a second. So check out our models and parameters. So a parameter is a value that the model sets\nitself. So learning rate equals possibly the most important learning hyper parameter. I don't\nneed learning there, do I? Hyper parameter. And a hyper parameter is a value that us as a data scientist\nor a machine learning engineer set ourselves, you can set. So the learning rate is, in our case,\nlet's go 0.01. You're like, Daniel, where did I get this value from? Well, again, these type of\nvalues come with experience. I think it actually says it in here, LR, LR 0.1. Yeah, okay, so the\ndefault is 0.1. But then if we go back to Optim, I think I saw it somewhere. Did I see it somewhere?\n0.0? Yeah, there we go. Yeah, so a lot of the default settings are pretty good in torch optimizers.\nHowever, the learning rate, what does it actually do? We could go 0.01. These are all common values\nhere. Triple zero one. I'm not sure exactly why. Oh, model, it's model zero. The learning rate says\nto our optimizer, yes, it's going to optimize our parameters here. But the higher the learning\nrate, the more it adjusts each of these parameters in one hit. So let's say it's 0.01. And it's going\nto optimize this value here. So it's going to take that big of a step. If we changed it to here,\nit's going to take a big step on this three. And if we changed it to all the way to the end 0.01,\nit's only going to change this value. So the smaller the learning rate, the smaller the change\nin the parameter, the larger the learning rate, the larger the change in the parameter.\nSo we've set up a loss function. We've set up an optimizer. Let's now move on to the next step\nin our training workflow. And that's by building a training loop. Far out. This is exciting. I'll\nsee you in the next video. Welcome back. In the last video, we set up a loss function. And we set\nup an optimizer. And we discussed the roles of each. So loss function measures how wrong our model\nis. The optimizer talks to the loss function and goes, well, if I change these parameters a certain\nway, does that reduce the loss function at all? And if it does, yes, let's keep adjusting them in\nthat direction. If it doesn't, let's adjust them in the opposite direction. And I just want to show\nyou I added a little bit of text here just to concretely put down what we were discussing.\nInside the optimizer, you'll often have to set two parameters, params and lr, where params is\nthe model parameters you'd like to optimize for an example, in our case, params equals our model\nzero parameters, which were, of course, a weight and a bias. And the learning rate, which is lr\nin optimizer, lr stands for learning rate. And the learning rate is a hyper parameter. Remember,\na hyper parameter is a value that we the data scientist or machine learning engineer sets,\nwhereas a parameter is what the model sets itself defines how big or smaller optimizer changes\nthe model parameters. So a small learning rate, so the smaller this value results in small\nchanges, a large learning rate results in large changes. So another question might be,\nwell, very valid question. Hey, I put this here already, is which loss function and which optimizer\nshould I use? So this is another tough one, because it's problem specific. But with experience\nand machine learning, I'm showing you one example here, you'll get an idea of what works for your\nparticular problem for a regression problem, like ours, a loss function of l1 loss, which is mai\nand pytorch. And an optimizer like torch dot opt in slash s gd like sarcastic gradient descent\nwill suffice. But for a classification problem, we're going to see this later on.\nNot this one specifically, whether a photo is a cat of a dog, that's just an example of a binary\nclassification problem, you might want to use a binary classification loss. But with that being\nsaid, we now are moving on to, well, here's our whole goal is to reduce the MAE of our model.\nLet's get the workflow. We've done these two steps. Now we want to build a training loop. So\nlet's get back into here. There's going to be a fair few steps going on. We've already covered\na few, but hey, nothing we can't handle together. So building a training loop in pytorch.\nSo I thought about just talking about what's going on in the training loop, but we can talk\nabout the steps after we've coded them. How about we do that? So we want to build a training loop\nand a testing loop. How about we do that? So a couple of things we need in a training loop.\nSo there's going to be a fair few steps here if you've never written a training loop before,\nbut that is completely fine because you'll find that the first couple of times that you write this,\nyou'll be like, oh my gosh, there's too much going on here. But then when you have practice,\nyou'll go, okay, I see what's going on here. And then eventually you'll write them with your\neyes closed. I've got a fun song for you to help you out remembering things. It's called the\nunofficial pytorch optimization loop song. We'll see that later on, or actually, I'll probably leave\nthat as an extension, but you'll see that you can also functionize these things, which we will do\nlater in the course so that you can just write them once and then forget about them. But we're\ngoing to write it all from scratch to begin with so we know what's happening. So we want to,\nor actually step zero, is loop through the data. So we want to look at the data multiple times\nbecause our model is going to, at first, start with random predictions on the data, make some\npredictions. We're trying to improve those. We're trying to minimize the loss to make those\npredictions. We do a forward pass. So forward pass. Why is it called a forward pass? So this\ninvolves data moving through our model's forward functions. Now that I say functions because there\nmight be plural, there might be more than one. And the forward method recall, we wrote in our model\nup here. Ford. A forward pass is our data going through this function here. And if you want to\nlook at it visually, let's look up a neural network graphic. Images, a forward pass is just\ndata moving from the inputs to the output layer. So starting here input layer moving through the\nmodel. So that's a forward pass, also called forward propagation. Another time we'll have\nmore than one name is used for the same thing. So we'll go back down here, forward pass. And\nI'll just write here also called forward propagation, propagation. Wonderful. And then we need to\ncalculate the loss. So forward pass. Let me write this. To calculate or to make predictions,\nmake predictions on data. So calculate the loss, compare forward pass predictions. Oh, there's\nan undergoing in the background here of my place. We might be in for a storm. Perfect time to write\ncode, compare forward pass predictions to ground truth labels. We're going to see all this in code\nin a second, calculate the loss. And then we're going to go optimise a zero grad. We haven't\nspoken about what this is, but that's okay. We're going to see that in a second. I'm not going to\nput too much there. Loss backward. We haven't discussed this one either. There's probably three\nsteps that we haven't really discussed. We've discussed the idea behind them, but not too much\nin depth. Optimise our step. So this one is loss backwards is move backwards. If the forward pass\nis forwards, like through the network, the forward pass is data goes into out. The backward pass\ndata goes, our calculations happen backwards. So we'll see what that is in a second. Where were\nwe over here? We've got too much going on. I'm getting rid of these moves backwards through\nthe network to calculate the gradients. Oh, oh, the gradients of each of the parameters\nof our model with respect to the loss. Oh my gosh, that is an absolute mouthful,\nbut that'll do for now. Optimise a step. This is going to use the optimiser to adjust our\nmodel's parameters to try and improve the loss. So remember how I said in a previous video\nthat I'd love you to watch the two videos I linked above. One on gradient descent and one\non back propagation. If you did, you might have seen like there's a fair bit of math going on in\nthere. Well, that's essentially how our model goes from random parameters to better parameters,\nusing math. Many people, one of the main things I get asked from machine learning is how do I\nlearn machine learning if I didn't do math? Well, the beautiful thing about PyTorch is that it\nimplements a lot of the math of back propagation. So this is back propagation. I'm going to write\nthis down here. This is an algorithm called back, back propagation, hence the loss backward. We're\ngoing to see this in code in a second, don't you worry? And this is gradient descent. So these\ntwo algorithms drive the majority of our learning. So back propagation, calculate the gradients of\nthe parameters of our model with respect to the loss function and optimise our step,\nwe'll trigger code to run gradient descent, which is to minimise the gradients because what is a\ngradient? Let's look this up. What is a gradient? I know we haven't written a code yet, but we're\ngoing to do that. Images. Gradient, there we go. Changing y, changing x. Gradient is from high\nschool math. Gradient is a slope. So if you were on a hill, let's find a picture of a hill.\nPicture of a hill. There we go. This is a great big hill. So if you were on the top of this hill,\nand you wanted to get to the bottom, how would you get to the bottom? Well, of course, you just\nwalked down the hill. But if you're a machine learning model, what are you trying to do? Let's\nimagine your loss is the height of this hill. You start off with your losses really high, and you\nwant to take your loss down to zero, which is the bottom, right? Well, if you measure the gradient\nof the hill, the bottom of the hill is in the opposite direction to where the gradient is steep.\nDoes that make sense? So the gradient here is an incline. We want our model to move towards the\ngradient being nothing, which is down here. And you could argue, yeah, the gradient's probably\nnothing up the top here, but let's just for argument's sake say that we want to get to the\nbottom of the hill. So we're measuring the gradient, and one of the ways an optimisation algorithm\nworks is it moves our model parameters so that the gradient equals zero, and then if the gradient\nof the loss equals zero, while the loss equals zero two. So now let's write some code. So we're\ngoing to set up a parameter called or a variable called epochs. And we're going to start with one,\neven though this could be any value, let me define these as we go. So we're going to write code to\ndo all of this. So epochs, an epoch is one loop through the data dot dot dot. So epochs, we're\ngoing to start with one. So one time through all of the data, we don't have much data. And so\nfor epoch, let's go this, this is step zero, zero, loop through the data. By the way, when I say\nloop through the data, I want you to do all of these steps within the loop. And do dot dot dot\nloop through the data. So for epoch in range epochs, even though it's only going to be one,\nwe can adjust this later. And because epochs, we've set this ourselves, it is a,\nthis is a hyper parameter, because we've set it ourselves. And I know you could argue that,\nhey, our machine learning parameters of model zero, or our model parameters, model zero aren't\nactually parameters, because we've set them. But in the models that you build in the future,\nthey will likely be set automatically rather than you setting them explicitly like we've done when\nwe created model zero. And oh my gosh, this is taking quite a while to run. That's all right.\nWe don't need it to run fast. We just, we need to write some more code, then you'll come on.\nThere's a step here I haven't discussed either. Set the model to training mode. So pytorch models\nhave a couple of different modes. The default is training mode. So we can set it to training\nmode by going like this. Train. So what does train mode do in a pytorch model? My goodness.\nIs there a reason my engineer is going this slide? That's all right. I'm just going to\ndiscuss this with talking again list. Train mode. Train mode in pytorch sets. Oh, there we go.\nRequires grad equals true. Now I wonder if we do with torch dot no grad member no grad is similar\nto inference mode. Will this adjust? See, I just wanted to take note of requires grad equals\ntrue. Actually, what I might do is we do this in a different cell. Watch this. This is just going\nto be rather than me just spit words at you. I reckon we might be able to get it work in doing\nthis. Oh, that didn't list the model parameters. Why did that not come out? Model zero dot eval.\nSo there's two modes of our mode and train mode model dot eval parameters. Hey, we're experimenting\ntogether on the fly here. And actually, this is what I want you to do is I want you to experiment\nwith different things. It's not going to say requires grad equals false. Hmm. With torch dot no\ngrad. Model zero dot parameters. I don't know if this will work, but it definitely works behind\nthe scenes. And what I mean by works behind the scenes are not here. It works behind the scenes\nwhen calculations have been made, but not if we're trying to explicitly print things out.\nWell, that's an experiment that I thought was going to work and it didn't work. So train\nmode in pytorch sets all parameters that require gradients to require gradients.\nSo do you remember with the picture of the hill? I spoke about how we're trying to minimize the\ngradient. So the gradient is the steepness of the hill. If the height of the hill is a loss function\nand we want to take that down to zero, we want to take the gradient down to zero. So same thing\nwith the gradients of our model parameters, which are here with respect to the loss function,\nwe want to try and minimize that gradient. So that's gradient descent is take that gradient down to\nzero. So model dot train. And then there's also model zero dot a vowel. So turns off gradient\ntracking. So we're going to see that later on. But for now, I feel like this video is getting far\ntoo long. Let's finish the training loop in the next video. I'll see you there.\nFriends, welcome back. In the last video, I promised a lot of code, but we didn't get there. We\ndiscussed some important steps. I forgot how much behind the scenes there is to apply towards training\nloop. And I think it's important to spend the time that we did discussing what's going on,\nbecause there's a fair few steps. But once you know what's going on, I mean, later on, we don't\nhave to write all the code that we're going to write in this video, you can functionize it. We're\ngoing to see that later on in the course, and it's going to run behind the scenes for us. But we're\nspending a fair bit of time here, because this is literally the crux of how our model learns. So\nlet's get into it. So now we're going to implement the forward pass, which involves our model's\nforward function, which we defined up here. When we built our model, the forward pass runs through\nthis code here. So let's just write that. So in our case, because we're training, I'm just\ngoing to write here. This is training. We're going to see dot of our later on. We'll talk\nabout that when it comes. Let's do the forward pass. So the forward pass, we want to pass data\nthrough our model's forward method. We can do this quite simply by going y pred. So y predictions,\nbecause remember, we're trying to use our ideal model is using x test to predict y test\non our test data set. We make predictions on our test data set. We learn on our training data set.\nSo we're passing, which is going to get rid of that because we don't need that. So we're\npassing our model x train and model zero is going to be our current model. There we go. So we learn\npatterns on the training data to evaluate our model on the test data. Number two, where we are.\nSo we have to calculate the loss. Now, in a previous video, we set up a loss function.\nSo this is going to help us calculate the what what kind of loss are we using? We want to calculate\nthe MAE. So the difference or the distance between our red dot and a green dot. And the formula would\nbe the same if we had 10,000 red dots and 10,000 green dots, we're calculating how far they are\napart. And then we're taking the mean of that value. So let's go back here. So calculate the loss.\nAnd in our case, we're going to set loss equal to our loss function, which is L one loss in\nPyTorch, but it is MAE. Y-pred and Y-train. So we're calculating the difference between our models\npredictions on the training data set and the ideal training values. And if you want to go into\ntorch dot NN loss functions, that's going to show you the order because sometimes this confuses me\nto what order the values go in here, but it goes prediction first, then labels and I may be wrong\nthere because I get confused here. My dyslexia kicks in, but I'm pretty sure it's predictions first,\nthen actual labels. Do we have an example of where it's used? Yeah, import first, target next.\nSo there we go. And truth be told, because it's mean absolute error, it shouldn't actually matter\ntoo much. But in the case of staying true to the documentation, let's do inputs first and then\ntargets next for the rest of the course. Then we're going to go optimizer zero grad. Hmm,\nhaven't discussed this one, but that's okay. I'm going to write the code and then I'm going to\ndiscuss what it does. So what does this do? Actually, before we discuss this, I'm going to write\nthese two steps because they kind of all work together. And it's a lot easier to discuss what\noptimizer zero grad does in the context of having everything else perform back propagation\non the loss with respect to the parameters of the model. Back propagation is going to take\nthe loss value. So lost backward, I always say backwards, but it's just backward. That's the code\nthere. And then number five is step the optimizer. So perform gradient descent. So optimizer dot\nstep. Oh, look at us. We just wrote the five major steps of a training loop. Now let's discuss\nhow all of these work together. So it's kind of strange, like the ordering of these, you might\nthink, Oh, what should I do the order? Typically the forward pass and the loss come straight up.\nThen there's a little bit of ambiguity around what order these have to come in. But the optimizer\nstep should come after the back propagation. So I just like to keep this order how it is because\nthis works. Let's just keep it that way. But what happens here? Well, it also is a little bit\nconfusing in the first iteration of the loop because we've got zero grad. But what happens here is\nthat the optimizer makes some calculations in how it should adjust model parameters with regards to\nthe back propagation of the loss. And so by default, these will by default, how the optimizer\nchanges will accumulate through the loop. So we have to zero them above in step three\nfor the next iteration of the loop. So a big long comment there. But what this is saying is,\nlet's say we go through the loop and the optimizer chooses a value of one, change it by one. And\nthen it goes through a loop again, if we didn't zero it, if we didn't take it to zero, because\nthat's what it is doing, it's going one to zero, it would go, okay, next one, two, three, four,\nfive, six, seven, eight, all through the loop, right? Because we're looping here. If this was\n10, it would accumulate the value that it's supposed to change 10 times. But we want it to start\nagain, start fresh each iteration of the loop. And now the reason why it accumulates, that's\npretty deep in the pytorch documentation. From my understanding, there's something to do with\nlike efficiency of computing. If you find out what the exact reason is, I'd love to know.\nSo we have to zero it, then we perform back propagation. If you recall, back propagation is\ndiscussed in here. And then with optimizer step, we form gradient descent. So the beauty of pytorch,\nthis is the beauty of pytorch, is that it will perform back propagation, we're going to have a\nlook at this in second, and gradient descent for us. So to prevent this video from getting too long,\nI know we've just written code, but I would like you to practice writing a training loop\nyourself, just write this code, and then run it and see what happens. Actually, you can comment\nthis out, we're going to write the testing loop in a second. So your extra curriculum for this\nvideo is to, one, rewrite this training loop, is to, two, sing the pytorch optimization loop\nsong, let's go into here. If you want to remember the steps, well, I've got a song for you. This is\nthe training loop song, we haven't discussed the test step, but maybe you could try this yourself.\nSo this is an old version of the song, actually, I've got a new one for you. But let's sing this\ntogether. It's training time. So we do the forward pass, calculate the loss, optimise a zero grad,\nloss backwards, optimise a step, step, step. Now you only have to call optimise a step once,\nthis is just for jingle purposes. But for test time, let's test with torch no grad, do the forward\npass, calculate the loss, watch it go down, down, down. That's from my Twitter, but this is a way\nthat I help myself remember the steps that are going on in the code here. And if you want the\nvideo version of it, well, you're just going to have to search unofficial pytorch optimisation loop\nsong. Oh, look at that, who's that guy? Well, he looks pretty cool. So I'll let you check that\nout in your own time. But for now, go back through the training loop steps. I've got a colorful\ngraphic coming up in the next video, we're going to write the testing steps. And then we're going\nto go back one more time and talk about what's happening in each of them. And again, if you'd\nlike some even more extra curriculum, don't forget the videos I've shown you on back propagation\nand gradient descent. But for now, let's leave this video here. I'll see you in the next one.\nFriends, welcome back. In the last few videos, we've been discussing the steps in a training\nloop in pytorch. And there's a fair bit going on. So in this video, we're going to step back\nthrough what we've done just to recap. And then we're going to get into testing. And it's nice\nand early where I am right now. The sun's about to come up. It's a very, very beautiful morning\nto be writing code. So let's jump in. We've got a little song here for what we're doing in the\ntraining steps. For an epoch in a range, comodel.train, do the forward pass, calculate the loss of the\nmeasure zero grad, last backward of the measure step step step. That's the little jingle I use to\nremember the steps in here, because the first time you write it, there's a fair bit going on.\nBut subsequent steps and subsequent times that you do write it, you'll start to memorize this.\nAnd even better later on, we're going to put it into a function so that we can just call it\nover and over and over and over again. With that being said, let's jump in to a colorful slide,\nbecause that's a lot of code on the page. Let's add some color to it, understand what's happening.\nThat way you can refer to this and go, Hmm, I see what's going on now. So for the loop, this is why\nit's called a training loop. We step through a number of epochs. One epoch is a single forward\npass through the data. So pass the data through the model for a number of epochs. Epox is a\nhyper parameter, which means you could set it to 100, you could set it to 1000, you could set it\nto one as we're going to see later on in this video. We skip this step with the colors, but\nwe put the model in we call model.train. This is the default mode that the model is in.\nEssentially, it sets up a whole bunch of settings behind the scenes in our model parameters so that\nthey can track the gradients and do a whole bunch of learning behind the scenes with these\nfunctions down here. PyTorch does a lot of this for us. So the next step is the forward pass.\nWe perform a forward pass on the training data in the training loop. This is an important note.\nIn the training loop is where the model learns patterns on the training data. Whereas in the\ntesting loop, we haven't got to that yet is where we evaluate the patterns that our model has learned\nor the parameters that our model has learned on unseen data. So we pass the data through the model,\nthis will perform the forward method located within the model object. So because we created\na model object, you can actually call your models whatever you want, but it's good practice to\nyou'll often see it just called model. And if you remember, we'll go back to the code.\nWe created a forward method in our model up here, which is this, because our linear regression model,\nclass, subclasses, nn.module, we need to create our own custom forward method. So that's why it's\ncalled a forward pass is because not only does it, well, the technical term is forward propagation.\nSo if we have a look at a neural network picture, forward propagation just means going through\nthe network from the input to the output, there's a thing called back propagation, which we're going\nto discuss in a second, which happens when we call loss.backward, which is going backward through\nthe model. But let's return to our colorful slide. We've done the forward pass, call a forward method,\nwhich performs some calculation on the data we pass it. Next is we calculate the loss value,\nhow wrong the model's predictions are. And this will depend on what loss function you use,\nwhat kind of predictions your model is outputting, and what kind of true values you have.\nBut that's what we're doing here. We're comparing our model's predictions on the training data\nto what they should ideally be. And these will be the training labels. The next step, we zero\nthe optimizer gradients. So why do we do this? Well, it's a little confusing for the first epoch in\nthe loop. But as we get down to optimizer dot step here, the gradients that the optimizer\ncalculates accumulate over time so that for each epoch for each loop step, we want them to go back\nto zero. And now the exact reason behind why the optimizer accumulates gradients is buried somewhere\nwithin the pie torch documentation. I'm not sure of the exact reason from memory. It's because of\ncompute optimization. It just adds them up in case you wanted to know what they were. But if\nyou find out exactly, I'd love to know. Next step is to perform back propagation on the loss function.\nThat's what we're calling loss. backward. Now back propagation is we compute the gradient of\nevery parameter with requires grad equals true. And if you recall, we go back to our code.\nWe've set requires grad equals true for our parameters. Now the reason we've set requires\ngrad equals true is not only so back propagation can be performed on it. But let me show you what\nthe gradients look like. So let's go loss function curve. That's a good idea. So we're looking for\nso we're looking for some sort of convex curve here. There we go. L two loss. We're using L one loss\nat the moment. Is there a better one here? All we need is just a nice looking curve. Here we go.\nSo this is why we keep track of the gradients behind the scenes. Pie torch is going to create\nsome sort of curve for all of our parameters that looks like this. Now this is just a 2d plot.\nSo the reason why we're just using an example from Google images is one, because you're going to\nspend a lot of your time Googling different things. And two, in practice, when you have your own\ncustom neural networks, right now we only have two parameters. So it's quite easy to visualize a\nloss function curve like this. But when you have say 10 million parameters, you basically can't\nvisualize what's going on. And so pie torch again will take care of these things behind the scenes.\nBut what it's doing is when we say requires grad pie torch is going to track the gradients\nof each of our parameters. And so what we're trying to do here with back propagation and\nsubsequently gradient descent is calculate where the lowest point is. Because this is a loss function,\nthis is MSC loss, we could trade this out to be MAE loss in our case or L1 loss for our specific\nproblem. But this is some sort of parameter. And we calculate the gradients because what is the\ngradient? Let's have a look. What is a gradient? A gradient is an inclined part of a road or railway.\nNow we want it in machine learning. What's it going to give us in machine learning, a gradient\nis a derivative of a function that has more than one input variable. Okay, let's dive in a little\ndeeper. See, here's some beautiful loss landscapes. We're trying to get to the bottom of here. This\nis what gradient descent is all about. So oh, there we go. So this is a cost function, which is also a\nloss function. We start with a random initial variable. What have we done? We started with a\nrandom initial variable. Right? Okay. And then we take a learning step. Beautiful. This is W. So\nthis could be our weight parameter. Okay, we're connecting the dots here. This is exciting.\nWe've got a lot of tabs here, but that's all right. We'll bring this all together in a second.\nAnd what we're trying to do is come to the minimum. Now, why do we need to calculate the gradients?\nWell, the gradient is what? Oh, value of weight. Here we go. This is even better.\nI love Google images. So this is our loss. And this is a value of a weight. So we calculate the\ngradients. Why? Because the gradient is the slope of a line or the steepness. And so if we\ncalculate the gradient here, and we find that it's really steep right up the top of this,\nthis incline, we might head in the opposite direction to that gradient. That's what gradient\ndescent is. And so if we go down here, now, what are these step points? There's a little thing that\nI wrote down in the last video at the end of the last video I haven't told you about yet,\nbut I was waiting for a moment like this. And if you recall, I said kind of all of these three steps\noptimizes zero grad loss backward, optimizes step are all together. So we calculate the\ngradients because we want to head in the opposite direction of that gradient to get to a gradient\nvalue of zero. And if we get to a gradient value of zero with a loss function, well, then the loss\nis also zero. So that's why we keep track of a gradient with requires grad equals true.\nAnd again, PyTorch does a lot of this behind the scenes. And if you want to dig more into\nwhat's going on here, I'm going to show you some extra resources for back propagation,\nwhich is calculating this gradient curve here, and gradient descent, which is finding the bottom\nof it towards the end of this video. And again, if we started over this side, we would just go\nin the opposite direction of this. So maybe this is a positive gradient here, and we just go in the\nopposite direction here. We want to get to the bottom. That is the main point of gradient descent.\nAnd so if we come back, I said, just keep this step size in mind here. If we come back to where\nwe created our loss function and optimizer, I put a little tidbit here for the optimizer.\nBecause we've written a lot of code, and we haven't really discussed what's going on, but\nI like to do things on the fly as we need them. So inside our optimizer, we'll have main two\nparameters, which is params. So the model parameters you'd like to optimize,\nparams equals model zero dot parameters in our case. And then PyTorch is going to create\nsomething similar to this curve, not visually, but just mathematically behind the scenes for\nevery parameter. Now, this is a value of weight. So this would just be potentially the weight\nparameter of our network. But again, if you have 10 million parameters, there's no way you could\njust create all of these curves yourself. That's the beauty of PyTorch. It's doing this behind the\nscenes through a mechanism called torch autograd, which is auto gradient calculation. And there's\nbeautiful documentation on this. If you'd like to read more on how it works, please go through\nthat. But essentially behind the scenes, it's doing a lot of this for us for each parameter.\nThat's the optimizer. Then within the optimizer, once we've told it what parameters to optimize,\nwe have the learning rate. So the learning rate is another hyper parameter that defines how big or\nsmall the optimizer changes the parameters with each step. So a small learning rate results in\nsmall changes, whereas a large learning rate is in large changes. And so if we look at this\ncurve here, we might at the beginning start with large steps, so we can get closer and closer to\nthe bottom. But then as we get closer and closer to the bottom, to prevent stepping over to this\nside of the curve, we might do smaller and smaller steps. And the optimizer in PyTorch,\nthere are optimizers that do that for us. But there is also another concept called learning\nrate scheduling, which is, again, something if you would like to look up and do more. But\nlearning rate scheduling essentially says, hey, maybe start with some big steps. And then as we\nget closer and closer to the bottom, reduce how big the steps are that we take. Because if you've\never seen a coin, coin at the back of couch. This is my favorite analogy for this. If you've ever\ntried to reach a coin at the back of a couch, like this excited young chap, if you're reaching\ntowards the back of a couch, you take quite big steps as you say your arm was over here,\nyou would take quite big steps until you get to about here. And in the closer you get to the coin,\nthe smaller and smaller your steps are. Otherwise, what's going to happen? The coin is going to be\nlost. Or if you took two small steps, you'd never get to the coin. It would take forever to get there.\nSo that's the concept of learning rate. If you take two big steps, you're going to just end up\nover here. If you take two small steps, it's going to take you forever to get to the bottom here.\nAnd this bottom point is called convergence. That's another term you're going to come across. I\nknow I'm throwing a lot of different terms at you, but that's the whole concept of the learning\nrate. How big is your step down here? In gradient descent. Gradient descent is this. Back propagation\nis calculating these derivative curves or the gradient curves for each of the parameters in our\nmodel. So let's get out of here. We'll go back to our training steps. Where were we? I think we're\nup to back propagation. Have we done backward? Yes. So the back propagation is where we do the\nbackward steps. So the forward pass, forward propagation, go from input to output. Back propagation,\nwe take the gradients of the loss function with respect to each parameter in our model\nby going backwards. That's what happens when we call loss.backward. PyTorch does that for us\nbehind the scenes. And then finally, step number five is step the optimizer. We've kind of discussed\nthat. As I said, if we take a step, let's get our loss curve back up. Loss function curve.\nDoesn't really matter what curve we use. The optimizer step is taking a step this way to try\nand optimize the parameters so that we can get down to the bottom here. And I also just noted\nhere that you can turn all of this into a function so we don't necessarily have to remember to\nwrite these every single time. The ordering of this, you'll want to do the forward pass first.\nAnd then calculate the loss because you can't calculate the loss unless you do the forward pass.\nI like this ordering here of these three as well. But you also want to do the optimizer step\nafter the loss backward. So this is my favorite ordering. It works. If you like this ordering,\nyou can take that as well. With that being said, I think this video has gotten long enough.\nIn the next video, I'd like to step through this training loop one epoch at a time so that we can\nsee, I know I've just thrown a lot of words at you that this optimizer is going to try and\noptimize our parameters each step. But let's see that in action how our parameters of our model\nactually change every time we go through each one of these steps. So I'll see you in the next video.\nLet's step through our model. Welcome back. And we've spent a fair bit of time on the training loop\nand the testing loop. Well, we haven't even got to that yet, but there's a reason behind this,\nbecause this is possibly one of the most important things aside from getting your data ready,\nwhich we're going to see later on in PyTorch deep learning is writing the training loop,\nbecause this is literally like how your model learns patterns and data. So that's why we're\nspending a fair bit of time on here. And we'll get to the testing loop, because that's how you\nevaluate the patterns that your model has learned from data, which is just as important as learning\nthe patterns themselves. And following on from the last couple of videos, I've just linked some\nYouTube videos that I would recommend for extra curriculum for back propagation,\nwhich is what happens when we call loss stop backward down here. And for the optimizer step,\ngradient descent is what's happening there. So I've linked some extra resources for what's going\non behind the scenes there from a mathematical point of view. Remember, this course focuses on\nwriting PyTorch code. But if you'd like to dive into what math PyTorch is triggering behind the\nscenes, I'd highly recommend these two videos. And I've also added a note here as to which\nloss function and optimizer should I use, which is a very valid question. And again,\nit's another one of those things that's going to be problem specific. But with experience over time,\nyou work with machine learning problems, you write a lot of code, you get an idea of what works\nand what doesn't with your particular problem set. For example, like a regression problem,\nlike ours, regression is again predicting a number. We use MAE loss, which PyTorch causes\nL1 loss. You could also use MSE loss and an optimizer like torch opt-in stochastic gradient\ndescent will suffice. But for classification, you might want to look into a binary classification,\na binary cross entropy loss, but we'll look at a classification problem later on in the course.\nFor now, I'd like to demonstrate what's going on in the steps here. So let's go model zero.\nLet's look up the state dict and see what the parameters are for now.\nNow they aren't the original ones I don't think. Let's re-instantiate our model so we get\nre new parameters. Yeah, we recreated it here. I might just get rid of that. So we'll rerun our\nmodel code, rerun model state dict. And we will create an instance of our model and just make\nsure your parameters should be something similar to this. If it's not exactly like that, it doesn't\nmatter. But yeah, I'm just going to showcase you'll see on my screen what's going on anyway.\nState dict 3367 for the weight and 012888 for the bias. And again, I can't stress enough. We've\nonly got two parameters for our model and we've set them ourselves future models that you build\nand later ones in the course will have much, much more. And we won't actually explicitly set any\nof them ourselves. We'll check out some predictions. They're going to be terrible because we're using\nrandom parameters to begin with. But we'll set up a new loss function and an optimizer. Optimizer\nis going to optimize our model zero parameters, the weight and bias. The learning rate is 0.01,\nwhich is relatively large step. That would be a bit smaller. Remember, the larger the learning rate,\nthe bigger the step, the more the optimizer will try to change these parameters every step.\nBut let's stop talking about it. Let's see it in action. I've set a manual seed here too, by the way,\nbecause the optimizer steps are going to be quite random as well, depending on how the models\npredictions go. But this is just to try and make it as reproduces possible. So keep this in mind,\nif you get different values to what we're going to output here from my screen to your screen,\ndon't worry too much. What's more important is the direction they're going. So ideally,\nwe're moving these values here. This is from we did one epoch before. We're moving these values\ncloser to the true values. And in practice, you won't necessarily know what the true values are.\nBut that's where evaluation of your model comes in. We're going to cover that when we write a\ntesting loop. So let's run one epoch. Now I'm going to keep that down there. Watch what happens.\nWe've done one epoch, just a single epoch. We've done the forward pass. We've calculated the loss.\nWe've done optimizer zero grad. We've performed back propagation. And we've stepped the optimizer.\nWhat is stepping the optimizer do? It updates our model parameters to try and get them further\ncloser towards the weight and bias. If it does that, the loss will be closer to zero. That's what\nit's trying to do. How about we print out the loss at the same time. Print loss and the loss.\nLet's take another step. So the loss is 0301. Now we check the weights and the bias. We've changed\nagain three, three, four, four, five, one, four, eight, eight. We go again. The loss is going down.\nCheck it. Hey, look at that. The values are getting closer to where they should be if over so slightly.\nLoss went down again. Oh my goodness, this is so amazing. Look, we're training our,\nlet's print this out in the same cell. Print our model. State dict. We're training our first\nmachine learning model here, people. This is very exciting, even if it's only step by step and it's\nonly a small model. This is very important. Loss is going down again. Values are getting closer to\nwhere they should be. Again, we won't really know where they should be in real problems, but for\nnow we do. So let's just get excited. The real way to sort of measure your model's progress and\npractice is a lower loss value. Remember, lower is better. A loss value measures how wrong your\nmodel is. We're going down. We're going in the right direction. So that's what I meant by,\nas long as your values are going in the similar direction. So down, we're writing similar code\nhere, but if your values are slightly different in terms of the exact numbers, don't worry too\nmuch because that's inherent to the randomness of machine learning, because the steps that the\noptimizer are taking are inherently random, but they're sort of pushed in a direction.\nSo we're doing gradient descent here. This is beautiful. How low can we get the loss? How about\nwe try to get to 0.1? Look at that. We're getting close to 0.1. And then, I mean, we don't have to\ndo this hand by hand. The bias is getting close to where it exactly should be. We're below 0.1.\nBeautiful. So that was only about, say, 10 passes through the data, but now you're seeing it in\npractice. You're seeing it happen. You're seeing gradient descent. Let's go gradient descent work\nin action. We've got images. This is what's happening. We've got our cost function. J is\nanother term for cost function, which is also our loss function. We start with an initial weight.\nWhat have we done? We started with an initial weight, this value here. And what are we doing?\nWe've measured the gradient pytorch has done that behind the scenes for us. Thank you pytorch.\nAnd we're taking steps towards the minimum. That's what we're trying to do. If we minimize the\ngradient of our weight, we minimize the cost function, which is also a loss function. We could\nkeep going here for hours and get as long as we want. But my challenge for you, or actually,\nhow about we make some predictions with our model we've got right now? Let's make some predictions.\nSo with torch dot inference mode, we'll make some predictions together. And then I'm going\nto set you a challenge. How about you run this code here for 100 epochs after this video,\nand then you make some predictions and see how that goes. So why preds? Remember how\npoor our predictions are? Why preds new equals, we just do the forward pass here. Model zero\non the test data. Let's just remind ourselves quickly of how poor our previous predictions were.\nPlot predictions, predictions equals y. Do we still have this saved? Why preds?\nHopefully, this is still saved. There we go. Shocking predictions, but we've just done 10 or so\nepochs. So 10 or so training steps have our predictions. Do they look any better? Let's run\nthis. We'll copy this code. You know my rule. I don't really like to copy code, but in this case,\nI just want to exemplify a point. I like to write all the code myself. What do we got? Why preds\nnew? Look at that. We are moving our predictions close at the red dots closer to the green dots.\nThis is what's happening. We're reducing the loss. In other words, we're reducing the difference\nbetween our models predictions and our ideal outcomes through the power of back propagation\nand gradient descent. So this is super exciting. We're training our first machine learning model.\nMy challenge to you is to run this code here. Change epochs to 100. See how low you can get this\nloss value and run some predictions, plot them. And I think it's time to start testing. So give\nthat a go yourself, and then we'll write some testing code in the next video. I'll see you there.\nWelcome back. In the last video, we did something super excited. We saw our loss go down. So the\nloss is remember how different our models predictions are to what we'd ideally like them. And we saw\nour model update its parameters through the power of back propagation and gradient descent, all\ntaken care of behind the scenes for us by PyTorch. So thank you, PyTorch. And again, if you'd like\nsome extra resources on what's actually happening from a math perspective for back propagation and\ngradient descent, I would refer to you to these. Otherwise, this is also how I learn about things.\nGradient descent. There we go. How does gradient descent work? And then we've got back propagation.\nAnd just to reiterate, I am doing this and just Googling these things because that's what you're\ngoing to do in practice. You're going to come across a lot of different things that aren't\ncovered in this course. And this is seriously what I do day to day as a machine learning engineer\nif I don't know what's going on. Just go to Google, read, watch a video, write some code,\nand then I build my own intuition for it. But with that being said, I also issued you the challenge\nof trying to run this training code for 100 epochs. Did you give that a go? I hope you did. And\nhow low did your loss value? Did the weights and bias get anywhere close to where they should have\nbeen? How do the predictions look? Now, I'm going to save that for later on, running this code for\n100 epochs. For now, let's write some testing code. And just a note, you don't necessarily have to\nwrite the training and testing loop together. You can functionize them, which we will be doing later\non. But for the sake of this intuition, building and code practicing and first time where we're\nwriting this code together, I'm going to write them together. So testing code, we call model.ofour,\nwhat does this do? So this turns off different settings in the model not needed for evaluation\nslash testing. This can be a little confusing to remember when you're writing testing code. But\nwe're going to do it a few times until it's habit. So just make it a habit. If you're training your\nmodel, call model dot train to make sure it's in training mode. If you're testing or evaluating\nyour model. So that's what a vowel stands for evaluate, call model dot a vowel. So it turns off\ndifferent settings in the model not needed for evaluation. So testing, this is things like drop\nout. We haven't seen what drop out is slash batch norm layers. But if we go into torch dot\nand end, I'm sure you'll come across these things in your future machine learning endeavors. So drop\nout drop out layers. There we go. And batch norm. Do we have batch batch norm? There we go. If you'd\nlike to work out what they are, feel free to check out the documentation. Just take it from me for\nnow that model of our turns off different settings not needed for evaluation and testing. Then we\nset up with torch dot inference mode, inference mode. So what does this do? Let's write down here.\nSo this turns off gradient tracking. So as we discussed, if we have parameters in our model,\nand it turns off actually a few more things and a couple more things behind the scenes,\nthese are things again, not needed for testing. So we discussed that if parameters in our model\nhave requires grad equals true, which is the default for many different parameters in pytorch,\npytorch will behind the scenes keep track of the gradients of our model and use them in\nlost up backward and optimizer step for back propagation and gradient descent. However,\nwe only need those two back propagation and gradient descent during training because that\nis when our model is learning. When we are testing, we are just evaluating the parameters the patterns\nthat our model has learned on the training data set. So we don't need to do any learning\nwhen we're testing. So we turn off the things that we don't need. And is this going to have\nthe correct spacing for me? I'm not sure we'll find out. So we still do the forward pass\nin testing mode, do the forward pass. And if you want to look up torch inference mode,\njust go torch inference mode. There's a great tweet about it that pytorch did, which explains\nwhat's happening. I think we've covered this before, but yeah, want to make your inference\ncode and pytorch run faster. Here's a quick thread on doing exactly that. So inference\nmode is torch no grad. Again, you might see torch no grad. I think I'll write that down just to\nlet you know. But here's what's happening behind the scenes. A lot of optimization code,\nwhich is beautiful. This is why we're using pytorch so that our code runs nice and far.\nLet me go there. You may also see with torch dot no grad in older pytorch code. It does\nsimilar things, but inference mode is the faster way of doing things according to the thread.\nAnd according to there's a blog post attached to there as well, I believe.\nSo you may also see torch dot no grad in older pytorch code, which would be valid. But again,\ninference mode is the better way of doing things. So do forward pass. So let's get our model. We\nwant to create test predictions here. So we're going to go model zero. There's a lot of code\ngoing on here, but I'm going to just step by step it in a second. We'll go back through it all.\nAnd then number two is calculate the loss. Now we're doing the test predictions here,\ncalculate the loss test predictions with model zero. So now we want to calculate the what we want\nto calculate the test loss. So this will be our loss function, the difference between the test\npred and the test labels. That's important. So for testing, we're working with test data,\nfor training, we're working with training data. Model learns patterns on the training data,\nand it evaluates those patterns that it's learned, the different parameters on the testing data. It\nhas never seen before, just like in a university course, you'd study the course materials, which\nis the training data, and you'd evaluate your knowledge on materials you'd hopefully never\nseen before, unless you sort of were friends with your professor, and they gave you the exam before\nthe actual exam that would be cheating right. So that's a very important point for the test data\nset. Don't let your model see the test data set before you evaluate it. Otherwise, you'll get\npoor results. And that's putting it out what's happening. Epoch, we're going to go Epoch,\nand then I will introduce you to my little jingle to remember all of these steps because\nthere's a lot going on. Don't you worry. I know there's a lot going on, but again, with practice,\nwe're going to know what's happening here. Like it's the back of our hand. All right.\nSo do we need this? Oh, yeah, we could say that. Oh, no, we don't need test here. Loss. This is\nloss, not test. Print out what's happening. Okay. And we don't actually need to do this\nevery epoch. We could just go say if epoch divided by 10 equals zero, print out what's happening.\nLet's do that rather than clutter everything up, print it out, and we'll print out this.\nSo let's just step through what's happening. We've got 100 epochs. That's what we're about to run,\n100 epochs. Our model is trained for about 10 so far. So it's got a good base. Maybe we'll just\nget rid of that base. Start a new instance of our model. So we'll come right back down.\nSo our model is back to randomly initialized parameters, but of course, randomly initialized\nflavored with a random seed of 42. Lovely, lovely. And so we've got our training code here. We've\ndiscussed what's happening there. Now, we've got our testing code. We call model dot eval,\nwhich turns off different settings in the model, not needed for evaluation slash testing. We call\nwith torch inference mode context manager, which turns off gradient tracking and a couple more\nthings behind the scenes to make our code faster. We do the forward pass. We do the test predictions.\nWe pass our model, the test data, the test features to calculate the test predictions.\nThen we calculate the loss using our loss function. We can use the same loss function that we used\nfor the training data. And it's called the test loss, because it's on the test data set.\nAnd then we print out what's happening, because we want to know what's happening while our\nmodel's training, we don't necessarily have to do this. But the beauty of PyTorch is you can\nuse basic Python printing statements to see what's happening with your model. And so,\nbecause we're doing 100 epochs, we don't want to clutter up everything here. So we'll just\nprint out what's happening every 10th epoch. Again, you can customize this as much as you like\nwhat's printing out here. This is just one example. If you had other metrics here, such as calculating\nmodel accuracy, we might see that later on, hint hint. We might print out our model accuracy.\nSo this is very exciting. Are you ready to run 100 epochs? How low do you think our loss can go?\nThis loss was after about 10. So let's just save this here. Let's give it a go. Ready?\nThree, two, one. Let's run. Oh my goodness. Look at that. Waits. Here we go. Every 10 epochs\nwere printing out what's happening. So the zero epoch, we started with losses 312. Look at it go\ndown. Yes, that's what we want. And our weights and bias, are they moving towards our ideal weight\nand bias values of 0.7 and 0.3? Yes, they're moving in the right direction here. The loss is\ngoing down. Epoch 20, wonderful. Epoch 30, even better. 40, 50, going down, down, down. Yes,\nthis is what we want. This is what we want. Now, we're predicting a straight line here. Look how\nlow the loss gets. After 100 epochs, we've got about three times less than what we had before.\nAnd then we've got these values are quite close to where they should be, 0.5629, 0.3573. We'll make\nsome predictions. What do they look like? Why preds new? This is the original predictions\nwith random values. And if we make why preds new, look how close it is after 100 epochs.\nNow, what's our, do we print out the test loss? Oh no, we're printing out loss as well.\nLet's get rid of that. I think this is this. Yeah, that's this statement here. Our code would have\nbeen a much cleaner if we didn't have that, but that's all right. Life goes on. So our test loss,\nbecause this is the test predictions that we're making, is not as low as our training loss.\nI wonder how we could get that lower. What do you think we could do? We just trained it for\nlonger. And what happened? How do you think you could get these red dots to line up with these\ngreen dots? Do you think you could? So that's my challenge to you for the next video.\nThink of something that you could do to get these red dots to match up with these green dots,\nmaybe train for longer. How do you think you could do that? So give that a shot. And I'll see\nin the next video, we'll review what our testing code is doing. I'll see you there.\nWelcome back. In the last video, we did something super exciting. We trained our model for 100 epochs\nand look how good the predictions got. But I finished it off with challenging you to see if you could\nalign the red dots with the green dots. And it's okay if you're not sure how the best way to do\nthat. That's what we're here for. We're here to learn what are the best way to do these things\ntogether. But you might have had the idea of potentially training the model for a little bit\nlonger. So how could we do that? Well, we could just rerun this code. So the model is going to\nremember the parameters that it has from what we've done here. And if we rerun it, well, it's going\nto start from where it finished off, which is already pretty good for our data set. And then\nit's going to try and improve them even more. This is, I can't stress enough, like what we are\ndoing here is going to be very similar throughout the entire rest of the course for training more\nand more models. So this step that we've done here for training our model and evaluating it\nis seriously like the fundamental steps of deep learning with PyTorch is training and evaluating\na model. And we've just done it. Although I'll be it to predict some red dots and green dots.\nThat's all right. So let's try to line them up, hey, red dots onto green dots. I reckon if we\ntrain it for another 100 epochs, we should get pretty darn close. Ready? Three, two, one. I'm\ngoing to run this cell again. Runs really quick because our data's nice and simple. But\nlook at this, lastly, we started 0244. Where do we get down to? 008. Oh my goodness. So we've\nimproved it by another three X or so. And now this is where our model has got really good.\nOn the test loss, we've gone from 00564. We've gone down to 005. So almost 10X improvement there.\nAnd so we make some more predictions. What are our model parameters? Remember the ideal ones here.\nWe won't necessarily know them in practice, but because we're working with a simple data set,\nwe know what the ideal parameters are. Model zero state dig weights. These are what they\npreviously were. What are they going to change to? Oh, would you look at that? Oh,\n06990. Now, again, if yours are very slightly different to mine, don't worry too much. That is\nthe inherent randomness of machine learning and deep learning. Even though we set a manual seed,\nit may be slightly different. The direction is more important. So if your number here is not\nexactly what mine is, it should still be quite close to 0.7. And the same thing with this one.\nIf it's not exactly what mine is, don't worry too much. The same with all of these loss values\nas well. The direction is more important. So we're pretty darn close. How do these predictions\nlook? Remember, these are the original ones. We started with random. And now we've trained a model.\nSo close. So close to being exactly that. So a little bit off. But that's all right. We could\ntweak a few things to improve this. But I think that's well and truly enough for this example\npurpose. You see what's happened. Of course, we could just create a model and set the parameters\nourselves manually. But where would be the fun in that? We just wrote some machine learning code\nto do it for us with the power of back propagation and gradient descent. Now in the last video,\nwe wrote the testing loop. We discussed a few other steps here. But now let's go over it with\na colorful slide. Hey, because I mean, code on a page is nice, but colors are even nicer. Oh,\nwe haven't done this. We might set up this in this video too. But let's just discuss what's going on.\nCreate an empty list for storing useful value. So this is helpful for tracking model progress.\nHow can we just do this right now? Hey, we'll go here and we'll go.\nSo what did we have? Epoch count equals that. And then we'll go\nlost values. So why do we keep track of these? It's because\nif we want to monitor our models progress, this is called tracking experiments. So track\ndifferent values. If we wanted to try and improve upon our current model with a future model. So\nour current results, such as this, if we wanted to try and improve upon it, we might build an\nentire other model. And we might train it in a different setup. We might use a different learning\nrate. We might use a whole bunch of different settings, but we track the values so that we\ncan compare future experiments to past experiments, like the brilliant scientists that we are.\nAnd so where could we use these lists? Well, we're calculating the loss here. And we're calculating\nthe test loss here. So maybe we each time append what's going on here as we do a status update.\nSo epoch count dot append, and we're going to go a current epoch. And then we'll go loss values\ndot append, a current loss value. And then we'll do test loss values dot append, the current test\nloss values. Wonderful. And now let's re-instantiate our model so that it starts from fresh. So this\nis just create another instance. So we're just going to re-initialize our model parameters to\nstart from zero. If we wanted to, we could functionize all of this so we don't have to\ngo right back up to the top of the code. But just for demo purposes, we're doing it how we're doing\nit. And I'm going to run this for let's say 200 epochs, because that's what we ended up doing,\nright? We ran it for 200 epochs, because we did 100 epochs twice. And I want to show you something\nbeautiful, one of the most beautiful sites in machine learning. So there we go, we run it for\n200 epochs, we start with a fairly high training loss value and a fairly high test loss value. So\nremember, what is our loss value? It's ma e. So if we go back, yeah, this is what we're measuring\nfor loss. So this means for the test loss on average, each of our dot points here, the red\npredictions are 0.481. That's the average distance between each dot point. And then ideally, what\nare we doing? We're trying to minimize this distance. That's the ma e. So the mean absolute error.\nAnd we get it right down to 0.05. And if we make predictions, what do we have here, we get very\nclose to the ideal weight and bias, make our predictions, have a look at the new predictions.\nYeah, very small distance here. Beautiful. That's a low loss value.\nIdeally, they'd line up, but we've got as close as we can for now. So this is one of the most\nbeautiful sites in machine learning. So plot the loss curves. So let's make a plot, because what\nwe're doing, we were tracking the value of epoch count, loss values and test loss values.\nLet's have a look at what these all look like. So epoch count goes up, loss values ideally go down.\nSo we'll get rid of that. We're going to create a plot p l t dot plot. We're going to step back\nthrough the test loop in a second with some colorful slides, label equals train loss.\nAnd then we're going to go plot. You might be able to tell what's going on here. Test loss\nvalues. We're going to visualize it, because that's the data explorer's motto, right, is visualize,\nvisualize, visualize. This is equals. See, collab does this auto correct. That doesn't really work\nvery well. And I don't know when it does it and why it doesn't. And we got, I know, we didn't,\nwe didn't say loss value. So that's a good auto correct. Thank you, collab.\nSo training and loss and test loss curves. So this is another term you're going to come across\noften is a loss curve. Now you might be able to think about a loss curve. If we're doing a loss\ncurve, and it's starting at the start of training, what do we want that curve to do?\nWhat do we want our loss value to do? We want it to go down. So what should an ideal loss\ncurve look like? Well, we're about to see a couple. Let's have a look. Oh, what do we got wrong?\nWell, we need to, I'll turn it into NumPy. Is this what we're getting wrong? So why is this wrong?\nLoss values. Why are we getting an issue? Test loss values.\nAh, it's because they're all tens of values. So I think we should, let's,\nI might change this to NumPy. Oh, can I just do that? If I just call this as a NumPy array,\nwe're going to try and fix this on the fly. People, NumPy array, we'll just turn this into a NumPy\narray. Let's see if we get NumPy. I'm figuring these things out together. NumPy as NumPy,\nbecause mapplotlib works with NumPy. Yeah, there we go. So can we do loss values? Maybe\nI'm going to try one thing, torch dot tensor, loss values, and then call\nCPU dot NumPy. See what happens here.\nThere we go. Okay, so let's just copy this. So what we're doing here is\nour loss values are still on PyTorch, and they can't be because mapplotlib works with\nNumPy. And so what we're doing here is we're converting our loss values of the training loss\nto NumPy. And if you call from the fundamental section, we call CPU and NumPy, I wonder if we\ncan just do straight up NumPy, because we're not working on there. Yeah, okay, we don't need\nCPU because we're not working on the GPU yet, but we might need that later on. Well, this work.\nBeautiful. There we go. One of the most beautiful sides in machine learning is a declining loss\ncurve. So this is how we keep track of our experiments, or one way, quite rudimentary. We'd like to\nautomate this later on. But I'm just showing you one way to keep track of what's happening.\nSo the training loss curve is going down here. The training loss starts at 0.3, and then it goes\nright down. The beautiful thing is they match up. If there was a two bigger distance behind the\ntrain loss and the test loss, or sorry, between, then we're running into some problems. But if they\nmatch up closely at some point, that means our model is converging and the loss is getting as\nclose to zero as it possibly can. If we trained for longer, maybe the loss will go almost basically\nto zero. But that's an experiment I'll leave you to try to train that model for longer.\nLet's just step back through our testing loop to finish off this video. So we did that. We created\nempty lists for strong useful values, storing useful values, strong useful values. Told the\nmodel what we want to evaluate or that we want to evaluate. So we put it in an evaluation mode.\nIt turns off functionality used for training, but not evaluations, such as drop out and batch\nnormalization layers. If you want to learn more about them, you can look them up in the documentation.\nTurn on torch inference mode. So this is for faster performance. So we don't necessarily need this,\nbut it's good practice. So I'm going to say that yes, turn on torch inference mode. So this\ndisables functionality such as gradient tracking for inference. Gradient tracking is not needed\nfor inference only for training. Now we pass the test data through the model. So this will call\nthe models implemented forward method. The forward pass is the exact same as what we did in the\ntraining loop, except we're doing it on the test data. So big notion there, training loop,\ntraining data, testing loop, testing data. Then we calculate the test loss value,\nhow wrong the models predictions are on the test data set. And of course, lower is better.\nAnd finally, we print out what's happening. So we can keep track of what's going on during\ntraining. We don't necessarily have to do this. You can customize this print value to print out\nalmost whatever you want, because it's pie torches, basically very beautifully interactive with pure\nPython. And then we keep track of the values of what's going on on epochs and train loss and test\nloss. We could keep track of other values here. But for now, we're just going, okay, what's the loss\nvalue at a particular epoch for the training set? And for the test set. And of course, all of this\ncould be put into a function. And that way we won't have to remember these steps off by heart.\nBut the reason why we've spent so much time on this is because we're going to be using this\ntraining and test functionality for all of the models that we build throughout this course.\nSo give yourself a pat in the back for getting through all of these videos. We've written a lot\nof code. We've discussed a lot of steps. But if you'd like a song to remember what's happening,\nlet's finish this video off with my unofficial PyTorch optimization loop song.\nSo for an epoch in a range, go model dot train, do the forward pass, calculate the loss, optimize\na zero grad, loss backward, optimize a step, step, step. No, you only have to call this once.\nBut now let's test, go model dot eval with torch inference mode, do the forward pass,\ncalculate the loss. And then the real song goes for another epoch because you keep going back\nthrough. But we finish off with print out what's happening. And then of course, we evaluate what's\ngoing on. With that being said, it's time to move on to another thing. But if you'd like to review\nwhat's happening, please, please, please try to run this code for yourself again and check out the\nslides and also check out the extra curriculum. Oh, by the way, if you want to link to all\nof the extra curriculum, just go to the book version of the course. And it's all going to be in here.\nSo that's there ready to go. Everything I link is extra curriculum will be in the extra curriculum\nof each chapter. I'll see you in the next video. Welcome back. In the last video, we saw how to\ntrain our model and evaluate it by not only looking at the loss metrics and the loss curves,\nbut we also plotted our predictions and we compared them. Hey, have a go at these random\npredictions. Quite terrible. But then we trained a model using the power of back propagation and\ngradient descent. And now look at our predictions. They're almost exactly where we want them to be.\nAnd so you might be thinking, well, we've trained this model and it took us a while to\nwrite all this code to get some good predictions. How might we run that model again? So I've took\nin a little break after the last video, but now I've come back and you might notice that my Google\nColab notebook has disconnected. So what does this mean if I was to run this? Is it going to work?\nI'm going to connect to a new Google Colab instance. But will we have all of the code that we've run\nabove? You might have already experienced this if you took a break before and came back to the\nvideos. Ah, so plot predictions is no longer defined. And do you know what that means? That\nmeans that our model is also no longer defined. So we would have lost our model. We would have\nlost all of that effort of training. Now, luckily, we didn't train the model for too long. So we can\njust go run time, run all. And it's going to rerun all of the previous cells and be quite quick.\nBecause we're working with a small data set and using a small model. But we've been through all\nof this code. Oh, what have we got wrong here? Model zero state dict. Well, that's all right.\nThis is good. We're finding errors. So if you want to as well, you can just go run after. It's going\nto run all of the cells after. Beautiful. And we come back down. There's our model training.\nWe're getting very similar values to what we got before. There's the lost curves. Beautiful.\nStill going. Okay. Now our predictions are back because we've rerun all the cells and we've got\nour model here. So what we might cover in this video is saving a model in PyTorch. Because if\nwe're training a model and you get to a certain point, especially when you have a larger model,\nyou probably want to save it and then reuse it in this particular notebook itself. Or you might\nwant to save it somewhere and send it to your friend so that your friend can try it out. Or you\nmight want to use it in a week's time. And if Google Colab is disconnected, you might want to\nbe able to load it back in somehow. So now let's see how we can save our models in PyTorch. So\nI'm going to write down here. There are three main methods you should know about\nfor saving and loading models in PyTorch because of course with saving comes loading. So we're\ngoing to over the next two videos discuss saving and loading. So one is torch.save. And as you might\nguess, this allows you to save a PyTorch object in Python's pickle format. So you may or may not\nbe aware of Python pickle. There we go. Python object serialization. There we go. So we've got\nthe pickle module implements a binary protocols or implements binary protocols for serializing\nand deserializing a Python object. So serializing means I understand it is saving and deserializing\nmeans that it's loading. So this is what PyTorch uses behind the scenes, which is from pure Python.\nSo if we go back here in Python's pickle format, number two is torch.load, which you might be able\nto guess what that does as well, allows you to load a saved PyTorch object. And number three is\nalso very important is torch.nn.module.loadStatedict. Now what does this allow you to do? Well,\nthis allows you to load a model's saved dictionary or save state dictionary. Yeah, that's what we'll\ncall it. Save state dictionary. Beautiful. And what's the model state dict? Well, let's have a look,\nmodel zero dot state dict. The beauty of PyTorch is that it stores a lot of your model's important\nparameters in just a simple Python dictionary. Now it might not be that simple because our model,\nagain, only has two parameters. In the future, you may be working with models with millions of\nparameters. So looking directly at the state deck may not be as simple as what we've got here.\nBut the principle is still the same. It's still a dictionary that holds the state of your model.\nAnd so I've got these three methods I want to show you where from because this is going to be\nyour extra curriculum, save and load models, your extra curriculum for this video.\nIf we go into here, this is a very, very, very important piece of PyTorch documentation,\nor maybe even a tutorial. So your extra curriculum for this video is to go through it.\nHere we go. We've got torch, save, torch, load, torch, module, state deck. That's where, or load\nstate deck, that's where I've got the three things that we've just written down. And there's a fair\nfew different pieces of information. So what is a state deck? So in PyTorch, the learnable\nparameters, i.e. the weights and biases of a torch and end module, which is our model.\nRemember, our model subclasses and end module are contained in the model's parameters. Access\nwith model.parameters, a state deck is simply a Python dictionary object that maps each layer\nto its parameter tensor. That's what we've seen. And so then if we define a model,\nwe can initialize the model. And if we wanted to print the state decked, we can use that.\nThe optimizer also has a state deck. So that's something to be aware of. You can go optimizer.state\ndeck. And then you get an output here. And this is our saving and loading model for inference. So\ninference, again, is making a prediction. That's probably what we want to do in the future at some\npoint. For now, we've made predictions right within our notebook. But if we wanted to use our model\noutside of our notebook, say in an application, or in another notebook that's not this one,\nyou'll want to know how to save and load it. So the recommended way of saving and loading a\nPyTorch model is by saving its state deck. Now, there is another method down here,\nwhich is saving and loading the entire model. So your extracurricular for this lesson,\nwe're going to go through the code to do this. But your extracurricular is to read all of the\nsections in here, and then figure out what the pros and cons are of saving and loading the entire\nmodel versus saving and loading just the state deck. So that's a challenge for you for this video.\nI'm going to link this in here. And now let's write some code to save our model.\nSo PyTorch save and load code. Code tutorial plus extracurricular. So if we go\nsaving our PyTorch model. So what might we want? What do you think the save parameter takes?\nIf we have torch.save, what do you think it takes inside it? Well, let's find out together.\nHey, so let's import part lib. We're going to see why in a second. This is Python's\nmodule for dealing with writing file paths. So if we wanted to save something to this is Google\nColab's file section over here. But just remember, if we do save this from within Google Colab,\nthe model will disappear if our Google Colab notebook instance disconnects. So I'll show you\nhow to download it from Google Colab if you want. Google Colab also has a way save from Google Colab\nGoogle Colab to Google Drive to save it to your Google Drive if you wanted to. But I'll leave you\nto look at that on your own if you like. So we're first going to create a model directory.\nSo create models directory. So this is going to help us create a folder over here called models.\nAnd of course, we could create this by hand by adding a new folder here somewhere. But I like\nto do it with code. So model path, we're going to set this to path, which is using the path library\nhere to create us a path called models. Simple. We're just going to save all of our models to\nmodels to the models file. And then we're going to create model path, we're going to make that\ndirectory model path dot mkdir for make directory. We're going to set parents to equals true.\nAnd we're also going to set exist okay equals to true. That means if it already existed,\nit won't throw us an error. It will try to create it. But if it already exists, it'll just recreate\nthe parents directory or it'll leave it there. It won't error out on us. We're also going to\ncreate a model save path. This way, we can give our model a name. Right now, it's just model zero.\nWe want to save it under some name to the models directory. So let's create the model name.\nModel name equals 01. I'm going to call it 01 for the section. That way, if we have more models\nlater on the course, we know which ones come from where you might create your own naming\nconvention, model workflow, pytorch workflow, model zero dot pth. And now this is another\nimportant point. Pytorch objects usually have the extension dot pth for pytorch or dot pth.\nSo if we go in here, and if we look up dot pth, yeah, a common convention is to save models\nusing either a dot pth or dot pth file extension. I'll let you choose which one you like. I like\ndot pth. So if we go down here dot pth, they both result in the same thing. You just have to remember\nto make sure you write the right loading path and right saving path. So now we're going to create\nour model save path, which is going to be our model path. And because we're using the path lib,\nwe can use this syntax that we've got here, model path slash model name. And then if we just print out\nmodel save path, what does this look like? There we go. So it creates a supposic path\nusing the path lib library of models slash 01 pytorch workflow model zero dot pth. We haven't\nsaved our model there yet. It's just got the path that we want to save our model ready. So if we\nrefresh this, we've got models over here. Do we have anything in there? No, we don't yet. So now\nis our step to save the model. So three is save the model state dict. Why are we saving the state\ndict? Because that's the recommended way of doing things. If we come up here, saving and loading the\nmodel for inference, save and load the state dict, which is recommended. We could also save the entire\nmodel. But that's part of your extra curriculum to look into that. So let's use some syntax. It's\nquite like this torch dot save. And then we pass it an object. And we pass it a path of where to\nsave it. We already have a path. And good thing is we already have a model. So we just have to call\nthis. Let's try it out. So let's go print f saving model to and we'll put in the path here.\nModel save path. I like to print out some things here and there that way. We know what's going on.\nAnd I don't need that capital. Why do I? Getting a little bit trigger happy here with the typing.\nSo torch dot save. And we're going to pass in the object parameter here. And if we looked up torch\nsave, we can go. What does this code take? So torch save object f. What is f? A file like object.\nOkay. Or a string or OS path like object. Beautiful. That's what we've got. A path like\nobject containing a file name. So let's jump back into here. The object is what? It's our model zero\ndot state dict. That's what we're saving. And then the file path is model save path. You ready?\nLet's run this and see what happens. Beautiful. Saving model to models. So it's our model path.\nAnd there's our model there. So if we refresh this, what do we have over here?\nWonderful. We've saved our trained model. So that means we could potentially if we wanted to,\nyou could download this file here. That's going to download it from Google CoLab to your local\nmachine. That's one way to do it. But there's also a guide here to save from Google Collaboratory\nto Google Drive. That way you could use it later on. So there's many different ways.\nThe beauty of pie torches is flexibility. So now we've got a saved model. But let's just check\nusing our LS command. We're going to check models. Yeah, let's just check models. This is going to\ncheck here. So this is list. Wonderful. There's our 01 pie torch workflow model zero dot pth. Now,\nof course, we've saved a model. How about we try loading it back in and seeing how it works. So if\nyou want to challenge, read ahead on the documentation and try to use torch dot load to bring our model\nback in. See what happens. I'll see in the next video. Welcome back. In the last video, we wrote\nsome code here to save our pie torch model. I'm just going to exit out of this couple of things\nthat we don't need just to clear up the screen. And now we've got our dot pth file, because remember\ndot pth or dot pth is a common convention for saving a pie torch model. We've got it saved there,\nand we didn't necessarily have to write all of this path style code. But this is just handy for\nlater on if we wanted to functionize this and create it in say a save dot pie file over here,\nso that we could just call our save function and pass it in a file path where we wanted to save\nlike a directory and a name, and then it'll save it exactly how we want it for later on.\nBut now we've got a saved model. I issued a challenge of trying to load that model in.\nSo do we have torch dot load in here? Did you try that out? We've got, oh, we've got a few options\nhere. Wonderful. But we're using one of the first ones. So let's go back up here. If we wanted to\ncheck the documentation for torch dot load, we've got this option here, load. What happens? Loads\nand objects saved with torch dot save from a file. Torch dot load uses Python's unpickling\nfacilities, but treat storages which underlie tenses specially. They are firstly serialized\non the CPU, and then I moved the device they were saved from. Wonderful. So this is moved to the\ndevice. If later on when we're using a GPU, this is just something to keep in mind. We'll see that\nwhen we start to use a CPU and a GPU. But for now, let's practice using the torch dot load method\nand see how we can do it. So we'll come back here and we'll go loading a pytorch model.\nAnd since we, she's going to start writing here, since we saved our models state debt,\nso just the dictionary of parameters from a model, rather than\nthe entire model, we'll create a new instance of our model class and load the state deck,\nload the saved state deck. That's better state deck into that.\nNow, this is just words on a page. Let's see this in action. So to load in a state deck,\nwhich is what we say, we didn't save the entire model itself, which is one option.\nThat's extra curriculum, but we saved just the model state deck. So if we remind ourselves what\nmodel zero dot state deck looks like, we saved just this. So to load this in, we have to\ninstantiate a new class or a new instance of our linear regression model class. So to load in a\nsaved state deck, we have to instantiate a new instance of our model class. So let's call this\nloaded model zero. I like that. That way we can differentiate because it's still going to be the\nsame parameters as model zero, but this way we know that this instance is the loaded version,\nnot just the version we've been training before. So we'll create a new version of it here,\nlinear regression model. This is just the code that we wrote above, linear regression model.\nAnd then we're going to load the saved state deck of model zero. And so this will update the new\ninstance with updated parameters. So let's just check before we load it, we haven't written any\ncode to actually load anything. What does loaded model zero? What does the state deck look like here?\nIt won't have anything. It'll be initialized with what?\nOh, loaded. That's what I called it loaded. See how it's initialized with random parameters.\nSo essentially all we're doing when we load a state dictionary into our new instance of our\nmodel is that we're going, hey, take the saved state deck from this model and plug it into this.\nSo let's see what happens when we do that. So loaded model zero. Remember how I said there's\na method to also be aware of up here, which is torch nn module dot load state deck. And because\nour model is a what, it's a subclass of torch dot nn dot module. So we can call load state deck\non our model directly or on our instance. So recall linear regression model is a subclass\nof nn dot module. So let's call in load state deck. And this is where we call the torch dot load\nmethod. And then we pass it the model save path. Is that what we call it? Because torch dot load,\nit takes in F. So what's F a file like object or a string or a OS path like object. So that's\nwhy we created this path like object up here. Model save path. So all we're doing here,\nwe're creating a new instance, linear regression model, which is a subclass of nn dot module.\nAnd then on that instance, we're calling in load state deck of torch dot load model save path.\nBecause what's saved at the model save path, our previous models state deck, which is here.\nSo if we run this, let's see what happens. All keys match successfully. That is beautiful.\nAnd so see the values here, loaded state deck of model zero. Well, let's check the loaded version\nof that. We now have wonderful, we have the exact same values as above. But there's a little\nway that we can test this. So how about we go make some predictions. So make some predictions.\nJust to make sure with our loaded model. So let's put it in a valve mode. Because when you make\npredictions, you want it in evaluation mode. So it goes a little bit faster. And we want to\nalso use inference mode. So with torch dot inference mode for making predictions. We want to write\nthis loaded model preds, we're going to make some predictions on the test data as well. So loaded\nmodel zero, we're going to forward pass on the X test data. And then we can have a look at the\nloaded model preds. Wonderful. And then to see if the two models are the same, we can compare\nloaded model preds with original model preds. So why preds? These should be equivalent equals\nequals loaded model preds. Do we have the same thing? False, false, false, what's going on here?\nWhy preds? How much different are they? Oh, where's that happened? Have we made some\nmodel preds with this yet? So how about we make some model preds? This is troubleshooting on\nthe fly team. So let's go model zero dot eval. And then with torch dot inference mode,\nthis is how we can check to see that our two models are actually equivalent. Why preds equals,\nI have a feeling why preds actually save somewhere else equals model zero. And then we pass it the\nX test data. And then we might move this above here. And then have a look at what why preds equals.\nDo we get the same output? Yes, we should. Wonderful. Okay, beautiful. So now we've covered\nsaving and loading models or specifically saving the models state deck. So we saved it here with\nthis code. And then we loaded it back in with load state deck plus torch load. And then we\nchecked to see by testing equivalents of the predictions of each of our models. So the original\none that we trained here, model zero, and the loaded version of it here. So that's saving and\nloading a model in pytorch. There are a few more things that we could cover. But I'm going to leave\nthat for extra curriculum. We've covered the two main things or three main things. One, two, three.\nIf you'd like to read more, I'd highly encourage you to go through and read this tutorial here.\nBut with that being said, we've covered a fair bit of ground over the last few videos. How about\nwe do a few videos where we put everything together just to reiterate what we've done.\nI think that'll be good practice. I'll see you in the next video.\nWelcome back. Over the past few videos, we've covered a whole bunch of ground in a pytorch\nworkflow, starting with data, then building a model. Well, we split the data, then we built a\nmodel. We looked at the model building essentials. We checked the contents of our model. We made\nsome predictions with a very poor model because it's based off random numbers. We spent a whole\nbunch of time figuring out how we could train a model. We figured out what the loss function is.\nWe saw an optimizer. We wrote a training and test loop. We then learned how to save and load a\nmodel in pytorch. So now I'd like to spend the next few videos putting all this together. We're\nnot going to spend as much time on each step, but we're just going to have some practice together\nso that we can reiterate all the things that we've done. So putting it all together,\nlet's go back through the steps above and see it all in one place. Wonderful.\nSo we're going to start off with 6.1 and we'll go have a look at our workflow. So 6.1 is data,\nbut we're going to do one step before that. And I'm just going to get rid of this so we have a bit\nmore space. So we've got our data ready. We've turned it into tenses way back at the start.\nThen we built a model and then we picked a loss function and an optimizer. We built a training\nloop. We trained our model. We made some predictions. We saw that they were better. We evaluated our\nmodel. We didn't use torch metrics, but we got visual. We saw our red dots starting to line up\nwith the green dots. We haven't really improved through experimentation. We did a little bit of\nit though, as in we saw that if we trained our model for more epochs, we got better results.\nSo you could argue that we have done a little bit of this, but there are other ways to experiment.\nWe're going to cover those throughout the course. And then we saw how to save and reload a trained\nmodel. So we've been through this entire workflow, which is quite exciting, actually.\nSo now let's go back through it, but we're going to do it a bit quicker than what we've done before,\nbecause I believe you've got the skills to do so now. So let's start by importing pytorch.\nSo you could start the code from here if you wanted to. And that plot live. And actually,\nif you want, you can pause this video and try to recode all of the steps that we've done\nby putting some headers here, like data, and then build a model and then train the model,\nsave and load a model, whatever, and try to code it out yourself. If not, feel free to follow along\nwith me and we'll do it together. So import torch from torch import. Oh, would help if I could spell\ntorch import and n because we've seen that we use an n quite a bit. And we're going to also\nimport map plot live because we like to make some plots because we like to get visual.\nVisualize visualize visualize as PLT. And we're going to check out pytorch version.\nThat way we know if you're on an older version, some of the code might not work here. But if you're\non a newer version, it should work. If it doesn't, let me know. There we go. 1.10. I'm using 1.10\nfor this. By the time you watch this video, there may be a later version out. And we're also going\nto let's create some device agnostic code. So create device agnostic code, because I think we're\nup to this step now. This means if we've got access to a GPU, our code will use it for potentially\nfaster computing. If no GPU is available, the code will default to using CPU. We don't necessarily\nneed to use a GPU for our particular problem that we're working on right now because it's a small\nmodel and it's a small data set, but it's good practice to write device agnostic code. So that\nmeans our code will use a GPU if it's available, or a CPU by default, if a GPU is not available.\nSo set up device agnostic code. We're going to be using a similar setup to this throughout the\nentire course from now on. So that's why we're bringing it back. CUDA is available. So remember\nCUDA is NVIDIA's programming framework for their GPUs, else use CPU. And we're going to print\nwhat device are we using? Device. So what we might do is if we ran this, it should be just a CPU\nfor now, right? Yours might be different to this if you've enabled a GPU, but let's change this\nover to use CUDA. And we can do that if you're using Google Colab, we can change the runtime type\nby selecting GPU here. And then I'm going to save this, but what's going to happen is it's\ngoing to restart the runtime. So we're going to lose all of the code that we've written above.\nHow can we get it all back? Well, we can go. Run all. This is going to run all of the cells\nabove here. They should all work and it should be quite quick because our model and data aren't\ntoo big. And if it all worked, we should have CUDA as our device that we can use here. Wonderful.\nSo the beauty of Google Colab is that they've given us access to on a video GPU. So thank you,\nGoogle Colab. Just once again, I'm paying for the paid version of Google Colab. You don't have to.\nThe free version should give you access to a GPU, or be it it might not be as a later version as\nGPU as the pro versions give access to. But this will be more than enough for what we're about to\nrecreate. So I feel like that's enough for this video. We've got some device agnostic code ready\nto go. And for the next few videos, we're going to be rebuilding this except using device agnostic\ncode. So give it a shot yourself. There's nothing in here that we haven't covered before. So I'll\nsee you in the next video. Let's create some data. Welcome back. In the last video, we set up some\ndevice agnostic code and we got ready to start putting everything we've learned together.\nSo now let's continue with that. We're going to recreate some data. Now we could just copy this\ncode, but we're going to write it out together so we can have some practice creating a dummy data\nset. And we want to get to about this stage in this video. So we want to have some data that we can\nplot so that we can build a model to once again, learn on the blue dots to predict the green dots.\nSo we'll come down here data. I'm going to get out of this as well so that we have a bit more room.\nLet's now create some data using the linear regression formula of y equals weight times\nfeatures plus bias. And you may have heard this as y equals mx plus c or mx plus b or something like\nthat, or you can substitute these for different names. Images when I learned this in high school,\nit was y equals mx plus c. Yours might be slightly different. Yeah, bx plus a. That's what they use\nhere. A whole bunch of different ways to name things, but they're all describing the same thing.\nSo let's see this in code rather than formulaic examples. So we're going to create our weight,\nwhich is 0.7 and a bias, which is 0.3. These are the values we previously used for a challenge you\ncould change these to 0.1 maybe and 0.2. These could be whatever values you'd like to set them as.\nSo weight and bias, the principle is going to be the same thing. We're going to try and build a\nmodel to estimate these values. So we're going to start at 0 and we're going to end at 1.\nSo we can just create a straight line and we're going to fill in those between 0 and 1 with a\nstep of 0.02. And now we'll create the x and y features x and y, which is features and labels\nactually. So x is our features and y are our labels. x equals torch dot a range and x is a\ncapital Y is that because typically x is a feature matrix. Even though ours is just a vector now,\nwe're going to unsqueeze this so we don't run into dimensionality issues later on.\nYou can check this for yourself without unsqueeze, errors will pop up and y equals weight times\nx plus bias. You see how we're going a little bit faster now? This is sort of the pace that we're\ngoing to start going for things that we've already covered. If we haven't covered something, we'll\nslow down, but if we have covered something, I'm going to step it through. We're going to start\nspeeding things up a little. So if we get some values here, wonderful. We've got some x values\nand they correlate to some y values. We're going to try and use the training values of x to predict\nthe training values of y and subsequently for the test values. Oh, and speaking of training and test\nvalues, how about we split the data? So let's split the data. Split data. So we'll create the\ntrain split equals int 0.8. We're going to use 80%, which is where 0.8 comes from,\nfor the length of x. So we use 80% of our samples for the training, which is a typical\ntraining and test split, 80, 20. They're abouts. You could use like 70, 30. You could use 90, 10.\nIt all depends on how much data you have. There's a lot of things in machine learning that are\nquite flexible. Train split, we're going to index on our data here so that we can create our splits.\nGoogle Colab auto corrected my code in a non-helpful way just then. And we're going to do the\nopposite split for the testing data. Now let's have a look at the lengths of these. If my calculations\nare correct, we should have about 40 training samples and 10 testing samples. And again, this\nmay change in the future. When you work with larger data sets, you might have 100,000 training\nsamples and 20,000 testing samples. The ratio will often be quite similar. And then let's plot\nwhat's going on here. So plot the data and note, if you don't have the plot predictions\nfunction loaded, this will error. So we can just run plot predictions here if we wanted to. And\nwe'll pass it in X train, Y train, X test, Y test. And this should come up with our\nplot. Wonderful. So we've just recreated the data that we've been previously using. We've got\nblue dots to predict green dots. But if this function errors out because you've started the notebook\nfrom here, right from this cell, and you've gone down from there, just remember, you'll just have\nto go up here and copy this function. We don't have to do it because we've run all the cells,\nbut if you haven't run that cell previously, you could put it here and then run it, run it,\nand we'll get the same outcome here. Wonderful. So what's next? Well, if we go back to our workflow,\nwe've just created some data. And have we turned it into tenses yet? I think it's just still, oh,\nyeah, it is. It's tenses because we use PyTorch to create it. But now we're up to building or\npicking a model. So we've built a model previously. We did that back in build model. So you could\nrefer to that code and try to build a model to fit the data that's going on here. So that's\nyour challenge for the next video. So building a PyTorch linear model. And why do we call it linear?\nBecause linear refers to a straight line. What's nonlinear? Non-straight. So I'll see you in the\nnext video. Give it a shot before we get there. But we're going to build a PyTorch linear model.\nWelcome back. We're going through some steps to recreate everything that we've done. In the last\nvideo, we created some dummy data. And we've got a straight line here. So now by the workflow,\nwe're up to building a model or picking a model. In our case, we're going to build one\nto suit our problem. So we've got some linear data. And I've put building a PyTorch linear model\nhere. I issued you the challenge of giving it a go. You could do exactly the same steps that\nwe've done in build model. But I'm going to be a little bit cheeky and introduce something\nnew here. And that is the power of torch.nn. So let's see it. What we're going to do is we're\ngoing to create a linear model by subclassingnn.module because why a lot of PyTorch models,\nsubclass, and then module. So class linear regression, what should we call this one?\nLinear regression model v2. How about that? And we'll subclassnn.module. So much similar code to\nwhat we've been writing so far. Or when we first created our linear regression model.\nAnd then we're going to put the standard constructor code here, def init underscore underscore.\nAnd it's going to take as an argument self. And then we're going to call super dot another\nunderscore init underscore underscore brackets. But we're going to instead of if you recall above\nback in the build model section, we initialized these parameters ourselves. And I've been hinting\nat in the past in videos we've seen before that oftentimes you won't necessarily initialize the\nparameters yourself. You'll instead initialize layers that have the parameters in built in those\nlayers. We still have to create a forward method. But what we're going to see is how we can use our\ntorch linear layer to do these steps for us. So let's write the code and then we'll step through it.\nSo we'll go usenn.linear because why we're building linear regression model and our data is linear.\nAnd in the past, our previous model has implemented linear regression formula. So for creating the\nmodel parameters. So we can go self dot linear layer equals. So this is constructing a variable\nthat this class can use self linear layer equals nn dot linear. Remember, nn in PyTorch stands for\nneural network. And we have in features as one of the parameters and out features as another\nparameter. This means we want to take as input of size one and output of size one. Where does that\ncome from? Well, if we have a look at x train and y train, we have one value of x. Maybe there's\ntoo many here. x five will be the first five five and five. So recall, we have one value of x\nequates to one value of y. So that means within this linear layer, we want to take as one feature\nx to output one feature y. And we're using just one layer here. So the input and the output shapes\nof your model in features, out features, what data goes in and what data comes out. These values\nwill be highly dependent on the data that you're working with. And we're going to see different\ndata or different examples of input features and output features all throughout this course. So\nbut that is what's happening. We have one in feature to one out feature. Now what's happening\ninside nn.linear. Let's have a look torch and then linear. We go the documentation\napplies a linear transformation to the incoming data. Where have we seen this before?\ny equals x a t plus b. Now they're using different letters, but we've got the same formula as\nwhat's happening up here. Look at the same formula as our data. Wait times x plus bias. And then if\nwe look up linear regression formula once again, linear regression formula. We've got this formula\nhere. Now again, these letters can be replaced by whatever letters you like. But this linear layer\nis implementing the linear regression formula that we created in our model before. So it's\nessentially doing this part for us. And behind the scenes, the layer creates these parameters for us.\nSo that's a big piece of the puzzle of pie torch is that as I've said, you won't always be\ninitializing the parameters your model yourself. You'll generally initialize layers. And then you'll\nuse those layers in some Ford computation. So let's see how we could do that. So we've got a linear\nlayer which takes us in features one and out features one. What should we do now? Well, because\nwe've subclassed nn.module we need to override the Ford method. So we need to tell our model\nwhat should it do as the Ford computation. And in here it's going to take itself as input,\nas well as x, which is conventional for the input data. And then we're just going to return\nhere, self dot linear layer x. Right. And actually, we might use some typing here to say that this\nshould be a torch tensor. And it's also going to return a torch dot tensor. That's using Python's\ntype ins. So this is just saying, hey, X should be a torch tensor. And I'm going to return you a\ntorch tensor, because I'm going to pass x through the linear layer, which is expecting one in feature\nand one out feature. And it's going to this linear transform. That's another word for it. Again,\npytorch and machine learning in general has many different names of the same thing. I would call\nthis linear layer. I'm going to write here, also called linear transform, probing layer,\nfully connected layer, dense layer, intensive flow. So a whole bunch of different names for\nthe same thing, but they're all implementing a linear transform. They're all implementing a\nversion of linear regression y equals x, a ranspose plus b, in features, out features,\nwonderful. So let's see this in action. So we're going to go set the manual seed so we can\nget reproducibility as well, torch dot manual seed. And we're going to set model one equals\nlinear regression. This is model one, because we've already got model zero, linear regression\nV two, and we're going to check model one, and we're going to check its state dictionary,\nstate dict. There we go. What do we have inside this ordered dict? Has that not created anything\nfor us? Model one, dot state dinked, ordered dink. We haven't got anything here in the regression\nmodel V two. Ideally, this should be outputting a weight and a bias. Yeah, variables, weight,\nand bias. Let's dig through our code line by line and see what we've got wrong. Ah, did you notice\nthis? The init function so the constructor had the wrong amount of underscores. So it was never\nactually constructing this linear layer troubleshooting on the fly team. There we go. Beautiful. So we\nhave a linear layer, and we have it is created for us inside a weight and a bias. So effectively,\nwe've replaced the code we wrote above for build model, initializing a weight and bias parameter\nwith the linear layer. And you might be wondering why the values are slightly different, even though\nwe've used the manual seed. This goes behind the scenes of how PyTorch creates its different\nlayers. It's probably using a different form of randomness to create different types of\nvariables. So just keep that in mind. And to see this in action, we have a conversion here.\nSo this is what's going on. We've converted, this is our original model class, linear regression.\nWe initialize our model parameters here. We've got a weight and a bias. But instead, we've\nswapped this in our linear regression model V2. This should be V2 to use linear layer. And then\nin the forward method, we had to write the formula manually here when we initialize the parameters\nmanually. But because of the power of torch.nn, we have just passed it through the linear layer,\nwhich is going to perform some predefined forward computation in this layer. So this\nstyle of what's going on here is how you're going to see the majority of your PyTorch\ndeep learning models created using pre-existing layers from the torch.nn module. So if we go back\ninto torch.nn, torch.nn, we have a lot of different layers here. So we have convolutional layers,\npooling layers, padding layers, normalization, recurrent, transformer, linear, we're using a\nlinear layer, dropout, et cetera, et cetera. So for all of the common layers in deep learning,\nbecause that's what neural networks are, they're layers of different mathematical transformations,\nPyTorch has a lot of pre-built implementations. So that's a little bit of a sneaky trick that\nI've done to alter our model. But we've still got basically the exact same model as we had before.\nSo what's next? Well, it's to train this model. So let's do that in the next video.\nWelcome back. So in the last video, we built a PyTorch linear model, nice and simple using a\nsingle nn.linear layer with one in feature, one out feature. And we over read the forward method\nof nn.module using the linear layer that we created up here. So what's going to happen is when we do\nthe forward parser on our model, we're going to put some data in and it's going to go through\nthis linear layer, which behind the scenes, as we saw with torch and n linear,\nbehind the scenes, it's going to perform the linear regression formula here. So y equals x,\na t plus b. But now case, we've got weight and bias. So let's go back. It's now time to write\nsome training code. But before we do, let's set the model to use the target device. And so in\nour case, we've got a device of CUDA. But because we've written device agnostic code, if we didn't\nhave access to a CUDA device, a GPU, our default device would be a CPU. So let's check the model\ndevice. We can do that first up here, check the model current device, because we're going to use\nthe GPU here, or we're going to write device agnostic code. That's better to say device agnostic code.\nThat's the proper terminology device. What device are we currently using? This is the CPU, right?\nSo by default, the model will end up on the CPU. But if we set it to model one call dot two device,\nwhat do you think it's going to do now? If our current target device is CUDA, we've seen what\ntwo does in the fundamental section, two is going to send the model to the GPU memory. So now let's\ncheck whether parameters of our model live dot device. If we send them to the device previously,\nit was the CPU, it's going to take a little bit longer while the GPU gets fired up and goes,\nPyTorch goes, Hey, I'm about to send you this model. You ready for it? Boom, there we go. Wonderful.\nSo now our model is on the device or the target device, which is CUDA. And if CUDA wasn't available,\nthe target device would be CPU. So this would just come out just exactly how we've got it here.\nBut with that being said, now let's get on to some training code. And this is the fun part.\nWhat do we have to do? We've already seen this for training. I'm just going to clear up our\nworkspace a little bit here. For training, we need, this is part of the PyTorch workflow,\nwe need a loss function. What does a loss function do? Measures how wrong our model is,\nwe need an optimizer, we need a training loop and a testing loop. And the optimizer, what does that\ndo? Well, it optimizes the parameters of our model. So in our case, model one dot state dig,\nwhat do we have? So we have some parameters here within the linear layer, we have a weight,\nand we have a bias. The optimizer is going to optimize these random parameters so that they\nhopefully reduce the loss function, which remember the loss function measures how wrong our model\nis. So in our case, because we're working with the regression problem, let's set up the loss\nfunction. And by the way, all of these steps are part of the workflow. We've got data ready,\nwe've built or picked a model, we're using a linear model. Now we're up to here 2.1 pick a loss\nfunction and an optimizer, we're going to do build a training loop in the same session,\nbecause you know what, we're getting pretty darn good at this, loss function equals what?\nWell, we're going to use l one loss. So let's set that up and then dot l one loss, which is the\nsame as ma and if we wanted to set up our optimizer, what optimizer could we use? Well, pytorch offers\na lot of optimizers in torch dot opt in SGD. That's stochastic gradient descent, because remember\ngradient descent is the algorithm that optimizes our model parameters. Adam is another popular option.\nFor now, we're going to stick with SGD. LR, which stands for learning rate. In other words,\nhow big of a step will our optimizer change our parameters with every iteration, a smaller\nlearning rate. So such as 0001 will be a small step. And then a large learning rate, such as 0.1\nwill be a larger step. Too big of a step. Our model learns too much. And it explodes too small of a\nstep. Our model never learns anything. But oh, we actually have to pass params first. I forgot\nabout that. I got ahead of myself with a learning rate. Params is the parameters we'd like our\noptimizer to optimize. So in our case, it's model one dot parameters, because model one is our current\ntarget model. Beautiful. So we've got a loss function and an optimizer. Now, let's write a training\nloop. So I'm going to set torch manual seeds so we can try and get as reproducible as results as\npossible. Remember, if you get different numbers to what I'm getting, don't worry too much if they're\nnot exactly the same, the direction is more important. So that means if my loss function is\ngetting smaller, yours should be getting smaller too. Don't worry too much if your fourth decimal\nplace isn't the same as what my values are. So we have a training loop ready to be written here.\nEpox, how many should we do? Well, we did 200 last time and that worked pretty well. So let's do 200\nagain. Did you go through the extra curriculum yet? Did you watch the video for the unofficial\nPyTorch optimization loop song yet? This one here, listen to the unofficial PyTorch optimization\nloop song. If not, it's okay. Let's sing it together. So for an epoch in range, epochs, we're going to\ngo through the song in a second. We're going to set the model to train. In our case, it's model one,\nmodel to train. Now, step number one is what? Do the forward pass. This is where we calculate\nthe predictions. So we calculate the predictions by passing the training data through our model.\nAnd in our case, because the forward method in model one implements the linear layer,\nthis data is going to go through the linear layer, which is torch.nn.linear and go through\nthe linear regression formula. And then we calculate the loss, which is how wrong our models predictions\nare. So the loss value equals loss fn. And here we're going to pass in y-pred and y-train.\nThen what do we do? We zero the optimizer, optimizer zero grad, which because by default,\nthe optimizer is going to accumulate gradients behind the scenes. So every epoch, we want to\nreduce those back to zero. So it starts from fresh. We're going to perform back propagation here,\nback propagation, by calling loss, stop backwards. If the forward pass goes forward through the\nnetwork, the backward pass goes backwards through the network, calculating the gradients for the\nloss function with respect to each parameter in the model. So optimizer step, this next part,\nis going to look at those gradients and go, you know what? Which way should I optimize the parameters?\nSo because the optimizer is optimizing the model parameters, it's going to look at the\nloss and go, you know what? I'm going to adjust the weight to be increased. And I'm going to lower\nthe bias and see if that reduces the loss. And then we can do testing. We can do both of these in\nthe same hit. Now we are moving quite fast through this because we spent a whole bunch of time\ndiscussing what's going on here. So for testing, what do we do? We set the model into evaluation\nmode. That's going to turn off things like dropout and batch normalization layers. We don't have any\nof that in our model for now, but just it's good practice to always call a vowel whenever you're\ndoing testing. And same with inference mode. We don't need to track gradients and a whole bunch of\nother things PyTorch does behind the scenes when we're testing or making predictions. So we use\nthe inference mode context manager. This is where we're going to create test pred, which is going\nto be our test predictions, because here we're going to pass the test data features, forward\npass through our model. And then we can calculate the test loss, which is our loss function. And we're\ngoing to compare the test pred to Y test. Wonderful. And then we can print out what's happening.\nSo what should we print out? How about if epoch divided by 10 equals zero. So every 10 epochs,\nlet's print something out, print. We'll do an F string here, epoch is epoch. And then we'll go\nloss, which is the training loss, and just be equal to the loss. And then we'll go test loss is\nequal to test loss. So do you think this will work? It's okay if you're not sure. But let's find\nout together, hey, oh, we've got a, we need a bracket there. Oh my goodness, what's going on?\nRun time error. Expected all tenses to be on the same device. Oh, of course. Do you know what's\nhappening here? But we found at least two devices, CUDA and CPU. Yes, of course, that's what's happened.\nSo what have we done? Up here, we put our model on the GPU. But what's going on here? Our data?\nHas our data on the GPU? No, it's not. By default, it's on the CPU. So we haven't written device\nagnostic code for our data. So let's write it here, put data on the target device.\nDevice agnostic code for data. So remember, one of the biggest issues with pytorch aside from\nshape errors is that you should have your data or all of the things that you're computing with\non the same device. So that's why if we set up device agnostic code for our model,\nwe have to do the same for our data. So now let's put X train to device. Y train equals Y train\nto device. This is going to create device agnostic code. In our case, it's going to use CUDA because\nwe have access to a CUDA device. But if we don't, this code will still work. It will still default\nto CPU. So this is good. I like that we got that error because that's the sum of the things you're\ngoing to come across in practice, right? So now let's run this. What's happening here?\nHey, look at that. Wonderful. So our loss starts up here nice and high. And then it starts to go\nright down here for the training data. And then the same for the testing data. Beautiful.\nRight up here. And then all the way down. Okay. So this looks pretty good on the test data set. So\nhow can we check this? How can we evaluate our model? Well, one way is to check its state\ndeck. So state decked. What do we got here? What are our weight and bias? Oh my gosh, so close.\nSo we just set weight and bias before to be 0.7 and 0.3. So this is what our model has estimated\nour parameters to be based on the training data. 0.6968. That's pretty close to 0.7,\nnearly perfect. And the same thing with the bias 0.3025 versus the perfect value is 0.93. But remember,\nin practice, you won't necessarily know what the ideal parameters are. This is just to exemplify\nwhat our model is doing behind the scenes. It's moving towards some ideal representative\nparameters of whatever data we're working with. So in the next video, I'd like you to give it a go\nof before we get to the next video, make some predictions with our model and plot them on the\noriginal data. How close to the green dots match up with the red dots? And you can use this plot\npredictions formula or function that we've been using in the past. So give that a go and I'll\nsee you in the next video. But congratulations. Look how quickly we just trained a model using\nthe steps that we've covered in a bunch of videos so far and device agnostic code. So good.\nI'll see you soon. In the last video, we did something very, very exciting. We worked through\ntraining an entire neural network. Some of these steps took us an hour or so worth of videos to\ngo back through before. But we coded that in one video. So you're ready listening the song just to\nremind ourselves of what's going on. For an epoch in a range, call model dot train, do the forward\npass, calculate the loss, optimizer zero grad, loss backward, optimizer step, step, step, let's\ntest, come on a dot eval with torch inference mode, do the forward pass, calculate the loss,\nprint out what's happening. And then we do it again, again, again, for another epoch in a range.\nNow I'm kidding. We'll just leave it there. We'll just leave it there. But that's the\nunofficial pytorch optimization loop song. We created some device agnostic code so that we could\nmake the calculations on the same device as what our model is because the models also using device\nagnostic code. And so now we've got to evaluate our models. We've looked at the loss and the test\nlost here. And we know that our models loss is going down. But what does this actually equate to\nwhen it makes predictions? That's what we're most interested in, right? And we've looked at the\nparameters. They're pretty close to the ideal parameters. So at the end of last video, I issued\nyou the challenge to making and evaluating predictions to make some predictions and plot them. I hope\nyou gave it a shot. Let's see what it looks like together. Hey, so turn the model into evaluation\nmode. Why? Because every time we're making predictions or inference, we want our model to be in a\nvowel mode. And every time we're training, we want our model to be in training mode. And then we're\ngoing to make predictions on the test data, because we train on the train data, and we evaluate our\nmodel on the test data data that our model has never actually seen, except for when it makes\npredictions. With torch inference mode, we turn on inference mode whenever we make inference or\npredictions. So we're going to set Y threads equal to model one, and the test data goes in here.\nLet's have a look at what the Y threads look like. Wonderful. So we've got a tensor here. It shows\nus that they're still on the device CUDA. Why is that? Well, that's because previously we set the\nmodel one to the device, the target device, the same with the test data. So subsequently,\nour predictions are also on the CUDA device. Now, let's bring in the plot predictions function here.\nSo check out our model predictions visually. We're going to adhere to the data explorer's motto\nof visualize visualize visualize plot predictions. And predictions are going to be set to\nequals Y threads. And let's have a look. How good do these look? Oh, no.\nOh, we've got another error type error. Can't convert CUDA device type tensor to NumPy.\nOh, of course. Look what we've done. So our plot predictions function, if we go back up,\nwhere did we define that? What does our plot predictions function use? It uses matplotlib,\nof course, and matplotlib works with NumPy, not pytorch. And NumPy is CPU based. So of course,\nwe're running into another error down here, because we just said that our predictions are on the CUDA\ndevice. They're not on the CPU. They're on a GPU. So it's giving us this helpful information here.\nUse tensor dot CPU to copy the tensor to host memory first. So this is our tensor. Let's call\ndot CPU and see what happens then. Is that going to go to CPU? Oh, my goodness. Look at that.\nLook at that. Go the linear layer. The red dots, the predictions are basically on top of the testing\ndata. That is very exciting. Now again, you may not get the exact same numbers here, and that is\nperfectly fine. But the direction should be quite similar. So your red dots should be basically on\ntop of the green dots, if not very slightly off. But that's okay. That's okay. We just want to focus\non the direction here. So thanks to the power of back propagation here and gradient descent,\nour models random parameters have updated themselves to be as close as possible to the ideal parameters.\nAnd now the predictions are looking pretty darn good for what we're trying to predict.\nBut we're not finished there. We've just finished training this model. What would happen if our\nnotebook disconnected right now? Well, that wouldn't be ideal, would it? So in the next part,\nwe're going to move on to 6.5, saving, and loading a trained model. So I'm going to give you a\nchallenge here as well, is to go ahead and go back and refer to this code here, saving model\nin PyTorch, loading a PyTorch model, and see if you can save model one, the state dictionary of\nmodel one, and load it back in and get something similar to this. Give that a shot, and I'll see you\nin the next video. Welcome back. In the last video, we saw the power of the torch.nn.linear layer,\nand back propagation and gradient descent. And we've got some pretty darn good predictions\nout of our model. So that's very exciting. Congratulations. You've now trained two machine\nlearning models. But it's not over yet. We've got to save and load our trained model. So I\nissued you the challenge in the last video to try and save and load the model yourself. I hope\nyou gave that a go. But we're going to do that together in this video. So we're going to start\nby importing path because we would like a file path to save our model to. And the first step we're\ngoing to do is create models directory. We don't have to recreate this because I believe we already\nhave one. But I'm going to put the code here just for completeness. And this is just so if you\ndidn't have a models directory, this would create one. So model path is going to go to path\nmodels. And then we'd like to model path dot maker, we're going to call maker for make directory.\nWe'll set parents equal to true. And if it exists, okay, that'll also be true. So we won't get an error.\nOh my gosh, Google collab. I didn't want that. We won't get an error if it already exists.\nAnd two, we're going to create a model save path. So if you recall that pytorch objects in general\nhave the extension of what? There's a little pop quiz before we get to the end of this sentence.\nSo this is going to be pytorch workflow for this module that we're going through. This one here,\nchapter 01 pytorch workflow model one. And they usually have the extension dot PT for pytorch or\nPT H for pytorch as well. I like PT H. But just remember, sometimes you might come across slightly\ndifferent versions of that PT or PT H. And we're going to create the model save name or the save\npath. It's probably a better way to do it is going to be model path. And then we can use because we're\nusing the path lib module from Python, we can save it under model name. And so if we look at this,\nwhat do we get model save path? We should get Oh, path is not defined. Oh, too many capitals here,\nDaniel. The reason why I'm doing these in capitals is because oftentimes hyper parameters such as epochs\nin machine learning are set as hyper parameters LR could be learning rate. And then you could have\nas well model name equals Yeah, yeah, yeah. But that's just a little bit of nomenclature trivia for\nlater on. And model save path, we've done that. Now we're going to save the model state dictionary\nrather than the whole model, save the model state deck, which you will find the pros and cons of\nin where in the pytorch documentation for saving and loading model, which was a little bit of extra\ncurriculum for a previous video. But let's have a look at our model save path will print it out.\nAnd we'll go torch save, we'll set the object that we're trying to save to equal model one dot state\ndeck, which is going to contain our trained model parameters. We can inspect what's going on in here,\nstate deck. They'll show us our model parameters. Remember, because we're only using a single linear\nlayer, we only have two parameters. But in practice, when you use a model with maybe hundreds of layers\nor tens of millions of parameters, viewing the state deck explicitly, like we are now,\nmight not be too viable of an option. But the principle still remains a state deck contains\nall of the models trained or associated parameters, and what state they're in. And the file path we're\ngoing to use is, of course, the model save path, which we've seen here is a POSIX path. Let's save\nour model. Wonderful saving model to this file path here. And if we have a look at our folder,\nwe should have two saved models now, beautiful to save models. This one for us from the workflow\nwe did before up here, saving a model in PyTorch, loading a PyTorch model. And now the one we've got,\nof course, model one is the one that we've just saved. Beautiful. So now let's load a model. We're\ngoing to do both of these in one video. Load a PyTorch model. You know what, because we've had a\nlittle bit of practice so far, and we're going to pick up the pace. So let's go loaded, let's call\nit, we'll create a new instance of loaded model one, which is, of course, our linear regression model\nV2, which is the version two of our linear regression model class, which subclasses, what?\nSubclasses and n.module. So if we go back here up here to where we created it. So linear regression\nmodel V2 uses a linear layer rather than the previous iteration of linear regression model,\nwhich we created right up here. If we go up to here, which explicitly defined the parameters,\nand then implemented a linear regression formula in the forward method, the difference between\nwhat we've got now is we use PyTorch's pre-built linear layer, and then we call that linear layer\nin the forward method, which is probably the far more popular way of building PyTorch models,\nis stacking together pre-built NN layers, and then calling them in some way in the forward method.\nSo let's load it in. So we'll create a new instance of linear regression model V2, and now what do\nwe do? We've created a new instance, I'm just going to get out of this, make some space for us.\nWe want to load the model state deck, the saved model one state deck, which is the state deck that\nwe just saved beforehand. So we can do this by going loaded model one, calling the load state\ndecked method, and then passing it torch dot load, and then the file path of where we saved that\nPyTorch object before. But the reason why we use the path lib is so that we can just call model\nsave path in here. Wonderful. And then let's check out what's going on. Or actually, we need to\nput the target model or the loaded model to the device. The reason being is because we're doing all\nour computing with device agnostic code. So let's send it to the device. And I think that'll be about\nit. Let's see if this works. Oh, there we go. Linear regression model V2 in features one,\nout features one, bias equals true. Wonderful. Let's check those parameters. Hey, next loaded model\none dot parameters. Are they on the right device? Let's have a look. Beautiful. And let's just check\nthe loaded state dictionary of loaded model one. Do we have the same values as we had previously?\nYes, we do. Okay. So to conclusively make sure what's going on, let's evaluate the loaded model.\nEvaluate loaded model, loaded model one. What do we do for making predictions? Or what do we do to\nevaluate? We call dot a vowel. And then if we're going to make some predictions, we use torch\ninference mode with torch inference mode. And then let's create loaded model one, threads\nequals loaded model one. And we'll pass it the test data. And now let's check for a quality\nbetween Y threads, which is our previous model one preds that we made up here, Y threads.\nAnd we're going to compare them to the fresh loaded model one preds. And should they be the same?\nYes, they are beautiful. And we can see that they're both on the device CUDA. How amazing is that? So\nI want to give you a big congratulations, because you've come such a long way. We've gone through\nthe entire PyTorch workflow from making data, preparing and loading it to building a model.\nAll of the steps that come in building a model, there's a whole bunch there, making predictions,\ntraining a model, we spent a lot of time going through the training steps. But trust me, it's\nworth it, because we're going to be using these exact steps all throughout the course. And in fact,\nyou're going to be using these exact steps when you build PyTorch models after this course. And\nthen we looked at how to save a model so we don't lose all our work, we looked at loading a model,\nand then we put it all together using the exact same problem, but in far less time. And as you'll\nsee later on, we can actually make this even quicker by functionalizing some of the code we've already\nwritten. But I'm going to save that for later. I'll see you in the next video, where I'm just\ngoing to show you where you can find some exercises and all of the extra curriculum I've been talking\nabout throughout this section 01 PyTorch workflow. I'll see you there. Welcome back. In the last\nvideo, we finished up putting things together by saving and loading our trained model, which is\nsuper exciting, because let's come to the end of the PyTorch workflow section. So now, this section\nis going to be exercises and extra curriculum, or better yet, where you can find them. So I'm\ngoing to turn this into markdown. And I'm going to write here for exercises and extra curriculum.\nRefer to. So within the book version of the course materials, which is at learnpytorch.io,\nwe're in the 01 section PyTorch workflow fundamentals. There'll be more here by the time you watch\nthis video likely. And then if we go down here, at the end of each of these sections, we've got\nthe table of contents over here. We've got exercises and extra curriculum. I listed a bunch of things\nthroughout this series of 01 videos, like what's gradient descent and what's back propagation. So\nI've got plenty of resources to learn more on that. There's the loading and saving PyTorch\ndocumentation. There's the PyTorch cheat sheet. There's a great article by Jeremy Howard for a\ndeeper understanding of what's going on in torch.nn. And there's, of course, the unofficial PyTorch\noptimization loop song by yours truly, which is a bit of fun. And here's some exercises. So\nthe exercises here are all based on the code that we wrote throughout section 01. So there's\nnothing in the exercises that we haven't exactly covered. And if so, I'll be sure to put a note\nin the exercise itself. But we've got create a straight line data set using the linear regression\nformula. And then build a model by subclassing and end up module. So for these exercises, there's an\nexercise notebook template, which is, of course, linked here. And in the PyTorch deep learning\nGitHub, if we go into here, and then if we go into extras, and if we go into exercises, you'll\nfind all of these templates here. They're numbered by the same section that we're in. This is PyTorch\nworkflow exercises. So if you wanted to complete these exercises, you could click this notebook\nhere, open in Google CoLab. I'll just wait for this to load. There we go. And you can start to\nwrite some code here. You could save a copy of this in your own Google Drive and go through this.\nIt's got some notes here on what you should be doing. You can, of course, refer to the text-based\nversion of them. They're all here. And then if you want an example of what some solutions look\nlike, now, please, I can't stress enough that I would highly, highly recommend trying the exercises\nyourself. You can use the book that we've got here. This is just all the code from the videos.\nYou can use this. You can use, I've got so many notebooks here now, you can use all of the code\nthat we've written here to try and complete the exercises. But please give them a go yourself.\nAnd then if you go back into the extras folder, you'll also find solutions. And this is just one\nexample solutions for section 01. But I'm going to get out of that so you can't cheat and look\nat the solutions first. But there's a whole bunch of extra resources all contained within\nthe PyTorch deep loaning repo, extras, exercises, solutions, and they're also in the book version\nof the course. So I'm just going to link this in here. I'm going to put this right at the bottom\nhere. Wonderful. But that is it. That is the end of the section 01 PyTorch workflow.\nSo exciting. We went through basically all of the steps in a PyTorch workflow,\ngetting data ready, turning into tenses, build or pick a model, picking a loss function on an\noptimizer. We built a training loop. We fit the model to the data. We made a prediction.\nWe evaluated our model. We improved through experimentation by training for more epochs.\nWe'll do more of this later on. And we saved and reload our trained model.\nBut that's going to finish 01. I will see you in the next section. Friends, welcome back.\nWe've got another very exciting module. You ready? Neural network classification with\nPyTorch. Now combining this module once we get to the end with the last one, which was\nregression. So remember classification is predicting a thing, but we're going to see this in a second.\nAnd regression is predicting a number. Once we've covered this, we've covered two of the\nthe biggest problems in machine learning, predicting a number or predicting a thing.\nSo let's start off with before we get into any ideas or code, where can you get help?\nFirst things first is follow along with the code. If you can, if in doubt, run the code.\nTry it for yourself. Write the code. I can't stress how important this is.\nIf you're still stuck, press shift, command, and space to read the doc string of any of the\nfunctions that we're running. If you are on Windows, it might be control. I'm on a Mac, so I put command\nhere. If you're still stuck, search for your problem. If an error comes up, just copy and paste\nthat into Google. That's what I do. You might come across resources like Stack Overflow or,\nof course, the PyTorch documentation. We'll be referring to this a lot again throughout this\nsection. And then finally, oh wait, if you're still stuck, try again. If in doubt, run the code.\nAnd then finally, if you're still stuck, don't forget, you can ask a question. The best place to\ndo so will be on the course GitHub, which will be at the discussions page, which is linked here.\nIf we load this up, there's nothing here yet, because as I record these videos, the course\nhasn't launched yet, but press new discussion. Talk about what you've got. Problem with XYZ.\nLet's go ahead. Leave a video number here and a timestamp, and that way, we'll be able to help\nyou out as best as possible. So video number, timestamp, and then your question here, and you\ncan select Q&A. Finally, don't forget that this notebook that we're about to go through is based\non chapter two of the Zero to Mastery Learn PyTorch for deep learning, which is neural network\nclassification with PyTorch. All of the text-based code that we're about to write is here. That\nwas a little spoiler. And don't forget, this is the home page. So my GitHub repo slash PyTorch\ndeep learning for all of the course materials, everything you need will be here. So that's very\nimportant. How can you get help? But this is the number one. Follow along with the code and try\nto write it yourself. Well, with that being said, when we're talking about classification,\nwhat is a classification problem? Now, as I said, classification is one of the main problems of\nmachine learning. So you probably already deal with classification problems or machine learning\npowered classification problems every day. So let's have a look at some examples. Is this email\nspam or not spam? Did you check your emails this morning or last night or whenever? So chances are\nthat there was some sort of machine learning model behind the scenes. It may have been a neural\nnetwork. It may have not that decided that some of your emails won't spam. So to Daniel,\nat mrdberg.com, hey, Daniel, this steep learning course is incredible. I can't wait to use what\nI've learned. Oh, that's such a nice message. If you want to send that email directly to me,\nyou can. That's my actual email address. But if you want to send me this email, well, hopefully\nmy email, which is hosted by some email service detects this as spam because although that is a\nlot of money and it would be very nice, I think if someone can't spell too well, are they really\ngoing to pay me this much money? So thank you email provider for classifying this as spam. And now\nbecause this is one thing or another, not spam or spam, this is binary classification. So in this\ncase, it might be one here and this is a zero or zero or one. So one thing or another, that's binary\nclassification. If you can split it into one thing or another, binary classification. And then we\nhave an example of say we had the question, we asked our photos app on our smartphone or whatever\ndevice you're using. Is this photo of sushi steak or pizza? We wanted to search our photos for every\ntime we've eaten sushi or every time we've eaten steak or every time we've eaten pizza far out\nand this looks delicious. But this is multi class classification. Now, why is this? Because we've\ngot more than two things. We've got 123. And now this could be 10 different foods. It could be 100\ndifferent foods. It could be 1000 different categories. So the image net data set, which is a popular\ndata set for computer vision, image net, we go to here, does it say 1000 anywhere, 1k or 1000?\nNo, it doesn't. But if we go image net 1k, download image net data, maybe it's here.\nIt won't say it, but you just, oh, there we go, 1000 object classes. So this is multi class\nclassification because it has 1000 classes, that's a lot, right? So that's multi class classification,\nmore than one thing or another. And finally, we might have multi label classification,\nwhich is what tags should this article have when I first got into machine learning, I got these\ntwo mixed up a whole bunch of times. Multi class classification has multiple classes such as sushi\nsteak pizza, but assigns one label to each. So this photo would be sushi in an ideal world. This is\nsteak and this is pizza. So one label to each. Whereas multi label classification means you could\nhave multiple different classes. But each of your target samples such as this Wikipedia article,\nwhat tags should this article have? It may have more than one label. It might have three labels,\nit might have 10 labels. In fact, what if we went to the Wikipedia page for deep learning Wikipedia\nand does it have any labels? Oh, there we go. Where was that? I mean, you can try this yourself.\nThis is just the Wikipedia page for deep learning. There is a lot, there we go categories deep\nlearning, artificial neural networks, artificial intelligence and emerging technologies. So that\nis an example. If we wanted to build a machine learning model to say, read all of the text in\nhere and then go tell me what are the most relevant categories to this article? It might come up\nwith something like these. In this case, because it has one, two, three, four, it has multiple labels\nrather than just one label of deep learning, it could be multi label classification. So we'll go\nback. But there's a few more. These will get you quite far in the world of classification.\nSo let's dig a little deeper on binary versus multi class classification. You may have already\nexperienced this. So in my case, if I search on my phone in the photos app for photos of a dog,\nit might come here. If I search for photos of a cat, it might come up with this. But if I wanted\nto train an algorithm to detect the difference between photos of these are my two dogs.\nAren't they cute? They're nice and tired and they're sleeping like a person. This is seven.\nNumber seven, that's her name. And this is Bella. This is a cat that me and my partner rescued.\nAnd so I'm not sure what this cat's name is actually. So I'd love to give it a name, but I can't.\nSo binary classification, if we wanted to build an algorithm, we wanted to feed it, say, 10,000\nphotos of dogs and 10,000 photos of cats. And then we wanted to find a random image on the\ninternet and pass it through to our model and say, hey, is this a dog or is this a cat? It would\nbe binary classification because the options are one thing or another dog or cat. But then for\nmulti-class classification, let's say we've been working on a farm and we've been taking some photos\nof chickens because they groovy, right? Well, we updated our model and added some chicken photos\nin there. We would now be working with a multi-class classification problem because we've got more\nthan one thing or another. So let's jump in to what we're going to cover. This is broadly,\nby the way, because this is just text on a page. You know, I like to just write code of what we're\nactually doing. So we're going to look at the architecture of a neural network classification\nmodel. We're going to check what the input shapes and output shapes of a classification model are\nfeatures and labels. In other words, because remember, machine learning models, neural networks\nlove to have numerical inputs. And those numerical inputs often come in tenses. Tenses have different\nshapes, depending on what data you're working with. We're going to see all of this in code, creating\ncustom data to view, fit and predict on. We're going to go back through our steps in modeling.\nWe covered this a fair bit in the previous section, but creating a model for neural network classification.\nIt's a little bit different to what we've done, but not too out landishly different. We're going to\nsee how we can set up a loss function and an optimizer for a classification model. We'll\nrecreate a training loop and a evaluating loop or a testing loop. We'll see how we can save and\nload our models. We'll harness the power of nonlinearity. Well, what does that even mean? Well,\nif you think of what a linear line is, what is that? It's a straight line. So you might be\nable to guess what a nonlinear line looks like. And then we'll look at different classification\nevaluation methods. So ways that we can evaluate our classification models. And how are we going\nto do all of this? Well, of course, we're going to be part cook, part chemist, part artist, part\nscience. But for me, I personally prefer the cook side of things because we're going to be cooking\nup lots of code. So in the next video, before we get into coding, let's do a little bit more on\nwhat are some classification inputs and outputs. I'll see you there. Welcome back. In the last\nvideo, we had a little bit of a brief overview of what a classification problem is. But now,\nlet's start to get more hands on by discussing what the actual inputs to a classification problem\nlook like and the outputs look like. And so let's say we had our beautiful food photos from before,\nand we were trying to build this app here called maybe food vision to understand what\nfoods are in the photos that we take. And so what might this look like? Well, let's break it down\nto inputs, some kind of machine learning algorithm, and then outputs. In this case,\nthe inputs we want to numerically represent these images in some way, shape or form. Then we want\nto build a machine learning algorithm. Hey, one might actually exist. We're going to see this later\non in the transfer learning section for our problem. And then we want some sort of outputs. And in\nthe case of food vision, we want to know, okay, this is a photo of sushi. And this is a photo of\nsteak. And this is a photo of pizza. You could get more hands on and technical and complicated, but\nwe're just going to stick with single label multi class classification. So it could be a sushi photo,\nit could be a steak photo, or it could be a pizza photo. So how might we numerically represent\nthese photos? Well, let's just say we had a function in our app that every photo that gets taken\nautomatically gets resized into a square into 224 width and 224 height. This is actually quite a\ncommon dimensionality for computer vision problems. And so we've got the width dimension, we've got\nthe height, and then we've got this C here, which isn't immediately recognizable. But in the case\nof pictures, they often get represented by width, height color channels. And the color channels is\nred, green and blue, which is each pixel in this image has some value of red, green or blue, that\nmakes whatever color is displayed here. And this is one way that we can numerically represent an\nimage by taking its width, its height and color channels, and whatever number makes up this\nparticular image. We're going to see this later on when we work with computer vision problems.\nSo we create a numerical encoding, which is the pixel values here. Then we import the pixel values\nof each of these images into a machine learning algorithm, which is often already exists. And if\nit doesn't exist for our particular problem, hey, well, we're learning the skills to build them now,\nwe could use pytorch to build a machine learning algorithm for this. And then outputs, what might\nthese look like? Well, in this case, these are prediction probabilities, which the outputs of\nmachine learning models are never actually discrete, which means it is definitely pizza.\nIt will give some sort of probability value between zero and one for say the closer to one,\nthe more confident our model is that it's going to be pizza. And the closer to zero is means that,\nhey, this photo of pizza, let's say this one, and we're trying to predict sushi. Well,\nit doesn't think that it's sushi. So it's giving it quite a low value here. And then the same for\nsteak, but it's really high, the value here for pizza. We're going to see this hands on. And then\nit's the opposite here. So it might have got this one wrong. But with more training and more data,\nwe could probably improve this prediction. That's the whole idea of machine learning,\nis that if you adjust the algorithm, if you adjust the data, you can improve your predictions. And so\nthe ideal outputs that we have here, this is what our models going to output. But for our case of\nbuilding out food vision, we want to bring them back to. So we could just put all of these numbers\non the screen here, but that's not really going to help people. We want to put out labels of what's\ngoing on here. So we can write code to transfer these prediction probabilities into these labels\ntoo. And so how did these labels come about? How do these predictions come about? Well,\nit comes from looking at lots of different samples. So this loop, we could keep going,\nimprove these, find the ones where it's wrong, add more images here, train the model again,\nand then make our app better. And so if we want to look at this from a shape perspective,\nwe want to create some tenses for an image classification example. So we're building food vision.\nWe've got an image again, this is just reiterating on some of the things that we've discussed.\nWe've got a width of 224 and a height of 224. This could be different. This could be 300, 300.\nThis could be whatever values that you decide to use. Then we numerically encoded in some way,\nshape or form. We use this as the inputs to our machine learning algorithm, because of what?\nComputers and machine learning algorithms, they love numbers. They can find patterns in here\nthat we couldn't necessarily find. Or maybe we could, if you had a long enough time,\nbut I'd rather write an algorithm to do it for me. Then it has some outputs,\nwhich comes in the formal prediction probabilities, the closer to one, the more confident model is\nand saying, hey, I'm pretty damn confident that this is a photo of sushi. I don't think it's a\nphoto of steak. So I'm giving that zero. It might be a photo of pizza, but I don't really think so.\nSo I'm giving it quite a low prediction probability. And so if we have a look at what the shapes are\nfor our tenses here, if this doesn't make sense, don't worry. We're going to see the code to do\nall of this later on. But for now, we're just focusing on a classification input and output.\nThe big takeaway from here is numerical encoding, outputs and numerical encoding. But we want to\nchange these numerical codings from the outputs to something that we understand, say the word sushi.\nBut this tensor may be batch size. We haven't seen what batch size is. That's all right. We're\ngoing to cover it. Color channels with height. So this is represented as a tensor of dimensions.\nIt could be none here. None is a typical value for a batch size, which means it's blank. So when\nwe use our model and we train it, all the code that we write with pytorch will fill in this behind\nthe scenes. And then we have three here, which is color channels. And we have 224, which is the width.\nAnd we have 224 as well, which is the height. Now there is some debate in the field on the ordering.\nWe're using an image as our particular example here on the ordering of these shapes. So say,\nfor example, you might have height width color channels, typically width and height come together\nin this order. Or they're just side by side in the tensor in terms of their whether dimension\nappears. But color channels sometimes comes first. That means after the batch size or at the end here.\nBut pytorch, the default for now is color channels with height, though you can write code to change\nthis order because tenses are quite flexible. And so or the shape could be 32 for the batch size,\nthree, two, two, four, two, two, four, because 32 is a very common batch size. And you don't believe me?\nWell, let's go here. Yarn LeCoon 32 batch size. Now what is a batch size? Great tweet. Just keep\nthis in mind for later on. Training with large mini batches is bad for your health. More importantly,\nit's bad for your test error. Friends don't let friends use mini batches larger than 32. So this\nis quite an old tweet. However, it still stands quite true. Because like today, it's 2022 when\nI'm recording these videos, there are batch sizes a lot larger than 32. But 32 works pretty darn\nwell for a lot of problems. And so this means that if we go back to our slide, that if we use\na batch size of 32, our machine learning algorithm looks at 32 images at a time. Now why does it do\nthis? Well, because sadly, our computers don't have infinite compute power. In an ideal world,\nwe look at thousands of images at a time, but it turns out that using a multiple of eight here\nis actually quite efficient. And so if we have a look at the output shape here, why is it three?\nWell, because we're working with three different classes, one, two, three. So we've got shape equals\nthree. Now, of course, as you could imagine, these might change depending on the problem you're working\nwith. So say if we just wanted to predict if a photo was a cat or a dog, we still might have this\nsame representation here because this is the image representation. However, the shape here\nmay be two, or will be two because it's cat or dog, rather than three classes here, but a little\nbit confusing as well with binary classification, you could have the shape just being one here.\nBut we're going to see this all hands on. Just remember, the shapes vary with whatever problem\nyou're working on. The principle of encoding your data as a numerical representation stays the same\nfor the inputs. And the outputs will often be some form of prediction probability based on whatever\nclass you're working with. So in the next video, right before we get into coding, let's just discuss\nthe high level architecture of a classification model. And remember, architecture is just like\nthe schematic of what a neural network is. I'll see you there. Welcome back. In the last video,\nwe saw some example classification inputs and outputs. The main takeaway that the inputs to a\nclassification model, particularly a neural network, want to be some form of numerical\nrepresentation. And the outputs are often some form of prediction probability. So let's discuss\nthe typical architecture of a classification model. And hey, this is just going to be text\non a page, but we're going to be building a fair few of these. So we've got some hyper parameters\nover here. We've got binary classification. And we've got multi class classification. Now,\nthere are some similarities between the two in terms of what problem we're working with.\nBut there also are some differences here. And by the way, this has all come from, if we go\nto the book version of the course, we've got what is a classification problem. And we've got\narchitecture of a classification neural network. So all of this text is available at learnpytorch.io\nand in section two. So we come back. So the input layer shape, which is typically\ndecided by the parameter in features, as you can see here, is the same of number of features.\nSo if we were working on a problem, such as we brought it to predict whether someone had\nheart disease or not, we might have five input features, such as one for age, a number for age,\nit might be in my case, 28, sex could be male, height, 180 centimeters. If I've been growing\novernight, it's really close to 177. Wait, well, it depends on how much I've eaten, but it's around\nabout 75 kilos and smoking status, which is zero. So it could be zero or one, because remember,\nwe want numerical representation. So for sex, it could be zero for males, one for female,\nheight could be its number, weight could be its number as well. All of these numbers could be\nmore, could be less as well. So this is really flexible. And it's a hyper parameter. Why? Because\nwe decide the values for each of these. So in the case of our image prediction problem,\nwe could have in features equals three for number of color channels. And then we go\nhidden layers. So there's the blue circle here. I forgot that this was all timed and colorful.\nBut let's just discuss hidden layers. Each of these is a layer and n dot linear and n dot linear\nand n dot relu and n dot linear. So that's the kind of the syntax you'll see in PyTorch for a\nlayer is nn dot something. Now, there are many different types of layers in this in PyTorch.\nIf we go torch and n, basically everything in here is a layer in a neural network. And then if we\nlook up what a neural network looks like, neural network, recall that all of these are different\nlayers of some kind of mathematical operation. Input layer, hidden layer, you could have as\nmany hidden layers as you want. Do we have ResNet architecture? The ResNet architecture,\nsome of them have 50 layers. Look at this. Each one of these is a layer. And this is only the\n34 layer version. I mean, there's ResNet 152, which is 152 layers. We're not at that yet.\nBut we're working up the tools to get to that stage. Let's come back to here. The neurons per\nhidden layer. So we've got these, out features, the green circle, the green square. Now, this is,\nif we go back to our neural network picture, this is these. Each one of these little things\nis a neuron, some sort of parameter. So if we had 100, what would that look like? Well,\nwe'd have a fairly big graphic. So this is why I like to teach with code because you could customize\nthis as flexible as you want. So behind the scenes, PyTorch is going to create 100 of these little\ncircles for us. And within each circle is what? Some sort of mathematical operation.\nSo if we come back, what do we got next? Output layer shape. So this is how many output features\nwe have. So in the case of binary classification is one, one class or the other. We're going to\nsee this later on. Multi-class classification is you might have three output features,\none per class, e.g., one for food, person or dog, if you're building a food, person or dog,\nimage classification model. Hidden layer activation, which is, we haven't seen these yet.\nRelu, which is a rectified linear unit, but can be many others because PyTorch, of course, has what?\nHas a lot of non-linear activations. We're going to see this later on. Remember, I'm kind of planting\nthe seed here. We've seen what a linear line is, but I want you to imagine what a non-linear line is.\nIt's going to be a bit of a superpower for our classification problem. What else do we have?\nOutput activation. We haven't got that here, but we'll also see this later on, which could be\nsigmoid for, which is generally sigmoid for binary classification, but softmax for multi-class\nclassification. A lot of these things are just names on a page. We haven't seen them yet.\nI like to teach them as we see them, but this is just a general overview of what we're going to\ncover. Loss function. What loss function or what does a loss function do? It measures how\nwrong our model's predictions are compared to what the ideal predictions are. So for binary\nclassification, we might use binary cross entropy loss in PyTorch, and for multi-class\nclassification, we might just use cross entropy rather than binary cross entropy. Get it?\nBinary classification? Binary cross entropy? And then optimizer. SGD is stochastic gradient descent.\nWe've seen that one before. Another common option is the atom optimizer, and of course,\nthe torch.optim package has plenty more options. So this is an example multi-class classification\nproblem. This network here. Why is that? And we haven't actually seen an end up sequential,\nbut as you could imagine, sequential stands for it just goes through each of these steps.\nSo multi-class classification, because it has three output features, more than one thing or\nanother. So three for food, person or dog, but going back to our food vision problem,\nwe could have the input as sushi, steak, or pizza. So we've got three output features,\nwhich would be one prediction probability per class of image. We have three classes, sushi,\nsteak, or pizza. Now, I think we've done enough talking here, and enough just pointing to text\non slides. How about in the next video? Let's code. I'll see you in Google CoLab.\nWelcome back. Now, we've done enough theory of what a classification problem is, what the inputs\nand outputs are and the typical architecture. Let's get in and write some code. So I'm going to\nget out of this, and going to go to colab.research.google.com, so we can start writing some PyTorch code.\nI'm going to click new notebook. We're going to start exactly from scratch. I'm going to name this\nsection two, and let's call it neural network classification with PyTorch. I'm going to put\nunderscore video, because I'll just show you, you'll see this in the GitHub repo. But for all the\nvideo notebooks, the ones that I write code during these videos that you're watching, the exact code\nis going to be saved on the GitHub repo under video notebooks. So there's 00, which is the\nfundamentals, and there's the workflow underscore video. But the reference notebook with all the\npretty pictures and stuff is in the main folder here. So PyTorch classification that I pi and b\nare actually, maybe we'll just rename it that PyTorch classification. But we know it's with\nneural networks. PyTorch classification. Okay, and let's go here. We'll add a nice title. So O2,\nneural network classification with PyTorch. And so we'll remind ourselves, classification is a\nproblem of predicting whether something is one thing or another. And there can be multiple\nthings as the options, such as email, spam or not spam, photos of dogs or cats or pizza or\nsushi or steak. Lots of talk about food. And then I'm just going to link in here, this resource,\nbecause this is the book version of the course. These are what the videos are based off. So book\nversion of this notebook. And then all the resources are in here. All other resources\nin the GitHub, and then stuck. Ask a question here, which is under the discussions tab. We'll\ncopy that in here. That way we've got everything linked and ready to go. But as always, what's our\nfirst step in our workflow? This is a little test. See if you remember. Well, it's data, of course,\nbecause all machine learning problems start with some form of data. We can't write a machine\nlearning algorithm to learn patterns and data that doesn't exist. So let's do this video. We're\ngoing to make some data. Of course, you might start with some of your own that exists. But for now,\nwe're going to focus on just the concepts around the workflow. So we're going to make our own\ncustom data set. And to do so, I'll write the code first, and then I'll show you where I get it from.\nWe're going to import the scikit loan library. One of the beautiful things about Google Colab\nis that it has scikit loan available. You're not sure what scikit loan is. It's a very popular\nmachine learning library. PyTorch is mainly focused on deep learning, but scikit loan is\nfocused on a lot of things around machine learning. So Google Colab, thank you for having scikit\nloan already installed for us. But we're going to import the make circles data set. And rather\nthan talk about what it does, let's see what it does. So make 1000 samples. We're going to go N\nsamples equals 1000. And we're going to create circles. You might be wondering why circles. Well,\nwe're going to see exactly why circles later on. So X and Y, we're going to use this variable.\nHow would you say nomenclature as capital X and Y. Why is that? Because X is typically a matrix\nfeatures and labels. So let's go here. Mate circles. And we're going to make N samples. So 1000 different\nsamples. We're going to add some noise in there. Just put a little bit of randomness. Why not?\nYou can increase this as you want. I found that 0.03 is fairly good for what we're doing. And\nthen we're going to also pass in the random state variable, which is equivalent to sitting a random\nor setting a random seed. So we're flavoring the randomness here. Wonderful. So now let's\nhave a look at the length of X, which should be what? And length of Y. Oh, we don't have Y\nunderscore getting a bit trigger happy with this keyboard here. 1000. So we have 1000 samples of\nX caught with 1000 or paired with 1000 samples of Y features labels. So let's have a look at the\nfirst five of X. So print first five samples of X. And then we'll put in here X. And we can index\non this five because we're adhering to the data, explorer's motto of visualize visualize visualize\nfirst five samples of Y. And then we're going to go why same thing here.\nWonderful. Let's have a look. Maybe we'll get a new line in here. Just so\nlooks a bit better. Wonderful. So numerical. Our samples are already numerical. This is one of\nthe reasons why we're creating our own data set. We'll see later on how we get non numerical data\ninto numbers. But for now, our data is numerical, which means we can learn it with our model or\nwe can build a model to learn patterns in here. So this sample has the label of one. And this\nsample has the label of one as well. Now, how many features do we have per sample? If I highlight\nthis line, how many features is this? It would make it a bit easier if there was a comma here,\nbut we have two features of X, which relates to one label of Y. And so far, we've only seen,\nlet's have a look at all of Y. We've got zero on one. So we've got two classes. What does this\nmean? Zero or one? One thing or another? Well, it looks like binary classification to me,\nbecause we've got only zero or only one. If there was zero, one, two, it would be\nmulti class classification, because we have more than two things. So let's X out of this.\nLet's keep going and do a little bit more data exploration. So how about we make a data frame?\nWith pandas of circle data. There is truly no real definite way of how to explore data.\nFor me, I like to visualize it multiple different ways, or even look at random samples. In the case\nof large data sets, such as images or text or whatnot. If you have 10 million samples, perhaps\nvisualizing them one by one is not the best way to do so. So random can help you out there.\nSo we're going to create a data frame, and we can insert a dictionary here. So I'm going to call\nthe features in this part of X, X1, and these are going to be X2. So let's say I'll write some code\nto index on this. So everything in the zero index will be X1. And everything in the first index,\nthere we go, will be X2. Let me clean up this code. This should be on different lines,\nenter. And then we've got, let's put in the label as Y. So this is just a dictionary here.\nSo X1 key to X0. X2, a little bit confusing because of zero indexing, but X feature one,\nX feature two, and the label is Y. Let's see what this looks like. We'll look at the first 10 samples.\nOkay, beautiful. So we've got X1, some numerical value, X2, another numerical value, correlates\nto or matches up with label zero. But then this one, 0442208, and negative that number matches up\nwith label zero. So I can't tell what the patterns are just looking at these numbers. You might be\nable to, but I definitely can't. We've got some ones. All these numbers look the same to me. So\nwhat can we do next? Well, how about we visualize, visualize, visualize, and instead of just numbers\nin a table, let's get graphical this time, visualize, visualize, visualize. So we're going to bring in\nour friendly mapplotlib, import mapplotlib, which is a very powerful plotting library. I'm just\ngoing to add some cells here. So we've got some space, mapplotlib.pyplot as PLT. That's right.\nWe've got this plot.scatter. We're going to do a scatterplot equals X. And we want the first index.\nAnd then Y is going to be X as well. So that's going to appear on the Y axis. And then we want to\ncolor it with labels. We're going to see what this looks like in a second. And then the color map,\nC map stands for color map is going to be plot dot color map PLT. And then red, yellow, blue,\none of my favorite color outputs. So let's see what this looks like. You ready?\nAh, there we go. There's our circles. That's a lot better for me. So what do you think we're\ngoing to try and do here? If this is our data and we're working on classification,\nwe're trying to predict if something is one thing or another. So our problem is we want to\ntry and separate these two circles. So say given a number here or given two numbers and X one\nand an X two, which are coordinates here, we want to predict the label. Is it going to be a blue\ndot or is it going to be a red dot? So we're working with binary classification. So we have\none thing or another. Do we have a blue dot or a red dot? So this is going to be our toy data here.\nAnd a toy problem is, let me just write this down. This is a common thing that you'll also\nhear in machine learning. Note, the data we're working with is often referred to as a toy data set,\na data set that is small enough to experiment on, but still sizable enough to practice the\nfundamentals. And that's what we're really after in this notebook is to practice the fundamentals\nof neural network classification. So we've got a perfect data set to do this. And by the way,\nwe've got this from scikit-learn. So this little function here made all of these samples for us.\nAnd how could you find out more about this function here? Well, you could go scikit-learn\nclassification data sets. There are actually a few more in here that we could have done.\nI just like the circle one. Toy data sets, we saw that. So this is like a toy box of different\ndata sets. So if you'd like to learn more about some data sets that you can have a look in here\nand potentially practice on with neural networks or other forms of machine learning models from\nscikit-learn, check out this scikit-learn. I can't speak highly enough. I know this is a pie-torch\ncourse. We're not focused on this, but they kind of all come together in terms of the machine\nlearning and deep learning world. You might use something from scikit-learn, like we've done here,\nto practice something. And then you might use pie-torch for something else, like what we're\ndoing here. Now, with that being said, what are the input and output shapes of our problem?\nHave a think about that. And also have a think about how we'd split this into training and test.\nSo give those a go. We covered those concepts in some previous videos,\nbut we'll do them together in the next video. I'll see you there.\nWelcome back. In the last video, we made some classification data so that we can\npractice building a neural network in pie-torch to separate the blue dots from the red dots.\nSo let's keep pushing forward on that. And I'll just clean up here a little bit,\nbut where are we in our workflow? What have we done so far? Well, we've got our data ready a\nlittle bit. We haven't turned it into tenses. So let's do that in this video, and then we'll\nkeep pushing through all of these. So in here, I'm going to make this heading 1.1. Check input\nand output shapes. The reason we're focused a lot on input and output shapes is why,\nbecause machine learning deals a lot with numerical representations as tenses. And input and output\nshapes are some of the most common errors, like if you have a mismatch between your input and\noutput shapes of a certain layer of an output layer, you're going to run into a lot of errors\nthere. So that's why it's good to get acquainted with whatever data you're using, what are the\ninput shapes and what are the output shapes you'd like. So in our case, we can go x dot shape\nand y dot shape. So we're working with NumPy arrays here if we just look at x. That's what the\nmake circles function is created for us. We've got an array, but as our workflow says,\nwe'd like it in tenses. If we're working with PyTorch, we want our data to be represented as\nPyTorch tenses of that data type. And so we've got a shape here, we've got a thousand samples,\nand x has two features, and y has no features. It's just a single number. It's a scalar. So it\ndoesn't have a shape here. So there's a thousand samples of y, thousand samples of x, two samples\nof x equals one y label. Now, if you're working with a larger problem, you might have a thousand\nsamples of x, but x is represented by 128 different numbers, or 200 numbers, or as high as you want,\nor just 10 or something like that. So just keep in mind that this number is quite flexible of how\nmany features represent a label. Why is the label here? But let's keep going. So view the first\nexample of features and labels. So let's make it explicit with what we've just been discussing.\nWe'll write some code to do so. We'll get the first sample of x, which is the zero index,\nand we'll get the first sample of y, which is also the zero index. We could get really anyone\nbecause they're all of the same shape. But print values for one sample of x. What does this equal?\nX sample, and the same for y, which is y sample. And then we want to go print f string for one\nsample of x. We'll get the shape here. X sample dot shape, and the same for y, and then we'll get\ny sample dot shape. Beautiful. What's this going to do? Well, we've got one sample of x. So this\nsample here of these numbers, we've got a lot going on here. 75424625 and 0231 48074. I mean,\nyou can try to find some patterns in those. If you do, all the best here, and the same for y. So this\nis, we have the y sample, this correlates to a number one, a label of one. And then we have\nshapes for one sample of x, which is two. So we have two features for y. It's a little bit confusing\nhere because y is a scalar, which doesn't actually have a shape. It's just one value. So for me,\nin terms of speaking this, teaching it out loud, we'll be two features of x trying to predict\none number for y. And so let's now create another heading, which is 1.2. Let's get our data into\ntenses, turn data into tenses. We have to convert them from NumPy. And we also want to create\ntrain and test splits. Now, even though we're working with a toy data set here, the principle\nof turning data into tenses and creating train and test splits will stay around for almost any\ndata set that you're working with. So let's see how we can do that. So we want to turn data\ninto tenses. And for this, we need to import torch, get pytorch and we'll check the torch version.\nIt has to be at least 1.10. And I might just put this down in the next cell. Just make sure we can\nimport pytorch. There we go, 1.10 plus CUDA 111. If your version is higher than that, that is okay.\nThe code below should still work. And if it doesn't, let me know. So x equals torch dot\nfrom NumPy. Why are we doing this? Well, it's because x is a NumPy array. And if we go x dot,\ndoes it have a d type attribute float 64? Can we just go type or maybe type? Oh, there we go.\nNumPy and DRA. We can just go type x. NumPy and DRA. So we want it in a torch tensor. So we're\ngoing to go from NumPy. We saw this in the fundamental section. And then we're going to change it into\ntype torch dot float. A float is an alias for float 32. We could type the same thing. These two are\nequivalent. I just going to type torch float for writing less code. And then we're going to go\nthe same with why torch from NumPy. Now, why do we turn it into a torch float? Well, that's\nbecause if you recall, the default type of NumPy arrays is, if we go might just put out this in\na comma x dot D type is float 64. There we go. However, pytorch, the default type is float 32.\nSo we're changing it into pytorch's default type. Otherwise, if we didn't have this little\nsection of code here dot type torch dot float, our tensors would be of float 64 as well. And that\nmay cause errors later on. So we're just going for the default data type within pytorch. And so\nnow let's have a look at the first five values of x and the first five values of y. What do we\nhave? Beautiful. We have tensor data types here. And now if we check the data type of x and we\ncheck the data type of y, what do we have? And then one more, we'll just go type x. So we have\nour data into tensors. Wonderful. But now so it's torch dot tensor. Beautiful. But now we would like\ntraining and test sets. So let's go split data into training and test sets. And a very, very popular\nway to split data is a random split. So before I issued the challenge of how you would split this\ninto a training and test set. So because these data points are kind of scattered all over the\nplace, we could split them randomly. So let's see what that looks like. To do so, I'm going to\nuse our faithful scikit learn again. Remember how I said scikit learn has a lot of beautiful methods\nand functions for a whole bunch of different machine learning purposes. Well, one of them is\nfor a train test split. Oh my goodness, pytorch I didn't want auto correct there. Train test split.\nNow you might be able to guess what this does. These videos are going to be a battle between me and\ncode labs auto correct. Sometimes it's good. Other times it's not. So we're going to set this code\nup. I'm going to write it or we're going to write it together. So we've got x train for our training\nfeatures and X tests for our testing features. And then we also want our training labels and\nour testing labels. That order is the order that train test split works in. And then we have train\ntest split. Now if we wrote this function and we wanted to find out more, I can press command\nship space, which is what I just did to have this. But truly, I don't have a great time reading all\nof this. You might. But for me, I just like going train test split. And possibly one of the first\nfunctions that appears, yes, is scikit learn. How good is that? So scikit learn dot model selection\ndot train test split. Now split arrays or matrices into random train and test subsets. Beautiful.\nWe've got a code example of what's going on here. You can read what the different parameters do.\nBut we're going to see them in action. This is just another example of where machine learning\nlibraries such as scikit learn, we've used matplotlib, we've used pandas, they all interact\ntogether to serve a great purpose. But now let's pass in our features and our labels.\nThis is the order that they come in, by the way. Oh, and we have the returns splitting. So the\norder here, I've got the order goes x train x test y train y test took me a little while to\nremember this order. But once you've created enough training test splits with this function,\nyou kind of know this off by heart. So just remember features first train first and then labels.\nAnd we jump back in here. So I'm going to put in the test size parameter of 0.2.\nThis is percentage wise. So let me just write here 0.2 equals 20% of data will be test.\nAnd 80% will be train. If we wanted to do a 50 50 split, that kind of split doesn't usually\nhappen, but you could go 0.5. But the test size says, hey, how big and percentage wise do you want\nyour test data to be? And so behind the scenes train test split will calculate what's 20% of\nour x and y samples. So we'll see how many there is in a second. But let's also put a random state\nin here. Because if you recall back in the documentation, train test split splits data\nrandomly into random train and test subsets. And random state, what does that do for us? Well,\nthis is a random seed equivalent of very similar to torch dot manual seed. However, because we are\nusing scikit learn, setting torch dot manual seed will only affect pytorch code rather than\nscikit learn code. So we do this so that we get similar random splits. As in, I get a similar\nrandom split to what your random split is. And in fact, they should be exactly the same. So let's\nrun this. And then we'll check the length of x train. And length of x test. So if we have 1000\ntotal samples, and I know that because above in our make circles function, we said we want\n1000 samples, that could be 10,000, that could be 100. That's the beauty of creating your own\ndata set. And we have length y train. If we have 20% testing values, how many samples are going\nto be dedicated to the test sample, 20% of 1000 years, 200, and 80%, which is because training is\ngoing to be training here. So 100 minus 20% is 80%. So 80% of 1000 years, let's find out.\nRun all beautiful. So we have 800 training samples, 200 testing samples. This is going to be the\ndata set that we're going to be working with. So in the next video, we've now got training and\ntest sets, we've started to move through our beautiful pytorch workflow here. We've got our\ndata ready, we've turned it into tenses, we've created a training and test split. Now it's time\nto build or pick a model. So I think we're still in the building phase. Let's do that in the next\nvideo. Welcome back. In the last video, we split our data into training and test sets. And because\nwe did 80 20 split, we've got about 800 samples to train on, and 200 samples to test on. Remember,\nthe training set is so that the model can learn patterns, patterns that represent this data set\nhere, the circles data set, red dots or blue dots. And the test data set is so that we can\nevaluate those patterns. And I took a little break before, but you can tell that because my\nnotebook is disconnected. But if I wanted to reconnect it, what could I do? We can go here,\nrun time, run before that's going to run all of the cells before. It shouldn't take too long\nbecause we haven't done any large computations. But this is good timing because we're up to part\ntwo, building a model. And so there's a fair few steps here, but nothing that we haven't covered\nbefore, we're going to break it down. So let's build a model to classify our blue and red dots.\nAnd to do so, we want to tenses. I want to not tenses. That's all right. So let me just make\nsome space here. There we go. So number one, let's set up device agnostic code. So we get in the\nhabit of creating that. So our code will run on an accelerator. I can't even spell accelerator.\nIt doesn't matter. You know what I mean? GPU. If there is one. Two. What should we do next?\nWell, we should construct a model. Because if we want to build a model, we need a model.\nConstruct a model. And we're going to go by subclassing and then dot module.\nNow we saw this in the previous section, we subclassed and then module. In fact, all models\nin PyTorch subclass and end up module. And let's go define loss function and optimizer.\nAnd finally, good collabs auto correct is not ideal. And then we'll create a training\nand test loop. Though this will probably be in the next section. We'll focus on building a model\nhere. And of course, all of these steps are in line with what they're in line with this.\nSo we don't have device agnostic code here, but we're just going to do it enough so that we have\na habit. These are the main steps. Pick or build a pre-trained model, suit your problem, pick a\nloss function and optimizer, build a training loop. So let's have a look. How can we start this off?\nSo we will import PyTorch. And and then we've already done this, but we're going to do it anyway\nfor completeness, just in case you wanted to run your code from here, import and then. And we're\ngoing to make device agnostic code. So we'll set the device equal to CUDA if torch dot CUDA\nis available else CPU, which will be the default. The CPU is the default. If there's no GPU,\nwhich means that CUDA is available, all of our PyTorch code will default to using the CPU\ndevice. Now we haven't set up a GPU yet so far. You may have, but as you see, my target device is\ncurrently CPU. How about we set up a GPU? We can go into here runtime change runtime type\nGPU. And I'm going to click save. Now this is going to restart the runtime and reconnect.\nSo once it reconnects beautiful, we could actually just run this code cell here.\nThis is going to set up the GPU device, but because we're only running this cell, if we were to just\nset up X train, we've not been defined. So because we restarted our runtime, let's run all or we can\njust run before. So this is going to rerun all of these cells here. And do we have X train now?\nLet's have a look. Wonderful. Yes, we do. Okay, beautiful. So we've got device agnostic code.\nIn the next video, let's get on to constructing a model. I'll see you there.\nWelcome back. In the last video, we set up some device agnostic code. So this is going to come in\nlater on when we send our model to the target device, and also our data to the target device.\nThis is an important step because that way, if someone else was able to run your code or you\nwere to run your code in the future, because we've set it up to be device agnostic,\nquite a fault it will run on the CPU. But if there's an accelerator present,\nwell, that means that it might go faster because it's using a GPU rather than just using a CPU.\nSo we're up to step two here construct a model by subclassing and in module. I think we're going to\nwrite a little bit of text here just to plan out the steps that we're doing. Now we've\nset up device agnostic code. Let's create a model that we're going to break it down. We've got some\nsub steps up here. We're going to break it down even this one down into some sub-sub steps. So number\none is we're going to subclass and then got module. And a reminder here, I want to make some space,\njust so we're coding in about the middle of the page. So almost all models in pytorch,\nsubclass, and then got module because there's some great things that it does for us behind the\nscenes. And step two is we're going to create two and then dot linear layers. And we want these\nthat are capable to handle our data. So that are capable of handling the shapes of our data.\nStep three, we want to define or defines a forward method. Why do we want to define a forward method?\nWell, because we're subclassing an end dot module, right? And so the forward method defines a forward\nmethod that outlines the forward pass or forward computation of the model. And number four, we want\nto instantiate, well, this doesn't really have to be the part of creating it, but we're going to do\nanyway, and instantiate an instance of our model class and send it to the target device. So I'm\ngoing to be a couple of little different steps here, but nothing too dramatic that we haven't really\ncovered before. So let's go number one, construct a model that subclasses an end dot module.\nSo I'm going to code this all out. Well, we're going to code this all out together. And then we'll\ngo back through and discuss it, and then maybe draw a few pictures or something to check out\nwhat's actually happening. So circle model V one, because we're going to try and split some circles,\nred and blue circles. This is our data up here. This is why it's called circle model, because we're\ntrying to separate the blue and red circle using a neural network. So we've subclassed an end dot\nmodule. And when we create a class in Python, we'll create a constructor here, a net, and then\nput in super dot underscore a net. And then inside the constructor, we're going to create our\nlayers. So this is number two, create two and then linear layers, capable of handling the shapes\nof our data. So I'm going to write this down here to create two, two, and then dot linear layers, capable\nof handling the shapes of our data. And so if we have a look at X train, what are the shapes here?\nWhat's the input shape? Because X train is our features, right? Now features are going to go\ninto our model. So we have 800 training samples. This is the first number here of size two each.\nSo 800 of these and inside each is two numbers. Again, depending on the data set you're working\nwith, your features may be 100 in length, a vector of 100, or maybe a different size tensor all\ntogether, or there may be millions. It really depends on what data set you're working with.\nBecause we're working with a simple data set, we're going to focus on that. But the principal\nis still the same. You need to define a neural network layer that is capable of handling your\ninput features. So we're going to make layer one equals an n dot linear. And then if we wanted\nto find out what's going on an n n dot linear, we could run shift command space on my computer,\nbecause it's a Mac, maybe shift control space if you're on Windows. So we're going to define the\nn features. What would n features be here? Well, we just decided that our X has two features.\nSo n features are going to be two. And now what is the out features? This one is a little bit tricky.\nSo in our case, we could have out features equal to one if we wanted to just pass a single linear\nlayer, but we want to create two linear layers here. So why would out features be one? Well,\nthat's because if we have a look at the first sample of Y train, we would want us to input,\nor maybe we'll look at the first five. We want to map one sample of X to one sample of Y and Y\nhas a shape of one. Oh, well, really, it's nothing because it's a scalar, but we would still put\none here so that it outputs just one number. But we're going to change this up. We're going to put\nit into five and we're going to create a second layer. Now, this is an important point of joining\ntogether neural networks in features here. What do you think the in features of our second layer is\ngoing to be? If we've produced an out feature of five here, now this number is arbitrary. We could\ndo 128. We could do 256. Generally, it's multiples of 8, 64. We're just doing five now because we're\nkeeping it nice and simple. We could do eight multiples of eight is because of the efficiency\nof computing. I don't know enough about computer hardware to know exactly why that's the case,\nbut that's just a rule of thumb in machine learning. So the in features here has to match up with the\nout features of a previous layer. Otherwise, we'll get shape mismatch errors. And so let's go here\nout features. So we're going to treat this as the output layer. So this is the out features equals\none. So takes in two features and upscales to five features. So five numbers. So what this does,\nwhat this layer is going to do is take in these two numbers of X, perform an end up linear. Let's\nhave a look at what equation it does. An end up linear is going to perform this function here\non the inputs. And it's going to upscale it to five features. Now, why would we do that? Well,\nthe rule of thumb here, because this is denoted as a hidden unit, or how many hidden neurons there\nare. The rule of thumb is that the more hidden features there are, the more opportunity our model\nhas to learn patterns in the data. So to begin with, it only has two numbers to learn patterns on,\nbut at when we upscale it to five, it has five numbers to learn patterns on. Now, you might think,\nwhy don't we just go straight to like 10,000 or something? But there is like an upper limit here\nto sort of where the benefits start to trail off. We're just using five because it keeps it nice\nand simple. And then the in features of the next layer is five, so that these two line up. We're\ngoing to map this out visually in a moment, but let's keep coding. We've got in features two for\nX. And now this is the output layer. So takes in five features from previous layer and outputs\na single feature. And now this is same shape. Same shape as why. So what is our next step? We\nwant to define a Ford method, a Ford computation of Ford pass. So the Ford method is going to\ndefine the Ford computation. And as an input, it's going to take X, which is some form of data.\nAnd now here's where we can use layer one and layer two. So now let's just go return.\nOr we'll put a note here of what we're doing. Three, we're going to go define a Ford method\nthat outlines the Ford pass. So Ford, and we're going to return. And here's some notation we\nhaven't quite seen yet. And then we're going to go self layer two. And inside the brackets we'll\nhave self layer one inside those brackets. We're going to have X. So the way this goes is X goes\ninto layer one. And then the output of layer one goes into layer two. So whatever data we have,\nso our training data, X train goes into layer one performs the linear calculation here. And then\nit goes into layer two. And then layer two is going to output, go to the output. So X is the input,\nlayer one computation layer two output. So we've done that. Now let's do step four, which is\ninstantiate an instance of our model class. And send it to the target device. So this is our model\nclass, circle model V zero. We're just going to create a model because it's the first model we've\ncreated up this section. Let's call it model zero. And we're going to go circle model V one. And then\nwe're going to go to two. And we're going to pass in device, because that's our target device.\nLet's now have a look at model zero. And then Oh, typo. Yeah, classic.\nWhat did we get wrong here? Oh, did we not pass in self self? Oh, there we go.\nLittle typo classic. But the beautiful thing about creating a class here is that we could put\nthis into a Python file, such as model dot pi. And then we wouldn't necessarily have to rewrite\nthis all the time, we could just call it. And so let's just check what the vice it's on.\nSo target device is CUDA, because we've got a GPU, thank you, Google Colab. And then if we wanted\nto, let's go next model zero dot parameters, we'll call the parameters, and then we'll go device.\nCUDA beautiful. So that means our models parameters are on the CUDA device. Now we've covered enough\ncode in here for this video. So if you want to understand it a little bit more, go back through\nit. But we're going to come back in the next video and make it a little bit more visual. So I'll see\nyou there. Welcome back. In the last video, we did something very, very exciting. We created our\nfirst multi layer neural network. But right now, this is just code on a page. But truly, this is\nwhat the majority of building machine learning models in PyTorch is going to look like. You're\ngoing to create some layers, or a simple or as complex as you like. And then you're going to\nuse those layers in some form of Ford computation to create the forward pass. So let's make this a\nlittle bit more visual. If we go over to the TensorFlow playground, and now TensorFlow is another\ndeep learning framework similar to PyTorch, it just allows you to write code such as this,\nto build neural networks, fit them to some sort of data to find patterns and data,\nand then use those machine learning models in your applications. But let's create this. Oh,\nby the way, this is playground.tensorFlow.org. This is a neural network that we can train in\nthe browser if we really wanted to. So that's pretty darn cool. But we've got a data set here,\nwhich is kind of similar to the data set that we're working with. We have a look at our circles one.\nLet's just say it's close enough. It's circular. That's what we're after. But if we increase this,\nwe've got five neurons now. We've got two features here, X1 and X2. Where is this\nreminding you of what's happening? There's a lot of things going on here that we haven't covered\nyet, but don't worry too much. We're just focused on this neural network here. So we've got some\nfeatures as the input. We've got five hidden units. This is exactly what's going on with the model\nthat we just built. We pass in X1 and X2, our values. So if we go back to our data set,\nthese are X1 and X2. We pass those in. So we've got two input features. And then we pass them to a\nhidden layer, a single hidden layer, with five neurons. What have we just built? If we come down\ninto here to our model, we've got in features two, out features five. And then that feeds into\nanother layer, which has in features five and out features one. So this is the exact same model\nthat we've built here. Now, if we just turn this back to linear activation, because we're sticking\nwith linear for now, we'll have a look at different forms of activation functions later on. And maybe\nwe put the learning rate, we've seen the learning rate to 0.01. We've got epochs here, got classification.\nAnd we're going to try and fit this neural network to this data. Let's see what happens.\nOh, the test loss, it's sitting about halfway 0.5. So about 50% loss. So if we only have two\nclasses and we've got a loss of 50%, what does that mean? Well, the perfect loss was zero.\nAnd the worst loss was one. Then we just divide one by two and get 50%. But we've only got two\nclasses. So that means if our model was just randomly guessing, it would get a loss of about 0.5,\nbecause you could just randomly guess whatever data point belongs to blue or orange in this case.\nSo in a binary classification problem, if you have the same number of samples in each class,\nin this case, blue dots and orange dots, randomly guessing will get you about 50%. Just like tossing\na coin, toss a coin 100 times and you get about 50 50 might be a little bit different, but it's\naround about that over the long term. So we've just fit for 3000 epochs. And we're still not getting\nany better loss. Hmm. I wonder if that's going to be the case for our neural network. And so to\ndraw this in a different way, I'm going to come to a little tool called fig jam, which is just a\nwhiteboard that we can put shapes on and it's based on the browser. So this is going to be nothing\nfancy. It's going to be a simple diagram. Say this is our input. And I'm going to make this green\nbecause my favorite color is green. And then we're going to have, let's make some different\ncolored dots. I want a blue dot here. So this can be dot one, and dot two, I'll put another dot\nhere. I'll zoom out a little so we have a bit more space. Well, maybe that was too much. 50%\nlooks all right. So let me just move this around, move these up a little. So we're building a neural\nnetwork here. This is exactly what we just built. And so we'll go here. Well, maybe we'll put this\nas input X one. So this will make a little bit more sense. And then we'll maybe we can copy this.\nNow this is X two. And then we have some form of output. Let's make this one. And we're going to\ncolor this orange. So output. Right. So you can imagine how we got connected dots here.\nThey will connect these. So our inputs are going to go through all of these. I wonder if I can\ndraw here. Okay, this is going to be a little bit more complex, but that's all right. So this\nis what we've done. We've got two input features here. And if we wanted to keep drawing these,\nwe could all of these input features are going to go through all of these hidden units that we\nhave. I just drew the same arrow twice. That's okay. But this is what's happening in the forward\ncomputation method. It can be a little bit confusing for when we coded it out. Why is that? Well,\nfrom here, it looks like we've only got an input layer into a single hidden layer in the blue.\nAnd an output layer. But truly, this is the same exact shape. You get the point. And then all of\nthese go to the output. But we're going to see this computationally later on. So whatever data set\nyou're working with, you're going to have to manufacture some form of input layer. Now this\nmay be you might have 10 of these if you have 10 features. Or four of them if you have four\nfeatures. And then if you wanted to adjust these, well, you could increase the number of hidden\nunits or the number of out features of a layer. What just has to match up is that the layer it's\ngoing into has to have a similar shape as the what's coming out of here. So just keep that in mind\nas you're going on. And in our case, we only have one output. So we have the output here,\nwhich is why. So this is a visual version. We've got the TensorFlow playground. You could play\naround with that. You can change this to increase. Maybe you want five hidden layers with five neurons\nin each. This is a fun way to explore. This is a challenge, actually, go to playground.tensorflow.org,\nreplicate this network and see if it fits on this type of data. What do you think, will it?\nWell, we're going to have to find out in the next few videos. So I'm going to show you in the\nnext video another way to create the network that we just created. This one here with even less\ncode than what we've done before. I'll see you there. Welcome back. In the last video, what we\ndiscussed, well, actually, in the previous video to last, we coded up this neural network here,\ncircle model V zero. By subclassing an end or module, we created two linear layers, which are\ncapable of handling the shape of our data in features two because why we have two X features.\nOut features were upscaling the two features to five so that it gives our network more of a\nchance to learn. And then because we've upscaled it to five features, the next subsequent layer\nhas to be able to handle five features as input. And then we have one output feature because that's\nthe same shape as our Y here. Then we got a little bit visual by using the TensorFlow\nplayground. Did you try out that challenge, make five in layers with five neurons? Did it work?\nAnd then we also got a little bit visual in Figma as well. This is just another way of\nvisualizing different things. You might have to do this a fair few times when you first start\nwith neural networks. But once you get a bit of practice, you can start to infer what's going on\nthrough just pure code. So now let's keep pushing forward. How about we replicate this\nwith a simpler way? Because our network is quite simple, that means it only has two layers.\nThat means we can use. Let's replicate the model above using nn.sequential. And I'm going to code\nthis out. And then we can look up what nn.sequential is. But I think you'll be able to comprehend what's\nhappening by just looking at it. So nn, which is torch.nn. We can do torch.nn, but we've already\nimported nn. We're going to call nn.sequential. And then we're going to go nn.linear. And what\nwas the in features of our nn.linear? Well, it was two because we have two in features. And then\nwe're going to replicate the same out features. Remember, we could customize this to whatever we\nwant 10, 100, 128. I'm going to keep it at five, nice and simple. And then we go nn.linear. And\nthe in features of this next layer is going to be five because the out features of the previous\nlayer was five. And then finally, the out features here is going to be one because we want one y\nvalue to our two x features. And then I'm going to send that to the device. And then I'm going to\nhave a look at model zero. So this is, of course, going to override our previous model zero. But\nhave a look. The only thing different is that this is from the circle model V zero class. We\nsubclassed an n dot module. And the only difference is the name here. This is just from sequential.\nAnd so can you see what's going on here? So as you might have guessed sequential,\nit implements most of this code for us behind the scenes. Because we've told it that it's going\nto be sequential, it's just going to go, hey, step the code through this layer, and then step\nthe code through this layer. And outputs basically the same model, rather than us creating our own\nforward method, you might be thinking, Daniel, why don't you show us this earlier? That looks\nlike such an easy way to create a neural network compared to this. Well, yes, you're 100% right.\nThat is an easier way to create a neural network. However, the benefit of subclassing, and that's\nwhy I started from here, is that when you have more complex operations, such as things you'd\nlike to construct in here, and a more complex forward pass, it's important to know how to\nbuild your own subclasses of nn dot module. But for simple straightforward stepping through\neach layer one by one, so this layer first, and then this layer, you can use nn dot sequential.\nIn fact, we could move this code up into here. So we could do this self dot, we'll call this\ntwo linear layers equals nn dot sequential. And we could have layer one, we could go self,\nself dot layer one. And or actually, we'll just recode it, we'll go nn dot linear. So it's so\nit's the same code is what we've got below in features. If I could type that'll be great,\nn features equals two, out features equals five. And then we go nn dot linear. And then we go\nn features equals what equals five, because it has to line up out features equals one.\nAnd then we've got two linear layers. And then if we wanted to get rid of this, return\nto linear layers, and we'll pass it X remake it. There we go. Well, because we've created these as\nwell, let's get rid of that. Beautiful. So that's the exact same model, but just using nn dot\nsequential. Now I'm going to get rid of this so that our code is not too verbose. That means a lot\ngoing on. But this is the flexibility of PyTorch. So just keep in mind that there's a fair few ways\nto make a model. The simplest is probably sequential. And then subclassing is this is a little bit\nmore complicated than what we've got. But this can extend to handle lot more complex neural networks,\nwhich you'll likely have to be building later on. So let's keep pushing forward. Let's see what\nhappens if we pass some data through here. So we'll just rerun this cell to make sure we've got\nour model zero instantiated. We'll make some predictions with the model. So of course, if we\nhave a look at our model zero dot state deck, oh, this will be a good experiment. So look at this.\nSo we have weight, a weight tensor, a bias tensor, a weight tensor, and a bias tensor. So this is\nfor the first of the zeroth layer, these two here with the zero dot, and then the one dot weight is\nfour, of course, the first index layer. Now have a look inside here. Now you see how out features\nis five. Well, that's why our bias parameter has five values here. And the same thing for this weight\nvalue here. And the weight value here, why is this have 10 samples? One, two, three, four, five, six,\nseven, eight, nine, 10, because two times five equals 10. So this is just with a simple two layer\nnetwork, look at all the numbers that are going on behind the scenes. Imagine coding all of these\nby hand. Like there's something like 20 numbers or something here. Now we've only done two layers\nhere. Now the beauty of this is that in the previous section, we created all of the weight\nand biases using an end dot parameter and random values. You'll notice that these are all random\ntwo. Again, if yours are different to mine, don't worry too much, because they're going to be started\nrandomly and we haven't set a random seed. But the thing to note here is that PyTorch is creating\nall of these parameters for us behind the scenes. And now when we do back propagation and gradient\ndescent, when we code our training loop, the optimizer is going to change all of these values\never so slightly to try and better fit or better represent the data so that we can split our two\ncircles. And so you can imagine how verbose this could get if we had say 50 layers with 128 different\nfeatures of each. So let's change this up, see what happens. Watch how quickly the numbers get\nout of hand. Look at that. We just changed one value and look how many parameters our model has.\nSo you might be able to calculate that by hand, but I personally don't want to. So we're going to\nlet PyTorch take care of a lot of that for us behind the scenes. So for now we're keeping it simple,\nbut that's how we can crack our models open and have a look at what's going on. Now that was a\nlittle detour. It's time to make some predictions with random numbers. I just wanted to highlight\nthe fact that our model is in fact instantiated with random numbers here. So the untrained threads\nmodel zero, we're going to pass in X test. And of course, we have to send the test data to the\ndevice. Otherwise, if it's on a different device, we'll get errors because PyTorch likes to make\ncalculations on the same device. So we'll go print. Let's do a nice print statement of length of\npredictions. We're going to go length or then untrained threads, we'll pass that in there.\nAnd then we'll go, oh no, we need to squiggle. And then we'll go shape. Shape is going to be\nuntrained spreads dot shape. So this is again, following the data explorer's motto of visualize,\nvisualize, visualize. And sometimes print is one of the best ones to do so. So length of test samples,\nyou might already know this, or we've already checked this together, haven't we? X test.\nAnd then we're going to get the shape, which is going to be X test dot shape. Wonderful. And then\nwe're going to print. What's our little error here? Oh no, collabs tricking me. So let's go first\n10 predictions. And we're going to go untrained threads. So how do you think these predictions will\nfare? They're doing it with random numbers. And what are we trying to predict again? Well,\nwe're trying to predict whether a dot is a red dot or a blue dot or zero or one. And then we'll go\nfirst 10 labels is going to be, we'll get this on the next line. And we'll go Y test.\nBeautiful. So let's have a look at this untrained predictions. So we have length of predictions\nis 200. Length of test samples is 200. But the shapes are different. What's going on here?\nY test. And let's have a look at X test. Oh, well, I better just have a look at Y test.\nWhy don't we have a two there? Oh, I've done X test dot shape. Oh, let's test samples. That's\nokay. And then the predictions are one. Oh, yes. So Y test. Let's just check the first 10 X test.\nSo a little bit of clarification needed here with your shapes. So maybe we'll get this over here\nbecause I like to do features first and then labels. What did we miss here? Oh, X test 10\nand Y test. See, we're troubleshooting on the fly here. This is what you're going to do with\na lot of your code. So there's our test values. There's the ideal labels. But our predictions,\nthey don't look like our labels. What's going on here? We can see that they're on the CUDA device,\nwhich is good. We said that. We can see that they got gradient tracking. Oh, we didn't with\ntouch. We didn't do inference mode here. That's a poor habit of us. Excuse me. Let's inference\nmode this. There we go. So you notice that the gradient tracking goes away there. And so our\npredictions are nowhere near what our test labels are. But also, they're not even in the same like\nball park. Like these are whole numbers, one or zero. And these are all floats between one and\nzero. Hmm. So maybe rounding them. Will that do something? So where's our threads here? So\nwe go torch dot round. What happens there? Oh, they're all zero. Well, our model is probably\ngoing to get about 50% accuracy. Why is that? Because all the predictions look like they're\ngoing to be zero. And they've only got two options, basically head or tails. So when we create our\nmodel and when we evaluate it, we want our predictions to be in the same format as our labels. But\nwe're going to cover some steps that we can take to do that in a second. What's important to take\naway from this is that there's another way to replicate the model we've made above using\nnn dot sequential. We've just replicated the same model as what we've got here. And n dot\nsequential is a simpler way of creating a pytorch model. But it's limited because it literally\njust sequentially steps through each layer in order. Whereas in here, you can get as creative as you\nwant with the forward computation. And then inside our model, pytorch has behind the scenes\ncreated us some weight and bias tensors for each of our layers with regards to the shapes that\nwe've set. And so the handy thing about this is that if we got quite ridiculous with our layers,\npytorch would still do the same thing behind the scenes, create a whole bunch of random numbers for\nus. And because our numbers are random, it looks like our model isn't making very good predictions.\nBut we're going to fix this in the next few videos when we move on to\nfitting the model to the data and making a prediction. But before we do that, we need to\npick up a loss function and an optimizer and build a training loop. So let's get on to these two things.\nWelcome back. So over the past few videos, we've been setting up a classification model to deal\nwith our specific shape of data. Now recall, depending on the data set that you're working\nwith will depend on what layers you use for now we're keeping it simple and n dot linear is one\nof the most simple layers in pytorch. We've got two input features, we're upscaling that to five\noutput features. So we have five hidden units, and then we have one output feature. And that's in line\nwith the shape of our data. So two features of x equals one number for y. So now let's continue\non modeling with where we're up to. We have build or pick a model. So we've built a model. Now we\nneed to pick a loss function and optimizer. We're getting good at this. So let's go here,\nset up loss function and optimizer. Now here comes the question. If we're working on classification\npreviously, we used, let's go to the next one, and an dot L one loss for regression, which is\nMAE mean absolute error, just a heads up that won't necessarily work with a classification problem.\nSo which loss function or optimizer should you use? So again, this is problem specific. But with\na little bit of practice, you'll get used to using different ones. So for example, for regression,\nyou might want, which is regressions predicting a number. And I know it can get fusing because\nit looks like we're predicting a number here, we are essentially predicting a number. But this\nrelates to a class. So for regression, you might want MAE or MSE, which is mean absolute\nabsolute error, or mean squared error. And for classification, you might want binary cross entropy\nor categorical cross entropy, which is sometimes just referred to as cross entropy. Now, where would\nyou find these things out? Well, through the internet, of course. So you could go, what is\nbinary cross entropy? I'm going to leave you this for your extra curriculum to read through this.\nWe've got a fair few resources here. Understanding binary cross entropy slash log loss\nby Daniel Godoy. Oh, yes. Great first name, my friend. This is actually the article that I\nwould recommend to if you want to learn what's going on behind the scenes through binary cross\nentropy. For now, there's a lot of math there. We're going to be writing code to implement this. So\nPyTorch has done this for us. Essentially, what does a loss function do? Let's remind ourselves.\nGo down here. As a reminder, the loss function measures how wrong your models' predictions are.\nSo I also going to leave a reference here to I've got a little table here in the book version of\nthis course. So 0.2 neural network classification with PyTorch set up loss function and optimizer.\nSo we've got some example loss functions and optimizers here, such as stochastic gradient\ndescent or SGD optimizer, atom optimizer is also very popular. So I've got problem type here,\nand then the PyTorch code that we can implement this with. We've got binary cross entropy loss.\nWe've got cross entropy loss, mean absolute error, MAE, mean squared error, MSE. So you want to use\nthese two for regression. There are other different loss functions you could use, but these are some\nof the most common. That's what I'm focusing on, the most common ones that are going to get you\nthrough a fair few problems. We've got binary classification, multi-class classification. What\nare we working with? We're working with binary classification. So we're going to look at torch.nn\nBCE, which stands for binary cross entropy, loss with logits. What the hell is a logit?\nAnd BCE loss. Now this is confusing. Then trust me, when I first started using PyTorch,\nI got a little bit confused about why they have two here, but we're going to explore that anyway.\nSo what is a logit? So if you search what is a logit, you'll get this and you'll get statistics\nand you'll get the log odds formula. In fact, if you want to read more, I would highly encourage it.\nSo you could go through all of this. We're going to practice writing code for it instead.\nLuckily PyTorch does this for us, but logit is kind of confusing in deep learning. So if we go\nwhat is a logit in deep learning, it kind of means a different thing. It's kind of just a name of what\nyeah, there we go. What is the word logits in TensorFlow? As I said, TensorFlow is another\ndeep learning framework. So let's close this. What do we got? We've got a whole bunch of\ndefinitions here. Logits layer. Yeah. This is one of my favorites. In context of deep learning,\nthe logits layer means the layer that feeds into the softmax. So softmax is a form of activation.\nWe're going to see all of this later on because this is just words on a page right now. Softmax\nor other such normalization. So the output of the softmax are the probabilities for the\nclassification task and its input is the logit's layer. Whoa, there's a lot going on here. So let's\njust take a step back and get into writing some code. And for optimizers, I'm just going to complete\nthis. And for optimizers, two of the most common and useful are SGD and Adam. However, PyTorch\nhas many built in options. And as you start to learn more about the world of machine learning,\nyou'll find that if you go to torch.optim or torch.nn. So if we have.nn, what do we have in here?\nLoss functions. There we go. Beautiful. That's what we're after. L1 loss, which is MAE,\nMSC loss, cross entropy loss, CTC loss, all of these different types of loss here will depend\non the problem you're working on. But I'm here to tell you that for regression and classification,\ntwo of the most common of these. See, this is that confusion again. BCE loss, BCE with\nlogit's loss. What the hell is a logit? My goodness. Okay, that's enough. And Optim,\nthese are different optimizers. We've got probably a dozen or so here. Algorithms.\nAdd a delta, add a grad. Adam, this can be pretty full on when you first get started. But for now,\njust stick with SGD and the atom optimizer. They're two of the most common. Again, they may not\nperform the best on every single problem, but they will get you fairly far just knowing those.\nAnd then you'll pick up some of these extra ones as you go. But let's just get rid of all of,\nmaybe we'll, so I'll put this in here, this link. So we'll create our loss function. For the loss\nfunction, we're going to use torch.nn.bce with logit's loss. This is exciting. For more on what\nbinary cross entropy, which is BCE, a lot of abbreviations in machine learning and deep learning\nis check out this article. And then for a definition on what a logit is, we're going to see a\nlogit in a second in deep learning. Because again, deep learning is one of those fields,\na machine learning, which likes to be a bit rebellious, you know, likes to be a bit different\nfrom the pure mathematics type of fields and statistics in general. It's this beautiful\ngestaltism and for different optimizers, see torch dot opt in. But we've covered a few of these\nthings before. And finally, I'm going to put up here, and then for some common choices of loss\nfunctions and optimizers. Now, don't worry too much. This is why I'm linking all of these extra\nresources. A lot of this is covered in the book. So as we just said, set up loss function,\noptimizer, we just talked about these things. But I mean, you can just go to this book website\nand reference it. Oh, we don't want that. We want this link. Come on, I knew you can't even\ncopy and paste. How are you supposed to code? I know I've been promising code this whole time,\nso let's write some. So let's set up the loss function. What did we say it was? We're going to\ncall it L O double S F N for loss function. And we're going to call B C E with logit's loss. So B\nC E with logit's loss. This has the sigmoid activation function built in. And we haven't covered what\nthe sigmoid activation function is, but we are going to don't you worry about that built in.\nIn fact, if you wanted to learn what the sigmoid activation function is, how could you find out\nsigmoid activation function? But we're going to see it in action. Activation functions in neural\nnetworks. This is the beautiful thing about machine learning. There's so much stuff out there.\nPeople have written some great articles. You've got formulas here. PyTorch has implemented that\nbehind the scenes for us. So thank you, PyTorch. But if you recall, sigmoid activation function\nbuilt in, where did we discuss the architecture of a classification network? What do we have here?\nRight back in the zeroth chapter of this little online book thing that we heard here. Binary\nclassification. We have output activation. Oh, oh, look at that. So sigmoid torch dot sigmoid and\npytorch. All right. And then for multi class classification, we need the softmax. Okay. Names\non a page again, but this is just a reference table so we can keep coming back to. So let's just\nkeep going with this. I just want to highlight the fact that nn dot BCE loss also exists. So\nthis requires BCE loss equals requires inputs to have gone through the sigmoid activation function\nprior to input to BCE loss. And so let's look up the documentation. I'm going to comment that\nout because we're going to stick with using this one. Now, why would we stick with using this one?\nLet's check out the documentation, hey, torch dot nn. And I realized this video is all over the\nplace, but we're going to step back through BCE loss with logits. Did I even say this right?\nWith logits loss. So with I got the width around the wrong way. So let's check this out. So this\nloss combines a sigmoid layer with the BCE loss in one single class. So if we go back to the code,\nBCE loss is this. So if we combined an n dot sequential, and then we passed in an n dot sigmoid,\nand then we went and then dot BCE loss, we'd get something similar to this. But if we keep reading\nin the documentation, because that's just I just literally read that it combines sigmoid with BCE\nloss. But if we go back to the documentation, why do we want to use it? So this version is more\nnumerically stable than using a plain sigmoid by a BCE loss, followed by a BCE loss. As by\ncombining the operations into one layer, we take advantage of the log sum x trick for numerical\nstability, beautiful. So if we use this log function, loss function for our binary cross entropy,\nwe get some numeric stability. Wonderful. So there's our loss function. We've got the sigmoid\nactivation function built in. And so we're going to see the difference between them later on,\nlike in the flesh, optimizer, we're going to choose, hmm, let's stick with SGD, hey,\nold faithful stochastic gradient descent. And we have to set the parameters here, the parameters\nparameter params equal to our model parameters would be like, hey, stochastic gradient descent,\nplease update. If we get another code cell behind here, please update our model parameters model\nwith respect to the loss, because we'd like our loss function to go down. So these two are going\nto work in tandem again, when we write our training loop, and we'll set our learning rate to 0.1.\nWe'll see where that gets us. So that's what the optimizer is going to do. It's going to optimize\nall of these parameters for us, which is amazing. And the principal would be the same, even if there\nwas 100 layers here, and 10,000, a million different parameters here. So we've got a loss function,\nwe've got an optimizer. And how about we create an evaluation metric. So let's calculate\naccuracy at the same time. Because that's very helpful with classification problems is accuracy.\nNow, what is accuracy? Well, we could look up formula for accuracy. So true positive over true\npositive plus true negative times 100. Okay, let's see if we can implement something similar to that\njust using pure pytorch. Now, why would we want accuracy? Because the accuracy is out of 100\nexamples. What percentage does our model get right? So for example, if we had a coin toss,\nand we did 100 coin tosses in our model predicted heads 99 out of 100 times, and it was right\nevery single time, it might have an accuracy of 99%, because it got one wrong. So 99 out of 100,\nit gets it right. So dev accuracy FN accuracy function, we're going to pass it y true. So\nremember, any type of evaluation function or loss function is comparing the predictions to\nthe ground truth labels. So correct equals, this is going to see how many of our y true\nor y threads are correct. So torch equal stands for, hey, how many of these samples y true are\nequal to y pred? And then we're going to get the sum of that, and we need to get the item from\nthat because we want it as a single value in Python. And then we're going to calculate the\naccuracy, ACC stands for accuracy, equals correct, divided by the length of samples that we have\nas input. And then we're going to times that by 100, and then return the accuracy. Wonderful.\nSo we now have an accuracy function, we're going to see how all the three of these come into play\nwhen we write a training loop, which we might as we get started on the next few videos, hey,\nI'll see you there. Welcome back. In the last video, we discussed some different loss function\noptions for our classification models. So we learned that if we're working with binary cross\nentropy or binary classification problems, we want to use binary cross entropy. And pie torch\nhas two different times of binary cross entropy, except one is a bit more numerically stable.\nThat's the BCE with logit's loss, because it has a sigmoid activation function built in.\nSo that's straight from the pie to its documentation. And that for optimizer wise, we have a few\ndifferent choices as well. So if we check out this section here on the pie torch book, we have a\nfew different loss functions and optimizers for different problems and the pie torch code that\nwe can implement. But the premise is still the same across the board of different problems.\nThe loss function measures how wrong our model is. And the goal of the optimizer is to optimize\nthe model parameters in such a way that the loss function goes down. And we also implemented our\nown accuracy function metric, which is going to evaluate our models predictions using accuracy\nas an evaluation metric, rather than just loss. So let's now work on training a model.\nSo what should we do first? Well, do you remember the steps in a pie torch training loop?\nSo to train our model, we're going to need to build a training loop. So if you watch the video\non the pie torch, so if you can Google unofficial pie torch song, you should find my, there we go,\nthe unofficial pie torch optimization loop song. We're not going to watch that. That's going to\nbe a little tidbit for the steps that we're going to code out. But that's just a fun little jingle\nto remember these steps here. So if we go into the book section, this is number three train model,\nexactly where we're up to here. But we have pie torch training loop steps. Remember, for an\nepoch in a range, do the forward pass, calculate the loss, optimizer zero grand, loss backward,\noptimizer step, step, step. We keep singing this all day. You could keep reading those steps all\nday, but it's better to code them. But let's write this out. So forward pass to calculate the loss,\nthree, optimizer zero grad, four. What do we do? Loss backward. So back propagation,\nI'll just write that up in here back propagation. We've linked to some extra resources. If you'd\nlike to find out what's going on in back propagation, we're focused on code here, and then gradient\ndescent. So optimizer step. So build a training loop with the following steps. However, I've kind\nof mentioned a few things that need to be taken care of before we talk about the forward pass.\nSo we've talked about logits. We looked up what the hell is a logit. So if we go into this stack\noverflow answer, we saw machine learning, what is a logit? How about we see that? We need to\ndo a few steps. So I'm going to write this down. Let's get a bit of clarity about us, Daniel.\nWe're kind of all over the place at the moment, but that's all right. That's the exciting part\nof machine learning. So let's go from going from raw logits to prediction probabilities\nto prediction labels. That's what we want. Because to truly evaluate our model, we want to\nso let's write in here our model outputs going to be raw logit. So that's the definition of a\nlogit in machine learning and deep learning. You might read some few other definitions, but for us,\nthe raw outputs of our model, model zero are going to be referred to as logits. So then model zero,\nso whatever comes out of here are logits. So we can convert these logits into prediction probabilities\nby passing them to some kind of activation function, e.g. sigmoid for binary cross entropy\nand softmax for multi-class classification. I've got binary class e-fication. I have to\nsound it out every time I spell it for binary classification. So class e-fication. So we're\ngoing to see multi-class classification later on, but we want some prediction probabilities.\nWe're going to see what they look like in a minute. So we want to go from logits to prediction\nprobabilities to prediction labels. Then we can convert our model's prediction probabilities to\nprediction labels by either rounding them or taking the argmax.\nSo round is for binary classification and argmax will be for the outputs of the softmax activation\nfunction, but let's see it in action first. So I've called the logits are the raw outputs of our\nmodel with no activation function. So view the first five outputs of the forward pass\non the test data. So of course, our model is still instantiated with random values. So we're\ngoing to set up a variable here, y logits, and model zero, we're going to pass at the test data.\nSo x test, not text, two device, because our model is currently on our CUDA device and we need\nour test data on the same device or target device. Remember, that's why we're writing device\nagnostic codes. So this would work regardless of whether there's a GPU active or not. Let's have\na look at the logits. Oh, okay. Right now, we've got some positive values here. And we can see that\nthey're on the CUDA device. And we can see that they're tracking gradients. Now, ideally,\nwe would have run torch dot inference mode here, because we're making predictions. And the rule\nof thumb is whenever you make predictions with your model, you turn it into a vowel mode.\nWe just have to remember to turn it back to train when we want to train and you run torch dot\ninference mode. So we get a very similar set up here. We just don't have the gradients being\ntracked anymore. Okay. So these are called logits. The logits are the raw outputs of our model,\nwithout being passed to any activation function. So an activation function is something a little\nseparate from a layer. So if we come up here, we've used layer. So in the neural networks that we\nstart to build and the ones that you'll subsequently build are comprised of layers and activation\nfunctions, we're going to make the concept of an activation function a little bit more clear later\non. But for now, just treat it all as some form of mathematical operation. So if we were to pass\ndata through this model, what is happening? Well, it's going through the linear layer. Now recall,\nwe've seen this a few times now torch and then linear. If we pass data through a linear layer,\nit's applying the linear transformation on the incoming data. So it's performing this\nmathematical operation behind the scenes. So why the output equals the input x multiplied by a\nweight tensor a this could really be w which is transposed so that this is doing a dot product\nplus a bias term here. And then if we jump into our model state deck, we've got weight\nand we've got bias. So that's the formula that's happening in these two layers. It will be different\ndepending on the layer that we choose. But for now, we're sticking with linear. And so that the\nraw output of our data going through our two layers, the logits is going to be this information\nhere. However, it's not in the same format as our test data. And so if we want to make a comparison\nto how good our model is performing, we need apples to apples. So we need this in the same format\nas this, which is not of course. So we need to go to a next step. Let's use the sigmoid. So use the\nsigmoid activation function on our model logits. So why are we using sigmoid? Well, recall in a\nbinary classification architecture, the output activation is the sigmoid function here. So now\nlet's jump back into here. And we're going to create some predprobs. And what this stands for\non our model logits to turn them into prediction probabilities, probabilities. So why predprobs\nequals torch sigmoid, why logits? And now let's have a look at why predprobs. What do we get from\nthis? Oh, when we still get numbers on a page, goodness gracious me. But the important point\nnow is that they've gone through the sigmoid activation function, which is now we can pass these\nto a torch dot round function. Let's have a look at this torch dot round. And what do we get?\nPredprobs. Oh, the same format as what we've got here. Now you might be asking like, why don't we\njust put torch dot round here? Well, that's a little, this step is required to, we can't just do it on\nthe raw logits. We need to use the sigmoid activation function here to turn it into prediction\nprobabilities. And now what is a prediction probability? Well, that's a value usually between 0 and 1\nfor how likely our model thinks it's a certain class. And in the case of binary cross entropy,\nthese prediction probability values, let me just write this out in text. So for our prediction\nprobability values, we need to perform a range style rounding on them. So this is a decision\nboundary. So this will make more sense when we go why predprobs, if it's equal to 0.5 or greater\nthan 0.5, we set y equal to one. So y equal one. So class one, whatever that is, a red dot or a\nblue dot, and then why predprobs, if it is less than 0.5, we set y equal to zero. So this is class\nzero. You can also adjust this decision boundary. So say, if you wanted to increase this value,\nso anything over 0.7 is one. And below that is zero. But generally, most commonly, you'll find\nit split at 0.5. So let's keep going. Let's actually see this in action. So how about we\nrecode this? So find the predicted probabilities. And so we want no, sorry, we want the predicted\nlabels, that's what we want. So when we're evaluating our model, we want to convert the outputs of\nour model, the outputs of our model are here, the logits, the raw outputs of our model are\nlogits. And then we can convert those logits to prediction probabilities using the sigmoid function\non the logits. And then we want to find the predicted labels. So we go raw logits output of our model,\nprediction probabilities after passing them through an activation function, and then prediction labels.\nThis is the steps we want to take with the outputs of our model. So find the predicted labels.\nLet's go in here a little bit different to our regression problem previously, but nothing we can't\nhandle. Torch round, we're going to go y-pred-probs. So I like to name it y-pred-probs for prediction\nprobabilities and y-preds for prediction labels. Now let's go in full if we wanted to. So y-pred\nlabels equals torch dot round torch dot sigmoid. So sigmoid activation function for binary cross\nentropy and model zero x test dot two device. Truly this should be within inference mode code,\nbut for now we'll just leave it like this to have a single example of what's going on here.\nNow I just need to count one, two, there we want. That's where we want the index. We just want it\non five examples. So check for equality. And we want print torch equal. We're going to check\ny-pred's dot squeeze is equal to y-pred labels. So just we're doing the exact same thing. And we\nneed squeeze here to get rid of the extra dimension that comes out. You can try doing this without\nsqueeze. So get rid of extra dimension once again. We want y-pred's dot squeeze. Fair bit of code\nthere, but this is what's happened here. We create y-pred's. So we turn the y-pred\nprobes into y-pred's. And then we just do the full step over again. So we make predictions with\nour model, we get the raw logits. So this is logits to pred probes to pred labels. So the raw\noutputs of our model are logits. We turn the logits into prediction probabilities using torch\nsigmoid. And we turn the prediction probabilities into prediction labels using torch dot round.\nAnd we fulfill this criteria here. So everything above 0.5. This is what torch dot round does.\nTurns it into a 1. Everything below 0.5 turns it into a 0. The predictions right now are going to\nbe quite terrible because our model is using random numbers. But y-pred's found with the steps above\nis the same as y-pred labels doing the more than one hit. Thanks to this check for equality using\ntorch equal y-pred's dot squeeze. And we just do the squeeze to get rid of the extra dimensions.\nAnd we have out here some labels that look like our actual y-test labels. They're in the same format,\nbut of course they're not the same values because this model is using random weights to make predictions.\nSo we've done a fair few steps here, but I believe we are now in the right space to start building\na training a test loop. So let's write that down here 3.2 building a training and testing loop.\nYou might want to have a go at this yourself. So we've got all the steps that we need to do the\nforward pass. But the reason we've done this step here, the logits, then the pred probes and the\npred labels, is because the inputs to our loss function up here, this requires, so BCE with\nlogits loss, requires what? Well, we're going to see that in the next video, but I'd encourage\nyou to give it a go at implementing these steps here. Remember the jingle for an epoch in a range,\ndo the forward pass, calculate the loss, which is BC with logits loss, optimise a zero grad,\nwhich is this one here, last backward, optimise a step, step, step. Let's do that together in the\nnext video. Welcome back. In the last few videos, we've been working through creating a model for\na classification problem. And now we're up to training a model. And we've got some steps here,\nbut we started off by discussing the concept of logits. Logits are the raw output of the model,\nwhatever comes out of the forward functions of the layers in our model. And then we discussed how\nwe can turn those logits into prediction probabilities using an activation function,\nsuch as sigmoid for binary classification, and softmax for multi class classification.\nWe haven't seen softmax yet, but we're going to stick with sigmoid for now because we have\nbinary classification data. And then we can convert that from prediction probabilities\nto prediction labels. Because remember, when we want to evaluate our model, we want to compare\napples to apples. We want our models predictions to be in the same format as our test labels.\nAnd so I took a little break after the previous video. So my collab notebook has once again\ndisconnected. So I'm just going to run all of the cells before here. It's going to reconnect up\nhere. We should still have a GPU present. That's a good thing about Google collab is that if you\nchange the runtime type to GPU, it'll save that wherever it saves the Google collab notebook,\nso that when you restart it, it should still have a GPU present. And how can we check that,\nof course, while we can type in device, we can run that cell. And we can also check\nNvidia SMI. It'll tell us if we have an Nvidia GPU with CUDA enabled ready to go.\nSo what's our device? CUDA. Wonderful. And Nvidia SMI. Excellent. I have a Tesla P100 GPU.\nReady to go. So with that being said, let's start to write a training loop. Now we've done this before,\nand we've got the steps up here. Do the forward pass, calculate the loss. We've spent enough on\nthis. So we're just going to start jumping into write code. There is a little tidbit in this one,\nthough, but we'll conquer that when we get to it. So I'm going to set a manual seed,\ntorch top manual seed. And I'm going to use my favorite number 42. This is just to ensure\nreproducibility, if possible. Now I also want you to be aware of there is also another\nform of random seed manual seed, which is a CUDA random seed. Do we have the PyTorch?\nYeah, reproducibility. So torch dot CUDA dot manual seed dot seed. Hmm. There is a CUDA\nseed somewhere. Let's try and find out. CUDA. I think PyTorch have just had an upgrade to\ntheir documentation. Seed. Yeah, there we go. Okay. I knew it was there. So torch dot CUDA\ndot manual seed. So if we're using CUDA, we have a CUDA manual seed as well. So let's see what\nhappens if we put that to watch that CUDA dot manual seed 42. We don't necessarily have to put\nthese. It's just to try and get as reproducible as numbers as possible on your screen and my screen.\nAgain, what is more important is not necessarily the numbers exactly being the same lining up\nbetween our screens. It's more so the direction of which way they're going. So let's set the number\nof epochs. We're going to train for 100 epochs. epochs equals 100. But again, as you might have\nguessed, the CUDA manual seed is for if you're doing operations on a CUDA device, which in our\ncase, we are. Well, then perhaps we'd want them to be as reproducible as possible. So speaking of\nCUDA devices, let's put the data to the target device because we're working with or we're writing\ndata agnostic code here. So I'm going to write x train y train equals x train two device,\ncomma y train dot two device, that'll take care of the training data. And I'm going to do the\nsame for the testing data equals x test two device. Because if we're going to run our model\non the CUDA device, we want our data to be there too. And the way we're writing our code,\nour code is going to be device agnostic. Have I said that enough yet? So let's also build our\ntraining and evaluation loop. Because we've covered the steps in here before, we're going to start\nworking a little bit faster through here. And don't worry, I think you're up to it. So for an epoch\nin a range of epochs, what do we do? We start with training. So let me just write this.\nTraining model zero dot train. That's the model we're working with. We call the train,\nwhich is the default, but we're going to do that anyway. And as you might have guessed,\nthe code that we're writing here is, you can functionize this. So we're going to do this later\non. But just for now, the next couple of videos, the next module or two, we're going to keep\nwriting out the training loop in full. So this is the part, the forward pass, where there's a\nlittle bit of a tidbit here compared to what we've done previously. And that is because we're\noutputting raw logits here, if we just pass our data straight to the model. So model zero\nx train. And we're going to squeeze them here to get rid of an extra dimension. You can try to\nsee what the output sizes look like without squeeze. But we're just going to call squeeze\nhere. Remember, squeeze removes an extra one dimension from a tensor. And then to convert it\ninto prediction labels, we go torch dot round. And torch dot sigmoid, because torch dot sigmoid\nis an activation function, which is going to convert logits into what convert the logits\ninto prediction probabilities. So why logits? And I'm just going to put a note here. So this\nis going to go turn logits into pred probes into pred labels. So we've done the forward pass.\nSo that's a little tidbit there. We could have done all of this in one step, but I'll show you\nwhy we broke this apart. So now we're going to calculate loss slash accuracy. We don't necessarily\nhave to calculate the accuracy. But we did make an accuracy function up here. So that we can\ncalculate the accuracy during training, we could just stick with only calculating the loss. But\nsometimes it's cool to visualize different metrics loss plus a few others while your model is training.\nSo let's write some code to do that here. So we'll start off by going loss equals loss\nf n and y logits. Ah, here's the difference of what we've done before. Previously in the notebook\nzero one, up to zero two now, we passed in the prediction right here. But because what's our\nloss function? Let's have a look at our loss function. Let's just call that see what it returns.\nBCE with logits loss. So the BCE with logits expects logits as input. So as you might have guessed,\nloss function without logits. If we had nn dot BCE loss, notice how we haven't got with logits.\nAnd then we called loss f n, f n stands for function, by the way, without logits. What do we get?\nSo BCE loss. So this loss expects prediction probabilities as input. So let's write some code\nto differentiate between these two. As I said, we're going to be sticking with this one.\nWhy is that because if we look up torch BCE with logits loss, the documentation states that it's\nmore numerically stable. So this loss combines a sigmoid layer and the BCE loss into one single\nclass, and is more numerically stable. So let's come back here, we'll keep writing some code.\nAnd the accuracy is going to be accuracy f n. So our accuracy function, there's a little bit of a\ndifference here is why true equals y train for the training data. So this will be the training\naccuracy. And then we have y pred equals y pred. So this is our own custom accuracy function\nthat we wrote ourselves. This is a testament to the Pythonic nature of PyTorch as well.\nWe've just got a pure Python function that we've slotted into our training loop,\nwhich is essentially what the loss function is behind the scenes.\nNow, let's write here, and then dot BCE with logits loss expects raw logits. So the raw output\nof our model as input. Now, what if we were using a BCE loss on its own here? Well, let's just write\nsome code for that. So let's call loss function. And then we want to pass in y pred. Or we can\njust go why or torch sigmoid. So why would we pass in torch sigmoid on the logits here? Because\nremember, calling torch dot sigmoid on our logits turns our logits into prediction probabilities.\nAnd then we would pass in y train here. So if this was BCE loss expects this expects prediction\nprobabilities as input. So does that make sense? That's the difference between with logits. So\nour loss function requires logits as input. Whereas if we just did straight up BCE loss,\nwe need to call torch dot sigmoid on the logits because it expects prediction probabilities as\ninput. Now, I'm going to comment that out because our loss function is BCE with logits loss. But\njust keep that in mind. For some reason, you stumble across some pytorch code that's using BCE loss,\nnot BCE with logits loss. And you find that torch dot sigmoid is calling here, or you come across\nsome errors, because your inputs to your loss function are not what it expects. So with that\nbeing said, we can keep going with our other steps. So we're up to optimizer zero grad. So\noptimizer dot zero grad. Oh, this is step three, by the way. And what's after this? Once we've\nzero grad the optimizer, we do number four, which is loss backward. We can go last backward. And then\nwe go what's next? Optimizer step step step. So optimizer dot step. And I'm singing the unofficial\npytorch optimization loop song there. This is back propagation. Calculate the gradients with respect\nto all of the parameters in the model. And the optimizer step is update the parameters to reduce\nthe gradients. So gradient descent, hence the descent. Now, if we want to do testing,\nwell, we know what to do here now, we go model zero, what do we do? We call model dot of\nal when we're testing. And if we're making predictions, that's what we do when we test,\nwe make predictions on the test data set, using the patterns that our model has learned on the\ntraining data set, we turn on inference mode, because we're doing inference, we want to do the\nforward pass. And of course, we're going to compute the test logits, because logits are the raw output\nof our model with no modifications. X test dot squeeze, we're going to get rid of an extra one\ndimension there. Then we create the test pred, which is we have to do a similar calculation to\nwhat we've done here for the test pred, which is torch dot round. For our binary classification,\nwe want prediction probabilities, which we're going to create by calling the sigmoid function\non the test logits, prediction probabilities that are 0.5 or above to go to one, and prediction\nprobabilities under 0.5 to go to level zero. So two is calculate the test loss, test loss\nslash accuracy. How would we do this? Well, just if we've done before, and we're going to go\nloss FN test logits, because our loss function, we're using what we're using BCE with logits loss,\nexpects logits as input, where do we find that out in the documentation, of course,\nthen we come back here, test logits, we're going to compare that to the Y test labels.\nAnd then for the test accuracy, what are we going to do? We're going to call accuracy FN\non Y true equals Y test, and Y pred equals test pred. Now you might be thinking, why did I switch\nup the order here for these? Oh, and by the way, this is important to know with logits loss.\nSo with these loss functions, the order here matters of which way you put in your parameters.\nSo predictions come first, and then true labels for our loss function. You might be\nwondering why I've done it the reverse for our accuracy function, Y true and Y pred. That's just\nbecause I like to be confusing. Well, not really. It's because if we go to scikit-learn, I base a\nlot of my structured code of how scikit-learn structures things. The scikit-learn metrics accuracy\nscore goes Y true Y pred. So I base it off that order, because the scikit-learn metrics package\nis very helpful. So I've based our metric evaluation metric function off this one. Whereas PyTorch's\nloss function does it in the reverse order, and it's important to get these in the right order.\nExactly why they do it in that order. I couldn't tell you why. And we've got one final step, which\nis to print out what's happening. So how about we go, we're doing a lot of epochs here, 100 epochs.\nSo we'll divide the epoch by 10 to print out every epoch or every 10th epoch, sorry. And we have a\ncouple of different metrics that we can print out this time. So we're going to print out the epoch\nnumber epoch. And then we're going to print out the loss. So loss, how many decimal points?\nWe'll go point five here. This is going to be the training loss. We'll also do the accuracy,\nwhich will be the training accuracy. We could write trainiac here for our variable to be a little bit,\nmake them a little bit more understandable. And then we go here, but we're just going to leave\nit as loss and accuracy for now, because we've got test loss over here, test loss. And we're\ngoing to do the same five decimal points here. And then we're going to go test accuracy as well.\nTest act dot, we'll go to for the accuracy. And because it's accuracy, we want a percentage. This\nis the percent out of 100 guesses. What's the percentage that our model gets right on the training\ndata and the testing data, as long as we've coded all the functions correctly. Now,\nwe've got a fair few steps here. My challenge to you is to run this. And if there are any errors,\ntry to fix them. No doubt there's probably one or two or maybe more that we're going to have to\nfix in the next video. But speaking of next videos, I'll see you there. Let's train our first\nclassification model. Well, this is very exciting. I'll see you soon.\nWelcome back. In the last video, we wrote a mammoth amount of code, but nothing that we\ncan't handle. We've been through a lot of these steps. We did have to talk about a few tidbits\nbetween using different loss functions, namely the BCE loss, which is binary cross entropy loss,\nand the BCE with logit's loss. We discussed that the BCE loss in PyTorch expects prediction\nprobabilities as input. So we have to convert our model's logits. Logits are the raw output of the\nmodel to prediction probabilities using the torch dot sigmoid activation function. And if we're using\nBCE with logits loss, it expects raw logits as input as sort of the name hints at. And so we\njust pass it straight away the raw logits. Whereas our own custom accuracy function compares labels\nto labels. And that's kind of what we've been stepping through over the last few videos,\nis going from logits to predprobs to pred labels, because that's the ideal output of our model is\nsome kind of label that we as humans can interpret. And so let's keep pushing forward. You may have\nalready tried to run this training loop. I don't know if it works. We wrote all this code to get\nthem in the last video. And it's probably an error somewhere. So you ready? We're going to train\nour first classification model together for 100 epochs. If it all goes to plan in three, two,\none, let's run. Oh my gosh, it actually worked the first time. I promise you, I didn't change\nanything in here from the last video. So let's inspect what's going on. It trains pretty fast.\nWhy? Well, because we're using a GPU, so it's going to be accelerated as much as it can anyway.\nAnd our data set is quite small. And our network is quite small. So you won't always\nget networks training this fast. They did 100 epochs in like a second. So the loss. Oh,\n0.69973. It doesn't go down very much. The accuracy even starts high and then goes down.\nWhat's going on here? Our model doesn't seem to be learning anything. So what would an ideal\naccuracy be? An ideal accuracy is 100. And what's an ideal loss value? Well, zero, because lower\nis better for loss. Hmm, this is confusing. And now if we go, have a look at our blue and red\ndots. Where's our data? So I reckon, do we still have a data frame here? How many samples do we\nhave of each? Let's inspect. Let's do some data analysis. Where do we create a data frame here?\nNow, circles, do we still have this instantiated circles dot label dot? We're going to call on\npandas here, value counts. Is this going to output how many of each? Okay. Wow, we've got 500 of\nclass one and 500 of class zero. So we have 500 red dots and blue dots, which means we have a\nbalanced data set. So if we're getting, we're basically trying to predict heads or tails here.\nSo if we're getting an accuracy of under 50%, or about 50%, if you rounded it up.\nOur model is basically doing as well as guessing. Well, what gives? Well, I think we should get\nvisual with this. So let's make some predictions with our model, because these are just numbers\non the page. It's hard to interpret what's going on. But our intuition now is because we have 500\nsamples of each, or in the case of the training data set, we have 400 of each because we have\n800 samples in the training data set. And we have in the testing data set, we have 200 total\nsamples. So we have 100 of each. We're basically doing a coin flip here. Our model is as good as\nguessing. So turn to investigate why our model is not learning. And one of the ways we can do\nthat is by visualizing our predictions. So let's write down here from the metrics. It looks like\nour model isn't learning anything. So to inspect it, let's make some predictions and make them\nvisual. And we're right down here. In other words, visualize, visualize, visualize. All right.\nSo we've trained a model. We've at least got the structure for the training code here.\nBut this is the right training code. We've written this code before. So you know that this set up\nfor training code does allow a model to train. So there must be something wrong with either\nhow we've built our model, the data set. But let's keep going and investigate together.\nSo to do so, I've got a function that I've pre-built earlier. Did I mention that we're learning side\nby side of a machine learning cooking show? So this is an ingredient I prepared earlier,\na part of a dish. So to do so, we're going to import a function called plot decision,\nor maybe I'll turn this into code, plot decision boundary.\nWelcome to the cooking show, cooking with machine learning. What model will we cook up today?\nSo if we go to pytorch deep learning, well, it's already over here, but this is the home repo for\nthe course, the link for this will be scattered everywhere. But there's a little function here\ncalled helper functions dot py, which I'm going to fill up with helper functions throughout the\ncourse. And this is the one I'm talking about here, plot decision boundary. Now we could just\ncopy this into our notebook, or I'm going to write some code to import this programmatically,\nso we can use other functions from in here. Here's our plot predictions function that we made in\nthe last section, zero one, but this plot decision boundary is a function that I got inspired by\nto create from madewithml.com. Now this is another resource, a little bit of an aside,\nI highly recommend going through this by Goku Mohandas. It gives you the foundations of neural\nnetworks and also ml ops, which is a field, which is based on getting your neural networks and machine\nlearning models into applications that other people can use. So I can't recommend this resource\nenough. So please, please, please check that out if you want another resource for machine learning,\nbut this is where this helper function came from. So thank you, Goku Mohandas. I've made a little\nbit of modifications for this course, but not too many. So we could either copy that, paste it in\nhere, or we could write some code to import it for us magically, or using the power of the internet,\nright, because that's what we are. We're programmers, we're machine learning engineers, we're data\nscientists. So from pathlib, so the request module in Python is a module that allows you to make\nrequests, a request is like going to a website, hey, I'd like to get this code from you, or this\ninformation from you, can you please send it to me? So that's what that allows us to do,\nand pathlib, we've seen pathlib before, but it allows us to create file parts. Because why? Well,\nwe want to save this helper function dot pi script to our Google collab files. And so we can do this\nwith a little bit of code. So download helper functions from learn pytorch repo. If it's not\nalready downloaded. So let's see how we can do that. So we're going to write some if else code to\ncheck to see if the path of helper functions dot pi already exist, we don't want to download it again.\nSo at the moment, it doesn't exist. So this if statement is going to return false. So let's just\nprint out what it does if it returns true helper functions dot pi already exists. We might we could\neven probably do a try and accept looping about if else will help us out for now. So if it exists\nelse, print downloading helper functions dot pi. So ours doesn't exist. So it's going to make a\nrequest or let's set up our request request dot get. And here's where we can put in a URL. But we\nneed the raw version of it. So this is the raw version. If we go back, this is just pytorch deep\nlearning the repo for this course slash helper functions. If I click raw, I'm going to copy that.\nOh, don't want to go in there want to go into request get type that in this has to be in a\nstring format. So we get the raw URL. And then we're going to go with open, we're going to open\na file called helper functions dot pi. And we're going to set the context to be right binary,\nwhich is wb as file F is a common short version of writing file. Because we're going to call\nfile dot write, and then request dot content. So this code is basically saying hey requests,\nget the information that's at this link here, which is of course, all of this code here,\nwhich is a Python script. And then we're going to create a file called helper functions dot pi,\nwhich gives us write permissions. We're going to name it F, which is short for file. And then\nwe're going to call on it file dot write the content of the request. So instead of talking\nthrough it, how about we see it in action? We'll know if it works if we can from helper functions\nimport plot predictions, we're going to use plot predictions later on, as well as plot decision\nboundary. So plot predictions we wrote in the last section. Wonderful. I'm going to write here,\ndownloading helper functions dot pi did at work. We have helper functions dot pi. Look at that,\nwe've done it programmatically. Can we view this in Google column? Oh my goodness, yes we can.\nAnd look at that. So this may evolve by the time you do the course, but these are just some general\nhelper functions rather than writing all of this out. If you would like to know what's going on\nin plot decision boundary, I encourage you to read through here. And what's going on,\nyou can step by step at yourself. There is nothing here that you can't tackle yourself. It's all\njust Python code, no secrets just Python code. We've got we're making predictions with a\nPyTorch model. And then we're testing for multi class or binary. So we're going to get out of that.\nBut now let's see the ultimate test is if the plot decision boundary function works. So again,\nwe could discuss plot decision boundary of the model. We could discuss what it does behind the scenes\nto the cows come home. But we're going to see it in real life here. I like to get visual.\nSo fig size 12, six, we're going to create a plot here, because we are adhering to the data\nexplorer's motto of visualize visualize visualize. And we want to subplot because we're going to\ncompare our training and test sets here, train. And then we're going to go PLT, or actually we'll\nplot the first one, plot decision boundary. Now, because we're doing a training plot here,\nwe're going to pass in model zero and X train and Y train. Now, this is the order that the\nparameters go in. If we press command shift space, I believe Google collab, if it's working with me,\nwe'll put up a doc string. There we go, plot decision boundary. Look at the inputs that it\ntakes model, which is torch and end up module. And we've got X, which is our X value, which is a\ntorch tensor, and Y, which is our torch tensor value here. So that's for the training data.\nNow, let's do the same for the testing data, plot dot subplot. This is going to be one, two,\ntwo for the index. This is just number of rows of the plot, number of columns. And this is the\nindex. So this plot will appear on the first slot. We're going to see this anyway. Anything\nbelow this code will appear on the second slot, PLT dot title. And we're going to call this one\ntest. Then we're going to call plot decision boundary. If this works, this is going to be some\nserious magic. I love visualization functions in machine learning. Okay, you ready? Three,\ntwo, one, let's check it out. How's our model doing? Oh, look at that. Oh, now it's clear.\nSo behind the scenes, this is the plots that plot decision boundary is making. Of course,\nthis is the training data. This is the testing data, not as many dot points here, but the same\nsort of line of what's going on. So this is the line that our model is trying to draw through the\ndata. No wonder it's getting about 50% accuracy and the loss isn't going down. It's just trying\nto split the data straight through the middle. It's drawing a straight line. But our data is\ncircular. Why do you think it's drawing a straight line? Well, do you think it has anything to do\nwith the fact that our model is just made with using pure linear layers? Let's go back to our model.\nWhat's it comprised on? Just a couple of linear layers. What's a linear line? If we look up linear\nline, is this going to work with me? I don't actually think it might. There we go. Linear line,\nall straight lines. So I want you to have a think about this, even if you're completely\nnew to deep learning, can we? You can answer this question. Can we ever separate this circular data\nwith straight lines? I mean, maybe we could if we drew straight lines here, but then trying to\ncurve them around. But there's an easier way. We're going to see that later on. For now,\nhow about we try to improve our model? So the model that we built, we've got 100 epochs.\nI wonder if our model will improve if we trained it for longer. So that's a little bit of a challenge\nbefore the next video. See if you can train the model for 1000 epochs. Does that improve the\nresults here? And if it doesn't improve the results here, have a think about why that might be.\nI'll see you in the next video. Welcome back. In the last video, we wrote some code to download\na series of helper functions from our helper functions dot pi. And later on, you'll see why\nthis is quite standard practice as you write more and more code is to write some code, store them\nsomewhere such as a Python script like this. And then instead of us rewriting everything that we\nhave and helper functions, we just import them and then use them later on. This is similar to\nwhat we've been doing with PyTorch. PyTorch is essentially just a collection of Python scripts\nthat we're using to build neural networks. Well, there's a lot more than what we've just done.\nI mean, we've got one here, but PyTorch is a collection of probably hundreds of different\nPython scripts. But that's beside the point. We're trying to train a model here to separate\nblue and red dots. But our current model is only drawing straight lines. And I got you to\nhave a think about whether our straight line model, our linear model could ever separate this data.\nMaybe it could. And I issued the challenge to see if it could if you trained for 1000 epochs.\nSo did it improve at anything? Is the accuracy any higher? Well, speaking of training for more\nepochs, we're up to section number five, improving a model. This is from a model perspective. So now\nlet's discuss some ways. If you were getting results after you train a machine learning model or a\ndeep learning model, whatever kind of model you're working with, and you weren't happy with those\nresults. So how could you go about improving them? So this is going to be a little bit of an overview\nof what we're going to get into. So one way is to add more layers. So give the model more chances\nto learn about patterns in the data. Why would that help? Because if our model currently has two\nlayers, model zero dot state dinked. Well, we've got however many numbers here, 20 or so. So this\nis zero flayer. This is the first layer. If we had 10 of these, well, we'd have 10 times the\namount of parameters to try and learn the patterns in this data, a representation of this data.\nAnother way is to add more hidden units. So what I mean by that is we created this model here,\nand each of these layers has five hidden units. The first one outputs, out features equals five,\nand this one takes in features equals five. So we could go from, go from five hidden units to\n10 hidden units. The same principle as above applies here is that the more parameters our model has\nto represent our data, the potentially now I say potentially here because some of these things\nmight not necessarily work. So our data sets quite simple. So maybe if we added too many layers,\nour models trying to learn things that are too complex, it's trying to adjust too many numbers\nfor the data set that we have the same thing for more hidden units. What other options do we\nhave? Well, we could fit for longer, give the model more of a chance to learn because every epoch\nis one pass through the data. So maybe 100 times looking at this data set wasn't enough.\nSo maybe you could fit for 1000 times, which was the challenge. Then there's change in the\nactivation functions, which we're using sigmoid at the moment, which is generally the activation\nfunction you use for a binary classification problem. But there are also activation functions\nyou can put within your model. Hmm, there's a little hint that we'll get to that later.\nThen there's change the learning rate. So the learning rate is the amount the optimizer will\nadjust these every epoch. And if it's too small, our model might not learn anything because it's\ntaking forever to change these numbers. But if also on the other side of things, if the learning\nrate is too high, these updates might be too large. And our model might just explode. There's an\nactual problem in machine learning called exploding gradient problem, where the numbers just get\ntoo large. On the other side, there's also a vanishing gradients problem, where the gradients\njust go basically to zero too quickly. And then there's also change the loss function. But I feel\nlike for now, sigmoid and binary cross entropy, pretty good, pretty standard. So we're going to\nhave a look at some options here, add more layers and fit for longer, maybe changing the learning\nrate. But let's just add a little bit of color to what we've been talking about. Right now,\nwe've fit the model to the data and made a prediction. I'm just going to step through this.\nWhere are we up to? We've done this, we've done this, we've done these two, we've built a training\nloop, we've fit the model to the data, made a prediction, we've evaluated our model visually,\nand we're not happy with that. So we're up to number five, we're going to improve through\nexperimentation. We don't need to use TensorBoard just yet, we're going to talk about this as our\nhigh level. TensorBoard is a tool or a utility from PyTorch, which helps you to monitor experiments.\nWe'll see that later on. And then we'll get to this, we won't save our model until we've got one\nthat we're happy with. And so if we look at what we've just talked about improving a model from a\nmodel's perspective, let's talk about the things we've talked about with some color this time. So\nsay we've got a model here, this isn't the exact model that we're working with, but it's similar\nstructure. We've got one, two, three, four layers, we've got a loss function BC with Logit's loss,\nwe've got an optimizer, optimizer stochastic gradient descent, and if we did write some training code,\nthis is 10 epochs. And then the testing code here, I've just cut it out because it wouldn't fit on\nthe slide. Then if we wanted to go to a larger model, let's add some color here so we can highlight\nwhat's happening, adding layers. Okay, so this one's got one, two, three, four, five, six layers.\nAnd we've got another color here, which is I'd say this is like a little bit of a greeny blue\nincrease the number of hidden units. Okay, so the hidden units are these features here.\nWe've gone from 100 to 128 to 128. Remember, the out features of a previous layer have to line up\nwith the in features of a next layer. Then we've gone to 256. Wow. So remember how I said multiples\nof eight are pretty good generally in deep learning? Well, this is where these numbers come from.\nAnd then what else do we have change slash add activation functions? We haven't seen this before\nand end up relu. If you want to jump ahead and have a look at what and end up relu is,\nhow would you find out about it? Well, I just Google and end up relu. But we're going to have\na look at what this is later on. We can see here that this one's got one, but this larger model has\nsome relu's scattered between the linear layers. Hmm, maybe that's a hint. If we combine a linear\nlayer with a relu, what's a relu layer? I'm not going to spoil this. We're going to find out\nlater on change the optimization function. Okay. So we've got SGD. Do you recall how I said\nAdam is another popular one that works fairly well across a lot of problems as well. So Adam\nmight be a better option for us here. The learning rate as well. So maybe this learning rate was a\nlittle too high. And so we've divided it by 10. And then finally, fitting for longer. So instead\nof 10 epochs, we've gone to 100. So how about we try to implement some of these with our own model\nto see if it improves what we've got going on here? Because frankly, like, this isn't\nsatisfactory. We're trying to build a neural network here. Neural networks are supposed to be\nthese models that can learn almost anything. And we can't even separate some blue dots from\nsome red dots. So in the next video, how about we run through writing some code to do some of\nthese steps here? In fact, if you want to try yourself, I'd highly encourage that. So I'd start\nwith trying to add some more layers and add some more hitting units and fitting for longer. You can\nkeep all of the other settings the same for now. But I'll see you in the next video. Welcome back.\nIn the last video, we discussed some options to improve our model from a model perspective. And\nnamely, we're trying to improve it so that the predictions are better, so that the patterns it\nlearns better represent the data. So we can separate blue dots from red dots. And you might be wondering\nwhy we said from a model perspective here. So let me just write these down. These options are all\nfrom a models perspective, because they deal directly with the model, rather than the data.\nSo there's another way to improve a models results is if the model was sound already,\nin machine learning and deep learning, you may be aware that generally if you have more data samples,\nthe model learns or gets better results because it has more opportunity to learn. There's a few\nother ways to improve a model from a data perspective, but we're going to focus on improving a model\nfrom a models perspective. So, and because these options are all values we as machine learning\nengineers and data scientists can change, they are referred to as hyper parameters.\nSo a little bit of an important distinction here. Parameters are the numbers within a model.\nThe parameters here, like these values, the weights and biases are parameters,\nare the values a model updates by itself. Hyper parameters are what we as machine learning\nengineers and data scientists, such as adding more layers, more hidden units, fitting for longer\nnumber of epochs, activation functions, learning rate, loss functions are hyper parameters because\nthey're values that we can change. So let's change some of the hyper parameters of our model.\nSo we'll create circle model v1. We're going to import from nn.module as well. We could write this\nmodel using nn.sequential, but we're going to subclass nn.module for practice.\nWhy would we use nn.sequential? Well, because as you'll see, our model is not too complicated,\nbut we subclass nn.module. In fact, nn.sequential. So if we write here, nn.sequential is also a\nversion of nn.module. But we subclass nn.module here for one for practice and for later on,\nif we wanted to, or if you wanted to make more complex models, you're going to see a subclass\nof nn.module a lot in the wild. So the first change we're going to update is the number\nof hidden units. So out features, I might write this down before we do it. Let's try and improve\nour model by adding more hidden units. So this will go from five and we'll increase it to 10.\nAnd we want to increase the number of layers. So we want to go from two to three. We'll add an\nextra layer and then increase the number of epochs. So we're going to go from 100 to 1,000. Now,\nwhat can you, we're going to put on our scientist hats for a second. What would be the problem with\nthe way we're running this experiment? If we're doing all three things in one hit, why might that\nbe problematic? Well, because we might not know which one offered the improvement if there is\nany improvement or degradation. So just to keep in mind going forward, I'm just doing this as an\nexample of how we can change all of these. But generally, when you're doing machine learning\nexperiments, you'd only like to change one value at a time and track the results. So that's called\nexperiment tracking and machine learning. We're going to have a look at experiment tracking a\nlittle later on in the course, but just keep that in mind. A scientist likes to change one\nvariable of what's going on so that they can control what's happening. But we're going to\ncreate this next layer here layer two. And of course, it takes the same number of out features as\nin features as the previous layer. This is two because why our X train has. Let's look at just\nthe first five samples has two features. So now we're going to create self layer three, which\nequals an n dot linear. The in features here is going to be 10. Why? Because the layer above\nhas out features equals 10. So what we've changed here so far is we've got hidden units previously\nin the zero of this model was five. And now we've got a third layout, which previously before was\ntwo. So these are two of our main changes here. And out features equals one, because why? Let's\nhave a look at speaking of why. Our why is just one number. So remember the shapes, the input and\noutput shapes of a model is one of the most important things in deep learning. We're going to see\ndifferent values for the shapes later on. But because we're working with this data set, we're\nfocused on two in features and one out feature. So now that we've got our layers prepared,\nwhat's next? Well, we have to override the forward method, because every subclass of\nan n dot module has to implement a forward method. So what are we going to do here? Well, we could,\nlet me just show you one option. We could go z, which would be z for logits. Logits is actually\nrepresented by z, fun fact. But you could actually put any variable here. So this could be x one,\nor you could reset x if you wanted to. I just look putting a different one because it's a little\nless confusing for me. And then we could go update z by going self layer two. And then the,\nbecause z above is the output of layer one, it now goes into here. And then if we go z,\nagain, equals self layer three, what's this going to take? It's going to take z from above.\nSo this is saying, hey, give me x, put it through layer one, assign it to z. And then\ncreate a new variable z or override z with self layer two with z from before as the input. And\nthen we've got z again, the output of layer two has the input for layer three. And then we could\nreturn z. So that's just passing our data through each one of these layers here. But a way that\nyou can leverage speedups in PyTorch is to call them all at once. So layer three, and we're going\nto put self dot layer two. And this is generally how I'm going to write them. But it also behind\nthe scenes, because it's performing all the operations at once, you leverage whatever speed\nups you can get. Oh, this should be layer one. So it goes in order here. So what's happening?\nWell, it's computing the inside of the brackets first. So layer one, x is going through layer one.\nAnd then the output of x into layer one is going into layer two. And then the same again,\nfor layer three. So this way, this way of writing operations, leverages, speed ups, where possible\nbehind the scenes. And so we've done our Ford method there. We're just passing our data through\nlayers with an extra hidden units, and an extra layer overall. So now let's create an instance of\ncircle model v one, which we're going to set to model one. And we're going to write circle model\nv one. And we're going to send it to the target device, because we like writing device agnostic code.\nAnd then we're going to check out model one. So let's have a look at what's going on there.\nBeautiful. So now we have a three layered model with more hidden units. So I wonder if we trained\nthis model for longer, are we going to get improvements here? So my challenge to you is we've already\ndone these steps before. We're going to do them over the next couple of videos for completeness.\nBut we need to what create a loss function. So I'll give you a hint. It's very similar to the one\nwe've already used. And we need to create an optimizer. And then once we've done that, we need to\nwrite a training and evaluation loop for model one. So give that a shot. Otherwise, I'll see you\nin the next video. We'll do this all together. Welcome back. In the last video, we subclassed\nnn.module to create circle model V one, which is an upgrade on circle model V zero. In the\nfact that we added more hidden units. So from five to 10. And we added a whole extra layer.\nAnd we've got an instance of it ready to go. So we're up to in the workflow. We've got our data.\nWell, we haven't changed the data. So we've built our new model. We now need to pick a loss function.\nAnd I hinted at before that we're going to use the same loss function as before.\nThe same optimizer. You might have already done all of these steps. So you may know whether this\nmodel works on our data set or not. But that's what we're going to work towards finding out in\nthis video. So we've built our new model. Now let's pick a loss function and optimizer. We could\nalmost do all of this with our eyes closed now, build a training loop, fit the model to the data,\nmake a prediction and evaluate the model. We'll come back here. And let's set up a loss function.\nAnd by the way, if you're wondering, like, why would adding more features here, we've kind of\nhinted at this before. And why would an extra layer improve our model? Well, again, it's back\nto the fact that if we add more neurons, if we add more hidden units, and if we add more layers,\nit just gives our model more numbers to adjust. So look at what's going on here, layer one,\nlayer two. Look how many more we have compared to model zero dot state date.\nWe have all of these. This is model zero. And we just upgraded it. Look how many more we have\nfrom just adding an extra layer and more hidden units. So now we have our optimizer can change\nthese values to hopefully create a better representation of the data we're trying to fit.\nSo we just have more opportunity to learn patterns in our target data set. So that's the theory\nbehind it. So let's get rid of ease. Let's create a loss function. What are we going to use? Well,\nwe're going to use nn dot BCE with logit's loss. And our optimizer is going to be what? We're\ngoing to keep that as the same as before, torch dot opt in dot SGD. But we have to be aware that\nbecause we're using a new model, we have to pass in params of model one. These are the parameters\nwe want to optimize. And the LR is going to be 0.1. Is that the same LR we use before learning\nrate? 0.1. Oh, potentially that our learning rate may be too big. 0.1. Where do we create our\noptimizer? So we've written a lot of code here. Optimizer. There we go. 0.1. That's all right.\nSo we'll keep it at 0.1 just to keep as many things the same as possible. So we're going to set up\ntorch dot manual seed 42 to make training as reproducible as possible torch dot CUDA dot manual\nseed 42. Now, as I said before, don't worry too much if your numbers aren't exactly the same as mine.\nThe direction is more important, whether it's good or bad direction. So now let's set up epochs.\nWe want to train for longer this time as well. So 1000 epochs. This is one of our three improvements\nthat we're trying to do. Adding more hidden units, increase the number of layers and increase the\nnumber of epochs. So we're going to give our model 1000 looks at the data to try and improve\nits patterns. So put data on the target device. We want to write device agnostic code. And yes,\nwe've already done this, but we're going to write it out again for practice because even though we\ncould functionize a lot of this, it's good while we're in still the foundation stages to practice\nwhat's going on here, because I want you to be able to do this with your eyes closed before we\nstart to functionize it. So put the training data and the testing data to the target device,\nwhatever it is, CPU or GPU. And then we're going to, well, what's our song? For an epoch in range.\nLet's loop through the epochs. We're going to start off with training. What do we do for training? Well,\nwe set model one to train. And then what's our first step? Well, we have to forward pass. What's\nour outputs of the model? Well, the raw outputs of a model are logits. So model one, we're going\nto pass it the training data. We're going to squeeze it so that we get rid of an extra one\ndimension. If you don't believe me that we would like to get rid of that one dimension,\ntry running the code without that dot squeeze. And why pred equals torch dot round.\nAnd torch dot sigmoid, why we're calling sigmoid on our logits to go from logits to prediction\nprobabilities to prediction labels. And then what do we do next? Well, we calculate the loss\nslash accuracy to here. And remember, accuracy is optional, but loss is not optional. So we're\ngoing to pass in here, our loss function is going to take in. I wonder if it'll work with just straight\nup why pred? I don't think it will because we're using we need logits in here. Why logits and why\ntrain? Because why? Oh, Google collab correcting the wrong thing. We have why logits because we're\nusing BCE with logits loss here. So let's keep pushing forward. We want our accuracy now,\nwhich is our accuracy function. And we're going to pass in the order here, which is the reverse\nof above, a little confusing, but I've kept the evaluation function in the same order as\nscikit loan. Why pred equals y pred? Three, we're going to zero the gradients of the optimizer,\noptimizer zero grad. And you might notice that we've started to pick up the pace a little.\nThat is perfectly fine. If I'm typing too fast, you can always slow down the video,\nor you could just watch what we're doing and then code it out yourself afterwards,\nthe code resources will always be available. We're going to take the last backward\nand perform back propagation. The only reason we're going faster is because we've covered\nthese steps. So anything that we sort of spend time here, we've covered in a previous video,\noptimizer step. And this is where the adjustments to all of our models parameters are going to take\nplace to hopefully create a better representation of the data. And then we've got testing. What's\nthe first step that we do in testing? Well, we call model one dot a vowel to put it in evaluation\nmode. And because we're making predictions, we're going to turn on torch inference mode\npredictions. I call them predictions. Some other places call it inference.\nRemember machine learning has a lot of different names for the same thing.\nForward pass. So we're going to create the test logits here. Equals model one X test.\nAnd we're going to squeeze them because we won't don't want the extra one dimension. Just going to\nadd some code cells here so that we have more space and I'm typing in the middle of the screen.\nThen I'm going to put in test pred here. How do we get from logits to predictions? Well,\nwe go torch dot round. And then we go torch dot sigmoid y sigmoid because we're working with a\nbinary classification problem. And to convert logits from a binary classification problem\nto prediction probabilities, we use the sigmoid activation function. And then we're going to\ncalculate the loss. So how wrong is our model on the test data? So test last equals loss function.\nWe're going to pass it in the test logits. And then we're going to pass it in Y test for the ideal\nlabels. And then we're going to also calculate test accuracy. And test accuracy is going to\ntake in Y true equals Y test. So the test labels and Y pred equals test pred. So the test predictions\ntest predictions here. And our final step is to print out what's happening. So print out what's\nhappening. Oh, every tutorial needs a song. If I could, I'd teach everything with song.\nSong and dance. So because we're training for 1000 epochs, how about every 100 epochs we print\nout something. So print f string, and we're going to write epoch in here. So we know what epoch our\nmodels on. And then we're going to print out the loss. Of course, this is going to be the training\nloss. Because the test loss has test at the front of it. And then accuracy here. Now, of course,\nthis is going to be the training accuracy. We go here. And then we're going to pipe. And we're\ngoing to print out the test loss. And we want the test loss here. We're going to take this to five\ndecimal places. Again, when we see the printouts of the different values, do not worry too much\nabout the exact numbers on my screen appearing on your screen, because that is inherent to the\nrandomness of machine learning. So have we got the direction is more important? Have we got,\nwe need a percentage sign here, because that's going to be a bit more complete for accuracy.\nHave we got any errors here? I don't know. I'm just, we've just all coded this free hand,\nright? There's a lot of code going on here. So we're about to train our next model,\nwhich is the biggest model we've built so far in this course, three layers, 10 hidden units on\neach layer. Let's see what we've got. Three, two, one, run. Oh, what? What? A thousand epochs,\nan extra hidden layer, more hidden units. And we still, our model is still basically a coin toss.\n50%. Now, this can't be for real. Let's plot the decision boundary.\nPlot the decision boundary. To find out, let's get a bit visual. Plot figure, actually, to prevent us\nfrom writing out all of the plot code, let's just go up here, and we'll copy this. Now, you know,\nI'm not the biggest fan of copying code. But for this case, we've already written it. So there's\nnothing really new here to cover. And we're going to just change this from model zero to model one,\nbecause why it's our new model that we just trained. And so behind the scenes, plot decision\nboundary is going to make predictions with the target model on the target data set and put it\ninto a nice visual representation for us. Oh, I said nice visual representation. What does this\nlook like? We've just got a coin toss on our data set. Our model is just again, it's trying\nto draw a straight line to separate circular data. Now, why is this? Our model is based on linear,\nis our data nonlinear? Hmm, maybe I've revealed a few of my tricks. I've done a couple of reveals\nover the past few videos. But this is still quite annoying. And it can be fairly annoying\nwhen you're training models and they're not working. So how about we verify that this model\ncan learn anything? Because right now it's just basically guessing for our data set.\nSo this model looks a lot like the model we built in section 01. Let's go back to this.\nThis is the learn pytorch.io book pytorch workflow fundamentals. Where did we create a model model\nbuilding essentials? Where did we build a model? Linear regression model? Yeah, here. And then\ndot linear. But we built this model down here. So all we've changed from 01 to here is we've added\na couple of layers. The forward computation is quite similar. If this model can learn something\non a straight line, can this model learn something on a straight line? So that's my challenge to you\nis grab the data set that we created in this previous notebook. So data, you could just\nreproduce this in exact data set. And see if you can write some code to fit the model that we built\nhere. This one here on the data set that we created in here. Because I want to verify that\nthis model can learn anything. Because right now it seems like it's not learning anything at all.\nAnd that's quite frustrating. So give that a shot. And I'll see you in the next video.\nWelcome back. In the past few videos, we've tried to build a model to separate the blue from red\ndots yet. Our previous efforts have proven futile, but don't worry. We're going to get there. I promise\nyou we're going to get there. And I may have a little bit of inside information here. But we're\ngoing to build a model to separate these blue dots from red dots, a fundamental classification model.\nAnd we tried a few things in the last couple of videos, such as training for longer, so more epochs.\nWe added another layer. We increased the hidden units because we learned of a few methods to\nimprove a model from a model perspective, such as upgrading the hyperparameters, such as number\nof layers, more hidden units, fitting for longer, changing the activation functions,\nchanging the learning rate, we haven't quite done that one yet, and changing the loss function.\nOne way that I like to troubleshoot problems is I'm going to put a subheading here, 5.1.\nWe're going to prepare or preparing data to see if our model can fit a straight line.\nSo one way to troubleshoot, this is my trick for troubleshooting problems, especially neural\nnetworks, but just machine learning in general, to troubleshoot a larger problem is to test out\na smaller problem. And so why is this? Well, because we know that we had something working\nin a previous section, so 01, PyTorch, workflow fundamentals, we built a model here that worked.\nAnd if we go right down, we know that this linear model can fit a straight line. So we're going\nto replicate a data set to fit a straight line to see if the model that we're building here\ncan learn anything at all, because right now it seems like it can't. It's just tossing a coin\ndisplayed between our data here, which is not ideal. So let's make some data. But yeah, this is the,\nlet's create a smaller problem, one that we know that works, and then add more complexity to try\nand solve our larger problem. So create some data. This is going to be the same as notebook 01.\nAnd I'm going to set up weight equals 0.7 bias equals 0.3. We're going to move quite quickly\nthrough this because we've seen this in module one, but the overall takeaway from this is we're\ngoing to see if our model works on any kind of problem at all, or do we have something fundamentally\nwrong, create data. We're going to call it x regression, because it's a straight line, and we\nwant it to predict a number rather than a class. So you might be thinking, oh, we might have to change\na few things of our model architecture. Well, we'll see that in a second dot unsqueeze. And we're\ngoing to go on the first dimension here or dim equals one. And why regression, we're going to use\nthe linear regression formula as well, wait times x, x regression, that is, because we're working\nwith a new data set here, plus the bias. So this is linear regression formula. Without epsilon. So it's a simplified version of linear regression, but the same formula that we've seen in a previous section. So now let's check the data. Nothing we really haven't covered here, but we're going to do a sanity check on it to make sure that we're dealing with what we're dealing with.\nWhat we're dealing with is not just a load of garbage. Because it's all about the data and machine learning. I can't stress to you enough. That's the data explorer's motto is to visualize, visualize, visualize. Oh, what did we get wrong here? Unsqueeze. Did you notice that typo? Why didn't you say something? I'm kidding. There we go. Okay, so we've got 100 samples of x. We've got a different step size here, but that's all right. Let's have a little bit of fun with this. And we've got one x-value, which is, you know, a little bit more.\nOne x value per y value is a very similar data set to what we use before. Now, what do we do once we have a data set? Well, if we haven't already got training and test splits, we better make them. So create train and test splits.\nAnd then we're going to go train split. We're going to use 80% equals int 0.8 times the length of, or we could just put 100 in there.\nBut we're going to be specific here. And then we're going to go x train regression, y train regression equals. What are these equal? Well, we're going to go on x regression.\nAnd we're going to index up to the train split on the x. And then for the y, y regression, we're going to index up to the train split.\nWonderful. And then we can do the same on the test or creating the test data. Nothing really new here that we need to discuss. We're creating training and test sets. What do they do for each of them?\nWell, the model is going to hopefully learn patterns in the training data set that is able to model the testing data set. And we're going to see that in a second.\nSo if we check the length of each, what do we have? Length x train regression. We might just check x train x test regression. What do we have here?\nAnd then we're going to go length y train regression. Long variable names here. Excuse me for that. But we want to keep it separate from our already existing x and y data. What values do we have here?\n80, 20, 80, 20, beautiful. So 80 training samples to 100 testing samples. That should be enough. Now, because we've got our helper functions file here. And if you don't have this, remember, we wrote some code up here before to where is it?\nTo download it from the course GitHub, and we imported plot predictions from it. Now, if we have a look at helper functions.py, it contains the plot predictions function that we created in the last section, section 0.1. There we go. Plot predictions.\nSo we're just running this exact same function here, or we're about to run it. It's going to save us from re typing out all of this. That's the beauty of having a helper functions.py file.\nSo if we come down here, let's plot our data to visually inspected. Right now, it's just numbers on a page. And we're not going to plot really any predictions because we don't have any predictions yet.\nBut we'll pass in the train data is equal to X train regression. And then the next one is the train labels, which is equal to Y train regression.\nAnd then we have the test data, which is equal to X test regression. And then we have the test labels. Now, I think this should be labels too. Yeah, there we go. Y test progression might be proven wrong as we try to run this function.\nOkay, there we go. So we have some training data and we have some testing data. Now, do you think that our model model one, we have a look what's model one could fit this data.\nDoes it have the right amount of in and out features? We may have to adjust these slightly. So I'd like you to think about that. Do we have to change the input features to our model for this data set?\nAnd do we have to change the out features of our model for this data set? We'll find out in the next video.\nWelcome back. We're currently working through a little side project here, but really the philosophy of what we're doing. We just created a straight line data set because we know that we've built a model in the past back in section 01 to fit a straight line data set.\nAnd why are we doing this? Well, because the model that we've built so far is not fitting or not working on our circular data set here on our classification data set.\nAnd so one way to troubleshoot a larger problem is to test out a smaller problem first. So later on, if you're working with a big machine learning data set, you'd probably start with a smaller portion of that data set first.\nLikewise, with a larger machine learning model, instead of starting with a huge model, you'll start with a small model.\nSo we're taking a step back here to see if our model is going to learn anything at all on a straight line data set so that we can improve it for a non-straight line data set.\nAnd there's another hint. Oh, we're going to cover it in a second. I promise you. But let's see how now we can adjust model one to fit a straight line.\nAnd I should do the question at the end of last video. Do we have to adjust the parameters of model one in any way shape or form to fit this straight line data?\nAnd you may have realized or you may not have that our model one is set up for our classification data, which has two X input features.\nWhereas this data, if we go X train regression, how many input features do we have? We just get the first sample.\nThere's only one value. Or maybe we get the first 10. There's only one value per, let's remind ourselves, this is input and output shapes, one of the most fundamental things in machine learning and deep learning.\nAnd trust me, I still get this wrong all the time. So that's why I'm harping on about it. We have one feature per one label. So we have to adjust our model slightly.\nWe have to change the end features to be one instead of two. The out features can stay the same because we want one number to come out.\nSo what we're going to do is code up a little bit different version of model one. So same architecture as model one. But using NN dot sequential, we're going to do the faster way of coding a model here.\nLet's create model two and NN dot sequential. The only thing that's going to change is the number of input features.\nSo this will be the exact same code as model one. And the only difference, as I said, will be features or in features is one. And then we'll go out features equals 10.\nSo 10 hidden units in the first layer. And of course, the second layer, the number of features here has to line up with the out features of the previous layer.\nThis one's going to output 10 features as well. So we're scaling things up from one feature to 10 to try and give our model as much of a chance or as many parameters as possible.\nOf course, we could make this number quite large. We could make it a thousand features if we want. But there is an upper bound on these things.\nAnd I'm going to let you find those in your experience as a machine learning engineer and a data scientist.\nBut for now, we're keeping it nice and small. So we can run as many experiments as possible. Beautiful. Look at that. We've created a sequential model. What happens with NN dot sequential?\nData goes in here, passes through this layer. Then it passes through this layer. Then it passes through this layer. And what happens when it goes through the layer?\nIt triggers the layers forward method, the internal forward method. In the case of NN dot linear, we've seen it. It's got the linear regression formula.\nSo if we go NN dot linear, it performs this mathematical operation, the linear transformation. But we've seen that before. Let's keep pushing forward.\nLet's create a loss and an optimizer loss and optimize. We're going to work through our workflow. So loss function, we have to adjust this slightly.\nWe're going to use the L1 loss because why we're dealing with a regression problem here rather than a classification problem. And our optimizer, what can we use for our optimizer?\nHow about we bring in just the exact same optimizer SGD that we've been using for our classification data. So model two dot params or parameters.\nAlways get a little bit confused. And we'll give it an LR of 0.1 because that's what we've been using so far. This is the params here.\nSo we want our optimizer to optimize our model two parameters here with a learning rate of 0.1. The learning rate is what?\nThe amount each parameter will be or the multiplier that will be applied to each parameter each epoch.\nSo now let's train the model. Do you think we could do that in this video? I think we can. So we might just train it on the training data set and then we can evaluate it on the test data set separately.\nSo we'll set up both manual seeds, CUDA and because we've set our model to the device up here. So it should be on the GPU or whatever device you have active.\nSo set the number of epochs. How many epochs should we set? Well, we set a thousand before, so we'll keep it at that.\nepochs equals a thousand. And now we're getting really good at this sort of stuff here. Let's put our data. Put the data on the target device.\nAnd I know we've done a lot of the similar steps before, but there's a reason for that. I've kept all these in here because I'd like you to buy the end of this course is to sort of know all of this stuff off by heart.\nAnd even if you don't know it all off my heart, because trust me, I don't, you know where to look.\nSo X train regression, we're going to send this to device. And then we're going to go Y train regression, just a reminder or something to get you to think while we're writing this code.\nWhat would happen if we didn't put our data on the same device as a model? We've seen that error come up before, but what happens?\nWell, I've just kind of given away, haven't you Daniel? Well, that was a great question. Our code will air off.\nOh, well, don't worry. There's plenty of questions I've been giving you that I haven't given the answer to yet.\nDevice a beautiful. We've got a device agnostic code for the model and for the data. And now let's loop through epochs.\nSo train. We're going to for epoch in range epochs for an epoch in a range. Do the forward pass.\nCalculate the loss. So Y pred equals model two. This is the forward pass. X train regression.\nIt's all going to work out hunky Dory because our model and our data are on the same device loss equals what we're going to bring in our loss function.\nThen we're going to compare the predictions to Y train regression to the Y labels. What do we do next?\nOptimize a zero grad. Optimize a dot zero grad. We're doing all of this with our comments. Look at us go.\nLoss backward and what's next? Optimize a step, step, step. And of course, we could do some testing here.\nTesting. We'll go model two dot a vowel. And then we'll go with torch dot inference mode.\nWe'll do the forward pass. We'll create the test predictions equals model two dot X test regression.\nAnd then we'll go the test loss equals loss FN on the test predictions and versus the Y test labels.\nBeautiful. Look at that. We've just done an optimization loop, something we spent a whole hour on before, maybe even longer, in about ten lines of code.\nAnd of course, we could shorten this by making these a function. But we're going to see that later on.\nI'd rather us give a little bit of practice while this is still a bit fresh. Print out what's happening.\nLet's print out what's happening. What should we do? So because we're training for a thousand epochs, I like the idea of printing out something every 100 epochs.\nThat should be about enough of a step. Epoch. What do we got? We'll put in the epoch here with the F string and then we'll go to loss, which will be loss.\nAnd maybe we'll get the first five of those five decimal places that is. We don't have an accuracy, do we?\nBecause we're working with regression. And we'll get the test loss out here. And that's going to be.5F as well.\nBeautiful. Have we got any mistakes? I don't think we do. We didn't even run this code cell before. We'll just run these three again, see if we got...\nLook at that. Oh my goodness. Is our loss... Our loss is going down.\nSo that means our model must be learning something.\nNow, what if we adjusted the learning rate here? I think if we went 0.01 or something, will that do anything?\nOh, yes. Look how low our loss gets on the test data set. But let's confirm that. We've got to make some predictions.\nWell, maybe we should do that in the next video. Yeah, this one's getting too long. But how good's that?\nWe created a straight line data set and we've created a model to fit it. We set up a loss and an optimizer already.\nAnd we put the data on the target device. We trained and we tested so our model must be learning something.\nBut I'd like you to give a shot at confirming that by using our plot predictions function.\nSo make some predictions with our trained model. Don't forget to turn on inference mode. And we should see some red dots here fairly close to the green dots on the next plot.\nGive that a shot and I'll see you in the next video.\nWelcome back. In the last video, we did something very exciting. We solved a smaller problem that's giving us a hint towards our larger problem.\nSo we know that the model that we've previously been building, model two, has the capacity to learn something.\nNow, how did we know that? Well, it's because we created this straight line data set. We replicated the architecture that we used for model one.\nRecall that model one didn't work very well on our classification data. But with a little bit of an adjustment such as changing the number of in features.\nAnd not too much different training code except for a different loss function because, well, we use MAE loss with regression data.\nAnd we changed the learning rate slightly because we found that maybe our model could learn a bit better.\nAnd again, I'd encourage you to play around with different values of the learning rate. In fact, anything that we've changed, try and change it yourself and just see what happens.\nThat's one of the best ways to learn what goes on with machine learning models.\nBut we trained for the same number of epochs. We set up device agnostic code. We did a training and testing loop.\nLook at this looks. Oh, my goodness. Well done. And our loss went down.\nSo, hmm. What does that tell us? Well, it tells us that model two or the specific architecture has some capacity to learn something.\nSo we must be missing something. And we're going to get to that in a minute, I promise you.\nBut we're just going to confirm that our model has learned something and it's not just numbers on a page going down by getting visual.\nSo turn on. We're going to make some predictions and plot them. And you may have already done this because I issued that challenge at the last of at the end of the last video.\nSo turn on evaluation mode. Let's go model two dot eval. And let's make predictions, which are also known as inference.\nAnd we're going to go with torch dot inference mode inference mode with torch dot inference mode.\nMake some predictions. We're going to save them as why preds and we're going to use model two and we're going to pass it through ex test regression.\nThis should all work because we've set up device agnostic code, plot data and predictions.\nTo do this, we can of course use our plot predictions function that we imported via our helper functions dot pi function.\nThe code for that is just a few cells above if you'd like to check that out.\nBut let's set up the train data here. Train data parameter, which is x train regression.\nAnd my goodness. Google collab. I'm already typing fast enough. You don't have to slow me down by giving me the wrong auto corrects train label equals y train regression.\nAnd then we're going to pass in our test data equals ex test regression.\nAnd then we're going to pass in test labels, which is why test regression got too many variables going on here. My goodness gracious.\nWe could have done better with naming, but this will do for now is why preds.\nAnd then if we plot this, what does it look like? Oh, no, we got an error.\nNow secretly, I kind of knew that that was coming ahead of time. That's the advantage of being the host of this machine learning cooking show. So type error. How do we fix this?\nRemember how I asked you in one of the last videos what would happen if our data wasn't on the same device as our model? Well, we get an error, right? But this is a little bit different as well.\nWe've seen this one before. We've got CUDA device type tensa to NumPy. Where is this coming from? Well, because our plot predictions function uses mapplotlib.\nAnd behind the scenes, mapplotlib references NumPy, which is another numerical computing library. However, NumPy uses a CPU rather than the GPU.\nSo we have to call dot CPU, this helpful message is telling us, call tensa dot CPU before we use our tensors with NumPy. So let's just call dot CPU on all of our tensor inputs here and see if this solves our problem.\nWonderful. Looks like it does. Oh my goodness. Look at those red dots so close. Well, okay. So this just confirms our suspicions. What we kind of already knew is that our model did have some capacity to learn.\nIt's just the data set when we changed the data set it worked. So, hmm. Is it our data that our model can't learn on? Like this circular data, or is the model itself?\nRemember, our model is only comprised of linear functions. What is linear? Linear is a straight line, but is our data made of just straight lines?\nI think it's got some nonlinearities in there. So the big secret I've been holding back will reveal itself starting from the next video. So if you want a head start of it, I'd go to torch and end.\nAnd if we have a look at the documentation, we've been speaking a lot about linear functions. What are these nonlinear activations? And I'll give you another spoiler. We've actually seen one of these nonlinear activations throughout this notebook.\nSo go and check that out. See what you can infer from that. And I'll see you in the next video. Let's get started with nonlinearities. Welcome back.\nIn the last video, we saw that the model that we've been building has some potential to learn. I mean, look at these predictions. You could get a little bit better, of course, get the red dots on top of the green dots.\nBut we're just going to leave that the trend is what we're after. Our model has some capacity to learn, except this is straight line data.\nAnd we've been hinting at it a fair bit is that we're using linear functions. And if we look up linear data, what does it look like?\nWell, it has a quite a straight line. If we go linear and just search linear, what does this give us? Linear means straight. There we go, straight.\nAnd then what happens if we search for nonlinear? I kind of hinted at this as well. Nonlinear. Oh, we get some curves. We get curved lines.\nSo linear functions. Straight. Nonlinear functions. Hmm.\nNow, this is one of the beautiful things about machine learning. And I'm not sure about you, but when I was in high school, I kind of learned a concept called line of best fit, or y equals mx plus c, or\ny equals mx plus b. And it looks something like this. And then if you wanted to go over these, you use quadratic functions and a whole bunch of other stuff.\nBut one of the most fundamental things about machine learning is that we build neural networks and deep down neural networks are just a combination.\nIt could be a large combination of linear functions and nonlinear functions.\nSo that's why in torch.nn, we have nonlinear activations and we have all these other different types of layers. But essentially, what they're doing deep down is combining straight lines with, if we go back up to our data, non straight lines.\nSo, of course, our model didn't work before because we've only given it the power to use linear lines. We've only given it the power to use straight lines.\nBut our data is what? It's curved. Although it's simple, we need nonlinearity to be able to model this data set.\nAnd now, let's say we were building a pizza detection model. So let's look up some images of pizza, one of my favorite foods, images.\nPizza, right? So could you model pizza with just straight lines?\nYou're thinking, Daniel, you can't be serious. A computer vision model doesn't look for just straight lines in this. And I'd argue that, yes, it does, except we also add some curved lines in here.\nThat's the beauty of machine learning. Could you imagine trying to write the rules of an algorithm to detect that this is a pizza? Maybe you could put in, oh, it's a curve here.\nAnd if you see red, no, no, no, no. Imagine if you're trying to do a hundred different foods. Your program would get really large. Instead, we give our machine learning models, if we come down to the model that we created.\nWe give our deep learning models the capacity to use linear and nonlinear functions. We haven't seen any nonlinear layers just yet.\nOr maybe we've hinted at some, but that's all right. So we stack these on top of each other, these layers.\nAnd then the model figures out what patterns in the data it should use, what lines it should draw to draw patterns to not only pizza, but another food such as sushi.\nIf we wanted to build a food image classification model, it would do this. The principle remains the same. So the question I'm going to pose to you, we'll get out of this, is, we'll come down here.\nWe've unlocked the missing piece or about to. We're going to cover it over the next couple of videos, the missing piece of our model.\nAnd this is a big one. This is going to follow you out throughout all of machine learning and deep learning, nonlinearity.\nSo the question here is, what patterns could you draw if you were given an infinite amount of straight and non straight lines?\nOr in machine learning terms, an infinite amount, but really it is finite. By infinite in machine learning terms, this is a technicality.\nIt could be a million parameters. It could be as we've got probably a hundred parameters in our model.\nSo just imagine a large amount of straight and non straight lines, an infinite amount of linear and nonlinear functions.\nYou could draw some pretty intricate patterns, couldn't you? And that's what gives machine learning and especially neural networks the capacity to not only fit a straight line here, but to separate two different circles.\nBut also to do crazy things like drive a self-driving car, or at least power the vision system of a self-driving car.\nOf course, after that, you need some programming to plan what to actually do with what you see in an image.\nBut we're getting ahead of ourselves here. Let's now start diving into nonlinearity.\nAnd the whole idea here is combining the power of linear and nonlinear functions.\nStraight lines and non straight lines. Our classification data is not comprised of just straight lines. It's circles, so we need nonlinearity here.\nSo recreating nonlinear data, red and blue circles. We don't need to recreate this, but we're going to do it anyway for completeness.\nSo let's get a little bit of a practice. Make and plot data. This is so that you can practice the use of nonlinearity on your own.\nAnd that plot little bit dot pie plot as PLT. We're going to go a bit faster here because we've covered this code above.\nSo import make circles. We're just going to recreate the exact same circle data set that we've created above.\nNumber of samples. We'll create a thousand. And we're going to create x and y equals what?\nMake circles. Pass it in number of samples. Beautiful.\nColab, please. I wonder if I can turn off autocorrect and colab. I'm happy to just see all of my errors in the flesh. See? Look at that. I don't want that.\nI want noise like that. Maybe I'll do that in the next video. We're not going to spend time here looking around how to do it.\nWe can work that out on the fly later. For now, I'm too excited to share with you the power of nonlinearity.\nSo here, x, we're just going to plot what's going on. We've got two x features and we're going to color it with the flavor of y because we're doing a binary classification.\nAnd we're going to use one of my favorite C maps, which is color map. And we're going to go PLT dot CM for C map and red blue.\nWhat do we get?\nOkay, red circle, blue circle. Hey, is it the same color as what's above? I like this color better.\nDid we get that right up here?\nOh, my goodness. Look how much code we've written. Yeah, I like the other blue. I'm going to bring this down here.\nIt's all about aesthetics and machine learning. It's not just numbers on a page, don't you? How could you be so crass? Let's go there.\nOkay, that's better color red and blue. That's small lively, isn't it? So now let's convert to train and test.\nAnd then we can start to build a model with nonlinearity. Oh, this is so good.\nOkay, convert data to tenses and then to train and test splits. Nothing we haven't covered here before.\nSo import torch, but it never hurts to practice code, right? Import torch from sklearn dot model selection.\nImport train test split so that we can split our red and blue dots randomly. And we're going to turn data into tenses.\nAnd we'll go X equals torch from NumPy and we'll pass in X here. And then we'll change it into type torch dot float.\nWhy do we do this? Well, because, oh, my goodness, autocorrect. It's getting the best of me here.\nYou know, watching me live code this stuff and battle with autocorrect. That's what this whole course is.\nAnd we're really teaching pie torch. Am I just battling with Google collab's autocorrect?\nWe are turning it into torch dot float with a type here because why NumPy's default, which is what makes circles users behind the scenes.\nNumPy is actually using a lot of other machine learning libraries, pandas, built on NumPy, scikit learn, does a lot of NumPy.\nMatplotlib, NumPy. That's just showing there. What's the word? Is it ubiquitous, ubiquity? I'm not sure, maybe.\nIf not, you can correct me. The ubiquity of NumPy.\nAnd test sets, but we're using pie torch to leverage the power of autograd, which is what powers our gradient descent.\nAnd the fact that it can use GPUs.\nSo we're creating training test splits here with train test split X Y.\nAnd we're going to go test size equals 0.2. And we're going to set random.\nRandom state equals 42. And then we'll view our first five samples. Are these going to be?\nTenses. Fingers crossed. We haven't got an error. Beautiful. We have tenses here.\nOkay. Now we're up to the exciting part. We've got our data set back.\nI think it's time to build a model with nonlinearity.\nSo if you'd like to peek ahead, check out TorchNN again. This is a little bit of a spoiler.\nGo into the nonlinear activation. See if you can find the one that we've already used. That's your challenge.\nCan you find the one we've already used? And go into here and search what is our nonlinear function.\nSo give that a go and see what comes up. I'll see you in the next video.\nWelcome back. Now put your hand up if you're ready to learn about nonlinearity.\nAnd I know I can't see your hands up, but I better see some hands up or I better feel some hands up\nbecause my hands up because nonlinearity is a magic piece of the puzzle that we're about to learn about.\nSo let's title this section building a model with nonlinearity.\nSo just to re-emphasize linear equals straight lines and in turn nonlinear equals non-straight lines.\nAnd I left off the end of the last video, giving you the challenge of checking out the TorchNN module,\nlooking for the nonlinear function that we've already used.\nNow where would you go to find such a thing and oh, what do we have here? Nonlinear activations.\nAnd there's going to be a fair few things here, but essentially all of the modules within TorchNN\nare either some form of layer in a neural network if we recall.\nLet's go to a neural network. We've seen the anatomy of a neural network.\nGenerally you'll have an input layer and then multiple hidden layers and some form of output layer.\nWell, these multiple hidden layers can be almost any combination of what's in TorchNN.\nAnd in fact, they can almost be any combination of function you could imagine.\nWhether they work or not is another question.\nBut PyTorch implements some of the most common layers that you would have as hidden layers.\nAnd they might be pooling layers, padding layers, activation functions.\nAnd they all have the same premise. They perform some sort of mathematical operation on an input.\nAnd so if we look into the nonlinear activation functions, you might have find an n dot sigmoid.\nWhere have we used this before? There's a sigmoid activation function in math terminology.\nIt takes some input x, performs this operation on it.\nAnd here's what it looks like if we did it on a straight line, but I think we should put this in practice.\nAnd if you want an example, well, there's an example there.\nAll of the other nonlinear activations have examples as well.\nBut I'll let you go through all of these in your own time.\nOtherwise we're going to be here forever.\nAnd then dot relu is another common function.\nWe saw that when we looked at the architecture of a classification network.\nSo with that being said, how about we start to code a classification model with nonlinearity.\nAnd of course, if you wanted to, you could look up what is a nonlinear function.\nIf you wanted to learn more, nonlinear means the graph is not a straight line.\nOh, beautiful. So that's how I'd learn about nonlinear functions.\nBut while we're here together, how about we write some code.\nSo let's go build a model with nonlinear activation functions.\nAnd just one more thing before, just to re-emphasize what we're doing here.\nBefore we write this code, I've got, I just remembered, I've got a nice slide,\nwhich is the question we posed in the previous video, the missing piece, nonlinearity.\nBut the question I want you to think about is what could you draw if you had an unlimited amount of straight,\nin other words, linear, and non-straight, nonlinear line.\nSo we've seen previously that we can build a model, a linear model to fit some data that's in a straight line, linear data.\nBut when we're working with nonlinear data, well, we need the power of nonlinear functions.\nSo this is circular data. And now, this is only a 2D plot, keep in mind there.\nWhereas neural networks and machine learning models can work with numbers that are in hundreds of dimensions,\nimpossible for us humans to visualize, but since computers love numbers, it's a piece of cake to them.\nSo from torch, import, and then we're going to create our first neural network with nonlinear activations.\nThis is so exciting. So let's create a class here.\nWe'll create circle model. We've got circle model V1 already. We're going to create circle model V2.\nAnd we'll inherit from an end dot module. And then we'll write the constructor, which is the init function,\nand we'll pass in self here. And then we'll go self, or super sorry, too many S words.\nDot underscore underscore init, underscore. There we go. So we've got the constructor here.\nAnd now let's create a layer one, self dot layer one equals just the same as what we've used before. And then dot linear.\nWe're going to create this quite similar to the model that we've built before, except with one added feature.\nAnd we're going to create in features, which is akin to the number of X features that we have here.\nAgain, if this was different, if we had three X features, we might change this to three.\nBut because we're working with two, we'll leave it as that. We'll keep out features as 10, so that we have 10 hidden units.\nAnd then we'll go layer two, and then dot linear. Again, these values here are very customizable because why, because they're hyper parameters.\nSo let's line up the out features of layer two, and we'll do the same with layer three.\nBecause layer three is going to take the outputs of layer two. So it needs in features of 10.\nAnd we want layer three to be the output layer, and we want one number as output, so we'll set one here.\nNow, here's the fun part. We're going to introduce a nonlinear function. We're going to introduce the relu function.\nNow, we've seen sigmoid. Relu is another very common one. It's actually quite simple.\nBut let's write it out first, and then dot relu.\nSo remember, torch dot nn stores a lot of existing nonlinear activation functions, so that we don't necessarily have to code them ourselves.\nHowever, if we did want to code a relu function, let me show you. It's actually quite simple.\nIf we dive into nn dot relu, or relu, however you want to say it, I usually say relu, applies the rectified linear unit function element wise.\nSo that means element wise on every element in our input tensor.\nAnd so it stands for rectified linear unit, and here's what it does. Basically, it takes an input.\nIf the input is negative, it turns the input to zero, and it leaves the positive inputs how they are.\nAnd so this line is not straight.\nNow, you could argue, yeah, well, it's straight here and then straight there, but this is a form of a nonlinear activation function.\nSo it goes boom, if it was linear, it would just stay straight there like that.\nBut let's see it in practice. Do you think this is going to improve our model?\nWell, let's find out together, hey, forward, we need to implement the forward method.\nAnd here's what we're going to do. Where should we put our nonlinear activation functions?\nSo I'm just going to put a node here. Relu is a nonlinear activation function.\nAnd remember, wherever I say function, it's just performing some sort of operation on a numerical input.\nSo we're going to put a nonlinear activation function in between each of our layers.\nSo let me show you what this looks like, self dot layer three.\nWe're going to start from the outside in self dot relu, and then we're going to go self dot layer two.\nAnd then we're going to go self dot relu.\nAnd then there's a fair bit going on here, but nothing we can't handle layer one. And then here's the X.\nSo what happens is our data goes into layer one, performs a linear operation with an end up linear.\nThen we pass the output of layer one to a relu function.\nSo we, where's relu up here, we turn all of the negative outputs of our model of our of layer one to zero,\nand we keep the positives how they are.\nAnd then we do the same here with layer two.\nAnd then finally, the outputs of layer three stay as they are. We've got out features there.\nWe don't have a relu on the end here, because we're going to pass the outputs to the sigmoid function later on.\nAnd if we really wanted to, we could put self dot sigmoid here equals an end dot sigmoid.\nBut I'm going to, that's just one way of constructing it.\nWe're just going to apply the sigmoid function to the logits of our model, because what are the logits, the raw output of our model.\nAnd so let's instantiate our model. This is going to be called model three, which is a little bit confusing, but we're up to model three,\nwhich is circle model V two, and we're going to send that to the target device.\nAnd then let's check model three. What does this look like?\nWonderful. So it doesn't actually show us where the relu's appear, but it just shows us what are the parameters of our circle model V two.\nNow, I'd like you to have a think about this. And my challenge to you is to go ahead and see if this model is capable of working on our data, on our circular data.\nSo we've got the data sets ready. You need to set up some training code.\nMy challenge to you is write that training code and see if this model works.\nBut we're going to go through that over the next few videos.\nAnd also, my other challenge to you is to go to the TensorFlow Playground and recreate our neural network here.\nYou can have two hidden layers. Does this go to 10? Well, it only goes to eight. We'll keep this at five.\nSo build something like this. So we've got two layers with five. It's a little bit different to ours because we've got two layers with 10.\nAnd then put the learning rate to 0.01. What do we have? 0.1 with stochastic gradient descent.\nWe've been using 0.1, so we'll leave that. So this is the TensorFlow Playground.\nAnd then change the activation here. Instead of linear, which we've used before, change it to relu, which is what we're using.\nAnd press play here and see what happens. I'll see you in the next video.\nWelcome back. In the last video, I left off leaving the challenge of recreating this model here.\nIt's not too difficult to do. We've got two hidden layers and five neurons. We've got our data set, which looks kind of like ours.\nBut the main points here are have to learning rate of 0.1, which is what we've been using.\nBut to change it from, we've previously used a linear activation to change it from linear to relu, which is what we've got set up here in the code.\nNow, remember, relu is a popular and effective nonlinear activation function.\nAnd we've been discussing that we need nonlinearity to model nonlinear data.\nAnd so that's the crux to what neural networks are.\nArtificial neural networks, not to get confused with the brain neural networks, but who knows?\nThis might be how they work, too. I don't know. I'm not a neurosurgeon or a neuroscientist.\nArtificial neural networks are a large combination of linear.\nSo this is straight and non-straight nonlinear functions, which are potentially able to find patterns in data.\nAnd so for our data set, it's quite small. It's just a blue and a red circle.\nBut this same principle applies for larger data sets and larger models combined linear and nonlinear functions.\nSo we've got a few tabs going on here. Let's get rid of some. Let's come back to here.\nDid you try this out? Does it work? Do you think it'll work?\nI don't know. Let's find out together. Ready? Three, two, one.\nLook at that.\nAlmost instantly the training loss goes down to zero and the test loss is basically zero as well. Look at that.\nThat's amazing. We can stop that there. And if we change the learning rate, maybe a little lower, let's see what happens.\nIt takes a little bit longer to get to where it wants to go to.\nSee, that's the power of changing the learning rate. Let's make it really small. What happens here?\nSo that was about 300 epochs. The loss started to go down.\nIf we change it to be really small, oh, we're getting a little bit of a trend.\nIs it starting to go down? We're already surpassed the epochs that we had.\nSo see how the learning rate is much smaller? That means our model is learning much slower.\nSo this is just a beautiful visual way of demonstrating different values of the learning rate.\nWe could sit here all day and that might not get to lower, but let's increase it by 10x.\nAnd that was over 1,000 epochs and it's still at about 0.5, let's say.\nOh, we got a better. Oh, we're going faster already.\nSo not even at 500 or so epochs, we're about 0.4.\nThat's the power of the learning rate. We'll increase it by another 10x.\nWe'll reset. Start again. Oh, would you look at that much faster this time.\nThat is beautiful. Oh, there's nothing better than watching a loss curve go down.\nIn the world of machine learning, that is. And then we reset that again.\nAnd let's change it right back to what we had. And we get to 0 in basically under 100 epochs.\nSo that's the power of the learning rate, little visual representation.\nWorking on learning rates, it's time for us to build an optimizer and a loss function.\nSo that's right here. We've got our nonlinear model set up loss and optimizer.\nYou might have already done this because the code, this is code that we've written before,\nbut we're going to redo it for completeness and practice.\nSo we want a loss function. We're working with logits here and we're working with binary cross entropy.\nSo what loss do we use?\nBinary cross entropy. Sorry, we're working with a binary classification problem.\nBlue dots or red dots, torch dot opt in.\nWhat are some other binary classification problems that you can think of?\nWe want model three dot parameters.\nThey're the parameters that we want to optimize this model here.\nAnd we're going to set our LR to 0.1, just like we had in the TensorFlow playground.\nBeautiful. So some other binary classification problems I can think of would be email.\nSpam or not spam credit cards.\nSo equals fraud or not fraud.\nWhat else? You might have insurance claims.\nEquals who's at fault or not at fault.\nIf someone puts in a claim speaking about a car crash, whose fault was it?\nWas the person submitting the claim? Were they at fault?\nOr was the person who was also mentioned in the claim? Are they not at fault?\nSo there's many more, but they're just some I can think of up the top of my head.\nBut now let's train our model with nonlinearity.\nOh, we're on a roll here.\nTraining a model with nonlinearity.\nSo we've seen that if we introduce a nonlinear activation function within a model,\nremember this is a linear activation function, and if we train this, the loss doesn't go down.\nBut if we just adjust this to add a relu in here, we get the loss going down.\nSo hopefully this replicates with our pure PyTorch code.\nSo let's do it, hey?\nSo we're going to create random seeds.\nBecause we're working with CUDA, we'll introduce the CUDA random seed as well.\nTorch.manual seed. Again, don't worry too much if your numbers on your screen aren't exactly what mine are.\nThat's due to the inherent randomness of machine learning.\nIn fact, stochastic gradient descent stochastic again stands for random.\nAnd we're just setting up the seeds here so that they can be as close as possible.\nBut the direction is more important.\nSo if my loss goes down, your loss should also go down on target device.\nAnd then we're going to go Xtrain.\nSo this is setting up device agnostic code. We've done this before.\nBut we're going to do it again for completeness.\nJust to practice every step of the puzzle. That's what we want to do.\nWe want to have experience. That's what this course is. It's a momentum builder.\nSo that when you go to other repos and machine learning projects that use PyTorch, you can go, oh, does this code set device agnostic code?\nWhat problem are we working on? Is it binary or multi-class classification?\nSo let's go loop through data.\nAgain, we've done this before, but we're going to set up the epochs.\nLet's do 1000 epochs. Why not?\nSo we can go for epoch in range epochs.\nWhat do we do here? Well, we want to train. So this is training code.\nWe set our model model three dot train.\nAnd I want you to start to think about how could we functionalize this training code?\nWe're going to start to move towards that in a future video.\nSo one is forward pass. We've got the logits. Why the logits?\nWell, because the raw output of our model without any activation functions towards the final layer.\nClassified as logits or called logits.\nAnd then we create y-pred as in prediction labels by rounding the output of torch dot sigmoid of the logits.\nSo this is going to take us from logits to prediction probabilities to prediction labels.\nAnd then we can go to, which is calculate the loss.\nThat's from my unofficial pytorch song. Calculate the last.\nWe go loss equals loss FN y logits.\nBecause remember, we've got BCE with logits loss and takes in logits as first input.\nAnd that's going to calculate the loss between our models, logits and the y training labels.\nAnd we will go here, we'll calculate accuracy using our accuracy function.\nAnd this one is a little bit backwards compared to pytorch, but we pass in the y training labels first.\nBut it's constructed this way because it's in the same style as scikit line.\nThree, we go optimizer zero grad. We zero the gradients of the optimizer so that it can start from fresh.\nCalculating the ideal gradients every epoch.\nSo it's going to reset every epoch, which is fine.\nThen we're going to perform back propagation pytorch is going to take care of that for us by calling loss backwards.\nAnd then we will perform gradient descent. So step the optimizer to see how we should improve our model parameters.\nSo optimizer dot step.\nOh, and I want to show you speaking of model parameters. Let's check our model three dot state dig.\nSo the relu activation function actually doesn't have any parameters.\nSo you'll notice here, we've got weight, we've got bias of layer one, layer two, and a layer three.\nSo the relu function here doesn't have any parameters to optimize. If we go nn dot relu.\nDoes it say what it implements? There we go.\nSo it's just the maximum of zero or x. So it takes the input and takes the max of zero or x.\nAnd so when it takes the max of zero or x, if it's a negative number, zero is going to be higher than a negative number.\nSo that's why it zeroes all of the negative inputs.\nAnd then it leaves the positive inputs how they are because the max of a positive input versus zero is the positive input.\nSo this has no parameters to optimize. That's why it's so effective because you think about it.\nEvery parameter in our model needs some little bit of computation to adjust.\nAnd so the more parameters we add to our model, the more compute that is required.\nSo generally, the kind of trade-off in machine learning is that, yes, more parameters have more of an ability to learn, but you need more compute.\nSo let's go model three dot a vowel. And we're going to go with torch dot inference mode.\nIf I could spell inference, that'd be fantastic. We're going to do what? We're going to do the forward pass.\nSo test logits equals model three on the test data.\nAnd then we're going to calculate the test pred labels by calling torch dot round on torch dot sigmoid on the test logits.\nAnd then we can calculate the test loss. How do we do that?\nAnd then we can also calculate the test accuracy. I'm just going to give myself some more space here.\nSo I can code in the middle of the screen equals accuracy function on what we're going to pass in y true equals y test.\nWe're going to pass in y true equals y test. And then we will pass in y pred equals test pred.\nBeautiful. A final step here is to print out what's happening.\nNow, this will be very important because one, it's fun to know what your model is doing.\nAnd two, if our model does actually learn, I'd like to see the loss values go down and the accuracy values go up.\nAs I said, there's nothing much more beautiful in the world of machine learning than watching a loss function go down or a loss value go down and watching a loss curve go down.\nSo let's print out the current epoch and then we'll print out the loss, which will just be the training loss.\nAnd we'll take that to four decimal places. And then we'll go accuracy here.\nAnd this will be a and we'll take this to two decimal places and we'll put a little percentage sign there and then we'll break it up by putting in the test loss here and we'll put in the test loss.\nBecause remember our model learns patterns on the training data set and then evaluates those patterns on the test data set.\nSo, and we'll pass in test act here and no doubt there might be an error or two within all of this code, but we're going to try and run this because we've seen this code before, but I think we're ready.\nWe're training our first model here with non-linearities built into the model.\nYou ready? Three, two, one, let's go.\nOh, of course. Module torch CUDA has no attribute manuals are just a typo standard man you out.\nThere we go. Have to sound that out.\nAnother one. What do we get wrong here? Oh, target size must be same as input size. Where did it mess up here?\nWhat do we get wrong? Test loss, test logits on Y test. Hmm.\nSo these two aren't matching up. Model three X test and Y test. What's the size of?\nSo let's do some troubleshooting on the fly. Hey, not everything always works out as you want.\nSo length of X test, we've got a shape issue here. Remember how I said one of the most common issues in deep learning is a shape issue?\nWe've got the same shape here.\nLet's check test logits dot shape and Y test dot shape. We'll print this out.\nSo 200. Oh, here's what we have to do. That's what we missed dot squeeze. Oh, see how I've been hinting at the fact that we needed to call dot squeeze.\nSo this is where the discrepancy is. Our test logits dot shape. We've got an extra dimension here.\nAnd what are we getting here? A value error on the target size, which is a shape mismatch.\nSo we've got target size 200 must be the same input size as torch size 201.\nSo did we squeeze this? Oh, that's why the training worked. Okay, so we've missed this.\nLet's just get rid of this. So we're getting rid of the extra one dimension by using squeeze, which is the one dimension here.\nWe should have everything lined up. There we go. Okay. Look at that. Yes.\nNow accuracy has gone up, albeit not by too much. It's still not perfect.\nSo really we'd like this to be towards 100% lost to be lower. But I feel like we've got a better performing model. Don't you?\nNow that is the power of non linearity. All we did was we added in a relu layer or just two of them.\nRelu here, relu here. But what did we do? We gave our model the power of straight lines. Oh, straight linear of straight lines and non straight lines.\nSo it can potentially draw a line to separate these circles.\nSo in the next video, let's draw a line, plot our model decision boundary using our function and see if it really did learn anything.\nI'll see you there.\nWelcome back. In the last video, we trained our first model, and as you can tell, I've got the biggest smile on my face,\nbut we trained our first model that harnesses both the power of straight lines and non straight lines or linear functions and non linear functions.\nAnd by the 1000th epoch, we look like we're getting a bit better results than just pure guessing, which is 50%.\nBecause we have 500 samples of red dots and 500 samples of blue dots. So we have evenly balanced classes.\nNow, we've seen that if we added a relu activation function with a data set similar to ours with a TensorFlow playground, the model starts to fit.\nBut it doesn't work with just linear. There's a few other activation functions that you could play around with here.\nYou could play around with the learning rate, regularization. If you're not sure what that is, I'll leave that as extra curriculum to look up.\nBut we're going to retire the TensorFlow program for now because we're going to go back to writing code.\nSo let's get out of that. Let's get out of that. We now have to evaluate our model because right now it's just numbers on a page.\nSo let's write down here 6.4. What do we like to do to evaluate things? It's visualize, visualize, visualize.\nSo evaluating a model trained with nonlinear activation functions.\nAnd we also discussed the point that neural networks are really just a big combination of linear and nonlinear functions trying to draw patterns in data.\nSo with that being said, let's make some predictions with our Model 3, our most recently trained model.\nWe'll put it into a Val mode and then we'll set up inference mode.\nAnd then we'll go yprads equals torch dot round and then torch dot sigmoid.\nWe could functionalize this, of course, Model 3 and then pass in X test.\nAnd you know what? We're going to squeeze these here because we ran into some troubles in the previous video.\nI actually really liked that we did because then we got to troubleshoot a shape error on the fly because that's one of the most common issues you're going to come across in deep learning.\nSo yprads, let's check them out and then let's check out y test.\nYou want y test 10.\nSo remember, when we're evaluating predictions, we want them to be in the same format as our original labels.\nWe want to compare apples to apples.\nAnd if we compare the format here, do these two things look the same?\nYes, they do. They're both on CUDA and they're both floats.\nWe can see that it's got this one wrong.\nWhereas the other ones look pretty good. Hmm, this might look pretty good if we visualize it.\nSo now let's, you might have already done this because I issued the challenge of plotting the decision boundaries.\nPlot decision boundaries and let's go PLT dot figure and we're going to set up the fig size to equal 12.6 because, again, one of the advantages of hosting a machine learning cooking show is that you can code ahead of time.\nAnd then we can go PLT dot title is train.\nAnd then we're going to call our plot decision boundary function, which we've seen before.\nPlot decision boundary.\nAnd we're going to pass this one in.\nWe could do model three, but we could also pass it in our older models to model one that doesn't use it on the reality.\nIn fact, I reckon that'll be a great comparison.\nSo we'll also create another plot here for the test data and this will be on index number two.\nSo remember, subplot is a number of rows, number of columns, index where the plot appears.\nWe'll give this one a title.\nPlot dot title.\nThis will be test and Google Colab.\nI didn't want that.\nAs I said, this course is also a battle between me and Google Colab's autocorrect.\nSo we're going model three and we'll pass in the test data here.\nAnd behind the scenes, our plot decision boundary function will create a beautiful graphic for us,\nperform some predictions on the X, the features input, and then we'll compare them with the Y values.\nLet's see what's going on here.\nOh, look at that.\nYes, our first nonlinear model.\nOkay, it's not perfect, but it is certainly much better than the models that we had before.\nLook at this.\nModel one has no linearity.\nModel one equals no nonlinearity.\nI've got double negative there.\nWhereas model three equals has nonlinearity.\nSo do you see the power of nonlinearity or better yet the power of linearity or linear straight lines with non straight lines?\nSo I feel like we could do better than this, though.\nHere's your challenge is to can you improve model three to do better?\nWhat did we get?\n79% accuracy to do better than 80% accuracy on the test data.\nI think you can.\nSo that's the challenge.\nAnd if you're looking for hints on how to do so, where can you look?\nWell, we've covered this improving a model.\nSo maybe you add some more layers, maybe you add more hidden units.\nMaybe you fit for longer.\nMaybe you if you add more layers, you put a relio activation function on top of those as well.\nMaybe you lower the learning rate because right now we've got 0.1.\nSo give this a shot, try and improve it.\nI think you can do it.\nBut we're going to push forward.\nThat's going to be your challenge for some extra curriculum.\nI think in the next section, we've seen our nonlinear activation functions in action.\nLet's write some code to replicate them.\nI'll see you there.\nWelcome back.\nIn the last video, I left off with the challenge of improving model three to do better than\n80% accuracy on the test data.\nI hope you gave it a shot.\nBut here are some of the things I would have done.\nAs I potentially add more layers, I maybe increase the number of hidden units,\nand then if we needed to fit for longer and maybe lower the learning rate to 0.01.\nBut I'll leave that for you to explore because that's the motto of the data scientists, right?\nIs to experiment, experiment, experiment.\nSo let's go in here.\nWe've seen our nonlinear activation functions in practice.\nLet's replicate them.\nSo replicating nonlinear activation functions.\nAnd remember neural networks rather than us telling the model what to learn.\nWe give it the tools to discover patterns in data.\nAnd it tries to figure out the best patterns on its own.\nAnd what are these tools?\nThat's right down here.\nWe've seen this in action.\nAnd these tools are linear and nonlinear functions.\nSo a neural network is a big stack of linear and nonlinear functions.\nFor us, we've only got about four layers or so, four or five layers.\nBut as I said, other networks can get much larger.\nBut the premise remains.\nSome form of linear and nonlinear manipulation of the data.\nSo let's get out of this.\nLet's make our workspace a little bit more cleaner.\nReplicating nonlinear activation functions.\nSo let's create a tensor to start with.\nEverything starts from the tensor.\nAnd we'll go A equals torch A range.\nAnd we're going to create a range from negative 10 to 10 with a step of one.\nAnd we can set the D type here to equal torch dot float 32.\nBut we don't actually need to.\nThat's going to be the default.\nSo if we set A here, A dot D type.\nThen we've got torch float 32 and I'm pretty sure if we've got rid of that.\nOh, we've got torch in 64.\nWhy is that happening?\nWell, let's check out A.\nOh, it's because we've got integers as our values because we have a step as one.\nIf we turn this into a float, what's going to happen?\nWe get float 32.\nBut we'll keep it.\nOtherwise, this is going to be what?\nAbout a hundred numbers?\nYeah, no, that's too many.\nLet's keep it at negative 10 to 10 and we'll set the D type here to torch float 32.\nBeautiful.\nSo it looks like PyTorch's default data type for integers is in 64.\nBut we're going to work with float 32 because float 32, if our data wasn't float 32 with\nthe functions we're about to create, we might run into some errors.\nSo let's visualize this data.\nI want you to guess, is this a straight line or non-straight line?\nYou've got three seconds.\nOne, two, three.\nStraight line.\nThere we go.\nWe've got negative 10 to positive 10 up here or nine.\nClose enough.\nAnd so how would we turn this straight line?\nIf it's a straight line, it's linear.\nHow would we perform the relu activation function on this?\nNow, we could of course call torch relu on A.\nActually, let's in fact just plot this.\nPLT dot plot on torch relu.\nWhat does this look like?\nBoom, there we go.\nBut we want to replicate the relu function.\nSo let's go nn dot relu.\nWhat does it do?\nWe've seen this before.\nSo we need the max.\nWe need to return based on an input.\nWe need the max of zero and x.\nSo let's give it a shot.\nWe'll come here.\nAgain, we need more space.\nThere can never be enough code space here.\nI like writing lots of code.\nI don't know about you.\nBut let's go relu.\nWe'll take an input x, which will be some form of tensor.\nAnd we'll go return torch dot maximum.\nI think you could just do torch dot max.\nBut we'll try maximum.\nTorch dot tensor zero.\nSo the maximum is going to return the max between whatever this is.\nOne option and whatever the other option is.\nSo inputs must be tensors.\nSo maybe we could just give a type hint here that this is torch dot tensor.\nAnd this should return a tensor too.\nReturn torch dot tensor.\nBeautiful.\nYou're ready to try it out.\nLet's see what our relu function does.\nRelu A.\nWonderful.\nIt looks like we got quite a similar output to before.\nHere's our original A.\nSo we've got negative numbers.\nThere we go.\nSo recall that the relu activation function turns all negative numbers into zero\nbecause it takes the maximum between zero and the input.\nAnd if the input's negative, well then zero is bigger than it.\nAnd it leaves all of the positive values as they are.\nSo that's the beauty of relu.\nQuite simple, but very effective.\nSo let's plot relu activation function.\nOur custom one.\nWe will go PLT dot plot.\nWe'll call our relu function on A.\nLet's see what this looks like.\nOh, look at us go.\nWell done.\nJust the exact same as the torch relu function.\nEasy as that.\nAnd what's another nonlinear activation function that we've used before?\nWell, I believe one of them is if we go down to here, what did we say before?\nSigmoid.\nWhere is that?\nWhere are you, Sigmoid?\nHere we go.\nHello, Sigmoid.\nOh, this has got a little bit more going on here.\nOne over one plus exponential of negative x.\nSo Sigmoid or this little symbol for Sigmoid of x, which is an input.\nWe get this.\nSo let's try and replicate this.\nI might just bring this one in here.\nRight now, let's do the same for Sigmoid.\nSo what do we have here?\nWell, we want to create a custom Sigmoid.\nAnd we want to have some sort of input, x.\nAnd we want to return one divided by, do we have the function in Sigmoid?\nOne divided by one plus exponential.\nOne plus torch dot exp for exponential on negative x.\nAnd we might put the bottom side in brackets so that it does that operation.\nI reckon that looks all right to me.\nSo one divided by one plus torch exponential of negative x.\nDo we have that?\nYes, we do.\nWell, there's only one real way to find out.\nLet's plot the torch version of Sigmoid.\nTorch dot Sigmoid and we'll pass in x.\nSee what happens.\nAnd then, oh, we have a.\nMy bad.\nA is our tensor.\nWhat do we get?\nWe get a curved line.\nWonderful.\nAnd then we go plt dot plot.\nAnd we're going to use our Sigmoid function on a.\nDid we replicate torch's Sigmoid function?\nYes, we did.\nOoh, now.\nSee, this is what's happening behind the scenes with our neural networks.\nOf course, you could do more complicated activation functions or layers and whatnot.\nAnd you can try to replicate them.\nIn fact, that's a great exercise to try and do.\nBut we've essentially across the videos and the sections that we've done, we've replicated our linear layer.\nAnd we've replicated the relu.\nSo we've actually built this model from scratch, or we could if we really wanted to.\nBut it's a lot easier to use PyTorch's layers because we're building neural networks here like Lego bricks,\nstacking together these layers in some way, shape, or form.\nAnd because they're a part of PyTorch, we know that they've been error-tested and they compute as fast as possible\nbehind the scenes and use GPU and get a whole bunch of benefits.\nPyTorch offers a lot of benefits by using these layers rather than writing them ourselves.\nAnd so this is what our model is doing.\nIt's literally like to learn these values and decrease the loss function and increase the accuracy.\nIt's combining linear layers and nonlinear layers or nonlinear functions.\nWhere's our relu function here?\nA relu function like this behind the scenes.\nSo just combining linear and nonlinear functions to fit a data set.\nAnd that premise remains even on our small data set and on very large data sets and very large models.\nSo with that being said, I think it's time for us to push on.\nWe've covered a fair bit of code here.\nBut we've worked on a binary classification problem.\nHave we worked on a multi-class classification problem yet?\nDo we have that here? Where's my fun graphic?\nWe have multi-class classification.\nI think that's what we cover next.\nWe're going to put together all of the steps in our workflow that we've covered for binary classification.\nBut now let's move on to a multi-class classification problem.\nIf you're with me, I'll see you in the next video.\nWelcome back.\nIn the last few videos we've been harnessing the power of nonlinearity.\nSpecifically non-straight line functions and we replicated some here.\nAnd we learned that a neural network combines linear and nonlinear functions to find patterns in data.\nAnd for our simple red versus blue dots, once we added a little bit of nonlinearity,\nwe found the secret source of to start separating our blue and red dots.\nAnd I also issued you the challenge to try and improve this and I think you can do it.\nSo hopefully you've given that a go.\nBut now let's keep pushing forward.\nWe're going to reiterate over basically everything that we've done,\nexcept this time from the point of view of a multi-class classification problem.\nSo I believe we're up to section eight, putting it all together with a multi-class classification problem.\nBeautiful.\nAnd recall the difference between binary classification equals one thing or another such as cat versus dog.\nIf you were building a cat versus dog image classifier, spam versus not spam for say emails that were spam or not spam or\neven internet posts on Facebook or Twitter or one of the other internet services.\nAnd then fraud or not fraud for credit card transactions.\nAnd then multi-class classification is more than one thing or another.\nSo we could have cat versus dog versus chicken.\nSo I think we've got all the skills to do this.\nOur architecture might be a little bit different for a multi-class classification problem.\nBut we've got so many building blocks now.\nIt's not funny.\nLet's clean up this and we'll add some more code cells and just to reiterate.\nSo we've gone over nonlinearity.\nThe question is what could you draw if you had an unlimited amount of straight linear and non-straight,\nnonlinear lines, I believe you could draw some pretty intricate patterns.\nAnd that is what our neural networks are doing behind the scenes.\nAnd so we also learned that if we wanted to just replicate some of these nonlinear functions,\nsome of the ones that we've used before, we could create a range.\nLinear activation is just the line itself.\nAnd then if we wanted to do sigmoid, we get this curl here.\nAnd then if we wanted to do relu, well, we saw how to replicate the relu function as one.\nThese both are nonlinear.\nAnd of course, torch.nn has far more nonlinear activations where they came from just as it has far more different layers.\nAnd you'll get used to these with practice.\nAnd that's what we're doing here.\nSo let's go back to the keynote.\nSo this is what we're going to be working on.\nMulti-class classification.\nSo there's one of the big differences here.\nWe use the softmax activation function versus sigmoid.\nThere's another big difference here.\nInstead of binary cross entropy, we use just cross entropy.\nBut I think most of it's going to stay the same.\nWe're going to see this in action in a second.\nBut let's just describe our problem space.\nJust to go visual, we've covered a fair bit here.\nWell done, everyone.\nSo binary versus multi-class classification.\nBinary one thing or another.\nZero or one.\nMulti-class could be three things.\nCould be a thousand things.\nCould be 5,000 things.\nCould be 25 things.\nSo more than one thing or another.\nBut that's the basic premise we're going to go with.\nLet's create some data, hey?\n8.1.\nCreating a 20 multi-class data set.\nAnd so to create our data set, we're going to import our dependencies.\nWe're going to re-import torch, even though we already have it.\nJust for a little bit of completeness.\nAnd we're going to go map plotlib.\nSo we can plot, as always, we like to get visual where we can.\nVisualize, visualize, visualize.\nWe're going to import from scikitlearn.datasets.\nLet's get make blobs.\nNow, where would I get this from?\nSKlearn.datasets.\nWhat do we get?\n20 data sets.\nDo we have classification?\n20 data sets.\nDo we have blobs?\nIf we just go make scikitlearn.\nClassification data sets.\nWhat do we get?\nHere's one option.\nThere's also make blobs.\nBeautiful.\nMake blobs.\nThis is a code for that.\nSo let's just copy this in here.\nAnd make blobs.\nWe're going to see this in action anyway.\nMake blobs.\nAs you might have guessed, it makes some blobs for us.\nI like blobs.\nIt's a fun word to say.\nBlobs.\nSo we want train test split because we want to make a data set and then we want to split\nit into train and test.\nLet's set the number of hyper parameters.\nSo set the hyper parameters for data creation.\nNow I got these from the documentation here.\nNumber of samples.\nHow many blobs do we want?\nHow many features do we want?\nSo say, for example, we wanted two different classes.\nThat would be binary classification.\nSay, for example, you wanted 10 classes.\nYou could set this to 10.\nAnd we're going to see what the others are in practice.\nBut if you want to read through them, you can well and truly do that.\nSo let's set up.\nWe want num classes.\nLet's double what we've been working with.\nWe've been working with two classes, red dots or blue dots.\nLet's step it up a notch.\nWe'll go to four classes.\nWatch out, everyone.\nAnd we're going to go number of features will be two.\nSo we have the same number of features.\nAnd then the random seed is going to be 42.\nYou might be wondering why these are capitalized.\nWell, generally, if we do have some hyper parameters that we say set at the start of a notebook,\nyou'll find it's quite common for people to write them as capital letters just to say\nthat, hey, these are some settings that you can change.\nYou don't have to, but I'm just going to introduce that anyway because you might stumble upon it yourself.\nSo create multi-class data.\nWe're going to use the make blobs function here.\nSo we're going to create some x blobs, some feature blobs and some label blobs.\nLet's see what these look like in a second.\nI know I'm just saying blobs a lot.\nBut we pass in here, none samples.\nHow many do we want?\nLet's create a thousand as well.\nThat could really be a hyper parameter, but we'll just leave that how it is for now.\nNumber of features is going to be num features.\nCentres equals num classes.\nSo we're going to create four classes because we've set up num classes equal to four.\nAnd then we're going to go center standard deviation.\nWe'll give them a little shake up, add a little bit of randomness in here.\nGive the clusters a little shake up.\nWe'll mix them up a bit.\nMake it a bit hard for our model.\nBut we'll see what this does in a second.\nRandom state equals random seed, which is our favorite random seed 42.\nOf course, you can set it whatever number you want, but I like 42.\nOh, and we need a comma here, of course.\nBeautiful.\nNow, what do we have to do here?\nWell, because we're using scikit-learn and scikit-learn leverages NumPy.\nSo let's turn our data into tenses.\nTurn data into tenses.\nAnd how do we do that?\nWell, we grab x blob and we call torch from NumPy from NumPy.\nIf I could type, that would be fantastic.\nThat's all right.\nWe're doing pretty well today.\nHaven't made too many typos.\nWe did make a few in a couple of videos before, but hey.\nI'm only human.\nSo we're going to torch from NumPy and we're going to pass in the y blob.\nAnd we'll turn it into torch dot float because remember NumPy defaults as float 64, whereas\nPyTorch likes float 32.\nSo split into training and test.\nAnd we're going to create x blob train y or x test.\nx blob test.\nWe'll keep the blob nomenclature here.\ny blob train and y blob test.\nAnd here's again where we're going to leverage the train test split function from scikit-learn.\nSo thank you for that scikit-learn.\nx blob and we're going to pass the y blob.\nSo features, labels, x is the features, y are the labels.\nAnd a test size, we've been using a test size of 20%.\nThat means 80% of the data will be for the training data.\nThat's a fair enough split with our data set.\nAnd we're going to set the random seed to random seed because generally normally train test split is random,\nbut because we want some reproducibility here, we're passing random seeds.\nFinally, we need to get visual.\nSo let's plot the data.\nRight now we've got a whole bunch of code and a whole bunch of talking, but not too much visuals going on.\nSo we'll write down here, visualize, visualize, visualize.\nAnd we can call in plot.figure.\nWhat size do we want?\nI'm going to use my favorite hand in poker, which is 10-7, because it's generally worked out to be a good plot size.\nIn my experience, anyway, we'll go x blob.\nAnd we want the zero index here, and then we'll grab x blob as well.\nAnd you might notice that we're visualizing the whole data set here.\nThat's perfectly fine.\nWe could visualize, train and test separately if we really wanted to, but I'll leave that as a level challenge to you.\nAnd we're going to go red, yellow, blue.\nWonderful.\nWhat do we get wrong?\nOh, of course we got something wrong.\nSanta STD, did we spell center wrong?\nCluster STD.\nThat's what I missed.\nSo, cluster STD.\nStandard deviation.\nWhat do we get wrong?\nRandom seed.\nOh, this needs to be random state.\nOh, another typo.\nYou know what?\nJust as I said, I wasn't getting too many typos.\nI'll get three.\nThere we go.\nLook at that.\nOur first multi-class classification data set.\nSo if we set this to zero, what does it do to our clusters?\nLet's take note of what's going on here, particularly the space between all of the dots.\nNow, if we set this cluster STD to zero, what happens?\nWe get dots that are really just, look at that.\nThat's too easy.\nLet's mix it up, all right?\nNow, you can pick whatever value you want here.\nI'm going to use 1.5, because now we need to build a model that's going to draw some lines between these four colors.\nTwo axes, four different classes.\nBut it's not going to be perfect because we've got some red dots that are basically in the blue dots.\nAnd so, what's our next step?\nWell, we've got some data ready.\nIt's now time to build a model.\nSo, I'll see you in the next video.\nLet's build our first multi-class classification model.\nWelcome back.\nIn the last video, we created our multi-class classification data set,\nusing scikit-learn's make-blobs function.\nAnd now, why are we doing this?\nWell, because we're going to put all of what we've covered so far together.\nBut instead of using binary classification or working with binary classification data,\nwe're going to do it with multi-class classification data.\nSo, with that being said, let's get into building our multi-class classification model.\nSo, we'll create a little heading here.\nBuilding a multi-class classification model in PyTorch.\nAnd now, I want you to have a think about this.\nWe spent the last few videos covering non-linearity.\nDoes this data set need non-linearity?\nAs in, could we separate this data set with pure straight lines?\nOr do we need some non-straight lines as well?\nHave a think about that.\nIt's okay if you're not sure, we're going to be building a model to fit this data anyway,\nor draw patterns in this data anyway.\nAnd now, before we get into coding a model,\nso for multi-class classification, we've got this.\nFor the input layer shape, we need to define the in features.\nSo, how many in features do we have for the hidden layers?\nWell, we could set this to whatever we want, but we're going to keep it nice and simple for now.\nFor the number of neurons per hidden layer, again, this could be almost whatever we want,\nbut because we're working with a relatively small data set,\nwe've only got four different classes, we've only got a thousand data points,\nwe'll keep it small as well, but you could change this.\nRemember, you can change any of these because they're hyper parameters.\nFor the output layer shape, well, how many output features do we want?\nWe need one per class, how many classes do we have?\nWe have four clusters of different dots here, so we'll need four output features.\nAnd then if we go back, we have an output activation of softmax, we haven't seen that yet,\nand then we have a loss function, rather than binary cross entropy, we have cross entropy.\nAnd then optimizer as well is the same as binary classification, two of the most common\nare SGDs, stochastic gradient descent, or the atom optimizer,\nbut of course, the torch.optim package has many different options as well.\nSo let's push forward and create our first multi-class classification model.\nFirst, we're going to create, we're going to get into the habit of creating\ndevice agnostic code, and we'll set the device here, equals CUDA,\nnothing we haven't seen before, but again, we're doing this to put it all together,\nso that we have a lot of practice.\nIs available, else CPU, and let's go device.\nSo we should have a GPU available, beautiful CUDA.\nNow, of course, if you don't, you can go change runtime type, select GPU here,\nthat will restart the runtime, you'll have to run all of the code that's before this cell as well,\nbut I'm going to be using a GPU.\nYou don't necessarily need one because our data set's quite small,\nand our models aren't going to be very large, but we set this up so we have device agnostic code.\nAnd so let's build a multi-class classification model.\nLook at us go, just covering all of the foundations of classification in general here,\nand we now know that we can combine linear and non-linear functions to create\nneural networks that can find patterns in almost any kind of data.\nSo I'm going to call my class here blob model, and it's going to, of course,\ninherit from nn.module, and we're going to upgrade our class here.\nWe're going to take some inputs here, and I'll show you how to do this.\nIf you're familiar with Python classes, you would have already done stuff like this,\nbut we're going to set some parameters for our models,\nbecause as you write more and more complex classes, you'll want to take inputs here.\nAnd I'm going to pre-build the, or pre-set the hidden units parameter to eight.\nBecause I've decided, you know what, I'm going to start off with eight hidden units,\nand if I wanted to change this to 128, I could.\nBut in the constructor here, we've got some options.\nSo we have input features.\nWe're going to set these programmatically as inputs to our class when we instantiate it.\nThe same with output features as well.\nAnd so here, we're going to call self.\nOh, no, super.\nSorry.\nI always get this mixed up dot init.\nAnd underscore underscore.\nBeautiful.\nSo we could do a doc string here as well.\nSo let's write in this.\nInitializes multi-class classification.\nIf I could spell class e-fication model.\nOh, this is great.\nAnd then we have some arcs here.\nThis is just a standard way of writing doc strings.\nIf you want to find out, this is Google Python doc string guide.\nThere we go.\nGoogle Python style guide.\nThis is where I get mine from.\nYou can scroll through this.\nThis is just a way to write Python code.\nYeah, there we go.\nSo we've got a little sentence saying what's going on.\nWe've got arcs.\nWe've got returns and we've got errors if something's going on.\nSo I highly recommend checking that out.\nJust a little tidbit.\nSo this is if someone was to use our class later on.\nThey know what the input features are.\nInput features, which is an int, which is number of input features to the model.\nAnd then, of course, we've got output features, which is also an int.\nWhich is number of output features of the model.\nAnd we've got the red line here is telling us we've got something wrong, but that's okay.\nAnd then the hidden features.\nOh, well, this is number of output classes for the case of multi-class classification.\nAnd then the hidden units.\nInt and then number of hidden units between layers and then the default is eight.\nBeautiful.\nAnd then under that, we'll just do that.\nIs that going to fix itself?\nYeah, there we go.\nWe could put in what it returns.\nReturns, whatever it returns.\nAnd then an example use case, but I'll leave that for you to fill out.\nIf you like.\nSo let's instantiate some things here.\nWhat we might do is write self dot linear layer stack.\nSelf dot linear layer stack.\nAnd we will set this as nn dot sequential.\nOoh, we haven't seen this before.\nBut we're just going to look at a different way of writing a model here.\nPreviously, when we created a model, what did we do?\nWell, we instantiated each layer as its own parameter here.\nAnd then we called on them one by one, but we did it in a straightforward fashion.\nSo that's why we're going to use sequential here to just step through our layers.\nWe're not doing anything too fancy, so we'll just set up a sequential stack of layers here.\nAnd recall that sequential just steps through, passes the data through each one of these layers one by one.\nAnd because we've set up the parameters up here, input features can equal to input features.\nAnd output features, what is this going to be?\nIs this going to be output features or is this going to be hidden units?\nIt's going to be hidden units because it's not the final layer.\nWe want the final layer to output our output features.\nSo input features, this will be hidden units because remember the subsequent layer needs to line up with the previous layer.\nOutput features, we're going to create another one that outputs hidden units.\nAnd then we'll go in n.linear in features equals hidden units because it takes the output features of the previous layer.\nSo as you see here, the output features of this feeds into here.\nThe output features of this feeds into here.\nAnd then finally, this is going to be our final layer.\nWe'll do three layers.\nOutput features equals output features.\nWonderful. So how do we know the values of each of these?\nWell, let's have a look at xtrain.shape and ytrain.shape.\nSo in the case of x, we have two input features.\nAnd in the case of y, well, this is a little confusing as well because y is a scalar.\nBut what do you think the values for y are going to be?\nWell, let's go NP. Or is there torch.unique? I'm not sure. Let's find out together, hey?\nTorch unique.\nZero on one, ytrain. Oh, we need y blob train. That's right, blob.\nI'm too used to writing blob.\nAnd we need blob train, but I believe it's the same here.\nAnd then blob.\nThere we go. So we have four classes.\nSo we need an output features value of four.\nAnd now if we wanted to add nonlinearity here, we could put it in between our layers here like this.\nBut I asked the question before, do you think that this data set needs nonlinearity?\nWell, let's leave it in there to begin with.\nAnd one of the challenges for you, oh, do we need commerce here?\nI think we need commerce here.\nOne of the challenges for you will be to test the model with nonlinearity\nand without nonlinearity.\nSo let's just leave it in there for the time being.\nWhat's missing from this?\nWell, we need a forward method.\nSo def forward self X. What can we do here?\nWell, because we've created this as a linear layer stack using nn.sequential,\nwe can just go return linear layer stack and pass it X.\nAnd what's going to happen?\nWhatever input goes into the forward method is just going to go through these layers sequentially.\nOh, we need to put self here because we've initialized it in the constructor.\nBeautiful.\nAnd now let's create an instance of blob model and send it to the target device.\nWe'll go model four equals blob model.\nAnd then we can use our input features parameter, which is this one here.\nAnd we're going to pass it a value of what?\nTwo.\nAnd then output features. Why? Because we have two X features.\nNow, the output feature is going to be the same as the number of classes that we have for.\nIf we had 10 classes, we'd set it to 10.\nSo we'll go four.\nAnd then the hidden units is going to be eight by default.\nSo we don't have to put this here, but we're going to put it there anyway.\nAnd then, of course, we're going to send this to device.\nAnd then we're going to go model four.\nWhat do we get wrong here?\nUnexpected keyword argument output features.\nDo we spell something wrong?\nNo doubt. We've got a spelling mistake.\nOutput features. Output features.\nOh, out features.\nAh, that's what we needed. Out features, not output.\nI've got a little confused there.\nOkay.\nThere we go. Okay, beautiful.\nSo just recall that the parameter here for an end up linear.\nDid you pick up on that?\nIs out features not output features.\nOutput features, a little confusing here, is our final layout output layers number of features there.\nSo we've now got a multi-class classification model that lines up with the data that we're using.\nSo the shapes line up. Beautiful.\nWell, what's next?\nWell, we have to create a loss function. And, of course, a training loop.\nSo I'll see you in the next few videos. And let's do that together.\nWelcome back. In the last video, we created our multi-class classification model.\nAnd we did so by subclassing an end up module.\nAnd we set up a few parameters for our class constructor here.\nSo that when we made an instance of the blob model, we could customize the input features.\nThe output features. Remember, this lines up with how many features X has.\nAnd the output features here lines up with how many classes are in our data.\nSo if we had 10 classes, we could change this to 10. And it would line up.\nAnd then if we wanted 128 hidden units, well, we could change that.\nSo we're getting a little bit more programmatic with how we create models here.\nAnd as you'll see later on, a lot of the things that we've built in here can also be functionalized in a similar matter.\nBut let's keep pushing forward. What's our next step?\nIf we build a model, if we refer to the workflow, you'd see that we have to create a loss function.\nAnd an optimizer for a multi-class classification model.\nAnd so what's our option here for creating a loss function?\nWhere do we find loss functions in PyTorch? I'm just going to get out of this.\nAnd I'll make a new tab here. And if we search torch.nn\nBecause torch.nn is the basic building box for graphs. In other words, neural networks.\nWhere do we find loss functions? Hmm, here we go. Beautiful.\nSo we've seen that L1 loss or MSE loss could be used for regression, predicting a number.\nAnd I'm here to tell you as well that for classification, we're going to be looking at cross entropy loss.\nNow, this is for multi-class classification. For binary classification, we work with BCE loss.\nAnd of course, there's a few more here, but I'm going to leave that as something that you can explore on your own.\nLet's jump in to cross entropy loss.\nSo what do we have here? This criterion computes. Remember, a loss function in PyTorch is also referred to as a criterion.\nYou might also see loss function referred to as cost function, C-O-S-T.\nBut I call them loss functions. So this criterion computes the cross entropy loss between input and target.\nOkay, so the input is something, and the target is our target labels.\nIt is useful when training a classification problem with C classes. There we go.\nSo that's what we're doing. We're training a classification problem with C classes, C is a number of classes.\nIf provided the optional argument, weight should be a 1D tensor assigning a weight to each of the classes.\nSo we don't have to apply a weight here, but why would you apply a weight? Well, it says, if we look at weight here,\nthis is particularly useful when you have an unbalanced training set. So just keep this in mind as you're going forward.\nIf you wanted to train a dataset that has imbalanced samples, in our case we have the same number of samples for each class,\nbut sometimes you might come across a dataset with maybe you only have 10 yellow dots.\nAnd maybe you have 500 blue dots and only 100 red and 100 light blue dots.\nSo you have an unbalanced dataset. So that's where you can come in and have a look at the weight parameter here.\nBut for now, we're just going to keep things simple. We have a balanced dataset, and we're going to focus on using this loss function.\nIf you'd like to read more, please, you can read on here. And if you wanted to find out more, you could go, what is cross entropy loss?\nAnd I'm sure you'll find a whole bunch of loss functions. There we go. There's the ML cheat sheet. I love that.\nThe ML glossary, that's one of my favorite websites. Towards data science, you'll find that website, Wikipedia.\nMachine learning mastery is also another fantastic website. But you can do that all in your own time.\nLet's code together, hey. We'll set up a loss function. Oh, and one more resource before we get into code is that we've got the architecture,\nwell, the typical architecture of a classification model. The loss function for multi-class classification is cross entropy or torch.nn.cross entropy loss.\nLet's code it out. If in doubt, code it out. So create a loss function for multi-class classification.\nAnd then we go, loss fn equals, and then dot cross entropy loss. Beautiful. And then we want to create an optimizer.\nCreate an optimizer for multi-class classification. And then the beautiful thing about optimizers is they're quite flexible.\nThey can go across a wide range of different problems. So the optimizer. So two of the most common, and I say most common because they work quite well.\nAcross a wide range of problems. So that's why I've only listed two here. But of course, within the torch dot opt in module, you will find a lot more different optimizers.\nBut let's stick with SGD for now. And we'll go back and go optimizer equals torch dot opt in for optimizer SGD for stochastic gradient descent.\nThe parameters we want our optimizer to optimize model four, we're up to our fourth model already. Oh my goodness.\nModel four dot parameters. And we'll set the learning rate to 0.1. Of course, you could change the learning rate if you wanted to.\nIn fact, I'd encourage you to see what happens if you do because why the learning rate is a hyper parameter.\nI'm better at writing code than I am at spelling. You can change. Wonderful. So we've now got a loss function and an optimizer for a multi class classification problem.\nWhat's next? Well, we could start to build.\nBuilding a training loop. We could start to do that, but I think we have a look at what the outputs of our model are.\nSo more specifically, so getting prediction probabilities for a multi class pie torch model.\nSo my challenge to you before the next video is to have a look at what happens if you pass x blob test through a model.\nAnd remember, what is a model's raw output? What is that referred to as?\nOh, I'll let you have a think about that before the next video. I'll see you there.\nWelcome back. In the last video, we created a loss function and an optimizer for our multi class classification model.\nAnd recall the loss function measures how wrong our model's predictions are.\nAnd the optimizer optimizer updates our model parameters to try and reduce the loss.\nSo that's what that does. And I also issued the challenge of doing a forward pass with model four, which is the most recent model that we created.\nAnd oh, did I just give you some code that wouldn't work? Did I do that on purpose? Maybe, maybe not, you'll never know.\nSo if this did work, what are the raw outputs of our model? Let's get some raw outputs of our model.\nAnd if you recall, the raw outputs of a model are called logits.\nSo we got a runtime error expected. All tensors to be on the same device are of course. Why did this come up?\nWell, because if we go next model for dot parameters, and if we check device, what happens here?\nOh, we need to bring this in. Our model is on the CUDA device, whereas our data is on the CPU still.\nCan we go X? Is our data a tensor? Can we check the device parameter of that? I think we can.\nI might be proven wrong here. Oh, it's on the CPU. Of course, we're getting a runtime error.\nDid you catch that one? If you did, well done. So let's see what happens.\nBut before we do a forward pass, how about we turn our model into a vowel mode to make some predictions with torch dot inference mode?\nWe'll make some predictions. We don't necessarily have to do this because it's just tests, but it's a good habit.\nOh, why prads? Equals, what do we get? Why prads? And maybe we'll just view the first 10.\nWhat do we get here? Oh, my goodness. How much are numbers on a page? Is this the same format as our data or our test labels?\nLet's have a look. No, it's not. Okay. Oh, we need why blob test. Excuse me.\nWe're going to make that mistake a fair few times here. So we need to get this into the format of this. Hmm.\nHow can we do that? Now, I want you to notice one thing as well is that we have one value here per one value, except that this is actually four values.\nNow, why is that? We have one, two, three, four. Well, that is because we set the out features up here. Our model outputs four features per sample.\nSo each sample right now has four numbers associated with it. And what are these called? These are the logits.\nNow, what we have to do here, so let's just write this down in order to evaluate and train and test our model.\nWe need to convert our model's outputs, outputs which are logits to prediction probabilities, and then to prediction labels.\nSo we've done this before, but for binary classification. So we have to go from logits to predprobs to pred labels.\nAll right, I think we can do this. So we've got some logits here. Now, how do we convert these logits to prediction probabilities?\nWell, we use an activation function. And if we go back to our architecture, what's our output activation here?\nFor a binary classification, we use sigmoid. But for multi-class classification, these are the two main differences between multi-class classification and binary classification.\nOne uses softmax, one uses cross entropy. And it's going to take a little bit of practice to know this off by heart.\nIt took me a while, but that's why we have nice tables like this. And that's why we write a lot of code together.\nSo we're going to use a softmax function here to convert out logits. Our models raw outputs, which is this here, to prediction probabilities.\nAnd let's see that. So convert our models, logit outputs to prediction probabilities.\nSo let's create why predprobs. So I like to call prediction probabilities predprobs for short.\nSo torch dot softmax. And then we go why logits. And we want it across the first dimension.\nSo let's have a look. If we print why logits, we'll get the first five values there. And then look at the conversion here.\nWhy logits? Oh, why predprobs? That's what we want to compare. Predprobs. Five. Let's check this out.\nOh, what did we get wrong here? Why logits? Do we have why logits? Oh, no. We should change this to why logits, because really that's the raw output of our model here.\nWhy logits? Let's rerun that. Check that. We know that these are different to these, but we ideally want these to be in the same format as these, our test labels.\nThese are our models predictions. And now we should be able to convert. There we go. Okay, beautiful. What's happening here? Let's just get out of this.\nAnd we will add a few code cells here. So we have some space. Now, if you wanted to find out what's happening with torch dot softmax, what could you do?\nWe could go torch softmax. See what's happening. Softmax. Okay, so here's the function that's happening. We replicated some nonlinear activation functions before.\nSo if you wanted to replicate this, what could you do? Well, if in doubt, code it out. You could code this out. You've got the tools to do so.\nWe've got softmax to some X input takes the exponential of X. So torch exponential over the sum of torch exponential of X. So I think you could code that out if you wanted to.\nBut let's for now just stick with what we've got. We've got some logits here, and we've got some softmax, some logits that have been passed through the softmax function.\nSo that's what's happened here. We've passed our logits as the input here, and it's gone through this activation function.\nThese are prediction probabilities. And you might be like, Daniel, these are still just numbers on a page. But you also notice that none of them are negative.\nOkay, and there's another little tidbit about what's going on here. If we sum one of them up, let's get the first one.\nWill this work? And if we go torch dot sum, what happens?\nOoh, they all sum up to one. So that's one of the effects of the softmax function. And then if we go torch dot max of Y-pred probes.\nSo this is a prediction probability.\nFor multi class, you'll find that for this particular sample here, the 0th sample, this is the maximum number. And so our model, what this is saying is our model is saying, this is the prediction probability.\nThis is how much I think it is class 0. This number here, it's in order. This is how much I think it is class 1. This is how much I think it is class 2.\nThis is how much I think it is class 3. And so we have one value for each of our four classes, a little bit confusing because it's 0th indexed.\nBut the maximum value here is this index. And so how would we get the particular index value of whatever the maximum number is across these values?\nWell, we can take the argmax and we get tensor 1. So for this particular sample, this one here, our model, and these guesses or these predictions aren't very good.\nWhy is that? Well, because our model is still just predicting with random numbers, we haven't trained it yet. So this is just random output here, basically.\nBut for now, the premise still remains that our model thinks that for this sample using random numbers, it thinks that index 1 is the right class or class number 1 for this particular sample.\nAnd then for this next one, what's the maximum number here? I think it would be the 0th index and the same for the next one. What's the maximum number here?\nWell, it would be the 0th index as well. But of course, these numbers are going to change once we've trained our model.\nSo how do we get the maximum index value of all of these? So this is where we can go, convert our model's prediction probabilities to prediction labels.\nSo let's do that. We can go ypreds equals torch dot argmax on ypredprobs. And if we go across the first dimension as well. So now let's have a look at ypreds.\nDo we have prediction labels in the same format as our ylob test? Beautiful. Yes, we do. Although many of them are wrong, as you can see, ideally they would line up with each other.\nBut because our model is predicting or making predictions with random numbers, so they haven't been our model hasn't been trained. All of these are basically random outputs.\nSo hopefully once we train our model, it's going to line up the values of the predictions are going to line up with the values of the test labels.\nBut that is how we go from our model's raw outputs to prediction probabilities to prediction labels for a multi-class classification problem.\nSo let's just add the steps here, logits, raw output of the model, predprobs, to get the prediction probabilities, use torch dot softmax or the softmax activation function, pred labels, take the argmax of the prediction probabilities.\nSo we're going to see this in action later on when we evaluate our model, but I feel like now that we know how to go from logits to prediction probabilities to pred labels, we can write a training loop.\nSo let's set that up. 8.5, create a training loop, and testing loop for a multi-class pytorch model. This is so exciting.\nI'll see you in the next video. Let's build our first training and testing loop for a multi-class pytorch model, and I'll give you a little hint.\nIt's quite similar to the training and testing loops we've built before, so you might want to give it a shot. I think you can.\nOtherwise, we'll do it together in the next video.\nWelcome back. In the last video, we covered how to go from raw logits, which is the output of the model, the raw output of the model for a multi-class pytorch model.\nThen we turned our logits into prediction probabilities using torch.softmax, and then we turn those prediction probabilities into prediction labels by taking the argmax, which returns the index of where the maximum value occurs in the prediction probability.\nSo for this particular sample, with these four values, because it outputs four values, because we're working with four classes, if we were working with 10 classes, it would have 10 values, the principle of these steps would still be the same.\nSo for this particular sample, this is the value that's the maximum, so we would take that index, which is 1. For this one, the index 0 has the maximum value.\nFor this sample, same again, and then same again, I mean, these prediction labels are just random, right? So they're quite terrible.\nBut now we're going to change that, because we're going to build a training and testing loop for our multi-class model.\nLet's do that. So fit the multi-class model to the data.\nLet's go set up some manual seeds.\nTorch dot manual seed, again, don't worry too much if our numbers on the page are not exactly the same. That's inherent to the randomness of machine learning.\nWe're setting up the manual seeds to try and get them as close as possible, but these do not guarantee complete determinism, which means the same output.\nBut we're going to try. The direction is more important.\nSet number of epochs. We're going to go epochs. How about we just do 100? I reckon we'll start with that. We can bump it up to 1000 if we really wanted to.\nLet's put the data to the target device. What's our target device? Well, it doesn't really matter because we've set device agnostic code.\nSo whether we're working with a CPU or a GPU, our code will use whatever device is available. I'm typing blog again.\nSo we've got x blob train, y blob train. This is going to go where? It's going to go to the device.\nAnd y blob train to device. And we're going to go x blob test. And then y blob test equals x blob test to device.\nOtherwise, we'll get device issues later on, and we'll send this to device as well. Beautiful. Now, what do we do now? Well, we loop through data.\nLoop through data. So for an epoch in range epochs for an epoch in a range.\nEpox. I don't want that auto correct. Come on, Google Colab. Work with me here.\nWe're training our first multi-class classification model. This is serious business. No, I'm joking. It's actually quite fun.\nSo model four dot train. And let's do the forward pass. I'm not going to put much commentary here because we've been through this before.\nBut what are the logits? The logits are raw outputs of our model. So we'll just go x blob train.\nAnd x test. I didn't want that. X blob train. Why did that do that? I need to turn off auto correct in Google Colab. I've been saying it for a long time.\nY pred equals torch dot softmax. So what are we doing here? We're going from logits to prediction probabilities here.\nSo torch softmax. Y logits. Across the first dimension. And then we can take the argmax of this and dim equals one.\nIn fact, I'm going to show you a little bit of, oh, I've written blog here. Maybe auto correct would have been helpful for that.\nA little trick. You don't actually have to do the torch softmax. The logits. If you just took the argmax of the logits is a little test for you.\nJust take the argmax of the logits. And see, do you get the same similar outputs as what you get here?\nSo I've seen that done before, but for completeness, we're going to use the softmax activation function because you'll often see this in practice.\nAnd now what do we do? We calculate the loss. So the loss FM. We're going to use categorical cross entropy here or just cross entropy loss.\nSo if we check our loss function, what do we have? We have cross entropy loss. We're going to compare our models, logits to y blob train.\nAnd then what are we going to do? We're going to calculate the accuracy because we're working with the classification problem.\nIt'd be nice if we had accuracy as well as loss. Accuracy is one of the main classification evaluation metrics.\ny pred equals y pred. y pred. And now what do we do? Well, we have to zero grab the optimizer. Optimizer zero grad.\nThen we go loss backward. And then we step the optimizer. Optimizer step, step, step.\nSo none of these steps we haven't covered before. We do the forward pass. We calculate the loss and any evaluation metric we choose to do so.\nWe zero the optimizer. We perform back propagation on the loss. And we step the optimizer.\nThe optimizer will hopefully behind the scenes update the parameters of our model to better represent the patterns in our training data.\nAnd so we're going to go testing code here. What do we do for testing code? Well, or inference code.\nWe set our model to a vowel mode.\nThat's going to turn off a few things behind the scenes that our model doesn't need such as dropout layers, which we haven't covered.\nBut you're more than welcome to check them out if you go torch and end.\nDropout layers. Do we have dropout? Dropout layers. Beautiful. And another one that it turns off is match norm.\nBeautiful. And also you could search this. What does model dot a vowel do?\nAnd you might come across stack overflow question. One of my favorite resources.\nSo there's a little bit of extra curriculum. But I prefer to see things in action.\nSo with torch inference mode, again, this turns off things like gradient tracking and a few more things.\nSo we get as fast as code as possible because we don't need to track gradients when we're making predictions.\nWe just need to use the parameters that our model has learned.\nWe want X blob test to go to our model here for the test logits. And then for the test preds, we're going to do the same step as what we've done here.\nWe're going to go torch dot softmax on the test logits across the first dimension.\nAnd we're going to call the argmax on that to get the index value of where the maximum prediction probability value occurs.\nWe're going to calculate the test loss or loss function. We're going to pass in what the test logits here.\nThen we're going to pass in why blob test compare the test logits behind the scenes.\nOur loss function is going to do some things that convert the test logits into the same format as our test labels and then return us a value for that.\nThen we'll also calculate the test accuracy here by passing in the why true as why blob test.\nAnd we have the y pred equals y pred.\nWonderful.\nAnd then what's our final step?\nWell, we want to print out what's happening because I love seeing metrics as our model trains.\nIt's one of my favorite things to watch.\nIf we go if epoch, let's do it every 10 epochs because we've got 100 so far.\nIt equals zero.\nLet's print out a nice f string with epoch.\nAnd then we're going to go loss.\nWhat do we put in here?\nWe'll get our loss value, but we'll take it to four decimal places and we'll get the training accuracy, which will be acc.\nAnd we'll take this to two decimal places and we'll get a nice percentage sign there.\nAnd we'll go test loss equals test loss and we'll go there.\nAnd finally, we'll go test act at the end here, test act.\nNow, I'm sure by now we've written a fair few of these.\nYou're either getting sick of them or you're like, wow, I can actually do the steps through here.\nAnd so don't worry, we're going to be functionalizing all of this later on,\nbut I thought I'm going to include them as much as possible so that we can practice as much as possible together.\nSo you ready?\nWe're about to train our first multi-class classification model.\nIn three, two, one, let's go.\nNo typos.\nOf course.\nWhat do we get wrong here?\nOh, this is a fun error.\nRuntime error.\nNLL loss for reduced CUDA kernel to the index not implemented for float.\nOkay, that's a pretty full on bunch of words there.\nI don't really know how to describe that.\nBut here's a little hint.\nWe've got float there.\nSo we know that float is what?\nFloat is a form of data.\nIt's a data type.\nSo potentially because that's our hint.\nWe said not implemented for float.\nSo maybe we've got something wrong up here.\nOur data is of the wrong type.\nCan you see anywhere where our data might be the wrong type?\nWell, let's print it out.\nWhere's our issue here?\nWhy logits?\nWhy blob train?\nOkay.\nWhy blob train?\nAnd why logits?\nWhat does why blob train look like?\nWhy blob train?\nOkay.\nAnd what's the D type here?\nFloat.\nOkay.\nSo it's not implemented for float.\nHmm.\nMaybe we have to turn them into a different data type.\nWhat if we went type torch long tensor?\nWhat happens here?\nExpected all tensors to be on the same device but found at least two devices.\nOh, my goodness.\nWhat do we got wrong here?\nType torch long tensor.\nFriends.\nGuess what?\nI found it.\nAnd so it was to do with this pesky little data type issue here.\nSo if we run this again and now this one took me a while to find and I want you to know that,\nthat behind the scenes, even though, again, this is a machine learning cooking show,\nit still takes a while to troubleshoot code and you're going to come across this.\nBut I thought rather than spend 10 minutes doing it in a video, I'll show you what I did.\nSo we went through this and we found that, hmm, there's something going on here.\nI don't quite know what this is.\nAnd that seems quite like a long string of words, not implemented for float.\nAnd then we looked back at the line where it went wrong.\nAnd so that we know that maybe the float is hinting at that one of these two tensors is of the wrong data type.\nNow, why would we think that it's the wrong data type?\nWell, because anytime you see float or int or something like that, it generally hints at one of your data types being wrong.\nAnd so the error is actually right back up here where we created our tensor data.\nSo we turned our labels here into float, which generally is okay in PyTorch.\nHowever, this one should be of type torch dot long tensor, which we haven't seen before.\nBut if we go into torch long tensor, let's have a look torch dot tensor.\nDo we have long tensor?\nHere we go.\n64 bit integer signed.\nSo why do we need torch dot long tensor?\nAnd again, this took me a while to find.\nAnd so I want to express this that in your own code, you probably will butt your head up against some issues and errors that do take you a while to find.\nAnd data types is one of the main ones.\nBut if we look in the documentation for cross entropy loss, the way I kind of found this out was this little hint here.\nThe performance of the criteria is generally better when the target contains class indices, as this allows for optimized computation.\nBut I read this and it says target contains class indices.\nI'm like, hmm, alza indices already, but maybe they should be integers and not floats.\nBut then if you actually just look at the sample code, you would find that they use d type equals torch dot long.\nNow, that's the thing with a lot of code around the internet is that sometimes the answer you're looking for is a little bit buried.\nBut if in doubt, run the code and butt your head up against a wall for a bit and keep going.\nSo let's just rerun all of this and see do we have an error here?\nLet's train our first multi-class classification model together.\nNo arrows, fingers crossed.\nBut what did we get wrong here?\nOK, so we've got different size.\nWe're slowly working through all of the errors in deep learning here.\nValue error, input batch size 200 to match target size 200.\nSo this is telling me maybe our test data, which is of size 200, is getting mixed up with our training data, which is of size 800.\nSo we've got test loss, the test logits, model four.\nWhat's the size?\nLet's print out print test logits dot shape and wine blob test.\nSo troubleshooting on the fly here, everyone.\nWhat do we got?\nTorch size 800.\nWhere are our test labels coming from?\nWine blob test equals, oh, there we go.\nAh, did you catch that before?\nMaybe you did, maybe you didn't.\nBut I think we should be right here.\nNow if we just comment out this line, so we've had a data type issue and we've had a shape issue.\nTwo of the main and machine learning, oh, and again, we've had some issues.\nWine blob test.\nWhat's going on here?\nI thought we just changed the shape.\nOh, no, we have to go up and reassign it again because now this is definitely why blob, yes.\nLet's rerun all of this, reassign our data.\nWe are running into every single error here, but I'm glad we're doing this because otherwise you might not see how to\ntroubleshoot these type of things.\nSo the size of a tensor much match the size.\nOh, we're getting the issue here.\nTest spreads.\nOh, my goodness.\nWe have written so much code here.\nTest spreads.\nSo instead of wire spread, this should be test spreads.\nFingers crossed.\nAre we training our first model yet or what?\nThere we go.\nOkay, I'm going to printing out some stuff.\nI don't really want to print out that stuff.\nI want to see the loss go down, so I'm going to.\nSo friends, I hope you know that we've just been through some of the most fundamental troubleshooting steps.\nAnd you might say, oh, Daniel, there's a cop out because you're just coding wrong.\nAnd in fact, I code wrong all the time.\nBut we've now worked out how to troubleshoot them shape errors and data type errors.\nBut look at this.\nAfter all of that, thank goodness.\nOur loss and accuracy go in the directions that we want them to go.\nSo our loss goes down and our accuracy goes up.\nBeautiful.\nSo it looks like that our model is working on a multi-class classification data set.\nSo how do we check that?\nWell, we're going to evaluate it in the next step by visualize, visualize, visualize.\nSo you might want to give that a shot.\nSee if you can use our plot decision boundary function.\nWe'll use our model to separate the data here.\nSo it's going to be much the same as what we did for binary classification.\nBut this time we're using a different model and a different data set.\nI'll see you there.\nWelcome back.\nIn the last video, we went through some of the steps that we've been through before\nin terms of training and testing a model.\nBut we also butted our heads up against two of the most common issues in machine learning and deep learning in general.\nAnd that is data type issues and shape issues.\nBut luckily we were able to resolve them.\nAnd trust me, you're going to run across many of them in your own deep learning and machine learning endeavors.\nSo I'm glad that we got to have a look at them and sort of I could show you what I do to troubleshoot them.\nBut in reality, it's a lot of experimentation.\nRun the code, see what errors come out, Google the errors, read the documentation, try again.\nBut with that being said, it looks like that our model, our multi-class classification model has learned something.\nThe loss is going down, the accuracy is going up.\nBut we can further evaluate this by making and evaluating predictions with a PyTorch multi-class model.\nSo how do we make predictions?\nWe've seen this step before, but let's reiterate.\nMake predictions, we're going to set our model to what mode, a vowel mode.\nAnd then we're going to turn on what context manager, inference mode.\nBecause we want to make inference, we want to make predictions.\nNow what do we make predictions on?\nOr what are the predictions? They're going to be logits because why?\nThey are the raw outputs of our model.\nSo we'll take model four, which we just trained and we'll pass it the test data.\nWell, it needs to be blob test, by the way.\nI keep getting that variable mixed up.\nWe just had enough problems with the data, Daniel.\nWe don't need any more. You're completely right.\nI agree with you.\nBut we're probably going to come across some more problems in the future.\nDon't you worry about that.\nSo let's view the first 10 predictions.\nWhy logits? What do they look like?\nAll right, just numbers on the page. They're raw logits.\nNow how do we go from go from logits to prediction probabilities?\nHow do we do that?\nWith a multi-class model, we go y-pred-probs equals torch.softmax on the y-logits.\nAnd we want to do it across the first dimension.\nAnd what do we have when we go pred-probs?\nLet's go up to the first 10.\nAre we apples to apples yet?\nWhat does our y-blog test look like?\nWe're not apples to apples yet, but we're close.\nSo these are prediction probabilities.\nYou'll notice that we get some fairly different values here.\nAnd remember, the one closest to one here, the value closest to one,\nwhich looks like it's this, the index of the maximum value\nis going to be our model's predicted class.\nSo this index is index one.\nAnd does it correlate here? Yes.\nOne, beautiful.\nThen we have index three, which is the maximum value here.\nThree, beautiful.\nAnd then we have, what do we have here?\nIndex two, yes.\nOkay, wonderful.\nBut let's not step through that.\nWe're programmers.\nWe can do this with code.\nSo now let's go from pred-probs to pred-labels.\nSo y-pred-equals, how do we do that?\nWell, we can do torch.argmax on the y-pred-probs.\nAnd then we can pass dim equals one.\nWe could also do it this way.\nSo y-pred-probs call dot-argmax.\nThere's no real difference between these two.\nBut we're just going to do it this way, called torch.argmax.\ny-pred-es, let's view the first 10.\nAre we now comparing apples to apples when we go y-blob test?\nYes, we are.\nHave a go at that.\nLook, one, three, two, one, zero, three, one, three, two, one, zero, three.\nBeautiful.\nNow, we could line these up and look at and compare them all day.\nI mean, that would be fun.\nBut I know what something that would be even more fun.\nLet's get visual.\nSo plot dot figure.\nAnd we're going to go fig size equals 12.6, just because the beauty of this\nbeing a cooking show is I kind of know what ingredients work from ahead of time.\nDespite what you saw in the last video with all of that trouble shooting.\nBut I'm actually glad that we did that because seriously.\nShape issues and data type issues.\nYou're going to come across a lot of them.\nThe two are the main issues I troubleshoot, aside from device issues.\nSo let's go x-blob train and y-blob train.\nAnd we're going to do another plot here.\nWe're going to get subplot one, two, two.\nAnd we're going to do this for the test data.\nTest and then plot decision boundary.\nPlot decision boundary with model four on x-blob test and y-blob test as well.\nLet's see this.\nDid we train a multi-class?\nOh my goodness.\nYes, we did.\nOur code worked faster than I can speak.\nLook at that beautiful looking plot.\nWe've separated our data almost as best as what we could.\nLike there's some here that are quite inconspicuous.\nAnd now what's the thing about these lines?\nWith this model have worked, I posed the question a fair few videos ago,\nwhenever we created our multi-class model that could we separate this data\nwithout nonlinear functions.\nSo how about we just test that?\nSince we've got the code ready, let's go back up.\nWe've got nonlinear functions here.\nWe've got relu here.\nSo I'm just going to recreate our model there.\nSo I just took relu out.\nThat's all I did.\nCommented it out, this code will still all work.\nOr fingers crossed it will.\nDon't count your chickens before they hatch.\nDaniel, come on.\nWe're just going to rerun all of these cells.\nAll the code's going to stay the same.\nAll we did was we took the nonlinearity out of our model.\nIs it still going to work?\nOh my goodness.\nIt still works.\nNow why is that?\nWell, you'll notice that the lines are a lot more straighter here.\nDid we get different metrics?\nI'll leave that for you to compare.\nMaybe these will be a little bit different.\nI don't think they're too far different.\nBut that is because our data is linearly separable.\nSo we can draw straight lines only to separate our data.\nHowever, a lot of the data that you deal with in practice\nwill require linear and nonlinear.\nHence why we spent a lot of time on that.\nLike the circle data that we covered before.\nAnd let's look up an image of a pizza.\nIf you're building a food vision model to take photos of food\nand separate different classes of food,\ncould you do this with just straight lines?\nYou might be able to, but I personally don't think\nthat I could build a model to do such a thing.\nAnd in fact, PyTorch makes it so easy to add nonlinearities\nto our model, we might as well have them in\nso that our model can use it if it needs it\nand if it doesn't need it, well, hey,\nit's going to build a pretty good model as we saw before\nif we included the nonlinearities in our model.\nSo we could uncomment these and our model is still\ngoing to perform quite well.\nThat is the beauty of neural networks,\nis that they decide the numbers that should\nrepresent outdated the best.\nAnd so, with that being said, we've evaluated our model,\nwe've trained our multi-class classification model,\nwe've put everything together, we've gone from binary\nclassification to multi-class classification.\nI think there's just one more thing that we should cover\nand that is, let's go here, section number nine,\na few more classification metrics.\nSo, as I said before, evaluating a model,\nlet's just put it here, to evaluate our model,\nour classification models, that is,\nevaluating a model is just as important as training a model.\nSo, I'll see you in the next video.\nLet's cover a few more classification metrics.\nWelcome back.\nIn the last video, we evaluated our\nmulti-class classification model visually.\nAnd we saw that it did pretty darn well,\nbecause our data turned out to be linearly separable.\nSo, our model, even without non-linear functions,\ncould separate the data here.\nHowever, as I said before, most of the data that you deal with\nwill require some form of linear and non-linear function.\nSo, just keep that in mind, and the beauty of PyTorch is\nthat it allows us to create models with linear\nand non-linear functions quite flexibly.\nSo, let's write down here.\nIf we wanted to further evaluate our classification models,\nwe've seen accuracy.\nSo, accuracy is one of the main methods\nof evaluating classification models.\nSo, this is like saying, out of 100 samples,\nhow many does our model get right?\nAnd so, we've seen our model right now\nis that testing accuracy of nearly 100%.\nSo, it's nearly perfect.\nBut, of course, there were a few tough samples,\nwhich I mean a little bit hard.\nSome of them are even within the other samples,\nso you can forgive it a little bit here\nfor not being exactly perfect.\nWhat are some other metrics here?\nWell, we've also got precision,\nand we've also got recall.\nBoth of these will be pretty important\nwhen you have classes with different amounts of values in them.\nSo, precision and recall.\nSo, accuracy is pretty good to use when you have balanced classes.\nSo, this is just text on a page for now.\nF1 score, which combines precision and recall.\nThere's also a confusion matrix,\nand there's also a classification report.\nSo, I'm going to show you a few code examples\nof where you can access these,\nand I'm going to leave it to you as extra curriculum\nto try each one of these out.\nSo, let's go into the keynote.\nAnd by the way, you should pay yourself on the back here\nbecause we've just gone through all of the PyTorch workflow\nfor a classification problem.\nNot only just binary classification,\nwe've done multi-class classification as well.\nSo, let's not stop there, though.\nRemember, building a model,\nevaluating a model is just as important as building a model.\nSo, we've been through non-linearity.\nWe've seen how we could replicate non-linear functions.\nWe've talked about the machine learning explorer's motto,\nvisualize, visualize, visualize.\nMachine learning practitioners motto is experiment, experiment, experiment.\nI think I called that the machine learning or data scientist motto.\nSame thing, you know?\nAnd steps in modeling with PyTorch.\nWe've seen this in practice,\nso we don't need to look at these slides.\nI mean, they'll be available on the GitHub if you want them,\nbut here we are.\nSome common classification evaluation methods.\nSo, we have accuracy.\nThere's the formal formula if you want,\nbut there's also code, which is what we've been focusing on.\nSo, we wrote our own accuracy function, which replicates this.\nBy the way, Tp stands for not toilet paper,\nit stands for true positive,\nTn is true negative, false positive, Fp, false negative, Fn.\nAnd so, the code, we could do torch metrics.\nOh, what's that?\nBut when should you use it?\nThe default metric for classification problems.\nNote, it is not the best for imbalanced classes.\nSo, if you had, for example,\n1,000 samples of one class,\nso, number one, label number one,\nbut you had only 10 samples of class zero.\nSo, accuracy might not be the best to use for then.\nFor imbalanced data sets,\nyou might want to look into precision and recall.\nSo, there's a great article called,\nI think it's beyond accuracy, precision and recall,\nwhich gives a fantastic overview of, there we go.\nThis is what I'd recommend.\nThere we go, by Will Coestron.\nSo, I'd highly recommend this article as some extra curriculum here.\nSee this article for when to use precision recall.\nWe'll go there.\nNow, if we look back,\nthere is the formal formula for precision,\ntrue positive over true positive plus false positive.\nSo, higher precision leads to less false positives.\nSo, if false positives are not ideal,\nyou probably want to increase precision.\nIf false negatives are not ideal,\nyou want to increase your recall metric.\nHowever, you should be aware that there is such thing as a precision recall trade-off.\nAnd you're going to find this in your experimentation.\nPrecision recall trade-off.\nSo, that means that, generally, if you increase precision,\nyou lower recall.\nAnd, inversely, if you increase precision, you lower recall.\nSo, check out that, just to be aware of that.\nBut, again, you're going to learn this through practice of evaluating your models.\nIf you'd like some code to do precision and recall,\nyou've got torchmetrics.precision, or torchmetrics.recall,\nas well as scikit-learn.\nSo scikit-learn has implementations for many different classification metrics.\nTorchmetrics is a PyTorch-like library.\nAnd then we have F1 score, which combines precision and recall.\nSo, it's a good combination if you want something in between these two.\nAnd then, finally, there's a confusion matrix.\nI haven't listed here a classification report, but I've listed it up here.\nAnd we can see a classification report in scikit-learn.\nClassification report.\nClassification report kind of just puts together all of the metrics that we've talked about.\nAnd we can go there.\nBut I've been talking a lot about torchmetrics.\nSo let's look up torchmetrics' accuracy.\nTorchmetrics.\nSo this is a library.\nI don't think it comes with Google Colab at the moment,\nbut you can import torchmetrics, and you can initialize a metric.\nSo we've built our own accuracy function, but the beauty of using torchmetrics\nis that it uses PyTorch-like code.\nSo we've got metric, preds, and target.\nAnd then we can find out what the value of the accuracy is.\nAnd if you wanted to implement your own metrics, you could subclass the metric class here.\nBut let's just practice this.\nSo let's check to see if I'm going to grab this and copy this in here.\nIf you want access to a lot of PyTorch metrics, see torchmetrics.\nSo can we import torchmetrics?\nMaybe it's already in Google Colab.\nNo, not here.\nBut that's all right.\nWe'll go pip install torchmetrics.\nSo Google Colab has access to torchmetrics.\nAnd that's going to download from torchmetrics.\nIt shouldn't take too long.\nIt's quite a small package.\nBeautiful.\nAnd now we're going to go from torchmetrics import accuracy.\nWonderful.\nAnd let's see how we can use this.\nSo setup metric.\nSo we're going to go torchmetric underscore accuracy.\nWe could call it whatever we want, really.\nBut we need accuracy here.\nWe're just going to set up the class.\nAnd then we're going to calculate the accuracy of our multi-class model by calling torchmetric accuracy.\nAnd we're going to pass it Y threads and Y blob test.\nLet's see what happens here.\nOh, what did we get wrong?\nRuntime error.\nExpected all tensors to be on the same device, but found at least two devices.\nOh, of course.\nNow, remember how I said torchmetrics implements PyTorch like code?\nWell, let's check what device this is on.\nOh, it's on the CPU.\nSo something to be aware of that if you use torchmetrics, you have to make sure your metrics\nare on the same device by using device agnostic code as your data.\nSo if we run this, what do we get?\nBeautiful.\nWe get an accuracy of 99.5%, which is in line with the accuracy function that we coded ourselves.\nSo if you'd like a lot of pre-built metrics functions, be sure to see either\nscikit-learn for any of these or torchmetrics for any PyTorch like metrics.\nBut just be aware, if you use the PyTorch version, they have to be on the same\ndevice.\nAnd if you'd like to install it, what do we have?\nWhere's the metrics?\nModule metrics?\nDo we have classification?\nThere we go.\nSo look how many different types of classification metrics there are from torchmetrics.\nSo I'll leave that for you to explore.\nThe resources for this will be here.\nThis is an extracurricular article for when to use precision recall.\nAnd another extracurricular would be to go through the torchmetrics module for 10 minutes\nand have a look at the different metrics for classification.\nSo with that being said, I think we've covered a fair bit.\nBut I think it's also time for you to practice what you've learned.\nSo let's cover some exercises in the next video.\nI'll see you there.\nWelcome back.\nIn the last video, we looked at a few more classification metrics, a little bit of a high\nlevel overview for some more ways to evaluate your classification models.\nAnd I linked some extracurricular here that you might want to look into as well.\nBut we have covered a whole bunch of code together.\nBut now it's time for you to practice some of this stuff on your own.\nAnd so I have some exercises prepared.\nNow, where do you go for the exercises?\nWell, remember on the learnpytorch.io book, for each one of these chapters, there's\na section.\nNow, just have a look at how much we've covered.\nIf I scroll, just keep scrolling.\nLook at that.\nWe've covered all of that in this module.\nThat's a fair bit of stuff.\nBut down the bottom of each one is an exercises section.\nSo all exercises are focusing on practicing the code in the sections above, all of these\nsections here.\nI've got number one, two, three, four, five, six, seven.\nYeah, seven exercises, nice, writing plenty of code.\nAnd then, of course, extracurricular.\nSo these are some challenges that I've mentioned throughout the entire section zero two.\nBut I'm going to link this in here.\nExercises.\nBut, of course, you can just find it on the learnpytorch.io book.\nSo if we come in here and we just create another heading.\nExercises.\nAnd extracurricular.\nAnd then we just write in here.\nSee exercises and extracurricular.\nHere.\nAnd so if you'd like a template of the exercise code, you can go to the PyTorch deep learning\nrepo.\nAnd then within the extras folder, we have exercises and solutions.\nYou might be able to guess what's in each of these exercises.\nWe have O2 PyTorch classification exercises.\nThis is going to be some skeleton code.\nAnd then, of course, we have the solutions as well.\nNow, this is just one form of solutions.\nBut I'm not going to look at those because I would recommend you looking at the exercises\nfirst before you go into the solutions.\nSo we have things like import torch.\nSet up device agnostic code.\nCreate a data set.\nTurn data into a data frame.\nAnd then et cetera, et cetera.\nFor the things that we've done throughout this section.\nSo give that a go.\nTry it on your own.\nAnd if you get stuck, you can refer to the notebook that we've coded together.\nAll of this code here.\nYou can refer to the documentation, of course.\nAnd then you can refer to as a last resort, the solutions notebooks.\nSo give that a shot.\nAnd congratulations on finishing.\nSection 02 PyTorch classification.\nNow, if you're still there, you're still with me.\nLet's move on to the next section.\nWe're going to cover a few more things of deep learning with PyTorch.\nI'll see you there.\nHello, and welcome back.\nWe've got another section.\nWe've got computer vision and convolutional neural networks with.\nPyTorch.\nNow, computer vision is one of my favorite, favorite deep learning topics.\nBut before we get into the materials, let's answer a very important question.\nAnd that is, where can you get help?\nSo, first and foremost, is to follow along with the code as best you can.\nWe're going to be writing a whole bunch of PyTorch computer vision code.\nAnd remember our motto.\nIf and out, run the code.\nSee what the inputs and outputs are of your code.\nAnd that's try it yourself.\nIf you need the doc string to read about what the function you're using does,\nyou can press shift command and space in Google CoLab.\nOr it might be control if you're on Windows.\nOtherwise, if you're still stuck, you can search for the code that you're running.\nYou might come across stack overflow or the PyTorch documentation.\nWe've spent a bunch of time in the PyTorch documentation already.\nAnd we're going to be referencing a whole bunch in the next module in section three.\nWe're up to now.\nIf you go through all of these four steps, the next step is to try it again.\nIf and out, run the code.\nAnd then, of course, if you're still stuck, you can ask a question on the PyTorch deep learning repo.\nDiscussions tab.\nNow, if we open this up, we can go new discussion.\nAnd you can write here section 03 for the computer vision.\nMy problem is, and then in here, you can write some code.\nBe sure to format it as best you can.\nThat way it'll help us answer it.\nAnd then go, what's happening here?\nNow, why do I format the code in these back ticks here?\nIt's so that it looks like code and that it's easier to read when it's formatted on the GitHub discussion.\nThen you can select a category.\nIf you have a general chat, an idea, a poll, a Q&A, or a show and tell of something you've made,\nor what you've learned from the course.\nFor question and answering, you want to put it as Q&A.\nThen you can click start discussion.\nAnd it'll appear here.\nAnd that way, they'll be searchable and we'll be able to help you out.\nSo I'm going to get out of this.\nAnd oh, speaking of resources, we've got the PyTorch deep learning repo.\nThe links will be where you need the links.\nAll of the code that we're going to write in this section is contained within the section 3 notebook.\nPyTorch computer vision.\nNow, this is a beautiful notebook annotated with heaps of text and images.\nYou can go through this on your own time and use it as a reference to help out.\nIf you get stuck on any of the code we write through the videos, check it out in this notebook because it's probably here somewhere.\nAnd then finally, let's get out of these.\nIf we come to the book version of the course,\nthis is learnpytorch.io.\nWe've got home.\nThis will probably be updated by the time you look at that.\nBut we have section 03, which is PyTorch computer vision.\nIt's got all of the information about what we're going to cover in a book format.\nAnd you can, of course, skip ahead to different subtitles.\nSee what we're going to write here.\nAll of the links you need and extra resources will be at learnpytorch.io.\nAnd for this section, it's PyTorch computer vision.\nWith that being said, speaking of computer vision, you might have the question,\nwhat is a computer vision problem?\nWell, if you can see it, it could probably be phrased at some sort of computer vision problem.\nThat's how broad computer vision is.\nSo let's have a few concrete examples.\nWe might have a binary classification problem,\nsuch as if we wanted to have two different images.\nIs this photo of steak or pizza?\nWe might build a model that understands what steak looks like in an image.\nThis is a beautiful dish that I cooked, by the way.\nThis is me eating pizza at a cafe with my dad.\nAnd so we could have binary classification, one thing or another.\nAnd so our machine learning model may take in the pixels of an image\nand understand the different patterns that go into what a steak looks like\nand the same thing with a pizza.\nNow, the important thing to note is that we won't actually be telling our model what to learn.\nIt will learn those patterns itself from different examples of images.\nThen we could step things up and have a multi-class classification problem.\nYou're noticing a trend here.\nWe've covered classification before, but classification can be quite broad.\nIt can be across different domains, such as vision or text or audio.\nBut if we were working with multi-class classification for an image problem,\nwe might have, is this a photo of sushi, steak or pizza?\nAnd then we have three classes instead of two.\nBut again, this could be 100 classes, such as what Nutrify uses,\nwhich is an app that I'm working on.\nWe go to Nutrify.app.\nThis is bare bones at the moment.\nBut right now, Nutrify can classify up to 100 different foods.\nSo if you were to upload an image of food, let's give it a try.\nNutrify, we'll go into images, and we'll go into sample food images.\nAnd how about some chicken wings?\nWhat does it classify this as?\nChicken wings. Beautiful.\nAnd then if we upload an image of not food, maybe.\nLet's go to Nutrify.\nThis is on my computer, by the way.\nYou might not have a sample folder set up like this.\nAnd then if we upload a photo of a Cybertruck, what does it say?\nNo food found.\nPlease try another image.\nSo behind the scenes, Nutrify is using the pixels of an image\nand then running them through a machine learning model\nand classifying it first, whether it's food or not food.\nAnd then if it is food, classifying it as what food it is.\nSo right now it works for 100 different foods.\nSo if we have a look at all these, it can classify apples,\nartichokes, avocados, barbecue sauce.\nEach of these work at different levels of performance,\nbut that's just something to keep in mind of what you can do.\nSo the whole premise of Nutrify is to upload a photo of food\nand then learn about the nutrition about it.\nSo let's go back to our keynote.\nWhat's another example?\nWell, we could use computer vision for object detection,\nwhere you might answer the question is,\nwhere's the thing we're looking for?\nSo for example, this car here, I caught them on security camera,\nactually did a hit and run on my new car,\nwasn't that much of an expensive car, but I parked it on the street\nand this person, the trailer came off the back of their car\nand hit my car and then they just picked the trailer up\nand drove away.\nBut I went to my neighbor's house and had to look at their security footage\nand they found this car.\nSo potentially, you could design a machine learning model\nto find this certain type of car.\nIt was an orange jute, by the way, but the images were in black and white\nto detect to see if it ever recognizes a similar car\nthat comes across the street and you could go,\nhey, did you crash into my car the other day?\nI didn't actually find who it was.\nSo sadly, it was a hit and run.\nBut that's object detection, finding something in an image.\nAnd then you might want to find out\nwhether the different sections in this image.\nSo this is a great example at what Apple uses on their devices,\niPhones and iPads and whatnot, to segregate or segment\nthe different sections of an image, so person one, person two,\nskin tones, hair, sky, original.\nAnd then it enhances each of these sections in different ways.\nSo that's a practice known as computational photography.\nBut the whole premise is how do you segment different portions of an image?\nAnd then there's a great blog post here\nthat talks about how it works and what it does\nand what kind of model that's used.\nI'll leave that as extra curriculum if you'd like to look into it.\nSo if you have these images, how do you enhance the sky?\nHow do you make the skin tones look how they should?\nHow do you remove the background if you really wanted to?\nSo all of this happens on device.\nSo that's where I got that image from, by the way.\nSemantic Mars.\nAnd this is another great blog, Apple Machine Learning Research.\nSo to keep this in mind, we're about to see another example for computer vision,\nwhich is Tesla Computer Vision.\nA lot of companies have websites such as Apple Machine Learning Research\nwhere they share a whole bunch of what they're up to in the world of machine learning.\nSo in Tesla's case, they have eight cameras on each of their self-driving cars\nthat fuels their full self-driving beta software.\nAnd so they use computer vision to understand what's going on in an image\nand then plan what's going on.\nSo this is three-dimensional vector space.\nAnd what this means is they're basically taking these different viewpoints\nfrom the eight different cameras, feeding them through some form of neural network,\nand turning the representation of the environment around the car into a vector.\nSo a long string of numbers.\nAnd why will it do that?\nWell, because computers understand numbers far more than they understand images.\nSo we might be able to recognize what's happening here.\nBut for a computer to understand it, we have to turn it into vector space.\nAnd so if you want to have a look at how Tesla uses computer vision,\nso this is from Tesla's AI Day video.\nI'm not going to play it all because it's three hours long,\nbut I watched it and I really enjoyed it.\nSo there's some information there.\nAnd there's a little tidbit there.\nIf you go to two hours and one minute and 31 seconds on the same video,\nhave a look at what Tesla do.\nWell, would you look at that? Where have we seen that before?\nThat's some device-agnostic code, but with Tesla's custom dojo chip.\nSo Tesla uses PyTorch.\nSo the exact same code that we're writing,\nTesla uses similar PyTorch code to, of course,\nthey write PyTorch code to suit their problem.\nBut nonetheless, they use PyTorch code to train their machine learning models\nthat power their self-driving software.\nSo how cool is that?\nAnd if you want to have a look at another example,\nthere's plenty of different Tesla self-driving videos.\nSo, oh, we can just play it right here.\nI was going to click the link.\nSo look, this is what happens.\nIf we have a look in the environment,\nTesla, the cameras, understand what's going on here.\nAnd then it computes it into this little graphic here\non your heads-up display in the car.\nAnd it kind of understands, well, I'm getting pretty close to this car.\nI'm getting pretty close to that car.\nAnd then it uses this information about what's happening,\nthis perception, to plan where it should drive next.\nAnd I believe here it ends up going into it.\nIt has to stop.\nYeah, there we go.\nBecause we've got a stop sign.\nLook at that.\nIt's perceiving the stop sign.\nIt's got two people here.\nIt just saw a car drive pass across this street.\nSo that is pretty darn cool.\nThat's just one example of computer vision, one of many.\nAnd how would you find out what computer vision can be used for?\nHere's what I do.\nWhat can computer vision be used for?\nPlenty more resources.\nSo, oh, there we go.\n27 most popular computer vision applications in 2022.\nSo we've covered a fair bit there.\nBut what are we going to cover specifically with PyTorch code?\nWell, broadly, like that.\nWe're going to get a vision data set to work with using torch vision.\nSo PyTorch has a lot of different domain libraries.\nTorch vision helps us deal with computer vision problems.\nAnd there's existing data sets that we can leverage to play around with computer vision.\nWe're going to have a look at the architecture of a convolutional neural network,\nalso known as a CNN with PyTorch.\nWe're going to look at an end-to-end multi-class image classification problem.\nSo multi-class is what?\nMore than one thing or another?\nCould be three classes, could be a hundred.\nWe're going to look at steps at modeling with CNNs in PyTorch.\nSo we're going to create a convolutional network with PyTorch.\nWe're going to pick a last function and optimize it to suit our problem.\nWe're going to train a model, training a model a model.\nA little bit of a typo there.\nAnd then we're going to evaluate a model, right?\nSo we might have typos with our text, but we'll have less typos in the code.\nAnd how are we going to do this?\nWell, we could do it cook, so we could do it chemis.\nWell, we're going to do it a little bit of both.\nPart art, part science.\nBut since this is a machine learning cooking show, we're going to be cooking up lots of code.\nSo in the next video, we're going to cover the inputs and outputs of a computer vision problem.\nI'll see you there.\nSo in the last video, we covered what we're going to cover, broadly.\nAnd we saw some examples of what computer vision problems are.\nEssentially, anything that you're able to see, you can potentially turn into a computer vision problem.\nAnd we're going to be cooking up lots of machine learning, or specifically pie torch, computer vision code.\nYou see I fixed that typo.\nNow let's talk about what the inputs and outputs are of a typical computer vision problem.\nSo let's start with a multi-classification example.\nAnd so we wanted to take photos of different images of food and recognize what they were.\nSo we're replicating the functionality of Nutrify.\nSo take a photo of food and learn about it.\nSo we might start with a bunch of food images that have a height and width of some sort.\nSo we have width equals 224, height equals 224, and then they have three color channels.\nWhy three?\nWell, that's because we have a value for red, green and blue.\nSo if we look at this up, if we go red, green, blue image format.\nSo 24-bit RGB images.\nSo a lot of images or digital images have some value for a red pixel, a green pixel and a blue pixel.\nAnd if you were to convert images into numbers, they get represented by some value of red, some value of green and some value of blue.\nThat is exactly the same as how we'd represent these images.\nSo for example, this pixel here might be a little bit more red, a little less blue, and a little less green because it's close to orange.\nAnd then we convert that into numbers.\nSo what we're trying to do here is essentially what we're trying to do with all of the data that we have with machine learning is represented as numbers.\nSo the typical image format to represent an image because we're using computer vision.\nSo we're trying to figure out what's in an image.\nThe typical way to represent that is in a tensor that has a value for the height, width and color channels.\nAnd so we might numerically encode these.\nIn other words, represent our images as a tensor.\nAnd this would be the inputs to our machine learning algorithm.\nAnd in many cases, depending on what problem you're working on, an existing algorithm already exists for many of the most popular computer vision problems.\nAnd if it doesn't, you can build one.\nAnd then you might fashion this machine learning algorithm to output the exact shapes that you want.\nIn our case, we want three outputs.\nWe want one output for each class that we have.\nWe want a prediction probability for sushi.\nWe want a prediction probability for steak.\nAnd we want a prediction probability for pizza.\nNow in our case, in this iteration, looks like our model got one of them wrong because the highest value was assigned to the wrong class here.\nSo for the second image, it assigned a prediction probability of 0.81 for sushi.\nNow, keep in mind that you could change these classes to whatever your particular problem is.\nI'm just simplifying this and making it three.\nYou could have a hundred.\nYou could have a thousand.\nYou could have five.\nIt's just, it depends on what you're working with.\nAnd so we might use these predicted outputs to enhance our app.\nSo if someone wants to take a photo of their plate of sushi, our app might say,\nhey, this is a photo of sushi.\nHere's some information about those, the sushi rolls or the same for steak, the same for pizza.\nNow it might not always get it right because after all, that's what machine learning is.\nIt's probabilistic.\nSo how would we improve these results here?\nWell, we could show our model more and more images of sushi steak and pizza\nso that it builds up a better internal representation of said images.\nSo when it looks at images it's never seen before or images outside its training data set,\nit's able to get better results.\nBut just keep in mind this whole process is similar no matter what computer vision problem you're working with.\nYou need a way to numerically encode your information.\nYou need a machine learning model that's capable of fitting the data\nin the way that you would like it to be fit in our case classification.\nYou might have a different type of model if you're working with object detection,\na different type of model if you're working with segmentation.\nAnd then you need to fashion the outputs in a way that best suit your problem as well.\nSo let's push forward.\nOh, by the way, the model that often does this is a convolutional neural network.\nIn other words, a CNN.\nHowever, you can use many other different types of machine learning algorithms here.\nIt's just that convolutional neural networks typically perform the best with image data.\nAlthough with recent research, there is the transformer architecture or deep learning model\nthat also performs fairly well or very well with image data.\nSo just keep that in mind going forward.\nBut for now we're going to focus on convolutional neural networks.\nAnd so we might have input and output shapes because remember one of the chief machine learning problems\nis making sure that your tensor shapes line up with each other, the input and output shapes.\nSo if we encoded this image of stake here, we might have a dimensionality of batch size\nwith height color channels.\nAnd now the ordering here could be improved.\nIt's usually height then width.\nSo alphabetical order.\nAnd then color channels last.\nSo we might have the shape of none, two, two, four, two, four, three.\nNow where does this come from?\nSo none could be the batch size.\nNow it's none because we can set the batch size to whatever we want, say for example 32.\nThen we might have a height of two to four and a width of two to four and three color channels.\nNow height and width are also customizable.\nYou might change this to be 512 by 512.\nWhat that would mean is that you have more numbers representing your image.\nAnd in sense would take more computation to figure out the patterns because there is simply more information encoded in your image.\nBut two, two, four, two, four is a common starting point for images.\nAnd then 32 is also a very common batch size, as we've seen in previous videos.\nBut again, this could be changed depending on the hardware you're using, depending on the model you're using.\nYou might have a batch size to 64.\nYou might have a batch size of 512.\nIt's all problem specific.\nAnd that's this line here.\nThese will vary depending on the problem you're working on.\nSo in our case, our output shape is three because we have three different classes for now.\nBut again, if you have a hundred, you might have an output shape of a hundred.\nIf you have a thousand, you might have an output shape of a thousand.\nThe same premise of this whole pattern remains though.\nNumerically encode your data, feed it into a model, and then make sure the output shape fits your specific problem.\nAnd so, for this section, Computer Vision with PyTorch, we're going to be building CNNs to do this part.\nWe're actually going to do all of the parts here, but we're going to focus on building a convolutional neural network\nto try and find patterns in data, because it's not always guaranteed that it will.\nFinally, let's look at one more problem.\nSay you had grayscale images of fashion items, and you have quite small images.\nThey're only 28 by 28.\nThe exact same pattern is going to happen.\nYou numerically represent it, use it as inputs to a machine learning algorithm,\nand then hopefully your machine learning algorithm outputs the right type of clothing that it is.\nIn this case, it's a t-shirt.\nBut I've got dot dot dot here because we're going to be working on a problem that uses ten different types of items of clothing.\nAnd the images are grayscale, so there's not much detail.\nSo hopefully our machine learning algorithm can recognize what's going on in these images.\nThere might be a boot, there might be a shirt, there might be pants, there might be a dress,\netc, etc.\nBut we numerically encode our images into dimensionality of batch size, height with color channels.\nThis is known as NHWC, or number of batches, or number of images in a batch, height with C, or color channels.\nThis is color channels last.\nWhy am I showing you two forms of this?\nDo you notice color channels in this one is color channels first?\nSo color channels height width?\nWell, because you come across a lot of different representations of data full stop,\nbut particularly image data in PyTorch and other libraries,\nmany libraries expect color channels last.\nHowever, PyTorch currently at the time of recording this video may change in the future,\ndefaults to representing image data with color channels first.\nNow this is very important because you will get errors if your dimensionality is in the wrong order.\nAnd so there are ways to go in between these two, and there's a lot of debate of which format is the best.\nIt looks like color channels last is going to win over the long term, just because it's more efficient,\nbut again, that's outside the scope, but just keep this in mind.\nWe're going to write code to interact between these two, but it's the same data just represented in different order.\nAnd so we could rearrange these shapes to how we want color channels last or color channels first.\nAnd once again, the shapes will vary depending on the problem that you're working on.\nSo with that being said, we've covered the input and output shapes.\nHow are we going to see them with code?\nWell, of course we're going to be following the PyTorch workflow that we've done.\nSo we need to get our data ready, turn it into tenses in some way, shape or form.\nWe can do that with taught division transforms.\nOh, we haven't seen that one yet, but we will.\nWe can use torchutilsdata.datasetutils.data.data loader.\nWe can then build a model or pick a pre-trained model to suit our problem.\nWe've got a whole bunch of modules to help us with that, torchNN module, torchvision.models.\nAnd then we have an optimizer and a loss function.\nWe can evaluate the model using torch metrics, or we can code our own metric functions.\nWe can of course improve through experimentation, which we will see later on,\nwhich we've actually done that, right?\nWe've done improvement through experimentation.\nWe've tried different models, we've tried different things.\nAnd then finally, we can save and reload our trained model if we wanted to use it elsewhere.\nSo with that being said, we've covered the workflow.\nThis is just a high-level overview of what we're going to code.\nYou might be asking the question, what is a convolutional neural network, or a CNN?\nLet's answer that in the next video.\nI'll see you there.\nWelcome back.\nIn the last video, we saw examples of computer vision input and output shapes.\nAnd we kind of hinted at the fact that convolutional neural networks are deep learning models, or CNNs,\nthat are quite good at recognizing patterns in images.\nSo we left off the last video with the question, what is a convolutional neural network?\nAnd where could you find out about that?\nWhat is a convolutional neural network?\nHere's one way to find out.\nAnd I'm sure, as you've seen, there's a lot of resources for such things.\nA comprehensive guide to convolutional neural networks.\nWhich one of these is the best?\nWell, it doesn't really matter.\nThe best one is the one that you understand the best.\nSo there we go.\nThere's a great video from Code Basics.\nI've seen that one before, simple explanation of convolutional neural network.\nI'll leave you to research these things on your own.\nAnd if you wanted to look at images, there's a whole bunch of images.\nI prefer to learn things by writing code.\nBecause remember, this course is code first.\nAs a machine learning engineer, 99% of my time is spent writing code.\nSo that's what we're going to focus on.\nBut anyway, here's the typical architecture of a CNN.\nIn other words, a convolutional neural network.\nIf you hear me say CNN, I'm not talking about the news website.\nIn this course, I'm talking about the architecture convolutional neural network.\nSo this is some PyTorch code that we're going to be working towards building.\nBut we have some hyperparameters slash layer types here.\nWe have an input layer.\nSo we have an input layer, which takes some in channels, and an input shape.\nBecause remember, it's very important in machine learning and deep learning to line up your\ninput and output shapes of whatever model you're using, whatever problem you're working with.\nThen we have some sort of convolutional layer.\nNow, what might happen in a convolutional layer?\nWell, as you might have guessed, as what happens in many neural networks, is that the layers\nperform some sort of mathematical operation.\nNow, convolutional layers perform convolving window operation across an image or across\na tensor.\nAnd discover patterns using, let's have a look, actually.\nLet's go, nn.com2d.\nThere we go.\nThis is what happens.\nSo the output of our network equals a bias plus the sum of the weight tensor over the\nconvolutional channel out, okay, times input.\nNow, if you want to dig deeper into what is actually going on here, you're more than welcome to\ndo that.\nBut we're going to be writing code that leverages the torch nn.com2d.\nAnd we're going to fix up all of these hyperparameters here so that it works with our problem.\nNow, what you need to know here is that this is a bias term.\nWe've seen this before.\nAnd this is a weight matrix.\nSo a bias vector typically and a weight matrix.\nAnd they operate over the input.\nBut we'll see these later on with code.\nSo just keep that in mind.\nThis is what's happening.\nAs with every layer in a neural network, some form of operation is happening on our input\ndata.\nThese operations happen layer by layer until eventually, hopefully, they can be turned into\nsome usable output.\nSo let's jump back in here.\nThen we have an hidden activation slash nonlinear activation because why do we use nonlinear\nactivations?\nWell, it's because if our data was nonlinear, non-straight lines, we need the help of straight\nand non-straight lines to model it, to draw patterns in it.\nThen we typically have a pooling layer.\nAnd I want you to take this architecture.\nI've said typical here for a reason because these type of architectures are changing all\nthe time.\nSo this is just one typical example of a CNN.\nIt's about as basic as a CNN as you can get.\nSo over time, you will start to learn to build more complex models.\nYou will not only start to learn to build them, you will just start to learn to use them,\nas we'll see later on in the transfer learning section of the course.\nAnd then we have an output layer.\nSo do you notice the trend here?\nWe have an input layer and then we have multiple hidden layers that perform some sort of mathematical\noperation on our data.\nAnd then we have an output slash linear layer that converts our output into the ideal shape\nthat we'd like.\nSo we have an output shape here.\nAnd then how does this look in process?\nWhile we put in some images, they go through all of these layers here because we've used\nan end up sequential.\nAnd then hopefully this forward method returns x in a usable status or usable state that\nwe can convert into class names.\nAnd then we could integrate this into our computer vision app in some way, shape or form.\nAnd here's the asterisk here.\nNote, there are almost an unlimited amount of ways you could stack together a convolutional\nneural network.\nThis slide only demonstrates one.\nSo just keep that in mind, only demonstrates one.\nBut the best way to practice this sort of stuff is not to stare at a page.\nIt's to if and out, code it out.\nSo let's code, I'll see you in Google CoLab.\nWelcome back.\nNow, we've discussed a bunch of fundamentals about computer vision problems and convolutional\nneural networks.\nBut rather than talk to more slides, well, let's start to code them out.\nI'm going to meet you at colab.research.google.com.\nShe's going to clean up some of these tabs.\nAnd I'm going to start a new notebook.\nAnd then I'm going to name this one, this is going to be 03 PyTorch computer vision.\nAnd I'm going to call mine video.\nSo just so it has the video tag, because if we go in here, if we go video notebooks of\nthe PyTorch deep learning repo, the video notebooks are stored in here.\nThey've got the underscore video tag.\nSo the video notebooks have all of the code I write exactly in the video.\nBut there are some reference notebooks to go along with it.\nLet me just write a heading here, PyTorch computer vision.\nAnd I'll put a resource here, see reference notebook.\nNow, of course, this is the one that's the ground truth.\nIt's got all of the code that we're going to be writing.\nI'm going to put that in here.\nExplain with text and images and whatnot.\nAnd then finally, as we got see reference online book.\nAnd that is at learnpytorch.io at section number three, PyTorch computer vision.\nI'm going to put that in there.\nAnd then I'm going to turn this into markdown with command mm.\nBeautiful.\nSo let's get started.\nI'm going to get rid of this, get rid of this.\nHow do we start this off?\nWell, I believe there are some computer vision libraries that you should be aware of.\nComputer vision libraries in PyTorch.\nSo this is just going to be a text based cell.\nBut the first one is torch vision, which is the base domain library for PyTorch computer vision.\nSo if we look up torch vision, what do we find?\nWe have torch vision 0.12.\nThat's the version that torch vision is currently up to at the time of recording this.\nSo in here, this is very important to get familiar with if you're working on computer vision problems.\nAnd of course, in the documentation, this is just another tidbit.\nWe have torch audio for audio problems.\nWe have torch text for text torch vision, which is what we're working on torch rack for recommendation systems\ntorch data for dealing with different data pipelines torch serve, which is for serving PyTorch models\nand PyTorch on XLA.\nSo I believe that stands for accelerated linear algebra devices.\nYou don't have to worry about these ones for now.\nWe're focused on torch vision.\nHowever, if you would like to learn more about a particular domain, this is where you would go to learn more.\nSo there's a bunch of different stuff that's going on here.\nTransforming and augmenting images.\nSo fundamentally, computer vision is dealing with things in the form of images.\nEven a video gets converted to an image.\nWe have models and pre-trained weights.\nSo as I referenced before, you can use an existing model that works on an existing computer vision problem for your own problem.\nWe're going to cover that in section, I think it's six, for transfer learning.\nAnd then we have data sets, which is a bunch of computer vision data sets, utils, operators, a whole bunch of stuff here.\nSo PyTorch is really, really good for computer vision.\nI mean, look at all the stuff that's going on here.\nBut that's enough talking about it.\nLet's just put it in here.\nTorch vision. This is the main one.\nI'm not going to link to all of these.\nAll of the links for these, by the way, is in the book version of the course PyTorch Computer Vision.\nAnd we have what we're going to cover.\nAnd finally, computer vision libraries in PyTorch.\nTorch vision, data sets, models, transforms, et cetera.\nBut let's just write down the other ones.\nSo we have torch vision, not data sets, something to be aware of.\nSo get data sets and data loading functions for computer vision here.\nThen we have torch vision.\nAnd from torch vision, models is get pre-trained computer vision.\nSo when I say pre-trained computer vision models, we're going to cover this more in transfer learning, as I said.\nPre-trained computer vision models are models that have been already trained on some existing vision data\nand have trained weights, trained patterns that you can leverage for your own problems,\nthat you can leverage for your own problems.\nThen we have torch vision.transforms.\nAnd then we have functions for manipulating your vision data, which is, of course, images to be suitable for use with an ML model.\nSo remember, what do we have to do when we have image data or almost any kind of data?\nFor machine learning, we have to prepare it in a way so it can't just be pure images, so that's what transforms help us out with.\nTransforms helps to turn our image data into numbers so we can use it with a machine learning model.\nAnd then, of course, we have some, these are the torch utils.\nThis is not vision specific, it's entirety of PyTorch specific, and that's data set.\nSo if we wanted to create our own data set with our own custom data, we have the base data set class for PyTorch.\nAnd then we have finally torch utils data.\nThese are just good to be aware of because you'll almost always use some form of data set slash data loader with whatever PyTorch problem you're working on.\nSo this creates a Python iterable over a data set.\nWonderful.\nI think these are most of the libraries that we're going to be using in this section.\nLet's import some of them, hey, so we can see what's going on.\nLet's go import PyTorch.\nImport PyTorch.\nSo import torch.\nWe're also going to get NN, which stands for neural network.\nWhat's in NN?\nWell, in NN, of course, we have lots of layers, lots of loss functions, a whole bunch of different stuff for building neural networks.\nWe're going to also import torch vision.\nAnd then we're going to go from torch vision.\nImport data sets because we're going to be using data sets later on to get a data set to work with from torch vision.\nWell, import transforms.\nYou could also go from torch vision dot transforms import to tensor.\nThis is one of the main ones you'll see for computer vision problems to tensor.\nYou can imagine what it does.\nBut let's have a look.\nTransforms to tensor.\nTransforming and augmenting images.\nSo look where we are.\nWe're in pytorch.org slash vision slash stable slash transforms.\nOver here.\nSo we're in the torch vision section.\nAnd we're just looking at transforming and augmenting images.\nSo transforming.\nWhat do we have?\nTransforms are common image transformations of our and the transforms module.\nThey can be trained together using compose.\nBeautiful.\nSo if we have two tensor, what does this do?\nConvert a pill image on NumPy and the array to a tensor.\nBeautiful.\nThat's what we want to do later on, isn't it?\nWell, this is kind of me giving you a spoiler is we want to convert our images into tensors so that we can use those with our models.\nBut there's a whole bunch of different transforms here and actually one of your extra curriculum is to be to read through each of these packages for 10 minutes.\nSo that's about an hour of reading, but it will definitely help you later on if you get familiar with using the pytorch documentation.\nAfter all, this course is just a momentum builder.\nWe're going to write heaves of pytorch code.\nBut fundamentally, you'll be teaching yourself a lot of stuff by reading the documentation.\nLet's keep going with this.\nWhere were we up to?\nWhen we're getting familiar with our data, mapplotlib is going to be fundamental for visualization.\nRemember, the data explorer's motto, visualize, visualize, visualize, become one with the data.\nSo we're going to import mapplotlib.pyplot as PLT.\nAnd then finally, let's check the versions.\nSo print torch.version or underscore, underscore version and print torch vision.\nSo by the time you watch this, there might be a newer version of each of these modules out.\nIf there's any errors in the code, please let me know.\nBut this is just a bare minimum version that you'll need to complete this section.\nI believe at the moment, Google Colab is running 1.11 for torch and maybe 1.10.\nWe'll find out in a second.\nIt just connected.\nSo we're importing pytorch.\nOkay, there we go.\nSo my pytorch version is 1.10 and it's got CUDA available and torch vision is 0.11.\nSo just make sure if you're running in Google Colab, if you're running this at a later date,\nyou probably have at minimum these versions, you might even have a later version.\nSo these are the minimum versions required for this upcoming section.\nSo we've covered the base computer vision libraries in pytorch.\nWe've got them ready to go.\nHow about in the next video, we cover getting a data set.\nI'll see you there.\nWelcome back.\nSo in the last video, we covered some of the fundamental computer vision libraries in pytorch.\nThe main one being torch vision and then modules that stem off torch vision.\nAnd then of course, we've got torch utils dot data dot data set, which is the base data set class for pytorch\nand data loader, which creates a Python irritable over a data set.\nSo let's begin where most machine learning projects do.\nAnd that is getting a data set, getting a data set.\nI'm going to turn this into markdown.\nAnd the data set that we're going to be used to demonstrating some computer vision techniques is fashion amnest.\nWhich is a take of the data set we'll be using is fashion amnest, which is a take on the original amnest data set,\namnest database, which is modified national institute of standards and technology database, which is kind of like the hello world\nin machine learning and computer vision, which is these are sample images from the amnest test data set,\nwhich are grayscale images of handwritten digits.\nSo this, I believe was originally used for trying to find out if you could use computer vision at a postal service\nto, I guess, recognize post codes and whatnot.\nI may be wrong about that, but that's what I know.\nYeah, 1998.\nSo all the way back at 1998, how cool is that?\nSo this was basically where convolutional neural networks were founded.\nI'll let you read up on the history of that.\nBut neural network started to get so good that this data set was quite easy for them to do really well.\nAnd that's when fashion amnest came out.\nSo this is a little bit harder if we go into here.\nThis is by Zalando research fashion amnest.\nAnd it's of grayscale images of pieces of clothing.\nSo like we saw before the input and output, what we're going to be trying to do is turning these images of clothing into numbers\nand then training a computer vision model to recognize what the different styles of clothing are.\nAnd here's a dimensionality plot of all the different items of clothing.\nVisualizing where similar items are grouped together, there's the shoes and whatnot.\nIs this interactive?\nOh no, it's a video.\nExcuse me.\nThere we go.\nTo serious machine learning researchers.\nWe are talking about replacing amnest.\nAmnest is too easy.\nAmnest is overused.\nAmnest cannot represent modern CV tasks.\nSo even now fashion amnest I would say has also been pretty much sold, but it's a good way to get started.\nNow, where could we find such a data set?\nWe could download it from GitHub.\nBut if we come back to the taught division documentation, have a look at data sets.\nWe have a whole bunch of built-in data sets.\nAnd remember, this is your extra curricular to read through these for 10 minutes or so each.\nBut we have an example.\nWe could download ImageNet if we want.\nWe also have some base classes here for custom data sets.\nWe'll see that later on.\nBut if we scroll through, we have image classification data sets.\nCaltech 101.\nI didn't even know what all of these are.\nThere's a lot here.\nCFAR 100.\nSo that's an example of 100 different items.\nSo that would be a 100 class, multi-class classification problem.\nCFAR 10 is 10 classes.\nWe have amnest.\nWe have fashion amnest.\nOh, that's the one we're after.\nBut this is basically what you would do to download a data set from taughtvision.datasets.\nYou would download the data in some way, shape, or form.\nAnd then you would turn it into a data loader.\nSo ImageNet is one of the most popular or is probably the gold standard data set for computer vision evaluation.\nIt's quite a big data set.\nIt's got millions of images.\nBut that's the beauty of taught vision is that it allows us to download example data sets\nthat we can practice on.\nI don't even perform research on from a built-in module.\nSo let's now have a look at the fashion amnest data set.\nHow might we get this?\nSo we've got some example code here, or this is the documentation.\ntaughtvision.datasets.fashion amnest.\nWe have to pass in a root.\nSo where do we want to download the data set?\nWe also have to pass in whether we want the training version of the data set\nor whether we want the testing version of the data set.\nDo we want to download it?\nYes or no?\nShould we transform the data in any way shape or form?\nSo we're going to be downloading images through this function call or this class call.\nDo we want to transform those images in some way?\nWhat do we have to do to images before we can use them with a model?\nWe have to turn them into a tensor, so we might look into that in a moment.\nAnd target transform is do we want to transform the labels in any way shape or form?\nSo often the data sets that you download from taughtvision.datasets\nare pre formatted in a way that they can be quite easily used with PyTorch.\nBut that won't always be the case with your own custom data sets.\nHowever, what we're about to cover is just important to get an idea of what the computer vision workflow is.\nAnd then later on you can start to customize how you get your data in the right format to be used with the model.\nThen we have some different parameters here and whatnot.\nLet's just rather than look at the documentation, if and down, code it out.\nSo we'll be using fashion MNIST and we'll start by, I'm going to just put this here, from taughtvision.datasets.\nAnd we'll put the link there and we'll start by getting the training data.\nSet up training data.\nI'm just going to make some code cells here so that I can code in the middle of the screen.\nSet up training data. Training data equals data sets dot fashion MNIST.\nBecause recall, we've already from taughtvision.\nWe don't need to import this again, I'm just doing it for demonstration purposes, but from taughtvision import data sets\nso we can just call data sets dot fashion MNIST.\nAnd then we're going to type in root.\nSee how the doc string comes up and tells us what's going on.\nI personally find this a bit hard to read in Google Colab, so if I'm looking up the documentation,\nI like to just go into here.\nBut let's code it out.\nSo root is going to be data, so where to download data to.\nWe'll see what this does in a minute.\nThen we're going to go train.\nWe want the training version of the data set.\nSo as I said, a lot of the data sets that you find in taughtvision.datasets\nhave been formatted into training data set and testing data set already.\nSo this Boolean tells us do we want the training data set?\nSo if that was false, we would get the testing data set of fashion MNIST.\nDo we want to download it?\nDo we want to download?\nYes, no.\nSo yes, we do. We're going to set that to true.\nNow what sort of transform do we want to do?\nSo because we're going to be downloading images and what do we have to do to our images\nto use them with a machine-loading model, we have to convert them into tensors.\nSo I'm going to pass the transform to tensor, but we could also just go torchvision.transforms.to tensor.\nThat would be the exact same thing as what we just did before.\nAnd then the target transform, do we want to transform the labels?\nNo, we don't.\nWe're going to see how they come, or the target, sorry.\nHigh torch, this is another way, another naming convention.\nOften uses target for the target that you're trying to predict.\nSo using data to predict the target, which is I often use data to predict a label.\nThey're the same thing.\nSo how do we want to transform the data?\nAnd how do we want to transform the labels?\nAnd then we're going to do the same for the test data.\nSo we're going to go data sets.\nYou might know what to do here.\nIt's going to be the exact same code as above, except we're going to change one line.\nWe want to store it in data.\nWe want to download the training data set as false because we want the testing version.\nDo we want to download it?\nYes, we do.\nDo we want to transform it the data?\nYes, we do, we want to use to tensor to convert our image data to tensors.\nAnd do we want to do a target transform?\nWell, no, we don't.\nWe want to keep the label slash the targets as they are.\nLet's see what happens when we run this.\nOh, downloading fashion, Evan is beautiful.\nSo this is going to download all of the labels.\nWhat do we have?\nTrain images, train labels, lovely, test images, test labels, beautiful.\nSo that's how quickly we can get a data set by using torch vision data sets.\nNow, if we have a look over here, we have a data folder because we set the root to be\ndata.\nNow, if we look what's inside here, we have fashion MNIST, exactly what we wanted.\nThen we have the raw, and then we have a whole bunch of files here, which torch vision has\nconverted into data sets for us.\nSo let's get out of that.\nAnd this process would be much the same if we used almost any data set in here.\nThey might be slightly different depending on what the documentation says and depending\non what the data set is.\nBut that is how easy torch vision data sets makes it to practice on example computer vision\ndata sets.\nSo let's go back.\nLet's check out some parameters or some attributes of our data.\nHow many samples do we have?\nSo we'll check the lengths.\nSo we have 60,000 training examples and 10,000 testing examples.\nSo what we're going to be doing is we're going to be building a computer vision model to\nfind patterns in the training data and then use those patterns to predict on the test\ndata.\nAnd so let's see a first training example.\nSee the first training example.\nSo we can just index on the train data.\nLet's get the zero index and then we're going to have a look at the image and the label.\nOh my goodness.\nA whole bunch of numbers.\nNow you see what the two tensor has done for us?\nSo we've downloaded some images and thanks to this torch vision transforms to tensor.\nHow would we find the documentation for this?\nWell, we could go and see what this does transforms to tensor.\nWe could go to tensor.\nThere we go.\nWhat does this do?\nConvert a pill image.\nSo that's Python image library image on NumPy array to a tensor.\nThis transform does not support torch script.\nSo converts a pill image on NumPy array height with color channels in the range 0 to 255\nto a torch float tensor of shape.\nSee here?\nThis is what I was talking about how PyTorch defaults with a lot of transforms to CHW.\nSo color channels first height then width in that range of zero to one.\nSo typically red, green and blue values are between zero and 255.\nBut neural networks like things between zero and one.\nAnd in this case, it is now in the shape of color channels first, then height, then width.\nHowever, some other machine learning libraries prefer height, width, then color channels.\nJust keep that in mind.\nWe're going to see this in practice later on.\nSo we've got an image.\nWe've got a label.\nLet's check out some more details about it.\nRemember how we discussed?\nOh, there's our label, by the way.\nSo nine, we can go traindata.classes, find some information about our class names.\nClass names.\nBeautiful.\nSo number nine would be 0, 1, 2, 3, 4, 5, 6, 7, 8, 9.\nSo this particular tensor seems to relate to an ankle boot.\nHow would we find that out?\nWell, one second.\nI'm just going to show you one more thing, class to IDX.\nLet's go traindata.class to IDX.\nWhat does this give us?\nClass to IDX.\nThis is going to give us a dictionary of different labels and their corresponding index.\nSo if our machine learning model predicted nine or class nine, we can convert that to\nankle boot using this attribute of the train data.\nThere are more attributes that you can have a look at if you like.\nYou can go traindata.dot, then I just push tab to find out a bunch of different things.\nYou can go data.\nThat'll be the images, and then I believe you can also go targets.\nSo targets, that's all the labels, which is one big long tensor.\nNow let's check the shape.\nCheck the shape of our image.\nSo image.shape and label.shape.\nWhat are we going to get from that?\nOh, label doesn't have a shape.\nWhy is that?\nWell, because it's only an integer.\nSo oh, beautiful.\nLook at that.\nSo our image shape is we have a color channel of one.\nSo let me print this out in something prettier, print image shape, which is going to be image\nshape.\nRemember how I said it's very important to be aware of the input and output shapes of\nyour models and your data.\nIt's all part of becoming one with the data.\nSo that is what our image shape is.\nAnd then if we go next, this is print image label, which is label, but we'll index on\nclass names for label.\nAnd then we'll do that wonderful.\nSo our image shape is currently in the format of color channels height width.\nWe got a bunch of different numbers that's representing our image.\nIt's black and white.\nIt only has one color channel.\nWhy do you think it only has one color channel?\nBecause it's black and white, so if we jump back into the keynote, fashion, we've already\ndiscussed this, grayscale images have one color channel.\nSo that means that for black, the pixel value is zero.\nAnd for white, it's some value for whatever color is going on here.\nSo if it's a very high number, say it's one, it's going to be pure white.\nIf it's like 0.001, it might be a faint white pixel.\nBut if it's exactly zero, it's going to be black.\nSo color images have three color channels for red, green and blue, grayscale have one\ncolor channel.\nBut I think we've done enough of visualizing our images as numbers.\nHow about in the next video, we visualize our image as an image?\nI'll see you there.\nWelcome back.\nSo in the last video, we checked the input output shapes of our data, and we downloaded\nthe fashion MNIST data set, which is comprised of images or grayscale images of T-shirts,\ntrousers, pullovers, dress, coat, sandal, shirt, sneaker, bag, ankle boot.\nNow we want to see if we can build a computer vision model to decipher what's going on in\nfashion MNIST.\nSo to separate, to classify different items of clothing based on their numerical representation.\nAnd part of becoming one with the data is, of course, checking the input output shapes\nof it.\nSo this is a fashion MNIST data set from Zalando Research.\nNow if you recall, why did we look at our input and output shapes?\nWell, this is what we looked at before.\nWe have 28 by 28 grayscale images that we want to represent as a tensor.\nWe want to use them as input into a machine learning algorithm, typically a computer vision\nalgorithm, such as a CNN.\nAnd we want to have some sort of outputs that are formatted in the ideal shape that we'd\nlike.\nSo in our case, we have 10 different types of clothing.\nSo we're going to have an output shape of 10, but our input shape is what?\nSo by default, PyTorch turns tensors into color channels first.\nSo we have an input shape of none, one, 28, 28.\nSo none is going to be our batch size, which of course we can set that to whatever we'd\nlike.\nNow input shape format is in NCHW, or in other words, color channels first.\nBut just remember, if you're working with some other machine learning libraries, you\nmay want to use color channels last.\nSo let's have a look at where that might be the case.\nWe're going to visualize our images.\nSo I make a little heading here, 1.2.\nNow this is all part of becoming one with the data.\nIn other words, understanding its input and output shapes, how many samples there are,\nwhat they look like, visualize, visualize, visualize.\nLet's import mapplotlib.\nI'm just going to add a few code cells here, import mapplotlib.pyplot as PLT.\nNow let's create our image and label is our train data zero, and we're going to print\nthe image shape so we can understand what inputs are going into our mapplotlib function.\nAnd then we're going to go plot.imshow, and we're going to pass in our image and see\nwhat happens, because recall what does our image look like, image?\nOur image is this big tensor of numbers.\nAnd we've got an image shape, 128, 128.\nNow what happens if we call plot.imshow?\nWhat happens there?\nOh, we get an error in valid shape, 128, 128 for image data.\nNow as I said, this is one of the most common errors in machine learning is a shape issue.\nSo the shape of your input tensor doesn't match the expected shape of that tensor.\nSo this is one of those scenarios where our data format, so color channels first, doesn't\nmatch up with what mapplotlib is expecting.\nSo mapplotlib expects either just height and width, so no color channel for gray style\nimages, or it also expects the color channels to be last.\nSo we'll see that later on, but for grayscale, we can get rid of that extra dimension by\npassing in image.squeeze.\nSo do you recall what squeeze does?\nIt's going to remove that singular dimension.\nIf we have a look at what goes on now, beautiful, we get an ankle boot.\nWell, that's a very pixelated ankle boot, but we're only dealing with 28 by 28 pixels,\nso not a very high definition image.\nLet's add the title to it.\nWe're going to add in the label.\nBeautiful.\nSo we've got the number nine here.\nSo where if we go up to here, that's an ankle boot.\nNow let's plot this in grayscale.\nHow might we do that?\nWe can do the same thing.\nWe can go plotplt.imshow.\nWe're going to pass in image.squeeze.\nAnd we're going to change the color map, C map equals gray.\nSo in mapplotlib, if you ever have to change the colors of your plot, you want to look\ninto the C map property or parameter, or sometimes it's also shortened to just C.\nBut in this case, M show is C map, and we want to plot title, and we're going to pull\nit in class names and the label integer here.\nSo we have a look at it now.\nWe have an ankle boot, and we can remove the accesses to if we wanted to plot.access,\nand turn that off.\nThat's going to remove the access.\nSo there we go.\nThat's the type of images that we're dealing with.\nBut that's only a singular image.\nHow about we harness the power of randomness and have a look at some random images from\nour data set?\nSo how would we do this?\nLet's go plot more images.\nWe'll set a random seed.\nSo you and I are both looking at as similar as possible images, 42.\nNow we'll create a plot by calling plot.figure, and we're going to give it a size.\nWe might create a nine by nine grid.\nSo we want to see nine random images from our data set.\nSo rows, calls, or sorry, maybe we'll do four by four.\nThat'll give us 16.\nWe're going to go four i in range, and we're going to go one to rows times columns plus\none.\nSo we can print i.\nWhat's that going to give us?\nWe want to see 16 images.\nOh, they're about.\nSo 16 random images, but used with a manual C to 42 of our data set.\nThis is one of my favorite things to do with any type of data set that I'm looking\nat, whether it be text, image, audio, doesn't matter.\nI like to randomly have a look at a whole bunch of samples at the start so that I can\nbecome one with the data.\nWith that being said, let's use this loop to grab some random indexes.\nWe can do so using tortures, rand, int, so random integer between zero and length of\nthe training data.\nThis is going to give us a random integer in the range of zero and however many training\nsamples we have, which in our case is what, 60,000 or thereabouts.\nSo we want to create the size of one, and we want to get the item from that so that we\nhave a random index.\nWhat is this going to give us?\nOh, excuse me, maybe we print that out.\nThere we go.\nSo we have random images.\nNow, because we're using manual seed, it will give us the same numbers every time.\nSo we have three, seven, five, four, two, three, seven, five, four, two.\nAnd then if we just commented out the random seed, we'll get different numbers every time.\nBut this is just to demonstrate, we'll keep the manual seed there for now.\nYou can comment that out if you want different numbers or different images, different indexes\neach time.\nSo we'll create the image and the label by indexing on the training data at the random\nindex that we're generating.\nAnd then we'll create our plot.\nSo Fig or we'll add a subplot, Fig add subplot, and we're going to go rows, calls, I.\nSo at the if index, we're going to add a subplot.\nRemember, we set rows and columns up to here.\nAnd then we're going to go PLT dot in show, we're going to show what we're going to show\nour image, but we have to squeeze it to get rid of that singular dimension as the color\nchannel.\nOtherwise, we end up with an issue with map plot lib.\nWe're going to use a color map of gray.\nSo it looks like the image we plotted above.\nAnd then for our title, it's going to be our class names indexed with our label.\nAnd then we don't want the accesses because that's going to clutter up our plot.\nLet's see what this looks like.\nOh my goodness, look at that.\nIt worked first.\nGo.\nUsually visualizations take a fair bit of trial and error.\nSo we have ankle boots, we have shirts, we have bags, we have ankle boots, sandal, shirt,\npull over.\nOh, do you notice something about the data set right now, pull over and shirt?\nTo me, they look quite similar.\nDo you think that will cause an issue later on when our model is trying to predict between\na pull over and a shirt?\nHow about if we look at some more images?\nWe'll get rid of the random seed so we can have a look at different styles.\nSo have a sandal ankle boot coat, t-shirt, top, shirt, oh, is that a little bit confusing\nthat we have a class for t-shirt and top and shirt?\nLike I'm not sure about you, but what's the difference between a t-shirt and a shirt?\nThis is just something to keep in mind as a t-shirt and top, does that look like it could\nbe maybe even a dress?\nLike the shape is there.\nSo this is just something to keep in mind going forward.\nThe chances are if we get confused on our, like you and I looking at our data set, if\nwe get confused about different samples and what they're labeled with, our model might\nget confused later on.\nSo let's have a look at one more and then we'll go into the next video.\nSo we have sneaker, trouser, shirt, sandal, dress, pull over, bag, bag, t-shirt, oh, that's\nquite a difficult one.\nIt doesn't look like there's even much going on in that image.\nBut the whole premise of building machine learning models to do this would be could you\nwrite a program that would take in the shapes of these images and figure out, write a rule-based\nprogram that would go, hey, if it's looked like a rectangle with a buckle in the middle,\nit's probably a bag?\nI mean, you probably could after a while, but I prefer to write machine learning algorithms\nto figure out patterns and data.\nSo let's start moving towards that.\nWe're now going to go on figuring out how we can prepare this data to be loaded into\na model.\nI'll see you there.\nAll right, all right, all right.\nSo we've got 60,000 images of clothing that we'd like to build a computer vision model\nto classify into 10 different classes.\nAnd now that we've visualized a fair few of these samples, do you think that we could\nmodel these with just linear lines, so straight lines, or do you think we'll need a model\nwith nonlinearity?\nSo I'm going to write that down.\nSo do you think these items of clothing images could be modeled with pure linear lines, or\ndo you think we'll need nonlinearity?\nDon't have to answer that now.\nWe could test that out later on.\nYou might want to skip ahead and try to build a model yourself with linear lines or nonlinearities.\nWe've covered linear lines and nonlinearities before, but let's now start to prepare our\ndata even further to prepare data loader.\nSo right now, our data is in the form of PyTorch data sets.\nSo let's have a look at it.\nSame data.\nThere we go.\nSo we have data set, which is of fashion MNIST.\nAnd then if we go test data, we see a similar thing except we have a different number of\ndata points.\nWe have the same transform on each, we've turned them into tenses.\nSo we want to convert them from a data set, which is a collection of all of our data,\ninto a data loader.\nPaul, that a data loader, turns our data set into a Python iterable.\nSo I'm going to turn this into Markdown, beautiful.\nMore specifically, specific Galilee, can I spell right?\nI don't know, we want to just code right, we're not here to learn spelling.\nWe want to turn our data into batches, or mini batches.\nWhy would we do this?\nWell, we may get away with it by building a model to look at all 60,000 samples of our\ncurrent data set, because it's quite small.\nIt's only comprised of images of 28 by 28 pixels.\nAnd when I say quite small, yes, 60,000 images is actually quite small for a deep learning\nscale data set.\nModern data sets could be in the millions of images.\nBut if our computer hardware was able to look at 60,000 samples of 28 by 28 at one time,\nit would need a fair bit of memory.\nSo we have RAM space up here, we have GPU memory, we have compute memory.\nBut chances are that it might not be able to store millions of images in memory.\nSo what you do is you break a data set from say 60,000 into groups of batches or mini\nbatches.\nSo we've seen batch size before, why would we do this?\nWell, one, it is more computationally efficient, as in your computing hardware may not be able\nto look store in memory at 60,000 images in one hit.\nSo we break it down to 32 images at a time.\nThis would be batch size of 32.\nNow again, 32 is a number that you can change.\n32 is just a common batch size that you'll see with many beginner style problems.\nAs you go on, you'll see different batch sizes.\nThis is just to exemplify the concept of mini batches, which is very common in deep\nlearning.\nAnd why else would we do this?\nThe second point or the second main point is it gives our neural network more chances\nto update its gradients per epoch.\nSo what I mean by this, this will make more sense when we write a training loop.\nBut if we were to just look at 60,000 images at one time, we would per epoch.\nSo per iteration through the data, we would only get one update per epoch across our entire\ndata set.\nWhereas if we look at 32 images at a time, our neural network updates its internal states,\nits weights, every 32 images, thanks to the optimizer.\nThis will make a lot more sense once we write our training loop.\nBut these are the two of the main reasons for turning our data into mini batches in the\nform of a data loader.\nNow if you'd like to learn more about the theory behind this, I would highly recommend\nlooking up Andrew Org mini batches.\nThere's a great lecture on that.\nSo yeah, large-scale machine learning, mini batch gradient descent, mini batch gradient\ndescent.\nYeah, that's what it's called mini batch gradient descent.\nIf you look up some results on that, you'll find a whole bunch of stuff.\nI might just link this one, I'm going to pause that, I'm going to link this in there.\nSo for more on mini batches, see here.\nNow to see this visually, I've got a slide prepared for this.\nSo this is what we're going to be working towards.\nThere's our input and output shapes.\nWe want to create batch size of 32 across all of our 60,000 training images.\nAnd we're actually going to do the same for our testing images, but we only have 10,000\ntesting images.\nSo this is what our data set's going to look like, batched.\nSo we're going to write some code, namely using the data loader from torch.util.data.\nWe're going to pass it a data set, which is our train data.\nWe're going to give it a batch size, which we can define as whatever we want.\nFor us, we're going to use 32 to begin with.\nAnd we're going to set shuffle equals true if we're using the training data.\nWhy would we set shuffle equals true?\nWell, in case our data set for some reason has order, say we had all of the pants images\nin a row, we had all of the T-shirt images in a row, we had all the sandal images in\na row.\nWe don't want our neural network to necessarily remember the order of our data.\nWe just want it to remember individual patterns between different classes.\nSo we shuffle up the data, we mix it, we mix it up.\nAnd then it looks something like this.\nSo we might have batch number zero, and then we have 32 samples.\nNow I ran out of space when I was creating these, but we got, that was fun up to 32.\nSo this is setting batch size equals 32.\nSo we look at 32 samples per batch.\nWe mix all the samples up, and we go batch, batch, batch, batch, batch, and we'll have,\nhowever many batches we have, we'll have number of samples divided by the batch size.\nSo 60,000 divided by 32, what's that, 1800 or something like that?\nSo this is what we're going to be working towards.\nI did want to write some code in this video, but I think to save it getting too long, we're\ngoing to write this code in the next video.\nIf you would like to give this a go on your own, here's most of the code we have to do.\nSo there's the train data loader, do the same for the test data loader.\nAnd I'll see you in the next video, and we're going to batchify our fashion MNIST data set.\nWelcome back.\nIn the last video, we had a brief overview of the concept of mini batches.\nAnd so rather than our computer looking at 60,000 images in one hit, we break things down.\nWe turn it into batches of 32.\nAgain, the batch size will vary depending on what problem you're working on.\nBut 32 is quite a good value to start with and try out.\nAnd we do this for two main reasons, if we jump back to the code, why would we do this?\nIt is more computationally efficient.\nSo if we have a GPU with, say, 10 gigabytes of memory, it might not be able to store all\n60,000 images in one hit.\nIn our data set, because it's quite small, it may be hour or two, but it's better practice\nfor later on to turn things into mini batches.\nAnd it also gives our neural network more chances to update its gradients per epoch,\nwhich will make a lot more sense once we write our training loop.\nBut for now, we've spoken enough about the theory.\nLet's write some code to do so.\nSo I'm going to import data loader from torch dot utils dot data, import data loader.\nAnd this principle, by the way, preparing a data loader goes the same for not only images,\nbut for text, for audio, whatever sort of data you're working with, mini batches will\nfollow you along or batches of data will follow you along throughout a lot of different deep\nlearning problems.\nSo set up the batch size hyper parameter.\nRemember, a hyper parameter is a value that you can set yourself.\nSo batch size equals 32.\nAnd it's practice.\nYou might see it typed as capitals.\nYou won't always see it, but you'll see it quite often a hyper parameter typed as capitals.\nAnd then we're going to turn data sets into iterables.\nSo batches.\nSo we're going to create a train data loader here of our fashion MNIST data set.\nWe're going to use data loader.\nWe're going to see what the doc string is.\nOr actually, let's look at the documentation torch data loader.\nThis is some extra curriculum for you too, by the way, is to read this data page torch\nutils not data because no matter what problem you're going with with deep learning or pytorch,\nyou're going to be working with data.\nSo spend 10 minutes just reading through here.\nI think I might have already assigned this, but this is just so important that it's worth\ngoing through again.\nRead through all of this.\nEven if you don't understand all of it, what's going on, it's just it helps you know where\nto look for certain things.\nSo what does it take?\nData loader takes a data set.\nWe need to set the batch size to something is the default of one.\nThat means that it would create a batch of one image at a time in our case.\nDo we want to shuffle it?\nDo we want to use a specific sampler?\nThere's a few more things going on.\nNumber of workers.\nNumber of workers stands for how many cores on our machine do we want to use to load data?\nGenerally the higher the better for this one, but we're going to keep most of these as\nthe default because most of them are set to pretty good values to begin with.\nI'll let you read more into the other parameters here.\nWe're going to focus on the first three data set batch size and shuffle true or false.\nLet's see what we can do.\nSo data set equals our train data, which is 60,000 fashion MNIST.\nAnd then we have a batch size, which we're going to set to our batch size hyper parameter.\nSo we're going to have a batch size of 32.\nAnd then finally, do we want to shuffle the training data?\nYes, we do.\nAnd then we're going to do the same thing for the test data loader, except we're not\ngoing to shuffle the test data.\nNow, you can shuffle the test data if you want, but in my practice, it's actually easier\nto evaluate different models when the test data isn't shuffled.\nSo you shuffle the training data to remove order.\nAnd so your model doesn't learn order.\nBut for evaluation purposes, it's generally good to have your test data in the same order\nbecause our model will never actually see the test data set during training.\nWe're just using it for evaluation.\nSo the order doesn't really matter to the test data loader.\nIt's just easier if we don't shuffle it, because then if we evaluate it multiple times, it's\nnot been shuffled every single time.\nSo let's run that.\nAnd then we're going to check it out, our train data loader and our test data loader.\nBeautiful.\nInstances of torch utils data, data loader, data loader.\nAnd now let's check out what we've created, hey, I always like to print different attributes\nof whatever we make, check out what we've created.\nThis is all part of becoming one with the data.\nSo print F, I'm going to go data loaders, and then pass in, this is just going to output\nbasically the exact same as what we've got above.\nThis data loader.\nAnd we can also see what attributes we can get from each of these by going train data\nloader.\nI don't need caps lock there, train data loader, full stop.\nAnd then we can go tab.\nWe've got a whole bunch of different attributes.\nWe've got a batch size.\nWe've got our data set.\nDo we want to drop the last as in if our batch size overlapped with our 60,000 samples?\nDo we want to get rid of the last batch?\nSay for example, the last batch only had 10 samples.\nDo we want to just drop that?\nDo we want to pin the memory that's going to help later on if we wanted to load our\ndata faster?\nA whole bunch of different stuff here.\nIf you'd like to research more, you can find all the stuff about what's going on here in\nthe documentation.\nBut let's just keep pushing forward.\nWhat else do we want to know?\nSo let's find the length of the train data loader.\nWe will go length train data loader.\nSo this is going to tell us how many batches there are, batches of, which of course is batch\nsize.\nAnd we want print length of test data loader.\nWe want length test data loader batches of batch size dot dot dot.\nSo let's find out some information.\nWhat do we have?\nOh, there we go.\nSo just we're seeing what we saw before with this one.\nBut this is more interesting here.\nLength of train data loader.\nYeah, we have about 1,875 batches of 32.\nSo if we do 60,000 training samples divided by 32, yeah, it comes out to 1,875.\nAnd if we did the same with 10,000 for testing samples of 32, it comes out at 313.\nThis gets rounded up.\nSo this is what I meant, that the last batch will have maybe not 32 because 32 doesn't\ndivide evenly into 10,000, but that's okay.\nAnd so this means that our model is going to look at 1,875 individual batches of 32\nimages, rather than just one big batch of 60,000 images.\nNow of course, the number of batches we have will change if we change the batch size.\nSo we have 469 batches of 128.\nAnd if we reduce this down to one, what do we get?\nWe have a batch per sample.\nSo 60,000 batches of 1, 10,000 batches of 1, we're going to stick with 32.\nBut now let's visualize.\nSo we've got them in train data loader.\nHow would we visualize a batch or a single image from a batch?\nSo let's show a sample.\nI'll show you how you can interact with a data loader.\nWe're going to use randomness as well.\nSo we'll set a manual seed and then we'll get a random index, random idx equals torch\nrand int.\nWe're going to go from zero to length of train features batch.\nOh, where did I get that from?\nExcuse me.\nGetting ahead of myself here.\nI want to check out what's inside the training data loader.\nWe'll check out what's inside the training data loader because the test data load is\ngoing to be similar.\nSo we want the train features batch.\nSo I say features as in the images themselves and the train labels batch is going to be\nthe labels of our data set or the targets in pytorch terminology.\nSo next idar data loader.\nSo because our data loader has 1875 batches of 32, we're going to turn it into an iterable\nwith ita and we're going to get the next batch with next and then we can go here train features\nbatch.shape and we'll get train labels batch.shape.\nWhat do you think this is going to give us?\nWell, there we go.\nLook at that.\nSo we have a tensor.\nEach batch we have 32 samples.\nSo this is batch size and this is color channels and this is height and this is width.\nAnd then we have 32 labels associated with the 32 samples.\nNow where have we seen this before, if we go back through our keynote input and output\nshapes.\nSo we have shape equals 32, 28, 28, 1.\nSo this is color channels last, but ours is currently in color channels first.\nNow again, I sound like a broken record here, but these will vary depending on the problem\nyou're working with.\nIf we had larger images, what would change or the height and width dimensions would change.\nIf we had color images, the color dimension would change, but the premise is still the\nsame.\nWe're turning our data into batches so that we can pass that to a model.\nLet's come back.\nLet's keep going with our visualization.\nSo we want to visualize one of the random samples from a batch and then we're going to\ngo image label equals train features batch and we're going to get the random IDX from\nthat and we'll get the train labels batch and we'll get the random IDX from that.\nSo we're matching up on the, we've got one batch here, train features batch, train labels\nbatch and we're just getting the image and the label at a random index within that batch.\nSo excuse me, I need to set this equal there.\nAnd then we're going to go PLT dot in show, what are we going to show?\nWe're going to show the image but we're going to have to squeeze it to remove that singular\ndimension and then we'll set the C map equal to gray and then we'll go PLT dot title, we'll\nset the title which is going to be the class names indexed by the label integer and then\nwe can turn off the accesses.\nYou can use off here or you can use false, depends on what you'd like to use.\nLet's print out the image size because you can never know enough about your data and\nthen print, let's also get the label, label and label shape or label size.\nOur label will be just a single integer so it might not have a shape but that's okay.\nLet's have a look.\nOh, bag.\nSee, look, that's quite hard to understand.\nI wouldn't be able to detect that that's a bag.\nCan you tell me that you could write a program to understand that?\nThat just looks like a warped rectangle to me.\nBut if we had to look at another one, we'll get another random, oh, we've got a random\nseed so it's going to produce the same image each time.\nSo we have a shirt, okay, a shirt.\nSo we see the image size there, 128, 28.\nNow, recall that the image size is, it's a single image so it doesn't have a batch dimension.\nSo this is just color channels height width.\nWe'll go again, label four, which is a coat and we could keep doing this to become more\nand more familiar with our data.\nBut these are all from this particular batch that we created here, coat and we'll do one\nmore, another coat.\nWe'll do one more just to make sure it's not a coat.\nThere we go.\nWe've got a bag.\nBeautiful.\nSo we've now turned our data into data loaders.\nSo we could use these to pass them into a model, but we don't have a model.\nSo I think it's time in the next video, we start to build model zero.\nWe start to build a baseline.\nI'll see you in the next video.\nWelcome back.\nSo in the last video, we got our data sets or our data set into data loaders.\nSo now we have 1,875 batches of 32 images off of the training data set rather than 60,000\nin a one big data set.\nAnd we have 13 or 313 batches of 32 for the test data set.\nThen we learned how to visualize it from a batch.\nAnd we saw that we have still the same image size, one color channel, 28, 28.\nAll we've done is we've turned them into batches so that we can pass them to our model.\nAnd speaking of model, let's have a look at our workflow.\nWhere are we up to?\nWell, we've got our data ready.\nWe've turned it into tensors through a combination of torch vision transforms, torch utils data\ndot data set.\nWe didn't have to use that one because torch vision dot data sets did it for us with the\nfashion MNIST data set, but we did use that one.\nWe did torch utils dot data, the data loader to turn our data sets into data loaders.\nNow we're up to building or picking a pre-trained model to suit your problem.\nSo let's start simply.\nLet's build a baseline model.\nAnd this is very exciting because we're going to build our first model, our first computer\nvision model, albeit a baseline, but that's an important step.\nSo I'm just going to write down here.\nWhen starting to build a series of machine learning modeling experiments, it's best practice\nto start with a baseline model.\nI'm going to turn this into markdown.\nA baseline model.\nSo a baseline model is a simple model.\nYou will try and improve upon with subsequent models, models slash experiments.\nSo you start simply, in other words, start simply and add complexity when necessary because\nneural networks are pretty powerful, right?\nAnd so they have a tendency to almost do too well on our data set.\nThat's a concept known as overfitting, which we'll cover a little bit more later.\nBut we built a simple model to begin with, a baseline.\nAnd then our whole goal will be to run experiments, according to the workflow, improve through\nexperimentation.\nAgain, this is just a guide.\nIt's not set in stone, but this is the general pattern of how things go.\nGet data ready, build a model, fit the model, evaluate, improve the model.\nSo the first model that we build is generally a baseline.\nAnd then later on, we want to improve through experimentation.\nSo let's start building a baseline.\nBut I'm going to introduce to you a new layer that we haven't seen before.\nThat is creating a flatten layer.\nNow what is a flatten layer?\nWell, this is best seen when we code it out.\nSo let's create a flatten model, which is just going to be nn.flatten.\nAnd where could we find the documentation for this?\nWe go nn flatten, flatten in pytorch, what does it do?\nFlattens a continuous range of dims into a tensor, for use with sequential.\nSo there's an example there, but I'd rather, if and doubt, code it out.\nSo we'll create the flatten layer.\nAnd of course, all nn.flatten or nn.modules could be used as a model on their own.\nSo we're going to get a single sample.\nSo x equals train features batch.\nLet's get the first one, zero.\nWhat does this look like?\nSo it's a tensor, x, maybe we get the shape of it as well, x shape.\nWhat do we get?\nThere we go.\nSo that's the shape of x.\nKeep that in mind when we pass it through the flatten layer.\nDo you have an inkling of what flatten might do?\nSo our shape to begin with is what, 128, 28.\nNow let's flatten the sample.\nSo output equals, we're going to pass it to the flatten model, x.\nSo this is going to perform the forward pass internally on the flatten layer.\nSo perform forward pass.\nNow let's print out what happened.\nPrint, shape before flattening equals x dot shape.\nAnd we're going to print shape after flattening equals output dot shape.\nSo we're just taking the output of the flatten model and printing its shape here.\nOh, do you notice what happened?\nWell we've gone from 128, 28 to 1784.\nWow what does the output look like?\nOutput.\nOh, the values are now in all one big vector and if we squeeze that we can remove the extra\ndimension.\nSo we've got one big vector of values.\nNow where did this number come from?\nWell, if we take this and this is what shape is it?\nWe've got color channels.\nWe've got height.\nWe've got width and now we've flattened it to be color channels, height, width.\nSo we've got one big feature vector because 28 by 28 equals what?\nWe've got one value per pixel, 784.\nOne value per pixel in our output vector.\nNow where did we see this before?\nIf we go back to our keynote, if we have a look at Tesla's takes eight cameras and then\nit turns it into a three dimensional vector space, vector space.\nSo that's what we're trying to do here.\nWe're trying to encode whatever data we're working with in Tesla's case.\nThey have eight cameras.\nNow theirs has more dimensions than ours because they have the time aspect because they're\ndealing with video and they have multiple different camera angles.\nWe're just dealing with a single image here.\nBut regardless, the concept is the same.\nWe're trying to condense information down into a single vector space.\nAnd so if we come back to here, why might we do this?\nWell, it's because we're going to build a baseline model and we're going to use a linear\nlayer as the baseline model.\nAnd the linear layer can't handle multi dimensional data like this.\nWe want it to have a single vector as input.\nNow this will make a lot more sense after we've coded up our model.\nLet's do that from torch import and then we're going to go class, fashion, amnest, model\nV zero.\nWe're going to inherit from an end dot module.\nAnd inside here, we're going to have an init function in the constructor.\nWe're going to pass in self.\nWe're going to have an input shape, which we'll use a type hint, which will take an integer\nbecause remember, input shape is very important for machine learning models.\nWe're going to define a number of hidden units, which will also be an integer, and then we're\ngoing to define our output shape, which will be what do you think our output shape will\nbe?\nHow many classes are we dealing with?\nWe're dealing with 10 different classes.\nSo our output shape will be, I'll save that for later on.\nI'll let you guess for now, or you might already know, we're going to initialize it.\nAnd then we're going to create our layer stack.\nself.layer stack equals nn.sequential, recall that sequential, whatever you put inside sequential,\nif data goes through sequential, it's going to go through it layer by layer.\nSo let's create our first layer, which is going to be nn.flatten.\nSo that means anything that comes into this first layer, what's going to happen to it?\nIt's going to flatten its external dimensions here.\nSo it's going to flatten these into something like this.\nSo we're going to flatten it first, flatten our data.\nThen we're going to pass in our linear layer.\nAnd we're going to have how many n features this is going to be input shape, because we're\ngoing to define our input shape here.\nAnd then we're going to go out features, equals hidden units.\nAnd then we're going to create another linear layer here.\nAnd we're going to set up n features, equals hidden units.\nWhy are we doing this?\nAnd then out features equals output shape.\nWhy are we putting the same out features here as the n features here?\nWell, because subsequent layers, the input of this layer here, its input shape has to\nline up with the output shape of this layer here.\nHence why we use out features as hidden units for the output of this nn.linear layer.\nAnd then we use n features as hidden units for the input value of this hidden layer here.\nSo let's keep going.\nLet's go def.\nWe'll create the forward pass here, because if we subclass nn.module, we have to override\nthe forward method.\nThe forward method is going to define what?\nIt's going to define the forward computation of our model.\nSo we're just going to return self.layer stack of x.\nSo our model is going to take some input, x, which could be here, x.\nIn our case, it's going to be a batch at a time, and then it's going to pass each sample\nthrough the flatten layer.\nIt's going to pass the output of the flatten layer to this first linear layer, and it's\ngoing to pass the output of this linear layer to this linear layer.\nSo that's it.\nOur model is just two linear layers with a flatten layer.\nThe flatten layer has no learnable parameters.\nOnly these two do.\nAnd we have no nonlinearities.\nSo do you think this will work?\nDoes our data set need nonlinearities?\nWell, we can find out once we fit our model to the data, but let's set up an instance\nof our model.\nSo torch dot manual seed.\nLet's go set up model with input parameters.\nSo we have model zero equals fashion MNIST model, which is just the same class that we\nwrote above.\nAnd here's where we're going to define the input shape equals 784.\nWhere will I get that from?\nWell, that's here.\nThat's 28 by 28.\nSo the output of flatten needs to be the input shape here.\nSo we could put 28 by 28 there, or we're just going to put 784 and then write a comment\nhere.\nThis is 28 by 28.\nNow if we go, I wonder if nn.linear will tell us, nn.linear will tell us what it expects\nas in features.\nSize of each input sample, shape, where star means any number of dimensions, including\nnone in features, linear weight, well, let's figure it out.\nLet's see what happens if in doubt coded out, hey, we'll see what we can do.\nIn units equals, let's go with 10 to begin with.\nHow many units in the hidden layer?\nAnd then the output shape is going to be what?\nOutput shape is length of class names, which will be 1 for every class.\nBeautiful.\nAnd now let's go model zero.\nWe're going to keep it on the CPU to begin with.\nWe could write device-agnostic code, but to begin, we're going to send it to the CPU.\nI might just put that up here, actually, to CPU.\nAnd then let's have a look at model zero.\nWonderful.\nSo we can try to do a dummy forward pass and see what happens.\nSo let's create dummy x equals torch, rand, we'll create it as the same size of image.\nJust a singular image.\nSo this is going to be a batch of one, color channel one, height 28, height 28.\nAnd we're going to go model zero and pass through dummy x.\nSo this is going to send dummy x through the forward method.\nLet's see what happens.\nOkay, wonderful.\nSo we get an output of 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 logits.\nBeautiful.\nThat's exactly what we want.\nWe have one logit value per class that we have.\nNow what would happen if we got rid of flatten?\nThen we ran this, ran this, ran this.\nWhat do we get?\nOh, mat one and mat two shapes cannot be multiplied.\nSo we have 28 by 28 and 7.\nOkay, what happens if we change our input shape to 28?\nWe're getting shape mismatches here.\nWhat happens here?\nOh, okay, we get an interesting output, but this is still not the right shape, is it?\nSo that's where the flatten layer comes in.\nWhat is the shape of this?\nOh, we get 1, 1, 28, 10.\nOh, so that's why we put in flatten so that it combines it into a vector.\nSo we get rid of this, see if we just leave it in this shape?\nWe get 28 different samples of 10, which is not what we want.\nWe want to compress our image into a singular vector and pass it in.\nSo let's reinstanceuate the flatten layer and let's make sure we've got the right input\nshape here, 28 by 28, and let's pass it through, torch size 110.\nThat's exactly what we want, 1 logit per class.\nSo this could be a bit fiddly when you first start, but it's also a lot of fun once you\nget it to work.\nAnd so just keep that in mind, I showed you what it looks like when you have an error.\nOne of the biggest errors that you're going to face in machine learning is different tensor\nshape mismatches.\nSo just keep in mind the data that you're working with and then have a look at the documentation\nfor what input shape certain layers expect.\nSo with that being said, I think it's now time that we start moving towards training\nour model.\nI'll see you in the next video.\nWelcome back.\nIn the last video, we created model zero, which is going to be our baseline model for\nour computer vision problem of detecting different types of clothing in 28 by 28 gray scale\nimages.\nAnd we also learned the concept of making sure our or we rehashed on the concept of\nmaking sure our input and output shapes line up with where they need to be.\nWe also did a dummy forward pass with some dummy data.\nThis is a great way to troubleshoot to see if your model shapes are correct.\nIf they come out correctly and if the inputs are lining up with where they need to be.\nAnd just to rehash on what our model is going to be or what's inside our model, if we check\nmodel zero state dict, what we see here is that our first layer has a weight tensor.\nIt also has a bias and our next layer has a weight tensor and it also has a bias.\nSo these are of course initialized with random values, but the whole premise of deep learning\nand machine learning is to pass data through our model and use our optimizer to update\nthese random values to better represent the features in our data.\nAnd I keep saying features, but I just want to rehash on that before we move on to the\nnext thing.\nFeaturing data could be almost anything.\nSo for example, the feature of this bag could be that it's got a rounded handle at the\ntop.\nIt has a edge over here.\nIt has an edge over there.\nNow, we aren't going to tell our model what features to learn about the data.\nThe whole premise of it is to, or the whole fun, the whole magic behind machine learning\nis that it figures out what features to learn.\nAnd so that is what the weights and bias matrices or tensors will represent is different features\nin our images.\nAnd there could be many because we have 60,000 images of 10 classes.\nSo let's keep pushing forward.\nIt's now time to set up a loss function and an optimizer.\nSpeaking of optimizers, so 3.1 set up loss optimizer and evaluation metrics.\nNow recall in notebook two, I'm going to turn this into markdown.\nWe created, oh, I don't need an emoji there.\nSo this is, by the way, we're just moving through this workflow.\nWe've got our data ready into tensors.\nWe've built a baseline model.\nIt's now time to pick a loss function and an optimizer.\nSo we go back to Google Chrome.\nThat's right here.\nLoss function.\nWhat's our loss function going to be?\nSince we're working with multi-class data, our loss function will be NN dot cross entropy\nloss.\nAnd our optimizer, we've got a few options here with the optimizer, but we've had practice\nin the past with SGD, which stands for stochastic gradient descent and the atom optimizer.\nSo our optimizer, let's just stick with SGD, which is kind of the entry level optimizer\ntorch opt in SGD for stochastic gradient descent.\nAnd finally, our evaluation metric, since we're working on a classification problem, let's\nuse accuracy as our evaluation metric.\nSo recall that accuracy is a classification evaluation metric.\nNow, where can we find this?\nWell, if we go into learnpytorch.io, this is the beauty of having online reference material.\nIn here, neural network classification with PyTorch, in this notebook, section 02, we\ncreated, do we have different classification methods?\nYes, we did.\nSo we've got a whole bunch of different options here for classification evaluation metrics.\nWe've got accuracy, precision, recall, F1 score, a confusion matrix.\nNow we have some code that we could use.\nIf we wanted to use torch metrics for accuracy, we could.\nAnd torch metrics is a beautiful library that has a lot of evaluation.\nOh, it doesn't exist.\nWhat happened to torch metrics?\nMaybe I need to fix that.\nLink.\nTorch metrics has a whole bunch of different PyTorch metrics.\nSo very useful library.\nBut we also coded a function in here, which is accuracy FN.\nSo we could copy this, straight into our notebook here.\nOr I've also, if we go to the PyTorch deep learning GitHub, I'll just bring it over here.\nI've also put it in helper functions.py.\nAnd this is a script of common functions that we've used throughout the course, including\nif we find accuracy function here.\nCalculate accuracy.\nNow, how would we get this helper functions file, this Python file, into our notebook?\nOne way is to just copy the code itself, straight here.\nBut let's import it as a Python script.\nSo import request, and we're going to go from pathlib import path.\nSo we want to download, and this is actually what you're going to see, very common practice\nin larger Python projects, especially deep learning and machine learning projects, is\ndifferent functionality split up in different Python files.\nAnd that way, you don't have to keep rewriting the same code over and over again.\nLike you know how we've written a training and testing loop a fair few times?\nWell, if we've written it once and it works, we might want to save that to a.py file so\nwe can import it later on.\nSo let's now write some code to import this helper functions.py file into our notebook\nhere.\nSo download helper functions from learn pytorch repo.\nSo we're going to check if our helper functions.py, if this already exists, we don't want\nto download it.\nSo we'll print helper functions.py already exists, skipping download, skipping download\n.dot.\nAnd we're going to go else here.\nIf it doesn't exist, so we're going to download it, downloading helper functions.py.\nAnd we're going to create a request here with the request library equals request.get.\nNow here's where we have to pass in the URL of this file.\nIt's not this URL here.\nWhen dealing with GitHub, to get the actual URL to the files, many files, you have to\nclick the raw button.\nSo I'll just go back and show you, click raw here.\nAnd we're going to copy this raw URL.\nSee how it's just text here?\nThis is what we want to download into our co-lab notebook.\nAnd we're going to write it in there, request equals request.get.\nAnd we're going to go with open, and here's where we're going to save our helper functions\n.py.\nWe're going to write binary as file, F is for file.\nWe're going to go F.write, request.content.\nSo what this is saying is Python is going to create a file called helper functions.py\nand give it write binary permissions as F, F is for file, short for file.\nAnd then we're going to say F.write, request, get that information from helper functions\n.py here, and write your content to this file here.\nSo let's give that a shot.\nBeautiful.\nSo downloading helper functions.py, let's have a look in here.\nDo we have helper functions.py?\nYes, we do.\nWonderful.\nWe can import our accuracy function.\nWhere is it?\nThere we go.\nImport accuracy function.\nSo this is very common practice when writing lots of Python code is to put helper functions\ninto.py scripts.\nSo let's import the accuracy metric.\nAccuracy metric from helper functions.\nOf course, we could have used torch metrics as well.\nThat's another perfectly valid option, but I just thought I'd show you what it's like\nto import your own helper function script.\nOf course, you can customize helper functions.py to have whatever you want in there.\nSo see this?\nWe've got from helper functions, import accuracy function.\nWhat's this saying?\nCould not be resolved.\nIs this going to work?\nIt did.\nAnd where you can go accuracy function, do we get a doc string?\nHmm.\nSeems like colab isn't picking things up, but that's all right.\nIt looks like it still worked.\nWe'll find out later on if it actually works when we train our model.\nSo set up loss function and optimizer.\nSo I'm going to set up the loss function equals nn dot cross entropy loss.\nAnd I'm going to set up the optimizer here as we discussed before as torch dot opt-in\ndot SGD for stochastic gradient descent.\nThe parameters I want to optimize are the parameters from model zero, our baseline model,\nwhich we had a look at before, which are all these random numbers.\nWe'd like our optimizer to tweak them in some way, shape, or form to better represent our\ndata.\nAnd then I'm going to set the learning rate here.\nHow much should they be tweaked each epoch?\nI'm going to set it to 0.1.\nNice and high because our data set is quite simple.\nIt's 28 by 28 images.\nThere are 60,000 of them.\nBut again, if this doesn't work, we can always adjust this and experiment, experiment,\nexperiment.\nSo let's run that.\nWe've got a loss function.\nIs this going to give me a doc string?\nThere we go.\nSo calculates accuracy between truth and predictions.\nNow, where does this doc string come from?\nWell, let's have a look, hope of functions.\nThat's what we wrote before.\nGood on us for writing good doc strings, accuracy function.\nWell, we're going to test all these out in the next video when we write a training loop.\nSo, oh, actually, I think we might do one more function before we write a training loop.\nHow about we create a function to time our experiments?\nYeah, let's give that a go in the next video.\nI'll see you there.\nWelcome back.\nIn the last video, we downloaded our helper functions.py script and imported our accuracy\nfunction that we made in notebook two.\nBut we could really beef this up, our helper functions.py file.\nWe could put a lot of different helper functions in there and import them so we didn't have\nto rewrite them.\nThat's just something to keep in mind for later on.\nBut now, let's create a function to time our experiments.\nSo creating a function to time our experiments.\nSo one of the things about machine learning is that it's very experimental.\nYou've probably gathered that so far.\nSo let's write here.\nSo machine learning is very experimental.\nTwo of the main things you'll often want to track are, one, your model's performance\nsuch as its loss and accuracy values, et cetera.\nAnd two, how fast it runs.\nSo usually you want a higher performance and a fast model, that's the ideal scenario.\nHowever, you could imagine that if you increase your model's performance, you might have\na bigger neural network.\nIt might have more layers.\nIt might have more hidden units.\nIt might degrade how fast it runs because you're simply making more calculations.\nSo there's often a trade-off between these two.\nAnd how fast it runs will really be important if you're running a model, say, on the internet\nor say on a dedicated GPU or say on a mobile device.\nSo these are two things to really keep in mind.\nSo because we're tracking our model's performance with our loss value and our accuracy function,\nlet's now write some code to check how fast it runs.\nAnd I did on purpose above, I kept our model on the CPU.\nSo we're also going to compare later on how fast our model runs on the CPU versus how\nfast it runs on the GPU.\nSo that's something that's coming up.\nLet's write a function here.\nWe're going to use the time module from Python.\nSo from time it, import the default timer, as I'm going to call it timer.\nSo if we go Python default timer, do we get the documentation for, here we go, time it.\nSo do we have default timer, wonderful.\nSo the default timer, which is always time.perf counter, you can read more about Python timing\nfunctions in here.\nBut this is essentially just going to say, hey, this is the exact time that our code\nstarted.\nAnd then we're going to create another stop for when our code stopped.\nAnd then we're going to compare the start and stop times.\nAnd that's going to basically be how long our model took to train.\nSo we're going to go def print train time.\nThis is just going to be a display function.\nSo start, we're going to get the float type hint, by the way, start an end time.\nSo the essence of this function will be to compare start and end time.\nAnd we're going to set the torch or the device here, we'll pass this in as torch dot device.\nAnd we're going to set that default to none, because we want to compare how fast our model\nruns on different devices.\nSo I'm just going to write a little doc string here, prints, difference between start and\nend time.\nAnd then of course, we could add more there for the arguments, but that's a quick one liner.\nTell us what our function does.\nSo total time equals end minus start.\nAnd then print, we're going to write here train time on, whichever device we're using\nmight be CPU, might be GPU.\nTotal time equals, we'll go to three and we'll say seconds, three decimal places that is\nand return total time.\nBeautiful.\nSo for example, we could do start time equals timer, and then end time equals timer.\nAnd then we can put in here some code between those two.\nAnd then if we go print train, oh, maybe we need a timer like this, we'll find out if\nand out code it out, you know, we'll see if it works.\nStart time and end equals end time and device equals.\nWe're running on the CPU right now, CPU, let's see if this works, wonderful.\nSo it's a very small number here.\nSo train time on CPU, very small number, because the start time is basically on this\nexact line, comment basically it takes no time to run, then end time is on here, we get\n3.304 times 10 to the power of negative five.\nSo quite a small number, but if we put some modeling code in here, it's going to measure\nthe start time of this cell, it's going to model our code in there, then we have the\nend time, and then we find out how long our model took the train.\nSo with that being said, I think we've got all of the pieces of the puzzle for creating\nsome training and testing functions.\nSo we've got a loss function, we've got an optimizer, we've got a valuation metric, we've\ngot a timing function, we've got a model, we've got some data.\nHow about we train our first baseline computer vision model in the next video?\nI'll see you there.\nGood morning.\nWell might not be morning wherever you are in the world.\nIt's nice and early here, I'm up recording some videos, because we have a lot of momentum\ngoing with this, but look at this, I took a little break last night, I have a runtime\ndisconnected, but this is just what's going to happen if you're using Google Colab.\nSince I use Google Colab Pro, completely unnecessary for the course, but I just found it worth\nit for how much I use Google Colab, I get longer idle timeouts, so that means that my\nColab notebook will stay persistent for a longer time.\nBut of course overnight it's going to disconnect, so I click reconnect, and then if I want to\nget back to wherever we were, because we downloaded some data from torchvision.datasets, I have\nto rerun all of these cells.\nSo a nice shortcut, we might have seen this before, is to just come down to where we were,\nand if all the code above works, oh there we go, I wrote myself some notes of where we're\nup to.\nLet's go run before, so this is just going to run all the cells above, and we're up\nto here, 3.3 creating a training loop, and training a model on batches of data.\nSo that's going to be a little bit interesting, and I wrote myself another reminder here, this\nis a little bit of behind the scenes, the optimise will update a model's parameters\nonce per batch rather than once per epoch.\nSo let's hold myself to that note, and make sure I let you know.\nSo we're going to make another title here.\nLet's go creating a training loop, and training a model on batches of data.\nSo something a little bit different to what we may have seen before if we haven't created\nbatches of data using data loader, and recall that just up above here, we've got something\nlike 1800 there, there we go.\nSo we've split our data into batches, rather than our model looking at 60,000 images of\nfashion MNIST data at one time, it's going to look at 1875 batches of 32, so 32 images\nat the time, of the training data set, and 313 batches of 32 of the test data set.\nSo let's go to training loop and train our first model.\nSo I'm going to write out a few steps actually, because we have to do a little bit differently\nto what we've done before.\nSo one, we want to loop through epochs, so a number of epochs.\nLoop through training batches, and by the way, you might be able to hear some birds singing,\nthe sun is about to rise, I hope you enjoy them as much as I do.\nSo we're going to perform training steps, and we're going to calculate calculate the\ntrain loss per batch.\nSo this is going to be one of the differences between our previous training loops.\nAnd this is going to, after number two, we're going to loop through the testing batches.\nSo we'll train and evaluate our model at the same step, or same loop.\nAnd we're going to perform testing steps.\nAnd then we're going to calculate the test loss per batch as well, per batch.\nWonderful, four, we're going to, of course, print out what's happening.\nYou may have seen the unofficial PyTorch optimization loop theme song.\nAnd we're going to time it all for fun, of course, because that's what our timing function\nis for.\nSo let's get started.\nThere's a fair few steps here, but nothing that we can't handle.\nAnd remember the motto, if and out, code it out.\nWell, there's another one, if and out, run the code, but we haven't written any code to\nrun just yet.\nSo we're going to import TQDM for a progress bar.\nIf you haven't seen TQDM before, it's a very good Python progress bar that you can add\nwith a few lines of code.\nSo this is just the GitHub.\nIt's open source software, one of my favorite pieces of software, and it's going to give\nus a progress bar to let us know how many epochs our training loop has gone through.\nIt doesn't have much overhead, but if you want to learn more about it, please refer\nto the TQDM GitHub.\nHowever, the beautiful thing is that Google CoLab has TQDM built in because it's so good\nand so popular.\nSo we're going to import from TQDM.auto.\nSo there's a few different types of TQDM progress bars.auto is just going to recognize what\ncompute environment we're using.\nAnd it's going to give us the best type of progress bar for what we're doing.\nSo for example, Google CoLab is running a Jupyter Notebook behind the scenes.\nSo the progress bar for Jupyter Notebooks is a little bit different to Python scripts.\nSo now let's set the seed and start the timer.\nWe want to write all of our training loop in this single cell here.\nAnd then once it starts, once we run this cell, we want the timer to start so that we\ncan time how long the entire cell takes to run.\nSo we'll go train time start on CPU equals, we set up our timer before, beautiful.\nNow we're going to set the number of epochs.\nNow we're going to keep this small for faster training time so we can run more experiments.\nSo we'll keep this small for faster training time.\nThat's another little tidbit.\nDo you notice how quickly all of the cells ran above?\nWell, that's because we're using a relatively small data set.\nIn the beginning, when you're running experiments, you want them to run quite quickly so that\nyou can run them more often.\nSo you can learn more about your data so that you can try different things, try different\nmodels.\nSo this is why we're using number of epochs equals three.\nWe start with three so that our experiment runs in 30 seconds or a minute or so.\nThat way, if something doesn't work, we haven't wasted so much time waiting for a model to\ntrain.\nLater on, we could train it for 100 epochs if we wanted to.\nSo we're going to create a training and test loop.\nSo for epoch in TQDM range epochs, let's get this going.\nSo for TQDM to work, we just wrap our iterator with TQDM and you'll see later on how this\ntracks the progress.\nSo I'm going to put out a little print statement here.\nWe'll go epoch.\nThis is just going to say what epoch we're on.\nWe'll go here.\nThat's something that I like to do quite often is put little print statements here and there\nso that we know what's going on.\nSo let's set up the training.\nWe're going to have to instantiate the train loss.\nWe're going to set that to zero to begin with.\nAnd we're going to cumulatively add some values to the train loss here and then we'll\nsee later on how this accumulates and we can calculate the training loss per batch.\nLet's what we're doing up here, calculate the train loss per batch.\nAnd then finally, at the end of the loop, we will divide our training loss by the number\nof batches so we can get the average training loss per batch and that will give us the training\nloss per epoch.\nNow that's a lot of talking.\nIf that doesn't make sense, remember.\nBut if and out, code it out.\nSo add a loop to loop through the training batches.\nSo because our data is batchified now and I've got a crow or maybe a cooker bar sitting\non the roof across from my apartment, it's singing its song this morning, lovely.\nSo we're going to loop through our training batch data.\nSo I've got four batch, comma x, y, because remember our training batches come in the\nform of X.\nSo that's our data or our images and why, which is label.\nYou could call this image label or target as part of which would, but it's convention\nto often call your features X and your labels Y.\nWe've seen this before in we're going to enumerate the train data loader as well.\nWe do this so we can keep track of the number of batches we've been through.\nSo that will give us batch there.\nI'm going to set model zero to training mode because even though that's the default, we\njust want to make sure that it's in training mode.\nNow we're going to do the forward pass.\nIf you remember, what are the steps in apply to our optimization loop?\nWe do the forward pass.\nWe calculate the loss of the minus zero grad, last backwards, up to minus a step, step,\nstep.\nSo let's do that.\nHey, model zero, we'll put the features through there and then we're going to calculate the\nloss.\nWe've been through these steps before.\nSo we're not going to spend too much time on the exact steps here, but we're just going\nto practice writing them out.\nAnd of course, later on, you might be thinking, then you'll, how come we haven't functionalized\nthis training loop already?\nWe've seemed to write the same generic code over and over again.\nWell, that's because we like to practice writing PyTorch code, right?\nWe're going to functionalize them later on.\nDon't you worry about that.\nSo here's another little step that we haven't done before is we have the training loss.\nAnd so because we've set that to zero to begin with, we're going to accumulate the training\nloss values every batch.\nSo we're going to just add it up here.\nAnd then later on, we're going to divide it by the total number of batches to get the\naverage loss per batch.\nSo you see how this loss calculation is within the batch loop here?\nSo this means that one batch of data is going to go through the model.\nAnd then we're going to calculate the loss on one batch of data.\nAnd this loop is going to continue until it's been through all of the batches in the train\ndata loader.\nSo 1875 steps or whatever there was.\nSo accumulate train loss.\nAnd then we're going to optimize a zero grad, optimizer dot zero grad.\nAnd then number four is what?\nLoss backward.\nLoss backward.\nWe'll do the back propagation step.\nAnd then finally, we've got number five, which is optimizer step.\nSo this is where I left my little note above to remind me and to also let you know, highlight\nthat the optimizer will update a model's parameters once per batch rather than once per epoch.\nSo you see how we've got a for loop inside our epoch loop here.\nSo the batch loop.\nSo this is what I meant that the optimizer, this is one of the advantages of using mini\nbatches is not only is it more memory efficient because we're not loading 60,000 images into\nmemory at a time.\nWe are updating our model's parameters once per batch rather than waiting for it to see\nthe whole data set with every batch.\nOur model is hopefully getting slightly better.\nSo that is because the optimizer dot step call is within the batch loop rather than the\nepoch loop.\nSo let's now print out what's happening.\nPrint out what's happening.\nSo if batch, let's do it every 400 or so batches because we have a lot of batches.\nWe don't want to print out too often, otherwise we'll just fill our screen with numbers.\nThat might not be a bad thing, but 400 seems a good number.\nThat'll be about five printouts if we have 2000 batches.\nSo print looked at, and of course you can adjust this to whatever you would like.\nThat's the flexibility of PyTorch, flexibility of Python as well.\nSo looked at how many samples have we looked at?\nSo we're going to take the batch number, multiply it by X, the length of X is going\nto be 32 because that is our batch size.\nThen we're going to just write down here the total number of items that we've got now\nof data set, and we can access that by going train data loader dot data set.\nSo that's going to give us length of the data set contained within our train data loader,\nwhich is you might be able to guess 60,000 or should be.\nNow we have to, because we've been accumulating the train loss, this is going to be quite\nhigh because we've been adding every single time we've calculated the loss, we've been\nadding it to the train loss, the overall value per batch.\nSo now let's adjust if we wanted to find out, see how now we've got this line, we're outside\nof the batch loop.\nWe want to adjust our training loss to get the average training loss per batch per epoch.\nSo we're coming back to the epoch loop here.\nA little bit confusing, but you just line up where the loops are, and this is going to\nhelp you figure out what context you're computing in.\nSo now we are in the epoch loop.\nSo divide total train loss by length of train data loader, oh, this is so exciting, training\nour biggest model yet.\nSo train loss equals or divide equals, we're going to reassign the train loss, we're going\nto divide it by the length of the train data loader.\nSo why do we do this?\nWell, because we've accumulated the train loss here for every batch in the train data\nloader, but we want to average it out across how many batches there are in the train data\nloader.\nSo this value will be quite high until we readjust it to find the average loss per epoch, because\nwe are in the epoch loop.\nAll right, there are a few steps going on, but that's all right, we'll figure this out,\nor what should happening in a minute, let's code up the testing loop.\nSo testing, what do we have to do for testing?\nWell, let's set up a test loss variable.\nWhy don't we do accuracy for testing as well?\nDid we do accuracy for training?\nWe didn't do accuracy for training, but that's all right, we'll stick to doing accuracy for\ntesting.\nWe'll go model zero dot eval, we'll put it in evaluation mode, and we'll turn on our\ninference mode context manager with torch dot inference mode.\nNow we'll do the same thing for x, y in test data loader, we don't need to keep track\nof the batches here again in the test data loader.\nSo we'll just loop through x, so features, images, and labels in our test data loader.\nWe're going to do the forward pass, because the test loop, we don't have an optimization\nstep, we are just passing our data through the model and evaluating the patterns it learned\non the training data.\nSo we're going to pass in x here.\nThis might be a little bit confusing, let's do this x test, y test.\nThat way we don't get confused with our x above for the training set.\nNow we're going to calculate the loss, a cum, relatively might small that wrong app to\nsound that out.\nWhat do we have here?\nSo we've got our test loss variable that we just assigned to zero above, just up here.\nSo we're going to do test loss plus equals.\nWe're doing this in one step here.\nTest spread, y test.\nSo we're comparing our test prediction to our y test labels, our test labels.\nNow we're going to back out of the for loop here, because that's all we have to do, the\nforward pass and calculate the loss for the test data set.\nOh, I said we're going to calculate the accuracy.\nSilly me.\nSo calculate accuracy.\nLet's go test act.\nAnd we've got plus equals.\nWe can bring out our accuracy function here.\nThat's what we downloaded from our helper functions dot pi before, y true equals y test.\nAnd then y pred equals test, pred dot arg max, dim equals one.\nWhy do we do this?\nWell, because recall that the outputs of our model, the raw outputs of our model are going\nto be logits and our accuracy function expects our true labels and our predictions to be\nin the same format.\nIf our test pred is just logits, we have to call arg max to find the logit value with\nthe highest index, and that will be the prediction label.\nAnd so then we're comparing labels to labels.\nThat's what the arg max does here.\nSo we can back out of the batch loop now, and we're going to now calculate Cal queue\nlength, the test loss, average per batch.\nSo let's go here, test loss, divide equals length test data loader.\nSo because we were in the context of the loop here of the batch loop, our test lost and\ntest accuracy values are per batch and accumulated every single batch.\nSo now we're just dividing them by how many batches we had, test data loader, and the\nsame thing for the accuracy, calculate the ACK or test ACK average per batch.\nSo this is giving us test loss and test accuracy per epoch, test ACK divided equals length,\ntest data loader, wonderful, we're so close to finishing this up.\nAnd now we'll come back to where's our epoch loop.\nWe can, these lines are very helpful in Google CoLab, we scroll down.\nI believe if you want them, you can go settings or something like that, yeah, settings.\nThat's where you can get these lines from if you don't have them.\nSo print out what's happening.\nWe are going to print f equals n, let's get the train loss in here.\nTen loss and we'll print that to four decimal places.\nAnd then we'll get the test loss, of course, test loss and we'll go, we'll get that to four\ndecimal places as well.\nAnd then we'll get the test ACK, test accuracy, we'll get that to four decimal places as well.\nFor f, wonderful.\nAnd then finally, one more step, ooh, we've written a lot of code in this video.\nWe want to calculate the training time because that's another thing that we want to track.\nWe want to see how long our model is taken to train.\nSo train time end on CPU is going to equal the timer and then we're going to get the\ntotal train time model zero so we can set up a variable for this so we can compare our\nmodeling experiments later on.\nWe're going to go print train time, start equals train time, start on CPU and equals\ntrain time end on CPU.\nAnd finally, the device is going to be string next model zero dot parameters.\nSo we're just, this is one way of checking where our model zero parameters live.\nSo beautiful, all right.\nHave we got enough brackets there?\nI don't think we do.\nOkay.\nThere we go.\nWhoo.\nI'll just show you what the output of this is.\nSo next, model zero dot parameters, what does this give us?\nOh, can we go device here?\nOh, what do we have here?\nModel zero dot parameters.\nI thought this was a little trick.\nAnd then if we go next parameter containing.\nI thought we could get device, oh, there we go.\nExcuse me.\nThat's how we get it.\nThat's how we get the device that it's on.\nSo let me just turn this.\nThis is what the output of that's going to be CPU.\nThat's what we're after.\nSo troubleshooting on the fly here.\nHopefully all of this code works.\nSo we went through all of our steps.\nWe're looping through epochs at the top level here.\nWe looped through the training batches, performed the training steps.\nSo our training loop, forward pass, loss calculation, optimizer zero grad, loss backwards, calculate\nthe loss per batch, accumulate those.\nWe do the same for the testing batches except without the optimizer steps and print out\nwhat's happening and we time it all for fun.\nA fair bit going on here, but if you don't think there's any errors, give that a go, run\nthat code.\nI'm going to leave this one on a cliffhanger and we're going to see if this works in the\nnext video.\nI'll see you there.\nWelcome back.\nThe last video was pretty full on.\nWe did a fair few steps, but this is all good practice.\nThe best way to learn PyTorch code is to write more PyTorch code.\nSo did you try it out?\nDid you run this code?\nDid it work?\nDid we probably have an error somewhere?\nWell, let's find out together.\nYou ready?\nLet's train our biggest model yet in three, two, one, bomb.\nOh, of course we did.\nWhat do we have?\nWhat's going on?\nIndentation error.\nAh, classic.\nSo print out what's happening.\nDo we not have an indent there?\nOh, is that not in line with where it needs to be?\nExcuse me.\nOkay.\nWhy is this not in line?\nSo this is strange to me, enter.\nHow did this all get off by one?\nI'm not sure, but this is just what you'll face.\nLike sometimes you'll write this beautiful code that should work, but the main error\nof your entire code is that it's off by a single space.\nI'm not sure how that happened, but we're just going to pull this all into line.\nWe could have done this by selecting it all, but we're going to do it line by line just\nto make sure that everything's in the right order, beautiful, and we print out what's\nhappening.\nThree, two, one, round two.\nWe're going.\nOkay.\nSo this is the progress bar I was talking about.\nLook at that.\nHow beautiful is that?\nOh, we're going quite quickly through all of our samples.\nI need to talk faster.\nOh, there we go.\nWe've got some good results.\nWe've got the tests, the train loss, the test loss and the test accuracy is pretty darn\ngood.\nOh my goodness.\nThis is a good baseline already, 67%.\nSo this is showing us it's about seven seconds per iteration.\nRemember TQDM is tracking how many epochs.\nWe're going through.\nSo we have three epochs and our print statement is just saying, hey, we've looked at zero\nout of 60,000 samples and we looked at 12,000 out of 60,000 samples and we finished on\nan epoch two because it's zero indexed and we have a train loss of 0.4550 and a test\nloss 476 and a test accuracy 834265 and a training time about just over 21 seconds or\njust under 22.\nSo keep in mind that your numbers may not be the exact same as mine.\nThey should be in the same realm as mine, but due to inherent randomness of machine learning,\neven if we set the manual seed might be slightly different.\nSo don't worry too much about that and what I mean by in the same realm, if your accuracy\nis 25 rather than 83, well then probably something's wrong there.\nBut if it's 83.6, well then that's not too bad.\nAnd the same with the train time on CPU, this will be heavily dependent, how long it takes\nto train will be heavily dependent on the hardware that you're using behind the scenes.\nSo I'm using Google Colab Pro.\nNow that may mean I get a faster CPU than the free version of Google Colab.\nIt also depends on what CPU is available in Google's computer warehouse where Google\nColab is hosting of how fast this will be.\nSo just keep that in mind.\nIf your time is 10 times that, then there's probably something wrong.\nIf your time is 10 times less than that, well, hey, keep using that hardware because that's\npretty darn good.\nSo let's keep pushing forward.\nThis will be our baseline that we try to improve upon.\nSo we have an accuracy of 83.5 and we have a train time of 20 or so seconds.\nSo we'll see what we can do with a model on the GPU later and then also later on a\nconvolutional neural network.\nSo let's evaluate our model where we up to what we just did.\nWe built a training loop.\nSo we've done that.\nThat was a fair bit of code.\nBut now we're up to we fit the model to the data and make a prediction.\nLet's do these two combined, hey, we'll evaluate our model.\nSo we'll come back.\nNumber four is make predictions and get model zero results.\nNow we're going to create a function to do this because we want to build multiple models\nand that way we can, if we have, say, model 0123, we can pass it to our function to evaluate\nthat model and then we can compare the results later on.\nSo that's something to keep in mind.\nIf you're going to be writing a bunch of code multiple times, you probably want to\nfunctionize it and we could definitely do that for our training and last loops.\nBut we'll see that later on.\nSo let's go deaf of our model.\nSo evaluate a given model, we'll pass it in a model, which will be a torch dot nn dot\nmodule, what of type.\nAnd we'll pass it in a data loader, which will be of type torch dot utils dot data dot\ndata loader.\nAnd then we'll pass in the loss function so that it can calculate the loss.\nWe could pass in an evaluation metric if we wanted to track that too.\nSo this will be torch nn dot module as well.\nAnd then, oh, there we go.\nSpeaking of an evaluation function, let's pass in our accuracy function as well.\nAnd I don't want L, I want that.\nSo we want to return a dictionary containing the results of model predicting on data loader.\nSo that's what we want.\nWe're going to return a dictionary of model results.\nThat way we could call this function multiple times with different models and different\ndata loaders and then compare the dictionaries full of results depending on which model we\npassed in here.\nSo let's set up loss and accuracy equals zero, zero, we'll start those off.\nWe'll go, this is going to be much the same as our testing loop above, except it's going\nto be functionalized and we're going to return a dictionary.\nSo we'll turn on our context manager for inferencing with torch dot inference mode.\nNow we're going to loop through the data loader and we'll get the x and y values.\nSo the x will be our data, the y will be our ideal labels, we'll make predictions with\nthe model.\nIn other words, do the forward pass.\nSo we'll go y pred equals model on x.\nNow we don't have to specify what model it is because we've got the model parameter up\nhere.\nSo we're starting to make our functions here or this function generalizable.\nSo it could be used with almost any model and any data loader.\nSo we want to accumulate the loss and accuracy values per batch because this is within the\nbatch loop here per batch.\nAnd then we're going to go loss plus equals loss function, we'll pass it in the y pred\nand the y the true label and we'll do the same with the accuracy.\nSo except this time we use our accuracy function, we'll send in y true equals y and y pred equals\ny pred dot argmax because the raw outputs of our model are logits.\nAnd if we want to convert them into labels, we could take the softmax for the prediction\nprobabilities, but we could also take the argmax and just by skipping the softmax step, the\nargmax will get the index where the highest value load it is, dim equals one.\nAnd then we're going to make sure that we're still within the context manager here.\nSo with torch inference mode, but outside the loop.\nSo that'll be this line here.\nWe're going to scale the loss and act to find the average loss slash act per batch.\nSo loss will divide and assign to the length of the data loader.\nSo that'll divide and reassign it to however many batches are in our data loader that we\npass into our of our model function, then we'll do the same thing for the accuracy here.\nLength data loader, beautiful.\nAnd now we're going to return a dictionary here.\nSo return, we can return the model name by inspecting the model.\nWe get an attribute of the model, which is its class name.\nI'll show you how you can do that.\nSo this is helpful to track if you've created multiple different models and given them different\nclass names, you can access the name attribute.\nSo this only works when model was created with a class.\nSo you just have to ensure that your models have different class names.\nIf you want to do it like that, because we're going to do it like that, we can set the model\nname to be its class name.\nWe'll get the model loss, which is just this value here.\nAfter it's been scaled, we'll turn it into a single value by taking dot item.\nAnd then we'll go model dot act, or we'll get model underscore act for the models accuracy.\nWe'll do the same thing here.\nAct.\nI don't think we need to take the item because accuracy comes back in a different form.\nWe'll find out, if in doubt, code it out.\nSo calculate model zero results on test data set.\nAnd I want to let you know that you can create your own functions here to do almost whatever\nyou want.\nI've just decided that this is going to be helpful for the models and the data that\nwe're building.\nBut keep that in mind that your models, your data sets might be different and will likely\nbe different in the future.\nSo you can create these functions for whatever use case you need.\nModel zero results equals a vowel model.\nSo we're just going to call our function that we've just created here.\nModel is going to equal model zero.\nThe data loader is going to equal what?\nThe test data loader, of course, because we want to evaluate it on the test data set.\nAnd we're going to send in our loss function, which is loss function that we assigned above\njust before our training loop.\nIf we come up here, our loss function is up here, and then if we go back down, we have\nour accuracy function is equal to our accuracy function.\nWe just pass another function in there, beautiful.\nAnd let's see if this works.\nModel zero results.\nDid you see any typos likely or errors in our code?\nHow do you think our model did?\nWell, let's find out.\nOh, there we go.\nWe got model accuracy.\nCan you see how we could reuse this dictionary later on?\nSo if we had model one results, model two results, we could use these dictionaries and compare\nthem all together.\nSo we've got our model name.\nOur version zero, the model has an accuracy of 83.42 and a loss of 0.47 on the test data\nloader.\nAgain, your numbers may be slightly different.\nThey should be in the same realm.\nBut if they're not the exact same, don't worry too much.\nIf they're 20 accuracy points less and the loss is 10 times higher, then you should probably\ngo back through your code and check if something is wrong.\nAnd I believe if we wanted to do a progress bar here, could we do that?\nTQDM.\nLet's have a look, eh?\nOh, look at that progress bar.\nThat's very nice.\nSo that's nice and quick because it's only on 313 batches.\nIt goes quite quick.\nSo now, what's next?\nWell, we've built model one, we've got a model zero, sorry, I'm getting ahead myself.\nWe've got a baseline here.\nWe've got a way to evaluate our model.\nWhat's our workflow say?\nSo we've got our data ready.\nWe've done that.\nWe've picked or built a model.\nWe've picked a loss function.\nWe've built an optimizer.\nWe've created a training loop.\nWe've fit the model to the data.\nWe've made a prediction.\nWe've evaluated the model using loss and accuracy.\nWe could evaluate it by making some predictions, but we'll save that for later on as in visualizing\nsome predictions.\nI think we're up to improving through experimentation.\nSo let's give that a go, hey?\nDo you recall that we trained model zero on the CPU?\nHow about we build model one and start to train it on the GPU?\nSo in the next section, let's create number five, is set up device agnostic code.\nSo we've done this one together for using a GPU if there is one.\nSo my challenge to you for the next video is to set up some device agnostic code.\nSo you might have to go into CoLab if you haven't got a GPU active, change runtime type\nto GPU, and then because it might restart the runtime, you might have to rerun all of\nthe cells above so that we get our helper functions file back and the data and whatnot.\nSo set up some device agnostic code and I'll see you in the next video.\nHow'd you go?\nYou should give it a shot, did you set up some device agnostic code?\nI hope you gave it a go, but let's do it together.\nThis won't take too long.\nThe last two videos have been quite long.\nSo if I wanted to set device agnostic code, I want to see if I have a GPU available, do\nI?\nI can check it from the video SMI.\nThat fails because I haven't activated a GPU in CoLab yet.\nI can also check here, torch CUDA is available.\nThat will PyTorch will check if there's a GPU available with CUDA and it's not.\nSo let's fix these two because we want to start using a GPU and we want to set up device\nagnostic code.\nSo no matter what hardware our system is running, PyTorch leverages it.\nSo we're going to select GPU here, I'm going to click save and you'll notice that our Google\nCoLab notebook will start to reset and we'll start to connect.\nThere we go.\nWe've got a GPU on the back end, Python, three Google Compute Engine back end GPU.\nDo we have to reset this?\nNVIDIA SMI, wonderful, I have a Tesla T4 GPU with 16 gigabytes of memory, that is wonderful.\nAnd now do we have a GPU available?\nOh, torch is not defined.\nWell, do you notice the numbers of these cells?\nOne, two, that means because we've reset our runtime to have a GPU, we have to rerun\nall the cells above.\nSo we can go run before, that's going to run all the cells above, make sure that we download\nthe data, make sure that we download the helper functions file, we go back up, we should see\nour data may be downloading.\nIt shouldn't take too long.\nThat is another advantage of using a relatively small data set that is already saved on PyTorch\ndata sets.\nJust keep in mind that if you use a larger data set and you have to re-download it into\nGoogle Colab, it may take a while to run, and if you build bigger models, they may take\na while to run.\nSo just keep that in mind for your experiments going forward, start small, increase when\nnecessary.\nSo we'll re-run this, we'll re-run this, and finally we're going to, oh, there we go,\nwe've got a GPU, wonderful, but we'll write some device-agnostic code here, set up device-agnostic\ncode.\nSo import-torch, now realistically you quite often do this at the start of every notebook,\nbut I just wanted to highlight how we might do it if we're in the middle, and I wanted\nto practice running a model on a CPU only before stepping things up and going to a GPU.\nSo device equals CUDA, this is for our device-agnostic code, if torch dot CUDA is available, and it\nlooks like this is going to return true, else use the CPU.\nAnd then we're going to check device, wonderful, CUDA.\nSo we've got some device-agnostic code ready to go, I think it's time we built another\nmodel.\nAnd I asked the question before, do you think that the data set that we're working with\nrequires nonlinearity?\nSo the shirts, and the bags, and the shoes, do we need nonlinear functions to model this?\nWell it looks like our baseline model without nonlinearities did pretty well at modeling\nour data, so we've got a pretty good test accuracy value, so 83%, so out of 100 images\nit predicts the right one, 83% of the time, 83 times out of 100, it did pretty well without\nnonlinearities.\nWhy don't we try a model that uses nonlinearities and it runs on the GPU?\nSo you might want to give that a go, see if you can create a model with nonlinear functions,\ntry nn.relu, run it on the GPU, and see how it goes, otherwise we'll do it together in\nthe next video, I'll see you there.\nHello everyone, and welcome back, we are making some terrific progress, let's see how far\nwe've come, we've got a data set, we've prepared our data loaders, we've built a baseline model,\nand we've trained it, evaluated it, now it's time, oh, and the last video we set up device\ndiagnostic code, but where are we in our little framework, we're up to improving through experimentation,\nand quite often that is building a different model and trying it out, it could be using\nmore data, it could be tweaking a whole bunch of different things.\nSo let's get into some coding, I'm going to write it here, model one, I believe we're\nup to section six now, model one is going to be building a better model with nonlinearity,\nso I asked you to do the challenge in the last video to give it a go, to try and build\na model with nonlinearity, I hope you gave it a go, because if anything that this course,\nI'm trying to impart on you in this course, it's to give things a go, to try things out\nbecause that's what machine learning and coding is all about, trying things out, giving it\na go, but let's write down here, we learned about the power of nonlinearity in notebook\nO2, so if we go to the learnpytorch.io book, we go to section number two, we'll just wait\nfor this to load, and then if we come down here, we can search for nonlinearity, the missing\npiece nonlinearity, so I'm going to get this and just copy that in there, if you want to\nsee what nonlinearity helps us do, it helps us model nonlinear data, and in the case of\na circle, can we model that with straight lines, in other words, linear lines?\nAll linear means straight, nonlinear means non-straight, and so we learned that through\nthe power of linear and nonlinear functions, neural networks can model almost any kind\nof data if we pair them in the right way, so you can go back through and read that there,\nbut I prefer to code things out and try it out on our data, so let's create a model with\nnonlinear and linear layers, but we also saw that our model with just linear layers can\nmodel our data, it's performing quite well, so that's where the experimentation side of\nthings will come into play, sometimes you won't know what a model will do, whether it\nwill work or won't work on your data set, but that is where we try different things\nout, so we come up here, we look at our data, hmm, that looks actually quite linear to\nme as a bag, like it's just some straight lines, you could maybe model that with just\nstraight lines, but there are some things which you could potentially classify as nonlinear\nin here, it's hard to tell without knowing, so let's give it a go, let's write a nonlinear\nmodel which is going to be quite similar to model zero here, except we're going to interspurse\nsome relu layers in between our linear layers, so recall that relu is a nonlinear activation\nfunction, and relu has the formula, if something comes in and it's a negative value, relu is\ngoing to turn that negative into a zero, and if something is positive, relu is just going\nto leave it there, so let's create another class here, fashion MNIST model V1, and we're\ngoing to subclass from nn.module, beautiful, and then we're going to initialize our model,\nit's going to be quite the same as what we created before, we want an input shape, that's\ngoing to be an integer, and then we want a number of hidden units, and that's going\nto be an int here, and then we want an output shape, int, and I want to stress as well that\nalthough we're creating a class here with these inputs, classes are as flexible as functions,\nso if you need different use cases for your modeling classes, just keep that in mind that\nyou can build that functionality in, self dot layer stack, we're going to spell layer stack\ncorrectly, and we're going to set this equal to nn dot sequential, because we just want\na sequential set of layers, the first one's going to be nn dot flatten, which is going\nto be flatten inputs into a single vector, and then we're going to go nn dot linear,\nbecause we want to flatten our stuff because we want it to be the right shape, if we don't\nflatten it, we get shape issues, input shape, and then the out features of our linear layer\nis going to be the hidden units, hidden units, I'm just going to make some code cells here\nso that my code goes into the middle of the screen, then here is where we're going to\nadd a nonlinear layer, so this is where we're going to add in a relu function, and where\nmight we put these? Well, generally, you'll have a linear function followed by a nonlinear\nfunction in the construction of neural networks. However, neural networks are as customizable\nas you can imagine, whether they work or not is a different question. So we'll go output\nshape here, as the out features, oh, do we miss this one up? Yes, we did. This needs\nto be hidden units. And why is that? Well, it's because the output shape of this linear\nlayer here needs to match up with the input shape of this linear layer here. The relu\nlayer won't change the shape of our data. And you could test that out by printing the\ndifferent shapes if you'd like. And then we're going to finish off with another nonlinear\nlayer at the end. Relu. Now, do you think that this will improve our model's results\nor not? Well, it's hard to tell without trying it out, right? So let's continue building\nour model. We have to override the forward method. Self X is going to be, we'll give\na type in here, this is going to be a torch tensor as the input. And then we're just going\nto return what's happening here, we go self dot layer stack X. So that just means that\nX is going to pass through our layer stack here. And we could customize this, we could\ntry it just with one nonlinear activation. This is actually our previous network, just\nwith those commented out. All we've done is added in two relu functions. And so I'm\ngoing to run that beautiful. And so what should we do next? Well, we shouldn't stand\nshaded but previously we ran our last model model zero on if we go parameters. Do we run\nthis on the GPU or the CPU? On the CPU. So how about we try out our fashion MNIST model\nor V one running on the device that we just set up which should be CUDA. Wonderful. So\nwe can instantiate. So create an instance of model one. So we want model one or actually\nwe'll set up a manual seed here so that whenever we create a new instance of a model, it's\ngoing to be instantiated with random numbers. We don't necessarily have to set a random\nseed, but we do so anyway so that our values are quite similar on your end and my end input\nshape is going to be 784. Where does that come from? Well, that's because this is the\noutput of the flatten layer after our 28 by 28 image goes in. Then we're going to set\nup the hidden units. We're going to use the same number of hidden units as before, which\nis going to be 10. And then the output shape is what? We need one value, one output neuron\nfor each of our classes. So length of the class names. And then we're going to send\nthis to the target device so we can write send to the GPU if it's available. So now\nthat we've set up device agnostic code in the last video, we can just put two device\ninstead of hard coding that. And so if we check, so this was the output for model zero's device,\nlet's now check model one's device, model one parameters, and we can check where those\nparameters live by using the device attribute. Beautiful. So our model one is now living\non the GPU CUDA at index zero. Index zero means that it's on the first GPU that we have\navailable. We only have one GPU available. So it's on this Tesla T for GPU. Now, we've\ngot a couple more things to do. Now that we've created another model, we can recreate if\nwe go back to our workflow, we've just built a model here. What do we have to do after\nwe built a model? We have to instantiate a loss function and an optimizer. Now we've\ndone both of those things for model zero. So that's what we're going to do in the next\nvideo. But I'd like you to go ahead and try to create a loss function for our model and\noptimizer for model one. The hint is that they can be the exact same loss function and\noptimizer as model zero. So give that a shot and I'll see you in the next video. Welcome\nback. In the last video, we created another model. So we're continuing with our modeling\nexperiments. And the only difference here between fashion MNIST model V1 and V0 is that\nwe've added in nonlinear layers. Now we don't know for now we could think or guess whether\nthey would help improve our model. And with practice, you can start to understand how\ndifferent functions will influence your neural networks. But I prefer to, if in doubt, code\nit out, run lots of different experiments. So let's continue. We now have to create\na loss function, loss, optimizer, and evaluation metrics. So we've done this for model zero.\nSo we're not going to spend too much time explaining what's going on here. And we've\ndone this a fair few times now. So from helper functions, which is the script we downloaded\nbefore, we're going to import our accuracy function. And we're going to set up a loss\nfunction, which is we're working with multi class classification. So what loss function\ndo we typically use? And then dot cross entropy loss. And as our optimizer is going to be\ntorch dot opt in dot SGD. And we're going to optimize this time. I'll put in the params\nkeyword here, model one dot parameters. And the learning rate, we're just going to keep\nit the same as our previous model. And that's a thing to keep a note for your experiments.\nWhen you're running fair few experiments, you only really want to tweak a couple of things\nor maybe just one thing per experiment, that way you can really narrow down what actually\ninfluences your model and what improves it slash what doesn't improve it. And a little\npop quiz. What does a loss function do? This is going to measure how wrong our model is.\nAnd what does the optimizer do? Tries to update our models parameters to reduce the\nloss. So that's what these two functions are going to be doing. The accuracy function is\nof a course going to be measuring our models accuracy. We measure the accuracy because that's\none of the base classification metrics. So we'll run this. Now what's next? We're getting\nquite good at this. We've picked a loss function and an optimizer. Now we're going to build\na training loop. However, we spent quite a bit of time doing that in a previous video.\nIf we go up here, that was our vowel model function. Oh, that was helpful. We turned it\ninto a function. How about we do the same with these? Why don't we make a function for\nour training loop as well as our testing loop? So I think you can give this a go. We're going\nto make a function in the next video for training. We're going to call that train step. And\nwe'll create a function for testing called test step. Now they'll both have to take in\nsome parameters. I'll let you figure out what they are. But otherwise, we're going to code\nthat up together in the next video. So I'll see you there.\nSo we've got a loss function ready and an optimizer. What's our next step? Well, it's\nto create training and evaluation loops. So let's make a heading here. We're going to\ncall this functionizing training and evaluation or slash testing loops because we've written\nsimilar code quite often for training and evaluating slash testing our models. Now we're\ngoing to start moving towards functionizing code that we've written before because that's\nnot only a best practice, it helps reduce errors because if you're writing a training\nloop all the time, we may get it wrong. If we've got one that works for our particular\nproblem, hey, we might as well save that as a function so we can continually call that\nover and over and over again. So how about we, and this is going to be very rare that\nI'm going to allow you to do this is that is we're going to copy this training and you\nmight have already attempted to create this. That is the function called, let's create\na function for one training loop. And we're going to call this train step. And we're going\nto create a function for the testing loop. You're going to call this test step. Now these\nare just what I'm calling them. You can call them whatever you want. I just understand\nit quite easily by calling it train step. And then we can for each epoch in a range,\nwe call our training step. And then the same thing for each epoch in a range, we can call\na testing step. This will make a lot more sense once we've coded it out. So let's put\nthe training code here. To functionize this, let's start it off with train step. Now what\nparameters should our train step function take in? Well, let's think about this. We\nneed a model. We need a data loader. We need a loss function. And we need an optimizer.\nWe could also put in an accuracy function here if we wanted to. And potentially it's\nnot here, but we could put in what target device we'd like to compute on and make our\ncode device agnostic. So this is just the exact same code we went through before. We\nloop through a data loader. We do the forward pass. We calculate the loss. We accumulate\nit. We zero the optimizer. We perform backpropagation in respect to the loss with the parameters\nof the model. And then we step the optimizer to hopefully improve the parameters of our\nmodel to better predict the data that we're trying to predict. So let's craft a train\nstep function here. We'll take a model, which is going to be torch nn.module, type hint.\nAnd we're going to put in a data loader, which is going to be of type torch utils dot data\ndot data loader. Now we don't necessarily need to put this in these type hints, but\nthey're relatively new addition to Python. And so you might start to see them more and\nmore. And it also just helps people understand what your code is expecting. So the loss\nfunction, we're going to put in an optimizer torch dot opt in, which is a type optimizer.\nWe also want an accuracy function. We don't necessarily need this either. These are a\nlot of nice to habs. The first four are probably the most important. And then the device. So\ntorch is going to be torch dot device equals device. So we'll just hard code that to be\nour already set device parameter. And we'll just write in here, performs training step\nwith model, trying to learn on data loader. Nice and simple, we could make that more\nexplanatory if we wanted to, but we'll leave it at that for now. And so right at the start,\nwe're going to set up train loss and train act equals zero zero. We're going to introduce\naccuracy here. So we can get rid of this. Let's just go through this line by line. What\ndo we need to do here? Well, we've got four batch XY in enumerate train data loader. But\nwe're going to change that to data loader up here. So we can just change this to data\nloader. Wonderful. And now we've got model zero dot train. Do we want that? Well, no,\nbecause we're going to keep this model agnostic, we want to be able to use any model with this\nfunction. So let's get rid of this model dot train. We are missing one step here is\nput data on target device. And we could actually put this model dot train up here. Put model\ninto training mode. Now, this will be the default for the model. But just in case we're\ngoing to call it anyway, model dot train, put data on the target device. So we're going\nto go XY equals X dot two device, Y dot two device. Wonderful. And the forward pass, we\ndon't need to use model zero anymore. We're just going to use model that's up here. The\nloss function can stay the same because we're passing in a loss function up there. The train\nloss can be accumulated. That's fine. But we might also accumulate now the train accuracy,\nlimit loss, and accuracy per batch. So train act equals or plus equals our accuracy function\non Y true equals Y and Y pred equals Y pred. So the outputs here, Y pred, we need to take\nbecause the raw outputs, outputs, the raw logits from the model, because our accuracy\nfunction expects our predictions to be in the same format as our true values. We need\nto make sure that they are we can call the argmax here on the first dimension. This is\ngoing to go from logits to prediction labels. We can keep the optimizer zero grab the same\nbecause we're passing in an optimizer up here. We can keep the loss backwards because the\nloss is just calculated there. We can keep optimizer step. And we could print out what's\nhappening. But we might change this up a little bit. We need to divide the total train loss\nand accuracy. I just want to type in accuracy here because now we've added in accuracy metric\nact. So train act divided equals length train data loader. Oh, no, sorry. We can just use\nthe data loader here, data loader, data loader. And we're not going to print out per batch\nhere. I'm just going to get rid of this. We'll make at the end of this step, we will make\nour print out here, print. Notice how it's at the end of the step because we're outside\nthe for loop now. So we're going to here, we're accumulating the loss on the training\ndata set and the accuracy on the training data set per batch. And then we're finding\nout at the end of the training steps. So after it's been through all the batches in\nthe data loader, we're finding out what the average loss is per batch. And the average\naccuracy is per batch. And now we're going to go train loss is going to be the train\nloss on 0.5. And then we're going to go train act is going to be train act. And we're going\nto set that to 0.2 F. Get that there, percentage. Wonderful. So if all this works, we should\nbe able to call our train step function and pass it in a model, a data loader, a loss\nfunction, an optimizer, an accuracy function and a device. And it should automatically\ndo all of these steps. So we're going to find that out in a later video. In the next video,\nwe're going to do the same thing we've just done for the training loop with the test step.\nBut here's your challenge for this video is to go up to the testing loop code we wrote\nbefore and try to recreate the test step function in the same format that we've done here. So\ngive that a go. And I'll see you in the next video. Welcome back. In the last video, we\nfunctionalized our training loop. So now we can call this train step function. And instead\nof writing all this training loop code again, well, we can train our model through the art\nof a function. Now let's do the same for our testing loop. So I issued you the challenge\nin the last video to give it a go. I hope you did because that's the best way to practice\nPyTorch code is to write more pytorch code. Let's put in a model, which is going to be\ntorch and then dot module. And we're going to put in a data loader. Because we need a\nmodel and we need data, the data loader is going to be, of course, the test data load\nhere, torch dot utils dot data dot data loader. And then we're going to put in a loss function,\nwhich is going to be torch and end up module as well. Because we're going to use an end\nup cross entropy loss. We'll see that later on. We're going to put in an accuracy function.\nWe don't need an optimizer because we're not doing any optimization in the testing loop.\nWe're just evaluating. And the device can be torch dot device. And we're going to set\nthat as a default to the target device parameter. Beautiful. So we'll put a little doctoring\nhere. So performs a testing loop step on model going over data loader. Wonderful. So now\nlet's set up a test loss and a test accuracy, because we'll measure test loss and accuracy\nwithout testing loop function. And we're going to set the model into, I'll just put a comment\nhere, put the model in a vowel mode. So model dot a vowel, we don't have to use any underscore\nhere as in model zero, because we have a model coming in the top here. Now, what should we\ndo? Well, because we're performing a test step, we should turn on inference mode. So\nturn on inference mode, inference mode context manager. Remember, whenever you're performing\npredictions with your model, you should put it in model dot a vowel. And if you want as\nmany speedups as you can get, make sure the predictions are done within the inference\nmode. Because remember, inference is another word for predictions within the inference\nmode context manager. So we're going to loop through our data loader for X and Y in data\nloader. We don't have to specify that this is X test. For Y test, we could if we wanted\nto. But because we're in another function here, we can just go for X, Y in data loader,\nwe can do the forward pass. After we send the data to the target device, target device,\nso we're going to have X, Y equals X dot two device. And the same thing with Y, we're\njust doing best practice here, creating device agnostic code. Then what should we do? Well,\nwe should do the thing that I said before, which is the forward pass. Now that our data\nand model be on the same device, we can create a variable here test pred equals model, we're\ngoing to pass in X. And then what do we do? We can calculate the loss. So to calculate\nthe loss slash accuracy, we're going to accumulate it per batch. So we'll set up test loss equals\nloss function. Oh, plus equals loss function. We're going to pass it in test pred and Y,\nwhich is our truth label. And then the test act where you will accumulate as well, using\nour accuracy function, we'll pass in Y true equals Y. And then Y pred, what do we have\nto do to Y pred? Well, our test pred, we have to take the argmax to convert it from.\nSo this is going to outputs raw logits. Remember, a models raw output is referred to as logits.\nAnd then here, we have to go from logits to prediction labels. Beautiful. Oh, little typo\nhere. Did you catch that one? Tab, tab. Beautiful. Oh, look how good this function is looking.\nNow we're going to adjust the metrics. So adjust metrics and print out. You might notice\nthat we're outside of the batch loop here, right? So if we draw down from this line for\nand we write some code here, we're still within the context manager. This is important because\nif we want to adapt a value created inside the context manager, we have to modify it\nstill with inside that context manager, otherwise pytorch will throw an error. So try to write\nthis code if you want outside the context manager and see if it still works. So test loss, we're\ngoing to adjust it to find out the average test loss and test accuracy per batch across\na whole step. So we're going to go length data loader. Now we're going to print out\nwhat's happening. Print out what's happening. So test loss, which we put in here, well,\nwe're going to get the test loss. Let's get this to five decimal places. And then we're\ngoing to go test act. And we will get that to two decimal places. You could do this as\nmany decimal as you want. You could even times it by 100 to get it in proper accuracy format.\nAnd we'll put a new line on the end here. Wonderful. So now it looks like we've got functions.\nI haven't run this cell yet for a training step and a test step. So how do you think we\ncould replicate if we go back up to our training loop that we wrote before? How do you think\nwe could replicate the functionality of this, except this time using our functions? Well,\nwe could still use this for epoch and TQDM range epochs. But then we would just call\nour training step for this training code, our training step function. And we would call\nour testing step function, passing in the appropriate parameters for our testing loop.\nSo that's what we'll do in the next video. We will leverage our two functions, train\nstep and test step to train model one. But here's your challenge for this video. Give\nthat a go. So use our training step and test step function to train model one for three\nepochs and see how you go. But we'll do it together in the next video. Welcome back.\nHow'd you go? Did you create a training loop or a PyTorch optimization loop using our training\nstep function and a test step function? Were there any errors? In fact, I don't even know.\nBut how about we find out together? Hey, how do we combine these two functions to create\nan optimization loop? So I'm going to go torch dot manual seed 42. And I'm going to measure\nthe time of how long our training and test loop takes. This time we're using a different\nmodel. So this model uses nonlinearities and it's on the GPU. So that's the main thing\nwe want to compare is how long our model took on CPU versus GPU. So I'm going to import\nfrom time it, import default timer as timer. And I'm going to start the train time. Train\ntime start on GPU equals timer. And then I'm just right here, set epochs. I'm going to\nset epochs equal to three, because we want to keep our training experiments as close\nto the same as possible. So we can see what little changes do what. And then it's create\na optimization and evaluation loop using train step and test step. So we're going to loop\nthrough the epochs for epoch in TQDM. So we get a nice progress bar in epochs. Then we're\ngoing to print epoch. A little print out of what's going on. Epoch. And we'll get a new\nline. And then maybe one, two, three, four, five, six, seven, eight or something like\nthat. Maybe I'm miscounted there. But that's all right. Train step. What do we have to\ndo for this? Now we have a little doc string. We have a model. What model would we like\nto use? We'd like to use model one. We have a data loader. What data loader would we\nlike to use? Well, we'd like to use our train data loader. We also have a loss function,\nwhich is our loss function. We have an optimizer, which is our optimizer. And we have an accuracy\nfunction, which is our accuracy function. And oops, forgot to put FM. And finally, we have\na device, which equals device, but we're going to set that anyway. So how beautiful is that\nfor creating a training loop? Thanks to the code that we've functionalized before. And\njust recall, we set our optimizer and loss function in a previous video. You could bring\nthese down here if you really wanted to, so that they're all in one place, either way\nup. But we can just get rid of that because we've already set it. Now we're going to do\nthe same thing for our test step. So what do we need here? Let's check the doc string.\nWe could put a little bit more information in this doc string if we wanted to to really\nmake our code more reusable, and so that if someone else was to use our code, or even\nus in the future knows what's going on. But let's just code it out because we're just\nstill fresh in our minds. Model equals model one. What's our data loader going to be for\nthe test step? It's going to be our test data loader. Then we're going to set in a loss\nfunction, which is going to be just the same loss function. We don't need to use an optimizer\nhere because we are only evaluating our model, but we can pass in our accuracy function.\nAccuracy function. And then finally, the device is already set, but we can just pass\nit in anyway. Look at that. Our whole optimization loop in a few lines of code. Isn't that beautiful?\nSo these functions are something that you could put in, like our helper functions dot\npi. And that way you could just import it later on. And you don't have to write your\ntraining loops all over again. But we'll see a more of an example of that later on in\nthe course. So let's keep going. We want to measure the train time, right? So we're\ngoing to create, once it's been through these steps, we're going to create train time end\non CPU. And then we're going to set that to the timer. So all this is going to do is\nmeasure at value in time, once this line of code is run, it's going to run all of these\nlines of code. So it's going to perform the training and optimization loop. And then it's\ngoing to, oh, excuse me, this should be GPU. It's going to measure a point in time here.\nSo once all this codes run, measure a point in time there. And then finally, we can go\ntotal train time for model one is equal to print train time, which is our function that\nwe wrote before. And we pass it in a start time. And it prints the difference between\nthe start and end time on a target device. So let's do that. Start equals what? Train\ntime start on GPU. The end is going to be train time end on GPU. And the device is going\nto be device. Beautiful. So are you ready to run our next modeling experiment model one?\nWe've got a model running on the GPU, and it's using nonlinear layers. And we want to\ncompare it to our first model, which our results were model zero results. And we have total\ntrain time on model zero. Yes, we do. So this is what we're going for. Does our model\none beat these results? And does it beat this result here? So three, two, one, do we\nhave any errors? No, we don't. Okay. Train step got an unexpected keyword loss. Oh, did\nyou catch that? I didn't type in loss function. Let's run it again. There we go. Okay, we're\nrunning. We've got a progress bar. It's going to output at the end of each epoch. There\nwe go. Training loss. All right. Test accuracy, training accuracy. This is so exciting. I\nlove watching neural networks train. Okay, we're improving per epoch. That's a good sign.\nBut we've still got a fair way to go. Oh, okay. So what do we have here? Well, we didn't\nbeat our, hmm, it looks like we didn't beat our model zero results with the nonlinear\nlayers. And we only just slightly had a faster training time. Now, again, your numbers might\nnot be the exact same as what I've got here. Right? So that's a big thing about machine\nlearning is that it uses randomness. So your numbers might be slightly different. The direction\nshould be quite similar. And we may be using different GPUs. So just keep that in mind.\nRight now I'm using a new video, SMI. I'm using a Tesla T4, which is at the time of\nrecording this video, Wednesday, April 20, 2022 is a relatively fast GPU for making\ninference. So just keep that in mind. Your GPU in the future may be different. And your\nCPU that you run may also have a different time here. So if these numbers are like 10\ntimes higher, you might want to look into seeing if your code is there's some error.\nIf they're 10 times lower, well, hey, you're running it on some fast hardware. So it looks\nlike my code is running on CUDA slightly faster than the CPU, but not dramatically faster.\nAnd that's probably akin to the fact that our data set isn't too complex and our model\nisn't too large. What I mean by that is our model doesn't have like a vast amount of\nlayers. And our data set is only comprised of like, this is the layers our model has.\nAnd our data set is only comprised of 60,000 images that are 28 by 28. So as you can imagine,\nthe more parameters in your model, the more features in your data, the higher this time\nis going to be. And you might sometimes even find that your model is faster on CPU. So\nthis is the train time on CPU. You might sometimes find that your model's training\ntime on a CPU is in fact faster for the exact same code running on a GPU. Now, why might\nthat be? Well, let's write down this here. Let's go note. Sometimes, depending on your\ndata slash hardware, you might find that your model trains faster on CPU than GPU. Now,\nwhy is this? So one of the number one reasons is that one, it could be that the overhead\nfor copying data slash model to and from the GPU outweighs the compute benefits offered\nby the GPU. So that's probably one of the number one reasons is that you have to, for\ndata to be processed on a GPU, you have to copy it because it is by default on the CPU.\nIf you have to copy it to that GPU, you have some overhead time for doing that copy into\nthe GPU memory. And then although the GPU will probably compute faster on that data\nonce it's there, you still have that back and forth of going between the CPU and the\nGPU. And the number two reason is that the hardware you're using has a better CPU in\nterms of compute capability than the GPU. Now, this is quite a bit rarer. Usually if\nyou're using a GPU like a fairly modern GPU, it will be faster at computing, deep learning\nor running deep learning algorithms than your general CPU. But sometimes these numbers\nof compute time are really dependent on the hardware that you're running. So you'll get\nthe biggest benefits of speedups on the GPU when you're running larger models, larger\ndata sets, and more compute intensive layers in your neural networks. And so if you'd like\na great article on how to get the most out of your GPUs, it's a little bit technical,\nbut this is something to keep in mind as you progress as a machine learning engineer is\nhow to make your GPUs go burr. And I mean that burr from first principles. There we\ngo. Making deep learning go burr as in your GPU is going burr because it's running so\nfast from first principles. So this is by Horace He who works on PyTorch. And it's\ngreat. It talks about compute as a first principle. So here's what I mean by copying\nmemory and compute. There might be a fair few things you're not familiar with here,\nbut that's okay. But just be aware bandwidth. So bandwidth costs are essentially the cost\npaid to move data from one place to another. That's what I was talking about copying stuff\nfrom the CPU to the GPU. And then also there's one more, where is it overhead? Overhead is\nbasically everything else. I called it overhead. There are different terms for different things.\nThis article is excellent. So I'm going to just copy this in here. And you'll find this\nin the resources, by the way. So for more on how to make your models compute faster,\nsee here. Lovely. So right now our baseline model is performing the best in terms of results.\nAnd in terms of, or actually our model computing on the GPU is performing faster than our CPU.\nAgain yours might be slightly different. For my case, for my particular hardware, CUDA\nis faster. Except model zero, our baseline is better than model one. So what's to do\nnext? Well, it's to keep experimenting, of course. I'll see you in the next video. Welcome\nback. Now, before we move on to the next modeling experiment, let's get a results dictionary\nfor our model one, a model that we trained on. So just like we've got one for model zero,\nlet's create one of these for model one results. And we can create that without a vowel model\nfunction. So we'll go right back down to where we were. I'll just get rid of this cell.\nAnd let's type in here, get model one results dictionary. This is helpful. So later on,\nwe can compare all of our modeling results, because they'll all be in dictionary format.\nSo we're going to model one results equals a vowel model on a model equals model one.\nAnd we can pass in a data loader, which is going to be our test data loader. Then we\ncan pass in a loss function, which is going to equal our loss function. And we can pass\nin our accuracy function equals accuracy function. Wonderful. And then if we check out our model\none results, what do we get? Oh, no, we get an error. Do we get the code right? That looks\nright to me. Oh, what does this say runtime error expected all tensors to be on the same\ndevice, but found at least two devices, CUDA and CPU. Of course. So why did this happen?\nWell, let's go back up to our of our model function, wherever we defined that. Here we\ngo. Ah, I see. So this is a little gotcha in pytorch or in deep learning in general. There's\na saying in the industry that deep learning models fail silently. And this is kind of\none of those ones. It's because our data and our model are on different devices. So remember\nhow I said the three big errors are shape mismatches with your data and your model device\nmismatches, which is what we've got so far. And then data type mismatches, which is if\nyour data is in the wrong data type to be computed on. So what we're going to have to\ndo to fix this is let's bring down our vowel model function down to where we were. And\njust like we've done in our test step and train step functions, where we've created\ndevice agnostic data here, we've sent our data to the target device, we'll do that exact\nsame thing in our vowel model function. And this is just a note for going forward. It's\nalways handy to where you can create device agnostic code. So we've got our new of our\nmodel function here for x, y in our data loader. Let's make our data device agnostic. So just\nlike our model is device agnostic, we've sent it to the target device, we will do the same\nhere, x dot two device, and then y dot two device. Let's see if that works. We will\njust rerun this cell up here. I'll grab this, we're just going to write the exact same\ncode as what we did before. But now it should work because we've sent our, we could actually\nalso just pass in the target device here, device equals device. That way we can pass\nin whatever device we want to run it on. And we're going to just add in device here,\ndevice equals device. And let's see if this runs correctly. Beautiful. So if we compare\nthis to our model zero results, it looks like our baseline's still out in front. But that's\nokay. We're going to in the next video, start to step things up a notch and move on to convolutional\nneural networks. This is very exciting. And by the way, just remember, if your numbers\nhere aren't exactly the same as mine, don't worry too much. If they're out landishly different,\njust go back through your code and see if it's maybe a cell hasn't been run correctly\nor something like that. If there are a few decimal places off, that's okay. That's due\nto the inherent randomness of machine learning and deep learning. But with that being said,\nI'll see you in the next video. Let's get our hands on convolutional neural networks.\nWelcome back. In the last video, we saw that our second modeling experiment, model one,\ndidn't quite beat our baseline. But now we're going to keep going with modeling experiments.\nAnd we're going to move on to model two. And this is very exciting. We're going to build\na convolutional neural network, which are also known as CNN. CNNs are also known as\ncom net. And CNNs are known for their capabilities to find patterns in visual data. So what are\nwe going to do? Well, let's jump back into the keynote. We had a look at this slide before\nwhere this is the typical architecture of a CNN. There's a fair bit going on here, but\nwe're going to step through it one by one. We have an input layer, just like any other\ndeep learning model. We have to input some kind of data. We have a bunch of hidden layers\nin our case in a convolutional neural network, you have convolutional layers. You often have\nhidden activations or nonlinear activation layers. You might have a pooling layer. You\ngenerally always have an output layer of some sort, which is usually a linear layer. And\nso the values for each of these different layers will depend on the problem you're working\non. So we're going to work towards building something like this. And you'll notice that\na lot of the code is quite similar to the code that we've been writing before for other\nPyTorch models. The only difference is in here is that we're going to use different\nlayer types. And so if we want to visualize a CNN in a colored block edition, we're going\nto code this out in a minute. So don't worry too much. We have a simple CNN. You might\nhave an input, which could be this image of my dad eating some pizza with two thumbs\nup. We're going to preprocess that input. We're going to, in other words, turn it into\na tensor in red, green and blue for an image. And then we're going to pass it through a\ncombination of convolutional layers, relu layers and pooling layers. Now again, this\nis a thing to note about deep learning models. I don't want you to get too bogged down in\nthe order of how these layers go, because they can be combined in many different ways.\nIn fact, research is coming out almost every day, every week about how to best construct\nthese layers. The overall principle is what's more important is how do you get your inputs\ninto an idolized output? That's the fun part. And then of course, we have the linear output\nlayer, which is going to output however many classes or value for however many classes\nthat we have in the case of classification. And then if you want to make your CNN deeper,\nthis is where the deep comes from deep learning, you can add more layers. So the theory behind\nthis, or the practice behind this, is that the more layers you add to your deep learning\nmodel, the more chances it has to find patterns in the data. Now, how does it find these patterns?\nWell, each one of these layers here is going to perform, just like what we've seen before,\na different combination of mathematical operations on whatever data we feed it. And each subsequent\nlayer receives its input from the previous layer. In this case, there are some advanced\nnetworks that you'll probably come across later in your research and machine learning\ncareer that use inputs from layers that are kind of over here or the way down here or\nsomething like that. They're known as residual connections. But that's beyond the scope of\nwhat we're covering for now. We just want to build our first convolutional neural network.\nAnd so let's go back to Google Chrome. I'm going to show you my favorite website to learn\nabout convolutional neural networks. It is the CNN explainer website. And this is going\nto be part of your extra curriculum for this video is to spend 20 minutes clicking and\ngoing through this entire website. We're not going to do that together because I would\nlike you to explore it yourself. That is the best way to learn. So what you'll notice up\nhere is we have some images of some different sort. And this is going to be our input. So\nlet's start with pizza. And then we have a convolutional layer, a relu layer, a conv\nlayer, a relu layer, max pool layer, com to relu to com to relu to max pool to this\narchitecture is a convolutional neural network. And it's running live in the browser. And\nso we pass this image, you'll notice that it breaks down into red, green and blue. And\nthen it goes through each of these layers and something happens. And then finally, we\nhave an output. And you notice that the output has 10 different classes here, because we\nhave one, two, three, four, five, six, seven, eight, nine, 10, different classes of image\nin this demo here. And of course, we could change this if we had 100 classes, we might\nchange this to 100. But the pieces of the puzzle here would still stay quite the same.\nAnd you'll notice that the class pizza has the highest output value here, because our\nimages of pizza, if we change to what is this one, espresso, it's got the highest\nvalue there. So this is a pretty well performing convolutional neural network. Then we have\na sport car. Now, if we clicked on each one of these, something is going to happen. Let's\nfind out. We have a convolutional layer. So we have an input of an image here that 64\n64 by three. This is color channels last format. So we have a kernel. And this kernel, this\nis what happens inside a convolutional layer. And you might be going, well, there's a lot\ngoing on here. And yes, of course, there is if this is the first time you ever seen this.\nBut essentially, what's happening is a kernel, which is also known as a filter, is going\nover our image pixel values, because of course, they will be in the format of a tensor. And\ntrying to find small little intricate patterns in that data. So if we have a look here, and\nthis is why it's so valuable to go through this and just play around with it, we start\nin a top left corner, and then slowly move along, you'll see on the output on the right\nhand side, we have another little square. And do you notice in the middle all of those\nnumbers changing? Well, that is the mathematical operation that's happening as a convolutional\nlayer convolves over our input image. How cool is that? And you might be able to see on the\noutput there that there's some slight values for like, look around the headlight here. Do\nyou notice on the right how there's some activation? There's some red tiles there? Well, that\njust means that potentially this layer or this hidden unit, and I want to zoom out for\na second, is we have 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 hidden units. Each one of these is\ngoing to learn a different feature about the data. And now the beauty of deep learning,\nbut also one of the curses of deep learning is that we don't actually control what each\none of these learns. The magic of deep learning is that it figures it out itself what is\nbest to learn. We go into here, notice that each one we click on has a different representation\non the right hand side. And so this is what's going to happen layer by layer as it goes\nthrough the convolutional neural network. And so if you want to read about what is a convolutional\nneural network, you can go through here. But we're going to replicate this exact neural\nnetwork here with PyTorch code. That's how I'd prefer to learn it. But if you want the\nintuition behind it, the math behind it, you can check out all of these resources here.\nThat is your extra curriculum for this video. So we have an input layer, we have a convolutional\nlayer, you can see how the input gets modified by some sort of mathematical operation, which\nis of course, the convolutional operation. And we have there all different numbers finding\ndifferent patterns and data. This is a really good example here. You notice that the outputs\neyes slightly changes, that'll be a trend throughout each layer. And then we can understand\nthe different hyper parameters, but I'm going to leave this for you to explore on your own.\nIn the next video, we're going to start to write PyTorch code to replicate everything\nthat's going on here. So I'm going to link this in here to find out what's happening\ninside CNN. See this website here. So join me in the next video. This is super exciting.\nWe're going to build our first convolutional neural network for computer vision. I'll see\nyou there. Welcome back. In the last video, we went briefly through the CNN explainer\nwebsite, which is my favorite resource for learning about convolutional neural networks.\nAnd of course, we could spend 20 minutes clicking through everything here to find out what's\ngoing on with a convolutional neural network, or we could start to code one up. So how about\nwe do that? Hey, if and down, code it out. So we're going to create a convolutional neural\nnetwork. And what I'm going to do is I'm going to build this, or we're going to build this\nmodel together in this video. And then because it's going to use layers or PyTorch layers\nthat we haven't looked at before, we're going to spend the next couple of videos stepping\nthrough those layers. So just bear with me, as we code this entire model together, we'll\ngo break it down in subsequent videos. So let's build our first convolutional neural\nnetwork. That's a mouthful, by the way, I'm just going to probably stick to saying CNN.\nFashion MNIST, we're up to model V2. We're going to subclass nn.module, as we always do\nwhen we're building a PyTorch model. And in here, we're going to say model architecture\nthat replicates the tiny VGG. And you might be thinking, where did you get that from, Daniel?\nModel from CNN explainer website. And so oftentimes, when convolutional neural networks or new\ntypes of architecture come out, the authors of the research paper that present the model\nget to name the model. And so that way, in the future, you can refer to different types of\nmodel architectures with just a simple name, like tiny VGG. And people kind of know what's going on.\nSo I believe somewhere on here, it's called tiny VGG, tiny VGG. We have nothing. Yeah,\nthere we go. In tiny VGG. And do we have more than one tiny, tiny, yeah, tiny VGG. And if we\nlook up VGG, conv net, VGG 16 was one of the original ones, VGG, very deep convolutional neural\nnetworks of VGG net. There's also ResNet, which is another convolutional neural network.\nYou can also, I don't want to give you my location, Google, you can go popular CNN\narchitectures. And this will give you a fair few options. Lynette is one of the first AlexNet,\nZF net, whole bunch of different resources. And also, how could you find out more about a\nconvolutional neural network? What is a convolutional neural network? You can go through that. But\nlet's stop that for a moment. Let's code this one up together. So we're going to initialize our\nclass here, def init. We're going to pass it in an input shape, just like we often do.\nWe're going to put in a number of hidden units, which is an int. And we're going to put in an\noutput shape, which is an int. Wonderful. So nothing to outlandish that we haven't seen before there.\nAnd we're going to go super dot init to initialize our initializer for lack of a better way of\nputting it. Now, we're going to create our neural network in a couple of blocks this time. And\nyou might often hear in when you learn more about convolutional neural networks, or I'll just tell\nyou that things are referred to are often referred to as convolutional blocks. So if we go back to\nour keynote, this here, this combination of layers might be referred to as a convolutional block.\nAnd a convolutional block, a deeper CNN, might be comprised of multiple convolutional blocks.\nSo to add to the confusion, a block is comprised of multiple layers. And then an overall architecture\nis comprised of multiple blocks. And so the deeper and deeper your models get, the more blocks\nit might be comprised of, and the more layers those blocks may be comprised of within them.\nSo it's kind of like Lego, which is very fun. So let's put together an an ensequential.\nNow, the first few layers here that we're going to create in conv block one, uh,\nnn.com 2d. Oh, look at that. Us writing us our first CNN layer. And we have to define something\nhere, which is in channels. So this channels refers to the number of channels in your visual data.\nAnd we're going to put in input shape. So we're defining the input shape. This is going to be\nthe first layer in our model. The input shape is going to be what we define when we instantiate\nthis class. And then the out channels. Oh, what's the out channels going to be? Well, it's going\nto be hidden units, just like we've done with our previous models. Now the difference here\nis that in nn.com 2d, we have a number of different hyper parameters that we can set.\nI'm going to set some pretty quickly here, but then we're going to step back through them,\nnot only in this video, but in subsequent videos. We've got a fair bit going on here.\nWe've got in channels, which is our input shape. We've got out channels, which are our hidden units.\nWe've got a kernel size, which equals three. Or this could be a tuple as well, three by three.\nBut I just like to keep it as three. We've got a stride and we've got padding. Now,\nbecause these are values, we can set ourselves. What are they referred to as?\nLet's write this down. Values, we can set ourselves in our neural networks.\nIn our nn's neural networks are called hyper parameters. So these are the hyper parameters\nof nn.com 2d. And you might be thinking, what is 2d for? Well, because we're working with\ntwo-dimensional data, our images have height and width. There's also com 1d for one-dimensional data,\n3d for three-dimensional data. We're going to stick with 2d for now.\nAnd so what do each of these hyper parameters do? Well, before we go through what each one of them\ndo, we're going to do that when we step by step through this particular layer. What we've just done\nis we've replicated this particular layer of the CNN explainer website. We've still got the\nrelu. We've still got another conv and a relu and a max pool and a conv and a relu and a\nconv and a relu and a max pool. But this is the block I was talking about. This is one block here\nof this neural network, or at least that's how I've broken it down. And this is another block.\nYou might notice that they're comprised of the same layers just stacked on top of each other.\nAnd then we're going to have an output layer. And if you want to learn about where the hyper\nparameters came from, what we just coded, where could you learn about those? Well, one, you could\ngo, of course, to the PyTorch documentation, PyTorch, and then com 2d. You can read about it there.\nThere's the mathematical operation that we talked about or briefly stepped on before,\nor touched on, stepped on. Is that the right word? So create a conv layer. It's there.\nBut also this is why I showed you this beautiful website so that you can read about these\nhyper parameters down here. Understanding hyper parameters. So your extra curriculum for this\nvideo is to go through this little graphic here and see if you can find out what padding means,\nwhat the kernel size means, and what the stride means. I'm not going to read through this for you.\nYou can have a look at this interactive plot. We're going to keep coding because that's what\nwe're all about here. If and out, code it out. So we're going to now add a relu layer.\nAnd then after that, we're going to add another conv 2d layer. And the in channels here is going\nto be the hidden units, because we're going to take the output size of this layer and use it as\nthe input size to this layer. We're going to keep going here. Out channels equals hidden units again\nin this case. And then the kernel size is going to be three as well. Stride will be one. Padding\nwill be one. Now, of course, we can change all of these values later on, but just bear with me\nwhile we set them how they are. We'll have another relu layer. And then we're going to finish off\nwith a nn max pool 2d layer. Again, the 2d comes from the same reason we use comf2d. We're working\nwith 2d data here. And we're going to set the kernel size here to be equal to two. And of course,\nthis can be a tuple as well. So it can be two two. Now, where could you find out about nn max\npool 2d? Well, we go nn max pool 2d. What does this do? applies a 2d max pooling over an input\nsignal composed of several input planes. So it's taking the max of an input. And we've got some\nparameters here, kernel size, the size of the window to take the max over. Now, where have we\nseen a window before? I'm just going to close these. We come back up. Where did we see a window?\nLet's dive into the max pool layer. See where my mouse is? Do you see that two by two? Well,\nthat's a window. Now, look at the difference between the input and the output. What's happening?\nWell, we have a tile that's two by two, a window of four. And the max, we're taking the max of that\ntile. In this case, it's zero. Let's find the actual value. There we go. So if you look at those\nfour numbers in the middle inside the max brackets, we have 0.07, 0.09, 0.06, 0.05. And the max of\nall those is 0.09. And you'll notice that the input and the output shapes are different. The\noutput is half the size of the input. So that's what max pooling does, is it tries to take the max\nvalue of whatever its input is, and then outputs it on the right here. And so as our data,\nthis is a trend in all of deep learning, actually. As our image moves through, this is what you'll\nnotice. Notice all the different shapes here. Even if you don't completely understand what's going\non here, you'll notice that the two values here on the left start to get smaller and smaller as\nthey go through the model. And what our model is trying to do here is take the input and learn a\ncompressed representation through each of these layers. So it's going to smoosh and smoosh and\nsmoosh trying to find the most generalizable patterns to get to the ideal output. And that\ninput is eventually going to be a feature vector to our final layer. So a lot going on there,\nbut let's keep coding. What we've just completed is this first block. We've got a cons layer,\na relu layer, a cons layer, a relu layer, and a max pool layer. Look at that, cons layer,\nrelu layer, cons layer, relu layer, max pool. Should we move on to the next block? We can do this\none a bit faster now because we've already coded the first one. So I'm going to do nn.sequential as\nwell. And then we're going to go nn.com2d. We're going to set the in channels. What should the\nin channels be here? Well, we're going to set it to hidden units as well because our network is\ngoing to flow just straight through all of these layers. And the output size of this is going to\nbe hidden units. And so we want the in channels to match up with the previous layers out channels.\nSo then we're going to go out channels equals hidden units as well. We're going to set the\nkernel size, kernel size equals three, stride equals one, padding equals one, then what comes\nnext? Well, because the two blocks are identical, the con block one and com two, we can just go\nthe exact same combination of layers. And then relu and n.com2d in channels equals hidden units.\nOut channels equals, you might already know this, hidden units. Then we have kernel size\nequals three, oh, 32, don't want it that big, stride equals one, padding equals one,\nand what comes next? Well, we have another relu layer, relu, and then what comes after that?\nWe have another max pool. And then max pool 2d, kernel size equals two, beautiful. Now,\nwhat have we coded up so far? We've got this block, number one, that's what this one on the inside\nhere. And then we have com two, relu two, com two, relu two, max pool two. So we've built these\ntwo blocks. Now, what do we need to do? Well, we need an output layer. And so what did we do before\nwhen we made model one? We flattened the inputs of the final layer before we put them to the last\nlinear layer. So flatten. So this is going to be the same kind of setup as our classifier layer.\nNow, I say that on purpose, because that's what you'll generally hear the last output layer\nin a classification model called is a classifier layer. So we're going to have these two layers\nare going to be feature extractors. In other words, they're trying to learn the patterns that\nbest represent our data. And this final layer is going to take those features and classify them\ninto our target classes. Whatever our model thinks best suits those features, or whatever our model\nthinks those features that it learned represents in terms of our classes. So let's code it out.\nWe'll go down here. Let's build our classifier layer. This is our biggest neural network yet.\nYou should be very proud. We have an end of sequential again. And we're going to pass in\nan end of flatten, because the output of these two blocks is going to be a multi-dimensional tensor,\nsomething similar to this size 131310. So we want to flatten the outputs into a single feature\nvector. And then we want to pass that feature vector to an nn.linear layer. And we're going to\ngo in features equals hidden units times something times something. Now, the reason I do this is\nbecause we're going to find something out later on, or time zero, just so it doesn't error. But\nsometimes calculating what you're in features needs to be is quite tricky. And I'm going to\nshow you a trick that I use later on to figure it out. And then we have out features relates\nto our output shape, which will be the length of how many classes we have, right? One value for\neach class that we have. And so with that being said, let's now that we've defined all of the\ncomponents of our tiny VGG architecture. There is a lot going on, but this is the same methodology\nwe've been using the whole time, defining some components, and then putting them together to\ncompute in some way in a forward method. So forward self X. How are we going to do this?\nAre we going to set X is equal to self, comp block one X. So X is going to go through comp block one,\nit's going to go through the comp 2D layer, relu layer, comp 2D layer, relu layer, max pool layer,\nwhich will be the equivalent of an image going through this layer, this layer, this layer,\nthis layer, this layer, and then ending up here. So we'll set it to that. And then we can print out\nX dot shape to get its shape. We'll check this later on. Then we pass X through comp block two,\nwhich is just going to go through all of the layers in this block, which is equivalent to\nthe output of this layer going through all of these layers. And then because we've constructed a\nclassifier layer, we're going to take the output of this block, which is going to be here, and we're\ngoing to pass it through our output layer, or what we've termed it, our classifier layer. I'll just\nprint out X dot shape here, so we can track the shape as our model moves through the architecture.\nX equals self dot classifier X. And then we're going to return X. Look at us go. We just built\nour first convolutional neural network by replicating what's on a CNN explainer website.\nNow, that is actually very common practice in machine learning is to find some sort of architecture\nthat someone has found to work on some sort of problem and replicate it with code and see if it\nworks on your own problem. You'll see this quite often. And so now let's instantiate a model.\nGo torch dot manual C. We're going to instantiate our first convolutional neural network.\nModel two equals fashion amnest. We will go model V two. And we are going to set the input shape.\nNow, what will the input shape be? Well, I'll come to the layer up here. The input shape\nis the number of channels in our images. So do we have an image ready to go image shape?\nThis is the number of color channels in our image. We have one. If we had color images,\nwe would set the input shape to three. So the difference between our convolutional neural network,\nour CNN, tiny VGG, and the CNN explainer tiny VGG is that they are using color images. So\ntheir input is three here. So one for each color channel, red, green and blue. Whereas we have\nblack and white images. So we have only one color channel. So we set the input shape to one.\nAnd then we're going to go hidden units equals 10, which is exactly the same as what tiny VGG\nhas used. 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. So that sets the hidden units value of each of our\nlayers. That's the power of creating an initializer with hidden units. And then finally, our output\nshape is going to be what we've seen this before. This is going to be the length of our class names,\none value for each class in our data set. And of course, we're going to send this model to the\ndevice. We're going to hit shift and enter. Oh, no, what did we get wrong? Out channels,\noutput shape. Where did I spell wrong? Out channels, out channels, out channels. I forgot an L.\nOf course, typo. Oh, kernel size and other typo. Did you notice that?\nKernel size, kernel size, kernel size, kernel size. Where did we spell this wrong? Oh, here.\nKernel size. Are there any other typos? Probably.\nA beautiful. There we go. Okay, what have we got? Initializing zero, obtenses and non-op.\nOh, so we've got an issue here and error here because I've got this. But this is just to,\nthere's a trick to calculating this. We're going to cover this in another video. But\npay yourself on the back. We've written a fair bit of code here. This is a convolutional neural\nnetwork that replicates the tiny VGG architecture on the CNN explainer website. Now, don't forget,\nyour extra curriculum is to go through this website for at least 20 minutes and read about\nwhat's happening in our models. We're focused on code here. But this is particularly where you\nwant to pay attention to. If you read through this understanding hyper parameters and play around\nwith this, the next couple of videos will make a lot more sense. So read about padding,\nread about kernel size and read about stride. I'll see you in the next video. We're going to go\nthrough our network step by step. Welcome back. Now, I'm super stoked because in the last video,\nwe coded together our first ever convolutional neural network in PyTorch. So well done. We\nreplicated the tiny VGG architecture from the CNN explainer website, my favorite place for learning\nabout CNNs in the browser. So now we introduced two new layers that we haven't seen before,\nconv2d and maxpool2d. But they all have the same sort of premise of what we've been doing so far\nis that they're trying to learn the best features to represent our data in some way, shape or form.\nNow, in the case of maxpool2d, it doesn't actually have any learnable parameters. It just takes\nthe max, but we're going to step through that later on. Let's use this video to step through\nand then conv2d. We're going to do that with code. So I'll make a new heading here. 7.1\nstepping through and then conv2d. Beautiful. Now, where could we find out what's going on\nin an end comp2d? Well, of course, we have the documentation and then comp2d. We've got PyTorch.\nSo if you want to learn the mathematical operation that's happening, we have this value here, this\noperation here. Essentially, it's saying the output is equal to the bias term times something,\nplus the sum of the weight times something times the input. So do you see how just the weight\nmatrix, the weight tensor and the bias value, manipulating our input in some way equals the output?\nNow, if we map this, we've got batch size, channels in, height, width, channels out, out, out,\net cetera, et cetera. But we're not going to focus too much on this. If you'd like to\nread more into that, you can. Let's try it with code. And we're going to reproduce this particular\nlayer here, the first layer of the CNN explainer website. And we're going to do it with a dummy input.\nIn fact, that's one of my favorite ways to test things. So I'm just going to link here the documentation.\nSee the documentation for an end comp2d here. And if you'd like to read through more of this,\nof course, this is a beautiful place to learn about what's going on.\nThere's the shape how to calculate the shape, height out, width out, et cetera. That's very\nhelpful if you need to calculate input and output shapes. But I'll show you my trick for doing so\nlater on. We have here, let's create some dummy data. So I'm going to set torch manual seed. We\nneed it to be the same size as our CNN explainer data. So 64, 64, 3. But we're going to do it\npie torch style. This is color channels last. We're going to do color channels first. So how\nabout we create a batch of images, we're going to be writing torch dot rand n. And we're going to\npass in size equals 32, three, 64, 64. And then we're going to create a singular image by taking\nthe first of that. So image is zero. Now, let's get the image batch shape. Because a lot of\nmachine learning, as I've said before, and deep learning is making sure your data has the right\nshape. So let's check images dot shape. And let's check single image shape. We're going to go test\nimage dot shape. And finally, we're going to print, what does the test image look like?\nWe'll get this on a new line, hey, new line test image, this is of course not going to be an\nactual image is just going to be a collection of random numbers. And of course, that is what\nour model is currently comprised of model two, if we have a look at what's on the insides,\nwe are going to see a whole bunch of random numbers. Look at all this. What do we have?\nWe scroll up is going to give us a name for something. We have comp block two, two, we have a weight,\nwe have a bias, keep going up, we go right to the top, we have another weight, keep going down,\nwe have a bias, a weight, et cetera, et cetera. Now, our model is comprised of random numbers,\nand what we are trying to do is just like all of our other models is pass data in and adjust the\nrandom numbers within these layers to best represent our data. So let's see what happens\nif we pass some random data through one of our comp2d layers. So let's go here, we're going to\ncreate a single comp2d layer. So comp layer equals, what is it equal? And then comp2d,\nand we're going to set the in channels is equal to what? Oh, revealed the answer too quickly.\nThree. Why is it three? Well, it's because the in channels is the same number of color channels\nas our images. So if we have a look at our test image shape, what do we have? Three, it has three\ncolor channels. That is the same as the value here, except the order is reversed. This is color\nchannels last, pytorch defaults to color channels first. So, or for now it does, in the future this\nmay change. So just keep that in mind. So out channels equals 10. This is equivalent to the\nnumber of hidden units we have. One. Oh, I don't want that one just yet. One, two, three, four,\nfive, six, seven, eight, nine, 10. So we have that 10 there. So we have 10 there. And then we have\nkernel size. Oh, what is the kernel? Well, it's not KFC. I can tell you that. And then we have\nstride. And then we have padding. We're going to step through these in a second. But let's check\nout the kernel. And this kernel can also be three by three. But it's a shortcut to just type in three.\nSo that's what it actually means. If you just type in a single number, it's equivalent to typing in\na tuple. Now, of course, you could find that out by reading through the documentation here.\nBut where did I get that value? Well, let's dive into this beautiful website. And let's see what\nhappening. So we have a kernel here, which is also called a filter. So the thing I'm talking about\nis this little square here, this kernel. Oh, we can see the weights there at the top. This is how\nbeautiful this website is. So if we go over there, this is what's going to happen. This is a convolution.\nIt starts with this little square, and it moves pixel by pixel across our image. And you'll notice\nthat the output is creating some sort of number there. And you'll notice in the middle, we have a\nmathematical operation. This operation here is what's happening here. I wait times the input.\nThat's what we've got there. Now, the beauty of PyTorch is it does all of this behind the\nscenes for us. So again, if you'd like to dig more into the mathematical operation behind the\nscenes, you've got the resource here. And you've also got plenty of other resources online. We're\ngoing to focus on code for now. So if we keep doing this across our entire image, we get this\noutput over here. So that's the kernel. And now where did I get three by three from? Well, look at\nthis. One, two, three, one, two, three, one, two, three, three by three, we have nine squares. Now,\nif we scroll down, this was your extracurricular for the last video, understanding hyperparameters.\nWhat happens if we change the kernel size to three by three? Have a look at the red square on the\nleft. Now, if we change it to two by two, it changed again. Three by three. This is our kernel,\nor also known as a filter, passing across our image, performing some sort of mathematical\noperation. And now the whole idea of a convolutional layer is to try and make sure that this kernel\nperforms the right operation to get the right output over here. Now, what do these kernels learn?\nWell, that is entirely up to the model. That's the beauty of deep learning is that it\nlearns how to best represent our data, hopefully, on its own by looking at more data. And then so\nif we jump back in here, so that's the equivalent of setting kernel size three by three. What if\nwe set the stride equal to one? Have we got this in the right order? It doesn't really matter.\nLet's go through stride next. If we go to here, what does stride say? Stride of the convolution\nof the convolving kernel. The default is one. Wonderful. Now, if we set the stride,\nor if we keep it at one, it's a default one, it's going to hop over, watch the red square on the\nleft. It's going to hop over one pixel at a time. So the convolution, the convolving, happens one\npixel at a time. That's what the stride sets. Now, watch what happens when I change the stride\nvalue to the output shape. Wow. Do you notice that it went down? So we have here, the kernel size\nis still the same. But now we're jumping over two pixels at a time. Notice how on the left,\ntwo pixels become available. And then if I jump over again, two pixels. So the reason why the\noutput compresses is because we're skipping some pixels as we go across the image. And now this\npattern happens throughout the entire network. That's one of the reasons why you see the size\nof our input or the size of each layer go down over time. What our convolutional layer is doing,\nand in fact, a lot of deep learning neural networks do, is they try to compress the input\ninto some representation that best suits the data. Because it would be no point of just memorizing\nthe exact patterns, you want to compress it in some way. Otherwise, you just might as well move\nyour input data around. You want to learn generalizable patterns that you can move around. And so we\nkeep going. We've got padding equals zero. Let's see what happens here. If we change the padding\nvalue, what happens? Up, down. Notice the size here. Oh, we've added two extra pixels around the\nedge. Now if we go down, one extra pixel. Now if we go zero, now why might we do that?\nIf we add some padding on the end, well, that's so that our kernel can operate on what's going on\nhere in the corner. In case there's some information on the edges of our image. Then you might be\nthinking, Daniel, there's a whole bunch of values here. How do we know what to set them?\nWell, you notice that I've just copied exactly what is going on here.\nThere's a three by three kernel. There's no padding on the image. And the stride is just going\none by one. And so that's often very common in machine learning, is that when you're just getting\nstarted and you're not sure what values to set these values to, you just copy some existing\nvalues from somewhere and see if it works on your own problem. And then if it doesn't, well,\nyou can adjust them. So let's see what happens when we do that. So pass the data through\nthe convolutional layer. So let's see what happens. Conv output equals conv layer.\nLet's pass it our test image. And we'll check the conv output. What happens?\nOh no, we get an error. Of course we get a shape error. One of the most common issues of machine\nlearning and deep learning. So this is saying that our input for the conv layer expects a four\ndimensional tensor, except it got a three dimensional input size of 364 64. Now, how do we add an\nextra dimension to our test image? Let's have a look. How would we add a batch dimension over on\nthe left here? We can go unsqueeze zero. So now we have a four dimensional tensor. Now, just keep\nin mind that if you're running this layer and then com2d on a pytorch version, that is, I believe\nthey fixed this or they changed it in pytorch. What am I on? I think this Google collab instance is\non 1.10. I think you might not get this error if you're running 1.11. So just keep that in mind.\nLike this should work if you're running 1.11. But if it doesn't, you can always unsqueeze here.\nAnd let's see what happens. Look at that. We get another tensor output. Again,\nthis is just all random numbers though, because our test image is just random numbers. And our\nconv layer is instantiated with random numbers. But we'll set the manual seed here. Now, if our\nnumbers are different to what's, if your numbers are different to what's on my screen, don't worry\ntoo much. Why is that? Because our conv layer is instantiated with random numbers. And our test\nimage is just random numbers as well. What we're paying attention to is the input and output shapes.\nDo you see what just happened? We put our input image in there with three channels.\nAnd now because we've set out channels to be 10, we've got 10. And we've got 62, 62. And this is\njust the batch size. It just means one image. So essentially our random numbers, our test image,\nhave gone through the convolutional layer that we created, have gone through this mathematical\noperation with regards to all the values that we've set, we've put the weight tensor, well,\nactually PyTorch created that for us. PyTorch has done this whole operation for us. Thank you,\nPyTorch. It's gone through all of these steps across. You could code this all by hand if you want,\nbut it's a lot easier and simpler to use a PyTorch layer. And it's done this. And now it's\ncreated this output. Now, whatever this output is, I don't know, it is random numbers, but this\nsame process will happen if we use actual data as well. So let's see what happens if we change\nthe values kernel size we increase. Notice how our output has gotten smaller because we're using\na bigger kernel to convolve across the image. What if we put this to three, three back to what it\nwas and stride of two? What do you think will happen? Well, our output size basically halves\nbecause we're skipping two pixels at a time. We'll put that back to one. What do you think will\nhappen if we set padding to one? 64, 64. We get basically the same size because we've added an\nextra pixel around the edges. So you can play around with this. And in fact, I encourage you to\ndo this is what we just did. Padding one, we just added an extra dummy zero pixel around the edges.\nSo practice with this, see what happens as you pass our test image, random numbers,\nthrough a conv 2d layer with different values here. What do you think will happen if you change\nthis to 64? Give that a shot and I'll see you in the next video. Who's ready to step through\nthe nn max pool 2d layer? Put your hand up. I've got my hand up. So let's do it together, hey,\nwe've got 7.2. Now you might have already given this a shot yourself. Stepping through\nnn max pool 2d. And this is this is what I do for a lot of different concepts that I haven't\ngone through before is I just write some test code and see what the inputs and outputs are.\nAnd so where could we find out about max pool 2d? Well, of course, we've got the documentation.\nI'm just going to link this in here. Max pool 2d. In the simplest case, the output value of\nlayer with input size nchw output nch out w out. By the way, this is number of batches,\ncolor channels, height, width. And this is the output of that layer. And kernel size, which is\na parameter up here, k h k w can be precisely described as out is going to be the max of some\nvalue, depending on the kernel size and the stride. So let's have a look at that in practice.\nAnd of course, you can read further through the documentation here. I'll just grab the link for\nthis actually. So it's here. Wonderful. And let's now first try it with our test image that we\ncreated above. So just highlight what the test image is. A bunch of random numbers in the same\nshape as what a single image would be if we were to replicate the image size of the CNN explainer.\nBy the way, we'll have a look at a visual in a second of max pool here. But you can go through\nthat on your own time. Let's if in doubt, code it out. So we're going to print out the original\nimage shape without unsqueezed dimension. Because recall that we had to add an extra dimension to\npass it through our com2d layer. Now, if you're using a later version of PyTorch, you might not\nget an error if you only use a three dimensional image tensor and pass it through a comp layer.\nSo we're going to pass it in test image, original shape, test image dot shape. So this is just going\nto tell us what the line of code in the cell above tells us. But that's fine. I like to make\npretty printouts, you know, test image with unsqueezed dimension. So this is just going to be our test\nimage. And we're going to see what happens when we unsqueeze a dimension, unsqueeze on zero\nfor dimension. That is about to say first, but it's the zero. Now we're going to create a sample\nnn max pool 2d layer. Because remember, even layers themselves in torch dot nn are models\nof their own accord. So we can just create a single, this is like creating a single layer model here.\nWe'll set the kernel size equal to two. And recall, if we go back to CNN explainer,\nkernel size equal to two results in a two by two square, a two by two kernel that's going to\nconvolve over our image, like so. And this is an example input, an example output. And you can see\nthe operation that max pooling does here. So just keep that in mind as we pass some sample data\nthrough our max pool layer. And now let's pass data through it. I actually will pass it through\njust the conv layer first, through just the conv layer. Because that's sort of how you might stack\nthings, you might put a convolutional layer and then a max pool layer on top of that convolutional\nlayer. So test image through conv. We'll create a variable here, equals our conv layer.\nIs going to take as an input, our test image dot unsqueeze on the zero dimension again.\nBeautiful. Now we're going to print out the shape here. This is just highlighting how I\nlike to troubleshoot things is I do one step, print the shape, one step, print the shape,\nsee what is happening as our data moves through various layers. So test image through conv.shape,\nwe'll see what our conv layer does to the shape of our data. And then we're going to pass data through\nmax pool layer, which is the layer we created a couple of lines above this one here.\nSo let's see what happens. Test image through current type at the moment through conv and max\npool. So quite a long variable name here, but this is to help us avoid confusion of what's\ngoing on. So we go test image through conv. So you notice how we're taking the output of our\nconvolutional layer, this here, and we're passing it through our max pool layer, which has another\ntypo. Wonderful. And finally, we'll print out the shape, shape after going through conv layer\nand max pool layer. What happens here? So we want test image through conv and max pool.\nLet's see how our max pool layer manipulates our test images shape. You ready? Three, two,\none, let's go. What do we get? Okay. So we have the test image original shape,\nrecall that our test image is just a collection of random numbers. And of course, our conv layer\nis going to be instantiated with random numbers. And max pool actually has no parameters. It just\ntakes the maximum of a certain range of inner tensor. So when we unsqueeze the test image as the\ninput, we get an extra dimension here. When we pass it through our conv layer. Oh, where did this\n64 come from? 164 64 64 64. Let's go back up to our conv layer. Do you notice how that we get the\n64 there because we changed the out channels value? If we change this back to 10, like what's in the\nCNN explainer model? One, two, three, four, five, six, seven, eight, nine, 10. What do you think will\nhappen there? Well, we get a little highlight here. 10. Then we keep going. I'll just get rid of\nthis extra cell. We don't need to check the version anymore. We'll check the test image\nshapes still three 64 64. But then as we pass it through the conv layer here, we get a different\nsize now. So it originally had three channels as the input for color channels, but we've upscaled\nit to 10 so that we have 10 hidden units in our layer. And then we have 64 64. Now, again,\nthese shapes will change if we change the values of what's going on here. So we might put padding\nto zero. What happens there? Instead of 64 64, we get 62 62. And then what happens after we pass\nit through the conv layer and then through the max pool layer? We've got 110 64 64. And now we have\n110 32 32. Now, why is that? Well, let's go back into the CNN explainer, jump into this max pool\nlayer here. Maybe this one because it's got a bit more going on. Do you notice on the left here is\nthe input? And we've got a two by two kernel here. And so the max pooling layer, what it does is it\ntakes the maximum of whatever the input is. So you'll notice the input is 60 60 in this case.\nWhereas the output over here is 30 30. Now, why is that? Well, because the max operation here is\nreducing it from section of four numbers. So let's get one with a few different numbers.\nThere we go. That'll do. So it's taking it from four numbers and finding the maximum value within\nthose four numbers here. Now, why would it do that? So as we've discussed before, what deep learning\nneural network is trying to do or in this case, a CNN is take some input data and figure out\nwhat features best represent whatever the input data is and compress them into a feature vector\nthat is going to be our output. Now, the reason being for that is because you could consider it\nfrom a neural networks perspective is that intelligence is compression. So you're trying to\ncompress the patterns that make up actual data into a smaller vector space, go from a higher\ndimensional space to a smaller vector space in terms of dimensionality of a tensor. But still,\nthis smaller dimensionality space represents the original data and can be used to predict on future\ndata. So that's the idea behind Max Paul is, hey, if we've got these learned features from our\nconvolutional layers, will the patterns, will the most important patterns stay around if we just\ntake the maximum of a certain section? So do you notice how the input here, we still have,\nyou can still see the outline of the car here, albeit a little bit more pixelated,\nbut just by taking the max of a certain region, we've got potentially the most important feature\nof that little section. And now, of course, you could customize this value here. If when we\ncreate our max pool layer, you could increase the kernel size to four by four. What do you think\nwill happen if we can increase it to four? So here, we've got a two by two kernel. If we increase it\nto four by four, what happens? Ah, do you notice that we've gone from 62 to 15, we've essentially\ndivided our feature space by four, we've compressed it even further. Now, will that work? Well,\nI'm not sure. That's part of the experimental nature of machine learning, but we're going to\nkeep it at two for now. And so this is with our tensor here 6464. But now let's do the same as\nwhat we've done above, but we'll do it with a smaller tensor so that we can really visualize\nthings. And we're going to just replicate the same operation that's going on here. So let's go here,\nwe'll create another random tensor. We'll set up the manual seed first. And we're going to create\na random tensor with a similar number of dimensions. Now, recall dimensions don't tell you, so this\nis a dimension 1364 64. That is a dimension. The dimensions can have different values within\nthemselves. So we want to create a four dimensional tensor to our images. So what that means is,\nlet me just show you it's way easy to explain things when we've got code is torch dot rand n.\nAnd we're going to set it up as size equals one, one, two, two. We can have a look at this random\ntensor. It's got four dimensions. One, two, three, four. So you could have a batch size,\ncolor channels, and height width, a very small image, but it's a random image here. But this is\nquite similar to what we've got going on here, right? Four numbers. Now, what do you think will\nhappen if we create a max pool layer, just like we've done above, create a max pool layer. So we\ngo max pool layer, just repeating the code that we have in the cell above, that's all right,\na little bit of practice. Kernel size equals two. And then we're going to pass the random tensor\nthrough the max pool layer. So we'll go max pool tensor equals max pool layer. And we're going\nto pass it in the random tensor. Wonderful. And then we can print out some shapes and print\nout some tenses. As we always do to visualize, visualize, visualize. So we're going to write in\nhere max pool tensor on a new line. We'll get in the max pool tensor. We'll see what this looks\nlike. And we'll also print out max pool tensor shape. And we can probably print out random tensor\nitself, as well as its shape as well. We'll get the shape here, dot shape. And we'll do the same\nfor the random tensor. So print, get a new line, random tensor, new line, random tensor. And then\nwe'll get the shape. Random tensor shape, random tensor. Oh, a lot of coding here. That's, that's\nthe fun part about machine learning, right? You get to write lots of code. Okay. So we're\nvisualizing what's going on with our random tensor. This is what's happening within the max pool layer.\nWe've seen this from a few different angles now. So we have a random tensor of numbers,\nand we've got a size here. But the max pool tensor, once we pass our random tensor,\nthrough the max pool layer, what happens? Well, we have 0.3367, 1288, 2345, 2303. Now,\nwhat's the max of all these? Well, it takes the max here is 3367. Oh, and we've got the random\ntensor down there. We don't want that. And see how we've reduced the shape from two by two to one\nby one. Now, what's going on here? Just for one last time to reiterate, the convolutional layer\nis trying to learn the most important features within an image. So if we jump into here,\nnow, what are they? Well, we don't decide what a convolutional layer learns. It learns these\nfeatures on its own. So the convolutional layer learns those features. We pass them through a\nrelu nonlinear activation in case our data requires nonlinear functions. And then we pass\nthose learned features through a max pool layer to compress them even further. So the convolutional\nlayer can compress the features into a smaller space. But the max pooling layer really compresses\nthem. So that's the entire idea. One more time, we start with some input data. We design a neural\nnetwork, in this case, a convolutional neural network, to learn a compressed representation\nof what our input data is, so that we can use this compressed representation to later on make\npredictions on images of our own. And in fact, you can try that out if you wanted to click here\nand add your own image. So I'd give that a go. That's your extension for this video. But now we've\nstepped through the max pool 2D layer and the conv 2D layer. I think it's time we started to try\nand use our tiny VGG network. This is your challenge is to create a dummy tensor and pass it through\nthis model. Pass it through its forward layer and see what happens to the shape of your dummy tensor\nas it moves through conv block 1 and conv block 2. And I'll show you my trick to calculating\nthe in features here for this final layer, which is equivalent to this final layer here.\nI'll see you in the next video.\nOver the last few videos, we've been replicating the tiny VGG architecture\nfrom the CNN explainer website. And I hope you know that this is this actually quite exciting\nbecause years ago, this would have taken months of work. And we've just covered we've broken it\ndown over the last few videos and rebuilt it ourselves with a few lines of PyTorch code.\nSo that's just goes to show how powerful PyTorch is and how far the deep learning field has come.\nBut we're not finished yet. Let's just go over to our keynote. This is what we've done.\nCNN explainer model. We have an input layer. We've created that. We have com2d layers.\nWe've created those. We have relo activation layers. We've created those.\nAnd finally, we have pulling layers. And then we finish off with an output layer.\nBut now let's see what happens when we actually pass some data through this entire model.\nAnd as I've said before, this is actually quite a common practice is you replicate a model\nthat you found somewhere and then test it out with your own data. So we're going to start off\nby using some dummy data to make sure that our model works. And then we're going to pass through.\nOh, I've got another slide for this. By the way, here's a breakdown of torch and\nN com2d. If you'd like to see it in text form, nothing here that we really haven't discussed before, but\nthis will be in the slides if you would like to see it. Then we have a video animation.\nWe've seen this before, though. And plus, I'd rather you go through the CNN explainer website\non your own and explore this different values rather than me just keep talking about it.\nHere's what we're working towards doing. We have a fashion MNIST data set. And we have\nour inputs. We're going to numerically encode them. We've done that already. Then we have our\nconvolutional neural network, which is a combination of convolutional layers, nonlinear activation\nlayers, pooling layers. But again, these could be comprised in many different ways, shapes and\nforms. In our case, we've just replicated the tiny VGG architecture. And then finally,\nwe want to have an output layer to predict what class of clothing a particular input image is.\nAnd so let's go back. We have our CNN model here. And we've got model two. So let's just practice\na dummy forward pass here. We're going to come back up a bit to where we were. We'll make sure\nwe've got model two. And we get an error here because I've times this by zero. So I'm going to\njust remove that and keep it there. Let's see what happens if we create a dummy tensor and pass it\nthrough here. Now, if you recall what our image is, do we have image? This is a fashion MNIST\nimage. So I wonder if we can go plot dot M not M show image. And I'm going to squeeze that.\nAnd I'm going to set the C map equal to gray. So this is our current image. Wonderful.\nSo there's our current image. So let's create a tensor. Or maybe we just try to pass this through\nthe model and see what happens. How about we try that model image? All right, we're going to try\nthe first pass forward pass. So pass image through model. What's going to happen? Well, we get an\nerror. Another shape mismatch. We've seen this before. How do we deal with this? Because what\nis the shape of our current image? 128, 28. Now, if you don't have this image instantiated,\nyou might have to go back up a few cells. Where did we create image? I'll just find this. So\njust we created this a fairly long time ago. So I'm going to probably recreate it down the\nbottom. My goodness, we've written a lot of code. Well, don't do us. We could create a dummy tensor\nif we wanted to. How about we do that? And then if you want to find, oh, right back up here,\nwe have an image. How about we do that? We can just do it with a dummy tensor. That's fine.\nWe can create one of the same size. But if you have image instantiated, you can try that out.\nSo there's an image. Let's now create an image that is, or a random tensor, that is the same\nshape as our image. So rand image tensor equals what torch dot rand n. And we're going to pass in\nsize equals 128, 28. Then if we get rand image tensor,\nwe check its shape. What do we get? So the same shape as our test image here,\nbut it's just going to be random numbers. But that's okay. We just want to highlight a point\nhere of input and output shapes. We want to make sure our model works. Can our random image tensor\ngo all the way through our model? That's what we want to find out. So we get an error here.\nWe have four dimensions, but our image is three dimensions. How do we add an extra dimension\nfor batch size? Now you might not get this error if you're running a later version of pie torch.\nJust keep that in mind. So unsqueeze zero. Oh, expected all tensors to be on the same device,\nbut found at least two devices. Again, we're going through all the three major issues in deep\nlearning. Shape mismatch, device mismatch, data type mismatch. So let's put this on the device,\ntwo target device, because we've set up device agnostic code.\nThat one and that two shapes cannot be multiplied. Oh, but we can output here.\nThat is very exciting. So what I might do is move this a couple of cells up so that we can\ntell what's going on. I'm going to delete this cell. So where do these shapes come from?\nWell, we printed out the shapes there. And so this is what's happened when our,\nI'll just create our random tensor. I'll bring our random tensor up a bit too. Let's bring this up.\nThere we go. So we pass our random to image tensor through our model, and we've made sure it's\ngot four dimensions by unsqueeze zero. And we make sure it's on the same device as our model,\nbecause our model has been sent to the GPU. And this is what happens as we pass our random\nimage tensor. We've got 12828 instead of previously we've seen 6464.3, which is going to clean this\nup a bit. And we get different shapes here. So you'll notice that as our input, if it was 6464.3\ngoes through these layers, it gets shaped into different values. Now this is going to be universal\nacross all of the different data sets you work on, you will be working with different shapes.\nSo it's important to, and also quite fun, to troubleshoot what shapes you need to use for\nyour different layers. So this is where my trick comes in. To find out the shapes for different\nlayers, I often construct my models, how we've done here, as best I can with the information\nthat I've got, such as replicating what's here. But I don't really know what the output\nshape is going to be before it goes into this final layer. And so I recreate the model as best\nI can. And then I pass data through it in the form of a dummy tensor in the same shape as my\nactual data. So we could customize this to be any shape that we wanted. And then I print the\nshapes of what's happening through each of the forward past steps. And so if we pass it through\nthis random tensor through the first column block, it goes through these layers here. And then it\noutputs a tensor with this size. So we've got 10, because that's how many output channels we've\nset. And then 14, 14, because our 2828 tensor has gone through a max pool 2d layer and gone through\na convolutional layer. And then it goes through the next block, column block two, which is because\nwe've put it in the forward method here. And then it outputs the shape. And if we go back down,\nwe have now a shape of one 10, seven, seven. So our previous tensor, the output of column block one,\nhas gone from 1414 to seven seven. So it's been compressed. So let me just write this down here,\noutput shape of column block one, just so we get a little bit more information.\nAnd I'm just going to copy this, put it in here, that will become block two.\nAnd then finally, I want to know if I get an output shape of classifier.\nSo if I rerun all of this, I don't get an output shape of classifier. So my model is running into\ntrouble. Once it gets to, so I get the output of conv block one, I don't get an output of classifier.\nSo this is telling me that I have an issue with my classifier layer. Now I know this, but I'm\nnot. Now I know this because, well, I've coded this model before, and the in features here,\nwe need a special calculation. So what is going on with our shapes?\nMat one and mat two shapes cannot be multiplied. So do you see here, what is the rule of matrix\nmultiplication? The inner dimensions here have to match. We've got 490. Where could that number\nhave come from? And we've got 10 times 10. Now, okay, I know I've set hidden units to 10.\nSo maybe that's where that 10 came from. And what is the output layer of the output shape of conv\nblock two? So if we look, we've got the output shape of conv block two. Where does that go?\nThe output of conv block two goes into our classifier model. And then it gets flattened.\nSo that's telling us something there. And then our NN linear layer is expecting the output of\nthe flatten layer as it's in features. So this is where my trick comes into play. I pass the\noutput of conv block two into the classifier layer. It gets flattened. And then that's what\nmy NN not linear layer is expecting. So what happens if we flatten this shape here? Do we get\nthis value? Let's have a look. So if we go 10 times seven times seven, 490. Now, where was this 10?\nWell, that's our hidden units. And where were these sevens? Well, these sevens are the output\nof conv block two. So that's my trick. I print the shapes of previous layers and see whether or\nnot they line up with subsequent layers. So if we go time seven times seven, we're going to have\nhidden units equals 10 times seven times seven. Where do we get the two sevens? Because that is\nthe output shape of conv block two. Do you see how this can be a little bit hard to calculate ahead\nof time? Now, you could calculate this by hand if you went into n conv 2d. But I prefer to write\ncode to calculate things for me. You can calculate that value by hand. If you go through,\nH out W out, you can add together all of the different parameters and multiply them and divide\nthem and whatnot. You can calculate the input and output shapes of your convolutional layers.\nYou're more than welcome to try that out by hand. But I prefer to code it out. If and out code it\nout. Now, let's see what happens if we run our random image tensor through our model. Now,\ndo you think it will work? Well, let's find out. All we've done is we've added this little line\nhere, times seven times seven. And we've calculated that because we've gone, huh, what if we pass a\ntensor of this dimension through a flattened layer? And what is our rule of matrix multiplication?\nThe inner dimensions here must match. And why do we know that these are matrices? Well,\nmat one and mat two shapes cannot be multiplied. And we know that inside a linear layer\nis a matrix multiplication. So let's now give this a go. We'll see if it works.\nOh, ho ho. Would you look at that? That is so exciting. We have the output shape of the classifier\nis one and 10. We have a look, we have one number one, two, three, four, five, six, seven, eight,\nnine, 10, one number for each class in our data set. Wow. Just like the CNN explain a website,\nwe have 10 outputs here. We just happen to have 10 classes as well. Now, this number again could be\nwhatever you want. It could be 100, could be 30, could be three, depending on how many classes\nyou have. But we have just figured out the input and output shapes of each layer in our model.\nSo that's very exciting. I think it's now time we've passed a random tensor through. How about we\npass some actual data through our model? In the next video, let's use our train and test step\nfunctions to train our first convolutional neural network. I'll see you there.\nWell, let's get ready to train our first CNN. So what do we need? Where are we up to in the\nworkflow? Well, we've built a model and we've stepped through it. We know what's going on,\nbut let's really see what's going on by training this CNN or see if it trains because we don't\nalways know if it will on our own data set, which is of fashion MNIST. So we're going to set up a\nloss function and optimizer for model two. And just as we've done before, model two, turn that\ninto markdown. I'll just show you the workflow again. So this is what we're doing. We've got some\ninputs. We've got a numerical encoding. We've built this architecture and hopefully it helps us\nlearn or it helps us make a predictive model that we can input images such as grayscale images of\nclothing and predict. And if we look where we are at the PyTorch workflow, we've got our data ready.\nWe've built our next model. Now here's where we're up to picking a loss function and an optimizer.\nSo let's do that, hey, loss function, or we can do evaluation metrics as well. So set up loss\nfunction slash eval metrics slash optimizer. And we want from helper functions, import accuracy\nfunction, we don't need to reimport it, but we're going to do it anyway for completeness. Loss\nfunction equals nn dot cross entropy loss, because we are working with a multi class classification\nproblem. And the optimizer, we're going to keep the same as what we've used before, torch dot\nopt in SGD. And we'll pass it in this time, the params that we're trying to optimize are the\nparameters of model two parameters. And we'll use a learning rate of 0.1. Run that. And just\nto reiterate, here's what we're trying to optimize model two state dig. We have a lot of random\nweights in model two. Have a look at all this. There's the bias, there's the weight. We're going\nto try and optimize these to help us predict on our fashion MNIST data set. So without any further\nado, let's in the next video, go to the workflow, we're going to build our training loop. But thanks\nto us before, we've now got functions to do this for us. So if you want to give this a go,\nuse our train step and test step function to train model two. Try that out. And we'll do it\ntogether in the next video. We're getting so close to training our model. Let's write some code to\ntrain our first thing in that model. Training and testing, I'm just going to make another heading\nhere. Model two, using our training and test functions. So we don't have to rewrite all of the\nsteps in a training loop and a testing loop, because we've already created that functionality\nbefore through our train step function. There we go. Performs the training, or this should be\nperforms a training step with model trying to learn on data loader. So let's set this up.\nWe're going to set up torch manual seed 42, and we can set up a CUDA manual seed as well.\nJust to try and make our experiments as reproducible as possible, because we're going to be using\nCUDA, we're going to measure the time because we want to compare our models, not only their\nperformance in evaluation metrics, but how long they take to train from time it, because there's\nno point having a model that performs really, really well, but takes 10 times longer to train.\nWell, maybe there is, depending on what you're working on. Model two equals timer,\nand we're going to train and test model, but the time is just something to be aware of,\nis that usually a better performing model will take longer to train. Not always the case, but\njust something to keep in mind. So for epoch in, we're going to use TQDM to measure the progress.\nWe're going to create a range of epochs. We're just going to train for three epochs,\nkeeping our experiment short for now, just to see how they work, epoch, and we're going to\nprint a new line here. So for an epoch in a range, we're going to do the training step,\nwhich is our train step function. The model is going to be equal to model two, which is our\nconvolutional neural network, our tiny VGG. The data loader is just going to be equal to the\ntrain data loader, the same one we've used before. The loss function is going to be equal to the\nloss function that we've set up above, loss FN. The optimizer as well is going to be\nthe optimizer in our case, stochastic gradient descent, optimizer equals optimizer,\nthen we set up the accuracy function, which is going to be equal to our accuracy function,\nand the device is going to be the target device. How easy was that? Now we do the same for the\ntrain or the testing step, sorry, the model is going to be equal to model two, and then the data\nloader is going to be the test data loader, and then the loss function is going to be our same\nour same loss function. And then we have no optimizer for this, we're just going to pass in the\naccuracy function here. And then of course, the device is going to be equal to the device.\nAnd then what do we do now? Well, we can measure the end time so that we know how long the code\nhere took to run. So let's go train time end for model two. This will be on the GPU, by the way,\nbut this time it's using a convolutional neural network. And the total train time,\ntotal train time for model two is going to be equal to print train time, our function that we\ncreated before as well, to help us measure start and end time. So we're going to pass in train\nto time start model two, and then end is going to be train time end model two. And then we're going\nto print out the device that it's using as well. So you're ready? Are you ready to train our first\nconvolutional neural network? Hopefully this code works. We've created these functions before,\nso it should be all right. But if and out, code it out, if and out, run the code, let's see what\nhappens. Oh my goodness. Oh, of course. Oh, we forgot to comment out the output shapes.\nSo we get a whole bunch of outputs for our model, because what have we done? Back up here,\nwe forgot to. So this means every time our data goes through the forward pass, it's going to\nbe printing out the output shapes. So let's just comment out these. And I think this cell is going\nto take quite a long time to run because it's got so many printouts. Yeah, see, streaming output\ntruncated to the last 5,000 lines. So we're going to try and stop that. Okay, there we go.\nBeautiful. That actually worked. Sometimes it doesn't stop so quickly. So we're going to rerun\nour fashion MSV to model cell so that we comment out these print lines. And then we'll just rerun\nthese cells down here. Just go back through fingers crossed, there's no errors. And we'll train our\nmodel again. Beautiful. Not as many printouts this time. So here we go. Our first CNN is training.\nHow do you think it'll go? Well, that's what we have printouts, right? So we can see the progress.\nSo you can see here all the functions that are being called behind the scenes from PyTorch. So\nthank you to PyTorch for that. There's our, oh, our train step function was in there.\nTrain step. Wonderful. Beautiful. So there's epoch zero. Oh, we get a pretty good test accuracy.\nHow good is that? Test accuracy is climbing as well. Have we beaten our baseline? We're looking at\nabout 14 seconds per epoch here. And then the final epoch. What do we finish at? Oh, 88.5. Wow.\nIn 41.979 or 42 there about seconds. Again, your mileage may vary. Don't worry too much if these\nnumbers aren't exactly the same on your screen and same with the training time because we might\nbe using slightly different hardware. What GPU do I have today? I have a Tesla P100 GPU. You might\nnot have the same GPU. So the training time, if this training time is something like 10 times\nhigher, you might want to look into what's going on. And if these values are like 10% lower or 10%\nhigher, you might want to see what's going on with your code as well. But let's now calculate\nour Model 2 results. I think it is the best performing model that we have so far. Let's get\na results dictionary. Model 2 results is so exciting. We're learning the power of convolutional neural\nnetworks. Model 2 results equals a vowel model. And this is a function that we've created before.\nSo returns a dictionary containing the results of a model predicting on data loader.\nSo now let's pass in the model, which will be our trained model to, and then we'll pass in the\ndata loader, which will be our test data loader. And then, oops, excuse me, typo, our loss function\nwill be, of course, our loss function. And the accuracy function will be accuracy function.\nAnd the device is already set, but we can reset anyway, device equals device. And we'll check\nout the Model 2 results. Make some predictions. Oh, look at that. Model accuracy 88. Does that\nbeat our baseline? Model 0 results. Oh, we did beat our baseline with a convolutional neural network.\nAll right. So I feel like that's, uh, that's quite exciting. But now let's keep going on. And, uh,\nlet's start to compare the results of all of our models. I'll see you in the next video.\nWelcome back. Now, in the last video, we trained our first convolutional neural network. And\nfrom the looks of things, it's improved upon our baseline. But let's make sure by comparing,\nthis is another important part of machine learning experiments is comparing the results\nacross your experiments. So and training time. Now, we've done that in a way where we've got\nthree dictionaries here of our model zero results, model one results, model two results. So how\nabout we create a data frame comparing them? So let's import pandas as PD. And we're going to\ncompare results equals PD dot data frame. And because our model results dictionaries, uh,\nall have the same keys. Let's pass them in as a list. So model zero results, model one results,\nand model two results to compare them. Wonderful. And what it looks like when we compare the results.\nAll righty. So recall our first model was our baseline V zero was just two linear layers.\nAnd so we have an accuracy of 83.4 and a loss of 0.47. The next model was we trained on the GPU\nand we introduced nonlinearities. So we actually found that that was worse off than our baseline.\nBut then we brought in the big guns. We brought in the tiny VGG architecture from the CNN explainer\nwebsite and trained our first convolutional neural network. And we got the best results so far.\nBut there's a lot more experiments that we could do. We could go back through our\ntiny VGG and we could increase the number of hidden units. Where do we create our model up here?\nWe could increase this to say 30 and see what happens. That would be a good experiment to\ntry. And if we found that nonlinearities didn't help with our second model, we could comment out\nthe relu layers. We could of course change the kernel size, change the padding, change the max\npool. A whole bunch of different things that we could try here. We could train it for longer.\nSo maybe if we train it for 10 epochs, it would perform better. But these are just things to\nkeep in mind and try out. I'd encourage you to give them a go yourself. But for now, we've kept\nall our experiments quite the same. How about we see the results we add in the training time?\nBecause that's another important thing that we've been tracking as well. So we'll add\ntraining time to results comparison. So the reason why we do this is because\nif this model is performing quite well, even compared to our CNN, so a difference in about\n5% accuracy, maybe that's tolerable in the space that we're working, except that this model\nmight actually train and perform inference 10 times faster than this model. So that's just\nsomething to be aware of. It's called the performance speed trade off. So let's add another column\nhere, compare results. And we're going to add in, oh, excuse me, got a little error there. That's\nall right. Got trigger happy on the shift and enter. Training time equals, we're going to add in,\nwe've got another list here is going to be total train time for model zero, and total train time\nfor model one, and total train time for model two. And then we have a look at our\nhow compare results dictionary, or sorry, compare results data frame. Wonderful. So we see, and\nnow this is another thing. I keep stressing this to keep in mind. If your numbers aren't exactly\nof what I've got here, don't worry too much. Go back through the code and see if you've set up\nthe random seeds correctly, you might need a koodle random seed. We may have missed one of those.\nIf your numbers are out landishly different to these numbers, then you should go back through\nyour code and see if there's something wrong. And again, the training time will be highly\ndependent on the compute environment you're using. So if you're running this notebook locally,\nyou might get faster training times. If you're running it on a different GPU to what I have,\nNVIDIA SMI, you might get different training times. So I'm using a Tesla P100, which is quite a fast\nGPU. But that's because I'm paying for Colab Pro, which generally gives you faster GPUs.\nAnd model zero was trained on the CPU. So depending on what compute resource Google allocates to you\nwith Google Colab, this number might vary here. So just keep that in mind. These values training\ntime will be very dependent on the hardware you're using. But if your numbers are dramatically\ndifferent, well, then you might want to change something in your code and see what's going on.\nAnd how about we finish this off with a graph? So let's go visualize our model results. And while\nwe're doing this, have a look at the data frame above. Is the performance here 10 seconds longer\ntraining time worth that extra 5% of the results on the accuracy? Now in our case, we're using a\nrelatively toy problem. What I mean by toy problem is quite a simple data set to try and test this\nout. But in your practice, that may be worth doing. If your model takes longer to train,\nbut gets quite a bit better performance, it really depends on the problem you're working with.\nCompare results. And we're going to set the index as the model name, because I think that's\nwhat we want our graph to be, not the model name. And then we're going to plot, we want to compare\nthe model accuracy. And we want to plot, the kind is going to be equal to bar h, horizontal bar chart.\nWe've got p x label, we're going to get accuracy as a percentage. And then we're going to go py label.\nThis is just something that you could share. If someone was asking, how did your modeling\nexperiments go on fashion MNIST? Well, here's what I've got. And then they ask you, well,\nwhat's the fashion MNIST model V2? Well, you could say that's a convolutional neural network that\ntrained, that's replicates the CNN explainer website that trained on a GPU. How long did that\ntake to train? Well, then you've got the training time here. We could just do it as a vertical bar\nchart. I did it as horizontal so that this looks a bit funny to me. So horizontal like that.\nSo the model names are over here. Wonderful. So now I feel like we've got a trained model.\nHow about we make some visual predictions? Because we've just got numbers on a page here,\nbut our model is trained on computer vision data. And the whole point of making a machine\nlearning model on computer vision data is to be able to visualize predictions. So let's give\nthat a shot, hey, in the next video, we're going to use our best performing model, fashion MNIST\nmodel V2 to make predictions on random samples from the test data set. You might want to give\nthat a shot, make some predictions on random samples from the test data set, and plot them out with\ntheir predictions as the title. So try that out. Otherwise, we'll do it together in the next video.\nIn the last video, we compared our models results. We tried three experiments. One was a basic linear\nmodel. One was a linear model with nonlinear activations. And fashion MNIST model V2 is a\nconvolutional neural network. And we saw that from an accuracy perspective, our convolutional neural\nnetwork performed the best. However, it had the longest training time. And I just want to exemplify\nthe fact that the training time will vary depending on the hardware that you run on. We spoke about\nthis in the last video. However, I took a break after finishing the last video, reran all of the\ncells that we've written, all of the code cells up here by coming back to the notebook and going\nrun all. And as you'll see, if you compare the training times here to the last video, we get\nsome different values. Now, I'm not sure exactly what hardware Google collab is using behind the\nscenes. But this is just something to keep in mind, at least from now on, we know how to track\nour different variables, such as how long our model takes to train and what its performance\nvalues are. But it's time to get visual. So let's create another heading, make and evaluate. This\nis one of my favorite steps after training a machine learning model. So make and evaluate random\npredictions with the best model. So we're going to follow the data explorer's model of getting\nvisual visual visual or visualize visualize visualize. Let's make a function called make\npredictions. And it's going to take a model, which will be a torch and end module type.\nIt's also going to take some data, which can be a list. It'll also take a device type,\nwhich will be torch dot device. And we'll set that by default to equal the default device that\nwe've already set up. And so what we're going to do is create an empty list for prediction\nprobabilities. Because what we'd like to do is just take random samples from the test data set,\nmake predictions on them using our model, and then plot those predictions. We want to visualize\nthem. And so we'll also turn our model into evaluation mode, because if you're making predictions with\nyour model, you should turn on evaluation mode. We'll also switch on the inference mode context\nmanager, because predictions is another word for inference. And we're going to loop through\nfor each sample in data. Let's prepare the sample. So this is going to take in\na single image. So we will unsqueeze it, because we need to add a batch size dimension\non the sample, we'll set dim equals to zero, and then we'll pass that to the device. So\nadd a batch dimension, that's with the unsqueeze, and pass to target device. That way, our data and\nmodel are on the same device. And we can do a forward pass. Well, we could actually up here go\nmodel dot two device. That way we know that we've got device agnostic code there.\nNow let's do the forward pass, forward pass model outputs raw logits. So recall that if we have a\nlinear layer at the end of our model, it outputs raw logits. So pred logit for a single sample is\ngoing to equal model. We pass the sample to our target model. And then we're going to get the\nprediction probability. How do we get the prediction probability? So we want to go from\nlogit to prediction probability. Well, if we're working with a multi class classification problem,\nwe're going to use the softmax activation function on our pred logit. And we're going to squeeze\nit so it gets rid of an extra dimension. And we're going to pass in dim equals zero. So that's going\nto give us our prediction probability for a given sample. Now let's also turn our prediction\nprobabilities into prediction labels. So get pred. Well, actually, I think we're just going\nto return the pred probes. Yeah, let's see what that looks like, because we've got a\nan empty list up here for pred probes. So for matplotlib, we're going to have to use our data\non the CPU. So let's make sure it's on the CPU, because matplotlib doesn't work with the GPU.\nSo get pred prob off GPU for further calculations. So we're just hard coded in here to make sure\nthat our prediction probabilities off the GPU. So pred probs, which is our list up here. We're\ngoing to append the pred prob that we just calculated. But we're going to put it on the CPU. And then\nlet's go down here. And we're going to. So if we've done it right, we're going to have a list of\nprediction probabilities relating to particular samples. So we're going to stack the pred probs\nto turn list into a tensor. So this is only one way of doing things. There are many different ways\nthat you could make predictions and visualize them. I'm just exemplifying one way. So we're\ngoing to torch stack, which is just going to say, hey, concatenate everything in the list to a\nsingle tensor. So we might need to tab that over, tab, tab. Beautiful. So let's try this function\nin action and see what happens. I'm going to import random. And then I'm going to set the random\nseed to 42. And then I'm going to create test samples as an empty list, because we want an empty\nor we want a list of test samples to iterate through. And I'm going to create test labels also as an\nempty list. So that remember, when we are evaluating predictions, we want to compare them to the\nground truth. So we want to get some test samples. And then we want to get their actual labels so\nthat when our model makes predictions, we can compare them to their actual labels. So for sample,\ncomma label, in, we're going to use random to sample the test data. Now note that this is not\nthe test data loader. This is just test data. And we're going to set k equals to nine. And recall,\nif you want to have a look at test data, what do we do here? We can just go test data,\nwhich is our data set, not converted into a data loader yet. And then if we wanted to get the first\n10 samples, can we do that? Only one element tensors can be converted into Python scalars. So if we\nget the first zero, and maybe we can go up to 10. Yeah, there we go. And what's the shape of this?\nTuple has no object shape. Okay, so we need to go image label equals that. And then can we check\nthe shape of the image label? Oh, because the labels are going to be integers.\nWonderful. So that's not the first 10 samples, but that's just what we get if we iterate through\nthe test data, we get an image tensor, and we get an associated label. So that's what we're doing\nwith this line here, we're just randomly sampling nine samples. And this could be any number you\nwant. I'm going to use nine, because this is a spoiler for later on, we're going to create a\nthree by three plot. So that just nine is just a fun number. So get some random samples from the\ntest data set. And then we can go test samples dot append sample. And we will go test labels dot\nappend label. And then let's go down here, view the first, maybe we go first sample shape.\nSo test samples zero dot shape.\nAnd then if we get test samples, zero, we're going to get a tensor of image values. And then\nif we wanted to plot that, can we go PLT, M show, C map, equals gray. And we may have to squeeze\nthis, I believe, to remove the batch tensor. Let's see what happens batch dimension. There we go.\nBeautiful. So that's to me, a shoe, a high heel shoe of some sort. If we get the title,\nPLT dot title, test labels, let's see what this looks like. It's a five, which is, of course,\nclass names will index on that. Sandal. Okay, beautiful. So we have nine random samples,\nnine labels that are associated with that sample. Now let's make some predictions. So make predictions.\nAnd this is one of my favorite things to do. I can't stress it enough is to randomly pick data\nsamples from the test data set and predict on them and do it over and over and over again to see\nwhat the model is doing. So not only at the start of a problem, I'll just get the prediction\nprobabilities here. We're going to call our make predictions function. So not only at the start of\na problem should you become one with the data, even after you've trained a model, you'll want to\nfurther become one with the data, but this time become one with your models predictions on the\ndata and see what happens. So view the first two prediction probabilities list. So we're just\nusing our make predictions function that we created before, passing at the model, the train model\nto, and we're passing at the data, which is the test samples, which is this list that we just\ncreated up here, which is comprised of random samples from the test data set. Wonderful. So\nlet's go. Pred probes. Oh, we don't want to view them all. That's going to give us\nOh, we want to the prediction probabilities for a given sample. And so how do we convert\nprediction probabilities into labels? Because if we're trying to, if we have a look at test\nlabels, if we're trying to compare apples to apples, when we're evaluating our model, we want to,\nwe can't really necessarily compare the prediction probabilities straight to the test labels. So we\nneed to convert these prediction probabilities into prediction labels. So how can we do that?\nWell, we can use argmax to take whichever value here, the index, in this case, this one,\nthe index of whichever value is the highest of these prediction probabilities. So let's see that\nin action. Convert prediction probabilities to labels. So we'll go pred classes equals\npred probes, and we'll get the argmax across the first dimension. And now let's have a look at the\npred classes. Wonderful. So are they in the same format as our test labels? Yes, they are. So if\nyou'd like to go ahead, in the next video, we're going to plot these and compare them. So we're\ngoing to write some code to create a mapplotlib plotting function that's going to plot nine\ndifferent samples, along with their original labels, and their predicted label. So give that a shot,\nwe've just written some code here to make some predictions on random samples. If you'd like them\nto be truly random, you can comment out the seed here, but I've just kept the seed at 42. So that\nour random dot sample selects the same samples on your end and on my end. So in the next video,\nlet's plot these. Let's now continue following the data explorer's motto of visualize visualize\nvisualize. We have some prediction classes. We have some labels we'd like to compare them to.\nYou can compare them visually. It looks like our model is doing pretty good. But let's,\nsince we're making predictions on images, let's plot those images along with the predictions.\nSo I'm going to write some code here to plot the predictions. I'm going to create a matplotlib\nfigure. I'm going to set the fig size to nine and nine. Because we've got nine random samples,\nyou could, of course, change this to however many you want. I just found that a three by three\nplot works pretty good in practice. And I'm going to set n rows. So for my matplotlib plot, I want\nthree rows. And I want three columns. And so I'm going to enumerate through the samples in test\nsamples. And then I'm going to create a subplot for each sample. So create a subplot. Because this\nis going to create a subplot because it's within the loop. Each time it goes through a new sample,\ncreate a subplot of n rows and calls. And the index it's going to be on is going to be i plus\none, because it can't start at zero. So we just put i plus one in there. What's going on here?\nEnumerate. Oh, excuse me. In enumerate, wonderful. So now we're going to plot the target image.\nWe can go plot dot in show, we're going to get sample dot squeeze. Because we need to remove the\nbatch dimension. And then we're going to set the C map is equal to gray. What's this telling me\nup here? Oh, no, that's correct. Next, we're going to find the prediction label in text form,\nbecause we don't want it in a numeric form, we could do that. But we want to look at things\nvisually with human readable language, such as sandal for whatever class sandal is, whatever number\nclass that is. So we're going to set the pred label equals class names. And we're going to index\nusing the pred classes I value. So right now we're going to plot our sample. We're going to find\nits prediction. And now we're going to get the truth label. So we also want this in text form.\nAnd what is the truth label going to be? Well, the truth label is we're going to have to index\nusing class names and index on that using test labels I. So we're just matching up our indexes\nhere. Finally, we're going to create a title, create a title for the plot. And now here's what I like\nto do as well. If we're getting visual, well, we might as well get really visual, right? So I\nthink we can change the color of the title text, depending if the prediction is right or wrong.\nSo I'm going to create a title using an F string, pred is going to be a pred label,\nand truth label. We could even plot the prediction probabilities here if we wanted to. That might\nbe an extension that you might want to try. And so here we're going to check for equality\nbetween pred and truth and change color of title text. So what I mean by this, it's going to be a\nlot easier to explain if we just if and doubt coded out. So if the pred label equals the truth\nlabel, so they're equal, I want the plot dot title to be the title text. But I want the font size,\nwell, the font size can be the same 10. I want the color to equal green. So if they're so green text,\nif prediction, same as truth, and else I'm going to set the plot title to have title text font\nsize equals 10. And the color is going to be red. So does that make sense? All we're doing is we're\nenumerating through our test samples that we got up here, test samples that we found randomly from\nthe test data set. And then each time we're creating a subplot, we're plotting our image,\nwe're finding the prediction label by indexing on the class names with our pred classes value,\nwe're getting the truth label, and we're creating a title for the plot that compares the pred label\nto the truth. And we're changing the color of the title text, depending if the pred label is\ncorrect or not. So let's see what happens. Did we get it right? Oh, yes, we did. Oh, I'm going to\ndo one more thing. I want to turn off the accesses, just so we get more real estate. I love these\nkind of plots. It helps that our model got all of these predictions right. So look at this,\npred sandal, truth, sandal, pred trouser, truth trouser. So that's pretty darn good, right? See how,\nfor me, I much appreciate, like, I much prefer visualizing things numbers on a page look good,\nbut there's something, there's nothing quite like visualizing your machine learning models\npredictions, especially when it gets it right. So how about we select some different random samples\nup here, we could functionize this as well to do like all of this code in one hit, but that's all\nright. We'll be a bit hacky for now. So this is just going to randomly sample with no seed at all.\nSo your samples might be different to mine, nine different samples. So this time we have an ankle\nboot, we'll make some predictions, we'll just step through all of this code here. And oh,\nthere we go. It got one wrong. So all of these are correct. But this is more interesting as\nwell is where does your model get things wrong? So it predicted address, but this is a coat.\nNow, do you think that this could be potentially address? To me, I could see that as being addressed.\nSo I kind of understand where the model's coming from there. Let's make some more random predictions.\nWe might do two more of these before we move on to the next video.\nOh, all correct. We're interested in getting some wrong here. So our model seems to be too good.\nAll correct again. Okay, one more time. If we don't get any wrong, we're going on to the next\nvideo. But this is just really, oh, there we go. Too wrong. Beautiful. So predicted address,\nand that's a shirt. Okay. I can kind of see where the model might have stuffed up there.\nIt's a little bit long for a shirt for me, but I can still understand that that would be a shirt.\nAnd this is a pullover, but the truth is a coat. So maybe, maybe there's some issues with the labels.\nAnd that's probably what you'll find in a lot of data sets, especially quite large ones.\nJust with a sheer law of large numbers, there may be some truth labels in your data sets that\nyou work with that are wrong. And so that's why I like to see, compare the models predictions\nversus the truth on a bunch of random samples to go, you know what, is our models results\nbetter or worse than they actually are. And that's what visualizing helps you do is figure out,\nyou know what, our model is actually, it says it's good on the accuracy. But when we visualize\nthe predictions, it's not too good. And vice versa, right? So you can keep playing around with this,\ntry, look at some more random samples by running this again. We'll do one more for good luck.\nAnd then we'll move on to the next video. We're going to go on to another way. Oh,\nsee, this is another example. Some labels here could be confusing. And speaking of confusing,\nwell, that's going to be a spoiler for the next video. But do you see how the prediction is a\nt-shirt top, but the truth is a shirt? To me, that label is kind of overlapping. Like, I don't know,\nwhat's the difference between a t-shirt and a shirt? So that's something that you'll find\nas you train models is maybe your model is going to tell you about your data as well.\nAnd so we hinted that this is going to be confused. The model is confused between t-shirt top and\nshirt. How about we plot a confusion matrix in the next video? I'll see you there.\nWe're up to a very exciting point in evaluating our machine learning model.\nAnd that is visualizing, visualizing, visualizing. And we saw that in the previous video, our model\nkind of gets a little bit confused. And in fact, I would personally get confused at the difference\nbetween t-shirt slash top and a shirt. So these kind of insights into our model predictions\ncan also give us insights into maybe some of our labels could be improved. And another way to\ncheck that is to make a confusion matrix. So let's do that, making a confusion matrix for further\nprediction evaluation. Now, a confusion matrix is another one of my favorite ways of evaluating\na classification model, because that's what we're doing. We're doing multi class classification.\nAnd if you recall, if we go back to section two of the lone pytorch.io book,\nand then if we scroll down, we have a section here, more classification evaluation metrics.\nSo accuracy is probably the gold standard of classification evaluation.\nThere's precision, there's recall, there's F1 score, and there's a confusion matrix here.\nSo how about we try to build one of those? I want to get this and copy this.\nSo, and write down a confusion matrix is a fantastic way of evaluating your classification models\nvisually. Beautiful. So we're going to break this down. First of all, we need to plot a\nconfusion matrix. We need to make predictions with our trained model on the test data set.\nNumber two, we're going to make a confusion matrix. And to do so, we're going to leverage\ntorch metrics tricks have to figure out how to spell metrics and confusion matrix. So recall\nthat torch metrics we've touched on this before is a great package torch metrics for a whole\nbunch of evaluation metrics of machine learning models in pytorch flavor. So if we find we've\ngot classification metrics, we've got audio image detection. Look how this is beautiful,\na bunch of different evaluation metrics. And if we go down over here, we've got confusion\nmatrix. So I only touched on five here, but or six. But if you look at torch metrics, they've got,\nhow many is that about 25 different classification metrics? So if you want some extra curriculum,\nyou can read through these. But let's go to confusion matrix. And if we look at some code here,\nwe've got torch metrics, confusion matrix, we need to pass in number of classes. We can\nnormalize if we want. And do you notice how this is quite similar to the pytorch documentation?\nWell, that's the beautiful thing about torch metrics is that it's created with pytorch in mind.\nSo let's try out if you wanted to try it out on some\ntester code, you could do it here. But since we've already got some of our own code,\nlet's just bring in this. And then number three is to plot it. We've got another helper package here,\nplot the confusion matrix using ML extend. So this is another one of my favorite helper\nlibraries for machine learning things. It's got a lot of functionality that you can code up\nyourself, but you often find yourself coding at a few too many times, such as plotting a confusion\nmatrix. So if we look up ML extend plot confusion matrix, this is a wonderful library. I believe it was\nit was created by Sebastian Rushka, who's a machine learning researcher and also author of\na great book. There he is. Yeah, this is a side note machine learning with pytorch and\nscikit loan. I just got this book it just got released in the start of 2022. And it's a great\nbook. So that's a little side note for learning more about machine learning with pytorch and scikit\nloan. So shout out to Sebastian Rushka. Thank you for this package as well. This is going to\njust help us plot a confusion matrix like this. So we'll have our predicted labels on the bottom\nand our true labels on the side here. But we can just copy this code in here.\nLink sorry, and then confusion matrix, we can copy that in here. The thing is that torch\nmetrics doesn't come with Google Colab. So if you're using Google Colab, I think ML extend does,\nbut we need a certain version of ML extend that Google Colab doesn't yet have yet. So we actually\nneed version 0.19.0. But we're going to import those in a second. Let's first make some predictions\nacross our entire test data set. So previously, we made some predictions only on nine random samples.\nSo random sample, we selected nine. You could, of course, change this number to make it on more.\nBut this was only on nine samples. Let's write some code to make predictions across our entire\ntest data set. So import tqdm.auto for progress bar tracking.\nSo tqdm.auto. We don't need to re-import it. I believe we've already got it above, but I'm just\ngoing to do it anyway for completeness. And so we're going to make, this is step one, above,\nmake predictions, make predictions with trained model. Our trained model is model two. So let's\ncreate an empty predictions list. So we can add our predictions to that. We're going to set our\nmodel into evaluation mode. And we're going to set with torch inference mode as our context manager.\nAnd then inside that, let's just build the same sort of code that we used for our testing loop,\nexcept this time we're going to append all of our predictions to a list.\nSo we're going to iterate through the test data loader. And we can give our tqdm description.\nWe're going to say making predictions dot dot dot. You'll see what that looks like in a minute.\nAnd here we are going to send the data and targets to target device. So x, y equals x\nto device and y to device. Wonderful. And we're going to do the forward pass.\nSo we're going to create y logit. Remember, the raw outputs of a model with a linear layer at the\nend are referred to as logits. And we don't need to calculate the loss, but we want to turn predictions\nfrom logits to prediction probabilities to prediction labels. So we'll set here y pred equals torch\ndot softmax. You could actually skip the torch softmax step if you wanted to and just take the\nargmax of the logits. But we will just go from prediction probabilities to pred labels for completeness.\nSo squeeze and we're going to do it across the first dimension or the zeroth dimension. And then\nwe'll take the argmax of that across the first dimension as well. And a little tidbit. If you\ntake different dimensions here, you'll probably get different values. So just check the inputs\nand outputs of your code to make sure you're using the right dimension here. And so let's go\nput predictions on CPU for evaluation. Because if we're going to plot anything, that plot lib will\nwant them on the CPU. So we're going to append our predictions to y preds, y pred dot CPU.\nBeautiful. And because we're going to have a list of different predictions, we can use concatenate\na list of predictions into a tensor. So let's just print out y preds. And so I can show you what\nit looks like. And then if we go y pred tensor, this is going to turn our list of predictions\ninto a single tensor. And then we'll go y pred tensor. And we'll view the first 10. Let's see if this\nworks. So making predictions. Oh, would you look at that? Okay, so yeah, here's our list of\npredictions. A big list of tensors. Right, we don't really want it like that. So if we get rid of\nthat, and there's our progress bar, it's going through each batch in the test data load, so there's\n313 batches of 32. So if we comment out print y preds, this line here torch dot cat y preds is\ngoing to turn this these tensors into a single tensor, or this list of tensors into a single\ntensor concatenate. Now, if we have a look, there we go, beautiful. And if we have a look at the\nwhole thing, we're making predictions every single time here, but that's all right. They are pretty\nquick. There we go. One big long tensor. And then if we check length y pred tensor, there should be\none prediction per test sample. 10,000 beautiful. So now we're going to, we need to install torch\nmetrics because torch metrics doesn't come with Google Colab at the time of recording. So let\nme just show you if we tried to import torch metrics. It doesn't, it might in the future, so just keep\nthat in mind, it might come with Google Colab because it's a pretty useful package. But let's\nnow install see if required packages are installed. And if not, install them. So we'll just install\ntorch metrics. We'll finish off this video by trying to import. We'll set up a try and accept\nloop. So Python is going to try import torch metrics and ML extend. I write it like this,\nbecause you may already have to which metrics and ML extend if you're running this code on a local\nmachine. But if you're running it in Google Colab, which I'm sure many of you are, we are\ngoing to try and import it anyway. And if it doesn't work, we're going to install it.\nSo ML extend, I'm just going to check the version here because we need version for our plot confusion\nmatrix function. This one, we need version 0.19.0 or higher. So I'm just going to write a little\nstatement here. Assert int ML extend dot version. So if these two, if this condition in the try\nloop is or try block is accepted, it will skip the next step dot split. And I'm just going to check\nthe first index string equals is greater than or equal to 19. Otherwise, I'm going to return an\nerror saying ML extend version should be 0.19.0 or higher. And so let me just show you what this\nlooks like. If we run this here, string and int, did I not turn it into a string? Oh, excuse me.\nThere we go. And I don't need that bracket on the end. There we go. So that's what I'm saying.\nSo this is just saying, hey, the version of ML extend that you have should be 0 or should be\n19 or higher. Because right now Google Colab by default has 14, this may change in the future.\nSo let's finish off this accept block. If the above condition fails, which it should,\nwe are going to pip install. So we're going to install this into Google Colab torch metrics.\nWe're going to do it quietly. And we're also going to pass the U tag for update ML extend.\nSo import torch metrics, ML extend afterwards, after it's been installed and upgraded. And print,\nwe're going to go ML extend version, going to go ML extend underscore version. And let's see what\nhappens if we run this. So we should see, yeah, some installation happening here. This is going\nto install torch metrics. Oh, do we not have ML extend the upgraded version? Let's have a look.\nWe may need to restart our Google Colab instance. Ah, okay, let's take this off. Quiet.\nIs this going to tell us to restart Google Colab?\nWell, let's restart our runtime. After you've run this cell, if you're using Google Colab,\nyou may have to restart your runtime to reflect the fact that we have the updated version of ML\nextend. So I'm going to restart my runtime now. Otherwise, we won't be able to plot our confusion\nmatrix. We need 0.19.0. And I'm going to run all of these cells. So I'm going to pause the video\nhere, run all of the cells by clicking run all. Note, if you run into any errors, you will have\nto run those cells manually. And then I'm going to get back down to this cell and make sure that I\nhave ML extend version 0.1.9. I'll see in a few seconds.\nI'm back. And just a little heads up. If you restart your runtime and click run all,\nyour Colab notebook will stop running cells if it runs into an error. So this is that error we\nfound in a previous video where our data and model were on different devices. So to skip past that,\nwe can just jump to the next cell and we can click run after. There we go. And it's going to run all\nof the cells after for us. It's going to retrain our models. Everything's going to get rerun.\nAnd then we're going to come right back down to where we were before trying to install the\nupdated version of ML extend. I'm going to write some more code while our code is running import\nML extend. And then I'm going to just make sure that we've got the right version here. You may\nrequire a runtime restart. You may not. So just try to see after you've run this install of\ntorch metrics and upgrade of ML extend. See if you can re import ML extend. And if you have the\nversion 0.19.0 or above, we should be able to run the code. Yeah, there we go. Wonderful.\nML extend 0.19.0. And we've got ML extend version, assert, import. Beautiful. So we've got a lot\nof extra code here. In the next video, let's move forward with creating a confusion matrix.\nI just wanted to show you how to install and upgrade some packages in Google Colab if you\ndon't have them. But now we've got predictions across our entire test data set. And we're going\nto be moving towards using confusion matrix function here to compare our predictions versus the target\ndata of our test data set. So I'll see in the next video, let's plot a confusion matrix.\nWelcome back. In the last video, we wrote a bunch of code to import some extra libraries that we\nneed for plotting a confusion matrix. This is really helpful, by the way. Google Colab comes\nwith a lot of prebuilt installed stuff. But definitely later on down the track, you're going to need\nto have some experience installing stuff. And this is just one way that you can do it. And we also\nmade predictions across our entire test data set. So we've got 10,000 predictions in this tensor.\nAnd what we're going to do with a confusion matrix is confirm or compare these predictions\nto the target labels in our test data set. So we've done step number one. And we've prepared\nourselves for step two and three, by installing torch metrics, and installing ML extend or the\nlater version of ML extend. So now let's go through step two, making a confusion matrix,\nand step three plotting that confusion matrix. This is going to look so good. I love how good\nconfusion matrix is look. So because we've got torch metrics now, we're going to import the\nconfusion matrix class. And from our ML extend, we're going to go into the plotting module,\nand import plot confusion matrix. Recall that the documentation for both of these are\nwithin torch metrics here, and within ML extend here. Let's see what they look like. So number two\nis set up confusion matrix instance, and compare predictions to targets. That's what evaluating a\nmodel is, right? Comparing our models predictions to the target predictions. So I'm going to set\nup a confusion matrix under the variable conf mat, then I'm going to call the confusion matrix class\nfrom torch metrics. And to set up an instance of it, I need to pass in the number of classes that\nwe have. So because we have 10 classes, they are all contained within class names. Recall that\nclass names is a list of all of the different classes that we're working with. So I'm just going\nto pass in the number of classes as the length of our class names. And then I can use that\nconf mat instance, confusion matrix instance, to create a confusion matrix tensor by passing\ninto conf mat, which is what I've just created up here. Conf mat, just like we do with our loss\nfunction, I'm going to pass in preds equals our Y pred tensor, which is just above Y pred tensor\nthat we calculated all of the predictions on the test data set. There we go. That's our preds.\nAnd our target is going to be equal to test data dot targets. And this is our test data data set\nthat we've seen before. So if we go test data and press tab, we've got a bunch of different\nattributes, we can get the classes. And of course, we can get the targets, which is the labels.\nPyTorch calls labels targets. I usually refer to them as labels, but the target is the test data\ntarget. So we want to compare our models predictions on the test data set to our test data targets.\nAnd so let's keep going forward. We're up to step number three now. So this is going to create\nour confusion matrix tensor. Oh, let's see what that looks like, actually. Conf mat tensor.\nOh, okay. So we've got a fair bit going on here. But let's turn this into a pretty version of this.\nSo along the bottom is going to be our predicted labels. And along the side here is going to be\nour true labels. But this is where the power of ML extend comes in. We're going to plot our\nconfusion matrix. So let's create a figure and an axes. We're going to call the function plot\nconfusion matrix that we've just imported above. And we're going to pass in our conf mat equals\nour conf mat tensor. But because we're working with map plot lib, it'll want it as NumPy.\nSo I'm just going to write here, map plot lib likes working with NumPy. And we're going to\npass in the class names so that we get labels for each of our rows and columns. Class names,\nthis is just a list of our text based class names. And then I'm going to set the fig size\nto my favorite hand and poker, which is 10, seven. Also happens to be a good dimension for\nGoogle Colab. Look at that. Oh, that is something beautiful to see. Now a confusion matrix. The\nideal confusion matrix will have all of the diagonal rows darkened with all of the values\nand no values here, no values here. Because that means that the predicted label lines up with the\ntrue label. So in our case, we have definitely a very dark diagonal here. But let's dive into\nsome of the highest numbers here. It looks like our model is predicting shirt when the true label\nis actually t shirt slash top. So that is reflective of what we saw before. Do we still have that\nimage there? Okay, we don't have an image there. But in a previous video, we saw that when we plotted\nour predictions, the model predicted t shirt slash top when it was actually a shirt. And of course,\nvice versa. So what's another one here? Looks like our model is predicting shirt when it's\nactually a coat. And now this is something that you can use to visually inspect your data to see\nif the the errors that your model is making make sense from a visual perspective. So it's getting\nconfused by predicting pull over when the actual label is coat, predicting pull over when the\nactual label is shirt. So a lot of these things clothing wise and data wise may in fact look\nquite the same. Here's a relatively large one as well. It's predicting sneaker when it should be\nan ankle boot. So it's confusing two different types of shoes there. So this is just a way to\nfurther evaluate your model and start to go. Hmm, maybe our labels are a little bit confusing.\nCould we expand them a little bit more? So keep that in mind, a confusion matrix is one of the\nmost powerful ways to visualize your classification model predictions. And a really, really, really\nhelpful way of creating one is to use torch metrics confusion matrix. And to plot it,\nyou can use plot confusion matrix from ML extend. However, if you're using Google Colab for these,\nyou may need to import them or install them. So that's a confusion matrix. If you'd like\nmore classification metrics, you've got them here. And you've got, of course, more in torch\nmetrics. So give that a look. I think in the next video, we've done a fair bit of evaluation.\nWhere are we up to in our workflow? I believe it's time we saved and loaded our best trained model.\nSo let's give that a go. I'll see you in the next video.\nIn the last video, we created a beautiful confusion matrix with the power of torch metrics\nand ML extend. But now it's time to save and load our best model. Because if we, if we evaluated it,\nour convolutional neural network and go, you know what, this model is pretty good. Let's export\nit to a file so we can use it somewhere else. Let's see how we do that. And by the way, if we go into\nour keynote, we've got a value at model torch metrics. We've been through this a fair few times\nnow. We've improved through experimentation. We haven't used tensor board yet, but that'll be\nin a later video and save and reload your trained model. So here's where we're up to. If we've gone\nthrough all these steps enough times and we're like, you know what, let's save our model so we\ncan use it elsewhere. And we can reload it in to make sure that it's, it's saved correctly.\nLet's go through with this step. We want number 11. We're going to go save and load\nbest performing model. You may have already done this before. So if you've been through the other\nparts of the course, you definitely have. So if you want to give that a go, pause the video now\nand try it out yourself. I believe we did it in notebook number one. We have here we go,\nsaving and loading a pie torch model. You can go through this section of section number one\non your own and see if you can do it. Otherwise, let's code it out together. So I'm going to start\nfrom with importing path from path lib, because I like to create a model directory path.\nSo create model directory path. So my model path is going to be set equal to path. And I'm going\nto save it to models. This is where I want to, I want to create file over here called models\nand save my models to their model path dot MKD for make directory parents. Yes, I wanted to make\nthe parent directories if they don't exist and exist. Okay. Also equals true. So if we try to\ncreate it, but it's already existing, we're not going to get an error. That's fine. And next,\nwe're going to create a model save path. Just going to add some code cells here. So we have\nmore space. Let's pass in here a model name. Going to set this equal to, since we're on section three,\nI'm going to call this O three pie torch, computer vision, model two is our best model. And I'm going\nto save it to PTH for pie torch. You can also save it to dot PT. I like to use PTH. And we're\ngoing to go model save path equal model path slash model name. So now if we have a look at this,\nwe're going to have a path called model save path. But it's going to be a POSIX path in models\nO three pie torch computer vision, model two dot PTH. And if we have a look over here,\nwe should have, yeah, we have a models directory now. That's not going to have anything in it at\nthe moment. We've got our data directory that we had before there's fashion MNIST. This is a good\nway to start setting up your directories, break them down data models, helper function files,\netc. But let's keep going. Let's save, save the model state dict. We're going to go print,\nsaving model to just going to give us some information about what's happening. Model save\npath. And we can save a model by calling torch dot save. And we pass in the object that we want\nto save using the object parameter, OBJ. When we get a doc string there, we're going to go model\ntwo, we want to save the state dict, recall that the state dict is going to be our models what\nour models learned parameters on the data set, so that all the weights and biases and all that\nsort of jazz. Beautiful. So when we first created model two, these were all random numbers. They've\nbeen or since we trained model two on our training data, these have all been updated to represent\nthe training images. And we can leverage these later on, as you've seen before, to make predictions.\nSo I'm not going to go through all those, but that's what we're saving. And the file path is\ngoing to be our model save path. So let's run this and see what happens. Beautiful. We're saving our\nmodel to our model directory. And now let's have a look in here. Do we have a model? Yes, we do.\nBeautiful. So that's how quickly we can save a model. Of course, you can customize what the name is,\nwhere you save it, et cetera, et cetera. Now, let's see what happens when we load it in.\nSo create a new instance, because we only saved the state dict of model two,\nwe need to create a new instance of our model two, or how it was created, which was with\nour class fashion MNIST V two. If we saved the whole model, we could just import it to a new\nvariable. But I'll let you read back more on that on the different ways of saving a model in here.\nThere's also a link to the pytorch documentation would highly recommend that. But let's see it in\naction, we need to create a new instance of our fashion MNIST model V two, which is our convolution\nor neural network. So I'm going to set the manual seed. That way when we create a new instance,\nit's instantiated with the same random numbers. So we're going to set up loaded model two,\nequals fashion MNIST V two. And it's important here that we set it up with the same parameters\nas our original saved model. So fashion MNIST V two. Oh, we've got a typo here.\nI'll fashion MNIST model V two. Wonderful. So the input shape is going to be one,\nbecause that is the number of color channels in our test, in our images, test image dot shape.\nDo we still have a test image should be? Oh, well, we've created a different one, but our image size,\nour image shape is 12828 image shape for color channels height width. Then we create it with\nhidden units, we use 10 for hidden units. So we can just set that here. This is important,\nthey just have to otherwise if the shapes aren't the same, what are we going to get? We're going\nto get a shape mismatch error. And our output shape is what is also going to be 10 or\nlength of class names. If you have the class names variable instantiated, that is. So we're\ngoing to load in the saved state dict, the one that we just saved. So we can go loaded model two,\ndot load state dict. And we can pass in torch dot load in here. And the file that we want to load\nor the file path is model save path up here. This is why I like to just save my path variables\nto a variable so that I can just use them later on, instead of re typing out this all the time,\nwhich is definitely prone to errors. So we're going to send the model to the target device.\nLoaded model two dot two device. Beautiful. Let's see what happens here.\nWonderful. So let's now evaluate the loaded model. So evaluate loaded model. The results\nshould be very much the same as our model two results. So model two results.\nSo this is what we're looking for. We want to make sure that our saved model saved these results\npretty closely. Now I say pretty closely because you might find some discrepancies in this lower\nthese lower decimals here, just because of the way files get saved and something gets lost,\net cetera, et cetera. So that's just to do with precision and computing. But as long as the first\nfew numbers are quite similar, well, then we're all gravy. So let's go torch manual seed.\nRemember, evaluating a model is almost as well is just as important as training a model. So this\nis what we're doing. We're making sure our model save correctly. Before we deployed it, if it didn't\nif we deployed it, it didn't save correctly. Well, then we'd get our we would get less than ideal\nresults, wouldn't we? So model equals loaded model two, we're going to use our same\nof our model function, by the way. And of course, we're going to evaluate it on the same test data\nset that we've been using test data loader. And we're going to create a loss function or just\nput in our loss function that we've created before. And our accuracy function is the accuracy\nfunction we've been using throughout this notebook. So now let's check out loaded model two results.\nThey should be quite similar to this one. We're going to make some predictions. And then if we go\ndown, do we have the same numbers? Yes, we do. So we have five, six, eight, two, nine, five, six,\neight, two, nine, wonderful. And three, one, three, five, eight, three, one, three, five, eight,\nbeautiful. It looks like our loaded model gets the same results as our previously trained model\nbefore we even saved it. And if you wanted to check if they were close, you can also use torch\ndot is close, check if model results, if you wanted to check if they were close programmatically,\nthat is, because we just looked at these visually, check if model results are close to each other.\nNow we can go torch is close, we're going to pass in torch dot tensor, we have to turn these\nvalues into a tensor. We're going to go model two results. And we'll compare the model loss.\nHow about we do that? We want to make sure the loss values are the same. Or very close,\nthat is with torch dot is close. Torch dot tensor model. Or we want this one to be loaded model two\nresults. Model loss. Another bracket on the end there. And we'll see how close they are true,\nwonderful. Now, if this doesn't return true, you can also adjust the tolerance levels in here.\nSo we go atal equals, this is going to be the absolute tolerance. So if we do one to the negative\neight, it's saying like, Hey, we need to make sure our results are basically the same up to eight\ndecimal points. That's probably quite low. I would say just make sure they're at least within two.\nBut if you're getting discrepancies here between your saved model and your loaded model, or sorry,\nthis model here, the original one and your loaded model, if they are quite large, so they're like\nmore than a few decimal points off in this column or even here, I'd go back through your code and\nmake sure that your model is saving correctly, make sure you've got random seeds set up. But\nif they're pretty close, like in terms of within three or two decimal places of each other,\nwell, then I'd say that's that's close enough. But you can also adjust the tolerance level here\nto check if your model results are close enough, programmatically. Wow, we have covered a fair bit\nhere. We've gone through this entire workflow for a computer vision problem. Let's in the next\nvideo, I think that's enough code for this section, section three, pytorch computer vision. I've got\nsome exercises and some extra curriculum lined up for you. So let's have a look at those in the\nnext video. I'll see you there. My goodness. Look how much computer vision pytorch code\nwe've written together. We started off right up the top. We looked at the reference notebook and\nthe online book. We checked out computer vision libraries and pytorch, the main one being torch\nvision. Then we got a data set, namely the fashion MNIST data set. There are a bunch more data sets\nthat we could have looked at. And in fact, I'd encourage you to try some out in the torch vision\ndot data sets, use all of the steps that we've done here to try it on another data set. We repaired\nour data loaders. So turned our data into batches. We built a baseline model, which is an important\nstep in machine learning, because the baseline model is usually relatively simple. And it's going\nto serve as a baseline that you're going to try and improve upon through just go back to the keynote\nthrough various experiments. We then made predictions with model zero. We evaluated it.\nWe timed our predictions to see if running our models on the GPU was faster when we learned that\nsometimes a GPU won't necessarily speed up code if it's a relatively small data set because of the\noverheads between copying data from CPU to GPU. We tried a model with non-linearity and we saw that\nit didn't really improve upon our baseline model. But then we brought in the big guns, a convolutional\nneural network, replicating the CNN explainer website. And by gosh, didn't we spend a lot of time\nhere? I'd encourage you as part of your extra curriculum to go through this again and again.\nI still even come back to refer to it too. I referred to it a lot making the materials for this\nvideo section and this code section. So be sure to go back and check out the CNN explainer website\nfor more of what's going on behind the scenes of your CNNs. But we coded one using pure pytorch.\nThat is amazing. We compared our model results across different experiments. We found that our\nconvolutional neural network did the best, although it took a little bit longer to train. And we also\nlearned that the training time values will definitely vary depending on the hardware you're using.\nSo that's just something to keep in mind. We made an evaluated random predictions with our best\nmodel, which is an important step in visualizing, visualizing, visualizing your model's predictions,\nbecause you could get evaluation metrics. But until you start to actually visualize what's going on,\nwell, in my case, that's how I best understand what my model is thinking. We saw a confusion\nmatrix using two different libraries torch metrics and ML extend a great way to evaluate\nyour classification models. And we saw how to save and load the best performing model to file\nand made sure that the results of our saved model weren't too different from the model that\nwe trained within the notebook. So now it is time I'd love for you to practice what\nyou've gone through. This is actually really exciting now because you've gone through an end-to-end\ncomputer vision problem. I've got some exercises prepared. If you go to the learn pytorch.io website\nin section 03, scroll down. You can read through all of this. This is all the materials that we've\njust covered in pure code. There's a lot of pictures in this notebook too that are helpful to learn\nthings what's going on. We have some exercises here. So all of the exercises are focused on\npracticing the code and the sections above. We have two resources. We also have some extra\ncurriculum that I've put together. If you want an in-depth understanding of what's going on\nbehind the scenes in the convolutional neural networks, because we've focused a lot on code,\nI'd highly recommend MIT's induction to deep computer vision lecture. You can spend 10 minutes\nclicking through the different options in the pytorch vision library, torch vision, look up most\ncommon convolutional neural networks in the torch vision model library, and then for a larger number\nof pre-trained pytorch computer vision models, and if you get deeper into computer vision,\nyou're probably going to run into the torch image models library, otherwise known as 10,\nbut I'm going to leave that as extra curriculum. I'm going to just link this exercises section\nhere. Again, it's at learn pytorch.io in the exercises section. We come down. There we go.\nBut there is also resource here, an exercise template notebook. So we've got one, what are\nthree areas in industry where computer vision is being currently used. Now this is in the\npytorch deep learning repo, extras exercises number three. I've put out some template code here\nfor you to fill in these different sections. So some of them are code related. Some of them\nare just text based, but they should all be able to be completed by referencing what we've gone\nthrough in this notebook here. And just as one more, if we go back to pytorch deep learning,\nthis will probably be updated by the time you get here, you can always find the exercise in\nextra curriculum by going computer vision, go to exercise in extra curriculum, or if we go into\nthe extras file, and then we go to solutions. I've now also started to add video walkthroughs\nof each of the solutions. So this is me going through each of the exercises myself and coding\nthem. And so you'll get to see the unedited videos. So they're just one long live stream.\nAnd I've done some for O2, O3, and O4, and there will be more here by the time you watch this video.\nBut if you'd like to see how I figure out the solutions to the exercises, you can watch those\nvideos and go through them yourself. But first and foremost, I would highly recommend trying out\nthe exercises on your own first. And then if you get stuck, refer to the notebook here,\nrefer to the pytorch documentation. And finally, you can check out what I would have coded as a\npotential solution. So there's number three, computer vision, exercise solutions. So congratulations\non going through the pytorch computer vision section. I'll see you in the next section. We're\ngoing to look at pytorch custom data sets, but no spoilers. I'll see you soon.\nHello, hello, hello, and welcome to section number four of the Learn pytorch for deep learning course.\nWe have custom data sets with pytorch. Now, before we dive into what we're going to cover,\nlet's answer the most important question. Where can you get help? Now, we've been through this\na few times now, but it's important to reiterate. Follow along with the code as best you can. We're\ngoing to be writing a bunch of pytorch code. Remember the motto, if and out, run the code.\nThat's in line with try it for yourself. If you'd like to read or read the doxtring,\nyou can press shift command plus space in Google Colab. Or if you're on Windows, command might\nbe control. Then if you're still stuck, you can search for it. Two of the resources you will\nprobably come across is stack overflow or the wonderful pytorch documentation, which we've\nhad a lot of experience with so far. Then, of course, try again, go back through your code,\nif and out, code it out, or if and out, run the code. And then finally, if you're still stuck,\nask a question on the pytorch deep learning discussions GitHub page. So if I click this link,\nwe come to Mr. D Burke slash pytorch deep learning, the URL is here. We've seen this before. If you\nhave a trouble or a problem with any of the course, you can start a discussion and you can\nselect the category, general ideas, polls, Q and A, and then we can go here, video,\nput the video number in. So 99, for example, my code doesn't do what I'd like it to. So say\nyour problem and then come in here, write some code here, code here, and then my question is\nsomething, something, something, click start discussion, and then we can help out. And then if\nwe come back to the discussions, of course, you can search for what's going on. So if you have an\nerror and you feel like someone else might have seen this error, you can, of course, search it\nand find out what's happening. Now, I just want to highlight again, the resources for this course\nare at learn pytorch.io. We are up to section four. This is a beautiful online book version of\nall the materials we are going to cover in this section. So spoiler alert, you can use this as a\nreference. And then, of course, in the GitHub, we have the same notebook here, pytorch custom\ndata sets. This is the ground truth notebook. So check that out if you get stuck. So I'm just\ngoing to exit out of this. We've got pytorch custom data sets at learn pytorch.io. And then,\nof course, the discussions tab for the Q&A. Now, if we jump back to the keynote, what do we have?\nWe might be asking, what is a custom data set? Now, we've built a fair few pytorch deeplining\nneural networks so far on various data sets, such as fashion MNIST. But you might be wondering,\nhey, I've got my own data set, or I'm working on my own problem. Can I build a model with pytorch\nto predict on that data set? And the answer is yes. However, you do have to go through a few\npre processing steps to make that data set compatible with pytorch. And that's what we're\ngoing to be covering in this section. And so I'd like to highlight the pytorch domain libraries.\nNow, we've had a little bit of experience before with torch vision, such as if we wanted to classify\nwhether a photo was a pizza, steak, or sushi. So a computer vision image classification problem.\nNow, there's also text, such as if these reviews are positive or negative. And you can use torch\ntext for that. But again, these are only just one problem within the vision space within the text\nspace. I want you to just understand that if you have any type of vision data, you probably\nwant to look into torch vision. And if you have any kind of text data, you probably want to look\ninto torch text. And then if you have audio, such as if you wanted to classify what song was playing,\nthis is what Shazam does, it uses the input sound of some sort of music, and then runs a neural network\nover it to classify it to a certain song, you can look into torch audio for that. And then if you'd\nlike to recommend something such as you have an online store, or if your Netflix or something\nlike that, and you'd like to have a homepage that updates for recommendations, you'd like to look\ninto torch rec, which stands for recommendation system. And so this is just something to keep in mind.\nBecause each of these domain libraries has a data sets module that helps you work with different\ndata sets from different domains. And so different domain libraries contain data loading functions\nfor different data sources. So torch vision, let's just go into the next slide, we have problem space\nvision for pre built data sets, so existing data sets like we've seen with fashion MNIST,\nas well as functions to load your own vision data sets, you want to look into torch vision\ndot data sets. So if we click on this, we have built in data sets, this is the pie torch documentation.\nAnd if we go here, we have torch audio, torch text, torch vision, torch rec, torch data. Now,\nat the time of recording, which is April 2022, this is torch data is currently in beta. But it's\ngoing to be updated over time. So just keep this in mind, updated over time to add even more ways\nto load different data resources. But for now, we're just going to get familiar with torch vision\ndata sets. If we went into torch text, there's another torch text dot data sets. And then if we\nwent into torch audio, we have torch audio dot data sets. And so you're noticing a trend here\nthat depending on the domain you're working in, whether it be vision, text, audio, or your data\nis recommendation data, you'll probably want to look into its custom library within pie torch.\nAnd of course, the bonus is torch data. It contains many different helper functions for loading data,\nand is currently in beta as of April 2022. So 2022. So the by the time you watch this torch data\nmay be out of beta. And then that should be something that's extra curriculum on top of what we're\ngoing to cover in this section. So let's keep going. So this is what we're going to work towards\nbuilding food vision mini. So we're going to load some data, namely some images of pizza,\nsushi, and steak from the food 101 data set, we're going to build an image classification model,\nsuch as the model that might power a food vision recognition app or a food image recognition app.\nAnd then we're going to see if it can classify an image of pizza as pizza, an image of sushi as sushi,\nand an image of steak as steak. So this is what we're going to focus on. We want to load,\nsay we had images existing already of pizza, sushi, and steak, we want to write some code\nto load these images of food. So our own custom data set for building this food vision mini model,\nwhich is quite similar to if you go to this is the project I'm working on personally,\nneutrify.app. This is a food image recognition model. Here we go. So it's still a work in progress as\nI'm going through it, but you can upload an image of food and neutrify will try to classify\nwhat type of food it is. So do we have steak? There we go. Let's upload that. Beautiful steak.\nSo we're going to be building a similar model to what powers neutrify. And then there's the\nmacro nutrients for the steak. If you'd like to find out how it works, I've got all the links here,\nbut that's at neutrify.app. So let's keep pushing forward. We'll go back to the keynote.\nThis is what we're working towards. As I said, we want to load these images into PyTorch so that\nwe can build a model. We've already built a computer vision model. So we want to figure out\nhow do we get our own data into that computer vision model. And so of course we'll be adhering\nto our PyTorch workflow that we've used a few times now. So we're going to learn how to load a\ndata set with our own custom data rather than an existing data set within PyTorch. We'll see how\nwe can build a model to fit our own custom data set. We'll go through all the steps that's involved\nin training a model such as picking a loss function and an optimizer. We'll build a training loop.\nWe'll evaluate our model. We'll improve through experimentation. And then we can see save and reloading\nour model. But we're also going to practice predicting on our own custom data, which is a very,\nvery important step whenever training your own models. So what we're going to cover broadly,\nwe're going to get a custom data set with PyTorch. As we said, we're going to become one with the\ndata. In other words, preparing and visualizing it. We'll learn how to transform data for use with\na model, very important step. We'll see how we can load custom data with pre-built functions\nand our own custom functions. We'll build a computer vision model, aka food vision mini,\nto classify pizza, steak, and sushi images. So a multi-class classification model. We'll compare\nmodels with and without data augmentation. We haven't covered that yet, but we will later on.\nAnd finally, we'll see how we can, as I said, make predictions on custom data. So this means\ndata that's not within our training or our test data set. And how are we going to do it? Well,\nwe could do it cooks or chemists. But I like to treat machine learning as a little bit of an art,\nso we're going to be cooking up lots of code. With that being said, I'll see you in Google Colab.\nLet's code. Welcome back to the PyTorch cooking show. Let's now learn how we can cook up some\ncustom data sets. I'm going to jump into Google Colab. So colab.research.google.com.\nAnd I'm going to click new notebook. I'm just going to make sure this is zoomed in enough for\nthe video. Wonderful. So I'm going to rename this notebook 04 because we're up to section 04.\nAnd I'm going to call it PyTorch custom data sets underscore video because this is going to be one\nof the video notebooks, which has all the code that I write during the videos, which is of course\ncontained within the video notebooks folder on the PyTorch deep learning repo. So if you'd like\nthe resource or the ground truth notebook for this, I'm going to just put a heading here.\n04 PyTorch custom data sets video notebook, make that bigger, and then put resources.\nSo book version of the course materials for 04. We'll go there, and then we'll go ground truth\nversion of notebook 04, which will be the reference notebook that we're going to use\nfor this section. Come into PyTorch custom data sets. And then we can put that in there.\nWonderful. So the whole synopsis of this custom data sets section is we've used some data sets\nwith PyTorch before, but how do you get your own data into PyTorch? Because that's what you\nwant to start working on, right? You want to start working on problems of your own. You want to\ncome into any sort of data that you've never worked with before, and you want to figure out how do\nyou get that into PyTorch. So one of the ways to do so is via custom data sets. And then I want\nto put a note down here. So we're going to go zero section zero is going to be importing\nPyTorch and setting up device agnostic code. But I want to just stress here that domain libraries.\nSo just to reiterate what we went through last video. So depending on what you're working on,\nwhether it be vision, text, audio, recommendation, something like that, you'll want to look into\neach of the PyTorch domain libraries for existing data loader or data loading functions and\ncustomizable data loading functions. So just keep that in mind. We've seen some of them. So if we\ngo torch vision, which is what we're going to be looking at, torch vision, we've got data sets,\nand we've got documentation, we've got data sets for each of the other domain libraries here as\nwell. So if you're working on a text problem, it's going to be a similar set of steps to what\nwe're going to do with our vision problem when we build food vision mini. What we have is a data\nset that exists somewhere. And what we want to do is bring that into PyTorch so we can build a\nmodel with it. So let's import the libraries that we need. So we're going to import torch and\nwe'll probably import an N. So we'll import that from PyTorch. And I'm just going to check the\ntorch version here. So note, we need PyTorch 1.10.0 plus is required for this course. So if you're\nusing Google Colab at a later date, you may have a later version of PyTorch. I'm just going to\nshow you what version I'm using. Just going to let this load. We're going to get this ready.\nWe're going to also set up device agnostic code right from the start this time because this is\nbest practice with PyTorch. So this way, if we have a CUDA device available, our model is going\nto use that CUDA device. And our data is going to be on that CUDA device. So there we go. Wonderful.\nWe've got PyTorch 1.10.0 plus CUDA. 111. Maybe that's 11.1. So let's check if CUDA.is available.\nNow, I'm using Google Colab. We haven't set up a GPU yet. So it probably won't be available yet.\nLet's have a look. Wonderful. So because we've started a new Colab instance, it's going to use\nthe CPU by default. So how do we change that? We come up to runtime, change runtime type. I'm going\nto go hard there accelerator GPU. We've done this a few times now. I am paying for Google Colab Pro.\nSo one of the benefits of that is that it our Google Colab reserves faster GPUs for you. You do\ndon't need Google Colab Pro. As I've said to complete this course, you can use the free version,\nbut just recall Google Colab Pro tends to give you a better GPU just because GPUs aren't free.\nWonderful. So now we've got access to a GPU CUDA. What GPU do I have?\nNvidia SMI. I have a Tesla P100 with 16 gigabytes of memory, which will be more than enough for\nthe problem that we're going to work on in this video. So I believe that's enough to cover for\nthe first coding video. Let's in the next section, we are working with custom datasets after all.\nLet's in the next video. Let's get some data, hey.\nNow, as I said in the last video, we can't cover custom datasets without some data. So let's get\nsome data and just remind ourselves what we're going to build. And that is food vision mini.\nSo we need a way of getting some food images. And if we go back to Google Chrome,\ntorch vision datasets has plenty of built-in datasets. And one of them is the food 101 dataset.\nFood 101. So if we go in here, this is going to take us to the original food 101 website.\nSo food 101 is 101 different classes of food. It has a challenging dataset of 101 different\nfood categories with 101,000 images. So that's a quite a beefy dataset. And so for each class,\n250 manually reviewed test images are provided. So we have per class, 101 classes, 250 testing\nimages, and we have 750 training images. Now, we could start working on this entire dataset\nstraight from the get go. But to practice, I've created a smaller subset of this dataset,\nand I'd encourage you to do the same with your own problems. Start small and upgrade when necessary.\nSo I've reduced the number of categories to three and the number of images to 10%.\nNow, you could reduce this to an arbitrary amount, but I've just decided three is enough to begin with\nand 10% of the data. And then if it works, hey, you could upscale that on your own accord.\nAnd so I just want to show you the notebook that I use to create this dataset and as extra curriculum,\nyou could go through this notebook. So if we go into extras, 04 custom data creation,\nthis is just how I created the subset of data. So making a dataset to use with notebook number\nfour, I created it in custom image data set or image classification style. So we have a top level\nfolder of pizza, steak, and sushi. We have a training directory with pizza, steak, and sushi\nimages. And we have a test directory with pizza, steak, and sushi images as well. So you can go\nthrough that to check it out how it was made. But now, oh, and also, if you go to loan pytorch.io\nsection four, there's more information here about what food 101 is. So get data. Here we go.\nThere's all the information about food 101. There's some resources, the original food 101 data set,\ntorch vision data sets, food 101, how I created this data set, and actually downloading the data.\nBut now we're going to write some code, because this data set, the smaller version that I've created\nis on the pytorch deep learning repo, under data. And then we have pizza, steak, sushi.zip.\nOh, this one is a little spoiler for one of the exercises for this section. But you'll see that\nlater. Let's go in here. Let's now write some code to get this data set from GitHub,\npizza, steak, sushi.zip. And then we'll explore it, we'll become one with the data.\nSo I just want to write down here, our data set is a subset of the food 101 data set.\nFood 101 starts with 101 different classes of food. So we could definitely build computer\nvision models for 101 classes, but we're going to start smaller. Our data set starts with three\nclasses of food, and only 10% of the images. So what's right here? And 1000 images per class,\nwhich is 750 training, 250 testing. And we have about 75 training images per class,\nand about 25 testing images per class. So why do this? When starting out ML projects,\nit's important to try things on a small scale and then increase the scale when necessary.\nThe whole point is to speed up how fast you can experiment.\nBecause there's no point trying to experiment on things that if we try to train on 100,000\nimages to begin with, our models might train take half an hour to train at a time. So at the\nbeginning, we want to increase the rate that we experiment at. And so let's get some data.\nWe're going to import requests so that we can request something from GitHub to download this\nURL here. Then we're also going to import zip file from Python, because our data is in the form\nof a zip file right now. Then we're going to get path lib, because I like to use paths whenever\nI'm dealing with file paths or directory paths. So now let's set up a path to a data folder.\nAnd this, of course, will depend on where your data set lives, what you'd like to do. But I\ntypically like to create a folder over here called data. And that's just going to store all of my\ndata for whatever project I'm working on. So data path equals path data. And then we're going to go\nimage path equals data path slash pizza steak sushi. That's how we're going to have images\nfrom those three classes. Pizza steak and sushi are three of the classes out of the 101 in food\n101. So if the image folder doesn't exist, so if our data folder already exists, we don't want to\nredownload it. But if it doesn't exist, we want to download it and unzip it. So if image path\nis der, so we want to print out the image path directory already exists skipping download.\nAnd then if it doesn't exist, we want to print image path does not exist, creating one. Beautiful.\nAnd so we're going to go image path dot mk der to make a directory. We want to make its parents\nif we need to. So the parent directories and we want to pass exist, okay, equals true. So we don't\nget any errors if it already exists. And so then we can write some code. I just want to show you\nwhat this does if we run it. So our target directory data slash pizza steak sushi does not exist.\nIt's creating one. So then we have now data and inside pizza steak sushi. Wonderful. But we're\ngoing to fill this up with some images so that we have some data to work with. And then the whole\npremise of this entire section will be loading this data of just images into PyTorch so that we\ncan build a computer vision model on it. But I just want to stress that this step will be very\nsimilar no matter what data you're working with. You'll have some folder over here or maybe it'll\nlive on the cloud somewhere. Who knows wherever your data is, but you'll want to write code to\nload it from here into PyTorch. So let's download pizza steak and sushi data. So I'm going to use\nwidth. I'll just X over here. So we have more screen space with open. I'm going to open the data\npath slash the file name that I'm trying to open, which will be pizza steak sushi dot zip. And I'm\ngoing to write binary as F. So this is essentially saying I'm doing this in advance because I know\nI'm going to download this folder here. So I know the the file name of it, pizza steak sushi dot zip.\nI'm going to download that into Google collab and I want to open it up. So request equals request\ndot get. And so when I want to get this file, I can click here. And then if I click download,\nit's going to what do you think it's going to do? Well, let's see. If I wanted to download it\nlocally, I could do that. And then I could come over here. And then I could click upload if I\nwanted to. So upload the session storage. I could upload it from that. But I prefer to write code\nso that I could just run this cell over again and have the file instead of being download to\nmy local computer. It just goes straight into Google collab. So to do that, we need the URL\nfrom here. And I'm just going to put that in there. It needs to be as a string.\nExcuse me. I'm getting trigger happy on the shift and enter. Wonderful. So now I've got a request\nto get the content that's in here. And GitHub can't really show this because this is a zip file\nof images, spoiler alert. Now let's keep going. We're going to print out that we're downloading\npizza, stake and sushi data dot dot dot. And then I'm going to write to file the request dot content.\nSo the content of the request that I just made to GitHub. So that's request is here.\nUsing the Python request library to get the information here from GitHub. This URL could be\nwherever your file has been stored. And then I'm going to write the content of that request\nto my target file, which is this. This here. So if I just copy this, I'm going to write the data\nto here data path slash pizza, stake sushi zip. And then because it's a zip file, I want to unzip it.\nSo unzip pizza, stake sushi data. Let's go with zip file. So we imported zip file up there,\nwhich is a Python library to help us deal with zip files. We're going to use zip file dot zip\nfile. We're going to pass it in the data path. So just the path that we did below,\ndata path slash pizza, stake sushi dot zip. And this time, instead of giving it right permissions,\nso that's what wb stands for, stands for right binary. I'm going to give it read permissions.\nSo I want to read this target file instead of writing it. And I'm going to go as zip ref.\nWe can call this anything really, but zip ref is kind of, you'll see this a lot in\ndifferent Python examples. So we're going to print out again. So unzipping pizza, stake,\nand sushi data. Then we're going to go zip underscore ref dot extract all. And we're going to go image\npath. So what this means is it's taking the zip ref here. And it's extracting all of the\ninformation that's within that zip ref. So within this zip file, to the image path,\nwhich is what we created up here. So if we have a look at image path, let's see that.\nImage path. Wonderful. So that's where all of the contents of that zip file are going to go\ninto this file. So let's see it in action. You're ready. Hopefully it works. Three, two, one, run.\nFile is not a zip file. Oh, no, what do we get wrong? So did I type this wrong?\nGot zip data path. Oh, we got the zip file here. Pizza, stake, sushi, zip, read data path.\nOkay, I found the error. So this is another thing that you'll have to keep in mind.\nAnd I believe we've covered this before, but I like to keep the errors in these videos so that\nyou can see where I get things wrong, because you never write code right the first time.\nSo we have this link in GitHub. We have to make sure that we have the raw link address. So if I\ncome down to here and copy the link address from the download button, you'll notice a slight\ndifference if we come back into here. So I'm just going to copy that there. So if we step\nthrough this GitHub, Mr. D Burke pytorch deep learning, we have raw instead of blob. So that\nis why we've had an error is that our code is correct. It's just downloading the wrong data.\nSo let's change this to the raw. So just keep that in mind, you must have raw here.\nAnd so let's see if this works.\nDo we have the correct data? Oh, we might have to delete this. Oh, there we go.\nTest. Beautiful. Train. Pizza steak sushi. Wonderful. So it looks like we've got some data. And if we\nopen this up, what do we have? We have various JPEGs. Okay. So this is our testing data. And if\nwe click on there, we've got an image of pizza. Beautiful. So we're going to explore this a\nlittle bit more in the next video. But that is some code that we've written to download data sets\nor download our own custom data set. Now, just recall that we are working specifically on a pizza\nsteak and sushi problem for computer vision. However, our whole premise is that we have some\ncustom data. And we want to convert these. How do we get these into tenses? That's what we want\nto do. And so the same process will be for your own problems. We'll be loading a target data set\nand then writing code to convert whatever the format the data set is in into tenses for PyTorch.\nSo I'll see you in the next video. Let's explore the data we've downloaded.\nWelcome back. In the last video, we wrote some code to download a target data set, our own custom\ndata set from the PyTorch deep learning data directory. And if you'd like to see how that\ndata set was made, you can go to PyTorch deep learning slash extras. It's going to be in the\ncustom data creation notebook here for 04. So I've got all the code there. All we've done is take\ndata from the food 101 data set, which you can download from this website here, or from torch\nvision. So if we go to torch vision, food 101. We've got the data set built into PyTorch there.\nSo I've used that data set from PyTorch and broken it down from 101 classes to three classes so that\nwe can start with a small experiment. So there we go. Get the training data, data sets food 101,\nand then I've customized it to be my own style. So if we go back to CoLab, we've now got\npizza steak sushi, a test folder, which will be our testing images, and a train folder,\nwhich will be our training images. This data is in standard image classification format. But we'll\ncover that in a second. All we're going to do in this video is kick off section number two,\nwhich is becoming one with the data, which is one of my favorite ways to refer to data preparation\nand data exploration. So we're coming one with the data. And I'd just like to show you one of my\nfavorite quotes from Abraham loss function. So if I had eight hours to build a machine learning model,\nI'd spend the first six hours preparing my data set. And that's what we're going to do. Abraham\nloss function sounds like he knows what is going on. But since we've just downloaded some data,\nlet's explore it. Hey, and we'll write some code now to walk through each of the directories. How\nyou explore your data will depend on what data you've got. So we've got a fair few different\ndirectories here with a fair few different folders within them. So how about we walk through each\nof these directories and see what's going on. If you have visual data, you probably want to\nvisualize an image. So we're going to do that in the second two, write a little doc string for\nthis helper function. So walks through the path, returning its contents. Now, just in case you didn't\nknow Abraham loss function does not exist as far as I know. But I did make up that quote. So we're\ngoing to use the OS dot walk function, OS dot walk. And we're going to pass it in a dirt path. And\nwhat does walk do? We can get the doc string here. Directory tree generator. For each directory\nin the directory tree rooted at the top, including top itself, but in excluding dot and dot dot,\nyields a three tuple, derpath, der names, and file names. You can step through this in the Python\ndocumentation, if you'd like. But essentially, it's just going to go through our target directory,\nwhich in this case will be this one here. And walk through each of these directories printing out\nsome information about each one. So let's see that in action. This is one of my favorite things to do\nif we're working with standard image classification format data. So there are lane, length,\nder names, directories. And let's go land, land, file names. We say at length, like I've got the\nG on the end, but it's just land images in, let's put in here, derpath. So a little bit confusing\nif you've never used walk before, but it's so exciting to see all of the information in all\nof your directories. Oh, we didn't read and run it. Let's check out function now walk through der.\nAnd we're going to pass it in the image path, which is what? Well, it's going to show us.\nHow beautiful. So let's compare what we've got in our printout here. There are two directories\nand zero images in data, pizza, steak sushi. So this one here, there's zero images, but there's\ntwo directories test and train wonderful. And there are three directories in data, pizza, steak, sushi,\ntest. Yes, that looks correct. Three directories, pizza, steak, sushi. And then we have zero\ndirectories and 19 images in pizza, steak, sushi, slash test, steak. We have a look at this. So that\nmeans there's 19 testing images for steak. Let's have a look at one of them. There we go. Now,\nagain, these are from the food 101 data set, the original food 101 data set, which is just a whole\nbunch of images of food, 100,000 of them. There's some steak there. Wonderful. And we're trying to\nbuild a food vision model to recognize what is in each image. Then if we jump down to here,\nwe have three directories in the training directory. So we have pizza, steak, sushi. And then we have\n75 steak images, 72 sushi images and 78 pizza. So slightly different, but very much the same\nnumbers. They're not too far off each other. So we've got about 75 or so training images,\nand we've got about 25 or so testing images per class. Now these were just randomly selected\nfrom the food 101 data set 10% of three different classes. So let's keep pushing forward. And we're\ngoing to set up our training and test parts. So I just want to show you, we'll just set up this,\nand then I'll just show you the standard image classification setup, image path.train. And we're\ngoing to go tester. So if you're working on image classification problem, we want to set this up\nas test. And then if we print out the trainer and the tester, this is what we're going to be\ntrying to do. We're going to write some code to go, Hey, look at this path for our training images.\nAnd look at this path for our testing images. And so this is the standard image classification\ndata format is that you have your overall data set folder. And then you have a training folder\ndedicated to all of the training images that you might have. And then you have a testing folder\ndedicated to all of the testing images that you might have. And you could have a validation\ndata set here as well if you wanted to. But to label each one of these images, the class name\nis the folder name. So all of the pizza images live in the pizza directory, the same for steak,\nand the same for sushi. So depending on your problem, your own data format will depend on\nwhatever you're working on, you might have folders of different text files or folders of\ndifferent audio files. But the premise remains, we're going to be writing code to get our data here\ninto tenses for use with PyTorch. And so where does this come from? This image data classification\nformat. Well, if we go to the torch vision dot data sets documentation, as you start to work\nwith more data sets, you'll start to realize that there are standardized ways of storing\nspecific types of data. So if we come down to here, base classes for custom data sets,\nwe'll be working towards using this image folder data set. But this is a generic data\nloader where the images are arranged in this way by default. So I've specifically formatted our data\nto mimic the style that this pre built data loading function is for. So we've got a root directory\nhere in case of we were classifying dog and cat images, we have root, then we have a dog folder,\nthen we have various images. And the same thing for cat, this would be dog versus cat. But the only\ndifference for us is that we have food images, and we have pizza steak sushi. If we wanted to use the\nentire food 101 data set, we would have 101 different folders of images here, which is totally\npossible. But to begin with, we're keeping things small. So let's keep pushing forward. As I said,\nwe're dealing with a computer vision problem. So what's another way to explore our data,\nother than just walking through the directories themselves. Let's visualize an image, hey? But\nwe've done that before with just clicking on the file. How about we write some code to do so.\nWe'll replicate this but with code. I'll see you in the next video.\nWelcome back. In the last video, we started to become one with the data. And we learned that we\nhave about 75 images per training class and about 25 images per testing class. And we also learned\nthat the standard image classification data structure is to have the steak images within the steak\nfolder of the training data set and the same for test, and the pizza images within the pizza\nfolder, and so on for each different image classification class that we might have.\nSo if you want to create your own data set, you might format it in such a way that your training\nimages are living in a directory with their classification name. So if you wanted to classify\nphotos of dogs and cats, you might create a training folder of train slash dog train slash\ncat, put images of dogs in the dog folder, images of cats in the cat folder, and then the same for\nthe testing data set. But the premise remains, I'm going to sound like a broken record here.\nWe want to get our data from these files, whatever files they may be in, whatever data structure\nthey might be in, into tenses. But before we do that, let's keep becoming one with the data.\nAnd we're going to visualize an image. So visualizing an image, and you know how much I love randomness.\nSo let's select a random image from all of the files that we have in here. And let's plot it,\nhey, because we could just click through them and visualize them. But I like to do things with\ncode. So specifically, let's let's plan this out. Let's write some code to number one is get all\nof the image paths. We'll see how we can do that with the path path lib library. We then want to\npick a random image path using we can use Python's random for that. Python's random dot choice will\npick a single image random dot choice. Then we want to get the image class name. And this is where\npart lib comes in handy. Class name, recall that whichever target image we pick, the class name will\nbe whichever directory that it's in. So in the case of if we picked a random image from this directory,\nthe class name would be pizza. So we can do that using, I think it's going to be path lib dot path.\nAnd then we'll get the parent folder, wherever that image lives. So the parent image parent\nfolder that parent directory of our target random image. And we're going to get the stem of that.\nSo we have stem, stem is the last little bit here. Number four, what should we do? Well,\nwe want to open the image. So since we're working with images, let's open the image\nwith Python's pill, which is Python image library, but we'll actually be pillow. So if we go Python\npillow, a little bit confusing when I started to learn about Python image manipulation. So pillow\nis a friendly pill for, but it's still called pill. So just think of pillow as a way to process\nimages with Python. So pill is the Python imaging library by Frederick Lund. And so Alex Clark and\ncontributors have created pillow. So thank you, everyone. And let's go to number five. What do\nwe want to do as well? We want to, yeah, let's get some metadata about the image. We'll then show\nthe image and print metadata. Wonderful. So let's import random, because machine learning is all\nabout harnessing the power of randomness. And I like to use randomness to explore data as well\nas model it. So let's set the seed. So we get the same image on both of our ends. So random dot seed.\nI'm going to use 42. You can use whatever you'd like. But if you'd like to get the same image as me,\nI'd suggest using 42 as well. Now let's get all the image paths. So we can do this because our image\npath list, we want to get our image path. So recall that our image path\nis this. So this folder here, I'm just going to close all this. So this is our image path,\nthis folder here, you can also go copy path if you wanted to, we're just going to get something\nvery similar there. That's going to error out. So I'll just comment that. So it doesn't error.\nThat's our path. But we're going to keep it in the POSIX path format. And we can go list. Let's\ncreate a list of image path dot glob, which stands for grab. I don't actually know what glob stands\nfor. But to me, it's like glob together. All of the images that are all of the files that suit\na certain pattern. So glob together for me means stick them all together. And you might be able\nto correct me if I've got the wrong meaning there. I'd appreciate that. And so we're going to pass\nin a certain combination. So we want star slash star. And then we want star dot jpg. Now why are\nwe doing this? Well, because we want every image path. So star is going to be this first\ndirectory here. So any combination, it can be train or test. And then this star means anything for\nwhat's inside tests. And let's say this first star is equal to test. This second star is equal to\nanything here. So it could be any of pizza, steak or sushi. And then finally, this star,\nlet's say it was test pizza. This star is anything in here. And that is before dot jpg.\nSo it could be any one of these files here. Now this will make more sense once we print it out.\nSo image path list, let's have a look. There we go. So now we've got a list of every single image\nthat's within pizza steak sushi. And this is just another way that I like to visualize data is to\njust get all of the paths and then randomly visualize it, whether it be an image or text or\naudio, you might want to randomly listen to it. Recall that each each of the domain libraries have\ndifferent input and output methods for different data sets. So if we come to torch vision, we have\nutils. So we have different ways to draw on images, reading and writing images and videos. So we\ncould load an image via read image, we could decode it, we could do a whole bunch of things.\nI'll let you explore that as extra curriculum. But now let's select a random image from here\nand plot it. So we'll go number two, which was our step up here, pick a random image. So pick a\nrandom image path. Let's get rid of this. And so we can go random image path equals random\ndot choice, harness the power of randomness to explore our data. Let's get a random image from\nimage path list, and then we'll print out random image path, which one was our lucky image that\nwe selected. Beautiful. So we have a test pizza image is our lucky random image. And\nbecause we've got a random seed, it's going to be the same one each time. Yes, it is.\nAnd if we comment out the random seed, we'll get a different one each time. We've got a stake\nimage. We've got another stake image. Another stake image. Oh, three in a row, four in a row.\nOh, pizza. Okay, let's keep going. So we'll get the image class\nfrom the path name. So the image class is the name of the directory, because our image data is\nin standard image classification format, where the image is stored. So let's do that image class\nequals random image path dot parent dot stem. And then we're going to print image class. What do we\nget? So we've got pizza. Wonderful. So the parent is this folder here. And then the stem is the end\nof that folder, which is pizza. Beautiful. Well, now what are we up to now? We're working with\nimages. Let's open up the image so we can open up the image using pill. We could also open up the\nimage with pytorch here. So with read image, but we're going to use pill to keep things a little\nbit generic for now. So open image, image equals image. So from pill import image, and the image\nclass has an open function. And we're just going to pass it in here, the random image path. Note\nif this is corrupt, if your images corrupt, this may error. So then you could potentially use this\nto clean up your data set. I've imported a lot of images with image dot open of our target data\nset here. I don't believe any of them are corrupt. But if they are, please let me know. And we'll find\nout later on when our model tries to train on it. So let's print some metadata. So when we open our\nimage, we get some information from it. So let's go our random image path is what? Random image path.\nWe're already printing this out, but we'll do it again anyway. And then we're going to go the image\nclass is equal to what will be the image class. Wonderful. And then we can print out, we can get\nsome metadata about our images. So the image height is going to be IMG dot height. We get that\nmetadata from using the pill library. And then we're going to print out image width. And we'll get\nIMG dot width. And then we'll print the image itself. Wonderful. And we can get rid of this,\nand we can get rid of this. Let's now have a look at some random images from our data set.\nLovely. We've got an image of pizza there. Now I will warn you that the downsides of working with\nfood data is it does make you a little bit hungry. So there we've got some sushi. And then we've got\nsome more sushi. Some steak. And we have a steak, we go one more for good luck. And we finish off\nwith some sushi. Oh, that could be a little bit confusing to me. I thought that might be steak\nto begin with. And this is the scene. Now we'll do one more. Why it's important to sort of visualize\nyour images randomly, because you never know what you're going to come across. And this way,\nonce we visualize enough images, you could do this a hundred more times. You could do this\n20 more times until you feel comfortable to go, Hey, I feel like I know enough about the data now.\nLet's see how well our model goes on this sort of data. So I'll finish off on this steak image.\nAnd now I'll set your little challenge before the next video is to visualize an image like we've\ndone here. But this time do it with matplotlib. So try to visualize an image with matplotlib.\nThat's your little challenge before the next video. So give that a go. We want to do a random\nimage as well. So quite a similar set up to this. But instead of printing out things like this,\nwe want to visualize it using matplotlib. So try that out and we'll do it together in the next video.\nOh, we are well on the way to creating our own PyTorch custom data set. We've started to\nbecome one with the data. But now let's continue to visualize another image. I set you the challenge\nin the last video to try and replicate what we've done here with the pill library with matplotlib.\nSo now let's give it a go. Hey, and why use matplotlib? Well, because matplotlib and I'm going to\nimport numpy as well, because we're going to have to convert this image into an array. That was a\nlittle trick that I didn't quite elaborate on. But I hope you tried to decode it out and figure\nit out from the errors you received. But matplotlib is one of the most fundamental data science\nlibraries. So you're going to see it everywhere. So it's just important to be aware of how to plot\nimages and data with matplotlib. So turn the image into an array. So we can go image as array. And\nI'm going to use the numpy method NP as array. We're going to pass it in the image, recall that\nthe image is the same image that we've just set up here. And we've already opened it with pill.\nAnd then I'm going to plot the image. So plot the image with matplotlib. plt.figure.\nAnd then we can go fig size equals 10, seven. And then we're going to go plt.im show image as\narray, pass it in the array of numbers. I'm going to set the title here as an f string. And then\nI'm going to pass in image class, equals image class. Then I'm going to pass in image shape. So\nwe can get the shape here. Now this is another important thing to be aware of of your different\ndatasets when you're exploring them is what is the shape of your data? Because what's one of the\nmain errors in machine learning and deep learning? It's shape mismatch issues. So if we know the\nshape of our data where we can start to go, okay, I kind of understand what shape I need my model\nlayers to be in what what shape I need my other data to be in. And I'm going to turn the axes off\nhere. Beautiful. So look at what we've got. Now I've just thrown this in here without really\nexplaining it. But we've seen this before in the computer vision section. As our image shape is\n512 3063. Now the dimensions here are height is 512 pixels. The width is 306 pixels. And it has\nthree color channels. So what format is this? This is color channels last, which is the default\nfor the pill library. There's also the default for map plot lib. But pytorch recall is default\nif we put the color channels at the start color channels first. Now there is a lot of debate as\nI've said over which is the best order. It looks like it's leading towards going towards this. But\nfor now pytorch defaults to color channels first. But that's okay. Because we can manipulate these\ndimensions to what we need for whatever code that we're writing. And the three color channels is what\nred, green and blue. So if you combine red, green and blue in some way, shape or form,\nyou get the different colors here that represent our image. And so if we have a look at our image\nas a ray. Our image is in numerical format. Wonderful. So okay. We've got one way to do this for\none image. I think we start moving towards scaling this up to do it for every image in our data\nfolder. So let's just finish off this video by visualizing one more image. What do we get? Same\npremise. The image is now as an array, different numerical values. We've got a delicious looking\npizza here of shave 512 512 with color channels last. And we've got the same thing up here. So\nthat is one way to become one with the data is to visualize different images, especially random\nimages. You could do the same thing visualizing different text samples that you're working with\nor listening to different audio samples. It depends what domain you're working in. So now in the\nnext video, let's start working towards turning all of the images in here. Now that we visualize\nsome of them and become one with the data, we've seen that the shapes are varying in terms of\nheight and width. But they all look like they have three color channels because we have color images.\nBut now we want to write code to turn all of these images into pytorch tenses.\nSo let's start moving towards that. I'll see you in the next video.\nHello and welcome back. In the last video, we converted an image to a NumPy array.\nAnd we saw how an image can be represented as an array. But what if we'd like to get this image\nfrom our custom data set over here, pizza steak sushi into pytorch? Well, let's cover that in\nthis video. So I'm going to create a new heading here. And it's going to be transforming data.\nAnd so what we'd like to do here is I've been hinting at the fact the whole time is we want\nto get our data into tensor format, because that is the data type that pytorch accepts.\nSo let's write down here before we can use our image data with pytorch. Now this goes for images,\nother vision data, it goes for text, it goes to audio, basically whatever kind of data set you're\nworking with, you need some way to turn it into tenses. So that's step number one. Turn your target\ndata into tenses. In our case, it's going to be a numerical representation of our images.\nAnd number two is turn it into a torch dot utils dot data dot data set. So recall from a previous\nvideo that we've used the data set to house all of our data in tensor format. And then subsequently,\nwe've turned our data sets, our pytorch data sets into torch dot utils dot data dot data loader.\nAnd a data loader creates an iterable or a batched version of our data set. So for short, we're going\nto call these data set and data loader. Now, as I discussed previously, if we go to the pytorch\ndocumentation torch vision for torch vision, this is going to be quite similar for torch audio torch\ntext, torch rec torch data eventually when it comes out of beta, there are different ways to\ncreate such data sets. So we can go into the data sets module, and then we can find built-in data\nsets, and then also base classes for custom data sets. But if we go into here, image folder,\nthere's another parameter I'd like to show you, and this is going to be universal across many of\nyour different data types is the transform parameter. Now, the transform parameter is\na parameter we can use to pass in some transforms on our data. So when we load our data sets from an\nimage folder, it performs a transform on those data samples that we've sent in here as the target\ndata folder. Now, this is a lot more easier to understand through illustration, rather than just\ntalking about it. So let's create a transform. And the main transform we're going to be doing is\ntransforming our data, and we're turning it into tenses. So let's see what that looks like. So we're\ngoing to just going to re import all of the main libraries that we're going to use. So from torch\nutils dot data, let's import data loader. And we're going to import from torch vision. I'm going to\nimport data sets. And I'm also going to import transforms. Beautiful. And I'm going to create\nanother little heading here, this is going to be 3.1, transforming data with torch vision dot\ntransform. So the main transform we're looking to here is turning out images from JPEGs.\nIf we go into train, and then we go into any folder, we've got JPEG images.\nAnd we want to turn these into tensor representation. So there's some pizza there.\nWe'll get out of this. Let's see what we can do. How about we create a transform here,\nwrite a transform for image. And let's start off by calling it data transform.\nAnd I'm going to show you how we can combine a few transforms together. If you want to\ncombine transforms together, you can use transforms dot compose. You can also use\nan n dot sequential to combine transforms. But we're going to stick with transforms dot\ncompose for now. And it takes a list. And so let's just write out three transforms to begin with.\nAnd then we can talk about them after we do so. So we want to resize our images\nto 6464. Now, why might we do this? Well, do you recall in the last section computer vision,\nwe use the tiny VGG architecture. And what size were the images that the tiny VGG architecture took?\nWell, we replicated the CNN website version or the CNN explainer website version, and they took\nimages of size 6464. So perhaps we want to leverage that computer vision model later on.\nSo we're going to resize our images to 6464. And then we're going to create another transform.\nAnd so this is, I just want to highlight how transforms can help you manipulate your data in a\ncertain way. So if we wanted to flip the images, which is a form of data augmentation, in other\nwords, artificially increasing the diversity of our data set, we can flip the images randomly on\nthe horizontal. So transforms dot random horizontal flip. And I'm going to put a probability in here\nof p equals 0.5. So that means 50% of the time, if an image goes through this transform pipeline,\nit will get flipped on the horizontal axis. As I said, this makes a lot more sense when we\nvisualize it. So we're going to do that very shortly. And finally, we're going to turn the image into\na torch tensor. So we can do this with transforms dot to tensor. And now where might you find such\ntransforms? So this transform here says to tensor, if we have a look at the doc string,\nwe got convert a pill image, which is what we're working with right now, or a NumPy array to a\ntensor. This transform does not support torch script. If you'd like to find out what that is,\nI'd like to read the documentation for that. It's essentially turning your pytorch code into a\nPython script. It converts a pill image or a NumPy array from height with color channels in the range\n0 to 255, which is what our values are up here. They're from 0 to 255, red, green and blue,\nto a torch float tensor of shape color channels height width in the range 0 to 1. So it will\ntake our tensor values here or our NumPy array values from 0 to 255 and convert them into a torch\ntensor in the range 0 to 1. We're going to see this later on in action. But this is our first\ntransform. So we can pass data data through that. In fact, I'd encourage you to try that out.\nSee what happens when you pass in data transform. What happens when you pass it in our image as a\nray? Image as a ray. Let's see what happens. Hey, oh, image should be pill image got class NumPy\narray. What if we just pass in our straight up image? So this is a pill image. There we go.\nBeautiful. So if we look at the shape of this, what do we get?\n3 64 64. There's 64. And if what if we wanted to change this to 224, which is another common value for\ncomputer vision models to 24 to 24. Do you see how powerful this is? This little transforms\nmodule, the torch vision library will change that back to 64 64. And then if we have a look at what\nD type of our transform tensor is, we get torch float 32. Beautiful. So now we've got a way to\ntransform our images into tensors. And so, but we're still only doing this with one image.\nHow about we progress towards doing it for every image in our data folder here?\nBut before we do that, I'd like to visualize what this looks like. So in the next video,\nlet's write some code to visualize what it looks like to transform multiple images at a time.\nAnd I think it'd be a good idea to compare the transform that we're doing to the original image.\nSo I'll see you in the next video. Let's write some visualization code.\nLet's now follow our data explorer's motto of visualizing our transformed images. So we saw what it looks\nlike to pass one image through a data transform. And if we wanted to find more documentation on\ntorch vision transforms, where could we go? There is a lot of these. So transforming and augmenting\nimages, this is actually going to be your extra curriculum for this video. So transforms are\ncommon image transformations available in the transforms module. They can be chained together\nusing compose, which is what we've already done. Beautiful. And so if you'd like to go through all\nof these, there's a whole bunch of different transforms that you can do, including some data\naugmentation transforms. And then if you'd like to see them visually, I'd encourage you to check\nout illustration of transforms. But let's write some code to explore our own transform visually\nfirst. So I'll leave this as a link. So I'm going up here, right here, transforms\nhelp you get your images ready to be used with a model slash perform data augmentation.\nWonderful. So we've got a way to turn images into tenses. That's what we want for our model.\nWe want our images as pytorch tenses. The same goes for any other data type that you're working\nwith. But now I'd just like to visualize what it looks like if we plot a number of transformed\nimages. So we're going to make a function here that takes in some image paths, a transform,\na number of images to transform at a time and a random seed here, because we're going to harness\nthe power of randomness. And sometimes we want to set the seed. Sometimes we don't. So we have\nan image path list that we've created before, which is just all of the image paths that we have\nof our data set. So data, pizza, steak sushi. Now how about we select some random image paths\nand then take the image from that path, run it through our data transform, and then compare the\noriginal image of what it looks like and the transformed image and what that looks like.\nLet's give it a try, hey? So I'm going to write a doc string of what this does,\nand then selects random images from a path of images and loads slash transforms them,\nthen plots the original verse, the transformed version. So that's quite a long doc string,\nbut that'll be enough. We can put in some stuff for the image paths, transforms, and seed. We'll\njust code this out. Let's go random seed, we'll create the seed. Maybe we do it if seed, random seed.\nLet's put that, and we'll set seed to equal none by default. That way we can, we'll see if this works,\nhey, if in doubt, coded out random image paths, and then we're going to go random sample from the\nimage paths and the number of sample that we're going to do. So random sample is going to, this will\nbe a list on which part in here that this is a list. So we're going to randomly sample\nk, which is going to be n. So three images from our image path list. And then we're going to go for\nimage path, we're going to loop through the randomly sampled image parts. You know how much I love\nharnessing the power of randomness for visualization. So for image path in random image paths, let's\nopen up that image using pill image dot open image path as f. And then we're going to create a\nfigure and an axes. And we're going to create a subplot with my plot lib. So subplots. And we\nwant it to create one row. So it goes n rows and calls. One row and n calls equals two. And then\non the first or the zeroth axis, we're going to plot the original image. So in show, we're just\ngoing to pass it straight in f. And then if we want to go x zero, we're going to set the title. So\nset title, we're going to set it to be the original. So we'll create this as an f string, original,\nand then new line will create a size variable. And this is going to be f dot size. So we're just\ngetting the size attribute from our file. So we'll keep going, and we'll turn off the axes here.\nSo axis, and we're going to set that to false. Now let's transform on the first axes plot. We're\ngoing to transform and plot target image. This is so that our images are going to be side by side,\nthe original and the transformed version. So there's one thing that we're going to have to do. I'll\njust, I'll code it out in a wrong way first. I think that'll be a good way to illustrate what's\ngoing on. f. So I'm just going to put a note here. Note, we will need to change shape for\nmatplotlib, because we're going to come back here. Because what does this do? What have we\nnoticed that our transform does? If we check the shape here, oh, excuse me, it converts our image\nto color channels first. Whereas matplotlib prefers color channels last. So just keep that\nin mind for when we're going forward. This code, I'm writing it, it will error on purpose. So\ntransformed image. And then we're going to go axe one as well. We're going to set the title,\nwhich is going to be transformed. And then we'll create a new line and we'll say size is going to be\ntransformed image dot shape. Or probably a bit of, yeah, we could probably go shape here. And then\nfinally, we're going to go axe one, we're going to turn the axis, we're going to set that to false.\nYou can also set it to off. So you could write false, or you could write off, you might see that\ndifferent versions of that somewhere. And I'm going to write a super title here, which we'll see what\nthis looks like class is going to be image path. So we're getting the target image path. And we're\njust going to get the attribute or the parent attribute, and then the stem attribute from that,\njust like we did before, to get the class name. And then I'm going to set this to a larger font\nsize, so that we make some nice looking plots, right? If we're going to visualize our data,\nwe might as well make our plots visually appealing. So let's plot some transformed data or transformed\nimages. So image paths, we're going to set this to image part list, which is just the variable we\nhave down below, which is the part list, a list containing all of our image paths. Our transform,\nwe're going to set our transform to be equal to our data transform. So this just means that if\nwe pass the transform in, our image is going to go through that transform, and then go through all\nof these is going to be resized, it's going to be randomly horizontally flipped, and it's going to\nbe converted to a tensor. And then so we're going to set that data transfer there or data transform,\nsorry, and is going to be three. So we plot three images, and we'll set the seed to 42 to begin with.\nLet's see if this works. Oh, what did we get wrong? We have invalid shape. As I said, I love seeing\nthis error, because we have seen this error many times, and we know what to do with it. We know that\nwe have to rearrange the shapes of our data in some way, shape or form. Wow, I said shape a lot\nthere. That's all right. Let's go here, permute. This is what we have to do. We have to permute,\nwe have to swap the order of the axes. So right now, our color channels is first. So we have to\nbring this color channel axis or dimension to the end. So we need to shuffle these across. So 64\ninto here, 64 into here, and three on the end. We need to, in other words, turn it from color\nchannels first to color channels last. So we can do that by permuting it to have the first\naxis come now in the zero dimension spot. And then number two was going to be in the first\ndimension spot. And then number zero was going to be at the back end. So this is essentially going\nfrom C H W, and we're just changing the order to be H W C. So the exact same data is going to be\nwithin that tensor. We're just changing the order of the dimensions. Let's see if this works.\nLook at that. Oh, I love seeing some manipulated data. We have a class of pizza and the original\nimage is there, and it's 512 by 512. But then we've resized it using our transform. Notice that\nit's a lot more pixelated now, but that makes sense because it's only 64 64 pixels. Now, why\nmight we do such a thing? Well, one, if is this image still look like that? Well, to me, it still\ndoes. But the most important thing will be does it look like that to our model? Does it still look\nlike the original to our model? Now 64 by 64, there is less information encoded in this image.\nSo our model will be able to compute faster on images of this size. However, we may lose\nsome performance because not as much information is encoded as the original image. Again, the size\nof an image is something that you can control. You can set it to be a hyper parameter. You can\ntune the size to see if it improves your model. But I've just decided to go 60 64 64 3 in line\nwith the CNN explainer website. So a little hint, we're going to be re replicating this model that\nwe've done before. Now you notice that our images are now the same size 64 64 3 as what the CNN\nexplainer model uses. So that's where I've got that from. But again, you could change this to\nsize to whatever you want. And we see, oh, we've got a stake image here. And you notice that our\nimage has been flipped on the horizontal. So the horizontal access, our image has just been flipped\nsame with this one here. So this is the power of torch transforms. Now there are a lot more\ntransforms, as I said, you can go through them here to have a look at what's going on. Illustrations\nof transforms is a great place. So there's resize, there's center crop, you can crop your\nimages, you can crop five different locations, you can do grayscale, you can change the color,\na whole bunch of different things. I'd encourage you to check this out. That's your extra curriculum\nfor this video. But now that we've visualized a transform, this is what I hinted at before that\nwe're going to use this transform for when we load all of our images in, using into a torch\ndata set. So I just wanted to make sure that they had been visualized first. We're going to use our\ndata transform in the next video when we load all of our data using a torch vision dot data sets\nhelper function. So let's give that a go. I'll see you in the next video.\nHave a look at that beautiful plot. We've got some original images and some transformed\nimages. And the beautiful thing about our transformed images is that they're in tensor format,\nwhich is what we need for our model. That's what we've been slowly working towards.\nWe've got a data set. And now we've got a way to turn it into tensors ready for a model. So\nlet's just visualize what another, I'll turn the seed off here so we can look at some more random\nimages. There we go. Okay, so we've got stake pixelated because we're downsizing 64, 64, 3.\nSame thing for this one. And it's been flipped on the horizontal. And then same thing for this\npizza image and we'll do one more to finish off. Wonderful. So that is the premise of transforms\nturning our images into tensors and also manipulating those images if we want to.\nSo let's get rid of this. I'm going to make another heading. We're up to section or part four now.\nAnd this is going to be option one. So loading image data using image folder. And now I'm going\nto turn that into markdown. And so let's go torch vision data sets. So recall how each one of the\ntorch vision domain libraries has its own data sets module that has built in functions for\nhelping you load data. In this case, we have an image folder. And there's a few others here if\nyou'd like to look into those. But an image folder, this class is going to help us load in data that\nis in this format, the generic image classification format. So this is a prebuilt data sets function.\nJust like there's prebuilt data sets, we can use prebuilt data set functions. Now option two\nlater on, this is a spoiler, is we're going to create our own custom version of a data set loader.\nBut we'll see that in a later video. So let's see how we can use image folder to load all of our\ncustom data, our custom images into tensors. So this is where the transform is going to come in\nhelpful. So let's write here, we can load image classification data using, let's write this,\nlet's write the full path name, torch vision dot data sets dot image folder. Put that in there,\nbeautiful. And so let's just start it out, use image folder to create data sets. Now in a previous\nvideo, I hinted at the fact that we can pass a transform to our image folder class. That's going\nto be right here. So let's see what that looks like in practice. So from torch vision, I'm going\nto import data sets, because that's where the image folder module lives. And then we can go train\ndata equals data sets dot image folder. And we're going to pass in the root, which is our train\nder, because we're going to do it for the training directory first. And then we're going to pass\nin a transform, which is going to be equal to our data transform. And then we're going to pass in\na target transform, but we're going to leave this as none, which is the default, I believe,\nwe go up to here. Yeah, target transform is optional. So what this means is this is going to be a\ntransform for the data. And this is going to be a transform for the label slash target.\nPyTorch likes to use target, I like to use label, but that's okay. So this means that we don't need\na target transform, because our labels are going to be inferred by the target directory where the\nimages live. So our pizza images are in this directory, and they're going to have pizza as the label,\nbecause our data set is in standard image classification format. Now, if your data set wasn't in a\nstandard image classification format, you might use a different data loader here. A lot of them\nwill have a transform for the data. So this transform is going to run our images, whatever images are\nloaded from these folders, through this transform that we've created here, it's going to resize them,\nrandomly flip them on the horizontal, and then turn them into tenses, which is exactly how we\nwant them for our PyTorch models. And if we wanted to transform the labels in some way, shape or form,\nwe could pass in a target transform here. But in our case, we don't need to transform the labels.\nSo let's now do the same thing for the test data. And so that's why I wanted to visualize\nour transforms in the previous videos, because otherwise we're just passing them in as a transform.\nSo really, what's going to happen behind the scenes is all of our images are going to go\nthrough these steps. And so that's what they're going to look like when we turn them into a data\nset. So let's create the test data here or the test data set. The transform, we're going to\ntransform the test data set in the same way we've transformed our training data set. And we're\njust going to leave that like that. So let's now print out what our data sets look like,\ntrain data, and test data. Beautiful. So we have a data set, a torch data set,\nwhich is an image folder. And we have number of data points. This is going to be for the training\ndata set. We have 225. So that means about 75 images per class. And we have the root location,\nwhich is the folder we've loaded them in from, which is our training directory. We've set these\ntwo up before, trained and tester. And then we have a transform here, which is a standard transform,\na resize, followed by random horizontal flip, followed by two tensor. Then we've got basically\nthe same output here for our test directory, except we have less samples there. So let's get a few\nlittle attributes from the image folder. This is one of the benefits of using a pytorch prebuilt\ndata loader, is that or data set loader is that it comes with a fair few attributes. So we could\ngo to the documentation, find this out from in here, inherits from data set folder, keep digging\ninto there, or we could just come straight into Google collab. Let's go get class names as a list.\nCan we go train data dot and then press tab? Beautiful. So we've got a fair few things here\nthat are attributes. Let's have a look at classes. This is going to give us a list of the class names,\nclass names. This is very helpful later on. So we've got pizza steak sushi. We're trying to\ndo everything with code here. So if we have this attribute of train data dot classes,\nwe can use this list later on for when we plot images straight from our data set,\nor make predictions on them and we want to label them. You can also get class names as a dictionary,\nmap to their integer index, that is, so we can go train data dot and press tab. We've got class\nto ID X. Let's see what this looks like. Class decked. Wonderful. So then we've got our string\nclass names mapped to their integer. So we've got pizza is zero, steak is one, sushi is two. Now,\nthis is where the target transform would come into play. If you wanted to transform those\nthese labels here in some way, shape or form, you could pass a transform into here.\nAnd then if we keep going, let's check the lengths of what's going on. Check the lengths\nof our data set. So we've seen this before, but this is going to just give us how many samples\nthat we have length, train data, length, test data, beautiful. And then of course, if you'd like\nto explore more attributes, you can go train data dot, and then we've got a few other things,\nfunctions, images, loader, samples, targets. If you wanted to just see the images, you can go dot\nsamples. If you wanted to see just the labels, you can go dot targets. This is going to be all\nof our labels. Look at that. And I believe they're going to be an order. So we're going to have\nzero, zero, zero, one, one, one, two, two, and then if we wanted to have a look, let's say we have a\nlook at the first sample, hey, we have data, pizza, steak sushi, train, pizza. There's the image path,\nand it's a label zero for pizza. Wonderful. So now we've done that. How about we, we've been\nvisualizing this whole time. So let's keep up that trend. And let's visualize a sample and a label\nfrom the train data data set. So in this video, we've used image folder to load our images\ninto tenses. And because our data is already in standard image classification format,\nwe can use one of torch vision dot data sets prebuilt functions.\nSo let's do some more visualization in the next video. I'll see you there.\nWelcome back. In the last video, we used data sets dot image folder to turn all of our\nimage data into tenses. And we did that with the help of our data transform, which is a little\npipeline up here to take in some data, or specifically an image, resize it to a value that we've set in\nour k6464 randomly flip it along the horizontal. We don't necessarily need this, but I've just put\nthat in there to indicate what happens when you pass an image through a transforms pipeline.\nAnd then most importantly, we've turned our images into a torch tensor. So that means that our data,\nour custom data set, this is so exciting, is now compatible to be used with a pytorch model.\nSo let's keep pushing forward. We're not finished yet. We're going to visualize some samples\nfrom the train data data set. So let's, how can we do this? Let's get, we can index on the train data\ndata set to get a single image and a label. So if we go, can we do train data zero? What does that\ngive us? Okay, so this is going to give us an image tensor. And it's associated label. In this\ncase, it's an image of pizza, because why it's associated label is pizza. So let's take the zero\nzero. So this is going to be our image. And the label is going to be train data zero. And we're\njust going to get the first index item there, which is going to be one. And then if we have a look\nat them separately, image and label, beautiful. So now one of our target images is in tensor format,\nexactly how we want it. And it's label is in numeric format as well, which is also exactly how\nwe want it. And then if we wanted to convert this back to a non label, we can go class names\nand index on that. And we see pizza. And I mean, non label is in non numeric, we can get it back\nto string format, which is human understandable. We can just index on class names. So let's print\nout some information about what's going on here. Print F, we're going to go image tensor.\nI love F strings if you haven't noticed yet. Image tensor. And we're going to set in\nnew line, we're going to pass it in our image, which is just the image that we've got here.\nThen we'll print in some more information about that. This is still all becoming one with the\ndata right where we're slowly finding out information about our data set so that if errors arise later\non, we can go, hmm, our image or we're getting a shape error. And I know our images are of this\nshape or we're getting a data type error, which is why I've got the dot D type here. And that\nmight be why we're getting a data type issue. So let's do one more with the image label,\nlabel, oh, well, actually, we'll do one more. We'll do print, we'll get the label data type as well.\nLabel, this will be important to take note of later on. Type, as I said, three big issues.\nShape mismatch, device mismatch, and data type mismatch. Can we get the type of our label?\nBeautiful. So we've got our image tensor and we've got its shape. It's of torch size 36464.\nThat's exactly how we want it. The data type is torch float 32, which is the default data type\nin PyTorch. Our image label is zero and the label data type is of integer. So let's try and plot\nthis and see what it looks like, hey, using matplotlib. So first of all, what do we have to do? Well,\nwe have to rearrange the order of dimensions. In other words, matplotlib likes color channels\nlast. So let's see what looks this looks like. We'll go image per mute. We've done this before,\nimage.permute 120 means we're reordering the dimensions. Zero would usually be here,\nexcept that we've taken the zero dimension, the color channels and put it on the end\nand shuffled the other two forward. So let's now print out different shapes. I love printing\nout the change in shapes. It helps me really understand what's going on. Because sometimes\nI look at a line like this and it doesn't really help me. But if I print out something of what\nthe shapes were originally and what they changed to, well, hey, that's a big help. That's what\nJupiter notebooks are all about, right? So this is going to be color channels first, height,\nwidth. And depending on what data you're using, if you're not using images, if you're using text,\nstill knowing the shape of your data is a very good thing. We're going to go image per mute.shape\nand this should be everything going right is height with color channels on the end here.\nAnd we're just going to plot the image. You can never get enough plotting practice.\nPlot the image. You're going to go PLT dot figure, we'll pass in fig size equals 10, 7.\nAnd then we're going to PLT dot in show. We'll pass in the permuted image,\nimage underscore permutes, and then we'll turn off the axes. And we will set the title to be\nclass names. And we're going to index on the label, just as we did before. And we're going to set\nthe font size equal to 14. So it's nice and big. Here we go. Beautiful. There is our image of pizza.\nIt is very pixelated because we're going from about 512 as the original size 512 by 512 to 64,\n64. I would encourage you to try this out. Potentially, you could use a different image here. So we've\nindexed on sample zero. Maybe you want to change this to just be a random image and go through these\nsteps here. And then if you'd like to see different transforms, I'd also encourage you to try\nchanging this out, our transform pipeline here, maybe increase the size and see what it looks\nlike. And if you're feeling really adventurous, you can go into torch vision and look at the\ntransforms library here and then try one of these and see what it does to our images.\nBut we're going to keep pushing forward. We are going to look at another way. Or actually,\nI think for completeness, let's now turn, we've got a data set. We want to, we wrote up here before\nthat we wanted to turn our images into a data set, and then subsequently a torch utils data\ndata loader. So we've done this before, by batching our images, or batching our data that we've\nbeen working with. So I'd encourage you to give this a shot yourself. Try to go through the next\nvideo and create a train data loader using our train data, wherever that is train data,\nand a test data loader using our test data. So give that a shot and we'll do it together in the\nnext video. We'll turn our data sets into data loaders. Welcome back. How'd you go? In the last\nvideo, I issued you the challenge to turn our data sets into data loaders. So let's do that\ntogether now. I hope you gave it a shot. That's the best way to practice. So turn loaded images\ninto data loaders. So we're still adhering to our PyTorch workflow here. We've got a custom\ndata set. We found a way to turn it into tenses in the form of data sets. And now we're going to\nturn it into a data loader. So we can turn our data sets into iterables or batchify our data.\nSo let's write down here, a data loader is going to help us turn our data sets into iterables.\nAnd we can customize the batch size, write this down. So our model can see batch size\nimages at a time. So this is very important. As we touched on in the last section computer vision,\nwe create a batch size because if we had 100,000 images, chances are if they were all in one data\nset, there's 100,000 images in the food 101 data set. We're only working with about 200.\nIf we try to load all 100,000 in one hit, chances are our hardware may run out of memory. And so\nthat's why we matchify our images. So if we have a look at this, NVIDIA SMI, our GPU only has 16\ngigabytes. I'm using a Tesla T4 right now, well, has about 15 gigabytes of memory. So if we tried\nto load 100,000 images into that whilst also computing on them with a PyTorch model,\npotentially we're going to run out of memory and run into issues. So instead, we can turn them\ninto a data loader so that our model looks at 32 images at a time and can leverage all of the\nmemory that it has rather than running out of memory. So let's turn our train and test data sets\ninto data loaders, turn train and test data sets into data loaders. Now, this is not just for image\ndata. This is for all kinds of data in PyTorch. Images, text, audio, you name it. So import data\nloader, then we're going to create a train data loader. We're going to set it equal to data loader.\nWe're going to pass in a data set. So let's set this to train data. Let's set the batch size.\nWhat should we set the batch size to? I'm going to come up here and set a laser capital variable.\nI'm going to use 32 because 32 is a good batch size. So we'll go 32 or actually,\nlet's start small. Let's just start with a batch size of one and see what happens.\nBatch size one, number of workers. So this parameter is going to be, this is an important one. I'm going\nto, I potentially have covered it before, but I'm going to introduce it again. Is this going to be\nhow many cores or how many CPU cores that is used to load your data? So the higher the better usually\nand you can set this via OS CPU count, which will count how many CPUs your compute hardware has.\nSo I'll just show you how this works. Import OS and this is a Python OS module. We can do\nCPU count to find out how many CPUs our Google Colab instance has. Mine has two,\nyour number may vary, but I believe most Colab instances have two CPUs. If you're running this on\nyour local machine, you may have more. If you're running it on dedicated deep learning hardware,\nyou may even have even more, right? So generally, if you set this to one, it will use one CPU core,\nbut if you set it to OS dot CPU count, it will use as many as possible. So we're just going to\nleave this as one right now. You can customize this to however you want. And I'm going to shuffle\nthe training data because I don't want my model to recognize any order in the training data. So I'm\ngoing to mix it up. And then I'm going to create the test data loader. Data set equals test data.\nAnd batch size equals one, num workers, I'm going to set this to equal one as well. Again,\nyou can customize each of these, their hyper parameters to whatever you want. Number of workers\ngenerally the more the better. And then I'm going to set shuffle equals false for the test data so\nthat if we want to evaluate our models later on, our test data set is always in the same order.\nSo now let's have a look at train data loader, see what happens. And test data loader.\nWonderful. So we get two instances of torch utils dot data dot data loader. And now we can\nsee if we can visualize something from the train data loader, as well as the test data loader.\nI actually maybe we just visualize something from one of them. So we're not just double\nhandling everything. We get a length here. Wonderful. Because we're using a batch size of one,\nour lengths of our data loaders are the same as our data sets. Now, of course, this would change\nif we set, oh, we didn't even set this to the batch size parameter batch size. Let's come down\nhere and do the same here batch size. So we'll watch this change. If we wanted to look at 32\nimages at a time, we definitely could do that. So now we have eight batches, because 22, 225\ndivided by 32 equals roughly eight. And then 75 divided by 32 also equals roughly three. And\nremember, these numbers are going to be rounded if there are some overlaps. So let's get rid of,\nwe'll change this back to one. And we'll keep that there. We'll get rid of these two.\nAnd let's see what it looks like to plot an image from our data loader. Or at least have a look at it.\nCheck out the shapes. That's probably the most important point at this time. We've already\nplotted in our things. So let's iterate through our train data loader. And we'll grab the next one.\nWe'll grab the image and the label. And we're going to print out here. So batch size will now be one.\nYou can change the batch size if you like. This is just again, another way of getting familiar\nwith the shapes of our data. So image shape. Let's go image dot shape. And we're going to\nwrite down here. This shape is going to be batch size. This is what our data loader is going to\nadd to our images is going to add a batch dimension, color channels, height, width. And then print.\nLet's check out that label shape. Same thing with the labels. It's going to add a batch\ndimension. Label. And let's see what happens. Oh, we forgot the end of the bracket. Beautiful.\nSo we've got image shape. Our label shape is only one because we have a batch size of one.\nAnd so now we've got batch size one, color channels three, height, width. And if we change this to\n32, what do you think's going to happen? We get a batch size of 32, still three color channels,\nstill 64, still 64. And now we have 32 labels. So that means within each batch, we have 32\nimages. And we have 32 labels. We could use this with a model. I'm going to change this back to one.\nAnd I think we've covered enough in terms of loading our data sets. How cool is this?\nWe've come a long way. We've downloaded a custom data set. We've loaded it into a data set using\nimage folder turned it into tenses using our data transform and now batchified our custom data set\nin data loaders. We've used these with models before. So if you wanted to, you could go right\nahead and build a convolutional neural network to try and find patterns in our image tenses.\nBut in the next video, let's pretend we didn't have this data loader,\nthis image folder class available to us. How could we load our image data set so that it's\ncompatible? Like our image data set here, how could we replicate this image folder class?\nSo that we could use it with a data loader. Because data load is part of torch utils.data,\nyou're going to see these everywhere. Let's pretend we didn't have the torch vision.data sets\nimage folder helper function. And we'll see in the next video, how we can replicate that functionality.\nI'll see you there. Welcome back. So over the past few videos, we've been working out how to get\nhow to get our data from our data folder, pizza, steak, and sushi. We've got images of different\nfood data here. And we're trying to get it into Tensor format. So we've seen how to do that\nwith an existing data loader helper function or data set function in image folder. However,\nwhat if image folder didn't exist? And we need to write our own custom data loading function.\nNow the premise of this is although it does exist, it's going to be good practice because you might\ncome across a case where you're trying to use a data set where a prebuilt function doesn't exist.\nSo let's replicate the functionality of image folder by creating our own data loading class.\nSo we want a few things. We want to be able to get the class names as a list from our loaded data.\nAnd we want to be able to get our class names as a dictionary as well. So the whole goal of this\nvideo is to start writing a function or a class that's capable of loading data from here into\nTensor format, capable of being used with the PyTorch's data loader class, like we've done here. So we\nwant to create a data set. Let's start it off. We're going to create another heading here. This is\ngoing to be number five, option two, loading image data with a custom data set. So we want a few\nfunctionality steps here. Number one is one, two, be able to load images from file to one,\ntwo, be able to get class names from the data set, and three, one, two, be able to get classes\nas dictionary from the data set. And so let's briefly discuss the pros and cons of creating\nyour own custom data set. We saw option one was to use a pre-existing data set loader helping\nfunction from torch vision. And it's going to be quite similar if we go torch vision data sets.\nQuite similar if you're using other domain libraries here, there we're going to be data\nloading utilities. But at the base level of PyTorch is torchutils.data.dataset. Now this is\nthe base data set class. So we want to build on top of this to create our own image folder loading\nclass. So what are the pros and cons of creating your own custom data set? Well, let's discuss some\npros. So one pro would be you can create a data set out of almost anything as long as you write\nthe right code to load it in. And another pro is that you're not limited to PyTorch pre-built\ndata set functions. A couple of cons would be that even though this is to point number one.\nSo even though you could create a data set out of almost anything, it doesn't mean that it will\nautomatically work. It will work. And of course, you can verify this through extensive testing,\nseeing if your model actually works, if it actually loads data in the way that you want it. And another\ncon is that using a custom data set requires us to write more code. So often results in us\nwriting more code, which could be prone to errors or performance issues. So typically if\nsomething makes it into the PyTorch standard library or the PyTorch domain libraries,\nif functionality makes it into here, it's generally been tested many, many times. And it can kind of\nbe verified that it works quite well with, or if you do use it, it works quite well. Whereas if\nwe write our own code, sure, we can test it ourselves, but it hasn't got the robustness to begin with,\nthat is, we could fix it over time, as something that's included in say the PyTorch standard library.\nNonetheless, it's important to be aware of how we could create such a custom data set.\nSo let's import a few things that we're going to use. We'll import OS, because we're going to be\nworking with Python's file system over here. We're going to import path lib, because we're going to\nbe working with file paths. We'll import torch, we don't need to again, but I'm just doing this\nfor completeness. We're going to import image from pill, the image class, because we want to be\nopening images. I'm going to import from torch utils dot data. I'm going to import data set,\nwhich is the base data set. And as I said over here, we can go to data sets, click on torch utils\ndata dot data set. This is an abstract class representing a data set. And you'll find that this\ndata set links to itself. So this is the base data set class. Many of the data sets in PyTorch,\nthe prebuilt functions, subclass this. So this is what we're going to be doing it.\nAnd as a few notes here, all subclasses should overwrite get item. And you should optionally\noverwrite land. These two methods, we're going to see this in a future video. For now, we're just\nwe're just setting the scene here. So from torch vision, we're going to import transforms, because\nwe want to not only import our images, but we want to transform them into tenses. And from the\nPython's typing module, I'm going to import tuple dict and list. So we can put type hints\nwhen we create our class and loading functions. Wonderful. So this is our instance of torch vision\ndot data sets image folder, torch vision dot data sets dot image folder. Let's have a look\nat the train data. So we want to write a function that can replicate getting the classes from a\nparticular directory, and also turning them into an index or dictionary that is. So let's build\na helper function to replicate this functionality here. In other words, I'd like to write a helper\nfunction that if we pass it in a file path, such as pizza steak sushi or this data folder,\nit's going to go in here. And it's going to return the class names as a list. And it's also going\nto turn them into a dictionary, because it's going to be helpful for later on when we'd like to access\nthe classes and the class to ID X. And if we really want to completely recreate image folder,\nwell, image folder has this functionality. So we'd like that too. So this is just a little high level\noverview of what we're going to be doing. I might link in here that we're going to subclass this.\nSo all custom data sets in pie torch, often subclass this. So here's what we're going to be doing.\nOver the next few videos, we want to be able to load images from a file. Now you could replace\nimages with whatever data that you're working with the same premise will be here. You want to be\nable to get the class names from the data set and want to be able to get classes as a dictionary\nfrom the data set. So we're going to map our samples, our image samples to that class name\nby just passing a file path to a function that we're about to write. And some pros and cons of\ncreating a custom data set. We've been through that. Let's in the next video, start coding up a\nhelper function to retrieve these two things from our target directory. In the last video,\nwe discussed the exciting concept of creating a custom data set. And we wrote down a few things\nthat we want to get. We discussed some pros and cons. And we learned that many custom data sets\ninherit from torch dot utils dot data data set. So that's what we'll be doing later on. In this\nvideo, let's focus on writing a helper function to recreate this functionality. So I'm going to\ntitle this 5.1, creating a helper function to get class names. I'm going to turn this into\nmarkdown. And if I go into here, so we want to function to let's write down some steps and then\nwe'll code it out. So we'll get the class names, we're going to use OS dot scanner. So it's going\nto scanner directory to traverse a target directory. And ideally, the directory is in standard image\nclassification format. So just like the image folder class, our custom data class is going to\nrequire our data already be formatted. In the standard image classification format, such as\ntrain and test for training and test images, and then images for a particular class are in a\nparticular directory. So let's keep going. And number two, what else do we want it to do? We want\nit to raise an error if the class names aren't found. So if this happens, there might be,\nwe want this to enter the fact that there might be something wrong with the directory structure.\nAnd number three, we also want to turn the class names into our dict and a list and return them.\nBeautiful. So let's get started. Let's set up the path directory\nfor the target directory. So our target directory is going to be what the directory we want to load\ndirectory, if I could spell, we want to load our data from, let's start with the training\nder, just for an example. So target directory, what do we get? So we're just going to use the\ntraining folder as an example to begin with. And we'll go print target der, we'll put in the target\ndirectory, just want to exemplify what we're doing. And then we're going to get the class names\nfrom the target directory. So I'll show you the functionality of our scanner. Of course,\nyou could look this up in the Python documentation. So class names found, let's set this to be sorted.\nAnd then we'll get the entry name, entry dot name for entry in list. So we're going to get OS list\nscanner of the image path slash target directory. Let's see what happens when we do this.\nTarget directory have we got the right brackets here.\nNow, is this going to work? Let's find out. Oh, image path slash target directory.\nWhat do we get wrong? Oh, we don't need the image path there. Let's put, let's just put target\ndirectory there. There we go. Beautiful. So we set up our target directory as been the training\nto. And so if we just go, let's just do list. What happens if we just run this function here?\nOh, a scanner. Yeah, so there we go. So we have three directory entries. So this is where we're\ngetting entry dot name for everything in the training directory. So if we look in the training\ndirectory, what do we have train? And we have one entry for pizza, one entry for sushi, one entry\nfor steak. Wonderful. So now we have a way to get a list of class names. And we could quite easily\nturn this into a dictionary, couldn't we? Which is exactly what we want to do. We want to recreate\nthis, which we've done. And we want to recreate this, which is also done. So now let's take this\nfunctionality here. And let's turn that into a function. All right, what can we do? What do we\ncall this? I'm going to call this def fine classes. And I'm going to say that it takes in a directory\nwhich is a string. And it's going to return. This is where I imported typing from Python type and\nimported tuple. And I'm going to return a list, which is a list of strings and a dictionary,\nwhich is strings map to integers. Beautiful. So let's keep going. We want to, we want this\nfunction to return given a target directory, we want it to return these two things. So we've seen\nhow we can get a list of the directories in a target directory by using OS scanner. So let's\nwrite finds the classes are the class folder names in a target directory. Beautiful. And we know\nthat it's going to return a list and a dictionary. So let's do step number one, we want to get the\nclass names by scanning the target directory. We'll go classes, just we're going to replicate the\nfunctionality we've done about, but for any given directory here. So classes equals sorted entry\ndot name for entry in OS scanner. And we're going to pass at the target directory. If entry dot is\ndirt, we're just going to make sure it's a directory as well. And so if we just return classes and see\nwhat happens. So find classes, let's pass it in our target directory, which is our training directory.\nWhat do we get? Beautiful. So we need to also return class to ID X. So let's keep going. So number\ntwo is let's go raise an error. If class names could not be found. So if not classes, let's say\nraise file, we're going to raise a file not found error. And then let's just write in here F\ncouldn't find any classes in directory. So we're just writing some error checking code here.\nSo if we can't find a class list within our target directory, we're going to raise this\nerror and say couldn't find any classes in directory, please check file structure. And there's another\ncheckup here that's going to help us as well to check if the entry is a directory. So finally,\nlet's do number three. What do we want to do? So we want to create a dictionary of index labels.\nSo computers, why do we do this? Well, computers prefer numbers rather than strings as labels. So we\ncan do this, we've already got a list of classes. So let's just create class to ID X equals class\nname, I for I class name in enumerate classes. Let's see what this looks like.\nSo we go class names, and then class to ID X, or we can just return it actually. Do we spell\nenumerate role? Yes, we did. So what this is going to do is going to map a class name to an integer\nor to I for I class name in enumerate classes. So it's going to go through this, and it's going\nto go for I. So the first one zero is going to be pizza. Ideally, one will be steak,\ntwo will be sushi. Let's see how this goes. Beautiful. Look at that. We've just replicated\nthe functionality of image folder. So now we can use this helper function in our own custom\ndata set, find classes to traverse through a target directory, such as train, we could do the\nsame for test if we wanted to to. And that way, we've got a list of classes. And we've also got\na dictionary mapping those classes to integers. So now let's in the next video move towards sub\nclassing torch utils dot data dot data set. And we're going to fully replicate image folder. So I'll see you there.\nIn the last video, we wrote a great helper function called find classes that takes in a target\ndirectory and returns a list of classes and a dictionary mapping those class names to an integer.\nSo let's move forward. And this time, we're going to create a custom data set. To replicate\nimage folder. Now we don't necessarily have to do this, right, because image folder already exists.\nAnd if something already exists in the pie torch library, chances are it's going to be tested well,\nit's going to work efficiently. And we should use it if we can. But if we needed some custom\nfunctionality, we can always build up our own custom data set by sub classing torch dot utils\ndot data data set. Or if a pre built data set function didn't exist, well, we're probably going\nto want to subclass torch utils data dot data set anyway. And if we go into the documentation here,\nthere's a few things that we need to keep in mind when we're creating our own custom data set.\nAll data sets that represent a map from keys to data samples. So that's what we want to do.\nWe want to map keys, in other words, targets or labels to data samples, which in our case are\nfood images. So we should subclass this class here. Now to note, all subclasses should overwrite\nget item. So get item is a method in Python, which is going to get an item or get a sample,\nsupporting fetching a data sample for a given key. So for example, if we wanted to get sample\nnumber 100, this is what get item should support and should return us sample number 100.\nAnd subclasses could also optionally override land, which is the length of a data set. So return\nthe size of the data set by many sampler implementations and the default options of data\nloader, because we want to use this custom data set with data loader later on. So we should keep\nthis in mind when we're building our own custom subclasses of torch utils data data set. Let's see\nthis hands on, we're going to break it down. It's going to be a fair bit of code, but that's all right.\nNothing that we can't handle. So to create our own custom data set, we want to number one,\nfirst things first is we're going to subclass subclass torch dot utils dot data dot data set.\nTwo, what do we want to do? We want to init our subclass with target directory. So the directory\nwe'd like to get data from, as well as a transform, if we'd like to transform our data. So just like\nwhen we used image folder, we could pass a transform to our data set, so that we could transform the\ndata that we were loading. We want to do the same thing. And we want to create several attributes.\nLet's write them down here. We want paths, which will be the parts of our images. What else do\nwe want? We want transform, which will be the transform we'd like to use. We want classes,\nwhich is going to be a list of the target classes. And we want class to ID X, which is going to be\na dict of the target classes, mapped to integer labels. Now, of course, these attributes will\ndiffer depending on your data set. But we're replicating image folder here. So these are just\nsome of the things that we've seen that come with image folder. But regardless of what data set\nyou're working with, there are probably some things that you want to cross them universal.\nYou probably want all the paths of where your data is coming from, the transforms you'd like to\nperform on your data, what classes you're working with, and a map of those classes to an index.\nSo let's keep pushing forward. We want to create a function to load images, because after all,\nwe want to open some images. So this function will open an image. Number five, we want to\noverwrite the LAN method to return the length of our data set. So just like it said in the documentation,\nif you subclass using torch.utils.data, the data set, you should overwrite get item,\nand you should optionally overwrite LAN. So we're going to, instead of optionally, we are going to\noverwrite length. And number six, we want to overwrite the get item method to return a given sample\nwhen passed an index. Excellent. So we've got a fair few steps here. But if they don't make\nsense now, it's okay. Let's code it out. Remember our motto, if and doubt, code it out. And if\nand doubt, run the code. So we're going to write a custom data set. This is so exciting, because\nwhen you work with prebuilt data sets, it's pretty cool in machine learning. But when you can write\ncode to create your own data sets, and that's, well, that's magic. So number one is we're going to,\nor number zero is we're going to import torch utils data set, we don't have to rewrite this,\nwe've already imported it, but we're going to do it anyway for completeness. Now step number one\nis to subclass it subclass torch utils data, the data set. So just like when we built a model,\nwe're going to subclass and in module, but in this time, we're going to call us our class\nimage folder custom. And we're going to inherit from data set. This means that all the functionality\nthat's contained within torch utils data data set, we're going to get for our own custom class.\nNumber two, let's initialize. So we're going to initialize\nour custom data set. And there's a few things that we'd like, and into our subclass with the\ntarget directory, the directory we'd like to get data from, as well as the transform if we'd\nlike to transform our data. So let's write a knit function, a knit, and we're going to go self,\ntarget, and target is going to be a string. And we're going to set a transform here,\nwe'll set it equal to none. Beautiful. So this way we can pass in a target directory of images\nthat we'd like to load. And we can also pass in a transform, just similar to the transforms that\nwe've created previously. So now we're up to number three, which is create several attributes. So\nlet's see what this looks like, create class attributes. So we'll get all of the image paths.\nSo we can do this just like we've done before, self paths equals list, path lib dot path,\nbecause what's our target directory going to be? Well, I'll give you a spoiler alert,\nit's going to be a path like the test directory, or it's going to be the train directory.\nBecause we're going to use this once for our test directory and our train directory,\njust like we use the original image folder. So we're going to go through the target directory\nand find out all of the paths. So this is getting all of the image paths that support\nor that follow the file name convention of star star dot jpg. So if we have a look at this,\nwe passed in the test folder. So test is the folder star would mean any of these 123 pizza\nsteak sushi, that's the first star, then slash would go into the pizza directory. The star here\nwould mean any of the file combinations here that end in dot jpg. So this is getting us a list of\nall of the image paths within a target directory. In other words, within the test directory and\nwithin the train directory, when we call these two separately. So let's keep going, we've got all\nof the image parts, what else did we have to do? We want to create transforms. So let's set up\ntransforms, self dot transforms equals transform. Oh, we'll just call that transform actually,\nset up transform equals transform. So we're going to get this from here. And I put it as\nnone because it transform can be optional. So let's create classes and class to ID X attributes,\nwhich is the next one on our list, which is here classes and class to ID X. Now, lucky us,\nin the previous video, we created a function to return just those things. So let's go self dot\nclasses and self dot class to ID X equals find classes. And we're going to pass in the target\nder or the target der from here. Now, what's next? We've done step number three, we need\nnumber four is create a function to load images. All right, let's see what this looks like. So\nnumber four, create a function to load images. So let's call it load image. And we're going to\npass in self. And we'll also pass in an index. So the index of the image we'd like to load.\nAnd this is going to return an image dot image. So where does that come from? Well, previously,\nwe imported from pill. So we're going to use Python image library or pillow to import our\nimages. So we're going to give on a file path from here, such as pizza, we're going to import\nit with the image class. And we can do that using, I believe it's image dot open. So let's give that\na try. I'll just write a note in here, opens an image via a path and returns it. So let's write\nimage path equals self. This is why we got all of the image paths above. So self dot paths. And\nwe're going to index it on the index. Beautiful. And then let's return image dot open image path.\nSo we're going to get a particular image path. And then we're just going to open it.\nSo now we're up to step number five, override the land method to return the length of our data set.\nThis is optional, but we're going to do it anyway. So overwrite.\nLen. So this just wants to return how many samples we have in our data set. So let's write that\ndef, Len. So if we call Len on our data set instance, it's going to return just how many numbers there\nare. So let's write this down. Returns the total number of samples. And this is just going to be\nsimply return length or Len of self dot paths. So for our target directory, if it was the training\ndirectory, we'd return the number of image paths that this code has found out here. And same for the\ntest directory. So next, I'm going to go number six is we want to overwrite, we put this up here,\nthe get item method. So this is required if we want to subclass torch utils data data set. So\nthis is in the documentation here. All subclasses should override get item. So we want get item to,\nif we pass it an index to our data set, we want it to return that particular item. So let's see\nwhat this looks like. Override the get item method to return our particular sample.\nAnd now this method is going to leverage get item, all of the code that we've created above.\nSo this is going to go take in self, which is the class itself. And it's going to take in an index,\nwhich will be of an integer. And it's going to return a tuple of torch dot tensor and an integer,\nwhich is the same thing that gets returned when we index on our training data. So if we have a\nlook image label equals train data, zero, get item is going to replicate this. We pass it an index here.\nLet's check out the image and the label. This is what we have to replicate. So remember train\ndata was created with image folder from torch vision dot data sets. And so we will now get item\nto return an image and a label, which is a tuple of a torch tensor, where the image is of a tensor\nhere. And the label is of an integer, which is the label here, the particular index as to which\nthis image relates to. So let's keep pushing forward. I'm going to write down here, returns one sample\nof data, data and label, X and, or we'll just go XY. So we know that it's a tuple. Beautiful.\nSo let's set up the image. What do we want the image to be? Well, this is where we're going to\ncall on our self dot load image function, which is what we've created up here. Do you see the\ncustomization capabilities of creating your own class? So we've got a fair bit of code here,\nright? But essentially, all we're doing is we're just creating functions that is going to help us\nload our images into some way, shape or form. Now, again, I can't stress this enough, regardless\nof the data that you're working on, the pattern here will be quite similar. You'll just have to\nchange the different functions you use to load your data. So let's load an image of a particular\nindex. So if we pass in an index here, it's going to load in that image. Then what do we do? Well,\nwe want to get the class name, which is going to be self dot paths. And we'll get the index here,\nand we can go parent dot name. So this expects path in format data,\nfolder slash class name slash image dot JPG. That's just something to be aware of. And the class\nID X is going to be self dot class to ID X. And we will get the class name here.\nSo now we have an image by loading in the image here. We have a class name by because our data\nis going to be or our data is currently in standard image classification format. You may have to\nchange this depending on the format your data is in, we can get the class name from that,\nand we can get the class ID X by indexing on our attribute up here, our dictionary of class names\nto indexes. Now we have one small little step. This is transform if necessary. So remember our\ntransform parameter up here. If we want to transform our target image, well, let's put in if self dot\ntransform if the transform exists, let's pass the image through that transform, transform image\nand then we're going to also return the class ID X. So do you notice how we've returned a\ntuple here? This is going to be a torch tensor. If our transform exists and the class ID X is also\ngoing to be returned, which is what we want here, X and Y, which is what gets returned here,\nimage as a tensor label as an integer. So return data label X, Y, and then if the transform doesn't\nexist, let's just return image class ID X, return untransformed image and label. Beautiful. So\nthat is a fair bit of code there. So you can see the pro of subclassing torch utils data that data\nset is that we can customize this in almost any way we wanted to to load whatever data that we're\nworking with, well, almost any data. However, because we've written so much code, this may be\nprone to errors, which we're going to find out in the next video to see if it actually works.\nBut essentially, all we've done is we've followed the documentation here torch dot utils data\ndot data set to replicate the functionality of an existing data loader function, namely image folder.\nSo if we scroll back up, ideally, if we've done it right, we should be able to write code like this,\npassing in a root directory, such as a training directory, a particular data transform.\nAnd we should get very similar instances as image folder, but using our own custom data set class.\nSo let's try that out in the next video. So now we've got a custom image folder class\nthat replicates the functionality of the original image folder, data loader class,\nor data set class, that is, let's test it out. Let's see if it works on our own custom data.\nSo we're going to create a transform here so that we can transform our images raw jpeg images into tenses,\nbecause that's the whole goal of importing data into pytorch. So let's set up a train transforms\ncompose. We're going to set it to equal to transforms dot compose. And I'm going to pass in a list here,\nthat it's going to be transforms, we're going to resize it to 6464. Whatever the image size will\nreduce it down to 6464. Then we're going to go transforms dot random horizontal flip. We don't\nneed to necessarily flip them, but we're going to do it anyway, just to see if it works. And then\nlet's put in here transforms dot to tensor, because our images are getting opened as a pill image,\nusing image dot open. But now we're using the to transform transform from pytorch or torch\nvisions dot transforms. So I'll just put this here. From torch vision dot transforms, that way you\nknow where importing transforms there. And let's create one for the test data set as well, test\ntransforms, we'll set this up. Oh, excuse me, I need to just go import transforms. And let's go\ntransforms dot compose. And we'll pass in another list, we're going to do the exact same as above,\nwe'll set up resize, and we'll set the size equal to 6464. And then transforms, we're going to go\ndot to tensor, we're going to skip the data augmentation for test data. Because typically,\nyou don't manipulate your test data in terms of data augmentation, you just convert it into a\ntensor, rather than manipulate its orientation, shape, size, etc, etc. So let's run this.\nAnd now let's see how image folder custom class works. Test out image folder custom.\nLet's go, we'll set up the train data custom is equal to image folder custom. And then we'll set up\nthe target, which is equal to the training directory. And then we'll pass in the transform,\nwhich is equal to the train transforms, which we just created above train transforms. And then\nwe're going to, I think that's all we need, actually, we only had two parameters that we're not going\nto use a target transform, because our labels, we've got to help a function to transform our labels.\nSo test data custom is going to be image folder custom. And I'm going to set up the target to be\nequal to the test directory. And the transform is going to be the test transforms from the cell\nabove there. And what's co lab telling me there? Oh, I'm going to set that up. Did we spell\nsomething? Oh, we spelled it wrong train transforms. There we go. Beautiful. Now let's have a look at\nour train data and test data custom. See if it worked. What do we have? Or we have an image folder\ncustom. Well, it doesn't give us as much rich information as just checking it out as it does\nfor the train data. But that's okay. We can still inspect these. So this is our original one made\nwith image folder. And we've got now train data custom and test data custom. Let's see if we can\nget some information from there. So let's check the original length of the train data and see if\nwe can use the land method on our train data custom. Did that work? Wonderful. Now how about we do it\nfor the original test data made with image folder and our custom version made with test data or\nimage folder custom. Beautiful. That's exactly what we want. And now let's have a look at the\ntrain data custom. Let's see if the classes attribute comes up. Dot classes. And we'll just leave that\nthere. We'll do the class dot ID X. Yes, it is. So this attribute here is I wonder if we get\ninformation from Google co lab loading. What do we get? Oh, classes to ID X classes load image\npaths transform. So if we go back up here, all these attributes are from here paths transform\nclasses class to ID X as well as load image. So this is all coming from the code that we wrote\nour custom data set class. So let's keep pushing forward. Let's have a look at the class to ID X.\nDo we get the same as what we wanted before? Yes, we do beautiful a dictionary containing our\nstring names and the integer associations. So let's now check for equality. We can do this by going\ncheck for equality between original image folder data set and image folder custom data set. Now\nwe've kind of already done that here, but let's just try it out. Let's go print. Let's go train\ndata custom dot classes. Is that equal to train? Oh, I don't want three equals train data. The\noriginal one classes and also print. Let's do test data custom dot classes. Is this equal to\ntest data? The original one classes. True and true. Now you could try this out. In fact,\nit's a little exercise to try it out to compare the others. But congratulations to us, we have\nreplicated the main functionality of the image folder data set class. And so the takeaways from\nthis is that whatever data you have, PyTorch gives you a base data set class to inherit from.\nAnd then you can write a function or a class that somehow interacts with whatever data you're\nworking with. So in our case, we load in an image. And then you, as long as you override the land\nmethod and the get item method and return some sort of values, well, you can create your own\ndata set loading function. How beautiful is that? So that's going to help you work with your own\ncustom data sets in PyTorch. So let's keep pushing forward. We've seen analytically that\nour custom data set is quite similar to the original PyTorch, torch vision dot data sets\nimage folder data set. But you know what I like to do? I like to visualize things. So let's in\nthe next video, create a function to display some random images from our trained data custom class.\nIt's time to follow the data explorer's motto of visualize, visualize, visualize. So let's\ncreate another section. I'm going to write here a title called create a function to display random\nimages. And sure, we've, we've had a look at the different attributes of our custom data set.\nWe see that it gives back a list of different class names. We see that the lengths are similar\nto the original, but there's nothing quite like visualizing some data. So let's go in here. We're\ngoing to write a function, a helper function. So step number one, we need to take in a data set.\nSo one of the data sets that we just created, whether it be trained data custom or trained data.\nAnd a number of other parameters, such as class names and how many images to visualize. And then\nstep number two is to prevent the display getting out of hand. Let's cap the number of\nimages to see at 10. Because look, if our data set is going to be thousands of images and we want\nto put in a number of images to look at, let's just make sure it's the maximum is 10. That should\nbe enough. So we'll set the random seed for reproducibility. Number four is, let's get a list of random\nsamples. So we want random sample indexes, don't just get rid of this s from what do we want it from\nfrom the target data set. So we want to take in a data set, and we want to count the number of\nimages we're seeing, we want to set a random seed. And do you see how much I use randomness here to\nreally get an understanding of our data? I really, really, really love harnessing the power of\nrandomness. So we want to get a random sample of indexes from all of our data set. And then we're\ngoing to set up a matplotlib plot. Then we want to loop through the random sample images.\nAnd plot them with matplotlib. And then as a side to this one, step seven is we need to make sure\nthe dimensions of our images line up with matplotlib. So matplotlib needs a height width color channels.\nAll right, let's take it on, hey? So number one is create a function to take in a data set.\nSo we're going to call this def, let's call it def display random images going to be one of our\nhelper functions. We've created a few type of functions like this. But let's take in a data set,\nwhich is torch utils of type that is of type data set. Then we're going to take in classes,\nwhich is going to be a list of different strings. So this is going to be our class names for\nwhichever data set we're using. I'm going to set this equal to none. And then we're going to take in\nn, which is the number of images we'd like to plot. And I'm going to set this to 10 by default. So\nwe can see 10 images at a time, 10 random images, that is, do we want to display the shape? Let's\nset that equal to true, so that we can display what the shape of the images, because we're passing\nit through our transform as it goes into a data set. So we want to see what the shape of our\nimages are just to make sure that that's okay. And we can also let's set up a seed, which is\ngoing to be an integer, and we'll set that to none to begin with as well. Okay, so step number two,\nwhat do we have above? We have to prevent the display getting out of hand, let's cap the number\nof images to see at 10. So we've got n is by default, it's going to be 10, but let's just make\nsure that it stays there. Adjust display, if n is too high. So if n is greater than 10,\nlet's just readjust this, let's set n equal to 10, and display shape, we'll turn off the\ndisplay shape, because if we have 10 images, our display may get out of hand. So just print out\nhere for display purposes, and shouldn't be larger than 10, setting to 10, and removing\nshape display. Now I only know this because I've had experience cooking this dish before.\nIn other words, I've written this type of code before. So you can customize the beautiful thing\nabout Python and PyTorch, as you can customize these display functions in any way you see fit.\nSo step number three, what are we doing? Set the random seed for reproducibility. Okay,\nset the seed. So if seed, let's set random dot seed equal to that seed value, and then we can keep\nand then we can keep going. So number four is let's get some random sample indexes. So we can do\nthat by going get random sample indexes, which is step number four here. So we've got a target\ndata set that we want to inspect. We want to get some random samples from that. So let's create a\nrandom samples IDX list. And I'm going to randomly sample from a length of our data set, or sorry,\na range of the length of our data set. And I'll show you what this means in a second.\nAnd the K, excuse me, have we got enough brackets there? I always get confused with the brackets.\nThe K is going to be n. So in this case, I want to randomly sample 10 images from the length of\nour data set or 10 indexes. So let's just have a look at what this looks like. We'll put in here,\nour train data custom here. So this is going to take a range of the length of our train data\ncustom, which is what 225. We looked at that before, just up here, length of this. So between zero\nand 255, we're going to get 10 indexes if we've done this correctly. Beautiful. So there's 10\nrandom samples from our train data custom, or 10 random indexes, that is. So we're up to step number\nfive, which was loop through the random sample images or indexes. Let's create this to indexes,\nindexes and plot them with matplotlib. So this is going to give us a list here.\nSo let's go loop through random indexes and plot them with matplotlib. Beautiful. So for\ni tug sample in enumerate, let's enumerate through the random, random samples, idx list.\nAnd then we're going to go tug image and tug label, because all of the samples in our target\ndata set are in the form of tuples. So we're going to get the target image and the target label,\nwhich is going to be data set tug sample. We'll take the index. So it might be one of these values\nhere. We'll index on that. And the zero index will be the image. And then we'll go on the data set as\nwell. We'll take the tug sample index. And then the index number one will be the label of our target\nsample. And then number seven, oh, excuse me, we've missed a step. That should be number six.\nDid you catch that? Number five is setup plot. So we can do this quite easily by going plot\nfigure. This is so that each time we iterate through another sample, we're going to have\nquite a big figure here. So we set up the plot outside the loop so that we can add a plot to this\noriginal plot here. And now this is number seven, where we make sure the dimensions of our images\nline up with matplotlib. So if we recall by default, pytorch is going to turn our image dimensions into\nwhat color channels first, however, matplotlib prefers color channels last. So let's go adjust,\ntensor dimensions for plotting. So let's go tag image. Let's call this tag image adjust equals\ntag image dot commute. And we're going to alter the order of the indexes. So this is going to go\nfrom color channels or the dimensions that is height width. And we're going to change this width,\nif I could spell, to height width color channels. Beautiful. That one will probably catch you off\nguard a few times. But we've seen it a couple of times now. So we're going to keep going with this\nplot adjusted samples. So now we can add a subplot to our matplotlib plot. And we want to create,\nwe want one row of n images, this will make a lot more sense when we visualize it. And then for\nthe index, we're going to keep track of i plus one. So let's keep going. So then we're going to go\nplot in show. And I'm going to go tug image adjust. So I'm going to plot this image here. And then\nlet's turn off the axis. And we can go if the classes variable exists, which is up here, a list\nof classes, let's adjust the title of the plot to be the particular index in the class list. So\ntitle equals f class. And then we're going to put in here classes. And we're going to index on that\nwith the target label index, which is going to come from here. Because that's going to be a new\nnumerical format. And then if display shape, let's set the title equal to title plus f. We're going\nto go new line shape. This is going to be the shape of the image, tug image adjust dot shape.\nAnd then we'll set the title to PLT dot title. So you see how if we have display shape, we're\njust adjusting the title variable that we created here. And then we're putting the title onto the\nplot. So let's see how this goes. That is quite a beautiful function. Let's pass in one of our\ndata sets and see what it looks like. Let's plot some random images. So which one should we start\nwith first? So let's display random images from the image folder created data sets. So this is the\ninbuilt pytorch image folder. Let's go display random images, the function we just created above.\nWe're going to pass in the train data. And then we can pass in the number of images. Let's have\na look at five. And the classes is going to be the class names, which is just a list of our\ndifferent class names. And then we can set the seed, we want it to be random. So we'll just set\nthe seed to equal none. Oh, doesn't that look good? So this is from our original train data\nmade with image folder. So option number one up here, option one, there we go. And we've\npassed in the class name. So this is sushi resize to 64, 64, three, same with all of the others,\nbut from different classes. Let's set the seed to 42, see what happens. I get these images,\nwe got a sushi, we got a pizza, we got pizza, sushi pizza. And then if we try a different one,\nwe just go none. We get random images again, wonderful. Now let's write the same code,\nbut this time using our train data custom data set. So display random images from the image folder\ncustom data set. So this is the one that we created display random images. I'm going to pass\nin train data custom, our own data set. Oh, this is exciting. Let's set any equal to 10 and just see\nsee how far we can go with with our plot. Or maybe we set it to 20 and just see if our\ncode for adjusting the plot makes sense. Class names and seed equals, I'm going to put in 42 this time.\nThere we go. For display purposes, and shouldn't be larger than 10 setting to 10 and removing shape\ndisplay. So we have a stake image, a pizza image, pizza, steak pizza, pizza, pizza, pizza, steak,\npizza. If we turn off the random seed, we should get another 10 random images here.\nBeautiful. Look at that. Steak, steak, sushi, pizza, steak, sushi class. I'm reading out\nthe different things here. Pizza, pizza, pizza, pizza. Okay. So it looks like our custom data set\nis working from both a qualitative standpoint, looking at the different images and a quantitative.\nHow about we change it to five and see what it looks like? Do we have a different shape? Yes,\nwe do the same shape as above. Wonderful. Okay. So we've got train data custom.\nAnd we've got train data, which is made from image folder. But the premises remain, we've built up\na lot of different ideas. And we're looking at things from different points of view. We are\ngetting our data from the folder structure here into tensor format. So there's still one more\nstep that we have to do. And that's go from data set to data loader. So in the next video,\nlet's see how we can turn our custom loaded images, train data custom, and test data custom\ninto data loaders. So you might want to go ahead and give that a try yourself. We've done it before\nup here. Turn loaded images into data loaders. We're going to replicate the same thing as we did\nin here for our option number two, except this time we'll be using our custom data set.\nI'll see you in the next video. I'll take some good looking images and even better that they're\nfrom our own custom data set. Now we've got one more step. We're going to turn our data set into a\ndata loader. In other words, we're going to batchify all of our images so they can be used with the\nmodel. And I gave you the challenge of trying this out yourself in the last video. So I hope\nyou gave that a go. But let's see what that might look like in here. So I'm going to go 5.4.\nLet's go. What should we call this? So turn custom loaded images into data loaders. So this\nis just goes to show that we can write our own custom data set class. And we can still use it\nwith PyTorch's data loader. So let's go from utils torch dot utils that is utils dot data import\ndata loader. We'll get that in here. We don't need to do that again, but I'm just doing it for\ncompleteness. So we're going to set this to train data loader custom. And I'm going to create an\ninstance of data loader here. And then inside I'm going to pass the data set, which is going to be\ntrain data custom. I'm just going to set a universal parameter here in capitals for batch size equals\n32. Because we can come down here, we can set the batch size, we're going to set this equal to 32.\nOr in other words, the batch size parameter we set up there, we can set the number of workers\nhere as well. If you set to zero, let's go see what the default is actually torch utils data loader.\nWhat's the default for number of workers? Zero. Okay, beautiful. And recall that number of workers\nis going to set how many cores load your data with a data loader. And generally higher is better.\nBut you can also experiment with this value and see what value suits your model and your\nhardware the best. So just keep in mind that number of workers is going to alter how much\ncompute your hardware that you're running your code on uses to load your data. So by default,\nit's set to zero. And then we're going to shuffle the training data. Wonderful. And let's do the\nsame for the test data loader. We'll create test data loader custom. And I'm going to create a\nnew instance. So let me make a few code cells here of data loader, and create a data set or pass\nin the data set parameter as the test data custom. So again, these data sets are what we've created\nusing our own custom data set class. I'm going to set the batch size equal to batch size. And\nlet's set the number workers equal to zero. In a previous video, we've also set it to CPU count.\nYou can also set it to one. You can hard code it to four all depends on what hardware you're using.\nI like to use OPA OS dot CPU count. And then we're not going to shuffle the test data.\nFalse. Beautiful. And let's have a look at what we get here. Train data loader custom and test\ndata loader custom. And actually, I'm just going to reset this instead of being OOS CPU count.\nI'm going to put it back to zero, just so we've got it in line with the one above.\nAnd of course, numb workers, we could also set this numb workers equals zero or OS dot CPU count.\nAnd then we could come down here and set this as numb workers and numb workers.\nAnd let's have a look to see if it works. Beautiful. So we've got two instances of utils.data.data\nloader. Now, let's just get a single sample from the train data loader here, just to make sure the\nimage shape and batch size is correct. Get image and label from custom data loader. We want image\ncustom. And I'm going to go label custom equals next. And I'm going to iter over the train data\nloader custom. And then let's go print out the shapes. We want image custom dot shape and label\ncustom. Do we get a shape here? Beautiful. There we go. So we have shape here of 32,\nbecause that is our batch size. Then we have three color channels, 64, 64, which is in line with\nwhat? Which is in line with our transform that we set all the way up here. Transform. We transform\nour image. You may want to change that to something different depending on the model you're using,\ndepending on how much data you want to be comprised within your image. Recall, generally a larger\nimage size encodes more information. And this is all coming from our original image folder\ncustom data set class. So look at us go. And I mean, this is a lot of code here or a fair bit of\ncode, right? But you could think of this as like you write it once. And then if your data set continues\nto be in this format, well, you can use this over and over again. So you might put this, this image\nfolder custom into a helper function file over here, such as data set dot pie or something like\nthat. And then you could call it in future code instead of rewriting it all the time. And so that's\njust exactly what pytorch is done with taught vision dot data sets dot image folder. So we've\ngot some shapes here. And if we wanted to change the batch size, what do we do? We just change it\nlike that 64. Remember, a good batch size is also a multiple of eight, because that's going to help\nout computing. And batch size equals one. We get a batch size equal of one. We've been through a\nfair bit. But we've covered a very important thing. And that is loading your own data with a custom\ndata set. So generally, you will be able to load your own data with an existing data loading function\nor data set function from one of the torch domain libraries, such as torch audio, torch text,\ntorch vision, torch rack. And later on, when it's out of beta, torch data. But if you need to create\nyour own custom one, while you can subclass torch dot utils dot data, dot data set, and then add\nyour own functionality to it. So let's keep pushing forward. Previously, we touched a little bit on\ntransforming data. And you may have heard me say that torch vision transforms can be used for data\naugmentation. And if you haven't, that is what the documentation says here. But data augmentation\nis manipulating our images in some way, shape or form, so that we can artificially increase the\ndiversity of our training data set. So let's have a look at that more in the next video. I'll see you\nthere. Over the last few videos, we've created functions and classes to load in our own custom\ndata set. And we learned that one of the biggest steps in loading a custom data set is transforming\nyour data, particularly turning your target data into tenses. And we also had a brief look at the\ntorch vision transforms module. And we saw that there's a fair few different ways that we can\ntransform our data. And that one of the ways that we can transform our image data is through\naugmentation. And so if we went into the illustration of transforms, let's have a look at all the\ndifferent ways we can do it. We've got resize going to change the size of the original image.\nWe've got center crop, which will crop. We've got five crop. We've got grayscale. We've got random\ntransforms. We've got Gaussian blur. We've got random rotation, random caffeine, random crop.\nWe could keep going. And in fact, I'd encourage you to check out all of the different options here.\nBut oh, there's auto augment. Wonderful. There's random augment. This is what I was hinting at.\nData augmentation. Do you notice how the original image gets augmented in different ways here?\nSo it gets artificially changed. So it gets rotated a little here. It gets dark and a little\nhere or maybe brightened, depending how you look at it, it gets shifted up here. And then the colors\nkind of change here. And so this process is known as data augmentation, as we've hinted at.\nAnd we're going to create another section here, which is number six, other forms of transforms.\nAnd this is data augmentation. So how could you find out about what data augmentation is?\nWell, you could go here. What is data augmentation? And I'm sure there's going to be plenty of\nresources here. Wikipedia. There we go. Data augmentation in data analysis are techniques\nused to increase the amount of data by adding slightly modified copies of already existing data\nor newly created synthetic data from existing data. So I'm going to write down here,\ndata augmentation is the process of artificially adding diversity to your training data.\nNow, in the case of image data, this may mean applying various image transformations to the\ntraining images. And we saw a whole bunch of those in the torch vision transformed package.\nBut now let's have a look at one type of data augmentation in particular. And that is trivial\naugment. But just to illustrate this, I've got a slide here ready to go. We've got what is data\naugmentation. And it's looking at the same image, but from different perspectives. And we do this,\nas I said, to artificially increase the diversity of a data set. So if we imagine our original\nimages over here on the left, and then if we wanted to rotate it, we could apply a rotation\ntransform. And then if we wanted to shift it on the vertical and the horizontal axis,\nwe could apply a shift transform. And if we wanted to zoom in on the image, we could apply\na zoom transform. And there are many different types of transforms. As I've got a note here,\nthere are many different kinds of data augmentation, such as cropping, replacing,\nshearing. And this slide only demonstrates a few. But I'd like to highlight another type of data\naugmentation. And that is one used to recently train pytorch torch vision image models to state\nof the art levels. So let's take a look at one particular type of data augmentation,\nused to train pytorch vision models to state of the art levels.\nNow, just in case you're not sure why we might do this, we would like to increase\nthe diversity of our training data so that our images become harder for our model to learn. Or\nit gets a chance to view the same image from different perspectives so that when you use your\nimage classification model in practice, it's seen the same sort of images, but from many different\nangles. So hopefully it learns patterns that are generalizable to those different angles.\nSo this practice, hopefully, results in a model that's more generalizable to unseen data.\nAnd so if we go to torch vision, state of the art, here we go. So this is a recent blog post\nby the pytorch team, how to train state of the art models, which is what we want to do,\nstate of the art means best in business, otherwise known as soda. You might see this acronym quite\noften using torch visions latest primitives. So torch vision is the package that we've been\nusing to work with vision data. And torch vision has a bunch of primitives, which are,\nin other words, functions that help us train really good performing models. So blog post here.\nAnd if we jump into this blog post and if we scroll down, we've got some improvements here.\nSo there's an original ResNet 50 model. ResNet 50 is a common computer vision architecture.\nSo accuracy at one. So what do we have? Well, let's just say they get a boost in what the previous\nresults were. So if we scroll down, there is a type of data augmentation here. So if we add up\nall of the improvements that they used, so there's a whole bunch here. Now, as your extra curriculum,\nI'd encourage you to look at what the improvements are. You're not going to get them all the first\ngo, but that's all right. Blog posts like this come out all the time and the recipes are continually\nchanging. So even though I'm showing you this now, this may change in the future. So I just\nscroll down to see if this table showed us what the previous results were. Doesn't look like it does.\nOh, no, there's the baseline. So 76 and with all these little additions, it got right up to nearly\n81. So nearly a boost of 5% accuracy. And that's pretty good. So what we're going to have a look\nat is trivial augment. So there's a bunch of different things such as learning rate optimization,\ntraining for longer. So these are ways you can improve your model. Random erasing of image data,\nlabel smoothing, you can add that as a parameter to your loss functions, such as cross entropy loss,\nmix up and cut mix, weight decay tuning, fixed res mitigations, exponential moving average,\nwhich is EMA, inference resize tuning. So there's a whole bunch of different recipe items here,\nbut we're going to focus on what we're going to break it down. Let's have a look at trivial\naugment. So we'll come in here. Let's look at trivial augment. So if we wanted to look at\ntrivial augment, can we find it in here? Oh, yes, we can. It's right here. Trivial augment.\nSo as you'll see, if you pass an image into trivial augment, it's going to change it in a few\ndifferent ways. So if we go into here, let's write that down. So let's see this in action on some\nof our own data. So we'll import from torch vision, import transforms. And we're going to create a\ntrain transform, which is equal to transforms dot compose. We'll pass it in there. And this is\ngoing to be very similar to what we've done before in terms of composing a transform. What do we\nwant to do? Well, let's say we wanted to resize one of our images or an image going through this\ntransform. Let's change its size to 224224, which is a common size in image classification. And\nthen it's going to go through transforms. We're going to pass in trivial augment wide. And there's\na parameter here, which is number of magnitude bins, which is basically a number from 0 to 31,\n31 being the max of how intense you want the augmentation to happen. So say we, we only put this as\n5, our augmentation would be of intensity from 0 to 5. And so in that case, the maximum wouldn't\nbe too intense. So if we put it to 31, it's going to be the max intensity. And what I mean by intensity\nis say this rotation, if we go on a scale of 0 to 31, this may be a 10, whereas 31 would be\ncompletely rotating. And same with all these others, right? So the lower this number, the less the\nmaximum up a bound of the applied transform will be. Then if we go transforms dot to tensor,\nwonderful. So there we've just implemented trivial augment. How beautiful is that? That is from\nthe PyTorch torch vision transforms library. We've got trivial augment wide. And it was used\ntrivial augment to train the latest state of the art vision models in the PyTorch torch vision\nmodels library or models repository. And if you wanted to look up trivial augment, how could you\nfind that? You could search it. Here is the paper if you'd like to read it. Oh, it's implemented.\nIt's actually a very, very, I would say, let's just say trivial augment. I didn't want to say\nsimple because I don't want to downplay it. Trivial augment leverages the power of randomness\nquite beautifully. So I'll let you read more on there. I would rather try it out on our data\nand visualize it first. Test transform. Let's go transforms compose. And you might have the\nquestion of which transforms should I use with my data? Well, that's the million dollar question,\nright? That's the same thing as asking, which model should I use for my data? There's a fair\nfew different answers there. And my best answer will be try out a few, see what work for other\npeople like we've done here by finding that trivial augment worked well for the PyTorch team.\nTry that on your own problems. If it works well, excellent. If it doesn't work well,\nwell, you can always excuse me. We've got a spelling mistake. If it doesn't work well,\nwell, you can always set up an experiment to try something else. So let's test out our\naugmentation pipeline. So we'll get all the image paths. We've already done this, but we're\ngoing to do it anyway. Again, just to reiterate, we've covered a fair bit here. So I might just\nrehash on a few things. We're going to get list, image path, which is our, let me just show you\nour image path. We just want to get all of the images within this file.\nSo we'll go image path dot glob, glob together all the files and folders that match this pattern.\nAnd then if we check, what do we get? We'll check the first 10. Beautiful. And then we can\nleverage our function from the four to plot some random images, plot random images.\nWe'll pass in or plot transformed random transformed images. That's what we want.\nLet's see what it looks like when it goes through our trivial augment. So image paths,\nequals image part list. This is a function that we've created before, by the way, transform equals\ntrain transform, which is the transform we just created above that contains trivial augment.\nAnd then we're going to put n equals three for five images. And we'll do seed equals none\nto plot. Oh, sorry, n equals three for three images, not five. Beautiful. And we'll set the\nseed equals none, by the way. So look at this. We've got class pizza. Now trivial augment,\nit resized this. Now, I'm not quite sure what it did to transform it per se. Maybe it got a little\nbit darker. This one looks like it's been the colors have been manipulated in some way, shape,\nor form. And this one looks like it's been resized and not too much has happened to that one from\nmy perspective. So if we go again, let's have a look at another three images. So trivial augment\nworks. And what I said before, it harnesses the power of randomness. It kind of selects randomly\nfrom all of these other augmentation types, and applies them at some level of intensity.\nSo all of these ones here, trivial augment is just going to select summit random, and then\napply them some random intensity from zero to 31, because that's what we've set on our data.\nAnd of course, you can read a little bit more in the documentation, or sorry, in the paper here.\nBut I like to see it happening. So this one looks like it's been cut off over here a little bit.\nThis one again, the colors have been changed in some way, shape, or form. This one's been darkened.\nAnd so do you see how we're artificially adding diversity to our training data set? So instead\nof all of our images being this one perspective like this, we're adding a bunch of different\nangles and telling our model, hey, you got to try and still learn these patterns, even if they've\nbeen manipulated. So we'll try one more of these. So look at that one. That's pretty\nmanipulated there, isn't it? But it's still an image of stake. So that's what we're trying to\nget our model to do is still recognize this image as an image of stake, even though it's been\nmanipulated a bit. Now, will this work or not? Hey, it might, it might not, but that's all the\nnature of experimentation is. So play around. I would encourage you to go in the transforms\ndocumentation like we've just done, illustrations, change this one out, trivial augment wine,\nfor another type of augmentation that you can find in here, and see what it does to some of\nour images randomly. I've just highlighted trivial augment because it's what the PyTorch team have\nused in their most recent blog post for their training recipe to train state-of-the-art vision\nmodels. So speaking of training models, let's move forward and we've got to build our first model\nfor this section. I'll see you in the next video.\nWelcome back. In the last video, we covered how the PyTorch team used trivial augment\nwide, which is the latest state-of-the-art in data augmentation at the time of recording this\nvideo to train their latest state-of-the-art computer vision models that are within\ntorch vision. And we saw how easily we could apply trivial augment thanks to torch vision\ndot transforms. And we'll just see one more of those in action, just to highlight what's going on.\nSo it doesn't look like much happened to that image when we augmented, but we see this one has\nbeen moved over. We've got some black space there. This one has been rotated a little,\nand now we've got some black space there. But now's time for us to build our first\ncomputer vision model on our own custom data set. So let's get started. We're going to go model zero.\nWe're going to reuse the tiny VGG architecture, which we covered in the computer vision section.\nAnd the first experiment that we're going to do, we're going to build a baseline,\nwhich is what we do with model zero. We're going to build it without data augmentation.\nSo rather than use trivial augment, which we've got up here, which is what the PyTorch team used\nto train their state-of-the-art computer vision models, we're going to start by training our\ncomputer vision model without data augmentation. And then so later on, we can try one to see\nwith data augmentation to see if it helps or doesn't. So let me just put a link in here,\nCNN explainer. This is the model architecture that we covered in depth in the last section.\nSo we're not going to go spend too much time here. All you have to know is that we're going\nto have an input of 64, 64, 3 into multiple different layers, such as convolutional layers,\nrelio layers, max pool layers. And then we're going to have some output layer that suits the\nnumber of classes that we have. In this case, there's 10 different classes, but in our case,\nwe have three different classes, one for pizza, steak, and sushi. So let's replicate the tiny VGG\narchitecture from the CNN explainer website. And this is going to be good practice, right?\nWe're not going to spend too much time referencing their architecture. We're going to spend more\ntime coding here. But of course, before we can train a model, what do we have to do? Well,\nlet's go 7.1. We're going to create some transforms and loading data. We're going to load data for\nmodel zero. Now, we could of course use some of the variables that we already have loaded. But\nwe're going to recreate them just to practice. So let's create a simple transform. And what is\nour whole premise of loading data for model zero? We want to get our data from the data folder,\nfrom pizza, steak sushi, from the training and test folders, from their respective folders,\nwe want to load these images and turn them into tenses. Now we've done this a few times now.\nAnd one of the ways that we can do that is by creating a transform equals transforms dot compose.\nAnd we're going to pass in, let's resize it. So transforms dot resize, we're going to resize our\nimages to be the same size as the tiny VGG architecture on the CNN explainer website. 64\n64 three. And then we're also going to pass in another transform to tensor. So that our\nimages get resized to 64 64. And then they get converted into tenses. And particularly,\nthese values within that tensor are going to be between zero and one. So there's our transform.\nNow we're going to load some data. If you want to pause the video here and try to load it yourself,\nI'd encourage you to try out option one, loading image data using the image folder class,\nand then turn that data set, that image folder data set into a data loader. So batchify it so\nthat we can use it with a pytorch model. So give that a shot. Otherwise, let's go ahead and do\nthat together. So one, we're going to load and transform data. We've done this before,\nbut let's just rehash on it what we're doing. So from torch vision import data sets, then we're\ngoing to create the train data simple. And I call this simple because we're going to use at first\na simple transform, one with no data augmentation. And then later on for another modeling experiment,\nwe're going to create another transform one with data augmentation. So let's put this here\ndata sets image folder. And let's go the route equals the training directory. And then the\ntransform is going to be what? It's going to be our simple transform that we've got above.\nAnd then we can put in test data simple here. And we're going to create data sets dot image\nfolder. And then we're going to pass in the route as the test directory. And we'll pass in the\ntransform is going to be the simple transform again above. So we're performing the same\ntransformation here on our training data, and on our testing data. Then what's the next step\nwe can do here? Well, we can to turn the data sets into data loaders. So let's try it out.\nFirst, we're going to import OS, then from torch dot utils dot data, we're going to import data\nloader. And then we're going to set up batch size and number of workers. So let's go batch size.\nWe're going to use a batch size of 32 for our first model.\nNumb workers, which will be the number of excuse me, got a typo up here classic number of workers,\nwhich will be the what the number of CPU cores that we dedicate towards loading our data.\nSo let's now create the data loaders. We're going to create train data loader simple,\nwhich will be equal to data loader. And the data set that goes in here will be train data\nsimple. Then we can set the batch size equal to the batch size parameter that we just created,\nor hyper parameter that is, recall a hyper parameter is something that you can set yourself. We\nwould like to shuffle the training data. And we're going to set numb workers equal to numb workers.\nSo in our case, how many calls does Google Colab have? Let's just run this. Find out how many\nnumb workers there are. I think there's going to be two CPUs. Wonderful. And then we're going to do\nthe same thing for the test data loader. Test data loader simple. We're going to go data loader.\nWe'll pass in the data set here, which is going to be the test data simple. And then we're going\nto go batch size equals batch size. We're not going to shuffle the test data set. And then the\nnumb workers will just set it to the same thing as we've got above. Beautiful. So I hope you gave\nthat a shot, but now do you see how quickly we can get our data loaded if it's in the right format?\nI know we spent a lot of time going through all of these steps over multiple videos and\nwriting lots of code, but this is how quickly we can get set up to load our data. We create a\nsimple transform, and then we load in and transform our data at the same time. And then we turn the\ndata sets into data loaders just like this. Now we're ready to use these data loaders with a model.\nSo speaking of models, how about we build the tiny VGG architecture in the next video? And in\nfact, we've already done this in notebook number three. So if you want to refer back to the model\nthat we built there, right down here, which was model number two, if you want to refer back to\nthis section and give it a go yourself, I'd encourage you to do so. Otherwise, we'll build tiny VGG\narchitecture in the next video. Welcome back. In the last video, we got set up starting to get\nready to model our first custom data set. And I issued you the challenge to try and replicate\nthe tiny VGG architecture from the CNN explainer website, which we covered in notebook number\nthree. But now let's see how fast we can do that together. Hey, I'm going to write down here section\nseven point two. And I know we've already coded this up before, but it's good practice to see what\nit's like to build pytorch models from scratch, create tiny VGG model class. So the model is going\nto come from here. Previously, we created our model, there would have been one big change from\nthe model that we created in section number three, which is that our model in section number three\nused black and white images. But now the images that we have are going to be color images. So\nthere's going to be three color channels rather than one. And there might be a little bit of a\ntrick that we have to do to find out the shape later on in the classifier layer. But let's get\nstarted. We've got class tiny VGG, we're going to inherit from nn.module. This is going to be\nthe model architecture copying tiny VGG from CNN explainer. And remember that it's a it's\nquite a common practice in machine learning to find a model that works for a problem similar to\nyours and then copy it and try it on your own problem. So I only want two underscores there.\nWe're going to initialize our class. We're going to give it an input shape, which will be an int.\nWe're going to say how many hidden units do we want, which will also be an int. And we're going\nto have an output shape, which will be an int as well. And it's going to return something none\nof type none. And if we go down here, we can initialize it with super dot underscore init.\nBeautiful. And now let's create the first COM block. So COM block one, which we'll recall\nwill be this section of layers here. So COM block one, let's do an nn.sequential to do so.\nNow we need com relu com relu max pool. So let's try this out. And then com to D.\nThe in channels is going to be the input shape of our model. The input shape parameter.\nThe out channels is going to be the number of hidden units we have, which is from\nOh, I'm gonna just put enter down here input shape hidden units. We're just getting those\nto there. Let's set the kernel size to three, which will be how big the convolving window will be\nover our image data. There's a stride of one and the padding equals one as well. So these are the\nsimilar parameters to what the CNN explainer website uses. And we're going to go and then\nrelu. And then we're going to go and then com to D. And I want to stress that even if someone\nelse uses like certain values for these, you don't have to copy them exactly. So just keep that in\nmind. You can try out various values of these. These are all hyper parameters that you can set\nyourself. Hidden units, out channels, equals hidden units as well. Then we're going to go kernel\nsize equals three stride equals one. And we're going to put padding equals one as well.\nThen we're going to have another relu layer. And I believe I forgot my comma up here.\nAnother relu layer here. And we're going to finish off\nwith an N dot max pool 2D. And we're going to put in the kernel size.\nThese equals two and the stride here equals two. Wonderful. So oh, by the way, for max\npool 2D, the default stride value is same as the kernel size. So let's have a go here.\nWhat can we do now? Well, we could just replicate this block as block two. So how about we copy this\ndown here? We've already had enough practice writing this sort of code. So we're going to\ngo comp block two, but we need to change the input shape here. The input shape of this block\ntwo is going to receive the output shape here. So we need to line those up. This is going to be\nhidden units. Hidden units. And I believe that's all we need to change there. Beautiful. So let's\ncreate the classifier layer. And the classifier layer recall is going to be this output layer\nhere. So we need at some point to add a linear layer. That's going to have a number of outputs\nequal to the number of classes that we're working with. And in this case, the number of classes is\n10. But in our case, our custom data set, we have three classes, pizza, steak, sushi. So let's\ncreate a classifier layer, which will be an end sequential. And then we're going to pass in an end\ndot flatten to turn the outputs of our convolutional blocks into feature vector into a feature vector\nsite. And then we're going to have an end dot linear. And the end features, do you remember my\ntrick for calculating the shape in features? I'm going to put hidden units here for the time being.\nOut features is going to be output shape. So I put hidden units here for the time being because\nwe don't quite yet know what the output shape of all of these operations is going to be. Of course,\nwe could calculate them by hand by looking up the formula for input and output shapes of convolutional\nlayers. So the input and output shapes are here. But I prefer to just do it programmatically and let\nthe errors tell me where I'm wrong. So we can do that by doing a forward pass. And speaking of a\nforward pass, let's create a forward method, because every time we have to subclass an end\ndot module, we have to override the forward method. We've done this a few times. But as you can see,\nI'm picking up the pace a little bit because you've got this. So let's pass in the conv block one,\nwe're going to go X, then we're going to print out x dot shape. And then we're going to reassign\nX to be self.com block two. So we're passing it through our second block of convolutional layers,\nprint X dot shape to check the shape here. Now this is where our model will probably error\nis because the input shape here isn't going to line up in features, hidden units, because we've\npassed all of the output of what's going through comp block one, comp block two to a flatten layer,\nbecause we want a feature vector to go into our nn.linear layer, our output layer, which has an\nout features size of output shape. And then we're going to return X. So I'm going to print x dot\nshape here. And I just want to let you in on one little secret as well. We haven't covered this\nbefore, but we could rewrite this entire forward method, this entire stack of code,\nby going return self dot classifier, and then going from the outside in. So we could pass in\ncomp block two here, comp block two, and then self comp block one, and then X on the inside.\nSo that is essentially the exact same thing as what we've done here, except this is going to\nbenefits from operator fusion. Now this topic is beyond the scope of this course,\nessentially, all you need to know is that operator fusion behind the scenes speeds up\nhow your GPU performs computations. So all of these are going to happen in one step,\nrather than here, we are reassigning X every time we make a computation through these layers.\nSo we're spending time going from computation back to memory, computation back to memory,\nwhereas this kind of just chunks it all together in one hit. If you'd like to read\nmore about this, I'd encourage you to look up the blog post, how to make your GPUs go\nbur from first principles, and bur means fast. That's why I love this post, right?\nBecause it's half satire, half legitimately, like GPU computer science. So if you go in here,\nyeah, here's what we want to avoid. We want to avoid all of this transportation between\nmemory and compute. And then if we look in here, we might have operator fusion. There we go.\nThis is operator fusion, the most important optimization in deep learning compilers. So\nI will link this, making deep learning go bur from first principles by Horace Hare,\na great blog post that I really like, right here. So if you'd like to read more on that,\nit's also going to be in the extracurricular section of the course. So don't worry, it'll be there.\nNow, we've got a model. Oh, where do we, where do we forget a comma? Right here, of course we did.\nAnd we've got another, we forgot another comma up here. Did you notice these?\nBeautiful. Okay. So now we can create our model by going torch or an instance of the tiny VGG\nto see if our model holds up. Let's create model zero equals tiny VGG. And I'm going to pass in\nthe input shape. What is the input shape? It's going to be the number of color channels of our\nimage. So number of color channels in our image data, which is three, because we have color images.\nAnd then we're going to put in hidden units, equals 10, which will be the same number of\nhidden units as the tiny VGG architecture. One, two, three, four, five, six, seven, eight, nine,\n10. Again, we could put in 10, we could put in 100, we could put in 64, which is a good multiple\nof eight. So let's just leave it at 10 for now. And then the output shape is going to be what?\nIt's going to be the length of our class names, because we want one hidden unit or one output unit\nper class. And then we're going to send it to the target device, which is of course CUDA. And then\nwe can check out our model zero here. Beautiful. So that took a few seconds, as you saw there,\nto move to the GPU memory. So that's just something to keep in mind for when you build\nlarge neural networks and you want to speed up their computation, is to use operator fusion\nwhere you can, because as you saw, it took a few seconds for our model to just move from the CPU,\nwhich is the default to the GPU. So we've got our architecture here. But of course, we know that\nthis potentially is wrong. And how would we find that out? Well, we could find the right hidden\nunit shape or we could find that it's wrong by passing some dummy data through our model. So\nthat's one of my favorite ways to troubleshoot a model. Let's in the next video pass some dummy\ndata through our model and see if we've implemented the forward pass correctly. And also check the\ninput and output shapes of each of our layers. I'll see you there. In the last video, we replicated\nthe tiny VGG architecture from the CNN explainer website, very similar to the model that we built\nin section 03. But this time, we're using color images instead of grayscale images. And we did\nit quite a bit faster than what we previously did, because we've already covered it, right?\nAnd you've had some experience now building pilotage models from scratch.\nSo we're going to pick up the pace when we build our models. But let's now go and try a dummy\nforward pass to check that our forward method is working correctly and that our input and output\nshapes are correct. So let's create a new heading. Try a forward pass on a single image. And this\nis one of my favorite ways to test the model. So let's first get a single image. Get a single\nimage. We want an image batch. Maybe we get an image batch, get a single image batch, because\nwe've got images that are batches already image batch. And then we'll get a label batch. And we'll\ngo next, it a train data loader. Simple. That's the data loader that we're working with for now.\nAnd then we'll check image batch dot shape and label batch dot shape.\nWonderful. And now let's see what happens. Try a forward pass.\nOh, I spelled single wrong up here. Try a forward pass. We could try this on a single image trying\nit on a same batch will result in similar results. So let's go model zero. And we're just going to\npass it in the image batch and see what happens. Oh, no. Of course, we get that input type,\ntorch float tensor and wait type torch CUDA float tensor should be the same or input should be.\nSo we've got tensors on a different device, right? So this is on the CPU, the image batch,\nwhereas our model is, of course, on the target device. So we've seen this error a number of times.\nLet's see if this fixes it. Oh, we get an other error. And we kind of expected this type of error.\nWe've got runtime error amount one and mat two shapes cannot be multiplied. 32. So that looks\nlike the batch size 2560 and 10. Hmm, what is 10? Well, recall that 10 is the number of hidden\nunits that we have. So this is the size here. That's 10 there. So it's trying to multiply\na matrix of this size by this size. So 10 has got something going on with it. We need to get\nthese two numbers, the middle numbers, to satisfy the rules of matrix multiplication,\nbecause that's what happens in our linear layer. We need to get these two numbers the same.\nAnd so our hint and my trick is to look at the previous layer. So if that's our batch size,\nwhere does this value come from? Well, could it be the fact that a tensor of this size goes\nthrough the flatten layer? Recall that we have this layer up here. So we've printed out the shape\nhere of the conv block, the output of conv block one. Now this shape here is the output of conv\nblock two. So we've got this number, the output of conv block one, and then the output of conv\nblock two. So that must be the input to our classifier layer. So if we go 10 times 16 times 16,\nwhat do we get? 2560. Beautiful. So we can multiply our hitting units 10 by 16 by 16, which is the\nshape here. And we get 2560. Let's see if that works. We'll go up here, times 16 times 16.\nAnd let's see what happens. We'll rerun the model, we'll rerun the image batch, and then we'll pass\nit. Oh, look at that. Our model works. Or the shapes at least line up. We don't know if it works\nyet. We haven't started training yet. But this is the output size. We've got the output. It's on\nthe CUDA device, of course. But we've got 32 samples with three numbers in each. Now these are going\nto be as good as random, because we haven't trained our model yet. We've only initialized it here\nwith random weights. So we've got 32 or a batch worth of random predictions on 32 images.\nSo you see how the output shape here three corresponds to the output shape we set up here.\nOutput shape equals length class names, which is exactly the number of classes that we're dealing\nwith. But I think our number is a little bit different to what's in the CNN explainer 1616.\nHow did they end up with 1313? You know what? I think we got one of these numbers wrong,\nkernel size, stride, padding. Let's have a look. Jump into here. If we wanted to truly replicate it,\nis there any padding here? I actually don't think there's any padding here. So what if we go back\nhere and see if we can change this to zero and change this to zero? Zero. I'm not sure if this\nwill work, by the way. If it doesn't, it's not too bad, but we're just trying to line up the shapes\nwith the CNN explainer to truly replicate it. So the output of the COM Block 1 should be 30-30-10.\nWhat are we working with at the moment? We've got 32-32-10. So let's see if removing the padding\nfrom our convolutional layers lines our shape up with the CNN explainer. So I'm going to rerun\nthis, rerun our model. I've set the padding to zero on all of our padding hyper parameters.\nOh, and we get another error. We get another shape error. Of course we do,\nbecause we've now got different shapes. Wow, do you see how often that these errors come up?\nTrust me, I spend a lot of time troubleshooting these shape errors. So we now have to line up\nthese shapes. So we've got 13-13-10. Now does that equal 16-90? Let's try it out. 13-13-10.\n16-90. Beautiful. And do our shapes line up with the CNN explainer? So we've got 30-30-10.\nRemember, these are in PyTorch. So color channels first, whereas this is color channels last. So\nyeah, we've got the output of our first COM Block is lining up here. That's correct.\nAnd then same with the second block. How good is that? We've officially replicated the CNN explainer\nmodel. So we can take this value 13-13-10 and bring it back up here. 13-13-10. Remember,\nhidden units is 10. So we're just going to multiply it by 13-13. You could calculate\nthese shapes by hand, but my trick is I like to let the error codes give me a hint of where to go.\nAnd boom, there we go. We get it working again. Some shape troubleshooting on the fly. So now\nwe've done a single forward pass on the model. We can kind of verify that our data at least flows\nthrough it. What's next? Well, I'd like to show you another little package that I like to use\nto also have a look at the input and output shapes of my model. And that is called Torch Info. So\nyou might want to give this a shot before we go into the next video. But in the next video,\nwe're going to see how we can use Torch Info to print out a summary of our model. So we're\ngoing to get something like this. So this is how beautifully easy Torch Info is to use. So\ngive that a shot, install it into Google CoLab and run it in a cell here. See if you can get\nsomething similar to this output for our model zero. And I'll see you in the next video. We'll try\nthat together. In the last video, we checked our model by doing a forward pass on a single batch.\nAnd we learned that our forward method so far looks like it's intact and that we don't get any\nshape errors as our data moves through the model. But I'd like to introduce to you one of my\nfavorite packages for finding out information from a PyTorch model. And that is Torch Info.\nSo let's use Torch Info to get an idea of the shapes going through our model. So you know how\nmuch I love doing things in a programmatic way? Well, that's what Torch Info does. Before,\nwe used print statements to find out the different shapes going through our model.\nAnd I'm just going to comment these out in our forward method so that when we run this later on\nduring training, we don't get excessive printouts of all the shapes. So let's see what Torch Info\ndoes. And in the last video, I issued a challenge to give it a go. It's quite straightforward of\nhow to use it. But let's see it together. This is the type of output we're looking for from our\ntiny VGG model. And of course, you could get this type of output from almost any PyTorch model.\nBut we have to install it first. And as far as I know, Google CoLab doesn't come with Torch Info\nby default. Now, you might as well try this in the future and see if it works. But yeah, I don't\nget this module because my Google CoLab instance doesn't have an install. No problem with that.\nLet's install Torch Info here. Install Torch Info and then we'll import it if it's available.\nSo we're going to try and import Torch Info. If it's already installed, we'll import it.\nAnd then if it doesn't work, if that try block fails, we're going to run pip install Torch Info.\nAnd then we will import Torch Info. And then we're going to run down here from Torch Info,\nimport summary. And then if this all works, we're going to get a summary of our model. We're going\nto pass it in model zero. And we have to put in an input size here. Now that is an example of the\nsize of data that will flow through our model. So in our case, let's put in an input size of 1,\n3, 64, 64. So this is an example of putting in a batch of one image. You could potentially\nput in 32 here if you wanted, but let's just put in a batch of a singular image. And of course,\nwe could change these values here if we wanted to, 24 to 24. But what you might notice is that if\nit doesn't get the right input size, it produces an error. There we go. So just like we got before\nwhen we printed out our input sizes manually, we get an error here. Because what Torch Info\nbehind the scenes is going to do is it's going to do a forward pass on whichever model you pass\nit with an input size of whichever input size you give it. So let's put in the input size that\nour model was built for. Wonderful. So what Torch Info gives us is, oh, excuse me, we didn't\ncomment out the printouts before. So just make sure we've commented out these printouts in the\nforward method of our 20 VGG class. So I'm just going to run this, then we run that, run that,\njust to make sure everything still works. We'll run Torch Info. There we go. So no printouts\nfrom our model, but this is, look how beautiful this is. I love how this prints out. So we have\nour tiny VGG class, and then we can see it's comprised of three sequential blocks. And then\ninside those sequential blocks, we have different combinations of layers. We have some conv layers,\nsome relu layers, some max pool layers. And then the final layer is our classification layer\nwith a flatten and a linear layer. And we can see the shapes changing throughout our model.\nAs our data goes in and gets manipulated by the various layers. So are these in line with\nthe CNN explainer? So if we check this last one, we've already verified this before.\nAnd we also get some other helpful information down here, which is total params. So you can see\nthat each of these layers has a different amount of parameters to learn. Now, recall that a parameter\nis a value such as a weight or a bias term within each of our layers, which starts off as a random\nnumber. And the whole goal of deep learning is to adjust those random numbers to better represent\nour data. So in our case, we have just over 8000 total parameters. Now this is actually quite small.\nIn the future, you'll probably play around with models that have a million parameters or more.\nAnd models now are starting to have many billions of parameters. And we also get some\ninformation here, such as how much the model size would be. Now this would be very helpful,\ndepending on where we had to put our model. So what you'll notice is that as a model gets larger,\nas more layers, it will have more parameters, more weights and bias terms that can be adjusted\nto learn patterns and data. But its input size and its estimated total size would definitely get\nbigger as well. So that's just something to keep in mind if you have size constraints in terms of\nstorage in your future applications. So ours is under a megabyte, which is quite small. But you\nmight find that some models in the future get up to 500 megabytes, maybe even over a gigabyte.\nSo just keep that in mind for going forward. And that's the crux of torch info, one of my\nfavorite packages, just gives you an idea of the input and output shapes of each of your layers.\nSo you can use torch info wherever you need. It should work with most of your PyTorch models.\nJust be sure to pass it in the right input size. You can also use it to verify like we did before,\nif the input and output shapes are correct. So check that out, big shout out to Tyler Yup,\nand everyone who's created the torch info package. Now in the next video, let's move towards training\nour tiny VGG model. We're going to have to create some training and test functions. If you want to\njump ahead, we've already done this. So I encourage you to go back to section 6.2 in the\nfunctionalizing training and test loops. And we're going to build functions very similar to this,\nbut for our custom data set. So if you want to replicate these functions in this notebook,\ngive that a go. Otherwise, I'll see you in the next video and we'll do it together.\nHow'd you go? Did you give it a shot? Did you try replicating the train step and the test step\nfunction? I hope you did. Otherwise, let's do that in this video, but this time we're going to do\nit for our custom data sets. And what you'll find is not much, if anything, changes, because\nwe've created our train and test loop functions in such a way that they're generic. So we want\nto create a train step function. And by generic, I mean they can be used with almost any model and\ndata loader. So train step is takes in a model and data loader and trains the model on the data\nloader. And we also want to create another function called test step, which takes in\na model and a data loader and other things and evaluates the model on the data loader. And of course,\nfor the train step and for the test step, each of them respectively are going to take a training\ndata loader. I just might make this a third heading so that our outline looks nice, beautiful.\nSection seven is turning out to be quite a big section. Of course, we want them to be\nrespectively taken their own data loader. So train takes in the train data loader, test takes in the\ntest data loader. Without any further ado, let's create the train step function. Now we've seen\nthis one in the computer vision section. So let's see what we can make here. So we need a train\nstep, which is going to take in a model, which will be a torch and then dot module. And we want\nit also to take in a data loader, which will be a torch dot utils dot data dot data loader.\nAnd then it's going to take in a loss function, which is going to be a torch and then\ndot module as well. And then it's going to take in an optimizer, which is going to be torch\nopt in dot optimizer. Wonderful. And then what do we do? What's the first thing that we do in\na training step? Well, we put the model in train mode. So let's go model dot train.\nThen what shall we do next? Well, let's set up some evaluation metrics, one of them being loss\nand one of them being accuracy. So set up train loss and train accuracy values. And we're going\nto accumulate these per batch because we're working with batches. So we've got train loss\nand train act equals zero, zero. Now we can loop through our data loader. So let's write loop through\ndata loader. And we'll loop through each of the batches in this because we've batchified our\ndata loader. So for batch x, y, in enumerate data loader, we want to send the data to the target\ndevice. So we could even put that device parameter up here. Device equals device. We'll set that\nto device by default. And then we can go x, y equals x dot two device. And y dot two device.\nBeautiful. And now what do we do? Well, remember the pie torch, the unofficial pie torch optimization\nsong, we do the forward pass. So y pred equals model om x. And then number two is we calculate the\nlast. So calculate the loss. Let's go loss equals loss function. And we're going to pass it in\ny pred y. We've done this a few times now. So that's why we're doing it a little bit faster.\nSo I hope you noticed that the things that we've covered before, I'm stepping up the pace a bit.\nSo it might be a bit of a challenge, but that's all right, you can handle it. And then, so that's\naccumulating the loss. So we're starting from zero up here. And then each batch, we're doing a forward\npass, calculating the loss, and then adding it to the overall train loss. And so we're going to\noptimize a zero grad. So zero, the gradients of the optimizer for each new batch. And then we're\ngoing to perform back propagation. So loss backwards. And then five, what do we do? Optimize a step,\nstep, step. Wonderful. Look at that. Look at us coding a train loop in a minute or so.\nNow, let's calculate the accuracy and accumulate it. Calculate the, you notice that we don't have\nan accuracy function here. That's because accuracy is quite a straightforward metric to calculate.\nSo we'll first get the, the y pred class, because this is going to output model logits.\nAs we've seen before, the raw output of a model is logits. So to get the class, we're going to take\nthe arg max torch dot softmax. So we'll get the prediction probabilities of y pred, which is the\nraw logits, what we've got up here, across dimension one, and then also across dimension one here.\nBeautiful. So that should give us the labels. And then we can find out if this is wrong by\nchecking it later on. And then we're going to create the accuracy by taking the y pred class,\nchecking for a quality with the right labels. So this is going to give us how many of these\nvalues equal true. And we want to take the sum of that, take the item of that, which is just a\nsingle integer. And then we want to divide it by the length of y pred. So we're just getting the\ntotal number that are right, and dividing it by the length of samples. So that's the formula for\naccuracy. Now we can come down here outside of the batch loop, we know that because we've got this\nhelpful line drawn here. And we can go adjust metrics to get the average loss and accuracy\nper batch. So we're going to set train loss is equal to train loss, divided by the length of\nthe data loader. So the number of batches in total. And the train accuracy is the train\nact, divided by the length of the data loader as well. So that's going to give us the average\nloss and average accuracy per epoch across all batches. So train act. Now that's a pretty good\nlooking function to me for a train step. Do you want to take on the test step? So pause the video,\ngive it a shot, and you'll get great inspiration from this notebook here. Otherwise, we're going\nto do it together in three, two, one, let's do the test step. So create a test step function.\nSo we want to be able to call these functions in an epoch loop. And that way, instead of writing\nout training and test code for multiple different models, we just write it out once, and we can\ncall those functions. So let's create def test step, we're going to do model, which is going to be\nif I could type torch and then module. And then we're going to do data loader,\nwhich is torch utils dot data, that data loader, capital L there. And then we're going to just\npass in a loss function here, because we don't need an optimizer for the test function. We're\nnot trying to optimize anything, we're just trying to evaluate how our model did on the training\ndataset. And let's put in the device here, why not? That way we can change the device if we need\nto. So put model in a val mode, because we're going to be evaluating or we're going to be testing.\nThen we can set up test loss and test accuracy values. So test loss and test act. We're going\nto make these zero, we're going to accumulate them per batch. But before we go through the batch,\nlet's turn on inference mode. So this is behind the scenes going to take care of a lot of pie torch\nfunctionality that we don't need. That's very helpful during training, such as tracking gradients.\nBut during testing, we don't need that. So loop through data loader or data batches.\nAnd we're going to go for batch x, y in enumerate data loader. You'll notice that above, we didn't\nactually use this batch term here. And we probably won't use it here either. But I just like to go\nthrough and have that there in case we wanted to use it anyway. So send data to the target device.\nSo we're going to go x, y equals x dot two device. And same with y dot two device. Beautiful. And\nthen what do we do for an evaluation step or a test step? Well, of course, we do the forward pass,\nforward pass. And we're going to, let's call these test pred logits and get the raw outputs of our\nmodel. And then we can calculate the loss on those raw outputs, calculate the loss. We get the loss\nis equal to loss function on test pred logits versus y. And then we're going to accumulate the\nloss. So test loss plus equals loss dot item. Remember, item just gets a single integer from\nwhatever term you call it on. And then we're going to calculate the accuracy. Now we can do this\nexactly how we've done for the training data set or the training step. So test pred labels,\nwe're going to, you don't, I just want to highlight the fact that you actually don't need to take\nthe softmax here, you could just take the argmax directly from this. The reason why we take the\nsoftmax. So you could do the same here, you could just directly take the argmax of the logits. The\nreason why we get the softmax is just for completeness. So if you wanted the prediction probabilities,\nyou could use torch dot softmax on the prediction logits. But it's not 100% necessary to get the\nsame values. And you can test this out yourself. So try this with and without the softmax and\nsee if you get the same results. So we're going to go test accuracy. Plus equals, now we'll just\ncreate our accuracy calculation on the fly test pred labels. We'll check for equality on the y,\nthen we'll get the sum of that, we'll get the item of that, and then we'll divide that by the\nlength of the test pred labels. Beautiful. So it's going to give us accuracy per batch. And so now\nwe want to adjust the metrics to get average loss and accuracy per batch. So test loss equals\ntest loss divided by length of the data loader. And then we're going to go test,\nac equals test, act divided by length of the data loader. And then finally, we're going to\nreturn the test loss, not lost, and test accuracy. Look at us go. Now, in previous videos, that took\nus, or in previous sections, that took us a fairly long time. But now we've done it in about 10\nminutes or so. So give yourself a pat in the back for all the progress you've been making.\nBut now let's in the next video, we did this in the computer vision section as well. We created,\ndo we create a train function? Oh, no, we didn't. But we could. So let's create a function to\nfunctionize this. We want to train our model. I think we did actually. Deaf train, we've done\nso much. I'm not sure what we've done. Oh, okay. So looks like we might not have. But in the next\nvideo, give yourself this challenge, create a function called train that combines these two\nfunctions and loops through them both with an epoch range. So just like we've done here in the\nprevious notebook, can you functionize this? So just this step here. So you'll need to take in a\nnumber of epochs, you'll need to take in a train data loader and a test data loader, a model, a\nloss function, an optimizer, and maybe a device. And I think you should be pretty on your way to\nall the steps we need for train. So give that a shot. But in the next video, we're going to create\na function that combines train step and test step to train a model. I'll see you there.\nHow'd you go? In the last video, I issued you the challenge to combine our train step function,\nas well as our test step function together in their own function so that we could just call\none function that calls both of these and train a model and evaluate it, of course.\nSo let's now do that together. I hope you gave it a shot. That's what it's all about. So we're\ngoing to create a train function. Now the role of this function is going to, as I said, combine\ntrain step and test step. Now we're doing all of this on purpose, right, because we want to not\nhave to rewrite all of our code all the time. So we want to be functionalizing as many things as\npossible, so that we can just import these later on, if we wanted to train more models and just\nleverage the code that we've written before, as long as it works. So let's see if it does,\nwe're going to create a train function. I'm going to first import TQDM, TQDM.auto,\nbecause I'd like to get a progress bar while our model is training. There's nothing quite like\nwatching a neural network train. So step number one is we need to create a train function that takes\nin various model parameters, plus optimizer, plus data loaders, plus a loss function. A whole\nbunch of different things. So let's create def train. And I'm going to pass in a model here,\nwhich is going to be torch and then dot module. You'll notice that the inputs of this are going\nto be quite similar to our train step and test step. I don't actually need that there.\nSo we also want a train data loader for the training data, torch dot utils dot data dot data\nloader. And we also want a test data loader, which is going to be torch dot utils dot data\ndot data loader. And then we want an optimizer. So the optimizer will only be used with our\ntraining data set, but that's okay. We can take it as an input of the miser. And then we want a\nloss function. This will generally be used for both our training and testing step. Because that's\nwhat we're combining here. Now, since we're working with multi class classification,\nI'm going to set our loss function to be a default of an n dot cross entropy loss.\nThen I'm going to get epochs. I'm going to set five, we'll train for five epochs by default.\nAnd then finally, I'm going to set the device equal to the device. So what do we get wrong here?\nThat's all right. We'll just keep coding. We'll ignore these little red lines. If they\nstay around, we'll come back to them. So step number two, I'm going to create. This is a step\nyou might not have seen, but I'm going to create an empty results dictionary. Now, this is going\nto help us track our results. Do you recall in a previous notebook, we outputted a model dictionary\nfor how a model went. So if we look at model one results, yeah, we got a dictionary like this.\nSo I'd like to create one of these on the fly, but keep track of the result every epoch. So what\nwas the loss on epoch number zero? What was the accuracy on epoch number three? So we'll show you\nhow I'll do that. We can use a dictionary and just update that while our model trains.\nSo results, I want to keep track of the train loss. So we're going to set that equal to an empty\nlist and just append to it. I also want to keep track of the train accuracy. We'll set that as\nan empty list as well. I also want to keep track of the test loss. And I also want to keep track\nof the test accuracy. Now, you'll notice over time that these, what you can track is actually\nvery flexible. And what your functions can do is also very flexible. So this is not the gold\nstandard of doing anything by any means. It's just one way that works. And you'll probably find in\nthe future that you need different functionality. And of course, you can code that out. So let's\nnow loop through our epochs. So for epoch in TQDM, let's create a range of our epochs above.\nAnd then we can set the train loss. Have I missed a comma up here somewhere?\nType annotation not supported for that type of expression. Okay, that's all right. We'll just leave\nthat there. So we're going to go train loss and train act, recall that our train step function\nthat we created in the previous video, train step returns our train loss and train act. So as I\nsaid, I want to keep track of these throughout our training. So I'm going to get them from train\nstep. Then for each epoch in our range of epochs, we're going to pass in our model and perform a\ntraining step. So the data loader here is of course going to be the train data loader. The\nloss function is just going to be the loss function that we pass into the train function.\nAnd then the optimizer is going to be the optimizer. And then the device is going to be device.\nBeautiful. Look at that. We just performed a training step in five lines of code.\nSo let's keep pushing forward. It's telling us we've got a whole bunch of different things here.\nEpox is not defined. Maybe we just have to get rid of this. We can't have the type annotation here.\nAnd that'll that'll stop. That'll stop Google Colab getting angry at us. If it does anymore,\nI'm just going to ignore it for now. Epox. Anyway, we'll leave it at that. We'll find out if there's\nan error later on. Test loss. You might be able to find it before I do. So test step. We're going\nto pass in the model. We're going to pass in a data loader. Now this is going to be the test data\nloader. Look at us go. Grading training and test step functions, loss function. And then we don't\nneed an optimizer. We're just going to pass in the device. And then behind the scenes,\nboth of these functions are going to train and test our model. How cool is that? So still within\nthe loop. This is important. Within the loop, we're going to have number four is we're going to\nprint out. Let's print out what's happening. Print out what's happening. We can go print.\nAnd we'll do a fancy little print statement here. We'll get the epoch. And then we will get\nthe train loss, which will be equal to the train loss. We'll get that to, let's go\nfour decimal places. How about that? And then we'll get the train accuracy, which is going to be the\ntrain act. We'll get that to four, maybe three decimal of four, just for just so it looks nice.\nIt looks aesthetic. And then we'll go test loss. We'll get that coming out here. And we'll pass\nin the test loss. We'll get that to four decimal places as well. And then finally, we'll get the\ntest accuracy. So a fairly long print statement here. But that's all right. We'd like to see how\nour model is doing while it's training. Beautiful. And so again, still within the epoch, we want to\nupdate our results dictionary so that we can keep track of how our model performed over time.\nSo let's pass in results. We want to update the train loss. And so this is going to be this.\nAnd then we can append our train loss value. So this is just going to expend the list in here\nwith the train loss value, every epoch. And then we'll do the same thing on the train accuracy,\nappend train act. And then we'll do the same thing again with test loss dot append test loss.\nAnd then we will finally do the same thing with the test accuracy test accuracy. Now,\nthis is a pretty big function. But this is why we write the code now so that we can use it\nmultiple times later on. So return the field results at the end of the epoch. So outside the\nepochs loop. So our loop, we're outside it now. Let's return results. Now, I've probably got an\nerror somewhere here and you might be able to spot it. Okay, train data loader. Where do we get\nthat invalid syntax? Maybe up here, we don't have a comma here. Was that the issue the whole time?\nWonderful. You might have seen that I'm completely missed that. But we now have a train function\nto train our model. And the train function, of course, is going to call out our train step\nfunction and our test step function. So what's left to do? Well, nothing less than train and\nevaluate model zero. So our model is way back up here. How about in the next video, we leverage\nour functions, namely just the train function, because it's going to call our train step function\nand our test step function and train our model. So I'm going to encourage you to give that a go.\nYou're going to have to go back to the workflow. Maybe you'll maybe already know this.\nSo what have we done? We've got our data ready and we turned it into tenses using a combination\nof these functions. We've built and picked a model while we've built a model, which is the\ntiny VGG architecture. Have we created a loss function yet? I don't think we have or an optimizer.\nI don't think we've done that yet. We've definitely built a training loop though.\nWe aren't using torch metrics. We're just using accuracy, but we could use this if we want.\nWe haven't improved through experimentation yet, but we're going to try this later on and\nthen save and reload the model. We've seen this before. So I think we're up to picking a loss\nfunction and an optimizer. So give that a shot. In the next video, we're going to create a loss\nfunction and an optimizer and then leverage the functions we've spent in the last two videos\ncreating to train our first model model zero on our own custom data set. This is super exciting.\nI'll see you in the next video.\nWho's ready to train and evaluate model zero? Put your hand up.\nI definitely am. So let's do it together. We're going to start off section 7.7 and we're going\nto put in train and evaluate model zero, our baseline model on our custom data set. Now,\nif we refer back to the PyTorch workflow, I issued you the challenge in the last video to try and\ncreate a loss function and an optimizer. I hope you gave that a go, but we've already built a\ntraining loop. So we're going to leverage our training loop functions, namely train, train step\nand test step. All we need to do now is instantiate a model, choose a loss function and an optimizer\nand pass those values to our training function. So let's do that. All right, this is so exciting.\nLet's set the random seeds. I'm going to set torch manual seed 42 and torch cuda manual seed 42.\nNow remember, I just want to highlight something. I read an article the other day about not using\nrandom seeds. The reason why we are using random seeds is for educational purposes. So to try and\nget our numbers on my screen and your screen as close as possible, but in practice, you quite\noften don't use random seeds all the time. The reason why is because you want your models performance\nto be similar regardless of the random seed that you use. So just keep that in mind going forward.\nWe're using random seeds to just exemplify how we can get similar numbers on our page. But\nideally, no matter what the random seed was, our models would go in the same direction.\nThat's where we want our models to eventually go. But we're going to train for five epochs.\nAnd now let's create a recreate an instance of tiny VGG. We can do so because we've created the\ntiny VGG class. So tiny VGG, which is our model zero. We don't have to do this, but we're going\nto do it any later. So we've got all the code in one place, tiny VGG. What is our input shape\ngoing to be? That is the number of color channels of our target images. And because we're dealing\nwith color images, we have an input shape of three. Previously, we used an input shape of one to\ndeal with grayscale images. I'm going to set hidden units to 10 in line with the CNN explainer website.\nAnd the output shape is going to be the number of classes in our training data set. And then,\nof course, we're going to send the target model to the target device. So what do we do now?\nWell, we set up a loss function and an optimizer, loss function, and optimizer.\nSo our loss function is going to be because we're dealing with multiclass classification,\nand then cross entropy, if I could spell cross entropy loss. And then we're going to have an\noptimizer. This time, how about we mix things up? How about we try the atom optimizer? Now,\nof course, the optimizer is one of the hyper parameters that you can set for your model,\nand a hyper parameter being a value that you can set yourself. So the parameters that we want to\noptimize are our model zero parameters. And we're going to set a learning rate of 0.001. Now,\nrecall that you can tweet this learning rate, if you like, but I believe, did I just see that\nthe default learning rate of atom is 0.001? Yeah, there we go. So Adam's default learning rate is\none to the power of 10 to the negative three. And so that is a default learning rate for Adam.\nAnd as I said, oftentimes, different variables in the pytorch library, such as optimizers,\nhave good default values that work across a wide range of problems. So we're just going to stick\nwith the default. If you want to, you can experiment with different values of this.\nBut now let's start the timer, because we want to time our models.\nWe're going to import from time it. We want to get the default timer class. And I'm going to\nimport that as timer, just so we don't have to type out default timer. So the start time is going\nto be timer. This is going to just put a line in the sand of what the start time is at this\nparticular line of code. It's going to measure that. And then we're going to train model zero.\nNow this is using, of course, our train function. So let's write model zero results, and then\nthey wrote model one, but we're not up to there yet. So let's go train model equals model zero.\nAnd this is just the training function that we wrote in a previous video. And the train data\nis going to be our train data loader. And we've got train data loader simple, because we're not\nusing data augmentation for model one. And then our test data loader is going to be our test data\nloader simple. And then we're going to set our optimizer, which is equal to the optimizer we just\ncreated. Friendly atom optimizer. And the loss function is going to be the loss function that\nwe just created, which is an n cross entropy loss. Finally, we can send in epochs is going to be\nnum epochs, which is what we set at the start of this video to five. And of course, we could train\nour model for longer if we wanted to. But the whole idea of when you first start training a model\nis to keep your experiments quick. So that's why we're only training for five, maybe later on you\ntrain for 10, 20, tweak the learning rate, do a whole bunch of different things. But let's go\ndown here, let's end the timer, see how long our models took to train, and the timer and print out\nhow long it took. So in a previous section, we created a helper function for this.\nWe're just going to simplify it in this section. And we're just going to print out how long the\ntraining time was. Total training time. Let's go n time minus start time. And then we're going to go\npoint, we'll take it to three decimal places, hey, seconds, you ready to train our first model,\nour first convolutional neural network on our own custom data set on pizza, stake and sushi\nimages. Let's do it. You're ready? Three, two, one, no errors. Oh, there we go. Okay,\nshould this be trained data loader? Did you notice that? What is our trained data\ntaker's input? Oh, we're not getting a doc string. Oh, there we go. We want trained data\nloader, data loader, and same with this, I believe. Let's try again. Beautiful. Oh, look at that\nlovely progress bar. Okay, how's our model is training quite fast? Okay. All right, what do we\nget? So we get an accuracy on the training data set of about 40%. And we get an accuracy on the\ntest data set of about 50%. Now, what's that telling us? It's telling us that about 50% of the time\nour model is getting the prediction correct. But we've only got three classes. So even if our model\nwas guessing, it would get things right 33% of the time. So even if you just guessed pizza every\nsingle time, because we only have three classes, if you guessed pizza every single time, you get\na baseline accuracy of 33%. So our model isn't doing too much better than our baseline accuracy.\nOf course, we'd like this number to go higher, and maybe it would if it trained for longer.\nSo I'll let you experiment with that. But if you'd like to see some different methods of\nimproving a model, recall back in section number O two, we had an improving a model section,\nimproving a model. Here we go. So here's some things you might want to try.\nWe can improve a model by adding more layers. So if we come back to our tiny VGG architecture,\nright up here, we're only using two convolutional blocks. Perhaps you wanted to add in a convolutional\nblock three. You can also add more hidden units. Right now we're using 10 hidden units. You might\nwant to double that and see what happens. Fitting for longer. This is what we just spoke about.\nSo right now we're only fitting for five epochs. So if you maybe wanted to try double that again,\nand then even double that again, changing the activation functions. So maybe relu is not the\nideal activation function for our specific use case. Change the learning rate. We've spoken\nabout that before. So right now our learning rate is 0.001 for Adam, which is the default.\nBut perhaps there's a better learning rate out there. Change the loss function. This is probably not\nin our case, not going to help too much because cross entropy loss is a pretty good loss for\nmulti class classification. But these are some things that you could try these first three,\nespecially. You could try quite quickly. You could try doubling the layers. You could try\nadding more hidden units. And you could try fitting for longer. So I'd give that a shot.\nBut in the next video, we're going to take our model zero results, which is a dictionary or at\nleast it should be. And we're going to plot some loss curves. So this is a good way to inspect how\nour model is training. Yes, we've got some values here. Let's plot these in the next video. I'll see you there.\nIn the last video, we trained our first convolutional neural network on custom data. So you should be\nvery proud of that. That is no small feat to take our own data set of whatever we want\nand train apply to its model on it. However, we did find that it didn't perform as well as we'd\nlike it to. We also highlighted a few different things that we could try to do to improve it.\nBut now let's plot our models results using a loss curve. So I'm going to write another heading\ndown here. We'll go, I believe we're up to 7.8. So plot the loss curves of model zero. So what\nis a loss curve? So I'm going to write down here, a loss curve is a way of tracking your models\nprogress over time. So if we just looked up Google and we looked up loss curves,\noh, there's a great guide by the way. I'm going to link this. But I'd rather if and doubt code it\nout than just look at guides. Yeah, loss curves. So yeah, loss over time. So there's our loss value\non the left. And there's say steps, which is epochs or batches or something like that.\nThen we've got a whole bunch of different loss curves over here. Essentially, what we want it\nto do is go down over time. So that's the idea loss curve. Let's go back down here.\nAnd a good guide for different loss curves can be seen here. We're not going to go through that\njust yet. Let's focus on plotting our own models, loss curves, and we can inspect those.\nLet's get the model keys. Get the model zero results keys. I'm going to type in model zero\nresults dot keys because it's a dictionary. Let's see if we can write some code to plot these\nvalues here. So yeah, over time. So we have one value for train loss, train,\nact, test loss, and test act for every epoch. And of course, these lists would be longer if we\ntrain for more epochs. But let's just how about we create a function called def plot loss curves,\nwhich will take in a results dictionary, which is of string and a list of floats. So this just\nmeans that our results parameter here is taking in a dictionary that has a string as a key.\nAnd it contains a list of floats. That's what this means here. So let's write a doc string\nplots training curves of a results dictionary. Beautiful. And so we're in this section of our\nworkflow, which is kind of like a, we're kind of doing something similar to TensorBoard, what it\ndoes. I'll let you look into that if you want to. Otherwise, we're going to see it later on.\nBut we're really evaluating our model here. Let's write some plotting code. We're going to use map plot\nlib. So we want to get the lost values of the results dictionary. So this is training and test.\nLet's set loss equal to results train loss. So this is going to be the loss on the training\ndata set. And then we'll create the test loss, which is going to be, well, index on the results\ndictionary and get the test loss. Beautiful. Now we'll do the same and we'll get the accuracy.\nGet the accuracy values of the results dictionary. So training and test.\nThen we're going to go accuracy equals results. This will be the training accuracy train\nact and accuracy. Oh, we'll call this test accuracy actually test accuracy equals results test act.\nNow let's create a number of epochs. So we want to figure out how many epochs we did. We can do\nthat by just counting the length of this value here. So figure out how many epochs there were.\nSo we'll set epochs equal to a range because we want to plot it over time. Our models results\nover time. That's that's the whole idea of a loss curve. So we'll just get the the length of\nour results here. And we'll get the range. So now we can set up a plot.\nLet's go PLT dot figure. And we'll set the fig size equal to something nice and big because\nwe're going to do four plots. We want one for maybe two plots, one for the loss, one for the accuracy.\nAnd then we'll go plot the loss. PLT dot subplot. We're going to create one row, two columns,\nand index number one. We want to put PLT dot plot. And here's where we're going to plot the\ntraining loss. So we get that a label of train loss. And then we'll add another plot with epochs\nand test loss. The label here is going to be test loss. And then we'll add a title, which will be\nloss PLT. Let's put a label on the X, which will be epochs. So we know how many steps we've done.\nThis plot over here, loss curves, it uses steps. I'm going to use epochs. They mean almost the\nsame thing. It depends on what scale you'd like to see your loss curves. We'll get a legend as well\nso that we are the labels appear. Now we're going to plot the accuracy. So PLT dot subplot.\nLet's go one, two, and then index number two that this plot's going to be on PLT dot plot.\nWe're going to go epochs accuracy. And the label here is going to be train accuracy.\nAnd then we'll get on the next plot, which is actually going to be on the same plot.\nWe'll put the test accuracy. That way we have the test accuracy and the training accuracy side\nby side, test accuracy same with the train loss and train, sorry, test loss. And then we'll give\nour plot a title. This plot is going to be accuracy. And then we're going to give it an\nX label, which is going to be epochs as well. And then finally, we'll get the plot, but legend,\na lot of plotting code here. But let's see what this looks like. Hey, if we've done it all right,\nwe should be able to pass it in a dictionary just like this and see some nice plots like this.\nLet's give it a go. And I'm going to call plot loss curves. And I'm going to pass in model 0 results.\nAll righty then. Okay. So that's not too bad. Now, why do I say that? Well, because we're\nlooking here for mainly trends, we haven't trained our model for too long. Quantitatively, we know\nthat our model hasn't performed at the way we'd like it to do. So we'd like the accuracy on both\nthe train and test data sets to be higher. And then of course, if the accuracy is going higher,\nthen the loss is going to come down. So the ideal trend for a loss curve is to go down from\nthe top left to the bottom right. In other words, the loss is going down over time. So that's,\nthe trend is all right here. So potentially, if we train for more epochs, which I'd encourage\nyou to give it a go, our model's loss might get lower. And the accuracy is also trending in the\nright way. Our accuracy, we want it to go up over time. So if we train for more epochs, these curves\nmay continue to go on. Now, they may not, they, you never really know, right? You can guess these\nthings. But until you try it, you don't really know. So in the next video, we're going to have a\nlook at some different forms of loss curves. But before we do that, I'd encourage you to go through\nthis guide here, interpreting loss curves. So I feel like if you just search out loss curves,\nyou're going to find Google's guide, or you could just search interpreting loss curves.\nBecause as you'll see, there's many different ways that loss curves can be interpreted. But the ideal\ntrend is for the loss to go down over time, and metrics like accuracy to go up over time.\nSo in the next video, let's cover a few different forms of loss curves, such as the ideal loss\ncurve, what it looks like when your model's underfitting, and what it looks like when your\nmodel's overfitting. And if you'd like to have a primer on those things, I'd read through this\nguide here. Don't worry too much if you're not sure what's happening. We're going to cover a bit\nmore about loss curves in the next video. I'll see you there. In the last video, we looked at our\nmodel's loss curves, and also the accuracy curves. And a loss curve is a way to evaluate a model's\nperformance over time, such as how long it was training for. And as you'll see, if you Google\nsome images of loss curves, you'll see many different types of loss curves. They come in all\ndifferent shapes and sizes. And there's many different ways to interpret loss curves. So\nthis is Google's testing and debugging and machine learning guide. So I'm going to set this as\nactually curriculum for this section. So we're up to number eight. Let's have a look at what should\nan ideal loss curve look like. So we'll just link that in there. Now, loss curve, I'll just\nrewrite here, is a loss curve is, I'll just make some space. A loss curve is one of the most\nhelpful ways to troubleshoot a model. So the trend of a loss curve, you want it to go down over time,\nand the trend typically of an evaluation metric, like accuracy, you want it to go up over time.\nSo let's go into the keynote, loss curves. So a way to evaluate your model's performance over time.\nThese are three of the main different forms of loss curve that you'll face. But again,\nthere's many different types as mentioned in here, interpreting loss curves. Sometimes you get it\ngoing all over the place. Sometimes your loss will explode. Sometimes your metrics will be\ncontradictory. Sometimes your testing loss will be higher than your training loss. We'll have a\nlook at what that is. Sometimes your model gets stuck. In other words, the loss doesn't reduce.\nLet's have a look at some loss curves here in the case of underfitting, overfitting, and just\nright. So this is the Goldilocks zone. Underfitting is when your model's loss on the training and\ntest data sets could be lower. So in our case, if we go back to our loss curves, of course,\nwe want this to be lower, and we want our accuracy to be higher. So from our perspective,\nit looks like our model is underfitting. And we would probably want to train it for longer,\nsay, 10, 20 epochs to see if this train continues. If it keeps going down, it may stop underfitting.\nSo underfitting is when your loss could be lower. Now, the inverse of underfitting is called\noverfitting. And so two of the biggest problems in machine learning is trying to\nunderfitting. So in other words, make your loss lower and also reduce overfitting. These are\nboth active areas of research because you always want your model to perform better,\nbut you also want it to perform pretty much the same on the training set as it does the test set.\nAnd so overfitting would be when your training loss is lower than your testing loss. And why\nwould this be overfitting? So it means overfitting because your model is essentially learning the\ntraining data too well. And that means the loss goes down on the training data set,\nwhich is typically a good thing. However, this learning is not reflected in the testing data set.\nSo your model is essentially memorizing patterns in the training data set that don't\ngeneralize well to the test data set. So this is where we come to the just right curve is that we\nwant, ideally, our training loss to reduce as much as our test loss. And quite often, you'll find\nthat the loss is slightly lower on the training set than it is on the test set. And that's just\nbecause the model is exposed to the training data, and it's never seen the test data before.\nSo it might be a little bit lower on the training data set than on the test data set.\nSo underfitting, the model's loss could be lower. Overfitting, the model is learning the training\ndata too well. Now, this would be equivalent to say you were studying for a final exam,\nand you just memorize the course materials, the training set. And when it came time to the final\nexam, because you don't even memorize the course materials, you couldn't adapt those skills to\nquestions you hadn't seen before. So the final exam would be the test set. So that's overfitting.\nThe train loss is lower than the test loss. And just right, ideally, you probably won't see\nloss curves this exact smooth. I mean, they might be a little bit jumpy. Ideally, your training loss\nand test loss go down at a similar rate. And of course, there's more combinations of these. If\nyou'd like to see them, check out the Google's loss curve guide that you can check that out there.\nThat's some extra curriculum. Now, you probably want to know how do you deal with underfitting\nand overfitting? Let's look at a few ways. We'll start with overfitting.\nSo we want to reduce overfitting. In other words, we want our model to perform just as\nwell on the training data set as it does on the test data set. So one of the best ways to\nreduce overfitting is to get more data. So this means that our training data set will be larger.\nOur model will be exposed to more examples. And with us, in theory, it doesn't always work.\nThese all come with a caveat, right? They don't always work as with many things in machine learning.\nSo get more data, give your model more chance to learn patterns, generalizable patterns in a\ndata set. You can use data augmentation. So make your models training data set harder to learn.\nSo we've seen a few examples of data augmentation. You can get better data. So not only more data,\nperhaps the data that you're using isn't that the quality isn't that good. So if you enhance the\nquality of your data set, your model may be able to learn better, more generalizable patterns and\nin turn reduce overfitting. Use transfer learning. So we're going to cover this in a later section\nof the course. But transfer learning is taking one model that works, taking its patterns that it's\nlearned and applying it to your own data set. So for example, I'll just go into the Torch Vision\nmodels library. Many of these models in here in Torch Vision, the models module, have already\nbeen trained on a certain data set and such as ImageNet. And you can take the weights or the\npatterns that these models have learned. And if they work well on an ImageNet data set, which is\nmillions of different images, you can adjust those patterns to your own problem. And oftentimes\nthat will help with overfitting. If you're still overfitting, you can try to simplify your model.\nUsually this means taking away things like extra layers, taking away more hidden units. So say you\nhad 10 layers, you might reduce it to five layers. Why does this? What's the theory behind this?\nWell, if you simplify your model and take away complexity from your model, you're kind of telling\nyour model, hey, use what you've got. And you're going to have to, because you've only got five\nlayers now, you're going to have to make sure that those five layers work really well, because\nyou've no longer got 10. And the same for hidden units. Say you started with 100 hidden units per\nlayer, you might reduce that to 50 and say, hey, you had 100 before. Now use those 50 and make your\npatterns generalizable. Use learning rate decay. So the learning rate is how much your optimizer\nupdates your model's weight every step. So learning rate decay is to decay the learning rate\nover time. So you might look this up, you can look this up, go high torch, learning rate,\nscheduling. So what this means is you want to decrease your learning rate over time.\nNow, I know I'm giving you a lot of different things here, but you've got this keynote as a\nreference. So you can come across these over time. So learning rate scheduling. So we might look\ninto here, do we have schedule, scheduler, beautiful. So this is going to adjust the learning rate\nover time. So for example, at the start of when a model is training, you might want a higher learning\nrate. And then as the model starts to learn patterns more and more and more, you might want to reduce\nthat learning rate over time so that your model doesn't update its patterns too much\nin later epochs. So that's the concept of learning rate scheduling. At the closer you get to convergence,\nthe lower you might want to set your learning rate, think of it like this. If you're reaching\nfor a coin at the back of a couch, can we get an image of that coin at back of couch?\nImages. So if you're trying to reach a coin in the cushions here, so the closer you get to that coin,\nat the beginning, you might take big steps. But then the closer you get to that coin, the smaller\nthe step you might take to pick that coin out. Because if you take a big step when you're really\nclose to the coin here, the coin might fall down the couch. The same thing with learning rate decay.\nAt the start of your model training, you might take bigger steps as your model works its way\ndown the loss curve. But then you get closer and closer to the ideal position on the loss curve.\nYou might start to lower and lower that learning rate until you get right very close to the end\nand you can pick up the coin. Or in other words, your model can converge. And then finally, use\nearly stopping. So if we go into an image, is there early stopping here? Early stopping.\nLoss curves early stopping. So what this means is that you stop. Yeah, there we go. So there's\nheaps of different guides early stopping with PyTorch. Beautiful. So what this means is before\nyour testing error starts to go up, you keep track of your model's testing error. And then you stop\nyour model from training or you save the weight or you save the patterns where your model's loss\nwas the lowest. So then you could just set your model to train for an infinite amount of training\nsteps. And as soon as the testing error starts to increase for say 10 steps in a row, you go back\nto this point here and go, I think that was where our model was the best. And the testing\nerror started to increase after that. So we're going to save that model there instead of the model\nhere. So that's the concept of early stopping. So that's dealing with overfitting. There are\nother methods to deal with underfitting. So recall underfitting is when we have a loss that isn't as\nlow as we'd like it to be. Our model is not fitting the data very well. So it's underfitting.\nSo to reduce underfitting, you can add more layers slash units to your model. You're trying to\nincrease your model's ability to learn by adding more layers or units. You can again tweak the\nlearning rate. Perhaps your learning rate is too high to begin with and your model doesn't learn\nvery well. So you can adjust the learning rate again, just like we discussed with reaching for\nthat coin at the back of a couch. If your model is still underfitting, you can train for longer. So\nthat means giving your model more opportunities to look at the data. So more epochs, that just\nmeans it's got looking at the training set over and over and over and over again and trying to\nlearn those patterns. However, you might find again, if you try to train for too long, your testing\nerror will start to go up. Your model might start overfitting if you train too long. So machine\nlearning is all about a balance between underfitting and overfitting. You want your model to fit quite\nwell. And so this is a great one. So you want your model to start fitting quite well. But then if you\ntry to reduce underfitting too much, you might start to overfit and then vice versa, right? If\nyou try to reduce overfitting too much, your model might underfit. So this is one of the\nmost fun dances in machine learning, the balance between overfitting and underfitting.\nFinally, you might use transfer learning. So transfer learning helps with overfitting and\nunderfitting. Recall transfer learning is using a model's learned patterns from Ron problem and\nadjusting them to your own. We're going to see this later on in the course. And then finally,\nuse less regularization. So regularization is holding your model back. So it's trying\nto prevent overfitting. So if you do too much preventing of overfitting, in other words,\nregularizing your model, you might end up underfitting. So if we go back, we have a look at the ideal\ncurves, underfitting. If you try to prevent underfitting too much, so increasing your model's\ncapability to learn, you might end up overfitting. And if you try to prevent overfitting too much,\nyou might end up underfitting. We are going for the just right section. And this is going to be a\nbalance between these two throughout your entire machine learning career. In fact, it's probably\nthe most prevalent area of research is trying to get models not to underfit, but also not to\noverfit. So keep that in mind. A loss curve is a great way to evaluate your model's performance\nover time. And a lot of what we do with the loss curves is try to work out whether our model is\nunderfitting or overfitting, and we're trying to get to this just right curve. We might not get\nexactly there, but we want to keep trying getting as close as we can. So with that being said,\nlet's now build another model in the next video. And we're going to try a method to try and see if\nwe can use data augmentation to prevent our model from overfitting. Although that experiment\ndoesn't sound like the most ideal one we could do right now, because it looks like our model is\nunderfitting. So with your knowledge of what you've just learned in the previous video,\nhow to prevent underfitting, what would you do to increase this model's capability of learning\npatterns in the training data set? Would you train it for longer? Would you add more layers?\nWould you add more hidden units? Have a think and we'll start building another model in the next video.\nWelcome back. In the last video, we covered the important concept of a loss curve and how it can\ngive us information about whether our model is underfitting. In other words, our model's loss\ncould be lower or whether it's overfitting. In other words, the training loss is lower than the test\nloss or far lower than the validation loss. That's another thing to note here is that I put training\nand test sets here. You could also do this with a validation data set and that the just right,\nthe Goldilocks zone, is when our training and test loss are quite similar over time.\nNow, there was a fair bit of information in that last video, so I just wanted to highlight\nthat you can get this all in section 04, which is the notebook that we're working on. And then\nif you come down over here, if we come to section 8, watch an ideal loss curve look like we've got\nunderfitting, overfitting, just right, how to deal with overfitting. We've got a few options here.\nWe've got how to deal with underfitting and then we've got a few options there. And then if we\nwanted to look for more, how to deal with overfitting. You could find a bunch of resources here and then\nhow to deal with underfitting. You could find a bunch of resources here as well. So that is a\nvery fine line, very fine balance that you're going to experience throughout all of your\nmachine learning career. But it's time now to move on. We're going to move on to creating\nanother model, which is tiny VGG, with data augmentation this time. So if we go back to the slide,\ndata augmentation is one way of dealing with overfitting. Now, it's probably not the most\nideal experiment that we could take because our model zero, our baseline model, looks like it's\nunderfitting. But data augmentation, as we've seen before, is a way of manipulating images\nto artificially increase the diversity of your training data set without collecting more data.\nSo we could take our photos of pizza, sushi, and steak and randomly rotate them 30 degrees\nand increase diversity forces a model to learn or hopefully learn. Again, all of these come with\na caveat of not always being the silver bullet to learn more generalizable patterns. Now,\nI should have spelled generalizable here rather than generalization, but similar thing.\nLet's go here. Let's create to start off with, we'll just write down.\nNow let's try another modeling experiment. So this is in line with our PyTorch workflow,\ntrying a model and trying another one and trying another one, so and so over again. This time,\nusing the same model as before, but with some slight data augmentation.\nOh, maybe we're not slight. That's probably not the best word. We'll just say with some data\naugmentation. And if we come down here, we're going to write section 9.1. We need to first\ncreate a transform with data augmentation. So we've seen what this looks like before.\nWe're going to use the trivial augment data augmentation, create training transform,\nwhich is, as we saw in a previous video, what PyTorch the PyTorch team have recently used\nto train their state-of-the-art computer vision models. So train transform trivial.\nThis is what I'm going to call my transform. And I'm just going to from Torch Vision import\ntransforms. We've done this before. We don't have to re-import it, but I'm going to do it anyway,\njust to show you that we're re-importing or we're using transforms. And we're going to compose\na transform here. Recall that transforms help us manipulate our data. So we're going to transform\nour images into size 64 by 64. Then we're going to set up a trivial augment transforms,\njust like we did before, trivial augment wide. And we're going to set the number of magnitude\nbins here to be 31, which is the default here, which means we'll randomly use some data augmentation\non each one of our images. And it will be applied at a magnitude of 0 to 31, also randomly selected.\nSo if we lower this to five, the upper bound of intensity of how much that data augmentation is\napplied to a certain image will be less than if we set it to say 31. Now, our final transform\nhere is going to be too tensor because we want our images in tensor format for our model.\nAnd then I'm going to create a test transform. I'm going to call this simple, which is just going\nto be transforms dot compose. And all that it's going to have, oh, I should put a list here,\nall that it's going to have, we'll just make some space over here, is going to be transforms.\nAll we want to do is resize the image size equals 64 64. Now we don't apply data augmentation\nto the test data set, because we only just want to evaluate our models on the test data set.\nOur models aren't going to be learning any generalizable patterns on the test data set,\nwhich is why we focus our data augmentations on the training data set. And I've just readjusted\nthat. I don't want to do that. Beautiful. So we've got a transform ready. Now let's load some data\nusing those transforms. So we'll create train and test data sets and data loaders\nwith data augmentation. So we've done this before. You might want to try it out on your own. So\npause the video if you'd like to test it out. Create a data set and a data loader using these\ntransforms here. And recall that our data set is going to be creating a data set from pizza,\nsteak and sushi for the train and test folders. And that our data loader is going to be batchifying\nour data set. So let's turn our image folders into data sets. Data sets, beautiful. And I'm going\nto write here train data augmented just so we know that it's it's been augmented. We've got a few\nof similar variable names throughout this notebook. So I just want to be as clear as possible. And\nI'm going to use, I'll just re import torch vision data sets. We've seen this before, the image\nfolder. So rather than our use our own custom class, we're going to use the existing image folder\nclass that's within torch vision data sets. And we have to pass in here a root. So I'll just get\nthe doc string there, root, which is going to be equal to our trainer, which recall is the path\nto our training directory. Got that saved. And then I'm going to pass in here, the transform is going\nto be train transform trivial. So our training data is going to be augmented. Thanks to this\ntransform here, and transforms trivial augment wide. You know where you can find more about\ntrivial augment wide, of course, in the pie torch documentation, or just searching transforms\ntrivial augment wide. And did I spell this wrong? trivial. Oh, train train transform. I spelled\nthat wrong. Of course I did. So test data, let's create this as test data simple, equals data sets\ndot image folder. And the root D is going to be here the test directory. And the transform is\njust going to be what the test transform simple. Beautiful. So now let's turn these data sets\ninto data loaders. So turn our data sets into data loaders. We're going to import os,\nI'm going to set the batch size here to equal to 32. The number of workers that are going to\nload our data loaders, I'm going to set this to os dot CPU count. So there'll be one worker\nper CPU on our machine. I'm going to set here the torch manual seed to 42, because we're going to\nshuffle our training data. Train data loader, I'm going to call this augmented equals data loader.\nNow I just want to I don't need to re import this, but I just want to show you again from\ntorch dot utils. You can never have enough practice right dot data. Let's import data loader.\nSo that's where we got the data loader class from. Now let's go train data augmented. We'll\npass in that as the data set. And I'll just put in here the parameter name for completeness.\nThat's our data set. And then we want to set the batch size, which is equal to batch size.\nI'm going to set shuffle equal to true. And I'm going to set num workers equal to num workers.\nBeautiful. And now let's do that again with the test data loader that this time test data\nloader. I'm going to call this test data loader simple. We're not using any data\naugmentation on the test data set, just turning our images, our test images into tenses.\nThe data set here is going to be test data simple. Going to pass in the batch size equal to batch\nsize. So both our data loaders will have a batch size of 32. Going to keep shuffle on false.\nAnd num workers, I'm going to set to num workers. Look at us go. We've already got a data set\nand a data loader. This time, our data loader is going to be augmented for the training data set.\nAnd it's going to be nice and simple for the test data set. So this is really similar,\nthis data loader to the previous one we made. The only difference in this modeling experiment\nis that we're going to be adding data augmentation, namely trivial augment wide.\nSo with that being said, we've got a data set, we've got a data loader. In the next video,\nlet's construct and train model one. In fact, you might want to give that a go. So you can use\nour tiny VGG class to make model one. And then you can use our train function to train a new\ntiny VGG instance with our training data loader augmented and our test data loader simple.\nSo give that a go and we'll do it together in the next video. I'll see you there.\nNow that we've got our data sets and data loaders with data augmentation ready,\nlet's now create another model. So 9.3, we're going to construct and train model one.\nAnd this time, I'm just going to write what we're going to doing, going to be doing sorry.\nThis time, we'll be using the same model architecture, but we're changing the data here.\nExcept this time, we've augmented the training data. So we'd like to see how this performs\ncompared to a model with no data augmentation. So that was our baseline up here. And that's what\nyou'll generally do with your experiments. You'll start as simple as possible and introduce\ncomplexity when required. So create model one and send it to the target device, that is,\nto the target device. And because of our helpful selves previously, we can create a manual seed\nhere, torch.manualseed. And we can create model one, leveraging the class that we created before.\nSo although we built tiny VGG from scratch in this video, in this section, sorry, in subsequent\ncoding sessions, because we've built it from scratch once and we know that it works, we can\njust recreate it by calling the class and passing in different variables here. So let's get the\nnumber of classes that we have in our train data augmented classes. And we're going to send it\nto device. And then if we inspect model one, let's have a look. Wonderful. Now let's keep going.\nWe can also leverage our training function that we did. You might have tried this before.\nSo let's now train our model. She's going to put here. Wonderful. Now we've got a model and\ndata loaders. Let's create what do we have to do? We have to create a loss function and an optimizer\nand call upon our train function that we created earlier to train and evaluate our model. Beautiful.\nSo I'm going to set the random seeds, torch dot manual seeds, and torch dot CUDA, because we're\ngoing to be using CUDA. Let's set the manual seed here 42. I'm going to set the number of epochs.\nWe're going to keep many of the parameters the same. Set the number of epochs, num epochs equals\nfive. We could of course train this model for longer if we really wanted to by increasing the\nnumber of epochs. But now let's set up the loss function. So loss FN equals NN cross entropy loss.\nDon't forget this just came into mind. Loss function often as well in PyTorch is called\ncriterion. So the criterion you're trying to reduce. But I just like to call it loss function.\nAnd then we're going to have optimizer. Let's use the same optimizer we use before torch dot\nopt in dot atom. Recall SGD and atom are two of the most popular optimizers. So model one dot\nparameters. Then the parameters we're going to optimize. We're going to set the learning rate to\nzero zero one, which is the default for the atom optimizer in PyTorch. Then we're going to start\nthe timer. So from time it, let's import the default timer as timer. And we'll go start time\nequals timer. And then let's go train model one. How can we do this? Well, we're going to get a\nresults dictionary as model one results. We're going to call upon our train function. Inside our\ntrain function, we'll pass the model parameter as model one. For the train data loader parameter,\nwe're going to pass in train data loader augmented. So our augmented training data loader.\nAnd for the test data loader, we can pass in here test data loader. Simple. Then we can write our\noptimizer, which will be the atom optimizer. Our loss function is going to be an n cross entropy\nloss, what we've created above. And then we can set the number of epochs is going to be equal to\nnum epochs. And then if we really wanted to, we could set the device equal to device, which will\nbe our target device. And now let's end the timer and print out how long it took.\nTook n time equals timer. And we'll go print total training time for model one is going to be\nn time minus start time. And oh, it would help if I could spell, we'll get that to three decimal\nplaces. And that'll be seconds. So you're ready? We look how quickly we built a training pipeline\nfor model one. And look how big easily we created it. So go ask for coding all of that stuff up\nbefore. Let's train our second model, our first model using data augmentation. You're ready? Three,\ntwo, one, let's go. No errors. Beautiful. We're going nice and quick here.\nSo oh, about just over seven seconds. So what what GPU do I have currently?\nJust keep this in mind that I'm using Google Colab Pro. So I get preference in terms of\nallocating a faster GPU. Your model training time may be longer than what I've got, depending on the\nGPU. It also may be faster, again, depending on the GPU. But we get about seven seconds, but it looks\nlike our model with data augmentation didn't perform as well as our model without data augmentation.\nHmm. So how long did our model before without data augmentation take the train? Oh, just over seven\nseconds as well. But we got better results in terms of accuracy on the training and test data sets\nfor model zero. So maybe data augmentation doesn't help in our case. And we kind of hinted at that\nbecause the loss here was already going down. We weren't really overfitting yet. So recall that data\naugmentation is a way to help with overfitting generally. So maybe that wasn't the best step to\ntry and improve our model. But let's nonetheless keep evaluating our model. In the next video,\nwe're going to plot the loss curves of model one. So in fact, you might want to give that a go.\nSo we've got a function plot loss curves, and we've got some results in a dictionary format.\nSo try that out, plot the loss curves, and see what you see. Let's do it together in the next video.\nI'll see you there.\nIn the last video, we did the really exciting thing of training our first model with data\naugmentation. But we also saw that quantitatively, it looks like that it didn't give us much improvement.\nSo let's keep evaluating our model here. I'm going to make a section. Recall that one of my\nfavorite ways or one of the best ways, not just my favorite, to evaluate the performance of a\nmodel over time is to plot the loss curves. So a loss curve helps you evaluate your model's performance\nover time. And it will also give you a great visual representation or a visual way to see if\nyour model is underfitting or overfitting. So let's plot the loss curves of model one results and see\nwhat happens. We're using this function we created before. And oh my goodness, is that going in the\nright direction? It looks like our test loss is going up here. Now, is that where we want it to go?\nRemember the ideal direction for a loss curve is to go down over time because loss is measuring\nwhat? It's measuring how wrong our model is. And the accuracy curve looks like it's all over the\nplace as well. I mean, it's going up kind of, but maybe we don't have enough time to measure these\nthings. So an experiment that you could do is train both of our models model zero and model one\nfor more epochs and see if these loss curves flatten out. So I'll pose you the question,\nis our model underfitting or overfitting right now or both? So if we want to have a look at the\nloss curves, our just right is for the loss that is, this is not for accuracy, this is for loss over\ntime, we want it to go down. So for me, our model is underfitting because our loss could be lower,\nbut it also looks like it's overfitting as well. So it's not doing a very good job because our test\nloss is far higher than our training loss. So if we go back to section four of the LearnPyTorch.io\nbook, what should an ideal loss curve look like? I'd like you to start thinking of some ways\nthat we could deal with overfitting of our model. So could we get more data? Could we simplify it?\nCould we use transfer learning? We're going to see that later on, but you might want to jump\nahead and have a look. And if we're dealing with underfitting, what are some other things that we\ncould try with our model? Could we add some more layers, potentially another convolutional block?\nCould we increase the number of hidden units per layer? So if we've got currently 10 hidden units\nper layer, maybe you want to increase that to 64 or something like that? Could we train it for\nlonger? That's probably one of the easiest things to try with our current training functions. We\ncould train for 20 epochs. So have a go at this, reference this, try out some experiments with,\nsee if you can get these loss curves more towards the ideal shape. And in the next video, we're going\nto keep pushing forward. We're going to compare our model results. So we've done two experiments.\nLet's now see them side by side. We've looked at our model results individually,\nand we know that they could be improved. But a good way to compare all of your experiments\nis to compare your model's results side by side. So that's what we're going to do in the next video.\nI'll see you there. Now that we've compared our models loss curves on their own individually,\nhow about we compare our model results to each other? So let's have a look at comparing our model\nresults. And so I'm going to write a little note here that after evaluating our modeling\nexperiments on their own, it's important to compare them to each other. And there's a few\ndifferent ways to do this. There's a few different ways to do this. Number one is hard coding.\nSo like we've done, we've written functions, we've written helper functions and whatnot,\nand manually plotted things. So I'm just going to write in here, this is what we're doing.\nThen, of course, there are tools to do this, such as PyTorch plus TensorBoard. So I'll link to this,\nPyTorch TensorBoard. We're going to see this in a later section of the course. TensorBoard is a\ngreat resource for tracking your experiments. If you'd like to jump forward and have a look at what\nthat is in the PyTorch documentation, I'd encourage you to do so. Then another one of my favorite\ntools is weights and biases. So these are all going to involve some code as well, but they help out\nwith automatically tracking different experiments. So weights and biases is one of my favorite,\nand you've got platform for experiments. That's what you'll be looking at. So if you run multiple\nexperiments, you can set up weights and biases pretty easy to track your different model hub\nparameters. So PyTorch, there we go. Import weights and biases, start a new run on weights and biases.\nYou can save the learning rate value and whatnot, go through your data and just log everything there.\nSo this is not a course about different tools. We're going to focus on just pure PyTorch,\nbut I thought I'd leave these here anyway, because you're going to come across them\neventually, and MLflow is another one of my favorites as well. We have ML tracking,\nprojects, models, registry, all that sort of stuff. If you'd like in to look into\nmore ways to track your experiments, there are some extensions. But for now, we're going to stick\nwith hard coding. We're just going to do it as simple as possible to begin with. And if we wanted\nto add other tools later on, we can sure do that. So let's create a data frame for each of our model\nresults. We can do this because our model results recall are in the form of dictionaries. So model\nzero results. But you can see what we're doing now by hard coding this, it's quite cumbersome.\nCan you imagine if we had say 10 models or even just five models, we'd have to really\nwrite a fair bit of code here for all of our dictionaries and whatnot, whereas these tools\nhere help you to track everything automatically. So we've got a data frame here. Model zero results\nover time. These are our number of epochs. We can notice that the training loss starts to go down.\nThe testing loss also starts to go down. And the accuracy on the training and test data set starts\nto go up. Now, those are the trends that we're looking for. So an experiment you could try would\nbe to train this model zero for longer to see if it improved. But we're currently just interested\nin comparing results. So let's set up a plot. I want to plot model zero results and model one\nresults on the same plot. So we'll need a plot for training loss. We'll need a plot for training\naccuracy, test loss and test accuracy. And then we want two separate lines on each of them. One\nfor model zero and one for model one. And this particular pattern would be similar regardless if\nwe had 10 different experiments, or if we had 10 different metrics we wanted to compare,\nyou generally want to plot them all against each other to make them visual. And that's what tools\nsuch as weights and biases, what TensorBoard, and what ML flow can help you to do. I'm just\ngoing to get out of that, clean up our browser. So let's set up a plot here. I'm going to use\nmatplotlib. I'm going to put in a figure. I'm going to make it quite large because we want four\nsubplots, one for each of the metrics we want to compare across our different models. Now,\nlet's get number of epochs. So epochs is going to be length, or we'll turn it into a range, actually,\nrange of Len model zero DF. So that's going to give us five. Beautiful range between zero and five.\nNow, let's create a plot for the train loss. We want to compare the train loss across model zero\nand the train loss across model one. So we can go PLT dot subplot. Let's create a plot with two\nrows and two columns. And this is going to be index number one will be the training loss.\nWe'll go PLT dot plot. I'm going to put in here epochs and then model zero DF. Inside here,\nI'm going to put train loss for our first metric. And then I'm going to label it with model zero.\nSo we're comparing the train loss on each of our modeling experiments. Recall that model zero was\nour baseline model. And that was tiny VGG without data augmentation. And then we tried out model one,\nwhich was the same model. But all we did was we added a data augmentation transform to our training\ndata. So PLT will go x label. They both used the same test data set and PLT dot legend. Let's see\nwhat this looks like. Wonderful. So there's our training loss across two different models.\nSo we notice that model zero is trending in the right way. Model one kind of exploded on epoch\nnumber that would be zero, one, two, or one, depending how you're counting. Let's just say epoch number\ntwo, because that's easier. The loss went up. But then it started to go back down. So again,\nif we continued training these models, we might notice that the overall trend of the loss is\ngoing down on the training data set, which is exactly what we'd like. So let's now plot,\nwe'll go the test loss. So I'm going to go test loss here. And then I'm going to change this.\nI believe if I hold control, or command, maybe, nope, or option on my Mac keyboard,\nyeah, so it might be a different key on Windows. But for me, I can press option and I can get a\nmulti cursor here. So I'm just going to come back in here. And that way I can backspace there\nand just turn this into test loss. Wonderful. So I'm going to put this as test loss as the title.\nAnd I need to change the index. So this will be index one, index two, index three, index four.\nLet's see what this looks like. Do we get the test loss? Beautiful. That's what we get.\nHowever, we noticed that model one is probably overfitting at this stage. So maybe the data\naugmentation wasn't the best change to make to our model. Recall that even if you make a change\nto your model, such as preventing overfitting or underfitting, it won't always guarantee that\nthe change takes your model's evaluation metrics in the right direction. Ideally, loss is going\nfrom top left to bottom right over time. So looks like model zero is winning out here at the moment\non the loss front. So now let's plot the accuracy for both training and test. So I'm going to change\nthis to train. I'm going to put this as accuracy. And this is going to be index number three on the\nplot. And do we save it as, yeah, just act? Wonderful. So I'm going to option click here on my Mac.\nThis is going to be train. And this is going to be accuracy here. And then I'll change this one to\naccuracy. And then I'm going to change this to accuracy. And this is going to be plot number four,\ntwo rows, two columns, index number four. And I'm going to option click here to have two cursors,\ntest, act. And then I'll change this to test, act. And I'm going to get rid of the legend here.\nIt takes a little bit to plot because we're doing four graphs in one hit. Wonderful. So that's\ncomparing our models. But do you see how we could potentially functionalize this to plot, however,\nmany model results that we have? But if we had say another five models, we did another five\nexperiments, which is actually not too many experiments on a problem, you might find that\nsometimes you do over a dozen experiments for a single modeling problem, maybe even more.\nThese graphs can get pretty outlandish with all the little lines going through. So that's\nagain what tools like TensorBoard, weights and biases and MLflow will help with. But if we have\na look at the accuracy, it seems that both of our models are heading in the right direction.\nWe want to go from the bottom left up in the case of accuracy. But the test accuracy that's training,\noh, excuse me, is this not training accuracy? I messed up that. Did you catch that one?\nSo training accuracy, we're heading in the right direction, but it looks like model one is\nyeah, still overfitting. So the results we're getting on the training data set\naren't coming over to the testing data set. And that's what we really want our models to shine\nis on the test data set. So metrics on the training data set are good. But ideally,\nwe want our models to perform well on the test data set data it hasn't seen before.\nSo that's just something to keep in mind. Whenever you do a series of modeling experiments,\nit's always good to not only evaluate them individually, evaluate them against each other.\nSo that way you can go back through your experiments, see what worked and what didn't.\nIf you were to ask me what I would do for both of these models, I would probably train them for\nlonger and maybe add some more hidden units to each of the layers and see where the results go from\nthere. So give that a shot. In the next video, let's see how we can use our trained models to\nmake a prediction on our own custom image of food. So yes, we used a custom data set of\npizza steak and sushi images. But what if we had our own, what if we finished this model training\nand we decided, you know what, this is a good enough model. And then we deployed it to an app like\nneutrify dot app, which is a food recognition app that I'm personally working on. Then we wanted to\nupload an image and have it be classified by our pytorch model. So let's give that a shot, see how\nwe can use our trained model to predict on an image that's not in our training data and not in our\ntesting data. I'll see you in the next video. Welcome back. In the last video, we compared our\nmodeling experiments. Now we're going to move on to one of the most exciting parts of deep learning.\nAnd that is making a prediction on a custom image. So although we've trained a model on custom data,\nhow do you make a prediction on a sample slash image in our case? That's not in either\nthe training or testing data set. So let's say you were building a food recognition app,\nsuch as neutrify, take a photo of food and learn about it. You wanted to use computer vision to\nessentially turn foods into QR codes. So I'll just show you the workflow here. If we were to upload\nthis image of my dad giving two thumbs up for a delicious pizza. And what does neutrify predicted\nas pizza? Beautiful. So macaronutrients that you get some nutrition information and then the time\ntaken. So we could replicate a similar process to this using our trained PyTorch model, or be it.\nIt's not going to be too great of results or performance because we've seen that we could\nimprove our models, but based on the accuracy here and based on the loss and whatnot. But let's just\nsee what it's like, the workflow. So the first thing we're going to do is get a custom image.\nNow we could upload one here, such as clicking the upload button in Google Colab, choosing an image\nand then importing it like that. But I'm going to do so programmatically, as you've seen before.\nSo let's write some code in this video to download a custom image. I'm going to do so using requests\nand like all good cooking shows, I've prepared a custom image for us. So custom image path. But\nagain, you could use this process that we're going to go through with any of your own images\nof pizza, steak or sushi. And if you wanted to train your own model on another set of custom data,\nthe workflow will be quite similar. So I'm going to download a photo called pizza dad,\nwhich is my dad, two big thumbs up. And so I'm going to download it from github. So this image is\non the course github. And let's write some code to download the image. If it doesn't already exist\nin our Colab instance. So if you wanted to upload a single image, you could click with this button.\nJust be aware that like all of our other data, it's going to disappear if Colab disconnects.\nSo that's why I like to write code. So we don't have to re upload it every time.\nSo if not custom image path is file, let's open a request here or open a file going to open up\nthe custom image path with right binary permissions as F short for file. And then when downloading,\nthis is because our image is stored on github. When downloading an image or when downloading\nfrom github in general, you typically want the raw link need to use the raw file link.\nSo let's write a request here equals request dot get. So if we go to the pytorch deep learning\nrepo, then if we go into, I believe it might be extras, not in extras, it's going to be in images,\nthat would make a lot more sense. Wouldn't it Daniel? Let's get O for pizza dad.\nSo if we have a look, this is pytorch deep learning images, O for pizza dad. There's a big version\nof the image there. And then if we click download, just going to give us the raw link. Yeah, there we\ngo. So that's the image. Hey dad, how you doing? Is that pizza delicious? It looks like it.\nLet's see if our model can get this right. What do you think? Will it? So of course, we want\nour model to predict pizza for this image because it's got a pizza in it. So custom image path,\nwe're going to download that. I've just put in the raw URL above. So notice the raw\ngithub user content. That's from the course github. Then I'm going to go f dot right. So file,\nwrite the request content. So the content from the request, in other words, the raw file from\ngithub here. Similar workflow for if you were getting another image from somewhere else on\nthe internet and else if it is already downloaded, let's just not download it. So print f custom image\npath already exists skipping download. And let's see if this works or run the code. So downloading\ndata o four pizza dad dot jpeg. And if we go into here, we refresh. There we go. Beautiful. So our\ndata or our custom image, sorry, is now in our data folder. So if we click on this, this is inside\nGoogle CoLab now. Beautiful. We got a big nice big image there. And there's a nice big pizza there.\nSo we're going to be writing some code over the next few videos to do the exact same process as\nwhat we've been doing to import our custom data set for our custom image. What do we still have to\ndo? We still have to turn it into tenses. And then we have to pass it through our model. So let's see\nwhat that looks like over the next few videos. We are up to one of the most exciting parts of\nbuilding dev learning models. And that is predicting on custom data in our case, a custom image of\na photo of my dad eating pizza. So of course, we're training a computer vision model on here on\npizza steak and sushi. So hopefully the ideal result for our model to predict on this image\nwill be pizza. So let's keep going. Let's figure out how we can get our image, our custom image,\nour singular image into Tensor form, loading in a custom image with pytorch, creating another\nsection here. So I'm just going to write down here, we have to make sure our custom image is in the\nsame format as the data our model was trained on. So namely, that was in Tensor form with data type\ntorch float 32. And then of shape 64 by 64 by three. So we might need to change the shape of our\nimage. And then we need to make sure that it's on the right device. Command MM, beautiful. So let's\nsee what this looks like. Hey, so if I'm going to import torch vision. Now the package you use to\nload your data will depend on the domain you're in. So let's open up the torch vision documentation.\nWe can go to models. That's okay. So if we're working with text, you might want to look in\nhere for some input and output functions, so some loading functions, torch audio, same thing.\nTorch vision is what we're working with. Let's click into torch vision. Now we want to look into\nreading and writing images and videos because we want to read in an image, right? We've got a\ncustom image. We want to read it in. So this is part of your extracurricular, by the way, to go\nthrough these for at least 10 minutes each. So spend an hour if you're going through torch vision.\nYou could do the same across these other ones. It will just really help you familiarize yourself\nwith all the functions of PyTorch domain libraries. So we want to look here's some options for video.\nWe're not working with video. Here's some options for images. Now what do we want to do? We want\nto read in an image. So we've got a few things here. Decode image. Oh, I've skipped over one.\nWe can write a JPEG if we wanted to. We can encode a PNG. Let's jump into this one. Read image.\nWhat does it do? Read the JPEG or PNG into a three-dimensional RGB or grayscale tensor.\nThat is what we want. And then optionally converts the image to the desired format.\nThe values of the output tensor are you int eight. Okay. Beautiful. So let's see what this looks like.\nOkay. Mode. The read mode used optionally for converting the image. Let's see what we can do\nwith this. I'm going to copy this in. So I'll write this down. We can read an image into PyTorch using\nand go with that. So let's see what this looks like in practice. Read in custom image. I can't\nexplain to you how much I love using deep learning models to predict on custom data. So custom image.\nWe're going to call it you int eight because as we read from the documentation here,\nit reads it in you int eight format. So let's have a look at what that looks like rather than\njust talking about it. Torch vision.io. Read image. What's our target image path?\nWell, we've got custom image path up here. This is why I like to do things programmatically.\nSo if our collab notebook reset, we could just run this cell again,\nget our custom image and then we've got it here. So custom image you int eight. Let's see what this\nlooks like. Oh, what did we get wrong? Unable to cast Python instance. Oh, does it need to be a\nstring expected a value type of string or what found POSIX path? So this the path needs to be a\nstring. Okay. If we have a look at our custom image path, what did we get wrong? Oh, we've got a\nPOSIX path. So let's convert this custom image path into a string and see what happens. Look at that.\nThat's how image in integer form. I wonder if this is plotable. Let's go PLT dot M show custom image\nyou int eight. Maybe we get a dimensionality problem here in valid shape. Okay. Let's\nsome permute it, permute, and we'll go one, two, zero. Is this going to plot? It's a fairly big image.\nThere we go. Two thumbs up. Look at us. So that is the power of torch vision.io. I owe stands for\ninput output. We were just able to read in our custom image. Now, how about we get some metadata\nabout this? Let's go. We'll print it up here, actually. I'll keep that there because that's\nfun to plot it. Let's find the shape of our data, the data type. And yeah, we've got it in Tensor\nformat, but it's you int eight right now. So we might have to convert that to float 32. We want\nto find out its shape. And we need to make sure that if we're predicting on a custom image,\nthe data that we're predicting on the custom image needs to be on the same device as our model.\nSo let's print out some info. Print. Let's go custom image Tensor. And this is going to be a new line.\nAnd then we will go custom image you int eight. Wonderful. And then let's go custom image\nshape. We will get the shape parameter custom image shape or attribute. Sorry. And then we also\nwant to know the data type custom image data type. But we have a kind of an inkling because the\ndocumentation said it would be you int eight, you int eight, and we'll go D type. Let's have a look.\nWhat do we have? So there's our image Tensor. And it's quite a big image. So custom image shape.\nSo what was our model trained on? Our model was trained on images of 64 by 64. So this image\nencodes a lot more information than what our model was trained on. So we're going to have to\nchange that shape to pass it through our model. And then we've got an image data type here or\nTensor data type of torch you int eight. So maybe that's going to be some errors for us later on.\nSo if you want to go ahead and see if you can resize this Tensor to 64 64 using a torch transform\nor torch vision transform, I'd encourage you to try that out. And if you know how to change a\ntorch tensor from you int eight to torch float 32, give that a shot as well. So let's try\nmake a prediction on our image in the next video. I'll see you there.\nIn the last video, we loaded in our own custom image and got two big thumbs up from my dad,\nand we turned it into a tensor. So we've got a custom image tensor here. It's quite big though,\nand we looked at a few things of what we have to do before we pass it through our model.\nSo we need to make sure it's in the data type torch float 32, shape 64, 64, 3, and on the right\ndevice. So let's make another section here. We'll go 11.2 and we'll call it making a prediction on a\ncustom image with a pie torch model with a trained pie torch model. And albeit, our models aren't\nquite the level we would like them at yet. I think it's important just to see what it's like to\nmake a prediction end to end on some custom data, because that's the fun part, right? So try to make\na prediction on an image. Now, I want to just highlight something about the importance of different\ndata types and shapes and whatnot and devices, three of the biggest errors in deep learning.\nIn let's see what happens if we try to predict on you int eight format. So we'll go model one\ndot eval and with torch dot inference mode. Let's make a prediction. We'll pass it through our model\none. We could use model zero if we wanted to here. They're both performing pretty poorly anyway.\nLet's send it to the device and see what happens. Oh, no. What did we get wrong here?\nRuntime error input type. Ah, so we've got you int eight. So this is one of our first errors\nthat we talked about. We need to make sure that our custom data is of the same data type that\nour model was originally trained on. So we've got torch CUDA float tensor. So we've got an issue\nhere. We've got a you into eight image data or image tensor trying to be predicted on by a model\nwith its data type of torch CUDA float tensor. So let's try fix this by loading the custom image\nand convert to torch dot float 32. So one of the ways we can do this is we'll just recreate the\ncustom image tensor. And I'm going to use torch vision dot IO dot read image. We don't have to\nfully reload our image, but I'm going to do it anyway for completeness and a little bit of practice.\nAnd then I'm going to set the type here with the type method to torch float 32. And then\nlet's just see what happens. We'll go custom image. Let's see what this looks like. I wonder if our\nmodel will work on this. Let's just try again, we'll bring this up, copy this down to make a\nprediction and custom image dot two device. Our image is in torch float 32 now. Let's see what\nhappens. Oh, we get an issue. Oh my goodness, that's a big matrix. Now I have a feeling that\nthat might be because our image, our custom image is of a shape that's far too large. Custom image\ndot shape. What do we get? Oh my gosh, 4000 and 3,024. And do you notice as well that our values\nhere are between zero and one, whereas our previous images, do we have an image? There we go. That\nour model was trained on what between zero and one. So how could we get these values to be between\nzero and one? Well, one of the ways to do so is by dividing by 255. Now, why would we divide by 255?\nWell, because that's a standard image format is to store the image tensor values in values from\nzero to 255 for red, green and blue color channels. So if we want to scale them, so this is what I\nmeant by zero to 255, if we wanted to scale these values to be between zero and one, we can divide\nthem by 255. Because that is the maximum value that they can be. So let's see what happens if we do\nthat. Okay, we get our image values between zero and one. Can we plot this image? So plt dot m\nshow, let's plot our custom image. We got a permute it. So it works nicely with mapplotlib.\nWhat do we get here?\nBeautiful. We get the same image, right? But it's still quite big. Look at that. We've got a pixel\nheight of or image height of almost 4000 pixels and a width of over 3000 pixels. So we need to do\nsome adjustments further on. So let's keep going. We've got custom image to device. We've got an\nerror here. So this is a shape error. So what can we do to transform our image shape? And you\nmight have already tried this. Well, let's create a transform pipeline to transform our image shape.\nSo create transform pipeline or composition to resize the image. Because remember, what are we\ntrying to do? We're trying to get our model to predict on the same type of data it was trained on.\nSo let's go custom image transform is transforms dot compose. And we're just going to, since our\nimage is already of a tensor, let's do transforms dot resize, and we'll set the size to the same shape\nthat our model was trained on, or the same size that is. So let's go from torch vision. We don't\nhave to rewrite this. It's already imported. But I just want to highlight that we're using the\ntransforms package. We'll run that. There we go. We've got a transform pipeline. Now let's see what\nhappens when we transform our target image, transform target image. What happens? Custom image\ntransformed. I love printing the inputs and outputs of our different pipelines here. So let's pass\nour custom image that we've just imported. So custom image transform, our custom image is recall\nof shape. Quite large. We're going to pass it through our transformation pipeline. And let's\nprint out the shapes. Let's go original shape. And then we'll go custom image dot shape. And then\nwe'll go print transformed shape is going to be custom image underscore transformed dot shape.\nLet's see the transformation. Oh, would you look at that? How good we've gone from quite a large image\nto a transformed image here. So it's going to be squished and squashed a little. So that's what\nhappens. Let's see what happens when we plot our transformed image. We've gone from 4000 pixels\non the height to 64. And we've gone from 3000 pixels on the height to 64. So this is what our\nmodel is going to see. Let's go custom image transformed. And we're going to permute it to be 120.\nOkay, so quite pixelated. Do you see how this might affect the accuracy of our model?\nBecause we've gone from custom image, is this going to, oh, yeah, we need to plot dot image.\nSo we've gone from this high definition image to an image that's of far lower quality here.\nAnd I can kind of see myself that this is still a pizza, but I know that it's a pizza. So just\nkeep this in mind going forward is that another way that we could potentially improve our model's\nperformance if we increased the size of the training image data. So instead of 64 64, we might want\nto upgrade our models capability to deal with images that are of 224 224. So if we have a look\nat what this looks like 224 224. Wow, that looks a lot better than 64 64. So that's something that\nyou might want to try out later on. But we're going to stick in line with the CNN explainer model.\nHow about we try to make another prediction? So since we transformed our\nimage to be the same size as the data our model was trained on. So with torch inference mode,\nlet's go custom image pred equals model one on custom image underscore transformed.\nDoes it work now? Oh my goodness, still not working expected all tensors on the same device. Of course,\nthat's what we forgot here. Let's go to device. Or actually, let's leave that error there. And\nwe'll just copy this code down here. And let's put this custom image transform back on the right\ndevice and see if we finally get a prediction to happen with our model. Oh, we still get an error.\nOh my goodness, what's going on here? Oh, we need to add a batch size to it. So I'm just gonna write\nup here. This will error. No batch size. And this will error. Image not on right device. And then\nlet's try again, we need to add a batch size to our image. So if we look at custom image transformed\ndot shape, recall that our images that passed through our model had a batch dimension. So this\nis another place where we get shape mismatch issues is if our model, because what's going on\nin neural network is a lot of tensor manipulation. If the dimensions don't line up, we want to perform\nmatrix multiplication and the rules. If we don't play to the rules, the matrix multiplication will\nfail. So let's fix this by adding a batch dimension. So we can do this by going a custom image transformed.\nLet's unsqueeze it on the first dimension and then check the shape. There we go. We add a single batch.\nSo that's what we want to do when we make a prediction on a single custom image. We want to pass it to\nour model as an image or a batch of one sample. So let's finally see if this will work.\nLet's just not comment what we'll do. This, or maybe we'll try anyway, this should work.\nAdded a batch size. So do you see the steps we've been through so far? And we're just going to\nunsqueeze this. Unsqueeze on the zero dimension to add a batch size. Oh, it didn't error. Oh my\ngoodness. It didn't error. Have a look at that. Yes, that's what we want. We get a prediction\nload it because the raw outputs of our model, we get a load it value for each of our custom classes.\nSo this could be pizza. This could be steak. And this could be sushi, depending on the order of\nour classes. Let's just have a look. Class to IDX. Did we not get that? Class names.\nBeautiful. So pizza steak sushi. We've still got a ways to go to convert this into that.\nBut I just want to highlight what we've done. So note, to make a prediction on a custom image,\nwe had to. And this is something you'll have to keep in mind for almost all of your custom data.\nIt needs to be formatted in the same way that your model was trained on. So we had to load the image\nand turn it into a tensor. We had to make sure the image was the same data type as the model.\nSo that was torch float 32. And then we had to make sure the image was the same shape as the data\nthe model was trained on, which was 64, 64, three with a batch size. So that was one,\nthree, 64, 64. And excuse me, this should actually be the other way around. This should be color\nchannels first, because we're dealing with pie torch here. 64. And then finally, we had to make\nsure the image was on the same device as our model. So they are three of the big ones that we've\ntalked about so much the same data type or data type mismatch will result in a bunch of issues.\nShape mismatch will result in a bunch of issues. And device mismatch will also result in a bunch\nof issues. If you want these to be highlighted, they are in the learn pie torch.io resource. We have\nputting things together. Where do we have it? Oh, yeah, no, it's in the main takeaway section,\nsorry, predicting on your own custom data with a trained model as possible, as long as you format\nthe data into a similar format to what the model was trained on. So make sure you take care of the\nthree big pie torch and deep learning errors. Wrong data types, wrong data shapes, and wrong\ndevices, regardless of whether that's images or audio or text, these three will follow you around.\nSo just keep them in mind. But now we've got some code to predict on custom images, but it's kind\nof all over the place. We've got about 10 coding cells here just to make a prediction on a custom\nimage. How about we functionize this and see if it works on our pizza dad image. I'll see you in the\nnext video. Welcome back. We're now well on our way to making custom predictions on our own custom\nimage data. Let's keep pushing forward. In the last video, we finished off getting some raw model\nlogits. So the raw outputs from our model. Now, let's see how we can convert these logits into\nprediction labels. Let's write some code. So convert logits to prediction labels. Or let's go\nconvert logits. Let's first convert them to prediction probabilities. Probabilities.\nSo how do we do that? Let's go custom image pred probes equals torch dot softmax\nto convert our custom image pred across the first dimension. So the first dimension of this tensor\nwill be the inner brackets, of course. So just this little section here. Let's see what these\nlook like. This will be prediction probabilities. Wonderful. So you'll notice that these are quite\nspread out. Now, this is not ideal. Ideally, we'd like our model to assign a fairly large\nprediction probability to the target class, the right target class that is. However, since our model\nwhen we trained it isn't actually performing that all that well. The prediction probabilities\nare quite spread out across all of the classes. But nonetheless, we're just highlighting what\nit's like to predict on custom data. So now let's convert the prediction probabilities\nto prediction labels. Now, you'll notice that we used softmax because why we are working with\nmulti class classification data. And so we can get the custom image pred labels, the integers,\nby taking the argmax of the prediction probabilities, custom image pred probes across the first\ndimension as well. So let's go custom image pred labels. Let's see what they look like.\nZero. So the index here with the highest value is index number zero. And you'll notice that it's\nstill on the coded device. So what would happen if we try to index on our class names with\nthe custom image pred labels? Or maybe that doesn't need to be a plural. Oh, there we go. We get pizza.\nBut you might also have to change this to the CPU later on. Otherwise, you might run into some\nerrors. So just be aware of that. So you notice how we just put it to the CPU. So we get pizza. We\ngot a correct prediction. But this is as good as guessing in my opinion, because these are kind\nof spread out. Ideally, this value would be higher, maybe something like 0.8 or above for our pizza\ndad image. But nonetheless, our model is getting two thumbs up even on this 64 by 64 image. But\nthat's a lot of code that we've written. Let's functionize it. So we can just pass in a file path\nand get a custom prediction from it. So putting custom image prediction together.\nLet's go building a function. So we want the ideal outcome is, let's plot our image as well.\nIdeal outcome is a function where we plot or where we pass an image path to and have our model predict\non that image and plot the image plus the prediction. So this is our ideal outcome. And I think I'm\ngoing to issue this as a challenge. So give that a go, put all of our code above together. And you'll\njust have to import the image, you'll have to process it and whatnot. I know I said we were going\nto build a function in this video, but we're going to say that to the next video. I'd like\nyou to give that a go. So start from way back up here, import the image via torture vision.io read\nimage, format it using what we've done, change the data type, change the shape, change the device,\nand then plot the image with its prediction as the title. So give that a go and we'll do it\ntogether in the next video. How'd you go? I just realized I had a typo in the previous cell,\nbut that's all right. Did you give it a shot? Did you put together the custom image prediction\nin a function format? I'd love it if you did. But if not, that's okay. Let's keep going. Let's see\nwhat that might look like. And there are many different ways that you could do this. But\nhere's one of the ways that I've thought of. So we want to function that's going to\npred and plot a target image. We wanted to take in a torch model. And so that's going to be ideally\na trained model. We wanted to also take in an image path, which will be of a string. It can\ntake in a class names list so that we can index it and get the prediction label in string format.\nSo let's put this as a list of strings. And by default, this can equal none. Just in case we\njust wanted the prediction, it wants to take in a transform so that we can pass it in some form of\ntransform to transform the image. And then it's going to take in a device, which will be by default\nthe target device. So let's write a little doc string here, makes a prediction on a target image\nwith a trained model and plots the image and prediction. Beautiful. Now what do we have to do\nfirst? Let's load in the image. Load in the image just like we did before with torch vision. So\ntarget image equals torch vision.io dot read image. And we'll go string on the image path,\nwhich will be the image path here. And we convert it to a string just in case it doesn't get passed\nin as a string. And then let's change it into type torch float 32. Because we want to make sure that\nour custom image or our custom data is in the same type as what we trained our model on. So now\nlet's divide the image pixel values by 255 to get them between zero or to get them between zero\none as a range. So we can just do this by target image equals target image divided by 255. And we\ncould also just do this in one step up here 255. But I've just put it out there just to let you know\nthat, hey, read image imports image data as between zero and 255. So our model prefers numbers\nbetween zero and one. So let's just scale it there. Now we want to transform our data if necessary.\nIn our case, it is, but it won't always be. So we want this function to be pretty generic\npredomplot image. So if the transform exists, let's set the target image to the transform,\nor we'll pass it through the transform that is wonderful. And the transform we're going to get\nfrom here. Now what's left to do? Well, let's make sure the model is on the target device.\nIt might be by default, but if we're passing in a device parameter, we may as well make sure the\nmodel is there too. And now we can make a prediction. So let's turn on a vowel slash inference mode\nand make a prediction with our model. So model, we call a vowel mode, and then with torch dot\ninference mode, because we're making a prediction, we want to turn our model into inference mode,\nor put it in inference mode context. Let's add an extra dimension to the image. Let's go target\nimage. We could do this step above, actually, but we're just going to do it here. From kind of\nremembering things on the fly here of what we need to do, we're adding a, this is, let's write\nthis down, this is the batch dimension. e g our model will predict on batches of one x image.\nSo we're just unsqueezing it to add an extra dimension at the zero dimension space,\njust like we did in a previous video. Now let's make a prediction\non the image with an extra dimension. Otherwise, if we don't have that extra dimension, we saw\nthat we get a shape issue. So right down here, target image pred. And remember, this is going\nto be the raw model outputs, raw logit outputs. We're going to target image pred. And yeah,\nI believe that's all we need for the prediction. Oh wait, there was one more thing, two device.\nMe too. Also make sure the target image is on the right device. Beautiful. So fair\nfew steps here, but nothing we can't handle. All we're really doing is replicating what we've done\nfor batches of images. But we want to make sure that if someone passed any image to our\npred and plot image function, that we've got functionality in here to handle that image.\nAnd do we get this? Oh, we want just target image to device. Did you catch that error?\nSo let's keep going. Now let's convert the logits. Our models raw logits. Let's convert those\nto prediction probabilities. This is so exciting. We're getting so close to making a function\nto predict on custom data. So we'll set this to target image pred probes, which is going to be\ntorch dot softmax. And we will pass in the target image pred here. We want to get the softmax of\nthe first dimension. Now let's convert our prediction probabilities, which is what we get in the line\nabove. We want to convert those to prediction labels. So let's get the target image pred labels\nlabels equals torch dot argmax. We want to get the argmax of, or in other words, the index,\nwhich is the maximum value from the pred probes of the first dimension as well. Now what should we\nreturn here? Well, we don't really need to return anything. We want to create a plot. So let's plot\nthe image alongside the prediction and prediction probability. Beautiful. So plot dot in show,\nwhat are we going to pass in here? We're going to pass in here our target image. Now we have to\nsqueeze this, I believe, because we've added an extra dimension up here. So we'll squeeze it to\nremove that batch size. And then we still have to permute it because map plot lib likes images\nin the format color channels last one, two, zero. So remove batch dimension.\nAnd rearrange shape to be hc hwc. That is color channels last. Now if the class names parameter\nexists, so we've passed in a list of class names, this function is really just replicating\neverything we've done in the past 10 cells, by the way. So right back up here, we're replicating\nall of this stuff in one function. So pretty large function, but once we've written it,\nwe can pass in our images as much as we like. So if class names exist, let's set the title\nto our showcase that class name. So the pred is going to be class names. Let's index on that\npred image, or target image pred label. And this is where we'll have to put it to the CPU,\nbecause if we're using a title with map plot lib, map plot lib cannot handle things that are on\nthe GPU. This is why we have to put it to the CPU. And then I believe that should be enough for\nthat. Let's add a little line in here, so that we can have it. Oh, I've missed something.\nAn outside bracket there. Wonderful. Let's add the prediction probability, because that's always\nfun to see. So we want target image pred probs. And we want to get the maximum pred problem from\nthat. And we'll also put that on the CPU. And I think we might get this three decimal places.\nNow this is saying, oh, pred labels, we don't need that. We need just non plural, beautiful. Now,\nif the class names doesn't exist, let's just set the title equal to f f string, we'll go pred,\ntarget image pred label. Is Google Colab still telling me this is wrong?\nTarget image pred label. Oh, no, we've still got the same thing. It just hasn't caught up with me,\nand I'm coding a bit fast here. And then we'll pass in the prob, which will be just the same as\nabove. I could even copy this in. Beautiful. And let's now set the title to the title. And we\nand we will turn the axes off. PLT axes false. Fair bit of code there. But this is going to be a\nsuper exciting moment. Let's see what this looks like. When we pass it in a target image and a\ntarget model, some class names, and a transform. Are you ready? We've got our transform ready,\nby the way, it's back up here. Custom image transform. It's just going to resize our image.\nSo let's see. Oh, this file was updated remotely or in another tab. Sometimes this happens, and\nusually Google Colab sorts itself out, but that's all right. It doesn't affect our code for now.\nPred on our custom image. Are you ready? Save failed. Would you like to override? Yes, I would.\nSo you might see that in Google Colab. Usually it fixes itself. There we go. Save successfully.\nPred and plot image. I was going to say, Google Colab, don't fail me now. We're about to predict\non our own custom data. Using a model trained on our own custom data. Image part. Let's pass in\ncustom image path, which is going to be the path to our pizza dad image. Let's go class names,\nequals class names, which is pizza, steak, and sushi. We'll pass in our transform to convert our\nimage to the right shape and size custom image transform. And then finally, the target device is\ngoing to be device. Are you ready? Let's make a prediction on custom data. One of my favorite\nthings. One of the most fun things to do when building deep learning models. Three, two, one.\nHow did it go? Oh, no. What did we get wrong? CPU. Okay. Such a so close, but yet so far.\nHas no attribute CPU. Oh, maybe we need to put this to CPU. That's where I got the square bracket\nwrong. So that's what we needed to change. We needed to because this is going to be potentially\non the GPU. Tag image pred label. We need to put it on the CPU. We need to do that. Why?\nBecause this is going to be the title of our map plot lib plot. And map plot lib doesn't interface\ntoo well with data on a GPU. Let's try it again. Three, two, one, running. Oh, look at that.\nPrediction on a custom image. And it gets it right. Two thumbs up. I didn't plan this. Our model is\nperforming actually quite poorly. So this is as good as a guess to me. You might want to try this\non your own image. And in fact, if you do, please share it with me. I would love to see it. But\nyou could potentially try this with another model. See what happens? Steak. Okay, there we go. So\neven though model one performs worse quantitatively, it performs better qualitatively. So that's the\npower of a visualize, visualize, visualize. And if we use model zero, also, which isn't performing\ntoo well, it gets it wrong with a prediction probability of 0.368, which isn't too high either.\nSo we've talked about a couple of different ways to improve our models. Now we've even\ngot a way to make predictions on our own custom images. So give that a shot. I'd love to see\nyour custom predictions, upload an image here if you want, or download it into Google Colab using\ncode that we've used before. But we've come a fairly long way. I feel like we've covered enough\nfor custom data sets. Let's summarize what we've covered in the next video. And I've got a bunch\nof exercises and extra curriculum for you. So this is exciting stuff. I'll see you in the next video.\nIn the last video, we did the very exciting thing of making a prediction on our own custom\nimage, although it's quite pixelated. And although our models performance quantitatively didn't\nturn out to be too good qualitatively, it happened to work out. But of course, there are a fair few\nways that we could improve our models performance. But the main takeaway here is that we had to do\na bunch of pre processing to make sure our custom image was in the same format as what our model\nexpected. And this is quite a lot of what I do behind the scenes for Nutrify. If you upload an\nimage here, it gets pre processed in a similar way to go through our image classification model\nto output a label like this. So let's get out of this. To summarize, I've got a colorful slide here,\nbut we've already covered this predicting on custom data. These are three things to make sure of,\nregardless of whether you're using images, text or audio, make sure your data is in the right\ndata type. In our case, it was torch float 32. Make sure your data is on the same device as the model.\nSo we had to put our custom image to the GPU, which was where our model also lived. And then we had\nto make sure our data was in the correct shape. So the original shape was 64, 64, 3. Actually,\nthis should be reversed, because it was color channels first. But the same principle remains here.\nWe had to add a batch dimension and rearrange if we needed. So in our case, we used images of this\nshape batches first color channels first height width. But depending on your problem will depend\non your shape, depending on the device you're using will depend on where your data and your\nmodel lives. And depending on the data type you're using will depend on what you're using for torch\nfloat 32 or something else. So let's summarize. If we go here main takeaways, you can read through\nthese, but some of the big ones are pie torch has many built in functions to deal with all kinds\nof data from vision to text to audio to recommendation systems. So if we look at the pie torch docs,\nyou're going to become very familiar with these over time. We've got torch audio data,\ntorch text, torch vision is what we practiced with. And we've got a whole bunch of things here for\ntransforming and augmenting images, data sets, utilities, operators, and torch data is currently\nin beta. But this is just something to be aware of later on. So it's a prototype library right now,\nbut by the time you watch this, it might be available. But it's another way of loading data.\nSo just be aware of this for later on. And if we come back to up here, if applied to watch built\nin data loading functions, don't suit your requirements, you can write your own custom\ndata set classes by subclassing torch dot utils dot data dot data set. And we saw that way back\nup here in option number two. Option two, here we go, loading image data with a custom data set,\nwrote plenty of code to do that. And then a lot of machine learning is dealing with the\nbalance between overfitting and underfitting. We've got a whole section in the book here to\ncheck out what an ideal loss curve should look like and how to deal with overfitting,\nhow to deal with underfitting. It's it is a fine line. So much of the research and machine\nlearning is actually dedicated towards this balance. And then three big things for being aware of\nwhen you're predicting on your own custom data, wrong data types, wrong data shapes,\nand wrong devices. This will follow you around, as I said, and we saw that in practice to get our\nown custom image ready for a trained model. Now, we have some exercises here. If you'd like\nthe link to it, you can go to loan pytorch.io section number four exercises, and of course,\nextra curriculum. A lot of the things I've mentioned throughout the course that would be a good\nresource to check out contained in here. But the exercises, this is this is your time to shine,\nyour time to practice. Let's go back to this notebook, scroll right down to the bottom.\nLook how much code we've written. Goodness me, exercises for all exercises and extra curriculum.\nSee here, turn that into markdown. Wonderful. And so if we go in here, you've got a couple of\nresources. There's an exercise template notebook for number four, and example solutions for notebook\nnumber four, which is what we're working on now. So of course, I'd encourage you to go through the\npytorch custom data sets exercises template first. Try to fill out all of the code here on your own.\nSo we've got some questions here. We've got some dummy code. We've got some comments.\nSo give that a go. Go through this. Use this book resource to reference. Use all the code\nwe've written. Use the documentation, whatever you want. But try to go through this on your own.\nAnd then if you get stuck somewhere, you can look at an example solution that I created,\nwhich is here, pytorch custom data sets exercise solutions. And just be aware that this is just\none way of doing things. It's not necessarily the best. It's just a way to reference what\nyou're writing to what I would do. And there's actually now live walkthroughs of the solutions,\nerrors and all on YouTube. So if you go to this video, which is going to mute. So this is me\nlive streaming the whole thing, writing a bunch of pytorch code. If you just keep going through all\nof that, you'll see me writing all of the solutions, running into errors, trying different things,\net cetera, et cetera. But that's on YouTube. You can check that out on your own time. But I feel\nlike we've covered enough exercises. Oh, by the way, this is in the extras exercises tab\nof the pytorch deep learning repo. So extras exercises and solutions that are contained in there.\nFar out. We've covered a lot. Look at all that. So that has been pytorch custom data sets.\nI will see you in the next section. Holy smokes. That was a lot of pytorch code.\nBut if you're still hungry for more, there is five more chapters available at learnpytorch.io,\nwhich cover transfer learning, my favorite topic, pytorch model experiment tracking,\npytorch paper replicating, and pytorch model deployment. How do you get your model into the\nhands of others? And if you'd like to learn in this video style, the videos for those chapters\nare available at zero to mastery.io. But otherwise, happy machine learning. And I'll see you next time.\n",
  "words": [
    "comprehensive",
    "course",
    "teach",
    "foundations",
    "machine",
    "learning",
    "deep",
    "learning",
    "using",
    "pytorch",
    "pytorch",
    "machine",
    "learning",
    "framework",
    "written",
    "python",
    "learn",
    "machine",
    "learning",
    "writing",
    "pytorch",
    "code",
    "doubt",
    "run",
    "provided",
    "code",
    "experiment",
    "teacher",
    "course",
    "daniel",
    "bourke",
    "daniel",
    "machine",
    "learning",
    "engineer",
    "popular",
    "course",
    "creator",
    "enjoy",
    "course",
    "watch",
    "whole",
    "thing",
    "one",
    "sitting",
    "hello",
    "welcome",
    "video",
    "quite",
    "big",
    "one",
    "come",
    "learn",
    "machine",
    "learning",
    "deep",
    "learning",
    "pytorch",
    "code",
    "well",
    "right",
    "place",
    "video",
    "tutorial",
    "focused",
    "beginners",
    "got",
    "three",
    "six",
    "months",
    "python",
    "coding",
    "experience",
    "going",
    "cover",
    "whole",
    "bunch",
    "important",
    "machine",
    "learning",
    "concepts",
    "writing",
    "pytorch",
    "code",
    "get",
    "stuck",
    "leave",
    "comment",
    "post",
    "course",
    "github",
    "discussions",
    "page",
    "github",
    "able",
    "find",
    "materials",
    "cover",
    "well",
    "learn",
    "online",
    "readable",
    "book",
    "version",
    "course",
    "finish",
    "video",
    "find",
    "hey",
    "would",
    "still",
    "like",
    "learn",
    "pytorch",
    "mean",
    "ca",
    "really",
    "cover",
    "pytorch",
    "day",
    "video",
    "titles",
    "apply",
    "words",
    "length",
    "video",
    "aside",
    "five",
    "chapters",
    "available",
    "learn",
    "covering",
    "everything",
    "transfer",
    "learning",
    "model",
    "deployment",
    "experiment",
    "tracking",
    "videos",
    "go",
    "available",
    "zero",
    "enough",
    "machine",
    "learning",
    "see",
    "inside",
    "hello",
    "name",
    "daniel",
    "welcome",
    "deep",
    "learning",
    "pytorch",
    "course",
    "good",
    "watch",
    "twice",
    "welcome",
    "deep",
    "learning",
    "cools",
    "fire",
    "pytorch",
    "course",
    "exciting",
    "going",
    "see",
    "animation",
    "quite",
    "bit",
    "mean",
    "fun",
    "pytorch",
    "symbol",
    "flame",
    "torch",
    "let",
    "get",
    "naturally",
    "come",
    "course",
    "might",
    "already",
    "researched",
    "deep",
    "learning",
    "going",
    "cover",
    "quite",
    "briefly",
    "sense",
    "much",
    "need",
    "know",
    "course",
    "going",
    "focused",
    "rather",
    "definitions",
    "going",
    "focused",
    "getting",
    "practical",
    "seeing",
    "things",
    "happen",
    "define",
    "machine",
    "learning",
    "see",
    "second",
    "deep",
    "learning",
    "subset",
    "machine",
    "learning",
    "machine",
    "learning",
    "turning",
    "things",
    "data",
    "almost",
    "anything",
    "images",
    "text",
    "tables",
    "numbers",
    "video",
    "audio",
    "files",
    "almost",
    "anything",
    "classified",
    "data",
    "numbers",
    "computers",
    "love",
    "numbers",
    "finding",
    "patterns",
    "numbers",
    "find",
    "patterns",
    "well",
    "computer",
    "part",
    "specifically",
    "machine",
    "learning",
    "algorithm",
    "deep",
    "learning",
    "algorithm",
    "things",
    "going",
    "building",
    "course",
    "code",
    "math",
    "course",
    "code",
    "focused",
    "want",
    "stress",
    "get",
    "focused",
    "writing",
    "code",
    "behind",
    "scenes",
    "code",
    "going",
    "trigger",
    "math",
    "find",
    "patterns",
    "numbers",
    "would",
    "like",
    "deep",
    "dive",
    "math",
    "behind",
    "code",
    "going",
    "linking",
    "extra",
    "resources",
    "however",
    "going",
    "getting",
    "hands",
    "writing",
    "lots",
    "code",
    "lots",
    "keep",
    "going",
    "break",
    "things",
    "little",
    "bit",
    "machine",
    "learning",
    "versus",
    "deep",
    "learning",
    "giant",
    "bubble",
    "artificial",
    "intelligence",
    "might",
    "seen",
    "something",
    "similar",
    "like",
    "internet",
    "copied",
    "put",
    "pretty",
    "colors",
    "course",
    "got",
    "overarching",
    "big",
    "bubble",
    "topic",
    "artificial",
    "intelligence",
    "could",
    "define",
    "almost",
    "anything",
    "want",
    "typically",
    "subset",
    "within",
    "artificial",
    "intelligence",
    "known",
    "machine",
    "learning",
    "quite",
    "broad",
    "topic",
    "within",
    "machine",
    "learning",
    "another",
    "topic",
    "called",
    "deep",
    "learning",
    "going",
    "focused",
    "working",
    "pytorch",
    "writing",
    "deep",
    "learning",
    "code",
    "could",
    "use",
    "pytorch",
    "lot",
    "different",
    "machine",
    "learning",
    "things",
    "truth",
    "told",
    "kind",
    "use",
    "two",
    "terms",
    "interchangeably",
    "yes",
    "ml",
    "broader",
    "topic",
    "deep",
    "learning",
    "bit",
    "nuanced",
    "want",
    "form",
    "definitions",
    "highly",
    "encourage",
    "course",
    "focused",
    "rather",
    "defining",
    "things",
    "seeing",
    "work",
    "focused",
    "break",
    "things",
    "familiar",
    "fundamentals",
    "machine",
    "learning",
    "probably",
    "understand",
    "paradigm",
    "going",
    "rehash",
    "anyway",
    "consider",
    "traditional",
    "programming",
    "let",
    "say",
    "like",
    "write",
    "computer",
    "program",
    "enabled",
    "ability",
    "reproduce",
    "grandmother",
    "favorite",
    "famous",
    "roast",
    "chicken",
    "dish",
    "might",
    "inputs",
    "beautiful",
    "vegetables",
    "chicken",
    "raised",
    "farm",
    "might",
    "write",
    "rules",
    "could",
    "program",
    "cut",
    "vegetables",
    "season",
    "chicken",
    "preheat",
    "oven",
    "cook",
    "chicken",
    "30",
    "minutes",
    "add",
    "vegetables",
    "might",
    "simple",
    "might",
    "actually",
    "sicilian",
    "grandmother",
    "great",
    "cook",
    "put",
    "things",
    "art",
    "step",
    "step",
    "inputs",
    "combined",
    "rules",
    "makes",
    "beautiful",
    "roast",
    "chicken",
    "dish",
    "traditional",
    "programming",
    "machine",
    "learning",
    "algorithm",
    "typically",
    "takes",
    "inputs",
    "desired",
    "outputs",
    "figures",
    "rules",
    "patterns",
    "inputs",
    "outputs",
    "traditional",
    "program",
    "hand",
    "write",
    "rules",
    "ideal",
    "machine",
    "learning",
    "algorithm",
    "figure",
    "bridge",
    "inputs",
    "idealized",
    "output",
    "machine",
    "learning",
    "sense",
    "typically",
    "described",
    "supervised",
    "learning",
    "kind",
    "input",
    "kind",
    "output",
    "also",
    "known",
    "features",
    "also",
    "known",
    "labels",
    "machine",
    "learning",
    "algorithm",
    "job",
    "figure",
    "relationships",
    "inputs",
    "features",
    "outputs",
    "label",
    "wanted",
    "write",
    "machine",
    "learning",
    "algorithm",
    "figure",
    "sicilian",
    "grandmother",
    "famous",
    "roast",
    "chicken",
    "dish",
    "would",
    "probably",
    "gather",
    "bunch",
    "inputs",
    "ingredients",
    "delicious",
    "vegetables",
    "chicken",
    "whole",
    "bunch",
    "outputs",
    "finished",
    "product",
    "see",
    "algorithm",
    "figure",
    "go",
    "inputs",
    "output",
    "almost",
    "enough",
    "cover",
    "difference",
    "traditional",
    "programming",
    "machine",
    "learning",
    "far",
    "definitions",
    "go",
    "going",
    "get",
    "hands",
    "encoding",
    "sort",
    "algorithms",
    "throughout",
    "course",
    "let",
    "go",
    "next",
    "video",
    "ask",
    "question",
    "use",
    "machine",
    "learning",
    "deep",
    "learning",
    "actually",
    "get",
    "like",
    "think",
    "going",
    "back",
    "saw",
    "paradigm",
    "traditional",
    "programming",
    "machine",
    "learning",
    "would",
    "want",
    "use",
    "machine",
    "learning",
    "algorithms",
    "rather",
    "traditional",
    "programming",
    "write",
    "rules",
    "could",
    "get",
    "cumbersome",
    "think",
    "cover",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "briefly",
    "difference",
    "traditional",
    "programming",
    "machine",
    "learning",
    "want",
    "spend",
    "much",
    "time",
    "definitions",
    "rather",
    "see",
    "practice",
    "left",
    "question",
    "would",
    "want",
    "use",
    "machine",
    "learning",
    "deep",
    "learning",
    "well",
    "let",
    "think",
    "good",
    "reason",
    "mean",
    "write",
    "handwritten",
    "rules",
    "reproduce",
    "alsace",
    "grandmother",
    "roast",
    "chicken",
    "dish",
    "time",
    "would",
    "quite",
    "cumbersome",
    "right",
    "well",
    "let",
    "draw",
    "line",
    "better",
    "reason",
    "kind",
    "said",
    "right",
    "complex",
    "problem",
    "think",
    "rules",
    "let",
    "imagine",
    "trying",
    "build",
    "car",
    "learned",
    "drive",
    "probably",
    "done",
    "maybe",
    "20",
    "hours",
    "100",
    "hours",
    "give",
    "task",
    "writing",
    "every",
    "single",
    "rule",
    "driving",
    "back",
    "driveway",
    "turn",
    "left",
    "go",
    "street",
    "park",
    "reverse",
    "park",
    "stop",
    "intersection",
    "know",
    "fast",
    "go",
    "somewhere",
    "listed",
    "half",
    "dozen",
    "rules",
    "could",
    "probably",
    "go",
    "fair",
    "might",
    "get",
    "thousands",
    "complex",
    "problem",
    "driving",
    "think",
    "rules",
    "well",
    "probably",
    "machine",
    "learning",
    "deep",
    "learning",
    "come",
    "help",
    "beautiful",
    "comment",
    "like",
    "share",
    "one",
    "youtube",
    "videos",
    "2020",
    "machine",
    "learning",
    "roadmap",
    "yashawing",
    "probably",
    "going",
    "mispronounce",
    "even",
    "try",
    "yashawing",
    "says",
    "think",
    "use",
    "ml",
    "ml",
    "machine",
    "learning",
    "going",
    "use",
    "lot",
    "throughout",
    "course",
    "way",
    "ml",
    "machine",
    "learning",
    "know",
    "literally",
    "anything",
    "long",
    "convert",
    "numbers",
    "ah",
    "said",
    "machine",
    "learning",
    "turning",
    "something",
    "computer",
    "readable",
    "numbers",
    "programming",
    "find",
    "patterns",
    "except",
    "machine",
    "learning",
    "algorithm",
    "typically",
    "write",
    "algorithm",
    "finds",
    "patterns",
    "us",
    "literally",
    "could",
    "anything",
    "input",
    "output",
    "universe",
    "pretty",
    "darn",
    "cool",
    "machine",
    "learning",
    "right",
    "always",
    "use",
    "could",
    "used",
    "anything",
    "well",
    "like",
    "also",
    "introduce",
    "google",
    "number",
    "one",
    "rule",
    "machine",
    "learning",
    "build",
    "simple",
    "rule",
    "based",
    "system",
    "step",
    "five",
    "rules",
    "map",
    "ingredients",
    "sicilian",
    "grandmothers",
    "roast",
    "chicken",
    "dish",
    "write",
    "five",
    "steps",
    "going",
    "work",
    "every",
    "time",
    "well",
    "probably",
    "build",
    "simple",
    "rule",
    "based",
    "system",
    "require",
    "machine",
    "learning",
    "course",
    "maybe",
    "simple",
    "maybe",
    "write",
    "rules",
    "solve",
    "problem",
    "working",
    "wise",
    "software",
    "engineer",
    "kind",
    "hinted",
    "rule",
    "one",
    "google",
    "machine",
    "learning",
    "handbook",
    "going",
    "highly",
    "recommend",
    "read",
    "going",
    "go",
    "video",
    "check",
    "google",
    "otherwise",
    "links",
    "get",
    "links",
    "keep",
    "mind",
    "although",
    "machine",
    "learning",
    "powerful",
    "fun",
    "excited",
    "mean",
    "always",
    "use",
    "know",
    "quite",
    "thing",
    "saying",
    "start",
    "deep",
    "learning",
    "machine",
    "learning",
    "course",
    "want",
    "keep",
    "mind",
    "simple",
    "rule",
    "based",
    "systems",
    "still",
    "good",
    "machine",
    "learning",
    "solve",
    "everything",
    "let",
    "look",
    "deep",
    "learning",
    "good",
    "going",
    "leave",
    "clip",
    "hammock",
    "going",
    "check",
    "next",
    "video",
    "see",
    "soon",
    "last",
    "video",
    "familiarized",
    "google",
    "number",
    "one",
    "rule",
    "machine",
    "learning",
    "basically",
    "need",
    "use",
    "mind",
    "actually",
    "looking",
    "use",
    "machine",
    "learning",
    "deep",
    "learning",
    "well",
    "problems",
    "long",
    "lists",
    "rules",
    "traditional",
    "approach",
    "fails",
    "remember",
    "traditional",
    "approach",
    "sort",
    "data",
    "input",
    "write",
    "list",
    "rules",
    "data",
    "manipulated",
    "way",
    "shape",
    "form",
    "outputs",
    "know",
    "long",
    "long",
    "list",
    "rules",
    "like",
    "rules",
    "driving",
    "car",
    "could",
    "hundreds",
    "could",
    "thousands",
    "could",
    "millions",
    "knows",
    "machine",
    "learning",
    "deep",
    "learning",
    "may",
    "help",
    "kind",
    "moment",
    "world",
    "cars",
    "machine",
    "learning",
    "deep",
    "learning",
    "state",
    "art",
    "approach",
    "continually",
    "changing",
    "environments",
    "whatever",
    "benefits",
    "deep",
    "learning",
    "keep",
    "learning",
    "needs",
    "adapt",
    "learn",
    "new",
    "scenarios",
    "update",
    "data",
    "model",
    "trained",
    "adjust",
    "new",
    "different",
    "kinds",
    "data",
    "future",
    "similarly",
    "driving",
    "car",
    "might",
    "know",
    "neighborhood",
    "well",
    "go",
    "somewhere",
    "sure",
    "draw",
    "foundations",
    "know",
    "going",
    "adapt",
    "fast",
    "go",
    "stop",
    "park",
    "kinds",
    "things",
    "problems",
    "long",
    "lists",
    "rules",
    "continually",
    "changing",
    "environments",
    "large",
    "large",
    "data",
    "set",
    "deep",
    "learning",
    "flourishing",
    "world",
    "technology",
    "let",
    "give",
    "example",
    "one",
    "favorites",
    "food",
    "101",
    "data",
    "set",
    "search",
    "online",
    "images",
    "101",
    "different",
    "kinds",
    "foods",
    "briefly",
    "looked",
    "rule",
    "list",
    "might",
    "look",
    "like",
    "cooking",
    "grandmother",
    "famous",
    "sicilian",
    "roast",
    "chicken",
    "dish",
    "imagine",
    "wanted",
    "build",
    "app",
    "could",
    "take",
    "photos",
    "different",
    "food",
    "long",
    "list",
    "rules",
    "would",
    "differentiate",
    "101",
    "different",
    "foods",
    "long",
    "need",
    "rule",
    "sets",
    "every",
    "single",
    "one",
    "let",
    "take",
    "one",
    "food",
    "example",
    "write",
    "program",
    "tell",
    "banana",
    "looks",
    "like",
    "mean",
    "code",
    "banana",
    "looks",
    "like",
    "banana",
    "everything",
    "banana",
    "looks",
    "like",
    "keep",
    "mind",
    "deep",
    "learning",
    "good",
    "problems",
    "long",
    "lists",
    "rules",
    "continually",
    "changing",
    "environments",
    "discovering",
    "insights",
    "within",
    "large",
    "collections",
    "data",
    "deep",
    "learning",
    "good",
    "going",
    "write",
    "typically",
    "problem",
    "specific",
    "deep",
    "learning",
    "quite",
    "powerful",
    "days",
    "things",
    "might",
    "change",
    "future",
    "keep",
    "open",
    "mind",
    "anything",
    "course",
    "tell",
    "exactly",
    "spark",
    "curiosity",
    "figure",
    "even",
    "better",
    "yet",
    "need",
    "explainability",
    "see",
    "patterns",
    "learned",
    "deep",
    "learning",
    "model",
    "lots",
    "numbers",
    "called",
    "weights",
    "biases",
    "look",
    "later",
    "typically",
    "uninterpretable",
    "human",
    "times",
    "deep",
    "learning",
    "models",
    "million",
    "10",
    "million",
    "100",
    "million",
    "billion",
    "models",
    "getting",
    "trillions",
    "parameters",
    "say",
    "parameters",
    "mean",
    "numbers",
    "patterns",
    "data",
    "remember",
    "machine",
    "learning",
    "turning",
    "things",
    "numbers",
    "writing",
    "machine",
    "learning",
    "model",
    "find",
    "patterns",
    "numbers",
    "sometimes",
    "patterns",
    "lists",
    "numbers",
    "millions",
    "imagine",
    "looking",
    "list",
    "numbers",
    "million",
    "different",
    "things",
    "going",
    "going",
    "quite",
    "hard",
    "find",
    "hard",
    "understand",
    "three",
    "four",
    "numbers",
    "let",
    "alone",
    "million",
    "traditional",
    "approach",
    "better",
    "option",
    "google",
    "rule",
    "number",
    "one",
    "machine",
    "learning",
    "need",
    "simple",
    "rule",
    "based",
    "system",
    "well",
    "maybe",
    "need",
    "use",
    "machine",
    "learning",
    "deep",
    "learning",
    "going",
    "use",
    "deep",
    "learning",
    "machine",
    "learning",
    "terms",
    "interchangeably",
    "concerned",
    "definitions",
    "form",
    "definitions",
    "know",
    "perspective",
    "ml",
    "deep",
    "learning",
    "quite",
    "similar",
    "arrows",
    "unacceptable",
    "since",
    "outputs",
    "deep",
    "learning",
    "model",
    "always",
    "predictable",
    "see",
    "deep",
    "learning",
    "models",
    "probabilistic",
    "means",
    "predict",
    "something",
    "making",
    "probabilistic",
    "bet",
    "whereas",
    "rule",
    "based",
    "system",
    "kind",
    "know",
    "outputs",
    "going",
    "every",
    "single",
    "time",
    "ca",
    "errors",
    "based",
    "probabilistic",
    "errors",
    "well",
    "probably",
    "use",
    "deep",
    "learning",
    "like",
    "go",
    "back",
    "simple",
    "rule",
    "based",
    "system",
    "finally",
    "much",
    "data",
    "deep",
    "learning",
    "models",
    "usually",
    "require",
    "fairly",
    "large",
    "amount",
    "data",
    "produce",
    "great",
    "results",
    "however",
    "caveat",
    "know",
    "start",
    "said",
    "typically",
    "going",
    "see",
    "techniques",
    "get",
    "great",
    "results",
    "without",
    "huge",
    "amounts",
    "data",
    "wrote",
    "typically",
    "techniques",
    "research",
    "deep",
    "learning",
    "explainability",
    "going",
    "find",
    "whole",
    "bunch",
    "stuff",
    "look",
    "examples",
    "machine",
    "learning",
    "versus",
    "deep",
    "learning",
    "arrows",
    "unacceptable",
    "ways",
    "make",
    "model",
    "reproducible",
    "predicts",
    "know",
    "going",
    "come",
    "lot",
    "testing",
    "verify",
    "well",
    "next",
    "ah",
    "got",
    "machine",
    "learning",
    "versus",
    "deep",
    "learning",
    "going",
    "look",
    "different",
    "problem",
    "spaces",
    "second",
    "mainly",
    "breaking",
    "terms",
    "kind",
    "data",
    "going",
    "prevent",
    "video",
    "getting",
    "long",
    "cover",
    "colorful",
    "beautiful",
    "pictures",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "things",
    "deep",
    "learning",
    "good",
    "deep",
    "learning",
    "typically",
    "good",
    "let",
    "dive",
    "little",
    "comparison",
    "machine",
    "learning",
    "versus",
    "deep",
    "learning",
    "going",
    "using",
    "terms",
    "quite",
    "interchangeably",
    "specific",
    "things",
    "typically",
    "want",
    "traditional",
    "style",
    "machine",
    "learning",
    "techniques",
    "versus",
    "deep",
    "learning",
    "however",
    "constantly",
    "changing",
    "talking",
    "absolutes",
    "talking",
    "general",
    "leave",
    "use",
    "curiosity",
    "research",
    "specific",
    "differences",
    "two",
    "typically",
    "machine",
    "learning",
    "like",
    "traditional",
    "style",
    "algorithms",
    "although",
    "still",
    "machine",
    "learning",
    "algorithms",
    "kind",
    "little",
    "bit",
    "confusing",
    "deep",
    "learning",
    "machine",
    "learning",
    "differ",
    "want",
    "use",
    "traditional",
    "machine",
    "learning",
    "algorithms",
    "structured",
    "data",
    "tables",
    "numbers",
    "mean",
    "structured",
    "rows",
    "columns",
    "structured",
    "data",
    "possibly",
    "one",
    "best",
    "algorithms",
    "type",
    "data",
    "gradient",
    "boosted",
    "machine",
    "xg",
    "boost",
    "algorithm",
    "see",
    "lot",
    "data",
    "science",
    "competitions",
    "also",
    "used",
    "production",
    "settings",
    "say",
    "production",
    "settings",
    "mean",
    "applications",
    "may",
    "interact",
    "internet",
    "use",
    "day",
    "day",
    "production",
    "xg",
    "boost",
    "typically",
    "favorite",
    "algorithm",
    "kinds",
    "situations",
    "structured",
    "data",
    "might",
    "look",
    "xg",
    "boost",
    "rather",
    "building",
    "deep",
    "learning",
    "algorithm",
    "rules",
    "set",
    "stone",
    "deep",
    "learning",
    "machine",
    "learning",
    "kind",
    "art",
    "kind",
    "science",
    "sometimes",
    "xg",
    "boost",
    "best",
    "structured",
    "data",
    "might",
    "exceptions",
    "rule",
    "deep",
    "learning",
    "typically",
    "better",
    "unstructured",
    "data",
    "mean",
    "data",
    "kind",
    "place",
    "nice",
    "standardized",
    "rows",
    "columns",
    "say",
    "natural",
    "language",
    "tweet",
    "person",
    "whose",
    "name",
    "quite",
    "similar",
    "mine",
    "twitter",
    "account",
    "oh",
    "maybe",
    "wrote",
    "learn",
    "machine",
    "learning",
    "need",
    "hear",
    "learn",
    "python",
    "learn",
    "math",
    "start",
    "probability",
    "software",
    "engineering",
    "build",
    "need",
    "google",
    "go",
    "rabbit",
    "hole",
    "resurfacing",
    "six",
    "nine",
    "months",
    "ring",
    "assess",
    "like",
    "whole",
    "bunch",
    "texts",
    "definition",
    "deep",
    "learning",
    "wikipedia",
    "reason",
    "covering",
    "many",
    "definitions",
    "course",
    "look",
    "simple",
    "look",
    "things",
    "wikipedia",
    "going",
    "able",
    "define",
    "deep",
    "learning",
    "far",
    "better",
    "focused",
    "getting",
    "involved",
    "working",
    "hands",
    "stuff",
    "defining",
    "images",
    "wanted",
    "build",
    "burger",
    "take",
    "photo",
    "app",
    "thing",
    "would",
    "work",
    "image",
    "data",
    "really",
    "much",
    "structure",
    "although",
    "see",
    "ways",
    "deep",
    "learning",
    "turn",
    "kind",
    "data",
    "sort",
    "structure",
    "beauty",
    "tensor",
    "might",
    "audio",
    "files",
    "talking",
    "voice",
    "assistant",
    "going",
    "say",
    "one",
    "whole",
    "bunch",
    "devices",
    "might",
    "go",
    "crazy",
    "say",
    "name",
    "voice",
    "assistant",
    "rhymes",
    "even",
    "going",
    "say",
    "loud",
    "typically",
    "unstructured",
    "data",
    "want",
    "use",
    "neural",
    "network",
    "kind",
    "structured",
    "data",
    "gradient",
    "boosted",
    "machine",
    "random",
    "forest",
    "tree",
    "based",
    "algorithm",
    "extra",
    "boost",
    "unstructured",
    "data",
    "neural",
    "networks",
    "let",
    "keep",
    "going",
    "let",
    "look",
    "common",
    "algorithms",
    "might",
    "use",
    "structured",
    "data",
    "machine",
    "learning",
    "versus",
    "unstructured",
    "data",
    "deep",
    "learning",
    "random",
    "forest",
    "one",
    "favorites",
    "gradient",
    "boosted",
    "models",
    "native",
    "base",
    "nearest",
    "neighbor",
    "support",
    "vector",
    "machine",
    "svm",
    "many",
    "since",
    "advent",
    "deep",
    "learning",
    "often",
    "referred",
    "shallow",
    "algorithms",
    "deep",
    "learning",
    "called",
    "deep",
    "learning",
    "well",
    "see",
    "many",
    "different",
    "layers",
    "algorithm",
    "might",
    "input",
    "layer",
    "100",
    "layers",
    "middle",
    "output",
    "layer",
    "get",
    "hands",
    "later",
    "common",
    "algorithms",
    "deep",
    "learning",
    "neural",
    "networks",
    "fully",
    "connected",
    "neural",
    "network",
    "convolutional",
    "neural",
    "network",
    "recurrent",
    "neural",
    "network",
    "transformers",
    "taken",
    "past",
    "couple",
    "years",
    "course",
    "many",
    "beautiful",
    "thing",
    "deep",
    "learning",
    "neural",
    "networks",
    "almost",
    "many",
    "problems",
    "applied",
    "many",
    "different",
    "ways",
    "construct",
    "putting",
    "dot",
    "points",
    "page",
    "understand",
    "much",
    "experience",
    "machine",
    "learning",
    "deep",
    "learning",
    "whole",
    "bunch",
    "information",
    "overload",
    "good",
    "news",
    "going",
    "focused",
    "building",
    "pytorch",
    "neural",
    "networks",
    "fully",
    "connected",
    "neural",
    "networks",
    "convolutional",
    "neural",
    "networks",
    "foundation",
    "deep",
    "learning",
    "excellent",
    "thing",
    "exciting",
    "thing",
    "learn",
    "foundational",
    "building",
    "blocks",
    "get",
    "styles",
    "things",
    "part",
    "art",
    "part",
    "science",
    "machine",
    "learning",
    "deep",
    "learning",
    "depending",
    "represent",
    "problem",
    "depending",
    "problem",
    "many",
    "algorithms",
    "used",
    "know",
    "kind",
    "bedazzled",
    "saying",
    "oh",
    "well",
    "kind",
    "use",
    "ones",
    "deep",
    "learning",
    "kind",
    "use",
    "ones",
    "machine",
    "learning",
    "depending",
    "problem",
    "also",
    "use",
    "little",
    "bit",
    "confusion",
    "machine",
    "learning",
    "fun",
    "part",
    "use",
    "curiosity",
    "figure",
    "best",
    "whatever",
    "working",
    "talk",
    "neural",
    "networks",
    "next",
    "video",
    "cover",
    "neural",
    "networks",
    "like",
    "google",
    "watch",
    "next",
    "video",
    "going",
    "hundreds",
    "definitions",
    "like",
    "start",
    "forming",
    "definition",
    "neural",
    "network",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "left",
    "cliffhanger",
    "question",
    "neural",
    "networks",
    "gave",
    "challenge",
    "googling",
    "might",
    "already",
    "done",
    "time",
    "got",
    "let",
    "together",
    "type",
    "neural",
    "networks",
    "already",
    "done",
    "neural",
    "networks",
    "explain",
    "neural",
    "networks",
    "neural",
    "network",
    "definition",
    "hundreds",
    "definitions",
    "things",
    "like",
    "online",
    "neural",
    "network",
    "five",
    "minutes",
    "three",
    "blue",
    "one",
    "brown",
    "highly",
    "recommend",
    "channel",
    "series",
    "neural",
    "networks",
    "going",
    "extracurricular",
    "stat",
    "quest",
    "also",
    "amazing",
    "hundreds",
    "different",
    "definitions",
    "read",
    "10",
    "five",
    "three",
    "make",
    "definition",
    "sake",
    "course",
    "going",
    "find",
    "neural",
    "networks",
    "data",
    "whatever",
    "might",
    "images",
    "food",
    "might",
    "tweets",
    "natural",
    "language",
    "might",
    "speech",
    "examples",
    "inputs",
    "unstructured",
    "data",
    "rows",
    "columns",
    "input",
    "data",
    "use",
    "neural",
    "network",
    "well",
    "data",
    "used",
    "neural",
    "network",
    "needs",
    "turned",
    "numbers",
    "humans",
    "like",
    "looking",
    "images",
    "raman",
    "spaghetti",
    "know",
    "raman",
    "know",
    "spaghetti",
    "seen",
    "one",
    "two",
    "times",
    "like",
    "reading",
    "good",
    "tweets",
    "like",
    "listening",
    "amazing",
    "music",
    "hearing",
    "friend",
    "talk",
    "phone",
    "audio",
    "file",
    "however",
    "computer",
    "understands",
    "going",
    "inputs",
    "needs",
    "turn",
    "numbers",
    "call",
    "numerical",
    "encoding",
    "representation",
    "numerical",
    "encoding",
    "square",
    "brackets",
    "indicate",
    "part",
    "matrix",
    "tensor",
    "going",
    "get",
    "hands",
    "throughout",
    "course",
    "inputs",
    "turned",
    "numbers",
    "pass",
    "neural",
    "network",
    "graphic",
    "neural",
    "network",
    "however",
    "graphics",
    "neural",
    "networks",
    "see",
    "get",
    "quite",
    "involved",
    "represent",
    "fundamentals",
    "go",
    "one",
    "example",
    "input",
    "layer",
    "multiple",
    "hidden",
    "layers",
    "however",
    "define",
    "design",
    "want",
    "output",
    "layer",
    "inputs",
    "go",
    "kind",
    "data",
    "hidden",
    "layers",
    "perform",
    "mathematical",
    "operations",
    "input",
    "numbers",
    "output",
    "oh",
    "three",
    "blue",
    "one",
    "brown",
    "neural",
    "networks",
    "ground",
    "great",
    "video",
    "highly",
    "recommend",
    "check",
    "come",
    "back",
    "got",
    "inputs",
    "turned",
    "numbers",
    "got",
    "neural",
    "networks",
    "put",
    "input",
    "typically",
    "input",
    "layer",
    "hidden",
    "layer",
    "many",
    "different",
    "layers",
    "want",
    "many",
    "different",
    "little",
    "dots",
    "called",
    "node",
    "lot",
    "information",
    "going",
    "get",
    "seeing",
    "looks",
    "like",
    "kind",
    "output",
    "neural",
    "network",
    "use",
    "well",
    "choose",
    "appropriate",
    "neural",
    "network",
    "problem",
    "could",
    "involve",
    "hand",
    "coding",
    "one",
    "steps",
    "could",
    "find",
    "one",
    "worked",
    "problems",
    "similar",
    "images",
    "might",
    "use",
    "cnn",
    "convolutional",
    "neural",
    "network",
    "natural",
    "language",
    "might",
    "use",
    "transformer",
    "speech",
    "might",
    "also",
    "use",
    "transformer",
    "fundamentally",
    "follow",
    "principle",
    "inputs",
    "manipulation",
    "outputs",
    "neural",
    "network",
    "learn",
    "representation",
    "want",
    "find",
    "learns",
    "going",
    "manipulate",
    "patterns",
    "way",
    "shape",
    "form",
    "say",
    "learns",
    "representation",
    "going",
    "also",
    "refer",
    "learns",
    "patterns",
    "data",
    "lot",
    "people",
    "refer",
    "features",
    "feature",
    "may",
    "fact",
    "word",
    "comes",
    "usually",
    "across",
    "whole",
    "bunch",
    "different",
    "languages",
    "feature",
    "almost",
    "anything",
    "want",
    "define",
    "neural",
    "network",
    "learns",
    "representations",
    "patterns",
    "features",
    "also",
    "called",
    "weights",
    "go",
    "well",
    "got",
    "sort",
    "numbers",
    "numerical",
    "encoding",
    "turned",
    "data",
    "numbers",
    "neural",
    "network",
    "learned",
    "representation",
    "thinks",
    "best",
    "represents",
    "patterns",
    "data",
    "outputs",
    "representation",
    "outputs",
    "use",
    "often",
    "hear",
    "referred",
    "features",
    "weight",
    "matrix",
    "weight",
    "tensor",
    "learned",
    "representation",
    "also",
    "another",
    "common",
    "one",
    "lot",
    "different",
    "terms",
    "things",
    "output",
    "convert",
    "outputs",
    "human",
    "understandable",
    "outputs",
    "look",
    "could",
    "said",
    "representations",
    "patterns",
    "neural",
    "network",
    "learns",
    "millions",
    "numbers",
    "nine",
    "imagine",
    "millions",
    "different",
    "numbers",
    "barely",
    "understand",
    "nine",
    "numbers",
    "going",
    "need",
    "way",
    "convert",
    "human",
    "understandable",
    "terms",
    "example",
    "might",
    "input",
    "data",
    "images",
    "food",
    "want",
    "neural",
    "network",
    "learn",
    "representations",
    "image",
    "ramen",
    "image",
    "spaghetti",
    "eventually",
    "take",
    "patterns",
    "learned",
    "convert",
    "whether",
    "thinks",
    "image",
    "ramen",
    "spaghetti",
    "case",
    "tweet",
    "tweet",
    "natural",
    "disaster",
    "natural",
    "disaster",
    "neural",
    "network",
    "well",
    "written",
    "code",
    "turn",
    "numbers",
    "pass",
    "neural",
    "network",
    "neural",
    "network",
    "learned",
    "kind",
    "patterns",
    "ideally",
    "want",
    "represent",
    "tweet",
    "disaster",
    "write",
    "code",
    "steps",
    "thing",
    "inputs",
    "going",
    "speech",
    "turning",
    "something",
    "might",
    "say",
    "smart",
    "speaker",
    "going",
    "say",
    "whole",
    "bunch",
    "devices",
    "might",
    "go",
    "let",
    "cover",
    "anatomy",
    "neural",
    "networks",
    "hinted",
    "little",
    "bit",
    "already",
    "like",
    "neural",
    "network",
    "anatomy",
    "highly",
    "customizable",
    "thing",
    "actually",
    "going",
    "see",
    "pytorch",
    "code",
    "later",
    "data",
    "goes",
    "input",
    "layer",
    "case",
    "number",
    "units",
    "slash",
    "neurons",
    "slash",
    "nodes",
    "two",
    "hidden",
    "layers",
    "put",
    "one",
    "hidden",
    "layer",
    "deep",
    "deep",
    "learning",
    "comes",
    "lots",
    "layers",
    "showing",
    "four",
    "layers",
    "might",
    "well",
    "three",
    "layers",
    "well",
    "might",
    "deep",
    "neural",
    "networks",
    "resnet",
    "152",
    "different",
    "layers",
    "34",
    "resnet",
    "resnet",
    "152",
    "152",
    "different",
    "layers",
    "common",
    "computer",
    "vision",
    "popular",
    "computer",
    "vision",
    "algorithm",
    "way",
    "lots",
    "terms",
    "throwing",
    "time",
    "start",
    "become",
    "familiar",
    "hidden",
    "layers",
    "almost",
    "many",
    "want",
    "got",
    "pictured",
    "one",
    "case",
    "three",
    "hidden",
    "units",
    "slash",
    "neurons",
    "output",
    "layer",
    "outputs",
    "learned",
    "representation",
    "prediction",
    "probabilities",
    "depending",
    "set",
    "see",
    "later",
    "case",
    "one",
    "hidden",
    "unit",
    "two",
    "input",
    "three",
    "one",
    "output",
    "customize",
    "number",
    "customize",
    "many",
    "layers",
    "customize",
    "goes",
    "customize",
    "goes",
    "talk",
    "overall",
    "architecture",
    "describing",
    "layers",
    "combined",
    "hear",
    "neural",
    "network",
    "architecture",
    "talks",
    "input",
    "hidden",
    "layers",
    "may",
    "one",
    "output",
    "layer",
    "terminology",
    "overall",
    "architecture",
    "say",
    "patterns",
    "arbitrary",
    "term",
    "hear",
    "embedding",
    "embedding",
    "might",
    "come",
    "hidden",
    "layers",
    "weights",
    "feature",
    "representation",
    "feature",
    "vectors",
    "referring",
    "similar",
    "things",
    "turn",
    "data",
    "numerical",
    "form",
    "build",
    "neural",
    "network",
    "figure",
    "patterns",
    "output",
    "desired",
    "output",
    "want",
    "get",
    "technical",
    "layer",
    "usually",
    "combination",
    "linear",
    "straight",
    "lines",
    "nonlinear",
    "functions",
    "mean",
    "linear",
    "function",
    "straight",
    "line",
    "nonlinear",
    "function",
    "line",
    "asked",
    "draw",
    "whatever",
    "want",
    "unlimited",
    "straight",
    "lines",
    "straight",
    "lines",
    "use",
    "straight",
    "lines",
    "curved",
    "lines",
    "kind",
    "patterns",
    "could",
    "draw",
    "fundamental",
    "level",
    "basically",
    "neural",
    "network",
    "using",
    "combination",
    "linear",
    "straight",
    "lines",
    "straight",
    "lines",
    "draw",
    "patterns",
    "data",
    "see",
    "looks",
    "like",
    "later",
    "next",
    "video",
    "let",
    "dive",
    "briefly",
    "different",
    "kinds",
    "learning",
    "looked",
    "neural",
    "network",
    "overall",
    "algorithm",
    "also",
    "different",
    "paradigms",
    "neural",
    "network",
    "learns",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "discussed",
    "brief",
    "overview",
    "anatomy",
    "neural",
    "network",
    "let",
    "discuss",
    "learning",
    "paradigms",
    "first",
    "one",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "transfer",
    "learning",
    "supervised",
    "learning",
    "data",
    "labels",
    "example",
    "gave",
    "start",
    "would",
    "build",
    "neural",
    "network",
    "machine",
    "learning",
    "algorithm",
    "figure",
    "rules",
    "cook",
    "sicilian",
    "grandmother",
    "famous",
    "roast",
    "chicken",
    "dish",
    "case",
    "supervised",
    "learning",
    "lot",
    "data",
    "inputs",
    "raw",
    "ingredients",
    "vegetables",
    "chicken",
    "lot",
    "examples",
    "inputs",
    "ideally",
    "look",
    "like",
    "case",
    "discerning",
    "photos",
    "cat",
    "dog",
    "might",
    "thousand",
    "photos",
    "cat",
    "thousand",
    "photos",
    "dog",
    "know",
    "photos",
    "cat",
    "photos",
    "dog",
    "pass",
    "photos",
    "machine",
    "learning",
    "algorithm",
    "discern",
    "case",
    "data",
    "photos",
    "labels",
    "aka",
    "cat",
    "dog",
    "photos",
    "supervised",
    "learning",
    "data",
    "labels",
    "unsupervised",
    "learning",
    "data",
    "labels",
    "case",
    "cat",
    "dog",
    "photos",
    "photos",
    "labels",
    "cat",
    "dog",
    "case",
    "learning",
    "could",
    "get",
    "machine",
    "learning",
    "algorithm",
    "learn",
    "inherent",
    "representation",
    "say",
    "representation",
    "mean",
    "patterns",
    "numbers",
    "mean",
    "weights",
    "mean",
    "features",
    "whole",
    "bunch",
    "different",
    "names",
    "describing",
    "thing",
    "could",
    "get",
    "learning",
    "algorithm",
    "figure",
    "fundamental",
    "patterns",
    "dog",
    "cat",
    "image",
    "would",
    "necessarily",
    "know",
    "difference",
    "two",
    "could",
    "come",
    "later",
    "go",
    "show",
    "patterns",
    "learned",
    "might",
    "show",
    "patterns",
    "could",
    "go",
    "okay",
    "patterns",
    "look",
    "like",
    "dog",
    "patterns",
    "look",
    "like",
    "cat",
    "unsupervised",
    "learning",
    "learn",
    "solely",
    "data",
    "finally",
    "transfer",
    "learning",
    "important",
    "paradigm",
    "deep",
    "learning",
    "taking",
    "patterns",
    "one",
    "model",
    "learned",
    "data",
    "set",
    "transferring",
    "another",
    "model",
    "case",
    "trying",
    "build",
    "supervised",
    "learning",
    "algorithm",
    "discerning",
    "cat",
    "dog",
    "photos",
    "might",
    "start",
    "model",
    "already",
    "learned",
    "patterns",
    "images",
    "transfer",
    "foundational",
    "patterns",
    "model",
    "model",
    "gets",
    "head",
    "start",
    "transfer",
    "learning",
    "powerful",
    "technique",
    "course",
    "going",
    "writing",
    "code",
    "focus",
    "two",
    "supervised",
    "learning",
    "transfer",
    "learning",
    "two",
    "common",
    "paradigms",
    "common",
    "types",
    "learning",
    "machine",
    "learning",
    "deep",
    "learning",
    "however",
    "style",
    "code",
    "though",
    "adapted",
    "across",
    "different",
    "learning",
    "paradigms",
    "want",
    "let",
    "know",
    "one",
    "mentioned",
    "kind",
    "bucket",
    "reinforcement",
    "learning",
    "leave",
    "extension",
    "wanted",
    "look",
    "essentially",
    "good",
    "one",
    "good",
    "photo",
    "actually",
    "shout",
    "katie",
    "nuggets",
    "whole",
    "idea",
    "reinforcement",
    "learning",
    "kind",
    "environment",
    "agent",
    "actions",
    "environment",
    "give",
    "rewards",
    "observations",
    "back",
    "agent",
    "say",
    "example",
    "wanted",
    "teach",
    "dog",
    "urinate",
    "outside",
    "well",
    "would",
    "reward",
    "actions",
    "urinating",
    "outside",
    "possibly",
    "reward",
    "actions",
    "urinating",
    "couch",
    "reinforcement",
    "learning",
    "kind",
    "paradigm",
    "picture",
    "good",
    "explanation",
    "unsupervised",
    "learning",
    "supervised",
    "learning",
    "separate",
    "two",
    "different",
    "things",
    "reinforcement",
    "learning",
    "kind",
    "like",
    "let",
    "research",
    "different",
    "learning",
    "paradigms",
    "little",
    "bit",
    "time",
    "said",
    "going",
    "focused",
    "writing",
    "code",
    "supervised",
    "learning",
    "transfer",
    "learning",
    "specifically",
    "pytorch",
    "code",
    "covered",
    "let",
    "get",
    "examples",
    "deep",
    "learning",
    "actually",
    "used",
    "get",
    "next",
    "video",
    "going",
    "issue",
    "challenge",
    "search",
    "question",
    "come",
    "ideas",
    "deep",
    "learning",
    "currently",
    "used",
    "give",
    "shot",
    "see",
    "next",
    "video",
    "go",
    "research",
    "find",
    "deep",
    "learning",
    "actually",
    "used",
    "bet",
    "found",
    "treasure",
    "trail",
    "things",
    "hey",
    "mean",
    "reading",
    "course",
    "chances",
    "probably",
    "already",
    "know",
    "use",
    "cases",
    "deep",
    "learning",
    "like",
    "daniel",
    "hurry",
    "get",
    "code",
    "well",
    "going",
    "get",
    "worry",
    "let",
    "look",
    "things",
    "deep",
    "learning",
    "used",
    "want",
    "remind",
    "comment",
    "yasha",
    "sway",
    "2020",
    "machine",
    "learning",
    "roadmap",
    "video",
    "think",
    "use",
    "ml",
    "remember",
    "ml",
    "machine",
    "learning",
    "remember",
    "deep",
    "learning",
    "part",
    "ml",
    "literally",
    "anything",
    "long",
    "convert",
    "numbers",
    "program",
    "find",
    "patterns",
    "literally",
    "could",
    "anything",
    "input",
    "output",
    "universe",
    "beautiful",
    "thing",
    "machine",
    "learning",
    "encode",
    "something",
    "numbers",
    "chances",
    "build",
    "machine",
    "learning",
    "algorithm",
    "find",
    "patterns",
    "numbers",
    "work",
    "well",
    "reason",
    "machine",
    "learning",
    "deep",
    "learning",
    "part",
    "art",
    "part",
    "science",
    "scientist",
    "would",
    "love",
    "know",
    "experiments",
    "would",
    "work",
    "artist",
    "kind",
    "excited",
    "fact",
    "know",
    "might",
    "work",
    "might",
    "something",
    "keep",
    "mind",
    "along",
    "rule",
    "number",
    "one",
    "machine",
    "learning",
    "need",
    "use",
    "use",
    "used",
    "almost",
    "anything",
    "let",
    "get",
    "little",
    "bit",
    "specific",
    "find",
    "deep",
    "learning",
    "use",
    "cases",
    "put",
    "reason",
    "lots",
    "interact",
    "day",
    "day",
    "life",
    "recommendation",
    "got",
    "programming",
    "video",
    "got",
    "programming",
    "podcast",
    "got",
    "jujitsu",
    "videos",
    "got",
    "runescape",
    "videos",
    "soundtrack",
    "favorite",
    "movie",
    "noticed",
    "whenever",
    "go",
    "youtube",
    "really",
    "search",
    "things",
    "anymore",
    "well",
    "sometimes",
    "might",
    "recommendation",
    "page",
    "pretty",
    "darn",
    "good",
    "powered",
    "deep",
    "learning",
    "last",
    "10",
    "years",
    "noticed",
    "translation",
    "got",
    "pretty",
    "good",
    "well",
    "powered",
    "deep",
    "learning",
    "well",
    "much",
    "hands",
    "experience",
    "use",
    "japan",
    "speak",
    "little",
    "amount",
    "japanese",
    "even",
    "smaller",
    "amount",
    "mandarin",
    "wanted",
    "translate",
    "deep",
    "learning",
    "epic",
    "spanish",
    "might",
    "come",
    "el",
    "aprendise",
    "profando",
    "es",
    "ebiko",
    "native",
    "spanish",
    "speakers",
    "watching",
    "video",
    "laugh",
    "australian",
    "version",
    "saying",
    "deep",
    "learning",
    "epic",
    "spanish",
    "cool",
    "google",
    "translate",
    "powered",
    "deep",
    "learning",
    "beautiful",
    "thing",
    "could",
    "say",
    "could",
    "click",
    "speaker",
    "would",
    "say",
    "speech",
    "recognition",
    "powered",
    "deep",
    "learning",
    "ask",
    "voice",
    "assistant",
    "biggest",
    "big",
    "dog",
    "course",
    "going",
    "say",
    "set",
    "voice",
    "assistant",
    "say",
    "part",
    "speech",
    "recognition",
    "computer",
    "vision",
    "oh",
    "look",
    "see",
    "photo",
    "photo",
    "person",
    "driving",
    "car",
    "hit",
    "run",
    "car",
    "front",
    "house",
    "apartment",
    "building",
    "car",
    "parked",
    "street",
    "car",
    "trailer",
    "came",
    "ran",
    "back",
    "car",
    "basically",
    "destroyed",
    "drove",
    "however",
    "next",
    "door",
    "neighbors",
    "security",
    "camera",
    "picked",
    "car",
    "became",
    "detective",
    "week",
    "thought",
    "hmm",
    "computer",
    "vision",
    "algorithm",
    "built",
    "camera",
    "could",
    "detected",
    "car",
    "hit",
    "mean",
    "took",
    "lot",
    "searching",
    "find",
    "turns",
    "car",
    "hit",
    "morning",
    "pitch",
    "black",
    "course",
    "get",
    "license",
    "plate",
    "person",
    "somewhere",
    "world",
    "hit",
    "run",
    "watching",
    "video",
    "remember",
    "computer",
    "vision",
    "might",
    "catch",
    "one",
    "day",
    "called",
    "object",
    "detection",
    "would",
    "place",
    "box",
    "around",
    "area",
    "pixels",
    "represent",
    "object",
    "looking",
    "computer",
    "vision",
    "could",
    "train",
    "object",
    "detector",
    "capture",
    "cars",
    "drive",
    "past",
    "certain",
    "camera",
    "someone",
    "hit",
    "run",
    "could",
    "capture",
    "fingers",
    "crossed",
    "dark",
    "read",
    "license",
    "plate",
    "go",
    "hey",
    "excuse",
    "please",
    "person",
    "hit",
    "car",
    "wrecked",
    "close",
    "home",
    "story",
    "computer",
    "vision",
    "could",
    "used",
    "finally",
    "natural",
    "language",
    "processing",
    "noticed",
    "well",
    "spam",
    "detector",
    "email",
    "inbox",
    "pretty",
    "darn",
    "good",
    "well",
    "powered",
    "deep",
    "learning",
    "hard",
    "tell",
    "days",
    "powered",
    "deep",
    "learning",
    "natural",
    "language",
    "processing",
    "process",
    "looking",
    "natural",
    "language",
    "text",
    "unstructured",
    "text",
    "whatever",
    "write",
    "email",
    "story",
    "wikipedia",
    "document",
    "deciding",
    "getting",
    "algorithm",
    "find",
    "patterns",
    "example",
    "would",
    "find",
    "email",
    "spam",
    "deep",
    "learning",
    "course",
    "incredible",
    "ca",
    "wait",
    "use",
    "learned",
    "thank",
    "much",
    "way",
    "real",
    "email",
    "want",
    "email",
    "spam",
    "hey",
    "daniel",
    "congratulations",
    "win",
    "lot",
    "money",
    "wow",
    "really",
    "like",
    "lot",
    "money",
    "somebody",
    "said",
    "think",
    "real",
    "would",
    "probably",
    "go",
    "spam",
    "inbox",
    "said",
    "wanted",
    "put",
    "problems",
    "little",
    "bit",
    "classification",
    "known",
    "sequence",
    "sequence",
    "put",
    "one",
    "sequence",
    "get",
    "one",
    "sequence",
    "sequence",
    "audio",
    "waves",
    "get",
    "text",
    "sequence",
    "sequence",
    "sec",
    "sec",
    "classification",
    "slash",
    "regression",
    "case",
    "regression",
    "predicting",
    "number",
    "regression",
    "problem",
    "would",
    "predict",
    "coordinates",
    "box",
    "corners",
    "say",
    "however",
    "many",
    "pixels",
    "x",
    "angle",
    "however",
    "many",
    "pixels",
    "angle",
    "corner",
    "would",
    "draw",
    "corners",
    "classification",
    "part",
    "would",
    "go",
    "hey",
    "car",
    "hit",
    "run",
    "us",
    "case",
    "classification",
    "classification",
    "predicting",
    "whether",
    "something",
    "one",
    "thing",
    "another",
    "perhaps",
    "one",
    "thing",
    "another",
    "class",
    "multi",
    "class",
    "classification",
    "email",
    "spam",
    "class",
    "email",
    "spam",
    "also",
    "class",
    "think",
    "got",
    "one",
    "direction",
    "go",
    "sort",
    "laid",
    "foundation",
    "course",
    "well",
    "let",
    "start",
    "talking",
    "pytorch",
    "see",
    "next",
    "video",
    "well",
    "let",
    "cover",
    "foundations",
    "pytorch",
    "first",
    "might",
    "asking",
    "pytorch",
    "well",
    "course",
    "could",
    "go",
    "friend",
    "internet",
    "look",
    "homepage",
    "pytorch",
    "course",
    "replacement",
    "everything",
    "homepage",
    "ground",
    "truth",
    "everything",
    "pytorch",
    "get",
    "started",
    "got",
    "big",
    "ecosystem",
    "got",
    "way",
    "set",
    "local",
    "computer",
    "got",
    "resources",
    "got",
    "docs",
    "pytorch",
    "got",
    "github",
    "got",
    "search",
    "got",
    "blog",
    "everything",
    "website",
    "place",
    "visiting",
    "throughout",
    "course",
    "writing",
    "pytorch",
    "code",
    "coming",
    "reading",
    "checking",
    "things",
    "looking",
    "examples",
    "sake",
    "course",
    "let",
    "break",
    "pytorch",
    "oh",
    "little",
    "flame",
    "animation",
    "forgot",
    "pytorch",
    "sync",
    "animations",
    "right",
    "pytorch",
    "popular",
    "research",
    "deep",
    "learning",
    "framework",
    "get",
    "second",
    "allows",
    "write",
    "fast",
    "deep",
    "learning",
    "code",
    "python",
    "know",
    "python",
    "programming",
    "language",
    "pytorch",
    "allows",
    "us",
    "write",
    "deep",
    "learning",
    "code",
    "accelerated",
    "gpus",
    "python",
    "enables",
    "access",
    "many",
    "deep",
    "learning",
    "models",
    "torch",
    "hub",
    "website",
    "lots",
    "remember",
    "said",
    "transfer",
    "learning",
    "way",
    "use",
    "deep",
    "learning",
    "models",
    "power",
    "torch",
    "hub",
    "resource",
    "torch",
    "looking",
    "throughout",
    "course",
    "provides",
    "ecosystem",
    "whole",
    "stack",
    "machine",
    "learning",
    "data",
    "getting",
    "data",
    "tenses",
    "started",
    "images",
    "represent",
    "numbers",
    "build",
    "models",
    "neural",
    "networks",
    "model",
    "data",
    "even",
    "deploy",
    "model",
    "application",
    "slash",
    "cloud",
    "well",
    "deploy",
    "pytorch",
    "model",
    "application",
    "slash",
    "cloud",
    "depending",
    "sort",
    "application",
    "slash",
    "cloud",
    "using",
    "generally",
    "run",
    "kind",
    "pytorch",
    "model",
    "originally",
    "designed",
    "used",
    "facebook",
    "slash",
    "meta",
    "pretty",
    "sure",
    "facebook",
    "renamed",
    "meta",
    "open",
    "source",
    "used",
    "companies",
    "tesla",
    "microsoft",
    "openai",
    "say",
    "popular",
    "deep",
    "learning",
    "research",
    "framework",
    "take",
    "word",
    "let",
    "look",
    "papers",
    "code",
    "dot",
    "com",
    "slash",
    "trends",
    "sure",
    "papers",
    "code",
    "website",
    "tracks",
    "latest",
    "greatest",
    "machine",
    "learning",
    "papers",
    "whether",
    "code",
    "languages",
    "deep",
    "learning",
    "frameworks",
    "pytorch",
    "tensorflow",
    "jax",
    "another",
    "one",
    "mxnet",
    "paddle",
    "paddle",
    "original",
    "torch",
    "pytorch",
    "evolution",
    "torch",
    "written",
    "python",
    "caf2",
    "mindspore",
    "look",
    "last",
    "date",
    "december",
    "oh",
    "going",
    "move",
    "every",
    "time",
    "move",
    "highlight",
    "pytorch",
    "58",
    "far",
    "large",
    "popular",
    "research",
    "machine",
    "learning",
    "framework",
    "used",
    "write",
    "code",
    "state",
    "art",
    "machine",
    "learning",
    "algorithms",
    "browse",
    "state",
    "art",
    "papers",
    "amazing",
    "website",
    "semantic",
    "segmentation",
    "image",
    "classification",
    "object",
    "detection",
    "image",
    "generation",
    "computer",
    "vision",
    "natural",
    "language",
    "processing",
    "medical",
    "let",
    "explore",
    "one",
    "favorite",
    "resources",
    "staying",
    "date",
    "field",
    "see",
    "papers",
    "code",
    "website",
    "tracked",
    "58",
    "implemented",
    "pytorch",
    "cool",
    "learning",
    "let",
    "jump",
    "pytorch",
    "well",
    "reasons",
    "spoke",
    "research",
    "favorite",
    "highlighting",
    "go",
    "go",
    "highlighted",
    "pytorch",
    "58",
    "nearly",
    "repos",
    "sure",
    "repo",
    "repo",
    "place",
    "store",
    "code",
    "online",
    "generally",
    "paper",
    "gets",
    "published",
    "machine",
    "learning",
    "fantastic",
    "research",
    "come",
    "code",
    "code",
    "access",
    "use",
    "applications",
    "research",
    "pytorch",
    "well",
    "tweet",
    "francois",
    "chale",
    "author",
    "keras",
    "another",
    "popular",
    "deep",
    "learning",
    "framework",
    "tools",
    "like",
    "colab",
    "going",
    "see",
    "colab",
    "second",
    "keras",
    "tensorflow",
    "added",
    "pytorch",
    "virtually",
    "anyone",
    "solve",
    "day",
    "initial",
    "investment",
    "problems",
    "would",
    "required",
    "engineering",
    "team",
    "working",
    "quarter",
    "hardware",
    "highlight",
    "good",
    "space",
    "deep",
    "learning",
    "machine",
    "learning",
    "tooling",
    "become",
    "colab",
    "keras",
    "tensorflow",
    "fantastic",
    "pytorch",
    "added",
    "list",
    "want",
    "check",
    "francois",
    "chale",
    "twitter",
    "prominent",
    "voice",
    "machine",
    "learning",
    "field",
    "pytorch",
    "want",
    "reasons",
    "well",
    "look",
    "look",
    "places",
    "using",
    "pytorch",
    "coming",
    "everywhere",
    "got",
    "andre",
    "kapathi",
    "director",
    "ai",
    "tesla",
    "go",
    "could",
    "search",
    "pytorch",
    "tesla",
    "got",
    "youtube",
    "talk",
    "andre",
    "kapathi",
    "director",
    "ai",
    "tesla",
    "tesla",
    "using",
    "pytorch",
    "computer",
    "vision",
    "models",
    "autopilot",
    "go",
    "videos",
    "maybe",
    "images",
    "come",
    "things",
    "like",
    "car",
    "detecting",
    "going",
    "scene",
    "course",
    "code",
    "planning",
    "let",
    "research",
    "come",
    "back",
    "openai",
    "one",
    "biggest",
    "open",
    "artificial",
    "intelligence",
    "research",
    "firms",
    "open",
    "sense",
    "publish",
    "lot",
    "research",
    "methodologies",
    "however",
    "recently",
    "debate",
    "go",
    "let",
    "say",
    "one",
    "biggest",
    "ai",
    "research",
    "entities",
    "world",
    "standardized",
    "pytorch",
    "got",
    "great",
    "blog",
    "got",
    "great",
    "research",
    "got",
    "openai",
    "api",
    "use",
    "api",
    "access",
    "models",
    "trained",
    "presumably",
    "pytorch",
    "blog",
    "post",
    "january",
    "2020",
    "says",
    "openai",
    "standardized",
    "across",
    "pytorch",
    "repo",
    "called",
    "incredible",
    "pytorch",
    "collects",
    "whole",
    "bunch",
    "different",
    "projects",
    "built",
    "top",
    "pytorch",
    "beauty",
    "pytorch",
    "build",
    "top",
    "build",
    "ai",
    "ag",
    "agriculture",
    "pytorch",
    "used",
    "let",
    "look",
    "pytorch",
    "agriculture",
    "go",
    "agricultural",
    "robots",
    "use",
    "pytorch",
    "medium",
    "article",
    "everywhere",
    "go",
    "using",
    "object",
    "detection",
    "beautiful",
    "object",
    "detection",
    "detect",
    "kind",
    "weeds",
    "sprayed",
    "fertilizer",
    "one",
    "many",
    "different",
    "things",
    "pytorch",
    "big",
    "tractor",
    "like",
    "used",
    "almost",
    "anywhere",
    "come",
    "back",
    "pytorch",
    "builds",
    "future",
    "ai",
    "machine",
    "learning",
    "facebook",
    "facebook",
    "also",
    "metaai",
    "little",
    "bit",
    "confusing",
    "even",
    "though",
    "says",
    "metaai",
    "may",
    "change",
    "time",
    "watch",
    "use",
    "pytorch",
    "machine",
    "learning",
    "applications",
    "microsoft",
    "huge",
    "pytorch",
    "game",
    "absolutely",
    "everywhere",
    "enough",
    "reason",
    "use",
    "pytorch",
    "well",
    "maybe",
    "wrong",
    "course",
    "seen",
    "enough",
    "reasons",
    "use",
    "pytorch",
    "going",
    "give",
    "one",
    "helps",
    "run",
    "code",
    "machine",
    "learning",
    "code",
    "accelerated",
    "gpu",
    "covered",
    "briefly",
    "gpu",
    "slash",
    "tpu",
    "newer",
    "chip",
    "days",
    "gpu",
    "graphics",
    "processing",
    "unit",
    "essentially",
    "fast",
    "crunching",
    "numbers",
    "originally",
    "designed",
    "video",
    "games",
    "ever",
    "designed",
    "played",
    "video",
    "game",
    "know",
    "graphics",
    "quite",
    "intense",
    "especially",
    "days",
    "render",
    "graphics",
    "need",
    "lot",
    "numerical",
    "calculations",
    "beautiful",
    "thing",
    "pytorch",
    "enables",
    "leverage",
    "gpu",
    "interface",
    "called",
    "cuda",
    "lot",
    "words",
    "going",
    "throw",
    "lot",
    "acronyms",
    "deep",
    "learning",
    "space",
    "cuda",
    "let",
    "search",
    "cuda",
    "cuda",
    "toolkit",
    "cuda",
    "parallel",
    "computing",
    "platform",
    "application",
    "programming",
    "interface",
    "api",
    "allows",
    "software",
    "use",
    "certain",
    "types",
    "graphics",
    "processing",
    "units",
    "general",
    "purpose",
    "computing",
    "want",
    "pytorch",
    "leverages",
    "cuda",
    "enable",
    "run",
    "machine",
    "learning",
    "code",
    "nvidia",
    "gpus",
    "also",
    "ability",
    "run",
    "pytorch",
    "code",
    "tpus",
    "tensor",
    "processing",
    "unit",
    "however",
    "gpus",
    "far",
    "popular",
    "running",
    "various",
    "types",
    "pytorch",
    "code",
    "going",
    "focus",
    "running",
    "pytorch",
    "code",
    "gpu",
    "give",
    "quick",
    "example",
    "pytorch",
    "tpu",
    "let",
    "see",
    "getting",
    "started",
    "pytorch",
    "cloud",
    "tpus",
    "plenty",
    "guys",
    "said",
    "gpus",
    "going",
    "far",
    "common",
    "practice",
    "going",
    "focus",
    "said",
    "said",
    "tensor",
    "processing",
    "unit",
    "reason",
    "called",
    "tensor",
    "processing",
    "units",
    "machine",
    "learning",
    "deep",
    "learning",
    "deals",
    "lot",
    "tensors",
    "next",
    "video",
    "let",
    "answer",
    "question",
    "tensor",
    "go",
    "answer",
    "perspective",
    "like",
    "research",
    "question",
    "open",
    "google",
    "favorite",
    "search",
    "engine",
    "type",
    "tensor",
    "see",
    "find",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "left",
    "cliffhanger",
    "question",
    "tensor",
    "also",
    "issued",
    "challenge",
    "research",
    "tensor",
    "said",
    "course",
    "telling",
    "exactly",
    "things",
    "sparking",
    "curiosity",
    "stumble",
    "upon",
    "answers",
    "things",
    "let",
    "look",
    "tensor",
    "remember",
    "graphic",
    "lot",
    "going",
    "neural",
    "network",
    "kind",
    "input",
    "kind",
    "numerical",
    "encoding",
    "start",
    "data",
    "case",
    "unstructured",
    "data",
    "images",
    "text",
    "audio",
    "file",
    "necessarily",
    "go",
    "time",
    "image",
    "could",
    "focus",
    "neural",
    "network",
    "specifically",
    "images",
    "text",
    "could",
    "focus",
    "neural",
    "network",
    "specifically",
    "text",
    "sound",
    "bite",
    "speech",
    "could",
    "focus",
    "neural",
    "network",
    "specifically",
    "speech",
    "however",
    "field",
    "sort",
    "also",
    "moving",
    "towards",
    "building",
    "neural",
    "networks",
    "capable",
    "handling",
    "three",
    "types",
    "inputs",
    "going",
    "start",
    "small",
    "build",
    "algorithms",
    "going",
    "focus",
    "neural",
    "networks",
    "focus",
    "one",
    "type",
    "data",
    "premise",
    "still",
    "kind",
    "input",
    "numerically",
    "encode",
    "form",
    "pass",
    "neural",
    "network",
    "learn",
    "representations",
    "patterns",
    "within",
    "numerical",
    "encoding",
    "output",
    "form",
    "representation",
    "convert",
    "representation",
    "things",
    "humans",
    "understand",
    "might",
    "already",
    "seen",
    "might",
    "already",
    "referenced",
    "fact",
    "tensors",
    "question",
    "comes",
    "tensors",
    "tensor",
    "could",
    "almost",
    "anything",
    "could",
    "almost",
    "representation",
    "numbers",
    "going",
    "get",
    "hands",
    "tensors",
    "actually",
    "fundamental",
    "building",
    "block",
    "pytorch",
    "aside",
    "neural",
    "network",
    "components",
    "torch",
    "dot",
    "tensor",
    "going",
    "see",
    "shortly",
    "important",
    "takeaway",
    "sort",
    "input",
    "data",
    "going",
    "numerically",
    "encode",
    "data",
    "turn",
    "tensor",
    "kind",
    "whatever",
    "kind",
    "depend",
    "problem",
    "working",
    "going",
    "pass",
    "neural",
    "network",
    "perform",
    "mathematical",
    "operations",
    "tensor",
    "lot",
    "mathematical",
    "operations",
    "taken",
    "care",
    "pytorch",
    "behind",
    "scenes",
    "writing",
    "code",
    "execute",
    "kind",
    "mathematical",
    "operations",
    "tensors",
    "neural",
    "network",
    "create",
    "one",
    "already",
    "created",
    "use",
    "problem",
    "output",
    "another",
    "tensor",
    "similar",
    "input",
    "manipulated",
    "certain",
    "way",
    "sort",
    "programmed",
    "take",
    "output",
    "tensor",
    "change",
    "something",
    "human",
    "understand",
    "remove",
    "lot",
    "text",
    "around",
    "make",
    "little",
    "bit",
    "clearer",
    "focusing",
    "building",
    "image",
    "classification",
    "model",
    "want",
    "classify",
    "whether",
    "photo",
    "raman",
    "spaghetti",
    "would",
    "images",
    "input",
    "would",
    "turn",
    "images",
    "numbers",
    "represented",
    "tensor",
    "would",
    "pass",
    "tensor",
    "numbers",
    "neural",
    "network",
    "might",
    "lots",
    "tensors",
    "might",
    "images",
    "might",
    "million",
    "images",
    "cases",
    "google",
    "facebook",
    "might",
    "working",
    "300",
    "million",
    "billion",
    "images",
    "time",
    "principle",
    "still",
    "stands",
    "encode",
    "data",
    "form",
    "numerical",
    "representation",
    "tensor",
    "pass",
    "tensor",
    "lots",
    "tensors",
    "neural",
    "network",
    "neural",
    "network",
    "performs",
    "mathematical",
    "operations",
    "tensors",
    "outputs",
    "tensor",
    "convert",
    "tensor",
    "something",
    "understand",
    "humans",
    "said",
    "covered",
    "lot",
    "fundamentals",
    "machine",
    "learning",
    "deep",
    "learning",
    "neural",
    "network",
    "well",
    "touched",
    "surface",
    "things",
    "get",
    "deep",
    "like",
    "covered",
    "use",
    "pytorch",
    "pytorch",
    "fundamental",
    "building",
    "block",
    "deep",
    "learning",
    "tensors",
    "covered",
    "let",
    "get",
    "bit",
    "specific",
    "next",
    "video",
    "going",
    "cover",
    "first",
    "module",
    "excited",
    "going",
    "start",
    "codes",
    "see",
    "next",
    "video",
    "time",
    "get",
    "specific",
    "going",
    "cover",
    "fundamentals",
    "module",
    "want",
    "reiterate",
    "fact",
    "going",
    "back",
    "last",
    "video",
    "challenge",
    "look",
    "tensor",
    "exactly",
    "would",
    "would",
    "come",
    "google",
    "would",
    "type",
    "question",
    "tensor",
    "go",
    "tensor",
    "pytorch",
    "knows",
    "google",
    "knows",
    "using",
    "deep",
    "learning",
    "data",
    "want",
    "know",
    "tensor",
    "pytorch",
    "tensor",
    "general",
    "thing",
    "associated",
    "pytorch",
    "got",
    "tensor",
    "wikipedia",
    "got",
    "tensor",
    "probably",
    "favorite",
    "video",
    "tensor",
    "dan",
    "flesch",
    "flesch",
    "probably",
    "saying",
    "wrong",
    "good",
    "first",
    "name",
    "going",
    "extra",
    "curriculum",
    "video",
    "previous",
    "video",
    "watch",
    "tensor",
    "might",
    "saying",
    "well",
    "gives",
    "come",
    "course",
    "learn",
    "pytorch",
    "guy",
    "daniel",
    "googling",
    "things",
    "question",
    "comes",
    "tell",
    "well",
    "tell",
    "everything",
    "deep",
    "learning",
    "machine",
    "learning",
    "pytorch",
    "course",
    "would",
    "far",
    "long",
    "purpose",
    "searching",
    "questions",
    "like",
    "purpose",
    "exactly",
    "day",
    "day",
    "machine",
    "learning",
    "engineer",
    "write",
    "code",
    "like",
    "know",
    "something",
    "literally",
    "go",
    "whatever",
    "search",
    "engine",
    "using",
    "google",
    "time",
    "type",
    "whatever",
    "error",
    "getting",
    "pytorch",
    "tensor",
    "something",
    "like",
    "want",
    "tell",
    "okay",
    "search",
    "questions",
    "like",
    "encouraged",
    "keep",
    "mind",
    "go",
    "whole",
    "course",
    "going",
    "see",
    "lot",
    "let",
    "get",
    "going",
    "cover",
    "go",
    "tweet",
    "elon",
    "musk",
    "decided",
    "know",
    "let",
    "base",
    "whole",
    "course",
    "tweet",
    "learning",
    "mldl",
    "university",
    "little",
    "bit",
    "small",
    "brain",
    "online",
    "courses",
    "well",
    "like",
    "one",
    "brain",
    "starting",
    "explode",
    "get",
    "little",
    "fireworks",
    "youtube",
    "oh",
    "watching",
    "youtube",
    "look",
    "shiny",
    "brain",
    "articles",
    "goodness",
    "lucky",
    "course",
    "comes",
    "article",
    "format",
    "go",
    "learn",
    "course",
    "materials",
    "online",
    "book",
    "format",
    "going",
    "get",
    "fundamental",
    "section",
    "shortly",
    "want",
    "reference",
    "course",
    "materials",
    "built",
    "book",
    "time",
    "watch",
    "going",
    "chapters",
    "covering",
    "bases",
    "finally",
    "memes",
    "would",
    "ascend",
    "godlike",
    "creature",
    "think",
    "hovering",
    "underwater",
    "best",
    "way",
    "learn",
    "machine",
    "learning",
    "going",
    "start",
    "mldl",
    "university",
    "online",
    "courses",
    "youtube",
    "articles",
    "memes",
    "kind",
    "going",
    "cover",
    "broadly",
    "module",
    "going",
    "cover",
    "pytorch",
    "basics",
    "fundamentals",
    "mainly",
    "dealing",
    "tensors",
    "tensor",
    "operations",
    "remember",
    "neural",
    "network",
    "input",
    "tensors",
    "performing",
    "operations",
    "tensors",
    "creating",
    "output",
    "operations",
    "later",
    "going",
    "focused",
    "data",
    "getting",
    "tensors",
    "turning",
    "data",
    "raw",
    "form",
    "images",
    "whatever",
    "numerical",
    "encoding",
    "tensor",
    "going",
    "look",
    "building",
    "using",
    "deep",
    "learning",
    "models",
    "specifically",
    "neural",
    "networks",
    "going",
    "fit",
    "model",
    "data",
    "going",
    "show",
    "model",
    "write",
    "code",
    "model",
    "learn",
    "patterns",
    "data",
    "going",
    "see",
    "make",
    "predictions",
    "model",
    "deep",
    "learning",
    "machine",
    "learning",
    "right",
    "using",
    "patterns",
    "past",
    "predict",
    "future",
    "going",
    "evaluate",
    "model",
    "predictions",
    "going",
    "learn",
    "save",
    "load",
    "models",
    "example",
    "wanted",
    "export",
    "model",
    "working",
    "application",
    "something",
    "like",
    "finally",
    "going",
    "see",
    "use",
    "trained",
    "model",
    "make",
    "predictions",
    "data",
    "custom",
    "data",
    "fun",
    "well",
    "see",
    "scientist",
    "faded",
    "little",
    "bit",
    "really",
    "true",
    "going",
    "like",
    "cooks",
    "chemists",
    "chemists",
    "quite",
    "precise",
    "everything",
    "exactly",
    "cooks",
    "like",
    "oh",
    "know",
    "little",
    "bit",
    "salt",
    "little",
    "bit",
    "butter",
    "taste",
    "good",
    "okay",
    "well",
    "machine",
    "learning",
    "little",
    "bit",
    "little",
    "bit",
    "science",
    "little",
    "bit",
    "art",
    "going",
    "like",
    "idea",
    "machine",
    "learning",
    "cooking",
    "show",
    "welcome",
    "cooking",
    "machine",
    "learning",
    "cooking",
    "pytorch",
    "daniel",
    "finally",
    "got",
    "workflow",
    "pytorch",
    "workflow",
    "one",
    "many",
    "going",
    "kind",
    "use",
    "throughout",
    "entire",
    "course",
    "step",
    "one",
    "going",
    "get",
    "data",
    "ready",
    "step",
    "two",
    "going",
    "build",
    "pick",
    "pre",
    "trained",
    "model",
    "suit",
    "whatever",
    "problem",
    "working",
    "step",
    "two",
    "point",
    "one",
    "pick",
    "loss",
    "function",
    "optimizer",
    "worry",
    "going",
    "cover",
    "soon",
    "step",
    "two",
    "point",
    "two",
    "build",
    "training",
    "loop",
    "kind",
    "part",
    "parcel",
    "step",
    "two",
    "hence",
    "got",
    "two",
    "point",
    "one",
    "two",
    "point",
    "two",
    "see",
    "means",
    "later",
    "number",
    "three",
    "going",
    "fit",
    "model",
    "data",
    "make",
    "prediction",
    "say",
    "working",
    "image",
    "classification",
    "raman",
    "spaghetti",
    "build",
    "neural",
    "network",
    "put",
    "images",
    "neural",
    "network",
    "get",
    "sort",
    "idea",
    "image",
    "see",
    "well",
    "value",
    "weight",
    "model",
    "see",
    "predicting",
    "bs",
    "actually",
    "going",
    "right",
    "number",
    "five",
    "going",
    "improve",
    "experimentation",
    "another",
    "big",
    "thing",
    "notice",
    "throughout",
    "machine",
    "learning",
    "throughout",
    "course",
    "experimental",
    "part",
    "art",
    "part",
    "science",
    "number",
    "six",
    "save",
    "reload",
    "trained",
    "model",
    "put",
    "numerical",
    "order",
    "kind",
    "mixed",
    "matched",
    "depending",
    "journey",
    "numerical",
    "order",
    "easy",
    "understand",
    "got",
    "one",
    "video",
    "maybe",
    "another",
    "one",
    "get",
    "code",
    "next",
    "video",
    "going",
    "cover",
    "important",
    "points",
    "approach",
    "course",
    "see",
    "might",
    "asking",
    "approach",
    "course",
    "might",
    "asking",
    "going",
    "answer",
    "anyway",
    "approach",
    "course",
    "would",
    "recommend",
    "approaching",
    "course",
    "machine",
    "learning",
    "engineer",
    "day",
    "day",
    "learning",
    "machine",
    "learning",
    "coding",
    "machine",
    "learning",
    "kind",
    "two",
    "different",
    "things",
    "remember",
    "first",
    "learned",
    "kind",
    "learned",
    "lot",
    "theory",
    "rather",
    "writing",
    "code",
    "take",
    "away",
    "theory",
    "important",
    "course",
    "going",
    "focusing",
    "writing",
    "machine",
    "learning",
    "specifically",
    "pytorch",
    "code",
    "number",
    "one",
    "step",
    "approaching",
    "course",
    "code",
    "along",
    "course",
    "focused",
    "purely",
    "writing",
    "code",
    "linking",
    "extracurricular",
    "resources",
    "learn",
    "going",
    "behind",
    "scenes",
    "code",
    "idea",
    "teaching",
    "code",
    "together",
    "write",
    "code",
    "see",
    "working",
    "going",
    "spark",
    "curiosity",
    "figure",
    "going",
    "behind",
    "scenes",
    "motto",
    "number",
    "one",
    "run",
    "code",
    "write",
    "run",
    "code",
    "see",
    "happens",
    "number",
    "two",
    "love",
    "explore",
    "experiment",
    "approach",
    "idea",
    "mind",
    "scientist",
    "chef",
    "science",
    "art",
    "experiment",
    "experiment",
    "experiment",
    "try",
    "things",
    "rigor",
    "like",
    "scientist",
    "would",
    "try",
    "things",
    "fun",
    "like",
    "chef",
    "would",
    "number",
    "three",
    "visualize",
    "understand",
    "ca",
    "emphasize",
    "one",
    "enough",
    "three",
    "models",
    "far",
    "run",
    "code",
    "going",
    "hear",
    "say",
    "lot",
    "experiment",
    "experiment",
    "experiment",
    "number",
    "three",
    "visualize",
    "visualize",
    "visualize",
    "well",
    "spoken",
    "machine",
    "learning",
    "deep",
    "learning",
    "deals",
    "lot",
    "data",
    "lot",
    "numbers",
    "find",
    "visualize",
    "numbers",
    "whatever",
    "form",
    "numbers",
    "page",
    "tend",
    "understand",
    "better",
    "great",
    "extracurricular",
    "resources",
    "going",
    "link",
    "also",
    "turn",
    "writing",
    "code",
    "fantastic",
    "visualizations",
    "number",
    "four",
    "ask",
    "questions",
    "including",
    "dumb",
    "questions",
    "really",
    "thing",
    "dumb",
    "question",
    "everyone",
    "different",
    "part",
    "learning",
    "journey",
    "fact",
    "quote",
    "unquote",
    "dumb",
    "question",
    "turns",
    "lot",
    "people",
    "probably",
    "one",
    "well",
    "sure",
    "ask",
    "questions",
    "going",
    "link",
    "resource",
    "minute",
    "ask",
    "questions",
    "please",
    "please",
    "please",
    "ask",
    "questions",
    "community",
    "google",
    "internet",
    "wherever",
    "ask",
    "questions",
    "code",
    "write",
    "code",
    "figure",
    "answer",
    "questions",
    "number",
    "five",
    "exercises",
    "great",
    "exercises",
    "created",
    "modules",
    "go",
    "got",
    "book",
    "version",
    "course",
    "within",
    "chapters",
    "bottom",
    "going",
    "exercises",
    "extra",
    "curriculum",
    "got",
    "exercises",
    "going",
    "jump",
    "would",
    "highly",
    "recommend",
    "follow",
    "along",
    "course",
    "code",
    "code",
    "please",
    "please",
    "please",
    "give",
    "exercises",
    "go",
    "going",
    "stretch",
    "knowledge",
    "going",
    "lot",
    "practice",
    "writing",
    "code",
    "together",
    "stuff",
    "exercises",
    "going",
    "give",
    "chance",
    "practice",
    "learned",
    "course",
    "extra",
    "curriculum",
    "well",
    "hey",
    "want",
    "learn",
    "plenty",
    "opportunities",
    "finally",
    "number",
    "six",
    "share",
    "work",
    "ca",
    "emphasize",
    "enough",
    "much",
    "writing",
    "learning",
    "deep",
    "learning",
    "sharing",
    "work",
    "github",
    "different",
    "code",
    "resources",
    "community",
    "helped",
    "learning",
    "learn",
    "something",
    "cool",
    "pytorch",
    "love",
    "see",
    "link",
    "somehow",
    "discord",
    "chat",
    "github",
    "whatever",
    "links",
    "find",
    "love",
    "see",
    "please",
    "share",
    "work",
    "great",
    "way",
    "learn",
    "something",
    "share",
    "write",
    "like",
    "would",
    "someone",
    "else",
    "understand",
    "also",
    "great",
    "way",
    "help",
    "others",
    "learn",
    "said",
    "approach",
    "course",
    "let",
    "go",
    "approach",
    "course",
    "would",
    "love",
    "avoid",
    "overthinking",
    "process",
    "brain",
    "brain",
    "fire",
    "avoid",
    "brain",
    "fire",
    "good",
    "place",
    "working",
    "pytorch",
    "going",
    "quite",
    "hot",
    "playing",
    "words",
    "name",
    "torch",
    "avoid",
    "brain",
    "catching",
    "fire",
    "avoid",
    "saying",
    "ca",
    "learn",
    "said",
    "lots",
    "times",
    "practiced",
    "turns",
    "actually",
    "learn",
    "things",
    "let",
    "draw",
    "red",
    "line",
    "oh",
    "think",
    "red",
    "line",
    "yeah",
    "go",
    "nice",
    "thick",
    "red",
    "line",
    "get",
    "really",
    "make",
    "sense",
    "says",
    "avoid",
    "crossed",
    "say",
    "ca",
    "learn",
    "prevent",
    "brain",
    "catching",
    "fire",
    "finally",
    "got",
    "one",
    "video",
    "going",
    "cover",
    "one",
    "gets",
    "long",
    "resources",
    "course",
    "get",
    "coding",
    "see",
    "fundamental",
    "resources",
    "would",
    "like",
    "aware",
    "go",
    "course",
    "going",
    "paramount",
    "working",
    "course",
    "three",
    "things",
    "github",
    "repo",
    "click",
    "link",
    "got",
    "pinned",
    "browser",
    "might",
    "want",
    "going",
    "course",
    "burks",
    "github",
    "slash",
    "pytorch",
    "deep",
    "learning",
    "still",
    "work",
    "progress",
    "time",
    "recording",
    "video",
    "time",
    "go",
    "wo",
    "look",
    "much",
    "different",
    "materials",
    "materials",
    "outline",
    "section",
    "cover",
    "see",
    "coming",
    "soon",
    "time",
    "recording",
    "probably",
    "done",
    "time",
    "watch",
    "exercise",
    "extra",
    "curriculum",
    "links",
    "basically",
    "everything",
    "need",
    "course",
    "github",
    "repo",
    "come",
    "back",
    "also",
    "github",
    "repo",
    "repo",
    "burks",
    "slash",
    "pytorch",
    "deep",
    "learning",
    "click",
    "discussions",
    "going",
    "q",
    "link",
    "q",
    "course",
    "question",
    "click",
    "new",
    "discussion",
    "go",
    "q",
    "type",
    "video",
    "title",
    "pytorch",
    "fundamentals",
    "go",
    "could",
    "type",
    "error",
    "well",
    "tensor",
    "type",
    "stuff",
    "hello",
    "trouble",
    "video",
    "x",
    "put",
    "name",
    "video",
    "way",
    "someone",
    "else",
    "help",
    "code",
    "go",
    "three",
    "back",
    "ticks",
    "write",
    "python",
    "go",
    "import",
    "torch",
    "torch",
    "dot",
    "rand",
    "n",
    "going",
    "create",
    "tensor",
    "going",
    "see",
    "second",
    "yeah",
    "yeah",
    "yeah",
    "post",
    "question",
    "formatting",
    "code",
    "helpful",
    "understand",
    "going",
    "going",
    "basically",
    "outline",
    "would",
    "ask",
    "question",
    "video",
    "going",
    "whatever",
    "going",
    "hello",
    "trouble",
    "code",
    "happening",
    "could",
    "even",
    "include",
    "error",
    "message",
    "click",
    "start",
    "discussion",
    "someone",
    "either",
    "someone",
    "else",
    "course",
    "able",
    "help",
    "beautiful",
    "thing",
    "one",
    "place",
    "start",
    "search",
    "nothing",
    "yet",
    "course",
    "yet",
    "go",
    "probably",
    "stuff",
    "issues",
    "code",
    "think",
    "needs",
    "fixed",
    "also",
    "open",
    "new",
    "issue",
    "let",
    "read",
    "going",
    "got",
    "issues",
    "already",
    "fact",
    "need",
    "record",
    "videos",
    "course",
    "need",
    "create",
    "stuff",
    "think",
    "something",
    "could",
    "improved",
    "make",
    "issue",
    "question",
    "course",
    "ask",
    "discussion",
    "come",
    "back",
    "keynote",
    "one",
    "resource",
    "course",
    "materials",
    "live",
    "github",
    "course",
    "q",
    "course",
    "github",
    "discussions",
    "tab",
    "course",
    "online",
    "book",
    "work",
    "art",
    "quite",
    "beautiful",
    "code",
    "automatically",
    "turn",
    "materials",
    "github",
    "come",
    "code",
    "click",
    "notebook",
    "zero",
    "zero",
    "going",
    "sometimes",
    "ever",
    "worked",
    "jupiter",
    "notebooks",
    "github",
    "take",
    "load",
    "materials",
    "automatically",
    "get",
    "converted",
    "book",
    "beautiful",
    "thing",
    "book",
    "got",
    "different",
    "headings",
    "readable",
    "online",
    "going",
    "images",
    "also",
    "search",
    "stuff",
    "pytorch",
    "training",
    "steps",
    "creating",
    "training",
    "loop",
    "pytorch",
    "beautiful",
    "going",
    "see",
    "later",
    "three",
    "big",
    "materials",
    "need",
    "aware",
    "three",
    "big",
    "resources",
    "specific",
    "course",
    "materials",
    "github",
    "course",
    "q",
    "course",
    "online",
    "book",
    "learn",
    "simple",
    "url",
    "remember",
    "materials",
    "specifically",
    "pytorch",
    "things",
    "pytorch",
    "pytorch",
    "website",
    "pytorch",
    "forums",
    "question",
    "course",
    "related",
    "pytorch",
    "related",
    "highly",
    "recommend",
    "go",
    "pytorch",
    "forums",
    "available",
    "got",
    "link",
    "pytorch",
    "website",
    "going",
    "home",
    "ground",
    "everything",
    "pytorch",
    "course",
    "documentation",
    "said",
    "course",
    "replacement",
    "getting",
    "familiar",
    "pytorch",
    "documentation",
    "course",
    "actually",
    "built",
    "pytorch",
    "documentation",
    "organized",
    "slightly",
    "different",
    "way",
    "plenty",
    "amazing",
    "resources",
    "everything",
    "pytorch",
    "home",
    "ground",
    "going",
    "see",
    "referring",
    "lot",
    "throughout",
    "course",
    "keep",
    "mind",
    "course",
    "materials",
    "github",
    "course",
    "discussions",
    "course",
    "things",
    "pytorch",
    "specific",
    "necessarily",
    "course",
    "pytorch",
    "general",
    "pytorch",
    "website",
    "pytorch",
    "forums",
    "said",
    "come",
    "far",
    "covered",
    "lot",
    "already",
    "guess",
    "time",
    "let",
    "write",
    "code",
    "see",
    "next",
    "video",
    "covered",
    "enough",
    "fundamentals",
    "far",
    "well",
    "theory",
    "point",
    "view",
    "let",
    "get",
    "coding",
    "going",
    "go",
    "google",
    "chrome",
    "going",
    "introduce",
    "tool",
    "one",
    "main",
    "tools",
    "going",
    "using",
    "entire",
    "course",
    "google",
    "colab",
    "way",
    "would",
    "suggest",
    "following",
    "along",
    "course",
    "remember",
    "one",
    "major",
    "ones",
    "code",
    "along",
    "going",
    "go",
    "got",
    "typo",
    "classic",
    "going",
    "see",
    "lots",
    "typos",
    "throughout",
    "course",
    "going",
    "load",
    "google",
    "colab",
    "follow",
    "along",
    "going",
    "like",
    "find",
    "use",
    "google",
    "colab",
    "perspective",
    "go",
    "probably",
    "recommend",
    "going",
    "overview",
    "collaboratory",
    "features",
    "essentially",
    "google",
    "colab",
    "going",
    "enable",
    "us",
    "create",
    "new",
    "notebook",
    "going",
    "practice",
    "writing",
    "pytorch",
    "code",
    "refer",
    "reference",
    "document",
    "actually",
    "colab",
    "notebooks",
    "book",
    "format",
    "online",
    "book",
    "format",
    "basis",
    "materials",
    "course",
    "going",
    "going",
    "every",
    "new",
    "module",
    "going",
    "start",
    "new",
    "notebook",
    "going",
    "zoom",
    "one",
    "first",
    "module",
    "going",
    "zero",
    "zero",
    "python",
    "code",
    "starts",
    "zero",
    "zero",
    "going",
    "call",
    "pytorch",
    "fundamentals",
    "going",
    "call",
    "mine",
    "video",
    "know",
    "notebook",
    "wrote",
    "video",
    "going",
    "click",
    "connect",
    "going",
    "give",
    "us",
    "space",
    "write",
    "python",
    "code",
    "go",
    "print",
    "hello",
    "excited",
    "learn",
    "pytorch",
    "hit",
    "shift",
    "enter",
    "comes",
    "like",
    "another",
    "beautiful",
    "benefit",
    "google",
    "colab",
    "ps",
    "using",
    "pro",
    "version",
    "costs",
    "10",
    "month",
    "price",
    "may",
    "different",
    "depending",
    "reason",
    "use",
    "colab",
    "time",
    "however",
    "use",
    "paid",
    "version",
    "course",
    "google",
    "colab",
    "comes",
    "free",
    "version",
    "able",
    "use",
    "complete",
    "course",
    "see",
    "worthwhile",
    "find",
    "pro",
    "version",
    "worthwhile",
    "another",
    "benefit",
    "google",
    "colab",
    "go",
    "go",
    "runtime",
    "let",
    "show",
    "runtime",
    "change",
    "runtime",
    "type",
    "hardware",
    "accelerator",
    "choose",
    "run",
    "code",
    "accelerator",
    "got",
    "gpu",
    "tpu",
    "going",
    "focused",
    "using",
    "gpu",
    "like",
    "look",
    "tpu",
    "leave",
    "click",
    "gpu",
    "click",
    "save",
    "code",
    "write",
    "way",
    "run",
    "gpu",
    "going",
    "see",
    "later",
    "code",
    "runs",
    "gpu",
    "lot",
    "faster",
    "terms",
    "compute",
    "time",
    "especially",
    "deep",
    "learning",
    "write",
    "video",
    "smi",
    "access",
    "gpu",
    "case",
    "tesla",
    "p100",
    "quite",
    "good",
    "gpu",
    "tend",
    "get",
    "better",
    "gpus",
    "pay",
    "google",
    "colab",
    "pay",
    "get",
    "free",
    "version",
    "get",
    "free",
    "gpu",
    "wo",
    "fast",
    "gpus",
    "typically",
    "get",
    "paid",
    "version",
    "keep",
    "mind",
    "whole",
    "bunch",
    "stuff",
    "going",
    "go",
    "much",
    "covered",
    "basically",
    "need",
    "cover",
    "come",
    "going",
    "write",
    "text",
    "cell",
    "oo",
    "dot",
    "pytorch",
    "fundamentals",
    "going",
    "link",
    "resource",
    "notebook",
    "come",
    "learn",
    "notebooks",
    "going",
    "sync",
    "00",
    "put",
    "resource",
    "notebook",
    "notebook",
    "going",
    "based",
    "one",
    "question",
    "going",
    "notebook",
    "come",
    "course",
    "github",
    "go",
    "back",
    "back",
    "see",
    "going",
    "pytorch",
    "deep",
    "learning",
    "projects",
    "see",
    "happening",
    "moment",
    "got",
    "pytorch",
    "course",
    "creation",
    "middle",
    "creating",
    "question",
    "come",
    "burke",
    "slash",
    "pytorch",
    "deep",
    "learning",
    "slash",
    "discussions",
    "tab",
    "ask",
    "question",
    "clicking",
    "new",
    "discussion",
    "discussions",
    "related",
    "notebook",
    "ask",
    "going",
    "turn",
    "right",
    "code",
    "cell",
    "colab",
    "basically",
    "comprised",
    "code",
    "text",
    "cells",
    "going",
    "turn",
    "text",
    "cell",
    "pressing",
    "command",
    "mm",
    "shift",
    "enter",
    "text",
    "cell",
    "wanted",
    "another",
    "code",
    "cell",
    "could",
    "go",
    "like",
    "text",
    "code",
    "text",
    "code",
    "yada",
    "yada",
    "yada",
    "going",
    "delete",
    "finish",
    "video",
    "going",
    "import",
    "pytorch",
    "going",
    "import",
    "torch",
    "going",
    "print",
    "torch",
    "dot",
    "dot",
    "version",
    "another",
    "beautiful",
    "thing",
    "google",
    "colab",
    "comes",
    "pytorch",
    "pre",
    "installed",
    "lot",
    "common",
    "python",
    "data",
    "science",
    "packages",
    "could",
    "also",
    "go",
    "import",
    "pandas",
    "pd",
    "import",
    "numpy",
    "mp",
    "import",
    "mapplot",
    "lib",
    "lib",
    "dot",
    "pyplot",
    "plt",
    "google",
    "colab",
    "far",
    "easiest",
    "way",
    "get",
    "started",
    "course",
    "run",
    "things",
    "locally",
    "like",
    "refer",
    "pytorch",
    "deep",
    "learning",
    "going",
    "set",
    "dot",
    "md",
    "getting",
    "set",
    "code",
    "pytorch",
    "gone",
    "number",
    "one",
    "setting",
    "google",
    "colab",
    "also",
    "another",
    "option",
    "getting",
    "started",
    "locally",
    "right",
    "document",
    "work",
    "progress",
    "finished",
    "time",
    "watch",
    "video",
    "replacement",
    "though",
    "pytorch",
    "documentation",
    "getting",
    "set",
    "locally",
    "like",
    "run",
    "locally",
    "machine",
    "rather",
    "going",
    "google",
    "colab",
    "please",
    "refer",
    "documentation",
    "set",
    "dot",
    "md",
    "like",
    "get",
    "started",
    "soon",
    "possible",
    "highly",
    "recommend",
    "using",
    "google",
    "colab",
    "fact",
    "entire",
    "course",
    "going",
    "able",
    "run",
    "google",
    "colab",
    "let",
    "finish",
    "video",
    "make",
    "sure",
    "got",
    "pytorch",
    "ready",
    "go",
    "course",
    "fundamental",
    "data",
    "science",
    "packages",
    "wonderful",
    "means",
    "pytorch",
    "version",
    "number",
    "far",
    "greater",
    "maybe",
    "watching",
    "video",
    "couple",
    "years",
    "future",
    "pytorch",
    "maybe",
    "code",
    "notebook",
    "wo",
    "work",
    "enough",
    "going",
    "plus",
    "q111",
    "cu111",
    "stands",
    "cuda",
    "version",
    "believe",
    "would",
    "mean",
    "came",
    "wanted",
    "install",
    "linux",
    "colab",
    "runs",
    "mac",
    "windows",
    "well",
    "got",
    "cuda",
    "yeah",
    "right",
    "recording",
    "video",
    "latest",
    "pytorch",
    "build",
    "need",
    "least",
    "pytorch",
    "complete",
    "course",
    "cuda",
    "cuda",
    "toolkit",
    "remember",
    "cuda",
    "toolkit",
    "nvidia",
    "programming",
    "go",
    "nvidia",
    "developer",
    "cuda",
    "enables",
    "us",
    "run",
    "pytorch",
    "code",
    "nvidia",
    "gpus",
    "access",
    "google",
    "colab",
    "beautiful",
    "set",
    "ready",
    "write",
    "code",
    "let",
    "get",
    "started",
    "next",
    "video",
    "writing",
    "pytorch",
    "code",
    "exciting",
    "see",
    "got",
    "set",
    "got",
    "access",
    "pytorch",
    "got",
    "google",
    "colab",
    "instance",
    "running",
    "got",
    "gpu",
    "gone",
    "runtime",
    "change",
    "runtime",
    "type",
    "hardware",
    "accelerator",
    "wo",
    "necessarily",
    "need",
    "gpu",
    "entire",
    "notebook",
    "wanted",
    "show",
    "get",
    "access",
    "gpu",
    "going",
    "using",
    "later",
    "let",
    "get",
    "rid",
    "one",
    "last",
    "thing",
    "recommend",
    "going",
    "course",
    "split",
    "window",
    "fashion",
    "example",
    "might",
    "video",
    "talking",
    "right",
    "writing",
    "code",
    "left",
    "side",
    "might",
    "another",
    "window",
    "side",
    "colab",
    "window",
    "go",
    "new",
    "notebook",
    "call",
    "whatever",
    "want",
    "notebook",
    "could",
    "call",
    "similar",
    "writing",
    "write",
    "code",
    "side",
    "video",
    "ca",
    "copy",
    "course",
    "write",
    "code",
    "go",
    "go",
    "go",
    "get",
    "stuck",
    "course",
    "reference",
    "notebook",
    "opportunity",
    "ask",
    "question",
    "said",
    "let",
    "get",
    "started",
    "first",
    "thing",
    "going",
    "look",
    "pytorch",
    "introduction",
    "tenses",
    "tenses",
    "main",
    "building",
    "block",
    "deep",
    "learning",
    "general",
    "data",
    "may",
    "watched",
    "video",
    "tensor",
    "sake",
    "course",
    "tenses",
    "way",
    "represent",
    "data",
    "especially",
    "multi",
    "dimensional",
    "data",
    "numeric",
    "data",
    "numeric",
    "data",
    "represents",
    "something",
    "else",
    "let",
    "go",
    "creating",
    "tenses",
    "first",
    "kind",
    "tensor",
    "going",
    "create",
    "actually",
    "called",
    "scalar",
    "know",
    "going",
    "throw",
    "lot",
    "different",
    "names",
    "things",
    "important",
    "aware",
    "nomenclature",
    "even",
    "though",
    "pytorch",
    "almost",
    "everything",
    "referred",
    "tensor",
    "different",
    "kinds",
    "tenses",
    "exemplify",
    "fact",
    "using",
    "reference",
    "notebook",
    "go",
    "see",
    "importing",
    "pytorch",
    "done",
    "introduction",
    "tenses",
    "got",
    "creating",
    "tenses",
    "got",
    "scalar",
    "etc",
    "etc",
    "etc",
    "going",
    "working",
    "let",
    "together",
    "scalar",
    "way",
    "oops",
    "done",
    "way",
    "create",
    "tensor",
    "pytorch",
    "going",
    "call",
    "scalar",
    "equals",
    "torch",
    "dot",
    "tensor",
    "going",
    "fill",
    "number",
    "seven",
    "press",
    "retype",
    "scalar",
    "get",
    "back",
    "seven",
    "wonderful",
    "got",
    "tensor",
    "data",
    "type",
    "would",
    "find",
    "torch",
    "dot",
    "tensor",
    "actually",
    "well",
    "let",
    "show",
    "would",
    "go",
    "torch",
    "dot",
    "tensor",
    "go",
    "got",
    "documentation",
    "possibly",
    "common",
    "class",
    "pytorch",
    "one",
    "going",
    "see",
    "later",
    "use",
    "torch",
    "dot",
    "nn",
    "basically",
    "everything",
    "pytorch",
    "works",
    "torch",
    "dot",
    "tensor",
    "like",
    "learn",
    "read",
    "fact",
    "would",
    "encourage",
    "read",
    "documentation",
    "least",
    "10",
    "minutes",
    "finish",
    "videos",
    "said",
    "going",
    "link",
    "pytorch",
    "tensors",
    "created",
    "using",
    "torch",
    "dot",
    "tensor",
    "got",
    "link",
    "oops",
    "typos",
    "got",
    "law",
    "daniel",
    "come",
    "better",
    "kidding",
    "going",
    "typos",
    "got",
    "law",
    "whole",
    "course",
    "okay",
    "attributes",
    "scalar",
    "details",
    "scalars",
    "let",
    "find",
    "many",
    "dimensions",
    "oh",
    "way",
    "warning",
    "perfect",
    "timing",
    "google",
    "colab",
    "give",
    "warnings",
    "depending",
    "whether",
    "using",
    "gpu",
    "reason",
    "google",
    "colab",
    "provides",
    "gpus",
    "free",
    "however",
    "gpus",
    "free",
    "google",
    "provide",
    "using",
    "gpu",
    "save",
    "resources",
    "allow",
    "someone",
    "else",
    "use",
    "gpu",
    "going",
    "none",
    "course",
    "always",
    "switch",
    "back",
    "going",
    "turn",
    "gpu",
    "someone",
    "else",
    "using",
    "gpu",
    "moment",
    "use",
    "also",
    "going",
    "see",
    "google",
    "colab",
    "instance",
    "ever",
    "restarts",
    "going",
    "rerun",
    "cells",
    "stop",
    "coding",
    "go",
    "break",
    "come",
    "back",
    "start",
    "notebook",
    "one",
    "downside",
    "google",
    "colab",
    "resets",
    "hours",
    "many",
    "hours",
    "know",
    "exactly",
    "reset",
    "time",
    "longer",
    "pro",
    "subscription",
    "free",
    "service",
    "way",
    "google",
    "calculate",
    "usage",
    "sort",
    "stuff",
    "ca",
    "give",
    "conclusive",
    "evidence",
    "conclusive",
    "answer",
    "long",
    "resets",
    "know",
    "come",
    "back",
    "might",
    "rerun",
    "cells",
    "shift",
    "enter",
    "scalar",
    "dimensions",
    "right",
    "single",
    "number",
    "move",
    "next",
    "thing",
    "actually",
    "wanted",
    "get",
    "number",
    "tensor",
    "type",
    "use",
    "scalar",
    "dot",
    "item",
    "going",
    "give",
    "back",
    "regular",
    "python",
    "integer",
    "wonderful",
    "go",
    "number",
    "seven",
    "back",
    "get",
    "tensor",
    "back",
    "python",
    "int",
    "next",
    "thing",
    "vector",
    "let",
    "write",
    "vector",
    "going",
    "created",
    "torch",
    "dot",
    "tensor",
    "also",
    "hear",
    "word",
    "vector",
    "used",
    "lot",
    "deal",
    "oops",
    "seven",
    "dot",
    "seven",
    "google",
    "colab",
    "auto",
    "complete",
    "bit",
    "funny",
    "always",
    "thing",
    "want",
    "see",
    "vector",
    "got",
    "two",
    "numbers",
    "really",
    "wanted",
    "find",
    "vector",
    "vector",
    "usually",
    "magnitude",
    "direction",
    "going",
    "see",
    "later",
    "go",
    "magnitude",
    "far",
    "going",
    "way",
    "going",
    "plotted",
    "got",
    "yeah",
    "vector",
    "equals",
    "magnitude",
    "would",
    "length",
    "direction",
    "would",
    "pointing",
    "oh",
    "go",
    "scalar",
    "vector",
    "matrix",
    "tensor",
    "working",
    "well",
    "thing",
    "vectors",
    "differ",
    "scalars",
    "remember",
    "rather",
    "magnitude",
    "direction",
    "vector",
    "typically",
    "one",
    "number",
    "go",
    "vector",
    "dim",
    "many",
    "dimensions",
    "one",
    "dimension",
    "kind",
    "confusing",
    "see",
    "tensors",
    "one",
    "dimension",
    "make",
    "sense",
    "another",
    "way",
    "remember",
    "many",
    "dimensions",
    "something",
    "number",
    "square",
    "brackets",
    "let",
    "check",
    "something",
    "else",
    "maybe",
    "go",
    "vector",
    "dot",
    "shape",
    "shape",
    "two",
    "difference",
    "dimension",
    "dimension",
    "like",
    "number",
    "square",
    "brackets",
    "say",
    "even",
    "though",
    "two",
    "mean",
    "number",
    "pairs",
    "closing",
    "square",
    "brackets",
    "one",
    "pair",
    "closing",
    "square",
    "brackets",
    "shape",
    "vector",
    "two",
    "two",
    "one",
    "elements",
    "means",
    "total",
    "two",
    "elements",
    "wanted",
    "step",
    "things",
    "notch",
    "let",
    "create",
    "matrix",
    "another",
    "term",
    "going",
    "hear",
    "might",
    "wondering",
    "capitalizing",
    "matrix",
    "well",
    "explain",
    "second",
    "matrix",
    "equals",
    "torch",
    "dot",
    "tensor",
    "going",
    "put",
    "two",
    "square",
    "brackets",
    "might",
    "thinking",
    "could",
    "two",
    "square",
    "brackets",
    "mean",
    "actually",
    "little",
    "bit",
    "challenge",
    "one",
    "pair",
    "square",
    "brackets",
    "endem",
    "one",
    "endem",
    "number",
    "dimensions",
    "two",
    "square",
    "brackets",
    "let",
    "create",
    "matrix",
    "beautiful",
    "got",
    "another",
    "tensor",
    "said",
    "things",
    "different",
    "names",
    "like",
    "traditional",
    "name",
    "scalar",
    "vector",
    "matrix",
    "still",
    "torch",
    "dot",
    "tensor",
    "little",
    "bit",
    "confusing",
    "thing",
    "remember",
    "pytorch",
    "basically",
    "anytime",
    "encode",
    "data",
    "numbers",
    "tensor",
    "data",
    "type",
    "many",
    "n",
    "number",
    "dimensions",
    "think",
    "matrix",
    "two",
    "go",
    "two",
    "square",
    "brackets",
    "wanted",
    "get",
    "matrix",
    "let",
    "index",
    "zeroth",
    "axis",
    "let",
    "see",
    "happens",
    "ah",
    "get",
    "seven",
    "eight",
    "get",
    "first",
    "dimension",
    "ah",
    "nine",
    "square",
    "brackets",
    "pairings",
    "come",
    "play",
    "got",
    "two",
    "square",
    "bracket",
    "pairings",
    "outside",
    "endem",
    "two",
    "get",
    "shape",
    "matrix",
    "think",
    "shape",
    "ah",
    "two",
    "two",
    "got",
    "two",
    "numbers",
    "two",
    "total",
    "four",
    "elements",
    "covering",
    "fair",
    "bit",
    "ground",
    "nice",
    "quick",
    "going",
    "teaching",
    "style",
    "course",
    "going",
    "get",
    "quite",
    "hands",
    "writing",
    "lot",
    "code",
    "interacting",
    "rather",
    "continually",
    "going",
    "back",
    "discussing",
    "going",
    "best",
    "way",
    "find",
    "happening",
    "within",
    "matrix",
    "write",
    "code",
    "similar",
    "matrices",
    "let",
    "stop",
    "matrix",
    "let",
    "upgrade",
    "tensor",
    "might",
    "put",
    "capitals",
    "well",
    "explained",
    "capitals",
    "mean",
    "yet",
    "see",
    "second",
    "let",
    "go",
    "torch",
    "dot",
    "tensor",
    "going",
    "time",
    "done",
    "one",
    "square",
    "bracket",
    "pairing",
    "done",
    "two",
    "square",
    "bracket",
    "pairings",
    "let",
    "three",
    "square",
    "bracket",
    "pairings",
    "get",
    "little",
    "bit",
    "adventurous",
    "right",
    "might",
    "thinking",
    "moment",
    "quite",
    "tedious",
    "going",
    "write",
    "bunch",
    "random",
    "numbers",
    "one",
    "two",
    "three",
    "three",
    "six",
    "nine",
    "two",
    "five",
    "four",
    "might",
    "thinking",
    "daniel",
    "said",
    "tensors",
    "could",
    "millions",
    "numbers",
    "write",
    "hand",
    "would",
    "quite",
    "tedious",
    "yes",
    "completely",
    "right",
    "fact",
    "though",
    "time",
    "wo",
    "crafting",
    "tensors",
    "hand",
    "pytorch",
    "lot",
    "behind",
    "scenes",
    "however",
    "important",
    "know",
    "fundamental",
    "building",
    "blocks",
    "models",
    "deep",
    "learning",
    "neural",
    "networks",
    "going",
    "building",
    "tensor",
    "capitals",
    "well",
    "three",
    "square",
    "brackets",
    "three",
    "square",
    "bracket",
    "pairings",
    "going",
    "refer",
    "three",
    "square",
    "brackets",
    "start",
    "going",
    "paired",
    "many",
    "n",
    "dim",
    "number",
    "dimensions",
    "think",
    "tensor",
    "three",
    "wonderful",
    "think",
    "shape",
    "tensor",
    "three",
    "elements",
    "three",
    "elements",
    "three",
    "elements",
    "one",
    "two",
    "three",
    "maybe",
    "tensor",
    "shape",
    "one",
    "three",
    "three",
    "hmm",
    "mean",
    "well",
    "got",
    "three",
    "one",
    "two",
    "three",
    "second",
    "square",
    "bracket",
    "one",
    "ah",
    "first",
    "dimension",
    "zeroth",
    "dimension",
    "remember",
    "pytorch",
    "zero",
    "indexed",
    "well",
    "let",
    "instead",
    "talking",
    "let",
    "get",
    "zeroth",
    "axis",
    "see",
    "happens",
    "zeroth",
    "dimension",
    "go",
    "okay",
    "far",
    "left",
    "one",
    "zero",
    "confusing",
    "got",
    "one",
    "got",
    "oops",
    "mean",
    "saying",
    "got",
    "one",
    "three",
    "three",
    "shape",
    "tensor",
    "outer",
    "bracket",
    "matches",
    "number",
    "one",
    "three",
    "matches",
    "next",
    "one",
    "one",
    "two",
    "three",
    "three",
    "matches",
    "one",
    "one",
    "two",
    "three",
    "like",
    "see",
    "pretty",
    "picture",
    "see",
    "dim",
    "zero",
    "lines",
    "blue",
    "bracket",
    "outer",
    "one",
    "lines",
    "one",
    "dim",
    "equals",
    "one",
    "one",
    "middle",
    "bracket",
    "lines",
    "middle",
    "dimension",
    "dim",
    "equals",
    "two",
    "inner",
    "lines",
    "three",
    "going",
    "take",
    "lot",
    "practice",
    "taken",
    "lot",
    "practice",
    "understand",
    "dimensions",
    "tensors",
    "practice",
    "would",
    "like",
    "write",
    "tensor",
    "put",
    "however",
    "many",
    "square",
    "brackets",
    "want",
    "interact",
    "end",
    "dim",
    "shape",
    "indexing",
    "done",
    "put",
    "combination",
    "numbers",
    "inside",
    "tensor",
    "little",
    "bit",
    "practice",
    "next",
    "video",
    "give",
    "shot",
    "move",
    "next",
    "topic",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "basic",
    "building",
    "blocks",
    "data",
    "representation",
    "deep",
    "learning",
    "tensor",
    "pytorch",
    "specifically",
    "within",
    "look",
    "scalar",
    "look",
    "vector",
    "look",
    "matrix",
    "look",
    "tensor",
    "issued",
    "challenge",
    "get",
    "creative",
    "like",
    "creating",
    "tensor",
    "hope",
    "gave",
    "shot",
    "see",
    "throughout",
    "course",
    "deep",
    "learning",
    "journey",
    "tensor",
    "represent",
    "almost",
    "shape",
    "size",
    "almost",
    "combination",
    "numbers",
    "within",
    "important",
    "able",
    "interact",
    "different",
    "tensors",
    "able",
    "understand",
    "different",
    "names",
    "things",
    "hear",
    "matrix",
    "go",
    "oh",
    "maybe",
    "two",
    "dimensional",
    "tensor",
    "hear",
    "vector",
    "maybe",
    "one",
    "dimensional",
    "tensor",
    "hear",
    "tensor",
    "could",
    "amount",
    "dimensions",
    "reference",
    "come",
    "back",
    "course",
    "reference",
    "got",
    "scalar",
    "single",
    "number",
    "number",
    "dimensions",
    "zero",
    "got",
    "vector",
    "number",
    "direction",
    "number",
    "dimensions",
    "one",
    "matrix",
    "tensor",
    "another",
    "little",
    "tidbit",
    "nomenclature",
    "things",
    "naming",
    "things",
    "typically",
    "see",
    "variable",
    "name",
    "scalar",
    "vector",
    "lowercase",
    "vector",
    "might",
    "lowercase",
    "storing",
    "data",
    "matrix",
    "tensor",
    "often",
    "see",
    "uppercase",
    "letter",
    "variable",
    "python",
    "case",
    "writing",
    "code",
    "exactly",
    "sure",
    "going",
    "see",
    "machine",
    "learning",
    "deep",
    "learning",
    "code",
    "research",
    "papers",
    "across",
    "board",
    "typical",
    "nomenclature",
    "scalars",
    "vectors",
    "lowercase",
    "matrix",
    "tensors",
    "uppercase",
    "naming",
    "comes",
    "given",
    "tensor",
    "uppercase",
    "said",
    "let",
    "jump",
    "another",
    "important",
    "concept",
    "tensors",
    "random",
    "tensors",
    "random",
    "tensors",
    "writing",
    "code",
    "cell",
    "could",
    "go",
    "comment",
    "python",
    "random",
    "tensors",
    "get",
    "rid",
    "could",
    "start",
    "another",
    "text",
    "cell",
    "three",
    "hashes",
    "going",
    "give",
    "us",
    "heading",
    "random",
    "tensors",
    "could",
    "turn",
    "markdown",
    "cell",
    "command",
    "mm",
    "using",
    "google",
    "colab",
    "random",
    "tensors",
    "let",
    "write",
    "random",
    "tensors",
    "done",
    "tedious",
    "thing",
    "creating",
    "tensors",
    "numbers",
    "defined",
    "whatever",
    "could",
    "define",
    "almost",
    "anything",
    "random",
    "tensors",
    "big",
    "part",
    "pytorch",
    "let",
    "write",
    "random",
    "tensors",
    "important",
    "way",
    "many",
    "neural",
    "networks",
    "learn",
    "start",
    "tensors",
    "full",
    "random",
    "numbers",
    "adjust",
    "random",
    "numbers",
    "better",
    "represent",
    "data",
    "seriously",
    "one",
    "big",
    "concepts",
    "neural",
    "networks",
    "going",
    "write",
    "code",
    "tick",
    "start",
    "random",
    "numbers",
    "look",
    "data",
    "update",
    "random",
    "numbers",
    "look",
    "data",
    "update",
    "random",
    "numbers",
    "crux",
    "neural",
    "networks",
    "let",
    "create",
    "random",
    "tensor",
    "pytorch",
    "remember",
    "said",
    "pytorch",
    "going",
    "create",
    "tensors",
    "behind",
    "scenes",
    "well",
    "one",
    "ways",
    "create",
    "random",
    "tensor",
    "give",
    "size",
    "random",
    "tensor",
    "size",
    "shape",
    "pytorch",
    "use",
    "independently",
    "size",
    "shape",
    "mean",
    "different",
    "versions",
    "thing",
    "random",
    "tensor",
    "equals",
    "torch",
    "dot",
    "rand",
    "going",
    "type",
    "three",
    "four",
    "beautiful",
    "thing",
    "google",
    "colab",
    "well",
    "wait",
    "long",
    "enough",
    "going",
    "pop",
    "doc",
    "string",
    "going",
    "personally",
    "find",
    "little",
    "hard",
    "read",
    "google",
    "colab",
    "see",
    "keep",
    "going",
    "might",
    "able",
    "read",
    "well",
    "go",
    "torch",
    "dot",
    "rand",
    "go",
    "documentation",
    "beautiful",
    "whole",
    "bunch",
    "stuff",
    "welcome",
    "read",
    "going",
    "go",
    "going",
    "see",
    "happens",
    "hands",
    "copy",
    "write",
    "notes",
    "torch",
    "random",
    "tensors",
    "done",
    "going",
    "make",
    "code",
    "cells",
    "got",
    "space",
    "get",
    "bit",
    "let",
    "see",
    "random",
    "tensor",
    "looks",
    "like",
    "go",
    "beautiful",
    "size",
    "three",
    "four",
    "got",
    "three",
    "four",
    "elements",
    "got",
    "three",
    "deep",
    "two",
    "pairs",
    "think",
    "number",
    "dimensions",
    "random",
    "tensor",
    "dim",
    "two",
    "beautiful",
    "random",
    "numbers",
    "beautiful",
    "thing",
    "pie",
    "torch",
    "going",
    "lot",
    "behind",
    "scenes",
    "wanted",
    "create",
    "size",
    "10",
    "10",
    "cases",
    "wo",
    "want",
    "one",
    "dimension",
    "going",
    "go",
    "10",
    "check",
    "number",
    "dimensions",
    "many",
    "think",
    "three",
    "got",
    "one",
    "10",
    "wanted",
    "create",
    "10",
    "10",
    "number",
    "dimensions",
    "going",
    "going",
    "change",
    "run",
    "cell",
    "yet",
    "got",
    "lot",
    "numbers",
    "find",
    "10",
    "times",
    "10",
    "times",
    "10",
    "know",
    "heads",
    "beauty",
    "collab",
    "got",
    "calculator",
    "right",
    "10",
    "times",
    "10",
    "times",
    "got",
    "thousand",
    "elements",
    "sometimes",
    "tenses",
    "hundreds",
    "thousands",
    "elements",
    "millions",
    "elements",
    "pie",
    "torch",
    "going",
    "take",
    "care",
    "lot",
    "behind",
    "scenes",
    "let",
    "clean",
    "bit",
    "space",
    "random",
    "tensor",
    "random",
    "numbers",
    "beautiful",
    "got",
    "two",
    "dimensions",
    "got",
    "three",
    "four",
    "put",
    "another",
    "one",
    "front",
    "going",
    "many",
    "dimensions",
    "three",
    "dimensions",
    "number",
    "dimensions",
    "could",
    "number",
    "inside",
    "could",
    "number",
    "let",
    "get",
    "rid",
    "let",
    "get",
    "bit",
    "specific",
    "right",
    "random",
    "tensor",
    "whatever",
    "dimension",
    "create",
    "random",
    "tensor",
    "similar",
    "shape",
    "image",
    "tensor",
    "lot",
    "time",
    "turn",
    "images",
    "image",
    "size",
    "tensor",
    "turn",
    "images",
    "tenses",
    "going",
    "let",
    "write",
    "code",
    "first",
    "size",
    "equals",
    "height",
    "width",
    "number",
    "color",
    "channels",
    "case",
    "going",
    "height",
    "color",
    "channels",
    "color",
    "channels",
    "red",
    "green",
    "blue",
    "let",
    "create",
    "random",
    "image",
    "tensor",
    "let",
    "view",
    "size",
    "shape",
    "random",
    "image",
    "size",
    "tensor",
    "view",
    "end",
    "dim",
    "beautiful",
    "okay",
    "got",
    "torch",
    "size",
    "size",
    "two",
    "two",
    "four",
    "two",
    "four",
    "three",
    "height",
    "width",
    "color",
    "channels",
    "got",
    "three",
    "dimensions",
    "one",
    "four",
    "height",
    "width",
    "color",
    "channels",
    "let",
    "go",
    "see",
    "example",
    "pytorch",
    "fundamentals",
    "notebook",
    "go",
    "say",
    "wanted",
    "encode",
    "image",
    "dad",
    "eating",
    "pizza",
    "thumbs",
    "square",
    "image",
    "two",
    "two",
    "four",
    "two",
    "two",
    "four",
    "input",
    "wanted",
    "encode",
    "tensor",
    "format",
    "well",
    "one",
    "ways",
    "representing",
    "image",
    "tensor",
    "common",
    "ways",
    "split",
    "color",
    "channels",
    "red",
    "green",
    "blue",
    "create",
    "almost",
    "color",
    "want",
    "tensor",
    "representation",
    "sometimes",
    "going",
    "see",
    "color",
    "channels",
    "come",
    "first",
    "switch",
    "around",
    "code",
    "quite",
    "easily",
    "going",
    "color",
    "channels",
    "also",
    "see",
    "color",
    "channels",
    "come",
    "end",
    "know",
    "saying",
    "lot",
    "kind",
    "covered",
    "yet",
    "main",
    "takeaway",
    "almost",
    "data",
    "represented",
    "tensor",
    "one",
    "common",
    "ways",
    "represent",
    "images",
    "format",
    "color",
    "channels",
    "height",
    "width",
    "values",
    "depend",
    "image",
    "done",
    "random",
    "way",
    "takeaway",
    "video",
    "pytorch",
    "enables",
    "create",
    "tensors",
    "quite",
    "easily",
    "random",
    "method",
    "however",
    "going",
    "lot",
    "creating",
    "tensors",
    "behind",
    "scenes",
    "random",
    "tensor",
    "valuable",
    "neural",
    "networks",
    "start",
    "random",
    "numbers",
    "look",
    "data",
    "image",
    "tensors",
    "adjust",
    "random",
    "numbers",
    "better",
    "represent",
    "data",
    "repeat",
    "steps",
    "onwards",
    "onwards",
    "onwards",
    "let",
    "finish",
    "video",
    "going",
    "challenge",
    "create",
    "random",
    "tensor",
    "whatever",
    "size",
    "shape",
    "want",
    "could",
    "5",
    "10",
    "10",
    "see",
    "looks",
    "like",
    "keep",
    "coding",
    "next",
    "video",
    "hope",
    "took",
    "challenge",
    "creating",
    "random",
    "tensor",
    "size",
    "little",
    "tidbit",
    "might",
    "seen",
    "previous",
    "video",
    "use",
    "size",
    "parameter",
    "case",
    "go",
    "either",
    "way",
    "go",
    "torch",
    "dot",
    "rand",
    "size",
    "equals",
    "put",
    "tuple",
    "three",
    "three",
    "got",
    "tensor",
    "three",
    "three",
    "also",
    "put",
    "size",
    "default",
    "going",
    "create",
    "similar",
    "tensor",
    "whether",
    "size",
    "going",
    "quite",
    "similar",
    "output",
    "depending",
    "shape",
    "put",
    "let",
    "get",
    "started",
    "another",
    "kind",
    "tensor",
    "might",
    "see",
    "zeros",
    "ones",
    "say",
    "wanted",
    "create",
    "tensor",
    "full",
    "random",
    "numbers",
    "wanted",
    "create",
    "tensor",
    "zeros",
    "helpful",
    "creating",
    "form",
    "mask",
    "covered",
    "mask",
    "essentially",
    "create",
    "tensor",
    "zeros",
    "happens",
    "multiply",
    "number",
    "zero",
    "zeros",
    "wanted",
    "multiply",
    "two",
    "together",
    "let",
    "zeros",
    "times",
    "random",
    "tensor",
    "go",
    "zeros",
    "maybe",
    "working",
    "random",
    "tensor",
    "wanted",
    "mask",
    "say",
    "numbers",
    "column",
    "reason",
    "could",
    "create",
    "tensor",
    "zeros",
    "column",
    "multiply",
    "target",
    "tensor",
    "would",
    "zero",
    "numbers",
    "telling",
    "model",
    "hey",
    "ignore",
    "numbers",
    "zeroed",
    "wanted",
    "create",
    "tensor",
    "ones",
    "create",
    "tensor",
    "ones",
    "go",
    "ones",
    "equals",
    "torch",
    "dot",
    "ones",
    "size",
    "equals",
    "three",
    "four",
    "look",
    "another",
    "parameter",
    "showed",
    "yet",
    "another",
    "important",
    "one",
    "type",
    "default",
    "data",
    "type",
    "type",
    "stands",
    "torch",
    "dot",
    "float",
    "actually",
    "using",
    "torch",
    "dot",
    "float",
    "whole",
    "time",
    "whenever",
    "create",
    "tensor",
    "pytorch",
    "using",
    "pytorch",
    "method",
    "unless",
    "explicitly",
    "define",
    "data",
    "type",
    "see",
    "later",
    "defining",
    "data",
    "type",
    "starts",
    "torch",
    "float",
    "float",
    "numbers",
    "create",
    "zeros",
    "ones",
    "zeros",
    "probably",
    "seen",
    "common",
    "ones",
    "use",
    "keep",
    "mind",
    "might",
    "come",
    "across",
    "lots",
    "different",
    "methods",
    "creating",
    "tensors",
    "truth",
    "told",
    "like",
    "random",
    "probably",
    "one",
    "common",
    "might",
    "see",
    "zeros",
    "ones",
    "field",
    "covered",
    "let",
    "move",
    "next",
    "video",
    "going",
    "create",
    "range",
    "go",
    "creating",
    "tensor",
    "full",
    "zeros",
    "whatever",
    "size",
    "want",
    "tensor",
    "full",
    "ones",
    "whatever",
    "size",
    "want",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "hope",
    "took",
    "challenge",
    "creating",
    "torch",
    "tensor",
    "zeros",
    "size",
    "ones",
    "size",
    "let",
    "investigate",
    "might",
    "create",
    "range",
    "tensors",
    "tensors",
    "like",
    "two",
    "common",
    "methods",
    "creating",
    "tensors",
    "let",
    "start",
    "creating",
    "range",
    "first",
    "use",
    "torch",
    "dot",
    "range",
    "depending",
    "watching",
    "video",
    "torch",
    "dot",
    "range",
    "may",
    "still",
    "play",
    "may",
    "deprecated",
    "write",
    "torch",
    "dot",
    "range",
    "right",
    "pie",
    "torch",
    "version",
    "using",
    "torch",
    "dot",
    "version",
    "torch",
    "pie",
    "torch",
    "point",
    "zero",
    "torch",
    "range",
    "deprecated",
    "removed",
    "future",
    "release",
    "keep",
    "mind",
    "come",
    "across",
    "code",
    "using",
    "torch",
    "dot",
    "range",
    "maybe",
    "whack",
    "way",
    "get",
    "around",
    "fix",
    "use",
    "range",
    "instead",
    "write",
    "torch",
    "dot",
    "range",
    "got",
    "tensors",
    "zero",
    "nine",
    "course",
    "starts",
    "zero",
    "index",
    "wanted",
    "one",
    "10",
    "could",
    "go",
    "like",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "go",
    "zero",
    "go",
    "1",
    "2",
    "10",
    "equals",
    "torch",
    "range",
    "wonderful",
    "also",
    "define",
    "step",
    "let",
    "let",
    "type",
    "start",
    "find",
    "documentation",
    "range",
    "sometimes",
    "google",
    "colab",
    "press",
    "shift",
    "tab",
    "find",
    "always",
    "work",
    "yeah",
    "could",
    "hover",
    "also",
    "go",
    "torch",
    "range",
    "look",
    "documentation",
    "torch",
    "range",
    "got",
    "start",
    "step",
    "let",
    "see",
    "three",
    "maybe",
    "start",
    "zero",
    "maybe",
    "want",
    "go",
    "thousand",
    "want",
    "step",
    "step",
    "fun",
    "number",
    "one",
    "10",
    "anymore",
    "go",
    "got",
    "start",
    "zero",
    "77",
    "plus",
    "77",
    "plus",
    "77",
    "way",
    "finishes",
    "thousand",
    "wanted",
    "take",
    "back",
    "one",
    "10",
    "go",
    "110",
    "default",
    "step",
    "going",
    "one",
    "oops",
    "needed",
    "end",
    "going",
    "finish",
    "end",
    "minus",
    "one",
    "go",
    "beautiful",
    "also",
    "create",
    "tensors",
    "like",
    "creating",
    "tensors",
    "like",
    "tensors",
    "like",
    "say",
    "particular",
    "shape",
    "tensor",
    "wanted",
    "replicate",
    "somewhere",
    "else",
    "want",
    "explicitly",
    "define",
    "shape",
    "shape",
    "one",
    "10",
    "one",
    "wanted",
    "create",
    "tensor",
    "full",
    "zeros",
    "shape",
    "use",
    "tensor",
    "like",
    "zeros",
    "like",
    "10",
    "zeros",
    "zeros",
    "equals",
    "even",
    "sure",
    "spelling",
    "zeros",
    "right",
    "zeros",
    "well",
    "might",
    "typo",
    "spelling",
    "zeros",
    "get",
    "saying",
    "torch",
    "zeros",
    "oh",
    "torch",
    "spell",
    "like",
    "spelling",
    "like",
    "zeros",
    "like",
    "one",
    "input",
    "going",
    "one",
    "look",
    "10",
    "zeros",
    "goodness",
    "taking",
    "quite",
    "run",
    "troubleshooting",
    "fly",
    "something",
    "happening",
    "like",
    "try",
    "stop",
    "something",
    "happening",
    "like",
    "click",
    "run",
    "stop",
    "well",
    "running",
    "fast",
    "ca",
    "click",
    "stop",
    "also",
    "run",
    "trouble",
    "go",
    "runtime",
    "restart",
    "runtime",
    "might",
    "show",
    "restart",
    "run",
    "going",
    "restart",
    "compute",
    "engine",
    "behind",
    "collab",
    "notebook",
    "run",
    "cells",
    "let",
    "see",
    "restart",
    "run",
    "runtime",
    "getting",
    "errors",
    "sometimes",
    "helps",
    "set",
    "stone",
    "way",
    "troubleshoot",
    "errors",
    "guess",
    "check",
    "go",
    "created",
    "10",
    "zeros",
    "torch",
    "zeros",
    "like",
    "one",
    "10",
    "tensor",
    "got",
    "zeros",
    "shape",
    "one",
    "like",
    "create",
    "tensors",
    "use",
    "torch",
    "arrange",
    "get",
    "deprecated",
    "message",
    "use",
    "torch",
    "arrange",
    "instead",
    "creating",
    "range",
    "tensors",
    "start",
    "end",
    "step",
    "wanted",
    "create",
    "tensors",
    "tensor",
    "like",
    "something",
    "else",
    "want",
    "look",
    "like",
    "method",
    "put",
    "input",
    "another",
    "tensor",
    "create",
    "similar",
    "tensor",
    "whatever",
    "method",
    "like",
    "fashion",
    "shape",
    "input",
    "said",
    "give",
    "try",
    "create",
    "range",
    "tensors",
    "try",
    "replicate",
    "range",
    "shape",
    "made",
    "zeros",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "let",
    "get",
    "important",
    "topic",
    "tensor",
    "data",
    "types",
    "briefly",
    "hinted",
    "said",
    "let",
    "create",
    "tensor",
    "begin",
    "float",
    "32",
    "tensor",
    "going",
    "go",
    "float",
    "32",
    "tensor",
    "equals",
    "torch",
    "dot",
    "tensor",
    "let",
    "put",
    "numbers",
    "three",
    "six",
    "nine",
    "ever",
    "played",
    "need",
    "speed",
    "underground",
    "know",
    "three",
    "six",
    "nine",
    "comes",
    "going",
    "go",
    "type",
    "equals",
    "let",
    "put",
    "none",
    "see",
    "happens",
    "hey",
    "float",
    "32",
    "tensor",
    "oh",
    "data",
    "type",
    "float",
    "32",
    "tensor",
    "dot",
    "type",
    "float",
    "32",
    "even",
    "though",
    "put",
    "none",
    "default",
    "data",
    "type",
    "pytorch",
    "even",
    "specified",
    "none",
    "going",
    "come",
    "float",
    "wanted",
    "change",
    "something",
    "else",
    "well",
    "let",
    "type",
    "float",
    "got",
    "float",
    "32",
    "tensor",
    "variable",
    "name",
    "lie",
    "float",
    "16",
    "tensor",
    "leave",
    "none",
    "let",
    "go",
    "another",
    "parameter",
    "creating",
    "tensors",
    "important",
    "device",
    "see",
    "later",
    "final",
    "one",
    "also",
    "important",
    "requires",
    "grad",
    "equals",
    "false",
    "could",
    "true",
    "course",
    "going",
    "set",
    "false",
    "three",
    "important",
    "parameters",
    "creating",
    "tensors",
    "wo",
    "necessarily",
    "always",
    "enter",
    "creating",
    "tensors",
    "pytorch",
    "lot",
    "tensor",
    "creation",
    "behind",
    "scenes",
    "let",
    "write",
    "data",
    "type",
    "data",
    "type",
    "tensor",
    "float",
    "32",
    "float",
    "like",
    "look",
    "data",
    "types",
    "available",
    "pytorch",
    "tensors",
    "go",
    "torch",
    "tensor",
    "write",
    "top",
    "unless",
    "documentation",
    "changes",
    "data",
    "types",
    "important",
    "data",
    "types",
    "first",
    "thing",
    "comes",
    "creating",
    "tensor",
    "floating",
    "point",
    "floating",
    "point",
    "16",
    "16",
    "complex",
    "common",
    "ones",
    "likely",
    "interact",
    "floating",
    "point",
    "floating",
    "point",
    "mean",
    "numbers",
    "actually",
    "mean",
    "well",
    "precision",
    "computing",
    "let",
    "look",
    "precision",
    "computing",
    "precision",
    "computer",
    "science",
    "computer",
    "science",
    "precision",
    "numerical",
    "quantity",
    "dealing",
    "numbers",
    "right",
    "measure",
    "detail",
    "quantity",
    "expressed",
    "usually",
    "measured",
    "bits",
    "sometimes",
    "decimal",
    "digits",
    "related",
    "precision",
    "mathematics",
    "describes",
    "number",
    "digits",
    "used",
    "express",
    "value",
    "us",
    "precision",
    "numerical",
    "quantity",
    "measure",
    "detail",
    "much",
    "detail",
    "quantity",
    "expressed",
    "going",
    "dive",
    "background",
    "computer",
    "science",
    "computers",
    "represent",
    "numbers",
    "important",
    "takeaway",
    "single",
    "precision",
    "floating",
    "point",
    "usually",
    "called",
    "float",
    "32",
    "means",
    "yeah",
    "number",
    "contains",
    "32",
    "bits",
    "computer",
    "memory",
    "imagine",
    "tensor",
    "using",
    "32",
    "bit",
    "floating",
    "point",
    "computer",
    "memory",
    "stores",
    "number",
    "32",
    "bits",
    "16",
    "bit",
    "floating",
    "point",
    "stores",
    "16",
    "bits",
    "16",
    "numbers",
    "representing",
    "sure",
    "bit",
    "equates",
    "single",
    "number",
    "computer",
    "memory",
    "means",
    "32",
    "bit",
    "tensor",
    "single",
    "precision",
    "half",
    "precision",
    "means",
    "default",
    "32",
    "float",
    "32",
    "torch",
    "dot",
    "float",
    "32",
    "seen",
    "code",
    "means",
    "going",
    "take",
    "certain",
    "amount",
    "space",
    "computer",
    "memory",
    "might",
    "thinking",
    "would",
    "anything",
    "default",
    "well",
    "like",
    "sacrifice",
    "detail",
    "number",
    "represented",
    "instead",
    "32",
    "bits",
    "represented",
    "16",
    "bits",
    "calculate",
    "faster",
    "numbers",
    "take",
    "less",
    "memory",
    "main",
    "differentiator",
    "32",
    "bit",
    "16",
    "bit",
    "need",
    "precision",
    "might",
    "go",
    "64",
    "bit",
    "keep",
    "mind",
    "go",
    "forward",
    "single",
    "precision",
    "half",
    "precision",
    "numbers",
    "represent",
    "represent",
    "much",
    "detail",
    "single",
    "number",
    "stored",
    "memory",
    "lot",
    "take",
    "talking",
    "10",
    "data",
    "types",
    "spending",
    "lot",
    "time",
    "going",
    "put",
    "note",
    "note",
    "tensor",
    "data",
    "types",
    "one",
    "three",
    "big",
    "issues",
    "pytorch",
    "deep",
    "learning",
    "issues",
    "going",
    "errors",
    "run",
    "deep",
    "learning",
    "three",
    "big",
    "errors",
    "run",
    "pytorch",
    "deep",
    "learning",
    "one",
    "tensors",
    "right",
    "data",
    "type",
    "two",
    "tensors",
    "right",
    "shape",
    "seen",
    "shapes",
    "four",
    "three",
    "tensors",
    "right",
    "device",
    "case",
    "tensor",
    "float",
    "16",
    "trying",
    "computations",
    "tensor",
    "float",
    "32",
    "might",
    "run",
    "errors",
    "tensors",
    "right",
    "data",
    "type",
    "important",
    "know",
    "type",
    "parameter",
    "tensors",
    "right",
    "shape",
    "well",
    "get",
    "onto",
    "matrix",
    "multiplication",
    "see",
    "one",
    "tensor",
    "certain",
    "shape",
    "another",
    "tensor",
    "another",
    "shape",
    "shapes",
    "line",
    "going",
    "run",
    "shape",
    "errors",
    "perfect",
    "segue",
    "device",
    "device",
    "equals",
    "none",
    "default",
    "going",
    "cpu",
    "using",
    "google",
    "colab",
    "enables",
    "us",
    "access",
    "oh",
    "want",
    "restart",
    "enables",
    "us",
    "access",
    "gpu",
    "said",
    "gpu",
    "enables",
    "us",
    "could",
    "change",
    "cuda",
    "would",
    "see",
    "write",
    "device",
    "agnostic",
    "code",
    "later",
    "device",
    "try",
    "operations",
    "two",
    "tensors",
    "device",
    "example",
    "one",
    "tensor",
    "lives",
    "gpu",
    "fast",
    "computing",
    "another",
    "tensor",
    "lives",
    "cpu",
    "try",
    "something",
    "pytorch",
    "going",
    "throw",
    "error",
    "finally",
    "last",
    "requirement",
    "grad",
    "want",
    "pytorch",
    "track",
    "gradients",
    "covered",
    "tensor",
    "goes",
    "certain",
    "numerical",
    "calculations",
    "bit",
    "bombardment",
    "thought",
    "throw",
    "important",
    "parameters",
    "aware",
    "since",
    "discussing",
    "data",
    "type",
    "really",
    "would",
    "reminiscent",
    "discuss",
    "data",
    "type",
    "without",
    "discussing",
    "right",
    "shape",
    "right",
    "device",
    "said",
    "let",
    "write",
    "device",
    "tensor",
    "whether",
    "track",
    "gradients",
    "tensor",
    "operations",
    "float",
    "32",
    "tensor",
    "might",
    "change",
    "tensor",
    "data",
    "type",
    "let",
    "create",
    "float",
    "16",
    "tensor",
    "saw",
    "could",
    "explicitly",
    "write",
    "float",
    "16",
    "tensor",
    "type",
    "float",
    "16",
    "tensor",
    "equals",
    "float",
    "32",
    "tensor",
    "dot",
    "type",
    "going",
    "type",
    "torch",
    "dot",
    "float",
    "16",
    "float",
    "16",
    "well",
    "define",
    "float",
    "16",
    "could",
    "use",
    "half",
    "thing",
    "things",
    "let",
    "half",
    "float",
    "16",
    "explicit",
    "let",
    "check",
    "float",
    "16",
    "tensor",
    "beautiful",
    "converted",
    "float",
    "32",
    "tensor",
    "float",
    "one",
    "ways",
    "able",
    "tackle",
    "tensors",
    "right",
    "data",
    "type",
    "issue",
    "run",
    "little",
    "note",
    "precision",
    "computing",
    "like",
    "read",
    "going",
    "link",
    "computers",
    "store",
    "numbers",
    "precision",
    "computing",
    "go",
    "get",
    "rid",
    "wonderful",
    "give",
    "try",
    "create",
    "tensors",
    "research",
    "go",
    "documentation",
    "torch",
    "dot",
    "tensor",
    "see",
    "find",
    "little",
    "bit",
    "type",
    "device",
    "requires",
    "grad",
    "create",
    "tensors",
    "different",
    "data",
    "types",
    "play",
    "around",
    "whatever",
    "ones",
    "want",
    "see",
    "run",
    "errors",
    "maybe",
    "try",
    "multiply",
    "two",
    "tensors",
    "together",
    "go",
    "float",
    "16",
    "tensor",
    "times",
    "float",
    "32",
    "tensor",
    "give",
    "try",
    "see",
    "happens",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "little",
    "bit",
    "tensor",
    "data",
    "types",
    "well",
    "common",
    "parameters",
    "see",
    "past",
    "torch",
    "dot",
    "tensor",
    "method",
    "challenge",
    "end",
    "last",
    "video",
    "create",
    "tensors",
    "different",
    "data",
    "types",
    "see",
    "happens",
    "multiply",
    "float",
    "16",
    "tensor",
    "float",
    "32",
    "tensor",
    "oh",
    "works",
    "like",
    "daniel",
    "said",
    "going",
    "tensors",
    "right",
    "data",
    "type",
    "well",
    "another",
    "kind",
    "gotcha",
    "caveat",
    "pie",
    "torch",
    "deep",
    "learning",
    "general",
    "sometimes",
    "find",
    "even",
    "think",
    "something",
    "may",
    "error",
    "two",
    "tensors",
    "different",
    "data",
    "types",
    "actually",
    "results",
    "error",
    "sometimes",
    "operations",
    "especially",
    "training",
    "large",
    "neural",
    "networks",
    "get",
    "data",
    "type",
    "issues",
    "important",
    "thing",
    "aware",
    "fact",
    "operations",
    "run",
    "error",
    "tensors",
    "right",
    "data",
    "type",
    "let",
    "try",
    "another",
    "type",
    "maybe",
    "try",
    "32",
    "bit",
    "integer",
    "torch",
    "dot",
    "try",
    "multiply",
    "float",
    "wonder",
    "happen",
    "let",
    "go",
    "32",
    "32",
    "tensor",
    "equals",
    "torch",
    "dot",
    "tensor",
    "make",
    "three",
    "notice",
    "floats",
    "dot",
    "points",
    "make",
    "float",
    "three",
    "six",
    "nine",
    "type",
    "torch",
    "32",
    "tensor",
    "look",
    "like",
    "typo",
    "course",
    "one",
    "many",
    "32",
    "tensor",
    "let",
    "go",
    "float",
    "32",
    "tensor",
    "see",
    "happens",
    "get",
    "pie",
    "torch",
    "throw",
    "error",
    "32",
    "tensor",
    "huh",
    "worked",
    "well",
    "maybe",
    "go",
    "happens",
    "still",
    "works",
    "see",
    "one",
    "confusing",
    "parts",
    "tensor",
    "operations",
    "long",
    "tensor",
    "torch",
    "long",
    "going",
    "still",
    "work",
    "ah",
    "torch",
    "attribute",
    "called",
    "long",
    "data",
    "type",
    "issue",
    "think",
    "long",
    "tensor",
    "long",
    "tensor",
    "work",
    "type",
    "must",
    "torch",
    "type",
    "torch",
    "long",
    "tensor",
    "could",
    "sworn",
    "torch",
    "dot",
    "tensor",
    "oh",
    "go",
    "torch",
    "dot",
    "long",
    "tensor",
    "another",
    "word",
    "64",
    "bit",
    "saying",
    "cpu",
    "tensor",
    "okay",
    "let",
    "see",
    "troubleshooting",
    "fly",
    "multiply",
    "float",
    "32",
    "times",
    "long",
    "works",
    "okay",
    "actually",
    "bit",
    "robust",
    "thought",
    "keep",
    "mind",
    "training",
    "models",
    "probably",
    "going",
    "run",
    "errors",
    "point",
    "tensor",
    "right",
    "data",
    "type",
    "pie",
    "torch",
    "throws",
    "us",
    "error",
    "saying",
    "tensors",
    "wrong",
    "data",
    "type",
    "well",
    "least",
    "know",
    "change",
    "data",
    "type",
    "set",
    "data",
    "type",
    "need",
    "said",
    "let",
    "formalize",
    "fair",
    "bit",
    "already",
    "getting",
    "information",
    "tensors",
    "three",
    "big",
    "things",
    "want",
    "get",
    "tensors",
    "line",
    "three",
    "big",
    "errors",
    "going",
    "face",
    "neural",
    "networks",
    "deep",
    "lining",
    "let",
    "copy",
    "going",
    "get",
    "copy",
    "want",
    "get",
    "information",
    "tensors",
    "check",
    "shape",
    "check",
    "data",
    "type",
    "check",
    "device",
    "let",
    "write",
    "get",
    "information",
    "get",
    "type",
    "let",
    "write",
    "data",
    "type",
    "tensor",
    "use",
    "tensor",
    "dot",
    "type",
    "let",
    "go",
    "get",
    "shape",
    "tensor",
    "use",
    "tensor",
    "dot",
    "shape",
    "get",
    "device",
    "tensor",
    "devices",
    "cpu",
    "gpu",
    "use",
    "tensor",
    "dot",
    "device",
    "let",
    "see",
    "three",
    "action",
    "run",
    "one",
    "three",
    "big",
    "problems",
    "deep",
    "learning",
    "neural",
    "networks",
    "general",
    "especially",
    "pytorch",
    "tensor",
    "right",
    "data",
    "type",
    "tensor",
    "right",
    "shape",
    "tensor",
    "right",
    "device",
    "let",
    "create",
    "tensor",
    "try",
    "three",
    "got",
    "tensor",
    "equals",
    "torch",
    "dot",
    "rand",
    "create",
    "three",
    "four",
    "let",
    "look",
    "looks",
    "like",
    "go",
    "random",
    "numbers",
    "shape",
    "three",
    "four",
    "let",
    "find",
    "details",
    "find",
    "details",
    "tensor",
    "print",
    "print",
    "tensor",
    "oops",
    "want",
    "print",
    "let",
    "format",
    "make",
    "f",
    "string",
    "shape",
    "tensor",
    "oh",
    "let",
    "data",
    "type",
    "first",
    "follow",
    "order",
    "data",
    "type",
    "tensor",
    "going",
    "go",
    "tensor",
    "dot",
    "dot",
    "type",
    "beautiful",
    "going",
    "print",
    "tensors",
    "right",
    "shape",
    "let",
    "go",
    "shape",
    "tensor",
    "equals",
    "tensor",
    "dot",
    "shape",
    "oh",
    "went",
    "bit",
    "fast",
    "could",
    "also",
    "use",
    "size",
    "let",
    "confirm",
    "actually",
    "code",
    "together",
    "experience",
    "tensor",
    "dot",
    "size",
    "tensor",
    "dot",
    "shape",
    "result",
    "thing",
    "true",
    "oh",
    "function",
    "oh",
    "tensor",
    "dot",
    "size",
    "function",
    "attribute",
    "go",
    "one",
    "use",
    "probably",
    "used",
    "using",
    "shape",
    "may",
    "come",
    "across",
    "dot",
    "size",
    "well",
    "realize",
    "quite",
    "thing",
    "except",
    "one",
    "function",
    "one",
    "attribute",
    "attribute",
    "written",
    "dot",
    "shape",
    "without",
    "curly",
    "brackets",
    "function",
    "method",
    "brackets",
    "end",
    "difference",
    "attributes",
    "type",
    "size",
    "going",
    "change",
    "shape",
    "tensor",
    "attributes",
    "getting",
    "probably",
    "write",
    "tensor",
    "attributes",
    "formal",
    "name",
    "things",
    "finally",
    "else",
    "want",
    "tensors",
    "device",
    "looking",
    "let",
    "get",
    "rid",
    "get",
    "rid",
    "print",
    "f",
    "device",
    "tensor",
    "default",
    "tensor",
    "cpu",
    "tensor",
    "dot",
    "device",
    "go",
    "got",
    "tensor",
    "tensor",
    "data",
    "type",
    "torch",
    "float",
    "32",
    "change",
    "anything",
    "else",
    "torch",
    "float",
    "32",
    "default",
    "shape",
    "three",
    "four",
    "makes",
    "lot",
    "sense",
    "passed",
    "three",
    "four",
    "device",
    "tensor",
    "cpu",
    "course",
    "default",
    "unless",
    "explicitly",
    "say",
    "put",
    "another",
    "device",
    "tensors",
    "create",
    "default",
    "cpu",
    "rather",
    "gpu",
    "see",
    "later",
    "put",
    "tensors",
    "things",
    "torch",
    "onto",
    "gpu",
    "said",
    "give",
    "shot",
    "create",
    "tensor",
    "get",
    "information",
    "tensor",
    "see",
    "change",
    "around",
    "see",
    "could",
    "create",
    "random",
    "tensor",
    "instead",
    "float",
    "32",
    "float",
    "probably",
    "another",
    "extracurricular",
    "covered",
    "yet",
    "see",
    "change",
    "device",
    "pytorch",
    "tensor",
    "give",
    "crack",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "look",
    "tensor",
    "attributes",
    "namely",
    "data",
    "type",
    "tensor",
    "shape",
    "tensor",
    "device",
    "tensor",
    "lives",
    "alluded",
    "fact",
    "help",
    "resolve",
    "three",
    "common",
    "issues",
    "building",
    "neural",
    "networks",
    "deep",
    "learning",
    "models",
    "specifically",
    "pytorch",
    "tensor",
    "right",
    "data",
    "type",
    "tensor",
    "right",
    "shape",
    "tensor",
    "right",
    "device",
    "let",
    "get",
    "manipulating",
    "tensors",
    "mean",
    "let",
    "write",
    "title",
    "manipulating",
    "tensors",
    "going",
    "tensor",
    "operations",
    "building",
    "neural",
    "networks",
    "neural",
    "networks",
    "comprised",
    "lots",
    "mathematical",
    "functions",
    "pytorch",
    "code",
    "going",
    "run",
    "behind",
    "scenes",
    "us",
    "let",
    "go",
    "tensor",
    "operations",
    "include",
    "addition",
    "subtraction",
    "regular",
    "addition",
    "subtraction",
    "multiplication",
    "two",
    "types",
    "multiplication",
    "typically",
    "see",
    "referenced",
    "deep",
    "learning",
    "neural",
    "networks",
    "division",
    "matrix",
    "multiplication",
    "ones",
    "addition",
    "subtraction",
    "multiplication",
    "division",
    "typical",
    "operations",
    "probably",
    "familiar",
    "matrix",
    "multiplication",
    "different",
    "one",
    "matrix",
    "multiplication",
    "going",
    "look",
    "minute",
    "find",
    "patterns",
    "numbers",
    "data",
    "set",
    "neural",
    "network",
    "combine",
    "functions",
    "way",
    "shape",
    "form",
    "takes",
    "tensor",
    "full",
    "random",
    "numbers",
    "performs",
    "kind",
    "combination",
    "addition",
    "subtraction",
    "multiplication",
    "division",
    "matrix",
    "multiplication",
    "could",
    "combination",
    "manipulate",
    "numbers",
    "way",
    "represent",
    "data",
    "set",
    "neural",
    "network",
    "learns",
    "comprise",
    "functions",
    "look",
    "data",
    "adjust",
    "numbers",
    "random",
    "tensor",
    "go",
    "said",
    "let",
    "look",
    "begin",
    "addition",
    "first",
    "thing",
    "need",
    "create",
    "tensor",
    "add",
    "something",
    "tensor",
    "go",
    "torch",
    "tensor",
    "let",
    "go",
    "one",
    "two",
    "three",
    "add",
    "something",
    "tensor",
    "tensor",
    "plus",
    "use",
    "plus",
    "addition",
    "operator",
    "like",
    "python",
    "tensor",
    "plus",
    "10",
    "going",
    "tensor",
    "11",
    "12",
    "13",
    "tensor",
    "plus",
    "100",
    "going",
    "expect",
    "plus",
    "let",
    "leave",
    "plus",
    "10",
    "add",
    "10",
    "might",
    "able",
    "guess",
    "would",
    "multiply",
    "let",
    "go",
    "multiply",
    "tensor",
    "go",
    "tensor",
    "star",
    "keyboard",
    "shift",
    "eight",
    "get",
    "10",
    "10",
    "reassign",
    "tensor",
    "still",
    "go",
    "reassign",
    "tensor",
    "equals",
    "tensor",
    "10",
    "check",
    "tensor",
    "got",
    "10",
    "thing",
    "10",
    "go",
    "back",
    "top",
    "delete",
    "reassignment",
    "oh",
    "get",
    "tensor",
    "oh",
    "happened",
    "oh",
    "got",
    "yeah",
    "okay",
    "see",
    "tensor",
    "10",
    "tensor",
    "still",
    "try",
    "subtract",
    "subtract",
    "10",
    "equals",
    "tensor",
    "minus",
    "also",
    "use",
    "well",
    "go",
    "one",
    "minus",
    "10",
    "eight",
    "minus",
    "10",
    "three",
    "minus",
    "also",
    "use",
    "like",
    "torch",
    "inbuilt",
    "functions",
    "pytorch",
    "try",
    "pytorch",
    "inbuilt",
    "functions",
    "torch",
    "dot",
    "mall",
    "short",
    "multiply",
    "pass",
    "tensor",
    "add",
    "going",
    "multiply",
    "element",
    "tensor",
    "taking",
    "original",
    "tensor",
    "created",
    "performing",
    "thing",
    "would",
    "recommend",
    "use",
    "operators",
    "python",
    "reason",
    "see",
    "torch",
    "dot",
    "mall",
    "maybe",
    "reason",
    "generally",
    "understandable",
    "use",
    "operators",
    "need",
    "straight",
    "multiplication",
    "straight",
    "addition",
    "straight",
    "subtraction",
    "torch",
    "also",
    "torch",
    "dot",
    "add",
    "torch",
    "dot",
    "add",
    "torch",
    "dot",
    "add",
    "might",
    "torch",
    "dot",
    "add",
    "sure",
    "oh",
    "go",
    "yeah",
    "torch",
    "dot",
    "add",
    "alluded",
    "two",
    "different",
    "types",
    "multiplication",
    "hear",
    "element",
    "wise",
    "matrix",
    "multiplication",
    "going",
    "cover",
    "matrix",
    "multiplication",
    "next",
    "video",
    "challenge",
    "though",
    "would",
    "like",
    "search",
    "matrix",
    "multiplication",
    "think",
    "first",
    "website",
    "comes",
    "matrix",
    "multiplication",
    "wikipedia",
    "yeah",
    "math",
    "fun",
    "great",
    "guide",
    "get",
    "matrix",
    "multiplication",
    "jump",
    "math",
    "fun",
    "look",
    "matrix",
    "multiplying",
    "think",
    "might",
    "able",
    "replicate",
    "pie",
    "torch",
    "even",
    "sure",
    "think",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "discussed",
    "basic",
    "tensor",
    "operations",
    "addition",
    "subtraction",
    "multiplication",
    "element",
    "wise",
    "division",
    "matrix",
    "multiplication",
    "actually",
    "go",
    "matrix",
    "multiplication",
    "let",
    "start",
    "particularly",
    "discussing",
    "difference",
    "element",
    "wise",
    "matrix",
    "multiplication",
    "come",
    "let",
    "write",
    "another",
    "heading",
    "matrix",
    "multiplication",
    "two",
    "ways",
    "two",
    "main",
    "ways",
    "yeah",
    "let",
    "write",
    "two",
    "main",
    "ways",
    "performing",
    "multiplication",
    "neural",
    "networks",
    "deep",
    "learning",
    "one",
    "simple",
    "version",
    "seen",
    "element",
    "wise",
    "multiplication",
    "number",
    "two",
    "matrix",
    "multiplication",
    "matrix",
    "multiplication",
    "actually",
    "possibly",
    "common",
    "tensor",
    "operation",
    "find",
    "inside",
    "neural",
    "networks",
    "last",
    "video",
    "issued",
    "extra",
    "curriculum",
    "look",
    "math",
    "fun",
    "dot",
    "com",
    "page",
    "multiply",
    "matrices",
    "first",
    "example",
    "go",
    "element",
    "wise",
    "multiplication",
    "means",
    "multiplying",
    "element",
    "specific",
    "number",
    "case",
    "two",
    "times",
    "four",
    "equals",
    "eight",
    "two",
    "times",
    "zero",
    "equals",
    "zero",
    "two",
    "times",
    "one",
    "equals",
    "two",
    "two",
    "times",
    "negative",
    "nine",
    "equals",
    "negative",
    "move",
    "matrix",
    "multiplication",
    "multiplying",
    "matrix",
    "another",
    "matrix",
    "need",
    "dot",
    "product",
    "something",
    "also",
    "hear",
    "matrix",
    "multiplication",
    "referred",
    "dot",
    "product",
    "two",
    "used",
    "interchangeably",
    "matrix",
    "multiplication",
    "dot",
    "product",
    "look",
    "symbol",
    "dot",
    "product",
    "find",
    "dot",
    "go",
    "heavy",
    "dot",
    "images",
    "go",
    "dot",
    "vector",
    "dot",
    "product",
    "different",
    "options",
    "let",
    "look",
    "looks",
    "like",
    "pytorch",
    "code",
    "first",
    "little",
    "bit",
    "difference",
    "get",
    "multiplying",
    "matrix",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "times",
    "seven",
    "eight",
    "nine",
    "10",
    "11",
    "12",
    "get",
    "58",
    "well",
    "start",
    "going",
    "difference",
    "element",
    "wise",
    "dot",
    "product",
    "way",
    "one",
    "times",
    "seven",
    "record",
    "seven",
    "two",
    "times",
    "nine",
    "first",
    "row",
    "first",
    "column",
    "two",
    "times",
    "nine",
    "three",
    "times",
    "11",
    "add",
    "seven",
    "plus",
    "18",
    "plus",
    "33",
    "get",
    "element",
    "throughout",
    "two",
    "matrices",
    "end",
    "something",
    "like",
    "encourage",
    "go",
    "step",
    "step",
    "reproduce",
    "good",
    "challenge",
    "would",
    "reproduce",
    "hand",
    "pytorch",
    "code",
    "let",
    "go",
    "back",
    "write",
    "pytorch",
    "code",
    "want",
    "link",
    "well",
    "information",
    "multiplying",
    "matrices",
    "going",
    "turn",
    "markdown",
    "let",
    "first",
    "see",
    "element",
    "wise",
    "element",
    "wise",
    "multiplication",
    "going",
    "start",
    "rudimentary",
    "example",
    "tensor",
    "moment",
    "multiply",
    "get",
    "let",
    "print",
    "something",
    "looks",
    "bit",
    "prettier",
    "print",
    "going",
    "turn",
    "string",
    "print",
    "tensor",
    "times",
    "tensor",
    "element",
    "wise",
    "multiplication",
    "going",
    "give",
    "us",
    "print",
    "equals",
    "let",
    "tensor",
    "times",
    "tensor",
    "go",
    "like",
    "wonderful",
    "get",
    "one",
    "times",
    "one",
    "equals",
    "one",
    "two",
    "times",
    "two",
    "equals",
    "four",
    "three",
    "times",
    "three",
    "equals",
    "nine",
    "matrix",
    "multiplication",
    "pytorch",
    "stores",
    "matrix",
    "multiplication",
    "similar",
    "torch",
    "dot",
    "mall",
    "torch",
    "dot",
    "mat",
    "mall",
    "space",
    "stands",
    "matrix",
    "multiplication",
    "let",
    "test",
    "let",
    "true",
    "exact",
    "thing",
    "instead",
    "element",
    "wise",
    "matrix",
    "multiplication",
    "123",
    "tensor",
    "happens",
    "oh",
    "goodness",
    "get",
    "14",
    "instead",
    "149",
    "guess",
    "got",
    "14",
    "think",
    "got",
    "14",
    "numbers",
    "recall",
    "back",
    "saw",
    "multiplying",
    "two",
    "smaller",
    "tensors",
    "way",
    "example",
    "larger",
    "one",
    "principle",
    "applies",
    "across",
    "different",
    "sizes",
    "tensors",
    "matrices",
    "say",
    "matrix",
    "multiplication",
    "also",
    "matrix",
    "multiplication",
    "tensors",
    "case",
    "using",
    "vectors",
    "add",
    "confusion",
    "difference",
    "element",
    "wise",
    "dot",
    "product",
    "well",
    "got",
    "one",
    "main",
    "addition",
    "addition",
    "code",
    "hand",
    "matrix",
    "multiplication",
    "hand",
    "recall",
    "elements",
    "tensor",
    "wanted",
    "matrix",
    "multiply",
    "one",
    "times",
    "one",
    "equivalent",
    "one",
    "times",
    "seven",
    "visual",
    "example",
    "plus",
    "going",
    "two",
    "times",
    "two",
    "two",
    "times",
    "two",
    "give",
    "us",
    "plus",
    "three",
    "times",
    "three",
    "give",
    "us",
    "three",
    "times",
    "three",
    "gives",
    "us",
    "got",
    "number",
    "could",
    "loop",
    "let",
    "gaze",
    "say",
    "gaze",
    "means",
    "look",
    "australian",
    "colloquialism",
    "look",
    "want",
    "show",
    "time",
    "difference",
    "might",
    "actually",
    "big",
    "difference",
    "hand",
    "versus",
    "using",
    "something",
    "like",
    "matmore",
    "another",
    "thing",
    "note",
    "pytorch",
    "method",
    "already",
    "implemented",
    "chances",
    "fast",
    "calculating",
    "version",
    "method",
    "know",
    "basic",
    "operators",
    "said",
    "usually",
    "best",
    "use",
    "straight",
    "basic",
    "operator",
    "something",
    "like",
    "matrix",
    "multiplication",
    "advanced",
    "operators",
    "instead",
    "basic",
    "operators",
    "probably",
    "want",
    "use",
    "torch",
    "version",
    "rather",
    "writing",
    "loop",
    "let",
    "go",
    "value",
    "equals",
    "zero",
    "matrix",
    "multiplication",
    "hand",
    "range",
    "len",
    "tensor",
    "element",
    "length",
    "tensor",
    "123",
    "want",
    "update",
    "value",
    "plus",
    "equal",
    "plus",
    "reassignment",
    "ith",
    "element",
    "tensor",
    "times",
    "ith",
    "element",
    "times",
    "long",
    "going",
    "take",
    "let",
    "return",
    "value",
    "get",
    "14",
    "print",
    "go",
    "milliseconds",
    "whatever",
    "cpu",
    "google",
    "collab",
    "using",
    "behind",
    "scenes",
    "time",
    "use",
    "torch",
    "method",
    "torch",
    "dot",
    "matmore",
    "tensor",
    "dot",
    "sensor",
    "using",
    "small",
    "tensor",
    "okay",
    "go",
    "actually",
    "showed",
    "much",
    "quicker",
    "even",
    "small",
    "tensor",
    "milliseconds",
    "252",
    "microseconds",
    "10",
    "times",
    "slower",
    "using",
    "loop",
    "pie",
    "torches",
    "vectorized",
    "version",
    "let",
    "look",
    "want",
    "find",
    "vectorization",
    "means",
    "type",
    "programming",
    "rather",
    "writing",
    "loops",
    "could",
    "imagine",
    "tensor",
    "let",
    "say",
    "million",
    "elements",
    "instead",
    "three",
    "loop",
    "elements",
    "one",
    "one",
    "going",
    "quite",
    "cumbersome",
    "lot",
    "pie",
    "torches",
    "functions",
    "behind",
    "scenes",
    "implement",
    "optimized",
    "functions",
    "perform",
    "mathematical",
    "operations",
    "matrix",
    "multiplication",
    "like",
    "one",
    "hand",
    "far",
    "faster",
    "manner",
    "see",
    "tensor",
    "three",
    "elements",
    "imagine",
    "speedups",
    "something",
    "like",
    "tensor",
    "million",
    "elements",
    "said",
    "crux",
    "matrix",
    "multiplication",
    "little",
    "bit",
    "encourage",
    "read",
    "documentation",
    "otherwise",
    "let",
    "look",
    "couple",
    "rules",
    "satisfy",
    "larger",
    "versions",
    "matrix",
    "multiplication",
    "right",
    "done",
    "simple",
    "tensor",
    "let",
    "step",
    "things",
    "notch",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "introduced",
    "matrix",
    "multiplication",
    "although",
    "seen",
    "yet",
    "one",
    "common",
    "operations",
    "neural",
    "networks",
    "saw",
    "always",
    "try",
    "use",
    "torches",
    "implementation",
    "certain",
    "operations",
    "except",
    "basic",
    "operations",
    "like",
    "plus",
    "multiplication",
    "whatnot",
    "chances",
    "lot",
    "faster",
    "version",
    "would",
    "things",
    "hand",
    "also",
    "lot",
    "less",
    "code",
    "like",
    "compared",
    "pretty",
    "verbose",
    "code",
    "compared",
    "matrix",
    "multiply",
    "two",
    "tensors",
    "something",
    "allude",
    "last",
    "video",
    "couple",
    "rules",
    "need",
    "satisfied",
    "performing",
    "matrix",
    "multiplication",
    "worked",
    "us",
    "rather",
    "simple",
    "tensor",
    "start",
    "build",
    "larger",
    "tensors",
    "might",
    "run",
    "one",
    "common",
    "errors",
    "deep",
    "learning",
    "going",
    "write",
    "actually",
    "one",
    "familiar",
    "one",
    "common",
    "errors",
    "deep",
    "learning",
    "already",
    "alluded",
    "well",
    "shape",
    "errors",
    "let",
    "jump",
    "back",
    "minute",
    "want",
    "write",
    "two",
    "rules",
    "performing",
    "two",
    "main",
    "rules",
    "performing",
    "matrix",
    "multiplication",
    "needs",
    "satisfy",
    "otherwise",
    "going",
    "get",
    "error",
    "number",
    "one",
    "inner",
    "dimensions",
    "must",
    "match",
    "let",
    "see",
    "means",
    "want",
    "two",
    "tensors",
    "shape",
    "three",
    "two",
    "going",
    "use",
    "symbol",
    "might",
    "asking",
    "symbol",
    "well",
    "symbol",
    "another",
    "like",
    "operator",
    "symbol",
    "matrix",
    "multiplication",
    "want",
    "give",
    "example",
    "go",
    "tensor",
    "stands",
    "matrix",
    "multiplication",
    "get",
    "tensor",
    "14",
    "exactly",
    "got",
    "use",
    "use",
    "mat",
    "mall",
    "would",
    "personally",
    "recommend",
    "use",
    "mat",
    "mall",
    "little",
    "bit",
    "clearer",
    "sometimes",
    "get",
    "confusing",
    "common",
    "seeing",
    "something",
    "like",
    "mat",
    "mall",
    "get",
    "rid",
    "using",
    "brevity",
    "going",
    "go",
    "three",
    "two",
    "wo",
    "work",
    "see",
    "second",
    "go",
    "two",
    "three",
    "three",
    "two",
    "work",
    "go",
    "reverse",
    "say",
    "threes",
    "outside",
    "twos",
    "twos",
    "inside",
    "threes",
    "outside",
    "work",
    "well",
    "rule",
    "number",
    "one",
    "inner",
    "dimensions",
    "must",
    "match",
    "inner",
    "dimensions",
    "mean",
    "let",
    "create",
    "torch",
    "round",
    "create",
    "size",
    "get",
    "shape",
    "created",
    "tensor",
    "like",
    "three",
    "two",
    "created",
    "another",
    "tensor",
    "well",
    "let",
    "show",
    "straight",
    "torch",
    "dot",
    "mat",
    "mall",
    "torch",
    "dot",
    "ran",
    "watch",
    "wo",
    "work",
    "get",
    "error",
    "go",
    "one",
    "common",
    "errors",
    "going",
    "face",
    "deep",
    "learning",
    "matrix",
    "one",
    "matrix",
    "two",
    "shapes",
    "multiplied",
    "satisfy",
    "rule",
    "number",
    "one",
    "inner",
    "dimensions",
    "must",
    "match",
    "mean",
    "inner",
    "dimensions",
    "dimension",
    "multiplied",
    "dimension",
    "say",
    "trying",
    "multiply",
    "three",
    "two",
    "three",
    "two",
    "inner",
    "dimensions",
    "work",
    "inner",
    "dimensions",
    "match",
    "two",
    "three",
    "three",
    "two",
    "two",
    "three",
    "three",
    "two",
    "notice",
    "inner",
    "dimensions",
    "inner",
    "inner",
    "match",
    "let",
    "see",
    "comes",
    "look",
    "rule",
    "two",
    "comes",
    "play",
    "two",
    "resulting",
    "matrix",
    "shape",
    "outer",
    "dimensions",
    "seen",
    "one",
    "two",
    "three",
    "three",
    "two",
    "remember",
    "matrix",
    "multiply",
    "matrix",
    "shape",
    "two",
    "three",
    "matrix",
    "multiply",
    "matrix",
    "three",
    "two",
    "inner",
    "dimensions",
    "match",
    "works",
    "resulting",
    "shape",
    "two",
    "two",
    "seen",
    "got",
    "shape",
    "two",
    "two",
    "reverse",
    "one",
    "also",
    "work",
    "three",
    "outside",
    "think",
    "going",
    "happen",
    "fact",
    "encourage",
    "pause",
    "video",
    "give",
    "go",
    "going",
    "result",
    "three",
    "three",
    "matrix",
    "take",
    "word",
    "let",
    "look",
    "three",
    "put",
    "two",
    "inside",
    "put",
    "two",
    "inside",
    "three",
    "outside",
    "give",
    "us",
    "oh",
    "look",
    "three",
    "three",
    "one",
    "two",
    "three",
    "one",
    "two",
    "three",
    "change",
    "two",
    "two",
    "almost",
    "number",
    "want",
    "let",
    "change",
    "going",
    "happen",
    "work",
    "resulting",
    "shape",
    "going",
    "inner",
    "dimensions",
    "match",
    "rule",
    "number",
    "two",
    "resulting",
    "matrix",
    "shape",
    "outer",
    "dimension",
    "think",
    "going",
    "shape",
    "resulting",
    "matrix",
    "multiplication",
    "well",
    "let",
    "look",
    "still",
    "three",
    "three",
    "wow",
    "go",
    "10",
    "10",
    "outside",
    "10",
    "10",
    "inside",
    "get",
    "well",
    "get",
    "going",
    "count",
    "go",
    "shape",
    "get",
    "10",
    "two",
    "main",
    "rules",
    "matrix",
    "multiplication",
    "running",
    "error",
    "matrix",
    "multiplication",
    "ca",
    "work",
    "let",
    "say",
    "10",
    "seven",
    "watch",
    "going",
    "happen",
    "ca",
    "multiply",
    "inner",
    "dimensions",
    "match",
    "10",
    "10",
    "seven",
    "change",
    "match",
    "get",
    "10",
    "beautiful",
    "let",
    "create",
    "little",
    "bit",
    "specific",
    "example",
    "create",
    "two",
    "tenses",
    "come",
    "actually",
    "prevent",
    "video",
    "long",
    "got",
    "error",
    "word",
    "error",
    "funny",
    "go",
    "one",
    "common",
    "errors",
    "deep",
    "learning",
    "shape",
    "errors",
    "seen",
    "going",
    "get",
    "little",
    "bit",
    "specific",
    "shape",
    "error",
    "next",
    "video",
    "look",
    "matrix",
    "multiplication",
    "website",
    "favorite",
    "website",
    "told",
    "got",
    "two",
    "one",
    "matrix",
    "multiplication",
    "dot",
    "xyz",
    "challenge",
    "next",
    "video",
    "put",
    "random",
    "numbers",
    "whatever",
    "want",
    "two",
    "10",
    "five",
    "six",
    "seven",
    "eight",
    "whatever",
    "want",
    "change",
    "around",
    "bit",
    "three",
    "four",
    "well",
    "five",
    "four",
    "multiply",
    "watch",
    "happens",
    "like",
    "watch",
    "happens",
    "going",
    "replicate",
    "something",
    "like",
    "pytorch",
    "code",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "discussed",
    "little",
    "bit",
    "matrix",
    "multiplication",
    "done",
    "looked",
    "two",
    "main",
    "rules",
    "matrix",
    "multiplication",
    "saw",
    "errors",
    "happens",
    "rules",
    "satisfied",
    "particularly",
    "inner",
    "dimensions",
    "match",
    "alluding",
    "one",
    "common",
    "errors",
    "deep",
    "learning",
    "shape",
    "errors",
    "neural",
    "networks",
    "comprised",
    "lots",
    "matrix",
    "multiplication",
    "operations",
    "sort",
    "tensor",
    "shape",
    "error",
    "somewhere",
    "neural",
    "network",
    "chances",
    "going",
    "get",
    "shape",
    "error",
    "let",
    "investigate",
    "deal",
    "let",
    "create",
    "tenses",
    "shapes",
    "matrix",
    "multiplication",
    "also",
    "showed",
    "website",
    "sorry",
    "matrix",
    "multiplication",
    "dot",
    "xyz",
    "hope",
    "go",
    "typing",
    "numbers",
    "visualizing",
    "happens",
    "going",
    "reproduce",
    "something",
    "similar",
    "happens",
    "pytorch",
    "code",
    "shapes",
    "matrix",
    "multiplication",
    "tensor",
    "let",
    "create",
    "torch",
    "dot",
    "tensor",
    "going",
    "create",
    "tensor",
    "elements",
    "one",
    "two",
    "way",
    "let",
    "go",
    "six",
    "hey",
    "enough",
    "six",
    "wonderful",
    "tensor",
    "b",
    "equal",
    "torch",
    "tensor",
    "going",
    "go",
    "one",
    "let",
    "go",
    "seven",
    "10",
    "little",
    "bit",
    "confusing",
    "one",
    "go",
    "eight",
    "11",
    "go",
    "12",
    "nine",
    "sort",
    "sequence",
    "going",
    "swapped",
    "around",
    "got",
    "vertical",
    "axis",
    "instead",
    "one",
    "two",
    "three",
    "four",
    "seven",
    "eight",
    "nine",
    "10",
    "11",
    "let",
    "try",
    "perform",
    "matrix",
    "multiplication",
    "torch",
    "dot",
    "mat",
    "mall",
    "matrix",
    "multiplication",
    "ps",
    "torch",
    "also",
    "torch",
    "dot",
    "mm",
    "stands",
    "matrix",
    "multiplication",
    "short",
    "version",
    "write",
    "know",
    "tensor",
    "tensor",
    "going",
    "write",
    "torch",
    "dot",
    "mm",
    "torch",
    "dot",
    "mat",
    "mall",
    "alias",
    "writing",
    "less",
    "code",
    "literally",
    "common",
    "matrix",
    "multiplications",
    "pytorch",
    "made",
    "torch",
    "dot",
    "mm",
    "alias",
    "mat",
    "mall",
    "type",
    "four",
    "less",
    "characters",
    "using",
    "torch",
    "dot",
    "mm",
    "instead",
    "mat",
    "mall",
    "like",
    "write",
    "mat",
    "mall",
    "little",
    "bit",
    "like",
    "explains",
    "little",
    "bit",
    "mm",
    "think",
    "going",
    "happen",
    "okay",
    "sure",
    "could",
    "probably",
    "find",
    "check",
    "shapes",
    "operation",
    "matrix",
    "multiplication",
    "satisfy",
    "rules",
    "discussed",
    "especially",
    "one",
    "main",
    "one",
    "inner",
    "dimensions",
    "must",
    "match",
    "well",
    "let",
    "look",
    "hey",
    "oh",
    "mat",
    "one",
    "mat",
    "two",
    "shapes",
    "multiplied",
    "three",
    "two",
    "three",
    "two",
    "similar",
    "went",
    "last",
    "video",
    "got",
    "actual",
    "numbers",
    "let",
    "check",
    "shape",
    "oh",
    "torch",
    "size",
    "three",
    "two",
    "torch",
    "size",
    "three",
    "two",
    "last",
    "video",
    "created",
    "random",
    "tensor",
    "could",
    "adjust",
    "shape",
    "fly",
    "tensors",
    "already",
    "exist",
    "might",
    "adjust",
    "shape",
    "well",
    "going",
    "introduce",
    "another",
    "common",
    "operation",
    "tensor",
    "manipulation",
    "see",
    "transpose",
    "fix",
    "tensor",
    "shape",
    "issues",
    "manipulate",
    "shape",
    "one",
    "tensors",
    "using",
    "transpose",
    "right",
    "going",
    "see",
    "anyway",
    "going",
    "define",
    "words",
    "transpose",
    "switches",
    "axes",
    "dimensions",
    "given",
    "tensor",
    "let",
    "see",
    "action",
    "go",
    "way",
    "go",
    "tensor",
    "b",
    "dot",
    "let",
    "see",
    "happens",
    "let",
    "look",
    "original",
    "tensor",
    "b",
    "well",
    "dot",
    "stands",
    "transpose",
    "little",
    "bit",
    "hard",
    "read",
    "might",
    "different",
    "lines",
    "tensor",
    "get",
    "rid",
    "see",
    "happened",
    "instead",
    "tensor",
    "b",
    "original",
    "one",
    "might",
    "put",
    "original",
    "top",
    "instead",
    "original",
    "one",
    "seven",
    "eight",
    "nine",
    "10",
    "11",
    "12",
    "vertical",
    "transpose",
    "transposed",
    "seven",
    "eight",
    "nine",
    "across",
    "horizontal",
    "10",
    "11",
    "12",
    "get",
    "shape",
    "tensor",
    "b",
    "dot",
    "shape",
    "let",
    "look",
    "let",
    "look",
    "original",
    "shape",
    "tensor",
    "b",
    "dot",
    "shape",
    "happened",
    "oh",
    "still",
    "got",
    "three",
    "two",
    "oh",
    "missed",
    "got",
    "typo",
    "excuse",
    "thought",
    "think",
    "code",
    "written",
    "working",
    "realize",
    "got",
    "something",
    "small",
    "dot",
    "missing",
    "throws",
    "whole",
    "train",
    "thought",
    "seeing",
    "arrows",
    "fly",
    "tensor",
    "b",
    "shape",
    "torch",
    "dot",
    "size",
    "three",
    "two",
    "try",
    "matrix",
    "multiply",
    "three",
    "two",
    "three",
    "two",
    "tensor",
    "tensor",
    "b",
    "get",
    "error",
    "inner",
    "dimensions",
    "match",
    "perform",
    "transpose",
    "tensor",
    "b",
    "switch",
    "dimensions",
    "around",
    "perform",
    "transpose",
    "tensor",
    "b",
    "dot",
    "transpose",
    "important",
    "point",
    "well",
    "still",
    "elements",
    "rearranged",
    "transposed",
    "tensor",
    "b",
    "still",
    "information",
    "encoded",
    "rearranged",
    "torch",
    "size",
    "two",
    "three",
    "try",
    "matrix",
    "multiply",
    "satisfy",
    "first",
    "criteria",
    "look",
    "output",
    "matrix",
    "multiplication",
    "tensor",
    "tensor",
    "b",
    "dot",
    "transposed",
    "three",
    "three",
    "second",
    "rule",
    "matrix",
    "multiplication",
    "resulting",
    "matrix",
    "shape",
    "outer",
    "dimensions",
    "got",
    "three",
    "two",
    "matrix",
    "multiply",
    "two",
    "three",
    "results",
    "shape",
    "three",
    "three",
    "let",
    "predify",
    "print",
    "going",
    "know",
    "step",
    "right",
    "got",
    "codal",
    "place",
    "bit",
    "let",
    "see",
    "matrix",
    "multiplication",
    "operation",
    "works",
    "tensor",
    "b",
    "transposed",
    "second",
    "going",
    "show",
    "looks",
    "like",
    "visually",
    "right",
    "done",
    "pytorch",
    "code",
    "might",
    "little",
    "confusing",
    "perfectly",
    "fine",
    "matrix",
    "multiplication",
    "takes",
    "little",
    "little",
    "practice",
    "original",
    "shapes",
    "going",
    "tensor",
    "dot",
    "shape",
    "let",
    "see",
    "tensor",
    "b",
    "equals",
    "tensor",
    "b",
    "dot",
    "shape",
    "reason",
    "spending",
    "much",
    "time",
    "see",
    "get",
    "neural",
    "networks",
    "deep",
    "learning",
    "matrix",
    "multiplication",
    "operation",
    "one",
    "common",
    "shape",
    "changed",
    "tensor",
    "shape",
    "changed",
    "tensor",
    "b",
    "shape",
    "transposed",
    "tensor",
    "b",
    "dot",
    "transpose",
    "equals",
    "want",
    "tensor",
    "b",
    "dot",
    "dot",
    "shape",
    "wonderful",
    "print",
    "let",
    "print",
    "oops",
    "print",
    "spelled",
    "wrong",
    "word",
    "print",
    "want",
    "multiplying",
    "one",
    "ways",
    "remember",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "well",
    "visualize",
    "visualize",
    "visualize",
    "things",
    "shape",
    "let",
    "symbol",
    "brevity",
    "tensor",
    "let",
    "get",
    "b",
    "dot",
    "dot",
    "shape",
    "put",
    "little",
    "rule",
    "inner",
    "dimensions",
    "must",
    "match",
    "print",
    "let",
    "get",
    "output",
    "output",
    "put",
    "new",
    "line",
    "output",
    "going",
    "equal",
    "torch",
    "dot",
    "outputs",
    "already",
    "going",
    "rewrite",
    "little",
    "bit",
    "practice",
    "tensor",
    "tensor",
    "b",
    "dot",
    "go",
    "print",
    "output",
    "finally",
    "print",
    "let",
    "get",
    "new",
    "line",
    "well",
    "output",
    "shape",
    "fair",
    "bit",
    "going",
    "going",
    "step",
    "going",
    "help",
    "us",
    "understand",
    "little",
    "bit",
    "going",
    "data",
    "visualizes",
    "motto",
    "go",
    "okay",
    "original",
    "shapes",
    "torch",
    "size",
    "three",
    "two",
    "torch",
    "size",
    "three",
    "two",
    "new",
    "shapes",
    "tensor",
    "stays",
    "changed",
    "tensor",
    "tensor",
    "b",
    "dot",
    "torch",
    "size",
    "two",
    "three",
    "multiply",
    "three",
    "two",
    "two",
    "three",
    "inner",
    "dimensions",
    "must",
    "match",
    "correct",
    "match",
    "two",
    "two",
    "output",
    "tensor",
    "27",
    "30",
    "33",
    "61",
    "68",
    "75",
    "etc",
    "output",
    "shape",
    "output",
    "shape",
    "outer",
    "dimensions",
    "three",
    "three",
    "course",
    "could",
    "rearrange",
    "maybe",
    "transpose",
    "tensor",
    "instead",
    "tensor",
    "b",
    "play",
    "around",
    "see",
    "create",
    "errors",
    "trying",
    "multiply",
    "two",
    "see",
    "happens",
    "transpose",
    "tensor",
    "instead",
    "tensor",
    "b",
    "challenge",
    "finish",
    "video",
    "recreate",
    "done",
    "cool",
    "website",
    "matrix",
    "multiplication",
    "tensor",
    "one",
    "six",
    "let",
    "recreate",
    "remove",
    "going",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "want",
    "increase",
    "going",
    "seven",
    "eight",
    "nine",
    "10",
    "11",
    "right",
    "way",
    "things",
    "already",
    "transposed",
    "let",
    "know",
    "equivalent",
    "tensor",
    "b",
    "right",
    "tensor",
    "b",
    "dot",
    "let",
    "show",
    "go",
    "tensor",
    "b",
    "dot",
    "transpose",
    "original",
    "version",
    "passing",
    "transpose",
    "version",
    "matrix",
    "multiplication",
    "website",
    "click",
    "multiply",
    "happening",
    "behind",
    "scenes",
    "pytorch",
    "code",
    "matmore",
    "one",
    "times",
    "seven",
    "plus",
    "two",
    "times",
    "see",
    "little",
    "flippy",
    "thing",
    "27",
    "comes",
    "come",
    "first",
    "element",
    "27",
    "matrix",
    "multiply",
    "thing",
    "next",
    "step",
    "get",
    "30",
    "61",
    "combination",
    "numbers",
    "33",
    "68",
    "95",
    "combination",
    "numbers",
    "finally",
    "end",
    "exactly",
    "little",
    "bit",
    "practice",
    "go",
    "create",
    "tensors",
    "almost",
    "whatever",
    "want",
    "try",
    "matrix",
    "multiply",
    "different",
    "shapes",
    "see",
    "happens",
    "transpose",
    "different",
    "values",
    "get",
    "like",
    "visualize",
    "could",
    "write",
    "something",
    "like",
    "really",
    "helps",
    "understand",
    "matrix",
    "multiplication",
    "really",
    "want",
    "visualize",
    "go",
    "website",
    "recreate",
    "target",
    "tensors",
    "something",
    "like",
    "sure",
    "long",
    "go",
    "yeah",
    "enough",
    "get",
    "started",
    "give",
    "try",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "videos",
    "covered",
    "one",
    "fundamental",
    "operations",
    "neural",
    "networks",
    "matrix",
    "multiplication",
    "time",
    "move",
    "let",
    "cover",
    "tensor",
    "aggregation",
    "mean",
    "finding",
    "min",
    "max",
    "mean",
    "sum",
    "et",
    "cetera",
    "tensor",
    "aggregation",
    "certain",
    "tensor",
    "values",
    "whatever",
    "reason",
    "may",
    "want",
    "find",
    "minimum",
    "value",
    "tensor",
    "maximum",
    "value",
    "mean",
    "sum",
    "going",
    "let",
    "look",
    "pytorch",
    "methods",
    "built",
    "finding",
    "one",
    "values",
    "called",
    "tensor",
    "aggregation",
    "going",
    "typically",
    "large",
    "amount",
    "numbers",
    "small",
    "amount",
    "numbers",
    "min",
    "tensor",
    "would",
    "turning",
    "nine",
    "elements",
    "one",
    "element",
    "hence",
    "aggregation",
    "let",
    "create",
    "tensor",
    "create",
    "tensor",
    "x",
    "equals",
    "torch",
    "dot",
    "let",
    "use",
    "range",
    "create",
    "maybe",
    "zero",
    "100",
    "step",
    "sounds",
    "good",
    "find",
    "min",
    "going",
    "torch",
    "dot",
    "min",
    "maybe",
    "could",
    "also",
    "go",
    "x",
    "dot",
    "min",
    "find",
    "max",
    "torch",
    "dot",
    "max",
    "x",
    "dot",
    "max",
    "think",
    "might",
    "get",
    "average",
    "let",
    "try",
    "find",
    "mean",
    "find",
    "mean",
    "torch",
    "dot",
    "mean",
    "oops",
    "going",
    "work",
    "happened",
    "mean",
    "input",
    "data",
    "type",
    "either",
    "floating",
    "point",
    "complex",
    "types",
    "got",
    "long",
    "instead",
    "ha",
    "ha",
    "finally",
    "knew",
    "error",
    "would",
    "show",
    "face",
    "eventually",
    "remember",
    "said",
    "right",
    "covered",
    "fair",
    "bit",
    "already",
    "right",
    "common",
    "errors",
    "going",
    "run",
    "tensor",
    "right",
    "data",
    "type",
    "right",
    "shape",
    "seen",
    "matrix",
    "multiplication",
    "right",
    "device",
    "seen",
    "yet",
    "right",
    "data",
    "type",
    "one",
    "times",
    "turns",
    "tensor",
    "created",
    "x",
    "data",
    "type",
    "x",
    "dot",
    "type",
    "64",
    "long",
    "go",
    "let",
    "look",
    "torch",
    "tensor",
    "getting",
    "long",
    "seen",
    "long",
    "n64",
    "long",
    "yeah",
    "long",
    "tenter",
    "saying",
    "turns",
    "torch",
    "mean",
    "function",
    "ca",
    "work",
    "tensors",
    "data",
    "type",
    "long",
    "well",
    "change",
    "data",
    "type",
    "let",
    "go",
    "torch",
    "mean",
    "x",
    "type",
    "change",
    "float",
    "go",
    "torch",
    "dot",
    "mean",
    "going",
    "tell",
    "us",
    "needs",
    "type",
    "oh",
    "type",
    "one",
    "option",
    "desired",
    "data",
    "type",
    "float",
    "32",
    "tell",
    "us",
    "ah",
    "another",
    "one",
    "little",
    "hidden",
    "things",
    "going",
    "come",
    "across",
    "really",
    "come",
    "across",
    "writing",
    "code",
    "sometimes",
    "documentation",
    "really",
    "tell",
    "explicitly",
    "type",
    "input",
    "input",
    "tensor",
    "however",
    "find",
    "error",
    "message",
    "either",
    "floating",
    "point",
    "complex",
    "type",
    "along",
    "convert",
    "torch",
    "float",
    "done",
    "gone",
    "x",
    "type",
    "type",
    "float",
    "let",
    "see",
    "happens",
    "45",
    "beautiful",
    "thing",
    "went",
    "x",
    "dot",
    "mean",
    "going",
    "work",
    "well",
    "oh",
    "thing",
    "go",
    "x",
    "dot",
    "type",
    "torch",
    "dot",
    "float",
    "32",
    "get",
    "mean",
    "go",
    "knew",
    "would",
    "come",
    "eventually",
    "beautiful",
    "example",
    "finding",
    "right",
    "data",
    "type",
    "let",
    "put",
    "note",
    "note",
    "torch",
    "dot",
    "mean",
    "function",
    "requires",
    "tensor",
    "float",
    "far",
    "seen",
    "two",
    "major",
    "errors",
    "pytorch",
    "data",
    "type",
    "shape",
    "issues",
    "another",
    "one",
    "said",
    "oh",
    "find",
    "sum",
    "find",
    "sum",
    "want",
    "x",
    "dot",
    "sum",
    "maybe",
    "torch",
    "dot",
    "sum",
    "first",
    "keep",
    "line",
    "going",
    "x",
    "dot",
    "sum",
    "one",
    "use",
    "like",
    "torch",
    "dot",
    "something",
    "x",
    "x",
    "dot",
    "sum",
    "personally",
    "prefer",
    "torch",
    "dot",
    "max",
    "also",
    "probably",
    "see",
    "points",
    "right",
    "really",
    "depends",
    "going",
    "would",
    "say",
    "pick",
    "whichever",
    "style",
    "prefer",
    "behind",
    "scenes",
    "calling",
    "methodology",
    "picture",
    "whichever",
    "style",
    "prefer",
    "stick",
    "throughout",
    "code",
    "let",
    "leave",
    "tensor",
    "aggregation",
    "finding",
    "min",
    "max",
    "mean",
    "sum",
    "next",
    "video",
    "going",
    "look",
    "finding",
    "positional",
    "min",
    "max",
    "also",
    "known",
    "arg",
    "max",
    "arg",
    "min",
    "vice",
    "versa",
    "actually",
    "little",
    "bit",
    "challenge",
    "next",
    "video",
    "see",
    "find",
    "positional",
    "min",
    "max",
    "mean",
    "index",
    "max",
    "value",
    "occur",
    "index",
    "tensor",
    "min",
    "occur",
    "probably",
    "want",
    "look",
    "methods",
    "arg",
    "min",
    "torch",
    "dot",
    "arg",
    "min",
    "one",
    "torch",
    "dot",
    "arg",
    "max",
    "cover",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "learned",
    "tensor",
    "aggregation",
    "found",
    "min",
    "max",
    "mean",
    "sum",
    "also",
    "ran",
    "one",
    "common",
    "issues",
    "pie",
    "torch",
    "deep",
    "learning",
    "neural",
    "networks",
    "general",
    "wrong",
    "data",
    "types",
    "solved",
    "issue",
    "converting",
    "functions",
    "torch",
    "dot",
    "mean",
    "require",
    "specific",
    "type",
    "data",
    "type",
    "input",
    "created",
    "tensor",
    "default",
    "torch",
    "however",
    "torch",
    "dot",
    "mean",
    "requires",
    "torch",
    "dot",
    "float",
    "saw",
    "error",
    "fix",
    "changing",
    "type",
    "inputs",
    "also",
    "issued",
    "challenge",
    "finding",
    "finding",
    "positional",
    "min",
    "max",
    "might",
    "found",
    "use",
    "arg",
    "min",
    "minimum",
    "let",
    "remind",
    "x",
    "means",
    "tensor",
    "index",
    "tensor",
    "find",
    "argument",
    "minimum",
    "value",
    "zero",
    "index",
    "zero",
    "get",
    "value",
    "zero",
    "zero",
    "zero",
    "index",
    "value",
    "arg",
    "min",
    "stands",
    "find",
    "position",
    "tensor",
    "minimum",
    "value",
    "arg",
    "min",
    "returns",
    "index",
    "position",
    "target",
    "tensor",
    "minimum",
    "value",
    "occurs",
    "let",
    "change",
    "x",
    "start",
    "one",
    "go",
    "arg",
    "min",
    "still",
    "position",
    "zero",
    "position",
    "zero",
    "index",
    "value",
    "index",
    "x",
    "zeroth",
    "index",
    "get",
    "one",
    "minimum",
    "value",
    "x",
    "one",
    "maximum",
    "might",
    "guess",
    "find",
    "position",
    "tensor",
    "maximum",
    "value",
    "arg",
    "max",
    "going",
    "thing",
    "except",
    "maximum",
    "position",
    "index",
    "nine",
    "go",
    "zero",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "nine",
    "index",
    "x",
    "ninth",
    "element",
    "get",
    "91",
    "beautiful",
    "two",
    "useful",
    "yes",
    "want",
    "define",
    "minimum",
    "tensor",
    "use",
    "min",
    "sometimes",
    "want",
    "actual",
    "minimum",
    "value",
    "want",
    "know",
    "appears",
    "particularly",
    "arg",
    "max",
    "value",
    "helpful",
    "use",
    "soft",
    "max",
    "activation",
    "function",
    "later",
    "covered",
    "yet",
    "going",
    "allude",
    "much",
    "remember",
    "find",
    "positional",
    "min",
    "max",
    "use",
    "arg",
    "min",
    "arg",
    "max",
    "need",
    "cover",
    "let",
    "keep",
    "going",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "covered",
    "fair",
    "bit",
    "ground",
    "let",
    "know",
    "took",
    "little",
    "break",
    "going",
    "like",
    "show",
    "get",
    "back",
    "tried",
    "write",
    "x",
    "press",
    "shift",
    "enter",
    "collab",
    "disconnected",
    "connecting",
    "soon",
    "press",
    "button",
    "collab",
    "going",
    "reconnect",
    "going",
    "try",
    "connect",
    "initialize",
    "x",
    "probably",
    "going",
    "stored",
    "memory",
    "anymore",
    "go",
    "name",
    "x",
    "defined",
    "collab",
    "state",
    "gets",
    "reset",
    "take",
    "break",
    "couple",
    "hours",
    "ensure",
    "google",
    "keep",
    "providing",
    "resources",
    "free",
    "deletes",
    "everything",
    "ensure",
    "compute",
    "resources",
    "wasted",
    "get",
    "back",
    "going",
    "go",
    "restart",
    "run",
    "necessarily",
    "restart",
    "notebook",
    "could",
    "also",
    "go",
    "run",
    "yeah",
    "could",
    "run",
    "run",
    "every",
    "cell",
    "could",
    "run",
    "could",
    "run",
    "selection",
    "cell",
    "going",
    "click",
    "run",
    "going",
    "go",
    "every",
    "single",
    "cell",
    "coded",
    "run",
    "however",
    "also",
    "stop",
    "errors",
    "left",
    "purpose",
    "remember",
    "ran",
    "shape",
    "error",
    "well",
    "error",
    "fix",
    "left",
    "purpose",
    "could",
    "keep",
    "seeing",
    "shape",
    "error",
    "going",
    "stop",
    "cell",
    "going",
    "run",
    "every",
    "cell",
    "error",
    "cell",
    "see",
    "going",
    "run",
    "run",
    "fine",
    "get",
    "right",
    "back",
    "little",
    "tidbit",
    "get",
    "back",
    "coding",
    "let",
    "cover",
    "reshaping",
    "stacking",
    "squeezing",
    "unsqueezing",
    "might",
    "thinking",
    "squeezing",
    "unsqueezing",
    "talking",
    "daniel",
    "well",
    "tenses",
    "like",
    "going",
    "squeeze",
    "tenses",
    "give",
    "hug",
    "going",
    "let",
    "go",
    "unsqueezing",
    "well",
    "let",
    "quickly",
    "define",
    "reshaping",
    "saw",
    "one",
    "common",
    "errors",
    "machine",
    "learning",
    "deep",
    "learning",
    "shape",
    "mismatches",
    "matrices",
    "satisfy",
    "certain",
    "rules",
    "reshape",
    "reshapes",
    "input",
    "tensor",
    "defined",
    "shape",
    "defining",
    "things",
    "words",
    "right",
    "going",
    "see",
    "code",
    "minute",
    "also",
    "view",
    "return",
    "view",
    "input",
    "tensor",
    "certain",
    "shape",
    "keep",
    "memory",
    "original",
    "tensor",
    "see",
    "view",
    "second",
    "reshaping",
    "view",
    "quite",
    "similar",
    "view",
    "always",
    "shares",
    "memory",
    "original",
    "tensor",
    "shows",
    "tensor",
    "different",
    "perspective",
    "different",
    "shape",
    "stacking",
    "combine",
    "multiple",
    "tensors",
    "top",
    "v",
    "stack",
    "vertical",
    "stack",
    "side",
    "side",
    "h",
    "stack",
    "let",
    "see",
    "different",
    "types",
    "torch",
    "stacks",
    "research",
    "different",
    "things",
    "wanted",
    "learn",
    "something",
    "new",
    "would",
    "search",
    "torch",
    "something",
    "stack",
    "concatenate",
    "sequence",
    "tensors",
    "along",
    "new",
    "dimension",
    "okay",
    "maybe",
    "h",
    "stack",
    "v",
    "stack",
    "define",
    "dimension",
    "like",
    "combine",
    "wonder",
    "torch",
    "v",
    "stack",
    "torch",
    "v",
    "stack",
    "oh",
    "torch",
    "h",
    "stack",
    "horizontal",
    "stack",
    "h",
    "stack",
    "beautiful",
    "focus",
    "plain",
    "stack",
    "want",
    "look",
    "v",
    "stack",
    "quite",
    "similar",
    "going",
    "stack",
    "h",
    "stack",
    "words",
    "going",
    "see",
    "code",
    "minute",
    "also",
    "squeeze",
    "removes",
    "one",
    "dimensions",
    "going",
    "put",
    "one",
    "code",
    "dimensions",
    "tensor",
    "see",
    "looks",
    "like",
    "unsqueeze",
    "adds",
    "one",
    "dimension",
    "target",
    "tensor",
    "finally",
    "permute",
    "return",
    "view",
    "input",
    "dimensions",
    "permuted",
    "swapped",
    "certain",
    "way",
    "fair",
    "methods",
    "essentially",
    "crust",
    "main",
    "point",
    "manipulate",
    "tensors",
    "way",
    "change",
    "shape",
    "change",
    "dimension",
    "one",
    "number",
    "one",
    "issues",
    "machine",
    "learning",
    "deep",
    "learning",
    "tensor",
    "shape",
    "issues",
    "let",
    "start",
    "creating",
    "tensor",
    "look",
    "let",
    "create",
    "tensor",
    "going",
    "import",
    "torch",
    "enable",
    "us",
    "run",
    "notebook",
    "directly",
    "cell",
    "wanted",
    "instead",
    "run",
    "everything",
    "let",
    "create",
    "another",
    "x",
    "torch",
    "dot",
    "range",
    "range",
    "deprecated",
    "going",
    "add",
    "code",
    "cells",
    "scroll",
    "middle",
    "screen",
    "beautiful",
    "let",
    "make",
    "one",
    "10",
    "nice",
    "simple",
    "let",
    "look",
    "x",
    "x",
    "dot",
    "shape",
    "give",
    "us",
    "okay",
    "beautiful",
    "got",
    "numbers",
    "one",
    "nine",
    "tensor",
    "shape",
    "torch",
    "size",
    "nine",
    "let",
    "start",
    "reshape",
    "add",
    "extra",
    "dimension",
    "x",
    "reshaped",
    "equals",
    "x",
    "dot",
    "reshape",
    "key",
    "thing",
    "keep",
    "mind",
    "reshape",
    "dimensions",
    "compatible",
    "original",
    "dimensions",
    "going",
    "change",
    "shape",
    "original",
    "tensor",
    "reshape",
    "try",
    "change",
    "shape",
    "one",
    "seven",
    "work",
    "number",
    "nine",
    "well",
    "let",
    "find",
    "hey",
    "let",
    "check",
    "x",
    "reshaped",
    "look",
    "x",
    "reshaped",
    "dot",
    "shape",
    "going",
    "oh",
    "get",
    "error",
    "well",
    "telling",
    "us",
    "pie",
    "torch",
    "actually",
    "really",
    "good",
    "giving",
    "us",
    "errors",
    "going",
    "wrong",
    "one",
    "seven",
    "invalid",
    "input",
    "size",
    "nine",
    "well",
    "well",
    "trying",
    "squeeze",
    "nine",
    "elements",
    "tensor",
    "one",
    "times",
    "seven",
    "seven",
    "elements",
    "change",
    "nine",
    "get",
    "ah",
    "notice",
    "happened",
    "added",
    "single",
    "dimension",
    "see",
    "single",
    "square",
    "bracket",
    "extra",
    "shape",
    "wanted",
    "add",
    "two",
    "ca",
    "well",
    "two",
    "nine",
    "invalid",
    "input",
    "size",
    "nine",
    "two",
    "times",
    "nine",
    "trying",
    "double",
    "amount",
    "elements",
    "without",
    "double",
    "amount",
    "elements",
    "change",
    "back",
    "one",
    "happens",
    "change",
    "around",
    "nine",
    "one",
    "oh",
    "little",
    "bit",
    "different",
    "instead",
    "adding",
    "one",
    "first",
    "dimension",
    "zeroth",
    "dimension",
    "python",
    "zero",
    "indexed",
    "added",
    "first",
    "dimension",
    "giving",
    "us",
    "square",
    "bracket",
    "go",
    "back",
    "add",
    "outside",
    "put",
    "one",
    "wanted",
    "add",
    "inside",
    "put",
    "one",
    "outside",
    "got",
    "torch",
    "size",
    "nine",
    "one",
    "let",
    "try",
    "change",
    "view",
    "change",
    "view",
    "reiterate",
    "reshape",
    "compatible",
    "original",
    "size",
    "change",
    "one",
    "10",
    "size",
    "10",
    "go",
    "five",
    "two",
    "happens",
    "oh",
    "compatible",
    "five",
    "times",
    "two",
    "equals",
    "another",
    "way",
    "could",
    "make",
    "12",
    "got",
    "12",
    "elements",
    "go",
    "three",
    "four",
    "code",
    "cells",
    "taking",
    "little",
    "run",
    "go",
    "back",
    "nine",
    "got",
    "original",
    "whoops",
    "going",
    "incompatible",
    "oh",
    "another",
    "thing",
    "good",
    "getting",
    "errors",
    "fly",
    "sometimes",
    "get",
    "saved",
    "failed",
    "google",
    "colab",
    "automatic",
    "saving",
    "failed",
    "fix",
    "either",
    "keep",
    "coding",
    "keep",
    "running",
    "cells",
    "colab",
    "fix",
    "background",
    "restart",
    "notebook",
    "close",
    "open",
    "got",
    "size",
    "nine",
    "size",
    "eight",
    "sorry",
    "incompatible",
    "good",
    "seeing",
    "errors",
    "come",
    "fly",
    "rather",
    "sort",
    "telling",
    "errors",
    "seeing",
    "come",
    "trying",
    "live",
    "code",
    "going",
    "happen",
    "start",
    "use",
    "google",
    "colab",
    "subsequently",
    "forms",
    "jupyter",
    "notebooks",
    "let",
    "get",
    "view",
    "go",
    "z",
    "equals",
    "let",
    "change",
    "view",
    "view",
    "change",
    "one",
    "nine",
    "go",
    "z",
    "z",
    "dot",
    "shape",
    "ah",
    "get",
    "thing",
    "view",
    "quite",
    "similar",
    "reshape",
    "remember",
    "though",
    "view",
    "shares",
    "memory",
    "original",
    "tensor",
    "z",
    "different",
    "view",
    "z",
    "shares",
    "memory",
    "x",
    "let",
    "exemplify",
    "changing",
    "z",
    "changes",
    "x",
    "view",
    "tensor",
    "shares",
    "memory",
    "original",
    "input",
    "let",
    "change",
    "z",
    "change",
    "first",
    "element",
    "using",
    "indexing",
    "targeting",
    "one",
    "set",
    "equal",
    "five",
    "see",
    "z",
    "x",
    "equal",
    "yeah",
    "see",
    "got",
    "z",
    "first",
    "one",
    "change",
    "first",
    "element",
    "zero",
    "element",
    "five",
    "thing",
    "happens",
    "x",
    "change",
    "first",
    "element",
    "z",
    "view",
    "x",
    "first",
    "element",
    "x",
    "changes",
    "well",
    "let",
    "keep",
    "going",
    "stack",
    "tenses",
    "top",
    "see",
    "stack",
    "function",
    "torch",
    "stack",
    "tenses",
    "top",
    "see",
    "press",
    "command",
    "save",
    "maybe",
    "get",
    "fixed",
    "maybe",
    "fix",
    "oh",
    "notebook",
    "saved",
    "unless",
    "made",
    "extensive",
    "changes",
    "worried",
    "losing",
    "could",
    "download",
    "notebook",
    "file",
    "download",
    "upload",
    "collab",
    "usually",
    "click",
    "yes",
    "sort",
    "resolves",
    "yeah",
    "go",
    "changes",
    "saved",
    "beautiful",
    "troubleshooting",
    "fly",
    "like",
    "x",
    "stack",
    "let",
    "stack",
    "tenses",
    "together",
    "equals",
    "torch",
    "stack",
    "let",
    "go",
    "x",
    "x",
    "x",
    "look",
    "doc",
    "string",
    "stack",
    "get",
    "collab",
    "go",
    "documentations",
    "yeah",
    "list",
    "takes",
    "list",
    "tenses",
    "concatenates",
    "sequence",
    "tenses",
    "along",
    "new",
    "dimension",
    "define",
    "dimension",
    "dimension",
    "default",
    "zero",
    "little",
    "bit",
    "hard",
    "read",
    "tenses",
    "dim",
    "equals",
    "zero",
    "come",
    "default",
    "dimension",
    "zero",
    "let",
    "see",
    "happens",
    "play",
    "around",
    "dimension",
    "got",
    "four",
    "x",
    "first",
    "one",
    "default",
    "x",
    "stack",
    "okay",
    "wonderful",
    "stacked",
    "vertically",
    "let",
    "see",
    "happens",
    "change",
    "one",
    "oh",
    "rearranged",
    "little",
    "stack",
    "like",
    "happens",
    "change",
    "two",
    "dimension",
    "oh",
    "ca",
    "well",
    "original",
    "shape",
    "x",
    "incompatible",
    "using",
    "dimension",
    "two",
    "real",
    "way",
    "get",
    "used",
    "happens",
    "stacking",
    "top",
    "play",
    "around",
    "different",
    "values",
    "dimension",
    "dim",
    "zero",
    "dim",
    "one",
    "look",
    "little",
    "bit",
    "different",
    "top",
    "first",
    "zero",
    "index",
    "zeroth",
    "tensor",
    "two",
    "three",
    "leave",
    "default",
    "also",
    "v",
    "stack",
    "h",
    "stack",
    "leave",
    "practice",
    "think",
    "memory",
    "v",
    "stack",
    "using",
    "dimension",
    "equals",
    "zero",
    "h",
    "stack",
    "like",
    "using",
    "dimension",
    "equals",
    "one",
    "may",
    "back",
    "front",
    "correct",
    "wrong",
    "let",
    "move",
    "going",
    "look",
    "squeeze",
    "unsqueeze",
    "actually",
    "going",
    "get",
    "practice",
    "see",
    "look",
    "torch",
    "squeeze",
    "torch",
    "unsqueeze",
    "see",
    "try",
    "created",
    "tensor",
    "used",
    "reshape",
    "view",
    "used",
    "stack",
    "usage",
    "squeeze",
    "unsqueeze",
    "quite",
    "similar",
    "give",
    "go",
    "prevent",
    "video",
    "getting",
    "long",
    "together",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "issued",
    "challenge",
    "trying",
    "torch",
    "dot",
    "squeeze",
    "removes",
    "single",
    "dimensions",
    "target",
    "tensor",
    "would",
    "try",
    "well",
    "would",
    "done",
    "go",
    "torch",
    "dot",
    "squeeze",
    "see",
    "happens",
    "open",
    "documentation",
    "squeeze",
    "input",
    "dimension",
    "returns",
    "tensor",
    "dimensions",
    "input",
    "size",
    "one",
    "removed",
    "demonstrations",
    "yes",
    "wow",
    "okay",
    "could",
    "copy",
    "straight",
    "notebook",
    "copy",
    "actually",
    "encourage",
    "quite",
    "often",
    "looking",
    "new",
    "torch",
    "method",
    "used",
    "code",
    "example",
    "hand",
    "practice",
    "inputs",
    "outputs",
    "look",
    "like",
    "x",
    "input",
    "check",
    "size",
    "x",
    "squeeze",
    "x",
    "well",
    "set",
    "squeeze",
    "x",
    "check",
    "size",
    "let",
    "replicate",
    "something",
    "similar",
    "go",
    "look",
    "x",
    "reshaped",
    "remind",
    "x",
    "reshaped",
    "dot",
    "shape",
    "see",
    "x",
    "reshaped",
    "dot",
    "squeeze",
    "looks",
    "like",
    "okay",
    "happened",
    "well",
    "started",
    "two",
    "square",
    "brackets",
    "started",
    "shape",
    "one",
    "nine",
    "removes",
    "single",
    "dimensions",
    "target",
    "tensor",
    "call",
    "squeeze",
    "method",
    "x",
    "reshaped",
    "one",
    "square",
    "bracket",
    "think",
    "shape",
    "x",
    "reshaped",
    "dot",
    "squeeze",
    "going",
    "check",
    "shape",
    "nine",
    "squeeze",
    "method",
    "removes",
    "single",
    "dimensions",
    "one",
    "one",
    "nine",
    "would",
    "remove",
    "ones",
    "would",
    "end",
    "nine",
    "well",
    "let",
    "write",
    "print",
    "statements",
    "little",
    "pretty",
    "output",
    "previous",
    "tensor",
    "like",
    "form",
    "visualize",
    "visualize",
    "visualize",
    "trying",
    "get",
    "head",
    "around",
    "something",
    "print",
    "successive",
    "change",
    "see",
    "happening",
    "way",
    "go",
    "oh",
    "okay",
    "called",
    "line",
    "code",
    "yes",
    "bit",
    "tedious",
    "half",
    "dozen",
    "times",
    "fair",
    "times",
    "mean",
    "still",
    "lot",
    "time",
    "even",
    "though",
    "written",
    "thousands",
    "lines",
    "machine",
    "learning",
    "code",
    "starts",
    "become",
    "instinct",
    "start",
    "go",
    "oh",
    "okay",
    "got",
    "dimension",
    "mismatch",
    "tensors",
    "need",
    "squeeze",
    "put",
    "certain",
    "function",
    "little",
    "practice",
    "like",
    "riding",
    "bike",
    "right",
    "try",
    "saying",
    "like",
    "first",
    "start",
    "wobbly",
    "place",
    "look",
    "documentation",
    "much",
    "documentation",
    "riding",
    "bike",
    "kind",
    "keep",
    "trying",
    "style",
    "coding",
    "like",
    "adopt",
    "try",
    "first",
    "stuck",
    "go",
    "documentation",
    "look",
    "something",
    "print",
    "like",
    "quite",
    "cumbersome",
    "going",
    "give",
    "us",
    "good",
    "explanation",
    "happening",
    "previous",
    "tensor",
    "x",
    "reshaped",
    "look",
    "shape",
    "x",
    "reshaped",
    "one",
    "nine",
    "call",
    "squeeze",
    "method",
    "removes",
    "single",
    "dimensions",
    "target",
    "tensor",
    "new",
    "tensor",
    "one",
    "square",
    "bracket",
    "removed",
    "new",
    "shape",
    "single",
    "dimensions",
    "removed",
    "still",
    "original",
    "values",
    "different",
    "dimension",
    "let",
    "done",
    "unsqueeze",
    "given",
    "tensors",
    "hug",
    "squeezed",
    "single",
    "dimensions",
    "going",
    "unsqueeze",
    "going",
    "take",
    "step",
    "back",
    "let",
    "grow",
    "bit",
    "torch",
    "unsqueeze",
    "adds",
    "single",
    "dimension",
    "target",
    "tensor",
    "specific",
    "dim",
    "dimension",
    "another",
    "thing",
    "note",
    "pytorch",
    "whenever",
    "says",
    "dim",
    "dimension",
    "zeroth",
    "dimension",
    "first",
    "dimension",
    "go",
    "two",
    "three",
    "four",
    "five",
    "six",
    "et",
    "cetera",
    "tensors",
    "unlimited",
    "dimensions",
    "let",
    "go",
    "previous",
    "target",
    "excused",
    "get",
    "squeezed",
    "version",
    "tensor",
    "x",
    "squeezed",
    "go",
    "print",
    "previous",
    "shape",
    "going",
    "x",
    "squeezed",
    "dot",
    "shape",
    "going",
    "add",
    "extra",
    "dimension",
    "unsqueeze",
    "go",
    "x",
    "unsqueezed",
    "equals",
    "x",
    "squeezed",
    "tensor",
    "remove",
    "single",
    "dimension",
    "going",
    "put",
    "unsqueeze",
    "dim",
    "zeroth",
    "dimension",
    "want",
    "think",
    "going",
    "output",
    "even",
    "run",
    "code",
    "think",
    "added",
    "extra",
    "dimension",
    "zeroth",
    "dimension",
    "new",
    "shape",
    "unsqueeze",
    "tensor",
    "going",
    "going",
    "go",
    "x",
    "unsqueezed",
    "going",
    "go",
    "print",
    "get",
    "new",
    "tensor",
    "shape",
    "going",
    "x",
    "unsqueezed",
    "dot",
    "shape",
    "right",
    "let",
    "look",
    "go",
    "previous",
    "tensor",
    "squeezed",
    "version",
    "single",
    "dimension",
    "new",
    "tensor",
    "unsqueeze",
    "method",
    "dimension",
    "zero",
    "added",
    "square",
    "bracket",
    "zeroth",
    "dimension",
    "one",
    "think",
    "going",
    "happen",
    "change",
    "one",
    "single",
    "dimension",
    "going",
    "added",
    "let",
    "look",
    "ah",
    "instead",
    "adding",
    "single",
    "dimension",
    "zeroth",
    "dimension",
    "added",
    "first",
    "dimension",
    "quite",
    "confusing",
    "python",
    "zero",
    "index",
    "kind",
    "want",
    "brain",
    "telling",
    "say",
    "first",
    "really",
    "zeroth",
    "index",
    "zeroth",
    "dimension",
    "let",
    "change",
    "back",
    "zero",
    "another",
    "way",
    "exploring",
    "things",
    "every",
    "time",
    "like",
    "parameter",
    "dim",
    "equals",
    "something",
    "like",
    "could",
    "shape",
    "could",
    "size",
    "whatever",
    "try",
    "changing",
    "values",
    "encourage",
    "even",
    "write",
    "print",
    "code",
    "like",
    "done",
    "one",
    "want",
    "try",
    "permute",
    "torch",
    "dot",
    "permute",
    "rearranges",
    "dimensions",
    "target",
    "tensor",
    "specified",
    "order",
    "wanted",
    "check",
    "let",
    "get",
    "rid",
    "extra",
    "tabs",
    "torch",
    "dot",
    "permute",
    "let",
    "look",
    "one",
    "took",
    "little",
    "bit",
    "practice",
    "get",
    "used",
    "working",
    "zeroth",
    "dimensions",
    "even",
    "though",
    "seems",
    "like",
    "first",
    "one",
    "returns",
    "view",
    "okay",
    "know",
    "view",
    "shares",
    "memory",
    "original",
    "input",
    "tensor",
    "dimensions",
    "permuted",
    "permuted",
    "really",
    "know",
    "word",
    "meant",
    "mapped",
    "memory",
    "permute",
    "means",
    "rearrange",
    "dimensions",
    "example",
    "start",
    "random",
    "tensor",
    "check",
    "size",
    "torch",
    "permute",
    "going",
    "swap",
    "order",
    "dimensions",
    "second",
    "dimension",
    "first",
    "zeroth",
    "dimension",
    "middle",
    "first",
    "dimension",
    "dimension",
    "values",
    "torch",
    "random",
    "two",
    "three",
    "five",
    "two",
    "zero",
    "one",
    "changed",
    "one",
    "zero",
    "one",
    "two",
    "three",
    "two",
    "three",
    "let",
    "try",
    "something",
    "similar",
    "one",
    "common",
    "places",
    "using",
    "permute",
    "might",
    "see",
    "permute",
    "used",
    "images",
    "data",
    "specific",
    "data",
    "format",
    "kind",
    "seen",
    "little",
    "bit",
    "much",
    "original",
    "equals",
    "torch",
    "dot",
    "rand",
    "size",
    "equals",
    "image",
    "tensor",
    "go",
    "height",
    "width",
    "color",
    "channels",
    "end",
    "write",
    "height",
    "width",
    "color",
    "channels",
    "remember",
    "much",
    "going",
    "spell",
    "color",
    "australian",
    "style",
    "much",
    "deep",
    "learning",
    "turning",
    "data",
    "numerical",
    "representations",
    "quite",
    "common",
    "numerical",
    "representation",
    "image",
    "data",
    "tensor",
    "dimension",
    "height",
    "tensor",
    "dimension",
    "width",
    "tensor",
    "dimension",
    "color",
    "channels",
    "red",
    "green",
    "blue",
    "certain",
    "number",
    "red",
    "green",
    "blue",
    "creates",
    "almost",
    "color",
    "want",
    "permute",
    "permute",
    "original",
    "tensor",
    "rearrange",
    "axis",
    "dimension",
    "axis",
    "dimension",
    "kind",
    "used",
    "light",
    "tensors",
    "dim",
    "order",
    "let",
    "switch",
    "color",
    "channels",
    "first",
    "zeroth",
    "dimension",
    "instead",
    "height",
    "width",
    "color",
    "channels",
    "color",
    "channels",
    "height",
    "width",
    "would",
    "permute",
    "let",
    "give",
    "shot",
    "x",
    "permuted",
    "equals",
    "x",
    "original",
    "dot",
    "permute",
    "going",
    "take",
    "second",
    "dimension",
    "takes",
    "series",
    "dims",
    "second",
    "dimension",
    "color",
    "channels",
    "remember",
    "zero",
    "one",
    "two",
    "two",
    "want",
    "two",
    "first",
    "want",
    "height",
    "zero",
    "want",
    "width",
    "one",
    "let",
    "shifts",
    "axis",
    "zero",
    "one",
    "one",
    "two",
    "two",
    "zero",
    "order",
    "well",
    "two",
    "maps",
    "zero",
    "zero",
    "maps",
    "first",
    "index",
    "one",
    "maps",
    "index",
    "enough",
    "talk",
    "let",
    "see",
    "looks",
    "like",
    "print",
    "previous",
    "shape",
    "x",
    "original",
    "dot",
    "shape",
    "go",
    "print",
    "new",
    "shape",
    "permuted",
    "version",
    "want",
    "x",
    "permuted",
    "dot",
    "shape",
    "let",
    "see",
    "looks",
    "like",
    "wonderful",
    "exactly",
    "wanted",
    "see",
    "let",
    "write",
    "little",
    "note",
    "color",
    "channels",
    "height",
    "width",
    "data",
    "going",
    "tenses",
    "x",
    "original",
    "x",
    "permuted",
    "viewed",
    "different",
    "point",
    "view",
    "remember",
    "permute",
    "view",
    "discuss",
    "view",
    "shares",
    "memory",
    "original",
    "tensor",
    "x",
    "permuted",
    "share",
    "place",
    "memory",
    "x",
    "original",
    "even",
    "though",
    "different",
    "shape",
    "little",
    "challenge",
    "move",
    "next",
    "video",
    "move",
    "next",
    "video",
    "try",
    "change",
    "one",
    "values",
    "x",
    "original",
    "look",
    "x",
    "original",
    "see",
    "value",
    "could",
    "let",
    "get",
    "one",
    "zero",
    "zero",
    "get",
    "dimensions",
    "zero",
    "see",
    "get",
    "single",
    "value",
    "maybe",
    "oops",
    "oh",
    "need",
    "zero",
    "getting",
    "practice",
    "indexing",
    "oh",
    "zero",
    "zero",
    "zero",
    "go",
    "okay",
    "maybe",
    "set",
    "value",
    "whatever",
    "choose",
    "see",
    "changes",
    "x",
    "permuted",
    "give",
    "shot",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "squeezing",
    "unsqueezing",
    "permuting",
    "going",
    "lie",
    "concepts",
    "quite",
    "lot",
    "take",
    "aware",
    "remember",
    "working",
    "towards",
    "helping",
    "us",
    "fix",
    "shape",
    "dimension",
    "issues",
    "tensors",
    "one",
    "common",
    "issues",
    "deep",
    "learning",
    "neural",
    "networks",
    "usually",
    "little",
    "challenge",
    "changing",
    "value",
    "x",
    "original",
    "highlight",
    "fact",
    "permute",
    "returns",
    "different",
    "view",
    "original",
    "tensor",
    "view",
    "pytorch",
    "shares",
    "memory",
    "original",
    "tensor",
    "change",
    "value",
    "zero",
    "zero",
    "zero",
    "x",
    "original",
    "case",
    "728218",
    "happens",
    "value",
    "gets",
    "copied",
    "across",
    "x",
    "permuted",
    "said",
    "looked",
    "selecting",
    "data",
    "tensors",
    "using",
    "technique",
    "called",
    "indexing",
    "let",
    "rehash",
    "another",
    "thing",
    "little",
    "bit",
    "hurdle",
    "first",
    "working",
    "multi",
    "dimensional",
    "tensors",
    "let",
    "see",
    "select",
    "data",
    "tensors",
    "indexing",
    "ever",
    "done",
    "indexing",
    "indexing",
    "pytorch",
    "similar",
    "indexing",
    "numpy",
    "ever",
    "worked",
    "numpy",
    "done",
    "indexing",
    "selecting",
    "data",
    "arrays",
    "numpy",
    "uses",
    "array",
    "main",
    "data",
    "type",
    "pytorch",
    "uses",
    "tensors",
    "similar",
    "let",
    "start",
    "creating",
    "tensor",
    "going",
    "add",
    "code",
    "cells",
    "make",
    "screen",
    "right",
    "middle",
    "going",
    "import",
    "torch",
    "need",
    "import",
    "torch",
    "time",
    "run",
    "notebook",
    "later",
    "x",
    "equals",
    "torch",
    "dot",
    "let",
    "create",
    "range",
    "nice",
    "simple",
    "like",
    "work",
    "fundamentals",
    "create",
    "small",
    "range",
    "reshape",
    "reshape",
    "compatible",
    "original",
    "dimension",
    "go",
    "one",
    "three",
    "three",
    "torch",
    "range",
    "going",
    "return",
    "us",
    "nine",
    "values",
    "start",
    "end",
    "minus",
    "one",
    "one",
    "times",
    "three",
    "times",
    "three",
    "nine",
    "let",
    "look",
    "x",
    "x",
    "dot",
    "shape",
    "beautiful",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "nine",
    "size",
    "one",
    "outer",
    "bracket",
    "going",
    "contain",
    "three",
    "one",
    "one",
    "two",
    "three",
    "three",
    "one",
    "two",
    "three",
    "let",
    "work",
    "let",
    "index",
    "new",
    "tensor",
    "let",
    "see",
    "happens",
    "get",
    "x",
    "zero",
    "going",
    "index",
    "first",
    "bracket",
    "get",
    "one",
    "indexed",
    "first",
    "dimension",
    "zero",
    "dimension",
    "one",
    "get",
    "inside",
    "let",
    "try",
    "let",
    "index",
    "middle",
    "bracket",
    "dimension",
    "one",
    "got",
    "go",
    "x",
    "zero",
    "zero",
    "let",
    "see",
    "happens",
    "going",
    "x",
    "zero",
    "zero",
    "go",
    "depends",
    "want",
    "use",
    "sometimes",
    "prefer",
    "go",
    "like",
    "know",
    "getting",
    "first",
    "bracket",
    "zeroth",
    "version",
    "first",
    "bracket",
    "three",
    "values",
    "think",
    "going",
    "happen",
    "index",
    "third",
    "dimension",
    "second",
    "dimension",
    "well",
    "let",
    "find",
    "let",
    "index",
    "bracket",
    "last",
    "dimension",
    "x",
    "zero",
    "zero",
    "zero",
    "numbers",
    "going",
    "give",
    "us",
    "back",
    "x",
    "zero",
    "zero",
    "dimension",
    "gives",
    "us",
    "back",
    "middle",
    "tensor",
    "x",
    "zero",
    "zero",
    "gives",
    "us",
    "back",
    "zeroth",
    "index",
    "middle",
    "tensor",
    "go",
    "x",
    "zero",
    "zero",
    "zero",
    "going",
    "give",
    "us",
    "zeroth",
    "tensor",
    "zeroth",
    "index",
    "zeroth",
    "element",
    "lot",
    "take",
    "done",
    "broken",
    "step",
    "step",
    "got",
    "first",
    "zero",
    "targets",
    "outer",
    "bracket",
    "returns",
    "us",
    "zero",
    "zero",
    "targets",
    "first",
    "first",
    "zero",
    "zero",
    "targets",
    "go",
    "zero",
    "zero",
    "zero",
    "target",
    "target",
    "get",
    "back",
    "getting",
    "zeroth",
    "index",
    "change",
    "one",
    "get",
    "back",
    "two",
    "change",
    "one",
    "get",
    "bit",
    "trivia",
    "challenge",
    "going",
    "one",
    "one",
    "one",
    "let",
    "see",
    "happens",
    "oh",
    "catch",
    "ran",
    "code",
    "one",
    "quite",
    "quickly",
    "index",
    "one",
    "bounds",
    "well",
    "dimension",
    "one",
    "index",
    "zero",
    "gets",
    "little",
    "bit",
    "confusing",
    "says",
    "one",
    "got",
    "zero",
    "dimension",
    "index",
    "zero",
    "mention",
    "011",
    "give",
    "us",
    "five",
    "beautiful",
    "like",
    "issue",
    "challenge",
    "getting",
    "number",
    "nine",
    "would",
    "get",
    "number",
    "nine",
    "rearrange",
    "code",
    "get",
    "number",
    "nine",
    "challenge",
    "want",
    "show",
    "well",
    "use",
    "also",
    "use",
    "might",
    "see",
    "semicolon",
    "select",
    "target",
    "dimension",
    "let",
    "say",
    "wanted",
    "get",
    "zeroth",
    "dimension",
    "zero",
    "element",
    "get",
    "let",
    "say",
    "want",
    "say",
    "get",
    "values",
    "zeroth",
    "first",
    "dimensions",
    "index",
    "one",
    "second",
    "dimension",
    "oh",
    "mouthful",
    "get",
    "values",
    "zeroth",
    "first",
    "dimensions",
    "index",
    "one",
    "second",
    "dimension",
    "let",
    "break",
    "step",
    "step",
    "want",
    "values",
    "zeroth",
    "first",
    "dimensions",
    "index",
    "one",
    "second",
    "dimension",
    "press",
    "enter",
    "shift",
    "enter",
    "get",
    "okay",
    "got",
    "elements",
    "zeroth",
    "first",
    "dimension",
    "return",
    "us",
    "thing",
    "want",
    "258",
    "first",
    "element",
    "second",
    "dimension",
    "three",
    "quite",
    "confusing",
    "practice",
    "figure",
    "select",
    "almost",
    "numbers",
    "want",
    "kind",
    "tensor",
    "let",
    "try",
    "get",
    "values",
    "zero",
    "dimension",
    "one",
    "index",
    "value",
    "first",
    "second",
    "dimension",
    "might",
    "look",
    "like",
    "let",
    "break",
    "come",
    "x",
    "going",
    "go",
    "values",
    "zero",
    "dimension",
    "zero",
    "comes",
    "first",
    "want",
    "one",
    "index",
    "value",
    "first",
    "one",
    "index",
    "value",
    "second",
    "going",
    "give",
    "us",
    "five",
    "oh",
    "selected",
    "middle",
    "tensor",
    "really",
    "line",
    "code",
    "exactly",
    "line",
    "code",
    "except",
    "got",
    "square",
    "brackets",
    "outside",
    "got",
    "semicolon",
    "change",
    "zero",
    "remove",
    "got",
    "semicolon",
    "selected",
    "dimensions",
    "get",
    "back",
    "square",
    "bracket",
    "something",
    "keep",
    "mind",
    "finally",
    "let",
    "go",
    "one",
    "get",
    "index",
    "zero",
    "zero",
    "first",
    "dimension",
    "values",
    "second",
    "dimension",
    "x",
    "zero",
    "zero",
    "zero",
    "index",
    "zero",
    "first",
    "dimension",
    "zero",
    "zero",
    "values",
    "second",
    "dimension",
    "done",
    "got",
    "tensor",
    "one",
    "two",
    "three",
    "lovely",
    "code",
    "equivalent",
    "done",
    "semicolon",
    "end",
    "line",
    "explicitly",
    "says",
    "without",
    "semicolon",
    "hey",
    "give",
    "us",
    "values",
    "remaining",
    "dimension",
    "challenge",
    "take",
    "tensor",
    "got",
    "index",
    "return",
    "nine",
    "write",
    "index",
    "x",
    "return",
    "nine",
    "look",
    "x",
    "well",
    "index",
    "x",
    "return",
    "three",
    "six",
    "nine",
    "values",
    "give",
    "go",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "go",
    "give",
    "challenge",
    "ago",
    "finished",
    "last",
    "video",
    "issuing",
    "challenge",
    "index",
    "x",
    "return",
    "nine",
    "index",
    "x",
    "return",
    "three",
    "six",
    "nine",
    "came",
    "different",
    "ways",
    "could",
    "approach",
    "found",
    "x",
    "one",
    "three",
    "three",
    "size",
    "well",
    "dimensions",
    "want",
    "select",
    "nine",
    "need",
    "zero",
    "first",
    "outer",
    "bracket",
    "get",
    "elements",
    "need",
    "two",
    "select",
    "bottom",
    "one",
    "need",
    "final",
    "two",
    "select",
    "second",
    "dimension",
    "bottom",
    "one",
    "three",
    "six",
    "nine",
    "need",
    "elements",
    "first",
    "dimension",
    "zeroth",
    "dimension",
    "elements",
    "first",
    "dimension",
    "get",
    "two",
    "three",
    "six",
    "nine",
    "set",
    "would",
    "practice",
    "indexing",
    "start",
    "whatever",
    "shape",
    "tensor",
    "like",
    "create",
    "something",
    "like",
    "see",
    "write",
    "different",
    "indexing",
    "select",
    "whatever",
    "number",
    "pick",
    "let",
    "move",
    "next",
    "part",
    "pytorch",
    "tensors",
    "numpy",
    "numpy",
    "popular",
    "scientific",
    "popular",
    "pytorch",
    "actually",
    "requires",
    "numpy",
    "install",
    "pytorch",
    "popular",
    "scientific",
    "python",
    "numerical",
    "computing",
    "library",
    "bit",
    "mouthful",
    "pytorch",
    "functionality",
    "interact",
    "quite",
    "often",
    "might",
    "start",
    "let",
    "change",
    "markdown",
    "might",
    "start",
    "data",
    "numerical",
    "format",
    "might",
    "start",
    "data",
    "numpy",
    "numpy",
    "array",
    "want",
    "pytorch",
    "tensor",
    "data",
    "might",
    "represented",
    "numpy",
    "started",
    "numpy",
    "say",
    "want",
    "deep",
    "learning",
    "want",
    "leverage",
    "pytorch",
    "deep",
    "learning",
    "capabilities",
    "well",
    "might",
    "want",
    "change",
    "data",
    "numpy",
    "pytorch",
    "tensor",
    "pytorch",
    "method",
    "torch",
    "numpy",
    "take",
    "nd",
    "array",
    "numpy",
    "main",
    "data",
    "type",
    "change",
    "torch",
    "tensor",
    "see",
    "second",
    "want",
    "go",
    "pytorch",
    "tensor",
    "numpy",
    "want",
    "use",
    "sort",
    "numpy",
    "method",
    "well",
    "method",
    "torch",
    "dot",
    "tensor",
    "call",
    "dot",
    "numpy",
    "talking",
    "words",
    "let",
    "see",
    "action",
    "numpy",
    "array",
    "tensor",
    "let",
    "try",
    "first",
    "import",
    "torch",
    "run",
    "cell",
    "import",
    "numpy",
    "np",
    "common",
    "naming",
    "convention",
    "numpy",
    "going",
    "create",
    "array",
    "numpy",
    "going",
    "put",
    "one",
    "eight",
    "range",
    "going",
    "go",
    "tensor",
    "equals",
    "torch",
    "numpy",
    "want",
    "go",
    "numpy",
    "array",
    "torch",
    "tensor",
    "use",
    "numpy",
    "pass",
    "array",
    "array",
    "tensor",
    "wonderful",
    "numpy",
    "array",
    "torch",
    "tensor",
    "data",
    "might",
    "notice",
    "type",
    "tensor",
    "torch",
    "dot",
    "float",
    "numpy",
    "default",
    "data",
    "type",
    "oh",
    "type",
    "float",
    "whereas",
    "tensor",
    "discussed",
    "pytorch",
    "default",
    "data",
    "type",
    "float",
    "well",
    "pytorch",
    "default",
    "data",
    "type",
    "create",
    "torch",
    "range",
    "default",
    "pytorch",
    "going",
    "create",
    "float",
    "aware",
    "going",
    "numpy",
    "pytorch",
    "default",
    "numpy",
    "data",
    "type",
    "float",
    "pytorch",
    "reflects",
    "data",
    "type",
    "use",
    "numpy",
    "method",
    "wonder",
    "type",
    "go",
    "type",
    "equals",
    "torch",
    "dot",
    "float",
    "32",
    "takes",
    "keyword",
    "okay",
    "could",
    "change",
    "data",
    "type",
    "well",
    "could",
    "go",
    "type",
    "torch",
    "float",
    "yeah",
    "give",
    "us",
    "tensor",
    "type",
    "float",
    "32",
    "instead",
    "float",
    "beautiful",
    "keep",
    "know",
    "warning",
    "converting",
    "numpy",
    "pytorch",
    "pytorch",
    "reflects",
    "numpy",
    "default",
    "data",
    "type",
    "float",
    "64",
    "unless",
    "specified",
    "otherwise",
    "discussed",
    "trying",
    "perform",
    "certain",
    "calculations",
    "might",
    "run",
    "data",
    "type",
    "issue",
    "might",
    "need",
    "convert",
    "type",
    "float",
    "64",
    "float",
    "let",
    "see",
    "happens",
    "think",
    "happen",
    "change",
    "array",
    "change",
    "value",
    "array",
    "well",
    "let",
    "find",
    "change",
    "value",
    "array",
    "question",
    "tensor",
    "used",
    "numpy",
    "method",
    "think",
    "change",
    "array",
    "tensor",
    "change",
    "let",
    "try",
    "array",
    "equals",
    "array",
    "plus",
    "one",
    "adding",
    "one",
    "every",
    "value",
    "array",
    "array",
    "tensor",
    "going",
    "look",
    "like",
    "uh",
    "huh",
    "array",
    "change",
    "first",
    "value",
    "oh",
    "sorry",
    "change",
    "every",
    "value",
    "one",
    "seven",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "change",
    "value",
    "array",
    "change",
    "value",
    "tensor",
    "something",
    "keep",
    "mind",
    "use",
    "numpy",
    "get",
    "new",
    "tensor",
    "memory",
    "original",
    "new",
    "tensor",
    "change",
    "change",
    "original",
    "array",
    "let",
    "go",
    "tensor",
    "numpy",
    "wanted",
    "go",
    "back",
    "numpy",
    "tensor",
    "numpy",
    "array",
    "start",
    "tensor",
    "could",
    "use",
    "one",
    "right",
    "going",
    "create",
    "another",
    "one",
    "create",
    "one",
    "ones",
    "fun",
    "one",
    "rhymes",
    "fun",
    "numpy",
    "tensor",
    "equals",
    "go",
    "numpy",
    "well",
    "torch",
    "dot",
    "tensor",
    "dot",
    "numpy",
    "simply",
    "call",
    "numpy",
    "tensor",
    "numpy",
    "tensor",
    "data",
    "type",
    "think",
    "numpy",
    "tensor",
    "going",
    "returned",
    "numpy",
    "pi",
    "torches",
    "default",
    "data",
    "type",
    "flight",
    "change",
    "numpy",
    "going",
    "type",
    "numpy",
    "tensor",
    "numpy",
    "tensor",
    "dot",
    "type",
    "reflects",
    "original",
    "type",
    "set",
    "tensor",
    "keep",
    "mind",
    "going",
    "pytorch",
    "numpy",
    "default",
    "data",
    "type",
    "numpy",
    "float",
    "64",
    "whereas",
    "default",
    "data",
    "type",
    "pytorch",
    "float",
    "may",
    "cause",
    "errors",
    "different",
    "kinds",
    "calculations",
    "think",
    "going",
    "happen",
    "went",
    "tensor",
    "array",
    "change",
    "tensor",
    "change",
    "tensor",
    "happens",
    "numpy",
    "tensor",
    "get",
    "tensor",
    "equals",
    "tensor",
    "plus",
    "one",
    "go",
    "numpy",
    "tensor",
    "oh",
    "get",
    "tensor",
    "well",
    "tensor",
    "twos",
    "added",
    "one",
    "ones",
    "numpy",
    "tensor",
    "remains",
    "remains",
    "unchanged",
    "means",
    "share",
    "memory",
    "go",
    "pytorch",
    "numpy",
    "like",
    "look",
    "encourage",
    "go",
    "pytorch",
    "numpy",
    "warm",
    "numpy",
    "beginner",
    "fair",
    "tutorials",
    "pytorch",
    "numpy",
    "prevalent",
    "work",
    "pretty",
    "well",
    "together",
    "look",
    "lot",
    "going",
    "links",
    "encourage",
    "check",
    "covered",
    "main",
    "ones",
    "see",
    "practice",
    "said",
    "let",
    "jump",
    "next",
    "video",
    "going",
    "look",
    "concept",
    "reproducibility",
    "like",
    "look",
    "encourage",
    "search",
    "pytorch",
    "reproducibility",
    "see",
    "find",
    "otherwise",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "time",
    "us",
    "cover",
    "topic",
    "reproducibility",
    "could",
    "even",
    "spell",
    "would",
    "fantastic",
    "reproducibility",
    "trying",
    "take",
    "random",
    "random",
    "touched",
    "upon",
    "concept",
    "neural",
    "networks",
    "harnessing",
    "power",
    "randomness",
    "mean",
    "actually",
    "built",
    "neural",
    "network",
    "yet",
    "created",
    "tenses",
    "full",
    "random",
    "values",
    "short",
    "neural",
    "network",
    "learns",
    "start",
    "random",
    "numbers",
    "perform",
    "tensor",
    "operations",
    "update",
    "random",
    "numbers",
    "try",
    "make",
    "better",
    "representations",
    "data",
    "however",
    "trying",
    "reproducible",
    "experiments",
    "sometimes",
    "want",
    "much",
    "randomness",
    "mean",
    "creating",
    "random",
    "tensors",
    "seen",
    "far",
    "every",
    "time",
    "create",
    "random",
    "tensor",
    "let",
    "create",
    "one",
    "torch",
    "dot",
    "rand",
    "create",
    "three",
    "three",
    "every",
    "time",
    "run",
    "cell",
    "gives",
    "us",
    "new",
    "numbers",
    "7",
    "7",
    "5",
    "go",
    "rand",
    "right",
    "get",
    "whole",
    "bunch",
    "random",
    "numbers",
    "every",
    "single",
    "time",
    "trying",
    "share",
    "notebook",
    "friend",
    "say",
    "went",
    "share",
    "clicked",
    "share",
    "link",
    "sent",
    "someone",
    "like",
    "hey",
    "try",
    "machine",
    "learning",
    "experiment",
    "wanted",
    "little",
    "less",
    "randomness",
    "neural",
    "networks",
    "start",
    "random",
    "numbers",
    "might",
    "well",
    "let",
    "write",
    "reduce",
    "randomness",
    "neural",
    "networks",
    "pytorch",
    "comes",
    "concept",
    "random",
    "seed",
    "going",
    "see",
    "action",
    "essentially",
    "let",
    "write",
    "essentially",
    "random",
    "seed",
    "flavor",
    "randomness",
    "computers",
    "work",
    "actually",
    "true",
    "randomness",
    "actually",
    "arguments",
    "quite",
    "big",
    "debate",
    "computer",
    "science",
    "topic",
    "whatnot",
    "computer",
    "scientist",
    "machine",
    "learning",
    "engineer",
    "computers",
    "fundamentally",
    "deterministic",
    "means",
    "run",
    "steps",
    "randomness",
    "referred",
    "pseudo",
    "randomness",
    "generated",
    "randomness",
    "random",
    "seed",
    "see",
    "lot",
    "machine",
    "learning",
    "experiments",
    "flavors",
    "randomness",
    "let",
    "see",
    "practice",
    "end",
    "video",
    "give",
    "two",
    "resources",
    "recommend",
    "learn",
    "little",
    "bit",
    "concept",
    "pseudo",
    "randomness",
    "reproducibility",
    "pytorch",
    "let",
    "start",
    "importing",
    "torch",
    "could",
    "start",
    "notebook",
    "right",
    "create",
    "two",
    "random",
    "tensors",
    "call",
    "random",
    "tensor",
    "equals",
    "torch",
    "dot",
    "rand",
    "go",
    "three",
    "four",
    "go",
    "random",
    "tensor",
    "b",
    "equals",
    "torch",
    "dot",
    "rand",
    "size",
    "three",
    "four",
    "look",
    "let",
    "go",
    "print",
    "random",
    "tensor",
    "print",
    "random",
    "tensor",
    "let",
    "print",
    "see",
    "equal",
    "anywhere",
    "random",
    "tensor",
    "equals",
    "equals",
    "equals",
    "random",
    "tensor",
    "think",
    "going",
    "look",
    "one",
    "equals",
    "one",
    "return",
    "true",
    "comparison",
    "operator",
    "compare",
    "two",
    "different",
    "tensors",
    "creating",
    "two",
    "random",
    "tensors",
    "going",
    "look",
    "expect",
    "full",
    "random",
    "values",
    "think",
    "values",
    "random",
    "tensors",
    "going",
    "equal",
    "well",
    "chance",
    "highly",
    "unlikely",
    "quite",
    "surprised",
    "oh",
    "connection",
    "might",
    "little",
    "bit",
    "oh",
    "go",
    "beautiful",
    "tensor",
    "tensor",
    "three",
    "four",
    "random",
    "numbers",
    "tensor",
    "b",
    "three",
    "four",
    "random",
    "numbers",
    "share",
    "notebook",
    "friend",
    "colleague",
    "even",
    "ran",
    "cell",
    "going",
    "get",
    "random",
    "numbers",
    "well",
    "every",
    "chance",
    "replicating",
    "one",
    "numbers",
    "highly",
    "unlikely",
    "getting",
    "automatic",
    "save",
    "failed",
    "might",
    "get",
    "internet",
    "connection",
    "dropping",
    "maybe",
    "something",
    "going",
    "internet",
    "connection",
    "seen",
    "usually",
    "resolves",
    "try",
    "times",
    "keep",
    "coding",
    "really",
    "resolve",
    "go",
    "file",
    "download",
    "notebook",
    "save",
    "copy",
    "drive",
    "download",
    "download",
    "notebook",
    "save",
    "local",
    "machine",
    "upload",
    "upload",
    "notebook",
    "start",
    "another",
    "google",
    "colab",
    "instance",
    "go",
    "fixed",
    "wonderful",
    "troubleshooting",
    "fly",
    "way",
    "make",
    "reproducible",
    "concept",
    "random",
    "seed",
    "let",
    "look",
    "let",
    "make",
    "random",
    "reproducible",
    "tenses",
    "import",
    "torch",
    "going",
    "set",
    "random",
    "seed",
    "going",
    "torch",
    "dot",
    "manual",
    "seed",
    "random",
    "oh",
    "random",
    "set",
    "yet",
    "going",
    "set",
    "random",
    "seed",
    "set",
    "random",
    "seed",
    "numerical",
    "value",
    "42",
    "common",
    "one",
    "might",
    "see",
    "zero",
    "might",
    "see",
    "one",
    "two",
    "three",
    "four",
    "essentially",
    "set",
    "whatever",
    "want",
    "think",
    "77",
    "100",
    "different",
    "flavors",
    "randomness",
    "like",
    "use",
    "42",
    "answer",
    "universe",
    "go",
    "random",
    "seed",
    "let",
    "create",
    "random",
    "tenses",
    "random",
    "tensor",
    "c",
    "flavor",
    "random",
    "seed",
    "three",
    "four",
    "going",
    "go",
    "torch",
    "tensor",
    "equals",
    "torch",
    "dot",
    "rand",
    "three",
    "four",
    "let",
    "see",
    "happens",
    "print",
    "random",
    "tensor",
    "print",
    "random",
    "tensor",
    "print",
    "see",
    "equal",
    "anywhere",
    "random",
    "tensor",
    "c",
    "equals",
    "random",
    "tensor",
    "let",
    "find",
    "happens",
    "huh",
    "gives",
    "well",
    "got",
    "randomness",
    "set",
    "random",
    "seed",
    "telling",
    "pytorch",
    "flavor",
    "randomness",
    "42",
    "torch",
    "manual",
    "seed",
    "hmm",
    "let",
    "try",
    "set",
    "manual",
    "seed",
    "time",
    "call",
    "random",
    "method",
    "go",
    "ah",
    "much",
    "better",
    "got",
    "flavored",
    "randomness",
    "thing",
    "keep",
    "mind",
    "want",
    "use",
    "torch",
    "manual",
    "seed",
    "generally",
    "works",
    "one",
    "block",
    "code",
    "using",
    "notebook",
    "something",
    "keep",
    "mind",
    "creating",
    "random",
    "tensors",
    "one",
    "using",
    "assignment",
    "like",
    "use",
    "torch",
    "dot",
    "manual",
    "seed",
    "every",
    "time",
    "want",
    "call",
    "rand",
    "method",
    "sort",
    "randomness",
    "however",
    "using",
    "torch",
    "processes",
    "usually",
    "might",
    "see",
    "torch",
    "manual",
    "seed",
    "set",
    "right",
    "start",
    "cell",
    "whole",
    "bunch",
    "code",
    "done",
    "calling",
    "subsequent",
    "methods",
    "reset",
    "random",
    "seed",
    "otherwise",
    "comment",
    "line",
    "going",
    "flavor",
    "randomness",
    "torch",
    "random",
    "tensor",
    "c",
    "torch",
    "manual",
    "seed",
    "random",
    "tensor",
    "going",
    "flavor",
    "going",
    "use",
    "random",
    "seed",
    "reset",
    "wonderful",
    "wonder",
    "seed",
    "method",
    "let",
    "go",
    "torch",
    "dot",
    "rand",
    "seed",
    "sometimes",
    "seed",
    "method",
    "seed",
    "okay",
    "right",
    "learn",
    "documentation",
    "torch",
    "dot",
    "rand",
    "said",
    "going",
    "link",
    "end",
    "video",
    "manual",
    "seed",
    "way",
    "random",
    "seed",
    "torch",
    "called",
    "manual",
    "seed",
    "way",
    "flavor",
    "randomness",
    "numbers",
    "see",
    "still",
    "quite",
    "random",
    "random",
    "seed",
    "makes",
    "reproducible",
    "share",
    "run",
    "block",
    "code",
    "ideally",
    "going",
    "get",
    "numerical",
    "output",
    "said",
    "like",
    "refer",
    "pie",
    "torch",
    "reproducibility",
    "document",
    "quite",
    "scratched",
    "surface",
    "reproducibility",
    "covered",
    "one",
    "main",
    "ones",
    "great",
    "document",
    "go",
    "reproducibility",
    "pie",
    "torch",
    "extra",
    "curriculum",
    "even",
    "understand",
    "going",
    "lot",
    "code",
    "aware",
    "reproducibility",
    "important",
    "topic",
    "machine",
    "learning",
    "deep",
    "learning",
    "put",
    "extra",
    "resources",
    "reproducibility",
    "go",
    "pie",
    "torch",
    "randomness",
    "change",
    "markdown",
    "finally",
    "concept",
    "random",
    "seed",
    "wikipedia",
    "random",
    "seed",
    "random",
    "seeds",
    "quite",
    "universal",
    "concept",
    "pie",
    "torch",
    "random",
    "seed",
    "numpy",
    "well",
    "like",
    "see",
    "means",
    "yeah",
    "initialize",
    "pseudo",
    "random",
    "number",
    "generator",
    "big",
    "word",
    "pseudo",
    "random",
    "number",
    "generator",
    "like",
    "learn",
    "random",
    "number",
    "generation",
    "computing",
    "random",
    "seed",
    "refer",
    "check",
    "documentation",
    "whoo",
    "far",
    "covered",
    "lot",
    "couple",
    "topics",
    "really",
    "aware",
    "finish",
    "pie",
    "torch",
    "fundamentals",
    "got",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "let",
    "talk",
    "important",
    "concept",
    "running",
    "tenses",
    "pie",
    "torch",
    "objects",
    "running",
    "tenses",
    "pie",
    "torch",
    "objects",
    "gpus",
    "making",
    "faster",
    "computations",
    "discussed",
    "gpus",
    "let",
    "scroll",
    "little",
    "bit",
    "gpus",
    "equal",
    "faster",
    "computation",
    "numbers",
    "thanks",
    "cuda",
    "plus",
    "nvidia",
    "hardware",
    "plus",
    "pie",
    "torch",
    "working",
    "behind",
    "scenes",
    "make",
    "everything",
    "hunky",
    "dory",
    "good",
    "hunky",
    "dory",
    "means",
    "way",
    "never",
    "heard",
    "let",
    "look",
    "first",
    "need",
    "talk",
    "let",
    "go",
    "one",
    "getting",
    "gpu",
    "different",
    "ways",
    "seen",
    "one",
    "number",
    "one",
    "easiest",
    "use",
    "using",
    "right",
    "use",
    "google",
    "colab",
    "free",
    "gpu",
    "also",
    "google",
    "colab",
    "pro",
    "think",
    "might",
    "even",
    "let",
    "look",
    "google",
    "colab",
    "pro",
    "choose",
    "best",
    "right",
    "use",
    "google",
    "colab",
    "pro",
    "use",
    "almost",
    "every",
    "day",
    "yeah",
    "pay",
    "colab",
    "pro",
    "use",
    "colab",
    "free",
    "might",
    "using",
    "also",
    "colab",
    "pro",
    "plus",
    "lot",
    "advantages",
    "well",
    "colab",
    "pro",
    "giving",
    "faster",
    "gpus",
    "access",
    "faster",
    "gpus",
    "means",
    "spend",
    "less",
    "time",
    "waiting",
    "code",
    "running",
    "memory",
    "longer",
    "run",
    "time",
    "last",
    "bit",
    "longer",
    "leave",
    "running",
    "idle",
    "colab",
    "pro",
    "step",
    "personally",
    "need",
    "yet",
    "use",
    "google",
    "colab",
    "pro",
    "plus",
    "complete",
    "whole",
    "course",
    "free",
    "tier",
    "well",
    "start",
    "code",
    "start",
    "run",
    "bigger",
    "models",
    "start",
    "want",
    "compute",
    "might",
    "want",
    "look",
    "something",
    "like",
    "google",
    "colab",
    "pro",
    "let",
    "go",
    "options",
    "upgrade",
    "well",
    "another",
    "way",
    "use",
    "gpu",
    "takes",
    "little",
    "bit",
    "setup",
    "requires",
    "investment",
    "purchasing",
    "gpu",
    "lots",
    "options",
    "one",
    "favorite",
    "posts",
    "getting",
    "gpu",
    "yeah",
    "best",
    "gpus",
    "deep",
    "learning",
    "2020",
    "something",
    "like",
    "got",
    "deep",
    "learning",
    "tim",
    "detmos",
    "yeah",
    "gpus",
    "get",
    "deep",
    "learning",
    "believe",
    "time",
    "video",
    "think",
    "updated",
    "since",
    "date",
    "take",
    "word",
    "fantastic",
    "blog",
    "post",
    "figuring",
    "gpus",
    "see",
    "post",
    "option",
    "get",
    "number",
    "three",
    "use",
    "cloud",
    "computing",
    "gcp",
    "google",
    "cloud",
    "platform",
    "aws",
    "amazon",
    "web",
    "services",
    "azure",
    "services",
    "azure",
    "microsoft",
    "allow",
    "rent",
    "computers",
    "cloud",
    "access",
    "first",
    "option",
    "using",
    "google",
    "colab",
    "using",
    "far",
    "easiest",
    "free",
    "big",
    "advantages",
    "however",
    "downside",
    "use",
    "website",
    "google",
    "colab",
    "ca",
    "run",
    "locally",
    "get",
    "benefit",
    "using",
    "cloud",
    "computing",
    "personal",
    "workflow",
    "run",
    "basically",
    "small",
    "scale",
    "experiments",
    "things",
    "like",
    "learning",
    "new",
    "stuff",
    "google",
    "colab",
    "want",
    "upgrade",
    "things",
    "run",
    "video",
    "experiments",
    "dedicated",
    "deep",
    "learning",
    "pc",
    "built",
    "big",
    "powerful",
    "gpu",
    "also",
    "use",
    "cloud",
    "computing",
    "necessary",
    "workflow",
    "start",
    "google",
    "colab",
    "two",
    "need",
    "larger",
    "experiments",
    "beginning",
    "course",
    "stick",
    "google",
    "colab",
    "time",
    "thought",
    "make",
    "aware",
    "two",
    "options",
    "like",
    "set",
    "gpu",
    "four",
    "two",
    "three",
    "pytorch",
    "plus",
    "gpu",
    "drivers",
    "cuda",
    "takes",
    "little",
    "bit",
    "setting",
    "refer",
    "pytorch",
    "setup",
    "documentation",
    "go",
    "great",
    "setup",
    "guides",
    "get",
    "started",
    "start",
    "locally",
    "want",
    "run",
    "local",
    "machine",
    "linux",
    "setup",
    "linux",
    "cuda",
    "going",
    "give",
    "conda",
    "install",
    "command",
    "use",
    "conda",
    "want",
    "use",
    "cloud",
    "partners",
    "alibaba",
    "cloud",
    "amazon",
    "web",
    "services",
    "google",
    "cloud",
    "platform",
    "want",
    "go",
    "link",
    "course",
    "going",
    "focusing",
    "using",
    "google",
    "colab",
    "let",
    "see",
    "might",
    "get",
    "gpu",
    "google",
    "colab",
    "already",
    "covered",
    "going",
    "recover",
    "know",
    "going",
    "change",
    "runtime",
    "type",
    "go",
    "notebook",
    "runtime",
    "type",
    "hardware",
    "accelerator",
    "select",
    "gpu",
    "click",
    "save",
    "going",
    "restart",
    "runtime",
    "connect",
    "us",
    "runtime",
    "aka",
    "google",
    "compute",
    "instance",
    "gpu",
    "run",
    "nvidia",
    "smi",
    "tesla",
    "p100",
    "gpu",
    "let",
    "look",
    "tesla",
    "p100",
    "gpu",
    "image",
    "yeah",
    "gpu",
    "got",
    "running",
    "tesla",
    "car",
    "gpu",
    "quite",
    "powerful",
    "gpu",
    "upgraded",
    "colab",
    "pro",
    "using",
    "colab",
    "pro",
    "might",
    "get",
    "something",
    "like",
    "tesla",
    "k80",
    "slightly",
    "less",
    "powerful",
    "gpu",
    "tesla",
    "p100",
    "still",
    "gpu",
    "nonetheless",
    "still",
    "work",
    "faster",
    "running",
    "pytorch",
    "code",
    "pure",
    "cpu",
    "default",
    "google",
    "colab",
    "default",
    "pytorch",
    "also",
    "check",
    "see",
    "gpu",
    "access",
    "pytorch",
    "let",
    "go",
    "number",
    "two",
    "check",
    "gpu",
    "access",
    "pytorch",
    "little",
    "command",
    "going",
    "allow",
    "us",
    "tell",
    "us",
    "pytorch",
    "gpu",
    "way",
    "another",
    "thing",
    "colab",
    "good",
    "setup",
    "connections",
    "pytorch",
    "nvidia",
    "gpu",
    "set",
    "us",
    "whereas",
    "set",
    "gpu",
    "using",
    "cloud",
    "computing",
    "steps",
    "go",
    "going",
    "cover",
    "course",
    "highly",
    "recommend",
    "go",
    "getting",
    "started",
    "locally",
    "set",
    "want",
    "connect",
    "pytorch",
    "gpu",
    "let",
    "check",
    "gpu",
    "access",
    "pytorch",
    "another",
    "advantage",
    "using",
    "google",
    "colab",
    "almost",
    "zero",
    "set",
    "get",
    "started",
    "import",
    "torch",
    "going",
    "go",
    "torch",
    "dot",
    "cuda",
    "dot",
    "available",
    "remember",
    "cuda",
    "nvidia",
    "programming",
    "interface",
    "allows",
    "us",
    "use",
    "gpus",
    "numerical",
    "computing",
    "go",
    "beautiful",
    "big",
    "advantage",
    "google",
    "colab",
    "get",
    "access",
    "free",
    "gpu",
    "case",
    "paying",
    "faster",
    "gpu",
    "case",
    "welcome",
    "use",
    "free",
    "version",
    "means",
    "slightly",
    "slower",
    "faster",
    "gpu",
    "access",
    "gpus",
    "pytorch",
    "one",
    "thing",
    "known",
    "device",
    "agnostic",
    "code",
    "set",
    "device",
    "agnostic",
    "code",
    "important",
    "concept",
    "pytorch",
    "wherever",
    "run",
    "pytorch",
    "might",
    "always",
    "access",
    "gpu",
    "access",
    "gpu",
    "like",
    "use",
    "available",
    "one",
    "ways",
    "done",
    "pytorch",
    "set",
    "device",
    "variable",
    "really",
    "could",
    "set",
    "variable",
    "want",
    "going",
    "see",
    "used",
    "device",
    "quite",
    "often",
    "cuda",
    "torch",
    "dot",
    "cuda",
    "available",
    "else",
    "cpu",
    "going",
    "say",
    "see",
    "use",
    "device",
    "variable",
    "later",
    "set",
    "device",
    "use",
    "cuda",
    "available",
    "true",
    "available",
    "access",
    "gpu",
    "pytorch",
    "use",
    "default",
    "cpu",
    "said",
    "one",
    "thing",
    "also",
    "count",
    "number",
    "gpus",
    "wo",
    "really",
    "apply",
    "us",
    "going",
    "stick",
    "using",
    "one",
    "gpu",
    "upgrade",
    "pytorch",
    "experiments",
    "machine",
    "learning",
    "experiments",
    "might",
    "access",
    "one",
    "gpu",
    "also",
    "count",
    "devices",
    "access",
    "one",
    "gpu",
    "reason",
    "might",
    "want",
    "count",
    "number",
    "devices",
    "running",
    "huge",
    "models",
    "large",
    "data",
    "sets",
    "might",
    "want",
    "run",
    "one",
    "model",
    "certain",
    "gpu",
    "another",
    "model",
    "another",
    "gpu",
    "final",
    "thing",
    "finish",
    "video",
    "go",
    "pytorch",
    "device",
    "agnostic",
    "code",
    "cuda",
    "semantics",
    "little",
    "section",
    "called",
    "best",
    "practices",
    "basically",
    "covered",
    "setting",
    "device",
    "argument",
    "using",
    "arg",
    "pass",
    "yeah",
    "go",
    "cuda",
    "cpu",
    "one",
    "way",
    "set",
    "python",
    "arguments",
    "running",
    "scripts",
    "using",
    "version",
    "running",
    "notebook",
    "check",
    "link",
    "device",
    "agnostic",
    "code",
    "okay",
    "sure",
    "going",
    "going",
    "cover",
    "little",
    "bit",
    "later",
    "throughout",
    "course",
    "right",
    "pytorch",
    "since",
    "capable",
    "running",
    "compute",
    "gpu",
    "cpu",
    "best",
    "practice",
    "set",
    "device",
    "agnostic",
    "code",
    "run",
    "gpu",
    "available",
    "else",
    "default",
    "cpu",
    "check",
    "best",
    "practices",
    "using",
    "cuda",
    "namely",
    "setting",
    "device",
    "agnostic",
    "code",
    "let",
    "next",
    "video",
    "see",
    "mean",
    "setting",
    "pytorch",
    "tensors",
    "objects",
    "target",
    "device",
    "welcome",
    "back",
    "last",
    "video",
    "checked",
    "different",
    "options",
    "getting",
    "gpu",
    "getting",
    "pytorch",
    "run",
    "gpu",
    "using",
    "google",
    "colab",
    "easiest",
    "way",
    "get",
    "set",
    "gives",
    "us",
    "free",
    "access",
    "gpu",
    "faster",
    "ones",
    "set",
    "colab",
    "pro",
    "comes",
    "pytorch",
    "automatically",
    "set",
    "use",
    "gpu",
    "available",
    "let",
    "see",
    "actually",
    "use",
    "gpu",
    "look",
    "putting",
    "tensors",
    "models",
    "gpu",
    "reason",
    "want",
    "tensors",
    "slash",
    "models",
    "gpu",
    "using",
    "gpu",
    "results",
    "faster",
    "computations",
    "getting",
    "machine",
    "learning",
    "models",
    "find",
    "patterns",
    "numbers",
    "gpus",
    "great",
    "numerical",
    "calculations",
    "numerical",
    "calculations",
    "going",
    "tensor",
    "operations",
    "like",
    "saw",
    "tensor",
    "operations",
    "well",
    "covered",
    "lot",
    "somewhere",
    "tensor",
    "operations",
    "go",
    "manipulating",
    "tensor",
    "operations",
    "run",
    "computations",
    "faster",
    "discover",
    "patterns",
    "data",
    "faster",
    "experiments",
    "work",
    "towards",
    "finding",
    "best",
    "possible",
    "model",
    "whatever",
    "problem",
    "working",
    "let",
    "see",
    "create",
    "tensor",
    "usual",
    "create",
    "tensor",
    "default",
    "cpu",
    "tensor",
    "equals",
    "torch",
    "dot",
    "tensor",
    "make",
    "nice",
    "simple",
    "one",
    "one",
    "two",
    "three",
    "let",
    "write",
    "tensor",
    "gpu",
    "print",
    "tensor",
    "use",
    "saw",
    "parameter",
    "device",
    "pass",
    "device",
    "equals",
    "cpu",
    "let",
    "see",
    "comes",
    "go",
    "print",
    "tensor",
    "123",
    "cpu",
    "even",
    "got",
    "rid",
    "device",
    "parameter",
    "default",
    "going",
    "cpu",
    "wonderful",
    "pytorch",
    "makes",
    "quite",
    "easy",
    "move",
    "things",
    "saying",
    "reason",
    "gpu",
    "even",
    "better",
    "target",
    "device",
    "gpu",
    "available",
    "use",
    "cuda",
    "uses",
    "cpu",
    "set",
    "device",
    "variable",
    "let",
    "see",
    "move",
    "tensor",
    "gpu",
    "available",
    "tensor",
    "gpu",
    "equals",
    "tensor",
    "dot",
    "two",
    "device",
    "let",
    "look",
    "tensor",
    "gpu",
    "going",
    "shift",
    "tensor",
    "created",
    "target",
    "device",
    "wonderful",
    "look",
    "tensor",
    "123",
    "device",
    "cuda",
    "zero",
    "index",
    "gpu",
    "using",
    "one",
    "going",
    "index",
    "zero",
    "later",
    "start",
    "bigger",
    "experiments",
    "work",
    "multiple",
    "gpus",
    "might",
    "different",
    "tensors",
    "stored",
    "different",
    "gpus",
    "sticking",
    "one",
    "gpu",
    "keeping",
    "nice",
    "simple",
    "might",
    "case",
    "want",
    "move",
    "oh",
    "actually",
    "reason",
    "set",
    "device",
    "agnostic",
    "code",
    "code",
    "would",
    "work",
    "run",
    "regardless",
    "wo",
    "error",
    "regardless",
    "gpu",
    "code",
    "work",
    "whatever",
    "device",
    "access",
    "whether",
    "cpu",
    "whether",
    "gpu",
    "tensor",
    "move",
    "whatever",
    "target",
    "device",
    "since",
    "gpu",
    "available",
    "goes",
    "see",
    "lot",
    "two",
    "method",
    "moves",
    "tensors",
    "also",
    "used",
    "models",
    "going",
    "see",
    "later",
    "keep",
    "two",
    "device",
    "mind",
    "might",
    "want",
    "computations",
    "using",
    "numpy",
    "numpy",
    "works",
    "cpu",
    "might",
    "want",
    "move",
    "tensors",
    "back",
    "cpu",
    "moving",
    "tensors",
    "back",
    "cpu",
    "guess",
    "might",
    "okay",
    "know",
    "covered",
    "lot",
    "things",
    "going",
    "challenge",
    "anyway",
    "fun",
    "part",
    "thinking",
    "something",
    "let",
    "see",
    "let",
    "write",
    "tensor",
    "gpu",
    "ca",
    "transform",
    "numpy",
    "let",
    "see",
    "happens",
    "take",
    "tensor",
    "gpu",
    "try",
    "go",
    "numpy",
    "happens",
    "well",
    "get",
    "error",
    "another",
    "huge",
    "error",
    "remember",
    "top",
    "three",
    "errors",
    "deep",
    "learning",
    "pytorch",
    "lots",
    "number",
    "one",
    "shape",
    "errors",
    "number",
    "two",
    "data",
    "type",
    "issues",
    "pytorch",
    "number",
    "three",
    "device",
    "issues",
    "ca",
    "convert",
    "cuda",
    "zero",
    "device",
    "type",
    "tensor",
    "numpy",
    "numpy",
    "work",
    "gpu",
    "use",
    "tensor",
    "dot",
    "cpu",
    "copy",
    "tensor",
    "host",
    "memory",
    "first",
    "call",
    "tensor",
    "dot",
    "cpu",
    "going",
    "bring",
    "target",
    "tensor",
    "back",
    "cpu",
    "able",
    "use",
    "numpy",
    "fix",
    "gpu",
    "tensor",
    "numpy",
    "issue",
    "first",
    "set",
    "cpu",
    "tensor",
    "back",
    "cpu",
    "equals",
    "tensor",
    "gpu",
    "dot",
    "cpu",
    "taking",
    "said",
    "beautiful",
    "thing",
    "pytorch",
    "helpful",
    "error",
    "messages",
    "going",
    "go",
    "numpy",
    "go",
    "tensor",
    "back",
    "cpu",
    "going",
    "work",
    "let",
    "look",
    "oh",
    "course",
    "typed",
    "wrong",
    "typed",
    "twice",
    "third",
    "time",
    "third",
    "time",
    "charm",
    "go",
    "okay",
    "works",
    "put",
    "back",
    "cpu",
    "first",
    "calling",
    "numpy",
    "refer",
    "back",
    "tensor",
    "gpu",
    "reassociated",
    "got",
    "typos",
    "galore",
    "classic",
    "reassigned",
    "tensor",
    "back",
    "cpu",
    "tensor",
    "gpu",
    "remains",
    "unchanged",
    "four",
    "main",
    "things",
    "working",
    "pytorch",
    "gpu",
    "tidbits",
    "multiple",
    "gpus",
    "got",
    "fundamentals",
    "going",
    "stick",
    "using",
    "one",
    "gpu",
    "like",
    "later",
    "learned",
    "bit",
    "research",
    "multiple",
    "gpus",
    "well",
    "might",
    "guessed",
    "pytorch",
    "functionality",
    "go",
    "getting",
    "access",
    "gpu",
    "using",
    "colab",
    "check",
    "see",
    "available",
    "set",
    "device",
    "agnostic",
    "code",
    "create",
    "dummy",
    "tensors",
    "set",
    "different",
    "devices",
    "see",
    "happens",
    "change",
    "device",
    "parameter",
    "run",
    "errors",
    "trying",
    "numpy",
    "calculations",
    "tensors",
    "gpu",
    "bring",
    "tensors",
    "gpu",
    "back",
    "numpy",
    "see",
    "happens",
    "think",
    "covered",
    "think",
    "reached",
    "end",
    "fundamentals",
    "covered",
    "fair",
    "bit",
    "introduction",
    "tensors",
    "minmax",
    "whole",
    "bunch",
    "stuff",
    "inside",
    "introduction",
    "tensors",
    "finding",
    "positional",
    "minmax",
    "reshaping",
    "indexing",
    "working",
    "tensors",
    "numpy",
    "reproducibility",
    "using",
    "gpu",
    "moving",
    "stuff",
    "back",
    "gpu",
    "far",
    "probably",
    "wondering",
    "daniel",
    "covered",
    "whole",
    "bunch",
    "practice",
    "well",
    "glad",
    "asked",
    "let",
    "cover",
    "next",
    "video",
    "welcome",
    "back",
    "proud",
    "self",
    "right",
    "lot",
    "covered",
    "whole",
    "bunch",
    "pytorch",
    "fundamentals",
    "going",
    "building",
    "blocks",
    "use",
    "throughout",
    "rest",
    "course",
    "moving",
    "next",
    "section",
    "encourage",
    "try",
    "learned",
    "exercises",
    "extra",
    "curriculum",
    "set",
    "exercises",
    "based",
    "everything",
    "covered",
    "go",
    "learn",
    "go",
    "section",
    "currently",
    "going",
    "case",
    "every",
    "section",
    "way",
    "keep",
    "mind",
    "working",
    "pytorch",
    "fundamentals",
    "go",
    "pytorch",
    "fundamentals",
    "notebook",
    "going",
    "refresh",
    "scroll",
    "table",
    "contents",
    "bottom",
    "one",
    "going",
    "exercises",
    "extra",
    "curriculum",
    "exercises",
    "documentation",
    "reading",
    "lot",
    "seen",
    "refer",
    "pytorch",
    "documentation",
    "almost",
    "everything",
    "covered",
    "lot",
    "important",
    "become",
    "familiar",
    "exercise",
    "number",
    "one",
    "read",
    "documentation",
    "exercise",
    "number",
    "two",
    "create",
    "random",
    "tensor",
    "shape",
    "seven",
    "seven",
    "three",
    "perform",
    "matrix",
    "multiplication",
    "tensor",
    "two",
    "another",
    "random",
    "tensor",
    "exercises",
    "based",
    "covered",
    "encourage",
    "reference",
    "covered",
    "whichever",
    "notebook",
    "choose",
    "could",
    "learn",
    "could",
    "going",
    "back",
    "one",
    "coded",
    "together",
    "video",
    "going",
    "link",
    "exercises",
    "see",
    "exercises",
    "notebook",
    "approach",
    "exercises",
    "one",
    "way",
    "would",
    "read",
    "collab",
    "go",
    "file",
    "new",
    "notebook",
    "wait",
    "notebook",
    "load",
    "could",
    "call",
    "zero",
    "zero",
    "pytorch",
    "exercises",
    "something",
    "like",
    "could",
    "start",
    "importing",
    "torch",
    "away",
    "go",
    "probably",
    "set",
    "one",
    "side",
    "screen",
    "one",
    "side",
    "screen",
    "exercises",
    "number",
    "one",
    "going",
    "really",
    "write",
    "much",
    "code",
    "could",
    "documentation",
    "reading",
    "encourages",
    "read",
    "go",
    "10",
    "minutes",
    "ones",
    "got",
    "create",
    "random",
    "tensor",
    "shape",
    "seven",
    "seven",
    "comment",
    "torch",
    "round",
    "seven",
    "seven",
    "go",
    "easy",
    "little",
    "bit",
    "complex",
    "go",
    "throughout",
    "course",
    "exercises",
    "going",
    "get",
    "little",
    "bit",
    "depth",
    "learned",
    "like",
    "exercise",
    "template",
    "come",
    "back",
    "github",
    "home",
    "course",
    "materials",
    "go",
    "extras",
    "exercises",
    "created",
    "templates",
    "exercises",
    "pytorch",
    "fundamentals",
    "exercises",
    "open",
    "template",
    "exercises",
    "see",
    "create",
    "random",
    "tensor",
    "shape",
    "seven",
    "seven",
    "headings",
    "like",
    "open",
    "colab",
    "work",
    "well",
    "copy",
    "link",
    "come",
    "google",
    "colab",
    "go",
    "file",
    "open",
    "notebook",
    "github",
    "type",
    "link",
    "click",
    "search",
    "going",
    "boom",
    "pytorch",
    "fundamentals",
    "exercises",
    "go",
    "exercises",
    "every",
    "module",
    "course",
    "test",
    "knowledge",
    "open",
    "book",
    "use",
    "notebook",
    "ones",
    "coded",
    "together",
    "would",
    "encourage",
    "try",
    "things",
    "first",
    "get",
    "stuck",
    "always",
    "reference",
    "back",
    "like",
    "see",
    "example",
    "solutions",
    "go",
    "back",
    "extras",
    "solutions",
    "folder",
    "well",
    "solutions",
    "live",
    "fundamental",
    "exercise",
    "solutions",
    "would",
    "encourage",
    "try",
    "least",
    "give",
    "go",
    "look",
    "solutions",
    "keep",
    "mind",
    "end",
    "every",
    "module",
    "exercises",
    "extra",
    "curriculum",
    "exercises",
    "code",
    "based",
    "extra",
    "curriculum",
    "usually",
    "like",
    "reading",
    "based",
    "spend",
    "one",
    "hour",
    "going",
    "pytorch",
    "basics",
    "tutorial",
    "recommend",
    "quick",
    "start",
    "tensor",
    "sections",
    "finally",
    "learn",
    "tensor",
    "represent",
    "data",
    "watch",
    "video",
    "tensor",
    "referred",
    "throughout",
    "massive",
    "effort",
    "finishing",
    "pytorch",
    "fundamentals",
    "section",
    "see",
    "next",
    "section",
    "friends",
    "welcome",
    "back",
    "pytorch",
    "workflow",
    "module",
    "let",
    "look",
    "going",
    "get",
    "pytorch",
    "workflow",
    "say",
    "one",
    "many",
    "get",
    "deep",
    "learning",
    "machine",
    "learning",
    "find",
    "fair",
    "ways",
    "things",
    "rough",
    "outline",
    "going",
    "going",
    "get",
    "data",
    "ready",
    "turn",
    "tensors",
    "remember",
    "tensor",
    "represent",
    "almost",
    "kind",
    "data",
    "going",
    "pick",
    "build",
    "pick",
    "model",
    "pick",
    "loss",
    "function",
    "optimize",
    "worry",
    "know",
    "going",
    "cover",
    "going",
    "build",
    "training",
    "loop",
    "fit",
    "model",
    "make",
    "prediction",
    "fit",
    "model",
    "data",
    "learn",
    "evaluate",
    "models",
    "see",
    "improve",
    "experimentation",
    "save",
    "reload",
    "trained",
    "model",
    "wanted",
    "export",
    "model",
    "notebook",
    "use",
    "somewhere",
    "else",
    "want",
    "get",
    "help",
    "probably",
    "important",
    "thing",
    "follow",
    "along",
    "code",
    "coding",
    "together",
    "remember",
    "model",
    "number",
    "one",
    "run",
    "code",
    "try",
    "learn",
    "best",
    "write",
    "code",
    "try",
    "get",
    "wrong",
    "try",
    "keep",
    "going",
    "get",
    "right",
    "read",
    "doc",
    "string",
    "going",
    "show",
    "documentation",
    "functions",
    "using",
    "mac",
    "use",
    "shift",
    "command",
    "space",
    "google",
    "colab",
    "windows",
    "pc",
    "might",
    "control",
    "still",
    "stuck",
    "try",
    "searching",
    "probably",
    "come",
    "across",
    "resources",
    "stack",
    "overflow",
    "pytorch",
    "documentation",
    "already",
    "seen",
    "whole",
    "bunch",
    "probably",
    "going",
    "see",
    "lot",
    "throughout",
    "entire",
    "course",
    "actually",
    "going",
    "ground",
    "truth",
    "everything",
    "pytorch",
    "try",
    "finally",
    "still",
    "stuck",
    "ask",
    "question",
    "best",
    "place",
    "ask",
    "question",
    "pytorch",
    "deep",
    "learning",
    "slash",
    "discussions",
    "tab",
    "go",
    "github",
    "deeburg",
    "pytorch",
    "deep",
    "learning",
    "course",
    "materials",
    "see",
    "ground",
    "truth",
    "entire",
    "course",
    "question",
    "go",
    "discussions",
    "tab",
    "new",
    "discussion",
    "ask",
    "question",
    "forget",
    "please",
    "put",
    "video",
    "code",
    "trying",
    "run",
    "way",
    "reference",
    "going",
    "help",
    "also",
    "forget",
    "book",
    "version",
    "course",
    "learn",
    "time",
    "watch",
    "video",
    "probably",
    "chapters",
    "working",
    "videos",
    "based",
    "going",
    "go",
    "fun",
    "reference",
    "material",
    "read",
    "time",
    "going",
    "focus",
    "coding",
    "together",
    "speaking",
    "coding",
    "let",
    "code",
    "see",
    "google",
    "colab",
    "oh",
    "right",
    "well",
    "let",
    "get",
    "hands",
    "code",
    "going",
    "come",
    "may",
    "already",
    "bookmark",
    "going",
    "start",
    "new",
    "notebook",
    "going",
    "everything",
    "scratch",
    "let",
    "load",
    "going",
    "zoom",
    "little",
    "bit",
    "beautiful",
    "going",
    "title",
    "01",
    "pytorch",
    "workflow",
    "going",
    "put",
    "video",
    "ending",
    "know",
    "notebook",
    "video",
    "course",
    "resources",
    "original",
    "notebook",
    "video",
    "notebook",
    "going",
    "based",
    "refer",
    "notebook",
    "reference",
    "going",
    "go",
    "got",
    "lot",
    "pictures",
    "beautiful",
    "text",
    "annotations",
    "going",
    "focused",
    "code",
    "videos",
    "course",
    "got",
    "book",
    "version",
    "notebook",
    "well",
    "different",
    "formatted",
    "version",
    "exact",
    "notebook",
    "going",
    "link",
    "let",
    "write",
    "pytorch",
    "workflow",
    "let",
    "explore",
    "example",
    "pytorch",
    "end",
    "end",
    "workflow",
    "going",
    "put",
    "resources",
    "ground",
    "truth",
    "notebook",
    "go",
    "also",
    "going",
    "put",
    "book",
    "version",
    "book",
    "version",
    "notebook",
    "finally",
    "ask",
    "question",
    "discussions",
    "page",
    "go",
    "beautiful",
    "let",
    "turn",
    "markdown",
    "let",
    "get",
    "started",
    "let",
    "jump",
    "right",
    "start",
    "covering",
    "trend",
    "want",
    "start",
    "getting",
    "towards",
    "rather",
    "spending",
    "whole",
    "bunch",
    "time",
    "going",
    "keynotes",
    "slides",
    "rather",
    "code",
    "together",
    "explain",
    "different",
    "things",
    "need",
    "explained",
    "going",
    "end",
    "writing",
    "lot",
    "pytorch",
    "going",
    "writing",
    "code",
    "looking",
    "things",
    "go",
    "get",
    "extra",
    "tabs",
    "think",
    "need",
    "two",
    "important",
    "covering",
    "let",
    "create",
    "little",
    "dictionary",
    "check",
    "wanted",
    "later",
    "referring",
    "pytorch",
    "workflows",
    "least",
    "example",
    "one",
    "going",
    "go",
    "going",
    "go",
    "six",
    "steps",
    "maybe",
    "little",
    "bit",
    "one",
    "see",
    "going",
    "really",
    "focused",
    "going",
    "go",
    "rest",
    "course",
    "like",
    "really",
    "dig",
    "deep",
    "covering",
    "number",
    "one",
    "data",
    "preparing",
    "loading",
    "number",
    "two",
    "going",
    "see",
    "build",
    "machine",
    "learning",
    "model",
    "pytorch",
    "deep",
    "learning",
    "model",
    "going",
    "see",
    "going",
    "fit",
    "model",
    "data",
    "called",
    "training",
    "fit",
    "another",
    "word",
    "said",
    "machine",
    "learning",
    "lot",
    "different",
    "names",
    "similar",
    "things",
    "kind",
    "confusing",
    "pick",
    "time",
    "going",
    "trained",
    "model",
    "going",
    "see",
    "make",
    "predictions",
    "evaluate",
    "predictions",
    "evaluating",
    "model",
    "make",
    "predictions",
    "often",
    "referred",
    "inference",
    "typically",
    "say",
    "making",
    "predictions",
    "inference",
    "another",
    "common",
    "term",
    "going",
    "look",
    "save",
    "load",
    "model",
    "going",
    "put",
    "together",
    "little",
    "bit",
    "different",
    "visual",
    "version",
    "pytorch",
    "workflow",
    "go",
    "back",
    "might",
    "zoom",
    "little",
    "go",
    "going",
    "focus",
    "one",
    "later",
    "improve",
    "experimentation",
    "going",
    "focus",
    "getting",
    "data",
    "ready",
    "building",
    "model",
    "fitting",
    "model",
    "evaluating",
    "model",
    "save",
    "reload",
    "see",
    "one",
    "like",
    "depth",
    "later",
    "hint",
    "different",
    "things",
    "working",
    "workflow",
    "let",
    "put",
    "wanted",
    "refer",
    "later",
    "go",
    "covering",
    "oh",
    "going",
    "connect",
    "course",
    "beautiful",
    "refer",
    "later",
    "wanted",
    "going",
    "start",
    "import",
    "torch",
    "going",
    "get",
    "pytorch",
    "ready",
    "go",
    "import",
    "nn",
    "write",
    "note",
    "seen",
    "one",
    "going",
    "see",
    "things",
    "seen",
    "okay",
    "explain",
    "go",
    "nn",
    "contains",
    "pytorch",
    "building",
    "blocks",
    "neural",
    "networks",
    "would",
    "learn",
    "torch",
    "nn",
    "well",
    "go",
    "learn",
    "pytorch",
    "documentation",
    "beautiful",
    "look",
    "basic",
    "building",
    "blocks",
    "graphs",
    "see",
    "word",
    "graph",
    "referring",
    "computational",
    "graph",
    "case",
    "neural",
    "networks",
    "let",
    "look",
    "photo",
    "neural",
    "network",
    "images",
    "graph",
    "start",
    "going",
    "go",
    "towards",
    "right",
    "going",
    "many",
    "different",
    "pictures",
    "yeah",
    "good",
    "one",
    "input",
    "layer",
    "hidden",
    "layer",
    "hidden",
    "layer",
    "output",
    "layer",
    "torch",
    "n",
    "comprises",
    "whole",
    "bunch",
    "different",
    "layers",
    "see",
    "layers",
    "layers",
    "layers",
    "one",
    "see",
    "input",
    "layer",
    "hidden",
    "layer",
    "one",
    "hidden",
    "layer",
    "two",
    "job",
    "data",
    "scientists",
    "machine",
    "learning",
    "engineers",
    "combine",
    "torch",
    "dot",
    "nn",
    "building",
    "blocks",
    "build",
    "things",
    "might",
    "exactly",
    "like",
    "beauty",
    "pytorch",
    "combine",
    "almost",
    "different",
    "way",
    "build",
    "kind",
    "neural",
    "network",
    "imagine",
    "let",
    "keep",
    "going",
    "torch",
    "nn",
    "going",
    "get",
    "hands",
    "rather",
    "talk",
    "going",
    "need",
    "map",
    "plot",
    "lib",
    "motto",
    "data",
    "explorers",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "let",
    "check",
    "pytorch",
    "version",
    "pytorch",
    "version",
    "torch",
    "dot",
    "version",
    "show",
    "need",
    "least",
    "version",
    "plus",
    "cuda",
    "means",
    "got",
    "cu",
    "stands",
    "cuda",
    "means",
    "got",
    "access",
    "cuda",
    "gpu",
    "runtime",
    "yet",
    "gone",
    "gpu",
    "might",
    "later",
    "version",
    "lower",
    "say",
    "want",
    "pytorch",
    "least",
    "version",
    "higher",
    "code",
    "still",
    "work",
    "enough",
    "video",
    "got",
    "workflow",
    "ready",
    "set",
    "notebook",
    "video",
    "notebook",
    "got",
    "resources",
    "got",
    "covering",
    "got",
    "dependencies",
    "let",
    "next",
    "one",
    "get",
    "started",
    "one",
    "data",
    "preparing",
    "loading",
    "see",
    "next",
    "video",
    "let",
    "get",
    "first",
    "step",
    "pytorch",
    "workflow",
    "data",
    "preparing",
    "loading",
    "want",
    "stress",
    "data",
    "almost",
    "anything",
    "machine",
    "learning",
    "mean",
    "could",
    "excel",
    "spreadsheet",
    "rows",
    "columns",
    "nice",
    "formatted",
    "data",
    "could",
    "images",
    "kind",
    "could",
    "videos",
    "mean",
    "youtube",
    "lots",
    "data",
    "could",
    "audio",
    "like",
    "songs",
    "podcasts",
    "could",
    "even",
    "dna",
    "days",
    "patents",
    "dna",
    "starting",
    "get",
    "discovered",
    "machine",
    "learning",
    "course",
    "could",
    "text",
    "like",
    "writing",
    "going",
    "focusing",
    "throughout",
    "entire",
    "course",
    "fact",
    "machine",
    "learning",
    "game",
    "two",
    "parts",
    "one",
    "get",
    "data",
    "numerical",
    "representation",
    "build",
    "model",
    "learn",
    "patterns",
    "numerical",
    "representation",
    "course",
    "around",
    "yes",
    "yes",
    "yes",
    "understand",
    "get",
    "complex",
    "like",
    "main",
    "two",
    "concepts",
    "machine",
    "learning",
    "say",
    "machine",
    "learning",
    "saying",
    "goes",
    "deep",
    "learning",
    "need",
    "kind",
    "oh",
    "number",
    "call",
    "number",
    "call",
    "like",
    "word",
    "number",
    "call",
    "representation",
    "want",
    "build",
    "model",
    "learn",
    "patterns",
    "numerical",
    "representation",
    "want",
    "got",
    "nice",
    "pretty",
    "picture",
    "describes",
    "machine",
    "learning",
    "game",
    "two",
    "parts",
    "let",
    "refer",
    "data",
    "remember",
    "data",
    "almost",
    "anything",
    "inputs",
    "first",
    "step",
    "want",
    "create",
    "form",
    "numerical",
    "encoding",
    "form",
    "tenses",
    "represent",
    "inputs",
    "looks",
    "dependent",
    "data",
    "depending",
    "numerical",
    "encoding",
    "choose",
    "use",
    "going",
    "build",
    "sort",
    "neural",
    "network",
    "learn",
    "representation",
    "also",
    "referred",
    "patterns",
    "features",
    "weights",
    "within",
    "numerical",
    "encoding",
    "going",
    "output",
    "representation",
    "want",
    "something",
    "without",
    "representation",
    "case",
    "image",
    "recognition",
    "image",
    "classification",
    "photo",
    "raman",
    "spaghetti",
    "tweet",
    "spam",
    "spam",
    "audio",
    "file",
    "saying",
    "says",
    "going",
    "say",
    "audio",
    "assistant",
    "also",
    "named",
    "word",
    "close",
    "want",
    "go",
    "game",
    "two",
    "parts",
    "one",
    "convert",
    "data",
    "numerical",
    "representation",
    "two",
    "build",
    "model",
    "use",
    "pre",
    "trained",
    "model",
    "find",
    "patterns",
    "numerical",
    "representation",
    "got",
    "little",
    "stationary",
    "picture",
    "turn",
    "data",
    "numbers",
    "part",
    "two",
    "build",
    "model",
    "learn",
    "patterns",
    "numbers",
    "said",
    "let",
    "create",
    "data",
    "showcase",
    "showcase",
    "let",
    "create",
    "known",
    "data",
    "using",
    "linear",
    "regression",
    "formula",
    "sure",
    "linear",
    "regression",
    "formula",
    "let",
    "look",
    "linear",
    "regression",
    "formula",
    "find",
    "okay",
    "fancy",
    "greek",
    "letters",
    "essentially",
    "equals",
    "function",
    "x",
    "b",
    "plus",
    "epsilon",
    "okay",
    "well",
    "go",
    "linear",
    "regression",
    "line",
    "equation",
    "form",
    "equals",
    "plus",
    "bx",
    "oh",
    "like",
    "one",
    "better",
    "nice",
    "simple",
    "going",
    "start",
    "simple",
    "possible",
    "work",
    "equals",
    "plus",
    "bx",
    "x",
    "explanatory",
    "variable",
    "dependent",
    "variable",
    "slope",
    "line",
    "slope",
    "also",
    "known",
    "gradient",
    "intercept",
    "okay",
    "value",
    "x",
    "equals",
    "zero",
    "text",
    "page",
    "formula",
    "page",
    "know",
    "like",
    "learn",
    "things",
    "let",
    "code",
    "let",
    "write",
    "use",
    "linear",
    "regression",
    "formula",
    "make",
    "straight",
    "line",
    "known",
    "parameters",
    "going",
    "write",
    "parameter",
    "common",
    "word",
    "going",
    "hear",
    "machine",
    "learning",
    "well",
    "parameter",
    "something",
    "model",
    "learns",
    "data",
    "set",
    "machine",
    "learning",
    "game",
    "two",
    "parts",
    "going",
    "start",
    "number",
    "one",
    "going",
    "done",
    "us",
    "going",
    "start",
    "known",
    "representation",
    "known",
    "data",
    "set",
    "want",
    "model",
    "learn",
    "representation",
    "talk",
    "daniel",
    "let",
    "get",
    "coding",
    "yes",
    "right",
    "right",
    "let",
    "create",
    "known",
    "parameters",
    "going",
    "use",
    "little",
    "bit",
    "different",
    "names",
    "google",
    "definition",
    "weight",
    "going",
    "bias",
    "going",
    "weight",
    "bias",
    "another",
    "common",
    "two",
    "terms",
    "going",
    "hear",
    "neural",
    "networks",
    "keep",
    "mind",
    "us",
    "going",
    "equivalent",
    "weight",
    "b",
    "bias",
    "forget",
    "time",
    "let",
    "focus",
    "code",
    "know",
    "numbers",
    "want",
    "build",
    "model",
    "able",
    "estimate",
    "numbers",
    "looking",
    "different",
    "examples",
    "let",
    "create",
    "data",
    "going",
    "create",
    "range",
    "numbers",
    "start",
    "equals",
    "zero",
    "equals",
    "one",
    "going",
    "create",
    "numbers",
    "zero",
    "one",
    "going",
    "gap",
    "step",
    "gap",
    "going",
    "going",
    "create",
    "x",
    "variable",
    "x",
    "capital",
    "well",
    "typically",
    "x",
    "machine",
    "learning",
    "find",
    "matrix",
    "tensor",
    "remember",
    "back",
    "fundamentals",
    "capital",
    "represents",
    "matrix",
    "tensor",
    "lowercase",
    "represents",
    "vector",
    "case",
    "going",
    "little",
    "confusing",
    "x",
    "vector",
    "later",
    "x",
    "start",
    "tensor",
    "matrix",
    "keep",
    "capital",
    "capital",
    "notation",
    "going",
    "create",
    "formula",
    "remember",
    "said",
    "weight",
    "case",
    "b",
    "bias",
    "got",
    "formula",
    "equals",
    "weight",
    "times",
    "x",
    "plus",
    "bias",
    "let",
    "look",
    "different",
    "numbers",
    "view",
    "first",
    "10",
    "x",
    "view",
    "first",
    "10",
    "look",
    "length",
    "x",
    "look",
    "length",
    "wonderful",
    "got",
    "values",
    "got",
    "50",
    "numbers",
    "little",
    "confusing",
    "let",
    "view",
    "first",
    "10",
    "x",
    "first",
    "look",
    "length",
    "going",
    "building",
    "model",
    "learn",
    "values",
    "look",
    "x",
    "values",
    "learn",
    "associated",
    "value",
    "relationship",
    "course",
    "know",
    "relationship",
    "x",
    "coded",
    "formula",
    "wo",
    "always",
    "know",
    "wild",
    "whole",
    "premise",
    "machine",
    "learning",
    "ideal",
    "output",
    "input",
    "whole",
    "premise",
    "machine",
    "learning",
    "learn",
    "representation",
    "input",
    "maps",
    "output",
    "input",
    "numbers",
    "output",
    "numbers",
    "know",
    "parameters",
    "weight",
    "bias",
    "could",
    "set",
    "whatever",
    "want",
    "way",
    "like",
    "number",
    "7",
    "could",
    "set",
    "whatever",
    "whatever",
    "premise",
    "would",
    "oh",
    "done",
    "kind",
    "coded",
    "without",
    "talking",
    "torch",
    "range",
    "starts",
    "0",
    "ends",
    "1",
    "step",
    "go",
    "000",
    "unsqueezed",
    "unsqueezed",
    "removes",
    "extra",
    "dimensions",
    "oh",
    "sorry",
    "ads",
    "extra",
    "dimension",
    "getting",
    "confused",
    "remove",
    "get",
    "extra",
    "square",
    "bracket",
    "add",
    "unsqueeze",
    "see",
    "need",
    "later",
    "models",
    "wonderful",
    "let",
    "leave",
    "enough",
    "video",
    "got",
    "data",
    "work",
    "worry",
    "little",
    "bit",
    "confusing",
    "going",
    "keep",
    "coding",
    "see",
    "build",
    "model",
    "infer",
    "patterns",
    "data",
    "right",
    "want",
    "think",
    "tensor",
    "data",
    "numbers",
    "page",
    "might",
    "better",
    "way",
    "hint",
    "hint",
    "way",
    "visualize",
    "data",
    "explorer",
    "motto",
    "let",
    "look",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "created",
    "numbers",
    "page",
    "using",
    "linear",
    "regression",
    "formula",
    "known",
    "parameters",
    "lot",
    "going",
    "right",
    "going",
    "keep",
    "building",
    "upon",
    "done",
    "learn",
    "video",
    "going",
    "cover",
    "one",
    "important",
    "concepts",
    "machine",
    "learning",
    "general",
    "splitting",
    "data",
    "training",
    "test",
    "sets",
    "one",
    "important",
    "concepts",
    "machine",
    "learning",
    "general",
    "know",
    "said",
    "already",
    "times",
    "one",
    "important",
    "concepts",
    "truly",
    "possibly",
    "terms",
    "data",
    "probably",
    "number",
    "one",
    "thing",
    "need",
    "aware",
    "come",
    "little",
    "bit",
    "machine",
    "learning",
    "background",
    "probably",
    "well",
    "truly",
    "know",
    "going",
    "recover",
    "anyway",
    "let",
    "jump",
    "pretty",
    "pictures",
    "oh",
    "look",
    "one",
    "speaking",
    "pretty",
    "pictures",
    "focused",
    "looking",
    "three",
    "data",
    "sets",
    "written",
    "possibly",
    "important",
    "concept",
    "machine",
    "learning",
    "definitely",
    "data",
    "perspective",
    "course",
    "materials",
    "imagine",
    "university",
    "going",
    "training",
    "set",
    "practice",
    "exam",
    "validation",
    "set",
    "final",
    "exam",
    "test",
    "set",
    "goal",
    "generalization",
    "let",
    "step",
    "back",
    "say",
    "trying",
    "learn",
    "something",
    "university",
    "course",
    "might",
    "materials",
    "training",
    "set",
    "model",
    "learns",
    "patterns",
    "practice",
    "done",
    "might",
    "practice",
    "exam",
    "mid",
    "semester",
    "exam",
    "something",
    "like",
    "let",
    "see",
    "learning",
    "course",
    "materials",
    "well",
    "case",
    "model",
    "might",
    "tune",
    "model",
    "plastic",
    "exam",
    "might",
    "find",
    "validation",
    "set",
    "model",
    "well",
    "adjusted",
    "bit",
    "retrain",
    "better",
    "finally",
    "end",
    "semester",
    "important",
    "exam",
    "final",
    "exam",
    "see",
    "gone",
    "entire",
    "course",
    "materials",
    "learned",
    "things",
    "adapt",
    "unseen",
    "material",
    "big",
    "point",
    "going",
    "see",
    "practice",
    "model",
    "learns",
    "something",
    "course",
    "materials",
    "never",
    "sees",
    "validation",
    "set",
    "test",
    "set",
    "say",
    "started",
    "100",
    "data",
    "points",
    "might",
    "use",
    "70",
    "data",
    "points",
    "training",
    "material",
    "might",
    "use",
    "15",
    "data",
    "points",
    "15",
    "practice",
    "might",
    "use",
    "15",
    "final",
    "exam",
    "final",
    "exam",
    "like",
    "university",
    "learning",
    "something",
    "see",
    "hey",
    "learned",
    "skills",
    "material",
    "ready",
    "go",
    "wild",
    "quote",
    "unquote",
    "real",
    "world",
    "final",
    "exam",
    "test",
    "model",
    "generalization",
    "never",
    "seen",
    "data",
    "let",
    "define",
    "generalization",
    "ability",
    "machine",
    "learning",
    "model",
    "deep",
    "learning",
    "model",
    "perform",
    "well",
    "data",
    "seen",
    "whole",
    "goal",
    "right",
    "want",
    "build",
    "machine",
    "learning",
    "model",
    "training",
    "data",
    "deploy",
    "application",
    "production",
    "setting",
    "data",
    "comes",
    "seen",
    "make",
    "decisions",
    "based",
    "new",
    "data",
    "patterns",
    "learned",
    "training",
    "set",
    "keep",
    "mind",
    "three",
    "data",
    "sets",
    "training",
    "validation",
    "test",
    "jump",
    "learn",
    "pytorch",
    "book",
    "got",
    "split",
    "data",
    "going",
    "create",
    "three",
    "sets",
    "case",
    "going",
    "create",
    "two",
    "training",
    "test",
    "always",
    "need",
    "validation",
    "set",
    "often",
    "use",
    "case",
    "validation",
    "set",
    "main",
    "two",
    "always",
    "used",
    "training",
    "set",
    "testing",
    "set",
    "much",
    "split",
    "well",
    "usually",
    "training",
    "set",
    "60",
    "80",
    "data",
    "create",
    "validation",
    "set",
    "somewhere",
    "10",
    "create",
    "testing",
    "set",
    "similar",
    "split",
    "validation",
    "set",
    "10",
    "20",
    "training",
    "always",
    "testing",
    "always",
    "validation",
    "often",
    "always",
    "said",
    "let",
    "refer",
    "materials",
    "want",
    "let",
    "create",
    "training",
    "test",
    "set",
    "data",
    "saw",
    "50",
    "points",
    "x",
    "one",
    "one",
    "ratio",
    "one",
    "value",
    "x",
    "relates",
    "one",
    "value",
    "know",
    "split",
    "training",
    "set",
    "60",
    "80",
    "test",
    "set",
    "10",
    "20",
    "let",
    "go",
    "upper",
    "bounds",
    "80",
    "20",
    "common",
    "split",
    "actually",
    "80",
    "let",
    "go",
    "create",
    "train",
    "test",
    "split",
    "going",
    "go",
    "train",
    "split",
    "create",
    "number",
    "see",
    "much",
    "want",
    "integer",
    "80",
    "length",
    "give",
    "us",
    "train",
    "split",
    "40",
    "samples",
    "wonderful",
    "going",
    "create",
    "40",
    "samples",
    "x",
    "40",
    "samples",
    "model",
    "train",
    "40",
    "samples",
    "predict",
    "10",
    "samples",
    "let",
    "see",
    "practice",
    "x",
    "train",
    "train",
    "equals",
    "going",
    "use",
    "indexing",
    "get",
    "samples",
    "train",
    "split",
    "colon",
    "hey",
    "x",
    "train",
    "split",
    "train",
    "split",
    "testing",
    "oh",
    "thanks",
    "auto",
    "correct",
    "cola",
    "actually",
    "need",
    "one",
    "x",
    "test",
    "test",
    "equals",
    "going",
    "get",
    "everything",
    "train",
    "split",
    "onwards",
    "index",
    "onwards",
    "notation",
    "means",
    "train",
    "split",
    "onwards",
    "well",
    "many",
    "different",
    "ways",
    "create",
    "train",
    "test",
    "split",
    "quite",
    "simple",
    "working",
    "quite",
    "simple",
    "data",
    "set",
    "one",
    "popular",
    "methods",
    "like",
    "scikit",
    "learns",
    "train",
    "test",
    "split",
    "going",
    "see",
    "one",
    "later",
    "adds",
    "little",
    "bit",
    "randomness",
    "splitting",
    "data",
    "another",
    "video",
    "make",
    "aware",
    "let",
    "go",
    "length",
    "x",
    "train",
    "40",
    "training",
    "samples",
    "many",
    "testing",
    "samples",
    "length",
    "x",
    "test",
    "length",
    "test",
    "wonderful",
    "40",
    "40",
    "10",
    "10",
    "training",
    "features",
    "training",
    "labels",
    "testing",
    "features",
    "testing",
    "labels",
    "essentially",
    "created",
    "training",
    "set",
    "split",
    "data",
    "training",
    "set",
    "could",
    "also",
    "referred",
    "training",
    "split",
    "yet",
    "another",
    "example",
    "machine",
    "learning",
    "different",
    "names",
    "different",
    "things",
    "set",
    "split",
    "thing",
    "training",
    "split",
    "test",
    "split",
    "created",
    "remember",
    "validation",
    "set",
    "used",
    "often",
    "always",
    "data",
    "set",
    "quite",
    "simple",
    "sticking",
    "necessities",
    "training",
    "test",
    "keep",
    "mind",
    "one",
    "biggest",
    "biggest",
    "biggest",
    "hurdles",
    "machine",
    "learning",
    "creating",
    "proper",
    "training",
    "test",
    "sets",
    "important",
    "concept",
    "said",
    "issue",
    "challenge",
    "last",
    "video",
    "visualize",
    "numbers",
    "page",
    "done",
    "video",
    "let",
    "move",
    "towards",
    "next",
    "like",
    "think",
    "could",
    "make",
    "visual",
    "right",
    "numbers",
    "page",
    "right",
    "maybe",
    "plot",
    "lib",
    "help",
    "let",
    "find",
    "hey",
    "hey",
    "hey",
    "welcome",
    "back",
    "last",
    "video",
    "split",
    "data",
    "training",
    "test",
    "sets",
    "later",
    "going",
    "building",
    "model",
    "learn",
    "patterns",
    "training",
    "data",
    "relate",
    "testing",
    "data",
    "said",
    "right",
    "data",
    "numbers",
    "page",
    "kind",
    "hard",
    "understand",
    "might",
    "able",
    "understand",
    "prefer",
    "get",
    "visual",
    "let",
    "write",
    "might",
    "better",
    "visualize",
    "data",
    "put",
    "capital",
    "grammatically",
    "correct",
    "data",
    "explorers",
    "motto",
    "comes",
    "visualize",
    "visualize",
    "visualize",
    "ha",
    "ha",
    "right",
    "ever",
    "understand",
    "concept",
    "one",
    "best",
    "ways",
    "start",
    "understanding",
    "visualize",
    "let",
    "write",
    "function",
    "going",
    "call",
    "plot",
    "predictions",
    "see",
    "call",
    "later",
    "benefit",
    "making",
    "videos",
    "got",
    "plan",
    "future",
    "although",
    "might",
    "seem",
    "like",
    "winging",
    "little",
    "bit",
    "behind",
    "scenes",
    "happening",
    "train",
    "data",
    "x",
    "train",
    "train",
    "labels",
    "train",
    "also",
    "test",
    "data",
    "yeah",
    "good",
    "idea",
    "x",
    "test",
    "also",
    "test",
    "labels",
    "equals",
    "test",
    "excuse",
    "looking",
    "many",
    "x",
    "predictions",
    "set",
    "none",
    "predictions",
    "yet",
    "might",
    "guessed",
    "might",
    "later",
    "put",
    "little",
    "doc",
    "string",
    "nice",
    "pythonic",
    "plots",
    "training",
    "data",
    "test",
    "data",
    "compares",
    "predictions",
    "nice",
    "simple",
    "nothing",
    "outlandish",
    "going",
    "create",
    "figure",
    "map",
    "plot",
    "lib",
    "comes",
    "plot",
    "figure",
    "go",
    "fig",
    "size",
    "equals",
    "10",
    "seven",
    "favorite",
    "hand",
    "poker",
    "plot",
    "training",
    "data",
    "blue",
    "also",
    "happens",
    "good",
    "dimension",
    "map",
    "plot",
    "plot",
    "dot",
    "scatter",
    "train",
    "data",
    "creating",
    "scatter",
    "plot",
    "see",
    "second",
    "color",
    "going",
    "give",
    "color",
    "b",
    "blue",
    "c",
    "stands",
    "map",
    "plot",
    "lib",
    "scatter",
    "go",
    "size",
    "equals",
    "four",
    "label",
    "equals",
    "training",
    "data",
    "could",
    "find",
    "information",
    "scatter",
    "function",
    "got",
    "command",
    "shift",
    "space",
    "going",
    "give",
    "us",
    "little",
    "bit",
    "doc",
    "string",
    "sometimes",
    "command",
    "space",
    "working",
    "also",
    "hover",
    "bracket",
    "think",
    "even",
    "hover",
    "go",
    "little",
    "hard",
    "read",
    "like",
    "got",
    "lot",
    "going",
    "x",
    "c",
    "c",
    "map",
    "like",
    "go",
    "map",
    "plot",
    "lib",
    "scatter",
    "go",
    "got",
    "whole",
    "bunch",
    "information",
    "little",
    "bit",
    "easier",
    "read",
    "see",
    "examples",
    "beautiful",
    "let",
    "jump",
    "back",
    "function",
    "plot",
    "predictions",
    "taken",
    "training",
    "data",
    "test",
    "data",
    "got",
    "training",
    "data",
    "plotting",
    "blue",
    "color",
    "use",
    "testing",
    "data",
    "green",
    "like",
    "idea",
    "test",
    "data",
    "green",
    "favorite",
    "color",
    "favorite",
    "color",
    "c",
    "equals",
    "might",
    "able",
    "plot",
    "favorite",
    "color",
    "remember",
    "though",
    "little",
    "bit",
    "different",
    "videos",
    "going",
    "call",
    "testing",
    "data",
    "exact",
    "line",
    "different",
    "set",
    "data",
    "let",
    "check",
    "predictions",
    "predictions",
    "predictions",
    "none",
    "let",
    "plot",
    "predictions",
    "plot",
    "predictions",
    "exist",
    "plot",
    "scatter",
    "test",
    "data",
    "plotting",
    "test",
    "data",
    "remember",
    "scatter",
    "function",
    "let",
    "go",
    "back",
    "takes",
    "x",
    "predictions",
    "going",
    "compared",
    "testing",
    "data",
    "labels",
    "whole",
    "game",
    "playing",
    "going",
    "train",
    "model",
    "training",
    "data",
    "evaluate",
    "going",
    "get",
    "model",
    "predict",
    "values",
    "input",
    "x",
    "test",
    "evaluate",
    "model",
    "compare",
    "good",
    "models",
    "predictions",
    "words",
    "predictions",
    "versus",
    "actual",
    "values",
    "test",
    "data",
    "set",
    "going",
    "see",
    "practice",
    "rather",
    "talk",
    "let",
    "predictions",
    "red",
    "label",
    "equals",
    "predictions",
    "wonderful",
    "let",
    "also",
    "show",
    "legend",
    "mean",
    "legends",
    "could",
    "put",
    "mirror",
    "kidding",
    "legend",
    "going",
    "show",
    "labels",
    "map",
    "plot",
    "prop",
    "equals",
    "size",
    "prop",
    "stands",
    "properties",
    "well",
    "may",
    "may",
    "like",
    "think",
    "remember",
    "beautiful",
    "function",
    "plot",
    "data",
    "try",
    "remember",
    "got",
    "hard",
    "coded",
    "inputs",
    "actually",
    "need",
    "input",
    "anything",
    "function",
    "got",
    "train",
    "test",
    "data",
    "ready",
    "go",
    "doubt",
    "run",
    "code",
    "let",
    "check",
    "make",
    "mistake",
    "plot",
    "predictions",
    "function",
    "might",
    "caught",
    "hey",
    "go",
    "beautiful",
    "predictions",
    "get",
    "red",
    "dots",
    "trying",
    "got",
    "simple",
    "straight",
    "line",
    "ca",
    "get",
    "much",
    "simple",
    "data",
    "set",
    "got",
    "training",
    "data",
    "blue",
    "got",
    "testing",
    "data",
    "green",
    "whole",
    "idea",
    "going",
    "machine",
    "learning",
    "model",
    "actually",
    "really",
    "need",
    "build",
    "machine",
    "learning",
    "model",
    "could",
    "things",
    "machine",
    "learning",
    "fun",
    "going",
    "take",
    "blue",
    "dots",
    "quite",
    "pattern",
    "right",
    "relationship",
    "x",
    "value",
    "value",
    "going",
    "build",
    "model",
    "try",
    "learn",
    "pattern",
    "blue",
    "dots",
    "fed",
    "model",
    "model",
    "x",
    "values",
    "green",
    "dots",
    "could",
    "predict",
    "appropriate",
    "values",
    "remember",
    "test",
    "data",
    "set",
    "pass",
    "model",
    "x",
    "test",
    "predict",
    "test",
    "blue",
    "dots",
    "input",
    "green",
    "dots",
    "ideal",
    "output",
    "ideal",
    "output",
    "perfect",
    "model",
    "would",
    "red",
    "dots",
    "top",
    "green",
    "dots",
    "try",
    "work",
    "towards",
    "know",
    "relationship",
    "x",
    "know",
    "well",
    "set",
    "weight",
    "bias",
    "created",
    "line",
    "equals",
    "weight",
    "times",
    "x",
    "plus",
    "bias",
    "simple",
    "version",
    "linear",
    "regression",
    "formula",
    "mx",
    "plus",
    "c",
    "might",
    "heard",
    "high",
    "school",
    "algebra",
    "gradient",
    "plus",
    "intercept",
    "got",
    "said",
    "let",
    "move",
    "next",
    "video",
    "build",
    "model",
    "well",
    "exciting",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "saw",
    "get",
    "visual",
    "data",
    "followed",
    "data",
    "explorer",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "got",
    "idea",
    "training",
    "data",
    "working",
    "testing",
    "data",
    "trying",
    "build",
    "model",
    "learn",
    "patterns",
    "training",
    "data",
    "essentially",
    "upwards",
    "trend",
    "able",
    "predict",
    "testing",
    "data",
    "want",
    "give",
    "another",
    "heads",
    "took",
    "little",
    "break",
    "recording",
    "last",
    "video",
    "colab",
    "notebook",
    "disconnected",
    "going",
    "click",
    "reconnect",
    "variables",
    "may",
    "work",
    "might",
    "happen",
    "end",
    "take",
    "break",
    "using",
    "google",
    "colab",
    "come",
    "back",
    "try",
    "run",
    "function",
    "might",
    "saved",
    "looks",
    "like",
    "go",
    "restart",
    "run",
    "typically",
    "one",
    "helpful",
    "troubleshooting",
    "steps",
    "using",
    "google",
    "colab",
    "cell",
    "say",
    "working",
    "always",
    "rerun",
    "cells",
    "may",
    "help",
    "lower",
    "cell",
    "function",
    "instantiated",
    "cell",
    "run",
    "could",
    "run",
    "cell",
    "calls",
    "function",
    "rerun",
    "cell",
    "run",
    "one",
    "let",
    "get",
    "building",
    "first",
    "pytorch",
    "model",
    "going",
    "jump",
    "straight",
    "code",
    "first",
    "pytorch",
    "model",
    "exciting",
    "let",
    "turn",
    "markdown",
    "going",
    "create",
    "linear",
    "regression",
    "model",
    "look",
    "linear",
    "regression",
    "formula",
    "going",
    "create",
    "model",
    "essentially",
    "going",
    "run",
    "computation",
    "need",
    "create",
    "model",
    "parameter",
    "parameter",
    "b",
    "case",
    "going",
    "weight",
    "bias",
    "way",
    "forward",
    "computation",
    "mean",
    "going",
    "see",
    "code",
    "let",
    "pure",
    "pytorch",
    "create",
    "linear",
    "regression",
    "model",
    "class",
    "experienced",
    "using",
    "python",
    "classes",
    "going",
    "using",
    "throughout",
    "course",
    "going",
    "call",
    "one",
    "linear",
    "regression",
    "model",
    "dealt",
    "python",
    "classes",
    "okay",
    "going",
    "explaining",
    "like",
    "deeper",
    "dive",
    "recommend",
    "real",
    "python",
    "classes",
    "oop",
    "python",
    "three",
    "good",
    "rhyming",
    "going",
    "link",
    "going",
    "building",
    "classes",
    "throughout",
    "course",
    "recommend",
    "getting",
    "familiar",
    "oop",
    "object",
    "oriented",
    "programming",
    "little",
    "bit",
    "mouthful",
    "hence",
    "oop",
    "python",
    "use",
    "following",
    "resource",
    "real",
    "python",
    "going",
    "go",
    "rather",
    "code",
    "talk",
    "got",
    "class",
    "first",
    "thing",
    "might",
    "notice",
    "class",
    "inherits",
    "might",
    "wondering",
    "well",
    "well",
    "let",
    "write",
    "almost",
    "everything",
    "pytorch",
    "inherits",
    "imagine",
    "lego",
    "building",
    "bricks",
    "pytorch",
    "model",
    "lot",
    "helpful",
    "inbuilt",
    "things",
    "going",
    "help",
    "us",
    "build",
    "pytorch",
    "models",
    "course",
    "could",
    "learn",
    "well",
    "could",
    "go",
    "pytorch",
    "module",
    "go",
    "base",
    "class",
    "neural",
    "network",
    "modules",
    "wonderful",
    "models",
    "also",
    "subclass",
    "class",
    "building",
    "building",
    "pytorch",
    "model",
    "documentation",
    "says",
    "models",
    "also",
    "subclass",
    "class",
    "another",
    "thing",
    "pytorch",
    "makes",
    "might",
    "seem",
    "confusing",
    "first",
    "begin",
    "modules",
    "contain",
    "modules",
    "mean",
    "lego",
    "brick",
    "stack",
    "modules",
    "top",
    "make",
    "progressively",
    "complex",
    "neural",
    "networks",
    "go",
    "leave",
    "later",
    "going",
    "start",
    "something",
    "nice",
    "simple",
    "let",
    "clean",
    "web",
    "browser",
    "going",
    "create",
    "constructor",
    "init",
    "function",
    "going",
    "take",
    "self",
    "parameter",
    "sure",
    "going",
    "follow",
    "along",
    "code",
    "encourage",
    "read",
    "documentation",
    "video",
    "super",
    "dot",
    "init",
    "know",
    "first",
    "started",
    "learning",
    "like",
    "write",
    "knit",
    "twice",
    "super",
    "jazz",
    "take",
    "required",
    "python",
    "syntax",
    "self",
    "dot",
    "weights",
    "means",
    "going",
    "create",
    "weights",
    "parameter",
    "see",
    "second",
    "create",
    "parameter",
    "going",
    "use",
    "nn",
    "dot",
    "parameter",
    "quick",
    "reminder",
    "imported",
    "nn",
    "torch",
    "remember",
    "nn",
    "building",
    "block",
    "layer",
    "neural",
    "networks",
    "within",
    "nn",
    "nn",
    "stands",
    "neural",
    "network",
    "module",
    "got",
    "nn",
    "dot",
    "parameter",
    "going",
    "start",
    "random",
    "parameters",
    "torch",
    "dot",
    "rand",
    "one",
    "going",
    "talk",
    "second",
    "also",
    "going",
    "put",
    "requires",
    "requires",
    "grad",
    "equals",
    "true",
    "touched",
    "okay",
    "type",
    "equals",
    "torch",
    "dot",
    "float",
    "let",
    "see",
    "nn",
    "parameter",
    "tells",
    "us",
    "kind",
    "tensor",
    "considered",
    "module",
    "parameter",
    "created",
    "module",
    "using",
    "nn",
    "module",
    "parameters",
    "torch",
    "tensor",
    "subclasses",
    "tensor",
    "special",
    "property",
    "used",
    "modules",
    "assigned",
    "module",
    "attribute",
    "automatically",
    "added",
    "list",
    "parameters",
    "appear",
    "e",
    "g",
    "module",
    "dot",
    "parameters",
    "iterator",
    "oh",
    "going",
    "see",
    "later",
    "assigning",
    "tensor",
    "effect",
    "creating",
    "parameter",
    "requires",
    "grad",
    "mean",
    "well",
    "let",
    "rather",
    "try",
    "read",
    "doc",
    "string",
    "collab",
    "let",
    "look",
    "nn",
    "dot",
    "parameter",
    "say",
    "requires",
    "grad",
    "optional",
    "parameter",
    "requires",
    "gradient",
    "hmm",
    "requires",
    "gradient",
    "mean",
    "well",
    "let",
    "come",
    "back",
    "second",
    "want",
    "think",
    "type",
    "equals",
    "torch",
    "dot",
    "float",
    "data",
    "type",
    "torch",
    "dot",
    "float",
    "discussed",
    "default",
    "pytorch",
    "watch",
    "dot",
    "float",
    "could",
    "also",
    "torch",
    "dot",
    "float",
    "going",
    "leave",
    "torch",
    "float",
    "32",
    "pytorch",
    "likes",
    "work",
    "flight",
    "default",
    "necessarily",
    "set",
    "requires",
    "grad",
    "equals",
    "true",
    "keep",
    "mind",
    "created",
    "parameter",
    "weights",
    "also",
    "create",
    "parameter",
    "bias",
    "let",
    "finish",
    "creating",
    "write",
    "code",
    "talk",
    "rand",
    "requires",
    "grad",
    "equals",
    "true",
    "type",
    "equals",
    "torch",
    "dot",
    "float",
    "go",
    "going",
    "write",
    "forward",
    "method",
    "forward",
    "method",
    "define",
    "computation",
    "model",
    "let",
    "go",
    "def",
    "forward",
    "self",
    "takes",
    "parameter",
    "x",
    "data",
    "x",
    "expected",
    "type",
    "torch",
    "tensor",
    "returns",
    "torch",
    "dot",
    "tensor",
    "go",
    "say",
    "x",
    "necessarily",
    "need",
    "comment",
    "going",
    "write",
    "anyway",
    "x",
    "input",
    "data",
    "case",
    "might",
    "training",
    "data",
    "want",
    "return",
    "self",
    "dot",
    "weights",
    "times",
    "x",
    "plus",
    "self",
    "dot",
    "bias",
    "seen",
    "well",
    "linear",
    "regression",
    "formula",
    "let",
    "take",
    "step",
    "back",
    "created",
    "data",
    "go",
    "back",
    "talk",
    "little",
    "bit",
    "going",
    "go",
    "back",
    "data",
    "create",
    "created",
    "see",
    "created",
    "known",
    "parameters",
    "weight",
    "bias",
    "created",
    "variable",
    "target",
    "using",
    "linear",
    "regression",
    "formula",
    "wait",
    "times",
    "x",
    "plus",
    "bias",
    "x",
    "range",
    "numbers",
    "done",
    "linear",
    "regression",
    "model",
    "created",
    "scratch",
    "go",
    "created",
    "parameter",
    "weights",
    "could",
    "weight",
    "wanted",
    "created",
    "parameter",
    "created",
    "data",
    "knew",
    "parameters",
    "weight",
    "bias",
    "whole",
    "goal",
    "model",
    "start",
    "random",
    "numbers",
    "going",
    "random",
    "parameters",
    "look",
    "data",
    "case",
    "training",
    "samples",
    "update",
    "random",
    "numbers",
    "represent",
    "pattern",
    "ideally",
    "model",
    "learning",
    "correctly",
    "take",
    "weight",
    "going",
    "random",
    "value",
    "bias",
    "going",
    "random",
    "value",
    "run",
    "forward",
    "calculation",
    "formula",
    "use",
    "create",
    "data",
    "adjust",
    "weight",
    "bias",
    "represent",
    "close",
    "possible",
    "perfect",
    "known",
    "parameters",
    "premise",
    "machine",
    "learning",
    "algorithm",
    "called",
    "gradient",
    "descent",
    "going",
    "write",
    "talked",
    "lot",
    "like",
    "tie",
    "together",
    "model",
    "start",
    "random",
    "values",
    "weight",
    "bias",
    "look",
    "training",
    "data",
    "adjust",
    "random",
    "values",
    "better",
    "represent",
    "get",
    "closer",
    "ideal",
    "values",
    "weight",
    "bias",
    "values",
    "use",
    "create",
    "data",
    "going",
    "going",
    "start",
    "random",
    "values",
    "continually",
    "look",
    "training",
    "data",
    "see",
    "adjust",
    "random",
    "values",
    "would",
    "represent",
    "straight",
    "line",
    "two",
    "main",
    "algorithms",
    "one",
    "gradient",
    "descent",
    "two",
    "back",
    "propagation",
    "going",
    "leave",
    "time",
    "going",
    "continue",
    "talking",
    "gradient",
    "descent",
    "requires",
    "grad",
    "equals",
    "true",
    "going",
    "run",
    "computations",
    "using",
    "model",
    "pytorch",
    "going",
    "keep",
    "track",
    "gradients",
    "weights",
    "parameter",
    "bias",
    "parameter",
    "going",
    "update",
    "combination",
    "gradient",
    "descent",
    "back",
    "propagation",
    "going",
    "leave",
    "extracurricular",
    "look",
    "gradient",
    "descent",
    "back",
    "propagation",
    "going",
    "add",
    "resources",
    "also",
    "plenty",
    "resources",
    "pytorch",
    "workflow",
    "fundamentals",
    "book",
    "chapter",
    "algorithms",
    "work",
    "behind",
    "scenes",
    "going",
    "focused",
    "code",
    "pytorch",
    "code",
    "trigger",
    "algorithms",
    "behind",
    "scenes",
    "pytorch",
    "lucky",
    "us",
    "implemented",
    "gradient",
    "descent",
    "back",
    "propagation",
    "us",
    "writing",
    "higher",
    "level",
    "code",
    "trigger",
    "two",
    "algorithms",
    "next",
    "video",
    "going",
    "step",
    "little",
    "bit",
    "discuss",
    "useful",
    "required",
    "modules",
    "pytorch",
    "particularly",
    "n",
    "couple",
    "others",
    "let",
    "leave",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "whole",
    "bunch",
    "creating",
    "first",
    "pytorch",
    "model",
    "inherits",
    "talked",
    "object",
    "oriented",
    "programming",
    "lot",
    "pytorch",
    "uses",
    "object",
    "oriented",
    "programming",
    "ca",
    "say",
    "might",
    "say",
    "oop",
    "done",
    "since",
    "last",
    "video",
    "though",
    "added",
    "two",
    "resources",
    "gradient",
    "descent",
    "back",
    "propagation",
    "two",
    "favorite",
    "videos",
    "youtube",
    "channel",
    "three",
    "blue",
    "one",
    "brown",
    "gradient",
    "descent",
    "would",
    "highly",
    "recommend",
    "watching",
    "entire",
    "series",
    "way",
    "extra",
    "curriculum",
    "video",
    "particular",
    "course",
    "overall",
    "go",
    "two",
    "videos",
    "even",
    "sure",
    "entirely",
    "happening",
    "gain",
    "intuition",
    "code",
    "going",
    "writing",
    "pytorch",
    "keep",
    "mind",
    "go",
    "forward",
    "lot",
    "pytorch",
    "behind",
    "scenes",
    "us",
    "taking",
    "care",
    "two",
    "algorithms",
    "us",
    "also",
    "created",
    "two",
    "parameters",
    "model",
    "instantiated",
    "random",
    "values",
    "one",
    "parameter",
    "ones",
    "use",
    "weight",
    "bias",
    "data",
    "set",
    "want",
    "keep",
    "mind",
    "working",
    "simple",
    "data",
    "set",
    "created",
    "known",
    "parameters",
    "data",
    "set",
    "created",
    "maybe",
    "gathered",
    "internet",
    "images",
    "wo",
    "necessarily",
    "defining",
    "parameters",
    "instead",
    "another",
    "module",
    "nn",
    "define",
    "parameters",
    "work",
    "parameters",
    "end",
    "since",
    "working",
    "simple",
    "data",
    "set",
    "define",
    "two",
    "parameters",
    "trying",
    "estimate",
    "key",
    "point",
    "model",
    "going",
    "start",
    "random",
    "values",
    "annotation",
    "added",
    "start",
    "random",
    "weight",
    "value",
    "using",
    "torch",
    "random",
    "told",
    "update",
    "via",
    "gradient",
    "descent",
    "pytorch",
    "going",
    "track",
    "gradients",
    "parameter",
    "us",
    "told",
    "type",
    "want",
    "float",
    "necessarily",
    "need",
    "two",
    "set",
    "explicitly",
    "lot",
    "time",
    "default",
    "pytorch",
    "set",
    "two",
    "requires",
    "grad",
    "equals",
    "true",
    "type",
    "equals",
    "torch",
    "dot",
    "float",
    "us",
    "behind",
    "scenes",
    "keep",
    "things",
    "fundamental",
    "straightforward",
    "possible",
    "set",
    "explicitly",
    "let",
    "jump",
    "keynote",
    "like",
    "explain",
    "going",
    "one",
    "time",
    "visual",
    "sense",
    "exact",
    "code",
    "written",
    "copied",
    "made",
    "little",
    "bit",
    "colorful",
    "going",
    "build",
    "model",
    "pytorch",
    "subclasses",
    "class",
    "contains",
    "building",
    "blocks",
    "neural",
    "networks",
    "class",
    "model",
    "subclasses",
    "inside",
    "constructor",
    "initialize",
    "model",
    "parameters",
    "see",
    "later",
    "bigger",
    "models",
    "wo",
    "necessarily",
    "always",
    "explicitly",
    "create",
    "weights",
    "biases",
    "might",
    "initialize",
    "whole",
    "layers",
    "concept",
    "touched",
    "yet",
    "might",
    "initialize",
    "list",
    "layers",
    "whatever",
    "need",
    "basically",
    "happens",
    "create",
    "whatever",
    "variables",
    "need",
    "model",
    "use",
    "could",
    "different",
    "layers",
    "single",
    "parameters",
    "done",
    "case",
    "hard",
    "coded",
    "values",
    "even",
    "functions",
    "explicitly",
    "set",
    "requires",
    "grad",
    "equals",
    "true",
    "model",
    "parameters",
    "turn",
    "means",
    "pytorch",
    "behind",
    "scenes",
    "track",
    "gradients",
    "parameters",
    "use",
    "grad",
    "grad",
    "module",
    "pytorch",
    "implements",
    "gradient",
    "descent",
    "lot",
    "happen",
    "behind",
    "scenes",
    "write",
    "pytorch",
    "training",
    "code",
    "like",
    "know",
    "happening",
    "behind",
    "scenes",
    "highly",
    "recommend",
    "checking",
    "two",
    "videos",
    "hence",
    "linked",
    "oh",
    "many",
    "modules",
    "requires",
    "grad",
    "true",
    "set",
    "default",
    "finally",
    "got",
    "forward",
    "method",
    "subclass",
    "done",
    "requires",
    "forward",
    "method",
    "see",
    "documentation",
    "go",
    "torch",
    "dot",
    "click",
    "module",
    "forward",
    "yeah",
    "go",
    "forward",
    "got",
    "lot",
    "things",
    "built",
    "see",
    "subclass",
    "forward",
    "forward",
    "defines",
    "computation",
    "performed",
    "every",
    "call",
    "call",
    "linear",
    "regression",
    "model",
    "put",
    "data",
    "forward",
    "method",
    "operation",
    "module",
    "model",
    "case",
    "forward",
    "method",
    "linear",
    "regression",
    "function",
    "keep",
    "mind",
    "subclass",
    "needs",
    "override",
    "forward",
    "method",
    "need",
    "define",
    "forward",
    "method",
    "going",
    "subclass",
    "see",
    "hands",
    "believe",
    "enough",
    "coverage",
    "done",
    "questions",
    "remember",
    "ask",
    "discussions",
    "got",
    "fair",
    "bit",
    "going",
    "think",
    "broken",
    "fair",
    "bit",
    "next",
    "step",
    "us",
    "know",
    "mentioned",
    "previous",
    "video",
    "cover",
    "pytorch",
    "model",
    "building",
    "essentials",
    "going",
    "cover",
    "seen",
    "already",
    "next",
    "way",
    "really",
    "start",
    "understand",
    "going",
    "check",
    "contents",
    "model",
    "train",
    "one",
    "make",
    "predictions",
    "let",
    "get",
    "hands",
    "next",
    "videos",
    "see",
    "welcome",
    "back",
    "last",
    "couple",
    "videos",
    "stepped",
    "creating",
    "first",
    "pytorch",
    "model",
    "looks",
    "like",
    "fair",
    "bit",
    "going",
    "main",
    "takeaways",
    "almost",
    "every",
    "model",
    "pytorch",
    "inherits",
    "going",
    "inherit",
    "override",
    "forward",
    "method",
    "define",
    "computation",
    "happening",
    "model",
    "later",
    "model",
    "learning",
    "things",
    "words",
    "updating",
    "weights",
    "bias",
    "values",
    "random",
    "values",
    "values",
    "better",
    "fit",
    "data",
    "going",
    "via",
    "gradient",
    "descent",
    "back",
    "propagation",
    "two",
    "videos",
    "extra",
    "curriculum",
    "happening",
    "behind",
    "scenes",
    "actually",
    "written",
    "code",
    "yet",
    "trigger",
    "two",
    "refer",
    "back",
    "actually",
    "write",
    "code",
    "got",
    "model",
    "defines",
    "forward",
    "computation",
    "speaking",
    "models",
    "let",
    "look",
    "couple",
    "pytorch",
    "model",
    "building",
    "essentials",
    "going",
    "write",
    "much",
    "code",
    "video",
    "going",
    "relatively",
    "short",
    "want",
    "introduce",
    "main",
    "classes",
    "going",
    "interacting",
    "pytorch",
    "seen",
    "already",
    "one",
    "first",
    "contains",
    "building",
    "blocks",
    "computational",
    "graphs",
    "computational",
    "graphs",
    "another",
    "word",
    "neural",
    "networks",
    "well",
    "actually",
    "computational",
    "graphs",
    "quite",
    "general",
    "write",
    "neural",
    "network",
    "considered",
    "computational",
    "graph",
    "seen",
    "parameters",
    "model",
    "try",
    "learn",
    "write",
    "often",
    "pytorch",
    "layer",
    "set",
    "us",
    "got",
    "seen",
    "base",
    "class",
    "neural",
    "network",
    "modules",
    "subclass",
    "overwrite",
    "forward",
    "done",
    "created",
    "forward",
    "method",
    "else",
    "cover",
    "going",
    "see",
    "later",
    "going",
    "put",
    "optimizers",
    "pytorch",
    "live",
    "help",
    "gradient",
    "descent",
    "optimizer",
    "optimizer",
    "said",
    "model",
    "starts",
    "random",
    "values",
    "looks",
    "training",
    "data",
    "adjusts",
    "random",
    "values",
    "better",
    "represent",
    "ideal",
    "values",
    "optimizer",
    "contains",
    "algorithm",
    "going",
    "optimize",
    "values",
    "instead",
    "random",
    "values",
    "better",
    "represent",
    "data",
    "algorithms",
    "live",
    "one",
    "link",
    "extra",
    "resources",
    "going",
    "cover",
    "go",
    "like",
    "things",
    "cover",
    "need",
    "forward",
    "method",
    "going",
    "explicitly",
    "say",
    "subclasses",
    "require",
    "overwrite",
    "forward",
    "method",
    "defines",
    "happens",
    "forward",
    "computation",
    "case",
    "pass",
    "data",
    "linear",
    "regression",
    "model",
    "forward",
    "method",
    "would",
    "take",
    "data",
    "perform",
    "computation",
    "models",
    "get",
    "bigger",
    "bigger",
    "quite",
    "straightforward",
    "forward",
    "computation",
    "simple",
    "complex",
    "like",
    "depending",
    "like",
    "model",
    "got",
    "nice",
    "fancy",
    "slide",
    "basically",
    "reiterates",
    "discussed",
    "pytorch",
    "central",
    "neural",
    "network",
    "building",
    "modules",
    "module",
    "actually",
    "talked",
    "yet",
    "believe",
    "one",
    "data",
    "loader",
    "going",
    "see",
    "two",
    "later",
    "helpful",
    "got",
    "bit",
    "complicated",
    "data",
    "set",
    "case",
    "got",
    "50",
    "integers",
    "data",
    "set",
    "got",
    "simple",
    "straight",
    "line",
    "need",
    "create",
    "complex",
    "data",
    "sets",
    "going",
    "use",
    "help",
    "us",
    "build",
    "models",
    "help",
    "us",
    "optimize",
    "models",
    "parameters",
    "help",
    "us",
    "load",
    "data",
    "like",
    "one",
    "favorite",
    "resources",
    "pytorch",
    "cheat",
    "sheet",
    "referring",
    "back",
    "documentation",
    "see",
    "documentation",
    "right",
    "said",
    "course",
    "replacement",
    "documentation",
    "interpretation",
    "one",
    "best",
    "become",
    "familiar",
    "pytorch",
    "got",
    "imports",
    "general",
    "import",
    "torch",
    "data",
    "loader",
    "oh",
    "look",
    "got",
    "mentioned",
    "data",
    "data",
    "set",
    "data",
    "loader",
    "torch",
    "script",
    "jit",
    "neural",
    "network",
    "api",
    "want",
    "let",
    "go",
    "covering",
    "fundamental",
    "ones",
    "course",
    "pytorch",
    "quite",
    "big",
    "library",
    "extra",
    "curricula",
    "video",
    "would",
    "go",
    "five",
    "10",
    "minutes",
    "read",
    "understand",
    "going",
    "start",
    "get",
    "familiar",
    "mean",
    "would",
    "require",
    "making",
    "videos",
    "whole",
    "documentation",
    "lot",
    "writing",
    "via",
    "code",
    "enough",
    "video",
    "link",
    "pytorch",
    "cheat",
    "sheet",
    "video",
    "next",
    "video",
    "actually",
    "checked",
    "happens",
    "create",
    "instance",
    "linear",
    "regression",
    "model",
    "think",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "pytorch",
    "model",
    "building",
    "essentials",
    "look",
    "linked",
    "cheat",
    "sheet",
    "lot",
    "going",
    "lot",
    "text",
    "going",
    "page",
    "course",
    "reference",
    "material",
    "learn",
    "pytorch",
    "book",
    "pytorch",
    "model",
    "building",
    "essentials",
    "notebook",
    "working",
    "could",
    "help",
    "wanted",
    "add",
    "color",
    "inspect",
    "model",
    "let",
    "add",
    "little",
    "bit",
    "color",
    "text",
    "page",
    "go",
    "whoa",
    "workflow",
    "covering",
    "video",
    "right",
    "module",
    "get",
    "data",
    "ready",
    "important",
    "pytorch",
    "modules",
    "see",
    "cover",
    "computer",
    "vision",
    "later",
    "set",
    "want",
    "create",
    "data",
    "set",
    "little",
    "bit",
    "complicated",
    "data",
    "set",
    "simple",
    "used",
    "either",
    "data",
    "set",
    "creator",
    "data",
    "loader",
    "go",
    "build",
    "picker",
    "model",
    "well",
    "use",
    "seen",
    "one",
    "seen",
    "case",
    "building",
    "model",
    "wanted",
    "model",
    "well",
    "computer",
    "vision",
    "models",
    "already",
    "built",
    "us",
    "torchvision",
    "stands",
    "pytorch",
    "computer",
    "vision",
    "module",
    "covered",
    "either",
    "spoiler",
    "coming",
    "later",
    "optimizer",
    "wanted",
    "optimize",
    "model",
    "parameters",
    "better",
    "represent",
    "data",
    "set",
    "go",
    "wanted",
    "evaluate",
    "model",
    "well",
    "got",
    "torch",
    "metrics",
    "seen",
    "going",
    "later",
    "wanted",
    "improve",
    "experimentation",
    "got",
    "hmm",
    "want",
    "pytorch",
    "cheat",
    "sheet",
    "adding",
    "little",
    "bit",
    "color",
    "little",
    "bit",
    "code",
    "pytorch",
    "workflow",
    "said",
    "let",
    "get",
    "little",
    "bit",
    "deeper",
    "built",
    "first",
    "pytorch",
    "model",
    "checking",
    "contents",
    "pytorch",
    "model",
    "created",
    "model",
    "let",
    "see",
    "inside",
    "might",
    "already",
    "able",
    "guess",
    "fact",
    "created",
    "constructor",
    "init",
    "function",
    "think",
    "inside",
    "model",
    "think",
    "look",
    "course",
    "questions",
    "might",
    "answer",
    "like",
    "daniel",
    "starting",
    "learn",
    "pytorch",
    "know",
    "asking",
    "start",
    "thinking",
    "different",
    "things",
    "know",
    "check",
    "model",
    "parameters",
    "inside",
    "model",
    "using",
    "wait",
    "dot",
    "parameters",
    "oh",
    "love",
    "things",
    "nice",
    "simple",
    "well",
    "let",
    "check",
    "hey",
    "well",
    "first",
    "things",
    "going",
    "let",
    "create",
    "random",
    "seed",
    "creating",
    "random",
    "seed",
    "well",
    "recall",
    "creating",
    "parameters",
    "random",
    "values",
    "create",
    "outer",
    "random",
    "seed",
    "would",
    "get",
    "different",
    "values",
    "every",
    "time",
    "sake",
    "educational",
    "sense",
    "sake",
    "video",
    "going",
    "create",
    "manual",
    "seed",
    "torch",
    "dot",
    "manual",
    "seed",
    "going",
    "use",
    "42",
    "maybe",
    "43",
    "could",
    "use",
    "43",
    "42",
    "love",
    "answer",
    "universe",
    "going",
    "create",
    "instance",
    "model",
    "created",
    "subclass",
    "end",
    "module",
    "let",
    "model",
    "zero",
    "going",
    "zeroth",
    "model",
    "first",
    "model",
    "ever",
    "created",
    "whole",
    "course",
    "amazing",
    "linear",
    "regression",
    "model",
    "class",
    "called",
    "call",
    "like",
    "calling",
    "class",
    "let",
    "see",
    "happens",
    "go",
    "model",
    "zero",
    "give",
    "us",
    "oh",
    "linear",
    "regression",
    "okay",
    "give",
    "us",
    "much",
    "want",
    "find",
    "going",
    "check",
    "parameters",
    "model",
    "zero",
    "dot",
    "parameters",
    "get",
    "oh",
    "generator",
    "well",
    "let",
    "turn",
    "list",
    "better",
    "look",
    "go",
    "oh",
    "exciting",
    "parameter",
    "containing",
    "look",
    "values",
    "tensor",
    "requires",
    "grad",
    "equals",
    "true",
    "parameter",
    "containing",
    "wonderful",
    "model",
    "parameters",
    "values",
    "well",
    "used",
    "torch",
    "rand",
    "let",
    "see",
    "happens",
    "go",
    "let",
    "create",
    "torch",
    "dot",
    "rand",
    "n",
    "one",
    "happens",
    "get",
    "value",
    "like",
    "run",
    "get",
    "values",
    "run",
    "keep",
    "one",
    "two",
    "three",
    "four",
    "five",
    "actually",
    "wow",
    "pretty",
    "cool",
    "got",
    "random",
    "value",
    "order",
    "four",
    "row",
    "twice",
    "row",
    "probably",
    "oh",
    "get",
    "one",
    "oh",
    "get",
    "different",
    "one",
    "get",
    "one",
    "twice",
    "oh",
    "gosh",
    "got",
    "value",
    "twice",
    "row",
    "saw",
    "saw",
    "incredible",
    "reason",
    "get",
    "one",
    "different",
    "every",
    "time",
    "random",
    "seed",
    "watch",
    "put",
    "random",
    "seed",
    "torch",
    "dot",
    "manual",
    "seed",
    "42",
    "3",
    "3",
    "6",
    "7",
    "happens",
    "3",
    "3",
    "6",
    "7",
    "happens",
    "3",
    "3",
    "6",
    "okay",
    "commented",
    "random",
    "seed",
    "initialized",
    "model",
    "different",
    "values",
    "two",
    "three",
    "five",
    "two",
    "three",
    "four",
    "five",
    "must",
    "like",
    "value",
    "oh",
    "goodness",
    "let",
    "know",
    "get",
    "value",
    "right",
    "keep",
    "going",
    "get",
    "different",
    "values",
    "every",
    "single",
    "time",
    "getting",
    "different",
    "values",
    "every",
    "single",
    "time",
    "might",
    "daniel",
    "sound",
    "like",
    "broken",
    "record",
    "trying",
    "really",
    "drive",
    "home",
    "fact",
    "initialize",
    "models",
    "random",
    "parameters",
    "essence",
    "machine",
    "learning",
    "models",
    "deep",
    "learning",
    "models",
    "going",
    "start",
    "random",
    "values",
    "weights",
    "bias",
    "maybe",
    "got",
    "two",
    "parameters",
    "future",
    "models",
    "build",
    "might",
    "thousands",
    "course",
    "going",
    "hand",
    "see",
    "later",
    "start",
    "random",
    "values",
    "ideal",
    "model",
    "look",
    "training",
    "data",
    "adjust",
    "random",
    "values",
    "get",
    "reproducible",
    "results",
    "get",
    "rid",
    "cell",
    "set",
    "random",
    "seed",
    "getting",
    "similar",
    "values",
    "maybe",
    "sort",
    "pytorch",
    "update",
    "random",
    "seeds",
    "calculated",
    "might",
    "get",
    "slightly",
    "different",
    "values",
    "use",
    "want",
    "aware",
    "little",
    "bit",
    "confusing",
    "list",
    "parameters",
    "understand",
    "better",
    "list",
    "name",
    "parameters",
    "way",
    "model",
    "zero",
    "call",
    "state",
    "dict",
    "going",
    "give",
    "us",
    "dictionary",
    "parameters",
    "model",
    "see",
    "got",
    "weights",
    "got",
    "bias",
    "random",
    "values",
    "weights",
    "bias",
    "come",
    "well",
    "course",
    "came",
    "weights",
    "bias",
    "course",
    "well",
    "got",
    "known",
    "parameters",
    "whole",
    "goal",
    "whole",
    "goal",
    "build",
    "code",
    "write",
    "code",
    "going",
    "allow",
    "model",
    "look",
    "blue",
    "dots",
    "adjust",
    "weight",
    "bias",
    "value",
    "weights",
    "close",
    "possible",
    "weight",
    "bias",
    "go",
    "well",
    "going",
    "see",
    "future",
    "videos",
    "closer",
    "get",
    "values",
    "two",
    "better",
    "going",
    "able",
    "predict",
    "model",
    "data",
    "principle",
    "stress",
    "enough",
    "fundamental",
    "entire",
    "foundation",
    "fundamental",
    "foundation",
    "well",
    "good",
    "description",
    "daniel",
    "entire",
    "foundation",
    "deep",
    "learning",
    "start",
    "random",
    "values",
    "use",
    "gradient",
    "descent",
    "back",
    "propagation",
    "plus",
    "whatever",
    "data",
    "working",
    "move",
    "random",
    "values",
    "close",
    "possible",
    "ideal",
    "values",
    "cases",
    "wo",
    "know",
    "ideal",
    "values",
    "simple",
    "case",
    "already",
    "know",
    "ideal",
    "values",
    "keep",
    "mind",
    "going",
    "forward",
    "premise",
    "deep",
    "learning",
    "start",
    "random",
    "values",
    "make",
    "representative",
    "closer",
    "ideal",
    "values",
    "said",
    "let",
    "try",
    "make",
    "predictions",
    "model",
    "mean",
    "got",
    "random",
    "values",
    "think",
    "predictions",
    "go",
    "think",
    "next",
    "video",
    "make",
    "predictions",
    "test",
    "data",
    "see",
    "look",
    "like",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "checked",
    "internals",
    "first",
    "pytorch",
    "model",
    "found",
    "creating",
    "model",
    "torch",
    "dot",
    "parameters",
    "model",
    "torch",
    "dot",
    "rand",
    "begin",
    "random",
    "variables",
    "also",
    "discussed",
    "entire",
    "premise",
    "deep",
    "learning",
    "start",
    "random",
    "numbers",
    "slowly",
    "progress",
    "towards",
    "ideal",
    "numbers",
    "slightly",
    "less",
    "random",
    "numbers",
    "based",
    "data",
    "let",
    "see",
    "start",
    "improve",
    "numbers",
    "let",
    "see",
    "predictive",
    "power",
    "like",
    "right",
    "might",
    "able",
    "guess",
    "well",
    "random",
    "numbers",
    "able",
    "predict",
    "data",
    "sure",
    "predicting",
    "means",
    "let",
    "look",
    "making",
    "predictions",
    "using",
    "torch",
    "dot",
    "inference",
    "mode",
    "something",
    "seen",
    "always",
    "going",
    "discuss",
    "use",
    "check",
    "models",
    "predictive",
    "power",
    "let",
    "see",
    "well",
    "predicts",
    "test",
    "based",
    "x",
    "test",
    "remember",
    "another",
    "premise",
    "machine",
    "learning",
    "model",
    "take",
    "features",
    "input",
    "make",
    "predictions",
    "close",
    "sort",
    "labels",
    "pass",
    "data",
    "model",
    "going",
    "run",
    "forward",
    "method",
    "little",
    "bit",
    "confusing",
    "defined",
    "forward",
    "method",
    "takes",
    "x",
    "input",
    "done",
    "little",
    "x",
    "going",
    "pass",
    "large",
    "x",
    "input",
    "reason",
    "done",
    "little",
    "x",
    "oftentimes",
    "pytorch",
    "code",
    "going",
    "find",
    "internet",
    "x",
    "quite",
    "common",
    "commonly",
    "used",
    "forward",
    "method",
    "like",
    "input",
    "data",
    "left",
    "going",
    "find",
    "quite",
    "often",
    "let",
    "test",
    "discussed",
    "inference",
    "mode",
    "yet",
    "make",
    "predictions",
    "model",
    "torch",
    "dot",
    "inference",
    "mode",
    "let",
    "use",
    "discuss",
    "going",
    "threads",
    "equals",
    "model",
    "zero",
    "x",
    "test",
    "passing",
    "x",
    "test",
    "data",
    "model",
    "pass",
    "x",
    "test",
    "let",
    "remind",
    "x",
    "test",
    "x",
    "test",
    "10",
    "variables",
    "trying",
    "ideal",
    "model",
    "predict",
    "exact",
    "values",
    "test",
    "model",
    "perfect",
    "model",
    "take",
    "x",
    "test",
    "values",
    "input",
    "return",
    "test",
    "values",
    "output",
    "ideal",
    "model",
    "predictions",
    "exact",
    "test",
    "data",
    "set",
    "think",
    "model",
    "go",
    "considering",
    "starting",
    "random",
    "values",
    "parameters",
    "well",
    "let",
    "find",
    "hey",
    "threads",
    "oh",
    "happened",
    "implemented",
    "error",
    "ah",
    "error",
    "get",
    "quite",
    "often",
    "google",
    "colab",
    "creating",
    "model",
    "usually",
    "happens",
    "glad",
    "stumbled",
    "upon",
    "think",
    "know",
    "fix",
    "might",
    "see",
    "little",
    "bit",
    "troubleshooting",
    "video",
    "create",
    "see",
    "implemented",
    "error",
    "right",
    "saying",
    "ford",
    "method",
    "go",
    "ford",
    "implemented",
    "go",
    "little",
    "bit",
    "rabbit",
    "hole",
    "implemented",
    "area",
    "come",
    "across",
    "fair",
    "times",
    "took",
    "figure",
    "reason",
    "spacing",
    "python",
    "know",
    "space",
    "space",
    "defines",
    "function",
    "space",
    "space",
    "another",
    "thing",
    "another",
    "line",
    "reason",
    "look",
    "line",
    "notebook",
    "way",
    "lines",
    "numbers",
    "go",
    "tools",
    "settings",
    "editor",
    "define",
    "show",
    "line",
    "numbers",
    "show",
    "notation",
    "guides",
    "sort",
    "jazz",
    "customize",
    "going",
    "two",
    "run",
    "error",
    "fair",
    "times",
    "ford",
    "method",
    "line",
    "bracket",
    "need",
    "highlight",
    "click",
    "shift",
    "tab",
    "move",
    "see",
    "line",
    "run",
    "wo",
    "change",
    "output",
    "see",
    "hidden",
    "gotcha",
    "ran",
    "found",
    "error",
    "run",
    "works",
    "keep",
    "mind",
    "really",
    "glad",
    "stumbled",
    "upon",
    "indentation",
    "errors",
    "implemented",
    "errors",
    "one",
    "common",
    "errors",
    "find",
    "pytorch",
    "well",
    "writing",
    "pytorch",
    "code",
    "google",
    "colab",
    "sure",
    "happens",
    "models",
    "predictions",
    "far",
    "running",
    "test",
    "data",
    "models",
    "ford",
    "method",
    "defined",
    "look",
    "test",
    "close",
    "oh",
    "gosh",
    "shocking",
    "visualize",
    "plot",
    "predictions",
    "going",
    "put",
    "predictions",
    "equals",
    "threads",
    "let",
    "look",
    "oh",
    "goodness",
    "way",
    "remember",
    "discussed",
    "ideal",
    "model",
    "red",
    "dots",
    "top",
    "green",
    "dots",
    "ideal",
    "model",
    "perfectly",
    "predicting",
    "test",
    "data",
    "right",
    "model",
    "initialized",
    "random",
    "parameters",
    "basically",
    "making",
    "random",
    "predictions",
    "extremely",
    "far",
    "ideal",
    "predictions",
    "training",
    "data",
    "model",
    "predictions",
    "first",
    "create",
    "model",
    "quite",
    "bad",
    "want",
    "write",
    "code",
    "hopefully",
    "move",
    "red",
    "dots",
    "closer",
    "green",
    "dots",
    "going",
    "see",
    "later",
    "videos",
    "one",
    "thing",
    "discussed",
    "torch",
    "dot",
    "inference",
    "mode",
    "context",
    "manager",
    "happens",
    "making",
    "predictions",
    "making",
    "predictions",
    "another",
    "word",
    "predictions",
    "inference",
    "torch",
    "uses",
    "inference",
    "try",
    "use",
    "bit",
    "like",
    "use",
    "predictions",
    "well",
    "could",
    "also",
    "go",
    "preds",
    "equals",
    "model",
    "zero",
    "dot",
    "x",
    "test",
    "going",
    "get",
    "quite",
    "similar",
    "output",
    "right",
    "put",
    "inference",
    "mode",
    "want",
    "start",
    "making",
    "habit",
    "later",
    "make",
    "predictions",
    "put",
    "inference",
    "mode",
    "might",
    "notice",
    "something",
    "different",
    "difference",
    "outputs",
    "preds",
    "equals",
    "model",
    "inference",
    "mode",
    "context",
    "manager",
    "notice",
    "grad",
    "function",
    "need",
    "go",
    "discussing",
    "exactly",
    "notice",
    "one",
    "lacking",
    "grad",
    "function",
    "remember",
    "behind",
    "scenes",
    "said",
    "pie",
    "torch",
    "things",
    "requires",
    "grad",
    "equals",
    "true",
    "keeps",
    "track",
    "gradients",
    "different",
    "parameters",
    "used",
    "gradient",
    "descent",
    "back",
    "propagation",
    "inference",
    "mode",
    "turns",
    "gradient",
    "tracking",
    "essentially",
    "removes",
    "inference",
    "training",
    "need",
    "keep",
    "track",
    "gradient",
    "need",
    "keep",
    "track",
    "update",
    "models",
    "inference",
    "mode",
    "disables",
    "useful",
    "things",
    "available",
    "training",
    "benefit",
    "well",
    "means",
    "pie",
    "torch",
    "behind",
    "scenes",
    "keeping",
    "track",
    "less",
    "data",
    "turn",
    "small",
    "data",
    "set",
    "probably",
    "wo",
    "dramatic",
    "larger",
    "data",
    "set",
    "means",
    "predictions",
    "potentially",
    "lot",
    "faster",
    "whole",
    "bunch",
    "numbers",
    "kept",
    "track",
    "whole",
    "bunch",
    "things",
    "need",
    "prediction",
    "mode",
    "inference",
    "mode",
    "called",
    "inference",
    "mode",
    "saved",
    "memory",
    "like",
    "learn",
    "go",
    "pie",
    "torch",
    "inference",
    "mode",
    "twitter",
    "remember",
    "search",
    "twitter",
    "big",
    "tweet",
    "storm",
    "go",
    "oh",
    "another",
    "thing",
    "cover",
    "going",
    "copy",
    "also",
    "blog",
    "post",
    "going",
    "behind",
    "scenes",
    "long",
    "story",
    "short",
    "makes",
    "code",
    "faster",
    "want",
    "make",
    "inference",
    "code",
    "pie",
    "torch",
    "run",
    "faster",
    "quick",
    "thread",
    "exactly",
    "going",
    "write",
    "see",
    "inference",
    "mode",
    "want",
    "highlight",
    "something",
    "well",
    "referenced",
    "torch",
    "grad",
    "torch",
    "inference",
    "mode",
    "context",
    "manager",
    "inference",
    "mode",
    "fairly",
    "new",
    "pie",
    "torch",
    "might",
    "see",
    "lot",
    "code",
    "existing",
    "pie",
    "torch",
    "code",
    "torch",
    "dot",
    "grad",
    "use",
    "well",
    "preds",
    "equals",
    "model",
    "zero",
    "much",
    "inference",
    "mode",
    "inference",
    "mode",
    "things",
    "advantages",
    "grad",
    "discussed",
    "thread",
    "get",
    "similar",
    "output",
    "got",
    "grad",
    "function",
    "read",
    "pie",
    "torch",
    "documentation",
    "inference",
    "mode",
    "favored",
    "way",
    "inference",
    "wanted",
    "highlight",
    "also",
    "something",
    "similar",
    "torch",
    "dot",
    "grad",
    "however",
    "inference",
    "mode",
    "preferred",
    "alrighty",
    "going",
    "comment",
    "one",
    "thing",
    "going",
    "main",
    "takeaway",
    "video",
    "making",
    "predictions",
    "use",
    "context",
    "manager",
    "torch",
    "dot",
    "inference",
    "mode",
    "right",
    "models",
    "variables",
    "internal",
    "parameters",
    "randomly",
    "initialized",
    "models",
    "predictions",
    "good",
    "random",
    "actually",
    "far",
    "values",
    "least",
    "red",
    "dots",
    "like",
    "scattered",
    "upcoming",
    "videos",
    "going",
    "writing",
    "pie",
    "torch",
    "training",
    "code",
    "move",
    "values",
    "closer",
    "green",
    "dots",
    "looking",
    "training",
    "data",
    "said",
    "see",
    "next",
    "video",
    "friends",
    "welcome",
    "back",
    "last",
    "video",
    "saw",
    "model",
    "performs",
    "pretty",
    "poorly",
    "like",
    "ideally",
    "red",
    "dots",
    "line",
    "green",
    "dots",
    "know",
    "well",
    "model",
    "initialized",
    "random",
    "parameters",
    "want",
    "put",
    "little",
    "note",
    "necessarily",
    "initialize",
    "model",
    "random",
    "parameters",
    "could",
    "initialize",
    "could",
    "zero",
    "yeah",
    "two",
    "values",
    "weights",
    "bias",
    "could",
    "zero",
    "could",
    "go",
    "could",
    "also",
    "use",
    "parameters",
    "another",
    "model",
    "going",
    "see",
    "later",
    "something",
    "called",
    "transfer",
    "learning",
    "little",
    "spoiler",
    "come",
    "also",
    "discussed",
    "ideal",
    "model",
    "replicate",
    "known",
    "parameters",
    "words",
    "start",
    "random",
    "unknown",
    "parameters",
    "two",
    "values",
    "want",
    "write",
    "code",
    "model",
    "move",
    "towards",
    "estimating",
    "ideal",
    "parameters",
    "want",
    "explicit",
    "write",
    "intuition",
    "jump",
    "training",
    "code",
    "exciting",
    "get",
    "training",
    "first",
    "machine",
    "learning",
    "model",
    "right",
    "whole",
    "idea",
    "training",
    "model",
    "move",
    "unknown",
    "parameters",
    "may",
    "random",
    "known",
    "parameters",
    "words",
    "poor",
    "representation",
    "representation",
    "data",
    "better",
    "representation",
    "data",
    "case",
    "would",
    "say",
    "models",
    "representation",
    "green",
    "dots",
    "red",
    "dots",
    "good",
    "representation",
    "poor",
    "representation",
    "mean",
    "know",
    "would",
    "say",
    "fairly",
    "poor",
    "representation",
    "one",
    "way",
    "measure",
    "representation",
    "models",
    "outputs",
    "case",
    "red",
    "dots",
    "predictions",
    "testing",
    "data",
    "use",
    "loss",
    "function",
    "going",
    "write",
    "moving",
    "towards",
    "moving",
    "towards",
    "training",
    "need",
    "way",
    "measure",
    "poorly",
    "models",
    "predictions",
    "one",
    "way",
    "measure",
    "poor",
    "wrong",
    "models",
    "predictions",
    "use",
    "loss",
    "function",
    "go",
    "pytorch",
    "loss",
    "functions",
    "going",
    "see",
    "pytorch",
    "fair",
    "loss",
    "functions",
    "built",
    "essence",
    "quite",
    "similar",
    "wait",
    "load",
    "internet",
    "going",
    "little",
    "bit",
    "slow",
    "today",
    "okay",
    "rush",
    "learning",
    "something",
    "fun",
    "search",
    "loss",
    "loss",
    "functions",
    "go",
    "yeah",
    "torch",
    "basic",
    "building",
    "blocks",
    "graphs",
    "whole",
    "bunch",
    "good",
    "stuff",
    "including",
    "loss",
    "functions",
    "beautiful",
    "another",
    "thing",
    "note",
    "well",
    "another",
    "one",
    "scenarios",
    "words",
    "thing",
    "might",
    "also",
    "see",
    "loss",
    "function",
    "referred",
    "criterion",
    "another",
    "word",
    "called",
    "cost",
    "function",
    "might",
    "write",
    "aware",
    "yeah",
    "cost",
    "function",
    "versus",
    "loss",
    "function",
    "maybe",
    "formal",
    "definitions",
    "maybe",
    "used",
    "different",
    "fields",
    "case",
    "focused",
    "machine",
    "learning",
    "right",
    "going",
    "go",
    "note",
    "loss",
    "function",
    "may",
    "also",
    "called",
    "cost",
    "function",
    "criterion",
    "different",
    "areas",
    "case",
    "going",
    "refer",
    "loss",
    "function",
    "let",
    "formally",
    "define",
    "loss",
    "function",
    "going",
    "go",
    "fair",
    "steps",
    "upcoming",
    "videos",
    "warning",
    "nothing",
    "ca",
    "handle",
    "want",
    "put",
    "formal",
    "definitions",
    "things",
    "going",
    "see",
    "practice",
    "prefer",
    "rather",
    "sit",
    "defining",
    "stuff",
    "lecture",
    "already",
    "enough",
    "text",
    "page",
    "hurry",
    "get",
    "coding",
    "daniel",
    "loss",
    "function",
    "function",
    "measure",
    "wrong",
    "models",
    "predictions",
    "ideal",
    "outputs",
    "lower",
    "better",
    "ideally",
    "think",
    "measurement",
    "could",
    "measure",
    "difference",
    "red",
    "dots",
    "green",
    "dots",
    "one",
    "simplest",
    "ways",
    "would",
    "measure",
    "distance",
    "right",
    "go",
    "let",
    "estimate",
    "035",
    "abouts",
    "difference",
    "could",
    "dots",
    "maybe",
    "take",
    "average",
    "worked",
    "loss",
    "functions",
    "might",
    "realized",
    "reproduced",
    "mean",
    "absolute",
    "error",
    "going",
    "get",
    "minute",
    "need",
    "loss",
    "function",
    "going",
    "write",
    "another",
    "little",
    "dot",
    "point",
    "setting",
    "intuition",
    "things",
    "need",
    "train",
    "need",
    "loss",
    "function",
    "pytorch",
    "machine",
    "learning",
    "general",
    "actually",
    "focused",
    "pytorch",
    "need",
    "optimizer",
    "optimizer",
    "takes",
    "account",
    "loss",
    "model",
    "adjusts",
    "model",
    "parameters",
    "parameters",
    "recall",
    "weight",
    "bias",
    "values",
    "weight",
    "biases",
    "check",
    "bias",
    "check",
    "going",
    "model",
    "dot",
    "parameter",
    "parameters",
    "also",
    "like",
    "oh",
    "going",
    "give",
    "us",
    "generator",
    "define",
    "model",
    "yet",
    "call",
    "model",
    "oh",
    "model",
    "zero",
    "excuse",
    "forgot",
    "going",
    "build",
    "lot",
    "models",
    "course",
    "giving",
    "numbers",
    "modeled",
    "parameters",
    "yeah",
    "got",
    "generator",
    "turn",
    "list",
    "model",
    "zero",
    "want",
    "get",
    "labeled",
    "want",
    "state",
    "dict",
    "go",
    "weight",
    "value",
    "random",
    "value",
    "set",
    "bias",
    "got",
    "two",
    "parameters",
    "model",
    "quite",
    "simple",
    "however",
    "principles",
    "learning",
    "going",
    "principles",
    "taking",
    "loss",
    "function",
    "trying",
    "minimize",
    "getting",
    "lower",
    "ideal",
    "model",
    "predict",
    "exactly",
    "test",
    "data",
    "optimizer",
    "take",
    "account",
    "loss",
    "adjust",
    "model",
    "parameter",
    "case",
    "weights",
    "bias",
    "let",
    "finish",
    "definition",
    "takes",
    "account",
    "loss",
    "model",
    "adjust",
    "model",
    "parameters",
    "weight",
    "bias",
    "case",
    "improve",
    "loss",
    "function",
    "specifically",
    "pytorch",
    "need",
    "training",
    "loop",
    "testing",
    "loop",
    "going",
    "work",
    "towards",
    "building",
    "throughout",
    "next",
    "couple",
    "videos",
    "going",
    "focus",
    "two",
    "first",
    "loss",
    "function",
    "optimizer",
    "formal",
    "definition",
    "going",
    "find",
    "many",
    "different",
    "definitions",
    "going",
    "find",
    "loss",
    "function",
    "measures",
    "wrong",
    "model",
    "predictions",
    "lower",
    "better",
    "optimizer",
    "takes",
    "account",
    "loss",
    "model",
    "wrong",
    "starts",
    "move",
    "two",
    "values",
    "way",
    "improves",
    "red",
    "dots",
    "end",
    "principles",
    "loss",
    "function",
    "optimizer",
    "models",
    "two",
    "parameters",
    "models",
    "millions",
    "parameters",
    "computer",
    "vision",
    "models",
    "could",
    "simple",
    "models",
    "like",
    "predict",
    "dots",
    "straight",
    "line",
    "said",
    "let",
    "jump",
    "next",
    "video",
    "start",
    "look",
    "little",
    "deeper",
    "loss",
    "function",
    "row",
    "problem",
    "optimizer",
    "see",
    "welcome",
    "back",
    "exciting",
    "streak",
    "videos",
    "coming",
    "mean",
    "whole",
    "course",
    "fun",
    "trust",
    "really",
    "exciting",
    "training",
    "first",
    "machine",
    "learning",
    "model",
    "seems",
    "little",
    "bit",
    "like",
    "magic",
    "even",
    "fun",
    "writing",
    "code",
    "going",
    "behind",
    "scenes",
    "discussed",
    "whole",
    "concept",
    "training",
    "going",
    "unknown",
    "parameters",
    "random",
    "parameters",
    "got",
    "far",
    "parameters",
    "better",
    "represent",
    "data",
    "spoke",
    "concept",
    "loss",
    "function",
    "want",
    "minimize",
    "loss",
    "function",
    "whole",
    "idea",
    "training",
    "loop",
    "pytorch",
    "optimization",
    "loop",
    "pytorch",
    "optimizer",
    "one",
    "ways",
    "nudge",
    "parameters",
    "model",
    "case",
    "weights",
    "bias",
    "towards",
    "values",
    "rather",
    "random",
    "values",
    "like",
    "towards",
    "values",
    "lower",
    "loss",
    "function",
    "lower",
    "loss",
    "function",
    "loss",
    "function",
    "measures",
    "wrong",
    "models",
    "predictions",
    "compared",
    "ideal",
    "outputs",
    "lower",
    "well",
    "hopefully",
    "move",
    "red",
    "dots",
    "towards",
    "green",
    "dots",
    "might",
    "guessed",
    "pytorch",
    "built",
    "functionality",
    "implementing",
    "loss",
    "functions",
    "optimizers",
    "way",
    "covering",
    "far",
    "train",
    "model",
    "section",
    "pytorch",
    "workflow",
    "fundamentals",
    "got",
    "little",
    "nice",
    "table",
    "describes",
    "loss",
    "function",
    "live",
    "pytorch",
    "common",
    "values",
    "going",
    "see",
    "hands",
    "like",
    "read",
    "course",
    "book",
    "version",
    "course",
    "loss",
    "functions",
    "pytorch",
    "look",
    "look",
    "loss",
    "functions",
    "far",
    "many",
    "us",
    "go",
    "one",
    "hit",
    "going",
    "focus",
    "common",
    "ones",
    "look",
    "got",
    "15",
    "loss",
    "functions",
    "something",
    "like",
    "well",
    "truth",
    "told",
    "one",
    "use",
    "really",
    "going",
    "know",
    "unless",
    "start",
    "work",
    "hands",
    "different",
    "problems",
    "case",
    "going",
    "looking",
    "l1",
    "loss",
    "another",
    "instance",
    "different",
    "machine",
    "learning",
    "libraries",
    "different",
    "names",
    "thing",
    "mean",
    "absolute",
    "error",
    "kind",
    "discussed",
    "last",
    "video",
    "took",
    "distance",
    "red",
    "dot",
    "green",
    "dot",
    "say",
    "took",
    "mean",
    "well",
    "got",
    "mean",
    "absolute",
    "error",
    "pytorch",
    "call",
    "l1",
    "loss",
    "little",
    "bit",
    "confusing",
    "go",
    "mse",
    "loss",
    "mean",
    "squared",
    "error",
    "l2",
    "naming",
    "conventions",
    "takes",
    "little",
    "bit",
    "getting",
    "used",
    "warning",
    "let",
    "look",
    "l1",
    "loss",
    "function",
    "making",
    "aware",
    "loss",
    "functions",
    "binary",
    "cross",
    "entropy",
    "loss",
    "later",
    "course",
    "maybe",
    "even",
    "categorical",
    "cross",
    "entropy",
    "see",
    "later",
    "others",
    "problem",
    "specific",
    "couple",
    "loss",
    "functions",
    "like",
    "l1",
    "loss",
    "mse",
    "loss",
    "use",
    "regression",
    "problems",
    "predicting",
    "number",
    "cross",
    "entropy",
    "loss",
    "loss",
    "use",
    "classification",
    "problems",
    "see",
    "hands",
    "later",
    "let",
    "look",
    "l1",
    "loss",
    "l1",
    "loss",
    "creates",
    "criterion",
    "said",
    "might",
    "hear",
    "word",
    "criterion",
    "used",
    "pytorch",
    "loss",
    "function",
    "typically",
    "call",
    "loss",
    "functions",
    "literature",
    "typically",
    "calls",
    "loss",
    "functions",
    "measures",
    "mean",
    "absolute",
    "error",
    "go",
    "l1",
    "loss",
    "mean",
    "absolute",
    "error",
    "element",
    "input",
    "x",
    "target",
    "extracurricular",
    "measure",
    "might",
    "guessed",
    "read",
    "documentation",
    "different",
    "loss",
    "functions",
    "especially",
    "l1",
    "loss",
    "sake",
    "video",
    "let",
    "implement",
    "oh",
    "want",
    "little",
    "bit",
    "graphic",
    "got",
    "one",
    "way",
    "picking",
    "loss",
    "function",
    "optimizer",
    "step",
    "two",
    "fun",
    "part",
    "right",
    "getting",
    "training",
    "model",
    "got",
    "mean",
    "absolute",
    "error",
    "graph",
    "seen",
    "oh",
    "look",
    "okay",
    "got",
    "difference",
    "actually",
    "measured",
    "past",
    "kind",
    "knew",
    "mean",
    "absolute",
    "error",
    "repeat",
    "samples",
    "set",
    "working",
    "take",
    "absolute",
    "difference",
    "two",
    "dots",
    "well",
    "take",
    "mean",
    "got",
    "mean",
    "absolute",
    "error",
    "mae",
    "loss",
    "equals",
    "torch",
    "mean",
    "could",
    "write",
    "beauty",
    "pine",
    "torch",
    "right",
    "could",
    "write",
    "could",
    "use",
    "torch",
    "n",
    "version",
    "recommended",
    "let",
    "jump",
    "colorful",
    "slide",
    "describing",
    "let",
    "go",
    "set",
    "loss",
    "function",
    "also",
    "going",
    "put",
    "set",
    "optimizer",
    "let",
    "call",
    "loss",
    "fn",
    "equals",
    "nn",
    "dot",
    "l1",
    "loss",
    "simple",
    "look",
    "loss",
    "function",
    "say",
    "oh",
    "goodness",
    "internet",
    "going",
    "quite",
    "slow",
    "today",
    "raining",
    "outside",
    "might",
    "delays",
    "somewhere",
    "right",
    "gives",
    "us",
    "chance",
    "sit",
    "mindful",
    "look",
    "okay",
    "loss",
    "function",
    "l1",
    "loss",
    "beautiful",
    "got",
    "loss",
    "function",
    "objective",
    "training",
    "machine",
    "learning",
    "model",
    "two",
    "let",
    "go",
    "back",
    "look",
    "colorful",
    "graphic",
    "minimize",
    "distances",
    "turn",
    "minimize",
    "overall",
    "value",
    "mae",
    "goal",
    "red",
    "dots",
    "line",
    "green",
    "dots",
    "loss",
    "value",
    "zero",
    "ideal",
    "point",
    "model",
    "let",
    "go",
    "need",
    "optimizer",
    "discussed",
    "optimizer",
    "takes",
    "account",
    "loss",
    "model",
    "two",
    "work",
    "tandem",
    "put",
    "similar",
    "steps",
    "go",
    "back",
    "slides",
    "put",
    "often",
    "picking",
    "loss",
    "function",
    "optimizer",
    "pytorch",
    "come",
    "part",
    "package",
    "work",
    "together",
    "optimizer",
    "objective",
    "give",
    "model",
    "values",
    "parameters",
    "like",
    "weight",
    "bias",
    "minimize",
    "loss",
    "function",
    "work",
    "tandem",
    "let",
    "see",
    "optimizer",
    "optimizes",
    "might",
    "search",
    "typically",
    "use",
    "search",
    "prefer",
    "using",
    "google",
    "search",
    "give",
    "us",
    "optimizer",
    "hey",
    "go",
    "pytorch",
    "optimizers",
    "let",
    "put",
    "link",
    "another",
    "bit",
    "extracurricular",
    "want",
    "read",
    "different",
    "optimizers",
    "pytorch",
    "might",
    "guessed",
    "package",
    "implementing",
    "various",
    "optimization",
    "algorithms",
    "commonly",
    "used",
    "methods",
    "already",
    "supported",
    "interface",
    "general",
    "enough",
    "sophisticated",
    "ones",
    "also",
    "easily",
    "integrated",
    "future",
    "look",
    "algorithms",
    "exist",
    "going",
    "throw",
    "lot",
    "names",
    "literature",
    "lot",
    "made",
    "already",
    "good",
    "working",
    "algorithms",
    "matter",
    "picking",
    "whichever",
    "one",
    "best",
    "problem",
    "find",
    "well",
    "sgd",
    "stochastic",
    "gradient",
    "descent",
    "possibly",
    "popular",
    "however",
    "iterations",
    "sgd",
    "adam",
    "another",
    "one",
    "really",
    "popular",
    "one",
    "machine",
    "learning",
    "part",
    "art",
    "part",
    "science",
    "trial",
    "error",
    "figuring",
    "works",
    "best",
    "problem",
    "us",
    "going",
    "start",
    "sgd",
    "popular",
    "paying",
    "attention",
    "previous",
    "video",
    "might",
    "seen",
    "said",
    "look",
    "gradient",
    "descent",
    "wherever",
    "got",
    "gradient",
    "descent",
    "go",
    "one",
    "main",
    "algorithms",
    "improves",
    "models",
    "gradient",
    "descent",
    "back",
    "propagation",
    "look",
    "stochastic",
    "gradient",
    "descent",
    "bit",
    "tongue",
    "twister",
    "random",
    "gradient",
    "descent",
    "stochastic",
    "means",
    "basically",
    "model",
    "improves",
    "taking",
    "random",
    "numbers",
    "let",
    "go",
    "randomly",
    "adjusting",
    "minimize",
    "loss",
    "optimizer",
    "right",
    "optimizer",
    "torch",
    "dot",
    "opt",
    "let",
    "implement",
    "sgd",
    "sgd",
    "stochastic",
    "gradient",
    "descent",
    "going",
    "write",
    "stochastic",
    "gradient",
    "descent",
    "starts",
    "randomly",
    "adjusting",
    "values",
    "found",
    "random",
    "values",
    "random",
    "steps",
    "minimized",
    "loss",
    "value",
    "going",
    "see",
    "action",
    "later",
    "going",
    "continue",
    "adjusting",
    "direction",
    "say",
    "says",
    "oh",
    "weights",
    "increase",
    "weights",
    "reduces",
    "loss",
    "going",
    "keep",
    "increasing",
    "weights",
    "weights",
    "longer",
    "reduce",
    "loss",
    "maybe",
    "gets",
    "point",
    "say",
    "increase",
    "weights",
    "anymore",
    "loss",
    "going",
    "go",
    "optimizer",
    "like",
    "well",
    "going",
    "stop",
    "bias",
    "thing",
    "happens",
    "decreases",
    "bias",
    "finds",
    "loss",
    "increases",
    "well",
    "going",
    "go",
    "well",
    "going",
    "try",
    "increasing",
    "bias",
    "instead",
    "one",
    "last",
    "summary",
    "going",
    "loss",
    "function",
    "measures",
    "wrong",
    "model",
    "optimizer",
    "adjust",
    "model",
    "parameters",
    "matter",
    "whether",
    "two",
    "parameters",
    "millions",
    "reduce",
    "loss",
    "couple",
    "things",
    "optimizer",
    "needs",
    "take",
    "needs",
    "take",
    "argument",
    "params",
    "go",
    "sgd",
    "going",
    "link",
    "well",
    "sgd",
    "formula",
    "sgd",
    "look",
    "go",
    "hmm",
    "lot",
    "going",
    "take",
    "understand",
    "like",
    "see",
    "code",
    "need",
    "params",
    "short",
    "parameters",
    "optimize",
    "optimizer",
    "also",
    "need",
    "lr",
    "stands",
    "going",
    "write",
    "comment",
    "lr",
    "equals",
    "learning",
    "rate",
    "possibly",
    "oh",
    "even",
    "type",
    "rate",
    "possibly",
    "important",
    "hyper",
    "parameter",
    "set",
    "let",
    "remind",
    "throwing",
    "lots",
    "words",
    "kind",
    "like",
    "trying",
    "write",
    "notes",
    "going",
    "see",
    "action",
    "second",
    "check",
    "models",
    "parameters",
    "parameter",
    "value",
    "model",
    "sets",
    "learning",
    "rate",
    "equals",
    "possibly",
    "important",
    "learning",
    "hyper",
    "parameter",
    "need",
    "learning",
    "hyper",
    "parameter",
    "hyper",
    "parameter",
    "value",
    "us",
    "data",
    "scientist",
    "machine",
    "learning",
    "engineer",
    "set",
    "set",
    "learning",
    "rate",
    "case",
    "let",
    "go",
    "like",
    "daniel",
    "get",
    "value",
    "well",
    "type",
    "values",
    "come",
    "experience",
    "think",
    "actually",
    "says",
    "lr",
    "lr",
    "yeah",
    "okay",
    "default",
    "go",
    "back",
    "optim",
    "think",
    "saw",
    "somewhere",
    "see",
    "somewhere",
    "yeah",
    "go",
    "yeah",
    "lot",
    "default",
    "settings",
    "pretty",
    "good",
    "torch",
    "optimizers",
    "however",
    "learning",
    "rate",
    "actually",
    "could",
    "go",
    "common",
    "values",
    "triple",
    "zero",
    "one",
    "sure",
    "exactly",
    "oh",
    "model",
    "model",
    "zero",
    "learning",
    "rate",
    "says",
    "optimizer",
    "yes",
    "going",
    "optimize",
    "parameters",
    "higher",
    "learning",
    "rate",
    "adjusts",
    "parameters",
    "one",
    "hit",
    "let",
    "say",
    "going",
    "optimize",
    "value",
    "going",
    "take",
    "big",
    "step",
    "changed",
    "going",
    "take",
    "big",
    "step",
    "three",
    "changed",
    "way",
    "end",
    "going",
    "change",
    "value",
    "smaller",
    "learning",
    "rate",
    "smaller",
    "change",
    "parameter",
    "larger",
    "learning",
    "rate",
    "larger",
    "change",
    "parameter",
    "set",
    "loss",
    "function",
    "set",
    "optimizer",
    "let",
    "move",
    "next",
    "step",
    "training",
    "workflow",
    "building",
    "training",
    "loop",
    "far",
    "exciting",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "set",
    "loss",
    "function",
    "set",
    "optimizer",
    "discussed",
    "roles",
    "loss",
    "function",
    "measures",
    "wrong",
    "model",
    "optimizer",
    "talks",
    "loss",
    "function",
    "goes",
    "well",
    "change",
    "parameters",
    "certain",
    "way",
    "reduce",
    "loss",
    "function",
    "yes",
    "let",
    "keep",
    "adjusting",
    "direction",
    "let",
    "adjust",
    "opposite",
    "direction",
    "want",
    "show",
    "added",
    "little",
    "bit",
    "text",
    "concretely",
    "put",
    "discussing",
    "inside",
    "optimizer",
    "often",
    "set",
    "two",
    "parameters",
    "params",
    "lr",
    "params",
    "model",
    "parameters",
    "like",
    "optimize",
    "example",
    "case",
    "params",
    "equals",
    "model",
    "zero",
    "parameters",
    "course",
    "weight",
    "bias",
    "learning",
    "rate",
    "lr",
    "optimizer",
    "lr",
    "stands",
    "learning",
    "rate",
    "learning",
    "rate",
    "hyper",
    "parameter",
    "remember",
    "hyper",
    "parameter",
    "value",
    "data",
    "scientist",
    "machine",
    "learning",
    "engineer",
    "sets",
    "whereas",
    "parameter",
    "model",
    "sets",
    "defines",
    "big",
    "smaller",
    "optimizer",
    "changes",
    "model",
    "parameters",
    "small",
    "learning",
    "rate",
    "smaller",
    "value",
    "results",
    "small",
    "changes",
    "large",
    "learning",
    "rate",
    "results",
    "large",
    "changes",
    "another",
    "question",
    "might",
    "well",
    "valid",
    "question",
    "hey",
    "put",
    "already",
    "loss",
    "function",
    "optimizer",
    "use",
    "another",
    "tough",
    "one",
    "problem",
    "specific",
    "experience",
    "machine",
    "learning",
    "showing",
    "one",
    "example",
    "get",
    "idea",
    "works",
    "particular",
    "problem",
    "regression",
    "problem",
    "like",
    "loss",
    "function",
    "l1",
    "loss",
    "mai",
    "pytorch",
    "optimizer",
    "like",
    "torch",
    "dot",
    "opt",
    "slash",
    "gd",
    "like",
    "sarcastic",
    "gradient",
    "descent",
    "suffice",
    "classification",
    "problem",
    "going",
    "see",
    "later",
    "one",
    "specifically",
    "whether",
    "photo",
    "cat",
    "dog",
    "example",
    "binary",
    "classification",
    "problem",
    "might",
    "want",
    "use",
    "binary",
    "classification",
    "loss",
    "said",
    "moving",
    "well",
    "whole",
    "goal",
    "reduce",
    "mae",
    "model",
    "let",
    "get",
    "workflow",
    "done",
    "two",
    "steps",
    "want",
    "build",
    "training",
    "loop",
    "let",
    "get",
    "back",
    "going",
    "fair",
    "steps",
    "going",
    "already",
    "covered",
    "hey",
    "nothing",
    "ca",
    "handle",
    "together",
    "building",
    "training",
    "loop",
    "pytorch",
    "thought",
    "talking",
    "going",
    "training",
    "loop",
    "talk",
    "steps",
    "coded",
    "want",
    "build",
    "training",
    "loop",
    "testing",
    "loop",
    "couple",
    "things",
    "need",
    "training",
    "loop",
    "going",
    "fair",
    "steps",
    "never",
    "written",
    "training",
    "loop",
    "completely",
    "fine",
    "find",
    "first",
    "couple",
    "times",
    "write",
    "like",
    "oh",
    "gosh",
    "much",
    "going",
    "practice",
    "go",
    "okay",
    "see",
    "going",
    "eventually",
    "write",
    "eyes",
    "closed",
    "got",
    "fun",
    "song",
    "help",
    "remembering",
    "things",
    "called",
    "unofficial",
    "pytorch",
    "optimization",
    "loop",
    "song",
    "see",
    "later",
    "actually",
    "probably",
    "leave",
    "extension",
    "see",
    "also",
    "functionize",
    "things",
    "later",
    "course",
    "write",
    "forget",
    "going",
    "write",
    "scratch",
    "begin",
    "know",
    "happening",
    "want",
    "actually",
    "step",
    "zero",
    "loop",
    "data",
    "want",
    "look",
    "data",
    "multiple",
    "times",
    "model",
    "going",
    "first",
    "start",
    "random",
    "predictions",
    "data",
    "make",
    "predictions",
    "trying",
    "improve",
    "trying",
    "minimize",
    "loss",
    "make",
    "predictions",
    "forward",
    "pass",
    "forward",
    "pass",
    "called",
    "forward",
    "pass",
    "involves",
    "data",
    "moving",
    "model",
    "forward",
    "functions",
    "say",
    "functions",
    "might",
    "plural",
    "might",
    "one",
    "forward",
    "method",
    "recall",
    "wrote",
    "model",
    "ford",
    "forward",
    "pass",
    "data",
    "going",
    "function",
    "want",
    "look",
    "visually",
    "let",
    "look",
    "neural",
    "network",
    "graphic",
    "images",
    "forward",
    "pass",
    "data",
    "moving",
    "inputs",
    "output",
    "layer",
    "starting",
    "input",
    "layer",
    "moving",
    "model",
    "forward",
    "pass",
    "also",
    "called",
    "forward",
    "propagation",
    "another",
    "time",
    "one",
    "name",
    "used",
    "thing",
    "go",
    "back",
    "forward",
    "pass",
    "write",
    "also",
    "called",
    "forward",
    "propagation",
    "propagation",
    "wonderful",
    "need",
    "calculate",
    "loss",
    "forward",
    "pass",
    "let",
    "write",
    "calculate",
    "make",
    "predictions",
    "make",
    "predictions",
    "data",
    "calculate",
    "loss",
    "compare",
    "forward",
    "pass",
    "predictions",
    "oh",
    "undergoing",
    "background",
    "place",
    "might",
    "storm",
    "perfect",
    "time",
    "write",
    "code",
    "compare",
    "forward",
    "pass",
    "predictions",
    "ground",
    "truth",
    "labels",
    "going",
    "see",
    "code",
    "second",
    "calculate",
    "loss",
    "going",
    "go",
    "optimise",
    "zero",
    "grad",
    "spoken",
    "okay",
    "going",
    "see",
    "second",
    "going",
    "put",
    "much",
    "loss",
    "backward",
    "discussed",
    "one",
    "either",
    "probably",
    "three",
    "steps",
    "really",
    "discussed",
    "discussed",
    "idea",
    "behind",
    "much",
    "depth",
    "optimise",
    "step",
    "one",
    "loss",
    "backwards",
    "move",
    "backwards",
    "forward",
    "pass",
    "forwards",
    "like",
    "network",
    "forward",
    "pass",
    "data",
    "goes",
    "backward",
    "pass",
    "data",
    "goes",
    "calculations",
    "happen",
    "backwards",
    "see",
    "second",
    "got",
    "much",
    "going",
    "getting",
    "rid",
    "moves",
    "backwards",
    "network",
    "calculate",
    "gradients",
    "oh",
    "oh",
    "gradients",
    "parameters",
    "model",
    "respect",
    "loss",
    "oh",
    "gosh",
    "absolute",
    "mouthful",
    "optimise",
    "step",
    "going",
    "use",
    "optimiser",
    "adjust",
    "model",
    "parameters",
    "try",
    "improve",
    "loss",
    "remember",
    "said",
    "previous",
    "video",
    "love",
    "watch",
    "two",
    "videos",
    "linked",
    "one",
    "gradient",
    "descent",
    "one",
    "back",
    "propagation",
    "might",
    "seen",
    "like",
    "fair",
    "bit",
    "math",
    "going",
    "well",
    "essentially",
    "model",
    "goes",
    "random",
    "parameters",
    "better",
    "parameters",
    "using",
    "math",
    "many",
    "people",
    "one",
    "main",
    "things",
    "get",
    "asked",
    "machine",
    "learning",
    "learn",
    "machine",
    "learning",
    "math",
    "well",
    "beautiful",
    "thing",
    "pytorch",
    "implements",
    "lot",
    "math",
    "back",
    "propagation",
    "back",
    "propagation",
    "going",
    "write",
    "algorithm",
    "called",
    "back",
    "back",
    "propagation",
    "hence",
    "loss",
    "backward",
    "going",
    "see",
    "code",
    "second",
    "worry",
    "gradient",
    "descent",
    "two",
    "algorithms",
    "drive",
    "majority",
    "learning",
    "back",
    "propagation",
    "calculate",
    "gradients",
    "parameters",
    "model",
    "respect",
    "loss",
    "function",
    "optimise",
    "step",
    "trigger",
    "code",
    "run",
    "gradient",
    "descent",
    "minimise",
    "gradients",
    "gradient",
    "let",
    "look",
    "gradient",
    "know",
    "written",
    "code",
    "yet",
    "going",
    "images",
    "gradient",
    "go",
    "changing",
    "changing",
    "gradient",
    "high",
    "school",
    "math",
    "gradient",
    "slope",
    "hill",
    "let",
    "find",
    "picture",
    "hill",
    "picture",
    "hill",
    "go",
    "great",
    "big",
    "hill",
    "top",
    "hill",
    "wanted",
    "get",
    "bottom",
    "would",
    "get",
    "bottom",
    "well",
    "course",
    "walked",
    "hill",
    "machine",
    "learning",
    "model",
    "trying",
    "let",
    "imagine",
    "loss",
    "height",
    "hill",
    "start",
    "losses",
    "really",
    "high",
    "want",
    "take",
    "loss",
    "zero",
    "bottom",
    "right",
    "well",
    "measure",
    "gradient",
    "hill",
    "bottom",
    "hill",
    "opposite",
    "direction",
    "gradient",
    "steep",
    "make",
    "sense",
    "gradient",
    "incline",
    "want",
    "model",
    "move",
    "towards",
    "gradient",
    "nothing",
    "could",
    "argue",
    "yeah",
    "gradient",
    "probably",
    "nothing",
    "top",
    "let",
    "argument",
    "sake",
    "say",
    "want",
    "get",
    "bottom",
    "hill",
    "measuring",
    "gradient",
    "one",
    "ways",
    "optimisation",
    "algorithm",
    "works",
    "moves",
    "model",
    "parameters",
    "gradient",
    "equals",
    "zero",
    "gradient",
    "loss",
    "equals",
    "zero",
    "loss",
    "equals",
    "zero",
    "two",
    "let",
    "write",
    "code",
    "going",
    "set",
    "parameter",
    "called",
    "variable",
    "called",
    "epochs",
    "going",
    "start",
    "one",
    "even",
    "though",
    "could",
    "value",
    "let",
    "define",
    "go",
    "going",
    "write",
    "code",
    "epochs",
    "epoch",
    "one",
    "loop",
    "data",
    "dot",
    "dot",
    "dot",
    "epochs",
    "going",
    "start",
    "one",
    "one",
    "time",
    "data",
    "much",
    "data",
    "epoch",
    "let",
    "go",
    "step",
    "zero",
    "zero",
    "loop",
    "data",
    "way",
    "say",
    "loop",
    "data",
    "want",
    "steps",
    "within",
    "loop",
    "dot",
    "dot",
    "dot",
    "loop",
    "data",
    "epoch",
    "range",
    "epochs",
    "even",
    "though",
    "going",
    "one",
    "adjust",
    "later",
    "epochs",
    "set",
    "hyper",
    "parameter",
    "set",
    "know",
    "could",
    "argue",
    "hey",
    "machine",
    "learning",
    "parameters",
    "model",
    "zero",
    "model",
    "parameters",
    "model",
    "zero",
    "actually",
    "parameters",
    "set",
    "models",
    "build",
    "future",
    "likely",
    "set",
    "automatically",
    "rather",
    "setting",
    "explicitly",
    "like",
    "done",
    "created",
    "model",
    "zero",
    "oh",
    "gosh",
    "taking",
    "quite",
    "run",
    "right",
    "need",
    "run",
    "fast",
    "need",
    "write",
    "code",
    "come",
    "step",
    "discussed",
    "either",
    "set",
    "model",
    "training",
    "mode",
    "pytorch",
    "models",
    "couple",
    "different",
    "modes",
    "default",
    "training",
    "mode",
    "set",
    "training",
    "mode",
    "going",
    "like",
    "train",
    "train",
    "mode",
    "pytorch",
    "model",
    "goodness",
    "reason",
    "engineer",
    "going",
    "slide",
    "right",
    "going",
    "discuss",
    "talking",
    "list",
    "train",
    "mode",
    "train",
    "mode",
    "pytorch",
    "sets",
    "oh",
    "go",
    "requires",
    "grad",
    "equals",
    "true",
    "wonder",
    "torch",
    "dot",
    "grad",
    "member",
    "grad",
    "similar",
    "inference",
    "mode",
    "adjust",
    "see",
    "wanted",
    "take",
    "note",
    "requires",
    "grad",
    "equals",
    "true",
    "actually",
    "might",
    "different",
    "cell",
    "watch",
    "going",
    "rather",
    "spit",
    "words",
    "reckon",
    "might",
    "able",
    "get",
    "work",
    "oh",
    "list",
    "model",
    "parameters",
    "come",
    "model",
    "zero",
    "dot",
    "eval",
    "two",
    "modes",
    "mode",
    "train",
    "mode",
    "model",
    "dot",
    "eval",
    "parameters",
    "hey",
    "experimenting",
    "together",
    "fly",
    "actually",
    "want",
    "want",
    "experiment",
    "different",
    "things",
    "going",
    "say",
    "requires",
    "grad",
    "equals",
    "false",
    "hmm",
    "torch",
    "dot",
    "grad",
    "model",
    "zero",
    "dot",
    "parameters",
    "know",
    "work",
    "definitely",
    "works",
    "behind",
    "scenes",
    "mean",
    "works",
    "behind",
    "scenes",
    "works",
    "behind",
    "scenes",
    "calculations",
    "made",
    "trying",
    "explicitly",
    "print",
    "things",
    "well",
    "experiment",
    "thought",
    "going",
    "work",
    "work",
    "train",
    "mode",
    "pytorch",
    "sets",
    "parameters",
    "require",
    "gradients",
    "require",
    "gradients",
    "remember",
    "picture",
    "hill",
    "spoke",
    "trying",
    "minimize",
    "gradient",
    "gradient",
    "steepness",
    "hill",
    "height",
    "hill",
    "loss",
    "function",
    "want",
    "take",
    "zero",
    "want",
    "take",
    "gradient",
    "zero",
    "thing",
    "gradients",
    "model",
    "parameters",
    "respect",
    "loss",
    "function",
    "want",
    "try",
    "minimize",
    "gradient",
    "gradient",
    "descent",
    "take",
    "gradient",
    "zero",
    "model",
    "dot",
    "train",
    "also",
    "model",
    "zero",
    "dot",
    "vowel",
    "turns",
    "gradient",
    "tracking",
    "going",
    "see",
    "later",
    "feel",
    "like",
    "video",
    "getting",
    "far",
    "long",
    "let",
    "finish",
    "training",
    "loop",
    "next",
    "video",
    "see",
    "friends",
    "welcome",
    "back",
    "last",
    "video",
    "promised",
    "lot",
    "code",
    "get",
    "discussed",
    "important",
    "steps",
    "forgot",
    "much",
    "behind",
    "scenes",
    "apply",
    "towards",
    "training",
    "loop",
    "think",
    "important",
    "spend",
    "time",
    "discussing",
    "going",
    "fair",
    "steps",
    "know",
    "going",
    "mean",
    "later",
    "write",
    "code",
    "going",
    "write",
    "video",
    "functionize",
    "going",
    "see",
    "later",
    "course",
    "going",
    "run",
    "behind",
    "scenes",
    "us",
    "spending",
    "fair",
    "bit",
    "time",
    "literally",
    "crux",
    "model",
    "learns",
    "let",
    "get",
    "going",
    "implement",
    "forward",
    "pass",
    "involves",
    "model",
    "forward",
    "function",
    "defined",
    "built",
    "model",
    "forward",
    "pass",
    "runs",
    "code",
    "let",
    "write",
    "case",
    "training",
    "going",
    "write",
    "training",
    "going",
    "see",
    "dot",
    "later",
    "talk",
    "comes",
    "let",
    "forward",
    "pass",
    "forward",
    "pass",
    "want",
    "pass",
    "data",
    "model",
    "forward",
    "method",
    "quite",
    "simply",
    "going",
    "pred",
    "predictions",
    "remember",
    "trying",
    "use",
    "ideal",
    "model",
    "using",
    "x",
    "test",
    "predict",
    "test",
    "test",
    "data",
    "set",
    "make",
    "predictions",
    "test",
    "data",
    "set",
    "learn",
    "training",
    "data",
    "set",
    "passing",
    "going",
    "get",
    "rid",
    "need",
    "passing",
    "model",
    "x",
    "train",
    "model",
    "zero",
    "going",
    "current",
    "model",
    "go",
    "learn",
    "patterns",
    "training",
    "data",
    "evaluate",
    "model",
    "test",
    "data",
    "number",
    "two",
    "calculate",
    "loss",
    "previous",
    "video",
    "set",
    "loss",
    "function",
    "going",
    "help",
    "us",
    "calculate",
    "kind",
    "loss",
    "using",
    "want",
    "calculate",
    "mae",
    "difference",
    "distance",
    "red",
    "dot",
    "green",
    "dot",
    "formula",
    "would",
    "red",
    "dots",
    "green",
    "dots",
    "calculating",
    "far",
    "apart",
    "taking",
    "mean",
    "value",
    "let",
    "go",
    "back",
    "calculate",
    "loss",
    "case",
    "going",
    "set",
    "loss",
    "equal",
    "loss",
    "function",
    "l",
    "one",
    "loss",
    "pytorch",
    "mae",
    "calculating",
    "difference",
    "models",
    "predictions",
    "training",
    "data",
    "set",
    "ideal",
    "training",
    "values",
    "want",
    "go",
    "torch",
    "dot",
    "nn",
    "loss",
    "functions",
    "going",
    "show",
    "order",
    "sometimes",
    "confuses",
    "order",
    "values",
    "go",
    "goes",
    "prediction",
    "first",
    "labels",
    "may",
    "wrong",
    "get",
    "confused",
    "dyslexia",
    "kicks",
    "pretty",
    "sure",
    "predictions",
    "first",
    "actual",
    "labels",
    "example",
    "used",
    "yeah",
    "import",
    "first",
    "target",
    "next",
    "go",
    "truth",
    "told",
    "mean",
    "absolute",
    "error",
    "actually",
    "matter",
    "much",
    "case",
    "staying",
    "true",
    "documentation",
    "let",
    "inputs",
    "first",
    "targets",
    "next",
    "rest",
    "course",
    "going",
    "go",
    "optimizer",
    "zero",
    "grad",
    "hmm",
    "discussed",
    "one",
    "okay",
    "going",
    "write",
    "code",
    "going",
    "discuss",
    "actually",
    "discuss",
    "going",
    "write",
    "two",
    "steps",
    "kind",
    "work",
    "together",
    "lot",
    "easier",
    "discuss",
    "optimizer",
    "zero",
    "grad",
    "context",
    "everything",
    "else",
    "perform",
    "back",
    "propagation",
    "loss",
    "respect",
    "parameters",
    "model",
    "back",
    "propagation",
    "going",
    "take",
    "loss",
    "value",
    "lost",
    "backward",
    "always",
    "say",
    "backwards",
    "backward",
    "code",
    "number",
    "five",
    "step",
    "optimizer",
    "perform",
    "gradient",
    "descent",
    "optimizer",
    "dot",
    "step",
    "oh",
    "look",
    "us",
    "wrote",
    "five",
    "major",
    "steps",
    "training",
    "loop",
    "let",
    "discuss",
    "work",
    "together",
    "kind",
    "strange",
    "like",
    "ordering",
    "might",
    "think",
    "oh",
    "order",
    "typically",
    "forward",
    "pass",
    "loss",
    "come",
    "straight",
    "little",
    "bit",
    "ambiguity",
    "around",
    "order",
    "come",
    "optimizer",
    "step",
    "come",
    "back",
    "propagation",
    "like",
    "keep",
    "order",
    "works",
    "let",
    "keep",
    "way",
    "happens",
    "well",
    "also",
    "little",
    "bit",
    "confusing",
    "first",
    "iteration",
    "loop",
    "got",
    "zero",
    "grad",
    "happens",
    "optimizer",
    "makes",
    "calculations",
    "adjust",
    "model",
    "parameters",
    "regards",
    "back",
    "propagation",
    "loss",
    "default",
    "default",
    "optimizer",
    "changes",
    "accumulate",
    "loop",
    "zero",
    "step",
    "three",
    "next",
    "iteration",
    "loop",
    "big",
    "long",
    "comment",
    "saying",
    "let",
    "say",
    "go",
    "loop",
    "optimizer",
    "chooses",
    "value",
    "one",
    "change",
    "one",
    "goes",
    "loop",
    "zero",
    "take",
    "zero",
    "going",
    "one",
    "zero",
    "would",
    "go",
    "okay",
    "next",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "loop",
    "right",
    "looping",
    "10",
    "would",
    "accumulate",
    "value",
    "supposed",
    "change",
    "10",
    "times",
    "want",
    "start",
    "start",
    "fresh",
    "iteration",
    "loop",
    "reason",
    "accumulates",
    "pretty",
    "deep",
    "pytorch",
    "documentation",
    "understanding",
    "something",
    "like",
    "efficiency",
    "computing",
    "find",
    "exact",
    "reason",
    "love",
    "know",
    "zero",
    "perform",
    "back",
    "propagation",
    "recall",
    "back",
    "propagation",
    "discussed",
    "optimizer",
    "step",
    "form",
    "gradient",
    "descent",
    "beauty",
    "pytorch",
    "beauty",
    "pytorch",
    "perform",
    "back",
    "propagation",
    "going",
    "look",
    "second",
    "gradient",
    "descent",
    "us",
    "prevent",
    "video",
    "getting",
    "long",
    "know",
    "written",
    "code",
    "would",
    "like",
    "practice",
    "writing",
    "training",
    "loop",
    "write",
    "code",
    "run",
    "see",
    "happens",
    "actually",
    "comment",
    "going",
    "write",
    "testing",
    "loop",
    "second",
    "extra",
    "curriculum",
    "video",
    "one",
    "rewrite",
    "training",
    "loop",
    "two",
    "sing",
    "pytorch",
    "optimization",
    "loop",
    "song",
    "let",
    "go",
    "want",
    "remember",
    "steps",
    "well",
    "got",
    "song",
    "training",
    "loop",
    "song",
    "discussed",
    "test",
    "step",
    "maybe",
    "could",
    "try",
    "old",
    "version",
    "song",
    "actually",
    "got",
    "new",
    "one",
    "let",
    "sing",
    "together",
    "training",
    "time",
    "forward",
    "pass",
    "calculate",
    "loss",
    "optimise",
    "zero",
    "grad",
    "loss",
    "backwards",
    "optimise",
    "step",
    "step",
    "step",
    "call",
    "optimise",
    "step",
    "jingle",
    "purposes",
    "test",
    "time",
    "let",
    "test",
    "torch",
    "grad",
    "forward",
    "pass",
    "calculate",
    "loss",
    "watch",
    "go",
    "twitter",
    "way",
    "help",
    "remember",
    "steps",
    "going",
    "code",
    "want",
    "video",
    "version",
    "well",
    "going",
    "search",
    "unofficial",
    "pytorch",
    "optimisation",
    "loop",
    "song",
    "oh",
    "look",
    "guy",
    "well",
    "looks",
    "pretty",
    "cool",
    "let",
    "check",
    "time",
    "go",
    "back",
    "training",
    "loop",
    "steps",
    "got",
    "colorful",
    "graphic",
    "coming",
    "next",
    "video",
    "going",
    "write",
    "testing",
    "steps",
    "going",
    "go",
    "back",
    "one",
    "time",
    "talk",
    "happening",
    "like",
    "even",
    "extra",
    "curriculum",
    "forget",
    "videos",
    "shown",
    "back",
    "propagation",
    "gradient",
    "descent",
    "let",
    "leave",
    "video",
    "see",
    "next",
    "one",
    "friends",
    "welcome",
    "back",
    "last",
    "videos",
    "discussing",
    "steps",
    "training",
    "loop",
    "pytorch",
    "fair",
    "bit",
    "going",
    "video",
    "going",
    "step",
    "back",
    "done",
    "recap",
    "going",
    "get",
    "testing",
    "nice",
    "early",
    "right",
    "sun",
    "come",
    "beautiful",
    "morning",
    "writing",
    "code",
    "let",
    "jump",
    "got",
    "little",
    "song",
    "training",
    "steps",
    "epoch",
    "range",
    "forward",
    "pass",
    "calculate",
    "loss",
    "measure",
    "zero",
    "grad",
    "last",
    "backward",
    "measure",
    "step",
    "step",
    "step",
    "little",
    "jingle",
    "use",
    "remember",
    "steps",
    "first",
    "time",
    "write",
    "fair",
    "bit",
    "going",
    "subsequent",
    "steps",
    "subsequent",
    "times",
    "write",
    "start",
    "memorize",
    "even",
    "better",
    "later",
    "going",
    "put",
    "function",
    "call",
    "said",
    "let",
    "jump",
    "colorful",
    "slide",
    "lot",
    "code",
    "page",
    "let",
    "add",
    "color",
    "understand",
    "happening",
    "way",
    "refer",
    "go",
    "hmm",
    "see",
    "going",
    "loop",
    "called",
    "training",
    "loop",
    "step",
    "number",
    "epochs",
    "one",
    "epoch",
    "single",
    "forward",
    "pass",
    "data",
    "pass",
    "data",
    "model",
    "number",
    "epochs",
    "epox",
    "hyper",
    "parameter",
    "means",
    "could",
    "set",
    "100",
    "could",
    "set",
    "1000",
    "could",
    "set",
    "one",
    "going",
    "see",
    "later",
    "video",
    "skip",
    "step",
    "colors",
    "put",
    "model",
    "call",
    "default",
    "mode",
    "model",
    "essentially",
    "sets",
    "whole",
    "bunch",
    "settings",
    "behind",
    "scenes",
    "model",
    "parameters",
    "track",
    "gradients",
    "whole",
    "bunch",
    "learning",
    "behind",
    "scenes",
    "functions",
    "pytorch",
    "lot",
    "us",
    "next",
    "step",
    "forward",
    "pass",
    "perform",
    "forward",
    "pass",
    "training",
    "data",
    "training",
    "loop",
    "important",
    "note",
    "training",
    "loop",
    "model",
    "learns",
    "patterns",
    "training",
    "data",
    "whereas",
    "testing",
    "loop",
    "got",
    "yet",
    "evaluate",
    "patterns",
    "model",
    "learned",
    "parameters",
    "model",
    "learned",
    "unseen",
    "data",
    "pass",
    "data",
    "model",
    "perform",
    "forward",
    "method",
    "located",
    "within",
    "model",
    "object",
    "created",
    "model",
    "object",
    "actually",
    "call",
    "models",
    "whatever",
    "want",
    "good",
    "practice",
    "often",
    "see",
    "called",
    "model",
    "remember",
    "go",
    "back",
    "code",
    "created",
    "forward",
    "method",
    "model",
    "linear",
    "regression",
    "model",
    "class",
    "subclasses",
    "need",
    "create",
    "custom",
    "forward",
    "method",
    "called",
    "forward",
    "pass",
    "well",
    "technical",
    "term",
    "forward",
    "propagation",
    "look",
    "neural",
    "network",
    "picture",
    "forward",
    "propagation",
    "means",
    "going",
    "network",
    "input",
    "output",
    "thing",
    "called",
    "back",
    "propagation",
    "going",
    "discuss",
    "second",
    "happens",
    "call",
    "going",
    "backward",
    "model",
    "let",
    "return",
    "colorful",
    "slide",
    "done",
    "forward",
    "pass",
    "call",
    "forward",
    "method",
    "performs",
    "calculation",
    "data",
    "pass",
    "next",
    "calculate",
    "loss",
    "value",
    "wrong",
    "model",
    "predictions",
    "depend",
    "loss",
    "function",
    "use",
    "kind",
    "predictions",
    "model",
    "outputting",
    "kind",
    "true",
    "values",
    "comparing",
    "model",
    "predictions",
    "training",
    "data",
    "ideally",
    "training",
    "labels",
    "next",
    "step",
    "zero",
    "optimizer",
    "gradients",
    "well",
    "little",
    "confusing",
    "first",
    "epoch",
    "loop",
    "get",
    "optimizer",
    "dot",
    "step",
    "gradients",
    "optimizer",
    "calculates",
    "accumulate",
    "time",
    "epoch",
    "loop",
    "step",
    "want",
    "go",
    "back",
    "zero",
    "exact",
    "reason",
    "behind",
    "optimizer",
    "accumulates",
    "gradients",
    "buried",
    "somewhere",
    "within",
    "pie",
    "torch",
    "documentation",
    "sure",
    "exact",
    "reason",
    "memory",
    "compute",
    "optimization",
    "adds",
    "case",
    "wanted",
    "know",
    "find",
    "exactly",
    "love",
    "know",
    "next",
    "step",
    "perform",
    "back",
    "propagation",
    "loss",
    "function",
    "calling",
    "loss",
    "backward",
    "back",
    "propagation",
    "compute",
    "gradient",
    "every",
    "parameter",
    "requires",
    "grad",
    "equals",
    "true",
    "recall",
    "go",
    "back",
    "code",
    "set",
    "requires",
    "grad",
    "equals",
    "true",
    "parameters",
    "reason",
    "set",
    "requires",
    "grad",
    "equals",
    "true",
    "back",
    "propagation",
    "performed",
    "let",
    "show",
    "gradients",
    "look",
    "like",
    "let",
    "go",
    "loss",
    "function",
    "curve",
    "good",
    "idea",
    "looking",
    "looking",
    "sort",
    "convex",
    "curve",
    "go",
    "l",
    "two",
    "loss",
    "using",
    "l",
    "one",
    "loss",
    "moment",
    "better",
    "one",
    "need",
    "nice",
    "looking",
    "curve",
    "go",
    "keep",
    "track",
    "gradients",
    "behind",
    "scenes",
    "pie",
    "torch",
    "going",
    "create",
    "sort",
    "curve",
    "parameters",
    "looks",
    "like",
    "2d",
    "plot",
    "reason",
    "using",
    "example",
    "google",
    "images",
    "one",
    "going",
    "spend",
    "lot",
    "time",
    "googling",
    "different",
    "things",
    "two",
    "practice",
    "custom",
    "neural",
    "networks",
    "right",
    "two",
    "parameters",
    "quite",
    "easy",
    "visualize",
    "loss",
    "function",
    "curve",
    "like",
    "say",
    "10",
    "million",
    "parameters",
    "basically",
    "ca",
    "visualize",
    "going",
    "pie",
    "torch",
    "take",
    "care",
    "things",
    "behind",
    "scenes",
    "say",
    "requires",
    "grad",
    "pie",
    "torch",
    "going",
    "track",
    "gradients",
    "parameters",
    "trying",
    "back",
    "propagation",
    "subsequently",
    "gradient",
    "descent",
    "calculate",
    "lowest",
    "point",
    "loss",
    "function",
    "msc",
    "loss",
    "could",
    "trade",
    "mae",
    "loss",
    "case",
    "l1",
    "loss",
    "specific",
    "problem",
    "sort",
    "parameter",
    "calculate",
    "gradients",
    "gradient",
    "let",
    "look",
    "gradient",
    "gradient",
    "inclined",
    "part",
    "road",
    "railway",
    "want",
    "machine",
    "learning",
    "going",
    "give",
    "us",
    "machine",
    "learning",
    "gradient",
    "derivative",
    "function",
    "one",
    "input",
    "variable",
    "okay",
    "let",
    "dive",
    "little",
    "deeper",
    "see",
    "beautiful",
    "loss",
    "landscapes",
    "trying",
    "get",
    "bottom",
    "gradient",
    "descent",
    "oh",
    "go",
    "cost",
    "function",
    "also",
    "loss",
    "function",
    "start",
    "random",
    "initial",
    "variable",
    "done",
    "started",
    "random",
    "initial",
    "variable",
    "right",
    "okay",
    "take",
    "learning",
    "step",
    "beautiful",
    "could",
    "weight",
    "parameter",
    "okay",
    "connecting",
    "dots",
    "exciting",
    "got",
    "lot",
    "tabs",
    "right",
    "bring",
    "together",
    "second",
    "trying",
    "come",
    "minimum",
    "need",
    "calculate",
    "gradients",
    "well",
    "gradient",
    "oh",
    "value",
    "weight",
    "go",
    "even",
    "better",
    "love",
    "google",
    "images",
    "loss",
    "value",
    "weight",
    "calculate",
    "gradients",
    "gradient",
    "slope",
    "line",
    "steepness",
    "calculate",
    "gradient",
    "find",
    "really",
    "steep",
    "right",
    "top",
    "incline",
    "might",
    "head",
    "opposite",
    "direction",
    "gradient",
    "gradient",
    "descent",
    "go",
    "step",
    "points",
    "little",
    "thing",
    "wrote",
    "last",
    "video",
    "end",
    "last",
    "video",
    "told",
    "yet",
    "waiting",
    "moment",
    "like",
    "recall",
    "said",
    "kind",
    "three",
    "steps",
    "optimizes",
    "zero",
    "grad",
    "loss",
    "backward",
    "optimizes",
    "step",
    "together",
    "calculate",
    "gradients",
    "want",
    "head",
    "opposite",
    "direction",
    "gradient",
    "get",
    "gradient",
    "value",
    "zero",
    "get",
    "gradient",
    "value",
    "zero",
    "loss",
    "function",
    "well",
    "loss",
    "also",
    "zero",
    "keep",
    "track",
    "gradient",
    "requires",
    "grad",
    "equals",
    "true",
    "pytorch",
    "lot",
    "behind",
    "scenes",
    "want",
    "dig",
    "going",
    "going",
    "show",
    "extra",
    "resources",
    "back",
    "propagation",
    "calculating",
    "gradient",
    "curve",
    "gradient",
    "descent",
    "finding",
    "bottom",
    "towards",
    "end",
    "video",
    "started",
    "side",
    "would",
    "go",
    "opposite",
    "direction",
    "maybe",
    "positive",
    "gradient",
    "go",
    "opposite",
    "direction",
    "want",
    "get",
    "bottom",
    "main",
    "point",
    "gradient",
    "descent",
    "come",
    "back",
    "said",
    "keep",
    "step",
    "size",
    "mind",
    "come",
    "back",
    "created",
    "loss",
    "function",
    "optimizer",
    "put",
    "little",
    "tidbit",
    "optimizer",
    "written",
    "lot",
    "code",
    "really",
    "discussed",
    "going",
    "like",
    "things",
    "fly",
    "need",
    "inside",
    "optimizer",
    "main",
    "two",
    "parameters",
    "params",
    "model",
    "parameters",
    "like",
    "optimize",
    "params",
    "equals",
    "model",
    "zero",
    "dot",
    "parameters",
    "case",
    "pytorch",
    "going",
    "create",
    "something",
    "similar",
    "curve",
    "visually",
    "mathematically",
    "behind",
    "scenes",
    "every",
    "parameter",
    "value",
    "weight",
    "would",
    "potentially",
    "weight",
    "parameter",
    "network",
    "10",
    "million",
    "parameters",
    "way",
    "could",
    "create",
    "curves",
    "beauty",
    "pytorch",
    "behind",
    "scenes",
    "mechanism",
    "called",
    "torch",
    "autograd",
    "auto",
    "gradient",
    "calculation",
    "beautiful",
    "documentation",
    "like",
    "read",
    "works",
    "please",
    "go",
    "essentially",
    "behind",
    "scenes",
    "lot",
    "us",
    "parameter",
    "optimizer",
    "within",
    "optimizer",
    "told",
    "parameters",
    "optimize",
    "learning",
    "rate",
    "learning",
    "rate",
    "another",
    "hyper",
    "parameter",
    "defines",
    "big",
    "small",
    "optimizer",
    "changes",
    "parameters",
    "step",
    "small",
    "learning",
    "rate",
    "results",
    "small",
    "changes",
    "whereas",
    "large",
    "learning",
    "rate",
    "large",
    "changes",
    "look",
    "curve",
    "might",
    "beginning",
    "start",
    "large",
    "steps",
    "get",
    "closer",
    "closer",
    "bottom",
    "get",
    "closer",
    "closer",
    "bottom",
    "prevent",
    "stepping",
    "side",
    "curve",
    "might",
    "smaller",
    "smaller",
    "steps",
    "optimizer",
    "pytorch",
    "optimizers",
    "us",
    "also",
    "another",
    "concept",
    "called",
    "learning",
    "rate",
    "scheduling",
    "something",
    "would",
    "like",
    "look",
    "learning",
    "rate",
    "scheduling",
    "essentially",
    "says",
    "hey",
    "maybe",
    "start",
    "big",
    "steps",
    "get",
    "closer",
    "closer",
    "bottom",
    "reduce",
    "big",
    "steps",
    "take",
    "ever",
    "seen",
    "coin",
    "coin",
    "back",
    "couch",
    "favorite",
    "analogy",
    "ever",
    "tried",
    "reach",
    "coin",
    "back",
    "couch",
    "like",
    "excited",
    "young",
    "chap",
    "reaching",
    "towards",
    "back",
    "couch",
    "take",
    "quite",
    "big",
    "steps",
    "say",
    "arm",
    "would",
    "take",
    "quite",
    "big",
    "steps",
    "get",
    "closer",
    "get",
    "coin",
    "smaller",
    "smaller",
    "steps",
    "otherwise",
    "going",
    "happen",
    "coin",
    "going",
    "lost",
    "took",
    "two",
    "small",
    "steps",
    "never",
    "get",
    "coin",
    "would",
    "take",
    "forever",
    "get",
    "concept",
    "learning",
    "rate",
    "take",
    "two",
    "big",
    "steps",
    "going",
    "end",
    "take",
    "two",
    "small",
    "steps",
    "going",
    "take",
    "forever",
    "get",
    "bottom",
    "bottom",
    "point",
    "called",
    "convergence",
    "another",
    "term",
    "going",
    "come",
    "across",
    "know",
    "throwing",
    "lot",
    "different",
    "terms",
    "whole",
    "concept",
    "learning",
    "rate",
    "big",
    "step",
    "gradient",
    "descent",
    "gradient",
    "descent",
    "back",
    "propagation",
    "calculating",
    "derivative",
    "curves",
    "gradient",
    "curves",
    "parameters",
    "model",
    "let",
    "get",
    "go",
    "back",
    "training",
    "steps",
    "think",
    "back",
    "propagation",
    "done",
    "backward",
    "yes",
    "back",
    "propagation",
    "backward",
    "steps",
    "forward",
    "pass",
    "forward",
    "propagation",
    "go",
    "input",
    "output",
    "back",
    "propagation",
    "take",
    "gradients",
    "loss",
    "function",
    "respect",
    "parameter",
    "model",
    "going",
    "backwards",
    "happens",
    "call",
    "pytorch",
    "us",
    "behind",
    "scenes",
    "finally",
    "step",
    "number",
    "five",
    "step",
    "optimizer",
    "kind",
    "discussed",
    "said",
    "take",
    "step",
    "let",
    "get",
    "loss",
    "curve",
    "back",
    "loss",
    "function",
    "curve",
    "really",
    "matter",
    "curve",
    "use",
    "optimizer",
    "step",
    "taking",
    "step",
    "way",
    "try",
    "optimize",
    "parameters",
    "get",
    "bottom",
    "also",
    "noted",
    "turn",
    "function",
    "necessarily",
    "remember",
    "write",
    "every",
    "single",
    "time",
    "ordering",
    "want",
    "forward",
    "pass",
    "first",
    "calculate",
    "loss",
    "ca",
    "calculate",
    "loss",
    "unless",
    "forward",
    "pass",
    "like",
    "ordering",
    "three",
    "well",
    "also",
    "want",
    "optimizer",
    "step",
    "loss",
    "backward",
    "favorite",
    "ordering",
    "works",
    "like",
    "ordering",
    "take",
    "well",
    "said",
    "think",
    "video",
    "gotten",
    "long",
    "enough",
    "next",
    "video",
    "like",
    "step",
    "training",
    "loop",
    "one",
    "epoch",
    "time",
    "see",
    "know",
    "thrown",
    "lot",
    "words",
    "optimizer",
    "going",
    "try",
    "optimize",
    "parameters",
    "step",
    "let",
    "see",
    "action",
    "parameters",
    "model",
    "actually",
    "change",
    "every",
    "time",
    "go",
    "one",
    "steps",
    "see",
    "next",
    "video",
    "let",
    "step",
    "model",
    "welcome",
    "back",
    "spent",
    "fair",
    "bit",
    "time",
    "training",
    "loop",
    "testing",
    "loop",
    "well",
    "even",
    "got",
    "yet",
    "reason",
    "behind",
    "possibly",
    "one",
    "important",
    "things",
    "aside",
    "getting",
    "data",
    "ready",
    "going",
    "see",
    "later",
    "pytorch",
    "deep",
    "learning",
    "writing",
    "training",
    "loop",
    "literally",
    "like",
    "model",
    "learns",
    "patterns",
    "data",
    "spending",
    "fair",
    "bit",
    "time",
    "get",
    "testing",
    "loop",
    "evaluate",
    "patterns",
    "model",
    "learned",
    "data",
    "important",
    "learning",
    "patterns",
    "following",
    "last",
    "couple",
    "videos",
    "linked",
    "youtube",
    "videos",
    "would",
    "recommend",
    "extra",
    "curriculum",
    "back",
    "propagation",
    "happens",
    "call",
    "loss",
    "stop",
    "backward",
    "optimizer",
    "step",
    "gradient",
    "descent",
    "happening",
    "linked",
    "extra",
    "resources",
    "going",
    "behind",
    "scenes",
    "mathematical",
    "point",
    "view",
    "remember",
    "course",
    "focuses",
    "writing",
    "pytorch",
    "code",
    "like",
    "dive",
    "math",
    "pytorch",
    "triggering",
    "behind",
    "scenes",
    "highly",
    "recommend",
    "two",
    "videos",
    "also",
    "added",
    "note",
    "loss",
    "function",
    "optimizer",
    "use",
    "valid",
    "question",
    "another",
    "one",
    "things",
    "going",
    "problem",
    "specific",
    "experience",
    "time",
    "work",
    "machine",
    "learning",
    "problems",
    "write",
    "lot",
    "code",
    "get",
    "idea",
    "works",
    "particular",
    "problem",
    "set",
    "example",
    "like",
    "regression",
    "problem",
    "like",
    "regression",
    "predicting",
    "number",
    "use",
    "mae",
    "loss",
    "pytorch",
    "causes",
    "l1",
    "loss",
    "could",
    "also",
    "use",
    "mse",
    "loss",
    "optimizer",
    "like",
    "torch",
    "stochastic",
    "gradient",
    "descent",
    "suffice",
    "classification",
    "might",
    "want",
    "look",
    "binary",
    "classification",
    "binary",
    "cross",
    "entropy",
    "loss",
    "look",
    "classification",
    "problem",
    "later",
    "course",
    "like",
    "demonstrate",
    "going",
    "steps",
    "let",
    "go",
    "model",
    "zero",
    "let",
    "look",
    "state",
    "dict",
    "see",
    "parameters",
    "original",
    "ones",
    "think",
    "let",
    "model",
    "get",
    "new",
    "parameters",
    "yeah",
    "recreated",
    "might",
    "get",
    "rid",
    "rerun",
    "model",
    "code",
    "rerun",
    "model",
    "state",
    "dict",
    "create",
    "instance",
    "model",
    "make",
    "sure",
    "parameters",
    "something",
    "similar",
    "exactly",
    "like",
    "matter",
    "yeah",
    "going",
    "showcase",
    "see",
    "screen",
    "going",
    "anyway",
    "state",
    "dict",
    "3367",
    "weight",
    "012888",
    "bias",
    "ca",
    "stress",
    "enough",
    "got",
    "two",
    "parameters",
    "model",
    "set",
    "future",
    "models",
    "build",
    "later",
    "ones",
    "course",
    "much",
    "much",
    "wo",
    "actually",
    "explicitly",
    "set",
    "check",
    "predictions",
    "going",
    "terrible",
    "using",
    "random",
    "parameters",
    "begin",
    "set",
    "new",
    "loss",
    "function",
    "optimizer",
    "optimizer",
    "going",
    "optimize",
    "model",
    "zero",
    "parameters",
    "weight",
    "bias",
    "learning",
    "rate",
    "relatively",
    "large",
    "step",
    "would",
    "bit",
    "smaller",
    "remember",
    "larger",
    "learning",
    "rate",
    "bigger",
    "step",
    "optimizer",
    "try",
    "change",
    "parameters",
    "every",
    "step",
    "let",
    "stop",
    "talking",
    "let",
    "see",
    "action",
    "set",
    "manual",
    "seed",
    "way",
    "optimizer",
    "steps",
    "going",
    "quite",
    "random",
    "well",
    "depending",
    "models",
    "predictions",
    "go",
    "try",
    "make",
    "reproduces",
    "possible",
    "keep",
    "mind",
    "get",
    "different",
    "values",
    "going",
    "output",
    "screen",
    "screen",
    "worry",
    "much",
    "important",
    "direction",
    "going",
    "ideally",
    "moving",
    "values",
    "one",
    "epoch",
    "moving",
    "values",
    "closer",
    "true",
    "values",
    "practice",
    "wo",
    "necessarily",
    "know",
    "true",
    "values",
    "evaluation",
    "model",
    "comes",
    "going",
    "cover",
    "write",
    "testing",
    "loop",
    "let",
    "run",
    "one",
    "epoch",
    "going",
    "keep",
    "watch",
    "happens",
    "done",
    "one",
    "epoch",
    "single",
    "epoch",
    "done",
    "forward",
    "pass",
    "calculated",
    "loss",
    "done",
    "optimizer",
    "zero",
    "grad",
    "performed",
    "back",
    "propagation",
    "stepped",
    "optimizer",
    "stepping",
    "optimizer",
    "updates",
    "model",
    "parameters",
    "try",
    "get",
    "closer",
    "towards",
    "weight",
    "bias",
    "loss",
    "closer",
    "zero",
    "trying",
    "print",
    "loss",
    "time",
    "print",
    "loss",
    "loss",
    "let",
    "take",
    "another",
    "step",
    "loss",
    "check",
    "weights",
    "bias",
    "changed",
    "three",
    "three",
    "four",
    "four",
    "five",
    "one",
    "four",
    "eight",
    "eight",
    "go",
    "loss",
    "going",
    "check",
    "hey",
    "look",
    "values",
    "getting",
    "closer",
    "slightly",
    "loss",
    "went",
    "oh",
    "goodness",
    "amazing",
    "look",
    "training",
    "let",
    "print",
    "cell",
    "print",
    "model",
    "state",
    "dict",
    "training",
    "first",
    "machine",
    "learning",
    "model",
    "people",
    "exciting",
    "even",
    "step",
    "step",
    "small",
    "model",
    "important",
    "loss",
    "going",
    "values",
    "getting",
    "closer",
    "wo",
    "really",
    "know",
    "real",
    "problems",
    "let",
    "get",
    "excited",
    "real",
    "way",
    "sort",
    "measure",
    "model",
    "progress",
    "practice",
    "lower",
    "loss",
    "value",
    "remember",
    "lower",
    "better",
    "loss",
    "value",
    "measures",
    "wrong",
    "model",
    "going",
    "going",
    "right",
    "direction",
    "meant",
    "long",
    "values",
    "going",
    "similar",
    "direction",
    "writing",
    "similar",
    "code",
    "values",
    "slightly",
    "different",
    "terms",
    "exact",
    "numbers",
    "worry",
    "much",
    "inherent",
    "randomness",
    "machine",
    "learning",
    "steps",
    "optimizer",
    "taking",
    "inherently",
    "random",
    "sort",
    "pushed",
    "direction",
    "gradient",
    "descent",
    "beautiful",
    "low",
    "get",
    "loss",
    "try",
    "get",
    "look",
    "getting",
    "close",
    "mean",
    "hand",
    "hand",
    "bias",
    "getting",
    "close",
    "exactly",
    "beautiful",
    "say",
    "10",
    "passes",
    "data",
    "seeing",
    "practice",
    "seeing",
    "happen",
    "seeing",
    "gradient",
    "descent",
    "let",
    "go",
    "gradient",
    "descent",
    "work",
    "action",
    "got",
    "images",
    "happening",
    "got",
    "cost",
    "function",
    "j",
    "another",
    "term",
    "cost",
    "function",
    "also",
    "loss",
    "function",
    "start",
    "initial",
    "weight",
    "done",
    "started",
    "initial",
    "weight",
    "value",
    "measured",
    "gradient",
    "pytorch",
    "done",
    "behind",
    "scenes",
    "us",
    "thank",
    "pytorch",
    "taking",
    "steps",
    "towards",
    "minimum",
    "trying",
    "minimize",
    "gradient",
    "weight",
    "minimize",
    "cost",
    "function",
    "also",
    "loss",
    "function",
    "could",
    "keep",
    "going",
    "hours",
    "get",
    "long",
    "want",
    "challenge",
    "actually",
    "make",
    "predictions",
    "model",
    "got",
    "right",
    "let",
    "make",
    "predictions",
    "torch",
    "dot",
    "inference",
    "mode",
    "make",
    "predictions",
    "together",
    "going",
    "set",
    "challenge",
    "run",
    "code",
    "100",
    "epochs",
    "video",
    "make",
    "predictions",
    "see",
    "goes",
    "preds",
    "remember",
    "poor",
    "predictions",
    "preds",
    "new",
    "equals",
    "forward",
    "pass",
    "model",
    "zero",
    "test",
    "data",
    "let",
    "remind",
    "quickly",
    "poor",
    "previous",
    "predictions",
    "plot",
    "predictions",
    "predictions",
    "equals",
    "still",
    "saved",
    "preds",
    "hopefully",
    "still",
    "saved",
    "go",
    "shocking",
    "predictions",
    "done",
    "10",
    "epochs",
    "10",
    "training",
    "steps",
    "predictions",
    "look",
    "better",
    "let",
    "run",
    "copy",
    "code",
    "know",
    "rule",
    "really",
    "like",
    "copy",
    "code",
    "case",
    "want",
    "exemplify",
    "point",
    "like",
    "write",
    "code",
    "got",
    "preds",
    "new",
    "look",
    "moving",
    "predictions",
    "close",
    "red",
    "dots",
    "closer",
    "green",
    "dots",
    "happening",
    "reducing",
    "loss",
    "words",
    "reducing",
    "difference",
    "models",
    "predictions",
    "ideal",
    "outcomes",
    "power",
    "back",
    "propagation",
    "gradient",
    "descent",
    "super",
    "exciting",
    "training",
    "first",
    "machine",
    "learning",
    "model",
    "challenge",
    "run",
    "code",
    "change",
    "epochs",
    "see",
    "low",
    "get",
    "loss",
    "value",
    "run",
    "predictions",
    "plot",
    "think",
    "time",
    "start",
    "testing",
    "give",
    "go",
    "write",
    "testing",
    "code",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "something",
    "super",
    "excited",
    "saw",
    "loss",
    "go",
    "loss",
    "remember",
    "different",
    "models",
    "predictions",
    "ideally",
    "like",
    "saw",
    "model",
    "update",
    "parameters",
    "power",
    "back",
    "propagation",
    "gradient",
    "descent",
    "taken",
    "care",
    "behind",
    "scenes",
    "us",
    "pytorch",
    "thank",
    "pytorch",
    "like",
    "extra",
    "resources",
    "actually",
    "happening",
    "math",
    "perspective",
    "back",
    "propagation",
    "gradient",
    "descent",
    "would",
    "refer",
    "otherwise",
    "also",
    "learn",
    "things",
    "gradient",
    "descent",
    "go",
    "gradient",
    "descent",
    "work",
    "got",
    "back",
    "propagation",
    "reiterate",
    "googling",
    "things",
    "going",
    "practice",
    "going",
    "come",
    "across",
    "lot",
    "different",
    "things",
    "covered",
    "course",
    "seriously",
    "day",
    "day",
    "machine",
    "learning",
    "engineer",
    "know",
    "going",
    "go",
    "google",
    "read",
    "watch",
    "video",
    "write",
    "code",
    "build",
    "intuition",
    "said",
    "also",
    "issued",
    "challenge",
    "trying",
    "run",
    "training",
    "code",
    "100",
    "epochs",
    "give",
    "go",
    "hope",
    "low",
    "loss",
    "value",
    "weights",
    "bias",
    "get",
    "anywhere",
    "close",
    "predictions",
    "look",
    "going",
    "save",
    "later",
    "running",
    "code",
    "100",
    "epochs",
    "let",
    "write",
    "testing",
    "code",
    "note",
    "necessarily",
    "write",
    "training",
    "testing",
    "loop",
    "together",
    "functionize",
    "later",
    "sake",
    "intuition",
    "building",
    "code",
    "practicing",
    "first",
    "time",
    "writing",
    "code",
    "together",
    "going",
    "write",
    "together",
    "testing",
    "code",
    "call",
    "turns",
    "different",
    "settings",
    "model",
    "needed",
    "evaluation",
    "slash",
    "testing",
    "little",
    "confusing",
    "remember",
    "writing",
    "testing",
    "code",
    "going",
    "times",
    "habit",
    "make",
    "habit",
    "training",
    "model",
    "call",
    "model",
    "dot",
    "train",
    "make",
    "sure",
    "training",
    "mode",
    "testing",
    "evaluating",
    "model",
    "vowel",
    "stands",
    "evaluate",
    "call",
    "model",
    "dot",
    "vowel",
    "turns",
    "different",
    "settings",
    "model",
    "needed",
    "evaluation",
    "testing",
    "things",
    "like",
    "drop",
    "seen",
    "drop",
    "slash",
    "batch",
    "norm",
    "layers",
    "go",
    "torch",
    "dot",
    "end",
    "sure",
    "come",
    "across",
    "things",
    "future",
    "machine",
    "learning",
    "endeavors",
    "drop",
    "drop",
    "layers",
    "go",
    "batch",
    "norm",
    "batch",
    "batch",
    "norm",
    "go",
    "like",
    "work",
    "feel",
    "free",
    "check",
    "documentation",
    "take",
    "model",
    "turns",
    "different",
    "settings",
    "needed",
    "evaluation",
    "testing",
    "set",
    "torch",
    "dot",
    "inference",
    "mode",
    "inference",
    "mode",
    "let",
    "write",
    "turns",
    "gradient",
    "tracking",
    "discussed",
    "parameters",
    "model",
    "turns",
    "actually",
    "things",
    "couple",
    "things",
    "behind",
    "scenes",
    "things",
    "needed",
    "testing",
    "discussed",
    "parameters",
    "model",
    "requires",
    "grad",
    "equals",
    "true",
    "default",
    "many",
    "different",
    "parameters",
    "pytorch",
    "pytorch",
    "behind",
    "scenes",
    "keep",
    "track",
    "gradients",
    "model",
    "use",
    "lost",
    "backward",
    "optimizer",
    "step",
    "back",
    "propagation",
    "gradient",
    "descent",
    "however",
    "need",
    "two",
    "back",
    "propagation",
    "gradient",
    "descent",
    "training",
    "model",
    "learning",
    "testing",
    "evaluating",
    "parameters",
    "patterns",
    "model",
    "learned",
    "training",
    "data",
    "set",
    "need",
    "learning",
    "testing",
    "turn",
    "things",
    "need",
    "going",
    "correct",
    "spacing",
    "sure",
    "find",
    "still",
    "forward",
    "pass",
    "testing",
    "mode",
    "forward",
    "pass",
    "want",
    "look",
    "torch",
    "inference",
    "mode",
    "go",
    "torch",
    "inference",
    "mode",
    "great",
    "tweet",
    "pytorch",
    "explains",
    "happening",
    "think",
    "covered",
    "yeah",
    "want",
    "make",
    "inference",
    "code",
    "pytorch",
    "run",
    "faster",
    "quick",
    "thread",
    "exactly",
    "inference",
    "mode",
    "torch",
    "grad",
    "might",
    "see",
    "torch",
    "grad",
    "think",
    "write",
    "let",
    "know",
    "happening",
    "behind",
    "scenes",
    "lot",
    "optimization",
    "code",
    "beautiful",
    "using",
    "pytorch",
    "code",
    "runs",
    "nice",
    "far",
    "let",
    "go",
    "may",
    "also",
    "see",
    "torch",
    "dot",
    "grad",
    "older",
    "pytorch",
    "code",
    "similar",
    "things",
    "inference",
    "mode",
    "faster",
    "way",
    "things",
    "according",
    "thread",
    "according",
    "blog",
    "post",
    "attached",
    "well",
    "believe",
    "may",
    "also",
    "see",
    "torch",
    "dot",
    "grad",
    "older",
    "pytorch",
    "code",
    "would",
    "valid",
    "inference",
    "mode",
    "better",
    "way",
    "things",
    "forward",
    "pass",
    "let",
    "get",
    "model",
    "want",
    "create",
    "test",
    "predictions",
    "going",
    "go",
    "model",
    "zero",
    "lot",
    "code",
    "going",
    "going",
    "step",
    "step",
    "second",
    "go",
    "back",
    "number",
    "two",
    "calculate",
    "loss",
    "test",
    "predictions",
    "calculate",
    "loss",
    "test",
    "predictions",
    "model",
    "zero",
    "want",
    "calculate",
    "want",
    "calculate",
    "test",
    "loss",
    "loss",
    "function",
    "difference",
    "test",
    "pred",
    "test",
    "labels",
    "important",
    "testing",
    "working",
    "test",
    "data",
    "training",
    "working",
    "training",
    "data",
    "model",
    "learns",
    "patterns",
    "training",
    "data",
    "evaluates",
    "patterns",
    "learned",
    "different",
    "parameters",
    "testing",
    "data",
    "never",
    "seen",
    "like",
    "university",
    "course",
    "study",
    "course",
    "materials",
    "training",
    "data",
    "evaluate",
    "knowledge",
    "materials",
    "hopefully",
    "never",
    "seen",
    "unless",
    "sort",
    "friends",
    "professor",
    "gave",
    "exam",
    "actual",
    "exam",
    "would",
    "cheating",
    "right",
    "important",
    "point",
    "test",
    "data",
    "set",
    "let",
    "model",
    "see",
    "test",
    "data",
    "set",
    "evaluate",
    "otherwise",
    "get",
    "poor",
    "results",
    "putting",
    "happening",
    "epoch",
    "going",
    "go",
    "epoch",
    "introduce",
    "little",
    "jingle",
    "remember",
    "steps",
    "lot",
    "going",
    "worry",
    "know",
    "lot",
    "going",
    "practice",
    "going",
    "know",
    "happening",
    "like",
    "back",
    "hand",
    "right",
    "need",
    "oh",
    "yeah",
    "could",
    "say",
    "oh",
    "need",
    "test",
    "loss",
    "loss",
    "test",
    "print",
    "happening",
    "okay",
    "actually",
    "need",
    "every",
    "epoch",
    "could",
    "go",
    "say",
    "epoch",
    "divided",
    "10",
    "equals",
    "zero",
    "print",
    "happening",
    "let",
    "rather",
    "clutter",
    "everything",
    "print",
    "print",
    "let",
    "step",
    "happening",
    "got",
    "100",
    "epochs",
    "run",
    "100",
    "epochs",
    "model",
    "trained",
    "10",
    "far",
    "got",
    "good",
    "base",
    "maybe",
    "get",
    "rid",
    "base",
    "start",
    "new",
    "instance",
    "model",
    "come",
    "right",
    "back",
    "model",
    "back",
    "randomly",
    "initialized",
    "parameters",
    "course",
    "randomly",
    "initialized",
    "flavored",
    "random",
    "seed",
    "lovely",
    "lovely",
    "got",
    "training",
    "code",
    "discussed",
    "happening",
    "got",
    "testing",
    "code",
    "call",
    "model",
    "dot",
    "eval",
    "turns",
    "different",
    "settings",
    "model",
    "needed",
    "evaluation",
    "slash",
    "testing",
    "call",
    "torch",
    "inference",
    "mode",
    "context",
    "manager",
    "turns",
    "gradient",
    "tracking",
    "couple",
    "things",
    "behind",
    "scenes",
    "make",
    "code",
    "faster",
    "forward",
    "pass",
    "test",
    "predictions",
    "pass",
    "model",
    "test",
    "data",
    "test",
    "features",
    "calculate",
    "test",
    "predictions",
    "calculate",
    "loss",
    "using",
    "loss",
    "function",
    "use",
    "loss",
    "function",
    "used",
    "training",
    "data",
    "called",
    "test",
    "loss",
    "test",
    "data",
    "set",
    "print",
    "happening",
    "want",
    "know",
    "happening",
    "model",
    "training",
    "necessarily",
    "beauty",
    "pytorch",
    "use",
    "basic",
    "python",
    "printing",
    "statements",
    "see",
    "happening",
    "model",
    "100",
    "epochs",
    "want",
    "clutter",
    "everything",
    "print",
    "happening",
    "every",
    "10th",
    "epoch",
    "customize",
    "much",
    "like",
    "printing",
    "one",
    "example",
    "metrics",
    "calculating",
    "model",
    "accuracy",
    "might",
    "see",
    "later",
    "hint",
    "hint",
    "might",
    "print",
    "model",
    "accuracy",
    "exciting",
    "ready",
    "run",
    "100",
    "epochs",
    "low",
    "think",
    "loss",
    "go",
    "loss",
    "let",
    "save",
    "let",
    "give",
    "go",
    "ready",
    "three",
    "two",
    "one",
    "let",
    "run",
    "oh",
    "goodness",
    "look",
    "waits",
    "go",
    "every",
    "10",
    "epochs",
    "printing",
    "happening",
    "zero",
    "epoch",
    "started",
    "losses",
    "look",
    "go",
    "yes",
    "want",
    "weights",
    "bias",
    "moving",
    "towards",
    "ideal",
    "weight",
    "bias",
    "values",
    "yes",
    "moving",
    "right",
    "direction",
    "loss",
    "going",
    "epoch",
    "20",
    "wonderful",
    "epoch",
    "30",
    "even",
    "better",
    "40",
    "50",
    "going",
    "yes",
    "want",
    "want",
    "predicting",
    "straight",
    "line",
    "look",
    "low",
    "loss",
    "gets",
    "100",
    "epochs",
    "got",
    "three",
    "times",
    "less",
    "got",
    "values",
    "quite",
    "close",
    "make",
    "predictions",
    "look",
    "like",
    "preds",
    "new",
    "original",
    "predictions",
    "random",
    "values",
    "make",
    "preds",
    "new",
    "look",
    "close",
    "100",
    "epochs",
    "print",
    "test",
    "loss",
    "oh",
    "printing",
    "loss",
    "well",
    "let",
    "get",
    "rid",
    "think",
    "yeah",
    "statement",
    "code",
    "would",
    "much",
    "cleaner",
    "right",
    "life",
    "goes",
    "test",
    "loss",
    "test",
    "predictions",
    "making",
    "low",
    "training",
    "loss",
    "wonder",
    "could",
    "get",
    "lower",
    "think",
    "could",
    "trained",
    "longer",
    "happened",
    "think",
    "could",
    "get",
    "red",
    "dots",
    "line",
    "green",
    "dots",
    "think",
    "could",
    "challenge",
    "next",
    "video",
    "think",
    "something",
    "could",
    "get",
    "red",
    "dots",
    "match",
    "green",
    "dots",
    "maybe",
    "train",
    "longer",
    "think",
    "could",
    "give",
    "shot",
    "see",
    "next",
    "video",
    "review",
    "testing",
    "code",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "something",
    "super",
    "exciting",
    "trained",
    "model",
    "100",
    "epochs",
    "look",
    "good",
    "predictions",
    "got",
    "finished",
    "challenging",
    "see",
    "could",
    "align",
    "red",
    "dots",
    "green",
    "dots",
    "okay",
    "sure",
    "best",
    "way",
    "learn",
    "best",
    "way",
    "things",
    "together",
    "might",
    "idea",
    "potentially",
    "training",
    "model",
    "little",
    "bit",
    "longer",
    "could",
    "well",
    "could",
    "rerun",
    "code",
    "model",
    "going",
    "remember",
    "parameters",
    "done",
    "rerun",
    "well",
    "going",
    "start",
    "finished",
    "already",
    "pretty",
    "good",
    "data",
    "set",
    "going",
    "try",
    "improve",
    "even",
    "ca",
    "stress",
    "enough",
    "like",
    "going",
    "similar",
    "throughout",
    "entire",
    "rest",
    "course",
    "training",
    "models",
    "step",
    "done",
    "training",
    "model",
    "evaluating",
    "seriously",
    "like",
    "fundamental",
    "steps",
    "deep",
    "learning",
    "pytorch",
    "training",
    "evaluating",
    "model",
    "done",
    "although",
    "predict",
    "red",
    "dots",
    "green",
    "dots",
    "right",
    "let",
    "try",
    "line",
    "hey",
    "red",
    "dots",
    "onto",
    "green",
    "dots",
    "reckon",
    "train",
    "another",
    "100",
    "epochs",
    "get",
    "pretty",
    "darn",
    "close",
    "ready",
    "three",
    "two",
    "one",
    "going",
    "run",
    "cell",
    "runs",
    "really",
    "quick",
    "data",
    "nice",
    "simple",
    "look",
    "lastly",
    "started",
    "get",
    "oh",
    "goodness",
    "improved",
    "another",
    "three",
    "x",
    "model",
    "got",
    "really",
    "good",
    "test",
    "loss",
    "gone",
    "gone",
    "almost",
    "10x",
    "improvement",
    "make",
    "predictions",
    "model",
    "parameters",
    "remember",
    "ideal",
    "ones",
    "wo",
    "necessarily",
    "know",
    "practice",
    "working",
    "simple",
    "data",
    "set",
    "know",
    "ideal",
    "parameters",
    "model",
    "zero",
    "state",
    "dig",
    "weights",
    "previously",
    "going",
    "change",
    "oh",
    "would",
    "look",
    "oh",
    "slightly",
    "different",
    "mine",
    "worry",
    "much",
    "inherent",
    "randomness",
    "machine",
    "learning",
    "deep",
    "learning",
    "even",
    "though",
    "set",
    "manual",
    "seed",
    "may",
    "slightly",
    "different",
    "direction",
    "important",
    "number",
    "exactly",
    "mine",
    "still",
    "quite",
    "close",
    "thing",
    "one",
    "exactly",
    "mine",
    "worry",
    "much",
    "loss",
    "values",
    "well",
    "direction",
    "important",
    "pretty",
    "darn",
    "close",
    "predictions",
    "look",
    "remember",
    "original",
    "ones",
    "started",
    "random",
    "trained",
    "model",
    "close",
    "close",
    "exactly",
    "little",
    "bit",
    "right",
    "could",
    "tweak",
    "things",
    "improve",
    "think",
    "well",
    "truly",
    "enough",
    "example",
    "purpose",
    "see",
    "happened",
    "course",
    "could",
    "create",
    "model",
    "set",
    "parameters",
    "manually",
    "would",
    "fun",
    "wrote",
    "machine",
    "learning",
    "code",
    "us",
    "power",
    "back",
    "propagation",
    "gradient",
    "descent",
    "last",
    "video",
    "wrote",
    "testing",
    "loop",
    "discussed",
    "steps",
    "let",
    "go",
    "colorful",
    "slide",
    "hey",
    "mean",
    "code",
    "page",
    "nice",
    "colors",
    "even",
    "nicer",
    "oh",
    "done",
    "might",
    "set",
    "video",
    "let",
    "discuss",
    "going",
    "create",
    "empty",
    "list",
    "storing",
    "useful",
    "value",
    "helpful",
    "tracking",
    "model",
    "progress",
    "right",
    "hey",
    "go",
    "go",
    "epoch",
    "count",
    "equals",
    "go",
    "lost",
    "values",
    "keep",
    "track",
    "want",
    "monitor",
    "models",
    "progress",
    "called",
    "tracking",
    "experiments",
    "track",
    "different",
    "values",
    "wanted",
    "try",
    "improve",
    "upon",
    "current",
    "model",
    "future",
    "model",
    "current",
    "results",
    "wanted",
    "try",
    "improve",
    "upon",
    "might",
    "build",
    "entire",
    "model",
    "might",
    "train",
    "different",
    "setup",
    "might",
    "use",
    "different",
    "learning",
    "rate",
    "might",
    "use",
    "whole",
    "bunch",
    "different",
    "settings",
    "track",
    "values",
    "compare",
    "future",
    "experiments",
    "past",
    "experiments",
    "like",
    "brilliant",
    "scientists",
    "could",
    "use",
    "lists",
    "well",
    "calculating",
    "loss",
    "calculating",
    "test",
    "loss",
    "maybe",
    "time",
    "append",
    "going",
    "status",
    "update",
    "epoch",
    "count",
    "dot",
    "append",
    "going",
    "go",
    "current",
    "epoch",
    "go",
    "loss",
    "values",
    "dot",
    "append",
    "current",
    "loss",
    "value",
    "test",
    "loss",
    "values",
    "dot",
    "append",
    "current",
    "test",
    "loss",
    "values",
    "wonderful",
    "let",
    "model",
    "starts",
    "fresh",
    "create",
    "another",
    "instance",
    "going",
    "model",
    "parameters",
    "start",
    "zero",
    "wanted",
    "could",
    "functionize",
    "go",
    "right",
    "back",
    "top",
    "code",
    "demo",
    "purposes",
    "going",
    "run",
    "let",
    "say",
    "200",
    "epochs",
    "ended",
    "right",
    "ran",
    "200",
    "epochs",
    "100",
    "epochs",
    "twice",
    "want",
    "show",
    "something",
    "beautiful",
    "one",
    "beautiful",
    "sites",
    "machine",
    "learning",
    "go",
    "run",
    "200",
    "epochs",
    "start",
    "fairly",
    "high",
    "training",
    "loss",
    "value",
    "fairly",
    "high",
    "test",
    "loss",
    "value",
    "remember",
    "loss",
    "value",
    "go",
    "back",
    "yeah",
    "measuring",
    "loss",
    "means",
    "test",
    "loss",
    "average",
    "dot",
    "points",
    "red",
    "predictions",
    "average",
    "distance",
    "dot",
    "point",
    "ideally",
    "trying",
    "minimize",
    "distance",
    "mean",
    "absolute",
    "error",
    "get",
    "right",
    "make",
    "predictions",
    "get",
    "close",
    "ideal",
    "weight",
    "bias",
    "make",
    "predictions",
    "look",
    "new",
    "predictions",
    "yeah",
    "small",
    "distance",
    "beautiful",
    "low",
    "loss",
    "value",
    "ideally",
    "line",
    "got",
    "close",
    "one",
    "beautiful",
    "sites",
    "machine",
    "learning",
    "plot",
    "loss",
    "curves",
    "let",
    "make",
    "plot",
    "tracking",
    "value",
    "epoch",
    "count",
    "loss",
    "values",
    "test",
    "loss",
    "values",
    "let",
    "look",
    "look",
    "like",
    "epoch",
    "count",
    "goes",
    "loss",
    "values",
    "ideally",
    "go",
    "get",
    "rid",
    "going",
    "create",
    "plot",
    "p",
    "l",
    "dot",
    "plot",
    "going",
    "step",
    "back",
    "test",
    "loop",
    "second",
    "colorful",
    "slides",
    "label",
    "equals",
    "train",
    "loss",
    "going",
    "go",
    "plot",
    "might",
    "able",
    "tell",
    "going",
    "test",
    "loss",
    "values",
    "going",
    "visualize",
    "data",
    "explorer",
    "motto",
    "right",
    "visualize",
    "visualize",
    "visualize",
    "equals",
    "see",
    "collab",
    "auto",
    "correct",
    "really",
    "work",
    "well",
    "know",
    "got",
    "know",
    "say",
    "loss",
    "value",
    "good",
    "auto",
    "correct",
    "thank",
    "collab",
    "training",
    "loss",
    "test",
    "loss",
    "curves",
    "another",
    "term",
    "going",
    "come",
    "across",
    "often",
    "loss",
    "curve",
    "might",
    "able",
    "think",
    "loss",
    "curve",
    "loss",
    "curve",
    "starting",
    "start",
    "training",
    "want",
    "curve",
    "want",
    "loss",
    "value",
    "want",
    "go",
    "ideal",
    "loss",
    "curve",
    "look",
    "like",
    "well",
    "see",
    "couple",
    "let",
    "look",
    "oh",
    "got",
    "wrong",
    "well",
    "need",
    "turn",
    "numpy",
    "getting",
    "wrong",
    "wrong",
    "loss",
    "values",
    "getting",
    "issue",
    "test",
    "loss",
    "values",
    "ah",
    "tens",
    "values",
    "think",
    "let",
    "might",
    "change",
    "numpy",
    "oh",
    "call",
    "numpy",
    "array",
    "going",
    "try",
    "fix",
    "fly",
    "people",
    "numpy",
    "array",
    "turn",
    "numpy",
    "array",
    "let",
    "see",
    "get",
    "numpy",
    "figuring",
    "things",
    "together",
    "numpy",
    "numpy",
    "mapplotlib",
    "works",
    "numpy",
    "yeah",
    "go",
    "loss",
    "values",
    "maybe",
    "going",
    "try",
    "one",
    "thing",
    "torch",
    "dot",
    "tensor",
    "loss",
    "values",
    "call",
    "cpu",
    "dot",
    "numpy",
    "see",
    "happens",
    "go",
    "okay",
    "let",
    "copy",
    "loss",
    "values",
    "still",
    "pytorch",
    "ca",
    "mapplotlib",
    "works",
    "numpy",
    "converting",
    "loss",
    "values",
    "training",
    "loss",
    "numpy",
    "call",
    "fundamental",
    "section",
    "call",
    "cpu",
    "numpy",
    "wonder",
    "straight",
    "numpy",
    "working",
    "yeah",
    "okay",
    "need",
    "cpu",
    "working",
    "gpu",
    "yet",
    "might",
    "need",
    "later",
    "well",
    "work",
    "beautiful",
    "go",
    "one",
    "beautiful",
    "sides",
    "machine",
    "learning",
    "declining",
    "loss",
    "curve",
    "keep",
    "track",
    "experiments",
    "one",
    "way",
    "quite",
    "rudimentary",
    "like",
    "automate",
    "later",
    "showing",
    "one",
    "way",
    "keep",
    "track",
    "happening",
    "training",
    "loss",
    "curve",
    "going",
    "training",
    "loss",
    "starts",
    "goes",
    "right",
    "beautiful",
    "thing",
    "match",
    "two",
    "bigger",
    "distance",
    "behind",
    "train",
    "loss",
    "test",
    "loss",
    "sorry",
    "running",
    "problems",
    "match",
    "closely",
    "point",
    "means",
    "model",
    "converging",
    "loss",
    "getting",
    "close",
    "zero",
    "possibly",
    "trained",
    "longer",
    "maybe",
    "loss",
    "go",
    "almost",
    "basically",
    "zero",
    "experiment",
    "leave",
    "try",
    "train",
    "model",
    "longer",
    "let",
    "step",
    "back",
    "testing",
    "loop",
    "finish",
    "video",
    "created",
    "empty",
    "lists",
    "strong",
    "useful",
    "values",
    "storing",
    "useful",
    "values",
    "strong",
    "useful",
    "values",
    "told",
    "model",
    "want",
    "evaluate",
    "want",
    "evaluate",
    "put",
    "evaluation",
    "mode",
    "turns",
    "functionality",
    "used",
    "training",
    "evaluations",
    "drop",
    "batch",
    "normalization",
    "layers",
    "want",
    "learn",
    "look",
    "documentation",
    "turn",
    "torch",
    "inference",
    "mode",
    "faster",
    "performance",
    "necessarily",
    "need",
    "good",
    "practice",
    "going",
    "say",
    "yes",
    "turn",
    "torch",
    "inference",
    "mode",
    "disables",
    "functionality",
    "gradient",
    "tracking",
    "inference",
    "gradient",
    "tracking",
    "needed",
    "inference",
    "training",
    "pass",
    "test",
    "data",
    "model",
    "call",
    "models",
    "implemented",
    "forward",
    "method",
    "forward",
    "pass",
    "exact",
    "training",
    "loop",
    "except",
    "test",
    "data",
    "big",
    "notion",
    "training",
    "loop",
    "training",
    "data",
    "testing",
    "loop",
    "testing",
    "data",
    "calculate",
    "test",
    "loss",
    "value",
    "wrong",
    "models",
    "predictions",
    "test",
    "data",
    "set",
    "course",
    "lower",
    "better",
    "finally",
    "print",
    "happening",
    "keep",
    "track",
    "going",
    "training",
    "necessarily",
    "customize",
    "print",
    "value",
    "print",
    "almost",
    "whatever",
    "want",
    "pie",
    "torches",
    "basically",
    "beautifully",
    "interactive",
    "pure",
    "python",
    "keep",
    "track",
    "values",
    "going",
    "epochs",
    "train",
    "loss",
    "test",
    "loss",
    "could",
    "keep",
    "track",
    "values",
    "going",
    "okay",
    "loss",
    "value",
    "particular",
    "epoch",
    "training",
    "set",
    "test",
    "set",
    "course",
    "could",
    "put",
    "function",
    "way",
    "wo",
    "remember",
    "steps",
    "heart",
    "reason",
    "spent",
    "much",
    "time",
    "going",
    "using",
    "training",
    "test",
    "functionality",
    "models",
    "build",
    "throughout",
    "course",
    "give",
    "pat",
    "back",
    "getting",
    "videos",
    "written",
    "lot",
    "code",
    "discussed",
    "lot",
    "steps",
    "like",
    "song",
    "remember",
    "happening",
    "let",
    "finish",
    "video",
    "unofficial",
    "pytorch",
    "optimization",
    "loop",
    "song",
    "epoch",
    "range",
    "go",
    "model",
    "dot",
    "train",
    "forward",
    "pass",
    "calculate",
    "loss",
    "optimize",
    "zero",
    "grad",
    "loss",
    "backward",
    "optimize",
    "step",
    "step",
    "step",
    "call",
    "let",
    "test",
    "go",
    "model",
    "dot",
    "eval",
    "torch",
    "inference",
    "mode",
    "forward",
    "pass",
    "calculate",
    "loss",
    "real",
    "song",
    "goes",
    "another",
    "epoch",
    "keep",
    "going",
    "back",
    "finish",
    "print",
    "happening",
    "course",
    "evaluate",
    "going",
    "said",
    "time",
    "move",
    "another",
    "thing",
    "like",
    "review",
    "happening",
    "please",
    "please",
    "please",
    "try",
    "run",
    "code",
    "check",
    "slides",
    "also",
    "check",
    "extra",
    "curriculum",
    "oh",
    "way",
    "want",
    "link",
    "extra",
    "curriculum",
    "go",
    "book",
    "version",
    "course",
    "going",
    "ready",
    "go",
    "everything",
    "link",
    "extra",
    "curriculum",
    "extra",
    "curriculum",
    "chapter",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "saw",
    "train",
    "model",
    "evaluate",
    "looking",
    "loss",
    "metrics",
    "loss",
    "curves",
    "also",
    "plotted",
    "predictions",
    "compared",
    "hey",
    "go",
    "random",
    "predictions",
    "quite",
    "terrible",
    "trained",
    "model",
    "using",
    "power",
    "back",
    "propagation",
    "gradient",
    "descent",
    "look",
    "predictions",
    "almost",
    "exactly",
    "want",
    "might",
    "thinking",
    "well",
    "trained",
    "model",
    "took",
    "us",
    "write",
    "code",
    "get",
    "good",
    "predictions",
    "might",
    "run",
    "model",
    "took",
    "little",
    "break",
    "last",
    "video",
    "come",
    "back",
    "might",
    "notice",
    "google",
    "colab",
    "notebook",
    "disconnected",
    "mean",
    "run",
    "going",
    "work",
    "going",
    "connect",
    "new",
    "google",
    "colab",
    "instance",
    "code",
    "run",
    "might",
    "already",
    "experienced",
    "took",
    "break",
    "came",
    "back",
    "videos",
    "ah",
    "plot",
    "predictions",
    "longer",
    "defined",
    "know",
    "means",
    "means",
    "model",
    "also",
    "longer",
    "defined",
    "would",
    "lost",
    "model",
    "would",
    "lost",
    "effort",
    "training",
    "luckily",
    "train",
    "model",
    "long",
    "go",
    "run",
    "time",
    "run",
    "going",
    "rerun",
    "previous",
    "cells",
    "quite",
    "quick",
    "working",
    "small",
    "data",
    "set",
    "using",
    "small",
    "model",
    "code",
    "oh",
    "got",
    "wrong",
    "model",
    "zero",
    "state",
    "dict",
    "well",
    "right",
    "good",
    "finding",
    "errors",
    "want",
    "well",
    "go",
    "run",
    "going",
    "run",
    "cells",
    "beautiful",
    "come",
    "back",
    "model",
    "training",
    "getting",
    "similar",
    "values",
    "got",
    "lost",
    "curves",
    "beautiful",
    "still",
    "going",
    "okay",
    "predictions",
    "back",
    "rerun",
    "cells",
    "got",
    "model",
    "might",
    "cover",
    "video",
    "saving",
    "model",
    "pytorch",
    "training",
    "model",
    "get",
    "certain",
    "point",
    "especially",
    "larger",
    "model",
    "probably",
    "want",
    "save",
    "reuse",
    "particular",
    "notebook",
    "might",
    "want",
    "save",
    "somewhere",
    "send",
    "friend",
    "friend",
    "try",
    "might",
    "want",
    "use",
    "week",
    "time",
    "google",
    "colab",
    "disconnected",
    "might",
    "want",
    "able",
    "load",
    "back",
    "somehow",
    "let",
    "see",
    "save",
    "models",
    "pytorch",
    "going",
    "write",
    "three",
    "main",
    "methods",
    "know",
    "saving",
    "loading",
    "models",
    "pytorch",
    "course",
    "saving",
    "comes",
    "loading",
    "going",
    "next",
    "two",
    "videos",
    "discuss",
    "saving",
    "loading",
    "one",
    "might",
    "guess",
    "allows",
    "save",
    "pytorch",
    "object",
    "python",
    "pickle",
    "format",
    "may",
    "may",
    "aware",
    "python",
    "pickle",
    "go",
    "python",
    "object",
    "serialization",
    "go",
    "got",
    "pickle",
    "module",
    "implements",
    "binary",
    "protocols",
    "implements",
    "binary",
    "protocols",
    "serializing",
    "deserializing",
    "python",
    "object",
    "serializing",
    "means",
    "understand",
    "saving",
    "deserializing",
    "means",
    "loading",
    "pytorch",
    "uses",
    "behind",
    "scenes",
    "pure",
    "python",
    "go",
    "back",
    "python",
    "pickle",
    "format",
    "number",
    "two",
    "might",
    "able",
    "guess",
    "well",
    "allows",
    "load",
    "saved",
    "pytorch",
    "object",
    "number",
    "three",
    "also",
    "important",
    "allow",
    "well",
    "allows",
    "load",
    "model",
    "saved",
    "dictionary",
    "save",
    "state",
    "dictionary",
    "yeah",
    "call",
    "save",
    "state",
    "dictionary",
    "beautiful",
    "model",
    "state",
    "dict",
    "well",
    "let",
    "look",
    "model",
    "zero",
    "dot",
    "state",
    "dict",
    "beauty",
    "pytorch",
    "stores",
    "lot",
    "model",
    "important",
    "parameters",
    "simple",
    "python",
    "dictionary",
    "might",
    "simple",
    "model",
    "two",
    "parameters",
    "future",
    "may",
    "working",
    "models",
    "millions",
    "parameters",
    "looking",
    "directly",
    "state",
    "deck",
    "may",
    "simple",
    "got",
    "principle",
    "still",
    "still",
    "dictionary",
    "holds",
    "state",
    "model",
    "got",
    "three",
    "methods",
    "want",
    "show",
    "going",
    "extra",
    "curriculum",
    "save",
    "load",
    "models",
    "extra",
    "curriculum",
    "video",
    "go",
    "important",
    "piece",
    "pytorch",
    "documentation",
    "maybe",
    "even",
    "tutorial",
    "extra",
    "curriculum",
    "video",
    "go",
    "go",
    "got",
    "torch",
    "save",
    "torch",
    "load",
    "torch",
    "module",
    "state",
    "deck",
    "load",
    "state",
    "deck",
    "got",
    "three",
    "things",
    "written",
    "fair",
    "different",
    "pieces",
    "information",
    "state",
    "deck",
    "pytorch",
    "learnable",
    "parameters",
    "weights",
    "biases",
    "torch",
    "end",
    "module",
    "model",
    "remember",
    "model",
    "subclasses",
    "end",
    "module",
    "contained",
    "model",
    "parameters",
    "access",
    "state",
    "deck",
    "simply",
    "python",
    "dictionary",
    "object",
    "maps",
    "layer",
    "parameter",
    "tensor",
    "seen",
    "define",
    "model",
    "initialize",
    "model",
    "wanted",
    "print",
    "state",
    "decked",
    "use",
    "optimizer",
    "also",
    "state",
    "deck",
    "something",
    "aware",
    "go",
    "deck",
    "get",
    "output",
    "saving",
    "loading",
    "model",
    "inference",
    "inference",
    "making",
    "prediction",
    "probably",
    "want",
    "future",
    "point",
    "made",
    "predictions",
    "right",
    "within",
    "notebook",
    "wanted",
    "use",
    "model",
    "outside",
    "notebook",
    "say",
    "application",
    "another",
    "notebook",
    "one",
    "want",
    "know",
    "save",
    "load",
    "recommended",
    "way",
    "saving",
    "loading",
    "pytorch",
    "model",
    "saving",
    "state",
    "deck",
    "another",
    "method",
    "saving",
    "loading",
    "entire",
    "model",
    "extracurricular",
    "lesson",
    "going",
    "go",
    "code",
    "extracurricular",
    "read",
    "sections",
    "figure",
    "pros",
    "cons",
    "saving",
    "loading",
    "entire",
    "model",
    "versus",
    "saving",
    "loading",
    "state",
    "deck",
    "challenge",
    "video",
    "going",
    "link",
    "let",
    "write",
    "code",
    "save",
    "model",
    "pytorch",
    "save",
    "load",
    "code",
    "code",
    "tutorial",
    "plus",
    "extracurricular",
    "go",
    "saving",
    "pytorch",
    "model",
    "might",
    "want",
    "think",
    "save",
    "parameter",
    "takes",
    "think",
    "takes",
    "inside",
    "well",
    "let",
    "find",
    "together",
    "hey",
    "let",
    "import",
    "part",
    "lib",
    "going",
    "see",
    "second",
    "python",
    "module",
    "dealing",
    "writing",
    "file",
    "paths",
    "wanted",
    "save",
    "something",
    "google",
    "colab",
    "file",
    "section",
    "remember",
    "save",
    "within",
    "google",
    "colab",
    "model",
    "disappear",
    "google",
    "colab",
    "notebook",
    "instance",
    "disconnects",
    "show",
    "download",
    "google",
    "colab",
    "want",
    "google",
    "colab",
    "also",
    "way",
    "save",
    "google",
    "colab",
    "google",
    "colab",
    "google",
    "drive",
    "save",
    "google",
    "drive",
    "wanted",
    "leave",
    "look",
    "like",
    "first",
    "going",
    "create",
    "model",
    "directory",
    "create",
    "models",
    "directory",
    "going",
    "help",
    "us",
    "create",
    "folder",
    "called",
    "models",
    "course",
    "could",
    "create",
    "hand",
    "adding",
    "new",
    "folder",
    "somewhere",
    "like",
    "code",
    "model",
    "path",
    "going",
    "set",
    "path",
    "using",
    "path",
    "library",
    "create",
    "us",
    "path",
    "called",
    "models",
    "simple",
    "going",
    "save",
    "models",
    "models",
    "models",
    "file",
    "going",
    "create",
    "model",
    "path",
    "going",
    "make",
    "directory",
    "model",
    "path",
    "dot",
    "mkdir",
    "make",
    "directory",
    "going",
    "set",
    "parents",
    "equals",
    "true",
    "also",
    "going",
    "set",
    "exist",
    "okay",
    "equals",
    "true",
    "means",
    "already",
    "existed",
    "wo",
    "throw",
    "us",
    "error",
    "try",
    "create",
    "already",
    "exists",
    "recreate",
    "parents",
    "directory",
    "leave",
    "wo",
    "error",
    "us",
    "also",
    "going",
    "create",
    "model",
    "save",
    "path",
    "way",
    "give",
    "model",
    "name",
    "right",
    "model",
    "zero",
    "want",
    "save",
    "name",
    "models",
    "directory",
    "let",
    "create",
    "model",
    "name",
    "model",
    "name",
    "equals",
    "going",
    "call",
    "01",
    "section",
    "way",
    "models",
    "later",
    "course",
    "know",
    "ones",
    "come",
    "might",
    "create",
    "naming",
    "convention",
    "model",
    "workflow",
    "pytorch",
    "workflow",
    "model",
    "zero",
    "dot",
    "pth",
    "another",
    "important",
    "point",
    "pytorch",
    "objects",
    "usually",
    "extension",
    "dot",
    "pth",
    "pytorch",
    "dot",
    "pth",
    "go",
    "look",
    "dot",
    "pth",
    "yeah",
    "common",
    "convention",
    "save",
    "models",
    "using",
    "either",
    "dot",
    "pth",
    "dot",
    "pth",
    "file",
    "extension",
    "let",
    "choose",
    "one",
    "like",
    "like",
    "dot",
    "pth",
    "go",
    "dot",
    "pth",
    "result",
    "thing",
    "remember",
    "make",
    "sure",
    "write",
    "right",
    "loading",
    "path",
    "right",
    "saving",
    "path",
    "going",
    "create",
    "model",
    "save",
    "path",
    "going",
    "model",
    "path",
    "using",
    "path",
    "lib",
    "use",
    "syntax",
    "got",
    "model",
    "path",
    "slash",
    "model",
    "name",
    "print",
    "model",
    "save",
    "path",
    "look",
    "like",
    "go",
    "creates",
    "supposic",
    "path",
    "using",
    "path",
    "lib",
    "library",
    "models",
    "slash",
    "01",
    "pytorch",
    "workflow",
    "model",
    "zero",
    "dot",
    "pth",
    "saved",
    "model",
    "yet",
    "got",
    "path",
    "want",
    "save",
    "model",
    "ready",
    "refresh",
    "got",
    "models",
    "anything",
    "yet",
    "step",
    "save",
    "model",
    "three",
    "save",
    "model",
    "state",
    "dict",
    "saving",
    "state",
    "dict",
    "recommended",
    "way",
    "things",
    "come",
    "saving",
    "loading",
    "model",
    "inference",
    "save",
    "load",
    "state",
    "dict",
    "recommended",
    "could",
    "also",
    "save",
    "entire",
    "model",
    "part",
    "extra",
    "curriculum",
    "look",
    "let",
    "use",
    "syntax",
    "quite",
    "like",
    "torch",
    "dot",
    "save",
    "pass",
    "object",
    "pass",
    "path",
    "save",
    "already",
    "path",
    "good",
    "thing",
    "already",
    "model",
    "call",
    "let",
    "try",
    "let",
    "go",
    "print",
    "f",
    "saving",
    "model",
    "put",
    "path",
    "model",
    "save",
    "path",
    "like",
    "print",
    "things",
    "way",
    "know",
    "going",
    "need",
    "capital",
    "getting",
    "little",
    "bit",
    "trigger",
    "happy",
    "typing",
    "torch",
    "dot",
    "save",
    "going",
    "pass",
    "object",
    "parameter",
    "looked",
    "torch",
    "save",
    "go",
    "code",
    "take",
    "torch",
    "save",
    "object",
    "f",
    "file",
    "like",
    "object",
    "okay",
    "string",
    "os",
    "path",
    "like",
    "object",
    "beautiful",
    "got",
    "path",
    "like",
    "object",
    "containing",
    "file",
    "name",
    "let",
    "jump",
    "back",
    "object",
    "model",
    "zero",
    "dot",
    "state",
    "dict",
    "saving",
    "file",
    "path",
    "model",
    "save",
    "path",
    "ready",
    "let",
    "run",
    "see",
    "happens",
    "beautiful",
    "saving",
    "model",
    "models",
    "model",
    "path",
    "model",
    "refresh",
    "wonderful",
    "saved",
    "trained",
    "model",
    "means",
    "could",
    "potentially",
    "wanted",
    "could",
    "download",
    "file",
    "going",
    "download",
    "google",
    "colab",
    "local",
    "machine",
    "one",
    "way",
    "also",
    "guide",
    "save",
    "google",
    "collaboratory",
    "google",
    "drive",
    "way",
    "could",
    "use",
    "later",
    "many",
    "different",
    "ways",
    "beauty",
    "pie",
    "torches",
    "flexibility",
    "got",
    "saved",
    "model",
    "let",
    "check",
    "using",
    "ls",
    "command",
    "going",
    "check",
    "models",
    "yeah",
    "let",
    "check",
    "models",
    "going",
    "check",
    "list",
    "wonderful",
    "01",
    "pie",
    "torch",
    "workflow",
    "model",
    "zero",
    "dot",
    "pth",
    "course",
    "saved",
    "model",
    "try",
    "loading",
    "back",
    "seeing",
    "works",
    "want",
    "challenge",
    "read",
    "ahead",
    "documentation",
    "try",
    "use",
    "torch",
    "dot",
    "load",
    "bring",
    "model",
    "back",
    "see",
    "happens",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "wrote",
    "code",
    "save",
    "pie",
    "torch",
    "model",
    "going",
    "exit",
    "couple",
    "things",
    "need",
    "clear",
    "screen",
    "got",
    "dot",
    "pth",
    "file",
    "remember",
    "dot",
    "pth",
    "dot",
    "pth",
    "common",
    "convention",
    "saving",
    "pie",
    "torch",
    "model",
    "got",
    "saved",
    "necessarily",
    "write",
    "path",
    "style",
    "code",
    "handy",
    "later",
    "wanted",
    "functionize",
    "create",
    "say",
    "save",
    "dot",
    "pie",
    "file",
    "could",
    "call",
    "save",
    "function",
    "pass",
    "file",
    "path",
    "wanted",
    "save",
    "like",
    "directory",
    "name",
    "save",
    "exactly",
    "want",
    "later",
    "got",
    "saved",
    "model",
    "issued",
    "challenge",
    "trying",
    "load",
    "model",
    "torch",
    "dot",
    "load",
    "try",
    "got",
    "oh",
    "got",
    "options",
    "wonderful",
    "using",
    "one",
    "first",
    "ones",
    "let",
    "go",
    "back",
    "wanted",
    "check",
    "documentation",
    "torch",
    "dot",
    "load",
    "got",
    "option",
    "load",
    "happens",
    "loads",
    "objects",
    "saved",
    "torch",
    "dot",
    "save",
    "file",
    "torch",
    "dot",
    "load",
    "uses",
    "python",
    "unpickling",
    "facilities",
    "treat",
    "storages",
    "underlie",
    "tenses",
    "specially",
    "firstly",
    "serialized",
    "cpu",
    "moved",
    "device",
    "saved",
    "wonderful",
    "moved",
    "device",
    "later",
    "using",
    "gpu",
    "something",
    "keep",
    "mind",
    "see",
    "start",
    "use",
    "cpu",
    "gpu",
    "let",
    "practice",
    "using",
    "torch",
    "dot",
    "load",
    "method",
    "see",
    "come",
    "back",
    "go",
    "loading",
    "pytorch",
    "model",
    "since",
    "going",
    "start",
    "writing",
    "since",
    "saved",
    "models",
    "state",
    "debt",
    "dictionary",
    "parameters",
    "model",
    "rather",
    "entire",
    "model",
    "create",
    "new",
    "instance",
    "model",
    "class",
    "load",
    "state",
    "deck",
    "load",
    "saved",
    "state",
    "deck",
    "better",
    "state",
    "deck",
    "words",
    "page",
    "let",
    "see",
    "action",
    "load",
    "state",
    "deck",
    "say",
    "save",
    "entire",
    "model",
    "one",
    "option",
    "extra",
    "curriculum",
    "saved",
    "model",
    "state",
    "deck",
    "remind",
    "model",
    "zero",
    "dot",
    "state",
    "deck",
    "looks",
    "like",
    "saved",
    "load",
    "instantiate",
    "new",
    "class",
    "new",
    "instance",
    "linear",
    "regression",
    "model",
    "class",
    "load",
    "saved",
    "state",
    "deck",
    "instantiate",
    "new",
    "instance",
    "model",
    "class",
    "let",
    "call",
    "loaded",
    "model",
    "zero",
    "like",
    "way",
    "differentiate",
    "still",
    "going",
    "parameters",
    "model",
    "zero",
    "way",
    "know",
    "instance",
    "loaded",
    "version",
    "version",
    "training",
    "create",
    "new",
    "version",
    "linear",
    "regression",
    "model",
    "code",
    "wrote",
    "linear",
    "regression",
    "model",
    "going",
    "load",
    "saved",
    "state",
    "deck",
    "model",
    "zero",
    "update",
    "new",
    "instance",
    "updated",
    "parameters",
    "let",
    "check",
    "load",
    "written",
    "code",
    "actually",
    "load",
    "anything",
    "loaded",
    "model",
    "zero",
    "state",
    "deck",
    "look",
    "like",
    "wo",
    "anything",
    "initialized",
    "oh",
    "loaded",
    "called",
    "loaded",
    "see",
    "initialized",
    "random",
    "parameters",
    "essentially",
    "load",
    "state",
    "dictionary",
    "new",
    "instance",
    "model",
    "going",
    "hey",
    "take",
    "saved",
    "state",
    "deck",
    "model",
    "plug",
    "let",
    "see",
    "happens",
    "loaded",
    "model",
    "zero",
    "remember",
    "said",
    "method",
    "also",
    "aware",
    "torch",
    "nn",
    "module",
    "dot",
    "load",
    "state",
    "deck",
    "model",
    "subclass",
    "torch",
    "dot",
    "nn",
    "dot",
    "module",
    "call",
    "load",
    "state",
    "deck",
    "model",
    "directly",
    "instance",
    "recall",
    "linear",
    "regression",
    "model",
    "subclass",
    "nn",
    "dot",
    "module",
    "let",
    "call",
    "load",
    "state",
    "deck",
    "call",
    "torch",
    "dot",
    "load",
    "method",
    "pass",
    "model",
    "save",
    "path",
    "call",
    "torch",
    "dot",
    "load",
    "takes",
    "f",
    "file",
    "like",
    "object",
    "string",
    "os",
    "path",
    "like",
    "object",
    "created",
    "path",
    "like",
    "object",
    "model",
    "save",
    "path",
    "creating",
    "new",
    "instance",
    "linear",
    "regression",
    "model",
    "subclass",
    "nn",
    "dot",
    "module",
    "instance",
    "calling",
    "load",
    "state",
    "deck",
    "torch",
    "dot",
    "load",
    "model",
    "save",
    "path",
    "saved",
    "model",
    "save",
    "path",
    "previous",
    "models",
    "state",
    "deck",
    "run",
    "let",
    "see",
    "happens",
    "keys",
    "match",
    "successfully",
    "beautiful",
    "see",
    "values",
    "loaded",
    "state",
    "deck",
    "model",
    "zero",
    "well",
    "let",
    "check",
    "loaded",
    "version",
    "wonderful",
    "exact",
    "values",
    "little",
    "way",
    "test",
    "go",
    "make",
    "predictions",
    "make",
    "predictions",
    "make",
    "sure",
    "loaded",
    "model",
    "let",
    "put",
    "valve",
    "mode",
    "make",
    "predictions",
    "want",
    "evaluation",
    "mode",
    "goes",
    "little",
    "bit",
    "faster",
    "want",
    "also",
    "use",
    "inference",
    "mode",
    "torch",
    "dot",
    "inference",
    "mode",
    "making",
    "predictions",
    "want",
    "write",
    "loaded",
    "model",
    "preds",
    "going",
    "make",
    "predictions",
    "test",
    "data",
    "well",
    "loaded",
    "model",
    "zero",
    "going",
    "forward",
    "pass",
    "x",
    "test",
    "data",
    "look",
    "loaded",
    "model",
    "preds",
    "wonderful",
    "see",
    "two",
    "models",
    "compare",
    "loaded",
    "model",
    "preds",
    "original",
    "model",
    "preds",
    "preds",
    "equivalent",
    "equals",
    "equals",
    "loaded",
    "model",
    "preds",
    "thing",
    "false",
    "false",
    "false",
    "going",
    "preds",
    "much",
    "different",
    "oh",
    "happened",
    "made",
    "model",
    "preds",
    "yet",
    "make",
    "model",
    "preds",
    "troubleshooting",
    "fly",
    "team",
    "let",
    "go",
    "model",
    "zero",
    "dot",
    "eval",
    "torch",
    "dot",
    "inference",
    "mode",
    "check",
    "see",
    "two",
    "models",
    "actually",
    "equivalent",
    "preds",
    "equals",
    "feeling",
    "preds",
    "actually",
    "save",
    "somewhere",
    "else",
    "equals",
    "model",
    "zero",
    "pass",
    "x",
    "test",
    "data",
    "might",
    "move",
    "look",
    "preds",
    "equals",
    "get",
    "output",
    "yes",
    "wonderful",
    "okay",
    "beautiful",
    "covered",
    "saving",
    "loading",
    "models",
    "specifically",
    "saving",
    "models",
    "state",
    "deck",
    "saved",
    "code",
    "loaded",
    "back",
    "load",
    "state",
    "deck",
    "plus",
    "torch",
    "load",
    "checked",
    "see",
    "testing",
    "equivalents",
    "predictions",
    "models",
    "original",
    "one",
    "trained",
    "model",
    "zero",
    "loaded",
    "version",
    "saving",
    "loading",
    "model",
    "pytorch",
    "things",
    "could",
    "cover",
    "going",
    "leave",
    "extra",
    "curriculum",
    "covered",
    "two",
    "main",
    "things",
    "three",
    "main",
    "things",
    "one",
    "two",
    "three",
    "like",
    "read",
    "highly",
    "encourage",
    "go",
    "read",
    "tutorial",
    "said",
    "covered",
    "fair",
    "bit",
    "ground",
    "last",
    "videos",
    "videos",
    "put",
    "everything",
    "together",
    "reiterate",
    "done",
    "think",
    "good",
    "practice",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "past",
    "videos",
    "covered",
    "whole",
    "bunch",
    "ground",
    "pytorch",
    "workflow",
    "starting",
    "data",
    "building",
    "model",
    "well",
    "split",
    "data",
    "built",
    "model",
    "looked",
    "model",
    "building",
    "essentials",
    "checked",
    "contents",
    "model",
    "made",
    "predictions",
    "poor",
    "model",
    "based",
    "random",
    "numbers",
    "spent",
    "whole",
    "bunch",
    "time",
    "figuring",
    "could",
    "train",
    "model",
    "figured",
    "loss",
    "function",
    "saw",
    "optimizer",
    "wrote",
    "training",
    "test",
    "loop",
    "learned",
    "save",
    "load",
    "model",
    "pytorch",
    "like",
    "spend",
    "next",
    "videos",
    "putting",
    "together",
    "going",
    "spend",
    "much",
    "time",
    "step",
    "going",
    "practice",
    "together",
    "reiterate",
    "things",
    "done",
    "putting",
    "together",
    "let",
    "go",
    "back",
    "steps",
    "see",
    "one",
    "place",
    "wonderful",
    "going",
    "start",
    "go",
    "look",
    "workflow",
    "data",
    "going",
    "one",
    "step",
    "going",
    "get",
    "rid",
    "bit",
    "space",
    "got",
    "data",
    "ready",
    "turned",
    "tenses",
    "way",
    "back",
    "start",
    "built",
    "model",
    "picked",
    "loss",
    "function",
    "optimizer",
    "built",
    "training",
    "loop",
    "trained",
    "model",
    "made",
    "predictions",
    "saw",
    "better",
    "evaluated",
    "model",
    "use",
    "torch",
    "metrics",
    "got",
    "visual",
    "saw",
    "red",
    "dots",
    "starting",
    "line",
    "green",
    "dots",
    "really",
    "improved",
    "experimentation",
    "little",
    "bit",
    "though",
    "saw",
    "trained",
    "model",
    "epochs",
    "got",
    "better",
    "results",
    "could",
    "argue",
    "done",
    "little",
    "bit",
    "ways",
    "experiment",
    "going",
    "cover",
    "throughout",
    "course",
    "saw",
    "save",
    "reload",
    "trained",
    "model",
    "entire",
    "workflow",
    "quite",
    "exciting",
    "actually",
    "let",
    "go",
    "back",
    "going",
    "bit",
    "quicker",
    "done",
    "believe",
    "got",
    "skills",
    "let",
    "start",
    "importing",
    "pytorch",
    "could",
    "start",
    "code",
    "wanted",
    "plot",
    "live",
    "actually",
    "want",
    "pause",
    "video",
    "try",
    "recode",
    "steps",
    "done",
    "putting",
    "headers",
    "like",
    "data",
    "build",
    "model",
    "train",
    "model",
    "save",
    "load",
    "model",
    "whatever",
    "try",
    "code",
    "feel",
    "free",
    "follow",
    "along",
    "together",
    "import",
    "torch",
    "torch",
    "import",
    "oh",
    "would",
    "help",
    "could",
    "spell",
    "torch",
    "import",
    "n",
    "seen",
    "use",
    "n",
    "quite",
    "bit",
    "going",
    "also",
    "import",
    "map",
    "plot",
    "live",
    "like",
    "make",
    "plots",
    "like",
    "get",
    "visual",
    "visualize",
    "visualize",
    "visualize",
    "plt",
    "going",
    "check",
    "pytorch",
    "version",
    "way",
    "know",
    "older",
    "version",
    "code",
    "might",
    "work",
    "newer",
    "version",
    "work",
    "let",
    "know",
    "go",
    "using",
    "time",
    "watch",
    "video",
    "may",
    "later",
    "version",
    "also",
    "going",
    "let",
    "create",
    "device",
    "agnostic",
    "code",
    "create",
    "device",
    "agnostic",
    "code",
    "think",
    "step",
    "means",
    "got",
    "access",
    "gpu",
    "code",
    "use",
    "potentially",
    "faster",
    "computing",
    "gpu",
    "available",
    "code",
    "default",
    "using",
    "cpu",
    "necessarily",
    "need",
    "use",
    "gpu",
    "particular",
    "problem",
    "working",
    "right",
    "small",
    "model",
    "small",
    "data",
    "set",
    "good",
    "practice",
    "write",
    "device",
    "agnostic",
    "code",
    "means",
    "code",
    "use",
    "gpu",
    "available",
    "cpu",
    "default",
    "gpu",
    "available",
    "set",
    "device",
    "agnostic",
    "code",
    "going",
    "using",
    "similar",
    "setup",
    "throughout",
    "entire",
    "course",
    "bringing",
    "back",
    "cuda",
    "available",
    "remember",
    "cuda",
    "nvidia",
    "programming",
    "framework",
    "gpus",
    "else",
    "use",
    "cpu",
    "going",
    "print",
    "device",
    "using",
    "device",
    "might",
    "ran",
    "cpu",
    "right",
    "might",
    "different",
    "enabled",
    "gpu",
    "let",
    "change",
    "use",
    "cuda",
    "using",
    "google",
    "colab",
    "change",
    "runtime",
    "type",
    "selecting",
    "gpu",
    "going",
    "save",
    "going",
    "happen",
    "going",
    "restart",
    "runtime",
    "going",
    "lose",
    "code",
    "written",
    "get",
    "back",
    "well",
    "go",
    "run",
    "going",
    "run",
    "cells",
    "work",
    "quite",
    "quick",
    "model",
    "data",
    "big",
    "worked",
    "cuda",
    "device",
    "use",
    "wonderful",
    "beauty",
    "google",
    "colab",
    "given",
    "us",
    "access",
    "video",
    "gpu",
    "thank",
    "google",
    "colab",
    "paying",
    "paid",
    "version",
    "google",
    "colab",
    "free",
    "version",
    "give",
    "access",
    "gpu",
    "might",
    "later",
    "version",
    "gpu",
    "pro",
    "versions",
    "give",
    "access",
    "enough",
    "recreate",
    "feel",
    "like",
    "enough",
    "video",
    "got",
    "device",
    "agnostic",
    "code",
    "ready",
    "go",
    "next",
    "videos",
    "going",
    "rebuilding",
    "except",
    "using",
    "device",
    "agnostic",
    "code",
    "give",
    "shot",
    "nothing",
    "covered",
    "see",
    "next",
    "video",
    "let",
    "create",
    "data",
    "welcome",
    "back",
    "last",
    "video",
    "set",
    "device",
    "agnostic",
    "code",
    "got",
    "ready",
    "start",
    "putting",
    "everything",
    "learned",
    "together",
    "let",
    "continue",
    "going",
    "recreate",
    "data",
    "could",
    "copy",
    "code",
    "going",
    "write",
    "together",
    "practice",
    "creating",
    "dummy",
    "data",
    "set",
    "want",
    "get",
    "stage",
    "video",
    "want",
    "data",
    "plot",
    "build",
    "model",
    "learn",
    "blue",
    "dots",
    "predict",
    "green",
    "dots",
    "come",
    "data",
    "going",
    "get",
    "well",
    "bit",
    "room",
    "let",
    "create",
    "data",
    "using",
    "linear",
    "regression",
    "formula",
    "equals",
    "weight",
    "times",
    "features",
    "plus",
    "bias",
    "may",
    "heard",
    "equals",
    "mx",
    "plus",
    "c",
    "mx",
    "plus",
    "b",
    "something",
    "like",
    "substitute",
    "different",
    "names",
    "images",
    "learned",
    "high",
    "school",
    "equals",
    "mx",
    "plus",
    "might",
    "slightly",
    "different",
    "yeah",
    "bx",
    "plus",
    "use",
    "whole",
    "bunch",
    "different",
    "ways",
    "name",
    "things",
    "describing",
    "thing",
    "let",
    "see",
    "code",
    "rather",
    "formulaic",
    "examples",
    "going",
    "create",
    "weight",
    "bias",
    "values",
    "previously",
    "used",
    "challenge",
    "could",
    "change",
    "maybe",
    "could",
    "whatever",
    "values",
    "like",
    "set",
    "weight",
    "bias",
    "principle",
    "going",
    "thing",
    "going",
    "try",
    "build",
    "model",
    "estimate",
    "values",
    "going",
    "start",
    "0",
    "going",
    "end",
    "create",
    "straight",
    "line",
    "going",
    "fill",
    "0",
    "1",
    "step",
    "create",
    "x",
    "features",
    "x",
    "features",
    "labels",
    "actually",
    "x",
    "features",
    "labels",
    "x",
    "equals",
    "torch",
    "dot",
    "range",
    "x",
    "capital",
    "typically",
    "x",
    "feature",
    "matrix",
    "even",
    "though",
    "vector",
    "going",
    "unsqueeze",
    "run",
    "dimensionality",
    "issues",
    "later",
    "check",
    "without",
    "unsqueeze",
    "errors",
    "pop",
    "equals",
    "weight",
    "times",
    "x",
    "plus",
    "bias",
    "see",
    "going",
    "little",
    "bit",
    "faster",
    "sort",
    "pace",
    "going",
    "start",
    "going",
    "things",
    "already",
    "covered",
    "covered",
    "something",
    "slow",
    "covered",
    "something",
    "going",
    "step",
    "going",
    "start",
    "speeding",
    "things",
    "little",
    "get",
    "values",
    "wonderful",
    "got",
    "x",
    "values",
    "correlate",
    "values",
    "going",
    "try",
    "use",
    "training",
    "values",
    "x",
    "predict",
    "training",
    "values",
    "subsequently",
    "test",
    "values",
    "oh",
    "speaking",
    "training",
    "test",
    "values",
    "split",
    "data",
    "let",
    "split",
    "data",
    "split",
    "data",
    "create",
    "train",
    "split",
    "equals",
    "int",
    "going",
    "use",
    "80",
    "comes",
    "length",
    "use",
    "80",
    "samples",
    "training",
    "typical",
    "training",
    "test",
    "split",
    "80",
    "abouts",
    "could",
    "use",
    "like",
    "70",
    "could",
    "use",
    "90",
    "depends",
    "much",
    "data",
    "lot",
    "things",
    "machine",
    "learning",
    "quite",
    "flexible",
    "train",
    "split",
    "going",
    "index",
    "data",
    "create",
    "splits",
    "google",
    "colab",
    "auto",
    "corrected",
    "code",
    "way",
    "going",
    "opposite",
    "split",
    "testing",
    "data",
    "let",
    "look",
    "lengths",
    "calculations",
    "correct",
    "40",
    "training",
    "samples",
    "10",
    "testing",
    "samples",
    "may",
    "change",
    "future",
    "work",
    "larger",
    "data",
    "sets",
    "might",
    "training",
    "samples",
    "testing",
    "samples",
    "ratio",
    "often",
    "quite",
    "similar",
    "let",
    "plot",
    "going",
    "plot",
    "data",
    "note",
    "plot",
    "predictions",
    "function",
    "loaded",
    "error",
    "run",
    "plot",
    "predictions",
    "wanted",
    "pass",
    "x",
    "train",
    "train",
    "x",
    "test",
    "test",
    "come",
    "plot",
    "wonderful",
    "recreated",
    "data",
    "previously",
    "using",
    "got",
    "blue",
    "dots",
    "predict",
    "green",
    "dots",
    "function",
    "errors",
    "started",
    "notebook",
    "right",
    "cell",
    "gone",
    "remember",
    "go",
    "copy",
    "function",
    "run",
    "cells",
    "run",
    "cell",
    "previously",
    "could",
    "put",
    "run",
    "run",
    "get",
    "outcome",
    "wonderful",
    "next",
    "well",
    "go",
    "back",
    "workflow",
    "created",
    "data",
    "turned",
    "tenses",
    "yet",
    "think",
    "still",
    "oh",
    "yeah",
    "tenses",
    "use",
    "pytorch",
    "create",
    "building",
    "picking",
    "model",
    "built",
    "model",
    "previously",
    "back",
    "build",
    "model",
    "could",
    "refer",
    "code",
    "try",
    "build",
    "model",
    "fit",
    "data",
    "going",
    "challenge",
    "next",
    "video",
    "building",
    "pytorch",
    "linear",
    "model",
    "call",
    "linear",
    "linear",
    "refers",
    "straight",
    "line",
    "nonlinear",
    "see",
    "next",
    "video",
    "give",
    "shot",
    "get",
    "going",
    "build",
    "pytorch",
    "linear",
    "model",
    "welcome",
    "back",
    "going",
    "steps",
    "recreate",
    "everything",
    "done",
    "last",
    "video",
    "created",
    "dummy",
    "data",
    "got",
    "straight",
    "line",
    "workflow",
    "building",
    "model",
    "picking",
    "model",
    "case",
    "going",
    "build",
    "one",
    "suit",
    "problem",
    "got",
    "linear",
    "data",
    "put",
    "building",
    "pytorch",
    "linear",
    "model",
    "issued",
    "challenge",
    "giving",
    "go",
    "could",
    "exactly",
    "steps",
    "done",
    "build",
    "model",
    "going",
    "little",
    "bit",
    "cheeky",
    "introduce",
    "something",
    "new",
    "power",
    "let",
    "see",
    "going",
    "going",
    "create",
    "linear",
    "model",
    "lot",
    "pytorch",
    "models",
    "subclass",
    "module",
    "class",
    "linear",
    "regression",
    "call",
    "one",
    "linear",
    "regression",
    "model",
    "v2",
    "much",
    "similar",
    "code",
    "writing",
    "far",
    "first",
    "created",
    "linear",
    "regression",
    "model",
    "going",
    "put",
    "standard",
    "constructor",
    "code",
    "def",
    "init",
    "underscore",
    "underscore",
    "going",
    "take",
    "argument",
    "self",
    "going",
    "call",
    "super",
    "dot",
    "another",
    "underscore",
    "init",
    "underscore",
    "underscore",
    "brackets",
    "going",
    "instead",
    "recall",
    "back",
    "build",
    "model",
    "section",
    "initialized",
    "parameters",
    "hinting",
    "past",
    "videos",
    "seen",
    "oftentimes",
    "wo",
    "necessarily",
    "initialize",
    "parameters",
    "instead",
    "initialize",
    "layers",
    "parameters",
    "built",
    "layers",
    "still",
    "create",
    "forward",
    "method",
    "going",
    "see",
    "use",
    "torch",
    "linear",
    "layer",
    "steps",
    "us",
    "let",
    "write",
    "code",
    "step",
    "go",
    "building",
    "linear",
    "regression",
    "model",
    "data",
    "linear",
    "past",
    "previous",
    "model",
    "implemented",
    "linear",
    "regression",
    "formula",
    "creating",
    "model",
    "parameters",
    "go",
    "self",
    "dot",
    "linear",
    "layer",
    "equals",
    "constructing",
    "variable",
    "class",
    "use",
    "self",
    "linear",
    "layer",
    "equals",
    "nn",
    "dot",
    "linear",
    "remember",
    "nn",
    "pytorch",
    "stands",
    "neural",
    "network",
    "features",
    "one",
    "parameters",
    "features",
    "another",
    "parameter",
    "means",
    "want",
    "take",
    "input",
    "size",
    "one",
    "output",
    "size",
    "one",
    "come",
    "well",
    "look",
    "x",
    "train",
    "train",
    "one",
    "value",
    "maybe",
    "many",
    "x",
    "five",
    "first",
    "five",
    "five",
    "five",
    "recall",
    "one",
    "value",
    "x",
    "equates",
    "one",
    "value",
    "means",
    "within",
    "linear",
    "layer",
    "want",
    "take",
    "one",
    "feature",
    "x",
    "output",
    "one",
    "feature",
    "using",
    "one",
    "layer",
    "input",
    "output",
    "shapes",
    "model",
    "features",
    "features",
    "data",
    "goes",
    "data",
    "comes",
    "values",
    "highly",
    "dependent",
    "data",
    "working",
    "going",
    "see",
    "different",
    "data",
    "different",
    "examples",
    "input",
    "features",
    "output",
    "features",
    "throughout",
    "course",
    "happening",
    "one",
    "feature",
    "one",
    "feature",
    "happening",
    "inside",
    "let",
    "look",
    "torch",
    "linear",
    "go",
    "documentation",
    "applies",
    "linear",
    "transformation",
    "incoming",
    "data",
    "seen",
    "equals",
    "x",
    "plus",
    "using",
    "different",
    "letters",
    "got",
    "formula",
    "happening",
    "look",
    "formula",
    "data",
    "wait",
    "times",
    "x",
    "plus",
    "bias",
    "look",
    "linear",
    "regression",
    "formula",
    "linear",
    "regression",
    "formula",
    "got",
    "formula",
    "letters",
    "replaced",
    "whatever",
    "letters",
    "like",
    "linear",
    "layer",
    "implementing",
    "linear",
    "regression",
    "formula",
    "created",
    "model",
    "essentially",
    "part",
    "us",
    "behind",
    "scenes",
    "layer",
    "creates",
    "parameters",
    "us",
    "big",
    "piece",
    "puzzle",
    "pie",
    "torch",
    "said",
    "wo",
    "always",
    "initializing",
    "parameters",
    "model",
    "generally",
    "initialize",
    "layers",
    "use",
    "layers",
    "ford",
    "computation",
    "let",
    "see",
    "could",
    "got",
    "linear",
    "layer",
    "takes",
    "us",
    "features",
    "one",
    "features",
    "one",
    "well",
    "subclassed",
    "need",
    "override",
    "ford",
    "method",
    "need",
    "tell",
    "model",
    "ford",
    "computation",
    "going",
    "take",
    "input",
    "well",
    "x",
    "conventional",
    "input",
    "data",
    "going",
    "return",
    "self",
    "dot",
    "linear",
    "layer",
    "right",
    "actually",
    "might",
    "use",
    "typing",
    "say",
    "torch",
    "tensor",
    "also",
    "going",
    "return",
    "torch",
    "dot",
    "tensor",
    "using",
    "python",
    "type",
    "ins",
    "saying",
    "hey",
    "x",
    "torch",
    "tensor",
    "going",
    "return",
    "torch",
    "tensor",
    "going",
    "pass",
    "x",
    "linear",
    "layer",
    "expecting",
    "one",
    "feature",
    "one",
    "feature",
    "going",
    "linear",
    "transform",
    "another",
    "word",
    "pytorch",
    "machine",
    "learning",
    "general",
    "many",
    "different",
    "names",
    "thing",
    "would",
    "call",
    "linear",
    "layer",
    "going",
    "write",
    "also",
    "called",
    "linear",
    "transform",
    "probing",
    "layer",
    "fully",
    "connected",
    "layer",
    "dense",
    "layer",
    "intensive",
    "flow",
    "whole",
    "bunch",
    "different",
    "names",
    "thing",
    "implementing",
    "linear",
    "transform",
    "implementing",
    "version",
    "linear",
    "regression",
    "equals",
    "x",
    "ranspose",
    "plus",
    "b",
    "features",
    "features",
    "wonderful",
    "let",
    "see",
    "action",
    "going",
    "go",
    "set",
    "manual",
    "seed",
    "get",
    "reproducibility",
    "well",
    "torch",
    "dot",
    "manual",
    "seed",
    "going",
    "set",
    "model",
    "one",
    "equals",
    "linear",
    "regression",
    "model",
    "one",
    "already",
    "got",
    "model",
    "zero",
    "linear",
    "regression",
    "v",
    "two",
    "going",
    "check",
    "model",
    "one",
    "going",
    "check",
    "state",
    "dictionary",
    "state",
    "dict",
    "go",
    "inside",
    "ordered",
    "dict",
    "created",
    "anything",
    "us",
    "model",
    "one",
    "dot",
    "state",
    "dinked",
    "ordered",
    "dink",
    "got",
    "anything",
    "regression",
    "model",
    "v",
    "two",
    "ideally",
    "outputting",
    "weight",
    "bias",
    "yeah",
    "variables",
    "weight",
    "bias",
    "let",
    "dig",
    "code",
    "line",
    "line",
    "see",
    "got",
    "wrong",
    "ah",
    "notice",
    "init",
    "function",
    "constructor",
    "wrong",
    "amount",
    "underscores",
    "never",
    "actually",
    "constructing",
    "linear",
    "layer",
    "troubleshooting",
    "fly",
    "team",
    "go",
    "beautiful",
    "linear",
    "layer",
    "created",
    "us",
    "inside",
    "weight",
    "bias",
    "effectively",
    "replaced",
    "code",
    "wrote",
    "build",
    "model",
    "initializing",
    "weight",
    "bias",
    "parameter",
    "linear",
    "layer",
    "might",
    "wondering",
    "values",
    "slightly",
    "different",
    "even",
    "though",
    "used",
    "manual",
    "seed",
    "goes",
    "behind",
    "scenes",
    "pytorch",
    "creates",
    "different",
    "layers",
    "probably",
    "using",
    "different",
    "form",
    "randomness",
    "create",
    "different",
    "types",
    "variables",
    "keep",
    "mind",
    "see",
    "action",
    "conversion",
    "going",
    "converted",
    "original",
    "model",
    "class",
    "linear",
    "regression",
    "initialize",
    "model",
    "parameters",
    "got",
    "weight",
    "bias",
    "instead",
    "swapped",
    "linear",
    "regression",
    "model",
    "v2",
    "v2",
    "use",
    "linear",
    "layer",
    "forward",
    "method",
    "write",
    "formula",
    "manually",
    "initialize",
    "parameters",
    "manually",
    "power",
    "passed",
    "linear",
    "layer",
    "going",
    "perform",
    "predefined",
    "forward",
    "computation",
    "layer",
    "style",
    "going",
    "going",
    "see",
    "majority",
    "pytorch",
    "deep",
    "learning",
    "models",
    "created",
    "using",
    "layers",
    "module",
    "go",
    "back",
    "lot",
    "different",
    "layers",
    "convolutional",
    "layers",
    "pooling",
    "layers",
    "padding",
    "layers",
    "normalization",
    "recurrent",
    "transformer",
    "linear",
    "using",
    "linear",
    "layer",
    "dropout",
    "et",
    "cetera",
    "et",
    "cetera",
    "common",
    "layers",
    "deep",
    "learning",
    "neural",
    "networks",
    "layers",
    "different",
    "mathematical",
    "transformations",
    "pytorch",
    "lot",
    "implementations",
    "little",
    "bit",
    "sneaky",
    "trick",
    "done",
    "alter",
    "model",
    "still",
    "got",
    "basically",
    "exact",
    "model",
    "next",
    "well",
    "train",
    "model",
    "let",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "built",
    "pytorch",
    "linear",
    "model",
    "nice",
    "simple",
    "using",
    "single",
    "layer",
    "one",
    "feature",
    "one",
    "feature",
    "read",
    "forward",
    "method",
    "using",
    "linear",
    "layer",
    "created",
    "going",
    "happen",
    "forward",
    "parser",
    "model",
    "going",
    "put",
    "data",
    "going",
    "go",
    "linear",
    "layer",
    "behind",
    "scenes",
    "saw",
    "torch",
    "n",
    "linear",
    "behind",
    "scenes",
    "going",
    "perform",
    "linear",
    "regression",
    "formula",
    "equals",
    "x",
    "plus",
    "case",
    "got",
    "weight",
    "bias",
    "let",
    "go",
    "back",
    "time",
    "write",
    "training",
    "code",
    "let",
    "set",
    "model",
    "use",
    "target",
    "device",
    "case",
    "got",
    "device",
    "cuda",
    "written",
    "device",
    "agnostic",
    "code",
    "access",
    "cuda",
    "device",
    "gpu",
    "default",
    "device",
    "would",
    "cpu",
    "let",
    "check",
    "model",
    "device",
    "first",
    "check",
    "model",
    "current",
    "device",
    "going",
    "use",
    "gpu",
    "going",
    "write",
    "device",
    "agnostic",
    "code",
    "better",
    "say",
    "device",
    "agnostic",
    "code",
    "proper",
    "terminology",
    "device",
    "device",
    "currently",
    "using",
    "cpu",
    "right",
    "default",
    "model",
    "end",
    "cpu",
    "set",
    "model",
    "one",
    "call",
    "dot",
    "two",
    "device",
    "think",
    "going",
    "current",
    "target",
    "device",
    "cuda",
    "seen",
    "two",
    "fundamental",
    "section",
    "two",
    "going",
    "send",
    "model",
    "gpu",
    "memory",
    "let",
    "check",
    "whether",
    "parameters",
    "model",
    "live",
    "dot",
    "device",
    "send",
    "device",
    "previously",
    "cpu",
    "going",
    "take",
    "little",
    "bit",
    "longer",
    "gpu",
    "gets",
    "fired",
    "goes",
    "pytorch",
    "goes",
    "hey",
    "send",
    "model",
    "ready",
    "boom",
    "go",
    "wonderful",
    "model",
    "device",
    "target",
    "device",
    "cuda",
    "cuda",
    "available",
    "target",
    "device",
    "would",
    "cpu",
    "would",
    "come",
    "exactly",
    "got",
    "said",
    "let",
    "get",
    "training",
    "code",
    "fun",
    "part",
    "already",
    "seen",
    "training",
    "going",
    "clear",
    "workspace",
    "little",
    "bit",
    "training",
    "need",
    "part",
    "pytorch",
    "workflow",
    "need",
    "loss",
    "function",
    "loss",
    "function",
    "measures",
    "wrong",
    "model",
    "need",
    "optimizer",
    "need",
    "training",
    "loop",
    "testing",
    "loop",
    "optimizer",
    "well",
    "optimizes",
    "parameters",
    "model",
    "case",
    "model",
    "one",
    "dot",
    "state",
    "dig",
    "parameters",
    "within",
    "linear",
    "layer",
    "weight",
    "bias",
    "optimizer",
    "going",
    "optimize",
    "random",
    "parameters",
    "hopefully",
    "reduce",
    "loss",
    "function",
    "remember",
    "loss",
    "function",
    "measures",
    "wrong",
    "model",
    "case",
    "working",
    "regression",
    "problem",
    "let",
    "set",
    "loss",
    "function",
    "way",
    "steps",
    "part",
    "workflow",
    "got",
    "data",
    "ready",
    "built",
    "picked",
    "model",
    "using",
    "linear",
    "model",
    "pick",
    "loss",
    "function",
    "optimizer",
    "going",
    "build",
    "training",
    "loop",
    "session",
    "know",
    "getting",
    "pretty",
    "darn",
    "good",
    "loss",
    "function",
    "equals",
    "well",
    "going",
    "use",
    "l",
    "one",
    "loss",
    "let",
    "set",
    "dot",
    "l",
    "one",
    "loss",
    "wanted",
    "set",
    "optimizer",
    "optimizer",
    "could",
    "use",
    "well",
    "pytorch",
    "offers",
    "lot",
    "optimizers",
    "torch",
    "dot",
    "opt",
    "sgd",
    "stochastic",
    "gradient",
    "descent",
    "remember",
    "gradient",
    "descent",
    "algorithm",
    "optimizes",
    "model",
    "parameters",
    "adam",
    "another",
    "popular",
    "option",
    "going",
    "stick",
    "sgd",
    "lr",
    "stands",
    "learning",
    "rate",
    "words",
    "big",
    "step",
    "optimizer",
    "change",
    "parameters",
    "every",
    "iteration",
    "smaller",
    "learning",
    "rate",
    "0001",
    "small",
    "step",
    "large",
    "learning",
    "rate",
    "larger",
    "step",
    "big",
    "step",
    "model",
    "learns",
    "much",
    "explodes",
    "small",
    "step",
    "model",
    "never",
    "learns",
    "anything",
    "oh",
    "actually",
    "pass",
    "params",
    "first",
    "forgot",
    "got",
    "ahead",
    "learning",
    "rate",
    "params",
    "parameters",
    "like",
    "optimizer",
    "optimize",
    "case",
    "model",
    "one",
    "dot",
    "parameters",
    "model",
    "one",
    "current",
    "target",
    "model",
    "beautiful",
    "got",
    "loss",
    "function",
    "optimizer",
    "let",
    "write",
    "training",
    "loop",
    "going",
    "set",
    "torch",
    "manual",
    "seeds",
    "try",
    "get",
    "reproducible",
    "results",
    "possible",
    "remember",
    "get",
    "different",
    "numbers",
    "getting",
    "worry",
    "much",
    "exactly",
    "direction",
    "important",
    "means",
    "loss",
    "function",
    "getting",
    "smaller",
    "getting",
    "smaller",
    "worry",
    "much",
    "fourth",
    "decimal",
    "place",
    "values",
    "training",
    "loop",
    "ready",
    "written",
    "epox",
    "many",
    "well",
    "200",
    "last",
    "time",
    "worked",
    "pretty",
    "well",
    "let",
    "200",
    "go",
    "extra",
    "curriculum",
    "yet",
    "watch",
    "video",
    "unofficial",
    "pytorch",
    "optimization",
    "loop",
    "song",
    "yet",
    "one",
    "listen",
    "unofficial",
    "pytorch",
    "optimization",
    "loop",
    "song",
    "okay",
    "let",
    "sing",
    "together",
    "epoch",
    "range",
    "epochs",
    "going",
    "go",
    "song",
    "second",
    "going",
    "set",
    "model",
    "train",
    "case",
    "model",
    "one",
    "model",
    "train",
    "step",
    "number",
    "one",
    "forward",
    "pass",
    "calculate",
    "predictions",
    "calculate",
    "predictions",
    "passing",
    "training",
    "data",
    "model",
    "case",
    "forward",
    "method",
    "model",
    "one",
    "implements",
    "linear",
    "layer",
    "data",
    "going",
    "go",
    "linear",
    "layer",
    "go",
    "linear",
    "regression",
    "formula",
    "calculate",
    "loss",
    "wrong",
    "models",
    "predictions",
    "loss",
    "value",
    "equals",
    "loss",
    "fn",
    "going",
    "pass",
    "zero",
    "optimizer",
    "optimizer",
    "zero",
    "grad",
    "default",
    "optimizer",
    "going",
    "accumulate",
    "gradients",
    "behind",
    "scenes",
    "every",
    "epoch",
    "want",
    "reduce",
    "back",
    "zero",
    "starts",
    "fresh",
    "going",
    "perform",
    "back",
    "propagation",
    "back",
    "propagation",
    "calling",
    "loss",
    "stop",
    "backwards",
    "forward",
    "pass",
    "goes",
    "forward",
    "network",
    "backward",
    "pass",
    "goes",
    "backwards",
    "network",
    "calculating",
    "gradients",
    "loss",
    "function",
    "respect",
    "parameter",
    "model",
    "optimizer",
    "step",
    "next",
    "part",
    "going",
    "look",
    "gradients",
    "go",
    "know",
    "way",
    "optimize",
    "parameters",
    "optimizer",
    "optimizing",
    "model",
    "parameters",
    "going",
    "look",
    "loss",
    "go",
    "know",
    "going",
    "adjust",
    "weight",
    "increased",
    "going",
    "lower",
    "bias",
    "see",
    "reduces",
    "loss",
    "testing",
    "hit",
    "moving",
    "quite",
    "fast",
    "spent",
    "whole",
    "bunch",
    "time",
    "discussing",
    "going",
    "testing",
    "set",
    "model",
    "evaluation",
    "mode",
    "going",
    "turn",
    "things",
    "like",
    "dropout",
    "batch",
    "normalization",
    "layers",
    "model",
    "good",
    "practice",
    "always",
    "call",
    "vowel",
    "whenever",
    "testing",
    "inference",
    "mode",
    "need",
    "track",
    "gradients",
    "whole",
    "bunch",
    "things",
    "pytorch",
    "behind",
    "scenes",
    "testing",
    "making",
    "predictions",
    "use",
    "inference",
    "mode",
    "context",
    "manager",
    "going",
    "create",
    "test",
    "pred",
    "going",
    "test",
    "predictions",
    "going",
    "pass",
    "test",
    "data",
    "features",
    "forward",
    "pass",
    "model",
    "calculate",
    "test",
    "loss",
    "loss",
    "function",
    "going",
    "compare",
    "test",
    "pred",
    "test",
    "wonderful",
    "print",
    "happening",
    "print",
    "epoch",
    "divided",
    "10",
    "equals",
    "zero",
    "every",
    "10",
    "epochs",
    "let",
    "print",
    "something",
    "print",
    "f",
    "string",
    "epoch",
    "epoch",
    "go",
    "loss",
    "training",
    "loss",
    "equal",
    "loss",
    "go",
    "test",
    "loss",
    "equal",
    "test",
    "loss",
    "think",
    "work",
    "okay",
    "sure",
    "let",
    "find",
    "together",
    "hey",
    "oh",
    "got",
    "need",
    "bracket",
    "oh",
    "goodness",
    "going",
    "run",
    "time",
    "error",
    "expected",
    "tenses",
    "device",
    "oh",
    "course",
    "know",
    "happening",
    "found",
    "least",
    "two",
    "devices",
    "cuda",
    "cpu",
    "yes",
    "course",
    "happened",
    "done",
    "put",
    "model",
    "gpu",
    "going",
    "data",
    "data",
    "gpu",
    "default",
    "cpu",
    "written",
    "device",
    "agnostic",
    "code",
    "data",
    "let",
    "write",
    "put",
    "data",
    "target",
    "device",
    "device",
    "agnostic",
    "code",
    "data",
    "remember",
    "one",
    "biggest",
    "issues",
    "pytorch",
    "aside",
    "shape",
    "errors",
    "data",
    "things",
    "computing",
    "device",
    "set",
    "device",
    "agnostic",
    "code",
    "model",
    "data",
    "let",
    "put",
    "x",
    "train",
    "device",
    "train",
    "equals",
    "train",
    "device",
    "going",
    "create",
    "device",
    "agnostic",
    "code",
    "case",
    "going",
    "use",
    "cuda",
    "access",
    "cuda",
    "device",
    "code",
    "still",
    "work",
    "still",
    "default",
    "cpu",
    "good",
    "like",
    "got",
    "error",
    "sum",
    "things",
    "going",
    "come",
    "across",
    "practice",
    "right",
    "let",
    "run",
    "happening",
    "hey",
    "look",
    "wonderful",
    "loss",
    "starts",
    "nice",
    "high",
    "starts",
    "go",
    "right",
    "training",
    "data",
    "testing",
    "data",
    "beautiful",
    "right",
    "way",
    "okay",
    "looks",
    "pretty",
    "good",
    "test",
    "data",
    "set",
    "check",
    "evaluate",
    "model",
    "well",
    "one",
    "way",
    "check",
    "state",
    "deck",
    "state",
    "decked",
    "got",
    "weight",
    "bias",
    "oh",
    "gosh",
    "close",
    "set",
    "weight",
    "bias",
    "model",
    "estimated",
    "parameters",
    "based",
    "training",
    "data",
    "pretty",
    "close",
    "nearly",
    "perfect",
    "thing",
    "bias",
    "versus",
    "perfect",
    "value",
    "remember",
    "practice",
    "wo",
    "necessarily",
    "know",
    "ideal",
    "parameters",
    "exemplify",
    "model",
    "behind",
    "scenes",
    "moving",
    "towards",
    "ideal",
    "representative",
    "parameters",
    "whatever",
    "data",
    "working",
    "next",
    "video",
    "like",
    "give",
    "go",
    "get",
    "next",
    "video",
    "make",
    "predictions",
    "model",
    "plot",
    "original",
    "data",
    "close",
    "green",
    "dots",
    "match",
    "red",
    "dots",
    "use",
    "plot",
    "predictions",
    "formula",
    "function",
    "using",
    "past",
    "give",
    "go",
    "see",
    "next",
    "video",
    "congratulations",
    "look",
    "quickly",
    "trained",
    "model",
    "using",
    "steps",
    "covered",
    "bunch",
    "videos",
    "far",
    "device",
    "agnostic",
    "code",
    "good",
    "see",
    "soon",
    "last",
    "video",
    "something",
    "exciting",
    "worked",
    "training",
    "entire",
    "neural",
    "network",
    "steps",
    "took",
    "us",
    "hour",
    "worth",
    "videos",
    "go",
    "back",
    "coded",
    "one",
    "video",
    "ready",
    "listening",
    "song",
    "remind",
    "going",
    "epoch",
    "range",
    "call",
    "model",
    "dot",
    "train",
    "forward",
    "pass",
    "calculate",
    "loss",
    "optimizer",
    "zero",
    "grad",
    "loss",
    "backward",
    "optimizer",
    "step",
    "step",
    "step",
    "let",
    "test",
    "come",
    "dot",
    "eval",
    "torch",
    "inference",
    "mode",
    "forward",
    "pass",
    "calculate",
    "loss",
    "print",
    "happening",
    "another",
    "epoch",
    "range",
    "kidding",
    "leave",
    "leave",
    "unofficial",
    "pytorch",
    "optimization",
    "loop",
    "song",
    "created",
    "device",
    "agnostic",
    "code",
    "could",
    "make",
    "calculations",
    "device",
    "model",
    "models",
    "also",
    "using",
    "device",
    "agnostic",
    "code",
    "got",
    "evaluate",
    "models",
    "looked",
    "loss",
    "test",
    "lost",
    "know",
    "models",
    "loss",
    "going",
    "actually",
    "equate",
    "makes",
    "predictions",
    "interested",
    "right",
    "looked",
    "parameters",
    "pretty",
    "close",
    "ideal",
    "parameters",
    "end",
    "last",
    "video",
    "issued",
    "challenge",
    "making",
    "evaluating",
    "predictions",
    "make",
    "predictions",
    "plot",
    "hope",
    "gave",
    "shot",
    "let",
    "see",
    "looks",
    "like",
    "together",
    "hey",
    "turn",
    "model",
    "evaluation",
    "mode",
    "every",
    "time",
    "making",
    "predictions",
    "inference",
    "want",
    "model",
    "vowel",
    "mode",
    "every",
    "time",
    "training",
    "want",
    "model",
    "training",
    "mode",
    "going",
    "make",
    "predictions",
    "test",
    "data",
    "train",
    "train",
    "data",
    "evaluate",
    "model",
    "test",
    "data",
    "data",
    "model",
    "never",
    "actually",
    "seen",
    "except",
    "makes",
    "predictions",
    "torch",
    "inference",
    "mode",
    "turn",
    "inference",
    "mode",
    "whenever",
    "make",
    "inference",
    "predictions",
    "going",
    "set",
    "threads",
    "equal",
    "model",
    "one",
    "test",
    "data",
    "goes",
    "let",
    "look",
    "threads",
    "look",
    "like",
    "wonderful",
    "got",
    "tensor",
    "shows",
    "us",
    "still",
    "device",
    "cuda",
    "well",
    "previously",
    "set",
    "model",
    "one",
    "device",
    "target",
    "device",
    "test",
    "data",
    "subsequently",
    "predictions",
    "also",
    "cuda",
    "device",
    "let",
    "bring",
    "plot",
    "predictions",
    "function",
    "check",
    "model",
    "predictions",
    "visually",
    "going",
    "adhere",
    "data",
    "explorer",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "plot",
    "predictions",
    "predictions",
    "going",
    "set",
    "equals",
    "threads",
    "let",
    "look",
    "good",
    "look",
    "oh",
    "oh",
    "got",
    "another",
    "error",
    "type",
    "error",
    "ca",
    "convert",
    "cuda",
    "device",
    "type",
    "tensor",
    "numpy",
    "oh",
    "course",
    "look",
    "done",
    "plot",
    "predictions",
    "function",
    "go",
    "back",
    "define",
    "plot",
    "predictions",
    "function",
    "use",
    "uses",
    "matplotlib",
    "course",
    "matplotlib",
    "works",
    "numpy",
    "pytorch",
    "numpy",
    "cpu",
    "based",
    "course",
    "running",
    "another",
    "error",
    "said",
    "predictions",
    "cuda",
    "device",
    "cpu",
    "gpu",
    "giving",
    "us",
    "helpful",
    "information",
    "use",
    "tensor",
    "dot",
    "cpu",
    "copy",
    "tensor",
    "host",
    "memory",
    "first",
    "tensor",
    "let",
    "call",
    "dot",
    "cpu",
    "see",
    "happens",
    "going",
    "go",
    "cpu",
    "oh",
    "goodness",
    "look",
    "look",
    "go",
    "linear",
    "layer",
    "red",
    "dots",
    "predictions",
    "basically",
    "top",
    "testing",
    "data",
    "exciting",
    "may",
    "get",
    "exact",
    "numbers",
    "perfectly",
    "fine",
    "direction",
    "quite",
    "similar",
    "red",
    "dots",
    "basically",
    "top",
    "green",
    "dots",
    "slightly",
    "okay",
    "okay",
    "want",
    "focus",
    "direction",
    "thanks",
    "power",
    "back",
    "propagation",
    "gradient",
    "descent",
    "models",
    "random",
    "parameters",
    "updated",
    "close",
    "possible",
    "ideal",
    "parameters",
    "predictions",
    "looking",
    "pretty",
    "darn",
    "good",
    "trying",
    "predict",
    "finished",
    "finished",
    "training",
    "model",
    "would",
    "happen",
    "notebook",
    "disconnected",
    "right",
    "well",
    "would",
    "ideal",
    "would",
    "next",
    "part",
    "going",
    "move",
    "saving",
    "loading",
    "trained",
    "model",
    "going",
    "give",
    "challenge",
    "well",
    "go",
    "ahead",
    "go",
    "back",
    "refer",
    "code",
    "saving",
    "model",
    "pytorch",
    "loading",
    "pytorch",
    "model",
    "see",
    "save",
    "model",
    "one",
    "state",
    "dictionary",
    "model",
    "one",
    "load",
    "back",
    "get",
    "something",
    "similar",
    "give",
    "shot",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "saw",
    "power",
    "layer",
    "back",
    "propagation",
    "gradient",
    "descent",
    "got",
    "pretty",
    "darn",
    "good",
    "predictions",
    "model",
    "exciting",
    "congratulations",
    "trained",
    "two",
    "machine",
    "learning",
    "models",
    "yet",
    "got",
    "save",
    "load",
    "trained",
    "model",
    "issued",
    "challenge",
    "last",
    "video",
    "try",
    "save",
    "load",
    "model",
    "hope",
    "gave",
    "go",
    "going",
    "together",
    "video",
    "going",
    "start",
    "importing",
    "path",
    "would",
    "like",
    "file",
    "path",
    "save",
    "model",
    "first",
    "step",
    "going",
    "create",
    "models",
    "directory",
    "recreate",
    "believe",
    "already",
    "one",
    "going",
    "put",
    "code",
    "completeness",
    "models",
    "directory",
    "would",
    "create",
    "one",
    "model",
    "path",
    "going",
    "go",
    "path",
    "models",
    "like",
    "model",
    "path",
    "dot",
    "maker",
    "going",
    "call",
    "maker",
    "make",
    "directory",
    "set",
    "parents",
    "equal",
    "true",
    "exists",
    "okay",
    "also",
    "true",
    "wo",
    "get",
    "error",
    "oh",
    "gosh",
    "google",
    "collab",
    "want",
    "wo",
    "get",
    "error",
    "already",
    "exists",
    "two",
    "going",
    "create",
    "model",
    "save",
    "path",
    "recall",
    "pytorch",
    "objects",
    "general",
    "extension",
    "little",
    "pop",
    "quiz",
    "get",
    "end",
    "sentence",
    "going",
    "pytorch",
    "workflow",
    "module",
    "going",
    "one",
    "chapter",
    "01",
    "pytorch",
    "workflow",
    "model",
    "one",
    "usually",
    "extension",
    "dot",
    "pt",
    "pytorch",
    "pt",
    "h",
    "pytorch",
    "well",
    "like",
    "pt",
    "remember",
    "sometimes",
    "might",
    "come",
    "across",
    "slightly",
    "different",
    "versions",
    "pt",
    "pt",
    "going",
    "create",
    "model",
    "save",
    "name",
    "save",
    "path",
    "probably",
    "better",
    "way",
    "going",
    "model",
    "path",
    "use",
    "using",
    "path",
    "lib",
    "module",
    "python",
    "save",
    "model",
    "name",
    "look",
    "get",
    "model",
    "save",
    "path",
    "get",
    "oh",
    "path",
    "defined",
    "oh",
    "many",
    "capitals",
    "daniel",
    "reason",
    "capitals",
    "oftentimes",
    "hyper",
    "parameters",
    "epochs",
    "machine",
    "learning",
    "set",
    "hyper",
    "parameters",
    "lr",
    "could",
    "learning",
    "rate",
    "could",
    "well",
    "model",
    "name",
    "equals",
    "yeah",
    "yeah",
    "yeah",
    "little",
    "bit",
    "nomenclature",
    "trivia",
    "later",
    "model",
    "save",
    "path",
    "done",
    "going",
    "save",
    "model",
    "state",
    "dictionary",
    "rather",
    "whole",
    "model",
    "save",
    "model",
    "state",
    "deck",
    "find",
    "pros",
    "cons",
    "pytorch",
    "documentation",
    "saving",
    "loading",
    "model",
    "little",
    "bit",
    "extra",
    "curriculum",
    "previous",
    "video",
    "let",
    "look",
    "model",
    "save",
    "path",
    "print",
    "go",
    "torch",
    "save",
    "set",
    "object",
    "trying",
    "save",
    "equal",
    "model",
    "one",
    "dot",
    "state",
    "deck",
    "going",
    "contain",
    "trained",
    "model",
    "parameters",
    "inspect",
    "going",
    "state",
    "deck",
    "show",
    "us",
    "model",
    "parameters",
    "remember",
    "using",
    "single",
    "linear",
    "layer",
    "two",
    "parameters",
    "practice",
    "use",
    "model",
    "maybe",
    "hundreds",
    "layers",
    "tens",
    "millions",
    "parameters",
    "viewing",
    "state",
    "deck",
    "explicitly",
    "like",
    "might",
    "viable",
    "option",
    "principle",
    "still",
    "remains",
    "state",
    "deck",
    "contains",
    "models",
    "trained",
    "associated",
    "parameters",
    "state",
    "file",
    "path",
    "going",
    "use",
    "course",
    "model",
    "save",
    "path",
    "seen",
    "posix",
    "path",
    "let",
    "save",
    "model",
    "wonderful",
    "saving",
    "model",
    "file",
    "path",
    "look",
    "folder",
    "two",
    "saved",
    "models",
    "beautiful",
    "save",
    "models",
    "one",
    "us",
    "workflow",
    "saving",
    "model",
    "pytorch",
    "loading",
    "pytorch",
    "model",
    "one",
    "got",
    "course",
    "model",
    "one",
    "one",
    "saved",
    "beautiful",
    "let",
    "load",
    "model",
    "going",
    "one",
    "video",
    "load",
    "pytorch",
    "model",
    "know",
    "little",
    "bit",
    "practice",
    "far",
    "going",
    "pick",
    "pace",
    "let",
    "go",
    "loaded",
    "let",
    "call",
    "create",
    "new",
    "instance",
    "loaded",
    "model",
    "one",
    "course",
    "linear",
    "regression",
    "model",
    "v2",
    "version",
    "two",
    "linear",
    "regression",
    "model",
    "class",
    "subclasses",
    "subclasses",
    "go",
    "back",
    "created",
    "linear",
    "regression",
    "model",
    "v2",
    "uses",
    "linear",
    "layer",
    "rather",
    "previous",
    "iteration",
    "linear",
    "regression",
    "model",
    "created",
    "right",
    "go",
    "explicitly",
    "defined",
    "parameters",
    "implemented",
    "linear",
    "regression",
    "formula",
    "forward",
    "method",
    "difference",
    "got",
    "use",
    "pytorch",
    "linear",
    "layer",
    "call",
    "linear",
    "layer",
    "forward",
    "method",
    "probably",
    "far",
    "popular",
    "way",
    "building",
    "pytorch",
    "models",
    "stacking",
    "together",
    "nn",
    "layers",
    "calling",
    "way",
    "forward",
    "method",
    "let",
    "load",
    "create",
    "new",
    "instance",
    "linear",
    "regression",
    "model",
    "v2",
    "created",
    "new",
    "instance",
    "going",
    "get",
    "make",
    "space",
    "us",
    "want",
    "load",
    "model",
    "state",
    "deck",
    "saved",
    "model",
    "one",
    "state",
    "deck",
    "state",
    "deck",
    "saved",
    "beforehand",
    "going",
    "loaded",
    "model",
    "one",
    "calling",
    "load",
    "state",
    "decked",
    "method",
    "passing",
    "torch",
    "dot",
    "load",
    "file",
    "path",
    "saved",
    "pytorch",
    "object",
    "reason",
    "use",
    "path",
    "lib",
    "call",
    "model",
    "save",
    "path",
    "wonderful",
    "let",
    "check",
    "going",
    "actually",
    "need",
    "put",
    "target",
    "model",
    "loaded",
    "model",
    "device",
    "reason",
    "computing",
    "device",
    "agnostic",
    "code",
    "let",
    "send",
    "device",
    "think",
    "let",
    "see",
    "works",
    "oh",
    "go",
    "linear",
    "regression",
    "model",
    "v2",
    "features",
    "one",
    "features",
    "one",
    "bias",
    "equals",
    "true",
    "wonderful",
    "let",
    "check",
    "parameters",
    "hey",
    "next",
    "loaded",
    "model",
    "one",
    "dot",
    "parameters",
    "right",
    "device",
    "let",
    "look",
    "beautiful",
    "let",
    "check",
    "loaded",
    "state",
    "dictionary",
    "loaded",
    "model",
    "one",
    "values",
    "previously",
    "yes",
    "okay",
    "conclusively",
    "make",
    "sure",
    "going",
    "let",
    "evaluate",
    "loaded",
    "model",
    "evaluate",
    "loaded",
    "model",
    "loaded",
    "model",
    "one",
    "making",
    "predictions",
    "evaluate",
    "call",
    "dot",
    "vowel",
    "going",
    "make",
    "predictions",
    "use",
    "torch",
    "inference",
    "mode",
    "torch",
    "inference",
    "mode",
    "let",
    "create",
    "loaded",
    "model",
    "one",
    "threads",
    "equals",
    "loaded",
    "model",
    "one",
    "pass",
    "test",
    "data",
    "let",
    "check",
    "quality",
    "threads",
    "previous",
    "model",
    "one",
    "preds",
    "made",
    "threads",
    "going",
    "compare",
    "fresh",
    "loaded",
    "model",
    "one",
    "preds",
    "yes",
    "beautiful",
    "see",
    "device",
    "cuda",
    "amazing",
    "want",
    "give",
    "big",
    "congratulations",
    "come",
    "long",
    "way",
    "gone",
    "entire",
    "pytorch",
    "workflow",
    "making",
    "data",
    "preparing",
    "loading",
    "building",
    "model",
    "steps",
    "come",
    "building",
    "model",
    "whole",
    "bunch",
    "making",
    "predictions",
    "training",
    "model",
    "spent",
    "lot",
    "time",
    "going",
    "training",
    "steps",
    "trust",
    "worth",
    "going",
    "using",
    "exact",
    "steps",
    "throughout",
    "course",
    "fact",
    "going",
    "using",
    "exact",
    "steps",
    "build",
    "pytorch",
    "models",
    "course",
    "looked",
    "save",
    "model",
    "lose",
    "work",
    "looked",
    "loading",
    "model",
    "put",
    "together",
    "using",
    "exact",
    "problem",
    "far",
    "less",
    "time",
    "see",
    "later",
    "actually",
    "make",
    "even",
    "quicker",
    "functionalizing",
    "code",
    "already",
    "written",
    "going",
    "save",
    "later",
    "see",
    "next",
    "video",
    "going",
    "show",
    "find",
    "exercises",
    "extra",
    "curriculum",
    "talking",
    "throughout",
    "section",
    "01",
    "pytorch",
    "workflow",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "finished",
    "putting",
    "things",
    "together",
    "saving",
    "loading",
    "trained",
    "model",
    "super",
    "exciting",
    "let",
    "come",
    "end",
    "pytorch",
    "workflow",
    "section",
    "section",
    "going",
    "exercises",
    "extra",
    "curriculum",
    "better",
    "yet",
    "find",
    "going",
    "turn",
    "markdown",
    "going",
    "write",
    "exercises",
    "extra",
    "curriculum",
    "refer",
    "within",
    "book",
    "version",
    "course",
    "materials",
    "01",
    "section",
    "pytorch",
    "workflow",
    "fundamentals",
    "time",
    "watch",
    "video",
    "likely",
    "go",
    "end",
    "sections",
    "got",
    "table",
    "contents",
    "got",
    "exercises",
    "extra",
    "curriculum",
    "listed",
    "bunch",
    "things",
    "throughout",
    "series",
    "01",
    "videos",
    "like",
    "gradient",
    "descent",
    "back",
    "propagation",
    "got",
    "plenty",
    "resources",
    "learn",
    "loading",
    "saving",
    "pytorch",
    "documentation",
    "pytorch",
    "cheat",
    "sheet",
    "great",
    "article",
    "jeremy",
    "howard",
    "deeper",
    "understanding",
    "going",
    "course",
    "unofficial",
    "pytorch",
    "optimization",
    "loop",
    "song",
    "truly",
    "bit",
    "fun",
    "exercises",
    "exercises",
    "based",
    "code",
    "wrote",
    "throughout",
    "section",
    "nothing",
    "exercises",
    "exactly",
    "covered",
    "sure",
    "put",
    "note",
    "exercise",
    "got",
    "create",
    "straight",
    "line",
    "data",
    "set",
    "using",
    "linear",
    "regression",
    "formula",
    "build",
    "model",
    "subclassing",
    "end",
    "module",
    "exercises",
    "exercise",
    "notebook",
    "template",
    "course",
    "linked",
    "pytorch",
    "deep",
    "learning",
    "github",
    "go",
    "go",
    "extras",
    "go",
    "exercises",
    "find",
    "templates",
    "numbered",
    "section",
    "pytorch",
    "workflow",
    "exercises",
    "wanted",
    "complete",
    "exercises",
    "could",
    "click",
    "notebook",
    "open",
    "google",
    "colab",
    "wait",
    "load",
    "go",
    "start",
    "write",
    "code",
    "could",
    "save",
    "copy",
    "google",
    "drive",
    "go",
    "got",
    "notes",
    "course",
    "refer",
    "version",
    "want",
    "example",
    "solutions",
    "look",
    "like",
    "please",
    "ca",
    "stress",
    "enough",
    "would",
    "highly",
    "highly",
    "recommend",
    "trying",
    "exercises",
    "use",
    "book",
    "got",
    "code",
    "videos",
    "use",
    "use",
    "got",
    "many",
    "notebooks",
    "use",
    "code",
    "written",
    "try",
    "complete",
    "exercises",
    "please",
    "give",
    "go",
    "go",
    "back",
    "extras",
    "folder",
    "also",
    "find",
    "solutions",
    "one",
    "example",
    "solutions",
    "section",
    "going",
    "get",
    "ca",
    "cheat",
    "look",
    "solutions",
    "first",
    "whole",
    "bunch",
    "extra",
    "resources",
    "contained",
    "within",
    "pytorch",
    "deep",
    "loaning",
    "repo",
    "extras",
    "exercises",
    "solutions",
    "also",
    "book",
    "version",
    "course",
    "going",
    "link",
    "going",
    "put",
    "right",
    "bottom",
    "wonderful",
    "end",
    "section",
    "01",
    "pytorch",
    "workflow",
    "exciting",
    "went",
    "basically",
    "steps",
    "pytorch",
    "workflow",
    "getting",
    "data",
    "ready",
    "turning",
    "tenses",
    "build",
    "pick",
    "model",
    "picking",
    "loss",
    "function",
    "optimizer",
    "built",
    "training",
    "loop",
    "fit",
    "model",
    "data",
    "made",
    "prediction",
    "evaluated",
    "model",
    "improved",
    "experimentation",
    "training",
    "epochs",
    "later",
    "saved",
    "reload",
    "trained",
    "model",
    "going",
    "finish",
    "see",
    "next",
    "section",
    "friends",
    "welcome",
    "back",
    "got",
    "another",
    "exciting",
    "module",
    "ready",
    "neural",
    "network",
    "classification",
    "pytorch",
    "combining",
    "module",
    "get",
    "end",
    "last",
    "one",
    "regression",
    "remember",
    "classification",
    "predicting",
    "thing",
    "going",
    "see",
    "second",
    "regression",
    "predicting",
    "number",
    "covered",
    "covered",
    "two",
    "biggest",
    "problems",
    "machine",
    "learning",
    "predicting",
    "number",
    "predicting",
    "thing",
    "let",
    "start",
    "get",
    "ideas",
    "code",
    "get",
    "help",
    "first",
    "things",
    "first",
    "follow",
    "along",
    "code",
    "doubt",
    "run",
    "code",
    "try",
    "write",
    "code",
    "ca",
    "stress",
    "important",
    "still",
    "stuck",
    "press",
    "shift",
    "command",
    "space",
    "read",
    "doc",
    "string",
    "functions",
    "running",
    "windows",
    "might",
    "control",
    "mac",
    "put",
    "command",
    "still",
    "stuck",
    "search",
    "problem",
    "error",
    "comes",
    "copy",
    "paste",
    "google",
    "might",
    "come",
    "across",
    "resources",
    "like",
    "stack",
    "overflow",
    "course",
    "pytorch",
    "documentation",
    "referring",
    "lot",
    "throughout",
    "section",
    "finally",
    "oh",
    "wait",
    "still",
    "stuck",
    "try",
    "doubt",
    "run",
    "code",
    "finally",
    "still",
    "stuck",
    "forget",
    "ask",
    "question",
    "best",
    "place",
    "course",
    "github",
    "discussions",
    "page",
    "linked",
    "load",
    "nothing",
    "yet",
    "record",
    "videos",
    "course",
    "launched",
    "yet",
    "press",
    "new",
    "discussion",
    "talk",
    "got",
    "problem",
    "xyz",
    "let",
    "go",
    "ahead",
    "leave",
    "video",
    "number",
    "timestamp",
    "way",
    "able",
    "help",
    "best",
    "possible",
    "video",
    "number",
    "timestamp",
    "question",
    "select",
    "q",
    "finally",
    "forget",
    "notebook",
    "go",
    "based",
    "chapter",
    "two",
    "zero",
    "mastery",
    "learn",
    "pytorch",
    "deep",
    "learning",
    "neural",
    "network",
    "classification",
    "pytorch",
    "code",
    "write",
    "little",
    "spoiler",
    "forget",
    "home",
    "page",
    "github",
    "repo",
    "slash",
    "pytorch",
    "deep",
    "learning",
    "course",
    "materials",
    "everything",
    "need",
    "important",
    "get",
    "help",
    "number",
    "one",
    "follow",
    "along",
    "code",
    "try",
    "write",
    "well",
    "said",
    "talking",
    "classification",
    "classification",
    "problem",
    "said",
    "classification",
    "one",
    "main",
    "problems",
    "machine",
    "learning",
    "probably",
    "already",
    "deal",
    "classification",
    "problems",
    "machine",
    "learning",
    "powered",
    "classification",
    "problems",
    "every",
    "day",
    "let",
    "look",
    "examples",
    "email",
    "spam",
    "spam",
    "check",
    "emails",
    "morning",
    "last",
    "night",
    "whenever",
    "chances",
    "sort",
    "machine",
    "learning",
    "model",
    "behind",
    "scenes",
    "may",
    "neural",
    "network",
    "may",
    "decided",
    "emails",
    "wo",
    "spam",
    "daniel",
    "hey",
    "daniel",
    "steep",
    "learning",
    "course",
    "incredible",
    "ca",
    "wait",
    "use",
    "learned",
    "oh",
    "nice",
    "message",
    "want",
    "send",
    "email",
    "directly",
    "actual",
    "email",
    "address",
    "want",
    "send",
    "email",
    "well",
    "hopefully",
    "email",
    "hosted",
    "email",
    "service",
    "detects",
    "spam",
    "although",
    "lot",
    "money",
    "would",
    "nice",
    "think",
    "someone",
    "ca",
    "spell",
    "well",
    "really",
    "going",
    "pay",
    "much",
    "money",
    "thank",
    "email",
    "provider",
    "classifying",
    "spam",
    "one",
    "thing",
    "another",
    "spam",
    "spam",
    "binary",
    "classification",
    "case",
    "might",
    "one",
    "zero",
    "zero",
    "one",
    "one",
    "thing",
    "another",
    "binary",
    "classification",
    "split",
    "one",
    "thing",
    "another",
    "binary",
    "classification",
    "example",
    "say",
    "question",
    "asked",
    "photos",
    "app",
    "smartphone",
    "whatever",
    "device",
    "using",
    "photo",
    "sushi",
    "steak",
    "pizza",
    "wanted",
    "search",
    "photos",
    "every",
    "time",
    "eaten",
    "sushi",
    "every",
    "time",
    "eaten",
    "steak",
    "every",
    "time",
    "eaten",
    "pizza",
    "far",
    "looks",
    "delicious",
    "multi",
    "class",
    "classification",
    "got",
    "two",
    "things",
    "got",
    "could",
    "10",
    "different",
    "foods",
    "could",
    "100",
    "different",
    "foods",
    "could",
    "1000",
    "different",
    "categories",
    "image",
    "net",
    "data",
    "set",
    "popular",
    "data",
    "set",
    "computer",
    "vision",
    "image",
    "net",
    "go",
    "say",
    "1000",
    "anywhere",
    "1k",
    "1000",
    "go",
    "image",
    "net",
    "1k",
    "download",
    "image",
    "net",
    "data",
    "maybe",
    "wo",
    "say",
    "oh",
    "go",
    "1000",
    "object",
    "classes",
    "multi",
    "class",
    "classification",
    "1000",
    "classes",
    "lot",
    "right",
    "multi",
    "class",
    "classification",
    "one",
    "thing",
    "another",
    "finally",
    "might",
    "multi",
    "label",
    "classification",
    "tags",
    "article",
    "first",
    "got",
    "machine",
    "learning",
    "got",
    "two",
    "mixed",
    "whole",
    "bunch",
    "times",
    "multi",
    "class",
    "classification",
    "multiple",
    "classes",
    "sushi",
    "steak",
    "pizza",
    "assigns",
    "one",
    "label",
    "photo",
    "would",
    "sushi",
    "ideal",
    "world",
    "steak",
    "pizza",
    "one",
    "label",
    "whereas",
    "multi",
    "label",
    "classification",
    "means",
    "could",
    "multiple",
    "different",
    "classes",
    "target",
    "samples",
    "wikipedia",
    "article",
    "tags",
    "article",
    "may",
    "one",
    "label",
    "might",
    "three",
    "labels",
    "might",
    "10",
    "labels",
    "fact",
    "went",
    "wikipedia",
    "page",
    "deep",
    "learning",
    "wikipedia",
    "labels",
    "oh",
    "go",
    "mean",
    "try",
    "wikipedia",
    "page",
    "deep",
    "learning",
    "lot",
    "go",
    "categories",
    "deep",
    "learning",
    "artificial",
    "neural",
    "networks",
    "artificial",
    "intelligence",
    "emerging",
    "technologies",
    "example",
    "wanted",
    "build",
    "machine",
    "learning",
    "model",
    "say",
    "read",
    "text",
    "go",
    "tell",
    "relevant",
    "categories",
    "article",
    "might",
    "come",
    "something",
    "like",
    "case",
    "one",
    "two",
    "three",
    "four",
    "multiple",
    "labels",
    "rather",
    "one",
    "label",
    "deep",
    "learning",
    "could",
    "multi",
    "label",
    "classification",
    "go",
    "back",
    "get",
    "quite",
    "far",
    "world",
    "classification",
    "let",
    "dig",
    "little",
    "deeper",
    "binary",
    "versus",
    "multi",
    "class",
    "classification",
    "may",
    "already",
    "experienced",
    "case",
    "search",
    "phone",
    "photos",
    "app",
    "photos",
    "dog",
    "might",
    "come",
    "search",
    "photos",
    "cat",
    "might",
    "come",
    "wanted",
    "train",
    "algorithm",
    "detect",
    "difference",
    "photos",
    "two",
    "dogs",
    "cute",
    "nice",
    "tired",
    "sleeping",
    "like",
    "person",
    "seven",
    "number",
    "seven",
    "name",
    "bella",
    "cat",
    "partner",
    "rescued",
    "sure",
    "cat",
    "name",
    "actually",
    "love",
    "give",
    "name",
    "ca",
    "binary",
    "classification",
    "wanted",
    "build",
    "algorithm",
    "wanted",
    "feed",
    "say",
    "photos",
    "dogs",
    "photos",
    "cats",
    "wanted",
    "find",
    "random",
    "image",
    "internet",
    "pass",
    "model",
    "say",
    "hey",
    "dog",
    "cat",
    "would",
    "binary",
    "classification",
    "options",
    "one",
    "thing",
    "another",
    "dog",
    "cat",
    "classification",
    "let",
    "say",
    "working",
    "farm",
    "taking",
    "photos",
    "chickens",
    "groovy",
    "right",
    "well",
    "updated",
    "model",
    "added",
    "chicken",
    "photos",
    "would",
    "working",
    "classification",
    "problem",
    "got",
    "one",
    "thing",
    "another",
    "let",
    "jump",
    "going",
    "cover",
    "broadly",
    "way",
    "text",
    "page",
    "know",
    "like",
    "write",
    "code",
    "actually",
    "going",
    "look",
    "architecture",
    "neural",
    "network",
    "classification",
    "model",
    "going",
    "check",
    "input",
    "shapes",
    "output",
    "shapes",
    "classification",
    "model",
    "features",
    "labels",
    "words",
    "remember",
    "machine",
    "learning",
    "models",
    "neural",
    "networks",
    "love",
    "numerical",
    "inputs",
    "numerical",
    "inputs",
    "often",
    "come",
    "tenses",
    "tenses",
    "different",
    "shapes",
    "depending",
    "data",
    "working",
    "going",
    "see",
    "code",
    "creating",
    "custom",
    "data",
    "view",
    "fit",
    "predict",
    "going",
    "go",
    "back",
    "steps",
    "modeling",
    "covered",
    "fair",
    "bit",
    "previous",
    "section",
    "creating",
    "model",
    "neural",
    "network",
    "classification",
    "little",
    "bit",
    "different",
    "done",
    "landishly",
    "different",
    "going",
    "see",
    "set",
    "loss",
    "function",
    "optimizer",
    "classification",
    "model",
    "recreate",
    "training",
    "loop",
    "evaluating",
    "loop",
    "testing",
    "loop",
    "see",
    "save",
    "load",
    "models",
    "harness",
    "power",
    "nonlinearity",
    "well",
    "even",
    "mean",
    "well",
    "think",
    "linear",
    "line",
    "straight",
    "line",
    "might",
    "able",
    "guess",
    "nonlinear",
    "line",
    "looks",
    "like",
    "look",
    "different",
    "classification",
    "evaluation",
    "methods",
    "ways",
    "evaluate",
    "classification",
    "models",
    "going",
    "well",
    "course",
    "going",
    "part",
    "cook",
    "part",
    "chemist",
    "part",
    "artist",
    "part",
    "science",
    "personally",
    "prefer",
    "cook",
    "side",
    "things",
    "going",
    "cooking",
    "lots",
    "code",
    "next",
    "video",
    "get",
    "coding",
    "let",
    "little",
    "bit",
    "classification",
    "inputs",
    "outputs",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "little",
    "bit",
    "brief",
    "overview",
    "classification",
    "problem",
    "let",
    "start",
    "get",
    "hands",
    "discussing",
    "actual",
    "inputs",
    "classification",
    "problem",
    "look",
    "like",
    "outputs",
    "look",
    "like",
    "let",
    "say",
    "beautiful",
    "food",
    "photos",
    "trying",
    "build",
    "app",
    "called",
    "maybe",
    "food",
    "vision",
    "understand",
    "foods",
    "photos",
    "take",
    "might",
    "look",
    "like",
    "well",
    "let",
    "break",
    "inputs",
    "kind",
    "machine",
    "learning",
    "algorithm",
    "outputs",
    "case",
    "inputs",
    "want",
    "numerically",
    "represent",
    "images",
    "way",
    "shape",
    "form",
    "want",
    "build",
    "machine",
    "learning",
    "algorithm",
    "hey",
    "one",
    "might",
    "actually",
    "exist",
    "going",
    "see",
    "later",
    "transfer",
    "learning",
    "section",
    "problem",
    "want",
    "sort",
    "outputs",
    "case",
    "food",
    "vision",
    "want",
    "know",
    "okay",
    "photo",
    "sushi",
    "photo",
    "steak",
    "photo",
    "pizza",
    "could",
    "get",
    "hands",
    "technical",
    "complicated",
    "going",
    "stick",
    "single",
    "label",
    "multi",
    "class",
    "classification",
    "could",
    "sushi",
    "photo",
    "could",
    "steak",
    "photo",
    "could",
    "pizza",
    "photo",
    "might",
    "numerically",
    "represent",
    "photos",
    "well",
    "let",
    "say",
    "function",
    "app",
    "every",
    "photo",
    "gets",
    "taken",
    "automatically",
    "gets",
    "resized",
    "square",
    "224",
    "width",
    "224",
    "height",
    "actually",
    "quite",
    "common",
    "dimensionality",
    "computer",
    "vision",
    "problems",
    "got",
    "width",
    "dimension",
    "got",
    "height",
    "got",
    "c",
    "immediately",
    "recognizable",
    "case",
    "pictures",
    "often",
    "get",
    "represented",
    "width",
    "height",
    "color",
    "channels",
    "color",
    "channels",
    "red",
    "green",
    "blue",
    "pixel",
    "image",
    "value",
    "red",
    "green",
    "blue",
    "makes",
    "whatever",
    "color",
    "displayed",
    "one",
    "way",
    "numerically",
    "represent",
    "image",
    "taking",
    "width",
    "height",
    "color",
    "channels",
    "whatever",
    "number",
    "makes",
    "particular",
    "image",
    "going",
    "see",
    "later",
    "work",
    "computer",
    "vision",
    "problems",
    "create",
    "numerical",
    "encoding",
    "pixel",
    "values",
    "import",
    "pixel",
    "values",
    "images",
    "machine",
    "learning",
    "algorithm",
    "often",
    "already",
    "exists",
    "exist",
    "particular",
    "problem",
    "hey",
    "well",
    "learning",
    "skills",
    "build",
    "could",
    "use",
    "pytorch",
    "build",
    "machine",
    "learning",
    "algorithm",
    "outputs",
    "might",
    "look",
    "like",
    "well",
    "case",
    "prediction",
    "probabilities",
    "outputs",
    "machine",
    "learning",
    "models",
    "never",
    "actually",
    "discrete",
    "means",
    "definitely",
    "pizza",
    "give",
    "sort",
    "probability",
    "value",
    "zero",
    "one",
    "say",
    "closer",
    "one",
    "confident",
    "model",
    "going",
    "pizza",
    "closer",
    "zero",
    "means",
    "hey",
    "photo",
    "pizza",
    "let",
    "say",
    "one",
    "trying",
    "predict",
    "sushi",
    "well",
    "think",
    "sushi",
    "giving",
    "quite",
    "low",
    "value",
    "steak",
    "really",
    "high",
    "value",
    "pizza",
    "going",
    "see",
    "hands",
    "opposite",
    "might",
    "got",
    "one",
    "wrong",
    "training",
    "data",
    "could",
    "probably",
    "improve",
    "prediction",
    "whole",
    "idea",
    "machine",
    "learning",
    "adjust",
    "algorithm",
    "adjust",
    "data",
    "improve",
    "predictions",
    "ideal",
    "outputs",
    "models",
    "going",
    "output",
    "case",
    "building",
    "food",
    "vision",
    "want",
    "bring",
    "back",
    "could",
    "put",
    "numbers",
    "screen",
    "really",
    "going",
    "help",
    "people",
    "want",
    "put",
    "labels",
    "going",
    "write",
    "code",
    "transfer",
    "prediction",
    "probabilities",
    "labels",
    "labels",
    "come",
    "predictions",
    "come",
    "well",
    "comes",
    "looking",
    "lots",
    "different",
    "samples",
    "loop",
    "could",
    "keep",
    "going",
    "improve",
    "find",
    "ones",
    "wrong",
    "add",
    "images",
    "train",
    "model",
    "make",
    "app",
    "better",
    "want",
    "look",
    "shape",
    "perspective",
    "want",
    "create",
    "tenses",
    "image",
    "classification",
    "example",
    "building",
    "food",
    "vision",
    "got",
    "image",
    "reiterating",
    "things",
    "discussed",
    "got",
    "width",
    "224",
    "height",
    "could",
    "different",
    "could",
    "300",
    "could",
    "whatever",
    "values",
    "decide",
    "use",
    "numerically",
    "encoded",
    "way",
    "shape",
    "form",
    "use",
    "inputs",
    "machine",
    "learning",
    "algorithm",
    "computers",
    "machine",
    "learning",
    "algorithms",
    "love",
    "numbers",
    "find",
    "patterns",
    "could",
    "necessarily",
    "find",
    "maybe",
    "could",
    "long",
    "enough",
    "time",
    "rather",
    "write",
    "algorithm",
    "outputs",
    "comes",
    "formal",
    "prediction",
    "probabilities",
    "closer",
    "one",
    "confident",
    "model",
    "saying",
    "hey",
    "pretty",
    "damn",
    "confident",
    "photo",
    "sushi",
    "think",
    "photo",
    "steak",
    "giving",
    "zero",
    "might",
    "photo",
    "pizza",
    "really",
    "think",
    "giving",
    "quite",
    "low",
    "prediction",
    "probability",
    "look",
    "shapes",
    "tenses",
    "make",
    "sense",
    "worry",
    "going",
    "see",
    "code",
    "later",
    "focusing",
    "classification",
    "input",
    "output",
    "big",
    "takeaway",
    "numerical",
    "encoding",
    "outputs",
    "numerical",
    "encoding",
    "want",
    "change",
    "numerical",
    "codings",
    "outputs",
    "something",
    "understand",
    "say",
    "word",
    "sushi",
    "tensor",
    "may",
    "batch",
    "size",
    "seen",
    "batch",
    "size",
    "right",
    "going",
    "cover",
    "color",
    "channels",
    "height",
    "represented",
    "tensor",
    "dimensions",
    "could",
    "none",
    "none",
    "typical",
    "value",
    "batch",
    "size",
    "means",
    "blank",
    "use",
    "model",
    "train",
    "code",
    "write",
    "pytorch",
    "fill",
    "behind",
    "scenes",
    "three",
    "color",
    "channels",
    "224",
    "width",
    "224",
    "well",
    "height",
    "debate",
    "field",
    "ordering",
    "using",
    "image",
    "particular",
    "example",
    "ordering",
    "shapes",
    "say",
    "example",
    "might",
    "height",
    "width",
    "color",
    "channels",
    "typically",
    "width",
    "height",
    "come",
    "together",
    "order",
    "side",
    "side",
    "tensor",
    "terms",
    "whether",
    "dimension",
    "appears",
    "color",
    "channels",
    "sometimes",
    "comes",
    "first",
    "means",
    "batch",
    "size",
    "end",
    "pytorch",
    "default",
    "color",
    "channels",
    "height",
    "though",
    "write",
    "code",
    "change",
    "order",
    "tenses",
    "quite",
    "flexible",
    "shape",
    "could",
    "32",
    "batch",
    "size",
    "three",
    "two",
    "two",
    "four",
    "two",
    "two",
    "four",
    "32",
    "common",
    "batch",
    "size",
    "believe",
    "well",
    "let",
    "go",
    "yarn",
    "lecoon",
    "32",
    "batch",
    "size",
    "batch",
    "size",
    "great",
    "tweet",
    "keep",
    "mind",
    "later",
    "training",
    "large",
    "mini",
    "batches",
    "bad",
    "health",
    "importantly",
    "bad",
    "test",
    "error",
    "friends",
    "let",
    "friends",
    "use",
    "mini",
    "batches",
    "larger",
    "quite",
    "old",
    "tweet",
    "however",
    "still",
    "stands",
    "quite",
    "true",
    "like",
    "today",
    "2022",
    "recording",
    "videos",
    "batch",
    "sizes",
    "lot",
    "larger",
    "32",
    "works",
    "pretty",
    "darn",
    "well",
    "lot",
    "problems",
    "means",
    "go",
    "back",
    "slide",
    "use",
    "batch",
    "size",
    "32",
    "machine",
    "learning",
    "algorithm",
    "looks",
    "32",
    "images",
    "time",
    "well",
    "sadly",
    "computers",
    "infinite",
    "compute",
    "power",
    "ideal",
    "world",
    "look",
    "thousands",
    "images",
    "time",
    "turns",
    "using",
    "multiple",
    "eight",
    "actually",
    "quite",
    "efficient",
    "look",
    "output",
    "shape",
    "three",
    "well",
    "working",
    "three",
    "different",
    "classes",
    "one",
    "two",
    "three",
    "got",
    "shape",
    "equals",
    "three",
    "course",
    "could",
    "imagine",
    "might",
    "change",
    "depending",
    "problem",
    "working",
    "say",
    "wanted",
    "predict",
    "photo",
    "cat",
    "dog",
    "still",
    "might",
    "representation",
    "image",
    "representation",
    "however",
    "shape",
    "may",
    "two",
    "two",
    "cat",
    "dog",
    "rather",
    "three",
    "classes",
    "little",
    "bit",
    "confusing",
    "well",
    "binary",
    "classification",
    "could",
    "shape",
    "one",
    "going",
    "see",
    "hands",
    "remember",
    "shapes",
    "vary",
    "whatever",
    "problem",
    "working",
    "principle",
    "encoding",
    "data",
    "numerical",
    "representation",
    "stays",
    "inputs",
    "outputs",
    "often",
    "form",
    "prediction",
    "probability",
    "based",
    "whatever",
    "class",
    "working",
    "next",
    "video",
    "right",
    "get",
    "coding",
    "let",
    "discuss",
    "high",
    "level",
    "architecture",
    "classification",
    "model",
    "remember",
    "architecture",
    "like",
    "schematic",
    "neural",
    "network",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "saw",
    "example",
    "classification",
    "inputs",
    "outputs",
    "main",
    "takeaway",
    "inputs",
    "classification",
    "model",
    "particularly",
    "neural",
    "network",
    "want",
    "form",
    "numerical",
    "representation",
    "outputs",
    "often",
    "form",
    "prediction",
    "probability",
    "let",
    "discuss",
    "typical",
    "architecture",
    "classification",
    "model",
    "hey",
    "going",
    "text",
    "page",
    "going",
    "building",
    "fair",
    "got",
    "hyper",
    "parameters",
    "got",
    "binary",
    "classification",
    "got",
    "multi",
    "class",
    "classification",
    "similarities",
    "two",
    "terms",
    "problem",
    "working",
    "also",
    "differences",
    "way",
    "come",
    "go",
    "book",
    "version",
    "course",
    "got",
    "classification",
    "problem",
    "got",
    "architecture",
    "classification",
    "neural",
    "network",
    "text",
    "available",
    "section",
    "two",
    "come",
    "back",
    "input",
    "layer",
    "shape",
    "typically",
    "decided",
    "parameter",
    "features",
    "see",
    "number",
    "features",
    "working",
    "problem",
    "brought",
    "predict",
    "whether",
    "someone",
    "heart",
    "disease",
    "might",
    "five",
    "input",
    "features",
    "one",
    "age",
    "number",
    "age",
    "might",
    "case",
    "28",
    "sex",
    "could",
    "male",
    "height",
    "180",
    "centimeters",
    "growing",
    "overnight",
    "really",
    "close",
    "wait",
    "well",
    "depends",
    "much",
    "eaten",
    "around",
    "75",
    "kilos",
    "smoking",
    "status",
    "zero",
    "could",
    "zero",
    "one",
    "remember",
    "want",
    "numerical",
    "representation",
    "sex",
    "could",
    "zero",
    "males",
    "one",
    "female",
    "height",
    "could",
    "number",
    "weight",
    "could",
    "number",
    "well",
    "numbers",
    "could",
    "could",
    "less",
    "well",
    "really",
    "flexible",
    "hyper",
    "parameter",
    "decide",
    "values",
    "case",
    "image",
    "prediction",
    "problem",
    "could",
    "features",
    "equals",
    "three",
    "number",
    "color",
    "channels",
    "go",
    "hidden",
    "layers",
    "blue",
    "circle",
    "forgot",
    "timed",
    "colorful",
    "let",
    "discuss",
    "hidden",
    "layers",
    "layer",
    "n",
    "dot",
    "linear",
    "n",
    "dot",
    "linear",
    "n",
    "dot",
    "relu",
    "n",
    "dot",
    "linear",
    "kind",
    "syntax",
    "see",
    "pytorch",
    "layer",
    "nn",
    "dot",
    "something",
    "many",
    "different",
    "types",
    "layers",
    "pytorch",
    "go",
    "torch",
    "n",
    "basically",
    "everything",
    "layer",
    "neural",
    "network",
    "look",
    "neural",
    "network",
    "looks",
    "like",
    "neural",
    "network",
    "recall",
    "different",
    "layers",
    "kind",
    "mathematical",
    "operation",
    "input",
    "layer",
    "hidden",
    "layer",
    "could",
    "many",
    "hidden",
    "layers",
    "want",
    "resnet",
    "architecture",
    "resnet",
    "architecture",
    "50",
    "layers",
    "look",
    "one",
    "layer",
    "34",
    "layer",
    "version",
    "mean",
    "resnet",
    "152",
    "152",
    "layers",
    "yet",
    "working",
    "tools",
    "get",
    "stage",
    "let",
    "come",
    "back",
    "neurons",
    "per",
    "hidden",
    "layer",
    "got",
    "features",
    "green",
    "circle",
    "green",
    "square",
    "go",
    "back",
    "neural",
    "network",
    "picture",
    "one",
    "little",
    "things",
    "neuron",
    "sort",
    "parameter",
    "100",
    "would",
    "look",
    "like",
    "well",
    "fairly",
    "big",
    "graphic",
    "like",
    "teach",
    "code",
    "could",
    "customize",
    "flexible",
    "want",
    "behind",
    "scenes",
    "pytorch",
    "going",
    "create",
    "100",
    "little",
    "circles",
    "us",
    "within",
    "circle",
    "sort",
    "mathematical",
    "operation",
    "come",
    "back",
    "got",
    "next",
    "output",
    "layer",
    "shape",
    "many",
    "output",
    "features",
    "case",
    "binary",
    "classification",
    "one",
    "one",
    "class",
    "going",
    "see",
    "later",
    "classification",
    "might",
    "three",
    "output",
    "features",
    "one",
    "per",
    "class",
    "one",
    "food",
    "person",
    "dog",
    "building",
    "food",
    "person",
    "dog",
    "image",
    "classification",
    "model",
    "hidden",
    "layer",
    "activation",
    "seen",
    "yet",
    "relu",
    "rectified",
    "linear",
    "unit",
    "many",
    "others",
    "pytorch",
    "course",
    "lot",
    "activations",
    "going",
    "see",
    "later",
    "remember",
    "kind",
    "planting",
    "seed",
    "seen",
    "linear",
    "line",
    "want",
    "imagine",
    "line",
    "going",
    "bit",
    "superpower",
    "classification",
    "problem",
    "else",
    "output",
    "activation",
    "got",
    "also",
    "see",
    "later",
    "could",
    "sigmoid",
    "generally",
    "sigmoid",
    "binary",
    "classification",
    "softmax",
    "classification",
    "lot",
    "things",
    "names",
    "page",
    "seen",
    "yet",
    "like",
    "teach",
    "see",
    "general",
    "overview",
    "going",
    "cover",
    "loss",
    "function",
    "loss",
    "function",
    "loss",
    "function",
    "measures",
    "wrong",
    "model",
    "predictions",
    "compared",
    "ideal",
    "predictions",
    "binary",
    "classification",
    "might",
    "use",
    "binary",
    "cross",
    "entropy",
    "loss",
    "pytorch",
    "classification",
    "might",
    "use",
    "cross",
    "entropy",
    "rather",
    "binary",
    "cross",
    "entropy",
    "get",
    "binary",
    "classification",
    "binary",
    "cross",
    "entropy",
    "optimizer",
    "sgd",
    "stochastic",
    "gradient",
    "descent",
    "seen",
    "one",
    "another",
    "common",
    "option",
    "atom",
    "optimizer",
    "course",
    "package",
    "plenty",
    "options",
    "example",
    "classification",
    "problem",
    "network",
    "actually",
    "seen",
    "end",
    "sequential",
    "could",
    "imagine",
    "sequential",
    "stands",
    "goes",
    "steps",
    "classification",
    "three",
    "output",
    "features",
    "one",
    "thing",
    "another",
    "three",
    "food",
    "person",
    "dog",
    "going",
    "back",
    "food",
    "vision",
    "problem",
    "could",
    "input",
    "sushi",
    "steak",
    "pizza",
    "got",
    "three",
    "output",
    "features",
    "would",
    "one",
    "prediction",
    "probability",
    "per",
    "class",
    "image",
    "three",
    "classes",
    "sushi",
    "steak",
    "pizza",
    "think",
    "done",
    "enough",
    "talking",
    "enough",
    "pointing",
    "text",
    "slides",
    "next",
    "video",
    "let",
    "code",
    "see",
    "google",
    "colab",
    "welcome",
    "back",
    "done",
    "enough",
    "theory",
    "classification",
    "problem",
    "inputs",
    "outputs",
    "typical",
    "architecture",
    "let",
    "get",
    "write",
    "code",
    "going",
    "get",
    "going",
    "go",
    "start",
    "writing",
    "pytorch",
    "code",
    "going",
    "click",
    "new",
    "notebook",
    "going",
    "start",
    "exactly",
    "scratch",
    "going",
    "name",
    "section",
    "two",
    "let",
    "call",
    "neural",
    "network",
    "classification",
    "pytorch",
    "going",
    "put",
    "underscore",
    "video",
    "show",
    "see",
    "github",
    "repo",
    "video",
    "notebooks",
    "ones",
    "write",
    "code",
    "videos",
    "watching",
    "exact",
    "code",
    "going",
    "saved",
    "github",
    "repo",
    "video",
    "notebooks",
    "00",
    "fundamentals",
    "workflow",
    "underscore",
    "video",
    "reference",
    "notebook",
    "pretty",
    "pictures",
    "stuff",
    "main",
    "folder",
    "pytorch",
    "classification",
    "pi",
    "b",
    "actually",
    "maybe",
    "rename",
    "pytorch",
    "classification",
    "know",
    "neural",
    "networks",
    "pytorch",
    "classification",
    "okay",
    "let",
    "go",
    "add",
    "nice",
    "title",
    "o2",
    "neural",
    "network",
    "classification",
    "pytorch",
    "remind",
    "classification",
    "problem",
    "predicting",
    "whether",
    "something",
    "one",
    "thing",
    "another",
    "multiple",
    "things",
    "options",
    "email",
    "spam",
    "spam",
    "photos",
    "dogs",
    "cats",
    "pizza",
    "sushi",
    "steak",
    "lots",
    "talk",
    "food",
    "going",
    "link",
    "resource",
    "book",
    "version",
    "course",
    "videos",
    "based",
    "book",
    "version",
    "notebook",
    "resources",
    "resources",
    "github",
    "stuck",
    "ask",
    "question",
    "discussions",
    "tab",
    "copy",
    "way",
    "got",
    "everything",
    "linked",
    "ready",
    "go",
    "always",
    "first",
    "step",
    "workflow",
    "little",
    "test",
    "see",
    "remember",
    "well",
    "data",
    "course",
    "machine",
    "learning",
    "problems",
    "start",
    "form",
    "data",
    "ca",
    "write",
    "machine",
    "learning",
    "algorithm",
    "learn",
    "patterns",
    "data",
    "exist",
    "let",
    "video",
    "going",
    "make",
    "data",
    "course",
    "might",
    "start",
    "exists",
    "going",
    "focus",
    "concepts",
    "around",
    "workflow",
    "going",
    "make",
    "custom",
    "data",
    "set",
    "write",
    "code",
    "first",
    "show",
    "get",
    "going",
    "import",
    "scikit",
    "loan",
    "library",
    "one",
    "beautiful",
    "things",
    "google",
    "colab",
    "scikit",
    "loan",
    "available",
    "sure",
    "scikit",
    "loan",
    "popular",
    "machine",
    "learning",
    "library",
    "pytorch",
    "mainly",
    "focused",
    "deep",
    "learning",
    "scikit",
    "loan",
    "focused",
    "lot",
    "things",
    "around",
    "machine",
    "learning",
    "google",
    "colab",
    "thank",
    "scikit",
    "loan",
    "already",
    "installed",
    "us",
    "going",
    "import",
    "make",
    "circles",
    "data",
    "set",
    "rather",
    "talk",
    "let",
    "see",
    "make",
    "1000",
    "samples",
    "going",
    "go",
    "n",
    "samples",
    "equals",
    "going",
    "create",
    "circles",
    "might",
    "wondering",
    "circles",
    "well",
    "going",
    "see",
    "exactly",
    "circles",
    "later",
    "x",
    "going",
    "use",
    "variable",
    "would",
    "say",
    "nomenclature",
    "capital",
    "x",
    "x",
    "typically",
    "matrix",
    "features",
    "labels",
    "let",
    "go",
    "mate",
    "circles",
    "going",
    "make",
    "n",
    "samples",
    "1000",
    "different",
    "samples",
    "going",
    "add",
    "noise",
    "put",
    "little",
    "bit",
    "randomness",
    "increase",
    "want",
    "found",
    "fairly",
    "good",
    "going",
    "also",
    "pass",
    "random",
    "state",
    "variable",
    "equivalent",
    "sitting",
    "random",
    "setting",
    "random",
    "seed",
    "flavoring",
    "randomness",
    "wonderful",
    "let",
    "look",
    "length",
    "x",
    "length",
    "oh",
    "underscore",
    "getting",
    "bit",
    "trigger",
    "happy",
    "keyboard",
    "1000",
    "samples",
    "x",
    "caught",
    "1000",
    "paired",
    "1000",
    "samples",
    "features",
    "labels",
    "let",
    "look",
    "first",
    "five",
    "print",
    "first",
    "five",
    "samples",
    "put",
    "index",
    "five",
    "adhering",
    "data",
    "explorer",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "first",
    "five",
    "samples",
    "going",
    "go",
    "thing",
    "wonderful",
    "let",
    "look",
    "maybe",
    "get",
    "new",
    "line",
    "looks",
    "bit",
    "better",
    "wonderful",
    "numerical",
    "samples",
    "already",
    "numerical",
    "one",
    "reasons",
    "creating",
    "data",
    "set",
    "see",
    "later",
    "get",
    "non",
    "numerical",
    "data",
    "numbers",
    "data",
    "numerical",
    "means",
    "learn",
    "model",
    "build",
    "model",
    "learn",
    "patterns",
    "sample",
    "label",
    "one",
    "sample",
    "label",
    "one",
    "well",
    "many",
    "features",
    "per",
    "sample",
    "highlight",
    "line",
    "many",
    "features",
    "would",
    "make",
    "bit",
    "easier",
    "comma",
    "two",
    "features",
    "x",
    "relates",
    "one",
    "label",
    "far",
    "seen",
    "let",
    "look",
    "got",
    "zero",
    "one",
    "got",
    "two",
    "classes",
    "mean",
    "zero",
    "one",
    "one",
    "thing",
    "another",
    "well",
    "looks",
    "like",
    "binary",
    "classification",
    "got",
    "zero",
    "one",
    "zero",
    "one",
    "two",
    "would",
    "multi",
    "class",
    "classification",
    "two",
    "things",
    "let",
    "x",
    "let",
    "keep",
    "going",
    "little",
    "bit",
    "data",
    "exploration",
    "make",
    "data",
    "frame",
    "pandas",
    "circle",
    "data",
    "truly",
    "real",
    "definite",
    "way",
    "explore",
    "data",
    "like",
    "visualize",
    "multiple",
    "different",
    "ways",
    "even",
    "look",
    "random",
    "samples",
    "case",
    "large",
    "data",
    "sets",
    "images",
    "text",
    "whatnot",
    "10",
    "million",
    "samples",
    "perhaps",
    "visualizing",
    "one",
    "one",
    "best",
    "way",
    "random",
    "help",
    "going",
    "create",
    "data",
    "frame",
    "insert",
    "dictionary",
    "going",
    "call",
    "features",
    "part",
    "x",
    "x1",
    "going",
    "x2",
    "let",
    "say",
    "write",
    "code",
    "index",
    "everything",
    "zero",
    "index",
    "x1",
    "everything",
    "first",
    "index",
    "go",
    "x2",
    "let",
    "clean",
    "code",
    "different",
    "lines",
    "enter",
    "got",
    "let",
    "put",
    "label",
    "dictionary",
    "x1",
    "key",
    "x0",
    "x2",
    "little",
    "bit",
    "confusing",
    "zero",
    "indexing",
    "x",
    "feature",
    "one",
    "x",
    "feature",
    "two",
    "label",
    "let",
    "see",
    "looks",
    "like",
    "look",
    "first",
    "10",
    "samples",
    "okay",
    "beautiful",
    "got",
    "x1",
    "numerical",
    "value",
    "x2",
    "another",
    "numerical",
    "value",
    "correlates",
    "matches",
    "label",
    "zero",
    "one",
    "0442208",
    "negative",
    "number",
    "matches",
    "label",
    "zero",
    "ca",
    "tell",
    "patterns",
    "looking",
    "numbers",
    "might",
    "able",
    "definitely",
    "ca",
    "got",
    "ones",
    "numbers",
    "look",
    "next",
    "well",
    "visualize",
    "visualize",
    "visualize",
    "instead",
    "numbers",
    "table",
    "let",
    "get",
    "graphical",
    "time",
    "visualize",
    "visualize",
    "visualize",
    "going",
    "bring",
    "friendly",
    "mapplotlib",
    "import",
    "mapplotlib",
    "powerful",
    "plotting",
    "library",
    "going",
    "add",
    "cells",
    "got",
    "space",
    "plt",
    "right",
    "got",
    "going",
    "scatterplot",
    "equals",
    "want",
    "first",
    "index",
    "going",
    "x",
    "well",
    "going",
    "appear",
    "axis",
    "want",
    "color",
    "labels",
    "going",
    "see",
    "looks",
    "like",
    "second",
    "color",
    "map",
    "c",
    "map",
    "stands",
    "color",
    "map",
    "going",
    "plot",
    "dot",
    "color",
    "map",
    "plt",
    "red",
    "yellow",
    "blue",
    "one",
    "favorite",
    "color",
    "outputs",
    "let",
    "see",
    "looks",
    "like",
    "ready",
    "ah",
    "go",
    "circles",
    "lot",
    "better",
    "think",
    "going",
    "try",
    "data",
    "working",
    "classification",
    "trying",
    "predict",
    "something",
    "one",
    "thing",
    "another",
    "problem",
    "want",
    "try",
    "separate",
    "two",
    "circles",
    "say",
    "given",
    "number",
    "given",
    "two",
    "numbers",
    "x",
    "one",
    "x",
    "two",
    "coordinates",
    "want",
    "predict",
    "label",
    "going",
    "blue",
    "dot",
    "going",
    "red",
    "dot",
    "working",
    "binary",
    "classification",
    "one",
    "thing",
    "another",
    "blue",
    "dot",
    "red",
    "dot",
    "going",
    "toy",
    "data",
    "toy",
    "problem",
    "let",
    "write",
    "common",
    "thing",
    "also",
    "hear",
    "machine",
    "learning",
    "note",
    "data",
    "working",
    "often",
    "referred",
    "toy",
    "data",
    "set",
    "data",
    "set",
    "small",
    "enough",
    "experiment",
    "still",
    "sizable",
    "enough",
    "practice",
    "fundamentals",
    "really",
    "notebook",
    "practice",
    "fundamentals",
    "neural",
    "network",
    "classification",
    "got",
    "perfect",
    "data",
    "set",
    "way",
    "got",
    "little",
    "function",
    "made",
    "samples",
    "us",
    "could",
    "find",
    "function",
    "well",
    "could",
    "go",
    "classification",
    "data",
    "sets",
    "actually",
    "could",
    "done",
    "like",
    "circle",
    "one",
    "toy",
    "data",
    "sets",
    "saw",
    "like",
    "toy",
    "box",
    "different",
    "data",
    "sets",
    "like",
    "learn",
    "data",
    "sets",
    "look",
    "potentially",
    "practice",
    "neural",
    "networks",
    "forms",
    "machine",
    "learning",
    "models",
    "check",
    "ca",
    "speak",
    "highly",
    "enough",
    "know",
    "course",
    "focused",
    "kind",
    "come",
    "together",
    "terms",
    "machine",
    "learning",
    "deep",
    "learning",
    "world",
    "might",
    "use",
    "something",
    "like",
    "done",
    "practice",
    "something",
    "might",
    "use",
    "something",
    "else",
    "like",
    "said",
    "input",
    "output",
    "shapes",
    "problem",
    "think",
    "also",
    "think",
    "split",
    "training",
    "test",
    "give",
    "go",
    "covered",
    "concepts",
    "previous",
    "videos",
    "together",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "made",
    "classification",
    "data",
    "practice",
    "building",
    "neural",
    "network",
    "separate",
    "blue",
    "dots",
    "red",
    "dots",
    "let",
    "keep",
    "pushing",
    "forward",
    "clean",
    "little",
    "bit",
    "workflow",
    "done",
    "far",
    "well",
    "got",
    "data",
    "ready",
    "little",
    "bit",
    "turned",
    "tenses",
    "let",
    "video",
    "keep",
    "pushing",
    "going",
    "make",
    "heading",
    "check",
    "input",
    "output",
    "shapes",
    "reason",
    "focused",
    "lot",
    "input",
    "output",
    "shapes",
    "machine",
    "learning",
    "deals",
    "lot",
    "numerical",
    "representations",
    "tenses",
    "input",
    "output",
    "shapes",
    "common",
    "errors",
    "like",
    "mismatch",
    "input",
    "output",
    "shapes",
    "certain",
    "layer",
    "output",
    "layer",
    "going",
    "run",
    "lot",
    "errors",
    "good",
    "get",
    "acquainted",
    "whatever",
    "data",
    "using",
    "input",
    "shapes",
    "output",
    "shapes",
    "like",
    "case",
    "go",
    "x",
    "dot",
    "shape",
    "dot",
    "shape",
    "working",
    "numpy",
    "arrays",
    "look",
    "make",
    "circles",
    "function",
    "created",
    "us",
    "got",
    "array",
    "workflow",
    "says",
    "like",
    "tenses",
    "working",
    "pytorch",
    "want",
    "data",
    "represented",
    "pytorch",
    "tenses",
    "data",
    "type",
    "got",
    "shape",
    "got",
    "thousand",
    "samples",
    "x",
    "two",
    "features",
    "features",
    "single",
    "number",
    "scalar",
    "shape",
    "thousand",
    "samples",
    "thousand",
    "samples",
    "x",
    "two",
    "samples",
    "x",
    "equals",
    "one",
    "label",
    "working",
    "larger",
    "problem",
    "might",
    "thousand",
    "samples",
    "x",
    "x",
    "represented",
    "128",
    "different",
    "numbers",
    "200",
    "numbers",
    "high",
    "want",
    "10",
    "something",
    "like",
    "keep",
    "mind",
    "number",
    "quite",
    "flexible",
    "many",
    "features",
    "represent",
    "label",
    "label",
    "let",
    "keep",
    "going",
    "view",
    "first",
    "example",
    "features",
    "labels",
    "let",
    "make",
    "explicit",
    "discussing",
    "write",
    "code",
    "get",
    "first",
    "sample",
    "x",
    "zero",
    "index",
    "get",
    "first",
    "sample",
    "also",
    "zero",
    "index",
    "could",
    "get",
    "really",
    "anyone",
    "shape",
    "print",
    "values",
    "one",
    "sample",
    "equal",
    "x",
    "sample",
    "sample",
    "want",
    "go",
    "print",
    "f",
    "string",
    "one",
    "sample",
    "get",
    "shape",
    "x",
    "sample",
    "dot",
    "shape",
    "get",
    "sample",
    "dot",
    "shape",
    "beautiful",
    "going",
    "well",
    "got",
    "one",
    "sample",
    "sample",
    "numbers",
    "got",
    "lot",
    "going",
    "75424625",
    "0231",
    "mean",
    "try",
    "find",
    "patterns",
    "best",
    "sample",
    "correlates",
    "number",
    "one",
    "label",
    "one",
    "shapes",
    "one",
    "sample",
    "x",
    "two",
    "two",
    "features",
    "little",
    "bit",
    "confusing",
    "scalar",
    "actually",
    "shape",
    "one",
    "value",
    "terms",
    "speaking",
    "teaching",
    "loud",
    "two",
    "features",
    "x",
    "trying",
    "predict",
    "one",
    "number",
    "let",
    "create",
    "another",
    "heading",
    "let",
    "get",
    "data",
    "tenses",
    "turn",
    "data",
    "tenses",
    "convert",
    "numpy",
    "also",
    "want",
    "create",
    "train",
    "test",
    "splits",
    "even",
    "though",
    "working",
    "toy",
    "data",
    "set",
    "principle",
    "turning",
    "data",
    "tenses",
    "creating",
    "train",
    "test",
    "splits",
    "stay",
    "around",
    "almost",
    "data",
    "set",
    "working",
    "let",
    "see",
    "want",
    "turn",
    "data",
    "tenses",
    "need",
    "import",
    "torch",
    "get",
    "pytorch",
    "check",
    "torch",
    "version",
    "least",
    "might",
    "put",
    "next",
    "cell",
    "make",
    "sure",
    "import",
    "pytorch",
    "go",
    "plus",
    "cuda",
    "version",
    "higher",
    "okay",
    "code",
    "still",
    "work",
    "let",
    "know",
    "x",
    "equals",
    "torch",
    "dot",
    "numpy",
    "well",
    "x",
    "numpy",
    "array",
    "go",
    "x",
    "dot",
    "type",
    "attribute",
    "float",
    "64",
    "go",
    "type",
    "maybe",
    "type",
    "oh",
    "go",
    "numpy",
    "dra",
    "go",
    "type",
    "numpy",
    "dra",
    "want",
    "torch",
    "tensor",
    "going",
    "go",
    "numpy",
    "saw",
    "fundamental",
    "section",
    "going",
    "change",
    "type",
    "torch",
    "dot",
    "float",
    "float",
    "alias",
    "float",
    "could",
    "type",
    "thing",
    "two",
    "equivalent",
    "going",
    "type",
    "torch",
    "float",
    "writing",
    "less",
    "code",
    "going",
    "go",
    "torch",
    "numpy",
    "turn",
    "torch",
    "float",
    "well",
    "recall",
    "default",
    "type",
    "numpy",
    "arrays",
    "go",
    "might",
    "put",
    "comma",
    "x",
    "dot",
    "type",
    "float",
    "go",
    "however",
    "pytorch",
    "default",
    "type",
    "float",
    "changing",
    "pytorch",
    "default",
    "type",
    "otherwise",
    "little",
    "section",
    "code",
    "dot",
    "type",
    "torch",
    "dot",
    "float",
    "tensors",
    "would",
    "float",
    "64",
    "well",
    "may",
    "cause",
    "errors",
    "later",
    "going",
    "default",
    "data",
    "type",
    "within",
    "pytorch",
    "let",
    "look",
    "first",
    "five",
    "values",
    "x",
    "first",
    "five",
    "values",
    "beautiful",
    "tensor",
    "data",
    "types",
    "check",
    "data",
    "type",
    "x",
    "check",
    "data",
    "type",
    "one",
    "go",
    "type",
    "data",
    "tensors",
    "wonderful",
    "torch",
    "dot",
    "tensor",
    "beautiful",
    "would",
    "like",
    "training",
    "test",
    "sets",
    "let",
    "go",
    "split",
    "data",
    "training",
    "test",
    "sets",
    "popular",
    "way",
    "split",
    "data",
    "random",
    "split",
    "issued",
    "challenge",
    "would",
    "split",
    "training",
    "test",
    "set",
    "data",
    "points",
    "kind",
    "scattered",
    "place",
    "could",
    "split",
    "randomly",
    "let",
    "see",
    "looks",
    "like",
    "going",
    "use",
    "faithful",
    "scikit",
    "learn",
    "remember",
    "said",
    "scikit",
    "learn",
    "lot",
    "beautiful",
    "methods",
    "functions",
    "whole",
    "bunch",
    "different",
    "machine",
    "learning",
    "purposes",
    "well",
    "one",
    "train",
    "test",
    "split",
    "oh",
    "goodness",
    "pytorch",
    "want",
    "auto",
    "correct",
    "train",
    "test",
    "split",
    "might",
    "able",
    "guess",
    "videos",
    "going",
    "battle",
    "code",
    "labs",
    "auto",
    "correct",
    "sometimes",
    "good",
    "times",
    "going",
    "set",
    "code",
    "going",
    "write",
    "going",
    "write",
    "together",
    "got",
    "x",
    "train",
    "training",
    "features",
    "x",
    "tests",
    "testing",
    "features",
    "also",
    "want",
    "training",
    "labels",
    "testing",
    "labels",
    "order",
    "order",
    "train",
    "test",
    "split",
    "works",
    "train",
    "test",
    "split",
    "wrote",
    "function",
    "wanted",
    "find",
    "press",
    "command",
    "ship",
    "space",
    "truly",
    "great",
    "time",
    "reading",
    "might",
    "like",
    "going",
    "train",
    "test",
    "split",
    "possibly",
    "one",
    "first",
    "functions",
    "appears",
    "yes",
    "scikit",
    "learn",
    "good",
    "scikit",
    "learn",
    "dot",
    "model",
    "selection",
    "dot",
    "train",
    "test",
    "split",
    "split",
    "arrays",
    "matrices",
    "random",
    "train",
    "test",
    "subsets",
    "beautiful",
    "got",
    "code",
    "example",
    "going",
    "read",
    "different",
    "parameters",
    "going",
    "see",
    "action",
    "another",
    "example",
    "machine",
    "learning",
    "libraries",
    "scikit",
    "learn",
    "used",
    "matplotlib",
    "used",
    "pandas",
    "interact",
    "together",
    "serve",
    "great",
    "purpose",
    "let",
    "pass",
    "features",
    "labels",
    "order",
    "come",
    "way",
    "oh",
    "returns",
    "splitting",
    "order",
    "got",
    "order",
    "goes",
    "x",
    "train",
    "x",
    "test",
    "train",
    "test",
    "took",
    "little",
    "remember",
    "order",
    "created",
    "enough",
    "training",
    "test",
    "splits",
    "function",
    "kind",
    "know",
    "heart",
    "remember",
    "features",
    "first",
    "train",
    "first",
    "labels",
    "jump",
    "back",
    "going",
    "put",
    "test",
    "size",
    "parameter",
    "percentage",
    "wise",
    "let",
    "write",
    "equals",
    "20",
    "data",
    "test",
    "80",
    "train",
    "wanted",
    "50",
    "50",
    "split",
    "kind",
    "split",
    "usually",
    "happen",
    "could",
    "go",
    "test",
    "size",
    "says",
    "hey",
    "big",
    "percentage",
    "wise",
    "want",
    "test",
    "data",
    "behind",
    "scenes",
    "train",
    "test",
    "split",
    "calculate",
    "20",
    "x",
    "samples",
    "see",
    "many",
    "second",
    "let",
    "also",
    "put",
    "random",
    "state",
    "recall",
    "back",
    "documentation",
    "train",
    "test",
    "split",
    "splits",
    "data",
    "randomly",
    "random",
    "train",
    "test",
    "subsets",
    "random",
    "state",
    "us",
    "well",
    "random",
    "seed",
    "equivalent",
    "similar",
    "torch",
    "dot",
    "manual",
    "seed",
    "however",
    "using",
    "scikit",
    "learn",
    "setting",
    "torch",
    "dot",
    "manual",
    "seed",
    "affect",
    "pytorch",
    "code",
    "rather",
    "scikit",
    "learn",
    "code",
    "get",
    "similar",
    "random",
    "splits",
    "get",
    "similar",
    "random",
    "split",
    "random",
    "split",
    "fact",
    "exactly",
    "let",
    "run",
    "check",
    "length",
    "x",
    "train",
    "length",
    "x",
    "test",
    "1000",
    "total",
    "samples",
    "know",
    "make",
    "circles",
    "function",
    "said",
    "want",
    "1000",
    "samples",
    "could",
    "could",
    "beauty",
    "creating",
    "data",
    "set",
    "length",
    "train",
    "20",
    "testing",
    "values",
    "many",
    "samples",
    "going",
    "dedicated",
    "test",
    "sample",
    "20",
    "1000",
    "years",
    "200",
    "80",
    "training",
    "going",
    "training",
    "100",
    "minus",
    "20",
    "80",
    "80",
    "1000",
    "years",
    "let",
    "find",
    "run",
    "beautiful",
    "800",
    "training",
    "samples",
    "200",
    "testing",
    "samples",
    "going",
    "data",
    "set",
    "going",
    "working",
    "next",
    "video",
    "got",
    "training",
    "test",
    "sets",
    "started",
    "move",
    "beautiful",
    "pytorch",
    "workflow",
    "got",
    "data",
    "ready",
    "turned",
    "tenses",
    "created",
    "training",
    "test",
    "split",
    "time",
    "build",
    "pick",
    "model",
    "think",
    "still",
    "building",
    "phase",
    "let",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "split",
    "data",
    "training",
    "test",
    "sets",
    "80",
    "20",
    "split",
    "got",
    "800",
    "samples",
    "train",
    "200",
    "samples",
    "test",
    "remember",
    "training",
    "set",
    "model",
    "learn",
    "patterns",
    "patterns",
    "represent",
    "data",
    "set",
    "circles",
    "data",
    "set",
    "red",
    "dots",
    "blue",
    "dots",
    "test",
    "data",
    "set",
    "evaluate",
    "patterns",
    "took",
    "little",
    "break",
    "tell",
    "notebook",
    "disconnected",
    "wanted",
    "reconnect",
    "could",
    "go",
    "run",
    "time",
    "run",
    "going",
    "run",
    "cells",
    "take",
    "long",
    "done",
    "large",
    "computations",
    "good",
    "timing",
    "part",
    "two",
    "building",
    "model",
    "fair",
    "steps",
    "nothing",
    "covered",
    "going",
    "break",
    "let",
    "build",
    "model",
    "classify",
    "blue",
    "red",
    "dots",
    "want",
    "tenses",
    "want",
    "tenses",
    "right",
    "let",
    "make",
    "space",
    "go",
    "number",
    "one",
    "let",
    "set",
    "device",
    "agnostic",
    "code",
    "get",
    "habit",
    "creating",
    "code",
    "run",
    "accelerator",
    "ca",
    "even",
    "spell",
    "accelerator",
    "matter",
    "know",
    "mean",
    "gpu",
    "one",
    "two",
    "next",
    "well",
    "construct",
    "model",
    "want",
    "build",
    "model",
    "need",
    "model",
    "construct",
    "model",
    "going",
    "go",
    "subclassing",
    "dot",
    "module",
    "saw",
    "previous",
    "section",
    "subclassed",
    "module",
    "fact",
    "models",
    "pytorch",
    "subclass",
    "end",
    "module",
    "let",
    "go",
    "define",
    "loss",
    "function",
    "optimizer",
    "finally",
    "good",
    "collabs",
    "auto",
    "correct",
    "ideal",
    "create",
    "training",
    "test",
    "loop",
    "though",
    "probably",
    "next",
    "section",
    "focus",
    "building",
    "model",
    "course",
    "steps",
    "line",
    "line",
    "device",
    "agnostic",
    "code",
    "going",
    "enough",
    "habit",
    "main",
    "steps",
    "pick",
    "build",
    "model",
    "suit",
    "problem",
    "pick",
    "loss",
    "function",
    "optimizer",
    "build",
    "training",
    "loop",
    "let",
    "look",
    "start",
    "import",
    "pytorch",
    "already",
    "done",
    "going",
    "anyway",
    "completeness",
    "case",
    "wanted",
    "run",
    "code",
    "import",
    "going",
    "make",
    "device",
    "agnostic",
    "code",
    "set",
    "device",
    "equal",
    "cuda",
    "torch",
    "dot",
    "cuda",
    "available",
    "else",
    "cpu",
    "default",
    "cpu",
    "default",
    "gpu",
    "means",
    "cuda",
    "available",
    "pytorch",
    "code",
    "default",
    "using",
    "cpu",
    "device",
    "set",
    "gpu",
    "yet",
    "far",
    "may",
    "see",
    "target",
    "device",
    "currently",
    "cpu",
    "set",
    "gpu",
    "go",
    "runtime",
    "change",
    "runtime",
    "type",
    "gpu",
    "going",
    "click",
    "save",
    "going",
    "restart",
    "runtime",
    "reconnect",
    "reconnects",
    "beautiful",
    "could",
    "actually",
    "run",
    "code",
    "cell",
    "going",
    "set",
    "gpu",
    "device",
    "running",
    "cell",
    "set",
    "x",
    "train",
    "defined",
    "restarted",
    "runtime",
    "let",
    "run",
    "run",
    "going",
    "rerun",
    "cells",
    "x",
    "train",
    "let",
    "look",
    "wonderful",
    "yes",
    "okay",
    "beautiful",
    "got",
    "device",
    "agnostic",
    "code",
    "next",
    "video",
    "let",
    "get",
    "constructing",
    "model",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "set",
    "device",
    "agnostic",
    "code",
    "going",
    "come",
    "later",
    "send",
    "model",
    "target",
    "device",
    "also",
    "data",
    "target",
    "device",
    "important",
    "step",
    "way",
    "someone",
    "else",
    "able",
    "run",
    "code",
    "run",
    "code",
    "future",
    "set",
    "device",
    "agnostic",
    "quite",
    "fault",
    "run",
    "cpu",
    "accelerator",
    "present",
    "well",
    "means",
    "might",
    "go",
    "faster",
    "using",
    "gpu",
    "rather",
    "using",
    "cpu",
    "step",
    "two",
    "construct",
    "model",
    "subclassing",
    "module",
    "think",
    "going",
    "write",
    "little",
    "bit",
    "text",
    "plan",
    "steps",
    "set",
    "device",
    "agnostic",
    "code",
    "let",
    "create",
    "model",
    "going",
    "break",
    "got",
    "sub",
    "steps",
    "going",
    "break",
    "even",
    "one",
    "steps",
    "number",
    "one",
    "going",
    "subclass",
    "got",
    "module",
    "reminder",
    "want",
    "make",
    "space",
    "coding",
    "middle",
    "page",
    "almost",
    "models",
    "pytorch",
    "subclass",
    "got",
    "module",
    "great",
    "things",
    "us",
    "behind",
    "scenes",
    "step",
    "two",
    "going",
    "create",
    "two",
    "dot",
    "linear",
    "layers",
    "want",
    "capable",
    "handle",
    "data",
    "capable",
    "handling",
    "shapes",
    "data",
    "step",
    "three",
    "want",
    "define",
    "defines",
    "forward",
    "method",
    "want",
    "define",
    "forward",
    "method",
    "well",
    "subclassing",
    "end",
    "dot",
    "module",
    "right",
    "forward",
    "method",
    "defines",
    "forward",
    "method",
    "outlines",
    "forward",
    "pass",
    "forward",
    "computation",
    "model",
    "number",
    "four",
    "want",
    "instantiate",
    "well",
    "really",
    "part",
    "creating",
    "going",
    "anyway",
    "instantiate",
    "instance",
    "model",
    "class",
    "send",
    "target",
    "device",
    "going",
    "couple",
    "little",
    "different",
    "steps",
    "nothing",
    "dramatic",
    "really",
    "covered",
    "let",
    "go",
    "number",
    "one",
    "construct",
    "model",
    "subclasses",
    "end",
    "dot",
    "module",
    "going",
    "code",
    "well",
    "going",
    "code",
    "together",
    "go",
    "back",
    "discuss",
    "maybe",
    "draw",
    "pictures",
    "something",
    "check",
    "actually",
    "happening",
    "circle",
    "model",
    "v",
    "one",
    "going",
    "try",
    "split",
    "circles",
    "red",
    "blue",
    "circles",
    "data",
    "called",
    "circle",
    "model",
    "trying",
    "separate",
    "blue",
    "red",
    "circle",
    "using",
    "neural",
    "network",
    "subclassed",
    "end",
    "dot",
    "module",
    "create",
    "class",
    "python",
    "create",
    "constructor",
    "net",
    "put",
    "super",
    "dot",
    "underscore",
    "net",
    "inside",
    "constructor",
    "going",
    "create",
    "layers",
    "number",
    "two",
    "create",
    "two",
    "linear",
    "layers",
    "capable",
    "handling",
    "shapes",
    "data",
    "going",
    "write",
    "create",
    "two",
    "two",
    "dot",
    "linear",
    "layers",
    "capable",
    "handling",
    "shapes",
    "data",
    "look",
    "x",
    "train",
    "shapes",
    "input",
    "shape",
    "x",
    "train",
    "features",
    "right",
    "features",
    "going",
    "go",
    "model",
    "800",
    "training",
    "samples",
    "first",
    "number",
    "size",
    "two",
    "800",
    "inside",
    "two",
    "numbers",
    "depending",
    "data",
    "set",
    "working",
    "features",
    "may",
    "100",
    "length",
    "vector",
    "100",
    "maybe",
    "different",
    "size",
    "tensor",
    "together",
    "may",
    "millions",
    "really",
    "depends",
    "data",
    "set",
    "working",
    "working",
    "simple",
    "data",
    "set",
    "going",
    "focus",
    "principal",
    "still",
    "need",
    "define",
    "neural",
    "network",
    "layer",
    "capable",
    "handling",
    "input",
    "features",
    "going",
    "make",
    "layer",
    "one",
    "equals",
    "n",
    "dot",
    "linear",
    "wanted",
    "find",
    "going",
    "n",
    "n",
    "dot",
    "linear",
    "could",
    "run",
    "shift",
    "command",
    "space",
    "computer",
    "mac",
    "maybe",
    "shift",
    "control",
    "space",
    "windows",
    "going",
    "define",
    "n",
    "features",
    "would",
    "n",
    "features",
    "well",
    "decided",
    "x",
    "two",
    "features",
    "n",
    "features",
    "going",
    "two",
    "features",
    "one",
    "little",
    "bit",
    "tricky",
    "case",
    "could",
    "features",
    "equal",
    "one",
    "wanted",
    "pass",
    "single",
    "linear",
    "layer",
    "want",
    "create",
    "two",
    "linear",
    "layers",
    "would",
    "features",
    "one",
    "well",
    "look",
    "first",
    "sample",
    "train",
    "would",
    "want",
    "us",
    "input",
    "maybe",
    "look",
    "first",
    "five",
    "want",
    "map",
    "one",
    "sample",
    "x",
    "one",
    "sample",
    "shape",
    "one",
    "oh",
    "well",
    "really",
    "nothing",
    "scalar",
    "would",
    "still",
    "put",
    "one",
    "outputs",
    "one",
    "number",
    "going",
    "change",
    "going",
    "put",
    "five",
    "going",
    "create",
    "second",
    "layer",
    "important",
    "point",
    "joining",
    "together",
    "neural",
    "networks",
    "features",
    "think",
    "features",
    "second",
    "layer",
    "going",
    "produced",
    "feature",
    "five",
    "number",
    "arbitrary",
    "could",
    "could",
    "generally",
    "multiples",
    "8",
    "five",
    "keeping",
    "nice",
    "simple",
    "could",
    "eight",
    "multiples",
    "eight",
    "efficiency",
    "computing",
    "know",
    "enough",
    "computer",
    "hardware",
    "know",
    "exactly",
    "case",
    "rule",
    "thumb",
    "machine",
    "learning",
    "features",
    "match",
    "features",
    "previous",
    "layer",
    "otherwise",
    "get",
    "shape",
    "mismatch",
    "errors",
    "let",
    "go",
    "features",
    "going",
    "treat",
    "output",
    "layer",
    "features",
    "equals",
    "one",
    "takes",
    "two",
    "features",
    "upscales",
    "five",
    "features",
    "five",
    "numbers",
    "layer",
    "going",
    "take",
    "two",
    "numbers",
    "x",
    "perform",
    "end",
    "linear",
    "let",
    "look",
    "equation",
    "end",
    "linear",
    "going",
    "perform",
    "function",
    "inputs",
    "going",
    "upscale",
    "five",
    "features",
    "would",
    "well",
    "rule",
    "thumb",
    "denoted",
    "hidden",
    "unit",
    "many",
    "hidden",
    "neurons",
    "rule",
    "thumb",
    "hidden",
    "features",
    "opportunity",
    "model",
    "learn",
    "patterns",
    "data",
    "begin",
    "two",
    "numbers",
    "learn",
    "patterns",
    "upscale",
    "five",
    "five",
    "numbers",
    "learn",
    "patterns",
    "might",
    "think",
    "go",
    "straight",
    "like",
    "something",
    "like",
    "upper",
    "limit",
    "sort",
    "benefits",
    "start",
    "trail",
    "using",
    "five",
    "keeps",
    "nice",
    "simple",
    "features",
    "next",
    "layer",
    "five",
    "two",
    "line",
    "going",
    "map",
    "visually",
    "moment",
    "let",
    "keep",
    "coding",
    "got",
    "features",
    "two",
    "output",
    "layer",
    "takes",
    "five",
    "features",
    "previous",
    "layer",
    "outputs",
    "single",
    "feature",
    "shape",
    "shape",
    "next",
    "step",
    "want",
    "define",
    "ford",
    "method",
    "ford",
    "computation",
    "ford",
    "pass",
    "ford",
    "method",
    "going",
    "define",
    "ford",
    "computation",
    "input",
    "going",
    "take",
    "x",
    "form",
    "data",
    "use",
    "layer",
    "one",
    "layer",
    "two",
    "let",
    "go",
    "return",
    "put",
    "note",
    "three",
    "going",
    "go",
    "define",
    "ford",
    "method",
    "outlines",
    "ford",
    "pass",
    "ford",
    "going",
    "return",
    "notation",
    "quite",
    "seen",
    "yet",
    "going",
    "go",
    "self",
    "layer",
    "two",
    "inside",
    "brackets",
    "self",
    "layer",
    "one",
    "inside",
    "brackets",
    "going",
    "way",
    "goes",
    "x",
    "goes",
    "layer",
    "one",
    "output",
    "layer",
    "one",
    "goes",
    "layer",
    "two",
    "whatever",
    "data",
    "training",
    "data",
    "x",
    "train",
    "goes",
    "layer",
    "one",
    "performs",
    "linear",
    "calculation",
    "goes",
    "layer",
    "two",
    "layer",
    "two",
    "going",
    "output",
    "go",
    "output",
    "x",
    "input",
    "layer",
    "one",
    "computation",
    "layer",
    "two",
    "output",
    "done",
    "let",
    "step",
    "four",
    "instantiate",
    "instance",
    "model",
    "class",
    "send",
    "target",
    "device",
    "model",
    "class",
    "circle",
    "model",
    "v",
    "zero",
    "going",
    "create",
    "model",
    "first",
    "model",
    "created",
    "section",
    "let",
    "call",
    "model",
    "zero",
    "going",
    "go",
    "circle",
    "model",
    "v",
    "one",
    "going",
    "go",
    "two",
    "going",
    "pass",
    "device",
    "target",
    "device",
    "let",
    "look",
    "model",
    "zero",
    "oh",
    "typo",
    "yeah",
    "classic",
    "get",
    "wrong",
    "oh",
    "pass",
    "self",
    "self",
    "oh",
    "go",
    "little",
    "typo",
    "classic",
    "beautiful",
    "thing",
    "creating",
    "class",
    "could",
    "put",
    "python",
    "file",
    "model",
    "dot",
    "pi",
    "would",
    "necessarily",
    "rewrite",
    "time",
    "could",
    "call",
    "let",
    "check",
    "vice",
    "target",
    "device",
    "cuda",
    "got",
    "gpu",
    "thank",
    "google",
    "colab",
    "wanted",
    "let",
    "go",
    "next",
    "model",
    "zero",
    "dot",
    "parameters",
    "call",
    "parameters",
    "go",
    "device",
    "cuda",
    "beautiful",
    "means",
    "models",
    "parameters",
    "cuda",
    "device",
    "covered",
    "enough",
    "code",
    "video",
    "want",
    "understand",
    "little",
    "bit",
    "go",
    "back",
    "going",
    "come",
    "back",
    "next",
    "video",
    "make",
    "little",
    "bit",
    "visual",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "something",
    "exciting",
    "created",
    "first",
    "multi",
    "layer",
    "neural",
    "network",
    "right",
    "code",
    "page",
    "truly",
    "majority",
    "building",
    "machine",
    "learning",
    "models",
    "pytorch",
    "going",
    "look",
    "like",
    "going",
    "create",
    "layers",
    "simple",
    "complex",
    "like",
    "going",
    "use",
    "layers",
    "form",
    "ford",
    "computation",
    "create",
    "forward",
    "pass",
    "let",
    "make",
    "little",
    "bit",
    "visual",
    "go",
    "tensorflow",
    "playground",
    "tensorflow",
    "another",
    "deep",
    "learning",
    "framework",
    "similar",
    "pytorch",
    "allows",
    "write",
    "code",
    "build",
    "neural",
    "networks",
    "fit",
    "sort",
    "data",
    "find",
    "patterns",
    "data",
    "use",
    "machine",
    "learning",
    "models",
    "applications",
    "let",
    "create",
    "oh",
    "way",
    "neural",
    "network",
    "train",
    "browser",
    "really",
    "wanted",
    "pretty",
    "darn",
    "cool",
    "got",
    "data",
    "set",
    "kind",
    "similar",
    "data",
    "set",
    "working",
    "look",
    "circles",
    "one",
    "let",
    "say",
    "close",
    "enough",
    "circular",
    "increase",
    "got",
    "five",
    "neurons",
    "got",
    "two",
    "features",
    "x1",
    "x2",
    "reminding",
    "happening",
    "lot",
    "things",
    "going",
    "covered",
    "yet",
    "worry",
    "much",
    "focused",
    "neural",
    "network",
    "got",
    "features",
    "input",
    "got",
    "five",
    "hidden",
    "units",
    "exactly",
    "going",
    "model",
    "built",
    "pass",
    "x1",
    "x2",
    "values",
    "go",
    "back",
    "data",
    "set",
    "x1",
    "x2",
    "pass",
    "got",
    "two",
    "input",
    "features",
    "pass",
    "hidden",
    "layer",
    "single",
    "hidden",
    "layer",
    "five",
    "neurons",
    "built",
    "come",
    "model",
    "got",
    "features",
    "two",
    "features",
    "five",
    "feeds",
    "another",
    "layer",
    "features",
    "five",
    "features",
    "one",
    "exact",
    "model",
    "built",
    "turn",
    "back",
    "linear",
    "activation",
    "sticking",
    "linear",
    "look",
    "different",
    "forms",
    "activation",
    "functions",
    "later",
    "maybe",
    "put",
    "learning",
    "rate",
    "seen",
    "learning",
    "rate",
    "got",
    "epochs",
    "got",
    "classification",
    "going",
    "try",
    "fit",
    "neural",
    "network",
    "data",
    "let",
    "see",
    "happens",
    "oh",
    "test",
    "loss",
    "sitting",
    "halfway",
    "50",
    "loss",
    "two",
    "classes",
    "got",
    "loss",
    "50",
    "mean",
    "well",
    "perfect",
    "loss",
    "zero",
    "worst",
    "loss",
    "one",
    "divide",
    "one",
    "two",
    "get",
    "50",
    "got",
    "two",
    "classes",
    "means",
    "model",
    "randomly",
    "guessing",
    "would",
    "get",
    "loss",
    "could",
    "randomly",
    "guess",
    "whatever",
    "data",
    "point",
    "belongs",
    "blue",
    "orange",
    "case",
    "binary",
    "classification",
    "problem",
    "number",
    "samples",
    "class",
    "case",
    "blue",
    "dots",
    "orange",
    "dots",
    "randomly",
    "guessing",
    "get",
    "50",
    "like",
    "tossing",
    "coin",
    "toss",
    "coin",
    "100",
    "times",
    "get",
    "50",
    "50",
    "might",
    "little",
    "bit",
    "different",
    "around",
    "long",
    "term",
    "fit",
    "3000",
    "epochs",
    "still",
    "getting",
    "better",
    "loss",
    "hmm",
    "wonder",
    "going",
    "case",
    "neural",
    "network",
    "draw",
    "different",
    "way",
    "going",
    "come",
    "little",
    "tool",
    "called",
    "fig",
    "jam",
    "whiteboard",
    "put",
    "shapes",
    "based",
    "browser",
    "going",
    "nothing",
    "fancy",
    "going",
    "simple",
    "diagram",
    "say",
    "input",
    "going",
    "make",
    "green",
    "favorite",
    "color",
    "green",
    "going",
    "let",
    "make",
    "different",
    "colored",
    "dots",
    "want",
    "blue",
    "dot",
    "dot",
    "one",
    "dot",
    "two",
    "put",
    "another",
    "dot",
    "zoom",
    "little",
    "bit",
    "space",
    "well",
    "maybe",
    "much",
    "50",
    "looks",
    "right",
    "let",
    "move",
    "around",
    "move",
    "little",
    "building",
    "neural",
    "network",
    "exactly",
    "built",
    "go",
    "well",
    "maybe",
    "put",
    "input",
    "x",
    "one",
    "make",
    "little",
    "bit",
    "sense",
    "maybe",
    "copy",
    "x",
    "two",
    "form",
    "output",
    "let",
    "make",
    "one",
    "going",
    "color",
    "orange",
    "output",
    "right",
    "imagine",
    "got",
    "connected",
    "dots",
    "connect",
    "inputs",
    "going",
    "go",
    "wonder",
    "draw",
    "okay",
    "going",
    "little",
    "bit",
    "complex",
    "right",
    "done",
    "got",
    "two",
    "input",
    "features",
    "wanted",
    "keep",
    "drawing",
    "could",
    "input",
    "features",
    "going",
    "go",
    "hidden",
    "units",
    "drew",
    "arrow",
    "twice",
    "okay",
    "happening",
    "forward",
    "computation",
    "method",
    "little",
    "bit",
    "confusing",
    "coded",
    "well",
    "looks",
    "like",
    "got",
    "input",
    "layer",
    "single",
    "hidden",
    "layer",
    "blue",
    "output",
    "layer",
    "truly",
    "exact",
    "shape",
    "get",
    "point",
    "go",
    "output",
    "going",
    "see",
    "computationally",
    "later",
    "whatever",
    "data",
    "set",
    "working",
    "going",
    "manufacture",
    "form",
    "input",
    "layer",
    "may",
    "might",
    "10",
    "10",
    "features",
    "four",
    "four",
    "features",
    "wanted",
    "adjust",
    "well",
    "could",
    "increase",
    "number",
    "hidden",
    "units",
    "number",
    "features",
    "layer",
    "match",
    "layer",
    "going",
    "similar",
    "shape",
    "coming",
    "keep",
    "mind",
    "going",
    "case",
    "one",
    "output",
    "output",
    "visual",
    "version",
    "got",
    "tensorflow",
    "playground",
    "could",
    "play",
    "around",
    "change",
    "increase",
    "maybe",
    "want",
    "five",
    "hidden",
    "layers",
    "five",
    "neurons",
    "fun",
    "way",
    "explore",
    "challenge",
    "actually",
    "go",
    "replicate",
    "network",
    "see",
    "fits",
    "type",
    "data",
    "think",
    "well",
    "going",
    "find",
    "next",
    "videos",
    "going",
    "show",
    "next",
    "video",
    "another",
    "way",
    "create",
    "network",
    "created",
    "one",
    "even",
    "less",
    "code",
    "done",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "discussed",
    "well",
    "actually",
    "previous",
    "video",
    "last",
    "coded",
    "neural",
    "network",
    "circle",
    "model",
    "v",
    "zero",
    "subclassing",
    "end",
    "module",
    "created",
    "two",
    "linear",
    "layers",
    "capable",
    "handling",
    "shape",
    "data",
    "features",
    "two",
    "two",
    "x",
    "features",
    "features",
    "upscaling",
    "two",
    "features",
    "five",
    "gives",
    "network",
    "chance",
    "learn",
    "upscaled",
    "five",
    "features",
    "next",
    "subsequent",
    "layer",
    "able",
    "handle",
    "five",
    "features",
    "input",
    "one",
    "output",
    "feature",
    "shape",
    "got",
    "little",
    "bit",
    "visual",
    "using",
    "tensorflow",
    "playground",
    "try",
    "challenge",
    "make",
    "five",
    "layers",
    "five",
    "neurons",
    "work",
    "also",
    "got",
    "little",
    "bit",
    "visual",
    "figma",
    "well",
    "another",
    "way",
    "visualizing",
    "different",
    "things",
    "might",
    "fair",
    "times",
    "first",
    "start",
    "neural",
    "networks",
    "get",
    "bit",
    "practice",
    "start",
    "infer",
    "going",
    "pure",
    "code",
    "let",
    "keep",
    "pushing",
    "forward",
    "replicate",
    "simpler",
    "way",
    "network",
    "quite",
    "simple",
    "means",
    "two",
    "layers",
    "means",
    "use",
    "let",
    "replicate",
    "model",
    "using",
    "going",
    "code",
    "look",
    "think",
    "able",
    "comprehend",
    "happening",
    "looking",
    "nn",
    "already",
    "imported",
    "nn",
    "going",
    "call",
    "going",
    "go",
    "features",
    "well",
    "two",
    "two",
    "features",
    "going",
    "replicate",
    "features",
    "remember",
    "could",
    "customize",
    "whatever",
    "want",
    "10",
    "100",
    "going",
    "keep",
    "five",
    "nice",
    "simple",
    "go",
    "features",
    "next",
    "layer",
    "going",
    "five",
    "features",
    "previous",
    "layer",
    "five",
    "finally",
    "features",
    "going",
    "one",
    "want",
    "one",
    "value",
    "two",
    "x",
    "features",
    "going",
    "send",
    "device",
    "going",
    "look",
    "model",
    "zero",
    "course",
    "going",
    "override",
    "previous",
    "model",
    "zero",
    "look",
    "thing",
    "different",
    "circle",
    "model",
    "v",
    "zero",
    "class",
    "subclassed",
    "n",
    "dot",
    "module",
    "difference",
    "name",
    "sequential",
    "see",
    "going",
    "might",
    "guessed",
    "sequential",
    "implements",
    "code",
    "us",
    "behind",
    "scenes",
    "told",
    "going",
    "sequential",
    "going",
    "go",
    "hey",
    "step",
    "code",
    "layer",
    "step",
    "code",
    "layer",
    "outputs",
    "basically",
    "model",
    "rather",
    "us",
    "creating",
    "forward",
    "method",
    "might",
    "thinking",
    "daniel",
    "show",
    "us",
    "earlier",
    "looks",
    "like",
    "easy",
    "way",
    "create",
    "neural",
    "network",
    "compared",
    "well",
    "yes",
    "100",
    "right",
    "easier",
    "way",
    "create",
    "neural",
    "network",
    "however",
    "benefit",
    "subclassing",
    "started",
    "complex",
    "operations",
    "things",
    "like",
    "construct",
    "complex",
    "forward",
    "pass",
    "important",
    "know",
    "build",
    "subclasses",
    "nn",
    "dot",
    "module",
    "simple",
    "straightforward",
    "stepping",
    "layer",
    "one",
    "one",
    "layer",
    "first",
    "layer",
    "use",
    "nn",
    "dot",
    "sequential",
    "fact",
    "could",
    "move",
    "code",
    "could",
    "self",
    "dot",
    "call",
    "two",
    "linear",
    "layers",
    "equals",
    "nn",
    "dot",
    "sequential",
    "could",
    "layer",
    "one",
    "could",
    "go",
    "self",
    "self",
    "dot",
    "layer",
    "one",
    "actually",
    "recode",
    "go",
    "nn",
    "dot",
    "linear",
    "code",
    "got",
    "features",
    "could",
    "type",
    "great",
    "n",
    "features",
    "equals",
    "two",
    "features",
    "equals",
    "five",
    "go",
    "nn",
    "dot",
    "linear",
    "go",
    "n",
    "features",
    "equals",
    "equals",
    "five",
    "line",
    "features",
    "equals",
    "one",
    "got",
    "two",
    "linear",
    "layers",
    "wanted",
    "get",
    "rid",
    "return",
    "linear",
    "layers",
    "pass",
    "x",
    "remake",
    "go",
    "well",
    "created",
    "well",
    "let",
    "get",
    "rid",
    "beautiful",
    "exact",
    "model",
    "using",
    "nn",
    "dot",
    "sequential",
    "going",
    "get",
    "rid",
    "code",
    "verbose",
    "means",
    "lot",
    "going",
    "flexibility",
    "pytorch",
    "keep",
    "mind",
    "fair",
    "ways",
    "make",
    "model",
    "simplest",
    "probably",
    "sequential",
    "subclassing",
    "little",
    "bit",
    "complicated",
    "got",
    "extend",
    "handle",
    "lot",
    "complex",
    "neural",
    "networks",
    "likely",
    "building",
    "later",
    "let",
    "keep",
    "pushing",
    "forward",
    "let",
    "see",
    "happens",
    "pass",
    "data",
    "rerun",
    "cell",
    "make",
    "sure",
    "got",
    "model",
    "zero",
    "instantiated",
    "make",
    "predictions",
    "model",
    "course",
    "look",
    "model",
    "zero",
    "dot",
    "state",
    "deck",
    "oh",
    "good",
    "experiment",
    "look",
    "weight",
    "weight",
    "tensor",
    "bias",
    "tensor",
    "weight",
    "tensor",
    "bias",
    "tensor",
    "first",
    "zeroth",
    "layer",
    "two",
    "zero",
    "dot",
    "one",
    "dot",
    "weight",
    "four",
    "course",
    "first",
    "index",
    "layer",
    "look",
    "inside",
    "see",
    "features",
    "five",
    "well",
    "bias",
    "parameter",
    "five",
    "values",
    "thing",
    "weight",
    "value",
    "weight",
    "value",
    "10",
    "samples",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "nine",
    "10",
    "two",
    "times",
    "five",
    "equals",
    "simple",
    "two",
    "layer",
    "network",
    "look",
    "numbers",
    "going",
    "behind",
    "scenes",
    "imagine",
    "coding",
    "hand",
    "like",
    "something",
    "like",
    "20",
    "numbers",
    "something",
    "done",
    "two",
    "layers",
    "beauty",
    "previous",
    "section",
    "created",
    "weight",
    "biases",
    "using",
    "end",
    "dot",
    "parameter",
    "random",
    "values",
    "notice",
    "random",
    "two",
    "different",
    "mine",
    "worry",
    "much",
    "going",
    "started",
    "randomly",
    "set",
    "random",
    "seed",
    "thing",
    "note",
    "pytorch",
    "creating",
    "parameters",
    "us",
    "behind",
    "scenes",
    "back",
    "propagation",
    "gradient",
    "descent",
    "code",
    "training",
    "loop",
    "optimizer",
    "going",
    "change",
    "values",
    "ever",
    "slightly",
    "try",
    "better",
    "fit",
    "better",
    "represent",
    "data",
    "split",
    "two",
    "circles",
    "imagine",
    "verbose",
    "could",
    "get",
    "say",
    "50",
    "layers",
    "128",
    "different",
    "features",
    "let",
    "change",
    "see",
    "happens",
    "watch",
    "quickly",
    "numbers",
    "get",
    "hand",
    "look",
    "changed",
    "one",
    "value",
    "look",
    "many",
    "parameters",
    "model",
    "might",
    "able",
    "calculate",
    "hand",
    "personally",
    "want",
    "going",
    "let",
    "pytorch",
    "take",
    "care",
    "lot",
    "us",
    "behind",
    "scenes",
    "keeping",
    "simple",
    "crack",
    "models",
    "open",
    "look",
    "going",
    "little",
    "detour",
    "time",
    "make",
    "predictions",
    "random",
    "numbers",
    "wanted",
    "highlight",
    "fact",
    "model",
    "fact",
    "instantiated",
    "random",
    "numbers",
    "untrained",
    "threads",
    "model",
    "zero",
    "going",
    "pass",
    "x",
    "test",
    "course",
    "send",
    "test",
    "data",
    "device",
    "otherwise",
    "different",
    "device",
    "get",
    "errors",
    "pytorch",
    "likes",
    "make",
    "calculations",
    "device",
    "go",
    "print",
    "let",
    "nice",
    "print",
    "statement",
    "length",
    "predictions",
    "going",
    "go",
    "length",
    "untrained",
    "threads",
    "pass",
    "go",
    "oh",
    "need",
    "squiggle",
    "go",
    "shape",
    "shape",
    "going",
    "untrained",
    "spreads",
    "dot",
    "shape",
    "following",
    "data",
    "explorer",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "sometimes",
    "print",
    "one",
    "best",
    "ones",
    "length",
    "test",
    "samples",
    "might",
    "already",
    "know",
    "already",
    "checked",
    "together",
    "x",
    "test",
    "going",
    "get",
    "shape",
    "going",
    "x",
    "test",
    "dot",
    "shape",
    "wonderful",
    "going",
    "print",
    "little",
    "error",
    "oh",
    "collabs",
    "tricking",
    "let",
    "go",
    "first",
    "10",
    "predictions",
    "going",
    "go",
    "untrained",
    "threads",
    "think",
    "predictions",
    "fare",
    "random",
    "numbers",
    "trying",
    "predict",
    "well",
    "trying",
    "predict",
    "whether",
    "dot",
    "red",
    "dot",
    "blue",
    "dot",
    "zero",
    "one",
    "go",
    "first",
    "10",
    "labels",
    "going",
    "get",
    "next",
    "line",
    "go",
    "test",
    "beautiful",
    "let",
    "look",
    "untrained",
    "predictions",
    "length",
    "predictions",
    "length",
    "test",
    "samples",
    "shapes",
    "different",
    "going",
    "test",
    "let",
    "look",
    "x",
    "test",
    "oh",
    "well",
    "better",
    "look",
    "test",
    "two",
    "oh",
    "done",
    "x",
    "test",
    "dot",
    "shape",
    "oh",
    "let",
    "test",
    "samples",
    "okay",
    "predictions",
    "one",
    "oh",
    "yes",
    "test",
    "let",
    "check",
    "first",
    "10",
    "x",
    "test",
    "little",
    "bit",
    "clarification",
    "needed",
    "shapes",
    "maybe",
    "get",
    "like",
    "features",
    "first",
    "labels",
    "miss",
    "oh",
    "x",
    "test",
    "10",
    "test",
    "see",
    "troubleshooting",
    "fly",
    "going",
    "lot",
    "code",
    "test",
    "values",
    "ideal",
    "labels",
    "predictions",
    "look",
    "like",
    "labels",
    "going",
    "see",
    "cuda",
    "device",
    "good",
    "said",
    "see",
    "got",
    "gradient",
    "tracking",
    "oh",
    "touch",
    "inference",
    "mode",
    "poor",
    "habit",
    "us",
    "excuse",
    "let",
    "inference",
    "mode",
    "go",
    "notice",
    "gradient",
    "tracking",
    "goes",
    "away",
    "predictions",
    "nowhere",
    "near",
    "test",
    "labels",
    "also",
    "even",
    "like",
    "ball",
    "park",
    "like",
    "whole",
    "numbers",
    "one",
    "zero",
    "floats",
    "one",
    "zero",
    "hmm",
    "maybe",
    "rounding",
    "something",
    "threads",
    "go",
    "torch",
    "dot",
    "round",
    "happens",
    "oh",
    "zero",
    "well",
    "model",
    "probably",
    "going",
    "get",
    "50",
    "accuracy",
    "predictions",
    "look",
    "like",
    "going",
    "zero",
    "got",
    "two",
    "options",
    "basically",
    "head",
    "tails",
    "create",
    "model",
    "evaluate",
    "want",
    "predictions",
    "format",
    "labels",
    "going",
    "cover",
    "steps",
    "take",
    "second",
    "important",
    "take",
    "away",
    "another",
    "way",
    "replicate",
    "model",
    "made",
    "using",
    "nn",
    "dot",
    "sequential",
    "replicated",
    "model",
    "got",
    "n",
    "dot",
    "sequential",
    "simpler",
    "way",
    "creating",
    "pytorch",
    "model",
    "limited",
    "literally",
    "sequentially",
    "steps",
    "layer",
    "order",
    "whereas",
    "get",
    "creative",
    "want",
    "forward",
    "computation",
    "inside",
    "model",
    "pytorch",
    "behind",
    "scenes",
    "created",
    "us",
    "weight",
    "bias",
    "tensors",
    "layers",
    "regards",
    "shapes",
    "set",
    "handy",
    "thing",
    "got",
    "quite",
    "ridiculous",
    "layers",
    "pytorch",
    "would",
    "still",
    "thing",
    "behind",
    "scenes",
    "create",
    "whole",
    "bunch",
    "random",
    "numbers",
    "us",
    "numbers",
    "random",
    "looks",
    "like",
    "model",
    "making",
    "good",
    "predictions",
    "going",
    "fix",
    "next",
    "videos",
    "move",
    "fitting",
    "model",
    "data",
    "making",
    "prediction",
    "need",
    "pick",
    "loss",
    "function",
    "optimizer",
    "build",
    "training",
    "loop",
    "let",
    "get",
    "two",
    "things",
    "welcome",
    "back",
    "past",
    "videos",
    "setting",
    "classification",
    "model",
    "deal",
    "specific",
    "shape",
    "data",
    "recall",
    "depending",
    "data",
    "set",
    "working",
    "depend",
    "layers",
    "use",
    "keeping",
    "simple",
    "n",
    "dot",
    "linear",
    "one",
    "simple",
    "layers",
    "pytorch",
    "got",
    "two",
    "input",
    "features",
    "upscaling",
    "five",
    "output",
    "features",
    "five",
    "hidden",
    "units",
    "one",
    "output",
    "feature",
    "line",
    "shape",
    "data",
    "two",
    "features",
    "x",
    "equals",
    "one",
    "number",
    "let",
    "continue",
    "modeling",
    "build",
    "pick",
    "model",
    "built",
    "model",
    "need",
    "pick",
    "loss",
    "function",
    "optimizer",
    "getting",
    "good",
    "let",
    "go",
    "set",
    "loss",
    "function",
    "optimizer",
    "comes",
    "question",
    "working",
    "classification",
    "previously",
    "used",
    "let",
    "go",
    "next",
    "one",
    "dot",
    "l",
    "one",
    "loss",
    "regression",
    "mae",
    "mean",
    "absolute",
    "error",
    "heads",
    "wo",
    "necessarily",
    "work",
    "classification",
    "problem",
    "loss",
    "function",
    "optimizer",
    "use",
    "problem",
    "specific",
    "little",
    "bit",
    "practice",
    "get",
    "used",
    "using",
    "different",
    "ones",
    "example",
    "regression",
    "might",
    "want",
    "regressions",
    "predicting",
    "number",
    "know",
    "get",
    "fusing",
    "looks",
    "like",
    "predicting",
    "number",
    "essentially",
    "predicting",
    "number",
    "relates",
    "class",
    "regression",
    "might",
    "want",
    "mae",
    "mse",
    "mean",
    "absolute",
    "absolute",
    "error",
    "mean",
    "squared",
    "error",
    "classification",
    "might",
    "want",
    "binary",
    "cross",
    "entropy",
    "categorical",
    "cross",
    "entropy",
    "sometimes",
    "referred",
    "cross",
    "entropy",
    "would",
    "find",
    "things",
    "well",
    "internet",
    "course",
    "could",
    "go",
    "binary",
    "cross",
    "entropy",
    "going",
    "leave",
    "extra",
    "curriculum",
    "read",
    "got",
    "fair",
    "resources",
    "understanding",
    "binary",
    "cross",
    "entropy",
    "slash",
    "log",
    "loss",
    "daniel",
    "godoy",
    "oh",
    "yes",
    "great",
    "first",
    "name",
    "friend",
    "actually",
    "article",
    "would",
    "recommend",
    "want",
    "learn",
    "going",
    "behind",
    "scenes",
    "binary",
    "cross",
    "entropy",
    "lot",
    "math",
    "going",
    "writing",
    "code",
    "implement",
    "pytorch",
    "done",
    "us",
    "essentially",
    "loss",
    "function",
    "let",
    "remind",
    "go",
    "reminder",
    "loss",
    "function",
    "measures",
    "wrong",
    "models",
    "predictions",
    "also",
    "going",
    "leave",
    "reference",
    "got",
    "little",
    "table",
    "book",
    "version",
    "course",
    "neural",
    "network",
    "classification",
    "pytorch",
    "set",
    "loss",
    "function",
    "optimizer",
    "got",
    "example",
    "loss",
    "functions",
    "optimizers",
    "stochastic",
    "gradient",
    "descent",
    "sgd",
    "optimizer",
    "atom",
    "optimizer",
    "also",
    "popular",
    "got",
    "problem",
    "type",
    "pytorch",
    "code",
    "implement",
    "got",
    "binary",
    "cross",
    "entropy",
    "loss",
    "got",
    "cross",
    "entropy",
    "loss",
    "mean",
    "absolute",
    "error",
    "mae",
    "mean",
    "squared",
    "error",
    "mse",
    "want",
    "use",
    "two",
    "regression",
    "different",
    "loss",
    "functions",
    "could",
    "use",
    "common",
    "focusing",
    "common",
    "ones",
    "going",
    "get",
    "fair",
    "problems",
    "got",
    "binary",
    "classification",
    "classification",
    "working",
    "working",
    "binary",
    "classification",
    "going",
    "look",
    "bce",
    "stands",
    "binary",
    "cross",
    "entropy",
    "loss",
    "logits",
    "hell",
    "logit",
    "bce",
    "loss",
    "confusing",
    "trust",
    "first",
    "started",
    "using",
    "pytorch",
    "got",
    "little",
    "bit",
    "confused",
    "two",
    "going",
    "explore",
    "anyway",
    "logit",
    "search",
    "logit",
    "get",
    "get",
    "statistics",
    "get",
    "log",
    "odds",
    "formula",
    "fact",
    "want",
    "read",
    "would",
    "highly",
    "encourage",
    "could",
    "go",
    "going",
    "practice",
    "writing",
    "code",
    "instead",
    "luckily",
    "pytorch",
    "us",
    "logit",
    "kind",
    "confusing",
    "deep",
    "learning",
    "go",
    "logit",
    "deep",
    "learning",
    "kind",
    "means",
    "different",
    "thing",
    "kind",
    "name",
    "yeah",
    "go",
    "word",
    "logits",
    "tensorflow",
    "said",
    "tensorflow",
    "another",
    "deep",
    "learning",
    "framework",
    "let",
    "close",
    "got",
    "got",
    "whole",
    "bunch",
    "definitions",
    "logits",
    "layer",
    "yeah",
    "one",
    "favorites",
    "context",
    "deep",
    "learning",
    "logits",
    "layer",
    "means",
    "layer",
    "feeds",
    "softmax",
    "softmax",
    "form",
    "activation",
    "going",
    "see",
    "later",
    "words",
    "page",
    "right",
    "softmax",
    "normalization",
    "output",
    "softmax",
    "probabilities",
    "classification",
    "task",
    "input",
    "logit",
    "layer",
    "whoa",
    "lot",
    "going",
    "let",
    "take",
    "step",
    "back",
    "get",
    "writing",
    "code",
    "optimizers",
    "going",
    "complete",
    "optimizers",
    "two",
    "common",
    "useful",
    "sgd",
    "adam",
    "however",
    "pytorch",
    "many",
    "built",
    "options",
    "start",
    "learn",
    "world",
    "machine",
    "learning",
    "find",
    "go",
    "loss",
    "functions",
    "go",
    "beautiful",
    "l1",
    "loss",
    "mae",
    "msc",
    "loss",
    "cross",
    "entropy",
    "loss",
    "ctc",
    "loss",
    "different",
    "types",
    "loss",
    "depend",
    "problem",
    "working",
    "tell",
    "regression",
    "classification",
    "two",
    "common",
    "see",
    "confusion",
    "bce",
    "loss",
    "bce",
    "logit",
    "loss",
    "hell",
    "logit",
    "goodness",
    "okay",
    "enough",
    "optim",
    "different",
    "optimizers",
    "got",
    "probably",
    "dozen",
    "algorithms",
    "add",
    "delta",
    "add",
    "grad",
    "adam",
    "pretty",
    "full",
    "first",
    "get",
    "started",
    "stick",
    "sgd",
    "atom",
    "optimizer",
    "two",
    "common",
    "may",
    "perform",
    "best",
    "every",
    "single",
    "problem",
    "get",
    "fairly",
    "far",
    "knowing",
    "pick",
    "extra",
    "ones",
    "go",
    "let",
    "get",
    "rid",
    "maybe",
    "put",
    "link",
    "create",
    "loss",
    "function",
    "loss",
    "function",
    "going",
    "use",
    "logit",
    "loss",
    "exciting",
    "binary",
    "cross",
    "entropy",
    "bce",
    "lot",
    "abbreviations",
    "machine",
    "learning",
    "deep",
    "learning",
    "check",
    "article",
    "definition",
    "logit",
    "going",
    "see",
    "logit",
    "second",
    "deep",
    "learning",
    "deep",
    "learning",
    "one",
    "fields",
    "machine",
    "learning",
    "likes",
    "bit",
    "rebellious",
    "know",
    "likes",
    "bit",
    "different",
    "pure",
    "mathematics",
    "type",
    "fields",
    "statistics",
    "general",
    "beautiful",
    "gestaltism",
    "different",
    "optimizers",
    "see",
    "torch",
    "dot",
    "opt",
    "covered",
    "things",
    "finally",
    "going",
    "put",
    "common",
    "choices",
    "loss",
    "functions",
    "optimizers",
    "worry",
    "much",
    "linking",
    "extra",
    "resources",
    "lot",
    "covered",
    "book",
    "said",
    "set",
    "loss",
    "function",
    "optimizer",
    "talked",
    "things",
    "mean",
    "go",
    "book",
    "website",
    "reference",
    "oh",
    "want",
    "want",
    "link",
    "come",
    "knew",
    "ca",
    "even",
    "copy",
    "paste",
    "supposed",
    "code",
    "know",
    "promising",
    "code",
    "whole",
    "time",
    "let",
    "write",
    "let",
    "set",
    "loss",
    "function",
    "say",
    "going",
    "call",
    "l",
    "double",
    "f",
    "n",
    "loss",
    "function",
    "going",
    "call",
    "b",
    "c",
    "e",
    "logit",
    "loss",
    "b",
    "c",
    "e",
    "logit",
    "loss",
    "sigmoid",
    "activation",
    "function",
    "built",
    "covered",
    "sigmoid",
    "activation",
    "function",
    "going",
    "worry",
    "built",
    "fact",
    "wanted",
    "learn",
    "sigmoid",
    "activation",
    "function",
    "could",
    "find",
    "sigmoid",
    "activation",
    "function",
    "going",
    "see",
    "action",
    "activation",
    "functions",
    "neural",
    "networks",
    "beautiful",
    "thing",
    "machine",
    "learning",
    "much",
    "stuff",
    "people",
    "written",
    "great",
    "articles",
    "got",
    "formulas",
    "pytorch",
    "implemented",
    "behind",
    "scenes",
    "us",
    "thank",
    "pytorch",
    "recall",
    "sigmoid",
    "activation",
    "function",
    "built",
    "discuss",
    "architecture",
    "classification",
    "network",
    "right",
    "back",
    "zeroth",
    "chapter",
    "little",
    "online",
    "book",
    "thing",
    "heard",
    "binary",
    "classification",
    "output",
    "activation",
    "oh",
    "oh",
    "look",
    "sigmoid",
    "torch",
    "dot",
    "sigmoid",
    "pytorch",
    "right",
    "multi",
    "class",
    "classification",
    "need",
    "softmax",
    "okay",
    "names",
    "page",
    "reference",
    "table",
    "keep",
    "coming",
    "back",
    "let",
    "keep",
    "going",
    "want",
    "highlight",
    "fact",
    "nn",
    "dot",
    "bce",
    "loss",
    "also",
    "exists",
    "requires",
    "bce",
    "loss",
    "equals",
    "requires",
    "inputs",
    "gone",
    "sigmoid",
    "activation",
    "function",
    "prior",
    "input",
    "bce",
    "loss",
    "let",
    "look",
    "documentation",
    "going",
    "comment",
    "going",
    "stick",
    "using",
    "one",
    "would",
    "stick",
    "using",
    "one",
    "let",
    "check",
    "documentation",
    "hey",
    "torch",
    "dot",
    "nn",
    "realized",
    "video",
    "place",
    "going",
    "step",
    "back",
    "bce",
    "loss",
    "logits",
    "even",
    "say",
    "right",
    "logits",
    "loss",
    "got",
    "width",
    "around",
    "wrong",
    "way",
    "let",
    "check",
    "loss",
    "combines",
    "sigmoid",
    "layer",
    "bce",
    "loss",
    "one",
    "single",
    "class",
    "go",
    "back",
    "code",
    "bce",
    "loss",
    "combined",
    "n",
    "dot",
    "sequential",
    "passed",
    "n",
    "dot",
    "sigmoid",
    "went",
    "dot",
    "bce",
    "loss",
    "get",
    "something",
    "similar",
    "keep",
    "reading",
    "documentation",
    "literally",
    "read",
    "combines",
    "sigmoid",
    "bce",
    "loss",
    "go",
    "back",
    "documentation",
    "want",
    "use",
    "version",
    "numerically",
    "stable",
    "using",
    "plain",
    "sigmoid",
    "bce",
    "loss",
    "followed",
    "bce",
    "loss",
    "combining",
    "operations",
    "one",
    "layer",
    "take",
    "advantage",
    "log",
    "sum",
    "x",
    "trick",
    "numerical",
    "stability",
    "beautiful",
    "use",
    "log",
    "function",
    "loss",
    "function",
    "binary",
    "cross",
    "entropy",
    "get",
    "numeric",
    "stability",
    "wonderful",
    "loss",
    "function",
    "got",
    "sigmoid",
    "activation",
    "function",
    "built",
    "going",
    "see",
    "difference",
    "later",
    "like",
    "flesh",
    "optimizer",
    "going",
    "choose",
    "hmm",
    "let",
    "stick",
    "sgd",
    "hey",
    "old",
    "faithful",
    "stochastic",
    "gradient",
    "descent",
    "set",
    "parameters",
    "parameters",
    "parameter",
    "params",
    "equal",
    "model",
    "parameters",
    "would",
    "like",
    "hey",
    "stochastic",
    "gradient",
    "descent",
    "please",
    "update",
    "get",
    "another",
    "code",
    "cell",
    "behind",
    "please",
    "update",
    "model",
    "parameters",
    "model",
    "respect",
    "loss",
    "like",
    "loss",
    "function",
    "go",
    "two",
    "going",
    "work",
    "tandem",
    "write",
    "training",
    "loop",
    "set",
    "learning",
    "rate",
    "see",
    "gets",
    "us",
    "optimizer",
    "going",
    "going",
    "optimize",
    "parameters",
    "us",
    "amazing",
    "principal",
    "would",
    "even",
    "100",
    "layers",
    "million",
    "different",
    "parameters",
    "got",
    "loss",
    "function",
    "got",
    "optimizer",
    "create",
    "evaluation",
    "metric",
    "let",
    "calculate",
    "accuracy",
    "time",
    "helpful",
    "classification",
    "problems",
    "accuracy",
    "accuracy",
    "well",
    "could",
    "look",
    "formula",
    "accuracy",
    "true",
    "positive",
    "true",
    "positive",
    "plus",
    "true",
    "negative",
    "times",
    "okay",
    "let",
    "see",
    "implement",
    "something",
    "similar",
    "using",
    "pure",
    "pytorch",
    "would",
    "want",
    "accuracy",
    "accuracy",
    "100",
    "examples",
    "percentage",
    "model",
    "get",
    "right",
    "example",
    "coin",
    "toss",
    "100",
    "coin",
    "tosses",
    "model",
    "predicted",
    "heads",
    "99",
    "100",
    "times",
    "right",
    "every",
    "single",
    "time",
    "might",
    "accuracy",
    "99",
    "got",
    "one",
    "wrong",
    "99",
    "100",
    "gets",
    "right",
    "dev",
    "accuracy",
    "fn",
    "accuracy",
    "function",
    "going",
    "pass",
    "true",
    "remember",
    "type",
    "evaluation",
    "function",
    "loss",
    "function",
    "comparing",
    "predictions",
    "ground",
    "truth",
    "labels",
    "correct",
    "equals",
    "going",
    "see",
    "many",
    "true",
    "threads",
    "correct",
    "torch",
    "equal",
    "stands",
    "hey",
    "many",
    "samples",
    "true",
    "equal",
    "pred",
    "going",
    "get",
    "sum",
    "need",
    "get",
    "item",
    "want",
    "single",
    "value",
    "python",
    "going",
    "calculate",
    "accuracy",
    "acc",
    "stands",
    "accuracy",
    "equals",
    "correct",
    "divided",
    "length",
    "samples",
    "input",
    "going",
    "times",
    "100",
    "return",
    "accuracy",
    "wonderful",
    "accuracy",
    "function",
    "going",
    "see",
    "three",
    "come",
    "play",
    "write",
    "training",
    "loop",
    "might",
    "get",
    "started",
    "next",
    "videos",
    "hey",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "discussed",
    "different",
    "loss",
    "function",
    "options",
    "classification",
    "models",
    "learned",
    "working",
    "binary",
    "cross",
    "entropy",
    "binary",
    "classification",
    "problems",
    "want",
    "use",
    "binary",
    "cross",
    "entropy",
    "pie",
    "torch",
    "two",
    "different",
    "times",
    "binary",
    "cross",
    "entropy",
    "except",
    "one",
    "bit",
    "numerically",
    "stable",
    "bce",
    "logit",
    "loss",
    "sigmoid",
    "activation",
    "function",
    "built",
    "straight",
    "pie",
    "documentation",
    "optimizer",
    "wise",
    "different",
    "choices",
    "well",
    "check",
    "section",
    "pie",
    "torch",
    "book",
    "different",
    "loss",
    "functions",
    "optimizers",
    "different",
    "problems",
    "pie",
    "torch",
    "code",
    "implement",
    "premise",
    "still",
    "across",
    "board",
    "different",
    "problems",
    "loss",
    "function",
    "measures",
    "wrong",
    "model",
    "goal",
    "optimizer",
    "optimize",
    "model",
    "parameters",
    "way",
    "loss",
    "function",
    "goes",
    "also",
    "implemented",
    "accuracy",
    "function",
    "metric",
    "going",
    "evaluate",
    "models",
    "predictions",
    "using",
    "accuracy",
    "evaluation",
    "metric",
    "rather",
    "loss",
    "let",
    "work",
    "training",
    "model",
    "first",
    "well",
    "remember",
    "steps",
    "pie",
    "torch",
    "training",
    "loop",
    "train",
    "model",
    "going",
    "need",
    "build",
    "training",
    "loop",
    "watch",
    "video",
    "pie",
    "torch",
    "google",
    "unofficial",
    "pie",
    "torch",
    "song",
    "find",
    "go",
    "unofficial",
    "pie",
    "torch",
    "optimization",
    "loop",
    "song",
    "going",
    "watch",
    "going",
    "little",
    "tidbit",
    "steps",
    "going",
    "code",
    "fun",
    "little",
    "jingle",
    "remember",
    "steps",
    "go",
    "book",
    "section",
    "number",
    "three",
    "train",
    "model",
    "exactly",
    "pie",
    "torch",
    "training",
    "loop",
    "steps",
    "remember",
    "epoch",
    "range",
    "forward",
    "pass",
    "calculate",
    "loss",
    "optimizer",
    "zero",
    "grand",
    "loss",
    "backward",
    "optimizer",
    "step",
    "step",
    "step",
    "keep",
    "singing",
    "day",
    "could",
    "keep",
    "reading",
    "steps",
    "day",
    "better",
    "code",
    "let",
    "write",
    "forward",
    "pass",
    "calculate",
    "loss",
    "three",
    "optimizer",
    "zero",
    "grad",
    "four",
    "loss",
    "backward",
    "back",
    "propagation",
    "write",
    "back",
    "propagation",
    "linked",
    "extra",
    "resources",
    "like",
    "find",
    "going",
    "back",
    "propagation",
    "focused",
    "code",
    "gradient",
    "descent",
    "optimizer",
    "step",
    "build",
    "training",
    "loop",
    "following",
    "steps",
    "however",
    "kind",
    "mentioned",
    "things",
    "need",
    "taken",
    "care",
    "talk",
    "forward",
    "pass",
    "talked",
    "logits",
    "looked",
    "hell",
    "logit",
    "go",
    "stack",
    "overflow",
    "answer",
    "saw",
    "machine",
    "learning",
    "logit",
    "see",
    "need",
    "steps",
    "going",
    "write",
    "let",
    "get",
    "bit",
    "clarity",
    "us",
    "daniel",
    "kind",
    "place",
    "moment",
    "right",
    "exciting",
    "part",
    "machine",
    "learning",
    "let",
    "go",
    "going",
    "raw",
    "logits",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "want",
    "truly",
    "evaluate",
    "model",
    "want",
    "let",
    "write",
    "model",
    "outputs",
    "going",
    "raw",
    "logit",
    "definition",
    "logit",
    "machine",
    "learning",
    "deep",
    "learning",
    "might",
    "read",
    "definitions",
    "us",
    "raw",
    "outputs",
    "model",
    "model",
    "zero",
    "going",
    "referred",
    "logits",
    "model",
    "zero",
    "whatever",
    "comes",
    "logits",
    "convert",
    "logits",
    "prediction",
    "probabilities",
    "passing",
    "kind",
    "activation",
    "function",
    "sigmoid",
    "binary",
    "cross",
    "entropy",
    "softmax",
    "classification",
    "got",
    "binary",
    "class",
    "sound",
    "every",
    "time",
    "spell",
    "binary",
    "classification",
    "class",
    "going",
    "see",
    "classification",
    "later",
    "want",
    "prediction",
    "probabilities",
    "going",
    "see",
    "look",
    "like",
    "minute",
    "want",
    "go",
    "logits",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "convert",
    "model",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "either",
    "rounding",
    "taking",
    "argmax",
    "round",
    "binary",
    "classification",
    "argmax",
    "outputs",
    "softmax",
    "activation",
    "function",
    "let",
    "see",
    "action",
    "first",
    "called",
    "logits",
    "raw",
    "outputs",
    "model",
    "activation",
    "function",
    "view",
    "first",
    "five",
    "outputs",
    "forward",
    "pass",
    "test",
    "data",
    "course",
    "model",
    "still",
    "instantiated",
    "random",
    "values",
    "going",
    "set",
    "variable",
    "logits",
    "model",
    "zero",
    "going",
    "pass",
    "test",
    "data",
    "x",
    "test",
    "text",
    "two",
    "device",
    "model",
    "currently",
    "cuda",
    "device",
    "need",
    "test",
    "data",
    "device",
    "target",
    "device",
    "remember",
    "writing",
    "device",
    "agnostic",
    "codes",
    "would",
    "work",
    "regardless",
    "whether",
    "gpu",
    "active",
    "let",
    "look",
    "logits",
    "oh",
    "okay",
    "right",
    "got",
    "positive",
    "values",
    "see",
    "cuda",
    "device",
    "see",
    "tracking",
    "gradients",
    "ideally",
    "would",
    "run",
    "torch",
    "dot",
    "inference",
    "mode",
    "making",
    "predictions",
    "rule",
    "thumb",
    "whenever",
    "make",
    "predictions",
    "model",
    "turn",
    "vowel",
    "mode",
    "remember",
    "turn",
    "back",
    "train",
    "want",
    "train",
    "run",
    "torch",
    "dot",
    "inference",
    "mode",
    "get",
    "similar",
    "set",
    "gradients",
    "tracked",
    "anymore",
    "okay",
    "called",
    "logits",
    "logits",
    "raw",
    "outputs",
    "model",
    "without",
    "passed",
    "activation",
    "function",
    "activation",
    "function",
    "something",
    "little",
    "separate",
    "layer",
    "come",
    "used",
    "layer",
    "neural",
    "networks",
    "start",
    "build",
    "ones",
    "subsequently",
    "build",
    "comprised",
    "layers",
    "activation",
    "functions",
    "going",
    "make",
    "concept",
    "activation",
    "function",
    "little",
    "bit",
    "clear",
    "later",
    "treat",
    "form",
    "mathematical",
    "operation",
    "pass",
    "data",
    "model",
    "happening",
    "well",
    "going",
    "linear",
    "layer",
    "recall",
    "seen",
    "times",
    "torch",
    "linear",
    "pass",
    "data",
    "linear",
    "layer",
    "applying",
    "linear",
    "transformation",
    "incoming",
    "data",
    "performing",
    "mathematical",
    "operation",
    "behind",
    "scenes",
    "output",
    "equals",
    "input",
    "x",
    "multiplied",
    "weight",
    "tensor",
    "could",
    "really",
    "w",
    "transposed",
    "dot",
    "product",
    "plus",
    "bias",
    "term",
    "jump",
    "model",
    "state",
    "deck",
    "got",
    "weight",
    "got",
    "bias",
    "formula",
    "happening",
    "two",
    "layers",
    "different",
    "depending",
    "layer",
    "choose",
    "sticking",
    "linear",
    "raw",
    "output",
    "data",
    "going",
    "two",
    "layers",
    "logits",
    "going",
    "information",
    "however",
    "format",
    "test",
    "data",
    "want",
    "make",
    "comparison",
    "good",
    "model",
    "performing",
    "need",
    "apples",
    "apples",
    "need",
    "format",
    "course",
    "need",
    "go",
    "next",
    "step",
    "let",
    "use",
    "sigmoid",
    "use",
    "sigmoid",
    "activation",
    "function",
    "model",
    "logits",
    "using",
    "sigmoid",
    "well",
    "recall",
    "binary",
    "classification",
    "architecture",
    "output",
    "activation",
    "sigmoid",
    "function",
    "let",
    "jump",
    "back",
    "going",
    "create",
    "predprobs",
    "stands",
    "model",
    "logits",
    "turn",
    "prediction",
    "probabilities",
    "probabilities",
    "predprobs",
    "equals",
    "torch",
    "sigmoid",
    "logits",
    "let",
    "look",
    "predprobs",
    "get",
    "oh",
    "still",
    "get",
    "numbers",
    "page",
    "goodness",
    "gracious",
    "important",
    "point",
    "gone",
    "sigmoid",
    "activation",
    "function",
    "pass",
    "torch",
    "dot",
    "round",
    "function",
    "let",
    "look",
    "torch",
    "dot",
    "round",
    "get",
    "predprobs",
    "oh",
    "format",
    "got",
    "might",
    "asking",
    "like",
    "put",
    "torch",
    "dot",
    "round",
    "well",
    "little",
    "step",
    "required",
    "ca",
    "raw",
    "logits",
    "need",
    "use",
    "sigmoid",
    "activation",
    "function",
    "turn",
    "prediction",
    "probabilities",
    "prediction",
    "probability",
    "well",
    "value",
    "usually",
    "0",
    "1",
    "likely",
    "model",
    "thinks",
    "certain",
    "class",
    "case",
    "binary",
    "cross",
    "entropy",
    "prediction",
    "probability",
    "values",
    "let",
    "write",
    "text",
    "prediction",
    "probability",
    "values",
    "need",
    "perform",
    "range",
    "style",
    "rounding",
    "decision",
    "boundary",
    "make",
    "sense",
    "go",
    "predprobs",
    "equal",
    "greater",
    "set",
    "equal",
    "one",
    "equal",
    "one",
    "class",
    "one",
    "whatever",
    "red",
    "dot",
    "blue",
    "dot",
    "predprobs",
    "less",
    "set",
    "equal",
    "zero",
    "class",
    "zero",
    "also",
    "adjust",
    "decision",
    "boundary",
    "say",
    "wanted",
    "increase",
    "value",
    "anything",
    "one",
    "zero",
    "generally",
    "commonly",
    "find",
    "split",
    "let",
    "keep",
    "going",
    "let",
    "actually",
    "see",
    "action",
    "recode",
    "find",
    "predicted",
    "probabilities",
    "want",
    "sorry",
    "want",
    "predicted",
    "labels",
    "want",
    "evaluating",
    "model",
    "want",
    "convert",
    "outputs",
    "model",
    "outputs",
    "model",
    "logits",
    "raw",
    "outputs",
    "model",
    "logits",
    "convert",
    "logits",
    "prediction",
    "probabilities",
    "using",
    "sigmoid",
    "function",
    "logits",
    "want",
    "find",
    "predicted",
    "labels",
    "go",
    "raw",
    "logits",
    "output",
    "model",
    "prediction",
    "probabilities",
    "passing",
    "activation",
    "function",
    "prediction",
    "labels",
    "steps",
    "want",
    "take",
    "outputs",
    "model",
    "find",
    "predicted",
    "labels",
    "let",
    "go",
    "little",
    "bit",
    "different",
    "regression",
    "problem",
    "previously",
    "nothing",
    "ca",
    "handle",
    "torch",
    "round",
    "going",
    "go",
    "like",
    "name",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "let",
    "go",
    "full",
    "wanted",
    "labels",
    "equals",
    "torch",
    "dot",
    "round",
    "torch",
    "dot",
    "sigmoid",
    "sigmoid",
    "activation",
    "function",
    "binary",
    "cross",
    "entropy",
    "model",
    "zero",
    "x",
    "test",
    "dot",
    "two",
    "device",
    "truly",
    "within",
    "inference",
    "mode",
    "code",
    "leave",
    "like",
    "single",
    "example",
    "going",
    "need",
    "count",
    "one",
    "two",
    "want",
    "want",
    "index",
    "want",
    "five",
    "examples",
    "check",
    "equality",
    "want",
    "print",
    "torch",
    "equal",
    "going",
    "check",
    "dot",
    "squeeze",
    "equal",
    "labels",
    "exact",
    "thing",
    "need",
    "squeeze",
    "get",
    "rid",
    "extra",
    "dimension",
    "comes",
    "try",
    "without",
    "squeeze",
    "get",
    "rid",
    "extra",
    "dimension",
    "want",
    "dot",
    "squeeze",
    "fair",
    "bit",
    "code",
    "happened",
    "create",
    "turn",
    "probes",
    "full",
    "step",
    "make",
    "predictions",
    "model",
    "get",
    "raw",
    "logits",
    "logits",
    "pred",
    "probes",
    "pred",
    "labels",
    "raw",
    "outputs",
    "model",
    "logits",
    "turn",
    "logits",
    "prediction",
    "probabilities",
    "using",
    "torch",
    "sigmoid",
    "turn",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "using",
    "torch",
    "dot",
    "round",
    "fulfill",
    "criteria",
    "everything",
    "torch",
    "dot",
    "round",
    "turns",
    "everything",
    "turns",
    "predictions",
    "right",
    "going",
    "quite",
    "terrible",
    "model",
    "using",
    "random",
    "numbers",
    "found",
    "steps",
    "labels",
    "one",
    "hit",
    "thanks",
    "check",
    "equality",
    "using",
    "torch",
    "equal",
    "dot",
    "squeeze",
    "squeeze",
    "get",
    "rid",
    "extra",
    "dimensions",
    "labels",
    "look",
    "like",
    "actual",
    "labels",
    "format",
    "course",
    "values",
    "model",
    "using",
    "random",
    "weights",
    "make",
    "predictions",
    "done",
    "fair",
    "steps",
    "believe",
    "right",
    "space",
    "start",
    "building",
    "training",
    "test",
    "loop",
    "let",
    "write",
    "building",
    "training",
    "testing",
    "loop",
    "might",
    "want",
    "go",
    "got",
    "steps",
    "need",
    "forward",
    "pass",
    "reason",
    "done",
    "step",
    "logits",
    "pred",
    "probes",
    "pred",
    "labels",
    "inputs",
    "loss",
    "function",
    "requires",
    "bce",
    "logits",
    "loss",
    "requires",
    "well",
    "going",
    "see",
    "next",
    "video",
    "encourage",
    "give",
    "go",
    "implementing",
    "steps",
    "remember",
    "jingle",
    "epoch",
    "range",
    "forward",
    "pass",
    "calculate",
    "loss",
    "bc",
    "logits",
    "loss",
    "optimise",
    "zero",
    "grad",
    "one",
    "last",
    "backward",
    "optimise",
    "step",
    "step",
    "step",
    "let",
    "together",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "videos",
    "working",
    "creating",
    "model",
    "classification",
    "problem",
    "training",
    "model",
    "got",
    "steps",
    "started",
    "discussing",
    "concept",
    "logits",
    "logits",
    "raw",
    "output",
    "model",
    "whatever",
    "comes",
    "forward",
    "functions",
    "layers",
    "model",
    "discussed",
    "turn",
    "logits",
    "prediction",
    "probabilities",
    "using",
    "activation",
    "function",
    "sigmoid",
    "binary",
    "classification",
    "softmax",
    "multi",
    "class",
    "classification",
    "seen",
    "softmax",
    "yet",
    "going",
    "stick",
    "sigmoid",
    "binary",
    "classification",
    "data",
    "convert",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "remember",
    "want",
    "evaluate",
    "model",
    "want",
    "compare",
    "apples",
    "apples",
    "want",
    "models",
    "predictions",
    "format",
    "test",
    "labels",
    "took",
    "little",
    "break",
    "previous",
    "video",
    "collab",
    "notebook",
    "disconnected",
    "going",
    "run",
    "cells",
    "going",
    "reconnect",
    "still",
    "gpu",
    "present",
    "good",
    "thing",
    "google",
    "collab",
    "change",
    "runtime",
    "type",
    "gpu",
    "save",
    "wherever",
    "saves",
    "google",
    "collab",
    "notebook",
    "restart",
    "still",
    "gpu",
    "present",
    "check",
    "course",
    "type",
    "device",
    "run",
    "cell",
    "also",
    "check",
    "nvidia",
    "smi",
    "tell",
    "us",
    "nvidia",
    "gpu",
    "cuda",
    "enabled",
    "ready",
    "go",
    "device",
    "cuda",
    "wonderful",
    "nvidia",
    "smi",
    "excellent",
    "tesla",
    "p100",
    "gpu",
    "ready",
    "go",
    "said",
    "let",
    "start",
    "write",
    "training",
    "loop",
    "done",
    "got",
    "steps",
    "forward",
    "pass",
    "calculate",
    "loss",
    "spent",
    "enough",
    "going",
    "start",
    "jumping",
    "write",
    "code",
    "little",
    "tidbit",
    "one",
    "though",
    "conquer",
    "get",
    "going",
    "set",
    "manual",
    "seed",
    "torch",
    "top",
    "manual",
    "seed",
    "going",
    "use",
    "favorite",
    "number",
    "ensure",
    "reproducibility",
    "possible",
    "also",
    "want",
    "aware",
    "also",
    "another",
    "form",
    "random",
    "seed",
    "manual",
    "seed",
    "cuda",
    "random",
    "seed",
    "pytorch",
    "yeah",
    "reproducibility",
    "torch",
    "dot",
    "cuda",
    "dot",
    "manual",
    "seed",
    "dot",
    "seed",
    "hmm",
    "cuda",
    "seed",
    "somewhere",
    "let",
    "try",
    "find",
    "cuda",
    "think",
    "pytorch",
    "upgrade",
    "documentation",
    "seed",
    "yeah",
    "go",
    "okay",
    "knew",
    "torch",
    "dot",
    "cuda",
    "dot",
    "manual",
    "seed",
    "using",
    "cuda",
    "cuda",
    "manual",
    "seed",
    "well",
    "let",
    "see",
    "happens",
    "put",
    "watch",
    "cuda",
    "dot",
    "manual",
    "seed",
    "necessarily",
    "put",
    "try",
    "get",
    "reproducible",
    "numbers",
    "possible",
    "screen",
    "screen",
    "important",
    "necessarily",
    "numbers",
    "exactly",
    "lining",
    "screens",
    "direction",
    "way",
    "going",
    "let",
    "set",
    "number",
    "epochs",
    "going",
    "train",
    "100",
    "epochs",
    "epochs",
    "equals",
    "might",
    "guessed",
    "cuda",
    "manual",
    "seed",
    "operations",
    "cuda",
    "device",
    "case",
    "well",
    "perhaps",
    "want",
    "reproducible",
    "possible",
    "speaking",
    "cuda",
    "devices",
    "let",
    "put",
    "data",
    "target",
    "device",
    "working",
    "writing",
    "data",
    "agnostic",
    "code",
    "going",
    "write",
    "x",
    "train",
    "train",
    "equals",
    "x",
    "train",
    "two",
    "device",
    "comma",
    "train",
    "dot",
    "two",
    "device",
    "take",
    "care",
    "training",
    "data",
    "going",
    "testing",
    "data",
    "equals",
    "x",
    "test",
    "two",
    "device",
    "going",
    "run",
    "model",
    "cuda",
    "device",
    "want",
    "data",
    "way",
    "writing",
    "code",
    "code",
    "going",
    "device",
    "agnostic",
    "said",
    "enough",
    "yet",
    "let",
    "also",
    "build",
    "training",
    "evaluation",
    "loop",
    "covered",
    "steps",
    "going",
    "start",
    "working",
    "little",
    "bit",
    "faster",
    "worry",
    "think",
    "epoch",
    "range",
    "epochs",
    "start",
    "training",
    "let",
    "write",
    "training",
    "model",
    "zero",
    "dot",
    "train",
    "model",
    "working",
    "call",
    "train",
    "default",
    "going",
    "anyway",
    "might",
    "guessed",
    "code",
    "writing",
    "functionize",
    "going",
    "later",
    "next",
    "couple",
    "videos",
    "next",
    "module",
    "two",
    "going",
    "keep",
    "writing",
    "training",
    "loop",
    "full",
    "part",
    "forward",
    "pass",
    "little",
    "bit",
    "tidbit",
    "compared",
    "done",
    "previously",
    "outputting",
    "raw",
    "logits",
    "pass",
    "data",
    "straight",
    "model",
    "model",
    "zero",
    "x",
    "train",
    "going",
    "squeeze",
    "get",
    "rid",
    "extra",
    "dimension",
    "try",
    "see",
    "output",
    "sizes",
    "look",
    "like",
    "without",
    "squeeze",
    "going",
    "call",
    "squeeze",
    "remember",
    "squeeze",
    "removes",
    "extra",
    "one",
    "dimension",
    "tensor",
    "convert",
    "prediction",
    "labels",
    "go",
    "torch",
    "dot",
    "round",
    "torch",
    "dot",
    "sigmoid",
    "torch",
    "dot",
    "sigmoid",
    "activation",
    "function",
    "going",
    "convert",
    "logits",
    "convert",
    "logits",
    "prediction",
    "probabilities",
    "logits",
    "going",
    "put",
    "note",
    "going",
    "go",
    "turn",
    "logits",
    "pred",
    "probes",
    "pred",
    "labels",
    "done",
    "forward",
    "pass",
    "little",
    "tidbit",
    "could",
    "done",
    "one",
    "step",
    "show",
    "broke",
    "apart",
    "going",
    "calculate",
    "loss",
    "slash",
    "accuracy",
    "necessarily",
    "calculate",
    "accuracy",
    "make",
    "accuracy",
    "function",
    "calculate",
    "accuracy",
    "training",
    "could",
    "stick",
    "calculating",
    "loss",
    "sometimes",
    "cool",
    "visualize",
    "different",
    "metrics",
    "loss",
    "plus",
    "others",
    "model",
    "training",
    "let",
    "write",
    "code",
    "start",
    "going",
    "loss",
    "equals",
    "loss",
    "f",
    "n",
    "logits",
    "ah",
    "difference",
    "done",
    "previously",
    "notebook",
    "zero",
    "one",
    "zero",
    "two",
    "passed",
    "prediction",
    "right",
    "loss",
    "function",
    "let",
    "look",
    "loss",
    "function",
    "let",
    "call",
    "see",
    "returns",
    "bce",
    "logits",
    "loss",
    "bce",
    "logits",
    "expects",
    "logits",
    "input",
    "might",
    "guessed",
    "loss",
    "function",
    "without",
    "logits",
    "nn",
    "dot",
    "bce",
    "loss",
    "notice",
    "got",
    "logits",
    "called",
    "loss",
    "f",
    "n",
    "f",
    "n",
    "stands",
    "function",
    "way",
    "without",
    "logits",
    "get",
    "bce",
    "loss",
    "loss",
    "expects",
    "prediction",
    "probabilities",
    "input",
    "let",
    "write",
    "code",
    "differentiate",
    "two",
    "said",
    "going",
    "sticking",
    "one",
    "look",
    "torch",
    "bce",
    "logits",
    "loss",
    "documentation",
    "states",
    "numerically",
    "stable",
    "loss",
    "combines",
    "sigmoid",
    "layer",
    "bce",
    "loss",
    "one",
    "single",
    "class",
    "numerically",
    "stable",
    "let",
    "come",
    "back",
    "keep",
    "writing",
    "code",
    "accuracy",
    "going",
    "accuracy",
    "f",
    "accuracy",
    "function",
    "little",
    "bit",
    "difference",
    "true",
    "equals",
    "train",
    "training",
    "data",
    "training",
    "accuracy",
    "pred",
    "equals",
    "pred",
    "custom",
    "accuracy",
    "function",
    "wrote",
    "testament",
    "pythonic",
    "nature",
    "pytorch",
    "well",
    "got",
    "pure",
    "python",
    "function",
    "slotted",
    "training",
    "loop",
    "essentially",
    "loss",
    "function",
    "behind",
    "scenes",
    "let",
    "write",
    "dot",
    "bce",
    "logits",
    "loss",
    "expects",
    "raw",
    "logits",
    "raw",
    "output",
    "model",
    "input",
    "using",
    "bce",
    "loss",
    "well",
    "let",
    "write",
    "code",
    "let",
    "call",
    "loss",
    "function",
    "want",
    "pass",
    "pred",
    "go",
    "torch",
    "sigmoid",
    "would",
    "pass",
    "torch",
    "sigmoid",
    "logits",
    "remember",
    "calling",
    "torch",
    "dot",
    "sigmoid",
    "logits",
    "turns",
    "logits",
    "prediction",
    "probabilities",
    "would",
    "pass",
    "train",
    "bce",
    "loss",
    "expects",
    "expects",
    "prediction",
    "probabilities",
    "input",
    "make",
    "sense",
    "difference",
    "logits",
    "loss",
    "function",
    "requires",
    "logits",
    "input",
    "whereas",
    "straight",
    "bce",
    "loss",
    "need",
    "call",
    "torch",
    "dot",
    "sigmoid",
    "logits",
    "expects",
    "prediction",
    "probabilities",
    "input",
    "going",
    "comment",
    "loss",
    "function",
    "bce",
    "logits",
    "loss",
    "keep",
    "mind",
    "reason",
    "stumble",
    "across",
    "pytorch",
    "code",
    "using",
    "bce",
    "loss",
    "bce",
    "logits",
    "loss",
    "find",
    "torch",
    "dot",
    "sigmoid",
    "calling",
    "come",
    "across",
    "errors",
    "inputs",
    "loss",
    "function",
    "expects",
    "said",
    "keep",
    "going",
    "steps",
    "optimizer",
    "zero",
    "grad",
    "optimizer",
    "dot",
    "zero",
    "grad",
    "oh",
    "step",
    "three",
    "way",
    "zero",
    "grad",
    "optimizer",
    "number",
    "four",
    "loss",
    "backward",
    "go",
    "last",
    "backward",
    "go",
    "next",
    "optimizer",
    "step",
    "step",
    "step",
    "optimizer",
    "dot",
    "step",
    "singing",
    "unofficial",
    "pytorch",
    "optimization",
    "loop",
    "song",
    "back",
    "propagation",
    "calculate",
    "gradients",
    "respect",
    "parameters",
    "model",
    "optimizer",
    "step",
    "update",
    "parameters",
    "reduce",
    "gradients",
    "gradient",
    "descent",
    "hence",
    "descent",
    "want",
    "testing",
    "well",
    "know",
    "go",
    "model",
    "zero",
    "call",
    "model",
    "dot",
    "al",
    "testing",
    "making",
    "predictions",
    "test",
    "make",
    "predictions",
    "test",
    "data",
    "set",
    "using",
    "patterns",
    "model",
    "learned",
    "training",
    "data",
    "set",
    "turn",
    "inference",
    "mode",
    "inference",
    "want",
    "forward",
    "pass",
    "course",
    "going",
    "compute",
    "test",
    "logits",
    "logits",
    "raw",
    "output",
    "model",
    "modifications",
    "x",
    "test",
    "dot",
    "squeeze",
    "going",
    "get",
    "rid",
    "extra",
    "one",
    "dimension",
    "create",
    "test",
    "pred",
    "similar",
    "calculation",
    "done",
    "test",
    "pred",
    "torch",
    "dot",
    "round",
    "binary",
    "classification",
    "want",
    "prediction",
    "probabilities",
    "going",
    "create",
    "calling",
    "sigmoid",
    "function",
    "test",
    "logits",
    "prediction",
    "probabilities",
    "go",
    "one",
    "prediction",
    "probabilities",
    "go",
    "level",
    "zero",
    "two",
    "calculate",
    "test",
    "loss",
    "test",
    "loss",
    "slash",
    "accuracy",
    "would",
    "well",
    "done",
    "going",
    "go",
    "loss",
    "fn",
    "test",
    "logits",
    "loss",
    "function",
    "using",
    "using",
    "bce",
    "logits",
    "loss",
    "expects",
    "logits",
    "input",
    "find",
    "documentation",
    "course",
    "come",
    "back",
    "test",
    "logits",
    "going",
    "compare",
    "test",
    "labels",
    "test",
    "accuracy",
    "going",
    "going",
    "call",
    "accuracy",
    "fn",
    "true",
    "equals",
    "test",
    "pred",
    "equals",
    "test",
    "pred",
    "might",
    "thinking",
    "switch",
    "order",
    "oh",
    "way",
    "important",
    "know",
    "logits",
    "loss",
    "loss",
    "functions",
    "order",
    "matters",
    "way",
    "put",
    "parameters",
    "predictions",
    "come",
    "first",
    "true",
    "labels",
    "loss",
    "function",
    "might",
    "wondering",
    "done",
    "reverse",
    "accuracy",
    "function",
    "true",
    "pred",
    "like",
    "confusing",
    "well",
    "really",
    "go",
    "base",
    "lot",
    "structured",
    "code",
    "structures",
    "things",
    "metrics",
    "accuracy",
    "score",
    "goes",
    "true",
    "pred",
    "base",
    "order",
    "metrics",
    "package",
    "helpful",
    "based",
    "metric",
    "evaluation",
    "metric",
    "function",
    "one",
    "whereas",
    "pytorch",
    "loss",
    "function",
    "reverse",
    "order",
    "important",
    "get",
    "right",
    "order",
    "exactly",
    "order",
    "could",
    "tell",
    "got",
    "one",
    "final",
    "step",
    "print",
    "happening",
    "go",
    "lot",
    "epochs",
    "100",
    "epochs",
    "divide",
    "epoch",
    "10",
    "print",
    "every",
    "epoch",
    "every",
    "10th",
    "epoch",
    "sorry",
    "couple",
    "different",
    "metrics",
    "print",
    "time",
    "going",
    "print",
    "epoch",
    "number",
    "epoch",
    "going",
    "print",
    "loss",
    "loss",
    "many",
    "decimal",
    "points",
    "go",
    "point",
    "five",
    "going",
    "training",
    "loss",
    "also",
    "accuracy",
    "training",
    "accuracy",
    "could",
    "write",
    "trainiac",
    "variable",
    "little",
    "bit",
    "make",
    "little",
    "bit",
    "understandable",
    "go",
    "going",
    "leave",
    "loss",
    "accuracy",
    "got",
    "test",
    "loss",
    "test",
    "loss",
    "going",
    "five",
    "decimal",
    "points",
    "going",
    "go",
    "test",
    "accuracy",
    "well",
    "test",
    "act",
    "dot",
    "go",
    "accuracy",
    "accuracy",
    "want",
    "percentage",
    "percent",
    "100",
    "guesses",
    "percentage",
    "model",
    "gets",
    "right",
    "training",
    "data",
    "testing",
    "data",
    "long",
    "coded",
    "functions",
    "correctly",
    "got",
    "fair",
    "steps",
    "challenge",
    "run",
    "errors",
    "try",
    "fix",
    "doubt",
    "probably",
    "one",
    "two",
    "maybe",
    "going",
    "fix",
    "next",
    "video",
    "speaking",
    "next",
    "videos",
    "see",
    "let",
    "train",
    "first",
    "classification",
    "model",
    "well",
    "exciting",
    "see",
    "soon",
    "welcome",
    "back",
    "last",
    "video",
    "wrote",
    "mammoth",
    "amount",
    "code",
    "nothing",
    "ca",
    "handle",
    "lot",
    "steps",
    "talk",
    "tidbits",
    "using",
    "different",
    "loss",
    "functions",
    "namely",
    "bce",
    "loss",
    "binary",
    "cross",
    "entropy",
    "loss",
    "bce",
    "logit",
    "loss",
    "discussed",
    "bce",
    "loss",
    "pytorch",
    "expects",
    "prediction",
    "probabilities",
    "input",
    "convert",
    "model",
    "logits",
    "logits",
    "raw",
    "output",
    "model",
    "prediction",
    "probabilities",
    "using",
    "torch",
    "dot",
    "sigmoid",
    "activation",
    "function",
    "using",
    "bce",
    "logits",
    "loss",
    "expects",
    "raw",
    "logits",
    "input",
    "sort",
    "name",
    "hints",
    "pass",
    "straight",
    "away",
    "raw",
    "logits",
    "whereas",
    "custom",
    "accuracy",
    "function",
    "compares",
    "labels",
    "labels",
    "kind",
    "stepping",
    "last",
    "videos",
    "going",
    "logits",
    "predprobs",
    "pred",
    "labels",
    "ideal",
    "output",
    "model",
    "kind",
    "label",
    "humans",
    "interpret",
    "let",
    "keep",
    "pushing",
    "forward",
    "may",
    "already",
    "tried",
    "run",
    "training",
    "loop",
    "know",
    "works",
    "wrote",
    "code",
    "get",
    "last",
    "video",
    "probably",
    "error",
    "somewhere",
    "ready",
    "going",
    "train",
    "first",
    "classification",
    "model",
    "together",
    "100",
    "epochs",
    "goes",
    "plan",
    "three",
    "two",
    "one",
    "let",
    "run",
    "oh",
    "gosh",
    "actually",
    "worked",
    "first",
    "time",
    "promise",
    "change",
    "anything",
    "last",
    "video",
    "let",
    "inspect",
    "going",
    "trains",
    "pretty",
    "fast",
    "well",
    "using",
    "gpu",
    "going",
    "accelerated",
    "much",
    "anyway",
    "data",
    "set",
    "quite",
    "small",
    "network",
    "quite",
    "small",
    "wo",
    "always",
    "get",
    "networks",
    "training",
    "fast",
    "100",
    "epochs",
    "like",
    "second",
    "loss",
    "oh",
    "go",
    "much",
    "accuracy",
    "even",
    "starts",
    "high",
    "goes",
    "going",
    "model",
    "seem",
    "learning",
    "anything",
    "would",
    "ideal",
    "accuracy",
    "ideal",
    "accuracy",
    "ideal",
    "loss",
    "value",
    "well",
    "zero",
    "lower",
    "better",
    "loss",
    "hmm",
    "confusing",
    "go",
    "look",
    "blue",
    "red",
    "dots",
    "data",
    "reckon",
    "still",
    "data",
    "frame",
    "many",
    "samples",
    "let",
    "inspect",
    "let",
    "data",
    "analysis",
    "create",
    "data",
    "frame",
    "circles",
    "still",
    "instantiated",
    "circles",
    "dot",
    "label",
    "dot",
    "going",
    "call",
    "pandas",
    "value",
    "counts",
    "going",
    "output",
    "many",
    "okay",
    "wow",
    "got",
    "500",
    "class",
    "one",
    "500",
    "class",
    "zero",
    "500",
    "red",
    "dots",
    "blue",
    "dots",
    "means",
    "balanced",
    "data",
    "set",
    "getting",
    "basically",
    "trying",
    "predict",
    "heads",
    "tails",
    "getting",
    "accuracy",
    "50",
    "50",
    "rounded",
    "model",
    "basically",
    "well",
    "guessing",
    "well",
    "gives",
    "well",
    "think",
    "get",
    "visual",
    "let",
    "make",
    "predictions",
    "model",
    "numbers",
    "page",
    "hard",
    "interpret",
    "going",
    "intuition",
    "500",
    "samples",
    "case",
    "training",
    "data",
    "set",
    "400",
    "800",
    "samples",
    "training",
    "data",
    "set",
    "testing",
    "data",
    "set",
    "200",
    "total",
    "samples",
    "100",
    "basically",
    "coin",
    "flip",
    "model",
    "good",
    "guessing",
    "turn",
    "investigate",
    "model",
    "learning",
    "one",
    "ways",
    "visualizing",
    "predictions",
    "let",
    "write",
    "metrics",
    "looks",
    "like",
    "model",
    "learning",
    "anything",
    "inspect",
    "let",
    "make",
    "predictions",
    "make",
    "visual",
    "right",
    "words",
    "visualize",
    "visualize",
    "visualize",
    "right",
    "trained",
    "model",
    "least",
    "got",
    "structure",
    "training",
    "code",
    "right",
    "training",
    "code",
    "written",
    "code",
    "know",
    "set",
    "training",
    "code",
    "allow",
    "model",
    "train",
    "must",
    "something",
    "wrong",
    "either",
    "built",
    "model",
    "data",
    "set",
    "let",
    "keep",
    "going",
    "investigate",
    "together",
    "got",
    "function",
    "earlier",
    "mention",
    "learning",
    "side",
    "side",
    "machine",
    "learning",
    "cooking",
    "show",
    "ingredient",
    "prepared",
    "earlier",
    "part",
    "dish",
    "going",
    "import",
    "function",
    "called",
    "plot",
    "decision",
    "maybe",
    "turn",
    "code",
    "plot",
    "decision",
    "boundary",
    "welcome",
    "cooking",
    "show",
    "cooking",
    "machine",
    "learning",
    "model",
    "cook",
    "today",
    "go",
    "pytorch",
    "deep",
    "learning",
    "well",
    "already",
    "home",
    "repo",
    "course",
    "link",
    "scattered",
    "everywhere",
    "little",
    "function",
    "called",
    "helper",
    "functions",
    "dot",
    "py",
    "going",
    "fill",
    "helper",
    "functions",
    "throughout",
    "course",
    "one",
    "talking",
    "plot",
    "decision",
    "boundary",
    "could",
    "copy",
    "notebook",
    "going",
    "write",
    "code",
    "import",
    "programmatically",
    "use",
    "functions",
    "plot",
    "predictions",
    "function",
    "made",
    "last",
    "section",
    "zero",
    "one",
    "plot",
    "decision",
    "boundary",
    "function",
    "got",
    "inspired",
    "create",
    "another",
    "resource",
    "little",
    "bit",
    "aside",
    "highly",
    "recommend",
    "going",
    "goku",
    "mohandas",
    "gives",
    "foundations",
    "neural",
    "networks",
    "also",
    "ml",
    "ops",
    "field",
    "based",
    "getting",
    "neural",
    "networks",
    "machine",
    "learning",
    "models",
    "applications",
    "people",
    "use",
    "ca",
    "recommend",
    "resource",
    "enough",
    "please",
    "please",
    "please",
    "check",
    "want",
    "another",
    "resource",
    "machine",
    "learning",
    "helper",
    "function",
    "came",
    "thank",
    "goku",
    "mohandas",
    "made",
    "little",
    "bit",
    "modifications",
    "course",
    "many",
    "could",
    "either",
    "copy",
    "paste",
    "could",
    "write",
    "code",
    "import",
    "us",
    "magically",
    "using",
    "power",
    "internet",
    "right",
    "programmers",
    "machine",
    "learning",
    "engineers",
    "data",
    "scientists",
    "pathlib",
    "request",
    "module",
    "python",
    "module",
    "allows",
    "make",
    "requests",
    "request",
    "like",
    "going",
    "website",
    "hey",
    "like",
    "get",
    "code",
    "information",
    "please",
    "send",
    "allows",
    "us",
    "pathlib",
    "seen",
    "pathlib",
    "allows",
    "us",
    "create",
    "file",
    "parts",
    "well",
    "want",
    "save",
    "helper",
    "function",
    "dot",
    "pi",
    "script",
    "google",
    "collab",
    "files",
    "little",
    "bit",
    "code",
    "download",
    "helper",
    "functions",
    "learn",
    "pytorch",
    "repo",
    "already",
    "downloaded",
    "let",
    "see",
    "going",
    "write",
    "else",
    "code",
    "check",
    "see",
    "path",
    "helper",
    "functions",
    "dot",
    "pi",
    "already",
    "exist",
    "want",
    "download",
    "moment",
    "exist",
    "statement",
    "going",
    "return",
    "false",
    "let",
    "print",
    "returns",
    "true",
    "helper",
    "functions",
    "dot",
    "pi",
    "already",
    "exists",
    "might",
    "could",
    "even",
    "probably",
    "try",
    "accept",
    "looping",
    "else",
    "help",
    "us",
    "exists",
    "else",
    "print",
    "downloading",
    "helper",
    "functions",
    "dot",
    "pi",
    "exist",
    "going",
    "make",
    "request",
    "let",
    "set",
    "request",
    "request",
    "dot",
    "get",
    "put",
    "url",
    "need",
    "raw",
    "version",
    "raw",
    "version",
    "go",
    "back",
    "pytorch",
    "deep",
    "learning",
    "repo",
    "course",
    "slash",
    "helper",
    "functions",
    "click",
    "raw",
    "going",
    "copy",
    "oh",
    "want",
    "go",
    "want",
    "go",
    "request",
    "get",
    "type",
    "string",
    "format",
    "get",
    "raw",
    "url",
    "going",
    "go",
    "open",
    "going",
    "open",
    "file",
    "called",
    "helper",
    "functions",
    "dot",
    "pi",
    "going",
    "set",
    "context",
    "right",
    "binary",
    "wb",
    "file",
    "f",
    "common",
    "short",
    "version",
    "writing",
    "file",
    "going",
    "call",
    "file",
    "dot",
    "write",
    "request",
    "dot",
    "content",
    "code",
    "basically",
    "saying",
    "hey",
    "requests",
    "get",
    "information",
    "link",
    "course",
    "code",
    "python",
    "script",
    "going",
    "create",
    "file",
    "called",
    "helper",
    "functions",
    "dot",
    "pi",
    "gives",
    "us",
    "write",
    "permissions",
    "going",
    "name",
    "f",
    "short",
    "file",
    "going",
    "call",
    "file",
    "dot",
    "write",
    "content",
    "request",
    "instead",
    "talking",
    "see",
    "action",
    "know",
    "works",
    "helper",
    "functions",
    "import",
    "plot",
    "predictions",
    "going",
    "use",
    "plot",
    "predictions",
    "later",
    "well",
    "plot",
    "decision",
    "boundary",
    "plot",
    "predictions",
    "wrote",
    "last",
    "section",
    "wonderful",
    "going",
    "write",
    "downloading",
    "helper",
    "functions",
    "dot",
    "pi",
    "work",
    "helper",
    "functions",
    "dot",
    "pi",
    "look",
    "done",
    "programmatically",
    "view",
    "google",
    "column",
    "oh",
    "goodness",
    "yes",
    "look",
    "may",
    "evolve",
    "time",
    "course",
    "general",
    "helper",
    "functions",
    "rather",
    "writing",
    "would",
    "like",
    "know",
    "going",
    "plot",
    "decision",
    "boundary",
    "encourage",
    "read",
    "going",
    "step",
    "step",
    "nothing",
    "ca",
    "tackle",
    "python",
    "code",
    "secrets",
    "python",
    "code",
    "got",
    "making",
    "predictions",
    "pytorch",
    "model",
    "testing",
    "multi",
    "class",
    "binary",
    "going",
    "get",
    "let",
    "see",
    "ultimate",
    "test",
    "plot",
    "decision",
    "boundary",
    "function",
    "works",
    "could",
    "discuss",
    "plot",
    "decision",
    "boundary",
    "model",
    "could",
    "discuss",
    "behind",
    "scenes",
    "cows",
    "come",
    "home",
    "going",
    "see",
    "real",
    "life",
    "like",
    "get",
    "visual",
    "fig",
    "size",
    "12",
    "six",
    "going",
    "create",
    "plot",
    "adhering",
    "data",
    "explorer",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "want",
    "subplot",
    "going",
    "compare",
    "training",
    "test",
    "sets",
    "train",
    "going",
    "go",
    "plt",
    "actually",
    "plot",
    "first",
    "one",
    "plot",
    "decision",
    "boundary",
    "training",
    "plot",
    "going",
    "pass",
    "model",
    "zero",
    "x",
    "train",
    "train",
    "order",
    "parameters",
    "go",
    "press",
    "command",
    "shift",
    "space",
    "believe",
    "google",
    "collab",
    "working",
    "put",
    "doc",
    "string",
    "go",
    "plot",
    "decision",
    "boundary",
    "look",
    "inputs",
    "takes",
    "model",
    "torch",
    "end",
    "module",
    "got",
    "x",
    "x",
    "value",
    "torch",
    "tensor",
    "torch",
    "tensor",
    "value",
    "training",
    "data",
    "let",
    "testing",
    "data",
    "plot",
    "dot",
    "subplot",
    "going",
    "one",
    "two",
    "two",
    "index",
    "number",
    "rows",
    "plot",
    "number",
    "columns",
    "index",
    "plot",
    "appear",
    "first",
    "slot",
    "going",
    "see",
    "anyway",
    "anything",
    "code",
    "appear",
    "second",
    "slot",
    "plt",
    "dot",
    "title",
    "going",
    "call",
    "one",
    "test",
    "going",
    "call",
    "plot",
    "decision",
    "boundary",
    "works",
    "going",
    "serious",
    "magic",
    "love",
    "visualization",
    "functions",
    "machine",
    "learning",
    "okay",
    "ready",
    "three",
    "two",
    "one",
    "let",
    "check",
    "model",
    "oh",
    "look",
    "oh",
    "clear",
    "behind",
    "scenes",
    "plots",
    "plot",
    "decision",
    "boundary",
    "making",
    "course",
    "training",
    "data",
    "testing",
    "data",
    "many",
    "dot",
    "points",
    "sort",
    "line",
    "going",
    "line",
    "model",
    "trying",
    "draw",
    "data",
    "wonder",
    "getting",
    "50",
    "accuracy",
    "loss",
    "going",
    "trying",
    "split",
    "data",
    "straight",
    "middle",
    "drawing",
    "straight",
    "line",
    "data",
    "circular",
    "think",
    "drawing",
    "straight",
    "line",
    "well",
    "think",
    "anything",
    "fact",
    "model",
    "made",
    "using",
    "pure",
    "linear",
    "layers",
    "let",
    "go",
    "back",
    "model",
    "comprised",
    "couple",
    "linear",
    "layers",
    "linear",
    "line",
    "look",
    "linear",
    "line",
    "going",
    "work",
    "actually",
    "think",
    "might",
    "go",
    "linear",
    "line",
    "straight",
    "lines",
    "want",
    "think",
    "even",
    "completely",
    "new",
    "deep",
    "learning",
    "answer",
    "question",
    "ever",
    "separate",
    "circular",
    "data",
    "straight",
    "lines",
    "mean",
    "maybe",
    "could",
    "drew",
    "straight",
    "lines",
    "trying",
    "curve",
    "around",
    "easier",
    "way",
    "going",
    "see",
    "later",
    "try",
    "improve",
    "model",
    "model",
    "built",
    "got",
    "100",
    "epochs",
    "wonder",
    "model",
    "improve",
    "trained",
    "longer",
    "little",
    "bit",
    "challenge",
    "next",
    "video",
    "see",
    "train",
    "model",
    "1000",
    "epochs",
    "improve",
    "results",
    "improve",
    "results",
    "think",
    "might",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "wrote",
    "code",
    "download",
    "series",
    "helper",
    "functions",
    "helper",
    "functions",
    "dot",
    "pi",
    "later",
    "see",
    "quite",
    "standard",
    "practice",
    "write",
    "code",
    "write",
    "code",
    "store",
    "somewhere",
    "python",
    "script",
    "like",
    "instead",
    "us",
    "rewriting",
    "everything",
    "helper",
    "functions",
    "import",
    "use",
    "later",
    "similar",
    "pytorch",
    "pytorch",
    "essentially",
    "collection",
    "python",
    "scripts",
    "using",
    "build",
    "neural",
    "networks",
    "well",
    "lot",
    "done",
    "mean",
    "got",
    "one",
    "pytorch",
    "collection",
    "probably",
    "hundreds",
    "different",
    "python",
    "scripts",
    "beside",
    "point",
    "trying",
    "train",
    "model",
    "separate",
    "blue",
    "red",
    "dots",
    "current",
    "model",
    "drawing",
    "straight",
    "lines",
    "got",
    "think",
    "whether",
    "straight",
    "line",
    "model",
    "linear",
    "model",
    "could",
    "ever",
    "separate",
    "data",
    "maybe",
    "could",
    "issued",
    "challenge",
    "see",
    "could",
    "trained",
    "1000",
    "epochs",
    "improve",
    "anything",
    "accuracy",
    "higher",
    "well",
    "speaking",
    "training",
    "epochs",
    "section",
    "number",
    "five",
    "improving",
    "model",
    "model",
    "perspective",
    "let",
    "discuss",
    "ways",
    "getting",
    "results",
    "train",
    "machine",
    "learning",
    "model",
    "deep",
    "learning",
    "model",
    "whatever",
    "kind",
    "model",
    "working",
    "happy",
    "results",
    "could",
    "go",
    "improving",
    "going",
    "little",
    "bit",
    "overview",
    "going",
    "get",
    "one",
    "way",
    "add",
    "layers",
    "give",
    "model",
    "chances",
    "learn",
    "patterns",
    "data",
    "would",
    "help",
    "model",
    "currently",
    "two",
    "layers",
    "model",
    "zero",
    "dot",
    "state",
    "dinked",
    "well",
    "got",
    "however",
    "many",
    "numbers",
    "20",
    "zero",
    "flayer",
    "first",
    "layer",
    "10",
    "well",
    "10",
    "times",
    "amount",
    "parameters",
    "try",
    "learn",
    "patterns",
    "data",
    "representation",
    "data",
    "another",
    "way",
    "add",
    "hidden",
    "units",
    "mean",
    "created",
    "model",
    "layers",
    "five",
    "hidden",
    "units",
    "first",
    "one",
    "outputs",
    "features",
    "equals",
    "five",
    "one",
    "takes",
    "features",
    "equals",
    "five",
    "could",
    "go",
    "go",
    "five",
    "hidden",
    "units",
    "10",
    "hidden",
    "units",
    "principle",
    "applies",
    "parameters",
    "model",
    "represent",
    "data",
    "potentially",
    "say",
    "potentially",
    "things",
    "might",
    "necessarily",
    "work",
    "data",
    "sets",
    "quite",
    "simple",
    "maybe",
    "added",
    "many",
    "layers",
    "models",
    "trying",
    "learn",
    "things",
    "complex",
    "trying",
    "adjust",
    "many",
    "numbers",
    "data",
    "set",
    "thing",
    "hidden",
    "units",
    "options",
    "well",
    "could",
    "fit",
    "longer",
    "give",
    "model",
    "chance",
    "learn",
    "every",
    "epoch",
    "one",
    "pass",
    "data",
    "maybe",
    "100",
    "times",
    "looking",
    "data",
    "set",
    "enough",
    "maybe",
    "could",
    "fit",
    "1000",
    "times",
    "challenge",
    "change",
    "activation",
    "functions",
    "using",
    "sigmoid",
    "moment",
    "generally",
    "activation",
    "function",
    "use",
    "binary",
    "classification",
    "problem",
    "also",
    "activation",
    "functions",
    "put",
    "within",
    "model",
    "hmm",
    "little",
    "hint",
    "get",
    "later",
    "change",
    "learning",
    "rate",
    "learning",
    "rate",
    "amount",
    "optimizer",
    "adjust",
    "every",
    "epoch",
    "small",
    "model",
    "might",
    "learn",
    "anything",
    "taking",
    "forever",
    "change",
    "numbers",
    "also",
    "side",
    "things",
    "learning",
    "rate",
    "high",
    "updates",
    "might",
    "large",
    "model",
    "might",
    "explode",
    "actual",
    "problem",
    "machine",
    "learning",
    "called",
    "exploding",
    "gradient",
    "problem",
    "numbers",
    "get",
    "large",
    "side",
    "also",
    "vanishing",
    "gradients",
    "problem",
    "gradients",
    "go",
    "basically",
    "zero",
    "quickly",
    "also",
    "change",
    "loss",
    "function",
    "feel",
    "like",
    "sigmoid",
    "binary",
    "cross",
    "entropy",
    "pretty",
    "good",
    "pretty",
    "standard",
    "going",
    "look",
    "options",
    "add",
    "layers",
    "fit",
    "longer",
    "maybe",
    "changing",
    "learning",
    "rate",
    "let",
    "add",
    "little",
    "bit",
    "color",
    "talking",
    "right",
    "fit",
    "model",
    "data",
    "made",
    "prediction",
    "going",
    "step",
    "done",
    "done",
    "done",
    "two",
    "built",
    "training",
    "loop",
    "fit",
    "model",
    "data",
    "made",
    "prediction",
    "evaluated",
    "model",
    "visually",
    "happy",
    "number",
    "five",
    "going",
    "improve",
    "experimentation",
    "need",
    "use",
    "tensorboard",
    "yet",
    "going",
    "talk",
    "high",
    "level",
    "tensorboard",
    "tool",
    "utility",
    "pytorch",
    "helps",
    "monitor",
    "experiments",
    "see",
    "later",
    "get",
    "wo",
    "save",
    "model",
    "got",
    "one",
    "happy",
    "look",
    "talked",
    "improving",
    "model",
    "model",
    "perspective",
    "let",
    "talk",
    "things",
    "talked",
    "color",
    "time",
    "say",
    "got",
    "model",
    "exact",
    "model",
    "working",
    "similar",
    "structure",
    "got",
    "one",
    "two",
    "three",
    "four",
    "layers",
    "got",
    "loss",
    "function",
    "bc",
    "logit",
    "loss",
    "got",
    "optimizer",
    "optimizer",
    "stochastic",
    "gradient",
    "descent",
    "write",
    "training",
    "code",
    "10",
    "epochs",
    "testing",
    "code",
    "cut",
    "would",
    "fit",
    "slide",
    "wanted",
    "go",
    "larger",
    "model",
    "let",
    "add",
    "color",
    "highlight",
    "happening",
    "adding",
    "layers",
    "okay",
    "one",
    "got",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "layers",
    "got",
    "another",
    "color",
    "say",
    "like",
    "little",
    "bit",
    "greeny",
    "blue",
    "increase",
    "number",
    "hidden",
    "units",
    "okay",
    "hidden",
    "units",
    "features",
    "gone",
    "100",
    "128",
    "remember",
    "features",
    "previous",
    "layer",
    "line",
    "features",
    "next",
    "layer",
    "gone",
    "wow",
    "remember",
    "said",
    "multiples",
    "eight",
    "pretty",
    "good",
    "generally",
    "deep",
    "learning",
    "well",
    "numbers",
    "come",
    "else",
    "change",
    "slash",
    "add",
    "activation",
    "functions",
    "seen",
    "end",
    "relu",
    "want",
    "jump",
    "ahead",
    "look",
    "end",
    "relu",
    "would",
    "find",
    "well",
    "google",
    "end",
    "relu",
    "going",
    "look",
    "later",
    "see",
    "one",
    "got",
    "one",
    "larger",
    "model",
    "relu",
    "scattered",
    "linear",
    "layers",
    "hmm",
    "maybe",
    "hint",
    "combine",
    "linear",
    "layer",
    "relu",
    "relu",
    "layer",
    "going",
    "spoil",
    "going",
    "find",
    "later",
    "change",
    "optimization",
    "function",
    "okay",
    "got",
    "sgd",
    "recall",
    "said",
    "adam",
    "another",
    "popular",
    "one",
    "works",
    "fairly",
    "well",
    "across",
    "lot",
    "problems",
    "well",
    "adam",
    "might",
    "better",
    "option",
    "us",
    "learning",
    "rate",
    "well",
    "maybe",
    "learning",
    "rate",
    "little",
    "high",
    "divided",
    "finally",
    "fitting",
    "longer",
    "instead",
    "10",
    "epochs",
    "gone",
    "try",
    "implement",
    "model",
    "see",
    "improves",
    "got",
    "going",
    "frankly",
    "like",
    "satisfactory",
    "trying",
    "build",
    "neural",
    "network",
    "neural",
    "networks",
    "supposed",
    "models",
    "learn",
    "almost",
    "anything",
    "ca",
    "even",
    "separate",
    "blue",
    "dots",
    "red",
    "dots",
    "next",
    "video",
    "run",
    "writing",
    "code",
    "steps",
    "fact",
    "want",
    "try",
    "highly",
    "encourage",
    "start",
    "trying",
    "add",
    "layers",
    "add",
    "hitting",
    "units",
    "fitting",
    "longer",
    "keep",
    "settings",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "discussed",
    "options",
    "improve",
    "model",
    "model",
    "perspective",
    "namely",
    "trying",
    "improve",
    "predictions",
    "better",
    "patterns",
    "learns",
    "better",
    "represent",
    "data",
    "separate",
    "blue",
    "dots",
    "red",
    "dots",
    "might",
    "wondering",
    "said",
    "model",
    "perspective",
    "let",
    "write",
    "options",
    "models",
    "perspective",
    "deal",
    "directly",
    "model",
    "rather",
    "data",
    "another",
    "way",
    "improve",
    "models",
    "results",
    "model",
    "sound",
    "already",
    "machine",
    "learning",
    "deep",
    "learning",
    "may",
    "aware",
    "generally",
    "data",
    "samples",
    "model",
    "learns",
    "gets",
    "better",
    "results",
    "opportunity",
    "learn",
    "ways",
    "improve",
    "model",
    "data",
    "perspective",
    "going",
    "focus",
    "improving",
    "model",
    "models",
    "perspective",
    "options",
    "values",
    "machine",
    "learning",
    "engineers",
    "data",
    "scientists",
    "change",
    "referred",
    "hyper",
    "parameters",
    "little",
    "bit",
    "important",
    "distinction",
    "parameters",
    "numbers",
    "within",
    "model",
    "parameters",
    "like",
    "values",
    "weights",
    "biases",
    "parameters",
    "values",
    "model",
    "updates",
    "hyper",
    "parameters",
    "machine",
    "learning",
    "engineers",
    "data",
    "scientists",
    "adding",
    "layers",
    "hidden",
    "units",
    "fitting",
    "longer",
    "number",
    "epochs",
    "activation",
    "functions",
    "learning",
    "rate",
    "loss",
    "functions",
    "hyper",
    "parameters",
    "values",
    "change",
    "let",
    "change",
    "hyper",
    "parameters",
    "model",
    "create",
    "circle",
    "model",
    "v1",
    "going",
    "import",
    "well",
    "could",
    "write",
    "model",
    "using",
    "going",
    "subclass",
    "practice",
    "would",
    "use",
    "well",
    "see",
    "model",
    "complicated",
    "subclass",
    "fact",
    "write",
    "also",
    "version",
    "subclass",
    "one",
    "practice",
    "later",
    "wanted",
    "wanted",
    "make",
    "complex",
    "models",
    "going",
    "see",
    "subclass",
    "lot",
    "wild",
    "first",
    "change",
    "going",
    "update",
    "number",
    "hidden",
    "units",
    "features",
    "might",
    "write",
    "let",
    "try",
    "improve",
    "model",
    "adding",
    "hidden",
    "units",
    "go",
    "five",
    "increase",
    "want",
    "increase",
    "number",
    "layers",
    "want",
    "go",
    "two",
    "three",
    "add",
    "extra",
    "layer",
    "increase",
    "number",
    "epochs",
    "going",
    "go",
    "100",
    "going",
    "put",
    "scientist",
    "hats",
    "second",
    "would",
    "problem",
    "way",
    "running",
    "experiment",
    "three",
    "things",
    "one",
    "hit",
    "might",
    "problematic",
    "well",
    "might",
    "know",
    "one",
    "offered",
    "improvement",
    "improvement",
    "degradation",
    "keep",
    "mind",
    "going",
    "forward",
    "example",
    "change",
    "generally",
    "machine",
    "learning",
    "experiments",
    "like",
    "change",
    "one",
    "value",
    "time",
    "track",
    "results",
    "called",
    "experiment",
    "tracking",
    "machine",
    "learning",
    "going",
    "look",
    "experiment",
    "tracking",
    "little",
    "later",
    "course",
    "keep",
    "mind",
    "scientist",
    "likes",
    "change",
    "one",
    "variable",
    "going",
    "control",
    "happening",
    "going",
    "create",
    "next",
    "layer",
    "layer",
    "two",
    "course",
    "takes",
    "number",
    "features",
    "features",
    "previous",
    "layer",
    "two",
    "x",
    "train",
    "let",
    "look",
    "first",
    "five",
    "samples",
    "two",
    "features",
    "going",
    "create",
    "self",
    "layer",
    "three",
    "equals",
    "n",
    "dot",
    "linear",
    "features",
    "going",
    "layer",
    "features",
    "equals",
    "changed",
    "far",
    "got",
    "hidden",
    "units",
    "previously",
    "zero",
    "model",
    "five",
    "got",
    "third",
    "layout",
    "previously",
    "two",
    "two",
    "main",
    "changes",
    "features",
    "equals",
    "one",
    "let",
    "look",
    "speaking",
    "one",
    "number",
    "remember",
    "shapes",
    "input",
    "output",
    "shapes",
    "model",
    "one",
    "important",
    "things",
    "deep",
    "learning",
    "going",
    "see",
    "different",
    "values",
    "shapes",
    "later",
    "working",
    "data",
    "set",
    "focused",
    "two",
    "features",
    "one",
    "feature",
    "got",
    "layers",
    "prepared",
    "next",
    "well",
    "override",
    "forward",
    "method",
    "every",
    "subclass",
    "n",
    "dot",
    "module",
    "implement",
    "forward",
    "method",
    "going",
    "well",
    "could",
    "let",
    "show",
    "one",
    "option",
    "could",
    "go",
    "z",
    "would",
    "z",
    "logits",
    "logits",
    "actually",
    "represented",
    "z",
    "fun",
    "fact",
    "could",
    "actually",
    "put",
    "variable",
    "could",
    "x",
    "one",
    "could",
    "reset",
    "x",
    "wanted",
    "look",
    "putting",
    "different",
    "one",
    "little",
    "less",
    "confusing",
    "could",
    "go",
    "update",
    "z",
    "going",
    "self",
    "layer",
    "two",
    "z",
    "output",
    "layer",
    "one",
    "goes",
    "go",
    "z",
    "equals",
    "self",
    "layer",
    "three",
    "going",
    "take",
    "going",
    "take",
    "z",
    "saying",
    "hey",
    "give",
    "x",
    "put",
    "layer",
    "one",
    "assign",
    "create",
    "new",
    "variable",
    "z",
    "override",
    "z",
    "self",
    "layer",
    "two",
    "z",
    "input",
    "got",
    "z",
    "output",
    "layer",
    "two",
    "input",
    "layer",
    "three",
    "could",
    "return",
    "passing",
    "data",
    "one",
    "layers",
    "way",
    "leverage",
    "speedups",
    "pytorch",
    "call",
    "layer",
    "three",
    "going",
    "put",
    "self",
    "dot",
    "layer",
    "two",
    "generally",
    "going",
    "write",
    "also",
    "behind",
    "scenes",
    "performing",
    "operations",
    "leverage",
    "whatever",
    "speed",
    "ups",
    "get",
    "oh",
    "layer",
    "one",
    "goes",
    "order",
    "happening",
    "well",
    "computing",
    "inside",
    "brackets",
    "first",
    "layer",
    "one",
    "x",
    "going",
    "layer",
    "one",
    "output",
    "x",
    "layer",
    "one",
    "going",
    "layer",
    "two",
    "layer",
    "three",
    "way",
    "way",
    "writing",
    "operations",
    "leverages",
    "speed",
    "ups",
    "possible",
    "behind",
    "scenes",
    "done",
    "ford",
    "method",
    "passing",
    "data",
    "layers",
    "extra",
    "hidden",
    "units",
    "extra",
    "layer",
    "overall",
    "let",
    "create",
    "instance",
    "circle",
    "model",
    "v",
    "one",
    "going",
    "set",
    "model",
    "one",
    "going",
    "write",
    "circle",
    "model",
    "v",
    "one",
    "going",
    "send",
    "target",
    "device",
    "like",
    "writing",
    "device",
    "agnostic",
    "code",
    "going",
    "check",
    "model",
    "one",
    "let",
    "look",
    "going",
    "beautiful",
    "three",
    "layered",
    "model",
    "hidden",
    "units",
    "wonder",
    "trained",
    "model",
    "longer",
    "going",
    "get",
    "improvements",
    "challenge",
    "already",
    "done",
    "steps",
    "going",
    "next",
    "couple",
    "videos",
    "completeness",
    "need",
    "create",
    "loss",
    "function",
    "give",
    "hint",
    "similar",
    "one",
    "already",
    "used",
    "need",
    "create",
    "optimizer",
    "done",
    "need",
    "write",
    "training",
    "evaluation",
    "loop",
    "model",
    "one",
    "give",
    "shot",
    "otherwise",
    "see",
    "next",
    "video",
    "together",
    "welcome",
    "back",
    "last",
    "video",
    "subclassed",
    "create",
    "circle",
    "model",
    "v",
    "one",
    "upgrade",
    "circle",
    "model",
    "v",
    "zero",
    "fact",
    "added",
    "hidden",
    "units",
    "five",
    "added",
    "whole",
    "extra",
    "layer",
    "got",
    "instance",
    "ready",
    "go",
    "workflow",
    "got",
    "data",
    "well",
    "changed",
    "data",
    "built",
    "new",
    "model",
    "need",
    "pick",
    "loss",
    "function",
    "hinted",
    "going",
    "use",
    "loss",
    "function",
    "optimizer",
    "might",
    "already",
    "done",
    "steps",
    "may",
    "know",
    "whether",
    "model",
    "works",
    "data",
    "set",
    "going",
    "work",
    "towards",
    "finding",
    "video",
    "built",
    "new",
    "model",
    "let",
    "pick",
    "loss",
    "function",
    "optimizer",
    "could",
    "almost",
    "eyes",
    "closed",
    "build",
    "training",
    "loop",
    "fit",
    "model",
    "data",
    "make",
    "prediction",
    "evaluate",
    "model",
    "come",
    "back",
    "let",
    "set",
    "loss",
    "function",
    "way",
    "wondering",
    "like",
    "would",
    "adding",
    "features",
    "kind",
    "hinted",
    "would",
    "extra",
    "layer",
    "improve",
    "model",
    "well",
    "back",
    "fact",
    "add",
    "neurons",
    "add",
    "hidden",
    "units",
    "add",
    "layers",
    "gives",
    "model",
    "numbers",
    "adjust",
    "look",
    "going",
    "layer",
    "one",
    "layer",
    "two",
    "look",
    "many",
    "compared",
    "model",
    "zero",
    "dot",
    "state",
    "date",
    "model",
    "zero",
    "upgraded",
    "look",
    "many",
    "adding",
    "extra",
    "layer",
    "hidden",
    "units",
    "optimizer",
    "change",
    "values",
    "hopefully",
    "create",
    "better",
    "representation",
    "data",
    "trying",
    "fit",
    "opportunity",
    "learn",
    "patterns",
    "target",
    "data",
    "set",
    "theory",
    "behind",
    "let",
    "get",
    "rid",
    "ease",
    "let",
    "create",
    "loss",
    "function",
    "going",
    "use",
    "well",
    "going",
    "use",
    "nn",
    "dot",
    "bce",
    "logit",
    "loss",
    "optimizer",
    "going",
    "going",
    "keep",
    "torch",
    "dot",
    "opt",
    "dot",
    "sgd",
    "aware",
    "using",
    "new",
    "model",
    "pass",
    "params",
    "model",
    "one",
    "parameters",
    "want",
    "optimize",
    "lr",
    "going",
    "lr",
    "use",
    "learning",
    "rate",
    "oh",
    "potentially",
    "learning",
    "rate",
    "may",
    "big",
    "create",
    "optimizer",
    "written",
    "lot",
    "code",
    "optimizer",
    "go",
    "right",
    "keep",
    "keep",
    "many",
    "things",
    "possible",
    "going",
    "set",
    "torch",
    "dot",
    "manual",
    "seed",
    "42",
    "make",
    "training",
    "reproducible",
    "possible",
    "torch",
    "dot",
    "cuda",
    "dot",
    "manual",
    "seed",
    "said",
    "worry",
    "much",
    "numbers",
    "exactly",
    "mine",
    "direction",
    "important",
    "whether",
    "good",
    "bad",
    "direction",
    "let",
    "set",
    "epochs",
    "want",
    "train",
    "longer",
    "time",
    "well",
    "1000",
    "epochs",
    "one",
    "three",
    "improvements",
    "trying",
    "adding",
    "hidden",
    "units",
    "increase",
    "number",
    "layers",
    "increase",
    "number",
    "epochs",
    "going",
    "give",
    "model",
    "1000",
    "looks",
    "data",
    "try",
    "improve",
    "patterns",
    "put",
    "data",
    "target",
    "device",
    "want",
    "write",
    "device",
    "agnostic",
    "code",
    "yes",
    "already",
    "done",
    "going",
    "write",
    "practice",
    "even",
    "though",
    "could",
    "functionize",
    "lot",
    "good",
    "still",
    "foundation",
    "stages",
    "practice",
    "going",
    "want",
    "able",
    "eyes",
    "closed",
    "start",
    "functionize",
    "put",
    "training",
    "data",
    "testing",
    "data",
    "target",
    "device",
    "whatever",
    "cpu",
    "gpu",
    "going",
    "well",
    "song",
    "epoch",
    "range",
    "let",
    "loop",
    "epochs",
    "going",
    "start",
    "training",
    "training",
    "well",
    "set",
    "model",
    "one",
    "train",
    "first",
    "step",
    "well",
    "forward",
    "pass",
    "outputs",
    "model",
    "well",
    "raw",
    "outputs",
    "model",
    "logits",
    "model",
    "one",
    "going",
    "pass",
    "training",
    "data",
    "going",
    "squeeze",
    "get",
    "rid",
    "extra",
    "one",
    "dimension",
    "believe",
    "would",
    "like",
    "get",
    "rid",
    "one",
    "dimension",
    "try",
    "running",
    "code",
    "without",
    "dot",
    "squeeze",
    "pred",
    "equals",
    "torch",
    "dot",
    "round",
    "torch",
    "dot",
    "sigmoid",
    "calling",
    "sigmoid",
    "logits",
    "go",
    "logits",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "next",
    "well",
    "calculate",
    "loss",
    "slash",
    "accuracy",
    "remember",
    "accuracy",
    "optional",
    "loss",
    "optional",
    "going",
    "pass",
    "loss",
    "function",
    "going",
    "take",
    "wonder",
    "work",
    "straight",
    "pred",
    "think",
    "using",
    "need",
    "logits",
    "logits",
    "train",
    "oh",
    "google",
    "collab",
    "correcting",
    "wrong",
    "thing",
    "logits",
    "using",
    "bce",
    "logits",
    "loss",
    "let",
    "keep",
    "pushing",
    "forward",
    "want",
    "accuracy",
    "accuracy",
    "function",
    "going",
    "pass",
    "order",
    "reverse",
    "little",
    "confusing",
    "kept",
    "evaluation",
    "function",
    "order",
    "scikit",
    "loan",
    "pred",
    "equals",
    "pred",
    "three",
    "going",
    "zero",
    "gradients",
    "optimizer",
    "optimizer",
    "zero",
    "grad",
    "might",
    "notice",
    "started",
    "pick",
    "pace",
    "little",
    "perfectly",
    "fine",
    "typing",
    "fast",
    "always",
    "slow",
    "video",
    "could",
    "watch",
    "code",
    "afterwards",
    "code",
    "resources",
    "always",
    "available",
    "going",
    "take",
    "last",
    "backward",
    "perform",
    "back",
    "propagation",
    "reason",
    "going",
    "faster",
    "covered",
    "steps",
    "anything",
    "sort",
    "spend",
    "time",
    "covered",
    "previous",
    "video",
    "optimizer",
    "step",
    "adjustments",
    "models",
    "parameters",
    "going",
    "take",
    "place",
    "hopefully",
    "create",
    "better",
    "representation",
    "data",
    "got",
    "testing",
    "first",
    "step",
    "testing",
    "well",
    "call",
    "model",
    "one",
    "dot",
    "vowel",
    "put",
    "evaluation",
    "mode",
    "making",
    "predictions",
    "going",
    "turn",
    "torch",
    "inference",
    "mode",
    "predictions",
    "call",
    "predictions",
    "places",
    "call",
    "inference",
    "remember",
    "machine",
    "learning",
    "lot",
    "different",
    "names",
    "thing",
    "forward",
    "pass",
    "going",
    "create",
    "test",
    "logits",
    "equals",
    "model",
    "one",
    "x",
    "test",
    "going",
    "squeeze",
    "wo",
    "want",
    "extra",
    "one",
    "dimension",
    "going",
    "add",
    "code",
    "cells",
    "space",
    "typing",
    "middle",
    "screen",
    "going",
    "put",
    "test",
    "pred",
    "get",
    "logits",
    "predictions",
    "well",
    "go",
    "torch",
    "dot",
    "round",
    "go",
    "torch",
    "dot",
    "sigmoid",
    "sigmoid",
    "working",
    "binary",
    "classification",
    "problem",
    "convert",
    "logits",
    "binary",
    "classification",
    "problem",
    "prediction",
    "probabilities",
    "use",
    "sigmoid",
    "activation",
    "function",
    "going",
    "calculate",
    "loss",
    "wrong",
    "model",
    "test",
    "data",
    "test",
    "last",
    "equals",
    "loss",
    "function",
    "going",
    "pass",
    "test",
    "logits",
    "going",
    "pass",
    "test",
    "ideal",
    "labels",
    "going",
    "also",
    "calculate",
    "test",
    "accuracy",
    "test",
    "accuracy",
    "going",
    "take",
    "true",
    "equals",
    "test",
    "test",
    "labels",
    "pred",
    "equals",
    "test",
    "pred",
    "test",
    "predictions",
    "test",
    "predictions",
    "final",
    "step",
    "print",
    "happening",
    "print",
    "happening",
    "oh",
    "every",
    "tutorial",
    "needs",
    "song",
    "could",
    "teach",
    "everything",
    "song",
    "song",
    "dance",
    "training",
    "1000",
    "epochs",
    "every",
    "100",
    "epochs",
    "print",
    "something",
    "print",
    "f",
    "string",
    "going",
    "write",
    "epoch",
    "know",
    "epoch",
    "models",
    "going",
    "print",
    "loss",
    "course",
    "going",
    "training",
    "loss",
    "test",
    "loss",
    "test",
    "front",
    "accuracy",
    "course",
    "going",
    "training",
    "accuracy",
    "go",
    "going",
    "pipe",
    "going",
    "print",
    "test",
    "loss",
    "want",
    "test",
    "loss",
    "going",
    "take",
    "five",
    "decimal",
    "places",
    "see",
    "printouts",
    "different",
    "values",
    "worry",
    "much",
    "exact",
    "numbers",
    "screen",
    "appearing",
    "screen",
    "inherent",
    "randomness",
    "machine",
    "learning",
    "got",
    "direction",
    "important",
    "got",
    "need",
    "percentage",
    "sign",
    "going",
    "bit",
    "complete",
    "accuracy",
    "got",
    "errors",
    "know",
    "coded",
    "free",
    "hand",
    "right",
    "lot",
    "code",
    "going",
    "train",
    "next",
    "model",
    "biggest",
    "model",
    "built",
    "far",
    "course",
    "three",
    "layers",
    "10",
    "hidden",
    "units",
    "layer",
    "let",
    "see",
    "got",
    "three",
    "two",
    "one",
    "run",
    "oh",
    "thousand",
    "epochs",
    "extra",
    "hidden",
    "layer",
    "hidden",
    "units",
    "still",
    "model",
    "still",
    "basically",
    "coin",
    "toss",
    "50",
    "ca",
    "real",
    "let",
    "plot",
    "decision",
    "boundary",
    "plot",
    "decision",
    "boundary",
    "find",
    "let",
    "get",
    "bit",
    "visual",
    "plot",
    "figure",
    "actually",
    "prevent",
    "us",
    "writing",
    "plot",
    "code",
    "let",
    "go",
    "copy",
    "know",
    "biggest",
    "fan",
    "copying",
    "code",
    "case",
    "already",
    "written",
    "nothing",
    "really",
    "new",
    "cover",
    "going",
    "change",
    "model",
    "zero",
    "model",
    "one",
    "new",
    "model",
    "trained",
    "behind",
    "scenes",
    "plot",
    "decision",
    "boundary",
    "going",
    "make",
    "predictions",
    "target",
    "model",
    "target",
    "data",
    "set",
    "put",
    "nice",
    "visual",
    "representation",
    "us",
    "oh",
    "said",
    "nice",
    "visual",
    "representation",
    "look",
    "like",
    "got",
    "coin",
    "toss",
    "data",
    "set",
    "model",
    "trying",
    "draw",
    "straight",
    "line",
    "separate",
    "circular",
    "data",
    "model",
    "based",
    "linear",
    "data",
    "nonlinear",
    "hmm",
    "maybe",
    "revealed",
    "tricks",
    "done",
    "couple",
    "reveals",
    "past",
    "videos",
    "still",
    "quite",
    "annoying",
    "fairly",
    "annoying",
    "training",
    "models",
    "working",
    "verify",
    "model",
    "learn",
    "anything",
    "right",
    "basically",
    "guessing",
    "data",
    "set",
    "model",
    "looks",
    "lot",
    "like",
    "model",
    "built",
    "section",
    "let",
    "go",
    "back",
    "learn",
    "book",
    "pytorch",
    "workflow",
    "fundamentals",
    "create",
    "model",
    "model",
    "building",
    "essentials",
    "build",
    "model",
    "linear",
    "regression",
    "model",
    "yeah",
    "dot",
    "linear",
    "built",
    "model",
    "changed",
    "01",
    "added",
    "couple",
    "layers",
    "forward",
    "computation",
    "quite",
    "similar",
    "model",
    "learn",
    "something",
    "straight",
    "line",
    "model",
    "learn",
    "something",
    "straight",
    "line",
    "challenge",
    "grab",
    "data",
    "set",
    "created",
    "previous",
    "notebook",
    "data",
    "could",
    "reproduce",
    "exact",
    "data",
    "set",
    "see",
    "write",
    "code",
    "fit",
    "model",
    "built",
    "one",
    "data",
    "set",
    "created",
    "want",
    "verify",
    "model",
    "learn",
    "anything",
    "right",
    "seems",
    "like",
    "learning",
    "anything",
    "quite",
    "frustrating",
    "give",
    "shot",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "past",
    "videos",
    "tried",
    "build",
    "model",
    "separate",
    "blue",
    "red",
    "dots",
    "yet",
    "previous",
    "efforts",
    "proven",
    "futile",
    "worry",
    "going",
    "get",
    "promise",
    "going",
    "get",
    "may",
    "little",
    "bit",
    "inside",
    "information",
    "going",
    "build",
    "model",
    "separate",
    "blue",
    "dots",
    "red",
    "dots",
    "fundamental",
    "classification",
    "model",
    "tried",
    "things",
    "last",
    "couple",
    "videos",
    "training",
    "longer",
    "epochs",
    "added",
    "another",
    "layer",
    "increased",
    "hidden",
    "units",
    "learned",
    "methods",
    "improve",
    "model",
    "model",
    "perspective",
    "upgrading",
    "hyperparameters",
    "number",
    "layers",
    "hidden",
    "units",
    "fitting",
    "longer",
    "changing",
    "activation",
    "functions",
    "changing",
    "learning",
    "rate",
    "quite",
    "done",
    "one",
    "yet",
    "changing",
    "loss",
    "function",
    "one",
    "way",
    "like",
    "troubleshoot",
    "problems",
    "going",
    "put",
    "subheading",
    "going",
    "prepare",
    "preparing",
    "data",
    "see",
    "model",
    "fit",
    "straight",
    "line",
    "one",
    "way",
    "troubleshoot",
    "trick",
    "troubleshooting",
    "problems",
    "especially",
    "neural",
    "networks",
    "machine",
    "learning",
    "general",
    "troubleshoot",
    "larger",
    "problem",
    "test",
    "smaller",
    "problem",
    "well",
    "know",
    "something",
    "working",
    "previous",
    "section",
    "01",
    "pytorch",
    "workflow",
    "fundamentals",
    "built",
    "model",
    "worked",
    "go",
    "right",
    "know",
    "linear",
    "model",
    "fit",
    "straight",
    "line",
    "going",
    "replicate",
    "data",
    "set",
    "fit",
    "straight",
    "line",
    "see",
    "model",
    "building",
    "learn",
    "anything",
    "right",
    "seems",
    "like",
    "ca",
    "tossing",
    "coin",
    "displayed",
    "data",
    "ideal",
    "let",
    "make",
    "data",
    "yeah",
    "let",
    "create",
    "smaller",
    "problem",
    "one",
    "know",
    "works",
    "add",
    "complexity",
    "try",
    "solve",
    "larger",
    "problem",
    "create",
    "data",
    "going",
    "notebook",
    "going",
    "set",
    "weight",
    "equals",
    "bias",
    "equals",
    "going",
    "move",
    "quite",
    "quickly",
    "seen",
    "module",
    "one",
    "overall",
    "takeaway",
    "going",
    "see",
    "model",
    "works",
    "kind",
    "problem",
    "something",
    "fundamentally",
    "wrong",
    "create",
    "data",
    "going",
    "call",
    "x",
    "regression",
    "straight",
    "line",
    "want",
    "predict",
    "number",
    "rather",
    "class",
    "might",
    "thinking",
    "oh",
    "might",
    "change",
    "things",
    "model",
    "architecture",
    "well",
    "see",
    "second",
    "dot",
    "unsqueeze",
    "going",
    "go",
    "first",
    "dimension",
    "dim",
    "equals",
    "one",
    "regression",
    "going",
    "use",
    "linear",
    "regression",
    "formula",
    "well",
    "wait",
    "times",
    "x",
    "x",
    "regression",
    "working",
    "new",
    "data",
    "set",
    "plus",
    "bias",
    "linear",
    "regression",
    "formula",
    "without",
    "epsilon",
    "simplified",
    "version",
    "linear",
    "regression",
    "formula",
    "seen",
    "previous",
    "section",
    "let",
    "check",
    "data",
    "nothing",
    "really",
    "covered",
    "going",
    "sanity",
    "check",
    "make",
    "sure",
    "dealing",
    "dealing",
    "dealing",
    "load",
    "garbage",
    "data",
    "machine",
    "learning",
    "ca",
    "stress",
    "enough",
    "data",
    "explorer",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "oh",
    "get",
    "wrong",
    "unsqueeze",
    "notice",
    "typo",
    "say",
    "something",
    "kidding",
    "go",
    "okay",
    "got",
    "100",
    "samples",
    "got",
    "different",
    "step",
    "size",
    "right",
    "let",
    "little",
    "bit",
    "fun",
    "got",
    "one",
    "know",
    "little",
    "bit",
    "one",
    "x",
    "value",
    "per",
    "value",
    "similar",
    "data",
    "set",
    "use",
    "data",
    "set",
    "well",
    "already",
    "got",
    "training",
    "test",
    "splits",
    "better",
    "make",
    "create",
    "train",
    "test",
    "splits",
    "going",
    "go",
    "train",
    "split",
    "going",
    "use",
    "80",
    "equals",
    "int",
    "times",
    "length",
    "could",
    "put",
    "100",
    "going",
    "specific",
    "going",
    "go",
    "x",
    "train",
    "regression",
    "train",
    "regression",
    "equals",
    "equal",
    "well",
    "going",
    "go",
    "x",
    "regression",
    "going",
    "index",
    "train",
    "split",
    "regression",
    "going",
    "index",
    "train",
    "split",
    "wonderful",
    "test",
    "creating",
    "test",
    "data",
    "nothing",
    "really",
    "new",
    "need",
    "discuss",
    "creating",
    "training",
    "test",
    "sets",
    "well",
    "model",
    "going",
    "hopefully",
    "learn",
    "patterns",
    "training",
    "data",
    "set",
    "able",
    "model",
    "testing",
    "data",
    "set",
    "going",
    "see",
    "second",
    "check",
    "length",
    "length",
    "x",
    "train",
    "regression",
    "might",
    "check",
    "x",
    "train",
    "x",
    "test",
    "regression",
    "going",
    "go",
    "length",
    "train",
    "regression",
    "long",
    "variable",
    "names",
    "excuse",
    "want",
    "keep",
    "separate",
    "already",
    "existing",
    "x",
    "data",
    "values",
    "80",
    "20",
    "80",
    "20",
    "beautiful",
    "80",
    "training",
    "samples",
    "100",
    "testing",
    "samples",
    "enough",
    "got",
    "helper",
    "functions",
    "file",
    "remember",
    "wrote",
    "code",
    "download",
    "course",
    "github",
    "imported",
    "plot",
    "predictions",
    "look",
    "helper",
    "contains",
    "plot",
    "predictions",
    "function",
    "created",
    "last",
    "section",
    "section",
    "go",
    "plot",
    "predictions",
    "running",
    "exact",
    "function",
    "run",
    "going",
    "save",
    "us",
    "typing",
    "beauty",
    "helper",
    "file",
    "come",
    "let",
    "plot",
    "data",
    "visually",
    "inspected",
    "right",
    "numbers",
    "page",
    "going",
    "plot",
    "really",
    "predictions",
    "predictions",
    "yet",
    "pass",
    "train",
    "data",
    "equal",
    "x",
    "train",
    "regression",
    "next",
    "one",
    "train",
    "labels",
    "equal",
    "train",
    "regression",
    "test",
    "data",
    "equal",
    "x",
    "test",
    "regression",
    "test",
    "labels",
    "think",
    "labels",
    "yeah",
    "go",
    "test",
    "progression",
    "might",
    "proven",
    "wrong",
    "try",
    "run",
    "function",
    "okay",
    "go",
    "training",
    "data",
    "testing",
    "data",
    "think",
    "model",
    "model",
    "one",
    "look",
    "model",
    "one",
    "could",
    "fit",
    "data",
    "right",
    "amount",
    "features",
    "may",
    "adjust",
    "slightly",
    "like",
    "think",
    "change",
    "input",
    "features",
    "model",
    "data",
    "set",
    "change",
    "features",
    "model",
    "data",
    "set",
    "find",
    "next",
    "video",
    "welcome",
    "back",
    "currently",
    "working",
    "little",
    "side",
    "project",
    "really",
    "philosophy",
    "created",
    "straight",
    "line",
    "data",
    "set",
    "know",
    "built",
    "model",
    "past",
    "back",
    "section",
    "01",
    "fit",
    "straight",
    "line",
    "data",
    "set",
    "well",
    "model",
    "built",
    "far",
    "fitting",
    "working",
    "circular",
    "data",
    "set",
    "classification",
    "data",
    "set",
    "one",
    "way",
    "troubleshoot",
    "larger",
    "problem",
    "test",
    "smaller",
    "problem",
    "first",
    "later",
    "working",
    "big",
    "machine",
    "learning",
    "data",
    "set",
    "probably",
    "start",
    "smaller",
    "portion",
    "data",
    "set",
    "first",
    "likewise",
    "larger",
    "machine",
    "learning",
    "model",
    "instead",
    "starting",
    "huge",
    "model",
    "start",
    "small",
    "model",
    "taking",
    "step",
    "back",
    "see",
    "model",
    "going",
    "learn",
    "anything",
    "straight",
    "line",
    "data",
    "set",
    "improve",
    "line",
    "data",
    "set",
    "another",
    "hint",
    "oh",
    "going",
    "cover",
    "second",
    "promise",
    "let",
    "see",
    "adjust",
    "model",
    "one",
    "fit",
    "straight",
    "line",
    "question",
    "end",
    "last",
    "video",
    "adjust",
    "parameters",
    "model",
    "one",
    "way",
    "shape",
    "form",
    "fit",
    "straight",
    "line",
    "data",
    "may",
    "realized",
    "may",
    "model",
    "one",
    "set",
    "classification",
    "data",
    "two",
    "x",
    "input",
    "features",
    "whereas",
    "data",
    "go",
    "x",
    "train",
    "regression",
    "many",
    "input",
    "features",
    "get",
    "first",
    "sample",
    "one",
    "value",
    "maybe",
    "get",
    "first",
    "one",
    "value",
    "per",
    "let",
    "remind",
    "input",
    "output",
    "shapes",
    "one",
    "fundamental",
    "things",
    "machine",
    "learning",
    "deep",
    "learning",
    "trust",
    "still",
    "get",
    "wrong",
    "time",
    "harping",
    "one",
    "feature",
    "per",
    "one",
    "label",
    "adjust",
    "model",
    "slightly",
    "change",
    "end",
    "features",
    "one",
    "instead",
    "two",
    "features",
    "stay",
    "want",
    "one",
    "number",
    "come",
    "going",
    "code",
    "little",
    "bit",
    "different",
    "version",
    "model",
    "one",
    "architecture",
    "model",
    "one",
    "using",
    "nn",
    "dot",
    "sequential",
    "going",
    "faster",
    "way",
    "coding",
    "model",
    "let",
    "create",
    "model",
    "two",
    "nn",
    "dot",
    "sequential",
    "thing",
    "going",
    "change",
    "number",
    "input",
    "features",
    "exact",
    "code",
    "model",
    "one",
    "difference",
    "said",
    "features",
    "features",
    "one",
    "go",
    "features",
    "equals",
    "10",
    "hidden",
    "units",
    "first",
    "layer",
    "course",
    "second",
    "layer",
    "number",
    "features",
    "line",
    "features",
    "previous",
    "layer",
    "one",
    "going",
    "output",
    "10",
    "features",
    "well",
    "scaling",
    "things",
    "one",
    "feature",
    "10",
    "try",
    "give",
    "model",
    "much",
    "chance",
    "many",
    "parameters",
    "possible",
    "course",
    "could",
    "make",
    "number",
    "quite",
    "large",
    "could",
    "make",
    "thousand",
    "features",
    "want",
    "upper",
    "bound",
    "things",
    "going",
    "let",
    "find",
    "experience",
    "machine",
    "learning",
    "engineer",
    "data",
    "scientist",
    "keeping",
    "nice",
    "small",
    "run",
    "many",
    "experiments",
    "possible",
    "beautiful",
    "look",
    "created",
    "sequential",
    "model",
    "happens",
    "nn",
    "dot",
    "sequential",
    "data",
    "goes",
    "passes",
    "layer",
    "passes",
    "layer",
    "passes",
    "layer",
    "happens",
    "goes",
    "layer",
    "triggers",
    "layers",
    "forward",
    "method",
    "internal",
    "forward",
    "method",
    "case",
    "nn",
    "dot",
    "linear",
    "seen",
    "got",
    "linear",
    "regression",
    "formula",
    "go",
    "nn",
    "dot",
    "linear",
    "performs",
    "mathematical",
    "operation",
    "linear",
    "transformation",
    "seen",
    "let",
    "keep",
    "pushing",
    "forward",
    "let",
    "create",
    "loss",
    "optimizer",
    "loss",
    "optimize",
    "going",
    "work",
    "workflow",
    "loss",
    "function",
    "adjust",
    "slightly",
    "going",
    "use",
    "l1",
    "loss",
    "dealing",
    "regression",
    "problem",
    "rather",
    "classification",
    "problem",
    "optimizer",
    "use",
    "optimizer",
    "bring",
    "exact",
    "optimizer",
    "sgd",
    "using",
    "classification",
    "data",
    "model",
    "two",
    "dot",
    "params",
    "parameters",
    "always",
    "get",
    "little",
    "bit",
    "confused",
    "give",
    "lr",
    "using",
    "far",
    "params",
    "want",
    "optimizer",
    "optimize",
    "model",
    "two",
    "parameters",
    "learning",
    "rate",
    "learning",
    "rate",
    "amount",
    "parameter",
    "multiplier",
    "applied",
    "parameter",
    "epoch",
    "let",
    "train",
    "model",
    "think",
    "could",
    "video",
    "think",
    "might",
    "train",
    "training",
    "data",
    "set",
    "evaluate",
    "test",
    "data",
    "set",
    "separately",
    "set",
    "manual",
    "seeds",
    "cuda",
    "set",
    "model",
    "device",
    "gpu",
    "whatever",
    "device",
    "active",
    "set",
    "number",
    "epochs",
    "many",
    "epochs",
    "set",
    "well",
    "set",
    "thousand",
    "keep",
    "epochs",
    "equals",
    "thousand",
    "getting",
    "really",
    "good",
    "sort",
    "stuff",
    "let",
    "put",
    "data",
    "put",
    "data",
    "target",
    "device",
    "know",
    "done",
    "lot",
    "similar",
    "steps",
    "reason",
    "kept",
    "like",
    "buy",
    "end",
    "course",
    "sort",
    "know",
    "stuff",
    "heart",
    "even",
    "know",
    "heart",
    "trust",
    "know",
    "look",
    "x",
    "train",
    "regression",
    "going",
    "send",
    "device",
    "going",
    "go",
    "train",
    "regression",
    "reminder",
    "something",
    "get",
    "think",
    "writing",
    "code",
    "would",
    "happen",
    "put",
    "data",
    "device",
    "model",
    "seen",
    "error",
    "come",
    "happens",
    "well",
    "kind",
    "given",
    "away",
    "daniel",
    "well",
    "great",
    "question",
    "code",
    "air",
    "oh",
    "well",
    "worry",
    "plenty",
    "questions",
    "giving",
    "given",
    "answer",
    "yet",
    "device",
    "beautiful",
    "got",
    "device",
    "agnostic",
    "code",
    "model",
    "data",
    "let",
    "loop",
    "epochs",
    "train",
    "going",
    "epoch",
    "range",
    "epochs",
    "epoch",
    "range",
    "forward",
    "pass",
    "calculate",
    "loss",
    "pred",
    "equals",
    "model",
    "two",
    "forward",
    "pass",
    "x",
    "train",
    "regression",
    "going",
    "work",
    "hunky",
    "dory",
    "model",
    "data",
    "device",
    "loss",
    "equals",
    "going",
    "bring",
    "loss",
    "function",
    "going",
    "compare",
    "predictions",
    "train",
    "regression",
    "labels",
    "next",
    "optimize",
    "zero",
    "grad",
    "optimize",
    "dot",
    "zero",
    "grad",
    "comments",
    "look",
    "us",
    "go",
    "loss",
    "backward",
    "next",
    "optimize",
    "step",
    "step",
    "step",
    "course",
    "could",
    "testing",
    "testing",
    "go",
    "model",
    "two",
    "dot",
    "vowel",
    "go",
    "torch",
    "dot",
    "inference",
    "mode",
    "forward",
    "pass",
    "create",
    "test",
    "predictions",
    "equals",
    "model",
    "two",
    "dot",
    "x",
    "test",
    "regression",
    "go",
    "test",
    "loss",
    "equals",
    "loss",
    "fn",
    "test",
    "predictions",
    "versus",
    "test",
    "labels",
    "beautiful",
    "look",
    "done",
    "optimization",
    "loop",
    "something",
    "spent",
    "whole",
    "hour",
    "maybe",
    "even",
    "longer",
    "ten",
    "lines",
    "code",
    "course",
    "could",
    "shorten",
    "making",
    "function",
    "going",
    "see",
    "later",
    "rather",
    "us",
    "give",
    "little",
    "bit",
    "practice",
    "still",
    "bit",
    "fresh",
    "print",
    "happening",
    "let",
    "print",
    "happening",
    "training",
    "thousand",
    "epochs",
    "like",
    "idea",
    "printing",
    "something",
    "every",
    "100",
    "epochs",
    "enough",
    "step",
    "epoch",
    "got",
    "put",
    "epoch",
    "f",
    "string",
    "go",
    "loss",
    "loss",
    "maybe",
    "get",
    "first",
    "five",
    "five",
    "decimal",
    "places",
    "accuracy",
    "working",
    "regression",
    "get",
    "test",
    "loss",
    "going",
    "well",
    "beautiful",
    "got",
    "mistakes",
    "think",
    "even",
    "run",
    "code",
    "cell",
    "run",
    "three",
    "see",
    "got",
    "look",
    "oh",
    "goodness",
    "loss",
    "loss",
    "going",
    "means",
    "model",
    "must",
    "learning",
    "something",
    "adjusted",
    "learning",
    "rate",
    "think",
    "went",
    "something",
    "anything",
    "oh",
    "yes",
    "look",
    "low",
    "loss",
    "gets",
    "test",
    "data",
    "set",
    "let",
    "confirm",
    "got",
    "make",
    "predictions",
    "well",
    "maybe",
    "next",
    "video",
    "yeah",
    "one",
    "getting",
    "long",
    "good",
    "created",
    "straight",
    "line",
    "data",
    "set",
    "created",
    "model",
    "fit",
    "set",
    "loss",
    "optimizer",
    "already",
    "put",
    "data",
    "target",
    "device",
    "trained",
    "tested",
    "model",
    "must",
    "learning",
    "something",
    "like",
    "give",
    "shot",
    "confirming",
    "using",
    "plot",
    "predictions",
    "function",
    "make",
    "predictions",
    "trained",
    "model",
    "forget",
    "turn",
    "inference",
    "mode",
    "see",
    "red",
    "dots",
    "fairly",
    "close",
    "green",
    "dots",
    "next",
    "plot",
    "give",
    "shot",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "something",
    "exciting",
    "solved",
    "smaller",
    "problem",
    "giving",
    "us",
    "hint",
    "towards",
    "larger",
    "problem",
    "know",
    "model",
    "previously",
    "building",
    "model",
    "two",
    "capacity",
    "learn",
    "something",
    "know",
    "well",
    "created",
    "straight",
    "line",
    "data",
    "set",
    "replicated",
    "architecture",
    "used",
    "model",
    "one",
    "recall",
    "model",
    "one",
    "work",
    "well",
    "classification",
    "data",
    "little",
    "bit",
    "adjustment",
    "changing",
    "number",
    "features",
    "much",
    "different",
    "training",
    "code",
    "except",
    "different",
    "loss",
    "function",
    "well",
    "use",
    "mae",
    "loss",
    "regression",
    "data",
    "changed",
    "learning",
    "rate",
    "slightly",
    "found",
    "maybe",
    "model",
    "could",
    "learn",
    "bit",
    "better",
    "encourage",
    "play",
    "around",
    "different",
    "values",
    "learning",
    "rate",
    "fact",
    "anything",
    "changed",
    "try",
    "change",
    "see",
    "happens",
    "one",
    "best",
    "ways",
    "learn",
    "goes",
    "machine",
    "learning",
    "models",
    "trained",
    "number",
    "epochs",
    "set",
    "device",
    "agnostic",
    "code",
    "training",
    "testing",
    "loop",
    "look",
    "looks",
    "oh",
    "goodness",
    "well",
    "done",
    "loss",
    "went",
    "hmm",
    "tell",
    "us",
    "well",
    "tells",
    "us",
    "model",
    "two",
    "specific",
    "architecture",
    "capacity",
    "learn",
    "something",
    "must",
    "missing",
    "something",
    "going",
    "get",
    "minute",
    "promise",
    "going",
    "confirm",
    "model",
    "learned",
    "something",
    "numbers",
    "page",
    "going",
    "getting",
    "visual",
    "turn",
    "going",
    "make",
    "predictions",
    "plot",
    "may",
    "already",
    "done",
    "issued",
    "challenge",
    "last",
    "end",
    "last",
    "video",
    "turn",
    "evaluation",
    "mode",
    "let",
    "go",
    "model",
    "two",
    "dot",
    "eval",
    "let",
    "make",
    "predictions",
    "also",
    "known",
    "inference",
    "going",
    "go",
    "torch",
    "dot",
    "inference",
    "mode",
    "inference",
    "mode",
    "torch",
    "dot",
    "inference",
    "mode",
    "make",
    "predictions",
    "going",
    "save",
    "preds",
    "going",
    "use",
    "model",
    "two",
    "going",
    "pass",
    "ex",
    "test",
    "regression",
    "work",
    "set",
    "device",
    "agnostic",
    "code",
    "plot",
    "data",
    "predictions",
    "course",
    "use",
    "plot",
    "predictions",
    "function",
    "imported",
    "via",
    "helper",
    "functions",
    "dot",
    "pi",
    "function",
    "code",
    "cells",
    "like",
    "check",
    "let",
    "set",
    "train",
    "data",
    "train",
    "data",
    "parameter",
    "x",
    "train",
    "regression",
    "goodness",
    "google",
    "collab",
    "already",
    "typing",
    "fast",
    "enough",
    "slow",
    "giving",
    "wrong",
    "auto",
    "corrects",
    "train",
    "label",
    "equals",
    "train",
    "regression",
    "going",
    "pass",
    "test",
    "data",
    "equals",
    "ex",
    "test",
    "regression",
    "going",
    "pass",
    "test",
    "labels",
    "test",
    "regression",
    "got",
    "many",
    "variables",
    "going",
    "goodness",
    "gracious",
    "could",
    "done",
    "better",
    "naming",
    "preds",
    "plot",
    "look",
    "like",
    "oh",
    "got",
    "error",
    "secretly",
    "kind",
    "knew",
    "coming",
    "ahead",
    "time",
    "advantage",
    "host",
    "machine",
    "learning",
    "cooking",
    "show",
    "type",
    "error",
    "fix",
    "remember",
    "asked",
    "one",
    "last",
    "videos",
    "would",
    "happen",
    "data",
    "device",
    "model",
    "well",
    "get",
    "error",
    "right",
    "little",
    "bit",
    "different",
    "well",
    "seen",
    "one",
    "got",
    "cuda",
    "device",
    "type",
    "tensa",
    "numpy",
    "coming",
    "well",
    "plot",
    "predictions",
    "function",
    "uses",
    "mapplotlib",
    "behind",
    "scenes",
    "mapplotlib",
    "references",
    "numpy",
    "another",
    "numerical",
    "computing",
    "library",
    "however",
    "numpy",
    "uses",
    "cpu",
    "rather",
    "gpu",
    "call",
    "dot",
    "cpu",
    "helpful",
    "message",
    "telling",
    "us",
    "call",
    "tensa",
    "dot",
    "cpu",
    "use",
    "tensors",
    "numpy",
    "let",
    "call",
    "dot",
    "cpu",
    "tensor",
    "inputs",
    "see",
    "solves",
    "problem",
    "wonderful",
    "looks",
    "like",
    "oh",
    "goodness",
    "look",
    "red",
    "dots",
    "close",
    "well",
    "okay",
    "confirms",
    "suspicions",
    "kind",
    "already",
    "knew",
    "model",
    "capacity",
    "learn",
    "data",
    "set",
    "changed",
    "data",
    "set",
    "worked",
    "hmm",
    "data",
    "model",
    "ca",
    "learn",
    "like",
    "circular",
    "data",
    "model",
    "remember",
    "model",
    "comprised",
    "linear",
    "functions",
    "linear",
    "linear",
    "straight",
    "line",
    "data",
    "made",
    "straight",
    "lines",
    "think",
    "got",
    "nonlinearities",
    "big",
    "secret",
    "holding",
    "back",
    "reveal",
    "starting",
    "next",
    "video",
    "want",
    "head",
    "start",
    "go",
    "torch",
    "end",
    "look",
    "documentation",
    "speaking",
    "lot",
    "linear",
    "functions",
    "nonlinear",
    "activations",
    "give",
    "another",
    "spoiler",
    "actually",
    "seen",
    "one",
    "nonlinear",
    "activations",
    "throughout",
    "notebook",
    "go",
    "check",
    "see",
    "infer",
    "see",
    "next",
    "video",
    "let",
    "get",
    "started",
    "nonlinearities",
    "welcome",
    "back",
    "last",
    "video",
    "saw",
    "model",
    "building",
    "potential",
    "learn",
    "mean",
    "look",
    "predictions",
    "could",
    "get",
    "little",
    "bit",
    "better",
    "course",
    "get",
    "red",
    "dots",
    "top",
    "green",
    "dots",
    "going",
    "leave",
    "trend",
    "model",
    "capacity",
    "learn",
    "except",
    "straight",
    "line",
    "data",
    "hinting",
    "fair",
    "bit",
    "using",
    "linear",
    "functions",
    "look",
    "linear",
    "data",
    "look",
    "like",
    "well",
    "quite",
    "straight",
    "line",
    "go",
    "linear",
    "search",
    "linear",
    "give",
    "us",
    "linear",
    "means",
    "straight",
    "go",
    "straight",
    "happens",
    "search",
    "nonlinear",
    "kind",
    "hinted",
    "well",
    "nonlinear",
    "oh",
    "get",
    "curves",
    "get",
    "curved",
    "lines",
    "linear",
    "functions",
    "straight",
    "nonlinear",
    "functions",
    "hmm",
    "one",
    "beautiful",
    "things",
    "machine",
    "learning",
    "sure",
    "high",
    "school",
    "kind",
    "learned",
    "concept",
    "called",
    "line",
    "best",
    "fit",
    "equals",
    "mx",
    "plus",
    "c",
    "equals",
    "mx",
    "plus",
    "looks",
    "something",
    "like",
    "wanted",
    "go",
    "use",
    "quadratic",
    "functions",
    "whole",
    "bunch",
    "stuff",
    "one",
    "fundamental",
    "things",
    "machine",
    "learning",
    "build",
    "neural",
    "networks",
    "deep",
    "neural",
    "networks",
    "combination",
    "could",
    "large",
    "combination",
    "linear",
    "functions",
    "nonlinear",
    "functions",
    "nonlinear",
    "activations",
    "different",
    "types",
    "layers",
    "essentially",
    "deep",
    "combining",
    "straight",
    "lines",
    "go",
    "back",
    "data",
    "non",
    "straight",
    "lines",
    "course",
    "model",
    "work",
    "given",
    "power",
    "use",
    "linear",
    "lines",
    "given",
    "power",
    "use",
    "straight",
    "lines",
    "data",
    "curved",
    "although",
    "simple",
    "need",
    "nonlinearity",
    "able",
    "model",
    "data",
    "set",
    "let",
    "say",
    "building",
    "pizza",
    "detection",
    "model",
    "let",
    "look",
    "images",
    "pizza",
    "one",
    "favorite",
    "foods",
    "images",
    "pizza",
    "right",
    "could",
    "model",
    "pizza",
    "straight",
    "lines",
    "thinking",
    "daniel",
    "ca",
    "serious",
    "computer",
    "vision",
    "model",
    "look",
    "straight",
    "lines",
    "argue",
    "yes",
    "except",
    "also",
    "add",
    "curved",
    "lines",
    "beauty",
    "machine",
    "learning",
    "could",
    "imagine",
    "trying",
    "write",
    "rules",
    "algorithm",
    "detect",
    "pizza",
    "maybe",
    "could",
    "put",
    "oh",
    "curve",
    "see",
    "red",
    "imagine",
    "trying",
    "hundred",
    "different",
    "foods",
    "program",
    "would",
    "get",
    "really",
    "large",
    "instead",
    "give",
    "machine",
    "learning",
    "models",
    "come",
    "model",
    "created",
    "give",
    "deep",
    "learning",
    "models",
    "capacity",
    "use",
    "linear",
    "nonlinear",
    "functions",
    "seen",
    "nonlinear",
    "layers",
    "yet",
    "maybe",
    "hinted",
    "right",
    "stack",
    "top",
    "layers",
    "model",
    "figures",
    "patterns",
    "data",
    "use",
    "lines",
    "draw",
    "draw",
    "patterns",
    "pizza",
    "another",
    "food",
    "sushi",
    "wanted",
    "build",
    "food",
    "image",
    "classification",
    "model",
    "would",
    "principle",
    "remains",
    "question",
    "going",
    "pose",
    "get",
    "come",
    "unlocked",
    "missing",
    "piece",
    "going",
    "cover",
    "next",
    "couple",
    "videos",
    "missing",
    "piece",
    "model",
    "big",
    "one",
    "going",
    "follow",
    "throughout",
    "machine",
    "learning",
    "deep",
    "learning",
    "nonlinearity",
    "question",
    "patterns",
    "could",
    "draw",
    "given",
    "infinite",
    "amount",
    "straight",
    "non",
    "straight",
    "lines",
    "machine",
    "learning",
    "terms",
    "infinite",
    "amount",
    "really",
    "finite",
    "infinite",
    "machine",
    "learning",
    "terms",
    "technicality",
    "could",
    "million",
    "parameters",
    "could",
    "got",
    "probably",
    "hundred",
    "parameters",
    "model",
    "imagine",
    "large",
    "amount",
    "straight",
    "non",
    "straight",
    "lines",
    "infinite",
    "amount",
    "linear",
    "nonlinear",
    "functions",
    "could",
    "draw",
    "pretty",
    "intricate",
    "patterns",
    "could",
    "gives",
    "machine",
    "learning",
    "especially",
    "neural",
    "networks",
    "capacity",
    "fit",
    "straight",
    "line",
    "separate",
    "two",
    "different",
    "circles",
    "also",
    "crazy",
    "things",
    "like",
    "drive",
    "car",
    "least",
    "power",
    "vision",
    "system",
    "car",
    "course",
    "need",
    "programming",
    "plan",
    "actually",
    "see",
    "image",
    "getting",
    "ahead",
    "let",
    "start",
    "diving",
    "nonlinearity",
    "whole",
    "idea",
    "combining",
    "power",
    "linear",
    "nonlinear",
    "functions",
    "straight",
    "lines",
    "non",
    "straight",
    "lines",
    "classification",
    "data",
    "comprised",
    "straight",
    "lines",
    "circles",
    "need",
    "nonlinearity",
    "recreating",
    "nonlinear",
    "data",
    "red",
    "blue",
    "circles",
    "need",
    "recreate",
    "going",
    "anyway",
    "completeness",
    "let",
    "get",
    "little",
    "bit",
    "practice",
    "make",
    "plot",
    "data",
    "practice",
    "use",
    "nonlinearity",
    "plot",
    "little",
    "bit",
    "dot",
    "pie",
    "plot",
    "plt",
    "going",
    "go",
    "bit",
    "faster",
    "covered",
    "code",
    "import",
    "make",
    "circles",
    "going",
    "recreate",
    "exact",
    "circle",
    "data",
    "set",
    "created",
    "number",
    "samples",
    "create",
    "thousand",
    "going",
    "create",
    "x",
    "equals",
    "make",
    "circles",
    "pass",
    "number",
    "samples",
    "beautiful",
    "colab",
    "please",
    "wonder",
    "turn",
    "autocorrect",
    "colab",
    "happy",
    "see",
    "errors",
    "flesh",
    "see",
    "look",
    "want",
    "want",
    "noise",
    "like",
    "maybe",
    "next",
    "video",
    "going",
    "spend",
    "time",
    "looking",
    "around",
    "work",
    "fly",
    "later",
    "excited",
    "share",
    "power",
    "nonlinearity",
    "x",
    "going",
    "plot",
    "going",
    "got",
    "two",
    "x",
    "features",
    "going",
    "color",
    "flavor",
    "binary",
    "classification",
    "going",
    "use",
    "one",
    "favorite",
    "c",
    "maps",
    "color",
    "map",
    "going",
    "go",
    "plt",
    "dot",
    "cm",
    "c",
    "map",
    "red",
    "blue",
    "get",
    "okay",
    "red",
    "circle",
    "blue",
    "circle",
    "hey",
    "color",
    "like",
    "color",
    "better",
    "get",
    "right",
    "oh",
    "goodness",
    "look",
    "much",
    "code",
    "written",
    "yeah",
    "like",
    "blue",
    "going",
    "bring",
    "aesthetics",
    "machine",
    "learning",
    "numbers",
    "page",
    "could",
    "crass",
    "let",
    "go",
    "okay",
    "better",
    "color",
    "red",
    "blue",
    "small",
    "lively",
    "let",
    "convert",
    "train",
    "test",
    "start",
    "build",
    "model",
    "nonlinearity",
    "oh",
    "good",
    "okay",
    "convert",
    "data",
    "tenses",
    "train",
    "test",
    "splits",
    "nothing",
    "covered",
    "import",
    "torch",
    "never",
    "hurts",
    "practice",
    "code",
    "right",
    "import",
    "torch",
    "sklearn",
    "dot",
    "model",
    "selection",
    "import",
    "train",
    "test",
    "split",
    "split",
    "red",
    "blue",
    "dots",
    "randomly",
    "going",
    "turn",
    "data",
    "tenses",
    "go",
    "x",
    "equals",
    "torch",
    "numpy",
    "pass",
    "x",
    "change",
    "type",
    "torch",
    "dot",
    "float",
    "well",
    "oh",
    "goodness",
    "autocorrect",
    "getting",
    "best",
    "know",
    "watching",
    "live",
    "code",
    "stuff",
    "battle",
    "autocorrect",
    "whole",
    "course",
    "really",
    "teaching",
    "pie",
    "torch",
    "battling",
    "google",
    "collab",
    "autocorrect",
    "turning",
    "torch",
    "dot",
    "float",
    "type",
    "numpy",
    "default",
    "makes",
    "circles",
    "users",
    "behind",
    "scenes",
    "numpy",
    "actually",
    "using",
    "lot",
    "machine",
    "learning",
    "libraries",
    "pandas",
    "built",
    "numpy",
    "scikit",
    "learn",
    "lot",
    "numpy",
    "matplotlib",
    "numpy",
    "showing",
    "word",
    "ubiquitous",
    "ubiquity",
    "sure",
    "maybe",
    "correct",
    "ubiquity",
    "numpy",
    "test",
    "sets",
    "using",
    "pie",
    "torch",
    "leverage",
    "power",
    "autograd",
    "powers",
    "gradient",
    "descent",
    "fact",
    "use",
    "gpus",
    "creating",
    "training",
    "test",
    "splits",
    "train",
    "test",
    "split",
    "x",
    "going",
    "go",
    "test",
    "size",
    "equals",
    "going",
    "set",
    "random",
    "random",
    "state",
    "equals",
    "view",
    "first",
    "five",
    "samples",
    "going",
    "tenses",
    "fingers",
    "crossed",
    "got",
    "error",
    "beautiful",
    "tenses",
    "okay",
    "exciting",
    "part",
    "got",
    "data",
    "set",
    "back",
    "think",
    "time",
    "build",
    "model",
    "nonlinearity",
    "like",
    "peek",
    "ahead",
    "check",
    "torchnn",
    "little",
    "bit",
    "spoiler",
    "go",
    "nonlinear",
    "activation",
    "see",
    "find",
    "one",
    "already",
    "used",
    "challenge",
    "find",
    "one",
    "already",
    "used",
    "go",
    "search",
    "nonlinear",
    "function",
    "give",
    "go",
    "see",
    "comes",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "put",
    "hand",
    "ready",
    "learn",
    "nonlinearity",
    "know",
    "ca",
    "see",
    "hands",
    "better",
    "see",
    "hands",
    "better",
    "feel",
    "hands",
    "hands",
    "nonlinearity",
    "magic",
    "piece",
    "puzzle",
    "learn",
    "let",
    "title",
    "section",
    "building",
    "model",
    "nonlinearity",
    "linear",
    "equals",
    "straight",
    "lines",
    "turn",
    "nonlinear",
    "equals",
    "lines",
    "left",
    "end",
    "last",
    "video",
    "giving",
    "challenge",
    "checking",
    "torchnn",
    "module",
    "looking",
    "nonlinear",
    "function",
    "already",
    "used",
    "would",
    "go",
    "find",
    "thing",
    "oh",
    "nonlinear",
    "activations",
    "going",
    "fair",
    "things",
    "essentially",
    "modules",
    "within",
    "torchnn",
    "either",
    "form",
    "layer",
    "neural",
    "network",
    "recall",
    "let",
    "go",
    "neural",
    "network",
    "seen",
    "anatomy",
    "neural",
    "network",
    "generally",
    "input",
    "layer",
    "multiple",
    "hidden",
    "layers",
    "form",
    "output",
    "layer",
    "well",
    "multiple",
    "hidden",
    "layers",
    "almost",
    "combination",
    "torchnn",
    "fact",
    "almost",
    "combination",
    "function",
    "could",
    "imagine",
    "whether",
    "work",
    "another",
    "question",
    "pytorch",
    "implements",
    "common",
    "layers",
    "would",
    "hidden",
    "layers",
    "might",
    "pooling",
    "layers",
    "padding",
    "layers",
    "activation",
    "functions",
    "premise",
    "perform",
    "sort",
    "mathematical",
    "operation",
    "input",
    "look",
    "nonlinear",
    "activation",
    "functions",
    "might",
    "find",
    "n",
    "dot",
    "sigmoid",
    "used",
    "sigmoid",
    "activation",
    "function",
    "math",
    "terminology",
    "takes",
    "input",
    "x",
    "performs",
    "operation",
    "looks",
    "like",
    "straight",
    "line",
    "think",
    "put",
    "practice",
    "want",
    "example",
    "well",
    "example",
    "nonlinear",
    "activations",
    "examples",
    "well",
    "let",
    "go",
    "time",
    "otherwise",
    "going",
    "forever",
    "dot",
    "relu",
    "another",
    "common",
    "function",
    "saw",
    "looked",
    "architecture",
    "classification",
    "network",
    "said",
    "start",
    "code",
    "classification",
    "model",
    "nonlinearity",
    "course",
    "wanted",
    "could",
    "look",
    "nonlinear",
    "function",
    "wanted",
    "learn",
    "nonlinear",
    "means",
    "graph",
    "straight",
    "line",
    "oh",
    "beautiful",
    "learn",
    "nonlinear",
    "functions",
    "together",
    "write",
    "code",
    "let",
    "go",
    "build",
    "model",
    "nonlinear",
    "activation",
    "functions",
    "one",
    "thing",
    "write",
    "code",
    "got",
    "remembered",
    "got",
    "nice",
    "slide",
    "question",
    "posed",
    "previous",
    "video",
    "missing",
    "piece",
    "nonlinearity",
    "question",
    "want",
    "think",
    "could",
    "draw",
    "unlimited",
    "amount",
    "straight",
    "words",
    "linear",
    "nonlinear",
    "line",
    "seen",
    "previously",
    "build",
    "model",
    "linear",
    "model",
    "fit",
    "data",
    "straight",
    "line",
    "linear",
    "data",
    "working",
    "nonlinear",
    "data",
    "well",
    "need",
    "power",
    "nonlinear",
    "functions",
    "circular",
    "data",
    "2d",
    "plot",
    "keep",
    "mind",
    "whereas",
    "neural",
    "networks",
    "machine",
    "learning",
    "models",
    "work",
    "numbers",
    "hundreds",
    "dimensions",
    "impossible",
    "us",
    "humans",
    "visualize",
    "since",
    "computers",
    "love",
    "numbers",
    "piece",
    "cake",
    "torch",
    "import",
    "going",
    "create",
    "first",
    "neural",
    "network",
    "nonlinear",
    "activations",
    "exciting",
    "let",
    "create",
    "class",
    "create",
    "circle",
    "model",
    "got",
    "circle",
    "model",
    "v1",
    "already",
    "going",
    "create",
    "circle",
    "model",
    "v2",
    "inherit",
    "end",
    "dot",
    "module",
    "write",
    "constructor",
    "init",
    "function",
    "pass",
    "self",
    "go",
    "self",
    "super",
    "sorry",
    "many",
    "words",
    "dot",
    "underscore",
    "underscore",
    "init",
    "underscore",
    "go",
    "got",
    "constructor",
    "let",
    "create",
    "layer",
    "one",
    "self",
    "dot",
    "layer",
    "one",
    "equals",
    "used",
    "dot",
    "linear",
    "going",
    "create",
    "quite",
    "similar",
    "model",
    "built",
    "except",
    "one",
    "added",
    "feature",
    "going",
    "create",
    "features",
    "akin",
    "number",
    "x",
    "features",
    "different",
    "three",
    "x",
    "features",
    "might",
    "change",
    "three",
    "working",
    "two",
    "leave",
    "keep",
    "features",
    "10",
    "10",
    "hidden",
    "units",
    "go",
    "layer",
    "two",
    "dot",
    "linear",
    "values",
    "customizable",
    "hyper",
    "parameters",
    "let",
    "line",
    "features",
    "layer",
    "two",
    "layer",
    "three",
    "layer",
    "three",
    "going",
    "take",
    "outputs",
    "layer",
    "two",
    "needs",
    "features",
    "want",
    "layer",
    "three",
    "output",
    "layer",
    "want",
    "one",
    "number",
    "output",
    "set",
    "one",
    "fun",
    "part",
    "going",
    "introduce",
    "nonlinear",
    "function",
    "going",
    "introduce",
    "relu",
    "function",
    "seen",
    "sigmoid",
    "relu",
    "another",
    "common",
    "one",
    "actually",
    "quite",
    "simple",
    "let",
    "write",
    "first",
    "dot",
    "relu",
    "remember",
    "torch",
    "dot",
    "nn",
    "stores",
    "lot",
    "existing",
    "nonlinear",
    "activation",
    "functions",
    "necessarily",
    "code",
    "however",
    "want",
    "code",
    "relu",
    "function",
    "let",
    "show",
    "actually",
    "quite",
    "simple",
    "dive",
    "nn",
    "dot",
    "relu",
    "relu",
    "however",
    "want",
    "say",
    "usually",
    "say",
    "relu",
    "applies",
    "rectified",
    "linear",
    "unit",
    "function",
    "element",
    "wise",
    "means",
    "element",
    "wise",
    "every",
    "element",
    "input",
    "tensor",
    "stands",
    "rectified",
    "linear",
    "unit",
    "basically",
    "takes",
    "input",
    "input",
    "negative",
    "turns",
    "input",
    "zero",
    "leaves",
    "positive",
    "inputs",
    "line",
    "straight",
    "could",
    "argue",
    "yeah",
    "well",
    "straight",
    "straight",
    "form",
    "nonlinear",
    "activation",
    "function",
    "goes",
    "boom",
    "linear",
    "would",
    "stay",
    "straight",
    "like",
    "let",
    "see",
    "practice",
    "think",
    "going",
    "improve",
    "model",
    "well",
    "let",
    "find",
    "together",
    "hey",
    "forward",
    "need",
    "implement",
    "forward",
    "method",
    "going",
    "put",
    "nonlinear",
    "activation",
    "functions",
    "going",
    "put",
    "node",
    "relu",
    "nonlinear",
    "activation",
    "function",
    "remember",
    "wherever",
    "say",
    "function",
    "performing",
    "sort",
    "operation",
    "numerical",
    "input",
    "going",
    "put",
    "nonlinear",
    "activation",
    "function",
    "layers",
    "let",
    "show",
    "looks",
    "like",
    "self",
    "dot",
    "layer",
    "three",
    "going",
    "start",
    "outside",
    "self",
    "dot",
    "relu",
    "going",
    "go",
    "self",
    "dot",
    "layer",
    "two",
    "going",
    "go",
    "self",
    "dot",
    "relu",
    "fair",
    "bit",
    "going",
    "nothing",
    "ca",
    "handle",
    "layer",
    "one",
    "happens",
    "data",
    "goes",
    "layer",
    "one",
    "performs",
    "linear",
    "operation",
    "end",
    "linear",
    "pass",
    "output",
    "layer",
    "one",
    "relu",
    "function",
    "relu",
    "turn",
    "negative",
    "outputs",
    "model",
    "layer",
    "one",
    "zero",
    "keep",
    "positives",
    "layer",
    "two",
    "finally",
    "outputs",
    "layer",
    "three",
    "stay",
    "got",
    "features",
    "relu",
    "end",
    "going",
    "pass",
    "outputs",
    "sigmoid",
    "function",
    "later",
    "really",
    "wanted",
    "could",
    "put",
    "self",
    "dot",
    "sigmoid",
    "equals",
    "end",
    "dot",
    "sigmoid",
    "going",
    "one",
    "way",
    "constructing",
    "going",
    "apply",
    "sigmoid",
    "function",
    "logits",
    "model",
    "logits",
    "raw",
    "output",
    "model",
    "let",
    "instantiate",
    "model",
    "going",
    "called",
    "model",
    "three",
    "little",
    "bit",
    "confusing",
    "model",
    "three",
    "circle",
    "model",
    "v",
    "two",
    "going",
    "send",
    "target",
    "device",
    "let",
    "check",
    "model",
    "three",
    "look",
    "like",
    "wonderful",
    "actually",
    "show",
    "us",
    "relu",
    "appear",
    "shows",
    "us",
    "parameters",
    "circle",
    "model",
    "v",
    "two",
    "like",
    "think",
    "challenge",
    "go",
    "ahead",
    "see",
    "model",
    "capable",
    "working",
    "data",
    "circular",
    "data",
    "got",
    "data",
    "sets",
    "ready",
    "need",
    "set",
    "training",
    "code",
    "challenge",
    "write",
    "training",
    "code",
    "see",
    "model",
    "works",
    "going",
    "go",
    "next",
    "videos",
    "also",
    "challenge",
    "go",
    "tensorflow",
    "playground",
    "recreate",
    "neural",
    "network",
    "two",
    "hidden",
    "layers",
    "go",
    "10",
    "well",
    "goes",
    "eight",
    "keep",
    "five",
    "build",
    "something",
    "like",
    "got",
    "two",
    "layers",
    "five",
    "little",
    "bit",
    "different",
    "got",
    "two",
    "layers",
    "put",
    "learning",
    "rate",
    "stochastic",
    "gradient",
    "descent",
    "using",
    "leave",
    "tensorflow",
    "playground",
    "change",
    "activation",
    "instead",
    "linear",
    "used",
    "change",
    "relu",
    "using",
    "press",
    "play",
    "see",
    "happens",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "left",
    "leaving",
    "challenge",
    "recreating",
    "model",
    "difficult",
    "got",
    "two",
    "hidden",
    "layers",
    "five",
    "neurons",
    "got",
    "data",
    "set",
    "looks",
    "kind",
    "like",
    "main",
    "points",
    "learning",
    "rate",
    "using",
    "change",
    "previously",
    "used",
    "linear",
    "activation",
    "change",
    "linear",
    "relu",
    "got",
    "set",
    "code",
    "remember",
    "relu",
    "popular",
    "effective",
    "nonlinear",
    "activation",
    "function",
    "discussing",
    "need",
    "nonlinearity",
    "model",
    "nonlinear",
    "data",
    "crux",
    "neural",
    "networks",
    "artificial",
    "neural",
    "networks",
    "get",
    "confused",
    "brain",
    "neural",
    "networks",
    "knows",
    "might",
    "work",
    "know",
    "neurosurgeon",
    "neuroscientist",
    "artificial",
    "neural",
    "networks",
    "large",
    "combination",
    "linear",
    "straight",
    "nonlinear",
    "functions",
    "potentially",
    "able",
    "find",
    "patterns",
    "data",
    "data",
    "set",
    "quite",
    "small",
    "blue",
    "red",
    "circle",
    "principle",
    "applies",
    "larger",
    "data",
    "sets",
    "larger",
    "models",
    "combined",
    "linear",
    "nonlinear",
    "functions",
    "got",
    "tabs",
    "going",
    "let",
    "get",
    "rid",
    "let",
    "come",
    "back",
    "try",
    "work",
    "think",
    "work",
    "know",
    "let",
    "find",
    "together",
    "ready",
    "three",
    "two",
    "one",
    "look",
    "almost",
    "instantly",
    "training",
    "loss",
    "goes",
    "zero",
    "test",
    "loss",
    "basically",
    "zero",
    "well",
    "look",
    "amazing",
    "stop",
    "change",
    "learning",
    "rate",
    "maybe",
    "little",
    "lower",
    "let",
    "see",
    "happens",
    "takes",
    "little",
    "bit",
    "longer",
    "get",
    "wants",
    "go",
    "see",
    "power",
    "changing",
    "learning",
    "rate",
    "let",
    "make",
    "really",
    "small",
    "happens",
    "300",
    "epochs",
    "loss",
    "started",
    "go",
    "change",
    "really",
    "small",
    "oh",
    "getting",
    "little",
    "bit",
    "trend",
    "starting",
    "go",
    "already",
    "surpassed",
    "epochs",
    "see",
    "learning",
    "rate",
    "much",
    "smaller",
    "means",
    "model",
    "learning",
    "much",
    "slower",
    "beautiful",
    "visual",
    "way",
    "demonstrating",
    "different",
    "values",
    "learning",
    "rate",
    "could",
    "sit",
    "day",
    "might",
    "get",
    "lower",
    "let",
    "increase",
    "10x",
    "epochs",
    "still",
    "let",
    "say",
    "oh",
    "got",
    "better",
    "oh",
    "going",
    "faster",
    "already",
    "even",
    "500",
    "epochs",
    "power",
    "learning",
    "rate",
    "increase",
    "another",
    "10x",
    "reset",
    "start",
    "oh",
    "would",
    "look",
    "much",
    "faster",
    "time",
    "beautiful",
    "oh",
    "nothing",
    "better",
    "watching",
    "loss",
    "curve",
    "go",
    "world",
    "machine",
    "learning",
    "reset",
    "let",
    "change",
    "right",
    "back",
    "get",
    "0",
    "basically",
    "100",
    "epochs",
    "power",
    "learning",
    "rate",
    "little",
    "visual",
    "representation",
    "working",
    "learning",
    "rates",
    "time",
    "us",
    "build",
    "optimizer",
    "loss",
    "function",
    "right",
    "got",
    "nonlinear",
    "model",
    "set",
    "loss",
    "optimizer",
    "might",
    "already",
    "done",
    "code",
    "code",
    "written",
    "going",
    "redo",
    "completeness",
    "practice",
    "want",
    "loss",
    "function",
    "working",
    "logits",
    "working",
    "binary",
    "cross",
    "entropy",
    "loss",
    "use",
    "binary",
    "cross",
    "entropy",
    "sorry",
    "working",
    "binary",
    "classification",
    "problem",
    "blue",
    "dots",
    "red",
    "dots",
    "torch",
    "dot",
    "opt",
    "binary",
    "classification",
    "problems",
    "think",
    "want",
    "model",
    "three",
    "dot",
    "parameters",
    "parameters",
    "want",
    "optimize",
    "model",
    "going",
    "set",
    "lr",
    "like",
    "tensorflow",
    "playground",
    "beautiful",
    "binary",
    "classification",
    "problems",
    "think",
    "would",
    "email",
    "spam",
    "spam",
    "credit",
    "cards",
    "equals",
    "fraud",
    "fraud",
    "else",
    "might",
    "insurance",
    "claims",
    "equals",
    "fault",
    "fault",
    "someone",
    "puts",
    "claim",
    "speaking",
    "car",
    "crash",
    "whose",
    "fault",
    "person",
    "submitting",
    "claim",
    "fault",
    "person",
    "also",
    "mentioned",
    "claim",
    "fault",
    "many",
    "think",
    "top",
    "head",
    "let",
    "train",
    "model",
    "nonlinearity",
    "oh",
    "roll",
    "training",
    "model",
    "nonlinearity",
    "seen",
    "introduce",
    "nonlinear",
    "activation",
    "function",
    "within",
    "model",
    "remember",
    "linear",
    "activation",
    "function",
    "train",
    "loss",
    "go",
    "adjust",
    "add",
    "relu",
    "get",
    "loss",
    "going",
    "hopefully",
    "replicates",
    "pure",
    "pytorch",
    "code",
    "let",
    "hey",
    "going",
    "create",
    "random",
    "seeds",
    "working",
    "cuda",
    "introduce",
    "cuda",
    "random",
    "seed",
    "well",
    "seed",
    "worry",
    "much",
    "numbers",
    "screen",
    "exactly",
    "mine",
    "due",
    "inherent",
    "randomness",
    "machine",
    "learning",
    "fact",
    "stochastic",
    "gradient",
    "descent",
    "stochastic",
    "stands",
    "random",
    "setting",
    "seeds",
    "close",
    "possible",
    "direction",
    "important",
    "loss",
    "goes",
    "loss",
    "also",
    "go",
    "target",
    "device",
    "going",
    "go",
    "xtrain",
    "setting",
    "device",
    "agnostic",
    "code",
    "done",
    "going",
    "completeness",
    "practice",
    "every",
    "step",
    "puzzle",
    "want",
    "want",
    "experience",
    "course",
    "momentum",
    "builder",
    "go",
    "repos",
    "machine",
    "learning",
    "projects",
    "use",
    "pytorch",
    "go",
    "oh",
    "code",
    "set",
    "device",
    "agnostic",
    "code",
    "problem",
    "working",
    "binary",
    "classification",
    "let",
    "go",
    "loop",
    "data",
    "done",
    "going",
    "set",
    "epochs",
    "let",
    "1000",
    "epochs",
    "go",
    "epoch",
    "range",
    "epochs",
    "well",
    "want",
    "train",
    "training",
    "code",
    "set",
    "model",
    "model",
    "three",
    "dot",
    "train",
    "want",
    "start",
    "think",
    "could",
    "functionalize",
    "training",
    "code",
    "going",
    "start",
    "move",
    "towards",
    "future",
    "video",
    "one",
    "forward",
    "pass",
    "got",
    "logits",
    "logits",
    "well",
    "raw",
    "output",
    "model",
    "without",
    "activation",
    "functions",
    "towards",
    "final",
    "layer",
    "classified",
    "logits",
    "called",
    "logits",
    "create",
    "prediction",
    "labels",
    "rounding",
    "output",
    "torch",
    "dot",
    "sigmoid",
    "logits",
    "going",
    "take",
    "us",
    "logits",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "go",
    "calculate",
    "loss",
    "unofficial",
    "pytorch",
    "song",
    "calculate",
    "last",
    "go",
    "loss",
    "equals",
    "loss",
    "fn",
    "logits",
    "remember",
    "got",
    "bce",
    "logits",
    "loss",
    "takes",
    "logits",
    "first",
    "input",
    "going",
    "calculate",
    "loss",
    "models",
    "logits",
    "training",
    "labels",
    "go",
    "calculate",
    "accuracy",
    "using",
    "accuracy",
    "function",
    "one",
    "little",
    "bit",
    "backwards",
    "compared",
    "pytorch",
    "pass",
    "training",
    "labels",
    "first",
    "constructed",
    "way",
    "style",
    "scikit",
    "line",
    "three",
    "go",
    "optimizer",
    "zero",
    "grad",
    "zero",
    "gradients",
    "optimizer",
    "start",
    "fresh",
    "calculating",
    "ideal",
    "gradients",
    "every",
    "epoch",
    "going",
    "reset",
    "every",
    "epoch",
    "fine",
    "going",
    "perform",
    "back",
    "propagation",
    "pytorch",
    "going",
    "take",
    "care",
    "us",
    "calling",
    "loss",
    "backwards",
    "perform",
    "gradient",
    "descent",
    "step",
    "optimizer",
    "see",
    "improve",
    "model",
    "parameters",
    "optimizer",
    "dot",
    "step",
    "oh",
    "want",
    "show",
    "speaking",
    "model",
    "parameters",
    "let",
    "check",
    "model",
    "three",
    "dot",
    "state",
    "dig",
    "relu",
    "activation",
    "function",
    "actually",
    "parameters",
    "notice",
    "got",
    "weight",
    "got",
    "bias",
    "layer",
    "one",
    "layer",
    "two",
    "layer",
    "three",
    "relu",
    "function",
    "parameters",
    "optimize",
    "go",
    "nn",
    "dot",
    "relu",
    "say",
    "implements",
    "go",
    "maximum",
    "zero",
    "takes",
    "input",
    "takes",
    "max",
    "zero",
    "takes",
    "max",
    "zero",
    "x",
    "negative",
    "number",
    "zero",
    "going",
    "higher",
    "negative",
    "number",
    "zeroes",
    "negative",
    "inputs",
    "leaves",
    "positive",
    "inputs",
    "max",
    "positive",
    "input",
    "versus",
    "zero",
    "positive",
    "input",
    "parameters",
    "optimize",
    "effective",
    "think",
    "every",
    "parameter",
    "model",
    "needs",
    "little",
    "bit",
    "computation",
    "adjust",
    "parameters",
    "add",
    "model",
    "compute",
    "required",
    "generally",
    "kind",
    "machine",
    "learning",
    "yes",
    "parameters",
    "ability",
    "learn",
    "need",
    "compute",
    "let",
    "go",
    "model",
    "three",
    "dot",
    "vowel",
    "going",
    "go",
    "torch",
    "dot",
    "inference",
    "mode",
    "could",
    "spell",
    "inference",
    "fantastic",
    "going",
    "going",
    "forward",
    "pass",
    "test",
    "logits",
    "equals",
    "model",
    "three",
    "test",
    "data",
    "going",
    "calculate",
    "test",
    "pred",
    "labels",
    "calling",
    "torch",
    "dot",
    "round",
    "torch",
    "dot",
    "sigmoid",
    "test",
    "logits",
    "calculate",
    "test",
    "loss",
    "also",
    "calculate",
    "test",
    "accuracy",
    "going",
    "give",
    "space",
    "code",
    "middle",
    "screen",
    "equals",
    "accuracy",
    "function",
    "going",
    "pass",
    "true",
    "equals",
    "test",
    "going",
    "pass",
    "true",
    "equals",
    "test",
    "pass",
    "pred",
    "equals",
    "test",
    "pred",
    "beautiful",
    "final",
    "step",
    "print",
    "happening",
    "important",
    "one",
    "fun",
    "know",
    "model",
    "two",
    "model",
    "actually",
    "learn",
    "like",
    "see",
    "loss",
    "values",
    "go",
    "accuracy",
    "values",
    "go",
    "said",
    "nothing",
    "much",
    "beautiful",
    "world",
    "machine",
    "learning",
    "watching",
    "loss",
    "function",
    "go",
    "loss",
    "value",
    "go",
    "watching",
    "loss",
    "curve",
    "go",
    "let",
    "print",
    "current",
    "epoch",
    "print",
    "loss",
    "training",
    "loss",
    "take",
    "four",
    "decimal",
    "places",
    "go",
    "accuracy",
    "take",
    "two",
    "decimal",
    "places",
    "put",
    "little",
    "percentage",
    "sign",
    "break",
    "putting",
    "test",
    "loss",
    "put",
    "test",
    "loss",
    "remember",
    "model",
    "learns",
    "patterns",
    "training",
    "data",
    "set",
    "evaluates",
    "patterns",
    "test",
    "data",
    "set",
    "pass",
    "test",
    "act",
    "doubt",
    "might",
    "error",
    "two",
    "within",
    "code",
    "going",
    "try",
    "run",
    "seen",
    "code",
    "think",
    "ready",
    "training",
    "first",
    "model",
    "built",
    "model",
    "ready",
    "three",
    "two",
    "one",
    "let",
    "go",
    "oh",
    "course",
    "module",
    "torch",
    "cuda",
    "attribute",
    "manuals",
    "typo",
    "standard",
    "man",
    "go",
    "sound",
    "another",
    "one",
    "get",
    "wrong",
    "oh",
    "target",
    "size",
    "must",
    "input",
    "size",
    "mess",
    "get",
    "wrong",
    "test",
    "loss",
    "test",
    "logits",
    "test",
    "hmm",
    "two",
    "matching",
    "model",
    "three",
    "x",
    "test",
    "test",
    "size",
    "let",
    "troubleshooting",
    "fly",
    "hey",
    "everything",
    "always",
    "works",
    "want",
    "length",
    "x",
    "test",
    "got",
    "shape",
    "issue",
    "remember",
    "said",
    "one",
    "common",
    "issues",
    "deep",
    "learning",
    "shape",
    "issue",
    "got",
    "shape",
    "let",
    "check",
    "test",
    "logits",
    "dot",
    "shape",
    "test",
    "dot",
    "shape",
    "print",
    "oh",
    "missed",
    "dot",
    "squeeze",
    "oh",
    "see",
    "hinting",
    "fact",
    "needed",
    "call",
    "dot",
    "squeeze",
    "discrepancy",
    "test",
    "logits",
    "dot",
    "shape",
    "got",
    "extra",
    "dimension",
    "getting",
    "value",
    "error",
    "target",
    "size",
    "shape",
    "mismatch",
    "got",
    "target",
    "size",
    "200",
    "must",
    "input",
    "size",
    "torch",
    "size",
    "squeeze",
    "oh",
    "training",
    "worked",
    "okay",
    "missed",
    "let",
    "get",
    "rid",
    "getting",
    "rid",
    "extra",
    "one",
    "dimension",
    "using",
    "squeeze",
    "one",
    "dimension",
    "everything",
    "lined",
    "go",
    "okay",
    "look",
    "yes",
    "accuracy",
    "gone",
    "albeit",
    "much",
    "still",
    "perfect",
    "really",
    "like",
    "towards",
    "100",
    "lost",
    "lower",
    "feel",
    "like",
    "got",
    "better",
    "performing",
    "model",
    "power",
    "non",
    "linearity",
    "added",
    "relu",
    "layer",
    "two",
    "relu",
    "relu",
    "gave",
    "model",
    "power",
    "straight",
    "lines",
    "oh",
    "straight",
    "linear",
    "straight",
    "lines",
    "non",
    "straight",
    "lines",
    "potentially",
    "draw",
    "line",
    "separate",
    "circles",
    "next",
    "video",
    "let",
    "draw",
    "line",
    "plot",
    "model",
    "decision",
    "boundary",
    "using",
    "function",
    "see",
    "really",
    "learn",
    "anything",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "trained",
    "first",
    "model",
    "tell",
    "got",
    "biggest",
    "smile",
    "face",
    "trained",
    "first",
    "model",
    "harnesses",
    "power",
    "straight",
    "lines",
    "non",
    "straight",
    "lines",
    "linear",
    "functions",
    "non",
    "linear",
    "functions",
    "1000th",
    "epoch",
    "look",
    "like",
    "getting",
    "bit",
    "better",
    "results",
    "pure",
    "guessing",
    "50",
    "500",
    "samples",
    "red",
    "dots",
    "500",
    "samples",
    "blue",
    "dots",
    "evenly",
    "balanced",
    "classes",
    "seen",
    "added",
    "relu",
    "activation",
    "function",
    "data",
    "set",
    "similar",
    "tensorflow",
    "playground",
    "model",
    "starts",
    "fit",
    "work",
    "linear",
    "activation",
    "functions",
    "could",
    "play",
    "around",
    "could",
    "play",
    "around",
    "learning",
    "rate",
    "regularization",
    "sure",
    "leave",
    "extra",
    "curriculum",
    "look",
    "going",
    "retire",
    "tensorflow",
    "program",
    "going",
    "go",
    "back",
    "writing",
    "code",
    "let",
    "get",
    "let",
    "get",
    "evaluate",
    "model",
    "right",
    "numbers",
    "page",
    "let",
    "write",
    "like",
    "evaluate",
    "things",
    "visualize",
    "visualize",
    "visualize",
    "evaluating",
    "model",
    "trained",
    "nonlinear",
    "activation",
    "functions",
    "also",
    "discussed",
    "point",
    "neural",
    "networks",
    "really",
    "big",
    "combination",
    "linear",
    "nonlinear",
    "functions",
    "trying",
    "draw",
    "patterns",
    "data",
    "said",
    "let",
    "make",
    "predictions",
    "model",
    "3",
    "recently",
    "trained",
    "model",
    "put",
    "val",
    "mode",
    "set",
    "inference",
    "mode",
    "go",
    "yprads",
    "equals",
    "torch",
    "dot",
    "round",
    "torch",
    "dot",
    "sigmoid",
    "could",
    "functionalize",
    "course",
    "model",
    "3",
    "pass",
    "x",
    "test",
    "know",
    "going",
    "squeeze",
    "ran",
    "troubles",
    "previous",
    "video",
    "actually",
    "really",
    "liked",
    "got",
    "troubleshoot",
    "shape",
    "error",
    "fly",
    "one",
    "common",
    "issues",
    "going",
    "come",
    "across",
    "deep",
    "learning",
    "yprads",
    "let",
    "check",
    "let",
    "check",
    "test",
    "want",
    "test",
    "remember",
    "evaluating",
    "predictions",
    "want",
    "format",
    "original",
    "labels",
    "want",
    "compare",
    "apples",
    "apples",
    "compare",
    "format",
    "two",
    "things",
    "look",
    "yes",
    "cuda",
    "floats",
    "see",
    "got",
    "one",
    "wrong",
    "whereas",
    "ones",
    "look",
    "pretty",
    "good",
    "hmm",
    "might",
    "look",
    "pretty",
    "good",
    "visualize",
    "let",
    "might",
    "already",
    "done",
    "issued",
    "challenge",
    "plotting",
    "decision",
    "boundaries",
    "plot",
    "decision",
    "boundaries",
    "let",
    "go",
    "plt",
    "dot",
    "figure",
    "going",
    "set",
    "fig",
    "size",
    "equal",
    "one",
    "advantages",
    "hosting",
    "machine",
    "learning",
    "cooking",
    "show",
    "code",
    "ahead",
    "time",
    "go",
    "plt",
    "dot",
    "title",
    "train",
    "going",
    "call",
    "plot",
    "decision",
    "boundary",
    "function",
    "seen",
    "plot",
    "decision",
    "boundary",
    "going",
    "pass",
    "one",
    "could",
    "model",
    "three",
    "could",
    "also",
    "pass",
    "older",
    "models",
    "model",
    "one",
    "use",
    "reality",
    "fact",
    "reckon",
    "great",
    "comparison",
    "also",
    "create",
    "another",
    "plot",
    "test",
    "data",
    "index",
    "number",
    "two",
    "remember",
    "subplot",
    "number",
    "rows",
    "number",
    "columns",
    "index",
    "plot",
    "appears",
    "give",
    "one",
    "title",
    "plot",
    "dot",
    "title",
    "test",
    "google",
    "colab",
    "want",
    "said",
    "course",
    "also",
    "battle",
    "google",
    "colab",
    "autocorrect",
    "going",
    "model",
    "three",
    "pass",
    "test",
    "data",
    "behind",
    "scenes",
    "plot",
    "decision",
    "boundary",
    "function",
    "create",
    "beautiful",
    "graphic",
    "us",
    "perform",
    "predictions",
    "x",
    "features",
    "input",
    "compare",
    "values",
    "let",
    "see",
    "going",
    "oh",
    "look",
    "yes",
    "first",
    "nonlinear",
    "model",
    "okay",
    "perfect",
    "certainly",
    "much",
    "better",
    "models",
    "look",
    "model",
    "one",
    "linearity",
    "model",
    "one",
    "equals",
    "nonlinearity",
    "got",
    "double",
    "negative",
    "whereas",
    "model",
    "three",
    "equals",
    "nonlinearity",
    "see",
    "power",
    "nonlinearity",
    "better",
    "yet",
    "power",
    "linearity",
    "linear",
    "straight",
    "lines",
    "non",
    "straight",
    "lines",
    "feel",
    "like",
    "could",
    "better",
    "though",
    "challenge",
    "improve",
    "model",
    "three",
    "better",
    "get",
    "79",
    "accuracy",
    "better",
    "80",
    "accuracy",
    "test",
    "data",
    "think",
    "challenge",
    "looking",
    "hints",
    "look",
    "well",
    "covered",
    "improving",
    "model",
    "maybe",
    "add",
    "layers",
    "maybe",
    "add",
    "hidden",
    "units",
    "maybe",
    "fit",
    "longer",
    "maybe",
    "add",
    "layers",
    "put",
    "relio",
    "activation",
    "function",
    "top",
    "well",
    "maybe",
    "lower",
    "learning",
    "rate",
    "right",
    "got",
    "give",
    "shot",
    "try",
    "improve",
    "think",
    "going",
    "push",
    "forward",
    "going",
    "challenge",
    "extra",
    "curriculum",
    "think",
    "next",
    "section",
    "seen",
    "nonlinear",
    "activation",
    "functions",
    "action",
    "let",
    "write",
    "code",
    "replicate",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "left",
    "challenge",
    "improving",
    "model",
    "three",
    "better",
    "80",
    "accuracy",
    "test",
    "data",
    "hope",
    "gave",
    "shot",
    "things",
    "would",
    "done",
    "potentially",
    "add",
    "layers",
    "maybe",
    "increase",
    "number",
    "hidden",
    "units",
    "needed",
    "fit",
    "longer",
    "maybe",
    "lower",
    "learning",
    "rate",
    "leave",
    "explore",
    "motto",
    "data",
    "scientists",
    "right",
    "experiment",
    "experiment",
    "experiment",
    "let",
    "go",
    "seen",
    "nonlinear",
    "activation",
    "functions",
    "practice",
    "let",
    "replicate",
    "replicating",
    "nonlinear",
    "activation",
    "functions",
    "remember",
    "neural",
    "networks",
    "rather",
    "us",
    "telling",
    "model",
    "learn",
    "give",
    "tools",
    "discover",
    "patterns",
    "data",
    "tries",
    "figure",
    "best",
    "patterns",
    "tools",
    "right",
    "seen",
    "action",
    "tools",
    "linear",
    "nonlinear",
    "functions",
    "neural",
    "network",
    "big",
    "stack",
    "linear",
    "nonlinear",
    "functions",
    "us",
    "got",
    "four",
    "layers",
    "four",
    "five",
    "layers",
    "said",
    "networks",
    "get",
    "much",
    "larger",
    "premise",
    "remains",
    "form",
    "linear",
    "nonlinear",
    "manipulation",
    "data",
    "let",
    "get",
    "let",
    "make",
    "workspace",
    "little",
    "bit",
    "cleaner",
    "replicating",
    "nonlinear",
    "activation",
    "functions",
    "let",
    "create",
    "tensor",
    "start",
    "everything",
    "starts",
    "tensor",
    "go",
    "equals",
    "torch",
    "range",
    "going",
    "create",
    "range",
    "negative",
    "10",
    "10",
    "step",
    "one",
    "set",
    "type",
    "equal",
    "torch",
    "dot",
    "float",
    "actually",
    "need",
    "going",
    "default",
    "set",
    "dot",
    "type",
    "got",
    "torch",
    "float",
    "32",
    "pretty",
    "sure",
    "got",
    "rid",
    "oh",
    "got",
    "torch",
    "happening",
    "well",
    "let",
    "check",
    "oh",
    "got",
    "integers",
    "values",
    "step",
    "one",
    "turn",
    "float",
    "going",
    "happen",
    "get",
    "float",
    "keep",
    "otherwise",
    "going",
    "hundred",
    "numbers",
    "yeah",
    "many",
    "let",
    "keep",
    "negative",
    "10",
    "10",
    "set",
    "type",
    "torch",
    "float",
    "beautiful",
    "looks",
    "like",
    "pytorch",
    "default",
    "data",
    "type",
    "integers",
    "going",
    "work",
    "float",
    "32",
    "float",
    "32",
    "data",
    "float",
    "32",
    "functions",
    "create",
    "might",
    "run",
    "errors",
    "let",
    "visualize",
    "data",
    "want",
    "guess",
    "straight",
    "line",
    "line",
    "got",
    "three",
    "seconds",
    "one",
    "two",
    "three",
    "straight",
    "line",
    "go",
    "got",
    "negative",
    "10",
    "positive",
    "10",
    "nine",
    "close",
    "enough",
    "would",
    "turn",
    "straight",
    "line",
    "straight",
    "line",
    "linear",
    "would",
    "perform",
    "relu",
    "activation",
    "function",
    "could",
    "course",
    "call",
    "torch",
    "relu",
    "actually",
    "let",
    "fact",
    "plot",
    "plt",
    "dot",
    "plot",
    "torch",
    "relu",
    "look",
    "like",
    "boom",
    "go",
    "want",
    "replicate",
    "relu",
    "function",
    "let",
    "go",
    "nn",
    "dot",
    "relu",
    "seen",
    "need",
    "max",
    "need",
    "return",
    "based",
    "input",
    "need",
    "max",
    "zero",
    "let",
    "give",
    "shot",
    "come",
    "need",
    "space",
    "never",
    "enough",
    "code",
    "space",
    "like",
    "writing",
    "lots",
    "code",
    "know",
    "let",
    "go",
    "relu",
    "take",
    "input",
    "x",
    "form",
    "tensor",
    "go",
    "return",
    "torch",
    "dot",
    "maximum",
    "think",
    "could",
    "torch",
    "dot",
    "max",
    "try",
    "maximum",
    "torch",
    "dot",
    "tensor",
    "zero",
    "maximum",
    "going",
    "return",
    "max",
    "whatever",
    "one",
    "option",
    "whatever",
    "option",
    "inputs",
    "must",
    "tensors",
    "maybe",
    "could",
    "give",
    "type",
    "hint",
    "torch",
    "dot",
    "tensor",
    "return",
    "tensor",
    "return",
    "torch",
    "dot",
    "tensor",
    "beautiful",
    "ready",
    "try",
    "let",
    "see",
    "relu",
    "function",
    "relu",
    "wonderful",
    "looks",
    "like",
    "got",
    "quite",
    "similar",
    "output",
    "original",
    "got",
    "negative",
    "numbers",
    "go",
    "recall",
    "relu",
    "activation",
    "function",
    "turns",
    "negative",
    "numbers",
    "zero",
    "takes",
    "maximum",
    "zero",
    "input",
    "input",
    "negative",
    "well",
    "zero",
    "bigger",
    "leaves",
    "positive",
    "values",
    "beauty",
    "relu",
    "quite",
    "simple",
    "effective",
    "let",
    "plot",
    "relu",
    "activation",
    "function",
    "custom",
    "one",
    "go",
    "plt",
    "dot",
    "plot",
    "call",
    "relu",
    "function",
    "let",
    "see",
    "looks",
    "like",
    "oh",
    "look",
    "us",
    "go",
    "well",
    "done",
    "exact",
    "torch",
    "relu",
    "function",
    "easy",
    "another",
    "nonlinear",
    "activation",
    "function",
    "used",
    "well",
    "believe",
    "one",
    "go",
    "say",
    "sigmoid",
    "sigmoid",
    "go",
    "hello",
    "sigmoid",
    "oh",
    "got",
    "little",
    "bit",
    "going",
    "one",
    "one",
    "plus",
    "exponential",
    "negative",
    "sigmoid",
    "little",
    "symbol",
    "sigmoid",
    "x",
    "input",
    "get",
    "let",
    "try",
    "replicate",
    "might",
    "bring",
    "one",
    "right",
    "let",
    "sigmoid",
    "well",
    "want",
    "create",
    "custom",
    "sigmoid",
    "want",
    "sort",
    "input",
    "want",
    "return",
    "one",
    "divided",
    "function",
    "sigmoid",
    "one",
    "divided",
    "one",
    "plus",
    "exponential",
    "one",
    "plus",
    "torch",
    "dot",
    "exp",
    "exponential",
    "negative",
    "might",
    "put",
    "bottom",
    "side",
    "brackets",
    "operation",
    "reckon",
    "looks",
    "right",
    "one",
    "divided",
    "one",
    "plus",
    "torch",
    "exponential",
    "negative",
    "yes",
    "well",
    "one",
    "real",
    "way",
    "find",
    "let",
    "plot",
    "torch",
    "version",
    "sigmoid",
    "torch",
    "dot",
    "sigmoid",
    "pass",
    "see",
    "happens",
    "oh",
    "bad",
    "tensor",
    "get",
    "get",
    "curved",
    "line",
    "wonderful",
    "go",
    "plt",
    "dot",
    "plot",
    "going",
    "use",
    "sigmoid",
    "function",
    "replicate",
    "torch",
    "sigmoid",
    "function",
    "yes",
    "ooh",
    "see",
    "happening",
    "behind",
    "scenes",
    "neural",
    "networks",
    "course",
    "could",
    "complicated",
    "activation",
    "functions",
    "layers",
    "whatnot",
    "try",
    "replicate",
    "fact",
    "great",
    "exercise",
    "try",
    "essentially",
    "across",
    "videos",
    "sections",
    "done",
    "replicated",
    "linear",
    "layer",
    "replicated",
    "relu",
    "actually",
    "built",
    "model",
    "scratch",
    "could",
    "really",
    "wanted",
    "lot",
    "easier",
    "use",
    "pytorch",
    "layers",
    "building",
    "neural",
    "networks",
    "like",
    "lego",
    "bricks",
    "stacking",
    "together",
    "layers",
    "way",
    "shape",
    "form",
    "part",
    "pytorch",
    "know",
    "compute",
    "fast",
    "possible",
    "behind",
    "scenes",
    "use",
    "gpu",
    "get",
    "whole",
    "bunch",
    "benefits",
    "pytorch",
    "offers",
    "lot",
    "benefits",
    "using",
    "layers",
    "rather",
    "writing",
    "model",
    "literally",
    "like",
    "learn",
    "values",
    "decrease",
    "loss",
    "function",
    "increase",
    "accuracy",
    "combining",
    "linear",
    "layers",
    "nonlinear",
    "layers",
    "nonlinear",
    "functions",
    "relu",
    "function",
    "relu",
    "function",
    "like",
    "behind",
    "scenes",
    "combining",
    "linear",
    "nonlinear",
    "functions",
    "fit",
    "data",
    "set",
    "premise",
    "remains",
    "even",
    "small",
    "data",
    "set",
    "large",
    "data",
    "sets",
    "large",
    "models",
    "said",
    "think",
    "time",
    "us",
    "push",
    "covered",
    "fair",
    "bit",
    "code",
    "worked",
    "binary",
    "classification",
    "problem",
    "worked",
    "classification",
    "problem",
    "yet",
    "fun",
    "graphic",
    "classification",
    "think",
    "cover",
    "next",
    "going",
    "put",
    "together",
    "steps",
    "workflow",
    "covered",
    "binary",
    "classification",
    "let",
    "move",
    "classification",
    "problem",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "videos",
    "harnessing",
    "power",
    "nonlinearity",
    "specifically",
    "line",
    "functions",
    "replicated",
    "learned",
    "neural",
    "network",
    "combines",
    "linear",
    "nonlinear",
    "functions",
    "find",
    "patterns",
    "data",
    "simple",
    "red",
    "versus",
    "blue",
    "dots",
    "added",
    "little",
    "bit",
    "nonlinearity",
    "found",
    "secret",
    "source",
    "start",
    "separating",
    "blue",
    "red",
    "dots",
    "also",
    "issued",
    "challenge",
    "try",
    "improve",
    "think",
    "hopefully",
    "given",
    "go",
    "let",
    "keep",
    "pushing",
    "forward",
    "going",
    "reiterate",
    "basically",
    "everything",
    "done",
    "except",
    "time",
    "point",
    "view",
    "classification",
    "problem",
    "believe",
    "section",
    "eight",
    "putting",
    "together",
    "classification",
    "problem",
    "beautiful",
    "recall",
    "difference",
    "binary",
    "classification",
    "equals",
    "one",
    "thing",
    "another",
    "cat",
    "versus",
    "dog",
    "building",
    "cat",
    "versus",
    "dog",
    "image",
    "classifier",
    "spam",
    "versus",
    "spam",
    "say",
    "emails",
    "spam",
    "spam",
    "even",
    "internet",
    "posts",
    "facebook",
    "twitter",
    "one",
    "internet",
    "services",
    "fraud",
    "fraud",
    "credit",
    "card",
    "transactions",
    "classification",
    "one",
    "thing",
    "another",
    "could",
    "cat",
    "versus",
    "dog",
    "versus",
    "chicken",
    "think",
    "got",
    "skills",
    "architecture",
    "might",
    "little",
    "bit",
    "different",
    "classification",
    "problem",
    "got",
    "many",
    "building",
    "blocks",
    "funny",
    "let",
    "clean",
    "add",
    "code",
    "cells",
    "reiterate",
    "gone",
    "nonlinearity",
    "question",
    "could",
    "draw",
    "unlimited",
    "amount",
    "straight",
    "linear",
    "nonlinear",
    "lines",
    "believe",
    "could",
    "draw",
    "pretty",
    "intricate",
    "patterns",
    "neural",
    "networks",
    "behind",
    "scenes",
    "also",
    "learned",
    "wanted",
    "replicate",
    "nonlinear",
    "functions",
    "ones",
    "used",
    "could",
    "create",
    "range",
    "linear",
    "activation",
    "line",
    "wanted",
    "sigmoid",
    "get",
    "curl",
    "wanted",
    "relu",
    "well",
    "saw",
    "replicate",
    "relu",
    "function",
    "one",
    "nonlinear",
    "course",
    "far",
    "nonlinear",
    "activations",
    "came",
    "far",
    "different",
    "layers",
    "get",
    "used",
    "practice",
    "let",
    "go",
    "back",
    "keynote",
    "going",
    "working",
    "classification",
    "one",
    "big",
    "differences",
    "use",
    "softmax",
    "activation",
    "function",
    "versus",
    "sigmoid",
    "another",
    "big",
    "difference",
    "instead",
    "binary",
    "cross",
    "entropy",
    "use",
    "cross",
    "entropy",
    "think",
    "going",
    "stay",
    "going",
    "see",
    "action",
    "second",
    "let",
    "describe",
    "problem",
    "space",
    "go",
    "visual",
    "covered",
    "fair",
    "bit",
    "well",
    "done",
    "everyone",
    "binary",
    "versus",
    "classification",
    "binary",
    "one",
    "thing",
    "another",
    "zero",
    "one",
    "could",
    "three",
    "things",
    "could",
    "thousand",
    "things",
    "could",
    "things",
    "could",
    "25",
    "things",
    "one",
    "thing",
    "another",
    "basic",
    "premise",
    "going",
    "go",
    "let",
    "create",
    "data",
    "hey",
    "creating",
    "20",
    "data",
    "set",
    "create",
    "data",
    "set",
    "going",
    "import",
    "dependencies",
    "going",
    "torch",
    "even",
    "though",
    "already",
    "little",
    "bit",
    "completeness",
    "going",
    "go",
    "map",
    "plotlib",
    "plot",
    "always",
    "like",
    "get",
    "visual",
    "visualize",
    "visualize",
    "visualize",
    "going",
    "import",
    "let",
    "get",
    "make",
    "blobs",
    "would",
    "get",
    "get",
    "20",
    "data",
    "sets",
    "classification",
    "20",
    "data",
    "sets",
    "blobs",
    "go",
    "make",
    "scikitlearn",
    "classification",
    "data",
    "sets",
    "get",
    "one",
    "option",
    "also",
    "make",
    "blobs",
    "beautiful",
    "make",
    "blobs",
    "code",
    "let",
    "copy",
    "make",
    "blobs",
    "going",
    "see",
    "action",
    "anyway",
    "make",
    "blobs",
    "might",
    "guessed",
    "makes",
    "blobs",
    "us",
    "like",
    "blobs",
    "fun",
    "word",
    "say",
    "blobs",
    "want",
    "train",
    "test",
    "split",
    "want",
    "make",
    "data",
    "set",
    "want",
    "split",
    "train",
    "test",
    "let",
    "set",
    "number",
    "hyper",
    "parameters",
    "set",
    "hyper",
    "parameters",
    "data",
    "creation",
    "got",
    "documentation",
    "number",
    "samples",
    "many",
    "blobs",
    "want",
    "many",
    "features",
    "want",
    "say",
    "example",
    "wanted",
    "two",
    "different",
    "classes",
    "would",
    "binary",
    "classification",
    "say",
    "example",
    "wanted",
    "10",
    "classes",
    "could",
    "set",
    "going",
    "see",
    "others",
    "practice",
    "want",
    "read",
    "well",
    "truly",
    "let",
    "set",
    "want",
    "num",
    "classes",
    "let",
    "double",
    "working",
    "working",
    "two",
    "classes",
    "red",
    "dots",
    "blue",
    "dots",
    "let",
    "step",
    "notch",
    "go",
    "four",
    "classes",
    "watch",
    "everyone",
    "going",
    "go",
    "number",
    "features",
    "two",
    "number",
    "features",
    "random",
    "seed",
    "going",
    "might",
    "wondering",
    "capitalized",
    "well",
    "generally",
    "hyper",
    "parameters",
    "say",
    "set",
    "start",
    "notebook",
    "find",
    "quite",
    "common",
    "people",
    "write",
    "capital",
    "letters",
    "say",
    "hey",
    "settings",
    "change",
    "going",
    "introduce",
    "anyway",
    "might",
    "stumble",
    "upon",
    "create",
    "data",
    "going",
    "use",
    "make",
    "blobs",
    "function",
    "going",
    "create",
    "x",
    "blobs",
    "feature",
    "blobs",
    "label",
    "blobs",
    "let",
    "see",
    "look",
    "like",
    "second",
    "know",
    "saying",
    "blobs",
    "lot",
    "pass",
    "none",
    "samples",
    "many",
    "want",
    "let",
    "create",
    "thousand",
    "well",
    "could",
    "really",
    "hyper",
    "parameter",
    "leave",
    "number",
    "features",
    "going",
    "num",
    "features",
    "centres",
    "equals",
    "num",
    "classes",
    "going",
    "create",
    "four",
    "classes",
    "set",
    "num",
    "classes",
    "equal",
    "four",
    "going",
    "go",
    "center",
    "standard",
    "deviation",
    "give",
    "little",
    "shake",
    "add",
    "little",
    "bit",
    "randomness",
    "give",
    "clusters",
    "little",
    "shake",
    "mix",
    "bit",
    "make",
    "bit",
    "hard",
    "model",
    "see",
    "second",
    "random",
    "state",
    "equals",
    "random",
    "seed",
    "favorite",
    "random",
    "seed",
    "course",
    "set",
    "whatever",
    "number",
    "want",
    "like",
    "oh",
    "need",
    "comma",
    "course",
    "beautiful",
    "well",
    "using",
    "leverages",
    "numpy",
    "let",
    "turn",
    "data",
    "tenses",
    "turn",
    "data",
    "tenses",
    "well",
    "grab",
    "x",
    "blob",
    "call",
    "torch",
    "numpy",
    "numpy",
    "could",
    "type",
    "would",
    "fantastic",
    "right",
    "pretty",
    "well",
    "today",
    "made",
    "many",
    "typos",
    "make",
    "couple",
    "videos",
    "hey",
    "human",
    "going",
    "torch",
    "numpy",
    "going",
    "pass",
    "blob",
    "turn",
    "torch",
    "dot",
    "float",
    "remember",
    "numpy",
    "defaults",
    "float",
    "64",
    "whereas",
    "pytorch",
    "likes",
    "float",
    "split",
    "training",
    "test",
    "going",
    "create",
    "x",
    "blob",
    "train",
    "x",
    "test",
    "x",
    "blob",
    "test",
    "keep",
    "blob",
    "nomenclature",
    "blob",
    "train",
    "blob",
    "test",
    "going",
    "leverage",
    "train",
    "test",
    "split",
    "function",
    "thank",
    "x",
    "blob",
    "going",
    "pass",
    "blob",
    "features",
    "labels",
    "x",
    "features",
    "labels",
    "test",
    "size",
    "using",
    "test",
    "size",
    "20",
    "means",
    "80",
    "data",
    "training",
    "data",
    "fair",
    "enough",
    "split",
    "data",
    "set",
    "going",
    "set",
    "random",
    "seed",
    "random",
    "seed",
    "generally",
    "normally",
    "train",
    "test",
    "split",
    "random",
    "want",
    "reproducibility",
    "passing",
    "random",
    "seeds",
    "finally",
    "need",
    "get",
    "visual",
    "let",
    "plot",
    "data",
    "right",
    "got",
    "whole",
    "bunch",
    "code",
    "whole",
    "bunch",
    "talking",
    "much",
    "visuals",
    "going",
    "write",
    "visualize",
    "visualize",
    "visualize",
    "call",
    "size",
    "want",
    "going",
    "use",
    "favorite",
    "hand",
    "poker",
    "generally",
    "worked",
    "good",
    "plot",
    "size",
    "experience",
    "anyway",
    "go",
    "x",
    "blob",
    "want",
    "zero",
    "index",
    "grab",
    "x",
    "blob",
    "well",
    "might",
    "notice",
    "visualizing",
    "whole",
    "data",
    "set",
    "perfectly",
    "fine",
    "could",
    "visualize",
    "train",
    "test",
    "separately",
    "really",
    "wanted",
    "leave",
    "level",
    "challenge",
    "going",
    "go",
    "red",
    "yellow",
    "blue",
    "wonderful",
    "get",
    "wrong",
    "oh",
    "course",
    "got",
    "something",
    "wrong",
    "santa",
    "std",
    "spell",
    "center",
    "wrong",
    "cluster",
    "std",
    "missed",
    "cluster",
    "std",
    "standard",
    "deviation",
    "get",
    "wrong",
    "random",
    "seed",
    "oh",
    "needs",
    "random",
    "state",
    "oh",
    "another",
    "typo",
    "know",
    "said",
    "getting",
    "many",
    "typos",
    "get",
    "three",
    "go",
    "look",
    "first",
    "classification",
    "data",
    "set",
    "set",
    "zero",
    "clusters",
    "let",
    "take",
    "note",
    "going",
    "particularly",
    "space",
    "dots",
    "set",
    "cluster",
    "std",
    "zero",
    "happens",
    "get",
    "dots",
    "really",
    "look",
    "easy",
    "let",
    "mix",
    "right",
    "pick",
    "whatever",
    "value",
    "want",
    "going",
    "use",
    "need",
    "build",
    "model",
    "going",
    "draw",
    "lines",
    "four",
    "colors",
    "two",
    "axes",
    "four",
    "different",
    "classes",
    "going",
    "perfect",
    "got",
    "red",
    "dots",
    "basically",
    "blue",
    "dots",
    "next",
    "step",
    "well",
    "got",
    "data",
    "ready",
    "time",
    "build",
    "model",
    "see",
    "next",
    "video",
    "let",
    "build",
    "first",
    "classification",
    "model",
    "welcome",
    "back",
    "last",
    "video",
    "created",
    "classification",
    "data",
    "set",
    "using",
    "function",
    "well",
    "going",
    "put",
    "covered",
    "far",
    "together",
    "instead",
    "using",
    "binary",
    "classification",
    "working",
    "binary",
    "classification",
    "data",
    "going",
    "classification",
    "data",
    "said",
    "let",
    "get",
    "building",
    "classification",
    "model",
    "create",
    "little",
    "heading",
    "building",
    "classification",
    "model",
    "pytorch",
    "want",
    "think",
    "spent",
    "last",
    "videos",
    "covering",
    "data",
    "set",
    "need",
    "could",
    "separate",
    "data",
    "set",
    "pure",
    "straight",
    "lines",
    "need",
    "lines",
    "well",
    "think",
    "okay",
    "sure",
    "going",
    "building",
    "model",
    "fit",
    "data",
    "anyway",
    "draw",
    "patterns",
    "data",
    "anyway",
    "get",
    "coding",
    "model",
    "classification",
    "got",
    "input",
    "layer",
    "shape",
    "need",
    "define",
    "features",
    "many",
    "features",
    "hidden",
    "layers",
    "well",
    "could",
    "set",
    "whatever",
    "want",
    "going",
    "keep",
    "nice",
    "simple",
    "number",
    "neurons",
    "per",
    "hidden",
    "layer",
    "could",
    "almost",
    "whatever",
    "want",
    "working",
    "relatively",
    "small",
    "data",
    "set",
    "got",
    "four",
    "different",
    "classes",
    "got",
    "thousand",
    "data",
    "points",
    "keep",
    "small",
    "well",
    "could",
    "change",
    "remember",
    "change",
    "hyper",
    "parameters",
    "output",
    "layer",
    "shape",
    "well",
    "many",
    "output",
    "features",
    "want",
    "need",
    "one",
    "per",
    "class",
    "many",
    "classes",
    "four",
    "clusters",
    "different",
    "dots",
    "need",
    "four",
    "output",
    "features",
    "go",
    "back",
    "output",
    "activation",
    "softmax",
    "seen",
    "yet",
    "loss",
    "function",
    "rather",
    "binary",
    "cross",
    "entropy",
    "cross",
    "entropy",
    "optimizer",
    "well",
    "binary",
    "classification",
    "two",
    "common",
    "sgds",
    "stochastic",
    "gradient",
    "descent",
    "atom",
    "optimizer",
    "course",
    "package",
    "many",
    "different",
    "options",
    "well",
    "let",
    "push",
    "forward",
    "create",
    "first",
    "classification",
    "model",
    "first",
    "going",
    "create",
    "going",
    "get",
    "habit",
    "creating",
    "device",
    "agnostic",
    "code",
    "set",
    "device",
    "equals",
    "cuda",
    "nothing",
    "seen",
    "put",
    "together",
    "lot",
    "practice",
    "available",
    "else",
    "cpu",
    "let",
    "go",
    "device",
    "gpu",
    "available",
    "beautiful",
    "cuda",
    "course",
    "go",
    "change",
    "runtime",
    "type",
    "select",
    "gpu",
    "restart",
    "runtime",
    "run",
    "code",
    "cell",
    "well",
    "going",
    "using",
    "gpu",
    "necessarily",
    "need",
    "one",
    "data",
    "set",
    "quite",
    "small",
    "models",
    "going",
    "large",
    "set",
    "device",
    "agnostic",
    "code",
    "let",
    "build",
    "classification",
    "model",
    "look",
    "us",
    "go",
    "covering",
    "foundations",
    "classification",
    "general",
    "know",
    "combine",
    "linear",
    "functions",
    "create",
    "neural",
    "networks",
    "find",
    "patterns",
    "almost",
    "kind",
    "data",
    "going",
    "call",
    "class",
    "blob",
    "model",
    "going",
    "course",
    "inherit",
    "going",
    "upgrade",
    "class",
    "going",
    "take",
    "inputs",
    "show",
    "familiar",
    "python",
    "classes",
    "would",
    "already",
    "done",
    "stuff",
    "like",
    "going",
    "set",
    "parameters",
    "models",
    "write",
    "complex",
    "classes",
    "want",
    "take",
    "inputs",
    "going",
    "hidden",
    "units",
    "parameter",
    "eight",
    "decided",
    "know",
    "going",
    "start",
    "eight",
    "hidden",
    "units",
    "wanted",
    "change",
    "128",
    "could",
    "constructor",
    "got",
    "options",
    "input",
    "features",
    "going",
    "set",
    "programmatically",
    "inputs",
    "class",
    "instantiate",
    "output",
    "features",
    "well",
    "going",
    "call",
    "self",
    "oh",
    "super",
    "sorry",
    "always",
    "get",
    "mixed",
    "dot",
    "init",
    "underscore",
    "underscore",
    "beautiful",
    "could",
    "doc",
    "string",
    "well",
    "let",
    "write",
    "initializes",
    "classification",
    "could",
    "spell",
    "class",
    "model",
    "oh",
    "great",
    "arcs",
    "standard",
    "way",
    "writing",
    "doc",
    "strings",
    "want",
    "find",
    "google",
    "python",
    "doc",
    "string",
    "guide",
    "go",
    "google",
    "python",
    "style",
    "guide",
    "get",
    "mine",
    "scroll",
    "way",
    "write",
    "python",
    "code",
    "yeah",
    "go",
    "got",
    "little",
    "sentence",
    "saying",
    "going",
    "got",
    "arcs",
    "got",
    "returns",
    "got",
    "errors",
    "something",
    "going",
    "highly",
    "recommend",
    "checking",
    "little",
    "tidbit",
    "someone",
    "use",
    "class",
    "later",
    "know",
    "input",
    "features",
    "input",
    "features",
    "int",
    "number",
    "input",
    "features",
    "model",
    "course",
    "got",
    "output",
    "features",
    "also",
    "int",
    "number",
    "output",
    "features",
    "model",
    "got",
    "red",
    "line",
    "telling",
    "us",
    "got",
    "something",
    "wrong",
    "okay",
    "hidden",
    "features",
    "oh",
    "well",
    "number",
    "output",
    "classes",
    "case",
    "classification",
    "hidden",
    "units",
    "int",
    "number",
    "hidden",
    "units",
    "layers",
    "default",
    "eight",
    "beautiful",
    "going",
    "fix",
    "yeah",
    "go",
    "could",
    "put",
    "returns",
    "returns",
    "whatever",
    "returns",
    "example",
    "use",
    "case",
    "leave",
    "fill",
    "like",
    "let",
    "instantiate",
    "things",
    "might",
    "write",
    "self",
    "dot",
    "linear",
    "layer",
    "stack",
    "self",
    "dot",
    "linear",
    "layer",
    "stack",
    "set",
    "nn",
    "dot",
    "sequential",
    "ooh",
    "seen",
    "going",
    "look",
    "different",
    "way",
    "writing",
    "model",
    "previously",
    "created",
    "model",
    "well",
    "instantiated",
    "layer",
    "parameter",
    "called",
    "one",
    "one",
    "straightforward",
    "fashion",
    "going",
    "use",
    "sequential",
    "step",
    "layers",
    "anything",
    "fancy",
    "set",
    "sequential",
    "stack",
    "layers",
    "recall",
    "sequential",
    "steps",
    "passes",
    "data",
    "one",
    "layers",
    "one",
    "one",
    "set",
    "parameters",
    "input",
    "features",
    "equal",
    "input",
    "features",
    "output",
    "features",
    "going",
    "going",
    "output",
    "features",
    "going",
    "hidden",
    "units",
    "going",
    "hidden",
    "units",
    "final",
    "layer",
    "want",
    "final",
    "layer",
    "output",
    "output",
    "features",
    "input",
    "features",
    "hidden",
    "units",
    "remember",
    "subsequent",
    "layer",
    "needs",
    "line",
    "previous",
    "layer",
    "output",
    "features",
    "going",
    "create",
    "another",
    "one",
    "outputs",
    "hidden",
    "units",
    "go",
    "features",
    "equals",
    "hidden",
    "units",
    "takes",
    "output",
    "features",
    "previous",
    "layer",
    "see",
    "output",
    "features",
    "feeds",
    "output",
    "features",
    "feeds",
    "finally",
    "going",
    "final",
    "layer",
    "three",
    "layers",
    "output",
    "features",
    "equals",
    "output",
    "features",
    "wonderful",
    "know",
    "values",
    "well",
    "let",
    "look",
    "case",
    "x",
    "two",
    "input",
    "features",
    "case",
    "well",
    "little",
    "confusing",
    "well",
    "scalar",
    "think",
    "values",
    "going",
    "well",
    "let",
    "go",
    "np",
    "sure",
    "let",
    "find",
    "together",
    "hey",
    "torch",
    "unique",
    "zero",
    "one",
    "ytrain",
    "oh",
    "need",
    "blob",
    "train",
    "right",
    "blob",
    "used",
    "writing",
    "blob",
    "need",
    "blob",
    "train",
    "believe",
    "blob",
    "go",
    "four",
    "classes",
    "need",
    "output",
    "features",
    "value",
    "four",
    "wanted",
    "add",
    "nonlinearity",
    "could",
    "put",
    "layers",
    "like",
    "asked",
    "question",
    "think",
    "data",
    "set",
    "needs",
    "nonlinearity",
    "well",
    "let",
    "leave",
    "begin",
    "one",
    "challenges",
    "oh",
    "need",
    "commerce",
    "think",
    "need",
    "commerce",
    "one",
    "challenges",
    "test",
    "model",
    "nonlinearity",
    "without",
    "nonlinearity",
    "let",
    "leave",
    "time",
    "missing",
    "well",
    "need",
    "forward",
    "method",
    "def",
    "forward",
    "self",
    "well",
    "created",
    "linear",
    "layer",
    "stack",
    "using",
    "go",
    "return",
    "linear",
    "layer",
    "stack",
    "pass",
    "going",
    "happen",
    "whatever",
    "input",
    "goes",
    "forward",
    "method",
    "going",
    "go",
    "layers",
    "sequentially",
    "oh",
    "need",
    "put",
    "self",
    "initialized",
    "constructor",
    "beautiful",
    "let",
    "create",
    "instance",
    "blob",
    "model",
    "send",
    "target",
    "device",
    "go",
    "model",
    "four",
    "equals",
    "blob",
    "model",
    "use",
    "input",
    "features",
    "parameter",
    "one",
    "going",
    "pass",
    "value",
    "two",
    "output",
    "features",
    "two",
    "x",
    "features",
    "output",
    "feature",
    "going",
    "number",
    "classes",
    "10",
    "classes",
    "set",
    "go",
    "four",
    "hidden",
    "units",
    "going",
    "eight",
    "default",
    "put",
    "going",
    "put",
    "anyway",
    "course",
    "going",
    "send",
    "device",
    "going",
    "go",
    "model",
    "four",
    "get",
    "wrong",
    "unexpected",
    "keyword",
    "argument",
    "output",
    "features",
    "spell",
    "something",
    "wrong",
    "doubt",
    "got",
    "spelling",
    "mistake",
    "output",
    "features",
    "output",
    "features",
    "oh",
    "features",
    "ah",
    "needed",
    "features",
    "output",
    "got",
    "little",
    "confused",
    "okay",
    "go",
    "okay",
    "beautiful",
    "recall",
    "parameter",
    "end",
    "linear",
    "pick",
    "features",
    "output",
    "features",
    "output",
    "features",
    "little",
    "confusing",
    "final",
    "layout",
    "output",
    "layers",
    "number",
    "features",
    "got",
    "classification",
    "model",
    "lines",
    "data",
    "using",
    "shapes",
    "line",
    "beautiful",
    "well",
    "next",
    "well",
    "create",
    "loss",
    "function",
    "course",
    "training",
    "loop",
    "see",
    "next",
    "videos",
    "let",
    "together",
    "welcome",
    "back",
    "last",
    "video",
    "created",
    "classification",
    "model",
    "subclassing",
    "end",
    "module",
    "set",
    "parameters",
    "class",
    "constructor",
    "made",
    "instance",
    "blob",
    "model",
    "could",
    "customize",
    "input",
    "features",
    "output",
    "features",
    "remember",
    "lines",
    "many",
    "features",
    "x",
    "output",
    "features",
    "lines",
    "many",
    "classes",
    "data",
    "10",
    "classes",
    "could",
    "change",
    "would",
    "line",
    "wanted",
    "128",
    "hidden",
    "units",
    "well",
    "could",
    "change",
    "getting",
    "little",
    "bit",
    "programmatic",
    "create",
    "models",
    "see",
    "later",
    "lot",
    "things",
    "built",
    "also",
    "functionalized",
    "similar",
    "matter",
    "let",
    "keep",
    "pushing",
    "forward",
    "next",
    "step",
    "build",
    "model",
    "refer",
    "workflow",
    "see",
    "create",
    "loss",
    "function",
    "optimizer",
    "classification",
    "model",
    "option",
    "creating",
    "loss",
    "function",
    "find",
    "loss",
    "functions",
    "pytorch",
    "going",
    "get",
    "make",
    "new",
    "tab",
    "search",
    "basic",
    "building",
    "box",
    "graphs",
    "words",
    "neural",
    "networks",
    "find",
    "loss",
    "functions",
    "hmm",
    "go",
    "beautiful",
    "seen",
    "l1",
    "loss",
    "mse",
    "loss",
    "could",
    "used",
    "regression",
    "predicting",
    "number",
    "tell",
    "well",
    "classification",
    "going",
    "looking",
    "cross",
    "entropy",
    "loss",
    "classification",
    "binary",
    "classification",
    "work",
    "bce",
    "loss",
    "course",
    "going",
    "leave",
    "something",
    "explore",
    "let",
    "jump",
    "cross",
    "entropy",
    "loss",
    "criterion",
    "computes",
    "remember",
    "loss",
    "function",
    "pytorch",
    "also",
    "referred",
    "criterion",
    "might",
    "also",
    "see",
    "loss",
    "function",
    "referred",
    "cost",
    "function",
    "call",
    "loss",
    "functions",
    "criterion",
    "computes",
    "cross",
    "entropy",
    "loss",
    "input",
    "target",
    "okay",
    "input",
    "something",
    "target",
    "target",
    "labels",
    "useful",
    "training",
    "classification",
    "problem",
    "c",
    "classes",
    "go",
    "training",
    "classification",
    "problem",
    "c",
    "classes",
    "c",
    "number",
    "classes",
    "provided",
    "optional",
    "argument",
    "weight",
    "1d",
    "tensor",
    "assigning",
    "weight",
    "classes",
    "apply",
    "weight",
    "would",
    "apply",
    "weight",
    "well",
    "says",
    "look",
    "weight",
    "particularly",
    "useful",
    "unbalanced",
    "training",
    "set",
    "keep",
    "mind",
    "going",
    "forward",
    "wanted",
    "train",
    "dataset",
    "imbalanced",
    "samples",
    "case",
    "number",
    "samples",
    "class",
    "sometimes",
    "might",
    "come",
    "across",
    "dataset",
    "maybe",
    "10",
    "yellow",
    "dots",
    "maybe",
    "500",
    "blue",
    "dots",
    "100",
    "red",
    "100",
    "light",
    "blue",
    "dots",
    "unbalanced",
    "dataset",
    "come",
    "look",
    "weight",
    "parameter",
    "going",
    "keep",
    "things",
    "simple",
    "balanced",
    "dataset",
    "going",
    "focus",
    "using",
    "loss",
    "function",
    "like",
    "read",
    "please",
    "read",
    "wanted",
    "find",
    "could",
    "go",
    "cross",
    "entropy",
    "loss",
    "sure",
    "find",
    "whole",
    "bunch",
    "loss",
    "functions",
    "go",
    "ml",
    "cheat",
    "sheet",
    "love",
    "ml",
    "glossary",
    "one",
    "favorite",
    "websites",
    "towards",
    "data",
    "science",
    "find",
    "website",
    "wikipedia",
    "machine",
    "learning",
    "mastery",
    "also",
    "another",
    "fantastic",
    "website",
    "time",
    "let",
    "code",
    "together",
    "hey",
    "set",
    "loss",
    "function",
    "oh",
    "one",
    "resource",
    "get",
    "code",
    "got",
    "architecture",
    "well",
    "typical",
    "architecture",
    "classification",
    "model",
    "loss",
    "function",
    "classification",
    "cross",
    "entropy",
    "entropy",
    "loss",
    "let",
    "code",
    "doubt",
    "code",
    "create",
    "loss",
    "function",
    "classification",
    "go",
    "loss",
    "fn",
    "equals",
    "dot",
    "cross",
    "entropy",
    "loss",
    "beautiful",
    "want",
    "create",
    "optimizer",
    "create",
    "optimizer",
    "classification",
    "beautiful",
    "thing",
    "optimizers",
    "quite",
    "flexible",
    "go",
    "across",
    "wide",
    "range",
    "different",
    "problems",
    "optimizer",
    "two",
    "common",
    "say",
    "common",
    "work",
    "quite",
    "well",
    "across",
    "wide",
    "range",
    "problems",
    "listed",
    "two",
    "course",
    "within",
    "torch",
    "dot",
    "opt",
    "module",
    "find",
    "lot",
    "different",
    "optimizers",
    "let",
    "stick",
    "sgd",
    "go",
    "back",
    "go",
    "optimizer",
    "equals",
    "torch",
    "dot",
    "opt",
    "optimizer",
    "sgd",
    "stochastic",
    "gradient",
    "descent",
    "parameters",
    "want",
    "optimizer",
    "optimize",
    "model",
    "four",
    "fourth",
    "model",
    "already",
    "oh",
    "goodness",
    "model",
    "four",
    "dot",
    "parameters",
    "set",
    "learning",
    "rate",
    "course",
    "could",
    "change",
    "learning",
    "rate",
    "wanted",
    "fact",
    "encourage",
    "see",
    "happens",
    "learning",
    "rate",
    "hyper",
    "parameter",
    "better",
    "writing",
    "code",
    "spelling",
    "change",
    "wonderful",
    "got",
    "loss",
    "function",
    "optimizer",
    "multi",
    "class",
    "classification",
    "problem",
    "next",
    "well",
    "could",
    "start",
    "build",
    "building",
    "training",
    "loop",
    "could",
    "start",
    "think",
    "look",
    "outputs",
    "model",
    "specifically",
    "getting",
    "prediction",
    "probabilities",
    "multi",
    "class",
    "pie",
    "torch",
    "model",
    "challenge",
    "next",
    "video",
    "look",
    "happens",
    "pass",
    "x",
    "blob",
    "test",
    "model",
    "remember",
    "model",
    "raw",
    "output",
    "referred",
    "oh",
    "let",
    "think",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "created",
    "loss",
    "function",
    "optimizer",
    "multi",
    "class",
    "classification",
    "model",
    "recall",
    "loss",
    "function",
    "measures",
    "wrong",
    "model",
    "predictions",
    "optimizer",
    "optimizer",
    "updates",
    "model",
    "parameters",
    "try",
    "reduce",
    "loss",
    "also",
    "issued",
    "challenge",
    "forward",
    "pass",
    "model",
    "four",
    "recent",
    "model",
    "created",
    "oh",
    "give",
    "code",
    "would",
    "work",
    "purpose",
    "maybe",
    "maybe",
    "never",
    "know",
    "work",
    "raw",
    "outputs",
    "model",
    "let",
    "get",
    "raw",
    "outputs",
    "model",
    "recall",
    "raw",
    "outputs",
    "model",
    "called",
    "logits",
    "got",
    "runtime",
    "error",
    "expected",
    "tensors",
    "device",
    "course",
    "come",
    "well",
    "go",
    "next",
    "model",
    "dot",
    "parameters",
    "check",
    "device",
    "happens",
    "oh",
    "need",
    "bring",
    "model",
    "cuda",
    "device",
    "whereas",
    "data",
    "cpu",
    "still",
    "go",
    "x",
    "data",
    "tensor",
    "check",
    "device",
    "parameter",
    "think",
    "might",
    "proven",
    "wrong",
    "oh",
    "cpu",
    "course",
    "getting",
    "runtime",
    "error",
    "catch",
    "one",
    "well",
    "done",
    "let",
    "see",
    "happens",
    "forward",
    "pass",
    "turn",
    "model",
    "vowel",
    "mode",
    "make",
    "predictions",
    "torch",
    "dot",
    "inference",
    "mode",
    "make",
    "predictions",
    "necessarily",
    "tests",
    "good",
    "habit",
    "oh",
    "prads",
    "equals",
    "get",
    "prads",
    "maybe",
    "view",
    "first",
    "get",
    "oh",
    "goodness",
    "much",
    "numbers",
    "page",
    "format",
    "data",
    "test",
    "labels",
    "let",
    "look",
    "okay",
    "oh",
    "need",
    "blob",
    "test",
    "excuse",
    "going",
    "make",
    "mistake",
    "fair",
    "times",
    "need",
    "get",
    "format",
    "hmm",
    "want",
    "notice",
    "one",
    "thing",
    "well",
    "one",
    "value",
    "per",
    "one",
    "value",
    "except",
    "actually",
    "four",
    "values",
    "one",
    "two",
    "three",
    "four",
    "well",
    "set",
    "features",
    "model",
    "outputs",
    "four",
    "features",
    "per",
    "sample",
    "sample",
    "right",
    "four",
    "numbers",
    "associated",
    "called",
    "logits",
    "let",
    "write",
    "order",
    "evaluate",
    "train",
    "test",
    "model",
    "need",
    "convert",
    "model",
    "outputs",
    "outputs",
    "logits",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "done",
    "binary",
    "classification",
    "go",
    "logits",
    "predprobs",
    "pred",
    "labels",
    "right",
    "think",
    "got",
    "logits",
    "convert",
    "logits",
    "prediction",
    "probabilities",
    "well",
    "use",
    "activation",
    "function",
    "go",
    "back",
    "architecture",
    "output",
    "activation",
    "binary",
    "classification",
    "use",
    "sigmoid",
    "classification",
    "two",
    "main",
    "differences",
    "classification",
    "binary",
    "classification",
    "one",
    "uses",
    "softmax",
    "one",
    "uses",
    "cross",
    "entropy",
    "going",
    "take",
    "little",
    "bit",
    "practice",
    "know",
    "heart",
    "took",
    "nice",
    "tables",
    "like",
    "write",
    "lot",
    "code",
    "together",
    "going",
    "use",
    "softmax",
    "function",
    "convert",
    "logits",
    "models",
    "raw",
    "outputs",
    "prediction",
    "probabilities",
    "let",
    "see",
    "convert",
    "models",
    "logit",
    "outputs",
    "prediction",
    "probabilities",
    "let",
    "create",
    "predprobs",
    "like",
    "call",
    "prediction",
    "probabilities",
    "predprobs",
    "short",
    "torch",
    "dot",
    "softmax",
    "go",
    "logits",
    "want",
    "across",
    "first",
    "dimension",
    "let",
    "look",
    "print",
    "logits",
    "get",
    "first",
    "five",
    "values",
    "look",
    "conversion",
    "logits",
    "oh",
    "predprobs",
    "want",
    "compare",
    "predprobs",
    "five",
    "let",
    "check",
    "oh",
    "get",
    "wrong",
    "logits",
    "logits",
    "oh",
    "change",
    "logits",
    "really",
    "raw",
    "output",
    "model",
    "logits",
    "let",
    "rerun",
    "check",
    "know",
    "different",
    "ideally",
    "want",
    "format",
    "test",
    "labels",
    "models",
    "predictions",
    "able",
    "convert",
    "go",
    "okay",
    "beautiful",
    "happening",
    "let",
    "get",
    "add",
    "code",
    "cells",
    "space",
    "wanted",
    "find",
    "happening",
    "torch",
    "dot",
    "softmax",
    "could",
    "could",
    "go",
    "torch",
    "softmax",
    "see",
    "happening",
    "softmax",
    "okay",
    "function",
    "happening",
    "replicated",
    "nonlinear",
    "activation",
    "functions",
    "wanted",
    "replicate",
    "could",
    "well",
    "doubt",
    "code",
    "could",
    "code",
    "got",
    "tools",
    "got",
    "softmax",
    "x",
    "input",
    "takes",
    "exponential",
    "torch",
    "exponential",
    "sum",
    "torch",
    "exponential",
    "think",
    "could",
    "code",
    "wanted",
    "let",
    "stick",
    "got",
    "got",
    "logits",
    "got",
    "softmax",
    "logits",
    "passed",
    "softmax",
    "function",
    "happened",
    "passed",
    "logits",
    "input",
    "gone",
    "activation",
    "function",
    "prediction",
    "probabilities",
    "might",
    "like",
    "daniel",
    "still",
    "numbers",
    "page",
    "also",
    "notice",
    "none",
    "negative",
    "okay",
    "another",
    "little",
    "tidbit",
    "going",
    "sum",
    "one",
    "let",
    "get",
    "first",
    "one",
    "work",
    "go",
    "torch",
    "dot",
    "sum",
    "happens",
    "ooh",
    "sum",
    "one",
    "one",
    "effects",
    "softmax",
    "function",
    "go",
    "torch",
    "dot",
    "max",
    "probes",
    "prediction",
    "probability",
    "multi",
    "class",
    "find",
    "particular",
    "sample",
    "0th",
    "sample",
    "maximum",
    "number",
    "model",
    "saying",
    "model",
    "saying",
    "prediction",
    "probability",
    "much",
    "think",
    "class",
    "number",
    "order",
    "much",
    "think",
    "class",
    "much",
    "think",
    "class",
    "much",
    "think",
    "class",
    "one",
    "value",
    "four",
    "classes",
    "little",
    "bit",
    "confusing",
    "0th",
    "indexed",
    "maximum",
    "value",
    "index",
    "would",
    "get",
    "particular",
    "index",
    "value",
    "whatever",
    "maximum",
    "number",
    "across",
    "values",
    "well",
    "take",
    "argmax",
    "get",
    "tensor",
    "particular",
    "sample",
    "one",
    "model",
    "guesses",
    "predictions",
    "good",
    "well",
    "model",
    "still",
    "predicting",
    "random",
    "numbers",
    "trained",
    "yet",
    "random",
    "output",
    "basically",
    "premise",
    "still",
    "remains",
    "model",
    "thinks",
    "sample",
    "using",
    "random",
    "numbers",
    "thinks",
    "index",
    "1",
    "right",
    "class",
    "class",
    "number",
    "1",
    "particular",
    "sample",
    "next",
    "one",
    "maximum",
    "number",
    "think",
    "would",
    "0th",
    "index",
    "next",
    "one",
    "maximum",
    "number",
    "well",
    "would",
    "0th",
    "index",
    "well",
    "course",
    "numbers",
    "going",
    "change",
    "trained",
    "model",
    "get",
    "maximum",
    "index",
    "value",
    "go",
    "convert",
    "model",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "let",
    "go",
    "ypreds",
    "equals",
    "torch",
    "dot",
    "argmax",
    "ypredprobs",
    "go",
    "across",
    "first",
    "dimension",
    "well",
    "let",
    "look",
    "ypreds",
    "prediction",
    "labels",
    "format",
    "ylob",
    "test",
    "beautiful",
    "yes",
    "although",
    "many",
    "wrong",
    "see",
    "ideally",
    "would",
    "line",
    "model",
    "predicting",
    "making",
    "predictions",
    "random",
    "numbers",
    "model",
    "trained",
    "basically",
    "random",
    "outputs",
    "hopefully",
    "train",
    "model",
    "going",
    "line",
    "values",
    "predictions",
    "going",
    "line",
    "values",
    "test",
    "labels",
    "go",
    "model",
    "raw",
    "outputs",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "classification",
    "problem",
    "let",
    "add",
    "steps",
    "logits",
    "raw",
    "output",
    "model",
    "predprobs",
    "get",
    "prediction",
    "probabilities",
    "use",
    "torch",
    "dot",
    "softmax",
    "softmax",
    "activation",
    "function",
    "pred",
    "labels",
    "take",
    "argmax",
    "prediction",
    "probabilities",
    "going",
    "see",
    "action",
    "later",
    "evaluate",
    "model",
    "feel",
    "like",
    "know",
    "go",
    "logits",
    "prediction",
    "probabilities",
    "pred",
    "labels",
    "write",
    "training",
    "loop",
    "let",
    "set",
    "create",
    "training",
    "loop",
    "testing",
    "loop",
    "pytorch",
    "model",
    "exciting",
    "see",
    "next",
    "video",
    "let",
    "build",
    "first",
    "training",
    "testing",
    "loop",
    "pytorch",
    "model",
    "give",
    "little",
    "hint",
    "quite",
    "similar",
    "training",
    "testing",
    "loops",
    "built",
    "might",
    "want",
    "give",
    "shot",
    "think",
    "otherwise",
    "together",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "go",
    "raw",
    "logits",
    "output",
    "model",
    "raw",
    "output",
    "model",
    "pytorch",
    "model",
    "turned",
    "logits",
    "prediction",
    "probabilities",
    "using",
    "turn",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "taking",
    "argmax",
    "returns",
    "index",
    "maximum",
    "value",
    "occurs",
    "prediction",
    "probability",
    "particular",
    "sample",
    "four",
    "values",
    "outputs",
    "four",
    "values",
    "working",
    "four",
    "classes",
    "working",
    "10",
    "classes",
    "would",
    "10",
    "values",
    "principle",
    "steps",
    "would",
    "still",
    "particular",
    "sample",
    "value",
    "maximum",
    "would",
    "take",
    "index",
    "one",
    "index",
    "0",
    "maximum",
    "value",
    "sample",
    "mean",
    "prediction",
    "labels",
    "random",
    "right",
    "quite",
    "terrible",
    "going",
    "change",
    "going",
    "build",
    "training",
    "testing",
    "loop",
    "model",
    "let",
    "fit",
    "model",
    "data",
    "let",
    "go",
    "set",
    "manual",
    "seeds",
    "torch",
    "dot",
    "manual",
    "seed",
    "worry",
    "much",
    "numbers",
    "page",
    "exactly",
    "inherent",
    "randomness",
    "machine",
    "learning",
    "setting",
    "manual",
    "seeds",
    "try",
    "get",
    "close",
    "possible",
    "guarantee",
    "complete",
    "determinism",
    "means",
    "output",
    "going",
    "try",
    "direction",
    "important",
    "set",
    "number",
    "epochs",
    "going",
    "go",
    "epochs",
    "100",
    "reckon",
    "start",
    "bump",
    "1000",
    "really",
    "wanted",
    "let",
    "put",
    "data",
    "target",
    "device",
    "target",
    "device",
    "well",
    "really",
    "matter",
    "set",
    "device",
    "agnostic",
    "code",
    "whether",
    "working",
    "cpu",
    "gpu",
    "code",
    "use",
    "whatever",
    "device",
    "available",
    "typing",
    "blog",
    "got",
    "x",
    "blob",
    "train",
    "blob",
    "train",
    "going",
    "go",
    "going",
    "go",
    "device",
    "blob",
    "train",
    "device",
    "going",
    "go",
    "x",
    "blob",
    "test",
    "blob",
    "test",
    "equals",
    "x",
    "blob",
    "test",
    "device",
    "otherwise",
    "get",
    "device",
    "issues",
    "later",
    "send",
    "device",
    "well",
    "beautiful",
    "well",
    "loop",
    "data",
    "loop",
    "data",
    "epoch",
    "range",
    "epochs",
    "epoch",
    "range",
    "epox",
    "want",
    "auto",
    "correct",
    "come",
    "google",
    "colab",
    "work",
    "training",
    "first",
    "classification",
    "model",
    "serious",
    "business",
    "joking",
    "actually",
    "quite",
    "fun",
    "model",
    "four",
    "dot",
    "train",
    "let",
    "forward",
    "pass",
    "going",
    "put",
    "much",
    "commentary",
    "logits",
    "logits",
    "raw",
    "outputs",
    "model",
    "go",
    "x",
    "blob",
    "train",
    "x",
    "test",
    "want",
    "x",
    "blob",
    "train",
    "need",
    "turn",
    "auto",
    "correct",
    "google",
    "colab",
    "saying",
    "long",
    "time",
    "pred",
    "equals",
    "torch",
    "dot",
    "softmax",
    "going",
    "logits",
    "prediction",
    "probabilities",
    "torch",
    "softmax",
    "logits",
    "across",
    "first",
    "dimension",
    "take",
    "argmax",
    "dim",
    "equals",
    "one",
    "fact",
    "going",
    "show",
    "little",
    "bit",
    "oh",
    "written",
    "blog",
    "maybe",
    "auto",
    "correct",
    "would",
    "helpful",
    "little",
    "trick",
    "actually",
    "torch",
    "softmax",
    "logits",
    "took",
    "argmax",
    "logits",
    "little",
    "test",
    "take",
    "argmax",
    "logits",
    "see",
    "get",
    "similar",
    "outputs",
    "get",
    "seen",
    "done",
    "completeness",
    "going",
    "use",
    "softmax",
    "activation",
    "function",
    "often",
    "see",
    "practice",
    "calculate",
    "loss",
    "loss",
    "fm",
    "going",
    "use",
    "categorical",
    "cross",
    "entropy",
    "cross",
    "entropy",
    "loss",
    "check",
    "loss",
    "function",
    "cross",
    "entropy",
    "loss",
    "going",
    "compare",
    "models",
    "logits",
    "blob",
    "train",
    "going",
    "going",
    "calculate",
    "accuracy",
    "working",
    "classification",
    "problem",
    "nice",
    "accuracy",
    "well",
    "loss",
    "accuracy",
    "one",
    "main",
    "classification",
    "evaluation",
    "metrics",
    "pred",
    "equals",
    "pred",
    "pred",
    "well",
    "zero",
    "grab",
    "optimizer",
    "optimizer",
    "zero",
    "grad",
    "go",
    "loss",
    "backward",
    "step",
    "optimizer",
    "optimizer",
    "step",
    "step",
    "step",
    "none",
    "steps",
    "covered",
    "forward",
    "pass",
    "calculate",
    "loss",
    "evaluation",
    "metric",
    "choose",
    "zero",
    "optimizer",
    "perform",
    "back",
    "propagation",
    "loss",
    "step",
    "optimizer",
    "optimizer",
    "hopefully",
    "behind",
    "scenes",
    "update",
    "parameters",
    "model",
    "better",
    "represent",
    "patterns",
    "training",
    "data",
    "going",
    "go",
    "testing",
    "code",
    "testing",
    "code",
    "well",
    "inference",
    "code",
    "set",
    "model",
    "vowel",
    "mode",
    "going",
    "turn",
    "things",
    "behind",
    "scenes",
    "model",
    "need",
    "dropout",
    "layers",
    "covered",
    "welcome",
    "check",
    "go",
    "torch",
    "end",
    "dropout",
    "layers",
    "dropout",
    "dropout",
    "layers",
    "beautiful",
    "another",
    "one",
    "turns",
    "match",
    "norm",
    "beautiful",
    "also",
    "could",
    "search",
    "model",
    "dot",
    "vowel",
    "might",
    "come",
    "across",
    "stack",
    "overflow",
    "question",
    "one",
    "favorite",
    "resources",
    "little",
    "bit",
    "extra",
    "curriculum",
    "prefer",
    "see",
    "things",
    "action",
    "torch",
    "inference",
    "mode",
    "turns",
    "things",
    "like",
    "gradient",
    "tracking",
    "things",
    "get",
    "fast",
    "code",
    "possible",
    "need",
    "track",
    "gradients",
    "making",
    "predictions",
    "need",
    "use",
    "parameters",
    "model",
    "learned",
    "want",
    "x",
    "blob",
    "test",
    "go",
    "model",
    "test",
    "logits",
    "test",
    "preds",
    "going",
    "step",
    "done",
    "going",
    "go",
    "torch",
    "dot",
    "softmax",
    "test",
    "logits",
    "across",
    "first",
    "dimension",
    "going",
    "call",
    "argmax",
    "get",
    "index",
    "value",
    "maximum",
    "prediction",
    "probability",
    "value",
    "occurs",
    "going",
    "calculate",
    "test",
    "loss",
    "loss",
    "function",
    "going",
    "pass",
    "test",
    "logits",
    "going",
    "pass",
    "blob",
    "test",
    "compare",
    "test",
    "logits",
    "behind",
    "scenes",
    "loss",
    "function",
    "going",
    "things",
    "convert",
    "test",
    "logits",
    "format",
    "test",
    "labels",
    "return",
    "us",
    "value",
    "also",
    "calculate",
    "test",
    "accuracy",
    "passing",
    "true",
    "blob",
    "test",
    "pred",
    "equals",
    "pred",
    "wonderful",
    "final",
    "step",
    "well",
    "want",
    "print",
    "happening",
    "love",
    "seeing",
    "metrics",
    "model",
    "trains",
    "one",
    "favorite",
    "things",
    "watch",
    "go",
    "epoch",
    "let",
    "every",
    "10",
    "epochs",
    "got",
    "100",
    "far",
    "equals",
    "zero",
    "let",
    "print",
    "nice",
    "f",
    "string",
    "epoch",
    "going",
    "go",
    "loss",
    "put",
    "get",
    "loss",
    "value",
    "take",
    "four",
    "decimal",
    "places",
    "get",
    "training",
    "accuracy",
    "acc",
    "take",
    "two",
    "decimal",
    "places",
    "get",
    "nice",
    "percentage",
    "sign",
    "go",
    "test",
    "loss",
    "equals",
    "test",
    "loss",
    "go",
    "finally",
    "go",
    "test",
    "act",
    "end",
    "test",
    "act",
    "sure",
    "written",
    "fair",
    "either",
    "getting",
    "sick",
    "like",
    "wow",
    "actually",
    "steps",
    "worry",
    "going",
    "functionalizing",
    "later",
    "thought",
    "going",
    "include",
    "much",
    "possible",
    "practice",
    "much",
    "possible",
    "together",
    "ready",
    "train",
    "first",
    "classification",
    "model",
    "three",
    "two",
    "one",
    "let",
    "go",
    "typos",
    "course",
    "get",
    "wrong",
    "oh",
    "fun",
    "error",
    "runtime",
    "error",
    "nll",
    "loss",
    "reduced",
    "cuda",
    "kernel",
    "index",
    "implemented",
    "float",
    "okay",
    "pretty",
    "full",
    "bunch",
    "words",
    "really",
    "know",
    "describe",
    "little",
    "hint",
    "got",
    "float",
    "know",
    "float",
    "float",
    "form",
    "data",
    "data",
    "type",
    "potentially",
    "hint",
    "said",
    "implemented",
    "float",
    "maybe",
    "got",
    "something",
    "wrong",
    "data",
    "wrong",
    "type",
    "see",
    "anywhere",
    "data",
    "might",
    "wrong",
    "type",
    "well",
    "let",
    "print",
    "issue",
    "logits",
    "blob",
    "train",
    "okay",
    "blob",
    "train",
    "logits",
    "blob",
    "train",
    "look",
    "like",
    "blob",
    "train",
    "okay",
    "type",
    "float",
    "okay",
    "implemented",
    "float",
    "hmm",
    "maybe",
    "turn",
    "different",
    "data",
    "type",
    "went",
    "type",
    "torch",
    "long",
    "tensor",
    "happens",
    "expected",
    "tensors",
    "device",
    "found",
    "least",
    "two",
    "devices",
    "oh",
    "goodness",
    "got",
    "wrong",
    "type",
    "torch",
    "long",
    "tensor",
    "friends",
    "guess",
    "found",
    "pesky",
    "little",
    "data",
    "type",
    "issue",
    "run",
    "one",
    "took",
    "find",
    "want",
    "know",
    "behind",
    "scenes",
    "even",
    "though",
    "machine",
    "learning",
    "cooking",
    "show",
    "still",
    "takes",
    "troubleshoot",
    "code",
    "going",
    "come",
    "across",
    "thought",
    "rather",
    "spend",
    "10",
    "minutes",
    "video",
    "show",
    "went",
    "found",
    "hmm",
    "something",
    "going",
    "quite",
    "know",
    "seems",
    "quite",
    "like",
    "long",
    "string",
    "words",
    "implemented",
    "float",
    "looked",
    "back",
    "line",
    "went",
    "wrong",
    "know",
    "maybe",
    "float",
    "hinting",
    "one",
    "two",
    "tensors",
    "wrong",
    "data",
    "type",
    "would",
    "think",
    "wrong",
    "data",
    "type",
    "well",
    "anytime",
    "see",
    "float",
    "int",
    "something",
    "like",
    "generally",
    "hints",
    "one",
    "data",
    "types",
    "wrong",
    "error",
    "actually",
    "right",
    "back",
    "created",
    "tensor",
    "data",
    "turned",
    "labels",
    "float",
    "generally",
    "okay",
    "pytorch",
    "however",
    "one",
    "type",
    "torch",
    "dot",
    "long",
    "tensor",
    "seen",
    "go",
    "torch",
    "long",
    "tensor",
    "let",
    "look",
    "torch",
    "dot",
    "tensor",
    "long",
    "tensor",
    "go",
    "64",
    "bit",
    "integer",
    "signed",
    "need",
    "torch",
    "dot",
    "long",
    "tensor",
    "took",
    "find",
    "want",
    "express",
    "code",
    "probably",
    "butt",
    "head",
    "issues",
    "errors",
    "take",
    "find",
    "data",
    "types",
    "one",
    "main",
    "ones",
    "look",
    "documentation",
    "cross",
    "entropy",
    "loss",
    "way",
    "kind",
    "found",
    "little",
    "hint",
    "performance",
    "criteria",
    "generally",
    "better",
    "target",
    "contains",
    "class",
    "indices",
    "allows",
    "optimized",
    "computation",
    "read",
    "says",
    "target",
    "contains",
    "class",
    "indices",
    "like",
    "hmm",
    "alza",
    "indices",
    "already",
    "maybe",
    "integers",
    "floats",
    "actually",
    "look",
    "sample",
    "code",
    "would",
    "find",
    "use",
    "type",
    "equals",
    "torch",
    "dot",
    "long",
    "thing",
    "lot",
    "code",
    "around",
    "internet",
    "sometimes",
    "answer",
    "looking",
    "little",
    "bit",
    "buried",
    "doubt",
    "run",
    "code",
    "butt",
    "head",
    "wall",
    "bit",
    "keep",
    "going",
    "let",
    "rerun",
    "see",
    "error",
    "let",
    "train",
    "first",
    "classification",
    "model",
    "together",
    "arrows",
    "fingers",
    "crossed",
    "get",
    "wrong",
    "ok",
    "got",
    "different",
    "size",
    "slowly",
    "working",
    "errors",
    "deep",
    "learning",
    "value",
    "error",
    "input",
    "batch",
    "size",
    "200",
    "match",
    "target",
    "size",
    "telling",
    "maybe",
    "test",
    "data",
    "size",
    "200",
    "getting",
    "mixed",
    "training",
    "data",
    "size",
    "got",
    "test",
    "loss",
    "test",
    "logits",
    "model",
    "four",
    "size",
    "let",
    "print",
    "print",
    "test",
    "logits",
    "dot",
    "shape",
    "wine",
    "blob",
    "test",
    "troubleshooting",
    "fly",
    "everyone",
    "got",
    "torch",
    "size",
    "test",
    "labels",
    "coming",
    "wine",
    "blob",
    "test",
    "equals",
    "oh",
    "go",
    "ah",
    "catch",
    "maybe",
    "maybe",
    "think",
    "right",
    "comment",
    "line",
    "data",
    "type",
    "issue",
    "shape",
    "issue",
    "two",
    "main",
    "machine",
    "learning",
    "oh",
    "issues",
    "wine",
    "blob",
    "test",
    "going",
    "thought",
    "changed",
    "shape",
    "oh",
    "go",
    "reassign",
    "definitely",
    "blob",
    "yes",
    "let",
    "rerun",
    "reassign",
    "data",
    "running",
    "every",
    "single",
    "error",
    "glad",
    "otherwise",
    "might",
    "see",
    "troubleshoot",
    "type",
    "things",
    "size",
    "tensor",
    "much",
    "match",
    "size",
    "oh",
    "getting",
    "issue",
    "test",
    "spreads",
    "oh",
    "goodness",
    "written",
    "much",
    "code",
    "test",
    "spreads",
    "instead",
    "wire",
    "spread",
    "test",
    "spreads",
    "fingers",
    "crossed",
    "training",
    "first",
    "model",
    "yet",
    "go",
    "okay",
    "going",
    "printing",
    "stuff",
    "really",
    "want",
    "print",
    "stuff",
    "want",
    "see",
    "loss",
    "go",
    "going",
    "friends",
    "hope",
    "know",
    "fundamental",
    "troubleshooting",
    "steps",
    "might",
    "say",
    "oh",
    "daniel",
    "cop",
    "coding",
    "wrong",
    "fact",
    "code",
    "wrong",
    "time",
    "worked",
    "troubleshoot",
    "shape",
    "errors",
    "data",
    "type",
    "errors",
    "look",
    "thank",
    "goodness",
    "loss",
    "accuracy",
    "go",
    "directions",
    "want",
    "go",
    "loss",
    "goes",
    "accuracy",
    "goes",
    "beautiful",
    "looks",
    "like",
    "model",
    "working",
    "classification",
    "data",
    "set",
    "check",
    "well",
    "going",
    "evaluate",
    "next",
    "step",
    "visualize",
    "visualize",
    "visualize",
    "might",
    "want",
    "give",
    "shot",
    "see",
    "use",
    "plot",
    "decision",
    "boundary",
    "function",
    "use",
    "model",
    "separate",
    "data",
    "going",
    "much",
    "binary",
    "classification",
    "time",
    "using",
    "different",
    "model",
    "different",
    "data",
    "set",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "went",
    "steps",
    "terms",
    "training",
    "testing",
    "model",
    "also",
    "butted",
    "heads",
    "two",
    "common",
    "issues",
    "machine",
    "learning",
    "deep",
    "learning",
    "general",
    "data",
    "type",
    "issues",
    "shape",
    "issues",
    "luckily",
    "able",
    "resolve",
    "trust",
    "going",
    "run",
    "across",
    "many",
    "deep",
    "learning",
    "machine",
    "learning",
    "endeavors",
    "glad",
    "got",
    "look",
    "sort",
    "could",
    "show",
    "troubleshoot",
    "reality",
    "lot",
    "experimentation",
    "run",
    "code",
    "see",
    "errors",
    "come",
    "google",
    "errors",
    "read",
    "documentation",
    "try",
    "said",
    "looks",
    "like",
    "model",
    "classification",
    "model",
    "learned",
    "something",
    "loss",
    "going",
    "accuracy",
    "going",
    "evaluate",
    "making",
    "evaluating",
    "predictions",
    "pytorch",
    "model",
    "make",
    "predictions",
    "seen",
    "step",
    "let",
    "reiterate",
    "make",
    "predictions",
    "going",
    "set",
    "model",
    "mode",
    "vowel",
    "mode",
    "going",
    "turn",
    "context",
    "manager",
    "inference",
    "mode",
    "want",
    "make",
    "inference",
    "want",
    "make",
    "predictions",
    "make",
    "predictions",
    "predictions",
    "going",
    "logits",
    "raw",
    "outputs",
    "model",
    "take",
    "model",
    "four",
    "trained",
    "pass",
    "test",
    "data",
    "well",
    "needs",
    "blob",
    "test",
    "way",
    "keep",
    "getting",
    "variable",
    "mixed",
    "enough",
    "problems",
    "data",
    "daniel",
    "need",
    "completely",
    "right",
    "agree",
    "probably",
    "going",
    "come",
    "across",
    "problems",
    "future",
    "worry",
    "let",
    "view",
    "first",
    "10",
    "predictions",
    "logits",
    "look",
    "like",
    "right",
    "numbers",
    "page",
    "raw",
    "logits",
    "go",
    "go",
    "logits",
    "prediction",
    "probabilities",
    "model",
    "go",
    "equals",
    "want",
    "across",
    "first",
    "dimension",
    "go",
    "let",
    "go",
    "first",
    "apples",
    "apples",
    "yet",
    "test",
    "look",
    "like",
    "apples",
    "apples",
    "yet",
    "close",
    "prediction",
    "probabilities",
    "notice",
    "get",
    "fairly",
    "different",
    "values",
    "remember",
    "one",
    "closest",
    "one",
    "value",
    "closest",
    "one",
    "looks",
    "like",
    "index",
    "maximum",
    "value",
    "going",
    "model",
    "predicted",
    "class",
    "index",
    "index",
    "one",
    "correlate",
    "yes",
    "one",
    "beautiful",
    "index",
    "three",
    "maximum",
    "value",
    "three",
    "beautiful",
    "index",
    "two",
    "yes",
    "okay",
    "wonderful",
    "let",
    "step",
    "programmers",
    "code",
    "let",
    "go",
    "well",
    "pass",
    "dim",
    "equals",
    "one",
    "could",
    "also",
    "way",
    "call",
    "real",
    "difference",
    "two",
    "going",
    "way",
    "called",
    "let",
    "view",
    "first",
    "comparing",
    "apples",
    "apples",
    "go",
    "test",
    "yes",
    "go",
    "look",
    "one",
    "three",
    "two",
    "one",
    "zero",
    "three",
    "one",
    "three",
    "two",
    "one",
    "zero",
    "three",
    "beautiful",
    "could",
    "line",
    "look",
    "compare",
    "day",
    "mean",
    "would",
    "fun",
    "know",
    "something",
    "would",
    "even",
    "fun",
    "let",
    "get",
    "visual",
    "plot",
    "dot",
    "figure",
    "going",
    "go",
    "fig",
    "size",
    "equals",
    "beauty",
    "cooking",
    "show",
    "kind",
    "know",
    "ingredients",
    "work",
    "ahead",
    "time",
    "despite",
    "saw",
    "last",
    "video",
    "trouble",
    "shooting",
    "actually",
    "glad",
    "seriously",
    "shape",
    "issues",
    "data",
    "type",
    "issues",
    "going",
    "come",
    "across",
    "lot",
    "two",
    "main",
    "issues",
    "troubleshoot",
    "aside",
    "device",
    "issues",
    "let",
    "go",
    "train",
    "train",
    "going",
    "another",
    "plot",
    "going",
    "get",
    "subplot",
    "one",
    "two",
    "two",
    "going",
    "test",
    "data",
    "test",
    "plot",
    "decision",
    "boundary",
    "plot",
    "decision",
    "boundary",
    "model",
    "four",
    "test",
    "test",
    "well",
    "let",
    "see",
    "train",
    "oh",
    "goodness",
    "yes",
    "code",
    "worked",
    "faster",
    "speak",
    "look",
    "beautiful",
    "looking",
    "plot",
    "separated",
    "data",
    "almost",
    "best",
    "could",
    "like",
    "quite",
    "inconspicuous",
    "thing",
    "lines",
    "model",
    "worked",
    "posed",
    "question",
    "fair",
    "videos",
    "ago",
    "whenever",
    "created",
    "model",
    "could",
    "separate",
    "data",
    "without",
    "nonlinear",
    "functions",
    "test",
    "since",
    "got",
    "code",
    "ready",
    "let",
    "go",
    "back",
    "got",
    "nonlinear",
    "functions",
    "got",
    "relu",
    "going",
    "recreate",
    "model",
    "took",
    "relu",
    "commented",
    "code",
    "still",
    "work",
    "fingers",
    "crossed",
    "count",
    "chickens",
    "hatch",
    "daniel",
    "come",
    "going",
    "rerun",
    "cells",
    "code",
    "going",
    "stay",
    "took",
    "nonlinearity",
    "model",
    "still",
    "going",
    "work",
    "oh",
    "goodness",
    "still",
    "works",
    "well",
    "notice",
    "lines",
    "lot",
    "straighter",
    "get",
    "different",
    "metrics",
    "leave",
    "compare",
    "maybe",
    "little",
    "bit",
    "different",
    "think",
    "far",
    "different",
    "data",
    "linearly",
    "separable",
    "draw",
    "straight",
    "lines",
    "separate",
    "data",
    "however",
    "lot",
    "data",
    "deal",
    "practice",
    "require",
    "linear",
    "nonlinear",
    "hence",
    "spent",
    "lot",
    "time",
    "like",
    "circle",
    "data",
    "covered",
    "let",
    "look",
    "image",
    "pizza",
    "building",
    "food",
    "vision",
    "model",
    "take",
    "photos",
    "food",
    "separate",
    "different",
    "classes",
    "food",
    "could",
    "straight",
    "lines",
    "might",
    "able",
    "personally",
    "think",
    "could",
    "build",
    "model",
    "thing",
    "fact",
    "pytorch",
    "makes",
    "easy",
    "add",
    "nonlinearities",
    "model",
    "might",
    "well",
    "model",
    "use",
    "needs",
    "need",
    "well",
    "hey",
    "going",
    "build",
    "pretty",
    "good",
    "model",
    "saw",
    "included",
    "nonlinearities",
    "model",
    "could",
    "uncomment",
    "model",
    "still",
    "going",
    "perform",
    "quite",
    "well",
    "beauty",
    "neural",
    "networks",
    "decide",
    "numbers",
    "represent",
    "outdated",
    "best",
    "said",
    "evaluated",
    "model",
    "trained",
    "classification",
    "model",
    "put",
    "everything",
    "together",
    "gone",
    "binary",
    "classification",
    "classification",
    "think",
    "one",
    "thing",
    "cover",
    "let",
    "go",
    "section",
    "number",
    "nine",
    "classification",
    "metrics",
    "said",
    "evaluating",
    "model",
    "let",
    "put",
    "evaluate",
    "model",
    "classification",
    "models",
    "evaluating",
    "model",
    "important",
    "training",
    "model",
    "see",
    "next",
    "video",
    "let",
    "cover",
    "classification",
    "metrics",
    "welcome",
    "back",
    "last",
    "video",
    "evaluated",
    "classification",
    "model",
    "visually",
    "saw",
    "pretty",
    "darn",
    "well",
    "data",
    "turned",
    "linearly",
    "separable",
    "model",
    "even",
    "without",
    "functions",
    "could",
    "separate",
    "data",
    "however",
    "said",
    "data",
    "deal",
    "require",
    "form",
    "linear",
    "function",
    "keep",
    "mind",
    "beauty",
    "pytorch",
    "allows",
    "us",
    "create",
    "models",
    "linear",
    "functions",
    "quite",
    "flexibly",
    "let",
    "write",
    "wanted",
    "evaluate",
    "classification",
    "models",
    "seen",
    "accuracy",
    "accuracy",
    "one",
    "main",
    "methods",
    "evaluating",
    "classification",
    "models",
    "like",
    "saying",
    "100",
    "samples",
    "many",
    "model",
    "get",
    "right",
    "seen",
    "model",
    "right",
    "testing",
    "accuracy",
    "nearly",
    "100",
    "nearly",
    "perfect",
    "course",
    "tough",
    "samples",
    "mean",
    "little",
    "bit",
    "hard",
    "even",
    "within",
    "samples",
    "forgive",
    "little",
    "bit",
    "exactly",
    "perfect",
    "metrics",
    "well",
    "also",
    "got",
    "precision",
    "also",
    "got",
    "recall",
    "pretty",
    "important",
    "classes",
    "different",
    "amounts",
    "values",
    "precision",
    "recall",
    "accuracy",
    "pretty",
    "good",
    "use",
    "balanced",
    "classes",
    "text",
    "page",
    "f1",
    "score",
    "combines",
    "precision",
    "recall",
    "also",
    "confusion",
    "matrix",
    "also",
    "classification",
    "report",
    "going",
    "show",
    "code",
    "examples",
    "access",
    "going",
    "leave",
    "extra",
    "curriculum",
    "try",
    "one",
    "let",
    "go",
    "keynote",
    "way",
    "pay",
    "back",
    "gone",
    "pytorch",
    "workflow",
    "classification",
    "problem",
    "binary",
    "classification",
    "done",
    "classification",
    "well",
    "let",
    "stop",
    "though",
    "remember",
    "building",
    "model",
    "evaluating",
    "model",
    "important",
    "building",
    "model",
    "seen",
    "could",
    "replicate",
    "functions",
    "talked",
    "machine",
    "learning",
    "explorer",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "machine",
    "learning",
    "practitioners",
    "motto",
    "experiment",
    "experiment",
    "experiment",
    "think",
    "called",
    "machine",
    "learning",
    "data",
    "scientist",
    "motto",
    "thing",
    "know",
    "steps",
    "modeling",
    "pytorch",
    "seen",
    "practice",
    "need",
    "look",
    "slides",
    "mean",
    "available",
    "github",
    "want",
    "common",
    "classification",
    "evaluation",
    "methods",
    "accuracy",
    "formal",
    "formula",
    "want",
    "also",
    "code",
    "focusing",
    "wrote",
    "accuracy",
    "function",
    "replicates",
    "way",
    "tp",
    "stands",
    "toilet",
    "paper",
    "stands",
    "true",
    "positive",
    "tn",
    "true",
    "negative",
    "false",
    "positive",
    "fp",
    "false",
    "negative",
    "fn",
    "code",
    "could",
    "torch",
    "metrics",
    "oh",
    "use",
    "default",
    "metric",
    "classification",
    "problems",
    "note",
    "best",
    "imbalanced",
    "classes",
    "example",
    "samples",
    "one",
    "class",
    "number",
    "one",
    "label",
    "number",
    "one",
    "10",
    "samples",
    "class",
    "zero",
    "accuracy",
    "might",
    "best",
    "use",
    "imbalanced",
    "data",
    "sets",
    "might",
    "want",
    "look",
    "precision",
    "recall",
    "great",
    "article",
    "called",
    "think",
    "beyond",
    "accuracy",
    "precision",
    "recall",
    "gives",
    "fantastic",
    "overview",
    "go",
    "recommend",
    "go",
    "coestron",
    "highly",
    "recommend",
    "article",
    "extra",
    "curriculum",
    "see",
    "article",
    "use",
    "precision",
    "recall",
    "go",
    "look",
    "back",
    "formal",
    "formula",
    "precision",
    "true",
    "positive",
    "true",
    "positive",
    "plus",
    "false",
    "positive",
    "higher",
    "precision",
    "leads",
    "less",
    "false",
    "positives",
    "false",
    "positives",
    "ideal",
    "probably",
    "want",
    "increase",
    "precision",
    "false",
    "negatives",
    "ideal",
    "want",
    "increase",
    "recall",
    "metric",
    "however",
    "aware",
    "thing",
    "precision",
    "recall",
    "going",
    "find",
    "experimentation",
    "precision",
    "recall",
    "means",
    "generally",
    "increase",
    "precision",
    "lower",
    "recall",
    "inversely",
    "increase",
    "precision",
    "lower",
    "recall",
    "check",
    "aware",
    "going",
    "learn",
    "practice",
    "evaluating",
    "models",
    "like",
    "code",
    "precision",
    "recall",
    "got",
    "well",
    "implementations",
    "many",
    "different",
    "classification",
    "metrics",
    "torchmetrics",
    "library",
    "f1",
    "score",
    "combines",
    "precision",
    "recall",
    "good",
    "combination",
    "want",
    "something",
    "two",
    "finally",
    "confusion",
    "matrix",
    "listed",
    "classification",
    "report",
    "listed",
    "see",
    "classification",
    "report",
    "classification",
    "report",
    "classification",
    "report",
    "kind",
    "puts",
    "together",
    "metrics",
    "talked",
    "go",
    "talking",
    "lot",
    "torchmetrics",
    "let",
    "look",
    "torchmetrics",
    "accuracy",
    "torchmetrics",
    "library",
    "think",
    "comes",
    "google",
    "colab",
    "moment",
    "import",
    "torchmetrics",
    "initialize",
    "metric",
    "built",
    "accuracy",
    "function",
    "beauty",
    "using",
    "torchmetrics",
    "uses",
    "code",
    "got",
    "metric",
    "preds",
    "target",
    "find",
    "value",
    "accuracy",
    "wanted",
    "implement",
    "metrics",
    "could",
    "subclass",
    "metric",
    "class",
    "let",
    "practice",
    "let",
    "check",
    "see",
    "going",
    "grab",
    "copy",
    "want",
    "access",
    "lot",
    "pytorch",
    "metrics",
    "see",
    "torchmetrics",
    "import",
    "torchmetrics",
    "maybe",
    "already",
    "google",
    "colab",
    "right",
    "go",
    "pip",
    "install",
    "torchmetrics",
    "google",
    "colab",
    "access",
    "torchmetrics",
    "going",
    "download",
    "torchmetrics",
    "take",
    "long",
    "quite",
    "small",
    "package",
    "beautiful",
    "going",
    "go",
    "torchmetrics",
    "import",
    "accuracy",
    "wonderful",
    "let",
    "see",
    "use",
    "setup",
    "metric",
    "going",
    "go",
    "torchmetric",
    "underscore",
    "accuracy",
    "could",
    "call",
    "whatever",
    "want",
    "really",
    "need",
    "accuracy",
    "going",
    "set",
    "class",
    "going",
    "calculate",
    "accuracy",
    "model",
    "calling",
    "torchmetric",
    "accuracy",
    "going",
    "pass",
    "threads",
    "blob",
    "test",
    "let",
    "see",
    "happens",
    "oh",
    "get",
    "wrong",
    "runtime",
    "error",
    "expected",
    "tensors",
    "device",
    "found",
    "least",
    "two",
    "devices",
    "oh",
    "course",
    "remember",
    "said",
    "torchmetrics",
    "implements",
    "pytorch",
    "like",
    "code",
    "well",
    "let",
    "check",
    "device",
    "oh",
    "cpu",
    "something",
    "aware",
    "use",
    "torchmetrics",
    "make",
    "sure",
    "metrics",
    "device",
    "using",
    "device",
    "agnostic",
    "code",
    "data",
    "run",
    "get",
    "beautiful",
    "get",
    "accuracy",
    "line",
    "accuracy",
    "function",
    "coded",
    "like",
    "lot",
    "metrics",
    "functions",
    "sure",
    "see",
    "either",
    "torchmetrics",
    "pytorch",
    "like",
    "metrics",
    "aware",
    "use",
    "pytorch",
    "version",
    "device",
    "like",
    "install",
    "metrics",
    "module",
    "metrics",
    "classification",
    "go",
    "look",
    "many",
    "different",
    "types",
    "classification",
    "metrics",
    "torchmetrics",
    "leave",
    "explore",
    "resources",
    "extracurricular",
    "article",
    "use",
    "precision",
    "recall",
    "another",
    "extracurricular",
    "would",
    "go",
    "torchmetrics",
    "module",
    "10",
    "minutes",
    "look",
    "different",
    "metrics",
    "classification",
    "said",
    "think",
    "covered",
    "fair",
    "bit",
    "think",
    "also",
    "time",
    "practice",
    "learned",
    "let",
    "cover",
    "exercises",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "looked",
    "classification",
    "metrics",
    "little",
    "bit",
    "high",
    "level",
    "overview",
    "ways",
    "evaluate",
    "classification",
    "models",
    "linked",
    "extracurricular",
    "might",
    "want",
    "look",
    "well",
    "covered",
    "whole",
    "bunch",
    "code",
    "together",
    "time",
    "practice",
    "stuff",
    "exercises",
    "prepared",
    "go",
    "exercises",
    "well",
    "remember",
    "book",
    "one",
    "chapters",
    "section",
    "look",
    "much",
    "covered",
    "scroll",
    "keep",
    "scrolling",
    "look",
    "covered",
    "module",
    "fair",
    "bit",
    "stuff",
    "bottom",
    "one",
    "exercises",
    "section",
    "exercises",
    "focusing",
    "practicing",
    "code",
    "sections",
    "sections",
    "got",
    "number",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "yeah",
    "seven",
    "exercises",
    "nice",
    "writing",
    "plenty",
    "code",
    "course",
    "extracurricular",
    "challenges",
    "mentioned",
    "throughout",
    "entire",
    "section",
    "zero",
    "two",
    "going",
    "link",
    "exercises",
    "course",
    "find",
    "book",
    "come",
    "create",
    "another",
    "heading",
    "exercises",
    "extracurricular",
    "write",
    "see",
    "exercises",
    "extracurricular",
    "like",
    "template",
    "exercise",
    "code",
    "go",
    "pytorch",
    "deep",
    "learning",
    "repo",
    "within",
    "extras",
    "folder",
    "exercises",
    "solutions",
    "might",
    "able",
    "guess",
    "exercises",
    "o2",
    "pytorch",
    "classification",
    "exercises",
    "going",
    "skeleton",
    "code",
    "course",
    "solutions",
    "well",
    "one",
    "form",
    "solutions",
    "going",
    "look",
    "would",
    "recommend",
    "looking",
    "exercises",
    "first",
    "go",
    "solutions",
    "things",
    "like",
    "import",
    "torch",
    "set",
    "device",
    "agnostic",
    "code",
    "create",
    "data",
    "set",
    "turn",
    "data",
    "data",
    "frame",
    "et",
    "cetera",
    "et",
    "cetera",
    "things",
    "done",
    "throughout",
    "section",
    "give",
    "go",
    "try",
    "get",
    "stuck",
    "refer",
    "notebook",
    "coded",
    "together",
    "code",
    "refer",
    "documentation",
    "course",
    "refer",
    "last",
    "resort",
    "solutions",
    "notebooks",
    "give",
    "shot",
    "congratulations",
    "finishing",
    "section",
    "02",
    "pytorch",
    "classification",
    "still",
    "still",
    "let",
    "move",
    "next",
    "section",
    "going",
    "cover",
    "things",
    "deep",
    "learning",
    "pytorch",
    "see",
    "hello",
    "welcome",
    "back",
    "got",
    "another",
    "section",
    "got",
    "computer",
    "vision",
    "convolutional",
    "neural",
    "networks",
    "pytorch",
    "computer",
    "vision",
    "one",
    "favorite",
    "favorite",
    "deep",
    "learning",
    "topics",
    "get",
    "materials",
    "let",
    "answer",
    "important",
    "question",
    "get",
    "help",
    "first",
    "foremost",
    "follow",
    "along",
    "code",
    "best",
    "going",
    "writing",
    "whole",
    "bunch",
    "pytorch",
    "computer",
    "vision",
    "code",
    "remember",
    "motto",
    "run",
    "code",
    "see",
    "inputs",
    "outputs",
    "code",
    "try",
    "need",
    "doc",
    "string",
    "read",
    "function",
    "using",
    "press",
    "shift",
    "command",
    "space",
    "google",
    "colab",
    "might",
    "control",
    "windows",
    "otherwise",
    "still",
    "stuck",
    "search",
    "code",
    "running",
    "might",
    "come",
    "across",
    "stack",
    "overflow",
    "pytorch",
    "documentation",
    "spent",
    "bunch",
    "time",
    "pytorch",
    "documentation",
    "already",
    "going",
    "referencing",
    "whole",
    "bunch",
    "next",
    "module",
    "section",
    "three",
    "go",
    "four",
    "steps",
    "next",
    "step",
    "try",
    "run",
    "code",
    "course",
    "still",
    "stuck",
    "ask",
    "question",
    "pytorch",
    "deep",
    "learning",
    "repo",
    "discussions",
    "tab",
    "open",
    "go",
    "new",
    "discussion",
    "write",
    "section",
    "03",
    "computer",
    "vision",
    "problem",
    "write",
    "code",
    "sure",
    "format",
    "best",
    "way",
    "help",
    "us",
    "answer",
    "go",
    "happening",
    "format",
    "code",
    "back",
    "ticks",
    "looks",
    "like",
    "code",
    "easier",
    "read",
    "formatted",
    "github",
    "discussion",
    "select",
    "category",
    "general",
    "chat",
    "idea",
    "poll",
    "q",
    "show",
    "tell",
    "something",
    "made",
    "learned",
    "course",
    "question",
    "answering",
    "want",
    "put",
    "q",
    "click",
    "start",
    "discussion",
    "appear",
    "way",
    "searchable",
    "able",
    "help",
    "going",
    "get",
    "oh",
    "speaking",
    "resources",
    "got",
    "pytorch",
    "deep",
    "learning",
    "repo",
    "links",
    "need",
    "links",
    "code",
    "going",
    "write",
    "section",
    "contained",
    "within",
    "section",
    "3",
    "notebook",
    "pytorch",
    "computer",
    "vision",
    "beautiful",
    "notebook",
    "annotated",
    "heaps",
    "text",
    "images",
    "go",
    "time",
    "use",
    "reference",
    "help",
    "get",
    "stuck",
    "code",
    "write",
    "videos",
    "check",
    "notebook",
    "probably",
    "somewhere",
    "finally",
    "let",
    "get",
    "come",
    "book",
    "version",
    "course",
    "got",
    "home",
    "probably",
    "updated",
    "time",
    "look",
    "section",
    "03",
    "pytorch",
    "computer",
    "vision",
    "got",
    "information",
    "going",
    "cover",
    "book",
    "format",
    "course",
    "skip",
    "ahead",
    "different",
    "subtitles",
    "see",
    "going",
    "write",
    "links",
    "need",
    "extra",
    "resources",
    "section",
    "pytorch",
    "computer",
    "vision",
    "said",
    "speaking",
    "computer",
    "vision",
    "might",
    "question",
    "computer",
    "vision",
    "problem",
    "well",
    "see",
    "could",
    "probably",
    "phrased",
    "sort",
    "computer",
    "vision",
    "problem",
    "broad",
    "computer",
    "vision",
    "let",
    "concrete",
    "examples",
    "might",
    "binary",
    "classification",
    "problem",
    "wanted",
    "two",
    "different",
    "images",
    "photo",
    "steak",
    "pizza",
    "might",
    "build",
    "model",
    "understands",
    "steak",
    "looks",
    "like",
    "image",
    "beautiful",
    "dish",
    "cooked",
    "way",
    "eating",
    "pizza",
    "cafe",
    "dad",
    "could",
    "binary",
    "classification",
    "one",
    "thing",
    "another",
    "machine",
    "learning",
    "model",
    "may",
    "take",
    "pixels",
    "image",
    "understand",
    "different",
    "patterns",
    "go",
    "steak",
    "looks",
    "like",
    "thing",
    "pizza",
    "important",
    "thing",
    "note",
    "wo",
    "actually",
    "telling",
    "model",
    "learn",
    "learn",
    "patterns",
    "different",
    "examples",
    "images",
    "could",
    "step",
    "things",
    "classification",
    "problem",
    "noticing",
    "trend",
    "covered",
    "classification",
    "classification",
    "quite",
    "broad",
    "across",
    "different",
    "domains",
    "vision",
    "text",
    "audio",
    "working",
    "classification",
    "image",
    "problem",
    "might",
    "photo",
    "sushi",
    "steak",
    "pizza",
    "three",
    "classes",
    "instead",
    "two",
    "could",
    "100",
    "classes",
    "nutrify",
    "uses",
    "app",
    "working",
    "go",
    "bare",
    "bones",
    "moment",
    "right",
    "nutrify",
    "classify",
    "100",
    "different",
    "foods",
    "upload",
    "image",
    "food",
    "let",
    "give",
    "try",
    "nutrify",
    "go",
    "images",
    "go",
    "sample",
    "food",
    "images",
    "chicken",
    "wings",
    "classify",
    "chicken",
    "wings",
    "beautiful",
    "upload",
    "image",
    "food",
    "maybe",
    "let",
    "go",
    "nutrify",
    "computer",
    "way",
    "might",
    "sample",
    "folder",
    "set",
    "like",
    "upload",
    "photo",
    "cybertruck",
    "say",
    "food",
    "found",
    "please",
    "try",
    "another",
    "image",
    "behind",
    "scenes",
    "nutrify",
    "using",
    "pixels",
    "image",
    "running",
    "machine",
    "learning",
    "model",
    "classifying",
    "first",
    "whether",
    "food",
    "food",
    "food",
    "classifying",
    "food",
    "right",
    "works",
    "100",
    "different",
    "foods",
    "look",
    "classify",
    "apples",
    "artichokes",
    "avocados",
    "barbecue",
    "sauce",
    "work",
    "different",
    "levels",
    "performance",
    "something",
    "keep",
    "mind",
    "whole",
    "premise",
    "nutrify",
    "upload",
    "photo",
    "food",
    "learn",
    "nutrition",
    "let",
    "go",
    "back",
    "keynote",
    "another",
    "example",
    "well",
    "could",
    "use",
    "computer",
    "vision",
    "object",
    "detection",
    "might",
    "answer",
    "question",
    "thing",
    "looking",
    "example",
    "car",
    "caught",
    "security",
    "camera",
    "actually",
    "hit",
    "run",
    "new",
    "car",
    "much",
    "expensive",
    "car",
    "parked",
    "street",
    "person",
    "trailer",
    "came",
    "back",
    "car",
    "hit",
    "car",
    "picked",
    "trailer",
    "drove",
    "away",
    "went",
    "neighbor",
    "house",
    "look",
    "security",
    "footage",
    "found",
    "car",
    "potentially",
    "could",
    "design",
    "machine",
    "learning",
    "model",
    "find",
    "certain",
    "type",
    "car",
    "orange",
    "jute",
    "way",
    "images",
    "black",
    "white",
    "detect",
    "see",
    "ever",
    "recognizes",
    "similar",
    "car",
    "comes",
    "across",
    "street",
    "could",
    "go",
    "hey",
    "crash",
    "car",
    "day",
    "actually",
    "find",
    "sadly",
    "hit",
    "run",
    "object",
    "detection",
    "finding",
    "something",
    "image",
    "might",
    "want",
    "find",
    "whether",
    "different",
    "sections",
    "image",
    "great",
    "example",
    "apple",
    "uses",
    "devices",
    "iphones",
    "ipads",
    "whatnot",
    "segregate",
    "segment",
    "different",
    "sections",
    "image",
    "person",
    "one",
    "person",
    "two",
    "skin",
    "tones",
    "hair",
    "sky",
    "original",
    "enhances",
    "sections",
    "different",
    "ways",
    "practice",
    "known",
    "computational",
    "photography",
    "whole",
    "premise",
    "segment",
    "different",
    "portions",
    "image",
    "great",
    "blog",
    "post",
    "talks",
    "works",
    "kind",
    "model",
    "used",
    "leave",
    "extra",
    "curriculum",
    "like",
    "look",
    "images",
    "enhance",
    "sky",
    "make",
    "skin",
    "tones",
    "look",
    "remove",
    "background",
    "really",
    "wanted",
    "happens",
    "device",
    "got",
    "image",
    "way",
    "semantic",
    "mars",
    "another",
    "great",
    "blog",
    "apple",
    "machine",
    "learning",
    "research",
    "keep",
    "mind",
    "see",
    "another",
    "example",
    "computer",
    "vision",
    "tesla",
    "computer",
    "vision",
    "lot",
    "companies",
    "websites",
    "apple",
    "machine",
    "learning",
    "research",
    "share",
    "whole",
    "bunch",
    "world",
    "machine",
    "learning",
    "tesla",
    "case",
    "eight",
    "cameras",
    "cars",
    "fuels",
    "full",
    "beta",
    "software",
    "use",
    "computer",
    "vision",
    "understand",
    "going",
    "image",
    "plan",
    "going",
    "vector",
    "space",
    "means",
    "basically",
    "taking",
    "different",
    "viewpoints",
    "eight",
    "different",
    "cameras",
    "feeding",
    "form",
    "neural",
    "network",
    "turning",
    "representation",
    "environment",
    "around",
    "car",
    "vector",
    "long",
    "string",
    "numbers",
    "well",
    "computers",
    "understand",
    "numbers",
    "far",
    "understand",
    "images",
    "might",
    "able",
    "recognize",
    "happening",
    "computer",
    "understand",
    "turn",
    "vector",
    "space",
    "want",
    "look",
    "tesla",
    "uses",
    "computer",
    "vision",
    "tesla",
    "ai",
    "day",
    "video",
    "going",
    "play",
    "three",
    "hours",
    "long",
    "watched",
    "really",
    "enjoyed",
    "information",
    "little",
    "tidbit",
    "go",
    "two",
    "hours",
    "one",
    "minute",
    "31",
    "seconds",
    "video",
    "look",
    "tesla",
    "well",
    "would",
    "look",
    "seen",
    "code",
    "tesla",
    "custom",
    "dojo",
    "chip",
    "tesla",
    "uses",
    "pytorch",
    "exact",
    "code",
    "writing",
    "tesla",
    "uses",
    "similar",
    "pytorch",
    "code",
    "course",
    "write",
    "pytorch",
    "code",
    "suit",
    "problem",
    "nonetheless",
    "use",
    "pytorch",
    "code",
    "train",
    "machine",
    "learning",
    "models",
    "power",
    "software",
    "cool",
    "want",
    "look",
    "another",
    "example",
    "plenty",
    "different",
    "tesla",
    "videos",
    "oh",
    "play",
    "right",
    "going",
    "click",
    "link",
    "look",
    "happens",
    "look",
    "environment",
    "tesla",
    "cameras",
    "understand",
    "going",
    "computes",
    "little",
    "graphic",
    "display",
    "car",
    "kind",
    "understands",
    "well",
    "getting",
    "pretty",
    "close",
    "car",
    "getting",
    "pretty",
    "close",
    "car",
    "uses",
    "information",
    "happening",
    "perception",
    "plan",
    "drive",
    "next",
    "believe",
    "ends",
    "going",
    "stop",
    "yeah",
    "go",
    "got",
    "stop",
    "sign",
    "look",
    "perceiving",
    "stop",
    "sign",
    "got",
    "two",
    "people",
    "saw",
    "car",
    "drive",
    "pass",
    "across",
    "street",
    "pretty",
    "darn",
    "cool",
    "one",
    "example",
    "computer",
    "vision",
    "one",
    "many",
    "would",
    "find",
    "computer",
    "vision",
    "used",
    "computer",
    "vision",
    "used",
    "plenty",
    "resources",
    "oh",
    "go",
    "27",
    "popular",
    "computer",
    "vision",
    "applications",
    "covered",
    "fair",
    "bit",
    "going",
    "cover",
    "specifically",
    "pytorch",
    "code",
    "well",
    "broadly",
    "like",
    "going",
    "get",
    "vision",
    "data",
    "set",
    "work",
    "using",
    "torch",
    "vision",
    "pytorch",
    "lot",
    "different",
    "domain",
    "libraries",
    "torch",
    "vision",
    "helps",
    "us",
    "deal",
    "computer",
    "vision",
    "problems",
    "existing",
    "data",
    "sets",
    "leverage",
    "play",
    "around",
    "computer",
    "vision",
    "going",
    "look",
    "architecture",
    "convolutional",
    "neural",
    "network",
    "also",
    "known",
    "cnn",
    "pytorch",
    "going",
    "look",
    "image",
    "classification",
    "problem",
    "one",
    "thing",
    "another",
    "could",
    "three",
    "classes",
    "could",
    "hundred",
    "going",
    "look",
    "steps",
    "modeling",
    "cnns",
    "pytorch",
    "going",
    "create",
    "convolutional",
    "network",
    "pytorch",
    "going",
    "pick",
    "last",
    "function",
    "optimize",
    "suit",
    "problem",
    "going",
    "train",
    "model",
    "training",
    "model",
    "model",
    "little",
    "bit",
    "typo",
    "going",
    "evaluate",
    "model",
    "right",
    "might",
    "typos",
    "text",
    "less",
    "typos",
    "code",
    "going",
    "well",
    "could",
    "cook",
    "could",
    "chemis",
    "well",
    "going",
    "little",
    "bit",
    "part",
    "art",
    "part",
    "science",
    "since",
    "machine",
    "learning",
    "cooking",
    "show",
    "going",
    "cooking",
    "lots",
    "code",
    "next",
    "video",
    "going",
    "cover",
    "inputs",
    "outputs",
    "computer",
    "vision",
    "problem",
    "see",
    "last",
    "video",
    "covered",
    "going",
    "cover",
    "broadly",
    "saw",
    "examples",
    "computer",
    "vision",
    "problems",
    "essentially",
    "anything",
    "able",
    "see",
    "potentially",
    "turn",
    "computer",
    "vision",
    "problem",
    "going",
    "cooking",
    "lots",
    "machine",
    "learning",
    "specifically",
    "pie",
    "torch",
    "computer",
    "vision",
    "code",
    "see",
    "fixed",
    "typo",
    "let",
    "talk",
    "inputs",
    "outputs",
    "typical",
    "computer",
    "vision",
    "problem",
    "let",
    "start",
    "example",
    "wanted",
    "take",
    "photos",
    "different",
    "images",
    "food",
    "recognize",
    "replicating",
    "functionality",
    "nutrify",
    "take",
    "photo",
    "food",
    "learn",
    "might",
    "start",
    "bunch",
    "food",
    "images",
    "height",
    "width",
    "sort",
    "width",
    "equals",
    "224",
    "height",
    "equals",
    "224",
    "three",
    "color",
    "channels",
    "three",
    "well",
    "value",
    "red",
    "green",
    "blue",
    "look",
    "go",
    "red",
    "green",
    "blue",
    "image",
    "format",
    "rgb",
    "images",
    "lot",
    "images",
    "digital",
    "images",
    "value",
    "red",
    "pixel",
    "green",
    "pixel",
    "blue",
    "pixel",
    "convert",
    "images",
    "numbers",
    "get",
    "represented",
    "value",
    "red",
    "value",
    "green",
    "value",
    "blue",
    "exactly",
    "represent",
    "images",
    "example",
    "pixel",
    "might",
    "little",
    "bit",
    "red",
    "little",
    "less",
    "blue",
    "little",
    "less",
    "green",
    "close",
    "orange",
    "convert",
    "numbers",
    "trying",
    "essentially",
    "trying",
    "data",
    "machine",
    "learning",
    "represented",
    "numbers",
    "typical",
    "image",
    "format",
    "represent",
    "image",
    "using",
    "computer",
    "vision",
    "trying",
    "figure",
    "image",
    "typical",
    "way",
    "represent",
    "tensor",
    "value",
    "height",
    "width",
    "color",
    "channels",
    "might",
    "numerically",
    "encode",
    "words",
    "represent",
    "images",
    "tensor",
    "would",
    "inputs",
    "machine",
    "learning",
    "algorithm",
    "many",
    "cases",
    "depending",
    "problem",
    "working",
    "existing",
    "algorithm",
    "already",
    "exists",
    "many",
    "popular",
    "computer",
    "vision",
    "problems",
    "build",
    "one",
    "might",
    "fashion",
    "machine",
    "learning",
    "algorithm",
    "output",
    "exact",
    "shapes",
    "want",
    "case",
    "want",
    "three",
    "outputs",
    "want",
    "one",
    "output",
    "class",
    "want",
    "prediction",
    "probability",
    "sushi",
    "want",
    "prediction",
    "probability",
    "steak",
    "want",
    "prediction",
    "probability",
    "pizza",
    "case",
    "iteration",
    "looks",
    "like",
    "model",
    "got",
    "one",
    "wrong",
    "highest",
    "value",
    "assigned",
    "wrong",
    "class",
    "second",
    "image",
    "assigned",
    "prediction",
    "probability",
    "sushi",
    "keep",
    "mind",
    "could",
    "change",
    "classes",
    "whatever",
    "particular",
    "problem",
    "simplifying",
    "making",
    "three",
    "could",
    "hundred",
    "could",
    "thousand",
    "could",
    "five",
    "depends",
    "working",
    "might",
    "use",
    "predicted",
    "outputs",
    "enhance",
    "app",
    "someone",
    "wants",
    "take",
    "photo",
    "plate",
    "sushi",
    "app",
    "might",
    "say",
    "hey",
    "photo",
    "sushi",
    "information",
    "sushi",
    "rolls",
    "steak",
    "pizza",
    "might",
    "always",
    "get",
    "right",
    "machine",
    "learning",
    "probabilistic",
    "would",
    "improve",
    "results",
    "well",
    "could",
    "show",
    "model",
    "images",
    "sushi",
    "steak",
    "pizza",
    "builds",
    "better",
    "internal",
    "representation",
    "said",
    "images",
    "looks",
    "images",
    "never",
    "seen",
    "images",
    "outside",
    "training",
    "data",
    "set",
    "able",
    "get",
    "better",
    "results",
    "keep",
    "mind",
    "whole",
    "process",
    "similar",
    "matter",
    "computer",
    "vision",
    "problem",
    "working",
    "need",
    "way",
    "numerically",
    "encode",
    "information",
    "need",
    "machine",
    "learning",
    "model",
    "capable",
    "fitting",
    "data",
    "way",
    "would",
    "like",
    "fit",
    "case",
    "classification",
    "might",
    "different",
    "type",
    "model",
    "working",
    "object",
    "detection",
    "different",
    "type",
    "model",
    "working",
    "segmentation",
    "need",
    "fashion",
    "outputs",
    "way",
    "best",
    "suit",
    "problem",
    "well",
    "let",
    "push",
    "forward",
    "oh",
    "way",
    "model",
    "often",
    "convolutional",
    "neural",
    "network",
    "words",
    "cnn",
    "however",
    "use",
    "many",
    "different",
    "types",
    "machine",
    "learning",
    "algorithms",
    "convolutional",
    "neural",
    "networks",
    "typically",
    "perform",
    "best",
    "image",
    "data",
    "although",
    "recent",
    "research",
    "transformer",
    "architecture",
    "deep",
    "learning",
    "model",
    "also",
    "performs",
    "fairly",
    "well",
    "well",
    "image",
    "data",
    "keep",
    "mind",
    "going",
    "forward",
    "going",
    "focus",
    "convolutional",
    "neural",
    "networks",
    "might",
    "input",
    "output",
    "shapes",
    "remember",
    "one",
    "chief",
    "machine",
    "learning",
    "problems",
    "making",
    "sure",
    "tensor",
    "shapes",
    "line",
    "input",
    "output",
    "shapes",
    "encoded",
    "image",
    "stake",
    "might",
    "dimensionality",
    "batch",
    "size",
    "height",
    "color",
    "channels",
    "ordering",
    "could",
    "improved",
    "usually",
    "height",
    "width",
    "alphabetical",
    "order",
    "color",
    "channels",
    "last",
    "might",
    "shape",
    "none",
    "two",
    "two",
    "four",
    "two",
    "four",
    "three",
    "come",
    "none",
    "could",
    "batch",
    "size",
    "none",
    "set",
    "batch",
    "size",
    "whatever",
    "want",
    "say",
    "example",
    "might",
    "height",
    "two",
    "four",
    "width",
    "two",
    "four",
    "three",
    "color",
    "channels",
    "height",
    "width",
    "also",
    "customizable",
    "might",
    "change",
    "512",
    "would",
    "mean",
    "numbers",
    "representing",
    "image",
    "sense",
    "would",
    "take",
    "computation",
    "figure",
    "patterns",
    "simply",
    "information",
    "encoded",
    "image",
    "two",
    "two",
    "four",
    "two",
    "four",
    "common",
    "starting",
    "point",
    "images",
    "32",
    "also",
    "common",
    "batch",
    "size",
    "seen",
    "previous",
    "videos",
    "could",
    "changed",
    "depending",
    "hardware",
    "using",
    "depending",
    "model",
    "using",
    "might",
    "batch",
    "size",
    "might",
    "batch",
    "size",
    "problem",
    "specific",
    "line",
    "vary",
    "depending",
    "problem",
    "working",
    "case",
    "output",
    "shape",
    "three",
    "three",
    "different",
    "classes",
    "hundred",
    "might",
    "output",
    "shape",
    "hundred",
    "thousand",
    "might",
    "output",
    "shape",
    "thousand",
    "premise",
    "whole",
    "pattern",
    "remains",
    "though",
    "numerically",
    "encode",
    "data",
    "feed",
    "model",
    "make",
    "sure",
    "output",
    "shape",
    "fits",
    "specific",
    "problem",
    "section",
    "computer",
    "vision",
    "pytorch",
    "going",
    "building",
    "cnns",
    "part",
    "actually",
    "going",
    "parts",
    "going",
    "focus",
    "building",
    "convolutional",
    "neural",
    "network",
    "try",
    "find",
    "patterns",
    "data",
    "always",
    "guaranteed",
    "finally",
    "let",
    "look",
    "one",
    "problem",
    "say",
    "grayscale",
    "images",
    "fashion",
    "items",
    "quite",
    "small",
    "images",
    "28",
    "exact",
    "pattern",
    "going",
    "happen",
    "numerically",
    "represent",
    "use",
    "inputs",
    "machine",
    "learning",
    "algorithm",
    "hopefully",
    "machine",
    "learning",
    "algorithm",
    "outputs",
    "right",
    "type",
    "clothing",
    "case",
    "got",
    "dot",
    "dot",
    "dot",
    "going",
    "working",
    "problem",
    "uses",
    "ten",
    "different",
    "types",
    "items",
    "clothing",
    "images",
    "grayscale",
    "much",
    "detail",
    "hopefully",
    "machine",
    "learning",
    "algorithm",
    "recognize",
    "going",
    "images",
    "might",
    "boot",
    "might",
    "shirt",
    "might",
    "pants",
    "might",
    "dress",
    "etc",
    "etc",
    "numerically",
    "encode",
    "images",
    "dimensionality",
    "batch",
    "size",
    "height",
    "color",
    "channels",
    "known",
    "nhwc",
    "number",
    "batches",
    "number",
    "images",
    "batch",
    "height",
    "c",
    "color",
    "channels",
    "color",
    "channels",
    "last",
    "showing",
    "two",
    "forms",
    "notice",
    "color",
    "channels",
    "one",
    "color",
    "channels",
    "first",
    "color",
    "channels",
    "height",
    "width",
    "well",
    "come",
    "across",
    "lot",
    "different",
    "representations",
    "data",
    "full",
    "stop",
    "particularly",
    "image",
    "data",
    "pytorch",
    "libraries",
    "many",
    "libraries",
    "expect",
    "color",
    "channels",
    "last",
    "however",
    "pytorch",
    "currently",
    "time",
    "recording",
    "video",
    "may",
    "change",
    "future",
    "defaults",
    "representing",
    "image",
    "data",
    "color",
    "channels",
    "first",
    "important",
    "get",
    "errors",
    "dimensionality",
    "wrong",
    "order",
    "ways",
    "go",
    "two",
    "lot",
    "debate",
    "format",
    "best",
    "looks",
    "like",
    "color",
    "channels",
    "last",
    "going",
    "win",
    "long",
    "term",
    "efficient",
    "outside",
    "scope",
    "keep",
    "mind",
    "going",
    "write",
    "code",
    "interact",
    "two",
    "data",
    "represented",
    "different",
    "order",
    "could",
    "rearrange",
    "shapes",
    "want",
    "color",
    "channels",
    "last",
    "color",
    "channels",
    "first",
    "shapes",
    "vary",
    "depending",
    "problem",
    "working",
    "said",
    "covered",
    "input",
    "output",
    "shapes",
    "going",
    "see",
    "code",
    "well",
    "course",
    "going",
    "following",
    "pytorch",
    "workflow",
    "done",
    "need",
    "get",
    "data",
    "ready",
    "turn",
    "tenses",
    "way",
    "shape",
    "form",
    "taught",
    "division",
    "transforms",
    "oh",
    "seen",
    "one",
    "yet",
    "use",
    "loader",
    "build",
    "model",
    "pick",
    "model",
    "suit",
    "problem",
    "got",
    "whole",
    "bunch",
    "modules",
    "help",
    "us",
    "torchnn",
    "module",
    "optimizer",
    "loss",
    "function",
    "evaluate",
    "model",
    "using",
    "torch",
    "metrics",
    "code",
    "metric",
    "functions",
    "course",
    "improve",
    "experimentation",
    "see",
    "later",
    "actually",
    "done",
    "right",
    "done",
    "improvement",
    "experimentation",
    "tried",
    "different",
    "models",
    "tried",
    "different",
    "things",
    "finally",
    "save",
    "reload",
    "trained",
    "model",
    "wanted",
    "use",
    "elsewhere",
    "said",
    "covered",
    "workflow",
    "overview",
    "going",
    "code",
    "might",
    "asking",
    "question",
    "convolutional",
    "neural",
    "network",
    "cnn",
    "let",
    "answer",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "saw",
    "examples",
    "computer",
    "vision",
    "input",
    "output",
    "shapes",
    "kind",
    "hinted",
    "fact",
    "convolutional",
    "neural",
    "networks",
    "deep",
    "learning",
    "models",
    "cnns",
    "quite",
    "good",
    "recognizing",
    "patterns",
    "images",
    "left",
    "last",
    "video",
    "question",
    "convolutional",
    "neural",
    "network",
    "could",
    "find",
    "convolutional",
    "neural",
    "network",
    "one",
    "way",
    "find",
    "sure",
    "seen",
    "lot",
    "resources",
    "things",
    "comprehensive",
    "guide",
    "convolutional",
    "neural",
    "networks",
    "one",
    "best",
    "well",
    "really",
    "matter",
    "best",
    "one",
    "one",
    "understand",
    "best",
    "go",
    "great",
    "video",
    "code",
    "basics",
    "seen",
    "one",
    "simple",
    "explanation",
    "convolutional",
    "neural",
    "network",
    "leave",
    "research",
    "things",
    "wanted",
    "look",
    "images",
    "whole",
    "bunch",
    "images",
    "prefer",
    "learn",
    "things",
    "writing",
    "code",
    "remember",
    "course",
    "code",
    "first",
    "machine",
    "learning",
    "engineer",
    "99",
    "time",
    "spent",
    "writing",
    "code",
    "going",
    "focus",
    "anyway",
    "typical",
    "architecture",
    "cnn",
    "words",
    "convolutional",
    "neural",
    "network",
    "hear",
    "say",
    "cnn",
    "talking",
    "news",
    "website",
    "course",
    "talking",
    "architecture",
    "convolutional",
    "neural",
    "network",
    "pytorch",
    "code",
    "going",
    "working",
    "towards",
    "building",
    "hyperparameters",
    "slash",
    "layer",
    "types",
    "input",
    "layer",
    "input",
    "layer",
    "takes",
    "channels",
    "input",
    "shape",
    "remember",
    "important",
    "machine",
    "learning",
    "deep",
    "learning",
    "line",
    "input",
    "output",
    "shapes",
    "whatever",
    "model",
    "using",
    "whatever",
    "problem",
    "working",
    "sort",
    "convolutional",
    "layer",
    "might",
    "happen",
    "convolutional",
    "layer",
    "well",
    "might",
    "guessed",
    "happens",
    "many",
    "neural",
    "networks",
    "layers",
    "perform",
    "sort",
    "mathematical",
    "operation",
    "convolutional",
    "layers",
    "perform",
    "convolving",
    "window",
    "operation",
    "across",
    "image",
    "across",
    "tensor",
    "discover",
    "patterns",
    "using",
    "let",
    "look",
    "actually",
    "let",
    "go",
    "go",
    "happens",
    "output",
    "network",
    "equals",
    "bias",
    "plus",
    "sum",
    "weight",
    "tensor",
    "convolutional",
    "channel",
    "okay",
    "times",
    "input",
    "want",
    "dig",
    "deeper",
    "actually",
    "going",
    "welcome",
    "going",
    "writing",
    "code",
    "leverages",
    "torch",
    "going",
    "fix",
    "hyperparameters",
    "works",
    "problem",
    "need",
    "know",
    "bias",
    "term",
    "seen",
    "weight",
    "matrix",
    "bias",
    "vector",
    "typically",
    "weight",
    "matrix",
    "operate",
    "input",
    "see",
    "later",
    "code",
    "keep",
    "mind",
    "happening",
    "every",
    "layer",
    "neural",
    "network",
    "form",
    "operation",
    "happening",
    "input",
    "data",
    "operations",
    "happen",
    "layer",
    "layer",
    "eventually",
    "hopefully",
    "turned",
    "usable",
    "output",
    "let",
    "jump",
    "back",
    "hidden",
    "activation",
    "slash",
    "nonlinear",
    "activation",
    "use",
    "nonlinear",
    "activations",
    "well",
    "data",
    "nonlinear",
    "lines",
    "need",
    "help",
    "straight",
    "lines",
    "model",
    "draw",
    "patterns",
    "typically",
    "pooling",
    "layer",
    "want",
    "take",
    "architecture",
    "said",
    "typical",
    "reason",
    "type",
    "architectures",
    "changing",
    "time",
    "one",
    "typical",
    "example",
    "cnn",
    "basic",
    "cnn",
    "get",
    "time",
    "start",
    "learn",
    "build",
    "complex",
    "models",
    "start",
    "learn",
    "build",
    "start",
    "learn",
    "use",
    "see",
    "later",
    "transfer",
    "learning",
    "section",
    "course",
    "output",
    "layer",
    "notice",
    "trend",
    "input",
    "layer",
    "multiple",
    "hidden",
    "layers",
    "perform",
    "sort",
    "mathematical",
    "operation",
    "data",
    "output",
    "slash",
    "linear",
    "layer",
    "converts",
    "output",
    "ideal",
    "shape",
    "like",
    "output",
    "shape",
    "look",
    "process",
    "put",
    "images",
    "go",
    "layers",
    "used",
    "end",
    "sequential",
    "hopefully",
    "forward",
    "method",
    "returns",
    "x",
    "usable",
    "status",
    "usable",
    "state",
    "convert",
    "class",
    "names",
    "could",
    "integrate",
    "computer",
    "vision",
    "app",
    "way",
    "shape",
    "form",
    "asterisk",
    "note",
    "almost",
    "unlimited",
    "amount",
    "ways",
    "could",
    "stack",
    "together",
    "convolutional",
    "neural",
    "network",
    "slide",
    "demonstrates",
    "one",
    "keep",
    "mind",
    "demonstrates",
    "one",
    "best",
    "way",
    "practice",
    "sort",
    "stuff",
    "stare",
    "page",
    "code",
    "let",
    "code",
    "see",
    "google",
    "colab",
    "welcome",
    "back",
    "discussed",
    "bunch",
    "fundamentals",
    "computer",
    "vision",
    "problems",
    "convolutional",
    "neural",
    "networks",
    "rather",
    "talk",
    "slides",
    "well",
    "let",
    "start",
    "code",
    "going",
    "meet",
    "going",
    "clean",
    "tabs",
    "going",
    "start",
    "new",
    "notebook",
    "going",
    "name",
    "one",
    "going",
    "03",
    "pytorch",
    "computer",
    "vision",
    "going",
    "call",
    "mine",
    "video",
    "video",
    "tag",
    "go",
    "go",
    "video",
    "notebooks",
    "pytorch",
    "deep",
    "learning",
    "repo",
    "video",
    "notebooks",
    "stored",
    "got",
    "underscore",
    "video",
    "tag",
    "video",
    "notebooks",
    "code",
    "write",
    "exactly",
    "video",
    "reference",
    "notebooks",
    "go",
    "along",
    "let",
    "write",
    "heading",
    "pytorch",
    "computer",
    "vision",
    "put",
    "resource",
    "see",
    "reference",
    "notebook",
    "course",
    "one",
    "ground",
    "truth",
    "got",
    "code",
    "going",
    "writing",
    "going",
    "put",
    "explain",
    "text",
    "images",
    "whatnot",
    "finally",
    "got",
    "see",
    "reference",
    "online",
    "book",
    "section",
    "number",
    "three",
    "pytorch",
    "computer",
    "vision",
    "going",
    "put",
    "going",
    "turn",
    "markdown",
    "command",
    "mm",
    "beautiful",
    "let",
    "get",
    "started",
    "going",
    "get",
    "rid",
    "get",
    "rid",
    "start",
    "well",
    "believe",
    "computer",
    "vision",
    "libraries",
    "aware",
    "computer",
    "vision",
    "libraries",
    "pytorch",
    "going",
    "text",
    "based",
    "cell",
    "first",
    "one",
    "torch",
    "vision",
    "base",
    "domain",
    "library",
    "pytorch",
    "computer",
    "vision",
    "look",
    "torch",
    "vision",
    "find",
    "torch",
    "vision",
    "version",
    "torch",
    "vision",
    "currently",
    "time",
    "recording",
    "important",
    "get",
    "familiar",
    "working",
    "computer",
    "vision",
    "problems",
    "course",
    "documentation",
    "another",
    "tidbit",
    "torch",
    "audio",
    "audio",
    "problems",
    "torch",
    "text",
    "text",
    "torch",
    "vision",
    "working",
    "torch",
    "rack",
    "recommendation",
    "systems",
    "torch",
    "data",
    "dealing",
    "different",
    "data",
    "pipelines",
    "torch",
    "serve",
    "serving",
    "pytorch",
    "models",
    "pytorch",
    "xla",
    "believe",
    "stands",
    "accelerated",
    "linear",
    "algebra",
    "devices",
    "worry",
    "ones",
    "focused",
    "torch",
    "vision",
    "however",
    "would",
    "like",
    "learn",
    "particular",
    "domain",
    "would",
    "go",
    "learn",
    "bunch",
    "different",
    "stuff",
    "going",
    "transforming",
    "augmenting",
    "images",
    "fundamentally",
    "computer",
    "vision",
    "dealing",
    "things",
    "form",
    "images",
    "even",
    "video",
    "gets",
    "converted",
    "image",
    "models",
    "weights",
    "referenced",
    "use",
    "existing",
    "model",
    "works",
    "existing",
    "computer",
    "vision",
    "problem",
    "problem",
    "going",
    "cover",
    "section",
    "think",
    "six",
    "transfer",
    "learning",
    "data",
    "sets",
    "bunch",
    "computer",
    "vision",
    "data",
    "sets",
    "utils",
    "operators",
    "whole",
    "bunch",
    "stuff",
    "pytorch",
    "really",
    "really",
    "good",
    "computer",
    "vision",
    "mean",
    "look",
    "stuff",
    "going",
    "enough",
    "talking",
    "let",
    "put",
    "torch",
    "vision",
    "main",
    "one",
    "going",
    "link",
    "links",
    "way",
    "book",
    "version",
    "course",
    "pytorch",
    "computer",
    "vision",
    "going",
    "cover",
    "finally",
    "computer",
    "vision",
    "libraries",
    "pytorch",
    "torch",
    "vision",
    "data",
    "sets",
    "models",
    "transforms",
    "et",
    "cetera",
    "let",
    "write",
    "ones",
    "torch",
    "vision",
    "data",
    "sets",
    "something",
    "aware",
    "get",
    "data",
    "sets",
    "data",
    "loading",
    "functions",
    "computer",
    "vision",
    "torch",
    "vision",
    "torch",
    "vision",
    "models",
    "get",
    "computer",
    "vision",
    "say",
    "computer",
    "vision",
    "models",
    "going",
    "cover",
    "transfer",
    "learning",
    "said",
    "computer",
    "vision",
    "models",
    "models",
    "already",
    "trained",
    "existing",
    "vision",
    "data",
    "trained",
    "weights",
    "trained",
    "patterns",
    "leverage",
    "problems",
    "leverage",
    "problems",
    "torch",
    "functions",
    "manipulating",
    "vision",
    "data",
    "course",
    "images",
    "suitable",
    "use",
    "ml",
    "model",
    "remember",
    "image",
    "data",
    "almost",
    "kind",
    "data",
    "machine",
    "learning",
    "prepare",
    "way",
    "ca",
    "pure",
    "images",
    "transforms",
    "help",
    "us",
    "transforms",
    "helps",
    "turn",
    "image",
    "data",
    "numbers",
    "use",
    "machine",
    "learning",
    "model",
    "course",
    "torch",
    "utils",
    "vision",
    "specific",
    "entirety",
    "pytorch",
    "specific",
    "data",
    "set",
    "wanted",
    "create",
    "data",
    "set",
    "custom",
    "data",
    "base",
    "data",
    "set",
    "class",
    "pytorch",
    "finally",
    "torch",
    "utils",
    "data",
    "good",
    "aware",
    "almost",
    "always",
    "use",
    "form",
    "data",
    "set",
    "slash",
    "data",
    "loader",
    "whatever",
    "pytorch",
    "problem",
    "working",
    "creates",
    "python",
    "iterable",
    "data",
    "set",
    "wonderful",
    "think",
    "libraries",
    "going",
    "using",
    "section",
    "let",
    "import",
    "hey",
    "see",
    "going",
    "let",
    "go",
    "import",
    "pytorch",
    "import",
    "pytorch",
    "import",
    "torch",
    "also",
    "going",
    "get",
    "nn",
    "stands",
    "neural",
    "network",
    "nn",
    "well",
    "nn",
    "course",
    "lots",
    "layers",
    "lots",
    "loss",
    "functions",
    "whole",
    "bunch",
    "different",
    "stuff",
    "building",
    "neural",
    "networks",
    "going",
    "also",
    "import",
    "torch",
    "vision",
    "going",
    "go",
    "torch",
    "vision",
    "import",
    "data",
    "sets",
    "going",
    "using",
    "data",
    "sets",
    "later",
    "get",
    "data",
    "set",
    "work",
    "torch",
    "vision",
    "well",
    "import",
    "transforms",
    "could",
    "also",
    "go",
    "torch",
    "vision",
    "dot",
    "transforms",
    "import",
    "tensor",
    "one",
    "main",
    "ones",
    "see",
    "computer",
    "vision",
    "problems",
    "tensor",
    "imagine",
    "let",
    "look",
    "transforms",
    "tensor",
    "transforming",
    "augmenting",
    "images",
    "look",
    "slash",
    "vision",
    "slash",
    "stable",
    "slash",
    "transforms",
    "torch",
    "vision",
    "section",
    "looking",
    "transforming",
    "augmenting",
    "images",
    "transforming",
    "transforms",
    "common",
    "image",
    "transformations",
    "transforms",
    "module",
    "trained",
    "together",
    "using",
    "compose",
    "beautiful",
    "two",
    "tensor",
    "convert",
    "pill",
    "image",
    "numpy",
    "array",
    "tensor",
    "beautiful",
    "want",
    "later",
    "well",
    "kind",
    "giving",
    "spoiler",
    "want",
    "convert",
    "images",
    "tensors",
    "use",
    "models",
    "whole",
    "bunch",
    "different",
    "transforms",
    "actually",
    "one",
    "extra",
    "curriculum",
    "read",
    "packages",
    "10",
    "minutes",
    "hour",
    "reading",
    "definitely",
    "help",
    "later",
    "get",
    "familiar",
    "using",
    "pytorch",
    "documentation",
    "course",
    "momentum",
    "builder",
    "going",
    "write",
    "heaves",
    "pytorch",
    "code",
    "fundamentally",
    "teaching",
    "lot",
    "stuff",
    "reading",
    "documentation",
    "let",
    "keep",
    "going",
    "getting",
    "familiar",
    "data",
    "mapplotlib",
    "going",
    "fundamental",
    "visualization",
    "remember",
    "data",
    "explorer",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "become",
    "one",
    "data",
    "going",
    "import",
    "plt",
    "finally",
    "let",
    "check",
    "versions",
    "print",
    "underscore",
    "underscore",
    "version",
    "print",
    "torch",
    "vision",
    "time",
    "watch",
    "might",
    "newer",
    "version",
    "modules",
    "errors",
    "code",
    "please",
    "let",
    "know",
    "bare",
    "minimum",
    "version",
    "need",
    "complete",
    "section",
    "believe",
    "moment",
    "google",
    "colab",
    "running",
    "torch",
    "maybe",
    "find",
    "second",
    "connected",
    "importing",
    "pytorch",
    "okay",
    "go",
    "pytorch",
    "version",
    "got",
    "cuda",
    "available",
    "torch",
    "vision",
    "make",
    "sure",
    "running",
    "google",
    "colab",
    "running",
    "later",
    "date",
    "probably",
    "minimum",
    "versions",
    "might",
    "even",
    "later",
    "version",
    "minimum",
    "versions",
    "required",
    "upcoming",
    "section",
    "covered",
    "base",
    "computer",
    "vision",
    "libraries",
    "pytorch",
    "got",
    "ready",
    "go",
    "next",
    "video",
    "cover",
    "getting",
    "data",
    "set",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "fundamental",
    "computer",
    "vision",
    "libraries",
    "pytorch",
    "main",
    "one",
    "torch",
    "vision",
    "modules",
    "stem",
    "torch",
    "vision",
    "course",
    "got",
    "torch",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "set",
    "base",
    "data",
    "set",
    "class",
    "pytorch",
    "data",
    "loader",
    "creates",
    "python",
    "irritable",
    "data",
    "set",
    "let",
    "begin",
    "machine",
    "learning",
    "projects",
    "getting",
    "data",
    "set",
    "getting",
    "data",
    "set",
    "going",
    "turn",
    "markdown",
    "data",
    "set",
    "going",
    "used",
    "demonstrating",
    "computer",
    "vision",
    "techniques",
    "fashion",
    "amnest",
    "take",
    "data",
    "set",
    "using",
    "fashion",
    "amnest",
    "take",
    "original",
    "amnest",
    "data",
    "set",
    "amnest",
    "database",
    "modified",
    "national",
    "institute",
    "standards",
    "technology",
    "database",
    "kind",
    "like",
    "hello",
    "world",
    "machine",
    "learning",
    "computer",
    "vision",
    "sample",
    "images",
    "amnest",
    "test",
    "data",
    "set",
    "grayscale",
    "images",
    "handwritten",
    "digits",
    "believe",
    "originally",
    "used",
    "trying",
    "find",
    "could",
    "use",
    "computer",
    "vision",
    "postal",
    "service",
    "guess",
    "recognize",
    "post",
    "codes",
    "whatnot",
    "may",
    "wrong",
    "know",
    "yeah",
    "way",
    "back",
    "1998",
    "cool",
    "basically",
    "convolutional",
    "neural",
    "networks",
    "founded",
    "let",
    "read",
    "history",
    "neural",
    "network",
    "started",
    "get",
    "good",
    "data",
    "set",
    "quite",
    "easy",
    "really",
    "well",
    "fashion",
    "amnest",
    "came",
    "little",
    "bit",
    "harder",
    "go",
    "zalando",
    "research",
    "fashion",
    "amnest",
    "grayscale",
    "images",
    "pieces",
    "clothing",
    "like",
    "saw",
    "input",
    "output",
    "going",
    "trying",
    "turning",
    "images",
    "clothing",
    "numbers",
    "training",
    "computer",
    "vision",
    "model",
    "recognize",
    "different",
    "styles",
    "clothing",
    "dimensionality",
    "plot",
    "different",
    "items",
    "clothing",
    "visualizing",
    "similar",
    "items",
    "grouped",
    "together",
    "shoes",
    "whatnot",
    "interactive",
    "oh",
    "video",
    "excuse",
    "go",
    "serious",
    "machine",
    "learning",
    "researchers",
    "talking",
    "replacing",
    "amnest",
    "amnest",
    "easy",
    "amnest",
    "overused",
    "amnest",
    "represent",
    "modern",
    "cv",
    "tasks",
    "even",
    "fashion",
    "amnest",
    "would",
    "say",
    "also",
    "pretty",
    "much",
    "sold",
    "good",
    "way",
    "get",
    "started",
    "could",
    "find",
    "data",
    "set",
    "could",
    "download",
    "github",
    "come",
    "back",
    "taught",
    "division",
    "documentation",
    "look",
    "data",
    "sets",
    "whole",
    "bunch",
    "data",
    "sets",
    "remember",
    "extra",
    "curricular",
    "read",
    "10",
    "minutes",
    "example",
    "could",
    "download",
    "imagenet",
    "want",
    "also",
    "base",
    "classes",
    "custom",
    "data",
    "sets",
    "see",
    "later",
    "scroll",
    "image",
    "classification",
    "data",
    "sets",
    "caltech",
    "even",
    "know",
    "lot",
    "cfar",
    "example",
    "100",
    "different",
    "items",
    "would",
    "100",
    "class",
    "classification",
    "problem",
    "cfar",
    "10",
    "10",
    "classes",
    "amnest",
    "fashion",
    "amnest",
    "oh",
    "one",
    "basically",
    "would",
    "download",
    "data",
    "set",
    "would",
    "download",
    "data",
    "way",
    "shape",
    "form",
    "would",
    "turn",
    "data",
    "loader",
    "imagenet",
    "one",
    "popular",
    "probably",
    "gold",
    "standard",
    "data",
    "set",
    "computer",
    "vision",
    "evaluation",
    "quite",
    "big",
    "data",
    "set",
    "got",
    "millions",
    "images",
    "beauty",
    "taught",
    "vision",
    "allows",
    "us",
    "download",
    "example",
    "data",
    "sets",
    "practice",
    "even",
    "perform",
    "research",
    "module",
    "let",
    "look",
    "fashion",
    "amnest",
    "data",
    "set",
    "might",
    "get",
    "got",
    "example",
    "code",
    "documentation",
    "amnest",
    "pass",
    "root",
    "want",
    "download",
    "data",
    "set",
    "also",
    "pass",
    "whether",
    "want",
    "training",
    "version",
    "data",
    "set",
    "whether",
    "want",
    "testing",
    "version",
    "data",
    "set",
    "want",
    "download",
    "yes",
    "transform",
    "data",
    "way",
    "shape",
    "form",
    "going",
    "downloading",
    "images",
    "function",
    "call",
    "class",
    "call",
    "want",
    "transform",
    "images",
    "way",
    "images",
    "use",
    "model",
    "turn",
    "tensor",
    "might",
    "look",
    "moment",
    "target",
    "transform",
    "want",
    "transform",
    "labels",
    "way",
    "shape",
    "form",
    "often",
    "data",
    "sets",
    "download",
    "pre",
    "formatted",
    "way",
    "quite",
    "easily",
    "used",
    "pytorch",
    "wo",
    "always",
    "case",
    "custom",
    "data",
    "sets",
    "however",
    "cover",
    "important",
    "get",
    "idea",
    "computer",
    "vision",
    "workflow",
    "later",
    "start",
    "customize",
    "get",
    "data",
    "right",
    "format",
    "used",
    "model",
    "different",
    "parameters",
    "whatnot",
    "let",
    "rather",
    "look",
    "documentation",
    "code",
    "using",
    "fashion",
    "mnist",
    "start",
    "going",
    "put",
    "put",
    "link",
    "start",
    "getting",
    "training",
    "data",
    "set",
    "training",
    "data",
    "going",
    "make",
    "code",
    "cells",
    "code",
    "middle",
    "screen",
    "set",
    "training",
    "data",
    "training",
    "data",
    "equals",
    "data",
    "sets",
    "dot",
    "fashion",
    "mnist",
    "recall",
    "already",
    "taughtvision",
    "need",
    "import",
    "demonstration",
    "purposes",
    "taughtvision",
    "import",
    "data",
    "sets",
    "call",
    "data",
    "sets",
    "dot",
    "fashion",
    "mnist",
    "going",
    "type",
    "root",
    "see",
    "doc",
    "string",
    "comes",
    "tells",
    "us",
    "going",
    "personally",
    "find",
    "bit",
    "hard",
    "read",
    "google",
    "colab",
    "looking",
    "documentation",
    "like",
    "go",
    "let",
    "code",
    "root",
    "going",
    "data",
    "download",
    "data",
    "see",
    "minute",
    "going",
    "go",
    "train",
    "want",
    "training",
    "version",
    "data",
    "set",
    "said",
    "lot",
    "data",
    "sets",
    "find",
    "formatted",
    "training",
    "data",
    "set",
    "testing",
    "data",
    "set",
    "already",
    "boolean",
    "tells",
    "us",
    "want",
    "training",
    "data",
    "set",
    "false",
    "would",
    "get",
    "testing",
    "data",
    "set",
    "fashion",
    "mnist",
    "want",
    "download",
    "want",
    "download",
    "yes",
    "yes",
    "going",
    "set",
    "true",
    "sort",
    "transform",
    "want",
    "going",
    "downloading",
    "images",
    "images",
    "use",
    "model",
    "convert",
    "tensors",
    "going",
    "pass",
    "transform",
    "tensor",
    "could",
    "also",
    "go",
    "tensor",
    "would",
    "exact",
    "thing",
    "target",
    "transform",
    "want",
    "transform",
    "labels",
    "going",
    "see",
    "come",
    "target",
    "sorry",
    "high",
    "torch",
    "another",
    "way",
    "another",
    "naming",
    "convention",
    "often",
    "uses",
    "target",
    "target",
    "trying",
    "predict",
    "using",
    "data",
    "predict",
    "target",
    "often",
    "use",
    "data",
    "predict",
    "label",
    "thing",
    "want",
    "transform",
    "data",
    "want",
    "transform",
    "labels",
    "going",
    "test",
    "data",
    "going",
    "go",
    "data",
    "sets",
    "might",
    "know",
    "going",
    "exact",
    "code",
    "except",
    "going",
    "change",
    "one",
    "line",
    "want",
    "store",
    "data",
    "want",
    "download",
    "training",
    "data",
    "set",
    "false",
    "want",
    "testing",
    "version",
    "want",
    "download",
    "yes",
    "want",
    "transform",
    "data",
    "yes",
    "want",
    "use",
    "tensor",
    "convert",
    "image",
    "data",
    "tensors",
    "want",
    "target",
    "transform",
    "well",
    "want",
    "keep",
    "label",
    "slash",
    "targets",
    "let",
    "see",
    "happens",
    "run",
    "oh",
    "downloading",
    "fashion",
    "evan",
    "beautiful",
    "going",
    "download",
    "labels",
    "train",
    "images",
    "train",
    "labels",
    "lovely",
    "test",
    "images",
    "test",
    "labels",
    "beautiful",
    "quickly",
    "get",
    "data",
    "set",
    "using",
    "torch",
    "vision",
    "data",
    "sets",
    "look",
    "data",
    "folder",
    "set",
    "root",
    "data",
    "look",
    "inside",
    "fashion",
    "mnist",
    "exactly",
    "wanted",
    "raw",
    "whole",
    "bunch",
    "files",
    "torch",
    "vision",
    "converted",
    "data",
    "sets",
    "us",
    "let",
    "get",
    "process",
    "would",
    "much",
    "used",
    "almost",
    "data",
    "set",
    "might",
    "slightly",
    "different",
    "depending",
    "documentation",
    "says",
    "depending",
    "data",
    "set",
    "easy",
    "torch",
    "vision",
    "data",
    "sets",
    "makes",
    "practice",
    "example",
    "computer",
    "vision",
    "data",
    "sets",
    "let",
    "go",
    "back",
    "let",
    "check",
    "parameters",
    "attributes",
    "data",
    "many",
    "samples",
    "check",
    "lengths",
    "training",
    "examples",
    "testing",
    "examples",
    "going",
    "going",
    "building",
    "computer",
    "vision",
    "model",
    "find",
    "patterns",
    "training",
    "data",
    "use",
    "patterns",
    "predict",
    "test",
    "data",
    "let",
    "see",
    "first",
    "training",
    "example",
    "see",
    "first",
    "training",
    "example",
    "index",
    "train",
    "data",
    "let",
    "get",
    "zero",
    "index",
    "going",
    "look",
    "image",
    "label",
    "oh",
    "goodness",
    "whole",
    "bunch",
    "numbers",
    "see",
    "two",
    "tensor",
    "done",
    "us",
    "downloaded",
    "images",
    "thanks",
    "torch",
    "vision",
    "transforms",
    "tensor",
    "would",
    "find",
    "documentation",
    "well",
    "could",
    "go",
    "see",
    "transforms",
    "tensor",
    "could",
    "go",
    "tensor",
    "go",
    "convert",
    "pill",
    "image",
    "python",
    "image",
    "library",
    "image",
    "numpy",
    "array",
    "tensor",
    "transform",
    "support",
    "torch",
    "script",
    "converts",
    "pill",
    "image",
    "numpy",
    "array",
    "height",
    "color",
    "channels",
    "range",
    "0",
    "255",
    "torch",
    "float",
    "tensor",
    "shape",
    "see",
    "talking",
    "pytorch",
    "defaults",
    "lot",
    "transforms",
    "chw",
    "color",
    "channels",
    "first",
    "height",
    "width",
    "range",
    "zero",
    "one",
    "typically",
    "red",
    "green",
    "blue",
    "values",
    "zero",
    "neural",
    "networks",
    "like",
    "things",
    "zero",
    "one",
    "case",
    "shape",
    "color",
    "channels",
    "first",
    "height",
    "width",
    "however",
    "machine",
    "learning",
    "libraries",
    "prefer",
    "height",
    "width",
    "color",
    "channels",
    "keep",
    "mind",
    "going",
    "see",
    "practice",
    "later",
    "got",
    "image",
    "got",
    "label",
    "let",
    "check",
    "details",
    "remember",
    "discussed",
    "oh",
    "label",
    "way",
    "nine",
    "go",
    "find",
    "information",
    "class",
    "names",
    "class",
    "names",
    "beautiful",
    "number",
    "nine",
    "would",
    "0",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "particular",
    "tensor",
    "seems",
    "relate",
    "ankle",
    "boot",
    "would",
    "find",
    "well",
    "one",
    "second",
    "going",
    "show",
    "one",
    "thing",
    "class",
    "idx",
    "let",
    "go",
    "idx",
    "give",
    "us",
    "class",
    "idx",
    "going",
    "give",
    "us",
    "dictionary",
    "different",
    "labels",
    "corresponding",
    "index",
    "machine",
    "learning",
    "model",
    "predicted",
    "nine",
    "class",
    "nine",
    "convert",
    "ankle",
    "boot",
    "using",
    "attribute",
    "train",
    "data",
    "attributes",
    "look",
    "like",
    "go",
    "push",
    "tab",
    "find",
    "bunch",
    "different",
    "things",
    "go",
    "data",
    "images",
    "believe",
    "also",
    "go",
    "targets",
    "targets",
    "labels",
    "one",
    "big",
    "long",
    "tensor",
    "let",
    "check",
    "shape",
    "check",
    "shape",
    "image",
    "going",
    "get",
    "oh",
    "label",
    "shape",
    "well",
    "integer",
    "oh",
    "beautiful",
    "look",
    "image",
    "shape",
    "color",
    "channel",
    "one",
    "let",
    "print",
    "something",
    "prettier",
    "print",
    "image",
    "shape",
    "going",
    "image",
    "shape",
    "remember",
    "said",
    "important",
    "aware",
    "input",
    "output",
    "shapes",
    "models",
    "data",
    "part",
    "becoming",
    "one",
    "data",
    "image",
    "shape",
    "go",
    "next",
    "print",
    "image",
    "label",
    "label",
    "index",
    "class",
    "names",
    "label",
    "wonderful",
    "image",
    "shape",
    "currently",
    "format",
    "color",
    "channels",
    "height",
    "width",
    "got",
    "bunch",
    "different",
    "numbers",
    "representing",
    "image",
    "black",
    "white",
    "one",
    "color",
    "channel",
    "think",
    "one",
    "color",
    "channel",
    "black",
    "white",
    "jump",
    "back",
    "keynote",
    "fashion",
    "already",
    "discussed",
    "grayscale",
    "images",
    "one",
    "color",
    "channel",
    "means",
    "black",
    "pixel",
    "value",
    "zero",
    "white",
    "value",
    "whatever",
    "color",
    "going",
    "high",
    "number",
    "say",
    "one",
    "going",
    "pure",
    "white",
    "like",
    "might",
    "faint",
    "white",
    "pixel",
    "exactly",
    "zero",
    "going",
    "black",
    "color",
    "images",
    "three",
    "color",
    "channels",
    "red",
    "green",
    "blue",
    "grayscale",
    "one",
    "color",
    "channel",
    "think",
    "done",
    "enough",
    "visualizing",
    "images",
    "numbers",
    "next",
    "video",
    "visualize",
    "image",
    "image",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "checked",
    "input",
    "output",
    "shapes",
    "data",
    "downloaded",
    "fashion",
    "mnist",
    "data",
    "set",
    "comprised",
    "images",
    "grayscale",
    "images",
    "trousers",
    "pullovers",
    "dress",
    "coat",
    "sandal",
    "shirt",
    "sneaker",
    "bag",
    "ankle",
    "boot",
    "want",
    "see",
    "build",
    "computer",
    "vision",
    "model",
    "decipher",
    "going",
    "fashion",
    "mnist",
    "separate",
    "classify",
    "different",
    "items",
    "clothing",
    "based",
    "numerical",
    "representation",
    "part",
    "becoming",
    "one",
    "data",
    "course",
    "checking",
    "input",
    "output",
    "shapes",
    "fashion",
    "mnist",
    "data",
    "set",
    "zalando",
    "research",
    "recall",
    "look",
    "input",
    "output",
    "shapes",
    "well",
    "looked",
    "28",
    "28",
    "grayscale",
    "images",
    "want",
    "represent",
    "tensor",
    "want",
    "use",
    "input",
    "machine",
    "learning",
    "algorithm",
    "typically",
    "computer",
    "vision",
    "algorithm",
    "cnn",
    "want",
    "sort",
    "outputs",
    "formatted",
    "ideal",
    "shape",
    "like",
    "case",
    "10",
    "different",
    "types",
    "clothing",
    "going",
    "output",
    "shape",
    "10",
    "input",
    "shape",
    "default",
    "pytorch",
    "turns",
    "tensors",
    "color",
    "channels",
    "first",
    "input",
    "shape",
    "none",
    "one",
    "28",
    "none",
    "going",
    "batch",
    "size",
    "course",
    "set",
    "whatever",
    "like",
    "input",
    "shape",
    "format",
    "nchw",
    "words",
    "color",
    "channels",
    "first",
    "remember",
    "working",
    "machine",
    "learning",
    "libraries",
    "may",
    "want",
    "use",
    "color",
    "channels",
    "last",
    "let",
    "look",
    "might",
    "case",
    "going",
    "visualize",
    "images",
    "make",
    "little",
    "heading",
    "part",
    "becoming",
    "one",
    "data",
    "words",
    "understanding",
    "input",
    "output",
    "shapes",
    "many",
    "samples",
    "look",
    "like",
    "visualize",
    "visualize",
    "visualize",
    "let",
    "import",
    "mapplotlib",
    "going",
    "add",
    "code",
    "cells",
    "import",
    "plt",
    "let",
    "create",
    "image",
    "label",
    "train",
    "data",
    "zero",
    "going",
    "print",
    "image",
    "shape",
    "understand",
    "inputs",
    "going",
    "mapplotlib",
    "function",
    "going",
    "go",
    "going",
    "pass",
    "image",
    "see",
    "happens",
    "recall",
    "image",
    "look",
    "like",
    "image",
    "image",
    "big",
    "tensor",
    "numbers",
    "got",
    "image",
    "shape",
    "128",
    "happens",
    "call",
    "happens",
    "oh",
    "get",
    "error",
    "valid",
    "shape",
    "128",
    "128",
    "image",
    "data",
    "said",
    "one",
    "common",
    "errors",
    "machine",
    "learning",
    "shape",
    "issue",
    "shape",
    "input",
    "tensor",
    "match",
    "expected",
    "shape",
    "tensor",
    "one",
    "scenarios",
    "data",
    "format",
    "color",
    "channels",
    "first",
    "match",
    "mapplotlib",
    "expecting",
    "mapplotlib",
    "expects",
    "either",
    "height",
    "width",
    "color",
    "channel",
    "gray",
    "style",
    "images",
    "also",
    "expects",
    "color",
    "channels",
    "last",
    "see",
    "later",
    "grayscale",
    "get",
    "rid",
    "extra",
    "dimension",
    "passing",
    "recall",
    "squeeze",
    "going",
    "remove",
    "singular",
    "dimension",
    "look",
    "goes",
    "beautiful",
    "get",
    "ankle",
    "boot",
    "well",
    "pixelated",
    "ankle",
    "boot",
    "dealing",
    "28",
    "28",
    "pixels",
    "high",
    "definition",
    "image",
    "let",
    "add",
    "title",
    "going",
    "add",
    "label",
    "beautiful",
    "got",
    "number",
    "nine",
    "go",
    "ankle",
    "boot",
    "let",
    "plot",
    "grayscale",
    "might",
    "thing",
    "go",
    "going",
    "pass",
    "going",
    "change",
    "color",
    "map",
    "c",
    "map",
    "equals",
    "gray",
    "mapplotlib",
    "ever",
    "change",
    "colors",
    "plot",
    "want",
    "look",
    "c",
    "map",
    "property",
    "parameter",
    "sometimes",
    "also",
    "shortened",
    "case",
    "show",
    "c",
    "map",
    "want",
    "plot",
    "title",
    "going",
    "pull",
    "class",
    "names",
    "label",
    "integer",
    "look",
    "ankle",
    "boot",
    "remove",
    "accesses",
    "wanted",
    "turn",
    "going",
    "remove",
    "access",
    "go",
    "type",
    "images",
    "dealing",
    "singular",
    "image",
    "harness",
    "power",
    "randomness",
    "look",
    "random",
    "images",
    "data",
    "set",
    "would",
    "let",
    "go",
    "plot",
    "images",
    "set",
    "random",
    "seed",
    "looking",
    "similar",
    "possible",
    "images",
    "create",
    "plot",
    "calling",
    "going",
    "give",
    "size",
    "might",
    "create",
    "nine",
    "nine",
    "grid",
    "want",
    "see",
    "nine",
    "random",
    "images",
    "data",
    "set",
    "rows",
    "calls",
    "sorry",
    "maybe",
    "four",
    "four",
    "give",
    "us",
    "going",
    "go",
    "four",
    "range",
    "going",
    "go",
    "one",
    "rows",
    "times",
    "columns",
    "plus",
    "one",
    "print",
    "going",
    "give",
    "us",
    "want",
    "see",
    "16",
    "images",
    "oh",
    "16",
    "random",
    "images",
    "used",
    "manual",
    "c",
    "42",
    "data",
    "set",
    "one",
    "favorite",
    "things",
    "type",
    "data",
    "set",
    "looking",
    "whether",
    "text",
    "image",
    "audio",
    "matter",
    "like",
    "randomly",
    "look",
    "whole",
    "bunch",
    "samples",
    "start",
    "become",
    "one",
    "data",
    "said",
    "let",
    "use",
    "loop",
    "grab",
    "random",
    "indexes",
    "using",
    "tortures",
    "rand",
    "int",
    "random",
    "integer",
    "zero",
    "length",
    "training",
    "data",
    "going",
    "give",
    "us",
    "random",
    "integer",
    "range",
    "zero",
    "however",
    "many",
    "training",
    "samples",
    "case",
    "thereabouts",
    "want",
    "create",
    "size",
    "one",
    "want",
    "get",
    "item",
    "random",
    "index",
    "going",
    "give",
    "us",
    "oh",
    "excuse",
    "maybe",
    "print",
    "go",
    "random",
    "images",
    "using",
    "manual",
    "seed",
    "give",
    "us",
    "numbers",
    "every",
    "time",
    "three",
    "seven",
    "five",
    "four",
    "two",
    "three",
    "seven",
    "five",
    "four",
    "two",
    "commented",
    "random",
    "seed",
    "get",
    "different",
    "numbers",
    "every",
    "time",
    "demonstrate",
    "keep",
    "manual",
    "seed",
    "comment",
    "want",
    "different",
    "numbers",
    "different",
    "images",
    "different",
    "indexes",
    "time",
    "create",
    "image",
    "label",
    "indexing",
    "training",
    "data",
    "random",
    "index",
    "generating",
    "create",
    "plot",
    "fig",
    "add",
    "subplot",
    "fig",
    "add",
    "subplot",
    "going",
    "go",
    "rows",
    "calls",
    "index",
    "going",
    "add",
    "subplot",
    "remember",
    "set",
    "rows",
    "columns",
    "going",
    "go",
    "plt",
    "dot",
    "show",
    "going",
    "show",
    "going",
    "show",
    "image",
    "squeeze",
    "get",
    "rid",
    "singular",
    "dimension",
    "color",
    "channel",
    "otherwise",
    "end",
    "issue",
    "map",
    "plot",
    "lib",
    "going",
    "use",
    "color",
    "map",
    "gray",
    "looks",
    "like",
    "image",
    "plotted",
    "title",
    "going",
    "class",
    "names",
    "indexed",
    "label",
    "want",
    "accesses",
    "going",
    "clutter",
    "plot",
    "let",
    "see",
    "looks",
    "like",
    "oh",
    "goodness",
    "look",
    "worked",
    "first",
    "go",
    "usually",
    "visualizations",
    "take",
    "fair",
    "bit",
    "trial",
    "error",
    "ankle",
    "boots",
    "shirts",
    "bags",
    "ankle",
    "boots",
    "sandal",
    "shirt",
    "pull",
    "oh",
    "notice",
    "something",
    "data",
    "set",
    "right",
    "pull",
    "shirt",
    "look",
    "quite",
    "similar",
    "think",
    "cause",
    "issue",
    "later",
    "model",
    "trying",
    "predict",
    "pull",
    "shirt",
    "look",
    "images",
    "get",
    "rid",
    "random",
    "seed",
    "look",
    "different",
    "styles",
    "sandal",
    "ankle",
    "boot",
    "coat",
    "top",
    "shirt",
    "oh",
    "little",
    "bit",
    "confusing",
    "class",
    "top",
    "shirt",
    "like",
    "sure",
    "difference",
    "shirt",
    "something",
    "keep",
    "mind",
    "top",
    "look",
    "like",
    "could",
    "maybe",
    "even",
    "dress",
    "like",
    "shape",
    "something",
    "keep",
    "mind",
    "going",
    "forward",
    "chances",
    "get",
    "confused",
    "like",
    "looking",
    "data",
    "set",
    "get",
    "confused",
    "different",
    "samples",
    "labeled",
    "model",
    "might",
    "get",
    "confused",
    "later",
    "let",
    "look",
    "one",
    "go",
    "next",
    "video",
    "sneaker",
    "trouser",
    "shirt",
    "sandal",
    "dress",
    "pull",
    "bag",
    "bag",
    "oh",
    "quite",
    "difficult",
    "one",
    "look",
    "like",
    "even",
    "much",
    "going",
    "image",
    "whole",
    "premise",
    "building",
    "machine",
    "learning",
    "models",
    "would",
    "could",
    "write",
    "program",
    "would",
    "take",
    "shapes",
    "images",
    "figure",
    "write",
    "program",
    "would",
    "go",
    "hey",
    "looked",
    "like",
    "rectangle",
    "buckle",
    "middle",
    "probably",
    "bag",
    "mean",
    "probably",
    "could",
    "prefer",
    "write",
    "machine",
    "learning",
    "algorithms",
    "figure",
    "patterns",
    "data",
    "let",
    "start",
    "moving",
    "towards",
    "going",
    "go",
    "figuring",
    "prepare",
    "data",
    "loaded",
    "model",
    "see",
    "right",
    "right",
    "right",
    "got",
    "images",
    "clothing",
    "like",
    "build",
    "computer",
    "vision",
    "model",
    "classify",
    "10",
    "different",
    "classes",
    "visualized",
    "fair",
    "samples",
    "think",
    "could",
    "model",
    "linear",
    "lines",
    "straight",
    "lines",
    "think",
    "need",
    "model",
    "nonlinearity",
    "going",
    "write",
    "think",
    "items",
    "clothing",
    "images",
    "could",
    "modeled",
    "pure",
    "linear",
    "lines",
    "think",
    "need",
    "nonlinearity",
    "answer",
    "could",
    "test",
    "later",
    "might",
    "want",
    "skip",
    "ahead",
    "try",
    "build",
    "model",
    "linear",
    "lines",
    "nonlinearities",
    "covered",
    "linear",
    "lines",
    "nonlinearities",
    "let",
    "start",
    "prepare",
    "data",
    "even",
    "prepare",
    "data",
    "loader",
    "right",
    "data",
    "form",
    "pytorch",
    "data",
    "sets",
    "let",
    "look",
    "data",
    "go",
    "data",
    "set",
    "fashion",
    "mnist",
    "go",
    "test",
    "data",
    "see",
    "similar",
    "thing",
    "except",
    "different",
    "number",
    "data",
    "points",
    "transform",
    "turned",
    "tenses",
    "want",
    "convert",
    "data",
    "set",
    "collection",
    "data",
    "data",
    "loader",
    "paul",
    "data",
    "loader",
    "turns",
    "data",
    "set",
    "python",
    "iterable",
    "going",
    "turn",
    "markdown",
    "beautiful",
    "specifically",
    "specific",
    "galilee",
    "spell",
    "right",
    "know",
    "want",
    "code",
    "right",
    "learn",
    "spelling",
    "want",
    "turn",
    "data",
    "batches",
    "mini",
    "batches",
    "would",
    "well",
    "may",
    "get",
    "away",
    "building",
    "model",
    "look",
    "samples",
    "current",
    "data",
    "set",
    "quite",
    "small",
    "comprised",
    "images",
    "28",
    "28",
    "pixels",
    "say",
    "quite",
    "small",
    "yes",
    "images",
    "actually",
    "quite",
    "small",
    "deep",
    "learning",
    "scale",
    "data",
    "set",
    "modern",
    "data",
    "sets",
    "could",
    "millions",
    "images",
    "computer",
    "hardware",
    "able",
    "look",
    "samples",
    "28",
    "28",
    "one",
    "time",
    "would",
    "need",
    "fair",
    "bit",
    "memory",
    "ram",
    "space",
    "gpu",
    "memory",
    "compute",
    "memory",
    "chances",
    "might",
    "able",
    "store",
    "millions",
    "images",
    "memory",
    "break",
    "data",
    "set",
    "say",
    "groups",
    "batches",
    "mini",
    "batches",
    "seen",
    "batch",
    "size",
    "would",
    "well",
    "one",
    "computationally",
    "efficient",
    "computing",
    "hardware",
    "may",
    "able",
    "look",
    "store",
    "memory",
    "images",
    "one",
    "hit",
    "break",
    "32",
    "images",
    "time",
    "would",
    "batch",
    "size",
    "32",
    "number",
    "change",
    "32",
    "common",
    "batch",
    "size",
    "see",
    "many",
    "beginner",
    "style",
    "problems",
    "go",
    "see",
    "different",
    "batch",
    "sizes",
    "exemplify",
    "concept",
    "mini",
    "batches",
    "common",
    "deep",
    "learning",
    "else",
    "would",
    "second",
    "point",
    "second",
    "main",
    "point",
    "gives",
    "neural",
    "network",
    "chances",
    "update",
    "gradients",
    "per",
    "epoch",
    "mean",
    "make",
    "sense",
    "write",
    "training",
    "loop",
    "look",
    "images",
    "one",
    "time",
    "would",
    "per",
    "epoch",
    "per",
    "iteration",
    "data",
    "would",
    "get",
    "one",
    "update",
    "per",
    "epoch",
    "across",
    "entire",
    "data",
    "set",
    "whereas",
    "look",
    "32",
    "images",
    "time",
    "neural",
    "network",
    "updates",
    "internal",
    "states",
    "weights",
    "every",
    "32",
    "images",
    "thanks",
    "optimizer",
    "make",
    "lot",
    "sense",
    "write",
    "training",
    "loop",
    "two",
    "main",
    "reasons",
    "turning",
    "data",
    "mini",
    "batches",
    "form",
    "data",
    "loader",
    "like",
    "learn",
    "theory",
    "behind",
    "would",
    "highly",
    "recommend",
    "looking",
    "andrew",
    "org",
    "mini",
    "batches",
    "great",
    "lecture",
    "yeah",
    "machine",
    "learning",
    "mini",
    "batch",
    "gradient",
    "descent",
    "mini",
    "batch",
    "gradient",
    "descent",
    "yeah",
    "called",
    "mini",
    "batch",
    "gradient",
    "descent",
    "look",
    "results",
    "find",
    "whole",
    "bunch",
    "stuff",
    "might",
    "link",
    "one",
    "going",
    "pause",
    "going",
    "link",
    "mini",
    "batches",
    "see",
    "see",
    "visually",
    "got",
    "slide",
    "prepared",
    "going",
    "working",
    "towards",
    "input",
    "output",
    "shapes",
    "want",
    "create",
    "batch",
    "size",
    "32",
    "across",
    "training",
    "images",
    "actually",
    "going",
    "testing",
    "images",
    "testing",
    "images",
    "data",
    "set",
    "going",
    "look",
    "like",
    "batched",
    "going",
    "write",
    "code",
    "namely",
    "using",
    "data",
    "loader",
    "going",
    "pass",
    "data",
    "set",
    "train",
    "data",
    "going",
    "give",
    "batch",
    "size",
    "define",
    "whatever",
    "want",
    "us",
    "going",
    "use",
    "32",
    "begin",
    "going",
    "set",
    "shuffle",
    "equals",
    "true",
    "using",
    "training",
    "data",
    "would",
    "set",
    "shuffle",
    "equals",
    "true",
    "well",
    "case",
    "data",
    "set",
    "reason",
    "order",
    "say",
    "pants",
    "images",
    "row",
    "images",
    "row",
    "sandal",
    "images",
    "row",
    "want",
    "neural",
    "network",
    "necessarily",
    "remember",
    "order",
    "data",
    "want",
    "remember",
    "individual",
    "patterns",
    "different",
    "classes",
    "shuffle",
    "data",
    "mix",
    "mix",
    "looks",
    "something",
    "like",
    "might",
    "batch",
    "number",
    "zero",
    "32",
    "samples",
    "ran",
    "space",
    "creating",
    "got",
    "fun",
    "setting",
    "batch",
    "size",
    "equals",
    "look",
    "32",
    "samples",
    "per",
    "batch",
    "mix",
    "samples",
    "go",
    "batch",
    "batch",
    "batch",
    "batch",
    "batch",
    "however",
    "many",
    "batches",
    "number",
    "samples",
    "divided",
    "batch",
    "size",
    "divided",
    "32",
    "1800",
    "something",
    "like",
    "going",
    "working",
    "towards",
    "want",
    "write",
    "code",
    "video",
    "think",
    "save",
    "getting",
    "long",
    "going",
    "write",
    "code",
    "next",
    "video",
    "would",
    "like",
    "give",
    "go",
    "code",
    "train",
    "data",
    "loader",
    "test",
    "data",
    "loader",
    "see",
    "next",
    "video",
    "going",
    "batchify",
    "fashion",
    "mnist",
    "data",
    "set",
    "welcome",
    "back",
    "last",
    "video",
    "brief",
    "overview",
    "concept",
    "mini",
    "batches",
    "rather",
    "computer",
    "looking",
    "images",
    "one",
    "hit",
    "break",
    "things",
    "turn",
    "batches",
    "batch",
    "size",
    "vary",
    "depending",
    "problem",
    "working",
    "32",
    "quite",
    "good",
    "value",
    "start",
    "try",
    "two",
    "main",
    "reasons",
    "jump",
    "back",
    "code",
    "would",
    "computationally",
    "efficient",
    "gpu",
    "say",
    "10",
    "gigabytes",
    "memory",
    "might",
    "able",
    "store",
    "images",
    "one",
    "hit",
    "data",
    "set",
    "quite",
    "small",
    "may",
    "hour",
    "two",
    "better",
    "practice",
    "later",
    "turn",
    "things",
    "mini",
    "batches",
    "also",
    "gives",
    "neural",
    "network",
    "chances",
    "update",
    "gradients",
    "per",
    "epoch",
    "make",
    "lot",
    "sense",
    "write",
    "training",
    "loop",
    "spoken",
    "enough",
    "theory",
    "let",
    "write",
    "code",
    "going",
    "import",
    "data",
    "loader",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "import",
    "data",
    "loader",
    "principle",
    "way",
    "preparing",
    "data",
    "loader",
    "goes",
    "images",
    "text",
    "audio",
    "whatever",
    "sort",
    "data",
    "working",
    "mini",
    "batches",
    "follow",
    "along",
    "batches",
    "data",
    "follow",
    "along",
    "throughout",
    "lot",
    "different",
    "deep",
    "learning",
    "problems",
    "set",
    "batch",
    "size",
    "hyper",
    "parameter",
    "remember",
    "hyper",
    "parameter",
    "value",
    "set",
    "batch",
    "size",
    "equals",
    "practice",
    "might",
    "see",
    "typed",
    "capitals",
    "wo",
    "always",
    "see",
    "see",
    "quite",
    "often",
    "hyper",
    "parameter",
    "typed",
    "capitals",
    "going",
    "turn",
    "data",
    "sets",
    "iterables",
    "batches",
    "going",
    "create",
    "train",
    "data",
    "loader",
    "fashion",
    "mnist",
    "data",
    "set",
    "going",
    "use",
    "data",
    "loader",
    "going",
    "see",
    "doc",
    "string",
    "actually",
    "let",
    "look",
    "documentation",
    "torch",
    "data",
    "loader",
    "extra",
    "curriculum",
    "way",
    "read",
    "data",
    "page",
    "torch",
    "utils",
    "data",
    "matter",
    "problem",
    "going",
    "deep",
    "learning",
    "pytorch",
    "going",
    "working",
    "data",
    "spend",
    "10",
    "minutes",
    "reading",
    "think",
    "might",
    "already",
    "assigned",
    "important",
    "worth",
    "going",
    "read",
    "even",
    "understand",
    "going",
    "helps",
    "know",
    "look",
    "certain",
    "things",
    "take",
    "data",
    "loader",
    "takes",
    "data",
    "set",
    "need",
    "set",
    "batch",
    "size",
    "something",
    "default",
    "one",
    "means",
    "would",
    "create",
    "batch",
    "one",
    "image",
    "time",
    "case",
    "want",
    "shuffle",
    "want",
    "use",
    "specific",
    "sampler",
    "things",
    "going",
    "number",
    "workers",
    "number",
    "workers",
    "stands",
    "many",
    "cores",
    "machine",
    "want",
    "use",
    "load",
    "data",
    "generally",
    "higher",
    "better",
    "one",
    "going",
    "keep",
    "default",
    "set",
    "pretty",
    "good",
    "values",
    "begin",
    "let",
    "read",
    "parameters",
    "going",
    "focus",
    "first",
    "three",
    "data",
    "set",
    "batch",
    "size",
    "shuffle",
    "true",
    "false",
    "let",
    "see",
    "data",
    "set",
    "equals",
    "train",
    "data",
    "fashion",
    "mnist",
    "batch",
    "size",
    "going",
    "set",
    "batch",
    "size",
    "hyper",
    "parameter",
    "going",
    "batch",
    "size",
    "finally",
    "want",
    "shuffle",
    "training",
    "data",
    "yes",
    "going",
    "thing",
    "test",
    "data",
    "loader",
    "except",
    "going",
    "shuffle",
    "test",
    "data",
    "shuffle",
    "test",
    "data",
    "want",
    "practice",
    "actually",
    "easier",
    "evaluate",
    "different",
    "models",
    "test",
    "data",
    "shuffled",
    "shuffle",
    "training",
    "data",
    "remove",
    "order",
    "model",
    "learn",
    "order",
    "evaluation",
    "purposes",
    "generally",
    "good",
    "test",
    "data",
    "order",
    "model",
    "never",
    "actually",
    "see",
    "test",
    "data",
    "set",
    "training",
    "using",
    "evaluation",
    "order",
    "really",
    "matter",
    "test",
    "data",
    "loader",
    "easier",
    "shuffle",
    "evaluate",
    "multiple",
    "times",
    "shuffled",
    "every",
    "single",
    "time",
    "let",
    "run",
    "going",
    "check",
    "train",
    "data",
    "loader",
    "test",
    "data",
    "loader",
    "beautiful",
    "instances",
    "torch",
    "utils",
    "data",
    "data",
    "loader",
    "data",
    "loader",
    "let",
    "check",
    "created",
    "hey",
    "always",
    "like",
    "print",
    "different",
    "attributes",
    "whatever",
    "make",
    "check",
    "created",
    "part",
    "becoming",
    "one",
    "data",
    "print",
    "f",
    "going",
    "go",
    "data",
    "loaders",
    "pass",
    "going",
    "output",
    "basically",
    "exact",
    "got",
    "data",
    "loader",
    "also",
    "see",
    "attributes",
    "get",
    "going",
    "train",
    "data",
    "loader",
    "need",
    "caps",
    "lock",
    "train",
    "data",
    "loader",
    "full",
    "stop",
    "go",
    "tab",
    "got",
    "whole",
    "bunch",
    "different",
    "attributes",
    "got",
    "batch",
    "size",
    "got",
    "data",
    "set",
    "want",
    "drop",
    "last",
    "batch",
    "size",
    "overlapped",
    "samples",
    "want",
    "get",
    "rid",
    "last",
    "batch",
    "say",
    "example",
    "last",
    "batch",
    "10",
    "samples",
    "want",
    "drop",
    "want",
    "pin",
    "memory",
    "going",
    "help",
    "later",
    "wanted",
    "load",
    "data",
    "faster",
    "whole",
    "bunch",
    "different",
    "stuff",
    "like",
    "research",
    "find",
    "stuff",
    "going",
    "documentation",
    "let",
    "keep",
    "pushing",
    "forward",
    "else",
    "want",
    "know",
    "let",
    "find",
    "length",
    "train",
    "data",
    "loader",
    "go",
    "length",
    "train",
    "data",
    "loader",
    "going",
    "tell",
    "us",
    "many",
    "batches",
    "batches",
    "course",
    "batch",
    "size",
    "want",
    "print",
    "length",
    "test",
    "data",
    "loader",
    "want",
    "length",
    "test",
    "data",
    "loader",
    "batches",
    "batch",
    "size",
    "dot",
    "dot",
    "dot",
    "let",
    "find",
    "information",
    "oh",
    "go",
    "seeing",
    "saw",
    "one",
    "interesting",
    "length",
    "train",
    "data",
    "loader",
    "yeah",
    "batches",
    "training",
    "samples",
    "divided",
    "32",
    "yeah",
    "comes",
    "testing",
    "samples",
    "32",
    "comes",
    "gets",
    "rounded",
    "meant",
    "last",
    "batch",
    "maybe",
    "32",
    "32",
    "divide",
    "evenly",
    "okay",
    "means",
    "model",
    "going",
    "look",
    "individual",
    "batches",
    "32",
    "images",
    "rather",
    "one",
    "big",
    "batch",
    "images",
    "course",
    "number",
    "batches",
    "change",
    "change",
    "batch",
    "size",
    "469",
    "batches",
    "reduce",
    "one",
    "get",
    "batch",
    "per",
    "sample",
    "batches",
    "1",
    "batches",
    "1",
    "going",
    "stick",
    "let",
    "visualize",
    "got",
    "train",
    "data",
    "loader",
    "would",
    "visualize",
    "batch",
    "single",
    "image",
    "batch",
    "let",
    "show",
    "sample",
    "show",
    "interact",
    "data",
    "loader",
    "going",
    "use",
    "randomness",
    "well",
    "set",
    "manual",
    "seed",
    "get",
    "random",
    "index",
    "random",
    "idx",
    "equals",
    "torch",
    "rand",
    "int",
    "going",
    "go",
    "zero",
    "length",
    "train",
    "features",
    "batch",
    "oh",
    "get",
    "excuse",
    "getting",
    "ahead",
    "want",
    "check",
    "inside",
    "training",
    "data",
    "loader",
    "check",
    "inside",
    "training",
    "data",
    "loader",
    "test",
    "data",
    "load",
    "going",
    "similar",
    "want",
    "train",
    "features",
    "batch",
    "say",
    "features",
    "images",
    "train",
    "labels",
    "batch",
    "going",
    "labels",
    "data",
    "set",
    "targets",
    "pytorch",
    "terminology",
    "next",
    "idar",
    "data",
    "loader",
    "data",
    "loader",
    "1875",
    "batches",
    "32",
    "going",
    "turn",
    "iterable",
    "ita",
    "going",
    "get",
    "next",
    "batch",
    "next",
    "go",
    "train",
    "features",
    "get",
    "train",
    "labels",
    "think",
    "going",
    "give",
    "us",
    "well",
    "go",
    "look",
    "tensor",
    "batch",
    "32",
    "samples",
    "batch",
    "size",
    "color",
    "channels",
    "height",
    "width",
    "32",
    "labels",
    "associated",
    "32",
    "samples",
    "seen",
    "go",
    "back",
    "keynote",
    "input",
    "output",
    "shapes",
    "shape",
    "equals",
    "32",
    "28",
    "28",
    "color",
    "channels",
    "last",
    "currently",
    "color",
    "channels",
    "first",
    "sound",
    "like",
    "broken",
    "record",
    "vary",
    "depending",
    "problem",
    "working",
    "larger",
    "images",
    "would",
    "change",
    "height",
    "width",
    "dimensions",
    "would",
    "change",
    "color",
    "images",
    "color",
    "dimension",
    "would",
    "change",
    "premise",
    "still",
    "turning",
    "data",
    "batches",
    "pass",
    "model",
    "let",
    "come",
    "back",
    "let",
    "keep",
    "going",
    "visualization",
    "want",
    "visualize",
    "one",
    "random",
    "samples",
    "batch",
    "going",
    "go",
    "image",
    "label",
    "equals",
    "train",
    "features",
    "batch",
    "going",
    "get",
    "random",
    "idx",
    "get",
    "train",
    "labels",
    "batch",
    "get",
    "random",
    "idx",
    "matching",
    "got",
    "one",
    "batch",
    "train",
    "features",
    "batch",
    "train",
    "labels",
    "batch",
    "getting",
    "image",
    "label",
    "random",
    "index",
    "within",
    "batch",
    "excuse",
    "need",
    "set",
    "equal",
    "going",
    "go",
    "plt",
    "dot",
    "show",
    "going",
    "show",
    "going",
    "show",
    "image",
    "going",
    "squeeze",
    "remove",
    "singular",
    "dimension",
    "set",
    "c",
    "map",
    "equal",
    "gray",
    "go",
    "plt",
    "dot",
    "title",
    "set",
    "title",
    "going",
    "class",
    "names",
    "indexed",
    "label",
    "integer",
    "turn",
    "accesses",
    "use",
    "use",
    "false",
    "depends",
    "like",
    "use",
    "let",
    "print",
    "image",
    "size",
    "never",
    "know",
    "enough",
    "data",
    "print",
    "let",
    "also",
    "get",
    "label",
    "label",
    "label",
    "shape",
    "label",
    "size",
    "label",
    "single",
    "integer",
    "might",
    "shape",
    "okay",
    "let",
    "look",
    "oh",
    "bag",
    "see",
    "look",
    "quite",
    "hard",
    "understand",
    "would",
    "able",
    "detect",
    "bag",
    "tell",
    "could",
    "write",
    "program",
    "understand",
    "looks",
    "like",
    "warped",
    "rectangle",
    "look",
    "another",
    "one",
    "get",
    "another",
    "random",
    "oh",
    "got",
    "random",
    "seed",
    "going",
    "produce",
    "image",
    "time",
    "shirt",
    "okay",
    "shirt",
    "see",
    "image",
    "size",
    "128",
    "recall",
    "image",
    "size",
    "single",
    "image",
    "batch",
    "dimension",
    "color",
    "channels",
    "height",
    "width",
    "go",
    "label",
    "four",
    "coat",
    "could",
    "keep",
    "become",
    "familiar",
    "data",
    "particular",
    "batch",
    "created",
    "coat",
    "one",
    "another",
    "coat",
    "one",
    "make",
    "sure",
    "coat",
    "go",
    "got",
    "bag",
    "beautiful",
    "turned",
    "data",
    "data",
    "loaders",
    "could",
    "use",
    "pass",
    "model",
    "model",
    "think",
    "time",
    "next",
    "video",
    "start",
    "build",
    "model",
    "zero",
    "start",
    "build",
    "baseline",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "got",
    "data",
    "sets",
    "data",
    "set",
    "data",
    "loaders",
    "batches",
    "32",
    "images",
    "training",
    "data",
    "set",
    "rather",
    "one",
    "big",
    "data",
    "set",
    "13",
    "313",
    "batches",
    "32",
    "test",
    "data",
    "set",
    "learned",
    "visualize",
    "batch",
    "saw",
    "still",
    "image",
    "size",
    "one",
    "color",
    "channel",
    "28",
    "done",
    "turned",
    "batches",
    "pass",
    "model",
    "speaking",
    "model",
    "let",
    "look",
    "workflow",
    "well",
    "got",
    "data",
    "ready",
    "turned",
    "tensors",
    "combination",
    "torch",
    "vision",
    "transforms",
    "torch",
    "utils",
    "data",
    "dot",
    "data",
    "set",
    "use",
    "one",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "us",
    "fashion",
    "mnist",
    "data",
    "set",
    "use",
    "one",
    "torch",
    "utils",
    "dot",
    "data",
    "data",
    "loader",
    "turn",
    "data",
    "sets",
    "data",
    "loaders",
    "building",
    "picking",
    "model",
    "suit",
    "problem",
    "let",
    "start",
    "simply",
    "let",
    "build",
    "baseline",
    "model",
    "exciting",
    "going",
    "build",
    "first",
    "model",
    "first",
    "computer",
    "vision",
    "model",
    "albeit",
    "baseline",
    "important",
    "step",
    "going",
    "write",
    "starting",
    "build",
    "series",
    "machine",
    "learning",
    "modeling",
    "experiments",
    "best",
    "practice",
    "start",
    "baseline",
    "model",
    "going",
    "turn",
    "markdown",
    "baseline",
    "model",
    "baseline",
    "model",
    "simple",
    "model",
    "try",
    "improve",
    "upon",
    "subsequent",
    "models",
    "models",
    "slash",
    "experiments",
    "start",
    "simply",
    "words",
    "start",
    "simply",
    "add",
    "complexity",
    "necessary",
    "neural",
    "networks",
    "pretty",
    "powerful",
    "right",
    "tendency",
    "almost",
    "well",
    "data",
    "set",
    "concept",
    "known",
    "overfitting",
    "cover",
    "little",
    "bit",
    "later",
    "built",
    "simple",
    "model",
    "begin",
    "baseline",
    "whole",
    "goal",
    "run",
    "experiments",
    "according",
    "workflow",
    "improve",
    "experimentation",
    "guide",
    "set",
    "stone",
    "general",
    "pattern",
    "things",
    "go",
    "get",
    "data",
    "ready",
    "build",
    "model",
    "fit",
    "model",
    "evaluate",
    "improve",
    "model",
    "first",
    "model",
    "build",
    "generally",
    "baseline",
    "later",
    "want",
    "improve",
    "experimentation",
    "let",
    "start",
    "building",
    "baseline",
    "going",
    "introduce",
    "new",
    "layer",
    "seen",
    "creating",
    "flatten",
    "layer",
    "flatten",
    "layer",
    "well",
    "best",
    "seen",
    "code",
    "let",
    "create",
    "flatten",
    "model",
    "going",
    "could",
    "find",
    "documentation",
    "go",
    "nn",
    "flatten",
    "flatten",
    "pytorch",
    "flattens",
    "continuous",
    "range",
    "dims",
    "tensor",
    "use",
    "sequential",
    "example",
    "rather",
    "doubt",
    "code",
    "create",
    "flatten",
    "layer",
    "course",
    "could",
    "used",
    "model",
    "going",
    "get",
    "single",
    "sample",
    "x",
    "equals",
    "train",
    "features",
    "batch",
    "let",
    "get",
    "first",
    "one",
    "zero",
    "look",
    "like",
    "tensor",
    "x",
    "maybe",
    "get",
    "shape",
    "well",
    "x",
    "shape",
    "get",
    "go",
    "shape",
    "keep",
    "mind",
    "pass",
    "flatten",
    "layer",
    "inkling",
    "flatten",
    "might",
    "shape",
    "begin",
    "128",
    "let",
    "flatten",
    "sample",
    "output",
    "equals",
    "going",
    "pass",
    "flatten",
    "model",
    "going",
    "perform",
    "forward",
    "pass",
    "internally",
    "flatten",
    "layer",
    "perform",
    "forward",
    "pass",
    "let",
    "print",
    "happened",
    "print",
    "shape",
    "flattening",
    "equals",
    "x",
    "dot",
    "shape",
    "going",
    "print",
    "shape",
    "flattening",
    "equals",
    "output",
    "dot",
    "shape",
    "taking",
    "output",
    "flatten",
    "model",
    "printing",
    "shape",
    "oh",
    "notice",
    "happened",
    "well",
    "gone",
    "128",
    "28",
    "wow",
    "output",
    "look",
    "like",
    "output",
    "oh",
    "values",
    "one",
    "big",
    "vector",
    "squeeze",
    "remove",
    "extra",
    "dimension",
    "got",
    "one",
    "big",
    "vector",
    "values",
    "number",
    "come",
    "well",
    "take",
    "shape",
    "got",
    "color",
    "channels",
    "got",
    "height",
    "got",
    "width",
    "flattened",
    "color",
    "channels",
    "height",
    "width",
    "got",
    "one",
    "big",
    "feature",
    "vector",
    "28",
    "28",
    "equals",
    "got",
    "one",
    "value",
    "per",
    "pixel",
    "one",
    "value",
    "per",
    "pixel",
    "output",
    "vector",
    "see",
    "go",
    "back",
    "keynote",
    "look",
    "tesla",
    "takes",
    "eight",
    "cameras",
    "turns",
    "three",
    "dimensional",
    "vector",
    "space",
    "vector",
    "space",
    "trying",
    "trying",
    "encode",
    "whatever",
    "data",
    "working",
    "tesla",
    "case",
    "eight",
    "cameras",
    "dimensions",
    "time",
    "aspect",
    "dealing",
    "video",
    "multiple",
    "different",
    "camera",
    "angles",
    "dealing",
    "single",
    "image",
    "regardless",
    "concept",
    "trying",
    "condense",
    "information",
    "single",
    "vector",
    "space",
    "come",
    "back",
    "might",
    "well",
    "going",
    "build",
    "baseline",
    "model",
    "going",
    "use",
    "linear",
    "layer",
    "baseline",
    "model",
    "linear",
    "layer",
    "ca",
    "handle",
    "multi",
    "dimensional",
    "data",
    "like",
    "want",
    "single",
    "vector",
    "input",
    "make",
    "lot",
    "sense",
    "coded",
    "model",
    "let",
    "torch",
    "import",
    "going",
    "go",
    "class",
    "fashion",
    "amnest",
    "model",
    "v",
    "zero",
    "going",
    "inherit",
    "end",
    "dot",
    "module",
    "inside",
    "going",
    "init",
    "function",
    "constructor",
    "going",
    "pass",
    "self",
    "going",
    "input",
    "shape",
    "use",
    "type",
    "hint",
    "take",
    "integer",
    "remember",
    "input",
    "shape",
    "important",
    "machine",
    "learning",
    "models",
    "going",
    "define",
    "number",
    "hidden",
    "units",
    "also",
    "integer",
    "going",
    "define",
    "output",
    "shape",
    "think",
    "output",
    "shape",
    "many",
    "classes",
    "dealing",
    "dealing",
    "10",
    "different",
    "classes",
    "output",
    "shape",
    "save",
    "later",
    "let",
    "guess",
    "might",
    "already",
    "know",
    "going",
    "initialize",
    "going",
    "create",
    "layer",
    "stack",
    "stack",
    "equals",
    "recall",
    "sequential",
    "whatever",
    "put",
    "inside",
    "sequential",
    "data",
    "goes",
    "sequential",
    "going",
    "go",
    "layer",
    "layer",
    "let",
    "create",
    "first",
    "layer",
    "going",
    "means",
    "anything",
    "comes",
    "first",
    "layer",
    "going",
    "happen",
    "going",
    "flatten",
    "external",
    "dimensions",
    "going",
    "flatten",
    "something",
    "like",
    "going",
    "flatten",
    "first",
    "flatten",
    "data",
    "going",
    "pass",
    "linear",
    "layer",
    "going",
    "many",
    "n",
    "features",
    "going",
    "input",
    "shape",
    "going",
    "define",
    "input",
    "shape",
    "going",
    "go",
    "features",
    "equals",
    "hidden",
    "units",
    "going",
    "create",
    "another",
    "linear",
    "layer",
    "going",
    "set",
    "n",
    "features",
    "equals",
    "hidden",
    "units",
    "features",
    "equals",
    "output",
    "shape",
    "putting",
    "features",
    "n",
    "features",
    "well",
    "subsequent",
    "layers",
    "input",
    "layer",
    "input",
    "shape",
    "line",
    "output",
    "shape",
    "layer",
    "hence",
    "use",
    "features",
    "hidden",
    "units",
    "output",
    "layer",
    "use",
    "n",
    "features",
    "hidden",
    "units",
    "input",
    "value",
    "hidden",
    "layer",
    "let",
    "keep",
    "going",
    "let",
    "go",
    "def",
    "create",
    "forward",
    "pass",
    "subclass",
    "override",
    "forward",
    "method",
    "forward",
    "method",
    "going",
    "define",
    "going",
    "define",
    "forward",
    "computation",
    "model",
    "going",
    "return",
    "stack",
    "model",
    "going",
    "take",
    "input",
    "x",
    "could",
    "case",
    "going",
    "batch",
    "time",
    "going",
    "pass",
    "sample",
    "flatten",
    "layer",
    "going",
    "pass",
    "output",
    "flatten",
    "layer",
    "first",
    "linear",
    "layer",
    "going",
    "pass",
    "output",
    "linear",
    "layer",
    "linear",
    "layer",
    "model",
    "two",
    "linear",
    "layers",
    "flatten",
    "layer",
    "flatten",
    "layer",
    "learnable",
    "parameters",
    "two",
    "nonlinearities",
    "think",
    "work",
    "data",
    "set",
    "need",
    "nonlinearities",
    "well",
    "find",
    "fit",
    "model",
    "data",
    "let",
    "set",
    "instance",
    "model",
    "torch",
    "dot",
    "manual",
    "seed",
    "let",
    "go",
    "set",
    "model",
    "input",
    "parameters",
    "model",
    "zero",
    "equals",
    "fashion",
    "mnist",
    "model",
    "class",
    "wrote",
    "going",
    "define",
    "input",
    "shape",
    "equals",
    "get",
    "well",
    "28",
    "output",
    "flatten",
    "needs",
    "input",
    "shape",
    "could",
    "put",
    "28",
    "28",
    "going",
    "put",
    "784",
    "write",
    "comment",
    "28",
    "go",
    "wonder",
    "tell",
    "us",
    "tell",
    "us",
    "expects",
    "features",
    "size",
    "input",
    "sample",
    "shape",
    "star",
    "means",
    "number",
    "dimensions",
    "including",
    "none",
    "features",
    "linear",
    "weight",
    "well",
    "let",
    "figure",
    "let",
    "see",
    "happens",
    "doubt",
    "coded",
    "hey",
    "see",
    "units",
    "equals",
    "let",
    "go",
    "10",
    "begin",
    "many",
    "units",
    "hidden",
    "layer",
    "output",
    "shape",
    "going",
    "output",
    "shape",
    "length",
    "class",
    "names",
    "1",
    "every",
    "class",
    "beautiful",
    "let",
    "go",
    "model",
    "zero",
    "going",
    "keep",
    "cpu",
    "begin",
    "could",
    "write",
    "code",
    "begin",
    "going",
    "send",
    "cpu",
    "might",
    "put",
    "actually",
    "cpu",
    "let",
    "look",
    "model",
    "zero",
    "wonderful",
    "try",
    "dummy",
    "forward",
    "pass",
    "see",
    "happens",
    "let",
    "create",
    "dummy",
    "x",
    "equals",
    "torch",
    "rand",
    "create",
    "size",
    "image",
    "singular",
    "image",
    "going",
    "batch",
    "one",
    "color",
    "channel",
    "one",
    "height",
    "28",
    "height",
    "going",
    "go",
    "model",
    "zero",
    "pass",
    "dummy",
    "going",
    "send",
    "dummy",
    "x",
    "forward",
    "method",
    "let",
    "see",
    "happens",
    "okay",
    "wonderful",
    "get",
    "output",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "10",
    "logits",
    "beautiful",
    "exactly",
    "want",
    "one",
    "logit",
    "value",
    "per",
    "class",
    "would",
    "happen",
    "got",
    "rid",
    "flatten",
    "ran",
    "ran",
    "ran",
    "get",
    "oh",
    "mat",
    "one",
    "mat",
    "two",
    "shapes",
    "multiplied",
    "28",
    "28",
    "okay",
    "happens",
    "change",
    "input",
    "shape",
    "28",
    "getting",
    "shape",
    "mismatches",
    "happens",
    "oh",
    "okay",
    "get",
    "interesting",
    "output",
    "still",
    "right",
    "shape",
    "flatten",
    "layer",
    "comes",
    "shape",
    "oh",
    "get",
    "1",
    "1",
    "28",
    "oh",
    "put",
    "flatten",
    "combines",
    "vector",
    "get",
    "rid",
    "see",
    "leave",
    "shape",
    "get",
    "28",
    "different",
    "samples",
    "10",
    "want",
    "want",
    "compress",
    "image",
    "singular",
    "vector",
    "pass",
    "let",
    "reinstanceuate",
    "flatten",
    "layer",
    "let",
    "make",
    "sure",
    "got",
    "right",
    "input",
    "shape",
    "28",
    "28",
    "let",
    "pass",
    "torch",
    "size",
    "exactly",
    "want",
    "1",
    "logit",
    "per",
    "class",
    "could",
    "bit",
    "fiddly",
    "first",
    "start",
    "also",
    "lot",
    "fun",
    "get",
    "work",
    "keep",
    "mind",
    "showed",
    "looks",
    "like",
    "error",
    "one",
    "biggest",
    "errors",
    "going",
    "face",
    "machine",
    "learning",
    "different",
    "tensor",
    "shape",
    "mismatches",
    "keep",
    "mind",
    "data",
    "working",
    "look",
    "documentation",
    "input",
    "shape",
    "certain",
    "layers",
    "expect",
    "said",
    "think",
    "time",
    "start",
    "moving",
    "towards",
    "training",
    "model",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "created",
    "model",
    "zero",
    "going",
    "baseline",
    "model",
    "computer",
    "vision",
    "problem",
    "detecting",
    "different",
    "types",
    "clothing",
    "28",
    "28",
    "gray",
    "scale",
    "images",
    "also",
    "learned",
    "concept",
    "making",
    "sure",
    "rehashed",
    "concept",
    "making",
    "sure",
    "input",
    "output",
    "shapes",
    "line",
    "need",
    "also",
    "dummy",
    "forward",
    "pass",
    "dummy",
    "data",
    "great",
    "way",
    "troubleshoot",
    "see",
    "model",
    "shapes",
    "correct",
    "come",
    "correctly",
    "inputs",
    "lining",
    "need",
    "rehash",
    "model",
    "going",
    "inside",
    "model",
    "check",
    "model",
    "zero",
    "state",
    "dict",
    "see",
    "first",
    "layer",
    "weight",
    "tensor",
    "also",
    "bias",
    "next",
    "layer",
    "weight",
    "tensor",
    "also",
    "bias",
    "course",
    "initialized",
    "random",
    "values",
    "whole",
    "premise",
    "deep",
    "learning",
    "machine",
    "learning",
    "pass",
    "data",
    "model",
    "use",
    "optimizer",
    "update",
    "random",
    "values",
    "better",
    "represent",
    "features",
    "data",
    "keep",
    "saying",
    "features",
    "want",
    "rehash",
    "move",
    "next",
    "thing",
    "featuring",
    "data",
    "could",
    "almost",
    "anything",
    "example",
    "feature",
    "bag",
    "could",
    "got",
    "rounded",
    "handle",
    "top",
    "edge",
    "edge",
    "going",
    "tell",
    "model",
    "features",
    "learn",
    "data",
    "whole",
    "premise",
    "whole",
    "fun",
    "whole",
    "magic",
    "behind",
    "machine",
    "learning",
    "figures",
    "features",
    "learn",
    "weights",
    "bias",
    "matrices",
    "tensors",
    "represent",
    "different",
    "features",
    "images",
    "could",
    "many",
    "images",
    "10",
    "classes",
    "let",
    "keep",
    "pushing",
    "forward",
    "time",
    "set",
    "loss",
    "function",
    "optimizer",
    "speaking",
    "optimizers",
    "set",
    "loss",
    "optimizer",
    "evaluation",
    "metrics",
    "recall",
    "notebook",
    "two",
    "going",
    "turn",
    "markdown",
    "created",
    "oh",
    "need",
    "emoji",
    "way",
    "moving",
    "workflow",
    "got",
    "data",
    "ready",
    "tensors",
    "built",
    "baseline",
    "model",
    "time",
    "pick",
    "loss",
    "function",
    "optimizer",
    "go",
    "back",
    "google",
    "chrome",
    "right",
    "loss",
    "function",
    "loss",
    "function",
    "going",
    "since",
    "working",
    "data",
    "loss",
    "function",
    "nn",
    "dot",
    "cross",
    "entropy",
    "loss",
    "optimizer",
    "got",
    "options",
    "optimizer",
    "practice",
    "past",
    "sgd",
    "stands",
    "stochastic",
    "gradient",
    "descent",
    "atom",
    "optimizer",
    "optimizer",
    "let",
    "stick",
    "sgd",
    "kind",
    "entry",
    "level",
    "optimizer",
    "torch",
    "opt",
    "sgd",
    "stochastic",
    "gradient",
    "descent",
    "finally",
    "evaluation",
    "metric",
    "since",
    "working",
    "classification",
    "problem",
    "let",
    "use",
    "accuracy",
    "evaluation",
    "metric",
    "recall",
    "accuracy",
    "classification",
    "evaluation",
    "metric",
    "find",
    "well",
    "go",
    "beauty",
    "online",
    "reference",
    "material",
    "neural",
    "network",
    "classification",
    "pytorch",
    "notebook",
    "section",
    "02",
    "created",
    "different",
    "classification",
    "methods",
    "yes",
    "got",
    "whole",
    "bunch",
    "different",
    "options",
    "classification",
    "evaluation",
    "metrics",
    "got",
    "accuracy",
    "precision",
    "recall",
    "f1",
    "score",
    "confusion",
    "matrix",
    "code",
    "could",
    "use",
    "wanted",
    "use",
    "torch",
    "metrics",
    "accuracy",
    "could",
    "torch",
    "metrics",
    "beautiful",
    "library",
    "lot",
    "evaluation",
    "oh",
    "exist",
    "happened",
    "torch",
    "metrics",
    "maybe",
    "need",
    "fix",
    "link",
    "torch",
    "metrics",
    "whole",
    "bunch",
    "different",
    "pytorch",
    "metrics",
    "useful",
    "library",
    "also",
    "coded",
    "function",
    "accuracy",
    "fn",
    "could",
    "copy",
    "straight",
    "notebook",
    "also",
    "go",
    "pytorch",
    "deep",
    "learning",
    "github",
    "bring",
    "also",
    "put",
    "helper",
    "script",
    "common",
    "functions",
    "used",
    "throughout",
    "course",
    "including",
    "find",
    "accuracy",
    "function",
    "calculate",
    "accuracy",
    "would",
    "get",
    "helper",
    "functions",
    "file",
    "python",
    "file",
    "notebook",
    "one",
    "way",
    "copy",
    "code",
    "straight",
    "let",
    "import",
    "python",
    "script",
    "import",
    "request",
    "going",
    "go",
    "pathlib",
    "import",
    "path",
    "want",
    "download",
    "actually",
    "going",
    "see",
    "common",
    "practice",
    "larger",
    "python",
    "projects",
    "especially",
    "deep",
    "learning",
    "machine",
    "learning",
    "projects",
    "different",
    "functionality",
    "split",
    "different",
    "python",
    "files",
    "way",
    "keep",
    "rewriting",
    "code",
    "like",
    "know",
    "written",
    "training",
    "testing",
    "loop",
    "fair",
    "times",
    "well",
    "written",
    "works",
    "might",
    "want",
    "save",
    "file",
    "import",
    "later",
    "let",
    "write",
    "code",
    "import",
    "helper",
    "file",
    "notebook",
    "download",
    "helper",
    "functions",
    "learn",
    "pytorch",
    "repo",
    "going",
    "check",
    "helper",
    "already",
    "exists",
    "want",
    "download",
    "print",
    "helper",
    "already",
    "exists",
    "skipping",
    "download",
    "skipping",
    "download",
    "going",
    "go",
    "else",
    "exist",
    "going",
    "download",
    "downloading",
    "helper",
    "going",
    "create",
    "request",
    "request",
    "library",
    "equals",
    "pass",
    "url",
    "file",
    "url",
    "dealing",
    "github",
    "get",
    "actual",
    "url",
    "files",
    "many",
    "files",
    "click",
    "raw",
    "button",
    "go",
    "back",
    "show",
    "click",
    "raw",
    "going",
    "copy",
    "raw",
    "url",
    "see",
    "text",
    "want",
    "download",
    "notebook",
    "going",
    "write",
    "request",
    "equals",
    "going",
    "go",
    "open",
    "going",
    "save",
    "helper",
    "functions",
    "going",
    "write",
    "binary",
    "file",
    "f",
    "file",
    "going",
    "go",
    "saying",
    "python",
    "going",
    "create",
    "file",
    "called",
    "helper",
    "give",
    "write",
    "binary",
    "permissions",
    "f",
    "f",
    "file",
    "short",
    "file",
    "going",
    "say",
    "request",
    "get",
    "information",
    "helper",
    "functions",
    "write",
    "content",
    "file",
    "let",
    "give",
    "shot",
    "beautiful",
    "downloading",
    "helper",
    "let",
    "look",
    "helper",
    "yes",
    "wonderful",
    "import",
    "accuracy",
    "function",
    "go",
    "import",
    "accuracy",
    "function",
    "common",
    "practice",
    "writing",
    "lots",
    "python",
    "code",
    "put",
    "helper",
    "functions",
    "scripts",
    "let",
    "import",
    "accuracy",
    "metric",
    "accuracy",
    "metric",
    "helper",
    "functions",
    "course",
    "could",
    "used",
    "torch",
    "metrics",
    "well",
    "another",
    "perfectly",
    "valid",
    "option",
    "thought",
    "show",
    "like",
    "import",
    "helper",
    "function",
    "script",
    "course",
    "customize",
    "helper",
    "whatever",
    "want",
    "see",
    "got",
    "helper",
    "functions",
    "import",
    "accuracy",
    "function",
    "saying",
    "could",
    "resolved",
    "going",
    "work",
    "go",
    "accuracy",
    "function",
    "get",
    "doc",
    "string",
    "hmm",
    "seems",
    "like",
    "colab",
    "picking",
    "things",
    "right",
    "looks",
    "like",
    "still",
    "worked",
    "find",
    "later",
    "actually",
    "works",
    "train",
    "model",
    "set",
    "loss",
    "function",
    "optimizer",
    "going",
    "set",
    "loss",
    "function",
    "equals",
    "nn",
    "dot",
    "cross",
    "entropy",
    "loss",
    "going",
    "set",
    "optimizer",
    "discussed",
    "torch",
    "dot",
    "dot",
    "sgd",
    "stochastic",
    "gradient",
    "descent",
    "parameters",
    "want",
    "optimize",
    "parameters",
    "model",
    "zero",
    "baseline",
    "model",
    "look",
    "random",
    "numbers",
    "like",
    "optimizer",
    "tweak",
    "way",
    "shape",
    "form",
    "better",
    "represent",
    "data",
    "going",
    "set",
    "learning",
    "rate",
    "much",
    "tweaked",
    "epoch",
    "going",
    "set",
    "nice",
    "high",
    "data",
    "set",
    "quite",
    "simple",
    "28",
    "28",
    "images",
    "work",
    "always",
    "adjust",
    "experiment",
    "experiment",
    "experiment",
    "let",
    "run",
    "got",
    "loss",
    "function",
    "going",
    "give",
    "doc",
    "string",
    "go",
    "calculates",
    "accuracy",
    "truth",
    "predictions",
    "doc",
    "string",
    "come",
    "well",
    "let",
    "look",
    "hope",
    "functions",
    "wrote",
    "good",
    "us",
    "writing",
    "good",
    "doc",
    "strings",
    "accuracy",
    "function",
    "well",
    "going",
    "test",
    "next",
    "video",
    "write",
    "training",
    "loop",
    "oh",
    "actually",
    "think",
    "might",
    "one",
    "function",
    "write",
    "training",
    "loop",
    "create",
    "function",
    "time",
    "experiments",
    "yeah",
    "let",
    "give",
    "go",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "downloaded",
    "helper",
    "script",
    "imported",
    "accuracy",
    "function",
    "made",
    "notebook",
    "two",
    "could",
    "really",
    "beef",
    "helper",
    "file",
    "could",
    "put",
    "lot",
    "different",
    "helper",
    "functions",
    "import",
    "rewrite",
    "something",
    "keep",
    "mind",
    "later",
    "let",
    "create",
    "function",
    "time",
    "experiments",
    "creating",
    "function",
    "time",
    "experiments",
    "one",
    "things",
    "machine",
    "learning",
    "experimental",
    "probably",
    "gathered",
    "far",
    "let",
    "write",
    "machine",
    "learning",
    "experimental",
    "two",
    "main",
    "things",
    "often",
    "want",
    "track",
    "one",
    "model",
    "performance",
    "loss",
    "accuracy",
    "values",
    "et",
    "cetera",
    "two",
    "fast",
    "runs",
    "usually",
    "want",
    "higher",
    "performance",
    "fast",
    "model",
    "ideal",
    "scenario",
    "however",
    "could",
    "imagine",
    "increase",
    "model",
    "performance",
    "might",
    "bigger",
    "neural",
    "network",
    "might",
    "layers",
    "might",
    "hidden",
    "units",
    "might",
    "degrade",
    "fast",
    "runs",
    "simply",
    "making",
    "calculations",
    "often",
    "two",
    "fast",
    "runs",
    "really",
    "important",
    "running",
    "model",
    "say",
    "internet",
    "say",
    "dedicated",
    "gpu",
    "say",
    "mobile",
    "device",
    "two",
    "things",
    "really",
    "keep",
    "mind",
    "tracking",
    "model",
    "performance",
    "loss",
    "value",
    "accuracy",
    "function",
    "let",
    "write",
    "code",
    "check",
    "fast",
    "runs",
    "purpose",
    "kept",
    "model",
    "cpu",
    "also",
    "going",
    "compare",
    "later",
    "fast",
    "model",
    "runs",
    "cpu",
    "versus",
    "fast",
    "runs",
    "gpu",
    "something",
    "coming",
    "let",
    "write",
    "function",
    "going",
    "use",
    "time",
    "module",
    "python",
    "time",
    "import",
    "default",
    "timer",
    "going",
    "call",
    "timer",
    "go",
    "python",
    "default",
    "timer",
    "get",
    "documentation",
    "go",
    "time",
    "default",
    "timer",
    "wonderful",
    "default",
    "timer",
    "always",
    "counter",
    "read",
    "python",
    "timing",
    "functions",
    "essentially",
    "going",
    "say",
    "hey",
    "exact",
    "time",
    "code",
    "started",
    "going",
    "create",
    "another",
    "stop",
    "code",
    "stopped",
    "going",
    "compare",
    "start",
    "stop",
    "times",
    "going",
    "basically",
    "long",
    "model",
    "took",
    "train",
    "going",
    "go",
    "def",
    "print",
    "train",
    "time",
    "going",
    "display",
    "function",
    "start",
    "going",
    "get",
    "float",
    "type",
    "hint",
    "way",
    "start",
    "end",
    "time",
    "essence",
    "function",
    "compare",
    "start",
    "end",
    "time",
    "going",
    "set",
    "torch",
    "device",
    "pass",
    "torch",
    "dot",
    "device",
    "going",
    "set",
    "default",
    "none",
    "want",
    "compare",
    "fast",
    "model",
    "runs",
    "different",
    "devices",
    "going",
    "write",
    "little",
    "doc",
    "string",
    "prints",
    "difference",
    "start",
    "end",
    "time",
    "course",
    "could",
    "add",
    "arguments",
    "quick",
    "one",
    "liner",
    "tell",
    "us",
    "function",
    "total",
    "time",
    "equals",
    "end",
    "minus",
    "start",
    "print",
    "going",
    "write",
    "train",
    "time",
    "whichever",
    "device",
    "using",
    "might",
    "cpu",
    "might",
    "gpu",
    "total",
    "time",
    "equals",
    "go",
    "three",
    "say",
    "seconds",
    "three",
    "decimal",
    "places",
    "return",
    "total",
    "time",
    "beautiful",
    "example",
    "could",
    "start",
    "time",
    "equals",
    "timer",
    "end",
    "time",
    "equals",
    "timer",
    "put",
    "code",
    "two",
    "go",
    "print",
    "train",
    "oh",
    "maybe",
    "need",
    "timer",
    "like",
    "find",
    "code",
    "know",
    "see",
    "works",
    "start",
    "time",
    "end",
    "equals",
    "end",
    "time",
    "device",
    "equals",
    "running",
    "cpu",
    "right",
    "cpu",
    "let",
    "see",
    "works",
    "wonderful",
    "small",
    "number",
    "train",
    "time",
    "cpu",
    "small",
    "number",
    "start",
    "time",
    "basically",
    "exact",
    "line",
    "comment",
    "basically",
    "takes",
    "time",
    "run",
    "end",
    "time",
    "get",
    "times",
    "10",
    "power",
    "negative",
    "five",
    "quite",
    "small",
    "number",
    "put",
    "modeling",
    "code",
    "going",
    "measure",
    "start",
    "time",
    "cell",
    "going",
    "model",
    "code",
    "end",
    "time",
    "find",
    "long",
    "model",
    "took",
    "train",
    "said",
    "think",
    "got",
    "pieces",
    "puzzle",
    "creating",
    "training",
    "testing",
    "functions",
    "got",
    "loss",
    "function",
    "got",
    "optimizer",
    "got",
    "valuation",
    "metric",
    "got",
    "timing",
    "function",
    "got",
    "model",
    "got",
    "data",
    "train",
    "first",
    "baseline",
    "computer",
    "vision",
    "model",
    "next",
    "video",
    "see",
    "good",
    "morning",
    "well",
    "might",
    "morning",
    "wherever",
    "world",
    "nice",
    "early",
    "recording",
    "videos",
    "lot",
    "momentum",
    "going",
    "look",
    "took",
    "little",
    "break",
    "last",
    "night",
    "runtime",
    "disconnected",
    "going",
    "happen",
    "using",
    "google",
    "colab",
    "since",
    "use",
    "google",
    "colab",
    "pro",
    "completely",
    "unnecessary",
    "course",
    "found",
    "worth",
    "much",
    "use",
    "google",
    "colab",
    "get",
    "longer",
    "idle",
    "timeouts",
    "means",
    "colab",
    "notebook",
    "stay",
    "persistent",
    "longer",
    "time",
    "course",
    "overnight",
    "going",
    "disconnect",
    "click",
    "reconnect",
    "want",
    "get",
    "back",
    "wherever",
    "downloaded",
    "data",
    "rerun",
    "cells",
    "nice",
    "shortcut",
    "might",
    "seen",
    "come",
    "code",
    "works",
    "oh",
    "go",
    "wrote",
    "notes",
    "let",
    "go",
    "run",
    "going",
    "run",
    "cells",
    "creating",
    "training",
    "loop",
    "training",
    "model",
    "batches",
    "data",
    "going",
    "little",
    "bit",
    "interesting",
    "wrote",
    "another",
    "reminder",
    "little",
    "bit",
    "behind",
    "scenes",
    "optimise",
    "update",
    "model",
    "parameters",
    "per",
    "batch",
    "rather",
    "per",
    "epoch",
    "let",
    "hold",
    "note",
    "make",
    "sure",
    "let",
    "know",
    "going",
    "make",
    "another",
    "title",
    "let",
    "go",
    "creating",
    "training",
    "loop",
    "training",
    "model",
    "batches",
    "data",
    "something",
    "little",
    "bit",
    "different",
    "may",
    "seen",
    "created",
    "batches",
    "data",
    "using",
    "data",
    "loader",
    "recall",
    "got",
    "something",
    "like",
    "1800",
    "go",
    "split",
    "data",
    "batches",
    "rather",
    "model",
    "looking",
    "images",
    "fashion",
    "mnist",
    "data",
    "one",
    "time",
    "going",
    "look",
    "1875",
    "batches",
    "32",
    "32",
    "images",
    "time",
    "training",
    "data",
    "set",
    "313",
    "batches",
    "32",
    "test",
    "data",
    "set",
    "let",
    "go",
    "training",
    "loop",
    "train",
    "first",
    "model",
    "going",
    "write",
    "steps",
    "actually",
    "little",
    "bit",
    "differently",
    "done",
    "one",
    "want",
    "loop",
    "epochs",
    "number",
    "epochs",
    "loop",
    "training",
    "batches",
    "way",
    "might",
    "able",
    "hear",
    "birds",
    "singing",
    "sun",
    "rise",
    "hope",
    "enjoy",
    "much",
    "going",
    "perform",
    "training",
    "steps",
    "going",
    "calculate",
    "calculate",
    "train",
    "loss",
    "per",
    "batch",
    "going",
    "one",
    "differences",
    "previous",
    "training",
    "loops",
    "going",
    "number",
    "two",
    "going",
    "loop",
    "testing",
    "batches",
    "train",
    "evaluate",
    "model",
    "step",
    "loop",
    "going",
    "perform",
    "testing",
    "steps",
    "going",
    "calculate",
    "test",
    "loss",
    "per",
    "batch",
    "well",
    "per",
    "batch",
    "wonderful",
    "four",
    "going",
    "course",
    "print",
    "happening",
    "may",
    "seen",
    "unofficial",
    "pytorch",
    "optimization",
    "loop",
    "theme",
    "song",
    "going",
    "time",
    "fun",
    "course",
    "timing",
    "function",
    "let",
    "get",
    "started",
    "fair",
    "steps",
    "nothing",
    "ca",
    "handle",
    "remember",
    "motto",
    "code",
    "well",
    "another",
    "one",
    "run",
    "code",
    "written",
    "code",
    "run",
    "yet",
    "going",
    "import",
    "tqdm",
    "progress",
    "bar",
    "seen",
    "tqdm",
    "good",
    "python",
    "progress",
    "bar",
    "add",
    "lines",
    "code",
    "github",
    "open",
    "source",
    "software",
    "one",
    "favorite",
    "pieces",
    "software",
    "going",
    "give",
    "us",
    "progress",
    "bar",
    "let",
    "us",
    "know",
    "many",
    "epochs",
    "training",
    "loop",
    "gone",
    "much",
    "overhead",
    "want",
    "learn",
    "please",
    "refer",
    "tqdm",
    "github",
    "however",
    "beautiful",
    "thing",
    "google",
    "colab",
    "tqdm",
    "built",
    "good",
    "popular",
    "going",
    "import",
    "different",
    "types",
    "tqdm",
    "progress",
    "going",
    "recognize",
    "compute",
    "environment",
    "using",
    "going",
    "give",
    "us",
    "best",
    "type",
    "progress",
    "bar",
    "example",
    "google",
    "colab",
    "running",
    "jupyter",
    "notebook",
    "behind",
    "scenes",
    "progress",
    "bar",
    "jupyter",
    "notebooks",
    "little",
    "bit",
    "different",
    "python",
    "scripts",
    "let",
    "set",
    "seed",
    "start",
    "timer",
    "want",
    "write",
    "training",
    "loop",
    "single",
    "cell",
    "starts",
    "run",
    "cell",
    "want",
    "timer",
    "start",
    "time",
    "long",
    "entire",
    "cell",
    "takes",
    "run",
    "go",
    "train",
    "time",
    "start",
    "cpu",
    "equals",
    "set",
    "timer",
    "beautiful",
    "going",
    "set",
    "number",
    "epochs",
    "going",
    "keep",
    "small",
    "faster",
    "training",
    "time",
    "run",
    "experiments",
    "keep",
    "small",
    "faster",
    "training",
    "time",
    "another",
    "little",
    "tidbit",
    "notice",
    "quickly",
    "cells",
    "ran",
    "well",
    "using",
    "relatively",
    "small",
    "data",
    "set",
    "beginning",
    "running",
    "experiments",
    "want",
    "run",
    "quite",
    "quickly",
    "run",
    "often",
    "learn",
    "data",
    "try",
    "different",
    "things",
    "try",
    "different",
    "models",
    "using",
    "number",
    "epochs",
    "equals",
    "three",
    "start",
    "three",
    "experiment",
    "runs",
    "30",
    "seconds",
    "minute",
    "way",
    "something",
    "work",
    "wasted",
    "much",
    "time",
    "waiting",
    "model",
    "train",
    "later",
    "could",
    "train",
    "100",
    "epochs",
    "wanted",
    "going",
    "create",
    "training",
    "test",
    "loop",
    "epoch",
    "tqdm",
    "range",
    "epochs",
    "let",
    "get",
    "going",
    "tqdm",
    "work",
    "wrap",
    "iterator",
    "tqdm",
    "see",
    "later",
    "tracks",
    "progress",
    "going",
    "put",
    "little",
    "print",
    "statement",
    "go",
    "epoch",
    "going",
    "say",
    "epoch",
    "go",
    "something",
    "like",
    "quite",
    "often",
    "put",
    "little",
    "print",
    "statements",
    "know",
    "going",
    "let",
    "set",
    "training",
    "going",
    "instantiate",
    "train",
    "loss",
    "going",
    "set",
    "zero",
    "begin",
    "going",
    "cumulatively",
    "add",
    "values",
    "train",
    "loss",
    "see",
    "later",
    "accumulates",
    "calculate",
    "training",
    "loss",
    "per",
    "batch",
    "let",
    "calculate",
    "train",
    "loss",
    "per",
    "batch",
    "finally",
    "end",
    "loop",
    "divide",
    "training",
    "loss",
    "number",
    "batches",
    "get",
    "average",
    "training",
    "loss",
    "per",
    "batch",
    "give",
    "us",
    "training",
    "loss",
    "per",
    "epoch",
    "lot",
    "talking",
    "make",
    "sense",
    "remember",
    "code",
    "add",
    "loop",
    "loop",
    "training",
    "batches",
    "data",
    "batchified",
    "got",
    "crow",
    "maybe",
    "cooker",
    "bar",
    "sitting",
    "roof",
    "across",
    "apartment",
    "singing",
    "song",
    "morning",
    "lovely",
    "going",
    "loop",
    "training",
    "batch",
    "data",
    "got",
    "four",
    "batch",
    "comma",
    "x",
    "remember",
    "training",
    "batches",
    "come",
    "form",
    "data",
    "images",
    "label",
    "could",
    "call",
    "image",
    "label",
    "target",
    "part",
    "would",
    "convention",
    "often",
    "call",
    "features",
    "x",
    "labels",
    "seen",
    "going",
    "enumerate",
    "train",
    "data",
    "loader",
    "well",
    "keep",
    "track",
    "number",
    "batches",
    "give",
    "us",
    "batch",
    "going",
    "set",
    "model",
    "zero",
    "training",
    "mode",
    "even",
    "though",
    "default",
    "want",
    "make",
    "sure",
    "training",
    "mode",
    "going",
    "forward",
    "pass",
    "remember",
    "steps",
    "apply",
    "optimization",
    "loop",
    "forward",
    "pass",
    "calculate",
    "loss",
    "minus",
    "zero",
    "grad",
    "last",
    "backwards",
    "minus",
    "step",
    "step",
    "step",
    "let",
    "hey",
    "model",
    "zero",
    "put",
    "features",
    "going",
    "calculate",
    "loss",
    "steps",
    "going",
    "spend",
    "much",
    "time",
    "exact",
    "steps",
    "going",
    "practice",
    "writing",
    "course",
    "later",
    "might",
    "thinking",
    "come",
    "functionalized",
    "training",
    "loop",
    "already",
    "seemed",
    "write",
    "generic",
    "code",
    "well",
    "like",
    "practice",
    "writing",
    "pytorch",
    "code",
    "right",
    "going",
    "functionalize",
    "later",
    "worry",
    "another",
    "little",
    "step",
    "done",
    "training",
    "loss",
    "set",
    "zero",
    "begin",
    "going",
    "accumulate",
    "training",
    "loss",
    "values",
    "every",
    "batch",
    "going",
    "add",
    "later",
    "going",
    "divide",
    "total",
    "number",
    "batches",
    "get",
    "average",
    "loss",
    "per",
    "batch",
    "see",
    "loss",
    "calculation",
    "within",
    "batch",
    "loop",
    "means",
    "one",
    "batch",
    "data",
    "going",
    "go",
    "model",
    "going",
    "calculate",
    "loss",
    "one",
    "batch",
    "data",
    "loop",
    "going",
    "continue",
    "batches",
    "train",
    "data",
    "loader",
    "1875",
    "steps",
    "whatever",
    "accumulate",
    "train",
    "loss",
    "going",
    "optimize",
    "zero",
    "grad",
    "optimizer",
    "dot",
    "zero",
    "grad",
    "number",
    "four",
    "loss",
    "backward",
    "loss",
    "backward",
    "back",
    "propagation",
    "step",
    "finally",
    "got",
    "number",
    "five",
    "optimizer",
    "step",
    "left",
    "little",
    "note",
    "remind",
    "also",
    "let",
    "know",
    "highlight",
    "optimizer",
    "update",
    "model",
    "parameters",
    "per",
    "batch",
    "rather",
    "per",
    "epoch",
    "see",
    "got",
    "loop",
    "inside",
    "epoch",
    "loop",
    "batch",
    "loop",
    "meant",
    "optimizer",
    "one",
    "advantages",
    "using",
    "mini",
    "batches",
    "memory",
    "efficient",
    "loading",
    "images",
    "memory",
    "time",
    "updating",
    "model",
    "parameters",
    "per",
    "batch",
    "rather",
    "waiting",
    "see",
    "whole",
    "data",
    "set",
    "every",
    "batch",
    "model",
    "hopefully",
    "getting",
    "slightly",
    "better",
    "optimizer",
    "dot",
    "step",
    "call",
    "within",
    "batch",
    "loop",
    "rather",
    "epoch",
    "loop",
    "let",
    "print",
    "happening",
    "print",
    "happening",
    "batch",
    "let",
    "every",
    "400",
    "batches",
    "lot",
    "batches",
    "want",
    "print",
    "often",
    "otherwise",
    "fill",
    "screen",
    "numbers",
    "might",
    "bad",
    "thing",
    "400",
    "seems",
    "good",
    "number",
    "five",
    "printouts",
    "2000",
    "batches",
    "print",
    "looked",
    "course",
    "adjust",
    "whatever",
    "would",
    "like",
    "flexibility",
    "pytorch",
    "flexibility",
    "python",
    "well",
    "looked",
    "many",
    "samples",
    "looked",
    "going",
    "take",
    "batch",
    "number",
    "multiply",
    "x",
    "length",
    "x",
    "going",
    "32",
    "batch",
    "size",
    "going",
    "write",
    "total",
    "number",
    "items",
    "got",
    "data",
    "set",
    "access",
    "going",
    "train",
    "data",
    "loader",
    "dot",
    "data",
    "set",
    "going",
    "give",
    "us",
    "length",
    "data",
    "set",
    "contained",
    "within",
    "train",
    "data",
    "loader",
    "might",
    "able",
    "guess",
    "accumulating",
    "train",
    "loss",
    "going",
    "quite",
    "high",
    "adding",
    "every",
    "single",
    "time",
    "calculated",
    "loss",
    "adding",
    "train",
    "loss",
    "overall",
    "value",
    "per",
    "batch",
    "let",
    "adjust",
    "wanted",
    "find",
    "see",
    "got",
    "line",
    "outside",
    "batch",
    "loop",
    "want",
    "adjust",
    "training",
    "loss",
    "get",
    "average",
    "training",
    "loss",
    "per",
    "batch",
    "per",
    "epoch",
    "coming",
    "back",
    "epoch",
    "loop",
    "little",
    "bit",
    "confusing",
    "line",
    "loops",
    "going",
    "help",
    "figure",
    "context",
    "computing",
    "epoch",
    "loop",
    "divide",
    "total",
    "train",
    "loss",
    "length",
    "train",
    "data",
    "loader",
    "oh",
    "exciting",
    "training",
    "biggest",
    "model",
    "yet",
    "train",
    "loss",
    "equals",
    "divide",
    "equals",
    "going",
    "reassign",
    "train",
    "loss",
    "going",
    "divide",
    "length",
    "train",
    "data",
    "loader",
    "well",
    "accumulated",
    "train",
    "loss",
    "every",
    "batch",
    "train",
    "data",
    "loader",
    "want",
    "average",
    "across",
    "many",
    "batches",
    "train",
    "data",
    "loader",
    "value",
    "quite",
    "high",
    "readjust",
    "find",
    "average",
    "loss",
    "per",
    "epoch",
    "epoch",
    "loop",
    "right",
    "steps",
    "going",
    "right",
    "figure",
    "happening",
    "minute",
    "let",
    "code",
    "testing",
    "loop",
    "testing",
    "testing",
    "well",
    "let",
    "set",
    "test",
    "loss",
    "variable",
    "accuracy",
    "testing",
    "well",
    "accuracy",
    "training",
    "accuracy",
    "training",
    "right",
    "stick",
    "accuracy",
    "testing",
    "go",
    "model",
    "zero",
    "dot",
    "eval",
    "put",
    "evaluation",
    "mode",
    "turn",
    "inference",
    "mode",
    "context",
    "manager",
    "torch",
    "dot",
    "inference",
    "mode",
    "thing",
    "x",
    "test",
    "data",
    "loader",
    "need",
    "keep",
    "track",
    "batches",
    "test",
    "data",
    "loader",
    "loop",
    "x",
    "features",
    "images",
    "labels",
    "test",
    "data",
    "loader",
    "going",
    "forward",
    "pass",
    "test",
    "loop",
    "optimization",
    "step",
    "passing",
    "data",
    "model",
    "evaluating",
    "patterns",
    "learned",
    "training",
    "data",
    "going",
    "pass",
    "x",
    "might",
    "little",
    "bit",
    "confusing",
    "let",
    "x",
    "test",
    "test",
    "way",
    "get",
    "confused",
    "x",
    "training",
    "set",
    "going",
    "calculate",
    "loss",
    "cum",
    "relatively",
    "might",
    "small",
    "wrong",
    "app",
    "sound",
    "got",
    "test",
    "loss",
    "variable",
    "assigned",
    "zero",
    "going",
    "test",
    "loss",
    "plus",
    "equals",
    "one",
    "step",
    "test",
    "spread",
    "test",
    "comparing",
    "test",
    "prediction",
    "test",
    "labels",
    "test",
    "labels",
    "going",
    "back",
    "loop",
    "forward",
    "pass",
    "calculate",
    "loss",
    "test",
    "data",
    "set",
    "oh",
    "said",
    "going",
    "calculate",
    "accuracy",
    "silly",
    "calculate",
    "accuracy",
    "let",
    "go",
    "test",
    "act",
    "got",
    "plus",
    "equals",
    "bring",
    "accuracy",
    "function",
    "downloaded",
    "helper",
    "functions",
    "dot",
    "pi",
    "true",
    "equals",
    "test",
    "pred",
    "equals",
    "test",
    "pred",
    "dot",
    "arg",
    "max",
    "dim",
    "equals",
    "one",
    "well",
    "recall",
    "outputs",
    "model",
    "raw",
    "outputs",
    "model",
    "going",
    "logits",
    "accuracy",
    "function",
    "expects",
    "true",
    "labels",
    "predictions",
    "format",
    "test",
    "pred",
    "logits",
    "call",
    "arg",
    "max",
    "find",
    "logit",
    "value",
    "highest",
    "index",
    "prediction",
    "label",
    "comparing",
    "labels",
    "labels",
    "arg",
    "max",
    "back",
    "batch",
    "loop",
    "going",
    "calculate",
    "cal",
    "queue",
    "length",
    "test",
    "loss",
    "average",
    "per",
    "batch",
    "let",
    "go",
    "test",
    "loss",
    "divide",
    "equals",
    "length",
    "test",
    "data",
    "loader",
    "context",
    "loop",
    "batch",
    "loop",
    "test",
    "lost",
    "test",
    "accuracy",
    "values",
    "per",
    "batch",
    "accumulated",
    "every",
    "single",
    "batch",
    "dividing",
    "many",
    "batches",
    "test",
    "data",
    "loader",
    "thing",
    "accuracy",
    "calculate",
    "ack",
    "test",
    "ack",
    "average",
    "per",
    "batch",
    "giving",
    "us",
    "test",
    "loss",
    "test",
    "accuracy",
    "per",
    "epoch",
    "test",
    "ack",
    "divided",
    "equals",
    "length",
    "test",
    "data",
    "loader",
    "wonderful",
    "close",
    "finishing",
    "come",
    "back",
    "epoch",
    "loop",
    "lines",
    "helpful",
    "google",
    "colab",
    "scroll",
    "believe",
    "want",
    "go",
    "settings",
    "something",
    "like",
    "yeah",
    "settings",
    "get",
    "lines",
    "print",
    "happening",
    "going",
    "print",
    "f",
    "equals",
    "n",
    "let",
    "get",
    "train",
    "loss",
    "ten",
    "loss",
    "print",
    "four",
    "decimal",
    "places",
    "get",
    "test",
    "loss",
    "course",
    "test",
    "loss",
    "go",
    "get",
    "four",
    "decimal",
    "places",
    "well",
    "get",
    "test",
    "ack",
    "test",
    "accuracy",
    "get",
    "four",
    "decimal",
    "places",
    "well",
    "f",
    "wonderful",
    "finally",
    "one",
    "step",
    "ooh",
    "written",
    "lot",
    "code",
    "video",
    "want",
    "calculate",
    "training",
    "time",
    "another",
    "thing",
    "want",
    "track",
    "want",
    "see",
    "long",
    "model",
    "taken",
    "train",
    "train",
    "time",
    "end",
    "cpu",
    "going",
    "equal",
    "timer",
    "going",
    "get",
    "total",
    "train",
    "time",
    "model",
    "zero",
    "set",
    "variable",
    "compare",
    "modeling",
    "experiments",
    "later",
    "going",
    "go",
    "print",
    "train",
    "time",
    "start",
    "equals",
    "train",
    "time",
    "start",
    "cpu",
    "equals",
    "train",
    "time",
    "end",
    "cpu",
    "finally",
    "device",
    "going",
    "string",
    "next",
    "model",
    "zero",
    "dot",
    "parameters",
    "one",
    "way",
    "checking",
    "model",
    "zero",
    "parameters",
    "live",
    "beautiful",
    "right",
    "got",
    "enough",
    "brackets",
    "think",
    "okay",
    "go",
    "whoo",
    "show",
    "output",
    "next",
    "model",
    "zero",
    "dot",
    "parameters",
    "give",
    "us",
    "oh",
    "go",
    "device",
    "oh",
    "model",
    "zero",
    "dot",
    "parameters",
    "thought",
    "little",
    "trick",
    "go",
    "next",
    "parameter",
    "containing",
    "thought",
    "could",
    "get",
    "device",
    "oh",
    "go",
    "excuse",
    "get",
    "get",
    "device",
    "let",
    "turn",
    "output",
    "going",
    "cpu",
    "troubleshooting",
    "fly",
    "hopefully",
    "code",
    "works",
    "went",
    "steps",
    "looping",
    "epochs",
    "top",
    "level",
    "looped",
    "training",
    "batches",
    "performed",
    "training",
    "steps",
    "training",
    "loop",
    "forward",
    "pass",
    "loss",
    "calculation",
    "optimizer",
    "zero",
    "grad",
    "loss",
    "backwards",
    "calculate",
    "loss",
    "per",
    "batch",
    "accumulate",
    "testing",
    "batches",
    "except",
    "without",
    "optimizer",
    "steps",
    "print",
    "happening",
    "time",
    "fun",
    "fair",
    "bit",
    "going",
    "think",
    "errors",
    "give",
    "go",
    "run",
    "code",
    "going",
    "leave",
    "one",
    "cliffhanger",
    "going",
    "see",
    "works",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "pretty",
    "full",
    "fair",
    "steps",
    "good",
    "practice",
    "best",
    "way",
    "learn",
    "pytorch",
    "code",
    "write",
    "pytorch",
    "code",
    "try",
    "run",
    "code",
    "work",
    "probably",
    "error",
    "somewhere",
    "well",
    "let",
    "find",
    "together",
    "ready",
    "let",
    "train",
    "biggest",
    "model",
    "yet",
    "three",
    "two",
    "one",
    "bomb",
    "oh",
    "course",
    "going",
    "indentation",
    "error",
    "ah",
    "classic",
    "print",
    "happening",
    "indent",
    "oh",
    "line",
    "needs",
    "excuse",
    "okay",
    "line",
    "strange",
    "enter",
    "get",
    "one",
    "sure",
    "face",
    "like",
    "sometimes",
    "write",
    "beautiful",
    "code",
    "work",
    "main",
    "error",
    "entire",
    "code",
    "single",
    "space",
    "sure",
    "happened",
    "going",
    "pull",
    "line",
    "could",
    "done",
    "selecting",
    "going",
    "line",
    "line",
    "make",
    "sure",
    "everything",
    "right",
    "order",
    "beautiful",
    "print",
    "happening",
    "three",
    "two",
    "one",
    "round",
    "two",
    "going",
    "okay",
    "progress",
    "bar",
    "talking",
    "look",
    "beautiful",
    "oh",
    "going",
    "quite",
    "quickly",
    "samples",
    "need",
    "talk",
    "faster",
    "oh",
    "go",
    "got",
    "good",
    "results",
    "got",
    "tests",
    "train",
    "loss",
    "test",
    "loss",
    "test",
    "accuracy",
    "pretty",
    "darn",
    "good",
    "oh",
    "goodness",
    "good",
    "baseline",
    "already",
    "67",
    "showing",
    "us",
    "seven",
    "seconds",
    "per",
    "iteration",
    "remember",
    "tqdm",
    "tracking",
    "many",
    "epochs",
    "going",
    "three",
    "epochs",
    "print",
    "statement",
    "saying",
    "hey",
    "looked",
    "zero",
    "samples",
    "looked",
    "samples",
    "finished",
    "epoch",
    "two",
    "zero",
    "indexed",
    "train",
    "loss",
    "test",
    "loss",
    "476",
    "test",
    "accuracy",
    "834265",
    "training",
    "time",
    "21",
    "seconds",
    "keep",
    "mind",
    "numbers",
    "may",
    "exact",
    "mine",
    "realm",
    "mine",
    "due",
    "inherent",
    "randomness",
    "machine",
    "learning",
    "even",
    "set",
    "manual",
    "seed",
    "might",
    "slightly",
    "different",
    "worry",
    "much",
    "mean",
    "realm",
    "accuracy",
    "25",
    "rather",
    "83",
    "well",
    "probably",
    "something",
    "wrong",
    "well",
    "bad",
    "train",
    "time",
    "cpu",
    "heavily",
    "dependent",
    "long",
    "takes",
    "train",
    "heavily",
    "dependent",
    "hardware",
    "using",
    "behind",
    "scenes",
    "using",
    "google",
    "colab",
    "pro",
    "may",
    "mean",
    "get",
    "faster",
    "cpu",
    "free",
    "version",
    "google",
    "colab",
    "also",
    "depends",
    "cpu",
    "available",
    "google",
    "computer",
    "warehouse",
    "google",
    "colab",
    "hosting",
    "fast",
    "keep",
    "mind",
    "time",
    "10",
    "times",
    "probably",
    "something",
    "wrong",
    "time",
    "10",
    "times",
    "less",
    "well",
    "hey",
    "keep",
    "using",
    "hardware",
    "pretty",
    "darn",
    "good",
    "let",
    "keep",
    "pushing",
    "forward",
    "baseline",
    "try",
    "improve",
    "upon",
    "accuracy",
    "train",
    "time",
    "20",
    "seconds",
    "see",
    "model",
    "gpu",
    "later",
    "also",
    "later",
    "convolutional",
    "neural",
    "network",
    "let",
    "evaluate",
    "model",
    "built",
    "training",
    "loop",
    "done",
    "fair",
    "bit",
    "code",
    "fit",
    "model",
    "data",
    "make",
    "prediction",
    "let",
    "two",
    "combined",
    "hey",
    "evaluate",
    "model",
    "come",
    "back",
    "number",
    "four",
    "make",
    "predictions",
    "get",
    "model",
    "zero",
    "results",
    "going",
    "create",
    "function",
    "want",
    "build",
    "multiple",
    "models",
    "way",
    "say",
    "model",
    "0123",
    "pass",
    "function",
    "evaluate",
    "model",
    "compare",
    "results",
    "later",
    "something",
    "keep",
    "mind",
    "going",
    "writing",
    "bunch",
    "code",
    "multiple",
    "times",
    "probably",
    "want",
    "functionize",
    "could",
    "definitely",
    "training",
    "last",
    "loops",
    "see",
    "later",
    "let",
    "go",
    "deaf",
    "model",
    "evaluate",
    "given",
    "model",
    "pass",
    "model",
    "torch",
    "dot",
    "nn",
    "dot",
    "module",
    "type",
    "pass",
    "data",
    "loader",
    "type",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "loader",
    "pass",
    "loss",
    "function",
    "calculate",
    "loss",
    "could",
    "pass",
    "evaluation",
    "metric",
    "wanted",
    "track",
    "torch",
    "nn",
    "dot",
    "module",
    "well",
    "oh",
    "go",
    "speaking",
    "evaluation",
    "function",
    "let",
    "pass",
    "accuracy",
    "function",
    "well",
    "want",
    "l",
    "want",
    "want",
    "return",
    "dictionary",
    "containing",
    "results",
    "model",
    "predicting",
    "data",
    "loader",
    "want",
    "going",
    "return",
    "dictionary",
    "model",
    "results",
    "way",
    "could",
    "call",
    "function",
    "multiple",
    "times",
    "different",
    "models",
    "different",
    "data",
    "loaders",
    "compare",
    "dictionaries",
    "full",
    "results",
    "depending",
    "model",
    "passed",
    "let",
    "set",
    "loss",
    "accuracy",
    "equals",
    "zero",
    "zero",
    "start",
    "go",
    "going",
    "much",
    "testing",
    "loop",
    "except",
    "going",
    "functionalized",
    "going",
    "return",
    "dictionary",
    "turn",
    "context",
    "manager",
    "inferencing",
    "torch",
    "dot",
    "inference",
    "mode",
    "going",
    "loop",
    "data",
    "loader",
    "get",
    "x",
    "values",
    "x",
    "data",
    "ideal",
    "labels",
    "make",
    "predictions",
    "model",
    "words",
    "forward",
    "pass",
    "go",
    "pred",
    "equals",
    "model",
    "specify",
    "model",
    "got",
    "model",
    "parameter",
    "starting",
    "make",
    "functions",
    "function",
    "generalizable",
    "could",
    "used",
    "almost",
    "model",
    "data",
    "loader",
    "want",
    "accumulate",
    "loss",
    "accuracy",
    "values",
    "per",
    "batch",
    "within",
    "batch",
    "loop",
    "per",
    "batch",
    "going",
    "go",
    "loss",
    "plus",
    "equals",
    "loss",
    "function",
    "pass",
    "pred",
    "true",
    "label",
    "accuracy",
    "except",
    "time",
    "use",
    "accuracy",
    "function",
    "send",
    "true",
    "equals",
    "pred",
    "equals",
    "pred",
    "dot",
    "argmax",
    "raw",
    "outputs",
    "model",
    "logits",
    "want",
    "convert",
    "labels",
    "could",
    "take",
    "softmax",
    "prediction",
    "probabilities",
    "could",
    "also",
    "take",
    "argmax",
    "skipping",
    "softmax",
    "step",
    "argmax",
    "get",
    "index",
    "highest",
    "value",
    "load",
    "dim",
    "equals",
    "one",
    "going",
    "make",
    "sure",
    "still",
    "within",
    "context",
    "manager",
    "torch",
    "inference",
    "mode",
    "outside",
    "loop",
    "line",
    "going",
    "scale",
    "loss",
    "act",
    "find",
    "average",
    "loss",
    "slash",
    "act",
    "per",
    "batch",
    "loss",
    "divide",
    "assign",
    "length",
    "data",
    "loader",
    "divide",
    "reassign",
    "however",
    "many",
    "batches",
    "data",
    "loader",
    "pass",
    "model",
    "function",
    "thing",
    "accuracy",
    "length",
    "data",
    "loader",
    "beautiful",
    "going",
    "return",
    "dictionary",
    "return",
    "return",
    "model",
    "name",
    "inspecting",
    "model",
    "get",
    "attribute",
    "model",
    "class",
    "name",
    "show",
    "helpful",
    "track",
    "created",
    "multiple",
    "different",
    "models",
    "given",
    "different",
    "class",
    "names",
    "access",
    "name",
    "attribute",
    "works",
    "model",
    "created",
    "class",
    "ensure",
    "models",
    "different",
    "class",
    "names",
    "want",
    "like",
    "going",
    "like",
    "set",
    "model",
    "name",
    "class",
    "name",
    "get",
    "model",
    "loss",
    "value",
    "scaled",
    "turn",
    "single",
    "value",
    "taking",
    "dot",
    "item",
    "go",
    "model",
    "dot",
    "act",
    "get",
    "model",
    "underscore",
    "act",
    "models",
    "accuracy",
    "thing",
    "act",
    "think",
    "need",
    "take",
    "item",
    "accuracy",
    "comes",
    "back",
    "different",
    "form",
    "find",
    "doubt",
    "code",
    "calculate",
    "model",
    "zero",
    "results",
    "test",
    "data",
    "set",
    "want",
    "let",
    "know",
    "create",
    "functions",
    "almost",
    "whatever",
    "want",
    "decided",
    "going",
    "helpful",
    "models",
    "data",
    "building",
    "keep",
    "mind",
    "models",
    "data",
    "sets",
    "might",
    "different",
    "likely",
    "different",
    "future",
    "create",
    "functions",
    "whatever",
    "use",
    "case",
    "need",
    "model",
    "zero",
    "results",
    "equals",
    "vowel",
    "model",
    "going",
    "call",
    "function",
    "created",
    "model",
    "going",
    "equal",
    "model",
    "zero",
    "data",
    "loader",
    "going",
    "equal",
    "test",
    "data",
    "loader",
    "course",
    "want",
    "evaluate",
    "test",
    "data",
    "set",
    "going",
    "send",
    "loss",
    "function",
    "loss",
    "function",
    "assigned",
    "training",
    "loop",
    "come",
    "loss",
    "function",
    "go",
    "back",
    "accuracy",
    "function",
    "equal",
    "accuracy",
    "function",
    "pass",
    "another",
    "function",
    "beautiful",
    "let",
    "see",
    "works",
    "model",
    "zero",
    "results",
    "see",
    "typos",
    "likely",
    "errors",
    "code",
    "think",
    "model",
    "well",
    "let",
    "find",
    "oh",
    "go",
    "got",
    "model",
    "accuracy",
    "see",
    "could",
    "reuse",
    "dictionary",
    "later",
    "model",
    "one",
    "results",
    "model",
    "two",
    "results",
    "could",
    "use",
    "dictionaries",
    "compare",
    "together",
    "got",
    "model",
    "name",
    "version",
    "zero",
    "model",
    "accuracy",
    "loss",
    "test",
    "data",
    "loader",
    "numbers",
    "may",
    "slightly",
    "different",
    "realm",
    "exact",
    "worry",
    "much",
    "20",
    "accuracy",
    "points",
    "less",
    "loss",
    "10",
    "times",
    "higher",
    "probably",
    "go",
    "back",
    "code",
    "check",
    "something",
    "wrong",
    "believe",
    "wanted",
    "progress",
    "bar",
    "could",
    "tqdm",
    "let",
    "look",
    "eh",
    "oh",
    "look",
    "progress",
    "bar",
    "nice",
    "nice",
    "quick",
    "313",
    "batches",
    "goes",
    "quite",
    "quick",
    "next",
    "well",
    "built",
    "model",
    "one",
    "got",
    "model",
    "zero",
    "sorry",
    "getting",
    "ahead",
    "got",
    "baseline",
    "got",
    "way",
    "evaluate",
    "model",
    "workflow",
    "say",
    "got",
    "data",
    "ready",
    "done",
    "picked",
    "built",
    "model",
    "picked",
    "loss",
    "function",
    "built",
    "optimizer",
    "created",
    "training",
    "loop",
    "fit",
    "model",
    "data",
    "made",
    "prediction",
    "evaluated",
    "model",
    "using",
    "loss",
    "accuracy",
    "could",
    "evaluate",
    "making",
    "predictions",
    "save",
    "later",
    "visualizing",
    "predictions",
    "think",
    "improving",
    "experimentation",
    "let",
    "give",
    "go",
    "hey",
    "recall",
    "trained",
    "model",
    "zero",
    "cpu",
    "build",
    "model",
    "one",
    "start",
    "train",
    "gpu",
    "next",
    "section",
    "let",
    "create",
    "number",
    "five",
    "set",
    "device",
    "agnostic",
    "code",
    "done",
    "one",
    "together",
    "using",
    "gpu",
    "one",
    "challenge",
    "next",
    "video",
    "set",
    "device",
    "agnostic",
    "code",
    "might",
    "go",
    "colab",
    "got",
    "gpu",
    "active",
    "change",
    "runtime",
    "type",
    "gpu",
    "might",
    "restart",
    "runtime",
    "might",
    "rerun",
    "cells",
    "get",
    "helper",
    "functions",
    "file",
    "back",
    "data",
    "whatnot",
    "set",
    "device",
    "agnostic",
    "code",
    "see",
    "next",
    "video",
    "go",
    "give",
    "shot",
    "set",
    "device",
    "agnostic",
    "code",
    "hope",
    "gave",
    "go",
    "let",
    "together",
    "wo",
    "take",
    "long",
    "last",
    "two",
    "videos",
    "quite",
    "long",
    "wanted",
    "set",
    "device",
    "agnostic",
    "code",
    "want",
    "see",
    "gpu",
    "available",
    "check",
    "video",
    "smi",
    "fails",
    "activated",
    "gpu",
    "colab",
    "yet",
    "also",
    "check",
    "torch",
    "cuda",
    "available",
    "pytorch",
    "check",
    "gpu",
    "available",
    "cuda",
    "let",
    "fix",
    "two",
    "want",
    "start",
    "using",
    "gpu",
    "want",
    "set",
    "device",
    "agnostic",
    "code",
    "matter",
    "hardware",
    "system",
    "running",
    "pytorch",
    "leverages",
    "going",
    "select",
    "gpu",
    "going",
    "click",
    "save",
    "notice",
    "google",
    "colab",
    "notebook",
    "start",
    "reset",
    "start",
    "connect",
    "go",
    "got",
    "gpu",
    "back",
    "end",
    "python",
    "three",
    "google",
    "compute",
    "engine",
    "back",
    "end",
    "gpu",
    "reset",
    "nvidia",
    "smi",
    "wonderful",
    "tesla",
    "t4",
    "gpu",
    "16",
    "gigabytes",
    "memory",
    "wonderful",
    "gpu",
    "available",
    "oh",
    "torch",
    "defined",
    "well",
    "notice",
    "numbers",
    "cells",
    "one",
    "two",
    "means",
    "reset",
    "runtime",
    "gpu",
    "rerun",
    "cells",
    "go",
    "run",
    "going",
    "run",
    "cells",
    "make",
    "sure",
    "download",
    "data",
    "make",
    "sure",
    "download",
    "helper",
    "functions",
    "file",
    "go",
    "back",
    "see",
    "data",
    "may",
    "downloading",
    "take",
    "long",
    "another",
    "advantage",
    "using",
    "relatively",
    "small",
    "data",
    "set",
    "already",
    "saved",
    "pytorch",
    "data",
    "sets",
    "keep",
    "mind",
    "use",
    "larger",
    "data",
    "set",
    "google",
    "colab",
    "may",
    "take",
    "run",
    "build",
    "bigger",
    "models",
    "may",
    "take",
    "run",
    "keep",
    "mind",
    "experiments",
    "going",
    "forward",
    "start",
    "small",
    "increase",
    "necessary",
    "finally",
    "going",
    "oh",
    "go",
    "got",
    "gpu",
    "wonderful",
    "write",
    "code",
    "set",
    "code",
    "realistically",
    "quite",
    "often",
    "start",
    "every",
    "notebook",
    "wanted",
    "highlight",
    "might",
    "middle",
    "wanted",
    "practice",
    "running",
    "model",
    "cpu",
    "stepping",
    "things",
    "going",
    "gpu",
    "device",
    "equals",
    "cuda",
    "code",
    "torch",
    "dot",
    "cuda",
    "available",
    "looks",
    "like",
    "going",
    "return",
    "true",
    "else",
    "use",
    "cpu",
    "going",
    "check",
    "device",
    "wonderful",
    "cuda",
    "got",
    "code",
    "ready",
    "go",
    "think",
    "time",
    "built",
    "another",
    "model",
    "asked",
    "question",
    "think",
    "data",
    "set",
    "working",
    "requires",
    "nonlinearity",
    "shirts",
    "bags",
    "shoes",
    "need",
    "nonlinear",
    "functions",
    "model",
    "well",
    "looks",
    "like",
    "baseline",
    "model",
    "without",
    "nonlinearities",
    "pretty",
    "well",
    "modeling",
    "data",
    "got",
    "pretty",
    "good",
    "test",
    "accuracy",
    "value",
    "83",
    "100",
    "images",
    "predicts",
    "right",
    "one",
    "83",
    "time",
    "83",
    "times",
    "100",
    "pretty",
    "well",
    "without",
    "nonlinearities",
    "try",
    "model",
    "uses",
    "nonlinearities",
    "runs",
    "gpu",
    "might",
    "want",
    "give",
    "go",
    "see",
    "create",
    "model",
    "nonlinear",
    "functions",
    "try",
    "run",
    "gpu",
    "see",
    "goes",
    "otherwise",
    "together",
    "next",
    "video",
    "see",
    "hello",
    "everyone",
    "welcome",
    "back",
    "making",
    "terrific",
    "progress",
    "let",
    "see",
    "far",
    "come",
    "got",
    "data",
    "set",
    "prepared",
    "data",
    "loaders",
    "built",
    "baseline",
    "model",
    "trained",
    "evaluated",
    "time",
    "oh",
    "last",
    "video",
    "set",
    "device",
    "diagnostic",
    "code",
    "little",
    "framework",
    "improving",
    "experimentation",
    "quite",
    "often",
    "building",
    "different",
    "model",
    "trying",
    "could",
    "using",
    "data",
    "could",
    "tweaking",
    "whole",
    "bunch",
    "different",
    "things",
    "let",
    "get",
    "coding",
    "going",
    "write",
    "model",
    "one",
    "believe",
    "section",
    "six",
    "model",
    "one",
    "going",
    "building",
    "better",
    "model",
    "nonlinearity",
    "asked",
    "challenge",
    "last",
    "video",
    "give",
    "go",
    "try",
    "build",
    "model",
    "nonlinearity",
    "hope",
    "gave",
    "go",
    "anything",
    "course",
    "trying",
    "impart",
    "course",
    "give",
    "things",
    "go",
    "try",
    "things",
    "machine",
    "learning",
    "coding",
    "trying",
    "things",
    "giving",
    "go",
    "let",
    "write",
    "learned",
    "power",
    "nonlinearity",
    "notebook",
    "o2",
    "go",
    "book",
    "go",
    "section",
    "number",
    "two",
    "wait",
    "load",
    "come",
    "search",
    "nonlinearity",
    "missing",
    "piece",
    "nonlinearity",
    "going",
    "get",
    "copy",
    "want",
    "see",
    "nonlinearity",
    "helps",
    "us",
    "helps",
    "us",
    "model",
    "nonlinear",
    "data",
    "case",
    "circle",
    "model",
    "straight",
    "lines",
    "words",
    "linear",
    "lines",
    "linear",
    "means",
    "straight",
    "nonlinear",
    "means",
    "learned",
    "power",
    "linear",
    "nonlinear",
    "functions",
    "neural",
    "networks",
    "model",
    "almost",
    "kind",
    "data",
    "pair",
    "right",
    "way",
    "go",
    "back",
    "read",
    "prefer",
    "code",
    "things",
    "try",
    "data",
    "let",
    "create",
    "model",
    "nonlinear",
    "linear",
    "layers",
    "also",
    "saw",
    "model",
    "linear",
    "layers",
    "model",
    "data",
    "performing",
    "quite",
    "well",
    "experimentation",
    "side",
    "things",
    "come",
    "play",
    "sometimes",
    "wo",
    "know",
    "model",
    "whether",
    "work",
    "wo",
    "work",
    "data",
    "set",
    "try",
    "different",
    "things",
    "come",
    "look",
    "data",
    "hmm",
    "looks",
    "actually",
    "quite",
    "linear",
    "bag",
    "like",
    "straight",
    "lines",
    "could",
    "maybe",
    "model",
    "straight",
    "lines",
    "things",
    "could",
    "potentially",
    "classify",
    "nonlinear",
    "hard",
    "tell",
    "without",
    "knowing",
    "let",
    "give",
    "go",
    "let",
    "write",
    "nonlinear",
    "model",
    "going",
    "quite",
    "similar",
    "model",
    "zero",
    "except",
    "going",
    "interspurse",
    "relu",
    "layers",
    "linear",
    "layers",
    "recall",
    "relu",
    "nonlinear",
    "activation",
    "function",
    "relu",
    "formula",
    "something",
    "comes",
    "negative",
    "value",
    "relu",
    "going",
    "turn",
    "negative",
    "zero",
    "something",
    "positive",
    "relu",
    "going",
    "leave",
    "let",
    "create",
    "another",
    "class",
    "fashion",
    "mnist",
    "model",
    "v1",
    "going",
    "subclass",
    "beautiful",
    "going",
    "initialize",
    "model",
    "going",
    "quite",
    "created",
    "want",
    "input",
    "shape",
    "going",
    "integer",
    "want",
    "number",
    "hidden",
    "units",
    "going",
    "int",
    "want",
    "output",
    "shape",
    "int",
    "want",
    "stress",
    "well",
    "although",
    "creating",
    "class",
    "inputs",
    "classes",
    "flexible",
    "functions",
    "need",
    "different",
    "use",
    "cases",
    "modeling",
    "classes",
    "keep",
    "mind",
    "build",
    "functionality",
    "self",
    "dot",
    "layer",
    "stack",
    "going",
    "spell",
    "layer",
    "stack",
    "correctly",
    "going",
    "set",
    "equal",
    "nn",
    "dot",
    "sequential",
    "want",
    "sequential",
    "set",
    "layers",
    "first",
    "one",
    "going",
    "nn",
    "dot",
    "flatten",
    "going",
    "flatten",
    "inputs",
    "single",
    "vector",
    "going",
    "go",
    "nn",
    "dot",
    "linear",
    "want",
    "flatten",
    "stuff",
    "want",
    "right",
    "shape",
    "flatten",
    "get",
    "shape",
    "issues",
    "input",
    "shape",
    "features",
    "linear",
    "layer",
    "going",
    "hidden",
    "units",
    "hidden",
    "units",
    "going",
    "make",
    "code",
    "cells",
    "code",
    "goes",
    "middle",
    "screen",
    "going",
    "add",
    "nonlinear",
    "layer",
    "going",
    "add",
    "relu",
    "function",
    "might",
    "put",
    "well",
    "generally",
    "linear",
    "function",
    "followed",
    "nonlinear",
    "function",
    "construction",
    "neural",
    "networks",
    "however",
    "neural",
    "networks",
    "customizable",
    "imagine",
    "whether",
    "work",
    "different",
    "question",
    "go",
    "output",
    "shape",
    "features",
    "oh",
    "miss",
    "one",
    "yes",
    "needs",
    "hidden",
    "units",
    "well",
    "output",
    "shape",
    "linear",
    "layer",
    "needs",
    "match",
    "input",
    "shape",
    "linear",
    "layer",
    "relu",
    "layer",
    "wo",
    "change",
    "shape",
    "data",
    "could",
    "test",
    "printing",
    "different",
    "shapes",
    "like",
    "going",
    "finish",
    "another",
    "nonlinear",
    "layer",
    "end",
    "relu",
    "think",
    "improve",
    "model",
    "results",
    "well",
    "hard",
    "tell",
    "without",
    "trying",
    "right",
    "let",
    "continue",
    "building",
    "model",
    "override",
    "forward",
    "method",
    "self",
    "x",
    "going",
    "give",
    "type",
    "going",
    "torch",
    "tensor",
    "input",
    "going",
    "return",
    "happening",
    "go",
    "self",
    "dot",
    "layer",
    "stack",
    "means",
    "x",
    "going",
    "pass",
    "layer",
    "stack",
    "could",
    "customize",
    "could",
    "try",
    "one",
    "nonlinear",
    "activation",
    "actually",
    "previous",
    "network",
    "commented",
    "done",
    "added",
    "two",
    "relu",
    "functions",
    "going",
    "run",
    "beautiful",
    "next",
    "well",
    "stand",
    "shaded",
    "previously",
    "ran",
    "last",
    "model",
    "model",
    "zero",
    "go",
    "parameters",
    "run",
    "gpu",
    "cpu",
    "cpu",
    "try",
    "fashion",
    "mnist",
    "model",
    "v",
    "one",
    "running",
    "device",
    "set",
    "cuda",
    "wonderful",
    "instantiate",
    "create",
    "instance",
    "model",
    "one",
    "want",
    "model",
    "one",
    "actually",
    "set",
    "manual",
    "seed",
    "whenever",
    "create",
    "new",
    "instance",
    "model",
    "going",
    "instantiated",
    "random",
    "numbers",
    "necessarily",
    "set",
    "random",
    "seed",
    "anyway",
    "values",
    "quite",
    "similar",
    "end",
    "end",
    "input",
    "shape",
    "going",
    "come",
    "well",
    "output",
    "flatten",
    "layer",
    "28",
    "28",
    "image",
    "goes",
    "going",
    "set",
    "hidden",
    "units",
    "going",
    "use",
    "number",
    "hidden",
    "units",
    "going",
    "output",
    "shape",
    "need",
    "one",
    "value",
    "one",
    "output",
    "neuron",
    "classes",
    "length",
    "class",
    "names",
    "going",
    "send",
    "target",
    "device",
    "write",
    "send",
    "gpu",
    "available",
    "set",
    "device",
    "agnostic",
    "code",
    "last",
    "video",
    "put",
    "two",
    "device",
    "instead",
    "hard",
    "coding",
    "check",
    "output",
    "model",
    "zero",
    "device",
    "let",
    "check",
    "model",
    "one",
    "device",
    "model",
    "one",
    "parameters",
    "check",
    "parameters",
    "live",
    "using",
    "device",
    "attribute",
    "beautiful",
    "model",
    "one",
    "living",
    "gpu",
    "cuda",
    "index",
    "zero",
    "index",
    "zero",
    "means",
    "first",
    "gpu",
    "available",
    "one",
    "gpu",
    "available",
    "tesla",
    "gpu",
    "got",
    "couple",
    "things",
    "created",
    "another",
    "model",
    "recreate",
    "go",
    "back",
    "workflow",
    "built",
    "model",
    "built",
    "model",
    "instantiate",
    "loss",
    "function",
    "optimizer",
    "done",
    "things",
    "model",
    "zero",
    "going",
    "next",
    "video",
    "like",
    "go",
    "ahead",
    "try",
    "create",
    "loss",
    "function",
    "model",
    "optimizer",
    "model",
    "one",
    "hint",
    "exact",
    "loss",
    "function",
    "optimizer",
    "model",
    "zero",
    "give",
    "shot",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "created",
    "another",
    "model",
    "continuing",
    "modeling",
    "experiments",
    "difference",
    "fashion",
    "mnist",
    "model",
    "v1",
    "v0",
    "added",
    "nonlinear",
    "layers",
    "know",
    "could",
    "think",
    "guess",
    "whether",
    "would",
    "help",
    "improve",
    "model",
    "practice",
    "start",
    "understand",
    "different",
    "functions",
    "influence",
    "neural",
    "networks",
    "prefer",
    "doubt",
    "code",
    "run",
    "lots",
    "different",
    "experiments",
    "let",
    "continue",
    "create",
    "loss",
    "function",
    "loss",
    "optimizer",
    "evaluation",
    "metrics",
    "done",
    "model",
    "zero",
    "going",
    "spend",
    "much",
    "time",
    "explaining",
    "going",
    "done",
    "fair",
    "times",
    "helper",
    "functions",
    "script",
    "downloaded",
    "going",
    "import",
    "accuracy",
    "function",
    "going",
    "set",
    "loss",
    "function",
    "working",
    "multi",
    "class",
    "classification",
    "loss",
    "function",
    "typically",
    "use",
    "dot",
    "cross",
    "entropy",
    "loss",
    "optimizer",
    "going",
    "torch",
    "dot",
    "opt",
    "dot",
    "sgd",
    "going",
    "optimize",
    "time",
    "put",
    "params",
    "keyword",
    "model",
    "one",
    "dot",
    "parameters",
    "learning",
    "rate",
    "going",
    "keep",
    "previous",
    "model",
    "thing",
    "keep",
    "note",
    "experiments",
    "running",
    "fair",
    "experiments",
    "really",
    "want",
    "tweak",
    "couple",
    "things",
    "maybe",
    "one",
    "thing",
    "per",
    "experiment",
    "way",
    "really",
    "narrow",
    "actually",
    "influences",
    "model",
    "improves",
    "slash",
    "improve",
    "little",
    "pop",
    "quiz",
    "loss",
    "function",
    "going",
    "measure",
    "wrong",
    "model",
    "optimizer",
    "tries",
    "update",
    "models",
    "parameters",
    "reduce",
    "loss",
    "two",
    "functions",
    "going",
    "accuracy",
    "function",
    "course",
    "going",
    "measuring",
    "models",
    "accuracy",
    "measure",
    "accuracy",
    "one",
    "base",
    "classification",
    "metrics",
    "run",
    "next",
    "getting",
    "quite",
    "good",
    "picked",
    "loss",
    "function",
    "optimizer",
    "going",
    "build",
    "training",
    "loop",
    "however",
    "spent",
    "quite",
    "bit",
    "time",
    "previous",
    "video",
    "go",
    "vowel",
    "model",
    "function",
    "oh",
    "helpful",
    "turned",
    "function",
    "make",
    "function",
    "training",
    "loop",
    "well",
    "testing",
    "loop",
    "think",
    "give",
    "go",
    "going",
    "make",
    "function",
    "next",
    "video",
    "training",
    "going",
    "call",
    "train",
    "step",
    "create",
    "function",
    "testing",
    "called",
    "test",
    "step",
    "take",
    "parameters",
    "let",
    "figure",
    "otherwise",
    "going",
    "code",
    "together",
    "next",
    "video",
    "see",
    "got",
    "loss",
    "function",
    "ready",
    "optimizer",
    "next",
    "step",
    "well",
    "create",
    "training",
    "evaluation",
    "loops",
    "let",
    "make",
    "heading",
    "going",
    "call",
    "functionizing",
    "training",
    "evaluation",
    "slash",
    "testing",
    "loops",
    "written",
    "similar",
    "code",
    "quite",
    "often",
    "training",
    "evaluating",
    "slash",
    "testing",
    "models",
    "going",
    "start",
    "moving",
    "towards",
    "functionizing",
    "code",
    "written",
    "best",
    "practice",
    "helps",
    "reduce",
    "errors",
    "writing",
    "training",
    "loop",
    "time",
    "may",
    "get",
    "wrong",
    "got",
    "one",
    "works",
    "particular",
    "problem",
    "hey",
    "might",
    "well",
    "save",
    "function",
    "continually",
    "call",
    "going",
    "rare",
    "going",
    "allow",
    "going",
    "copy",
    "training",
    "might",
    "already",
    "attempted",
    "create",
    "function",
    "called",
    "let",
    "create",
    "function",
    "one",
    "training",
    "loop",
    "going",
    "call",
    "train",
    "step",
    "going",
    "create",
    "function",
    "testing",
    "loop",
    "going",
    "call",
    "test",
    "step",
    "calling",
    "call",
    "whatever",
    "want",
    "understand",
    "quite",
    "easily",
    "calling",
    "train",
    "step",
    "epoch",
    "range",
    "call",
    "training",
    "step",
    "thing",
    "epoch",
    "range",
    "call",
    "testing",
    "step",
    "make",
    "lot",
    "sense",
    "coded",
    "let",
    "put",
    "training",
    "code",
    "functionize",
    "let",
    "start",
    "train",
    "step",
    "parameters",
    "train",
    "step",
    "function",
    "take",
    "well",
    "let",
    "think",
    "need",
    "model",
    "need",
    "data",
    "loader",
    "need",
    "loss",
    "function",
    "need",
    "optimizer",
    "could",
    "also",
    "put",
    "accuracy",
    "function",
    "wanted",
    "potentially",
    "could",
    "put",
    "target",
    "device",
    "like",
    "compute",
    "make",
    "code",
    "device",
    "agnostic",
    "exact",
    "code",
    "went",
    "loop",
    "data",
    "loader",
    "forward",
    "pass",
    "calculate",
    "loss",
    "accumulate",
    "zero",
    "optimizer",
    "perform",
    "backpropagation",
    "respect",
    "loss",
    "parameters",
    "model",
    "step",
    "optimizer",
    "hopefully",
    "improve",
    "parameters",
    "model",
    "better",
    "predict",
    "data",
    "trying",
    "predict",
    "let",
    "craft",
    "train",
    "step",
    "function",
    "take",
    "model",
    "going",
    "torch",
    "type",
    "hint",
    "going",
    "put",
    "data",
    "loader",
    "going",
    "type",
    "torch",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "loader",
    "necessarily",
    "need",
    "put",
    "type",
    "hints",
    "relatively",
    "new",
    "addition",
    "python",
    "might",
    "start",
    "see",
    "also",
    "helps",
    "people",
    "understand",
    "code",
    "expecting",
    "loss",
    "function",
    "going",
    "put",
    "optimizer",
    "torch",
    "dot",
    "opt",
    "type",
    "optimizer",
    "also",
    "want",
    "accuracy",
    "function",
    "necessarily",
    "need",
    "either",
    "lot",
    "nice",
    "habs",
    "first",
    "four",
    "probably",
    "important",
    "device",
    "torch",
    "going",
    "torch",
    "dot",
    "device",
    "equals",
    "device",
    "hard",
    "code",
    "already",
    "set",
    "device",
    "parameter",
    "write",
    "performs",
    "training",
    "step",
    "model",
    "trying",
    "learn",
    "data",
    "loader",
    "nice",
    "simple",
    "could",
    "make",
    "explanatory",
    "wanted",
    "leave",
    "right",
    "start",
    "going",
    "set",
    "train",
    "loss",
    "train",
    "act",
    "equals",
    "zero",
    "zero",
    "going",
    "introduce",
    "accuracy",
    "get",
    "rid",
    "let",
    "go",
    "line",
    "line",
    "need",
    "well",
    "got",
    "four",
    "batch",
    "xy",
    "enumerate",
    "train",
    "data",
    "loader",
    "going",
    "change",
    "data",
    "loader",
    "change",
    "data",
    "loader",
    "wonderful",
    "got",
    "model",
    "zero",
    "dot",
    "train",
    "want",
    "well",
    "going",
    "keep",
    "model",
    "agnostic",
    "want",
    "able",
    "use",
    "model",
    "function",
    "let",
    "get",
    "rid",
    "model",
    "dot",
    "train",
    "missing",
    "one",
    "step",
    "put",
    "data",
    "target",
    "device",
    "could",
    "actually",
    "put",
    "model",
    "dot",
    "train",
    "put",
    "model",
    "training",
    "mode",
    "default",
    "model",
    "case",
    "going",
    "call",
    "anyway",
    "model",
    "dot",
    "train",
    "put",
    "data",
    "target",
    "device",
    "going",
    "go",
    "xy",
    "equals",
    "x",
    "dot",
    "two",
    "device",
    "dot",
    "two",
    "device",
    "wonderful",
    "forward",
    "pass",
    "need",
    "use",
    "model",
    "zero",
    "anymore",
    "going",
    "use",
    "model",
    "loss",
    "function",
    "stay",
    "passing",
    "loss",
    "function",
    "train",
    "loss",
    "accumulated",
    "fine",
    "might",
    "also",
    "accumulate",
    "train",
    "accuracy",
    "limit",
    "loss",
    "accuracy",
    "per",
    "batch",
    "train",
    "act",
    "equals",
    "plus",
    "equals",
    "accuracy",
    "function",
    "true",
    "equals",
    "pred",
    "equals",
    "pred",
    "outputs",
    "pred",
    "need",
    "take",
    "raw",
    "outputs",
    "outputs",
    "raw",
    "logits",
    "model",
    "accuracy",
    "function",
    "expects",
    "predictions",
    "format",
    "true",
    "values",
    "need",
    "make",
    "sure",
    "call",
    "argmax",
    "first",
    "dimension",
    "going",
    "go",
    "logits",
    "prediction",
    "labels",
    "keep",
    "optimizer",
    "zero",
    "grab",
    "passing",
    "optimizer",
    "keep",
    "loss",
    "backwards",
    "loss",
    "calculated",
    "keep",
    "optimizer",
    "step",
    "could",
    "print",
    "happening",
    "might",
    "change",
    "little",
    "bit",
    "need",
    "divide",
    "total",
    "train",
    "loss",
    "accuracy",
    "want",
    "type",
    "accuracy",
    "added",
    "accuracy",
    "metric",
    "act",
    "train",
    "act",
    "divided",
    "equals",
    "length",
    "train",
    "data",
    "loader",
    "oh",
    "sorry",
    "use",
    "data",
    "loader",
    "data",
    "loader",
    "data",
    "loader",
    "going",
    "print",
    "per",
    "batch",
    "going",
    "get",
    "rid",
    "make",
    "end",
    "step",
    "make",
    "print",
    "print",
    "notice",
    "end",
    "step",
    "outside",
    "loop",
    "going",
    "accumulating",
    "loss",
    "training",
    "data",
    "set",
    "accuracy",
    "training",
    "data",
    "set",
    "per",
    "batch",
    "finding",
    "end",
    "training",
    "steps",
    "batches",
    "data",
    "loader",
    "finding",
    "average",
    "loss",
    "per",
    "batch",
    "average",
    "accuracy",
    "per",
    "batch",
    "going",
    "go",
    "train",
    "loss",
    "going",
    "train",
    "loss",
    "going",
    "go",
    "train",
    "act",
    "going",
    "train",
    "act",
    "going",
    "set",
    "get",
    "percentage",
    "wonderful",
    "works",
    "able",
    "call",
    "train",
    "step",
    "function",
    "pass",
    "model",
    "data",
    "loader",
    "loss",
    "function",
    "optimizer",
    "accuracy",
    "function",
    "device",
    "automatically",
    "steps",
    "going",
    "find",
    "later",
    "video",
    "next",
    "video",
    "going",
    "thing",
    "done",
    "training",
    "loop",
    "test",
    "step",
    "challenge",
    "video",
    "go",
    "testing",
    "loop",
    "code",
    "wrote",
    "try",
    "recreate",
    "test",
    "step",
    "function",
    "format",
    "done",
    "give",
    "go",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "functionalized",
    "training",
    "loop",
    "call",
    "train",
    "step",
    "function",
    "instead",
    "writing",
    "training",
    "loop",
    "code",
    "well",
    "train",
    "model",
    "art",
    "function",
    "let",
    "testing",
    "loop",
    "issued",
    "challenge",
    "last",
    "video",
    "give",
    "go",
    "hope",
    "best",
    "way",
    "practice",
    "pytorch",
    "code",
    "write",
    "pytorch",
    "code",
    "let",
    "put",
    "model",
    "going",
    "torch",
    "dot",
    "module",
    "going",
    "put",
    "data",
    "loader",
    "need",
    "model",
    "need",
    "data",
    "data",
    "loader",
    "going",
    "course",
    "test",
    "data",
    "load",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "loader",
    "going",
    "put",
    "loss",
    "function",
    "going",
    "torch",
    "end",
    "module",
    "well",
    "going",
    "use",
    "end",
    "cross",
    "entropy",
    "loss",
    "see",
    "later",
    "going",
    "put",
    "accuracy",
    "function",
    "need",
    "optimizer",
    "optimization",
    "testing",
    "loop",
    "evaluating",
    "device",
    "torch",
    "dot",
    "device",
    "going",
    "set",
    "default",
    "target",
    "device",
    "parameter",
    "beautiful",
    "put",
    "little",
    "doctoring",
    "performs",
    "testing",
    "loop",
    "step",
    "model",
    "going",
    "data",
    "loader",
    "wonderful",
    "let",
    "set",
    "test",
    "loss",
    "test",
    "accuracy",
    "measure",
    "test",
    "loss",
    "accuracy",
    "without",
    "testing",
    "loop",
    "function",
    "going",
    "set",
    "model",
    "put",
    "comment",
    "put",
    "model",
    "vowel",
    "mode",
    "model",
    "dot",
    "vowel",
    "use",
    "underscore",
    "model",
    "zero",
    "model",
    "coming",
    "top",
    "well",
    "performing",
    "test",
    "step",
    "turn",
    "inference",
    "mode",
    "turn",
    "inference",
    "mode",
    "inference",
    "mode",
    "context",
    "manager",
    "remember",
    "whenever",
    "performing",
    "predictions",
    "model",
    "put",
    "model",
    "dot",
    "vowel",
    "want",
    "many",
    "speedups",
    "get",
    "make",
    "sure",
    "predictions",
    "done",
    "within",
    "inference",
    "mode",
    "remember",
    "inference",
    "another",
    "word",
    "predictions",
    "within",
    "inference",
    "mode",
    "context",
    "manager",
    "going",
    "loop",
    "data",
    "loader",
    "x",
    "data",
    "loader",
    "specify",
    "x",
    "test",
    "test",
    "could",
    "wanted",
    "another",
    "function",
    "go",
    "x",
    "data",
    "loader",
    "forward",
    "pass",
    "send",
    "data",
    "target",
    "device",
    "target",
    "device",
    "going",
    "x",
    "equals",
    "x",
    "dot",
    "two",
    "device",
    "thing",
    "best",
    "practice",
    "creating",
    "device",
    "agnostic",
    "code",
    "well",
    "thing",
    "said",
    "forward",
    "pass",
    "data",
    "model",
    "device",
    "create",
    "variable",
    "test",
    "pred",
    "equals",
    "model",
    "going",
    "pass",
    "calculate",
    "loss",
    "calculate",
    "loss",
    "slash",
    "accuracy",
    "going",
    "accumulate",
    "per",
    "batch",
    "set",
    "test",
    "loss",
    "equals",
    "loss",
    "function",
    "oh",
    "plus",
    "equals",
    "loss",
    "function",
    "going",
    "pass",
    "test",
    "pred",
    "truth",
    "label",
    "test",
    "act",
    "accumulate",
    "well",
    "using",
    "accuracy",
    "function",
    "pass",
    "true",
    "equals",
    "pred",
    "pred",
    "well",
    "test",
    "pred",
    "take",
    "argmax",
    "convert",
    "going",
    "outputs",
    "raw",
    "logits",
    "remember",
    "models",
    "raw",
    "output",
    "referred",
    "logits",
    "go",
    "logits",
    "prediction",
    "labels",
    "beautiful",
    "oh",
    "little",
    "typo",
    "catch",
    "one",
    "tab",
    "tab",
    "beautiful",
    "oh",
    "look",
    "good",
    "function",
    "looking",
    "going",
    "adjust",
    "metrics",
    "adjust",
    "metrics",
    "print",
    "might",
    "notice",
    "outside",
    "batch",
    "loop",
    "right",
    "draw",
    "line",
    "write",
    "code",
    "still",
    "within",
    "context",
    "manager",
    "important",
    "want",
    "adapt",
    "value",
    "created",
    "inside",
    "context",
    "manager",
    "modify",
    "still",
    "inside",
    "context",
    "manager",
    "otherwise",
    "pytorch",
    "throw",
    "error",
    "try",
    "write",
    "code",
    "want",
    "outside",
    "context",
    "manager",
    "see",
    "still",
    "works",
    "test",
    "loss",
    "going",
    "adjust",
    "find",
    "average",
    "test",
    "loss",
    "test",
    "accuracy",
    "per",
    "batch",
    "across",
    "whole",
    "step",
    "going",
    "go",
    "length",
    "data",
    "loader",
    "going",
    "print",
    "happening",
    "print",
    "happening",
    "test",
    "loss",
    "put",
    "well",
    "going",
    "get",
    "test",
    "loss",
    "let",
    "get",
    "five",
    "decimal",
    "places",
    "going",
    "go",
    "test",
    "act",
    "get",
    "two",
    "decimal",
    "places",
    "could",
    "many",
    "decimal",
    "want",
    "could",
    "even",
    "times",
    "100",
    "get",
    "proper",
    "accuracy",
    "format",
    "put",
    "new",
    "line",
    "end",
    "wonderful",
    "looks",
    "like",
    "got",
    "functions",
    "run",
    "cell",
    "yet",
    "training",
    "step",
    "test",
    "step",
    "think",
    "could",
    "replicate",
    "go",
    "back",
    "training",
    "loop",
    "wrote",
    "think",
    "could",
    "replicate",
    "functionality",
    "except",
    "time",
    "using",
    "functions",
    "well",
    "could",
    "still",
    "use",
    "epoch",
    "tqdm",
    "range",
    "epochs",
    "would",
    "call",
    "training",
    "step",
    "training",
    "code",
    "training",
    "step",
    "function",
    "would",
    "call",
    "testing",
    "step",
    "function",
    "passing",
    "appropriate",
    "parameters",
    "testing",
    "loop",
    "next",
    "video",
    "leverage",
    "two",
    "functions",
    "train",
    "step",
    "test",
    "step",
    "train",
    "model",
    "one",
    "challenge",
    "video",
    "give",
    "go",
    "use",
    "training",
    "step",
    "test",
    "step",
    "function",
    "train",
    "model",
    "one",
    "three",
    "epochs",
    "see",
    "go",
    "together",
    "next",
    "video",
    "welcome",
    "back",
    "go",
    "create",
    "training",
    "loop",
    "pytorch",
    "optimization",
    "loop",
    "using",
    "training",
    "step",
    "function",
    "test",
    "step",
    "function",
    "errors",
    "fact",
    "even",
    "know",
    "find",
    "together",
    "hey",
    "combine",
    "two",
    "functions",
    "create",
    "optimization",
    "loop",
    "going",
    "go",
    "torch",
    "dot",
    "manual",
    "seed",
    "going",
    "measure",
    "time",
    "long",
    "training",
    "test",
    "loop",
    "takes",
    "time",
    "using",
    "different",
    "model",
    "model",
    "uses",
    "nonlinearities",
    "gpu",
    "main",
    "thing",
    "want",
    "compare",
    "long",
    "model",
    "took",
    "cpu",
    "versus",
    "gpu",
    "going",
    "import",
    "time",
    "import",
    "default",
    "timer",
    "timer",
    "going",
    "start",
    "train",
    "time",
    "train",
    "time",
    "start",
    "gpu",
    "equals",
    "timer",
    "right",
    "set",
    "epochs",
    "going",
    "set",
    "epochs",
    "equal",
    "three",
    "want",
    "keep",
    "training",
    "experiments",
    "close",
    "possible",
    "see",
    "little",
    "changes",
    "create",
    "optimization",
    "evaluation",
    "loop",
    "using",
    "train",
    "step",
    "test",
    "step",
    "going",
    "loop",
    "epochs",
    "epoch",
    "tqdm",
    "get",
    "nice",
    "progress",
    "bar",
    "epochs",
    "going",
    "print",
    "epoch",
    "little",
    "print",
    "going",
    "epoch",
    "get",
    "new",
    "line",
    "maybe",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "something",
    "like",
    "maybe",
    "miscounted",
    "right",
    "train",
    "step",
    "little",
    "doc",
    "string",
    "model",
    "model",
    "would",
    "like",
    "use",
    "like",
    "use",
    "model",
    "one",
    "data",
    "loader",
    "data",
    "loader",
    "would",
    "like",
    "use",
    "well",
    "like",
    "use",
    "train",
    "data",
    "loader",
    "also",
    "loss",
    "function",
    "loss",
    "function",
    "optimizer",
    "optimizer",
    "accuracy",
    "function",
    "accuracy",
    "function",
    "oops",
    "forgot",
    "put",
    "fm",
    "finally",
    "device",
    "equals",
    "device",
    "going",
    "set",
    "anyway",
    "beautiful",
    "creating",
    "training",
    "loop",
    "thanks",
    "code",
    "functionalized",
    "recall",
    "set",
    "optimizer",
    "loss",
    "function",
    "previous",
    "video",
    "could",
    "bring",
    "really",
    "wanted",
    "one",
    "place",
    "either",
    "way",
    "get",
    "rid",
    "already",
    "set",
    "going",
    "thing",
    "test",
    "step",
    "need",
    "let",
    "check",
    "doc",
    "string",
    "could",
    "put",
    "little",
    "bit",
    "information",
    "doc",
    "string",
    "wanted",
    "really",
    "make",
    "code",
    "reusable",
    "someone",
    "else",
    "use",
    "code",
    "even",
    "us",
    "future",
    "knows",
    "going",
    "let",
    "code",
    "still",
    "fresh",
    "minds",
    "model",
    "equals",
    "model",
    "one",
    "data",
    "loader",
    "going",
    "test",
    "step",
    "going",
    "test",
    "data",
    "loader",
    "going",
    "set",
    "loss",
    "function",
    "going",
    "loss",
    "function",
    "need",
    "use",
    "optimizer",
    "evaluating",
    "model",
    "pass",
    "accuracy",
    "function",
    "accuracy",
    "function",
    "finally",
    "device",
    "already",
    "set",
    "pass",
    "anyway",
    "look",
    "whole",
    "optimization",
    "loop",
    "lines",
    "code",
    "beautiful",
    "functions",
    "something",
    "could",
    "put",
    "like",
    "helper",
    "functions",
    "dot",
    "pi",
    "way",
    "could",
    "import",
    "later",
    "write",
    "training",
    "loops",
    "see",
    "example",
    "later",
    "course",
    "let",
    "keep",
    "going",
    "want",
    "measure",
    "train",
    "time",
    "right",
    "going",
    "create",
    "steps",
    "going",
    "create",
    "train",
    "time",
    "end",
    "cpu",
    "going",
    "set",
    "timer",
    "going",
    "measure",
    "value",
    "time",
    "line",
    "code",
    "run",
    "going",
    "run",
    "lines",
    "code",
    "going",
    "perform",
    "training",
    "optimization",
    "loop",
    "going",
    "oh",
    "excuse",
    "gpu",
    "going",
    "measure",
    "point",
    "time",
    "codes",
    "run",
    "measure",
    "point",
    "time",
    "finally",
    "go",
    "total",
    "train",
    "time",
    "model",
    "one",
    "equal",
    "print",
    "train",
    "time",
    "function",
    "wrote",
    "pass",
    "start",
    "time",
    "prints",
    "difference",
    "start",
    "end",
    "time",
    "target",
    "device",
    "let",
    "start",
    "equals",
    "train",
    "time",
    "start",
    "gpu",
    "end",
    "going",
    "train",
    "time",
    "end",
    "gpu",
    "device",
    "going",
    "device",
    "beautiful",
    "ready",
    "run",
    "next",
    "modeling",
    "experiment",
    "model",
    "one",
    "got",
    "model",
    "running",
    "gpu",
    "using",
    "nonlinear",
    "layers",
    "want",
    "compare",
    "first",
    "model",
    "results",
    "model",
    "zero",
    "results",
    "total",
    "train",
    "time",
    "model",
    "zero",
    "yes",
    "going",
    "model",
    "one",
    "beat",
    "results",
    "beat",
    "result",
    "three",
    "two",
    "one",
    "errors",
    "okay",
    "train",
    "step",
    "got",
    "unexpected",
    "keyword",
    "loss",
    "oh",
    "catch",
    "type",
    "loss",
    "function",
    "let",
    "run",
    "go",
    "okay",
    "running",
    "got",
    "progress",
    "bar",
    "going",
    "output",
    "end",
    "epoch",
    "go",
    "training",
    "loss",
    "right",
    "test",
    "accuracy",
    "training",
    "accuracy",
    "exciting",
    "love",
    "watching",
    "neural",
    "networks",
    "train",
    "okay",
    "improving",
    "per",
    "epoch",
    "good",
    "sign",
    "still",
    "got",
    "fair",
    "way",
    "go",
    "oh",
    "okay",
    "well",
    "beat",
    "hmm",
    "looks",
    "like",
    "beat",
    "model",
    "zero",
    "results",
    "nonlinear",
    "layers",
    "slightly",
    "faster",
    "training",
    "time",
    "numbers",
    "might",
    "exact",
    "got",
    "right",
    "big",
    "thing",
    "machine",
    "learning",
    "uses",
    "randomness",
    "numbers",
    "might",
    "slightly",
    "different",
    "direction",
    "quite",
    "similar",
    "may",
    "using",
    "different",
    "gpus",
    "keep",
    "mind",
    "right",
    "using",
    "new",
    "video",
    "smi",
    "using",
    "tesla",
    "t4",
    "time",
    "recording",
    "video",
    "wednesday",
    "april",
    "20",
    "2022",
    "relatively",
    "fast",
    "gpu",
    "making",
    "inference",
    "keep",
    "mind",
    "gpu",
    "future",
    "may",
    "different",
    "cpu",
    "run",
    "may",
    "also",
    "different",
    "time",
    "numbers",
    "like",
    "10",
    "times",
    "higher",
    "might",
    "want",
    "look",
    "seeing",
    "code",
    "error",
    "10",
    "times",
    "lower",
    "well",
    "hey",
    "running",
    "fast",
    "hardware",
    "looks",
    "like",
    "code",
    "running",
    "cuda",
    "slightly",
    "faster",
    "cpu",
    "dramatically",
    "faster",
    "probably",
    "akin",
    "fact",
    "data",
    "set",
    "complex",
    "model",
    "large",
    "mean",
    "model",
    "like",
    "vast",
    "amount",
    "layers",
    "data",
    "set",
    "comprised",
    "like",
    "layers",
    "model",
    "data",
    "set",
    "comprised",
    "images",
    "28",
    "imagine",
    "parameters",
    "model",
    "features",
    "data",
    "higher",
    "time",
    "going",
    "might",
    "sometimes",
    "even",
    "find",
    "model",
    "faster",
    "cpu",
    "train",
    "time",
    "cpu",
    "might",
    "sometimes",
    "find",
    "model",
    "training",
    "time",
    "cpu",
    "fact",
    "faster",
    "exact",
    "code",
    "running",
    "gpu",
    "might",
    "well",
    "let",
    "write",
    "let",
    "go",
    "note",
    "sometimes",
    "depending",
    "data",
    "slash",
    "hardware",
    "might",
    "find",
    "model",
    "trains",
    "faster",
    "cpu",
    "gpu",
    "one",
    "number",
    "one",
    "reasons",
    "one",
    "could",
    "overhead",
    "copying",
    "data",
    "slash",
    "model",
    "gpu",
    "outweighs",
    "compute",
    "benefits",
    "offered",
    "gpu",
    "probably",
    "one",
    "number",
    "one",
    "reasons",
    "data",
    "processed",
    "gpu",
    "copy",
    "default",
    "cpu",
    "copy",
    "gpu",
    "overhead",
    "time",
    "copy",
    "gpu",
    "memory",
    "although",
    "gpu",
    "probably",
    "compute",
    "faster",
    "data",
    "still",
    "back",
    "forth",
    "going",
    "cpu",
    "gpu",
    "number",
    "two",
    "reason",
    "hardware",
    "using",
    "better",
    "cpu",
    "terms",
    "compute",
    "capability",
    "gpu",
    "quite",
    "bit",
    "rarer",
    "usually",
    "using",
    "gpu",
    "like",
    "fairly",
    "modern",
    "gpu",
    "faster",
    "computing",
    "deep",
    "learning",
    "running",
    "deep",
    "learning",
    "algorithms",
    "general",
    "cpu",
    "sometimes",
    "numbers",
    "compute",
    "time",
    "really",
    "dependent",
    "hardware",
    "running",
    "get",
    "biggest",
    "benefits",
    "speedups",
    "gpu",
    "running",
    "larger",
    "models",
    "larger",
    "data",
    "sets",
    "compute",
    "intensive",
    "layers",
    "neural",
    "networks",
    "like",
    "great",
    "article",
    "get",
    "gpus",
    "little",
    "bit",
    "technical",
    "something",
    "keep",
    "mind",
    "progress",
    "machine",
    "learning",
    "engineer",
    "make",
    "gpus",
    "go",
    "burr",
    "mean",
    "burr",
    "first",
    "principles",
    "go",
    "making",
    "deep",
    "learning",
    "go",
    "burr",
    "gpu",
    "going",
    "burr",
    "running",
    "fast",
    "first",
    "principles",
    "horace",
    "works",
    "pytorch",
    "great",
    "talks",
    "compute",
    "first",
    "principle",
    "mean",
    "copying",
    "memory",
    "compute",
    "might",
    "fair",
    "things",
    "familiar",
    "okay",
    "aware",
    "bandwidth",
    "bandwidth",
    "costs",
    "essentially",
    "cost",
    "paid",
    "move",
    "data",
    "one",
    "place",
    "another",
    "talking",
    "copying",
    "stuff",
    "cpu",
    "gpu",
    "also",
    "one",
    "overhead",
    "overhead",
    "basically",
    "everything",
    "else",
    "called",
    "overhead",
    "different",
    "terms",
    "different",
    "things",
    "article",
    "excellent",
    "going",
    "copy",
    "find",
    "resources",
    "way",
    "make",
    "models",
    "compute",
    "faster",
    "see",
    "lovely",
    "right",
    "baseline",
    "model",
    "performing",
    "best",
    "terms",
    "results",
    "terms",
    "actually",
    "model",
    "computing",
    "gpu",
    "performing",
    "faster",
    "cpu",
    "might",
    "slightly",
    "different",
    "case",
    "particular",
    "hardware",
    "cuda",
    "faster",
    "except",
    "model",
    "zero",
    "baseline",
    "better",
    "model",
    "one",
    "next",
    "well",
    "keep",
    "experimenting",
    "course",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "move",
    "next",
    "modeling",
    "experiment",
    "let",
    "get",
    "results",
    "dictionary",
    "model",
    "one",
    "model",
    "trained",
    "like",
    "got",
    "one",
    "model",
    "zero",
    "let",
    "create",
    "one",
    "model",
    "one",
    "results",
    "create",
    "without",
    "vowel",
    "model",
    "function",
    "go",
    "right",
    "back",
    "get",
    "rid",
    "cell",
    "let",
    "type",
    "get",
    "model",
    "one",
    "results",
    "dictionary",
    "helpful",
    "later",
    "compare",
    "modeling",
    "results",
    "dictionary",
    "format",
    "going",
    "model",
    "one",
    "results",
    "equals",
    "vowel",
    "model",
    "model",
    "equals",
    "model",
    "one",
    "pass",
    "data",
    "loader",
    "going",
    "test",
    "data",
    "loader",
    "pass",
    "loss",
    "function",
    "going",
    "equal",
    "loss",
    "function",
    "pass",
    "accuracy",
    "function",
    "equals",
    "accuracy",
    "function",
    "wonderful",
    "check",
    "model",
    "one",
    "results",
    "get",
    "oh",
    "get",
    "error",
    "get",
    "code",
    "right",
    "looks",
    "right",
    "oh",
    "say",
    "runtime",
    "error",
    "expected",
    "tensors",
    "device",
    "found",
    "least",
    "two",
    "devices",
    "cuda",
    "cpu",
    "course",
    "happen",
    "well",
    "let",
    "go",
    "back",
    "model",
    "function",
    "wherever",
    "defined",
    "go",
    "ah",
    "see",
    "little",
    "gotcha",
    "pytorch",
    "deep",
    "learning",
    "general",
    "saying",
    "industry",
    "deep",
    "learning",
    "models",
    "fail",
    "silently",
    "kind",
    "one",
    "ones",
    "data",
    "model",
    "different",
    "devices",
    "remember",
    "said",
    "three",
    "big",
    "errors",
    "shape",
    "mismatches",
    "data",
    "model",
    "device",
    "mismatches",
    "got",
    "far",
    "data",
    "type",
    "mismatches",
    "data",
    "wrong",
    "data",
    "type",
    "computed",
    "going",
    "fix",
    "let",
    "bring",
    "vowel",
    "model",
    "function",
    "like",
    "done",
    "test",
    "step",
    "train",
    "step",
    "functions",
    "created",
    "device",
    "agnostic",
    "data",
    "sent",
    "data",
    "target",
    "device",
    "exact",
    "thing",
    "vowel",
    "model",
    "function",
    "note",
    "going",
    "forward",
    "always",
    "handy",
    "create",
    "device",
    "agnostic",
    "code",
    "got",
    "new",
    "model",
    "function",
    "x",
    "data",
    "loader",
    "let",
    "make",
    "data",
    "device",
    "agnostic",
    "like",
    "model",
    "device",
    "agnostic",
    "sent",
    "target",
    "device",
    "x",
    "dot",
    "two",
    "device",
    "dot",
    "two",
    "device",
    "let",
    "see",
    "works",
    "rerun",
    "cell",
    "grab",
    "going",
    "write",
    "exact",
    "code",
    "work",
    "sent",
    "could",
    "actually",
    "also",
    "pass",
    "target",
    "device",
    "device",
    "equals",
    "device",
    "way",
    "pass",
    "whatever",
    "device",
    "want",
    "run",
    "going",
    "add",
    "device",
    "device",
    "equals",
    "device",
    "let",
    "see",
    "runs",
    "correctly",
    "beautiful",
    "compare",
    "model",
    "zero",
    "results",
    "looks",
    "like",
    "baseline",
    "still",
    "front",
    "okay",
    "going",
    "next",
    "video",
    "start",
    "step",
    "things",
    "notch",
    "move",
    "convolutional",
    "neural",
    "networks",
    "exciting",
    "way",
    "remember",
    "numbers",
    "exactly",
    "mine",
    "worry",
    "much",
    "landishly",
    "different",
    "go",
    "back",
    "code",
    "see",
    "maybe",
    "cell",
    "run",
    "correctly",
    "something",
    "like",
    "decimal",
    "places",
    "okay",
    "due",
    "inherent",
    "randomness",
    "machine",
    "learning",
    "deep",
    "learning",
    "said",
    "see",
    "next",
    "video",
    "let",
    "get",
    "hands",
    "convolutional",
    "neural",
    "networks",
    "welcome",
    "back",
    "last",
    "video",
    "saw",
    "second",
    "modeling",
    "experiment",
    "model",
    "one",
    "quite",
    "beat",
    "baseline",
    "going",
    "keep",
    "going",
    "modeling",
    "experiments",
    "going",
    "move",
    "model",
    "two",
    "exciting",
    "going",
    "build",
    "convolutional",
    "neural",
    "network",
    "also",
    "known",
    "cnn",
    "cnns",
    "also",
    "known",
    "com",
    "net",
    "cnns",
    "known",
    "capabilities",
    "find",
    "patterns",
    "visual",
    "data",
    "going",
    "well",
    "let",
    "jump",
    "back",
    "keynote",
    "look",
    "slide",
    "typical",
    "architecture",
    "cnn",
    "fair",
    "bit",
    "going",
    "going",
    "step",
    "one",
    "one",
    "input",
    "layer",
    "like",
    "deep",
    "learning",
    "model",
    "input",
    "kind",
    "data",
    "bunch",
    "hidden",
    "layers",
    "case",
    "convolutional",
    "neural",
    "network",
    "convolutional",
    "layers",
    "often",
    "hidden",
    "activations",
    "nonlinear",
    "activation",
    "layers",
    "might",
    "pooling",
    "layer",
    "generally",
    "always",
    "output",
    "layer",
    "sort",
    "usually",
    "linear",
    "layer",
    "values",
    "different",
    "layers",
    "depend",
    "problem",
    "working",
    "going",
    "work",
    "towards",
    "building",
    "something",
    "like",
    "notice",
    "lot",
    "code",
    "quite",
    "similar",
    "code",
    "writing",
    "pytorch",
    "models",
    "difference",
    "going",
    "use",
    "different",
    "layer",
    "types",
    "want",
    "visualize",
    "cnn",
    "colored",
    "block",
    "edition",
    "going",
    "code",
    "minute",
    "worry",
    "much",
    "simple",
    "cnn",
    "might",
    "input",
    "could",
    "image",
    "dad",
    "eating",
    "pizza",
    "two",
    "thumbs",
    "going",
    "preprocess",
    "input",
    "going",
    "words",
    "turn",
    "tensor",
    "red",
    "green",
    "blue",
    "image",
    "going",
    "pass",
    "combination",
    "convolutional",
    "layers",
    "relu",
    "layers",
    "pooling",
    "layers",
    "thing",
    "note",
    "deep",
    "learning",
    "models",
    "want",
    "get",
    "bogged",
    "order",
    "layers",
    "go",
    "combined",
    "many",
    "different",
    "ways",
    "fact",
    "research",
    "coming",
    "almost",
    "every",
    "day",
    "every",
    "week",
    "best",
    "construct",
    "layers",
    "overall",
    "principle",
    "important",
    "get",
    "inputs",
    "idolized",
    "output",
    "fun",
    "part",
    "course",
    "linear",
    "output",
    "layer",
    "going",
    "output",
    "however",
    "many",
    "classes",
    "value",
    "however",
    "many",
    "classes",
    "case",
    "classification",
    "want",
    "make",
    "cnn",
    "deeper",
    "deep",
    "comes",
    "deep",
    "learning",
    "add",
    "layers",
    "theory",
    "behind",
    "practice",
    "behind",
    "layers",
    "add",
    "deep",
    "learning",
    "model",
    "chances",
    "find",
    "patterns",
    "data",
    "find",
    "patterns",
    "well",
    "one",
    "layers",
    "going",
    "perform",
    "like",
    "seen",
    "different",
    "combination",
    "mathematical",
    "operations",
    "whatever",
    "data",
    "feed",
    "subsequent",
    "layer",
    "receives",
    "input",
    "previous",
    "layer",
    "case",
    "advanced",
    "networks",
    "probably",
    "come",
    "across",
    "later",
    "research",
    "machine",
    "learning",
    "career",
    "use",
    "inputs",
    "layers",
    "kind",
    "way",
    "something",
    "like",
    "known",
    "residual",
    "connections",
    "beyond",
    "scope",
    "covering",
    "want",
    "build",
    "first",
    "convolutional",
    "neural",
    "network",
    "let",
    "go",
    "back",
    "google",
    "chrome",
    "going",
    "show",
    "favorite",
    "website",
    "learn",
    "convolutional",
    "neural",
    "networks",
    "cnn",
    "explainer",
    "website",
    "going",
    "part",
    "extra",
    "curriculum",
    "video",
    "spend",
    "20",
    "minutes",
    "clicking",
    "going",
    "entire",
    "website",
    "going",
    "together",
    "would",
    "like",
    "explore",
    "best",
    "way",
    "learn",
    "notice",
    "images",
    "different",
    "sort",
    "going",
    "input",
    "let",
    "start",
    "pizza",
    "convolutional",
    "layer",
    "relu",
    "layer",
    "conv",
    "layer",
    "relu",
    "layer",
    "max",
    "pool",
    "layer",
    "com",
    "relu",
    "com",
    "relu",
    "max",
    "pool",
    "architecture",
    "convolutional",
    "neural",
    "network",
    "running",
    "live",
    "browser",
    "pass",
    "image",
    "notice",
    "breaks",
    "red",
    "green",
    "blue",
    "goes",
    "layers",
    "something",
    "happens",
    "finally",
    "output",
    "notice",
    "output",
    "10",
    "different",
    "classes",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "nine",
    "10",
    "different",
    "classes",
    "image",
    "demo",
    "course",
    "could",
    "change",
    "100",
    "classes",
    "might",
    "change",
    "pieces",
    "puzzle",
    "would",
    "still",
    "stay",
    "quite",
    "notice",
    "class",
    "pizza",
    "highest",
    "output",
    "value",
    "images",
    "pizza",
    "change",
    "one",
    "espresso",
    "got",
    "highest",
    "value",
    "pretty",
    "well",
    "performing",
    "convolutional",
    "neural",
    "network",
    "sport",
    "car",
    "clicked",
    "one",
    "something",
    "going",
    "happen",
    "let",
    "find",
    "convolutional",
    "layer",
    "input",
    "image",
    "64",
    "64",
    "three",
    "color",
    "channels",
    "last",
    "format",
    "kernel",
    "kernel",
    "happens",
    "inside",
    "convolutional",
    "layer",
    "might",
    "going",
    "well",
    "lot",
    "going",
    "yes",
    "course",
    "first",
    "time",
    "ever",
    "seen",
    "essentially",
    "happening",
    "kernel",
    "also",
    "known",
    "filter",
    "going",
    "image",
    "pixel",
    "values",
    "course",
    "format",
    "tensor",
    "trying",
    "find",
    "small",
    "little",
    "intricate",
    "patterns",
    "data",
    "look",
    "valuable",
    "go",
    "play",
    "around",
    "start",
    "top",
    "left",
    "corner",
    "slowly",
    "move",
    "along",
    "see",
    "output",
    "right",
    "hand",
    "side",
    "another",
    "little",
    "square",
    "notice",
    "middle",
    "numbers",
    "changing",
    "well",
    "mathematical",
    "operation",
    "happening",
    "convolutional",
    "layer",
    "convolves",
    "input",
    "image",
    "cool",
    "might",
    "able",
    "see",
    "output",
    "slight",
    "values",
    "like",
    "look",
    "around",
    "headlight",
    "notice",
    "right",
    "activation",
    "red",
    "tiles",
    "well",
    "means",
    "potentially",
    "layer",
    "hidden",
    "unit",
    "want",
    "zoom",
    "second",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "10",
    "hidden",
    "units",
    "one",
    "going",
    "learn",
    "different",
    "feature",
    "data",
    "beauty",
    "deep",
    "learning",
    "also",
    "one",
    "curses",
    "deep",
    "learning",
    "actually",
    "control",
    "one",
    "learns",
    "magic",
    "deep",
    "learning",
    "figures",
    "best",
    "learn",
    "go",
    "notice",
    "one",
    "click",
    "different",
    "representation",
    "right",
    "hand",
    "side",
    "going",
    "happen",
    "layer",
    "layer",
    "goes",
    "convolutional",
    "neural",
    "network",
    "want",
    "read",
    "convolutional",
    "neural",
    "network",
    "go",
    "going",
    "replicate",
    "exact",
    "neural",
    "network",
    "pytorch",
    "code",
    "prefer",
    "learn",
    "want",
    "intuition",
    "behind",
    "math",
    "behind",
    "check",
    "resources",
    "extra",
    "curriculum",
    "video",
    "input",
    "layer",
    "convolutional",
    "layer",
    "see",
    "input",
    "gets",
    "modified",
    "sort",
    "mathematical",
    "operation",
    "course",
    "convolutional",
    "operation",
    "different",
    "numbers",
    "finding",
    "different",
    "patterns",
    "data",
    "really",
    "good",
    "example",
    "notice",
    "outputs",
    "eyes",
    "slightly",
    "changes",
    "trend",
    "throughout",
    "layer",
    "understand",
    "different",
    "hyper",
    "parameters",
    "going",
    "leave",
    "explore",
    "next",
    "video",
    "going",
    "start",
    "write",
    "pytorch",
    "code",
    "replicate",
    "everything",
    "going",
    "going",
    "link",
    "find",
    "happening",
    "inside",
    "cnn",
    "see",
    "website",
    "join",
    "next",
    "video",
    "super",
    "exciting",
    "going",
    "build",
    "first",
    "convolutional",
    "neural",
    "network",
    "computer",
    "vision",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "went",
    "briefly",
    "cnn",
    "explainer",
    "website",
    "favorite",
    "resource",
    "learning",
    "convolutional",
    "neural",
    "networks",
    "course",
    "could",
    "spend",
    "20",
    "minutes",
    "clicking",
    "everything",
    "find",
    "going",
    "convolutional",
    "neural",
    "network",
    "could",
    "start",
    "code",
    "one",
    "hey",
    "code",
    "going",
    "create",
    "convolutional",
    "neural",
    "network",
    "going",
    "going",
    "build",
    "going",
    "build",
    "model",
    "together",
    "video",
    "going",
    "use",
    "layers",
    "pytorch",
    "layers",
    "looked",
    "going",
    "spend",
    "next",
    "couple",
    "videos",
    "stepping",
    "layers",
    "bear",
    "code",
    "entire",
    "model",
    "together",
    "go",
    "break",
    "subsequent",
    "videos",
    "let",
    "build",
    "first",
    "convolutional",
    "neural",
    "network",
    "mouthful",
    "way",
    "going",
    "probably",
    "stick",
    "saying",
    "cnn",
    "fashion",
    "mnist",
    "model",
    "v2",
    "going",
    "subclass",
    "always",
    "building",
    "pytorch",
    "model",
    "going",
    "say",
    "model",
    "architecture",
    "replicates",
    "tiny",
    "vgg",
    "might",
    "thinking",
    "get",
    "daniel",
    "model",
    "cnn",
    "explainer",
    "website",
    "oftentimes",
    "convolutional",
    "neural",
    "networks",
    "new",
    "types",
    "architecture",
    "come",
    "authors",
    "research",
    "paper",
    "present",
    "model",
    "get",
    "name",
    "model",
    "way",
    "future",
    "refer",
    "different",
    "types",
    "model",
    "architectures",
    "simple",
    "name",
    "like",
    "tiny",
    "vgg",
    "people",
    "kind",
    "know",
    "going",
    "believe",
    "somewhere",
    "called",
    "tiny",
    "vgg",
    "tiny",
    "vgg",
    "nothing",
    "yeah",
    "go",
    "tiny",
    "vgg",
    "one",
    "tiny",
    "tiny",
    "yeah",
    "tiny",
    "vgg",
    "look",
    "vgg",
    "conv",
    "net",
    "vgg",
    "16",
    "one",
    "original",
    "ones",
    "vgg",
    "deep",
    "convolutional",
    "neural",
    "networks",
    "vgg",
    "net",
    "also",
    "resnet",
    "another",
    "convolutional",
    "neural",
    "network",
    "also",
    "want",
    "give",
    "location",
    "google",
    "go",
    "popular",
    "cnn",
    "architectures",
    "give",
    "fair",
    "options",
    "lynette",
    "one",
    "first",
    "alexnet",
    "zf",
    "net",
    "whole",
    "bunch",
    "different",
    "resources",
    "also",
    "could",
    "find",
    "convolutional",
    "neural",
    "network",
    "convolutional",
    "neural",
    "network",
    "go",
    "let",
    "stop",
    "moment",
    "let",
    "code",
    "one",
    "together",
    "going",
    "initialize",
    "class",
    "def",
    "init",
    "going",
    "pass",
    "input",
    "shape",
    "like",
    "often",
    "going",
    "put",
    "number",
    "hidden",
    "units",
    "int",
    "going",
    "put",
    "output",
    "shape",
    "int",
    "wonderful",
    "nothing",
    "outlandish",
    "seen",
    "going",
    "go",
    "super",
    "dot",
    "init",
    "initialize",
    "initializer",
    "lack",
    "better",
    "way",
    "putting",
    "going",
    "create",
    "neural",
    "network",
    "couple",
    "blocks",
    "time",
    "might",
    "often",
    "hear",
    "learn",
    "convolutional",
    "neural",
    "networks",
    "tell",
    "things",
    "referred",
    "often",
    "referred",
    "convolutional",
    "blocks",
    "go",
    "back",
    "keynote",
    "combination",
    "layers",
    "might",
    "referred",
    "convolutional",
    "block",
    "convolutional",
    "block",
    "deeper",
    "cnn",
    "might",
    "comprised",
    "multiple",
    "convolutional",
    "blocks",
    "add",
    "confusion",
    "block",
    "comprised",
    "multiple",
    "layers",
    "overall",
    "architecture",
    "comprised",
    "multiple",
    "blocks",
    "deeper",
    "deeper",
    "models",
    "get",
    "blocks",
    "might",
    "comprised",
    "layers",
    "blocks",
    "may",
    "comprised",
    "within",
    "kind",
    "like",
    "lego",
    "fun",
    "let",
    "put",
    "together",
    "ensequential",
    "first",
    "layers",
    "going",
    "create",
    "conv",
    "block",
    "one",
    "uh",
    "2d",
    "oh",
    "look",
    "us",
    "writing",
    "us",
    "first",
    "cnn",
    "layer",
    "define",
    "something",
    "channels",
    "channels",
    "refers",
    "number",
    "channels",
    "visual",
    "data",
    "going",
    "put",
    "input",
    "shape",
    "defining",
    "input",
    "shape",
    "going",
    "first",
    "layer",
    "model",
    "input",
    "shape",
    "going",
    "define",
    "instantiate",
    "class",
    "channels",
    "oh",
    "channels",
    "going",
    "well",
    "going",
    "hidden",
    "units",
    "like",
    "done",
    "previous",
    "models",
    "difference",
    "2d",
    "number",
    "different",
    "hyper",
    "parameters",
    "set",
    "going",
    "set",
    "pretty",
    "quickly",
    "going",
    "step",
    "back",
    "video",
    "subsequent",
    "videos",
    "got",
    "fair",
    "bit",
    "going",
    "got",
    "channels",
    "input",
    "shape",
    "got",
    "channels",
    "hidden",
    "units",
    "got",
    "kernel",
    "size",
    "equals",
    "three",
    "could",
    "tuple",
    "well",
    "three",
    "three",
    "like",
    "keep",
    "three",
    "got",
    "stride",
    "got",
    "padding",
    "values",
    "set",
    "referred",
    "let",
    "write",
    "values",
    "set",
    "neural",
    "networks",
    "nn",
    "neural",
    "networks",
    "called",
    "hyper",
    "parameters",
    "hyper",
    "parameters",
    "2d",
    "might",
    "thinking",
    "2d",
    "well",
    "working",
    "data",
    "images",
    "height",
    "width",
    "also",
    "com",
    "1d",
    "data",
    "3d",
    "data",
    "going",
    "stick",
    "2d",
    "hyper",
    "parameters",
    "well",
    "go",
    "one",
    "going",
    "step",
    "step",
    "particular",
    "layer",
    "done",
    "replicated",
    "particular",
    "layer",
    "cnn",
    "explainer",
    "website",
    "still",
    "got",
    "relu",
    "still",
    "got",
    "another",
    "conv",
    "relu",
    "max",
    "pool",
    "conv",
    "relu",
    "conv",
    "relu",
    "max",
    "pool",
    "block",
    "talking",
    "one",
    "block",
    "neural",
    "network",
    "least",
    "broken",
    "another",
    "block",
    "might",
    "notice",
    "comprised",
    "layers",
    "stacked",
    "top",
    "going",
    "output",
    "layer",
    "want",
    "learn",
    "hyper",
    "parameters",
    "came",
    "coded",
    "could",
    "learn",
    "well",
    "one",
    "could",
    "go",
    "course",
    "pytorch",
    "documentation",
    "pytorch",
    "com",
    "2d",
    "read",
    "mathematical",
    "operation",
    "talked",
    "briefly",
    "stepped",
    "touched",
    "stepped",
    "right",
    "word",
    "create",
    "conv",
    "layer",
    "also",
    "showed",
    "beautiful",
    "website",
    "read",
    "hyper",
    "parameters",
    "understanding",
    "hyper",
    "parameters",
    "extra",
    "curriculum",
    "video",
    "go",
    "little",
    "graphic",
    "see",
    "find",
    "padding",
    "means",
    "kernel",
    "size",
    "means",
    "stride",
    "means",
    "going",
    "read",
    "look",
    "interactive",
    "plot",
    "going",
    "keep",
    "coding",
    "code",
    "going",
    "add",
    "relu",
    "layer",
    "going",
    "add",
    "another",
    "conv",
    "2d",
    "layer",
    "channels",
    "going",
    "hidden",
    "units",
    "going",
    "take",
    "output",
    "size",
    "layer",
    "use",
    "input",
    "size",
    "layer",
    "going",
    "keep",
    "going",
    "channels",
    "equals",
    "hidden",
    "units",
    "case",
    "kernel",
    "size",
    "going",
    "three",
    "well",
    "stride",
    "one",
    "padding",
    "one",
    "course",
    "change",
    "values",
    "later",
    "bear",
    "set",
    "another",
    "relu",
    "layer",
    "going",
    "finish",
    "nn",
    "max",
    "pool",
    "2d",
    "layer",
    "2d",
    "comes",
    "reason",
    "use",
    "comf2d",
    "working",
    "2d",
    "data",
    "going",
    "set",
    "kernel",
    "size",
    "equal",
    "two",
    "course",
    "tuple",
    "well",
    "two",
    "two",
    "could",
    "find",
    "nn",
    "max",
    "pool",
    "2d",
    "well",
    "go",
    "nn",
    "max",
    "pool",
    "2d",
    "applies",
    "2d",
    "max",
    "pooling",
    "input",
    "signal",
    "composed",
    "several",
    "input",
    "planes",
    "taking",
    "max",
    "input",
    "got",
    "parameters",
    "kernel",
    "size",
    "size",
    "window",
    "take",
    "max",
    "seen",
    "window",
    "going",
    "close",
    "come",
    "back",
    "see",
    "window",
    "let",
    "dive",
    "max",
    "pool",
    "layer",
    "see",
    "mouse",
    "see",
    "two",
    "two",
    "well",
    "window",
    "look",
    "difference",
    "input",
    "output",
    "happening",
    "well",
    "tile",
    "two",
    "two",
    "window",
    "four",
    "max",
    "taking",
    "max",
    "tile",
    "case",
    "zero",
    "let",
    "find",
    "actual",
    "value",
    "go",
    "look",
    "four",
    "numbers",
    "middle",
    "inside",
    "max",
    "brackets",
    "max",
    "notice",
    "input",
    "output",
    "shapes",
    "different",
    "output",
    "half",
    "size",
    "input",
    "max",
    "pooling",
    "tries",
    "take",
    "max",
    "value",
    "whatever",
    "input",
    "outputs",
    "right",
    "data",
    "trend",
    "deep",
    "learning",
    "actually",
    "image",
    "moves",
    "notice",
    "notice",
    "different",
    "shapes",
    "even",
    "completely",
    "understand",
    "going",
    "notice",
    "two",
    "values",
    "left",
    "start",
    "get",
    "smaller",
    "smaller",
    "go",
    "model",
    "model",
    "trying",
    "take",
    "input",
    "learn",
    "compressed",
    "representation",
    "layers",
    "going",
    "smoosh",
    "smoosh",
    "smoosh",
    "trying",
    "find",
    "generalizable",
    "patterns",
    "get",
    "ideal",
    "output",
    "input",
    "eventually",
    "going",
    "feature",
    "vector",
    "final",
    "layer",
    "lot",
    "going",
    "let",
    "keep",
    "coding",
    "completed",
    "first",
    "block",
    "got",
    "cons",
    "layer",
    "relu",
    "layer",
    "cons",
    "layer",
    "relu",
    "layer",
    "max",
    "pool",
    "layer",
    "look",
    "cons",
    "layer",
    "relu",
    "layer",
    "cons",
    "layer",
    "relu",
    "layer",
    "max",
    "pool",
    "move",
    "next",
    "block",
    "one",
    "bit",
    "faster",
    "already",
    "coded",
    "first",
    "one",
    "going",
    "well",
    "going",
    "go",
    "going",
    "set",
    "channels",
    "channels",
    "well",
    "going",
    "set",
    "hidden",
    "units",
    "well",
    "network",
    "going",
    "flow",
    "straight",
    "layers",
    "output",
    "size",
    "going",
    "hidden",
    "units",
    "want",
    "channels",
    "match",
    "previous",
    "layers",
    "channels",
    "going",
    "go",
    "channels",
    "equals",
    "hidden",
    "units",
    "well",
    "going",
    "set",
    "kernel",
    "size",
    "kernel",
    "size",
    "equals",
    "three",
    "stride",
    "equals",
    "one",
    "padding",
    "equals",
    "one",
    "comes",
    "next",
    "well",
    "two",
    "blocks",
    "identical",
    "con",
    "block",
    "one",
    "com",
    "two",
    "go",
    "exact",
    "combination",
    "layers",
    "relu",
    "channels",
    "equals",
    "hidden",
    "units",
    "channels",
    "equals",
    "might",
    "already",
    "know",
    "hidden",
    "units",
    "kernel",
    "size",
    "equals",
    "three",
    "oh",
    "32",
    "want",
    "big",
    "stride",
    "equals",
    "one",
    "padding",
    "equals",
    "one",
    "comes",
    "next",
    "well",
    "another",
    "relu",
    "layer",
    "relu",
    "comes",
    "another",
    "max",
    "pool",
    "max",
    "pool",
    "2d",
    "kernel",
    "size",
    "equals",
    "two",
    "beautiful",
    "coded",
    "far",
    "got",
    "block",
    "number",
    "one",
    "one",
    "inside",
    "com",
    "two",
    "relu",
    "two",
    "com",
    "two",
    "relu",
    "two",
    "max",
    "pool",
    "two",
    "built",
    "two",
    "blocks",
    "need",
    "well",
    "need",
    "output",
    "layer",
    "made",
    "model",
    "one",
    "flattened",
    "inputs",
    "final",
    "layer",
    "put",
    "last",
    "linear",
    "layer",
    "flatten",
    "going",
    "kind",
    "setup",
    "classifier",
    "layer",
    "say",
    "purpose",
    "generally",
    "hear",
    "last",
    "output",
    "layer",
    "classification",
    "model",
    "called",
    "classifier",
    "layer",
    "going",
    "two",
    "layers",
    "going",
    "feature",
    "extractors",
    "words",
    "trying",
    "learn",
    "patterns",
    "best",
    "represent",
    "data",
    "final",
    "layer",
    "going",
    "take",
    "features",
    "classify",
    "target",
    "classes",
    "whatever",
    "model",
    "thinks",
    "best",
    "suits",
    "features",
    "whatever",
    "model",
    "thinks",
    "features",
    "learned",
    "represents",
    "terms",
    "classes",
    "let",
    "code",
    "go",
    "let",
    "build",
    "classifier",
    "layer",
    "biggest",
    "neural",
    "network",
    "yet",
    "proud",
    "end",
    "sequential",
    "going",
    "pass",
    "end",
    "flatten",
    "output",
    "two",
    "blocks",
    "going",
    "tensor",
    "something",
    "similar",
    "size",
    "want",
    "flatten",
    "outputs",
    "single",
    "feature",
    "vector",
    "want",
    "pass",
    "feature",
    "vector",
    "layer",
    "going",
    "go",
    "features",
    "equals",
    "hidden",
    "units",
    "times",
    "something",
    "times",
    "something",
    "reason",
    "going",
    "find",
    "something",
    "later",
    "time",
    "zero",
    "error",
    "sometimes",
    "calculating",
    "features",
    "needs",
    "quite",
    "tricky",
    "going",
    "show",
    "trick",
    "use",
    "later",
    "figure",
    "features",
    "relates",
    "output",
    "shape",
    "length",
    "many",
    "classes",
    "right",
    "one",
    "value",
    "class",
    "said",
    "let",
    "defined",
    "components",
    "tiny",
    "vgg",
    "architecture",
    "lot",
    "going",
    "methodology",
    "using",
    "whole",
    "time",
    "defining",
    "components",
    "putting",
    "together",
    "compute",
    "way",
    "forward",
    "method",
    "forward",
    "self",
    "going",
    "going",
    "set",
    "x",
    "equal",
    "self",
    "comp",
    "block",
    "one",
    "x",
    "going",
    "go",
    "comp",
    "block",
    "one",
    "going",
    "go",
    "comp",
    "2d",
    "layer",
    "relu",
    "layer",
    "comp",
    "2d",
    "layer",
    "relu",
    "layer",
    "max",
    "pool",
    "layer",
    "equivalent",
    "image",
    "going",
    "layer",
    "layer",
    "layer",
    "layer",
    "layer",
    "ending",
    "set",
    "print",
    "x",
    "dot",
    "shape",
    "get",
    "shape",
    "check",
    "later",
    "pass",
    "x",
    "comp",
    "block",
    "two",
    "going",
    "go",
    "layers",
    "block",
    "equivalent",
    "output",
    "layer",
    "going",
    "layers",
    "constructed",
    "classifier",
    "layer",
    "going",
    "take",
    "output",
    "block",
    "going",
    "going",
    "pass",
    "output",
    "layer",
    "termed",
    "classifier",
    "layer",
    "print",
    "x",
    "dot",
    "shape",
    "track",
    "shape",
    "model",
    "moves",
    "architecture",
    "x",
    "equals",
    "self",
    "dot",
    "classifier",
    "going",
    "return",
    "look",
    "us",
    "go",
    "built",
    "first",
    "convolutional",
    "neural",
    "network",
    "replicating",
    "cnn",
    "explainer",
    "website",
    "actually",
    "common",
    "practice",
    "machine",
    "learning",
    "find",
    "sort",
    "architecture",
    "someone",
    "found",
    "work",
    "sort",
    "problem",
    "replicate",
    "code",
    "see",
    "works",
    "problem",
    "see",
    "quite",
    "often",
    "let",
    "instantiate",
    "model",
    "go",
    "torch",
    "dot",
    "manual",
    "going",
    "instantiate",
    "first",
    "convolutional",
    "neural",
    "network",
    "model",
    "two",
    "equals",
    "fashion",
    "amnest",
    "go",
    "model",
    "v",
    "two",
    "going",
    "set",
    "input",
    "shape",
    "input",
    "shape",
    "well",
    "come",
    "layer",
    "input",
    "shape",
    "number",
    "channels",
    "images",
    "image",
    "ready",
    "go",
    "image",
    "shape",
    "number",
    "color",
    "channels",
    "image",
    "one",
    "color",
    "images",
    "would",
    "set",
    "input",
    "shape",
    "three",
    "difference",
    "convolutional",
    "neural",
    "network",
    "cnn",
    "tiny",
    "vgg",
    "cnn",
    "explainer",
    "tiny",
    "vgg",
    "using",
    "color",
    "images",
    "input",
    "three",
    "one",
    "color",
    "channel",
    "red",
    "green",
    "blue",
    "whereas",
    "black",
    "white",
    "images",
    "one",
    "color",
    "channel",
    "set",
    "input",
    "shape",
    "one",
    "going",
    "go",
    "hidden",
    "units",
    "equals",
    "10",
    "exactly",
    "tiny",
    "vgg",
    "used",
    "10",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "sets",
    "hidden",
    "units",
    "value",
    "layers",
    "power",
    "creating",
    "initializer",
    "hidden",
    "units",
    "finally",
    "output",
    "shape",
    "going",
    "seen",
    "going",
    "length",
    "class",
    "names",
    "one",
    "value",
    "class",
    "data",
    "set",
    "course",
    "going",
    "send",
    "model",
    "device",
    "going",
    "hit",
    "shift",
    "enter",
    "oh",
    "get",
    "wrong",
    "channels",
    "output",
    "shape",
    "spell",
    "wrong",
    "channels",
    "channels",
    "channels",
    "forgot",
    "course",
    "typo",
    "oh",
    "kernel",
    "size",
    "typo",
    "notice",
    "kernel",
    "size",
    "kernel",
    "size",
    "kernel",
    "size",
    "kernel",
    "size",
    "spell",
    "wrong",
    "oh",
    "kernel",
    "size",
    "typos",
    "probably",
    "beautiful",
    "go",
    "okay",
    "got",
    "initializing",
    "zero",
    "obtenses",
    "oh",
    "got",
    "issue",
    "error",
    "got",
    "trick",
    "calculating",
    "going",
    "cover",
    "another",
    "video",
    "pay",
    "back",
    "written",
    "fair",
    "bit",
    "code",
    "convolutional",
    "neural",
    "network",
    "replicates",
    "tiny",
    "vgg",
    "architecture",
    "cnn",
    "explainer",
    "website",
    "forget",
    "extra",
    "curriculum",
    "go",
    "website",
    "least",
    "20",
    "minutes",
    "read",
    "happening",
    "models",
    "focused",
    "code",
    "particularly",
    "want",
    "pay",
    "attention",
    "read",
    "understanding",
    "hyper",
    "parameters",
    "play",
    "around",
    "next",
    "couple",
    "videos",
    "make",
    "lot",
    "sense",
    "read",
    "padding",
    "read",
    "kernel",
    "size",
    "read",
    "stride",
    "see",
    "next",
    "video",
    "going",
    "go",
    "network",
    "step",
    "step",
    "welcome",
    "back",
    "super",
    "stoked",
    "last",
    "video",
    "coded",
    "together",
    "first",
    "ever",
    "convolutional",
    "neural",
    "network",
    "pytorch",
    "well",
    "done",
    "replicated",
    "tiny",
    "vgg",
    "architecture",
    "cnn",
    "explainer",
    "website",
    "favorite",
    "place",
    "learning",
    "cnns",
    "browser",
    "introduced",
    "two",
    "new",
    "layers",
    "seen",
    "conv2d",
    "maxpool2d",
    "sort",
    "premise",
    "far",
    "trying",
    "learn",
    "best",
    "features",
    "represent",
    "data",
    "way",
    "shape",
    "form",
    "case",
    "maxpool2d",
    "actually",
    "learnable",
    "parameters",
    "takes",
    "max",
    "going",
    "step",
    "later",
    "let",
    "use",
    "video",
    "step",
    "conv2d",
    "going",
    "code",
    "make",
    "new",
    "heading",
    "stepping",
    "conv2d",
    "beautiful",
    "could",
    "find",
    "going",
    "end",
    "comp2d",
    "well",
    "course",
    "documentation",
    "comp2d",
    "got",
    "pytorch",
    "want",
    "learn",
    "mathematical",
    "operation",
    "happening",
    "value",
    "operation",
    "essentially",
    "saying",
    "output",
    "equal",
    "bias",
    "term",
    "times",
    "something",
    "plus",
    "sum",
    "weight",
    "times",
    "something",
    "times",
    "input",
    "see",
    "weight",
    "matrix",
    "weight",
    "tensor",
    "bias",
    "value",
    "manipulating",
    "input",
    "way",
    "equals",
    "output",
    "map",
    "got",
    "batch",
    "size",
    "channels",
    "height",
    "width",
    "channels",
    "et",
    "cetera",
    "et",
    "cetera",
    "going",
    "focus",
    "much",
    "like",
    "read",
    "let",
    "try",
    "code",
    "going",
    "reproduce",
    "particular",
    "layer",
    "first",
    "layer",
    "cnn",
    "explainer",
    "website",
    "going",
    "dummy",
    "input",
    "fact",
    "one",
    "favorite",
    "ways",
    "test",
    "things",
    "going",
    "link",
    "documentation",
    "see",
    "documentation",
    "end",
    "comp2d",
    "like",
    "read",
    "course",
    "beautiful",
    "place",
    "learn",
    "going",
    "shape",
    "calculate",
    "shape",
    "height",
    "width",
    "et",
    "cetera",
    "helpful",
    "need",
    "calculate",
    "input",
    "output",
    "shapes",
    "show",
    "trick",
    "later",
    "let",
    "create",
    "dummy",
    "data",
    "going",
    "set",
    "torch",
    "manual",
    "seed",
    "need",
    "size",
    "cnn",
    "explainer",
    "data",
    "64",
    "64",
    "going",
    "pie",
    "torch",
    "style",
    "color",
    "channels",
    "last",
    "going",
    "color",
    "channels",
    "first",
    "create",
    "batch",
    "images",
    "going",
    "writing",
    "torch",
    "dot",
    "rand",
    "going",
    "pass",
    "size",
    "equals",
    "32",
    "three",
    "64",
    "going",
    "create",
    "singular",
    "image",
    "taking",
    "first",
    "image",
    "zero",
    "let",
    "get",
    "image",
    "batch",
    "shape",
    "lot",
    "machine",
    "learning",
    "said",
    "deep",
    "learning",
    "making",
    "sure",
    "data",
    "right",
    "shape",
    "let",
    "check",
    "images",
    "dot",
    "shape",
    "let",
    "check",
    "single",
    "image",
    "shape",
    "going",
    "go",
    "test",
    "image",
    "dot",
    "shape",
    "finally",
    "going",
    "print",
    "test",
    "image",
    "look",
    "like",
    "get",
    "new",
    "line",
    "hey",
    "new",
    "line",
    "test",
    "image",
    "course",
    "going",
    "actual",
    "image",
    "going",
    "collection",
    "random",
    "numbers",
    "course",
    "model",
    "currently",
    "comprised",
    "model",
    "two",
    "look",
    "insides",
    "going",
    "see",
    "whole",
    "bunch",
    "random",
    "numbers",
    "look",
    "scroll",
    "going",
    "give",
    "us",
    "name",
    "something",
    "comp",
    "block",
    "two",
    "two",
    "weight",
    "bias",
    "keep",
    "going",
    "go",
    "right",
    "top",
    "another",
    "weight",
    "keep",
    "going",
    "bias",
    "weight",
    "et",
    "cetera",
    "et",
    "cetera",
    "model",
    "comprised",
    "random",
    "numbers",
    "trying",
    "like",
    "models",
    "pass",
    "data",
    "adjust",
    "random",
    "numbers",
    "within",
    "layers",
    "best",
    "represent",
    "data",
    "let",
    "see",
    "happens",
    "pass",
    "random",
    "data",
    "one",
    "comp2d",
    "layers",
    "let",
    "go",
    "going",
    "create",
    "single",
    "comp2d",
    "layer",
    "comp",
    "layer",
    "equals",
    "equal",
    "comp2d",
    "going",
    "set",
    "channels",
    "equal",
    "oh",
    "revealed",
    "answer",
    "quickly",
    "three",
    "three",
    "well",
    "channels",
    "number",
    "color",
    "channels",
    "images",
    "look",
    "test",
    "image",
    "shape",
    "three",
    "three",
    "color",
    "channels",
    "value",
    "except",
    "order",
    "reversed",
    "color",
    "channels",
    "last",
    "pytorch",
    "defaults",
    "color",
    "channels",
    "first",
    "future",
    "may",
    "change",
    "keep",
    "mind",
    "channels",
    "equals",
    "equivalent",
    "number",
    "hidden",
    "units",
    "one",
    "oh",
    "want",
    "one",
    "yet",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "nine",
    "10",
    "10",
    "kernel",
    "size",
    "oh",
    "kernel",
    "well",
    "kfc",
    "tell",
    "stride",
    "padding",
    "going",
    "step",
    "second",
    "let",
    "check",
    "kernel",
    "kernel",
    "also",
    "three",
    "three",
    "shortcut",
    "type",
    "three",
    "actually",
    "means",
    "type",
    "single",
    "number",
    "equivalent",
    "typing",
    "tuple",
    "course",
    "could",
    "find",
    "reading",
    "documentation",
    "get",
    "value",
    "well",
    "let",
    "dive",
    "beautiful",
    "website",
    "let",
    "see",
    "happening",
    "kernel",
    "also",
    "called",
    "filter",
    "thing",
    "talking",
    "little",
    "square",
    "kernel",
    "oh",
    "see",
    "weights",
    "top",
    "beautiful",
    "website",
    "go",
    "going",
    "happen",
    "convolution",
    "starts",
    "little",
    "square",
    "moves",
    "pixel",
    "pixel",
    "across",
    "image",
    "notice",
    "output",
    "creating",
    "sort",
    "number",
    "notice",
    "middle",
    "mathematical",
    "operation",
    "operation",
    "happening",
    "wait",
    "times",
    "input",
    "got",
    "beauty",
    "pytorch",
    "behind",
    "scenes",
    "us",
    "like",
    "dig",
    "mathematical",
    "operation",
    "behind",
    "scenes",
    "got",
    "resource",
    "also",
    "got",
    "plenty",
    "resources",
    "online",
    "going",
    "focus",
    "code",
    "keep",
    "across",
    "entire",
    "image",
    "get",
    "output",
    "kernel",
    "get",
    "three",
    "three",
    "well",
    "look",
    "one",
    "two",
    "three",
    "one",
    "two",
    "three",
    "one",
    "two",
    "three",
    "three",
    "three",
    "nine",
    "squares",
    "scroll",
    "extracurricular",
    "last",
    "video",
    "understanding",
    "hyperparameters",
    "happens",
    "change",
    "kernel",
    "size",
    "three",
    "three",
    "look",
    "red",
    "square",
    "left",
    "change",
    "two",
    "two",
    "changed",
    "three",
    "three",
    "kernel",
    "also",
    "known",
    "filter",
    "passing",
    "across",
    "image",
    "performing",
    "sort",
    "mathematical",
    "operation",
    "whole",
    "idea",
    "convolutional",
    "layer",
    "try",
    "make",
    "sure",
    "kernel",
    "performs",
    "right",
    "operation",
    "get",
    "right",
    "output",
    "kernels",
    "learn",
    "well",
    "entirely",
    "model",
    "beauty",
    "deep",
    "learning",
    "learns",
    "best",
    "represent",
    "data",
    "hopefully",
    "looking",
    "data",
    "jump",
    "back",
    "equivalent",
    "setting",
    "kernel",
    "size",
    "three",
    "three",
    "set",
    "stride",
    "equal",
    "one",
    "got",
    "right",
    "order",
    "really",
    "matter",
    "let",
    "go",
    "stride",
    "next",
    "go",
    "stride",
    "say",
    "stride",
    "convolution",
    "convolving",
    "kernel",
    "default",
    "one",
    "wonderful",
    "set",
    "stride",
    "keep",
    "one",
    "default",
    "one",
    "going",
    "hop",
    "watch",
    "red",
    "square",
    "left",
    "going",
    "hop",
    "one",
    "pixel",
    "time",
    "convolution",
    "convolving",
    "happens",
    "one",
    "pixel",
    "time",
    "stride",
    "sets",
    "watch",
    "happens",
    "change",
    "stride",
    "value",
    "output",
    "shape",
    "wow",
    "notice",
    "went",
    "kernel",
    "size",
    "still",
    "jumping",
    "two",
    "pixels",
    "time",
    "notice",
    "left",
    "two",
    "pixels",
    "become",
    "available",
    "jump",
    "two",
    "pixels",
    "reason",
    "output",
    "compresses",
    "skipping",
    "pixels",
    "go",
    "across",
    "image",
    "pattern",
    "happens",
    "throughout",
    "entire",
    "network",
    "one",
    "reasons",
    "see",
    "size",
    "input",
    "size",
    "layer",
    "go",
    "time",
    "convolutional",
    "layer",
    "fact",
    "lot",
    "deep",
    "learning",
    "neural",
    "networks",
    "try",
    "compress",
    "input",
    "representation",
    "best",
    "suits",
    "data",
    "would",
    "point",
    "memorizing",
    "exact",
    "patterns",
    "want",
    "compress",
    "way",
    "otherwise",
    "might",
    "well",
    "move",
    "input",
    "data",
    "around",
    "want",
    "learn",
    "generalizable",
    "patterns",
    "move",
    "around",
    "keep",
    "going",
    "got",
    "padding",
    "equals",
    "zero",
    "let",
    "see",
    "happens",
    "change",
    "padding",
    "value",
    "happens",
    "notice",
    "size",
    "oh",
    "added",
    "two",
    "extra",
    "pixels",
    "around",
    "edge",
    "go",
    "one",
    "extra",
    "pixel",
    "go",
    "zero",
    "might",
    "add",
    "padding",
    "end",
    "well",
    "kernel",
    "operate",
    "going",
    "corner",
    "case",
    "information",
    "edges",
    "image",
    "might",
    "thinking",
    "daniel",
    "whole",
    "bunch",
    "values",
    "know",
    "set",
    "well",
    "notice",
    "copied",
    "exactly",
    "going",
    "three",
    "three",
    "kernel",
    "padding",
    "image",
    "stride",
    "going",
    "one",
    "one",
    "often",
    "common",
    "machine",
    "learning",
    "getting",
    "started",
    "sure",
    "values",
    "set",
    "values",
    "copy",
    "existing",
    "values",
    "somewhere",
    "see",
    "works",
    "problem",
    "well",
    "adjust",
    "let",
    "see",
    "happens",
    "pass",
    "data",
    "convolutional",
    "layer",
    "let",
    "see",
    "happens",
    "conv",
    "output",
    "equals",
    "conv",
    "layer",
    "let",
    "pass",
    "test",
    "image",
    "check",
    "conv",
    "output",
    "happens",
    "oh",
    "get",
    "error",
    "course",
    "get",
    "shape",
    "error",
    "one",
    "common",
    "issues",
    "machine",
    "learning",
    "deep",
    "learning",
    "saying",
    "input",
    "conv",
    "layer",
    "expects",
    "four",
    "dimensional",
    "tensor",
    "except",
    "got",
    "three",
    "dimensional",
    "input",
    "size",
    "364",
    "add",
    "extra",
    "dimension",
    "test",
    "image",
    "let",
    "look",
    "would",
    "add",
    "batch",
    "dimension",
    "left",
    "go",
    "unsqueeze",
    "zero",
    "four",
    "dimensional",
    "tensor",
    "keep",
    "mind",
    "running",
    "layer",
    "com2d",
    "pytorch",
    "version",
    "believe",
    "fixed",
    "changed",
    "pytorch",
    "think",
    "google",
    "collab",
    "instance",
    "think",
    "might",
    "get",
    "error",
    "running",
    "keep",
    "mind",
    "like",
    "work",
    "running",
    "always",
    "unsqueeze",
    "let",
    "see",
    "happens",
    "look",
    "get",
    "another",
    "tensor",
    "output",
    "random",
    "numbers",
    "though",
    "test",
    "image",
    "random",
    "numbers",
    "conv",
    "layer",
    "instantiated",
    "random",
    "numbers",
    "set",
    "manual",
    "seed",
    "numbers",
    "different",
    "numbers",
    "different",
    "screen",
    "worry",
    "much",
    "conv",
    "layer",
    "instantiated",
    "random",
    "numbers",
    "test",
    "image",
    "random",
    "numbers",
    "well",
    "paying",
    "attention",
    "input",
    "output",
    "shapes",
    "see",
    "happened",
    "put",
    "input",
    "image",
    "three",
    "channels",
    "set",
    "channels",
    "10",
    "got",
    "got",
    "62",
    "batch",
    "size",
    "means",
    "one",
    "image",
    "essentially",
    "random",
    "numbers",
    "test",
    "image",
    "gone",
    "convolutional",
    "layer",
    "created",
    "gone",
    "mathematical",
    "operation",
    "regards",
    "values",
    "set",
    "put",
    "weight",
    "tensor",
    "well",
    "actually",
    "pytorch",
    "created",
    "us",
    "pytorch",
    "done",
    "whole",
    "operation",
    "us",
    "thank",
    "pytorch",
    "gone",
    "steps",
    "across",
    "could",
    "code",
    "hand",
    "want",
    "lot",
    "easier",
    "simpler",
    "use",
    "pytorch",
    "layer",
    "done",
    "created",
    "output",
    "whatever",
    "output",
    "know",
    "random",
    "numbers",
    "process",
    "happen",
    "use",
    "actual",
    "data",
    "well",
    "let",
    "see",
    "happens",
    "change",
    "values",
    "kernel",
    "size",
    "increase",
    "notice",
    "output",
    "gotten",
    "smaller",
    "using",
    "bigger",
    "kernel",
    "convolve",
    "across",
    "image",
    "put",
    "three",
    "three",
    "back",
    "stride",
    "two",
    "think",
    "happen",
    "well",
    "output",
    "size",
    "basically",
    "halves",
    "skipping",
    "two",
    "pixels",
    "time",
    "put",
    "back",
    "one",
    "think",
    "happen",
    "set",
    "padding",
    "one",
    "64",
    "get",
    "basically",
    "size",
    "added",
    "extra",
    "pixel",
    "around",
    "edges",
    "play",
    "around",
    "fact",
    "encourage",
    "padding",
    "one",
    "added",
    "extra",
    "dummy",
    "zero",
    "pixel",
    "around",
    "edges",
    "practice",
    "see",
    "happens",
    "pass",
    "test",
    "image",
    "random",
    "numbers",
    "conv",
    "2d",
    "layer",
    "different",
    "values",
    "think",
    "happen",
    "change",
    "64",
    "give",
    "shot",
    "see",
    "next",
    "video",
    "ready",
    "step",
    "nn",
    "max",
    "pool",
    "2d",
    "layer",
    "put",
    "hand",
    "got",
    "hand",
    "let",
    "together",
    "hey",
    "got",
    "might",
    "already",
    "given",
    "shot",
    "stepping",
    "nn",
    "max",
    "pool",
    "2d",
    "lot",
    "different",
    "concepts",
    "gone",
    "write",
    "test",
    "code",
    "see",
    "inputs",
    "outputs",
    "could",
    "find",
    "max",
    "pool",
    "2d",
    "well",
    "course",
    "got",
    "documentation",
    "going",
    "link",
    "max",
    "pool",
    "2d",
    "simplest",
    "case",
    "output",
    "value",
    "layer",
    "input",
    "size",
    "nchw",
    "output",
    "nch",
    "w",
    "way",
    "number",
    "batches",
    "color",
    "channels",
    "height",
    "width",
    "output",
    "layer",
    "kernel",
    "size",
    "parameter",
    "k",
    "h",
    "k",
    "w",
    "precisely",
    "described",
    "going",
    "max",
    "value",
    "depending",
    "kernel",
    "size",
    "stride",
    "let",
    "look",
    "practice",
    "course",
    "read",
    "documentation",
    "grab",
    "link",
    "actually",
    "wonderful",
    "let",
    "first",
    "try",
    "test",
    "image",
    "created",
    "highlight",
    "test",
    "image",
    "bunch",
    "random",
    "numbers",
    "shape",
    "single",
    "image",
    "would",
    "replicate",
    "image",
    "size",
    "cnn",
    "explainer",
    "way",
    "look",
    "visual",
    "second",
    "max",
    "pool",
    "go",
    "time",
    "let",
    "doubt",
    "code",
    "going",
    "print",
    "original",
    "image",
    "shape",
    "without",
    "unsqueezed",
    "dimension",
    "recall",
    "add",
    "extra",
    "dimension",
    "pass",
    "com2d",
    "layer",
    "using",
    "later",
    "version",
    "pytorch",
    "might",
    "get",
    "error",
    "use",
    "three",
    "dimensional",
    "image",
    "tensor",
    "pass",
    "comp",
    "layer",
    "going",
    "pass",
    "test",
    "image",
    "original",
    "shape",
    "test",
    "image",
    "dot",
    "shape",
    "going",
    "tell",
    "us",
    "line",
    "code",
    "cell",
    "tells",
    "us",
    "fine",
    "like",
    "make",
    "pretty",
    "printouts",
    "know",
    "test",
    "image",
    "unsqueezed",
    "dimension",
    "going",
    "test",
    "image",
    "going",
    "see",
    "happens",
    "unsqueeze",
    "dimension",
    "unsqueeze",
    "zero",
    "dimension",
    "say",
    "first",
    "zero",
    "going",
    "create",
    "sample",
    "nn",
    "max",
    "pool",
    "2d",
    "layer",
    "remember",
    "even",
    "layers",
    "torch",
    "dot",
    "nn",
    "models",
    "accord",
    "create",
    "single",
    "like",
    "creating",
    "single",
    "layer",
    "model",
    "set",
    "kernel",
    "size",
    "equal",
    "two",
    "recall",
    "go",
    "back",
    "cnn",
    "explainer",
    "kernel",
    "size",
    "equal",
    "two",
    "results",
    "two",
    "two",
    "square",
    "two",
    "two",
    "kernel",
    "going",
    "convolve",
    "image",
    "like",
    "example",
    "input",
    "example",
    "output",
    "see",
    "operation",
    "max",
    "pooling",
    "keep",
    "mind",
    "pass",
    "sample",
    "data",
    "max",
    "pool",
    "layer",
    "let",
    "pass",
    "data",
    "actually",
    "pass",
    "conv",
    "layer",
    "first",
    "conv",
    "layer",
    "sort",
    "might",
    "stack",
    "things",
    "might",
    "put",
    "convolutional",
    "layer",
    "max",
    "pool",
    "layer",
    "top",
    "convolutional",
    "layer",
    "test",
    "image",
    "conv",
    "create",
    "variable",
    "equals",
    "conv",
    "layer",
    "going",
    "take",
    "input",
    "test",
    "image",
    "dot",
    "unsqueeze",
    "zero",
    "dimension",
    "beautiful",
    "going",
    "print",
    "shape",
    "highlighting",
    "like",
    "troubleshoot",
    "things",
    "one",
    "step",
    "print",
    "shape",
    "one",
    "step",
    "print",
    "shape",
    "see",
    "happening",
    "data",
    "moves",
    "various",
    "layers",
    "test",
    "image",
    "see",
    "conv",
    "layer",
    "shape",
    "data",
    "going",
    "pass",
    "data",
    "max",
    "pool",
    "layer",
    "layer",
    "created",
    "couple",
    "lines",
    "one",
    "let",
    "see",
    "happens",
    "test",
    "image",
    "current",
    "type",
    "moment",
    "conv",
    "max",
    "pool",
    "quite",
    "long",
    "variable",
    "name",
    "help",
    "us",
    "avoid",
    "confusion",
    "going",
    "go",
    "test",
    "image",
    "conv",
    "notice",
    "taking",
    "output",
    "convolutional",
    "layer",
    "passing",
    "max",
    "pool",
    "layer",
    "another",
    "typo",
    "wonderful",
    "finally",
    "print",
    "shape",
    "shape",
    "going",
    "conv",
    "layer",
    "max",
    "pool",
    "layer",
    "happens",
    "want",
    "test",
    "image",
    "conv",
    "max",
    "pool",
    "let",
    "see",
    "max",
    "pool",
    "layer",
    "manipulates",
    "test",
    "images",
    "shape",
    "ready",
    "three",
    "two",
    "one",
    "let",
    "go",
    "get",
    "okay",
    "test",
    "image",
    "original",
    "shape",
    "recall",
    "test",
    "image",
    "collection",
    "random",
    "numbers",
    "course",
    "conv",
    "layer",
    "going",
    "instantiated",
    "random",
    "numbers",
    "max",
    "pool",
    "actually",
    "parameters",
    "takes",
    "maximum",
    "certain",
    "range",
    "inner",
    "tensor",
    "unsqueeze",
    "test",
    "image",
    "input",
    "get",
    "extra",
    "dimension",
    "pass",
    "conv",
    "layer",
    "oh",
    "64",
    "come",
    "164",
    "64",
    "64",
    "let",
    "go",
    "back",
    "conv",
    "layer",
    "notice",
    "get",
    "64",
    "changed",
    "channels",
    "value",
    "change",
    "back",
    "10",
    "like",
    "cnn",
    "explainer",
    "model",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "nine",
    "think",
    "happen",
    "well",
    "get",
    "little",
    "highlight",
    "keep",
    "going",
    "get",
    "rid",
    "extra",
    "cell",
    "need",
    "check",
    "version",
    "anymore",
    "check",
    "test",
    "image",
    "shapes",
    "still",
    "three",
    "64",
    "pass",
    "conv",
    "layer",
    "get",
    "different",
    "size",
    "originally",
    "three",
    "channels",
    "input",
    "color",
    "channels",
    "upscaled",
    "10",
    "10",
    "hidden",
    "units",
    "layer",
    "64",
    "shapes",
    "change",
    "change",
    "values",
    "going",
    "might",
    "put",
    "padding",
    "zero",
    "happens",
    "instead",
    "64",
    "64",
    "get",
    "62",
    "happens",
    "pass",
    "conv",
    "layer",
    "max",
    "pool",
    "layer",
    "got",
    "110",
    "64",
    "110",
    "32",
    "well",
    "let",
    "go",
    "back",
    "cnn",
    "explainer",
    "jump",
    "max",
    "pool",
    "layer",
    "maybe",
    "one",
    "got",
    "bit",
    "going",
    "notice",
    "left",
    "input",
    "got",
    "two",
    "two",
    "kernel",
    "max",
    "pooling",
    "layer",
    "takes",
    "maximum",
    "whatever",
    "input",
    "notice",
    "input",
    "60",
    "60",
    "case",
    "whereas",
    "output",
    "30",
    "well",
    "max",
    "operation",
    "reducing",
    "section",
    "four",
    "numbers",
    "let",
    "get",
    "one",
    "different",
    "numbers",
    "go",
    "taking",
    "four",
    "numbers",
    "finding",
    "maximum",
    "value",
    "within",
    "four",
    "numbers",
    "would",
    "discussed",
    "deep",
    "learning",
    "neural",
    "network",
    "trying",
    "case",
    "cnn",
    "take",
    "input",
    "data",
    "figure",
    "features",
    "best",
    "represent",
    "whatever",
    "input",
    "data",
    "compress",
    "feature",
    "vector",
    "going",
    "output",
    "reason",
    "could",
    "consider",
    "neural",
    "networks",
    "perspective",
    "intelligence",
    "compression",
    "trying",
    "compress",
    "patterns",
    "make",
    "actual",
    "data",
    "smaller",
    "vector",
    "space",
    "go",
    "higher",
    "dimensional",
    "space",
    "smaller",
    "vector",
    "space",
    "terms",
    "dimensionality",
    "tensor",
    "still",
    "smaller",
    "dimensionality",
    "space",
    "represents",
    "original",
    "data",
    "used",
    "predict",
    "future",
    "data",
    "idea",
    "behind",
    "max",
    "paul",
    "hey",
    "got",
    "learned",
    "features",
    "convolutional",
    "layers",
    "patterns",
    "important",
    "patterns",
    "stay",
    "around",
    "take",
    "maximum",
    "certain",
    "section",
    "notice",
    "input",
    "still",
    "still",
    "see",
    "outline",
    "car",
    "albeit",
    "little",
    "bit",
    "pixelated",
    "taking",
    "max",
    "certain",
    "region",
    "got",
    "potentially",
    "important",
    "feature",
    "little",
    "section",
    "course",
    "could",
    "customize",
    "value",
    "create",
    "max",
    "pool",
    "layer",
    "could",
    "increase",
    "kernel",
    "size",
    "four",
    "four",
    "think",
    "happen",
    "increase",
    "four",
    "got",
    "two",
    "two",
    "kernel",
    "increase",
    "four",
    "four",
    "happens",
    "ah",
    "notice",
    "gone",
    "62",
    "15",
    "essentially",
    "divided",
    "feature",
    "space",
    "four",
    "compressed",
    "even",
    "work",
    "well",
    "sure",
    "part",
    "experimental",
    "nature",
    "machine",
    "learning",
    "going",
    "keep",
    "two",
    "tensor",
    "let",
    "done",
    "smaller",
    "tensor",
    "really",
    "visualize",
    "things",
    "going",
    "replicate",
    "operation",
    "going",
    "let",
    "go",
    "create",
    "another",
    "random",
    "tensor",
    "set",
    "manual",
    "seed",
    "first",
    "going",
    "create",
    "random",
    "tensor",
    "similar",
    "number",
    "dimensions",
    "recall",
    "dimensions",
    "tell",
    "dimension",
    "1364",
    "dimension",
    "dimensions",
    "different",
    "values",
    "within",
    "want",
    "create",
    "four",
    "dimensional",
    "tensor",
    "images",
    "means",
    "let",
    "show",
    "way",
    "easy",
    "explain",
    "things",
    "got",
    "code",
    "torch",
    "dot",
    "rand",
    "going",
    "set",
    "size",
    "equals",
    "one",
    "one",
    "two",
    "two",
    "look",
    "random",
    "tensor",
    "got",
    "four",
    "dimensions",
    "one",
    "two",
    "three",
    "four",
    "could",
    "batch",
    "size",
    "color",
    "channels",
    "height",
    "width",
    "small",
    "image",
    "random",
    "image",
    "quite",
    "similar",
    "got",
    "going",
    "right",
    "four",
    "numbers",
    "think",
    "happen",
    "create",
    "max",
    "pool",
    "layer",
    "like",
    "done",
    "create",
    "max",
    "pool",
    "layer",
    "go",
    "max",
    "pool",
    "layer",
    "repeating",
    "code",
    "cell",
    "right",
    "little",
    "bit",
    "practice",
    "kernel",
    "size",
    "equals",
    "two",
    "going",
    "pass",
    "random",
    "tensor",
    "max",
    "pool",
    "layer",
    "go",
    "max",
    "pool",
    "tensor",
    "equals",
    "max",
    "pool",
    "layer",
    "going",
    "pass",
    "random",
    "tensor",
    "wonderful",
    "print",
    "shapes",
    "print",
    "tenses",
    "always",
    "visualize",
    "visualize",
    "visualize",
    "going",
    "write",
    "max",
    "pool",
    "tensor",
    "new",
    "line",
    "get",
    "max",
    "pool",
    "tensor",
    "see",
    "looks",
    "like",
    "also",
    "print",
    "max",
    "pool",
    "tensor",
    "shape",
    "probably",
    "print",
    "random",
    "tensor",
    "well",
    "shape",
    "well",
    "get",
    "shape",
    "dot",
    "shape",
    "random",
    "tensor",
    "print",
    "get",
    "new",
    "line",
    "random",
    "tensor",
    "new",
    "line",
    "random",
    "tensor",
    "get",
    "shape",
    "random",
    "tensor",
    "shape",
    "random",
    "tensor",
    "oh",
    "lot",
    "coding",
    "fun",
    "part",
    "machine",
    "learning",
    "right",
    "get",
    "write",
    "lots",
    "code",
    "okay",
    "visualizing",
    "going",
    "random",
    "tensor",
    "happening",
    "within",
    "max",
    "pool",
    "layer",
    "seen",
    "different",
    "angles",
    "random",
    "tensor",
    "numbers",
    "got",
    "size",
    "max",
    "pool",
    "tensor",
    "pass",
    "random",
    "tensor",
    "max",
    "pool",
    "layer",
    "happens",
    "well",
    "1288",
    "2345",
    "max",
    "well",
    "takes",
    "max",
    "oh",
    "got",
    "random",
    "tensor",
    "want",
    "see",
    "reduced",
    "shape",
    "two",
    "two",
    "one",
    "one",
    "going",
    "one",
    "last",
    "time",
    "reiterate",
    "convolutional",
    "layer",
    "trying",
    "learn",
    "important",
    "features",
    "within",
    "image",
    "jump",
    "well",
    "decide",
    "convolutional",
    "layer",
    "learns",
    "learns",
    "features",
    "convolutional",
    "layer",
    "learns",
    "features",
    "pass",
    "relu",
    "nonlinear",
    "activation",
    "case",
    "data",
    "requires",
    "nonlinear",
    "functions",
    "pass",
    "learned",
    "features",
    "max",
    "pool",
    "layer",
    "compress",
    "even",
    "convolutional",
    "layer",
    "compress",
    "features",
    "smaller",
    "space",
    "max",
    "pooling",
    "layer",
    "really",
    "compresses",
    "entire",
    "idea",
    "one",
    "time",
    "start",
    "input",
    "data",
    "design",
    "neural",
    "network",
    "case",
    "convolutional",
    "neural",
    "network",
    "learn",
    "compressed",
    "representation",
    "input",
    "data",
    "use",
    "compressed",
    "representation",
    "later",
    "make",
    "predictions",
    "images",
    "fact",
    "try",
    "wanted",
    "click",
    "add",
    "image",
    "give",
    "go",
    "extension",
    "video",
    "stepped",
    "max",
    "pool",
    "2d",
    "layer",
    "conv",
    "2d",
    "layer",
    "think",
    "time",
    "started",
    "try",
    "use",
    "tiny",
    "vgg",
    "network",
    "challenge",
    "create",
    "dummy",
    "tensor",
    "pass",
    "model",
    "pass",
    "forward",
    "layer",
    "see",
    "happens",
    "shape",
    "dummy",
    "tensor",
    "moves",
    "conv",
    "block",
    "1",
    "conv",
    "block",
    "show",
    "trick",
    "calculating",
    "features",
    "final",
    "layer",
    "equivalent",
    "final",
    "layer",
    "see",
    "next",
    "video",
    "last",
    "videos",
    "replicating",
    "tiny",
    "vgg",
    "architecture",
    "cnn",
    "explainer",
    "website",
    "hope",
    "know",
    "actually",
    "quite",
    "exciting",
    "years",
    "ago",
    "would",
    "taken",
    "months",
    "work",
    "covered",
    "broken",
    "last",
    "videos",
    "rebuilt",
    "lines",
    "pytorch",
    "code",
    "goes",
    "show",
    "powerful",
    "pytorch",
    "far",
    "deep",
    "learning",
    "field",
    "come",
    "finished",
    "yet",
    "let",
    "go",
    "keynote",
    "done",
    "cnn",
    "explainer",
    "model",
    "input",
    "layer",
    "created",
    "com2d",
    "layers",
    "created",
    "relo",
    "activation",
    "layers",
    "created",
    "finally",
    "pulling",
    "layers",
    "finish",
    "output",
    "layer",
    "let",
    "see",
    "happens",
    "actually",
    "pass",
    "data",
    "entire",
    "model",
    "said",
    "actually",
    "quite",
    "common",
    "practice",
    "replicate",
    "model",
    "found",
    "somewhere",
    "test",
    "data",
    "going",
    "start",
    "using",
    "dummy",
    "data",
    "make",
    "sure",
    "model",
    "works",
    "going",
    "pass",
    "oh",
    "got",
    "another",
    "slide",
    "way",
    "breakdown",
    "torch",
    "n",
    "com2d",
    "like",
    "see",
    "text",
    "form",
    "nothing",
    "really",
    "discussed",
    "slides",
    "would",
    "like",
    "see",
    "video",
    "animation",
    "seen",
    "though",
    "plus",
    "rather",
    "go",
    "cnn",
    "explainer",
    "website",
    "explore",
    "different",
    "values",
    "rather",
    "keep",
    "talking",
    "working",
    "towards",
    "fashion",
    "mnist",
    "data",
    "set",
    "inputs",
    "going",
    "numerically",
    "encode",
    "done",
    "already",
    "convolutional",
    "neural",
    "network",
    "combination",
    "convolutional",
    "layers",
    "nonlinear",
    "activation",
    "layers",
    "pooling",
    "layers",
    "could",
    "comprised",
    "many",
    "different",
    "ways",
    "shapes",
    "forms",
    "case",
    "replicated",
    "tiny",
    "vgg",
    "architecture",
    "finally",
    "want",
    "output",
    "layer",
    "predict",
    "class",
    "clothing",
    "particular",
    "input",
    "image",
    "let",
    "go",
    "back",
    "cnn",
    "model",
    "got",
    "model",
    "two",
    "let",
    "practice",
    "dummy",
    "forward",
    "pass",
    "going",
    "come",
    "back",
    "bit",
    "make",
    "sure",
    "got",
    "model",
    "two",
    "get",
    "error",
    "times",
    "zero",
    "going",
    "remove",
    "keep",
    "let",
    "see",
    "happens",
    "create",
    "dummy",
    "tensor",
    "pass",
    "recall",
    "image",
    "image",
    "fashion",
    "mnist",
    "image",
    "wonder",
    "go",
    "plot",
    "dot",
    "show",
    "image",
    "going",
    "squeeze",
    "going",
    "set",
    "c",
    "map",
    "equal",
    "gray",
    "current",
    "image",
    "wonderful",
    "current",
    "image",
    "let",
    "create",
    "tensor",
    "maybe",
    "try",
    "pass",
    "model",
    "see",
    "happens",
    "try",
    "model",
    "image",
    "right",
    "going",
    "try",
    "first",
    "pass",
    "forward",
    "pass",
    "pass",
    "image",
    "model",
    "going",
    "happen",
    "well",
    "get",
    "error",
    "another",
    "shape",
    "mismatch",
    "seen",
    "deal",
    "shape",
    "current",
    "image",
    "128",
    "image",
    "instantiated",
    "might",
    "go",
    "back",
    "cells",
    "create",
    "image",
    "find",
    "created",
    "fairly",
    "long",
    "time",
    "ago",
    "going",
    "probably",
    "recreate",
    "bottom",
    "goodness",
    "written",
    "lot",
    "code",
    "well",
    "us",
    "could",
    "create",
    "dummy",
    "tensor",
    "wanted",
    "want",
    "find",
    "oh",
    "right",
    "back",
    "image",
    "dummy",
    "tensor",
    "fine",
    "create",
    "one",
    "size",
    "image",
    "instantiated",
    "try",
    "image",
    "let",
    "create",
    "image",
    "random",
    "tensor",
    "shape",
    "image",
    "rand",
    "image",
    "tensor",
    "equals",
    "torch",
    "dot",
    "rand",
    "going",
    "pass",
    "size",
    "equals",
    "128",
    "get",
    "rand",
    "image",
    "tensor",
    "check",
    "shape",
    "get",
    "shape",
    "test",
    "image",
    "going",
    "random",
    "numbers",
    "okay",
    "want",
    "highlight",
    "point",
    "input",
    "output",
    "shapes",
    "want",
    "make",
    "sure",
    "model",
    "works",
    "random",
    "image",
    "tensor",
    "go",
    "way",
    "model",
    "want",
    "find",
    "get",
    "error",
    "four",
    "dimensions",
    "image",
    "three",
    "dimensions",
    "add",
    "extra",
    "dimension",
    "batch",
    "size",
    "might",
    "get",
    "error",
    "running",
    "later",
    "version",
    "pie",
    "torch",
    "keep",
    "mind",
    "unsqueeze",
    "zero",
    "oh",
    "expected",
    "tensors",
    "device",
    "found",
    "least",
    "two",
    "devices",
    "going",
    "three",
    "major",
    "issues",
    "deep",
    "learning",
    "shape",
    "mismatch",
    "device",
    "mismatch",
    "data",
    "type",
    "mismatch",
    "let",
    "put",
    "device",
    "two",
    "target",
    "device",
    "set",
    "device",
    "agnostic",
    "code",
    "one",
    "two",
    "shapes",
    "multiplied",
    "oh",
    "output",
    "exciting",
    "might",
    "move",
    "couple",
    "cells",
    "tell",
    "going",
    "going",
    "delete",
    "cell",
    "shapes",
    "come",
    "well",
    "printed",
    "shapes",
    "happened",
    "create",
    "random",
    "tensor",
    "bring",
    "random",
    "tensor",
    "bit",
    "let",
    "bring",
    "go",
    "pass",
    "random",
    "image",
    "tensor",
    "model",
    "made",
    "sure",
    "got",
    "four",
    "dimensions",
    "unsqueeze",
    "zero",
    "make",
    "sure",
    "device",
    "model",
    "model",
    "sent",
    "gpu",
    "happens",
    "pass",
    "random",
    "image",
    "tensor",
    "got",
    "12828",
    "instead",
    "previously",
    "seen",
    "going",
    "clean",
    "bit",
    "get",
    "different",
    "shapes",
    "notice",
    "input",
    "goes",
    "layers",
    "gets",
    "shaped",
    "different",
    "values",
    "going",
    "universal",
    "across",
    "different",
    "data",
    "sets",
    "work",
    "working",
    "different",
    "shapes",
    "important",
    "also",
    "quite",
    "fun",
    "troubleshoot",
    "shapes",
    "need",
    "use",
    "different",
    "layers",
    "trick",
    "comes",
    "find",
    "shapes",
    "different",
    "layers",
    "often",
    "construct",
    "models",
    "done",
    "best",
    "information",
    "got",
    "replicating",
    "really",
    "know",
    "output",
    "shape",
    "going",
    "goes",
    "final",
    "layer",
    "recreate",
    "model",
    "best",
    "pass",
    "data",
    "form",
    "dummy",
    "tensor",
    "shape",
    "actual",
    "data",
    "could",
    "customize",
    "shape",
    "wanted",
    "print",
    "shapes",
    "happening",
    "forward",
    "past",
    "steps",
    "pass",
    "random",
    "tensor",
    "first",
    "column",
    "block",
    "goes",
    "layers",
    "outputs",
    "tensor",
    "size",
    "got",
    "10",
    "many",
    "output",
    "channels",
    "set",
    "14",
    "14",
    "2828",
    "tensor",
    "gone",
    "max",
    "pool",
    "2d",
    "layer",
    "gone",
    "convolutional",
    "layer",
    "goes",
    "next",
    "block",
    "column",
    "block",
    "two",
    "put",
    "forward",
    "method",
    "outputs",
    "shape",
    "go",
    "back",
    "shape",
    "one",
    "10",
    "seven",
    "seven",
    "previous",
    "tensor",
    "output",
    "column",
    "block",
    "one",
    "gone",
    "1414",
    "seven",
    "seven",
    "compressed",
    "let",
    "write",
    "output",
    "shape",
    "column",
    "block",
    "one",
    "get",
    "little",
    "bit",
    "information",
    "going",
    "copy",
    "put",
    "become",
    "block",
    "two",
    "finally",
    "want",
    "know",
    "get",
    "output",
    "shape",
    "classifier",
    "rerun",
    "get",
    "output",
    "shape",
    "classifier",
    "model",
    "running",
    "trouble",
    "gets",
    "get",
    "output",
    "conv",
    "block",
    "one",
    "get",
    "output",
    "classifier",
    "telling",
    "issue",
    "classifier",
    "layer",
    "know",
    "know",
    "well",
    "coded",
    "model",
    "features",
    "need",
    "special",
    "calculation",
    "going",
    "shapes",
    "mat",
    "one",
    "mat",
    "two",
    "shapes",
    "multiplied",
    "see",
    "rule",
    "matrix",
    "multiplication",
    "inner",
    "dimensions",
    "match",
    "got",
    "could",
    "number",
    "come",
    "got",
    "10",
    "times",
    "okay",
    "know",
    "set",
    "hidden",
    "units",
    "maybe",
    "10",
    "came",
    "output",
    "layer",
    "output",
    "shape",
    "conv",
    "block",
    "two",
    "look",
    "got",
    "output",
    "shape",
    "conv",
    "block",
    "two",
    "go",
    "output",
    "conv",
    "block",
    "two",
    "goes",
    "classifier",
    "model",
    "gets",
    "flattened",
    "telling",
    "us",
    "something",
    "nn",
    "linear",
    "layer",
    "expecting",
    "output",
    "flatten",
    "layer",
    "features",
    "trick",
    "comes",
    "play",
    "pass",
    "output",
    "conv",
    "block",
    "two",
    "classifier",
    "layer",
    "gets",
    "flattened",
    "nn",
    "linear",
    "layer",
    "expecting",
    "happens",
    "flatten",
    "shape",
    "get",
    "value",
    "let",
    "look",
    "go",
    "10",
    "times",
    "seven",
    "times",
    "seven",
    "10",
    "well",
    "hidden",
    "units",
    "sevens",
    "well",
    "sevens",
    "output",
    "conv",
    "block",
    "two",
    "trick",
    "print",
    "shapes",
    "previous",
    "layers",
    "see",
    "whether",
    "line",
    "subsequent",
    "layers",
    "go",
    "time",
    "seven",
    "times",
    "seven",
    "going",
    "hidden",
    "units",
    "equals",
    "10",
    "times",
    "seven",
    "times",
    "seven",
    "get",
    "two",
    "sevens",
    "output",
    "shape",
    "conv",
    "block",
    "two",
    "see",
    "little",
    "bit",
    "hard",
    "calculate",
    "ahead",
    "time",
    "could",
    "calculate",
    "hand",
    "went",
    "n",
    "conv",
    "2d",
    "prefer",
    "write",
    "code",
    "calculate",
    "things",
    "calculate",
    "value",
    "hand",
    "go",
    "h",
    "w",
    "add",
    "together",
    "different",
    "parameters",
    "multiply",
    "divide",
    "whatnot",
    "calculate",
    "input",
    "output",
    "shapes",
    "convolutional",
    "layers",
    "welcome",
    "try",
    "hand",
    "prefer",
    "code",
    "code",
    "let",
    "see",
    "happens",
    "run",
    "random",
    "image",
    "tensor",
    "model",
    "think",
    "work",
    "well",
    "let",
    "find",
    "done",
    "added",
    "little",
    "line",
    "times",
    "seven",
    "times",
    "seven",
    "calculated",
    "gone",
    "huh",
    "pass",
    "tensor",
    "dimension",
    "flattened",
    "layer",
    "rule",
    "matrix",
    "multiplication",
    "inner",
    "dimensions",
    "must",
    "match",
    "know",
    "matrices",
    "well",
    "mat",
    "one",
    "mat",
    "two",
    "shapes",
    "multiplied",
    "know",
    "inside",
    "linear",
    "layer",
    "matrix",
    "multiplication",
    "let",
    "give",
    "go",
    "see",
    "works",
    "oh",
    "ho",
    "ho",
    "would",
    "look",
    "exciting",
    "output",
    "shape",
    "classifier",
    "one",
    "look",
    "one",
    "number",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "nine",
    "10",
    "one",
    "number",
    "class",
    "data",
    "set",
    "wow",
    "like",
    "cnn",
    "explain",
    "website",
    "10",
    "outputs",
    "happen",
    "10",
    "classes",
    "well",
    "number",
    "could",
    "whatever",
    "want",
    "could",
    "100",
    "could",
    "30",
    "could",
    "three",
    "depending",
    "many",
    "classes",
    "figured",
    "input",
    "output",
    "shapes",
    "layer",
    "model",
    "exciting",
    "think",
    "time",
    "passed",
    "random",
    "tensor",
    "pass",
    "actual",
    "data",
    "model",
    "next",
    "video",
    "let",
    "use",
    "train",
    "test",
    "step",
    "functions",
    "train",
    "first",
    "convolutional",
    "neural",
    "network",
    "see",
    "well",
    "let",
    "get",
    "ready",
    "train",
    "first",
    "cnn",
    "need",
    "workflow",
    "well",
    "built",
    "model",
    "stepped",
    "know",
    "going",
    "let",
    "really",
    "see",
    "going",
    "training",
    "cnn",
    "see",
    "trains",
    "always",
    "know",
    "data",
    "set",
    "fashion",
    "mnist",
    "going",
    "set",
    "loss",
    "function",
    "optimizer",
    "model",
    "two",
    "done",
    "model",
    "two",
    "turn",
    "markdown",
    "show",
    "workflow",
    "got",
    "inputs",
    "got",
    "numerical",
    "encoding",
    "built",
    "architecture",
    "hopefully",
    "helps",
    "us",
    "learn",
    "helps",
    "us",
    "make",
    "predictive",
    "model",
    "input",
    "images",
    "grayscale",
    "images",
    "clothing",
    "predict",
    "look",
    "pytorch",
    "workflow",
    "got",
    "data",
    "ready",
    "built",
    "next",
    "model",
    "picking",
    "loss",
    "function",
    "optimizer",
    "let",
    "hey",
    "loss",
    "function",
    "evaluation",
    "metrics",
    "well",
    "set",
    "loss",
    "function",
    "slash",
    "eval",
    "metrics",
    "slash",
    "optimizer",
    "want",
    "helper",
    "functions",
    "import",
    "accuracy",
    "function",
    "need",
    "reimport",
    "going",
    "anyway",
    "completeness",
    "loss",
    "function",
    "equals",
    "nn",
    "dot",
    "cross",
    "entropy",
    "loss",
    "working",
    "multi",
    "class",
    "classification",
    "problem",
    "optimizer",
    "going",
    "keep",
    "used",
    "torch",
    "dot",
    "opt",
    "sgd",
    "pass",
    "time",
    "params",
    "trying",
    "optimize",
    "parameters",
    "model",
    "two",
    "parameters",
    "use",
    "learning",
    "rate",
    "run",
    "reiterate",
    "trying",
    "optimize",
    "model",
    "two",
    "state",
    "dig",
    "lot",
    "random",
    "weights",
    "model",
    "two",
    "look",
    "bias",
    "weight",
    "going",
    "try",
    "optimize",
    "help",
    "us",
    "predict",
    "fashion",
    "mnist",
    "data",
    "set",
    "without",
    "ado",
    "let",
    "next",
    "video",
    "go",
    "workflow",
    "going",
    "build",
    "training",
    "loop",
    "thanks",
    "us",
    "got",
    "functions",
    "us",
    "want",
    "give",
    "go",
    "use",
    "train",
    "step",
    "test",
    "step",
    "function",
    "train",
    "model",
    "two",
    "try",
    "together",
    "next",
    "video",
    "getting",
    "close",
    "training",
    "model",
    "let",
    "write",
    "code",
    "train",
    "first",
    "thing",
    "model",
    "training",
    "testing",
    "going",
    "make",
    "another",
    "heading",
    "model",
    "two",
    "using",
    "training",
    "test",
    "functions",
    "rewrite",
    "steps",
    "training",
    "loop",
    "testing",
    "loop",
    "already",
    "created",
    "functionality",
    "train",
    "step",
    "function",
    "go",
    "performs",
    "training",
    "performs",
    "training",
    "step",
    "model",
    "trying",
    "learn",
    "data",
    "loader",
    "let",
    "set",
    "going",
    "set",
    "torch",
    "manual",
    "seed",
    "42",
    "set",
    "cuda",
    "manual",
    "seed",
    "well",
    "try",
    "make",
    "experiments",
    "reproducible",
    "possible",
    "going",
    "using",
    "cuda",
    "going",
    "measure",
    "time",
    "want",
    "compare",
    "models",
    "performance",
    "evaluation",
    "metrics",
    "long",
    "take",
    "train",
    "time",
    "point",
    "model",
    "performs",
    "really",
    "really",
    "well",
    "takes",
    "10",
    "times",
    "longer",
    "train",
    "well",
    "maybe",
    "depending",
    "working",
    "model",
    "two",
    "equals",
    "timer",
    "going",
    "train",
    "test",
    "model",
    "time",
    "something",
    "aware",
    "usually",
    "better",
    "performing",
    "model",
    "take",
    "longer",
    "train",
    "always",
    "case",
    "something",
    "keep",
    "mind",
    "epoch",
    "going",
    "use",
    "tqdm",
    "measure",
    "progress",
    "going",
    "create",
    "range",
    "epochs",
    "going",
    "train",
    "three",
    "epochs",
    "keeping",
    "experiment",
    "short",
    "see",
    "work",
    "epoch",
    "going",
    "print",
    "new",
    "line",
    "epoch",
    "range",
    "going",
    "training",
    "step",
    "train",
    "step",
    "function",
    "model",
    "going",
    "equal",
    "model",
    "two",
    "convolutional",
    "neural",
    "network",
    "tiny",
    "vgg",
    "data",
    "loader",
    "going",
    "equal",
    "train",
    "data",
    "loader",
    "one",
    "used",
    "loss",
    "function",
    "going",
    "equal",
    "loss",
    "function",
    "set",
    "loss",
    "fn",
    "optimizer",
    "well",
    "going",
    "optimizer",
    "case",
    "stochastic",
    "gradient",
    "descent",
    "optimizer",
    "equals",
    "optimizer",
    "set",
    "accuracy",
    "function",
    "going",
    "equal",
    "accuracy",
    "function",
    "device",
    "going",
    "target",
    "device",
    "easy",
    "train",
    "testing",
    "step",
    "sorry",
    "model",
    "going",
    "equal",
    "model",
    "two",
    "data",
    "loader",
    "going",
    "test",
    "data",
    "loader",
    "loss",
    "function",
    "going",
    "loss",
    "function",
    "optimizer",
    "going",
    "pass",
    "accuracy",
    "function",
    "course",
    "device",
    "going",
    "equal",
    "device",
    "well",
    "measure",
    "end",
    "time",
    "know",
    "long",
    "code",
    "took",
    "run",
    "let",
    "go",
    "train",
    "time",
    "end",
    "model",
    "two",
    "gpu",
    "way",
    "time",
    "using",
    "convolutional",
    "neural",
    "network",
    "total",
    "train",
    "time",
    "total",
    "train",
    "time",
    "model",
    "two",
    "going",
    "equal",
    "print",
    "train",
    "time",
    "function",
    "created",
    "well",
    "help",
    "us",
    "measure",
    "start",
    "end",
    "time",
    "going",
    "pass",
    "train",
    "time",
    "start",
    "model",
    "two",
    "end",
    "going",
    "train",
    "time",
    "end",
    "model",
    "two",
    "going",
    "print",
    "device",
    "using",
    "well",
    "ready",
    "ready",
    "train",
    "first",
    "convolutional",
    "neural",
    "network",
    "hopefully",
    "code",
    "works",
    "created",
    "functions",
    "right",
    "code",
    "run",
    "code",
    "let",
    "see",
    "happens",
    "oh",
    "goodness",
    "oh",
    "course",
    "oh",
    "forgot",
    "comment",
    "output",
    "shapes",
    "get",
    "whole",
    "bunch",
    "outputs",
    "model",
    "done",
    "back",
    "forgot",
    "means",
    "every",
    "time",
    "data",
    "goes",
    "forward",
    "pass",
    "going",
    "printing",
    "output",
    "shapes",
    "let",
    "comment",
    "think",
    "cell",
    "going",
    "take",
    "quite",
    "long",
    "time",
    "run",
    "got",
    "many",
    "printouts",
    "yeah",
    "see",
    "streaming",
    "output",
    "truncated",
    "last",
    "lines",
    "going",
    "try",
    "stop",
    "okay",
    "go",
    "beautiful",
    "actually",
    "worked",
    "sometimes",
    "stop",
    "quickly",
    "going",
    "rerun",
    "fashion",
    "msv",
    "model",
    "cell",
    "comment",
    "print",
    "lines",
    "rerun",
    "cells",
    "go",
    "back",
    "fingers",
    "crossed",
    "errors",
    "train",
    "model",
    "beautiful",
    "many",
    "printouts",
    "time",
    "go",
    "first",
    "cnn",
    "training",
    "think",
    "go",
    "well",
    "printouts",
    "right",
    "see",
    "progress",
    "see",
    "functions",
    "called",
    "behind",
    "scenes",
    "pytorch",
    "thank",
    "pytorch",
    "oh",
    "train",
    "step",
    "function",
    "train",
    "step",
    "wonderful",
    "beautiful",
    "epoch",
    "zero",
    "oh",
    "get",
    "pretty",
    "good",
    "test",
    "accuracy",
    "good",
    "test",
    "accuracy",
    "climbing",
    "well",
    "beaten",
    "baseline",
    "looking",
    "14",
    "seconds",
    "per",
    "epoch",
    "final",
    "epoch",
    "finish",
    "oh",
    "wow",
    "42",
    "seconds",
    "mileage",
    "may",
    "vary",
    "worry",
    "much",
    "numbers",
    "exactly",
    "screen",
    "training",
    "time",
    "might",
    "using",
    "slightly",
    "different",
    "hardware",
    "gpu",
    "today",
    "tesla",
    "p100",
    "gpu",
    "might",
    "gpu",
    "training",
    "time",
    "training",
    "time",
    "something",
    "like",
    "10",
    "times",
    "higher",
    "might",
    "want",
    "look",
    "going",
    "values",
    "like",
    "10",
    "lower",
    "10",
    "higher",
    "might",
    "want",
    "see",
    "going",
    "code",
    "well",
    "let",
    "calculate",
    "model",
    "2",
    "results",
    "think",
    "best",
    "performing",
    "model",
    "far",
    "let",
    "get",
    "results",
    "dictionary",
    "model",
    "2",
    "results",
    "exciting",
    "learning",
    "power",
    "convolutional",
    "neural",
    "networks",
    "model",
    "2",
    "results",
    "equals",
    "vowel",
    "model",
    "function",
    "created",
    "returns",
    "dictionary",
    "containing",
    "results",
    "model",
    "predicting",
    "data",
    "loader",
    "let",
    "pass",
    "model",
    "trained",
    "model",
    "pass",
    "data",
    "loader",
    "test",
    "data",
    "loader",
    "oops",
    "excuse",
    "typo",
    "loss",
    "function",
    "course",
    "loss",
    "function",
    "accuracy",
    "function",
    "accuracy",
    "function",
    "device",
    "already",
    "set",
    "reset",
    "anyway",
    "device",
    "equals",
    "device",
    "check",
    "model",
    "2",
    "results",
    "make",
    "predictions",
    "oh",
    "look",
    "model",
    "accuracy",
    "beat",
    "baseline",
    "model",
    "0",
    "results",
    "oh",
    "beat",
    "baseline",
    "convolutional",
    "neural",
    "network",
    "right",
    "feel",
    "like",
    "uh",
    "quite",
    "exciting",
    "let",
    "keep",
    "going",
    "uh",
    "let",
    "start",
    "compare",
    "results",
    "models",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "trained",
    "first",
    "convolutional",
    "neural",
    "network",
    "looks",
    "things",
    "improved",
    "upon",
    "baseline",
    "let",
    "make",
    "sure",
    "comparing",
    "another",
    "important",
    "part",
    "machine",
    "learning",
    "experiments",
    "comparing",
    "results",
    "across",
    "experiments",
    "training",
    "time",
    "done",
    "way",
    "got",
    "three",
    "dictionaries",
    "model",
    "zero",
    "results",
    "model",
    "one",
    "results",
    "model",
    "two",
    "results",
    "create",
    "data",
    "frame",
    "comparing",
    "let",
    "import",
    "pandas",
    "pd",
    "going",
    "compare",
    "results",
    "equals",
    "pd",
    "dot",
    "data",
    "frame",
    "model",
    "results",
    "dictionaries",
    "uh",
    "keys",
    "let",
    "pass",
    "list",
    "model",
    "zero",
    "results",
    "model",
    "one",
    "results",
    "model",
    "two",
    "results",
    "compare",
    "wonderful",
    "looks",
    "like",
    "compare",
    "results",
    "righty",
    "recall",
    "first",
    "model",
    "baseline",
    "v",
    "zero",
    "two",
    "linear",
    "layers",
    "accuracy",
    "loss",
    "next",
    "model",
    "trained",
    "gpu",
    "introduced",
    "nonlinearities",
    "actually",
    "found",
    "worse",
    "baseline",
    "brought",
    "big",
    "guns",
    "brought",
    "tiny",
    "vgg",
    "architecture",
    "cnn",
    "explainer",
    "website",
    "trained",
    "first",
    "convolutional",
    "neural",
    "network",
    "got",
    "best",
    "results",
    "far",
    "lot",
    "experiments",
    "could",
    "could",
    "go",
    "back",
    "tiny",
    "vgg",
    "could",
    "increase",
    "number",
    "hidden",
    "units",
    "create",
    "model",
    "could",
    "increase",
    "say",
    "30",
    "see",
    "happens",
    "would",
    "good",
    "experiment",
    "try",
    "found",
    "nonlinearities",
    "help",
    "second",
    "model",
    "could",
    "comment",
    "relu",
    "layers",
    "could",
    "course",
    "change",
    "kernel",
    "size",
    "change",
    "padding",
    "change",
    "max",
    "pool",
    "whole",
    "bunch",
    "different",
    "things",
    "could",
    "try",
    "could",
    "train",
    "longer",
    "maybe",
    "train",
    "10",
    "epochs",
    "would",
    "perform",
    "better",
    "things",
    "keep",
    "mind",
    "try",
    "encourage",
    "give",
    "go",
    "kept",
    "experiments",
    "quite",
    "see",
    "results",
    "add",
    "training",
    "time",
    "another",
    "important",
    "thing",
    "tracking",
    "well",
    "add",
    "training",
    "time",
    "results",
    "comparison",
    "reason",
    "model",
    "performing",
    "quite",
    "well",
    "even",
    "compared",
    "cnn",
    "difference",
    "5",
    "accuracy",
    "maybe",
    "tolerable",
    "space",
    "working",
    "except",
    "model",
    "might",
    "actually",
    "train",
    "perform",
    "inference",
    "10",
    "times",
    "faster",
    "model",
    "something",
    "aware",
    "called",
    "performance",
    "speed",
    "trade",
    "let",
    "add",
    "another",
    "column",
    "compare",
    "results",
    "going",
    "add",
    "oh",
    "excuse",
    "got",
    "little",
    "error",
    "right",
    "got",
    "trigger",
    "happy",
    "shift",
    "enter",
    "training",
    "time",
    "equals",
    "going",
    "add",
    "got",
    "another",
    "list",
    "going",
    "total",
    "train",
    "time",
    "model",
    "zero",
    "total",
    "train",
    "time",
    "model",
    "one",
    "total",
    "train",
    "time",
    "model",
    "two",
    "look",
    "compare",
    "results",
    "dictionary",
    "sorry",
    "compare",
    "results",
    "data",
    "frame",
    "wonderful",
    "see",
    "another",
    "thing",
    "keep",
    "stressing",
    "keep",
    "mind",
    "numbers",
    "exactly",
    "got",
    "worry",
    "much",
    "go",
    "back",
    "code",
    "see",
    "set",
    "random",
    "seeds",
    "correctly",
    "might",
    "need",
    "koodle",
    "random",
    "seed",
    "may",
    "missed",
    "one",
    "numbers",
    "landishly",
    "different",
    "numbers",
    "go",
    "back",
    "code",
    "see",
    "something",
    "wrong",
    "training",
    "time",
    "highly",
    "dependent",
    "compute",
    "environment",
    "using",
    "running",
    "notebook",
    "locally",
    "might",
    "get",
    "faster",
    "training",
    "times",
    "running",
    "different",
    "gpu",
    "nvidia",
    "smi",
    "might",
    "get",
    "different",
    "training",
    "times",
    "using",
    "tesla",
    "p100",
    "quite",
    "fast",
    "gpu",
    "paying",
    "colab",
    "pro",
    "generally",
    "gives",
    "faster",
    "gpus",
    "model",
    "zero",
    "trained",
    "cpu",
    "depending",
    "compute",
    "resource",
    "google",
    "allocates",
    "google",
    "colab",
    "number",
    "might",
    "vary",
    "keep",
    "mind",
    "values",
    "training",
    "time",
    "dependent",
    "hardware",
    "using",
    "numbers",
    "dramatically",
    "different",
    "well",
    "might",
    "want",
    "change",
    "something",
    "code",
    "see",
    "going",
    "finish",
    "graph",
    "let",
    "go",
    "visualize",
    "model",
    "results",
    "look",
    "data",
    "frame",
    "performance",
    "10",
    "seconds",
    "longer",
    "training",
    "time",
    "worth",
    "extra",
    "5",
    "results",
    "accuracy",
    "case",
    "using",
    "relatively",
    "toy",
    "problem",
    "mean",
    "toy",
    "problem",
    "quite",
    "simple",
    "data",
    "set",
    "try",
    "test",
    "practice",
    "may",
    "worth",
    "model",
    "takes",
    "longer",
    "train",
    "gets",
    "quite",
    "bit",
    "better",
    "performance",
    "really",
    "depends",
    "problem",
    "working",
    "compare",
    "results",
    "going",
    "set",
    "index",
    "model",
    "name",
    "think",
    "want",
    "graph",
    "model",
    "name",
    "going",
    "plot",
    "want",
    "compare",
    "model",
    "accuracy",
    "want",
    "plot",
    "kind",
    "going",
    "equal",
    "bar",
    "h",
    "horizontal",
    "bar",
    "chart",
    "got",
    "p",
    "x",
    "label",
    "going",
    "get",
    "accuracy",
    "percentage",
    "going",
    "go",
    "py",
    "label",
    "something",
    "could",
    "share",
    "someone",
    "asking",
    "modeling",
    "experiments",
    "go",
    "fashion",
    "mnist",
    "well",
    "got",
    "ask",
    "well",
    "fashion",
    "mnist",
    "model",
    "v2",
    "well",
    "could",
    "say",
    "convolutional",
    "neural",
    "network",
    "trained",
    "replicates",
    "cnn",
    "explainer",
    "website",
    "trained",
    "gpu",
    "long",
    "take",
    "train",
    "well",
    "got",
    "training",
    "time",
    "could",
    "vertical",
    "bar",
    "chart",
    "horizontal",
    "looks",
    "bit",
    "funny",
    "horizontal",
    "like",
    "model",
    "names",
    "wonderful",
    "feel",
    "like",
    "got",
    "trained",
    "model",
    "make",
    "visual",
    "predictions",
    "got",
    "numbers",
    "page",
    "model",
    "trained",
    "computer",
    "vision",
    "data",
    "whole",
    "point",
    "making",
    "machine",
    "learning",
    "model",
    "computer",
    "vision",
    "data",
    "able",
    "visualize",
    "predictions",
    "let",
    "give",
    "shot",
    "hey",
    "next",
    "video",
    "going",
    "use",
    "best",
    "performing",
    "model",
    "fashion",
    "mnist",
    "model",
    "v2",
    "make",
    "predictions",
    "random",
    "samples",
    "test",
    "data",
    "set",
    "might",
    "want",
    "give",
    "shot",
    "make",
    "predictions",
    "random",
    "samples",
    "test",
    "data",
    "set",
    "plot",
    "predictions",
    "title",
    "try",
    "otherwise",
    "together",
    "next",
    "video",
    "last",
    "video",
    "compared",
    "models",
    "results",
    "tried",
    "three",
    "experiments",
    "one",
    "basic",
    "linear",
    "model",
    "one",
    "linear",
    "model",
    "nonlinear",
    "activations",
    "fashion",
    "mnist",
    "model",
    "v2",
    "convolutional",
    "neural",
    "network",
    "saw",
    "accuracy",
    "perspective",
    "convolutional",
    "neural",
    "network",
    "performed",
    "best",
    "however",
    "longest",
    "training",
    "time",
    "want",
    "exemplify",
    "fact",
    "training",
    "time",
    "vary",
    "depending",
    "hardware",
    "run",
    "spoke",
    "last",
    "video",
    "however",
    "took",
    "break",
    "finishing",
    "last",
    "video",
    "reran",
    "cells",
    "written",
    "code",
    "cells",
    "coming",
    "back",
    "notebook",
    "going",
    "run",
    "see",
    "compare",
    "training",
    "times",
    "last",
    "video",
    "get",
    "different",
    "values",
    "sure",
    "exactly",
    "hardware",
    "google",
    "collab",
    "using",
    "behind",
    "scenes",
    "something",
    "keep",
    "mind",
    "least",
    "know",
    "track",
    "different",
    "variables",
    "long",
    "model",
    "takes",
    "train",
    "performance",
    "values",
    "time",
    "get",
    "visual",
    "let",
    "create",
    "another",
    "heading",
    "make",
    "evaluate",
    "one",
    "favorite",
    "steps",
    "training",
    "machine",
    "learning",
    "model",
    "make",
    "evaluate",
    "random",
    "predictions",
    "best",
    "model",
    "going",
    "follow",
    "data",
    "explorer",
    "model",
    "getting",
    "visual",
    "visual",
    "visual",
    "visualize",
    "visualize",
    "visualize",
    "let",
    "make",
    "function",
    "called",
    "make",
    "predictions",
    "going",
    "take",
    "model",
    "torch",
    "end",
    "module",
    "type",
    "also",
    "going",
    "take",
    "data",
    "list",
    "also",
    "take",
    "device",
    "type",
    "torch",
    "dot",
    "device",
    "set",
    "default",
    "equal",
    "default",
    "device",
    "already",
    "set",
    "going",
    "create",
    "empty",
    "list",
    "prediction",
    "probabilities",
    "like",
    "take",
    "random",
    "samples",
    "test",
    "data",
    "set",
    "make",
    "predictions",
    "using",
    "model",
    "plot",
    "predictions",
    "want",
    "visualize",
    "also",
    "turn",
    "model",
    "evaluation",
    "mode",
    "making",
    "predictions",
    "model",
    "turn",
    "evaluation",
    "mode",
    "also",
    "switch",
    "inference",
    "mode",
    "context",
    "manager",
    "predictions",
    "another",
    "word",
    "inference",
    "going",
    "loop",
    "sample",
    "data",
    "let",
    "prepare",
    "sample",
    "going",
    "take",
    "single",
    "image",
    "unsqueeze",
    "need",
    "add",
    "batch",
    "size",
    "dimension",
    "sample",
    "set",
    "dim",
    "equals",
    "zero",
    "pass",
    "device",
    "add",
    "batch",
    "dimension",
    "unsqueeze",
    "pass",
    "target",
    "device",
    "way",
    "data",
    "model",
    "device",
    "forward",
    "pass",
    "well",
    "could",
    "actually",
    "go",
    "model",
    "dot",
    "two",
    "device",
    "way",
    "know",
    "got",
    "device",
    "agnostic",
    "code",
    "let",
    "forward",
    "pass",
    "forward",
    "pass",
    "model",
    "outputs",
    "raw",
    "logits",
    "recall",
    "linear",
    "layer",
    "end",
    "model",
    "outputs",
    "raw",
    "logits",
    "pred",
    "logit",
    "single",
    "sample",
    "going",
    "equal",
    "model",
    "pass",
    "sample",
    "target",
    "model",
    "going",
    "get",
    "prediction",
    "probability",
    "get",
    "prediction",
    "probability",
    "want",
    "go",
    "logit",
    "prediction",
    "probability",
    "well",
    "working",
    "multi",
    "class",
    "classification",
    "problem",
    "going",
    "use",
    "softmax",
    "activation",
    "function",
    "pred",
    "logit",
    "going",
    "squeeze",
    "gets",
    "rid",
    "extra",
    "dimension",
    "going",
    "pass",
    "dim",
    "equals",
    "zero",
    "going",
    "give",
    "us",
    "prediction",
    "probability",
    "given",
    "sample",
    "let",
    "also",
    "turn",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "get",
    "pred",
    "well",
    "actually",
    "think",
    "going",
    "return",
    "pred",
    "probes",
    "yeah",
    "let",
    "see",
    "looks",
    "like",
    "got",
    "empty",
    "list",
    "pred",
    "probes",
    "matplotlib",
    "going",
    "use",
    "data",
    "cpu",
    "let",
    "make",
    "sure",
    "cpu",
    "matplotlib",
    "work",
    "gpu",
    "get",
    "pred",
    "prob",
    "gpu",
    "calculations",
    "hard",
    "coded",
    "make",
    "sure",
    "prediction",
    "probabilities",
    "gpu",
    "pred",
    "probs",
    "list",
    "going",
    "append",
    "pred",
    "prob",
    "calculated",
    "going",
    "put",
    "cpu",
    "let",
    "go",
    "going",
    "done",
    "right",
    "going",
    "list",
    "prediction",
    "probabilities",
    "relating",
    "particular",
    "samples",
    "going",
    "stack",
    "pred",
    "probs",
    "turn",
    "list",
    "tensor",
    "one",
    "way",
    "things",
    "many",
    "different",
    "ways",
    "could",
    "make",
    "predictions",
    "visualize",
    "exemplifying",
    "one",
    "way",
    "going",
    "torch",
    "stack",
    "going",
    "say",
    "hey",
    "concatenate",
    "everything",
    "list",
    "single",
    "tensor",
    "might",
    "need",
    "tab",
    "tab",
    "tab",
    "beautiful",
    "let",
    "try",
    "function",
    "action",
    "see",
    "happens",
    "going",
    "import",
    "random",
    "going",
    "set",
    "random",
    "seed",
    "going",
    "create",
    "test",
    "samples",
    "empty",
    "list",
    "want",
    "empty",
    "want",
    "list",
    "test",
    "samples",
    "iterate",
    "going",
    "create",
    "test",
    "labels",
    "also",
    "empty",
    "list",
    "remember",
    "evaluating",
    "predictions",
    "want",
    "compare",
    "ground",
    "truth",
    "want",
    "get",
    "test",
    "samples",
    "want",
    "get",
    "actual",
    "labels",
    "model",
    "makes",
    "predictions",
    "compare",
    "actual",
    "labels",
    "sample",
    "comma",
    "label",
    "going",
    "use",
    "random",
    "sample",
    "test",
    "data",
    "note",
    "test",
    "data",
    "loader",
    "test",
    "data",
    "going",
    "set",
    "k",
    "equals",
    "nine",
    "recall",
    "want",
    "look",
    "test",
    "data",
    "go",
    "test",
    "data",
    "data",
    "set",
    "converted",
    "data",
    "loader",
    "yet",
    "wanted",
    "get",
    "first",
    "10",
    "samples",
    "one",
    "element",
    "tensors",
    "converted",
    "python",
    "scalars",
    "get",
    "first",
    "zero",
    "maybe",
    "go",
    "yeah",
    "go",
    "shape",
    "tuple",
    "object",
    "shape",
    "okay",
    "need",
    "go",
    "image",
    "label",
    "equals",
    "check",
    "shape",
    "image",
    "label",
    "oh",
    "labels",
    "going",
    "integers",
    "wonderful",
    "first",
    "10",
    "samples",
    "get",
    "iterate",
    "test",
    "data",
    "get",
    "image",
    "tensor",
    "get",
    "associated",
    "label",
    "line",
    "randomly",
    "sampling",
    "nine",
    "samples",
    "could",
    "number",
    "want",
    "going",
    "use",
    "nine",
    "spoiler",
    "later",
    "going",
    "create",
    "three",
    "three",
    "plot",
    "nine",
    "fun",
    "number",
    "get",
    "random",
    "samples",
    "test",
    "data",
    "set",
    "go",
    "test",
    "samples",
    "dot",
    "append",
    "sample",
    "go",
    "test",
    "labels",
    "dot",
    "append",
    "label",
    "let",
    "go",
    "view",
    "first",
    "maybe",
    "go",
    "first",
    "sample",
    "shape",
    "test",
    "samples",
    "zero",
    "dot",
    "shape",
    "get",
    "test",
    "samples",
    "zero",
    "going",
    "get",
    "tensor",
    "image",
    "values",
    "wanted",
    "plot",
    "go",
    "plt",
    "show",
    "c",
    "map",
    "equals",
    "gray",
    "may",
    "squeeze",
    "believe",
    "remove",
    "batch",
    "tensor",
    "let",
    "see",
    "happens",
    "batch",
    "dimension",
    "go",
    "beautiful",
    "shoe",
    "high",
    "heel",
    "shoe",
    "sort",
    "get",
    "title",
    "plt",
    "dot",
    "title",
    "test",
    "labels",
    "let",
    "see",
    "looks",
    "like",
    "five",
    "course",
    "class",
    "names",
    "index",
    "sandal",
    "okay",
    "beautiful",
    "nine",
    "random",
    "samples",
    "nine",
    "labels",
    "associated",
    "sample",
    "let",
    "make",
    "predictions",
    "make",
    "predictions",
    "one",
    "favorite",
    "things",
    "ca",
    "stress",
    "enough",
    "randomly",
    "pick",
    "data",
    "samples",
    "test",
    "data",
    "set",
    "predict",
    "see",
    "model",
    "start",
    "problem",
    "get",
    "prediction",
    "probabilities",
    "going",
    "call",
    "make",
    "predictions",
    "function",
    "start",
    "problem",
    "become",
    "one",
    "data",
    "even",
    "trained",
    "model",
    "want",
    "become",
    "one",
    "data",
    "time",
    "become",
    "one",
    "models",
    "predictions",
    "data",
    "see",
    "happens",
    "view",
    "first",
    "two",
    "prediction",
    "probabilities",
    "list",
    "using",
    "make",
    "predictions",
    "function",
    "created",
    "passing",
    "model",
    "train",
    "model",
    "passing",
    "data",
    "test",
    "samples",
    "list",
    "created",
    "comprised",
    "random",
    "samples",
    "test",
    "data",
    "set",
    "wonderful",
    "let",
    "go",
    "pred",
    "probes",
    "oh",
    "want",
    "view",
    "going",
    "give",
    "us",
    "oh",
    "want",
    "prediction",
    "probabilities",
    "given",
    "sample",
    "convert",
    "prediction",
    "probabilities",
    "labels",
    "trying",
    "look",
    "test",
    "labels",
    "trying",
    "compare",
    "apples",
    "apples",
    "evaluating",
    "model",
    "want",
    "ca",
    "really",
    "necessarily",
    "compare",
    "prediction",
    "probabilities",
    "straight",
    "test",
    "labels",
    "need",
    "convert",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "well",
    "use",
    "argmax",
    "take",
    "whichever",
    "value",
    "index",
    "case",
    "one",
    "index",
    "whichever",
    "value",
    "highest",
    "prediction",
    "probabilities",
    "let",
    "see",
    "action",
    "convert",
    "prediction",
    "probabilities",
    "labels",
    "go",
    "pred",
    "classes",
    "equals",
    "pred",
    "probes",
    "get",
    "argmax",
    "across",
    "first",
    "dimension",
    "let",
    "look",
    "pred",
    "classes",
    "wonderful",
    "format",
    "test",
    "labels",
    "yes",
    "like",
    "go",
    "ahead",
    "next",
    "video",
    "going",
    "plot",
    "compare",
    "going",
    "write",
    "code",
    "create",
    "mapplotlib",
    "plotting",
    "function",
    "going",
    "plot",
    "nine",
    "different",
    "samples",
    "along",
    "original",
    "labels",
    "predicted",
    "label",
    "give",
    "shot",
    "written",
    "code",
    "make",
    "predictions",
    "random",
    "samples",
    "like",
    "truly",
    "random",
    "comment",
    "seed",
    "kept",
    "seed",
    "random",
    "dot",
    "sample",
    "selects",
    "samples",
    "end",
    "end",
    "next",
    "video",
    "let",
    "plot",
    "let",
    "continue",
    "following",
    "data",
    "explorer",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "prediction",
    "classes",
    "labels",
    "like",
    "compare",
    "compare",
    "visually",
    "looks",
    "like",
    "model",
    "pretty",
    "good",
    "let",
    "since",
    "making",
    "predictions",
    "images",
    "let",
    "plot",
    "images",
    "along",
    "predictions",
    "going",
    "write",
    "code",
    "plot",
    "predictions",
    "going",
    "create",
    "matplotlib",
    "figure",
    "going",
    "set",
    "fig",
    "size",
    "nine",
    "nine",
    "got",
    "nine",
    "random",
    "samples",
    "could",
    "course",
    "change",
    "however",
    "many",
    "want",
    "found",
    "three",
    "three",
    "plot",
    "works",
    "pretty",
    "good",
    "practice",
    "going",
    "set",
    "n",
    "rows",
    "matplotlib",
    "plot",
    "want",
    "three",
    "rows",
    "want",
    "three",
    "columns",
    "going",
    "enumerate",
    "samples",
    "test",
    "samples",
    "going",
    "create",
    "subplot",
    "sample",
    "create",
    "subplot",
    "going",
    "create",
    "subplot",
    "within",
    "loop",
    "time",
    "goes",
    "new",
    "sample",
    "create",
    "subplot",
    "n",
    "rows",
    "calls",
    "index",
    "going",
    "going",
    "plus",
    "one",
    "ca",
    "start",
    "zero",
    "put",
    "plus",
    "one",
    "going",
    "enumerate",
    "oh",
    "excuse",
    "enumerate",
    "wonderful",
    "going",
    "plot",
    "target",
    "image",
    "go",
    "plot",
    "dot",
    "show",
    "going",
    "get",
    "sample",
    "dot",
    "squeeze",
    "need",
    "remove",
    "batch",
    "dimension",
    "going",
    "set",
    "c",
    "map",
    "equal",
    "gray",
    "telling",
    "oh",
    "correct",
    "next",
    "going",
    "find",
    "prediction",
    "label",
    "text",
    "form",
    "want",
    "numeric",
    "form",
    "could",
    "want",
    "look",
    "things",
    "visually",
    "human",
    "readable",
    "language",
    "sandal",
    "whatever",
    "class",
    "sandal",
    "whatever",
    "number",
    "class",
    "going",
    "set",
    "pred",
    "label",
    "equals",
    "class",
    "names",
    "going",
    "index",
    "using",
    "pred",
    "classes",
    "value",
    "right",
    "going",
    "plot",
    "sample",
    "going",
    "find",
    "prediction",
    "going",
    "get",
    "truth",
    "label",
    "also",
    "want",
    "text",
    "form",
    "truth",
    "label",
    "going",
    "well",
    "truth",
    "label",
    "going",
    "index",
    "using",
    "class",
    "names",
    "index",
    "using",
    "test",
    "labels",
    "matching",
    "indexes",
    "finally",
    "going",
    "create",
    "title",
    "create",
    "title",
    "plot",
    "like",
    "well",
    "getting",
    "visual",
    "well",
    "might",
    "well",
    "get",
    "really",
    "visual",
    "right",
    "think",
    "change",
    "color",
    "title",
    "text",
    "depending",
    "prediction",
    "right",
    "wrong",
    "going",
    "create",
    "title",
    "using",
    "f",
    "string",
    "pred",
    "going",
    "pred",
    "label",
    "truth",
    "label",
    "could",
    "even",
    "plot",
    "prediction",
    "probabilities",
    "wanted",
    "might",
    "extension",
    "might",
    "want",
    "try",
    "going",
    "check",
    "equality",
    "pred",
    "truth",
    "change",
    "color",
    "title",
    "text",
    "mean",
    "going",
    "lot",
    "easier",
    "explain",
    "doubt",
    "coded",
    "pred",
    "label",
    "equals",
    "truth",
    "label",
    "equal",
    "want",
    "plot",
    "dot",
    "title",
    "title",
    "text",
    "want",
    "font",
    "size",
    "well",
    "font",
    "size",
    "want",
    "color",
    "equal",
    "green",
    "green",
    "text",
    "prediction",
    "truth",
    "else",
    "going",
    "set",
    "plot",
    "title",
    "title",
    "text",
    "font",
    "size",
    "equals",
    "color",
    "going",
    "red",
    "make",
    "sense",
    "enumerating",
    "test",
    "samples",
    "got",
    "test",
    "samples",
    "found",
    "randomly",
    "test",
    "data",
    "set",
    "time",
    "creating",
    "subplot",
    "plotting",
    "image",
    "finding",
    "prediction",
    "label",
    "indexing",
    "class",
    "names",
    "pred",
    "classes",
    "value",
    "getting",
    "truth",
    "label",
    "creating",
    "title",
    "plot",
    "compares",
    "pred",
    "label",
    "truth",
    "changing",
    "color",
    "title",
    "text",
    "depending",
    "pred",
    "label",
    "correct",
    "let",
    "see",
    "happens",
    "get",
    "right",
    "oh",
    "yes",
    "oh",
    "going",
    "one",
    "thing",
    "want",
    "turn",
    "accesses",
    "get",
    "real",
    "estate",
    "love",
    "kind",
    "plots",
    "helps",
    "model",
    "got",
    "predictions",
    "right",
    "look",
    "pred",
    "sandal",
    "truth",
    "sandal",
    "pred",
    "trouser",
    "truth",
    "trouser",
    "pretty",
    "darn",
    "good",
    "right",
    "see",
    "much",
    "appreciate",
    "like",
    "much",
    "prefer",
    "visualizing",
    "things",
    "numbers",
    "page",
    "look",
    "good",
    "something",
    "nothing",
    "quite",
    "like",
    "visualizing",
    "machine",
    "learning",
    "models",
    "predictions",
    "especially",
    "gets",
    "right",
    "select",
    "different",
    "random",
    "samples",
    "could",
    "functionize",
    "well",
    "like",
    "code",
    "one",
    "hit",
    "right",
    "bit",
    "hacky",
    "going",
    "randomly",
    "sample",
    "seed",
    "samples",
    "might",
    "different",
    "mine",
    "nine",
    "different",
    "samples",
    "time",
    "ankle",
    "boot",
    "make",
    "predictions",
    "step",
    "code",
    "oh",
    "go",
    "got",
    "one",
    "wrong",
    "correct",
    "interesting",
    "well",
    "model",
    "get",
    "things",
    "wrong",
    "predicted",
    "address",
    "coat",
    "think",
    "could",
    "potentially",
    "address",
    "could",
    "see",
    "addressed",
    "kind",
    "understand",
    "model",
    "coming",
    "let",
    "make",
    "random",
    "predictions",
    "might",
    "two",
    "move",
    "next",
    "video",
    "oh",
    "correct",
    "interested",
    "getting",
    "wrong",
    "model",
    "seems",
    "good",
    "correct",
    "okay",
    "one",
    "time",
    "get",
    "wrong",
    "going",
    "next",
    "video",
    "really",
    "oh",
    "go",
    "wrong",
    "beautiful",
    "predicted",
    "address",
    "shirt",
    "okay",
    "kind",
    "see",
    "model",
    "might",
    "stuffed",
    "little",
    "bit",
    "long",
    "shirt",
    "still",
    "understand",
    "would",
    "shirt",
    "pullover",
    "truth",
    "coat",
    "maybe",
    "maybe",
    "issues",
    "labels",
    "probably",
    "find",
    "lot",
    "data",
    "sets",
    "especially",
    "quite",
    "large",
    "ones",
    "sheer",
    "law",
    "large",
    "numbers",
    "may",
    "truth",
    "labels",
    "data",
    "sets",
    "work",
    "wrong",
    "like",
    "see",
    "compare",
    "models",
    "predictions",
    "versus",
    "truth",
    "bunch",
    "random",
    "samples",
    "go",
    "know",
    "models",
    "results",
    "better",
    "worse",
    "actually",
    "visualizing",
    "helps",
    "figure",
    "know",
    "model",
    "actually",
    "says",
    "good",
    "accuracy",
    "visualize",
    "predictions",
    "good",
    "vice",
    "versa",
    "right",
    "keep",
    "playing",
    "around",
    "try",
    "look",
    "random",
    "samples",
    "running",
    "one",
    "good",
    "luck",
    "move",
    "next",
    "video",
    "going",
    "go",
    "another",
    "way",
    "oh",
    "see",
    "another",
    "example",
    "labels",
    "could",
    "confusing",
    "speaking",
    "confusing",
    "well",
    "going",
    "spoiler",
    "next",
    "video",
    "see",
    "prediction",
    "top",
    "truth",
    "shirt",
    "label",
    "kind",
    "overlapping",
    "like",
    "know",
    "difference",
    "shirt",
    "something",
    "find",
    "train",
    "models",
    "maybe",
    "model",
    "going",
    "tell",
    "data",
    "well",
    "hinted",
    "going",
    "confused",
    "model",
    "confused",
    "top",
    "shirt",
    "plot",
    "confusion",
    "matrix",
    "next",
    "video",
    "see",
    "exciting",
    "point",
    "evaluating",
    "machine",
    "learning",
    "model",
    "visualizing",
    "visualizing",
    "visualizing",
    "saw",
    "previous",
    "video",
    "model",
    "kind",
    "gets",
    "little",
    "bit",
    "confused",
    "fact",
    "would",
    "personally",
    "get",
    "confused",
    "difference",
    "slash",
    "top",
    "shirt",
    "kind",
    "insights",
    "model",
    "predictions",
    "also",
    "give",
    "us",
    "insights",
    "maybe",
    "labels",
    "could",
    "improved",
    "another",
    "way",
    "check",
    "make",
    "confusion",
    "matrix",
    "let",
    "making",
    "confusion",
    "matrix",
    "prediction",
    "evaluation",
    "confusion",
    "matrix",
    "another",
    "one",
    "favorite",
    "ways",
    "evaluating",
    "classification",
    "model",
    "multi",
    "class",
    "classification",
    "recall",
    "go",
    "back",
    "section",
    "two",
    "lone",
    "book",
    "scroll",
    "section",
    "classification",
    "evaluation",
    "metrics",
    "accuracy",
    "probably",
    "gold",
    "standard",
    "classification",
    "evaluation",
    "precision",
    "recall",
    "f1",
    "score",
    "confusion",
    "matrix",
    "try",
    "build",
    "one",
    "want",
    "get",
    "copy",
    "write",
    "confusion",
    "matrix",
    "fantastic",
    "way",
    "evaluating",
    "classification",
    "models",
    "visually",
    "beautiful",
    "going",
    "break",
    "first",
    "need",
    "plot",
    "confusion",
    "matrix",
    "need",
    "make",
    "predictions",
    "trained",
    "model",
    "test",
    "data",
    "set",
    "number",
    "two",
    "going",
    "make",
    "confusion",
    "matrix",
    "going",
    "leverage",
    "torch",
    "metrics",
    "tricks",
    "figure",
    "spell",
    "metrics",
    "confusion",
    "matrix",
    "recall",
    "torch",
    "metrics",
    "touched",
    "great",
    "package",
    "torch",
    "metrics",
    "whole",
    "bunch",
    "evaluation",
    "metrics",
    "machine",
    "learning",
    "models",
    "pytorch",
    "flavor",
    "find",
    "got",
    "classification",
    "metrics",
    "got",
    "audio",
    "image",
    "detection",
    "look",
    "beautiful",
    "bunch",
    "different",
    "evaluation",
    "metrics",
    "go",
    "got",
    "confusion",
    "matrix",
    "touched",
    "five",
    "six",
    "look",
    "torch",
    "metrics",
    "got",
    "many",
    "25",
    "different",
    "classification",
    "metrics",
    "want",
    "extra",
    "curriculum",
    "read",
    "let",
    "go",
    "confusion",
    "matrix",
    "look",
    "code",
    "got",
    "torch",
    "metrics",
    "confusion",
    "matrix",
    "need",
    "pass",
    "number",
    "classes",
    "normalize",
    "want",
    "notice",
    "quite",
    "similar",
    "pytorch",
    "documentation",
    "well",
    "beautiful",
    "thing",
    "torch",
    "metrics",
    "created",
    "pytorch",
    "mind",
    "let",
    "try",
    "wanted",
    "try",
    "tester",
    "code",
    "could",
    "since",
    "already",
    "got",
    "code",
    "let",
    "bring",
    "number",
    "three",
    "plot",
    "got",
    "another",
    "helper",
    "package",
    "plot",
    "confusion",
    "matrix",
    "using",
    "ml",
    "extend",
    "another",
    "one",
    "favorite",
    "helper",
    "libraries",
    "machine",
    "learning",
    "things",
    "got",
    "lot",
    "functionality",
    "code",
    "often",
    "find",
    "coding",
    "many",
    "times",
    "plotting",
    "confusion",
    "matrix",
    "look",
    "ml",
    "extend",
    "plot",
    "confusion",
    "matrix",
    "wonderful",
    "library",
    "believe",
    "created",
    "sebastian",
    "rushka",
    "machine",
    "learning",
    "researcher",
    "also",
    "author",
    "great",
    "book",
    "yeah",
    "side",
    "note",
    "machine",
    "learning",
    "pytorch",
    "scikit",
    "loan",
    "got",
    "book",
    "got",
    "released",
    "start",
    "great",
    "book",
    "little",
    "side",
    "note",
    "learning",
    "machine",
    "learning",
    "pytorch",
    "scikit",
    "loan",
    "shout",
    "sebastian",
    "rushka",
    "thank",
    "package",
    "well",
    "going",
    "help",
    "us",
    "plot",
    "confusion",
    "matrix",
    "like",
    "predicted",
    "labels",
    "bottom",
    "true",
    "labels",
    "side",
    "copy",
    "code",
    "link",
    "sorry",
    "confusion",
    "matrix",
    "copy",
    "thing",
    "torch",
    "metrics",
    "come",
    "google",
    "colab",
    "using",
    "google",
    "colab",
    "think",
    "ml",
    "extend",
    "need",
    "certain",
    "version",
    "ml",
    "extend",
    "google",
    "colab",
    "yet",
    "yet",
    "actually",
    "need",
    "version",
    "going",
    "import",
    "second",
    "let",
    "first",
    "make",
    "predictions",
    "across",
    "entire",
    "test",
    "data",
    "set",
    "previously",
    "made",
    "predictions",
    "nine",
    "random",
    "samples",
    "random",
    "sample",
    "selected",
    "nine",
    "could",
    "course",
    "change",
    "number",
    "make",
    "nine",
    "samples",
    "let",
    "write",
    "code",
    "make",
    "predictions",
    "across",
    "entire",
    "test",
    "data",
    "set",
    "import",
    "progress",
    "bar",
    "tracking",
    "need",
    "believe",
    "already",
    "got",
    "going",
    "anyway",
    "completeness",
    "going",
    "make",
    "step",
    "one",
    "make",
    "predictions",
    "make",
    "predictions",
    "trained",
    "model",
    "trained",
    "model",
    "model",
    "two",
    "let",
    "create",
    "empty",
    "predictions",
    "list",
    "add",
    "predictions",
    "going",
    "set",
    "model",
    "evaluation",
    "mode",
    "going",
    "set",
    "torch",
    "inference",
    "mode",
    "context",
    "manager",
    "inside",
    "let",
    "build",
    "sort",
    "code",
    "used",
    "testing",
    "loop",
    "except",
    "time",
    "going",
    "append",
    "predictions",
    "list",
    "going",
    "iterate",
    "test",
    "data",
    "loader",
    "give",
    "tqdm",
    "description",
    "going",
    "say",
    "making",
    "predictions",
    "dot",
    "dot",
    "dot",
    "see",
    "looks",
    "like",
    "minute",
    "going",
    "send",
    "data",
    "targets",
    "target",
    "device",
    "x",
    "equals",
    "x",
    "device",
    "device",
    "wonderful",
    "going",
    "forward",
    "pass",
    "going",
    "create",
    "logit",
    "remember",
    "raw",
    "outputs",
    "model",
    "linear",
    "layer",
    "end",
    "referred",
    "logits",
    "need",
    "calculate",
    "loss",
    "want",
    "turn",
    "predictions",
    "logits",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "set",
    "pred",
    "equals",
    "torch",
    "dot",
    "softmax",
    "could",
    "actually",
    "skip",
    "torch",
    "softmax",
    "step",
    "wanted",
    "take",
    "argmax",
    "logits",
    "go",
    "prediction",
    "probabilities",
    "pred",
    "labels",
    "completeness",
    "squeeze",
    "going",
    "across",
    "first",
    "dimension",
    "zeroth",
    "dimension",
    "take",
    "argmax",
    "across",
    "first",
    "dimension",
    "well",
    "little",
    "tidbit",
    "take",
    "different",
    "dimensions",
    "probably",
    "get",
    "different",
    "values",
    "check",
    "inputs",
    "outputs",
    "code",
    "make",
    "sure",
    "using",
    "right",
    "dimension",
    "let",
    "go",
    "put",
    "predictions",
    "cpu",
    "evaluation",
    "going",
    "plot",
    "anything",
    "plot",
    "lib",
    "want",
    "cpu",
    "going",
    "append",
    "predictions",
    "preds",
    "pred",
    "dot",
    "cpu",
    "beautiful",
    "going",
    "list",
    "different",
    "predictions",
    "use",
    "concatenate",
    "list",
    "predictions",
    "tensor",
    "let",
    "print",
    "preds",
    "show",
    "looks",
    "like",
    "go",
    "pred",
    "tensor",
    "going",
    "turn",
    "list",
    "predictions",
    "single",
    "tensor",
    "go",
    "pred",
    "tensor",
    "view",
    "first",
    "let",
    "see",
    "works",
    "making",
    "predictions",
    "oh",
    "would",
    "look",
    "okay",
    "yeah",
    "list",
    "predictions",
    "big",
    "list",
    "tensors",
    "right",
    "really",
    "want",
    "like",
    "get",
    "rid",
    "progress",
    "bar",
    "going",
    "batch",
    "test",
    "data",
    "load",
    "313",
    "batches",
    "comment",
    "print",
    "preds",
    "line",
    "torch",
    "dot",
    "cat",
    "preds",
    "going",
    "turn",
    "tensors",
    "single",
    "tensor",
    "list",
    "tensors",
    "single",
    "tensor",
    "concatenate",
    "look",
    "go",
    "beautiful",
    "look",
    "whole",
    "thing",
    "making",
    "predictions",
    "every",
    "single",
    "time",
    "right",
    "pretty",
    "quick",
    "go",
    "one",
    "big",
    "long",
    "tensor",
    "check",
    "length",
    "pred",
    "tensor",
    "one",
    "prediction",
    "per",
    "test",
    "sample",
    "beautiful",
    "going",
    "need",
    "install",
    "torch",
    "metrics",
    "torch",
    "metrics",
    "come",
    "google",
    "colab",
    "time",
    "recording",
    "let",
    "show",
    "tried",
    "import",
    "torch",
    "metrics",
    "might",
    "future",
    "keep",
    "mind",
    "might",
    "come",
    "google",
    "colab",
    "pretty",
    "useful",
    "package",
    "let",
    "install",
    "see",
    "required",
    "packages",
    "installed",
    "install",
    "install",
    "torch",
    "metrics",
    "finish",
    "video",
    "trying",
    "import",
    "set",
    "try",
    "accept",
    "loop",
    "python",
    "going",
    "try",
    "import",
    "torch",
    "metrics",
    "ml",
    "extend",
    "write",
    "like",
    "may",
    "already",
    "metrics",
    "ml",
    "extend",
    "running",
    "code",
    "local",
    "machine",
    "running",
    "google",
    "colab",
    "sure",
    "many",
    "going",
    "try",
    "import",
    "anyway",
    "work",
    "going",
    "install",
    "ml",
    "extend",
    "going",
    "check",
    "version",
    "need",
    "version",
    "plot",
    "confusion",
    "matrix",
    "function",
    "one",
    "need",
    "version",
    "higher",
    "going",
    "write",
    "little",
    "statement",
    "assert",
    "int",
    "ml",
    "extend",
    "dot",
    "version",
    "two",
    "condition",
    "try",
    "loop",
    "try",
    "block",
    "accepted",
    "skip",
    "next",
    "step",
    "dot",
    "split",
    "going",
    "check",
    "first",
    "index",
    "string",
    "equals",
    "greater",
    "equal",
    "otherwise",
    "going",
    "return",
    "error",
    "saying",
    "ml",
    "extend",
    "version",
    "higher",
    "let",
    "show",
    "looks",
    "like",
    "run",
    "string",
    "int",
    "turn",
    "string",
    "oh",
    "excuse",
    "go",
    "need",
    "bracket",
    "end",
    "go",
    "saying",
    "saying",
    "hey",
    "version",
    "ml",
    "extend",
    "0",
    "19",
    "higher",
    "right",
    "google",
    "colab",
    "default",
    "14",
    "may",
    "change",
    "future",
    "let",
    "finish",
    "accept",
    "block",
    "condition",
    "fails",
    "going",
    "pip",
    "install",
    "going",
    "install",
    "google",
    "colab",
    "torch",
    "metrics",
    "going",
    "quietly",
    "also",
    "going",
    "pass",
    "u",
    "tag",
    "update",
    "ml",
    "extend",
    "import",
    "torch",
    "metrics",
    "ml",
    "extend",
    "afterwards",
    "installed",
    "upgraded",
    "print",
    "going",
    "go",
    "ml",
    "extend",
    "version",
    "going",
    "go",
    "ml",
    "extend",
    "underscore",
    "version",
    "let",
    "see",
    "happens",
    "run",
    "see",
    "yeah",
    "installation",
    "happening",
    "going",
    "install",
    "torch",
    "metrics",
    "oh",
    "ml",
    "extend",
    "upgraded",
    "version",
    "let",
    "look",
    "may",
    "need",
    "restart",
    "google",
    "colab",
    "instance",
    "ah",
    "okay",
    "let",
    "take",
    "quiet",
    "going",
    "tell",
    "us",
    "restart",
    "google",
    "colab",
    "well",
    "let",
    "restart",
    "runtime",
    "run",
    "cell",
    "using",
    "google",
    "colab",
    "may",
    "restart",
    "runtime",
    "reflect",
    "fact",
    "updated",
    "version",
    "ml",
    "extend",
    "going",
    "restart",
    "runtime",
    "otherwise",
    "wo",
    "able",
    "plot",
    "confusion",
    "matrix",
    "need",
    "going",
    "run",
    "cells",
    "going",
    "pause",
    "video",
    "run",
    "cells",
    "clicking",
    "run",
    "note",
    "run",
    "errors",
    "run",
    "cells",
    "manually",
    "going",
    "get",
    "back",
    "cell",
    "make",
    "sure",
    "ml",
    "extend",
    "version",
    "see",
    "seconds",
    "back",
    "little",
    "heads",
    "restart",
    "runtime",
    "click",
    "run",
    "colab",
    "notebook",
    "stop",
    "running",
    "cells",
    "runs",
    "error",
    "error",
    "found",
    "previous",
    "video",
    "data",
    "model",
    "different",
    "devices",
    "skip",
    "past",
    "jump",
    "next",
    "cell",
    "click",
    "run",
    "go",
    "going",
    "run",
    "cells",
    "us",
    "going",
    "retrain",
    "models",
    "everything",
    "going",
    "get",
    "rerun",
    "going",
    "come",
    "right",
    "back",
    "trying",
    "install",
    "updated",
    "version",
    "ml",
    "extend",
    "going",
    "write",
    "code",
    "code",
    "running",
    "import",
    "ml",
    "extend",
    "going",
    "make",
    "sure",
    "got",
    "right",
    "version",
    "may",
    "require",
    "runtime",
    "restart",
    "may",
    "try",
    "see",
    "run",
    "install",
    "torch",
    "metrics",
    "upgrade",
    "ml",
    "extend",
    "see",
    "import",
    "ml",
    "extend",
    "version",
    "able",
    "run",
    "code",
    "yeah",
    "go",
    "wonderful",
    "ml",
    "extend",
    "got",
    "ml",
    "extend",
    "version",
    "assert",
    "import",
    "beautiful",
    "got",
    "lot",
    "extra",
    "code",
    "next",
    "video",
    "let",
    "move",
    "forward",
    "creating",
    "confusion",
    "matrix",
    "wanted",
    "show",
    "install",
    "upgrade",
    "packages",
    "google",
    "colab",
    "got",
    "predictions",
    "across",
    "entire",
    "test",
    "data",
    "set",
    "going",
    "moving",
    "towards",
    "using",
    "confusion",
    "matrix",
    "function",
    "compare",
    "predictions",
    "versus",
    "target",
    "data",
    "test",
    "data",
    "set",
    "see",
    "next",
    "video",
    "let",
    "plot",
    "confusion",
    "matrix",
    "welcome",
    "back",
    "last",
    "video",
    "wrote",
    "bunch",
    "code",
    "import",
    "extra",
    "libraries",
    "need",
    "plotting",
    "confusion",
    "matrix",
    "really",
    "helpful",
    "way",
    "google",
    "colab",
    "comes",
    "lot",
    "prebuilt",
    "installed",
    "stuff",
    "definitely",
    "later",
    "track",
    "going",
    "need",
    "experience",
    "installing",
    "stuff",
    "one",
    "way",
    "also",
    "made",
    "predictions",
    "across",
    "entire",
    "test",
    "data",
    "set",
    "got",
    "predictions",
    "tensor",
    "going",
    "confusion",
    "matrix",
    "confirm",
    "compare",
    "predictions",
    "target",
    "labels",
    "test",
    "data",
    "set",
    "done",
    "step",
    "number",
    "one",
    "prepared",
    "step",
    "two",
    "three",
    "installing",
    "torch",
    "metrics",
    "installing",
    "ml",
    "extend",
    "later",
    "version",
    "ml",
    "extend",
    "let",
    "go",
    "step",
    "two",
    "making",
    "confusion",
    "matrix",
    "step",
    "three",
    "plotting",
    "confusion",
    "matrix",
    "going",
    "look",
    "good",
    "love",
    "good",
    "confusion",
    "matrix",
    "look",
    "got",
    "torch",
    "metrics",
    "going",
    "import",
    "confusion",
    "matrix",
    "class",
    "ml",
    "extend",
    "going",
    "go",
    "plotting",
    "module",
    "import",
    "plot",
    "confusion",
    "matrix",
    "recall",
    "documentation",
    "within",
    "torch",
    "metrics",
    "within",
    "ml",
    "extend",
    "let",
    "see",
    "look",
    "like",
    "number",
    "two",
    "set",
    "confusion",
    "matrix",
    "instance",
    "compare",
    "predictions",
    "targets",
    "evaluating",
    "model",
    "right",
    "comparing",
    "models",
    "predictions",
    "target",
    "predictions",
    "going",
    "set",
    "confusion",
    "matrix",
    "variable",
    "conf",
    "mat",
    "going",
    "call",
    "confusion",
    "matrix",
    "class",
    "torch",
    "metrics",
    "set",
    "instance",
    "need",
    "pass",
    "number",
    "classes",
    "10",
    "classes",
    "contained",
    "within",
    "class",
    "names",
    "recall",
    "class",
    "names",
    "list",
    "different",
    "classes",
    "working",
    "going",
    "pass",
    "number",
    "classes",
    "length",
    "class",
    "names",
    "use",
    "conf",
    "mat",
    "instance",
    "confusion",
    "matrix",
    "instance",
    "create",
    "confusion",
    "matrix",
    "tensor",
    "passing",
    "conf",
    "mat",
    "created",
    "conf",
    "mat",
    "like",
    "loss",
    "function",
    "going",
    "pass",
    "preds",
    "equals",
    "pred",
    "tensor",
    "pred",
    "tensor",
    "calculated",
    "predictions",
    "test",
    "data",
    "set",
    "go",
    "preds",
    "target",
    "going",
    "equal",
    "test",
    "data",
    "dot",
    "targets",
    "test",
    "data",
    "data",
    "set",
    "seen",
    "go",
    "test",
    "data",
    "press",
    "tab",
    "got",
    "bunch",
    "different",
    "attributes",
    "get",
    "classes",
    "course",
    "get",
    "targets",
    "labels",
    "pytorch",
    "calls",
    "labels",
    "targets",
    "usually",
    "refer",
    "labels",
    "target",
    "test",
    "data",
    "target",
    "want",
    "compare",
    "models",
    "predictions",
    "test",
    "data",
    "set",
    "test",
    "data",
    "targets",
    "let",
    "keep",
    "going",
    "forward",
    "step",
    "number",
    "three",
    "going",
    "create",
    "confusion",
    "matrix",
    "tensor",
    "oh",
    "let",
    "see",
    "looks",
    "like",
    "actually",
    "conf",
    "mat",
    "tensor",
    "oh",
    "okay",
    "got",
    "fair",
    "bit",
    "going",
    "let",
    "turn",
    "pretty",
    "version",
    "along",
    "bottom",
    "going",
    "predicted",
    "labels",
    "along",
    "side",
    "going",
    "true",
    "labels",
    "power",
    "ml",
    "extend",
    "comes",
    "going",
    "plot",
    "confusion",
    "matrix",
    "let",
    "create",
    "figure",
    "axes",
    "going",
    "call",
    "function",
    "plot",
    "confusion",
    "matrix",
    "imported",
    "going",
    "pass",
    "conf",
    "mat",
    "equals",
    "conf",
    "mat",
    "tensor",
    "working",
    "map",
    "plot",
    "lib",
    "want",
    "numpy",
    "going",
    "write",
    "map",
    "plot",
    "lib",
    "likes",
    "working",
    "numpy",
    "going",
    "pass",
    "class",
    "names",
    "get",
    "labels",
    "rows",
    "columns",
    "class",
    "names",
    "list",
    "text",
    "based",
    "class",
    "names",
    "going",
    "set",
    "fig",
    "size",
    "favorite",
    "hand",
    "poker",
    "10",
    "seven",
    "also",
    "happens",
    "good",
    "dimension",
    "google",
    "colab",
    "look",
    "oh",
    "something",
    "beautiful",
    "see",
    "confusion",
    "matrix",
    "ideal",
    "confusion",
    "matrix",
    "diagonal",
    "rows",
    "darkened",
    "values",
    "values",
    "values",
    "means",
    "predicted",
    "label",
    "lines",
    "true",
    "label",
    "case",
    "definitely",
    "dark",
    "diagonal",
    "let",
    "dive",
    "highest",
    "numbers",
    "looks",
    "like",
    "model",
    "predicting",
    "shirt",
    "true",
    "label",
    "actually",
    "shirt",
    "slash",
    "top",
    "reflective",
    "saw",
    "still",
    "image",
    "okay",
    "image",
    "previous",
    "video",
    "saw",
    "plotted",
    "predictions",
    "model",
    "predicted",
    "shirt",
    "slash",
    "top",
    "actually",
    "shirt",
    "course",
    "vice",
    "versa",
    "another",
    "one",
    "looks",
    "like",
    "model",
    "predicting",
    "shirt",
    "actually",
    "coat",
    "something",
    "use",
    "visually",
    "inspect",
    "data",
    "see",
    "errors",
    "model",
    "making",
    "make",
    "sense",
    "visual",
    "perspective",
    "getting",
    "confused",
    "predicting",
    "pull",
    "actual",
    "label",
    "coat",
    "predicting",
    "pull",
    "actual",
    "label",
    "shirt",
    "lot",
    "things",
    "clothing",
    "wise",
    "data",
    "wise",
    "may",
    "fact",
    "look",
    "quite",
    "relatively",
    "large",
    "one",
    "well",
    "predicting",
    "sneaker",
    "ankle",
    "boot",
    "confusing",
    "two",
    "different",
    "types",
    "shoes",
    "way",
    "evaluate",
    "model",
    "start",
    "go",
    "hmm",
    "maybe",
    "labels",
    "little",
    "bit",
    "confusing",
    "could",
    "expand",
    "little",
    "bit",
    "keep",
    "mind",
    "confusion",
    "matrix",
    "one",
    "powerful",
    "ways",
    "visualize",
    "classification",
    "model",
    "predictions",
    "really",
    "really",
    "really",
    "helpful",
    "way",
    "creating",
    "one",
    "use",
    "torch",
    "metrics",
    "confusion",
    "matrix",
    "plot",
    "use",
    "plot",
    "confusion",
    "matrix",
    "ml",
    "extend",
    "however",
    "using",
    "google",
    "colab",
    "may",
    "need",
    "import",
    "install",
    "confusion",
    "matrix",
    "like",
    "classification",
    "metrics",
    "got",
    "got",
    "course",
    "torch",
    "metrics",
    "give",
    "look",
    "think",
    "next",
    "video",
    "done",
    "fair",
    "bit",
    "evaluation",
    "workflow",
    "believe",
    "time",
    "saved",
    "loaded",
    "best",
    "trained",
    "model",
    "let",
    "give",
    "go",
    "see",
    "next",
    "video",
    "last",
    "video",
    "created",
    "beautiful",
    "confusion",
    "matrix",
    "power",
    "torch",
    "metrics",
    "ml",
    "extend",
    "time",
    "save",
    "load",
    "best",
    "model",
    "evaluated",
    "convolutional",
    "neural",
    "network",
    "go",
    "know",
    "model",
    "pretty",
    "good",
    "let",
    "export",
    "file",
    "use",
    "somewhere",
    "else",
    "let",
    "see",
    "way",
    "go",
    "keynote",
    "got",
    "value",
    "model",
    "torch",
    "metrics",
    "fair",
    "times",
    "improved",
    "experimentation",
    "used",
    "tensor",
    "board",
    "yet",
    "later",
    "video",
    "save",
    "reload",
    "trained",
    "model",
    "gone",
    "steps",
    "enough",
    "times",
    "like",
    "know",
    "let",
    "save",
    "model",
    "use",
    "elsewhere",
    "reload",
    "make",
    "sure",
    "saved",
    "correctly",
    "let",
    "go",
    "step",
    "want",
    "number",
    "going",
    "go",
    "save",
    "load",
    "best",
    "performing",
    "model",
    "may",
    "already",
    "done",
    "parts",
    "course",
    "definitely",
    "want",
    "give",
    "go",
    "pause",
    "video",
    "try",
    "believe",
    "notebook",
    "number",
    "one",
    "go",
    "saving",
    "loading",
    "pie",
    "torch",
    "model",
    "go",
    "section",
    "section",
    "number",
    "one",
    "see",
    "otherwise",
    "let",
    "code",
    "together",
    "going",
    "start",
    "importing",
    "path",
    "path",
    "lib",
    "like",
    "create",
    "model",
    "directory",
    "path",
    "create",
    "model",
    "directory",
    "path",
    "model",
    "path",
    "going",
    "set",
    "equal",
    "path",
    "going",
    "save",
    "models",
    "want",
    "want",
    "create",
    "file",
    "called",
    "models",
    "save",
    "models",
    "model",
    "path",
    "dot",
    "mkd",
    "make",
    "directory",
    "parents",
    "yes",
    "wanted",
    "make",
    "parent",
    "directories",
    "exist",
    "exist",
    "okay",
    "also",
    "equals",
    "true",
    "try",
    "create",
    "already",
    "existing",
    "going",
    "get",
    "error",
    "fine",
    "next",
    "going",
    "create",
    "model",
    "save",
    "path",
    "going",
    "add",
    "code",
    "cells",
    "space",
    "let",
    "pass",
    "model",
    "name",
    "going",
    "set",
    "equal",
    "since",
    "section",
    "three",
    "going",
    "call",
    "three",
    "pie",
    "torch",
    "computer",
    "vision",
    "model",
    "two",
    "best",
    "model",
    "going",
    "save",
    "pth",
    "pie",
    "torch",
    "also",
    "save",
    "dot",
    "pt",
    "like",
    "use",
    "pth",
    "going",
    "go",
    "model",
    "save",
    "path",
    "equal",
    "model",
    "path",
    "slash",
    "model",
    "name",
    "look",
    "going",
    "path",
    "called",
    "model",
    "save",
    "path",
    "going",
    "posix",
    "path",
    "models",
    "three",
    "pie",
    "torch",
    "computer",
    "vision",
    "model",
    "two",
    "dot",
    "pth",
    "look",
    "yeah",
    "models",
    "directory",
    "going",
    "anything",
    "moment",
    "got",
    "data",
    "directory",
    "fashion",
    "mnist",
    "good",
    "way",
    "start",
    "setting",
    "directories",
    "break",
    "data",
    "models",
    "helper",
    "function",
    "files",
    "etc",
    "let",
    "keep",
    "going",
    "let",
    "save",
    "save",
    "model",
    "state",
    "dict",
    "going",
    "go",
    "print",
    "saving",
    "model",
    "going",
    "give",
    "us",
    "information",
    "happening",
    "model",
    "save",
    "path",
    "save",
    "model",
    "calling",
    "torch",
    "dot",
    "save",
    "pass",
    "object",
    "want",
    "save",
    "using",
    "object",
    "parameter",
    "obj",
    "get",
    "doc",
    "string",
    "going",
    "go",
    "model",
    "two",
    "want",
    "save",
    "state",
    "dict",
    "recall",
    "state",
    "dict",
    "going",
    "models",
    "models",
    "learned",
    "parameters",
    "data",
    "set",
    "weights",
    "biases",
    "sort",
    "jazz",
    "beautiful",
    "first",
    "created",
    "model",
    "two",
    "random",
    "numbers",
    "since",
    "trained",
    "model",
    "two",
    "training",
    "data",
    "updated",
    "represent",
    "training",
    "images",
    "leverage",
    "later",
    "seen",
    "make",
    "predictions",
    "going",
    "go",
    "saving",
    "file",
    "path",
    "going",
    "model",
    "save",
    "path",
    "let",
    "run",
    "see",
    "happens",
    "beautiful",
    "saving",
    "model",
    "model",
    "directory",
    "let",
    "look",
    "model",
    "yes",
    "beautiful",
    "quickly",
    "save",
    "model",
    "course",
    "customize",
    "name",
    "save",
    "et",
    "cetera",
    "et",
    "cetera",
    "let",
    "see",
    "happens",
    "load",
    "create",
    "new",
    "instance",
    "saved",
    "state",
    "dict",
    "model",
    "two",
    "need",
    "create",
    "new",
    "instance",
    "model",
    "two",
    "created",
    "class",
    "fashion",
    "mnist",
    "v",
    "two",
    "saved",
    "whole",
    "model",
    "could",
    "import",
    "new",
    "variable",
    "let",
    "read",
    "back",
    "different",
    "ways",
    "saving",
    "model",
    "also",
    "link",
    "pytorch",
    "documentation",
    "would",
    "highly",
    "recommend",
    "let",
    "see",
    "action",
    "need",
    "create",
    "new",
    "instance",
    "fashion",
    "mnist",
    "model",
    "v",
    "two",
    "convolution",
    "neural",
    "network",
    "going",
    "set",
    "manual",
    "seed",
    "way",
    "create",
    "new",
    "instance",
    "instantiated",
    "random",
    "numbers",
    "going",
    "set",
    "loaded",
    "model",
    "two",
    "equals",
    "fashion",
    "mnist",
    "v",
    "two",
    "important",
    "set",
    "parameters",
    "original",
    "saved",
    "model",
    "fashion",
    "mnist",
    "v",
    "two",
    "oh",
    "got",
    "typo",
    "fashion",
    "mnist",
    "model",
    "v",
    "two",
    "wonderful",
    "input",
    "shape",
    "going",
    "one",
    "number",
    "color",
    "channels",
    "test",
    "images",
    "test",
    "image",
    "dot",
    "shape",
    "still",
    "test",
    "image",
    "oh",
    "well",
    "created",
    "different",
    "one",
    "image",
    "size",
    "image",
    "shape",
    "12828",
    "image",
    "shape",
    "color",
    "channels",
    "height",
    "width",
    "create",
    "hidden",
    "units",
    "use",
    "10",
    "hidden",
    "units",
    "set",
    "important",
    "otherwise",
    "shapes",
    "going",
    "get",
    "going",
    "get",
    "shape",
    "mismatch",
    "error",
    "output",
    "shape",
    "also",
    "going",
    "10",
    "length",
    "class",
    "names",
    "class",
    "names",
    "variable",
    "instantiated",
    "going",
    "load",
    "saved",
    "state",
    "dict",
    "one",
    "saved",
    "go",
    "loaded",
    "model",
    "two",
    "dot",
    "load",
    "state",
    "dict",
    "pass",
    "torch",
    "dot",
    "load",
    "file",
    "want",
    "load",
    "file",
    "path",
    "model",
    "save",
    "path",
    "like",
    "save",
    "path",
    "variables",
    "variable",
    "use",
    "later",
    "instead",
    "typing",
    "time",
    "definitely",
    "prone",
    "errors",
    "going",
    "send",
    "model",
    "target",
    "device",
    "loaded",
    "model",
    "two",
    "dot",
    "two",
    "device",
    "beautiful",
    "let",
    "see",
    "happens",
    "wonderful",
    "let",
    "evaluate",
    "loaded",
    "model",
    "evaluate",
    "loaded",
    "model",
    "results",
    "much",
    "model",
    "two",
    "results",
    "model",
    "two",
    "results",
    "looking",
    "want",
    "make",
    "sure",
    "saved",
    "model",
    "saved",
    "results",
    "pretty",
    "closely",
    "say",
    "pretty",
    "closely",
    "might",
    "find",
    "discrepancies",
    "lower",
    "lower",
    "decimals",
    "way",
    "files",
    "get",
    "saved",
    "something",
    "gets",
    "lost",
    "et",
    "cetera",
    "et",
    "cetera",
    "precision",
    "computing",
    "long",
    "first",
    "numbers",
    "quite",
    "similar",
    "well",
    "gravy",
    "let",
    "go",
    "torch",
    "manual",
    "seed",
    "remember",
    "evaluating",
    "model",
    "almost",
    "well",
    "important",
    "training",
    "model",
    "making",
    "sure",
    "model",
    "save",
    "correctly",
    "deployed",
    "deployed",
    "save",
    "correctly",
    "well",
    "get",
    "would",
    "get",
    "less",
    "ideal",
    "results",
    "would",
    "model",
    "equals",
    "loaded",
    "model",
    "two",
    "going",
    "use",
    "model",
    "function",
    "way",
    "course",
    "going",
    "evaluate",
    "test",
    "data",
    "set",
    "using",
    "test",
    "data",
    "loader",
    "going",
    "create",
    "loss",
    "function",
    "put",
    "loss",
    "function",
    "created",
    "accuracy",
    "function",
    "accuracy",
    "function",
    "using",
    "throughout",
    "notebook",
    "let",
    "check",
    "loaded",
    "model",
    "two",
    "results",
    "quite",
    "similar",
    "one",
    "going",
    "make",
    "predictions",
    "go",
    "numbers",
    "yes",
    "five",
    "six",
    "eight",
    "two",
    "nine",
    "five",
    "six",
    "eight",
    "two",
    "nine",
    "wonderful",
    "three",
    "one",
    "three",
    "five",
    "eight",
    "three",
    "one",
    "three",
    "five",
    "eight",
    "beautiful",
    "looks",
    "like",
    "loaded",
    "model",
    "gets",
    "results",
    "previously",
    "trained",
    "model",
    "even",
    "saved",
    "wanted",
    "check",
    "close",
    "also",
    "use",
    "torch",
    "dot",
    "close",
    "check",
    "model",
    "results",
    "wanted",
    "check",
    "close",
    "programmatically",
    "looked",
    "visually",
    "check",
    "model",
    "results",
    "close",
    "go",
    "torch",
    "close",
    "going",
    "pass",
    "torch",
    "dot",
    "tensor",
    "turn",
    "values",
    "tensor",
    "going",
    "go",
    "model",
    "two",
    "results",
    "compare",
    "model",
    "loss",
    "want",
    "make",
    "sure",
    "loss",
    "values",
    "close",
    "torch",
    "dot",
    "close",
    "torch",
    "dot",
    "tensor",
    "model",
    "want",
    "one",
    "loaded",
    "model",
    "two",
    "results",
    "model",
    "loss",
    "another",
    "bracket",
    "end",
    "see",
    "close",
    "true",
    "wonderful",
    "return",
    "true",
    "also",
    "adjust",
    "tolerance",
    "levels",
    "go",
    "atal",
    "equals",
    "going",
    "absolute",
    "tolerance",
    "one",
    "negative",
    "eight",
    "saying",
    "like",
    "hey",
    "need",
    "make",
    "sure",
    "results",
    "basically",
    "eight",
    "decimal",
    "points",
    "probably",
    "quite",
    "low",
    "would",
    "say",
    "make",
    "sure",
    "least",
    "within",
    "two",
    "getting",
    "discrepancies",
    "saved",
    "model",
    "loaded",
    "model",
    "sorry",
    "model",
    "original",
    "one",
    "loaded",
    "model",
    "quite",
    "large",
    "like",
    "decimal",
    "points",
    "column",
    "even",
    "go",
    "back",
    "code",
    "make",
    "sure",
    "model",
    "saving",
    "correctly",
    "make",
    "sure",
    "got",
    "random",
    "seeds",
    "set",
    "pretty",
    "close",
    "like",
    "terms",
    "within",
    "three",
    "two",
    "decimal",
    "places",
    "well",
    "say",
    "close",
    "enough",
    "also",
    "adjust",
    "tolerance",
    "level",
    "check",
    "model",
    "results",
    "close",
    "enough",
    "programmatically",
    "wow",
    "covered",
    "fair",
    "bit",
    "gone",
    "entire",
    "workflow",
    "computer",
    "vision",
    "problem",
    "let",
    "next",
    "video",
    "think",
    "enough",
    "code",
    "section",
    "section",
    "three",
    "pytorch",
    "computer",
    "vision",
    "got",
    "exercises",
    "extra",
    "curriculum",
    "lined",
    "let",
    "look",
    "next",
    "video",
    "see",
    "goodness",
    "look",
    "much",
    "computer",
    "vision",
    "pytorch",
    "code",
    "written",
    "together",
    "started",
    "right",
    "top",
    "looked",
    "reference",
    "notebook",
    "online",
    "book",
    "checked",
    "computer",
    "vision",
    "libraries",
    "pytorch",
    "main",
    "one",
    "torch",
    "vision",
    "got",
    "data",
    "set",
    "namely",
    "fashion",
    "mnist",
    "data",
    "set",
    "bunch",
    "data",
    "sets",
    "could",
    "looked",
    "fact",
    "encourage",
    "try",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "use",
    "steps",
    "done",
    "try",
    "another",
    "data",
    "set",
    "repaired",
    "data",
    "loaders",
    "turned",
    "data",
    "batches",
    "built",
    "baseline",
    "model",
    "important",
    "step",
    "machine",
    "learning",
    "baseline",
    "model",
    "usually",
    "relatively",
    "simple",
    "going",
    "serve",
    "baseline",
    "going",
    "try",
    "improve",
    "upon",
    "go",
    "back",
    "keynote",
    "various",
    "experiments",
    "made",
    "predictions",
    "model",
    "zero",
    "evaluated",
    "timed",
    "predictions",
    "see",
    "running",
    "models",
    "gpu",
    "faster",
    "learned",
    "sometimes",
    "gpu",
    "wo",
    "necessarily",
    "speed",
    "code",
    "relatively",
    "small",
    "data",
    "set",
    "overheads",
    "copying",
    "data",
    "cpu",
    "gpu",
    "tried",
    "model",
    "saw",
    "really",
    "improve",
    "upon",
    "baseline",
    "model",
    "brought",
    "big",
    "guns",
    "convolutional",
    "neural",
    "network",
    "replicating",
    "cnn",
    "explainer",
    "website",
    "gosh",
    "spend",
    "lot",
    "time",
    "encourage",
    "part",
    "extra",
    "curriculum",
    "go",
    "still",
    "even",
    "come",
    "back",
    "refer",
    "referred",
    "lot",
    "making",
    "materials",
    "video",
    "section",
    "code",
    "section",
    "sure",
    "go",
    "back",
    "check",
    "cnn",
    "explainer",
    "website",
    "going",
    "behind",
    "scenes",
    "cnns",
    "coded",
    "one",
    "using",
    "pure",
    "pytorch",
    "amazing",
    "compared",
    "model",
    "results",
    "across",
    "different",
    "experiments",
    "found",
    "convolutional",
    "neural",
    "network",
    "best",
    "although",
    "took",
    "little",
    "bit",
    "longer",
    "train",
    "also",
    "learned",
    "training",
    "time",
    "values",
    "definitely",
    "vary",
    "depending",
    "hardware",
    "using",
    "something",
    "keep",
    "mind",
    "made",
    "evaluated",
    "random",
    "predictions",
    "best",
    "model",
    "important",
    "step",
    "visualizing",
    "visualizing",
    "visualizing",
    "model",
    "predictions",
    "could",
    "get",
    "evaluation",
    "metrics",
    "start",
    "actually",
    "visualize",
    "going",
    "well",
    "case",
    "best",
    "understand",
    "model",
    "thinking",
    "saw",
    "confusion",
    "matrix",
    "using",
    "two",
    "different",
    "libraries",
    "torch",
    "metrics",
    "ml",
    "extend",
    "great",
    "way",
    "evaluate",
    "classification",
    "models",
    "saw",
    "save",
    "load",
    "best",
    "performing",
    "model",
    "file",
    "made",
    "sure",
    "results",
    "saved",
    "model",
    "different",
    "model",
    "trained",
    "within",
    "notebook",
    "time",
    "love",
    "practice",
    "gone",
    "actually",
    "really",
    "exciting",
    "gone",
    "computer",
    "vision",
    "problem",
    "got",
    "exercises",
    "prepared",
    "go",
    "learn",
    "website",
    "section",
    "03",
    "scroll",
    "read",
    "materials",
    "covered",
    "pure",
    "code",
    "lot",
    "pictures",
    "notebook",
    "helpful",
    "learn",
    "things",
    "going",
    "exercises",
    "exercises",
    "focused",
    "practicing",
    "code",
    "sections",
    "two",
    "resources",
    "also",
    "extra",
    "curriculum",
    "put",
    "together",
    "want",
    "understanding",
    "going",
    "behind",
    "scenes",
    "convolutional",
    "neural",
    "networks",
    "focused",
    "lot",
    "code",
    "highly",
    "recommend",
    "mit",
    "induction",
    "deep",
    "computer",
    "vision",
    "lecture",
    "spend",
    "10",
    "minutes",
    "clicking",
    "different",
    "options",
    "pytorch",
    "vision",
    "library",
    "torch",
    "vision",
    "look",
    "common",
    "convolutional",
    "neural",
    "networks",
    "torch",
    "vision",
    "model",
    "library",
    "larger",
    "number",
    "pytorch",
    "computer",
    "vision",
    "models",
    "get",
    "deeper",
    "computer",
    "vision",
    "probably",
    "going",
    "run",
    "torch",
    "image",
    "models",
    "library",
    "otherwise",
    "known",
    "10",
    "going",
    "leave",
    "extra",
    "curriculum",
    "going",
    "link",
    "exercises",
    "section",
    "learn",
    "exercises",
    "section",
    "come",
    "go",
    "also",
    "resource",
    "exercise",
    "template",
    "notebook",
    "got",
    "one",
    "three",
    "areas",
    "industry",
    "computer",
    "vision",
    "currently",
    "used",
    "pytorch",
    "deep",
    "learning",
    "repo",
    "extras",
    "exercises",
    "number",
    "three",
    "put",
    "template",
    "code",
    "fill",
    "different",
    "sections",
    "code",
    "related",
    "text",
    "based",
    "able",
    "completed",
    "referencing",
    "gone",
    "notebook",
    "one",
    "go",
    "back",
    "pytorch",
    "deep",
    "learning",
    "probably",
    "updated",
    "time",
    "get",
    "always",
    "find",
    "exercise",
    "extra",
    "curriculum",
    "going",
    "computer",
    "vision",
    "go",
    "exercise",
    "extra",
    "curriculum",
    "go",
    "extras",
    "file",
    "go",
    "solutions",
    "also",
    "started",
    "add",
    "video",
    "walkthroughs",
    "solutions",
    "going",
    "exercises",
    "coding",
    "get",
    "see",
    "unedited",
    "videos",
    "one",
    "long",
    "live",
    "stream",
    "done",
    "o2",
    "o3",
    "o4",
    "time",
    "watch",
    "video",
    "like",
    "see",
    "figure",
    "solutions",
    "exercises",
    "watch",
    "videos",
    "go",
    "first",
    "foremost",
    "would",
    "highly",
    "recommend",
    "trying",
    "exercises",
    "first",
    "get",
    "stuck",
    "refer",
    "notebook",
    "refer",
    "pytorch",
    "documentation",
    "finally",
    "check",
    "would",
    "coded",
    "potential",
    "solution",
    "number",
    "three",
    "computer",
    "vision",
    "exercise",
    "solutions",
    "congratulations",
    "going",
    "pytorch",
    "computer",
    "vision",
    "section",
    "see",
    "next",
    "section",
    "going",
    "look",
    "pytorch",
    "custom",
    "data",
    "sets",
    "spoilers",
    "see",
    "soon",
    "hello",
    "hello",
    "hello",
    "welcome",
    "section",
    "number",
    "four",
    "learn",
    "pytorch",
    "deep",
    "learning",
    "course",
    "custom",
    "data",
    "sets",
    "pytorch",
    "dive",
    "going",
    "cover",
    "let",
    "answer",
    "important",
    "question",
    "get",
    "help",
    "times",
    "important",
    "reiterate",
    "follow",
    "along",
    "code",
    "best",
    "going",
    "writing",
    "bunch",
    "pytorch",
    "code",
    "remember",
    "motto",
    "run",
    "code",
    "line",
    "try",
    "like",
    "read",
    "read",
    "doxtring",
    "press",
    "shift",
    "command",
    "plus",
    "space",
    "google",
    "colab",
    "windows",
    "command",
    "might",
    "control",
    "still",
    "stuck",
    "search",
    "two",
    "resources",
    "probably",
    "come",
    "across",
    "stack",
    "overflow",
    "wonderful",
    "pytorch",
    "documentation",
    "lot",
    "experience",
    "far",
    "course",
    "try",
    "go",
    "back",
    "code",
    "code",
    "run",
    "code",
    "finally",
    "still",
    "stuck",
    "ask",
    "question",
    "pytorch",
    "deep",
    "learning",
    "discussions",
    "github",
    "page",
    "click",
    "link",
    "come",
    "burke",
    "slash",
    "pytorch",
    "deep",
    "learning",
    "url",
    "seen",
    "trouble",
    "problem",
    "course",
    "start",
    "discussion",
    "select",
    "category",
    "general",
    "ideas",
    "polls",
    "q",
    "go",
    "video",
    "put",
    "video",
    "number",
    "99",
    "example",
    "code",
    "like",
    "say",
    "problem",
    "come",
    "write",
    "code",
    "code",
    "question",
    "something",
    "something",
    "something",
    "click",
    "start",
    "discussion",
    "help",
    "come",
    "back",
    "discussions",
    "course",
    "search",
    "going",
    "error",
    "feel",
    "like",
    "someone",
    "else",
    "might",
    "seen",
    "error",
    "course",
    "search",
    "find",
    "happening",
    "want",
    "highlight",
    "resources",
    "course",
    "learn",
    "section",
    "four",
    "beautiful",
    "online",
    "book",
    "version",
    "materials",
    "going",
    "cover",
    "section",
    "spoiler",
    "alert",
    "use",
    "reference",
    "course",
    "github",
    "notebook",
    "pytorch",
    "custom",
    "data",
    "sets",
    "ground",
    "truth",
    "notebook",
    "check",
    "get",
    "stuck",
    "going",
    "exit",
    "got",
    "pytorch",
    "custom",
    "data",
    "sets",
    "learn",
    "course",
    "discussions",
    "tab",
    "q",
    "jump",
    "back",
    "keynote",
    "might",
    "asking",
    "custom",
    "data",
    "set",
    "built",
    "fair",
    "pytorch",
    "deeplining",
    "neural",
    "networks",
    "far",
    "various",
    "data",
    "sets",
    "fashion",
    "mnist",
    "might",
    "wondering",
    "hey",
    "got",
    "data",
    "set",
    "working",
    "problem",
    "build",
    "model",
    "pytorch",
    "predict",
    "data",
    "set",
    "answer",
    "yes",
    "however",
    "go",
    "pre",
    "processing",
    "steps",
    "make",
    "data",
    "set",
    "compatible",
    "pytorch",
    "going",
    "covering",
    "section",
    "like",
    "highlight",
    "pytorch",
    "domain",
    "libraries",
    "little",
    "bit",
    "experience",
    "torch",
    "vision",
    "wanted",
    "classify",
    "whether",
    "photo",
    "pizza",
    "steak",
    "sushi",
    "computer",
    "vision",
    "image",
    "classification",
    "problem",
    "also",
    "text",
    "reviews",
    "positive",
    "negative",
    "use",
    "torch",
    "text",
    "one",
    "problem",
    "within",
    "vision",
    "space",
    "within",
    "text",
    "space",
    "want",
    "understand",
    "type",
    "vision",
    "data",
    "probably",
    "want",
    "look",
    "torch",
    "vision",
    "kind",
    "text",
    "data",
    "probably",
    "want",
    "look",
    "torch",
    "text",
    "audio",
    "wanted",
    "classify",
    "song",
    "playing",
    "shazam",
    "uses",
    "input",
    "sound",
    "sort",
    "music",
    "runs",
    "neural",
    "network",
    "classify",
    "certain",
    "song",
    "look",
    "torch",
    "audio",
    "like",
    "recommend",
    "something",
    "online",
    "store",
    "netflix",
    "something",
    "like",
    "like",
    "homepage",
    "updates",
    "recommendations",
    "like",
    "look",
    "torch",
    "rec",
    "stands",
    "recommendation",
    "system",
    "something",
    "keep",
    "mind",
    "domain",
    "libraries",
    "data",
    "sets",
    "module",
    "helps",
    "work",
    "different",
    "data",
    "sets",
    "different",
    "domains",
    "different",
    "domain",
    "libraries",
    "contain",
    "data",
    "loading",
    "functions",
    "different",
    "data",
    "sources",
    "torch",
    "vision",
    "let",
    "go",
    "next",
    "slide",
    "problem",
    "space",
    "vision",
    "pre",
    "built",
    "data",
    "sets",
    "existing",
    "data",
    "sets",
    "like",
    "seen",
    "fashion",
    "mnist",
    "well",
    "functions",
    "load",
    "vision",
    "data",
    "sets",
    "want",
    "look",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "click",
    "built",
    "data",
    "sets",
    "pie",
    "torch",
    "documentation",
    "go",
    "torch",
    "audio",
    "torch",
    "text",
    "torch",
    "vision",
    "torch",
    "rec",
    "torch",
    "data",
    "time",
    "recording",
    "april",
    "2022",
    "torch",
    "data",
    "currently",
    "beta",
    "going",
    "updated",
    "time",
    "keep",
    "mind",
    "updated",
    "time",
    "add",
    "even",
    "ways",
    "load",
    "different",
    "data",
    "resources",
    "going",
    "get",
    "familiar",
    "torch",
    "vision",
    "data",
    "sets",
    "went",
    "torch",
    "text",
    "another",
    "torch",
    "text",
    "dot",
    "data",
    "sets",
    "went",
    "torch",
    "audio",
    "torch",
    "audio",
    "dot",
    "data",
    "sets",
    "noticing",
    "trend",
    "depending",
    "domain",
    "working",
    "whether",
    "vision",
    "text",
    "audio",
    "data",
    "recommendation",
    "data",
    "probably",
    "want",
    "look",
    "custom",
    "library",
    "within",
    "pie",
    "torch",
    "course",
    "bonus",
    "torch",
    "data",
    "contains",
    "many",
    "different",
    "helper",
    "functions",
    "loading",
    "data",
    "currently",
    "beta",
    "april",
    "time",
    "watch",
    "torch",
    "data",
    "may",
    "beta",
    "something",
    "extra",
    "curriculum",
    "top",
    "going",
    "cover",
    "section",
    "let",
    "keep",
    "going",
    "going",
    "work",
    "towards",
    "building",
    "food",
    "vision",
    "mini",
    "going",
    "load",
    "data",
    "namely",
    "images",
    "pizza",
    "sushi",
    "steak",
    "food",
    "101",
    "data",
    "set",
    "going",
    "build",
    "image",
    "classification",
    "model",
    "model",
    "might",
    "power",
    "food",
    "vision",
    "recognition",
    "app",
    "food",
    "image",
    "recognition",
    "app",
    "going",
    "see",
    "classify",
    "image",
    "pizza",
    "pizza",
    "image",
    "sushi",
    "sushi",
    "image",
    "steak",
    "steak",
    "going",
    "focus",
    "want",
    "load",
    "say",
    "images",
    "existing",
    "already",
    "pizza",
    "sushi",
    "steak",
    "want",
    "write",
    "code",
    "load",
    "images",
    "food",
    "custom",
    "data",
    "set",
    "building",
    "food",
    "vision",
    "mini",
    "model",
    "quite",
    "similar",
    "go",
    "project",
    "working",
    "personally",
    "food",
    "image",
    "recognition",
    "model",
    "go",
    "still",
    "work",
    "progress",
    "going",
    "upload",
    "image",
    "food",
    "neutrify",
    "try",
    "classify",
    "type",
    "food",
    "steak",
    "go",
    "let",
    "upload",
    "beautiful",
    "steak",
    "going",
    "building",
    "similar",
    "model",
    "powers",
    "neutrify",
    "macro",
    "nutrients",
    "steak",
    "like",
    "find",
    "works",
    "got",
    "links",
    "let",
    "keep",
    "pushing",
    "forward",
    "go",
    "back",
    "keynote",
    "working",
    "towards",
    "said",
    "want",
    "load",
    "images",
    "pytorch",
    "build",
    "model",
    "already",
    "built",
    "computer",
    "vision",
    "model",
    "want",
    "figure",
    "get",
    "data",
    "computer",
    "vision",
    "model",
    "course",
    "adhering",
    "pytorch",
    "workflow",
    "used",
    "times",
    "going",
    "learn",
    "load",
    "data",
    "set",
    "custom",
    "data",
    "rather",
    "existing",
    "data",
    "set",
    "within",
    "pytorch",
    "see",
    "build",
    "model",
    "fit",
    "custom",
    "data",
    "set",
    "go",
    "steps",
    "involved",
    "training",
    "model",
    "picking",
    "loss",
    "function",
    "optimizer",
    "build",
    "training",
    "loop",
    "evaluate",
    "model",
    "improve",
    "experimentation",
    "see",
    "save",
    "reloading",
    "model",
    "also",
    "going",
    "practice",
    "predicting",
    "custom",
    "data",
    "important",
    "step",
    "whenever",
    "training",
    "models",
    "going",
    "cover",
    "broadly",
    "going",
    "get",
    "custom",
    "data",
    "set",
    "pytorch",
    "said",
    "going",
    "become",
    "one",
    "data",
    "words",
    "preparing",
    "visualizing",
    "learn",
    "transform",
    "data",
    "use",
    "model",
    "important",
    "step",
    "see",
    "load",
    "custom",
    "data",
    "functions",
    "custom",
    "functions",
    "build",
    "computer",
    "vision",
    "model",
    "aka",
    "food",
    "vision",
    "mini",
    "classify",
    "pizza",
    "steak",
    "sushi",
    "images",
    "classification",
    "model",
    "compare",
    "models",
    "without",
    "data",
    "augmentation",
    "covered",
    "yet",
    "later",
    "finally",
    "see",
    "said",
    "make",
    "predictions",
    "custom",
    "data",
    "means",
    "data",
    "within",
    "training",
    "test",
    "data",
    "set",
    "going",
    "well",
    "could",
    "cooks",
    "chemists",
    "like",
    "treat",
    "machine",
    "learning",
    "little",
    "bit",
    "art",
    "going",
    "cooking",
    "lots",
    "code",
    "said",
    "see",
    "google",
    "colab",
    "let",
    "code",
    "welcome",
    "back",
    "pytorch",
    "cooking",
    "show",
    "let",
    "learn",
    "cook",
    "custom",
    "data",
    "sets",
    "going",
    "jump",
    "google",
    "colab",
    "going",
    "click",
    "new",
    "notebook",
    "going",
    "make",
    "sure",
    "zoomed",
    "enough",
    "video",
    "wonderful",
    "going",
    "rename",
    "notebook",
    "04",
    "section",
    "going",
    "call",
    "pytorch",
    "custom",
    "data",
    "sets",
    "underscore",
    "video",
    "going",
    "one",
    "video",
    "notebooks",
    "code",
    "write",
    "videos",
    "course",
    "contained",
    "within",
    "video",
    "notebooks",
    "folder",
    "pytorch",
    "deep",
    "learning",
    "repo",
    "like",
    "resource",
    "ground",
    "truth",
    "notebook",
    "going",
    "put",
    "heading",
    "04",
    "pytorch",
    "custom",
    "data",
    "sets",
    "video",
    "notebook",
    "make",
    "bigger",
    "put",
    "resources",
    "book",
    "version",
    "course",
    "materials",
    "go",
    "go",
    "ground",
    "truth",
    "version",
    "notebook",
    "04",
    "reference",
    "notebook",
    "going",
    "use",
    "section",
    "come",
    "pytorch",
    "custom",
    "data",
    "sets",
    "put",
    "wonderful",
    "whole",
    "synopsis",
    "custom",
    "data",
    "sets",
    "section",
    "used",
    "data",
    "sets",
    "pytorch",
    "get",
    "data",
    "pytorch",
    "want",
    "start",
    "working",
    "right",
    "want",
    "start",
    "working",
    "problems",
    "want",
    "come",
    "sort",
    "data",
    "never",
    "worked",
    "want",
    "figure",
    "get",
    "pytorch",
    "one",
    "ways",
    "via",
    "custom",
    "data",
    "sets",
    "want",
    "put",
    "note",
    "going",
    "go",
    "zero",
    "section",
    "zero",
    "going",
    "importing",
    "pytorch",
    "setting",
    "device",
    "agnostic",
    "code",
    "want",
    "stress",
    "domain",
    "libraries",
    "reiterate",
    "went",
    "last",
    "video",
    "depending",
    "working",
    "whether",
    "vision",
    "text",
    "audio",
    "recommendation",
    "something",
    "like",
    "want",
    "look",
    "pytorch",
    "domain",
    "libraries",
    "existing",
    "data",
    "loader",
    "data",
    "loading",
    "functions",
    "customizable",
    "data",
    "loading",
    "functions",
    "keep",
    "mind",
    "seen",
    "go",
    "torch",
    "vision",
    "going",
    "looking",
    "torch",
    "vision",
    "got",
    "data",
    "sets",
    "got",
    "documentation",
    "got",
    "data",
    "sets",
    "domain",
    "libraries",
    "well",
    "working",
    "text",
    "problem",
    "going",
    "similar",
    "set",
    "steps",
    "going",
    "vision",
    "problem",
    "build",
    "food",
    "vision",
    "mini",
    "data",
    "set",
    "exists",
    "somewhere",
    "want",
    "bring",
    "pytorch",
    "build",
    "model",
    "let",
    "import",
    "libraries",
    "need",
    "going",
    "import",
    "torch",
    "probably",
    "import",
    "import",
    "pytorch",
    "going",
    "check",
    "torch",
    "version",
    "note",
    "need",
    "pytorch",
    "plus",
    "required",
    "course",
    "using",
    "google",
    "colab",
    "later",
    "date",
    "may",
    "later",
    "version",
    "pytorch",
    "going",
    "show",
    "version",
    "using",
    "going",
    "let",
    "load",
    "going",
    "get",
    "ready",
    "going",
    "also",
    "set",
    "device",
    "agnostic",
    "code",
    "right",
    "start",
    "time",
    "best",
    "practice",
    "pytorch",
    "way",
    "cuda",
    "device",
    "available",
    "model",
    "going",
    "use",
    "cuda",
    "device",
    "data",
    "going",
    "cuda",
    "device",
    "go",
    "wonderful",
    "got",
    "pytorch",
    "plus",
    "cuda",
    "maybe",
    "let",
    "check",
    "available",
    "using",
    "google",
    "colab",
    "set",
    "gpu",
    "yet",
    "probably",
    "wo",
    "available",
    "yet",
    "let",
    "look",
    "wonderful",
    "started",
    "new",
    "colab",
    "instance",
    "going",
    "use",
    "cpu",
    "default",
    "change",
    "come",
    "runtime",
    "change",
    "runtime",
    "type",
    "going",
    "go",
    "hard",
    "accelerator",
    "gpu",
    "done",
    "times",
    "paying",
    "google",
    "colab",
    "pro",
    "one",
    "benefits",
    "google",
    "colab",
    "reserves",
    "faster",
    "gpus",
    "need",
    "google",
    "colab",
    "pro",
    "said",
    "complete",
    "course",
    "use",
    "free",
    "version",
    "recall",
    "google",
    "colab",
    "pro",
    "tends",
    "give",
    "better",
    "gpu",
    "gpus",
    "free",
    "wonderful",
    "got",
    "access",
    "gpu",
    "cuda",
    "gpu",
    "nvidia",
    "smi",
    "tesla",
    "p100",
    "16",
    "gigabytes",
    "memory",
    "enough",
    "problem",
    "going",
    "work",
    "video",
    "believe",
    "enough",
    "cover",
    "first",
    "coding",
    "video",
    "let",
    "next",
    "section",
    "working",
    "custom",
    "datasets",
    "let",
    "next",
    "video",
    "let",
    "get",
    "data",
    "hey",
    "said",
    "last",
    "video",
    "ca",
    "cover",
    "custom",
    "datasets",
    "without",
    "data",
    "let",
    "get",
    "data",
    "remind",
    "going",
    "build",
    "food",
    "vision",
    "mini",
    "need",
    "way",
    "getting",
    "food",
    "images",
    "go",
    "back",
    "google",
    "chrome",
    "torch",
    "vision",
    "datasets",
    "plenty",
    "datasets",
    "one",
    "food",
    "101",
    "dataset",
    "food",
    "go",
    "going",
    "take",
    "us",
    "original",
    "food",
    "101",
    "website",
    "food",
    "101",
    "101",
    "different",
    "classes",
    "food",
    "challenging",
    "dataset",
    "101",
    "different",
    "food",
    "categories",
    "images",
    "quite",
    "beefy",
    "dataset",
    "class",
    "250",
    "manually",
    "reviewed",
    "test",
    "images",
    "provided",
    "per",
    "class",
    "101",
    "classes",
    "250",
    "testing",
    "images",
    "750",
    "training",
    "images",
    "could",
    "start",
    "working",
    "entire",
    "dataset",
    "straight",
    "get",
    "go",
    "practice",
    "created",
    "smaller",
    "subset",
    "dataset",
    "encourage",
    "problems",
    "start",
    "small",
    "upgrade",
    "necessary",
    "reduced",
    "number",
    "categories",
    "three",
    "number",
    "images",
    "10",
    "could",
    "reduce",
    "arbitrary",
    "amount",
    "decided",
    "three",
    "enough",
    "begin",
    "10",
    "data",
    "works",
    "hey",
    "could",
    "upscale",
    "accord",
    "want",
    "show",
    "notebook",
    "use",
    "create",
    "dataset",
    "extra",
    "curriculum",
    "could",
    "go",
    "notebook",
    "go",
    "extras",
    "04",
    "custom",
    "data",
    "creation",
    "created",
    "subset",
    "data",
    "making",
    "dataset",
    "use",
    "notebook",
    "number",
    "four",
    "created",
    "custom",
    "image",
    "data",
    "set",
    "image",
    "classification",
    "style",
    "top",
    "level",
    "folder",
    "pizza",
    "steak",
    "sushi",
    "training",
    "directory",
    "pizza",
    "steak",
    "sushi",
    "images",
    "test",
    "directory",
    "pizza",
    "steak",
    "sushi",
    "images",
    "well",
    "go",
    "check",
    "made",
    "oh",
    "also",
    "go",
    "loan",
    "section",
    "four",
    "information",
    "food",
    "101",
    "get",
    "data",
    "go",
    "information",
    "food",
    "resources",
    "original",
    "food",
    "101",
    "data",
    "set",
    "torch",
    "vision",
    "data",
    "sets",
    "food",
    "101",
    "created",
    "data",
    "set",
    "actually",
    "downloading",
    "data",
    "going",
    "write",
    "code",
    "data",
    "set",
    "smaller",
    "version",
    "created",
    "pytorch",
    "deep",
    "learning",
    "repo",
    "data",
    "pizza",
    "steak",
    "oh",
    "one",
    "little",
    "spoiler",
    "one",
    "exercises",
    "section",
    "see",
    "later",
    "let",
    "go",
    "let",
    "write",
    "code",
    "get",
    "data",
    "set",
    "github",
    "pizza",
    "steak",
    "explore",
    "become",
    "one",
    "data",
    "want",
    "write",
    "data",
    "set",
    "subset",
    "food",
    "101",
    "data",
    "set",
    "food",
    "101",
    "starts",
    "101",
    "different",
    "classes",
    "food",
    "could",
    "definitely",
    "build",
    "computer",
    "vision",
    "models",
    "101",
    "classes",
    "going",
    "start",
    "smaller",
    "data",
    "set",
    "starts",
    "three",
    "classes",
    "food",
    "10",
    "images",
    "right",
    "1000",
    "images",
    "per",
    "class",
    "750",
    "training",
    "250",
    "testing",
    "75",
    "training",
    "images",
    "per",
    "class",
    "25",
    "testing",
    "images",
    "per",
    "class",
    "starting",
    "ml",
    "projects",
    "important",
    "try",
    "things",
    "small",
    "scale",
    "increase",
    "scale",
    "necessary",
    "whole",
    "point",
    "speed",
    "fast",
    "experiment",
    "point",
    "trying",
    "experiment",
    "things",
    "try",
    "train",
    "images",
    "begin",
    "models",
    "might",
    "train",
    "take",
    "half",
    "hour",
    "train",
    "time",
    "beginning",
    "want",
    "increase",
    "rate",
    "experiment",
    "let",
    "get",
    "data",
    "going",
    "import",
    "requests",
    "request",
    "something",
    "github",
    "download",
    "url",
    "also",
    "going",
    "import",
    "zip",
    "file",
    "python",
    "data",
    "form",
    "zip",
    "file",
    "right",
    "going",
    "get",
    "path",
    "lib",
    "like",
    "use",
    "paths",
    "whenever",
    "dealing",
    "file",
    "paths",
    "directory",
    "paths",
    "let",
    "set",
    "path",
    "data",
    "folder",
    "course",
    "depend",
    "data",
    "set",
    "lives",
    "like",
    "typically",
    "like",
    "create",
    "folder",
    "called",
    "data",
    "going",
    "store",
    "data",
    "whatever",
    "project",
    "working",
    "data",
    "path",
    "equals",
    "path",
    "data",
    "going",
    "go",
    "image",
    "path",
    "equals",
    "data",
    "path",
    "slash",
    "pizza",
    "steak",
    "sushi",
    "going",
    "images",
    "three",
    "classes",
    "pizza",
    "steak",
    "sushi",
    "three",
    "classes",
    "101",
    "food",
    "image",
    "folder",
    "exist",
    "data",
    "folder",
    "already",
    "exists",
    "want",
    "redownload",
    "exist",
    "want",
    "download",
    "unzip",
    "image",
    "path",
    "der",
    "want",
    "print",
    "image",
    "path",
    "directory",
    "already",
    "exists",
    "skipping",
    "download",
    "exist",
    "want",
    "print",
    "image",
    "path",
    "exist",
    "creating",
    "one",
    "beautiful",
    "going",
    "go",
    "image",
    "path",
    "dot",
    "mk",
    "der",
    "make",
    "directory",
    "want",
    "make",
    "parents",
    "need",
    "parent",
    "directories",
    "want",
    "pass",
    "exist",
    "okay",
    "equals",
    "true",
    "get",
    "errors",
    "already",
    "exists",
    "write",
    "code",
    "want",
    "show",
    "run",
    "target",
    "directory",
    "data",
    "slash",
    "pizza",
    "steak",
    "sushi",
    "exist",
    "creating",
    "one",
    "data",
    "inside",
    "pizza",
    "steak",
    "sushi",
    "wonderful",
    "going",
    "fill",
    "images",
    "data",
    "work",
    "whole",
    "premise",
    "entire",
    "section",
    "loading",
    "data",
    "images",
    "pytorch",
    "build",
    "computer",
    "vision",
    "model",
    "want",
    "stress",
    "step",
    "similar",
    "matter",
    "data",
    "working",
    "folder",
    "maybe",
    "live",
    "cloud",
    "somewhere",
    "knows",
    "wherever",
    "data",
    "want",
    "write",
    "code",
    "load",
    "pytorch",
    "let",
    "download",
    "pizza",
    "steak",
    "sushi",
    "data",
    "going",
    "use",
    "width",
    "x",
    "screen",
    "space",
    "open",
    "going",
    "open",
    "data",
    "path",
    "slash",
    "file",
    "name",
    "trying",
    "open",
    "pizza",
    "steak",
    "sushi",
    "dot",
    "zip",
    "going",
    "write",
    "binary",
    "essentially",
    "saying",
    "advance",
    "know",
    "going",
    "download",
    "folder",
    "know",
    "file",
    "name",
    "pizza",
    "steak",
    "sushi",
    "dot",
    "zip",
    "going",
    "download",
    "google",
    "collab",
    "want",
    "open",
    "request",
    "equals",
    "request",
    "dot",
    "get",
    "want",
    "get",
    "file",
    "click",
    "click",
    "download",
    "going",
    "think",
    "going",
    "well",
    "let",
    "see",
    "wanted",
    "download",
    "locally",
    "could",
    "could",
    "come",
    "could",
    "click",
    "upload",
    "wanted",
    "upload",
    "session",
    "storage",
    "could",
    "upload",
    "prefer",
    "write",
    "code",
    "could",
    "run",
    "cell",
    "file",
    "instead",
    "download",
    "local",
    "computer",
    "goes",
    "straight",
    "google",
    "collab",
    "need",
    "url",
    "going",
    "put",
    "needs",
    "string",
    "excuse",
    "getting",
    "trigger",
    "happy",
    "shift",
    "enter",
    "wonderful",
    "got",
    "request",
    "get",
    "content",
    "github",
    "ca",
    "really",
    "show",
    "zip",
    "file",
    "images",
    "spoiler",
    "alert",
    "let",
    "keep",
    "going",
    "going",
    "print",
    "downloading",
    "pizza",
    "stake",
    "sushi",
    "data",
    "dot",
    "dot",
    "dot",
    "going",
    "write",
    "file",
    "request",
    "dot",
    "content",
    "content",
    "request",
    "made",
    "github",
    "request",
    "using",
    "python",
    "request",
    "library",
    "get",
    "information",
    "github",
    "url",
    "could",
    "wherever",
    "file",
    "stored",
    "going",
    "write",
    "content",
    "request",
    "target",
    "file",
    "copy",
    "going",
    "write",
    "data",
    "data",
    "path",
    "slash",
    "pizza",
    "stake",
    "sushi",
    "zip",
    "zip",
    "file",
    "want",
    "unzip",
    "unzip",
    "pizza",
    "stake",
    "sushi",
    "data",
    "let",
    "go",
    "zip",
    "file",
    "imported",
    "zip",
    "file",
    "python",
    "library",
    "help",
    "us",
    "deal",
    "zip",
    "files",
    "going",
    "use",
    "zip",
    "file",
    "dot",
    "zip",
    "file",
    "going",
    "pass",
    "data",
    "path",
    "path",
    "data",
    "path",
    "slash",
    "pizza",
    "stake",
    "sushi",
    "dot",
    "zip",
    "time",
    "instead",
    "giving",
    "right",
    "permissions",
    "wb",
    "stands",
    "stands",
    "right",
    "binary",
    "going",
    "give",
    "read",
    "permissions",
    "want",
    "read",
    "target",
    "file",
    "instead",
    "writing",
    "going",
    "go",
    "zip",
    "ref",
    "call",
    "anything",
    "really",
    "zip",
    "ref",
    "kind",
    "see",
    "lot",
    "different",
    "python",
    "examples",
    "going",
    "print",
    "unzipping",
    "pizza",
    "stake",
    "sushi",
    "data",
    "going",
    "go",
    "zip",
    "underscore",
    "ref",
    "dot",
    "extract",
    "going",
    "go",
    "image",
    "path",
    "means",
    "taking",
    "zip",
    "ref",
    "extracting",
    "information",
    "within",
    "zip",
    "ref",
    "within",
    "zip",
    "file",
    "image",
    "path",
    "created",
    "look",
    "image",
    "path",
    "let",
    "see",
    "image",
    "path",
    "wonderful",
    "contents",
    "zip",
    "file",
    "going",
    "go",
    "file",
    "let",
    "see",
    "action",
    "ready",
    "hopefully",
    "works",
    "three",
    "two",
    "one",
    "run",
    "file",
    "zip",
    "file",
    "oh",
    "get",
    "wrong",
    "type",
    "wrong",
    "got",
    "zip",
    "data",
    "path",
    "oh",
    "got",
    "zip",
    "file",
    "pizza",
    "stake",
    "sushi",
    "zip",
    "read",
    "data",
    "path",
    "okay",
    "found",
    "error",
    "another",
    "thing",
    "keep",
    "mind",
    "believe",
    "covered",
    "like",
    "keep",
    "errors",
    "videos",
    "see",
    "get",
    "things",
    "wrong",
    "never",
    "write",
    "code",
    "right",
    "first",
    "time",
    "link",
    "github",
    "make",
    "sure",
    "raw",
    "link",
    "address",
    "come",
    "copy",
    "link",
    "address",
    "download",
    "button",
    "notice",
    "slight",
    "difference",
    "come",
    "back",
    "going",
    "copy",
    "step",
    "github",
    "burke",
    "pytorch",
    "deep",
    "learning",
    "raw",
    "instead",
    "blob",
    "error",
    "code",
    "correct",
    "downloading",
    "wrong",
    "data",
    "let",
    "change",
    "raw",
    "keep",
    "mind",
    "must",
    "raw",
    "let",
    "see",
    "works",
    "correct",
    "data",
    "oh",
    "might",
    "delete",
    "oh",
    "go",
    "test",
    "beautiful",
    "train",
    "pizza",
    "steak",
    "sushi",
    "wonderful",
    "looks",
    "like",
    "got",
    "data",
    "open",
    "various",
    "jpegs",
    "okay",
    "testing",
    "data",
    "click",
    "got",
    "image",
    "pizza",
    "beautiful",
    "going",
    "explore",
    "little",
    "bit",
    "next",
    "video",
    "code",
    "written",
    "download",
    "data",
    "sets",
    "download",
    "custom",
    "data",
    "set",
    "recall",
    "working",
    "specifically",
    "pizza",
    "steak",
    "sushi",
    "problem",
    "computer",
    "vision",
    "however",
    "whole",
    "premise",
    "custom",
    "data",
    "want",
    "convert",
    "get",
    "tenses",
    "want",
    "process",
    "problems",
    "loading",
    "target",
    "data",
    "set",
    "writing",
    "code",
    "convert",
    "whatever",
    "format",
    "data",
    "set",
    "tenses",
    "pytorch",
    "see",
    "next",
    "video",
    "let",
    "explore",
    "data",
    "downloaded",
    "welcome",
    "back",
    "last",
    "video",
    "wrote",
    "code",
    "download",
    "target",
    "data",
    "set",
    "custom",
    "data",
    "set",
    "pytorch",
    "deep",
    "learning",
    "data",
    "directory",
    "like",
    "see",
    "data",
    "set",
    "made",
    "go",
    "pytorch",
    "deep",
    "learning",
    "slash",
    "extras",
    "going",
    "custom",
    "data",
    "creation",
    "notebook",
    "got",
    "code",
    "done",
    "take",
    "data",
    "food",
    "101",
    "data",
    "set",
    "download",
    "website",
    "torch",
    "vision",
    "go",
    "torch",
    "vision",
    "food",
    "got",
    "data",
    "set",
    "built",
    "pytorch",
    "used",
    "data",
    "set",
    "pytorch",
    "broken",
    "101",
    "classes",
    "three",
    "classes",
    "start",
    "small",
    "experiment",
    "go",
    "get",
    "training",
    "data",
    "data",
    "sets",
    "food",
    "101",
    "customized",
    "style",
    "go",
    "back",
    "colab",
    "got",
    "pizza",
    "steak",
    "sushi",
    "test",
    "folder",
    "testing",
    "images",
    "train",
    "folder",
    "training",
    "images",
    "data",
    "standard",
    "image",
    "classification",
    "format",
    "cover",
    "second",
    "going",
    "video",
    "kick",
    "section",
    "number",
    "two",
    "becoming",
    "one",
    "data",
    "one",
    "favorite",
    "ways",
    "refer",
    "data",
    "preparation",
    "data",
    "exploration",
    "coming",
    "one",
    "data",
    "like",
    "show",
    "one",
    "favorite",
    "quotes",
    "abraham",
    "loss",
    "function",
    "eight",
    "hours",
    "build",
    "machine",
    "learning",
    "model",
    "spend",
    "first",
    "six",
    "hours",
    "preparing",
    "data",
    "set",
    "going",
    "abraham",
    "loss",
    "function",
    "sounds",
    "like",
    "knows",
    "going",
    "since",
    "downloaded",
    "data",
    "let",
    "explore",
    "hey",
    "write",
    "code",
    "walk",
    "directories",
    "explore",
    "data",
    "depend",
    "data",
    "got",
    "got",
    "fair",
    "different",
    "directories",
    "fair",
    "different",
    "folders",
    "within",
    "walk",
    "directories",
    "see",
    "going",
    "visual",
    "data",
    "probably",
    "want",
    "visualize",
    "image",
    "going",
    "second",
    "two",
    "write",
    "little",
    "doc",
    "string",
    "helper",
    "function",
    "walks",
    "path",
    "returning",
    "contents",
    "case",
    "know",
    "abraham",
    "loss",
    "function",
    "exist",
    "far",
    "know",
    "make",
    "quote",
    "going",
    "use",
    "os",
    "dot",
    "walk",
    "function",
    "os",
    "dot",
    "walk",
    "going",
    "pass",
    "dirt",
    "path",
    "walk",
    "get",
    "doc",
    "string",
    "directory",
    "tree",
    "generator",
    "directory",
    "directory",
    "tree",
    "rooted",
    "top",
    "including",
    "top",
    "excluding",
    "dot",
    "dot",
    "dot",
    "yields",
    "three",
    "tuple",
    "derpath",
    "der",
    "names",
    "file",
    "names",
    "step",
    "python",
    "documentation",
    "like",
    "essentially",
    "going",
    "go",
    "target",
    "directory",
    "case",
    "one",
    "walk",
    "directories",
    "printing",
    "information",
    "one",
    "let",
    "see",
    "action",
    "one",
    "favorite",
    "things",
    "working",
    "standard",
    "image",
    "classification",
    "format",
    "data",
    "lane",
    "length",
    "der",
    "names",
    "directories",
    "let",
    "go",
    "land",
    "land",
    "file",
    "names",
    "say",
    "length",
    "like",
    "got",
    "g",
    "end",
    "land",
    "images",
    "let",
    "put",
    "derpath",
    "little",
    "bit",
    "confusing",
    "never",
    "used",
    "walk",
    "exciting",
    "see",
    "information",
    "directories",
    "oh",
    "read",
    "run",
    "let",
    "check",
    "function",
    "walk",
    "der",
    "going",
    "pass",
    "image",
    "path",
    "well",
    "going",
    "show",
    "us",
    "beautiful",
    "let",
    "compare",
    "got",
    "printout",
    "two",
    "directories",
    "zero",
    "images",
    "data",
    "pizza",
    "steak",
    "sushi",
    "one",
    "zero",
    "images",
    "two",
    "directories",
    "test",
    "train",
    "wonderful",
    "three",
    "directories",
    "data",
    "pizza",
    "steak",
    "sushi",
    "test",
    "yes",
    "looks",
    "correct",
    "three",
    "directories",
    "pizza",
    "steak",
    "sushi",
    "zero",
    "directories",
    "19",
    "images",
    "pizza",
    "steak",
    "sushi",
    "slash",
    "test",
    "steak",
    "look",
    "means",
    "19",
    "testing",
    "images",
    "steak",
    "let",
    "look",
    "one",
    "go",
    "food",
    "101",
    "data",
    "set",
    "original",
    "food",
    "101",
    "data",
    "set",
    "whole",
    "bunch",
    "images",
    "food",
    "steak",
    "wonderful",
    "trying",
    "build",
    "food",
    "vision",
    "model",
    "recognize",
    "image",
    "jump",
    "three",
    "directories",
    "training",
    "directory",
    "pizza",
    "steak",
    "sushi",
    "75",
    "steak",
    "images",
    "72",
    "sushi",
    "images",
    "78",
    "pizza",
    "slightly",
    "different",
    "much",
    "numbers",
    "far",
    "got",
    "75",
    "training",
    "images",
    "got",
    "25",
    "testing",
    "images",
    "per",
    "class",
    "randomly",
    "selected",
    "food",
    "101",
    "data",
    "set",
    "10",
    "three",
    "different",
    "classes",
    "let",
    "keep",
    "pushing",
    "forward",
    "going",
    "set",
    "training",
    "test",
    "parts",
    "want",
    "show",
    "set",
    "show",
    "standard",
    "image",
    "classification",
    "setup",
    "image",
    "going",
    "go",
    "tester",
    "working",
    "image",
    "classification",
    "problem",
    "want",
    "set",
    "test",
    "print",
    "trainer",
    "tester",
    "going",
    "trying",
    "going",
    "write",
    "code",
    "go",
    "hey",
    "look",
    "path",
    "training",
    "images",
    "look",
    "path",
    "testing",
    "images",
    "standard",
    "image",
    "classification",
    "data",
    "format",
    "overall",
    "data",
    "set",
    "folder",
    "training",
    "folder",
    "dedicated",
    "training",
    "images",
    "might",
    "testing",
    "folder",
    "dedicated",
    "testing",
    "images",
    "might",
    "could",
    "validation",
    "data",
    "set",
    "well",
    "wanted",
    "label",
    "one",
    "images",
    "class",
    "name",
    "folder",
    "name",
    "pizza",
    "images",
    "live",
    "pizza",
    "directory",
    "steak",
    "sushi",
    "depending",
    "problem",
    "data",
    "format",
    "depend",
    "whatever",
    "working",
    "might",
    "folders",
    "different",
    "text",
    "files",
    "folders",
    "different",
    "audio",
    "files",
    "premise",
    "remains",
    "going",
    "writing",
    "code",
    "get",
    "data",
    "tenses",
    "use",
    "pytorch",
    "come",
    "image",
    "data",
    "classification",
    "format",
    "well",
    "go",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "documentation",
    "start",
    "work",
    "data",
    "sets",
    "start",
    "realize",
    "standardized",
    "ways",
    "storing",
    "specific",
    "types",
    "data",
    "come",
    "base",
    "classes",
    "custom",
    "data",
    "sets",
    "working",
    "towards",
    "using",
    "image",
    "folder",
    "data",
    "set",
    "generic",
    "data",
    "loader",
    "images",
    "arranged",
    "way",
    "default",
    "specifically",
    "formatted",
    "data",
    "mimic",
    "style",
    "pre",
    "built",
    "data",
    "loading",
    "function",
    "got",
    "root",
    "directory",
    "case",
    "classifying",
    "dog",
    "cat",
    "images",
    "root",
    "dog",
    "folder",
    "various",
    "images",
    "thing",
    "cat",
    "would",
    "dog",
    "versus",
    "cat",
    "difference",
    "us",
    "food",
    "images",
    "pizza",
    "steak",
    "sushi",
    "wanted",
    "use",
    "entire",
    "food",
    "101",
    "data",
    "set",
    "would",
    "101",
    "different",
    "folders",
    "images",
    "totally",
    "possible",
    "begin",
    "keeping",
    "things",
    "small",
    "let",
    "keep",
    "pushing",
    "forward",
    "said",
    "dealing",
    "computer",
    "vision",
    "problem",
    "another",
    "way",
    "explore",
    "data",
    "walking",
    "directories",
    "let",
    "visualize",
    "image",
    "hey",
    "done",
    "clicking",
    "file",
    "write",
    "code",
    "replicate",
    "code",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "started",
    "become",
    "one",
    "data",
    "learned",
    "75",
    "images",
    "per",
    "training",
    "class",
    "25",
    "images",
    "per",
    "testing",
    "class",
    "also",
    "learned",
    "standard",
    "image",
    "classification",
    "data",
    "structure",
    "steak",
    "images",
    "within",
    "steak",
    "folder",
    "training",
    "data",
    "set",
    "test",
    "pizza",
    "images",
    "within",
    "pizza",
    "folder",
    "different",
    "image",
    "classification",
    "class",
    "might",
    "want",
    "create",
    "data",
    "set",
    "might",
    "format",
    "way",
    "training",
    "images",
    "living",
    "directory",
    "classification",
    "name",
    "wanted",
    "classify",
    "photos",
    "dogs",
    "cats",
    "might",
    "create",
    "training",
    "folder",
    "train",
    "slash",
    "dog",
    "train",
    "slash",
    "cat",
    "put",
    "images",
    "dogs",
    "dog",
    "folder",
    "images",
    "cats",
    "cat",
    "folder",
    "testing",
    "data",
    "set",
    "premise",
    "remains",
    "going",
    "sound",
    "like",
    "broken",
    "record",
    "want",
    "get",
    "data",
    "files",
    "whatever",
    "files",
    "may",
    "whatever",
    "data",
    "structure",
    "might",
    "tenses",
    "let",
    "keep",
    "becoming",
    "one",
    "data",
    "going",
    "visualize",
    "image",
    "visualizing",
    "image",
    "know",
    "much",
    "love",
    "randomness",
    "let",
    "select",
    "random",
    "image",
    "files",
    "let",
    "plot",
    "hey",
    "could",
    "click",
    "visualize",
    "like",
    "things",
    "code",
    "specifically",
    "let",
    "let",
    "plan",
    "let",
    "write",
    "code",
    "number",
    "one",
    "get",
    "image",
    "paths",
    "see",
    "path",
    "path",
    "lib",
    "library",
    "want",
    "pick",
    "random",
    "image",
    "path",
    "using",
    "use",
    "python",
    "random",
    "python",
    "random",
    "dot",
    "choice",
    "pick",
    "single",
    "image",
    "random",
    "dot",
    "choice",
    "want",
    "get",
    "image",
    "class",
    "name",
    "part",
    "lib",
    "comes",
    "handy",
    "class",
    "name",
    "recall",
    "whichever",
    "target",
    "image",
    "pick",
    "class",
    "name",
    "whichever",
    "directory",
    "case",
    "picked",
    "random",
    "image",
    "directory",
    "class",
    "name",
    "would",
    "pizza",
    "using",
    "think",
    "going",
    "path",
    "lib",
    "dot",
    "path",
    "get",
    "parent",
    "folder",
    "wherever",
    "image",
    "lives",
    "parent",
    "image",
    "parent",
    "folder",
    "parent",
    "directory",
    "target",
    "random",
    "image",
    "going",
    "get",
    "stem",
    "stem",
    "stem",
    "last",
    "little",
    "bit",
    "number",
    "four",
    "well",
    "want",
    "open",
    "image",
    "since",
    "working",
    "images",
    "let",
    "open",
    "image",
    "python",
    "pill",
    "python",
    "image",
    "library",
    "actually",
    "pillow",
    "go",
    "python",
    "pillow",
    "little",
    "bit",
    "confusing",
    "started",
    "learn",
    "python",
    "image",
    "manipulation",
    "pillow",
    "friendly",
    "pill",
    "still",
    "called",
    "pill",
    "think",
    "pillow",
    "way",
    "process",
    "images",
    "python",
    "pill",
    "python",
    "imaging",
    "library",
    "frederick",
    "lund",
    "alex",
    "clark",
    "contributors",
    "created",
    "pillow",
    "thank",
    "everyone",
    "let",
    "go",
    "number",
    "five",
    "want",
    "well",
    "want",
    "yeah",
    "let",
    "get",
    "metadata",
    "image",
    "show",
    "image",
    "print",
    "metadata",
    "wonderful",
    "let",
    "import",
    "random",
    "machine",
    "learning",
    "harnessing",
    "power",
    "randomness",
    "like",
    "use",
    "randomness",
    "explore",
    "data",
    "well",
    "model",
    "let",
    "set",
    "seed",
    "get",
    "image",
    "ends",
    "random",
    "dot",
    "seed",
    "going",
    "use",
    "use",
    "whatever",
    "like",
    "like",
    "get",
    "image",
    "suggest",
    "using",
    "42",
    "well",
    "let",
    "get",
    "image",
    "paths",
    "image",
    "path",
    "list",
    "want",
    "get",
    "image",
    "path",
    "recall",
    "image",
    "path",
    "folder",
    "going",
    "close",
    "image",
    "path",
    "folder",
    "also",
    "go",
    "copy",
    "path",
    "wanted",
    "going",
    "get",
    "something",
    "similar",
    "going",
    "error",
    "comment",
    "error",
    "path",
    "going",
    "keep",
    "posix",
    "path",
    "format",
    "go",
    "list",
    "let",
    "create",
    "list",
    "image",
    "path",
    "dot",
    "glob",
    "stands",
    "grab",
    "actually",
    "know",
    "glob",
    "stands",
    "like",
    "glob",
    "together",
    "images",
    "files",
    "suit",
    "certain",
    "pattern",
    "glob",
    "together",
    "means",
    "stick",
    "together",
    "might",
    "able",
    "correct",
    "got",
    "wrong",
    "meaning",
    "appreciate",
    "going",
    "pass",
    "certain",
    "combination",
    "want",
    "star",
    "slash",
    "star",
    "want",
    "star",
    "dot",
    "jpg",
    "well",
    "want",
    "every",
    "image",
    "path",
    "star",
    "going",
    "first",
    "directory",
    "combination",
    "train",
    "test",
    "star",
    "means",
    "anything",
    "inside",
    "tests",
    "let",
    "say",
    "first",
    "star",
    "equal",
    "test",
    "second",
    "star",
    "equal",
    "anything",
    "could",
    "pizza",
    "steak",
    "sushi",
    "finally",
    "star",
    "let",
    "say",
    "test",
    "pizza",
    "star",
    "anything",
    "dot",
    "jpg",
    "could",
    "one",
    "files",
    "make",
    "sense",
    "print",
    "image",
    "path",
    "list",
    "let",
    "look",
    "go",
    "got",
    "list",
    "every",
    "single",
    "image",
    "within",
    "pizza",
    "steak",
    "sushi",
    "another",
    "way",
    "like",
    "visualize",
    "data",
    "get",
    "paths",
    "randomly",
    "visualize",
    "whether",
    "image",
    "text",
    "audio",
    "might",
    "want",
    "randomly",
    "listen",
    "recall",
    "domain",
    "libraries",
    "different",
    "input",
    "output",
    "methods",
    "different",
    "data",
    "sets",
    "come",
    "torch",
    "vision",
    "utils",
    "different",
    "ways",
    "draw",
    "images",
    "reading",
    "writing",
    "images",
    "videos",
    "could",
    "load",
    "image",
    "via",
    "read",
    "image",
    "could",
    "decode",
    "could",
    "whole",
    "bunch",
    "things",
    "let",
    "explore",
    "extra",
    "curriculum",
    "let",
    "select",
    "random",
    "image",
    "plot",
    "go",
    "number",
    "two",
    "step",
    "pick",
    "random",
    "image",
    "pick",
    "random",
    "image",
    "path",
    "let",
    "get",
    "rid",
    "go",
    "random",
    "image",
    "path",
    "equals",
    "random",
    "dot",
    "choice",
    "harness",
    "power",
    "randomness",
    "explore",
    "data",
    "let",
    "get",
    "random",
    "image",
    "image",
    "path",
    "list",
    "print",
    "random",
    "image",
    "path",
    "one",
    "lucky",
    "image",
    "selected",
    "beautiful",
    "test",
    "pizza",
    "image",
    "lucky",
    "random",
    "image",
    "got",
    "random",
    "seed",
    "going",
    "one",
    "time",
    "yes",
    "comment",
    "random",
    "seed",
    "get",
    "different",
    "one",
    "time",
    "got",
    "stake",
    "image",
    "got",
    "another",
    "stake",
    "image",
    "another",
    "stake",
    "image",
    "oh",
    "three",
    "row",
    "four",
    "row",
    "oh",
    "pizza",
    "okay",
    "let",
    "keep",
    "going",
    "get",
    "image",
    "class",
    "path",
    "name",
    "image",
    "class",
    "name",
    "directory",
    "image",
    "data",
    "standard",
    "image",
    "classification",
    "format",
    "image",
    "stored",
    "let",
    "image",
    "class",
    "equals",
    "random",
    "image",
    "path",
    "dot",
    "parent",
    "dot",
    "stem",
    "going",
    "print",
    "image",
    "class",
    "get",
    "got",
    "pizza",
    "wonderful",
    "parent",
    "folder",
    "stem",
    "end",
    "folder",
    "pizza",
    "beautiful",
    "well",
    "working",
    "images",
    "let",
    "open",
    "image",
    "open",
    "image",
    "using",
    "pill",
    "could",
    "also",
    "open",
    "image",
    "pytorch",
    "read",
    "image",
    "going",
    "use",
    "pill",
    "keep",
    "things",
    "little",
    "bit",
    "generic",
    "open",
    "image",
    "image",
    "equals",
    "image",
    "pill",
    "import",
    "image",
    "image",
    "class",
    "open",
    "function",
    "going",
    "pass",
    "random",
    "image",
    "path",
    "note",
    "corrupt",
    "images",
    "corrupt",
    "may",
    "error",
    "could",
    "potentially",
    "use",
    "clean",
    "data",
    "set",
    "imported",
    "lot",
    "images",
    "image",
    "dot",
    "open",
    "target",
    "data",
    "set",
    "believe",
    "corrupt",
    "please",
    "let",
    "know",
    "find",
    "later",
    "model",
    "tries",
    "train",
    "let",
    "print",
    "metadata",
    "open",
    "image",
    "get",
    "information",
    "let",
    "go",
    "random",
    "image",
    "path",
    "random",
    "image",
    "path",
    "already",
    "printing",
    "anyway",
    "going",
    "go",
    "image",
    "class",
    "equal",
    "image",
    "class",
    "wonderful",
    "print",
    "get",
    "metadata",
    "images",
    "image",
    "height",
    "going",
    "img",
    "dot",
    "height",
    "get",
    "metadata",
    "using",
    "pill",
    "library",
    "going",
    "print",
    "image",
    "width",
    "get",
    "img",
    "dot",
    "width",
    "print",
    "image",
    "wonderful",
    "get",
    "rid",
    "get",
    "rid",
    "let",
    "look",
    "random",
    "images",
    "data",
    "set",
    "lovely",
    "got",
    "image",
    "pizza",
    "warn",
    "downsides",
    "working",
    "food",
    "data",
    "make",
    "little",
    "bit",
    "hungry",
    "got",
    "sushi",
    "got",
    "sushi",
    "steak",
    "steak",
    "go",
    "one",
    "good",
    "luck",
    "finish",
    "sushi",
    "oh",
    "could",
    "little",
    "bit",
    "confusing",
    "thought",
    "might",
    "steak",
    "begin",
    "scene",
    "one",
    "important",
    "sort",
    "visualize",
    "images",
    "randomly",
    "never",
    "know",
    "going",
    "come",
    "across",
    "way",
    "visualize",
    "enough",
    "images",
    "could",
    "hundred",
    "times",
    "could",
    "20",
    "times",
    "feel",
    "comfortable",
    "go",
    "hey",
    "feel",
    "like",
    "know",
    "enough",
    "data",
    "let",
    "see",
    "well",
    "model",
    "goes",
    "sort",
    "data",
    "finish",
    "steak",
    "image",
    "set",
    "little",
    "challenge",
    "next",
    "video",
    "visualize",
    "image",
    "like",
    "done",
    "time",
    "matplotlib",
    "try",
    "visualize",
    "image",
    "matplotlib",
    "little",
    "challenge",
    "next",
    "video",
    "give",
    "go",
    "want",
    "random",
    "image",
    "well",
    "quite",
    "similar",
    "set",
    "instead",
    "printing",
    "things",
    "like",
    "want",
    "visualize",
    "using",
    "matplotlib",
    "try",
    "together",
    "next",
    "video",
    "oh",
    "well",
    "way",
    "creating",
    "pytorch",
    "custom",
    "data",
    "set",
    "started",
    "become",
    "one",
    "data",
    "let",
    "continue",
    "visualize",
    "another",
    "image",
    "set",
    "challenge",
    "last",
    "video",
    "try",
    "replicate",
    "done",
    "pill",
    "library",
    "matplotlib",
    "let",
    "give",
    "go",
    "hey",
    "use",
    "matplotlib",
    "well",
    "matplotlib",
    "going",
    "import",
    "numpy",
    "well",
    "going",
    "convert",
    "image",
    "array",
    "little",
    "trick",
    "quite",
    "elaborate",
    "hope",
    "tried",
    "decode",
    "figure",
    "errors",
    "received",
    "matplotlib",
    "one",
    "fundamental",
    "data",
    "science",
    "libraries",
    "going",
    "see",
    "everywhere",
    "important",
    "aware",
    "plot",
    "images",
    "data",
    "matplotlib",
    "turn",
    "image",
    "array",
    "go",
    "image",
    "array",
    "going",
    "use",
    "numpy",
    "method",
    "np",
    "array",
    "going",
    "pass",
    "image",
    "recall",
    "image",
    "image",
    "set",
    "already",
    "opened",
    "pill",
    "going",
    "plot",
    "image",
    "plot",
    "image",
    "matplotlib",
    "go",
    "fig",
    "size",
    "equals",
    "10",
    "seven",
    "going",
    "go",
    "show",
    "image",
    "array",
    "pass",
    "array",
    "numbers",
    "going",
    "set",
    "title",
    "f",
    "string",
    "going",
    "pass",
    "image",
    "class",
    "equals",
    "image",
    "class",
    "going",
    "pass",
    "image",
    "shape",
    "get",
    "shape",
    "another",
    "important",
    "thing",
    "aware",
    "different",
    "datasets",
    "exploring",
    "shape",
    "data",
    "one",
    "main",
    "errors",
    "machine",
    "learning",
    "deep",
    "learning",
    "shape",
    "mismatch",
    "issues",
    "know",
    "shape",
    "data",
    "start",
    "go",
    "okay",
    "kind",
    "understand",
    "shape",
    "need",
    "model",
    "layers",
    "shape",
    "need",
    "data",
    "going",
    "turn",
    "axes",
    "beautiful",
    "look",
    "got",
    "thrown",
    "without",
    "really",
    "explaining",
    "seen",
    "computer",
    "vision",
    "section",
    "image",
    "shape",
    "512",
    "dimensions",
    "height",
    "512",
    "pixels",
    "width",
    "306",
    "pixels",
    "three",
    "color",
    "channels",
    "format",
    "color",
    "channels",
    "last",
    "default",
    "pill",
    "library",
    "also",
    "default",
    "map",
    "plot",
    "lib",
    "pytorch",
    "recall",
    "default",
    "put",
    "color",
    "channels",
    "start",
    "color",
    "channels",
    "first",
    "lot",
    "debate",
    "said",
    "best",
    "order",
    "looks",
    "like",
    "leading",
    "towards",
    "going",
    "towards",
    "pytorch",
    "defaults",
    "color",
    "channels",
    "first",
    "okay",
    "manipulate",
    "dimensions",
    "need",
    "whatever",
    "code",
    "writing",
    "three",
    "color",
    "channels",
    "red",
    "green",
    "blue",
    "combine",
    "red",
    "green",
    "blue",
    "way",
    "shape",
    "form",
    "get",
    "different",
    "colors",
    "represent",
    "image",
    "look",
    "image",
    "ray",
    "image",
    "numerical",
    "format",
    "wonderful",
    "okay",
    "got",
    "one",
    "way",
    "one",
    "image",
    "think",
    "start",
    "moving",
    "towards",
    "scaling",
    "every",
    "image",
    "data",
    "folder",
    "let",
    "finish",
    "video",
    "visualizing",
    "one",
    "image",
    "get",
    "premise",
    "image",
    "array",
    "different",
    "numerical",
    "values",
    "got",
    "delicious",
    "looking",
    "pizza",
    "shave",
    "512",
    "512",
    "color",
    "channels",
    "last",
    "got",
    "thing",
    "one",
    "way",
    "become",
    "one",
    "data",
    "visualize",
    "different",
    "images",
    "especially",
    "random",
    "images",
    "could",
    "thing",
    "visualizing",
    "different",
    "text",
    "samples",
    "working",
    "listening",
    "different",
    "audio",
    "samples",
    "depends",
    "domain",
    "working",
    "next",
    "video",
    "let",
    "start",
    "working",
    "towards",
    "turning",
    "images",
    "visualize",
    "become",
    "one",
    "data",
    "seen",
    "shapes",
    "varying",
    "terms",
    "height",
    "width",
    "look",
    "like",
    "three",
    "color",
    "channels",
    "color",
    "images",
    "want",
    "write",
    "code",
    "turn",
    "images",
    "pytorch",
    "tenses",
    "let",
    "start",
    "moving",
    "towards",
    "see",
    "next",
    "video",
    "hello",
    "welcome",
    "back",
    "last",
    "video",
    "converted",
    "image",
    "numpy",
    "array",
    "saw",
    "image",
    "represented",
    "array",
    "like",
    "get",
    "image",
    "custom",
    "data",
    "set",
    "pizza",
    "steak",
    "sushi",
    "pytorch",
    "well",
    "let",
    "cover",
    "video",
    "going",
    "create",
    "new",
    "heading",
    "going",
    "transforming",
    "data",
    "like",
    "hinting",
    "fact",
    "whole",
    "time",
    "want",
    "get",
    "data",
    "tensor",
    "format",
    "data",
    "type",
    "pytorch",
    "accepts",
    "let",
    "write",
    "use",
    "image",
    "data",
    "pytorch",
    "goes",
    "images",
    "vision",
    "data",
    "goes",
    "text",
    "goes",
    "audio",
    "basically",
    "whatever",
    "kind",
    "data",
    "set",
    "working",
    "need",
    "way",
    "turn",
    "tenses",
    "step",
    "number",
    "one",
    "turn",
    "target",
    "data",
    "tenses",
    "case",
    "going",
    "numerical",
    "representation",
    "images",
    "number",
    "two",
    "turn",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "set",
    "recall",
    "previous",
    "video",
    "used",
    "data",
    "set",
    "house",
    "data",
    "tensor",
    "format",
    "subsequently",
    "turned",
    "data",
    "sets",
    "pytorch",
    "data",
    "sets",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "loader",
    "data",
    "loader",
    "creates",
    "iterable",
    "batched",
    "version",
    "data",
    "set",
    "short",
    "going",
    "call",
    "data",
    "set",
    "data",
    "loader",
    "discussed",
    "previously",
    "go",
    "pytorch",
    "documentation",
    "torch",
    "vision",
    "torch",
    "vision",
    "going",
    "quite",
    "similar",
    "torch",
    "audio",
    "torch",
    "text",
    "torch",
    "rec",
    "torch",
    "data",
    "eventually",
    "comes",
    "beta",
    "different",
    "ways",
    "create",
    "data",
    "sets",
    "go",
    "data",
    "sets",
    "module",
    "find",
    "data",
    "sets",
    "also",
    "base",
    "classes",
    "custom",
    "data",
    "sets",
    "go",
    "image",
    "folder",
    "another",
    "parameter",
    "like",
    "show",
    "going",
    "universal",
    "across",
    "many",
    "different",
    "data",
    "types",
    "transform",
    "parameter",
    "transform",
    "parameter",
    "parameter",
    "use",
    "pass",
    "transforms",
    "data",
    "load",
    "data",
    "sets",
    "image",
    "folder",
    "performs",
    "transform",
    "data",
    "samples",
    "sent",
    "target",
    "data",
    "folder",
    "lot",
    "easier",
    "understand",
    "illustration",
    "rather",
    "talking",
    "let",
    "create",
    "transform",
    "main",
    "transform",
    "going",
    "transforming",
    "data",
    "turning",
    "tenses",
    "let",
    "see",
    "looks",
    "like",
    "going",
    "going",
    "import",
    "main",
    "libraries",
    "going",
    "use",
    "torch",
    "utils",
    "dot",
    "data",
    "let",
    "import",
    "data",
    "loader",
    "going",
    "import",
    "torch",
    "vision",
    "going",
    "import",
    "data",
    "sets",
    "also",
    "going",
    "import",
    "transforms",
    "beautiful",
    "going",
    "create",
    "another",
    "little",
    "heading",
    "going",
    "transforming",
    "data",
    "torch",
    "vision",
    "dot",
    "transform",
    "main",
    "transform",
    "looking",
    "turning",
    "images",
    "jpegs",
    "go",
    "train",
    "go",
    "folder",
    "got",
    "jpeg",
    "images",
    "want",
    "turn",
    "tensor",
    "representation",
    "pizza",
    "get",
    "let",
    "see",
    "create",
    "transform",
    "write",
    "transform",
    "image",
    "let",
    "start",
    "calling",
    "data",
    "transform",
    "going",
    "show",
    "combine",
    "transforms",
    "together",
    "want",
    "combine",
    "transforms",
    "together",
    "use",
    "transforms",
    "dot",
    "compose",
    "also",
    "use",
    "n",
    "dot",
    "sequential",
    "combine",
    "transforms",
    "going",
    "stick",
    "transforms",
    "dot",
    "compose",
    "takes",
    "list",
    "let",
    "write",
    "three",
    "transforms",
    "begin",
    "talk",
    "want",
    "resize",
    "images",
    "might",
    "well",
    "recall",
    "last",
    "section",
    "computer",
    "vision",
    "use",
    "tiny",
    "vgg",
    "architecture",
    "size",
    "images",
    "tiny",
    "vgg",
    "architecture",
    "took",
    "well",
    "replicated",
    "cnn",
    "website",
    "version",
    "cnn",
    "explainer",
    "website",
    "version",
    "took",
    "images",
    "size",
    "perhaps",
    "want",
    "leverage",
    "computer",
    "vision",
    "model",
    "later",
    "going",
    "resize",
    "images",
    "going",
    "create",
    "another",
    "transform",
    "want",
    "highlight",
    "transforms",
    "help",
    "manipulate",
    "data",
    "certain",
    "way",
    "wanted",
    "flip",
    "images",
    "form",
    "data",
    "augmentation",
    "words",
    "artificially",
    "increasing",
    "diversity",
    "data",
    "set",
    "flip",
    "images",
    "randomly",
    "horizontal",
    "transforms",
    "dot",
    "random",
    "horizontal",
    "flip",
    "going",
    "put",
    "probability",
    "p",
    "equals",
    "means",
    "50",
    "time",
    "image",
    "goes",
    "transform",
    "pipeline",
    "get",
    "flipped",
    "horizontal",
    "axis",
    "said",
    "makes",
    "lot",
    "sense",
    "visualize",
    "going",
    "shortly",
    "finally",
    "going",
    "turn",
    "image",
    "torch",
    "tensor",
    "transforms",
    "dot",
    "tensor",
    "might",
    "find",
    "transforms",
    "transform",
    "says",
    "tensor",
    "look",
    "doc",
    "string",
    "got",
    "convert",
    "pill",
    "image",
    "working",
    "right",
    "numpy",
    "array",
    "tensor",
    "transform",
    "support",
    "torch",
    "script",
    "like",
    "find",
    "like",
    "read",
    "documentation",
    "essentially",
    "turning",
    "pytorch",
    "code",
    "python",
    "script",
    "converts",
    "pill",
    "image",
    "numpy",
    "array",
    "height",
    "color",
    "channels",
    "range",
    "0",
    "255",
    "values",
    "0",
    "255",
    "red",
    "green",
    "blue",
    "torch",
    "float",
    "tensor",
    "shape",
    "color",
    "channels",
    "height",
    "width",
    "range",
    "0",
    "take",
    "tensor",
    "values",
    "numpy",
    "array",
    "values",
    "0",
    "255",
    "convert",
    "torch",
    "tensor",
    "range",
    "0",
    "going",
    "see",
    "later",
    "action",
    "first",
    "transform",
    "pass",
    "data",
    "data",
    "fact",
    "encourage",
    "try",
    "see",
    "happens",
    "pass",
    "data",
    "transform",
    "happens",
    "pass",
    "image",
    "ray",
    "image",
    "ray",
    "let",
    "see",
    "happens",
    "hey",
    "oh",
    "image",
    "pill",
    "image",
    "got",
    "class",
    "numpy",
    "array",
    "pass",
    "straight",
    "image",
    "pill",
    "image",
    "go",
    "beautiful",
    "look",
    "shape",
    "get",
    "3",
    "64",
    "wanted",
    "change",
    "224",
    "another",
    "common",
    "value",
    "computer",
    "vision",
    "models",
    "24",
    "see",
    "powerful",
    "little",
    "transforms",
    "module",
    "torch",
    "vision",
    "library",
    "change",
    "back",
    "64",
    "look",
    "type",
    "transform",
    "tensor",
    "get",
    "torch",
    "float",
    "beautiful",
    "got",
    "way",
    "transform",
    "images",
    "tensors",
    "still",
    "one",
    "image",
    "progress",
    "towards",
    "every",
    "image",
    "data",
    "folder",
    "like",
    "visualize",
    "looks",
    "like",
    "next",
    "video",
    "let",
    "write",
    "code",
    "visualize",
    "looks",
    "like",
    "transform",
    "multiple",
    "images",
    "time",
    "think",
    "good",
    "idea",
    "compare",
    "transform",
    "original",
    "image",
    "see",
    "next",
    "video",
    "let",
    "write",
    "visualization",
    "code",
    "let",
    "follow",
    "data",
    "explorer",
    "motto",
    "visualizing",
    "transformed",
    "images",
    "saw",
    "looks",
    "like",
    "pass",
    "one",
    "image",
    "data",
    "transform",
    "wanted",
    "find",
    "documentation",
    "torch",
    "vision",
    "transforms",
    "could",
    "go",
    "lot",
    "transforming",
    "augmenting",
    "images",
    "actually",
    "going",
    "extra",
    "curriculum",
    "video",
    "transforms",
    "common",
    "image",
    "transformations",
    "available",
    "transforms",
    "module",
    "chained",
    "together",
    "using",
    "compose",
    "already",
    "done",
    "beautiful",
    "like",
    "go",
    "whole",
    "bunch",
    "different",
    "transforms",
    "including",
    "data",
    "augmentation",
    "transforms",
    "like",
    "see",
    "visually",
    "encourage",
    "check",
    "illustration",
    "transforms",
    "let",
    "write",
    "code",
    "explore",
    "transform",
    "visually",
    "first",
    "leave",
    "link",
    "going",
    "right",
    "transforms",
    "help",
    "get",
    "images",
    "ready",
    "used",
    "model",
    "slash",
    "perform",
    "data",
    "augmentation",
    "wonderful",
    "got",
    "way",
    "turn",
    "images",
    "tenses",
    "want",
    "model",
    "want",
    "images",
    "pytorch",
    "tenses",
    "goes",
    "data",
    "type",
    "working",
    "like",
    "visualize",
    "looks",
    "like",
    "plot",
    "number",
    "transformed",
    "images",
    "going",
    "make",
    "function",
    "takes",
    "image",
    "paths",
    "transform",
    "number",
    "images",
    "transform",
    "time",
    "random",
    "seed",
    "going",
    "harness",
    "power",
    "randomness",
    "sometimes",
    "want",
    "set",
    "seed",
    "sometimes",
    "image",
    "path",
    "list",
    "created",
    "image",
    "paths",
    "data",
    "set",
    "data",
    "pizza",
    "steak",
    "sushi",
    "select",
    "random",
    "image",
    "paths",
    "take",
    "image",
    "path",
    "run",
    "data",
    "transform",
    "compare",
    "original",
    "image",
    "looks",
    "like",
    "transformed",
    "image",
    "looks",
    "like",
    "let",
    "give",
    "try",
    "hey",
    "going",
    "write",
    "doc",
    "string",
    "selects",
    "random",
    "images",
    "path",
    "images",
    "loads",
    "slash",
    "transforms",
    "plots",
    "original",
    "verse",
    "transformed",
    "version",
    "quite",
    "long",
    "doc",
    "string",
    "enough",
    "put",
    "stuff",
    "image",
    "paths",
    "transforms",
    "seed",
    "code",
    "let",
    "go",
    "random",
    "seed",
    "create",
    "seed",
    "maybe",
    "seed",
    "random",
    "seed",
    "let",
    "put",
    "set",
    "seed",
    "equal",
    "none",
    "default",
    "way",
    "see",
    "works",
    "hey",
    "doubt",
    "coded",
    "random",
    "image",
    "paths",
    "going",
    "go",
    "random",
    "sample",
    "image",
    "paths",
    "number",
    "sample",
    "going",
    "random",
    "sample",
    "going",
    "list",
    "part",
    "list",
    "going",
    "randomly",
    "sample",
    "k",
    "going",
    "three",
    "images",
    "image",
    "path",
    "list",
    "going",
    "go",
    "image",
    "path",
    "going",
    "loop",
    "randomly",
    "sampled",
    "image",
    "parts",
    "know",
    "much",
    "love",
    "harnessing",
    "power",
    "randomness",
    "visualization",
    "image",
    "path",
    "random",
    "image",
    "paths",
    "let",
    "open",
    "image",
    "using",
    "pill",
    "image",
    "dot",
    "open",
    "image",
    "path",
    "going",
    "create",
    "figure",
    "axes",
    "going",
    "create",
    "subplot",
    "plot",
    "lib",
    "subplots",
    "want",
    "create",
    "one",
    "row",
    "goes",
    "n",
    "rows",
    "calls",
    "one",
    "row",
    "n",
    "calls",
    "equals",
    "two",
    "first",
    "zeroth",
    "axis",
    "going",
    "plot",
    "original",
    "image",
    "show",
    "going",
    "pass",
    "straight",
    "want",
    "go",
    "x",
    "zero",
    "going",
    "set",
    "title",
    "set",
    "title",
    "going",
    "set",
    "original",
    "create",
    "f",
    "string",
    "original",
    "new",
    "line",
    "create",
    "size",
    "variable",
    "going",
    "f",
    "dot",
    "size",
    "getting",
    "size",
    "attribute",
    "file",
    "keep",
    "going",
    "turn",
    "axes",
    "axis",
    "going",
    "set",
    "false",
    "let",
    "transform",
    "first",
    "axes",
    "plot",
    "going",
    "transform",
    "plot",
    "target",
    "image",
    "images",
    "going",
    "side",
    "side",
    "original",
    "transformed",
    "version",
    "one",
    "thing",
    "going",
    "code",
    "wrong",
    "way",
    "first",
    "think",
    "good",
    "way",
    "illustrate",
    "going",
    "going",
    "put",
    "note",
    "note",
    "need",
    "change",
    "shape",
    "matplotlib",
    "going",
    "come",
    "back",
    "noticed",
    "transform",
    "check",
    "shape",
    "oh",
    "excuse",
    "converts",
    "image",
    "color",
    "channels",
    "first",
    "whereas",
    "matplotlib",
    "prefers",
    "color",
    "channels",
    "last",
    "keep",
    "mind",
    "going",
    "forward",
    "code",
    "writing",
    "error",
    "purpose",
    "transformed",
    "image",
    "going",
    "go",
    "axe",
    "one",
    "well",
    "going",
    "set",
    "title",
    "going",
    "transformed",
    "create",
    "new",
    "line",
    "say",
    "size",
    "going",
    "transformed",
    "image",
    "dot",
    "shape",
    "probably",
    "bit",
    "yeah",
    "could",
    "probably",
    "go",
    "shape",
    "finally",
    "going",
    "go",
    "axe",
    "one",
    "going",
    "turn",
    "axis",
    "going",
    "set",
    "false",
    "also",
    "set",
    "could",
    "write",
    "false",
    "could",
    "write",
    "might",
    "see",
    "different",
    "versions",
    "somewhere",
    "going",
    "write",
    "super",
    "title",
    "see",
    "looks",
    "like",
    "class",
    "going",
    "image",
    "path",
    "getting",
    "target",
    "image",
    "path",
    "going",
    "get",
    "attribute",
    "parent",
    "attribute",
    "stem",
    "attribute",
    "like",
    "get",
    "class",
    "name",
    "going",
    "set",
    "larger",
    "font",
    "size",
    "make",
    "nice",
    "looking",
    "plots",
    "right",
    "going",
    "visualize",
    "data",
    "might",
    "well",
    "make",
    "plots",
    "visually",
    "appealing",
    "let",
    "plot",
    "transformed",
    "data",
    "transformed",
    "images",
    "image",
    "paths",
    "going",
    "set",
    "image",
    "part",
    "list",
    "variable",
    "part",
    "list",
    "list",
    "containing",
    "image",
    "paths",
    "transform",
    "going",
    "set",
    "transform",
    "equal",
    "data",
    "transform",
    "means",
    "pass",
    "transform",
    "image",
    "going",
    "go",
    "transform",
    "go",
    "going",
    "resized",
    "going",
    "randomly",
    "horizontally",
    "flipped",
    "going",
    "converted",
    "tensor",
    "going",
    "set",
    "data",
    "transfer",
    "data",
    "transform",
    "sorry",
    "going",
    "three",
    "plot",
    "three",
    "images",
    "set",
    "seed",
    "42",
    "begin",
    "let",
    "see",
    "works",
    "oh",
    "get",
    "wrong",
    "invalid",
    "shape",
    "said",
    "love",
    "seeing",
    "error",
    "seen",
    "error",
    "many",
    "times",
    "know",
    "know",
    "rearrange",
    "shapes",
    "data",
    "way",
    "shape",
    "form",
    "wow",
    "said",
    "shape",
    "lot",
    "right",
    "let",
    "go",
    "permute",
    "permute",
    "swap",
    "order",
    "axes",
    "right",
    "color",
    "channels",
    "first",
    "bring",
    "color",
    "channel",
    "axis",
    "dimension",
    "end",
    "need",
    "shuffle",
    "across",
    "64",
    "64",
    "three",
    "end",
    "need",
    "words",
    "turn",
    "color",
    "channels",
    "first",
    "color",
    "channels",
    "last",
    "permuting",
    "first",
    "axis",
    "come",
    "zero",
    "dimension",
    "spot",
    "number",
    "two",
    "going",
    "first",
    "dimension",
    "spot",
    "number",
    "zero",
    "going",
    "back",
    "end",
    "essentially",
    "going",
    "c",
    "h",
    "w",
    "changing",
    "order",
    "h",
    "w",
    "exact",
    "data",
    "going",
    "within",
    "tensor",
    "changing",
    "order",
    "dimensions",
    "let",
    "see",
    "works",
    "look",
    "oh",
    "love",
    "seeing",
    "manipulated",
    "data",
    "class",
    "pizza",
    "original",
    "image",
    "512",
    "resized",
    "using",
    "transform",
    "notice",
    "lot",
    "pixelated",
    "makes",
    "sense",
    "64",
    "64",
    "pixels",
    "might",
    "thing",
    "well",
    "one",
    "image",
    "still",
    "look",
    "like",
    "well",
    "still",
    "important",
    "thing",
    "look",
    "like",
    "model",
    "still",
    "look",
    "like",
    "original",
    "model",
    "64",
    "64",
    "less",
    "information",
    "encoded",
    "image",
    "model",
    "able",
    "compute",
    "faster",
    "images",
    "size",
    "however",
    "may",
    "lose",
    "performance",
    "much",
    "information",
    "encoded",
    "original",
    "image",
    "size",
    "image",
    "something",
    "control",
    "set",
    "hyper",
    "parameter",
    "tune",
    "size",
    "see",
    "improves",
    "model",
    "decided",
    "go",
    "60",
    "64",
    "64",
    "3",
    "line",
    "cnn",
    "explainer",
    "website",
    "little",
    "hint",
    "going",
    "replicating",
    "model",
    "done",
    "notice",
    "images",
    "size",
    "64",
    "64",
    "3",
    "cnn",
    "explainer",
    "model",
    "uses",
    "got",
    "could",
    "change",
    "size",
    "whatever",
    "want",
    "see",
    "oh",
    "got",
    "stake",
    "image",
    "notice",
    "image",
    "flipped",
    "horizontal",
    "horizontal",
    "access",
    "image",
    "flipped",
    "one",
    "power",
    "torch",
    "transforms",
    "lot",
    "transforms",
    "said",
    "go",
    "look",
    "going",
    "illustrations",
    "transforms",
    "great",
    "place",
    "resize",
    "center",
    "crop",
    "crop",
    "images",
    "crop",
    "five",
    "different",
    "locations",
    "grayscale",
    "change",
    "color",
    "whole",
    "bunch",
    "different",
    "things",
    "encourage",
    "check",
    "extra",
    "curriculum",
    "video",
    "visualized",
    "transform",
    "hinted",
    "going",
    "use",
    "transform",
    "load",
    "images",
    "using",
    "torch",
    "data",
    "set",
    "wanted",
    "make",
    "sure",
    "visualized",
    "first",
    "going",
    "use",
    "data",
    "transform",
    "next",
    "video",
    "load",
    "data",
    "using",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "helper",
    "function",
    "let",
    "give",
    "go",
    "see",
    "next",
    "video",
    "look",
    "beautiful",
    "plot",
    "got",
    "original",
    "images",
    "transformed",
    "images",
    "beautiful",
    "thing",
    "transformed",
    "images",
    "tensor",
    "format",
    "need",
    "model",
    "slowly",
    "working",
    "towards",
    "got",
    "data",
    "set",
    "got",
    "way",
    "turn",
    "tensors",
    "ready",
    "model",
    "let",
    "visualize",
    "another",
    "turn",
    "seed",
    "look",
    "random",
    "images",
    "go",
    "okay",
    "got",
    "stake",
    "pixelated",
    "downsizing",
    "64",
    "64",
    "thing",
    "one",
    "flipped",
    "horizontal",
    "thing",
    "pizza",
    "image",
    "one",
    "finish",
    "wonderful",
    "premise",
    "transforms",
    "turning",
    "images",
    "tensors",
    "also",
    "manipulating",
    "images",
    "want",
    "let",
    "get",
    "rid",
    "going",
    "make",
    "another",
    "heading",
    "section",
    "part",
    "four",
    "going",
    "option",
    "one",
    "loading",
    "image",
    "data",
    "using",
    "image",
    "folder",
    "going",
    "turn",
    "markdown",
    "let",
    "go",
    "torch",
    "vision",
    "data",
    "sets",
    "recall",
    "one",
    "torch",
    "vision",
    "domain",
    "libraries",
    "data",
    "sets",
    "module",
    "built",
    "functions",
    "helping",
    "load",
    "data",
    "case",
    "image",
    "folder",
    "others",
    "like",
    "look",
    "image",
    "folder",
    "class",
    "going",
    "help",
    "us",
    "load",
    "data",
    "format",
    "generic",
    "image",
    "classification",
    "format",
    "prebuilt",
    "data",
    "sets",
    "function",
    "like",
    "prebuilt",
    "data",
    "sets",
    "use",
    "prebuilt",
    "data",
    "set",
    "functions",
    "option",
    "two",
    "later",
    "spoiler",
    "going",
    "create",
    "custom",
    "version",
    "data",
    "set",
    "loader",
    "see",
    "later",
    "video",
    "let",
    "see",
    "use",
    "image",
    "folder",
    "load",
    "custom",
    "data",
    "custom",
    "images",
    "tensors",
    "transform",
    "going",
    "come",
    "helpful",
    "let",
    "write",
    "load",
    "image",
    "classification",
    "data",
    "using",
    "let",
    "write",
    "let",
    "write",
    "full",
    "path",
    "name",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "dot",
    "image",
    "folder",
    "put",
    "beautiful",
    "let",
    "start",
    "use",
    "image",
    "folder",
    "create",
    "data",
    "sets",
    "previous",
    "video",
    "hinted",
    "fact",
    "pass",
    "transform",
    "image",
    "folder",
    "class",
    "going",
    "right",
    "let",
    "see",
    "looks",
    "like",
    "practice",
    "torch",
    "vision",
    "going",
    "import",
    "data",
    "sets",
    "image",
    "folder",
    "module",
    "lives",
    "go",
    "train",
    "data",
    "equals",
    "data",
    "sets",
    "dot",
    "image",
    "folder",
    "going",
    "pass",
    "root",
    "train",
    "der",
    "going",
    "training",
    "directory",
    "first",
    "going",
    "pass",
    "transform",
    "going",
    "equal",
    "data",
    "transform",
    "going",
    "pass",
    "target",
    "transform",
    "going",
    "leave",
    "none",
    "default",
    "believe",
    "go",
    "yeah",
    "target",
    "transform",
    "optional",
    "means",
    "going",
    "transform",
    "data",
    "going",
    "transform",
    "label",
    "slash",
    "target",
    "pytorch",
    "likes",
    "use",
    "target",
    "like",
    "use",
    "label",
    "okay",
    "means",
    "need",
    "target",
    "transform",
    "labels",
    "going",
    "inferred",
    "target",
    "directory",
    "images",
    "live",
    "pizza",
    "images",
    "directory",
    "going",
    "pizza",
    "label",
    "data",
    "set",
    "standard",
    "image",
    "classification",
    "format",
    "data",
    "set",
    "standard",
    "image",
    "classification",
    "format",
    "might",
    "use",
    "different",
    "data",
    "loader",
    "lot",
    "transform",
    "data",
    "transform",
    "going",
    "run",
    "images",
    "whatever",
    "images",
    "loaded",
    "folders",
    "transform",
    "created",
    "going",
    "resize",
    "randomly",
    "flip",
    "horizontal",
    "turn",
    "tenses",
    "exactly",
    "want",
    "pytorch",
    "models",
    "wanted",
    "transform",
    "labels",
    "way",
    "shape",
    "form",
    "could",
    "pass",
    "target",
    "transform",
    "case",
    "need",
    "transform",
    "labels",
    "let",
    "thing",
    "test",
    "data",
    "wanted",
    "visualize",
    "transforms",
    "previous",
    "videos",
    "otherwise",
    "passing",
    "transform",
    "really",
    "going",
    "happen",
    "behind",
    "scenes",
    "images",
    "going",
    "go",
    "steps",
    "going",
    "look",
    "like",
    "turn",
    "data",
    "set",
    "let",
    "create",
    "test",
    "data",
    "test",
    "data",
    "set",
    "transform",
    "going",
    "transform",
    "test",
    "data",
    "set",
    "way",
    "transformed",
    "training",
    "data",
    "set",
    "going",
    "leave",
    "like",
    "let",
    "print",
    "data",
    "sets",
    "look",
    "like",
    "train",
    "data",
    "test",
    "data",
    "beautiful",
    "data",
    "set",
    "torch",
    "data",
    "set",
    "image",
    "folder",
    "number",
    "data",
    "points",
    "going",
    "training",
    "data",
    "set",
    "means",
    "75",
    "images",
    "per",
    "class",
    "root",
    "location",
    "folder",
    "loaded",
    "training",
    "directory",
    "set",
    "two",
    "trained",
    "tester",
    "transform",
    "standard",
    "transform",
    "resize",
    "followed",
    "random",
    "horizontal",
    "flip",
    "followed",
    "two",
    "tensor",
    "got",
    "basically",
    "output",
    "test",
    "directory",
    "except",
    "less",
    "samples",
    "let",
    "get",
    "little",
    "attributes",
    "image",
    "folder",
    "one",
    "benefits",
    "using",
    "pytorch",
    "prebuilt",
    "data",
    "loader",
    "data",
    "set",
    "loader",
    "comes",
    "fair",
    "attributes",
    "could",
    "go",
    "documentation",
    "find",
    "inherits",
    "data",
    "set",
    "folder",
    "keep",
    "digging",
    "could",
    "come",
    "straight",
    "google",
    "collab",
    "let",
    "go",
    "get",
    "class",
    "names",
    "list",
    "go",
    "train",
    "data",
    "dot",
    "press",
    "tab",
    "beautiful",
    "got",
    "fair",
    "things",
    "attributes",
    "let",
    "look",
    "classes",
    "going",
    "give",
    "us",
    "list",
    "class",
    "names",
    "class",
    "names",
    "helpful",
    "later",
    "got",
    "pizza",
    "steak",
    "sushi",
    "trying",
    "everything",
    "code",
    "attribute",
    "train",
    "data",
    "dot",
    "classes",
    "use",
    "list",
    "later",
    "plot",
    "images",
    "straight",
    "data",
    "set",
    "make",
    "predictions",
    "want",
    "label",
    "also",
    "get",
    "class",
    "names",
    "dictionary",
    "map",
    "integer",
    "index",
    "go",
    "train",
    "data",
    "dot",
    "press",
    "tab",
    "got",
    "class",
    "id",
    "let",
    "see",
    "looks",
    "like",
    "class",
    "decked",
    "wonderful",
    "got",
    "string",
    "class",
    "names",
    "mapped",
    "integer",
    "got",
    "pizza",
    "zero",
    "steak",
    "one",
    "sushi",
    "two",
    "target",
    "transform",
    "would",
    "come",
    "play",
    "wanted",
    "transform",
    "labels",
    "way",
    "shape",
    "form",
    "could",
    "pass",
    "transform",
    "keep",
    "going",
    "let",
    "check",
    "lengths",
    "going",
    "check",
    "lengths",
    "data",
    "set",
    "seen",
    "going",
    "give",
    "us",
    "many",
    "samples",
    "length",
    "train",
    "data",
    "length",
    "test",
    "data",
    "beautiful",
    "course",
    "like",
    "explore",
    "attributes",
    "go",
    "train",
    "data",
    "dot",
    "got",
    "things",
    "functions",
    "images",
    "loader",
    "samples",
    "targets",
    "wanted",
    "see",
    "images",
    "go",
    "dot",
    "samples",
    "wanted",
    "see",
    "labels",
    "go",
    "dot",
    "targets",
    "going",
    "labels",
    "look",
    "believe",
    "going",
    "order",
    "going",
    "zero",
    "zero",
    "zero",
    "one",
    "one",
    "one",
    "two",
    "two",
    "wanted",
    "look",
    "let",
    "say",
    "look",
    "first",
    "sample",
    "hey",
    "data",
    "pizza",
    "steak",
    "sushi",
    "train",
    "pizza",
    "image",
    "path",
    "label",
    "zero",
    "pizza",
    "wonderful",
    "done",
    "visualizing",
    "whole",
    "time",
    "let",
    "keep",
    "trend",
    "let",
    "visualize",
    "sample",
    "label",
    "train",
    "data",
    "data",
    "set",
    "video",
    "used",
    "image",
    "folder",
    "load",
    "images",
    "tenses",
    "data",
    "already",
    "standard",
    "image",
    "classification",
    "format",
    "use",
    "one",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "prebuilt",
    "functions",
    "let",
    "visualization",
    "next",
    "video",
    "see",
    "welcome",
    "back",
    "last",
    "video",
    "used",
    "data",
    "sets",
    "dot",
    "image",
    "folder",
    "turn",
    "image",
    "data",
    "tenses",
    "help",
    "data",
    "transform",
    "little",
    "pipeline",
    "take",
    "data",
    "specifically",
    "image",
    "resize",
    "value",
    "set",
    "k6464",
    "randomly",
    "flip",
    "along",
    "horizontal",
    "necessarily",
    "need",
    "put",
    "indicate",
    "happens",
    "pass",
    "image",
    "transforms",
    "pipeline",
    "importantly",
    "turned",
    "images",
    "torch",
    "tensor",
    "means",
    "data",
    "custom",
    "data",
    "set",
    "exciting",
    "compatible",
    "used",
    "pytorch",
    "model",
    "let",
    "keep",
    "pushing",
    "forward",
    "finished",
    "yet",
    "going",
    "visualize",
    "samples",
    "train",
    "data",
    "data",
    "set",
    "let",
    "let",
    "get",
    "index",
    "train",
    "data",
    "data",
    "set",
    "get",
    "single",
    "image",
    "label",
    "go",
    "train",
    "data",
    "zero",
    "give",
    "us",
    "okay",
    "going",
    "give",
    "us",
    "image",
    "tensor",
    "associated",
    "label",
    "case",
    "image",
    "pizza",
    "associated",
    "label",
    "pizza",
    "let",
    "take",
    "zero",
    "zero",
    "going",
    "image",
    "label",
    "going",
    "train",
    "data",
    "zero",
    "going",
    "get",
    "first",
    "index",
    "item",
    "going",
    "one",
    "look",
    "separately",
    "image",
    "label",
    "beautiful",
    "one",
    "target",
    "images",
    "tensor",
    "format",
    "exactly",
    "want",
    "label",
    "numeric",
    "format",
    "well",
    "also",
    "exactly",
    "want",
    "wanted",
    "convert",
    "back",
    "non",
    "label",
    "go",
    "class",
    "names",
    "index",
    "see",
    "pizza",
    "mean",
    "non",
    "label",
    "non",
    "numeric",
    "get",
    "back",
    "string",
    "format",
    "human",
    "understandable",
    "index",
    "class",
    "names",
    "let",
    "print",
    "information",
    "going",
    "print",
    "f",
    "going",
    "go",
    "image",
    "tensor",
    "love",
    "f",
    "strings",
    "noticed",
    "yet",
    "image",
    "tensor",
    "going",
    "set",
    "new",
    "line",
    "going",
    "pass",
    "image",
    "image",
    "got",
    "print",
    "information",
    "still",
    "becoming",
    "one",
    "data",
    "right",
    "slowly",
    "finding",
    "information",
    "data",
    "set",
    "errors",
    "arise",
    "later",
    "go",
    "hmm",
    "image",
    "getting",
    "shape",
    "error",
    "know",
    "images",
    "shape",
    "getting",
    "data",
    "type",
    "error",
    "got",
    "dot",
    "type",
    "might",
    "getting",
    "data",
    "type",
    "issue",
    "let",
    "one",
    "image",
    "label",
    "label",
    "oh",
    "well",
    "actually",
    "one",
    "print",
    "get",
    "label",
    "data",
    "type",
    "well",
    "label",
    "important",
    "take",
    "note",
    "later",
    "type",
    "said",
    "three",
    "big",
    "issues",
    "shape",
    "mismatch",
    "device",
    "mismatch",
    "data",
    "type",
    "mismatch",
    "get",
    "type",
    "label",
    "beautiful",
    "got",
    "image",
    "tensor",
    "got",
    "shape",
    "torch",
    "size",
    "exactly",
    "want",
    "data",
    "type",
    "torch",
    "float",
    "32",
    "default",
    "data",
    "type",
    "pytorch",
    "image",
    "label",
    "zero",
    "label",
    "data",
    "type",
    "integer",
    "let",
    "try",
    "plot",
    "see",
    "looks",
    "like",
    "hey",
    "using",
    "matplotlib",
    "first",
    "well",
    "rearrange",
    "order",
    "dimensions",
    "words",
    "matplotlib",
    "likes",
    "color",
    "channels",
    "last",
    "let",
    "see",
    "looks",
    "looks",
    "like",
    "go",
    "image",
    "per",
    "mute",
    "done",
    "120",
    "means",
    "reordering",
    "dimensions",
    "zero",
    "would",
    "usually",
    "except",
    "taken",
    "zero",
    "dimension",
    "color",
    "channels",
    "put",
    "end",
    "shuffled",
    "two",
    "forward",
    "let",
    "print",
    "different",
    "shapes",
    "love",
    "printing",
    "change",
    "shapes",
    "helps",
    "really",
    "understand",
    "going",
    "sometimes",
    "look",
    "line",
    "like",
    "really",
    "help",
    "print",
    "something",
    "shapes",
    "originally",
    "changed",
    "well",
    "hey",
    "big",
    "help",
    "jupiter",
    "notebooks",
    "right",
    "going",
    "color",
    "channels",
    "first",
    "height",
    "width",
    "depending",
    "data",
    "using",
    "using",
    "images",
    "using",
    "text",
    "still",
    "knowing",
    "shape",
    "data",
    "good",
    "thing",
    "going",
    "go",
    "image",
    "per",
    "everything",
    "going",
    "right",
    "height",
    "color",
    "channels",
    "end",
    "going",
    "plot",
    "image",
    "never",
    "get",
    "enough",
    "plotting",
    "practice",
    "plot",
    "image",
    "going",
    "go",
    "plt",
    "dot",
    "figure",
    "pass",
    "fig",
    "size",
    "equals",
    "10",
    "going",
    "plt",
    "dot",
    "show",
    "pass",
    "permuted",
    "image",
    "image",
    "underscore",
    "permutes",
    "turn",
    "axes",
    "set",
    "title",
    "class",
    "names",
    "going",
    "index",
    "label",
    "going",
    "set",
    "font",
    "size",
    "equal",
    "nice",
    "big",
    "go",
    "beautiful",
    "image",
    "pizza",
    "pixelated",
    "going",
    "512",
    "original",
    "size",
    "512",
    "512",
    "64",
    "would",
    "encourage",
    "try",
    "potentially",
    "could",
    "use",
    "different",
    "image",
    "indexed",
    "sample",
    "zero",
    "maybe",
    "want",
    "change",
    "random",
    "image",
    "go",
    "steps",
    "like",
    "see",
    "different",
    "transforms",
    "also",
    "encourage",
    "try",
    "changing",
    "transform",
    "pipeline",
    "maybe",
    "increase",
    "size",
    "see",
    "looks",
    "like",
    "feeling",
    "really",
    "adventurous",
    "go",
    "torch",
    "vision",
    "look",
    "transforms",
    "library",
    "try",
    "one",
    "see",
    "images",
    "going",
    "keep",
    "pushing",
    "forward",
    "going",
    "look",
    "another",
    "way",
    "actually",
    "think",
    "completeness",
    "let",
    "turn",
    "got",
    "data",
    "set",
    "want",
    "wrote",
    "wanted",
    "turn",
    "images",
    "data",
    "set",
    "subsequently",
    "torch",
    "utils",
    "data",
    "data",
    "loader",
    "done",
    "batching",
    "images",
    "batching",
    "data",
    "working",
    "encourage",
    "give",
    "shot",
    "try",
    "go",
    "next",
    "video",
    "create",
    "train",
    "data",
    "loader",
    "using",
    "train",
    "data",
    "wherever",
    "train",
    "data",
    "test",
    "data",
    "loader",
    "using",
    "test",
    "data",
    "give",
    "shot",
    "together",
    "next",
    "video",
    "turn",
    "data",
    "sets",
    "data",
    "loaders",
    "welcome",
    "back",
    "go",
    "last",
    "video",
    "issued",
    "challenge",
    "turn",
    "data",
    "sets",
    "data",
    "loaders",
    "let",
    "together",
    "hope",
    "gave",
    "shot",
    "best",
    "way",
    "practice",
    "turn",
    "loaded",
    "images",
    "data",
    "loaders",
    "still",
    "adhering",
    "pytorch",
    "workflow",
    "got",
    "custom",
    "data",
    "set",
    "found",
    "way",
    "turn",
    "tenses",
    "form",
    "data",
    "sets",
    "going",
    "turn",
    "data",
    "loader",
    "turn",
    "data",
    "sets",
    "iterables",
    "batchify",
    "data",
    "let",
    "write",
    "data",
    "loader",
    "going",
    "help",
    "us",
    "turn",
    "data",
    "sets",
    "iterables",
    "customize",
    "batch",
    "size",
    "write",
    "model",
    "see",
    "batch",
    "size",
    "images",
    "time",
    "important",
    "touched",
    "last",
    "section",
    "computer",
    "vision",
    "create",
    "batch",
    "size",
    "images",
    "chances",
    "one",
    "data",
    "set",
    "images",
    "food",
    "101",
    "data",
    "set",
    "working",
    "try",
    "load",
    "one",
    "hit",
    "chances",
    "hardware",
    "may",
    "run",
    "memory",
    "matchify",
    "images",
    "look",
    "nvidia",
    "smi",
    "gpu",
    "16",
    "gigabytes",
    "using",
    "tesla",
    "t4",
    "right",
    "well",
    "15",
    "gigabytes",
    "memory",
    "tried",
    "load",
    "images",
    "whilst",
    "also",
    "computing",
    "pytorch",
    "model",
    "potentially",
    "going",
    "run",
    "memory",
    "run",
    "issues",
    "instead",
    "turn",
    "data",
    "loader",
    "model",
    "looks",
    "32",
    "images",
    "time",
    "leverage",
    "memory",
    "rather",
    "running",
    "memory",
    "let",
    "turn",
    "train",
    "test",
    "data",
    "sets",
    "data",
    "loaders",
    "turn",
    "train",
    "test",
    "data",
    "sets",
    "data",
    "loaders",
    "image",
    "data",
    "kinds",
    "data",
    "pytorch",
    "images",
    "text",
    "audio",
    "name",
    "import",
    "data",
    "loader",
    "going",
    "create",
    "train",
    "data",
    "loader",
    "going",
    "set",
    "equal",
    "data",
    "loader",
    "going",
    "pass",
    "data",
    "set",
    "let",
    "set",
    "train",
    "data",
    "let",
    "set",
    "batch",
    "size",
    "set",
    "batch",
    "size",
    "going",
    "come",
    "set",
    "laser",
    "capital",
    "variable",
    "going",
    "use",
    "32",
    "32",
    "good",
    "batch",
    "size",
    "go",
    "32",
    "actually",
    "let",
    "start",
    "small",
    "let",
    "start",
    "batch",
    "size",
    "one",
    "see",
    "happens",
    "batch",
    "size",
    "one",
    "number",
    "workers",
    "parameter",
    "going",
    "important",
    "one",
    "going",
    "potentially",
    "covered",
    "going",
    "introduce",
    "going",
    "many",
    "cores",
    "many",
    "cpu",
    "cores",
    "used",
    "load",
    "data",
    "higher",
    "better",
    "usually",
    "set",
    "via",
    "os",
    "cpu",
    "count",
    "count",
    "many",
    "cpus",
    "compute",
    "hardware",
    "show",
    "works",
    "import",
    "os",
    "python",
    "os",
    "module",
    "cpu",
    "count",
    "find",
    "many",
    "cpus",
    "google",
    "colab",
    "instance",
    "mine",
    "two",
    "number",
    "may",
    "vary",
    "believe",
    "colab",
    "instances",
    "two",
    "cpus",
    "running",
    "local",
    "machine",
    "may",
    "running",
    "dedicated",
    "deep",
    "learning",
    "hardware",
    "may",
    "even",
    "even",
    "right",
    "generally",
    "set",
    "one",
    "use",
    "one",
    "cpu",
    "core",
    "set",
    "os",
    "dot",
    "cpu",
    "count",
    "use",
    "many",
    "possible",
    "going",
    "leave",
    "one",
    "right",
    "customize",
    "however",
    "want",
    "going",
    "shuffle",
    "training",
    "data",
    "want",
    "model",
    "recognize",
    "order",
    "training",
    "data",
    "going",
    "mix",
    "going",
    "create",
    "test",
    "data",
    "loader",
    "data",
    "set",
    "equals",
    "test",
    "data",
    "batch",
    "size",
    "equals",
    "one",
    "num",
    "workers",
    "going",
    "set",
    "equal",
    "one",
    "well",
    "customize",
    "hyper",
    "parameters",
    "whatever",
    "want",
    "number",
    "workers",
    "generally",
    "better",
    "going",
    "set",
    "shuffle",
    "equals",
    "false",
    "test",
    "data",
    "want",
    "evaluate",
    "models",
    "later",
    "test",
    "data",
    "set",
    "always",
    "order",
    "let",
    "look",
    "train",
    "data",
    "loader",
    "see",
    "happens",
    "test",
    "data",
    "loader",
    "wonderful",
    "get",
    "two",
    "instances",
    "torch",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "loader",
    "see",
    "visualize",
    "something",
    "train",
    "data",
    "loader",
    "well",
    "test",
    "data",
    "loader",
    "actually",
    "maybe",
    "visualize",
    "something",
    "one",
    "double",
    "handling",
    "everything",
    "get",
    "length",
    "wonderful",
    "using",
    "batch",
    "size",
    "one",
    "lengths",
    "data",
    "loaders",
    "data",
    "sets",
    "course",
    "would",
    "change",
    "set",
    "oh",
    "even",
    "set",
    "batch",
    "size",
    "parameter",
    "batch",
    "size",
    "let",
    "come",
    "batch",
    "size",
    "watch",
    "change",
    "wanted",
    "look",
    "32",
    "images",
    "time",
    "definitely",
    "could",
    "eight",
    "batches",
    "22",
    "225",
    "divided",
    "32",
    "equals",
    "roughly",
    "eight",
    "75",
    "divided",
    "32",
    "also",
    "equals",
    "roughly",
    "three",
    "remember",
    "numbers",
    "going",
    "rounded",
    "overlaps",
    "let",
    "get",
    "rid",
    "change",
    "back",
    "one",
    "keep",
    "get",
    "rid",
    "two",
    "let",
    "see",
    "looks",
    "like",
    "plot",
    "image",
    "data",
    "loader",
    "least",
    "look",
    "check",
    "shapes",
    "probably",
    "important",
    "point",
    "time",
    "already",
    "plotted",
    "things",
    "let",
    "iterate",
    "train",
    "data",
    "loader",
    "grab",
    "next",
    "one",
    "grab",
    "image",
    "label",
    "going",
    "print",
    "batch",
    "size",
    "one",
    "change",
    "batch",
    "size",
    "like",
    "another",
    "way",
    "getting",
    "familiar",
    "shapes",
    "data",
    "image",
    "shape",
    "let",
    "go",
    "image",
    "dot",
    "shape",
    "going",
    "write",
    "shape",
    "going",
    "batch",
    "size",
    "data",
    "loader",
    "going",
    "add",
    "images",
    "going",
    "add",
    "batch",
    "dimension",
    "color",
    "channels",
    "height",
    "width",
    "print",
    "let",
    "check",
    "label",
    "shape",
    "thing",
    "labels",
    "going",
    "add",
    "batch",
    "dimension",
    "label",
    "let",
    "see",
    "happens",
    "oh",
    "forgot",
    "end",
    "bracket",
    "beautiful",
    "got",
    "image",
    "shape",
    "label",
    "shape",
    "one",
    "batch",
    "size",
    "one",
    "got",
    "batch",
    "size",
    "one",
    "color",
    "channels",
    "three",
    "height",
    "width",
    "change",
    "32",
    "think",
    "going",
    "happen",
    "get",
    "batch",
    "size",
    "32",
    "still",
    "three",
    "color",
    "channels",
    "still",
    "64",
    "still",
    "32",
    "labels",
    "means",
    "within",
    "batch",
    "32",
    "images",
    "32",
    "labels",
    "could",
    "use",
    "model",
    "going",
    "change",
    "back",
    "one",
    "think",
    "covered",
    "enough",
    "terms",
    "loading",
    "data",
    "sets",
    "cool",
    "come",
    "long",
    "way",
    "downloaded",
    "custom",
    "data",
    "set",
    "loaded",
    "data",
    "set",
    "using",
    "image",
    "folder",
    "turned",
    "tenses",
    "using",
    "data",
    "transform",
    "batchified",
    "custom",
    "data",
    "set",
    "data",
    "loaders",
    "used",
    "models",
    "wanted",
    "could",
    "go",
    "right",
    "ahead",
    "build",
    "convolutional",
    "neural",
    "network",
    "try",
    "find",
    "patterns",
    "image",
    "tenses",
    "next",
    "video",
    "let",
    "pretend",
    "data",
    "loader",
    "image",
    "folder",
    "class",
    "available",
    "us",
    "could",
    "load",
    "image",
    "data",
    "set",
    "compatible",
    "like",
    "image",
    "data",
    "set",
    "could",
    "replicate",
    "image",
    "folder",
    "class",
    "could",
    "use",
    "data",
    "loader",
    "data",
    "load",
    "part",
    "torch",
    "going",
    "see",
    "everywhere",
    "let",
    "pretend",
    "torch",
    "sets",
    "image",
    "folder",
    "helper",
    "function",
    "see",
    "next",
    "video",
    "replicate",
    "functionality",
    "see",
    "welcome",
    "back",
    "past",
    "videos",
    "working",
    "get",
    "get",
    "data",
    "data",
    "folder",
    "pizza",
    "steak",
    "sushi",
    "got",
    "images",
    "different",
    "food",
    "data",
    "trying",
    "get",
    "tensor",
    "format",
    "seen",
    "existing",
    "data",
    "loader",
    "helper",
    "function",
    "data",
    "set",
    "function",
    "image",
    "folder",
    "however",
    "image",
    "folder",
    "exist",
    "need",
    "write",
    "custom",
    "data",
    "loading",
    "function",
    "premise",
    "although",
    "exist",
    "going",
    "good",
    "practice",
    "might",
    "come",
    "across",
    "case",
    "trying",
    "use",
    "data",
    "set",
    "prebuilt",
    "function",
    "exist",
    "let",
    "replicate",
    "functionality",
    "image",
    "folder",
    "creating",
    "data",
    "loading",
    "class",
    "want",
    "things",
    "want",
    "able",
    "get",
    "class",
    "names",
    "list",
    "loaded",
    "data",
    "want",
    "able",
    "get",
    "class",
    "names",
    "dictionary",
    "well",
    "whole",
    "goal",
    "video",
    "start",
    "writing",
    "function",
    "class",
    "capable",
    "loading",
    "data",
    "tensor",
    "format",
    "capable",
    "used",
    "pytorch",
    "data",
    "loader",
    "class",
    "like",
    "done",
    "want",
    "create",
    "data",
    "set",
    "let",
    "start",
    "going",
    "create",
    "another",
    "heading",
    "going",
    "number",
    "five",
    "option",
    "two",
    "loading",
    "image",
    "data",
    "custom",
    "data",
    "set",
    "want",
    "functionality",
    "steps",
    "number",
    "one",
    "one",
    "two",
    "able",
    "load",
    "images",
    "file",
    "one",
    "two",
    "able",
    "get",
    "class",
    "names",
    "data",
    "set",
    "three",
    "one",
    "two",
    "able",
    "get",
    "classes",
    "dictionary",
    "data",
    "set",
    "let",
    "briefly",
    "discuss",
    "pros",
    "cons",
    "creating",
    "custom",
    "data",
    "set",
    "saw",
    "option",
    "one",
    "use",
    "data",
    "set",
    "loader",
    "helping",
    "function",
    "torch",
    "vision",
    "going",
    "quite",
    "similar",
    "go",
    "torch",
    "vision",
    "data",
    "sets",
    "quite",
    "similar",
    "using",
    "domain",
    "libraries",
    "going",
    "data",
    "loading",
    "utilities",
    "base",
    "level",
    "pytorch",
    "base",
    "data",
    "set",
    "class",
    "want",
    "build",
    "top",
    "create",
    "image",
    "folder",
    "loading",
    "class",
    "pros",
    "cons",
    "creating",
    "custom",
    "data",
    "set",
    "well",
    "let",
    "discuss",
    "pros",
    "one",
    "pro",
    "would",
    "create",
    "data",
    "set",
    "almost",
    "anything",
    "long",
    "write",
    "right",
    "code",
    "load",
    "another",
    "pro",
    "limited",
    "pytorch",
    "data",
    "set",
    "functions",
    "couple",
    "cons",
    "would",
    "even",
    "though",
    "point",
    "number",
    "one",
    "even",
    "though",
    "could",
    "create",
    "data",
    "set",
    "almost",
    "anything",
    "mean",
    "automatically",
    "work",
    "work",
    "course",
    "verify",
    "extensive",
    "testing",
    "seeing",
    "model",
    "actually",
    "works",
    "actually",
    "loads",
    "data",
    "way",
    "want",
    "another",
    "con",
    "using",
    "custom",
    "data",
    "set",
    "requires",
    "us",
    "write",
    "code",
    "often",
    "results",
    "us",
    "writing",
    "code",
    "could",
    "prone",
    "errors",
    "performance",
    "issues",
    "typically",
    "something",
    "makes",
    "pytorch",
    "standard",
    "library",
    "pytorch",
    "domain",
    "libraries",
    "functionality",
    "makes",
    "generally",
    "tested",
    "many",
    "many",
    "times",
    "kind",
    "verified",
    "works",
    "quite",
    "well",
    "use",
    "works",
    "quite",
    "well",
    "whereas",
    "write",
    "code",
    "sure",
    "test",
    "got",
    "robustness",
    "begin",
    "could",
    "fix",
    "time",
    "something",
    "included",
    "say",
    "pytorch",
    "standard",
    "library",
    "nonetheless",
    "important",
    "aware",
    "could",
    "create",
    "custom",
    "data",
    "set",
    "let",
    "import",
    "things",
    "going",
    "use",
    "import",
    "os",
    "going",
    "working",
    "python",
    "file",
    "system",
    "going",
    "import",
    "path",
    "lib",
    "going",
    "working",
    "file",
    "paths",
    "import",
    "torch",
    "need",
    "completeness",
    "going",
    "import",
    "image",
    "pill",
    "image",
    "class",
    "want",
    "opening",
    "images",
    "going",
    "import",
    "torch",
    "utils",
    "dot",
    "data",
    "going",
    "import",
    "data",
    "set",
    "base",
    "data",
    "set",
    "said",
    "go",
    "data",
    "sets",
    "click",
    "torch",
    "utils",
    "data",
    "dot",
    "data",
    "set",
    "abstract",
    "class",
    "representing",
    "data",
    "set",
    "find",
    "data",
    "set",
    "links",
    "base",
    "data",
    "set",
    "class",
    "many",
    "data",
    "sets",
    "pytorch",
    "prebuilt",
    "functions",
    "subclass",
    "going",
    "notes",
    "subclasses",
    "overwrite",
    "get",
    "item",
    "optionally",
    "overwrite",
    "land",
    "two",
    "methods",
    "going",
    "see",
    "future",
    "video",
    "setting",
    "scene",
    "torch",
    "vision",
    "going",
    "import",
    "transforms",
    "want",
    "import",
    "images",
    "want",
    "transform",
    "tenses",
    "python",
    "typing",
    "module",
    "going",
    "import",
    "tuple",
    "dict",
    "list",
    "put",
    "type",
    "hints",
    "create",
    "class",
    "loading",
    "functions",
    "wonderful",
    "instance",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "image",
    "folder",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "dot",
    "image",
    "folder",
    "let",
    "look",
    "train",
    "data",
    "want",
    "write",
    "function",
    "replicate",
    "getting",
    "classes",
    "particular",
    "directory",
    "also",
    "turning",
    "index",
    "dictionary",
    "let",
    "build",
    "helper",
    "function",
    "replicate",
    "functionality",
    "words",
    "like",
    "write",
    "helper",
    "function",
    "pass",
    "file",
    "path",
    "pizza",
    "steak",
    "sushi",
    "data",
    "folder",
    "going",
    "go",
    "going",
    "return",
    "class",
    "names",
    "list",
    "also",
    "going",
    "turn",
    "dictionary",
    "going",
    "helpful",
    "later",
    "like",
    "access",
    "classes",
    "class",
    "id",
    "really",
    "want",
    "completely",
    "recreate",
    "image",
    "folder",
    "well",
    "image",
    "folder",
    "functionality",
    "like",
    "little",
    "high",
    "level",
    "overview",
    "going",
    "might",
    "link",
    "going",
    "subclass",
    "custom",
    "data",
    "sets",
    "pie",
    "torch",
    "often",
    "subclass",
    "going",
    "next",
    "videos",
    "want",
    "able",
    "load",
    "images",
    "file",
    "could",
    "replace",
    "images",
    "whatever",
    "data",
    "working",
    "premise",
    "want",
    "able",
    "get",
    "class",
    "names",
    "data",
    "set",
    "want",
    "able",
    "get",
    "classes",
    "dictionary",
    "data",
    "set",
    "going",
    "map",
    "samples",
    "image",
    "samples",
    "class",
    "name",
    "passing",
    "file",
    "path",
    "function",
    "write",
    "pros",
    "cons",
    "creating",
    "custom",
    "data",
    "set",
    "let",
    "next",
    "video",
    "start",
    "coding",
    "helper",
    "function",
    "retrieve",
    "two",
    "things",
    "target",
    "directory",
    "last",
    "video",
    "discussed",
    "exciting",
    "concept",
    "creating",
    "custom",
    "data",
    "set",
    "wrote",
    "things",
    "want",
    "get",
    "discussed",
    "pros",
    "cons",
    "learned",
    "many",
    "custom",
    "data",
    "sets",
    "inherit",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "data",
    "set",
    "later",
    "video",
    "let",
    "focus",
    "writing",
    "helper",
    "function",
    "recreate",
    "functionality",
    "going",
    "title",
    "creating",
    "helper",
    "function",
    "get",
    "class",
    "names",
    "going",
    "turn",
    "markdown",
    "go",
    "want",
    "function",
    "let",
    "write",
    "steps",
    "code",
    "get",
    "class",
    "names",
    "going",
    "use",
    "os",
    "dot",
    "scanner",
    "going",
    "scanner",
    "directory",
    "traverse",
    "target",
    "directory",
    "ideally",
    "directory",
    "standard",
    "image",
    "classification",
    "format",
    "like",
    "image",
    "folder",
    "class",
    "custom",
    "data",
    "class",
    "going",
    "require",
    "data",
    "already",
    "formatted",
    "standard",
    "image",
    "classification",
    "format",
    "train",
    "test",
    "training",
    "test",
    "images",
    "images",
    "particular",
    "class",
    "particular",
    "directory",
    "let",
    "keep",
    "going",
    "number",
    "two",
    "else",
    "want",
    "want",
    "raise",
    "error",
    "class",
    "names",
    "found",
    "happens",
    "might",
    "want",
    "enter",
    "fact",
    "might",
    "something",
    "wrong",
    "directory",
    "structure",
    "number",
    "three",
    "also",
    "want",
    "turn",
    "class",
    "names",
    "dict",
    "list",
    "return",
    "beautiful",
    "let",
    "get",
    "started",
    "let",
    "set",
    "path",
    "directory",
    "target",
    "directory",
    "target",
    "directory",
    "going",
    "directory",
    "want",
    "load",
    "directory",
    "could",
    "spell",
    "want",
    "load",
    "data",
    "let",
    "start",
    "training",
    "der",
    "example",
    "target",
    "directory",
    "get",
    "going",
    "use",
    "training",
    "folder",
    "example",
    "begin",
    "go",
    "print",
    "target",
    "der",
    "put",
    "target",
    "directory",
    "want",
    "exemplify",
    "going",
    "get",
    "class",
    "names",
    "target",
    "directory",
    "show",
    "functionality",
    "scanner",
    "course",
    "could",
    "look",
    "python",
    "documentation",
    "class",
    "names",
    "found",
    "let",
    "set",
    "sorted",
    "get",
    "entry",
    "name",
    "entry",
    "dot",
    "name",
    "entry",
    "list",
    "going",
    "get",
    "os",
    "list",
    "scanner",
    "image",
    "path",
    "slash",
    "target",
    "directory",
    "let",
    "see",
    "happens",
    "target",
    "directory",
    "got",
    "right",
    "brackets",
    "going",
    "work",
    "let",
    "find",
    "oh",
    "image",
    "path",
    "slash",
    "target",
    "directory",
    "get",
    "wrong",
    "oh",
    "need",
    "image",
    "path",
    "let",
    "put",
    "let",
    "put",
    "target",
    "directory",
    "go",
    "beautiful",
    "set",
    "target",
    "directory",
    "training",
    "go",
    "let",
    "list",
    "happens",
    "run",
    "function",
    "oh",
    "scanner",
    "yeah",
    "go",
    "three",
    "directory",
    "entries",
    "getting",
    "entry",
    "dot",
    "name",
    "everything",
    "training",
    "directory",
    "look",
    "training",
    "directory",
    "train",
    "one",
    "entry",
    "pizza",
    "one",
    "entry",
    "sushi",
    "one",
    "entry",
    "steak",
    "wonderful",
    "way",
    "get",
    "list",
    "class",
    "names",
    "could",
    "quite",
    "easily",
    "turn",
    "dictionary",
    "could",
    "exactly",
    "want",
    "want",
    "recreate",
    "done",
    "want",
    "recreate",
    "also",
    "done",
    "let",
    "take",
    "functionality",
    "let",
    "turn",
    "function",
    "right",
    "call",
    "going",
    "call",
    "def",
    "fine",
    "classes",
    "going",
    "say",
    "takes",
    "directory",
    "string",
    "going",
    "return",
    "imported",
    "typing",
    "python",
    "type",
    "imported",
    "tuple",
    "going",
    "return",
    "list",
    "list",
    "strings",
    "dictionary",
    "strings",
    "map",
    "integers",
    "beautiful",
    "let",
    "keep",
    "going",
    "want",
    "want",
    "function",
    "return",
    "given",
    "target",
    "directory",
    "want",
    "return",
    "two",
    "things",
    "seen",
    "get",
    "list",
    "directories",
    "target",
    "directory",
    "using",
    "os",
    "scanner",
    "let",
    "write",
    "finds",
    "classes",
    "class",
    "folder",
    "names",
    "target",
    "directory",
    "beautiful",
    "know",
    "going",
    "return",
    "list",
    "dictionary",
    "let",
    "step",
    "number",
    "one",
    "want",
    "get",
    "class",
    "names",
    "scanning",
    "target",
    "directory",
    "go",
    "classes",
    "going",
    "replicate",
    "functionality",
    "done",
    "given",
    "directory",
    "classes",
    "equals",
    "sorted",
    "entry",
    "dot",
    "name",
    "entry",
    "os",
    "scanner",
    "going",
    "pass",
    "target",
    "directory",
    "entry",
    "dot",
    "dirt",
    "going",
    "make",
    "sure",
    "directory",
    "well",
    "return",
    "classes",
    "see",
    "happens",
    "find",
    "classes",
    "let",
    "pass",
    "target",
    "directory",
    "training",
    "directory",
    "get",
    "beautiful",
    "need",
    "also",
    "return",
    "class",
    "id",
    "let",
    "keep",
    "going",
    "number",
    "two",
    "let",
    "go",
    "raise",
    "error",
    "class",
    "names",
    "could",
    "found",
    "classes",
    "let",
    "say",
    "raise",
    "file",
    "going",
    "raise",
    "file",
    "found",
    "error",
    "let",
    "write",
    "f",
    "could",
    "find",
    "classes",
    "directory",
    "writing",
    "error",
    "checking",
    "code",
    "ca",
    "find",
    "class",
    "list",
    "within",
    "target",
    "directory",
    "going",
    "raise",
    "error",
    "say",
    "could",
    "find",
    "classes",
    "directory",
    "please",
    "check",
    "file",
    "structure",
    "another",
    "checkup",
    "going",
    "help",
    "us",
    "well",
    "check",
    "entry",
    "directory",
    "finally",
    "let",
    "number",
    "three",
    "want",
    "want",
    "create",
    "dictionary",
    "index",
    "labels",
    "computers",
    "well",
    "computers",
    "prefer",
    "numbers",
    "rather",
    "strings",
    "labels",
    "already",
    "got",
    "list",
    "classes",
    "let",
    "create",
    "class",
    "id",
    "x",
    "equals",
    "class",
    "name",
    "class",
    "name",
    "enumerate",
    "classes",
    "let",
    "see",
    "looks",
    "like",
    "go",
    "class",
    "names",
    "class",
    "id",
    "x",
    "return",
    "actually",
    "spell",
    "enumerate",
    "role",
    "yes",
    "going",
    "going",
    "map",
    "class",
    "name",
    "integer",
    "class",
    "name",
    "enumerate",
    "classes",
    "going",
    "go",
    "going",
    "go",
    "first",
    "one",
    "zero",
    "going",
    "pizza",
    "ideally",
    "one",
    "steak",
    "two",
    "sushi",
    "let",
    "see",
    "goes",
    "beautiful",
    "look",
    "replicated",
    "functionality",
    "image",
    "folder",
    "use",
    "helper",
    "function",
    "custom",
    "data",
    "set",
    "find",
    "classes",
    "traverse",
    "target",
    "directory",
    "train",
    "could",
    "test",
    "wanted",
    "way",
    "got",
    "list",
    "classes",
    "also",
    "got",
    "dictionary",
    "mapping",
    "classes",
    "integers",
    "let",
    "next",
    "video",
    "move",
    "towards",
    "sub",
    "classing",
    "torch",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "set",
    "going",
    "fully",
    "replicate",
    "image",
    "folder",
    "see",
    "last",
    "video",
    "wrote",
    "great",
    "helper",
    "function",
    "called",
    "find",
    "classes",
    "takes",
    "target",
    "directory",
    "returns",
    "list",
    "classes",
    "dictionary",
    "mapping",
    "class",
    "names",
    "integer",
    "let",
    "move",
    "forward",
    "time",
    "going",
    "create",
    "custom",
    "data",
    "set",
    "replicate",
    "image",
    "folder",
    "necessarily",
    "right",
    "image",
    "folder",
    "already",
    "exists",
    "something",
    "already",
    "exists",
    "pie",
    "torch",
    "library",
    "chances",
    "going",
    "tested",
    "well",
    "going",
    "work",
    "efficiently",
    "use",
    "needed",
    "custom",
    "functionality",
    "always",
    "build",
    "custom",
    "data",
    "set",
    "sub",
    "classing",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "data",
    "set",
    "pre",
    "built",
    "data",
    "set",
    "function",
    "exist",
    "well",
    "probably",
    "going",
    "want",
    "subclass",
    "torch",
    "utils",
    "data",
    "dot",
    "data",
    "set",
    "anyway",
    "go",
    "documentation",
    "things",
    "need",
    "keep",
    "mind",
    "creating",
    "custom",
    "data",
    "set",
    "data",
    "sets",
    "represent",
    "map",
    "keys",
    "data",
    "samples",
    "want",
    "want",
    "map",
    "keys",
    "words",
    "targets",
    "labels",
    "data",
    "samples",
    "case",
    "food",
    "images",
    "subclass",
    "class",
    "note",
    "subclasses",
    "overwrite",
    "get",
    "item",
    "get",
    "item",
    "method",
    "python",
    "going",
    "get",
    "item",
    "get",
    "sample",
    "supporting",
    "fetching",
    "data",
    "sample",
    "given",
    "key",
    "example",
    "wanted",
    "get",
    "sample",
    "number",
    "100",
    "get",
    "item",
    "support",
    "return",
    "us",
    "sample",
    "number",
    "subclasses",
    "could",
    "also",
    "optionally",
    "override",
    "land",
    "length",
    "data",
    "set",
    "return",
    "size",
    "data",
    "set",
    "many",
    "sampler",
    "implementations",
    "default",
    "options",
    "data",
    "loader",
    "want",
    "use",
    "custom",
    "data",
    "set",
    "data",
    "loader",
    "later",
    "keep",
    "mind",
    "building",
    "custom",
    "subclasses",
    "torch",
    "utils",
    "data",
    "data",
    "set",
    "let",
    "see",
    "hands",
    "going",
    "break",
    "going",
    "fair",
    "bit",
    "code",
    "right",
    "nothing",
    "ca",
    "handle",
    "create",
    "custom",
    "data",
    "set",
    "want",
    "number",
    "one",
    "first",
    "things",
    "first",
    "going",
    "subclass",
    "subclass",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "set",
    "two",
    "want",
    "want",
    "init",
    "subclass",
    "target",
    "directory",
    "directory",
    "like",
    "get",
    "data",
    "well",
    "transform",
    "like",
    "transform",
    "data",
    "like",
    "used",
    "image",
    "folder",
    "could",
    "pass",
    "transform",
    "data",
    "set",
    "could",
    "transform",
    "data",
    "loading",
    "want",
    "thing",
    "want",
    "create",
    "several",
    "attributes",
    "let",
    "write",
    "want",
    "paths",
    "parts",
    "images",
    "else",
    "want",
    "want",
    "transform",
    "transform",
    "like",
    "use",
    "want",
    "classes",
    "going",
    "list",
    "target",
    "classes",
    "want",
    "class",
    "id",
    "x",
    "going",
    "dict",
    "target",
    "classes",
    "mapped",
    "integer",
    "labels",
    "course",
    "attributes",
    "differ",
    "depending",
    "data",
    "set",
    "replicating",
    "image",
    "folder",
    "things",
    "seen",
    "come",
    "image",
    "folder",
    "regardless",
    "data",
    "set",
    "working",
    "probably",
    "things",
    "want",
    "cross",
    "universal",
    "probably",
    "want",
    "paths",
    "data",
    "coming",
    "transforms",
    "like",
    "perform",
    "data",
    "classes",
    "working",
    "map",
    "classes",
    "index",
    "let",
    "keep",
    "pushing",
    "forward",
    "want",
    "create",
    "function",
    "load",
    "images",
    "want",
    "open",
    "images",
    "function",
    "open",
    "image",
    "number",
    "five",
    "want",
    "overwrite",
    "lan",
    "method",
    "return",
    "length",
    "data",
    "set",
    "like",
    "said",
    "documentation",
    "subclass",
    "using",
    "data",
    "set",
    "overwrite",
    "get",
    "item",
    "optionally",
    "overwrite",
    "lan",
    "going",
    "instead",
    "optionally",
    "going",
    "overwrite",
    "length",
    "number",
    "six",
    "want",
    "overwrite",
    "get",
    "item",
    "method",
    "return",
    "given",
    "sample",
    "passed",
    "index",
    "excellent",
    "got",
    "fair",
    "steps",
    "make",
    "sense",
    "okay",
    "let",
    "code",
    "remember",
    "motto",
    "doubt",
    "code",
    "doubt",
    "run",
    "code",
    "going",
    "write",
    "custom",
    "data",
    "set",
    "exciting",
    "work",
    "prebuilt",
    "data",
    "sets",
    "pretty",
    "cool",
    "machine",
    "learning",
    "write",
    "code",
    "create",
    "data",
    "sets",
    "well",
    "magic",
    "number",
    "one",
    "going",
    "number",
    "zero",
    "going",
    "import",
    "torch",
    "utils",
    "data",
    "set",
    "rewrite",
    "already",
    "imported",
    "going",
    "anyway",
    "completeness",
    "step",
    "number",
    "one",
    "subclass",
    "subclass",
    "torch",
    "utils",
    "data",
    "data",
    "set",
    "like",
    "built",
    "model",
    "going",
    "subclass",
    "module",
    "time",
    "going",
    "call",
    "us",
    "class",
    "image",
    "folder",
    "custom",
    "going",
    "inherit",
    "data",
    "set",
    "means",
    "functionality",
    "contained",
    "within",
    "torch",
    "utils",
    "data",
    "data",
    "set",
    "going",
    "get",
    "custom",
    "class",
    "number",
    "two",
    "let",
    "initialize",
    "going",
    "initialize",
    "custom",
    "data",
    "set",
    "things",
    "like",
    "subclass",
    "target",
    "directory",
    "directory",
    "like",
    "get",
    "data",
    "well",
    "transform",
    "like",
    "transform",
    "data",
    "let",
    "write",
    "knit",
    "function",
    "knit",
    "going",
    "go",
    "self",
    "target",
    "target",
    "going",
    "string",
    "going",
    "set",
    "transform",
    "set",
    "equal",
    "none",
    "beautiful",
    "way",
    "pass",
    "target",
    "directory",
    "images",
    "like",
    "load",
    "also",
    "pass",
    "transform",
    "similar",
    "transforms",
    "created",
    "previously",
    "number",
    "three",
    "create",
    "several",
    "attributes",
    "let",
    "see",
    "looks",
    "like",
    "create",
    "class",
    "attributes",
    "get",
    "image",
    "paths",
    "like",
    "done",
    "self",
    "paths",
    "equals",
    "list",
    "path",
    "lib",
    "dot",
    "path",
    "target",
    "directory",
    "going",
    "well",
    "give",
    "spoiler",
    "alert",
    "going",
    "path",
    "like",
    "test",
    "directory",
    "going",
    "train",
    "directory",
    "going",
    "use",
    "test",
    "directory",
    "train",
    "directory",
    "like",
    "use",
    "original",
    "image",
    "folder",
    "going",
    "go",
    "target",
    "directory",
    "find",
    "paths",
    "getting",
    "image",
    "paths",
    "support",
    "follow",
    "file",
    "name",
    "convention",
    "star",
    "star",
    "dot",
    "jpg",
    "look",
    "passed",
    "test",
    "folder",
    "test",
    "folder",
    "star",
    "would",
    "mean",
    "123",
    "pizza",
    "steak",
    "sushi",
    "first",
    "star",
    "slash",
    "would",
    "go",
    "pizza",
    "directory",
    "star",
    "would",
    "mean",
    "file",
    "combinations",
    "end",
    "dot",
    "jpg",
    "getting",
    "us",
    "list",
    "image",
    "paths",
    "within",
    "target",
    "directory",
    "words",
    "within",
    "test",
    "directory",
    "within",
    "train",
    "directory",
    "call",
    "two",
    "separately",
    "let",
    "keep",
    "going",
    "got",
    "image",
    "parts",
    "else",
    "want",
    "create",
    "transforms",
    "let",
    "set",
    "transforms",
    "self",
    "dot",
    "transforms",
    "equals",
    "transform",
    "oh",
    "call",
    "transform",
    "actually",
    "set",
    "transform",
    "equals",
    "transform",
    "going",
    "get",
    "put",
    "none",
    "transform",
    "optional",
    "let",
    "create",
    "classes",
    "class",
    "id",
    "x",
    "attributes",
    "next",
    "one",
    "list",
    "classes",
    "class",
    "id",
    "lucky",
    "us",
    "previous",
    "video",
    "created",
    "function",
    "return",
    "things",
    "let",
    "go",
    "self",
    "dot",
    "classes",
    "self",
    "dot",
    "class",
    "id",
    "x",
    "equals",
    "find",
    "classes",
    "going",
    "pass",
    "target",
    "der",
    "target",
    "der",
    "next",
    "done",
    "step",
    "number",
    "three",
    "need",
    "number",
    "four",
    "create",
    "function",
    "load",
    "images",
    "right",
    "let",
    "see",
    "looks",
    "like",
    "number",
    "four",
    "create",
    "function",
    "load",
    "images",
    "let",
    "call",
    "load",
    "image",
    "going",
    "pass",
    "self",
    "also",
    "pass",
    "index",
    "index",
    "image",
    "like",
    "load",
    "going",
    "return",
    "image",
    "dot",
    "image",
    "come",
    "well",
    "previously",
    "imported",
    "pill",
    "going",
    "use",
    "python",
    "image",
    "library",
    "pillow",
    "import",
    "images",
    "going",
    "give",
    "file",
    "path",
    "pizza",
    "going",
    "import",
    "image",
    "class",
    "using",
    "believe",
    "image",
    "dot",
    "open",
    "let",
    "give",
    "try",
    "write",
    "note",
    "opens",
    "image",
    "via",
    "path",
    "returns",
    "let",
    "write",
    "image",
    "path",
    "equals",
    "self",
    "got",
    "image",
    "paths",
    "self",
    "dot",
    "paths",
    "going",
    "index",
    "index",
    "beautiful",
    "let",
    "return",
    "image",
    "dot",
    "open",
    "image",
    "path",
    "going",
    "get",
    "particular",
    "image",
    "path",
    "going",
    "open",
    "step",
    "number",
    "five",
    "override",
    "land",
    "method",
    "return",
    "length",
    "data",
    "set",
    "optional",
    "going",
    "anyway",
    "overwrite",
    "len",
    "wants",
    "return",
    "many",
    "samples",
    "data",
    "set",
    "let",
    "write",
    "def",
    "len",
    "call",
    "len",
    "data",
    "set",
    "instance",
    "going",
    "return",
    "many",
    "numbers",
    "let",
    "write",
    "returns",
    "total",
    "number",
    "samples",
    "going",
    "simply",
    "return",
    "length",
    "len",
    "self",
    "dot",
    "paths",
    "target",
    "directory",
    "training",
    "directory",
    "return",
    "number",
    "image",
    "paths",
    "code",
    "found",
    "test",
    "directory",
    "next",
    "going",
    "go",
    "number",
    "six",
    "want",
    "overwrite",
    "put",
    "get",
    "item",
    "method",
    "required",
    "want",
    "subclass",
    "torch",
    "utils",
    "data",
    "data",
    "set",
    "documentation",
    "subclasses",
    "override",
    "get",
    "item",
    "want",
    "get",
    "item",
    "pass",
    "index",
    "data",
    "set",
    "want",
    "return",
    "particular",
    "item",
    "let",
    "see",
    "looks",
    "like",
    "override",
    "get",
    "item",
    "method",
    "return",
    "particular",
    "sample",
    "method",
    "going",
    "leverage",
    "get",
    "item",
    "code",
    "created",
    "going",
    "go",
    "take",
    "self",
    "class",
    "going",
    "take",
    "index",
    "integer",
    "going",
    "return",
    "tuple",
    "torch",
    "dot",
    "tensor",
    "integer",
    "thing",
    "gets",
    "returned",
    "index",
    "training",
    "data",
    "look",
    "image",
    "label",
    "equals",
    "train",
    "data",
    "zero",
    "get",
    "item",
    "going",
    "replicate",
    "pass",
    "index",
    "let",
    "check",
    "image",
    "label",
    "replicate",
    "remember",
    "train",
    "data",
    "created",
    "image",
    "folder",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "get",
    "item",
    "return",
    "image",
    "label",
    "tuple",
    "torch",
    "tensor",
    "image",
    "tensor",
    "label",
    "integer",
    "label",
    "particular",
    "index",
    "image",
    "relates",
    "let",
    "keep",
    "pushing",
    "forward",
    "going",
    "write",
    "returns",
    "one",
    "sample",
    "data",
    "data",
    "label",
    "x",
    "go",
    "xy",
    "know",
    "tuple",
    "beautiful",
    "let",
    "set",
    "image",
    "want",
    "image",
    "well",
    "going",
    "call",
    "self",
    "dot",
    "load",
    "image",
    "function",
    "created",
    "see",
    "customization",
    "capabilities",
    "creating",
    "class",
    "got",
    "fair",
    "bit",
    "code",
    "right",
    "essentially",
    "creating",
    "functions",
    "going",
    "help",
    "us",
    "load",
    "images",
    "way",
    "shape",
    "form",
    "ca",
    "stress",
    "enough",
    "regardless",
    "data",
    "working",
    "pattern",
    "quite",
    "similar",
    "change",
    "different",
    "functions",
    "use",
    "load",
    "data",
    "let",
    "load",
    "image",
    "particular",
    "index",
    "pass",
    "index",
    "going",
    "load",
    "image",
    "well",
    "want",
    "get",
    "class",
    "name",
    "going",
    "self",
    "dot",
    "paths",
    "get",
    "index",
    "go",
    "parent",
    "dot",
    "name",
    "expects",
    "path",
    "format",
    "data",
    "folder",
    "slash",
    "class",
    "name",
    "slash",
    "image",
    "dot",
    "jpg",
    "something",
    "aware",
    "class",
    "id",
    "x",
    "going",
    "self",
    "dot",
    "class",
    "id",
    "get",
    "class",
    "name",
    "image",
    "loading",
    "image",
    "class",
    "name",
    "data",
    "going",
    "data",
    "currently",
    "standard",
    "image",
    "classification",
    "format",
    "may",
    "change",
    "depending",
    "format",
    "data",
    "get",
    "class",
    "name",
    "get",
    "class",
    "id",
    "x",
    "indexing",
    "attribute",
    "dictionary",
    "class",
    "names",
    "indexes",
    "one",
    "small",
    "little",
    "step",
    "transform",
    "necessary",
    "remember",
    "transform",
    "parameter",
    "want",
    "transform",
    "target",
    "image",
    "well",
    "let",
    "put",
    "self",
    "dot",
    "transform",
    "transform",
    "exists",
    "let",
    "pass",
    "image",
    "transform",
    "transform",
    "image",
    "going",
    "also",
    "return",
    "class",
    "id",
    "notice",
    "returned",
    "tuple",
    "going",
    "torch",
    "tensor",
    "transform",
    "exists",
    "class",
    "id",
    "x",
    "also",
    "going",
    "returned",
    "want",
    "x",
    "gets",
    "returned",
    "image",
    "tensor",
    "label",
    "integer",
    "return",
    "data",
    "label",
    "x",
    "transform",
    "exist",
    "let",
    "return",
    "image",
    "class",
    "id",
    "x",
    "return",
    "untransformed",
    "image",
    "label",
    "beautiful",
    "fair",
    "bit",
    "code",
    "see",
    "pro",
    "subclassing",
    "torch",
    "utils",
    "data",
    "data",
    "set",
    "customize",
    "almost",
    "way",
    "wanted",
    "load",
    "whatever",
    "data",
    "working",
    "well",
    "almost",
    "data",
    "however",
    "written",
    "much",
    "code",
    "may",
    "prone",
    "errors",
    "going",
    "find",
    "next",
    "video",
    "see",
    "actually",
    "works",
    "essentially",
    "done",
    "followed",
    "documentation",
    "torch",
    "dot",
    "utils",
    "data",
    "dot",
    "data",
    "set",
    "replicate",
    "functionality",
    "existing",
    "data",
    "loader",
    "function",
    "namely",
    "image",
    "folder",
    "scroll",
    "back",
    "ideally",
    "done",
    "right",
    "able",
    "write",
    "code",
    "like",
    "passing",
    "root",
    "directory",
    "training",
    "directory",
    "particular",
    "data",
    "transform",
    "get",
    "similar",
    "instances",
    "image",
    "folder",
    "using",
    "custom",
    "data",
    "set",
    "class",
    "let",
    "try",
    "next",
    "video",
    "got",
    "custom",
    "image",
    "folder",
    "class",
    "replicates",
    "functionality",
    "original",
    "image",
    "folder",
    "data",
    "loader",
    "class",
    "data",
    "set",
    "class",
    "let",
    "test",
    "let",
    "see",
    "works",
    "custom",
    "data",
    "going",
    "create",
    "transform",
    "transform",
    "images",
    "raw",
    "jpeg",
    "images",
    "tenses",
    "whole",
    "goal",
    "importing",
    "data",
    "pytorch",
    "let",
    "set",
    "train",
    "transforms",
    "compose",
    "going",
    "set",
    "equal",
    "transforms",
    "dot",
    "compose",
    "going",
    "pass",
    "list",
    "going",
    "transforms",
    "going",
    "resize",
    "whatever",
    "image",
    "size",
    "reduce",
    "going",
    "go",
    "transforms",
    "dot",
    "random",
    "horizontal",
    "flip",
    "need",
    "necessarily",
    "flip",
    "going",
    "anyway",
    "see",
    "works",
    "let",
    "put",
    "transforms",
    "dot",
    "tensor",
    "images",
    "getting",
    "opened",
    "pill",
    "image",
    "using",
    "image",
    "dot",
    "open",
    "using",
    "transform",
    "transform",
    "pytorch",
    "torch",
    "visions",
    "dot",
    "transforms",
    "put",
    "torch",
    "vision",
    "dot",
    "transforms",
    "way",
    "know",
    "importing",
    "transforms",
    "let",
    "create",
    "one",
    "test",
    "data",
    "set",
    "well",
    "test",
    "transforms",
    "set",
    "oh",
    "excuse",
    "need",
    "go",
    "import",
    "transforms",
    "let",
    "go",
    "transforms",
    "dot",
    "compose",
    "pass",
    "another",
    "list",
    "going",
    "exact",
    "set",
    "resize",
    "set",
    "size",
    "equal",
    "transforms",
    "going",
    "go",
    "dot",
    "tensor",
    "going",
    "skip",
    "data",
    "augmentation",
    "test",
    "data",
    "typically",
    "manipulate",
    "test",
    "data",
    "terms",
    "data",
    "augmentation",
    "convert",
    "tensor",
    "rather",
    "manipulate",
    "orientation",
    "shape",
    "size",
    "etc",
    "etc",
    "let",
    "run",
    "let",
    "see",
    "image",
    "folder",
    "custom",
    "class",
    "works",
    "test",
    "image",
    "folder",
    "custom",
    "let",
    "go",
    "set",
    "train",
    "data",
    "custom",
    "equal",
    "image",
    "folder",
    "custom",
    "set",
    "target",
    "equal",
    "training",
    "directory",
    "pass",
    "transform",
    "equal",
    "train",
    "transforms",
    "created",
    "train",
    "transforms",
    "going",
    "think",
    "need",
    "actually",
    "two",
    "parameters",
    "going",
    "use",
    "target",
    "transform",
    "labels",
    "got",
    "help",
    "function",
    "transform",
    "labels",
    "test",
    "data",
    "custom",
    "going",
    "image",
    "folder",
    "custom",
    "going",
    "set",
    "target",
    "equal",
    "test",
    "directory",
    "transform",
    "going",
    "test",
    "transforms",
    "cell",
    "co",
    "lab",
    "telling",
    "oh",
    "going",
    "set",
    "spell",
    "something",
    "oh",
    "spelled",
    "wrong",
    "train",
    "transforms",
    "go",
    "beautiful",
    "let",
    "look",
    "train",
    "data",
    "test",
    "data",
    "custom",
    "see",
    "worked",
    "image",
    "folder",
    "custom",
    "well",
    "give",
    "us",
    "much",
    "rich",
    "information",
    "checking",
    "train",
    "data",
    "okay",
    "still",
    "inspect",
    "original",
    "one",
    "made",
    "image",
    "folder",
    "got",
    "train",
    "data",
    "custom",
    "test",
    "data",
    "custom",
    "let",
    "see",
    "get",
    "information",
    "let",
    "check",
    "original",
    "length",
    "train",
    "data",
    "see",
    "use",
    "land",
    "method",
    "train",
    "data",
    "custom",
    "work",
    "wonderful",
    "original",
    "test",
    "data",
    "made",
    "image",
    "folder",
    "custom",
    "version",
    "made",
    "test",
    "data",
    "image",
    "folder",
    "custom",
    "beautiful",
    "exactly",
    "want",
    "let",
    "look",
    "train",
    "data",
    "custom",
    "let",
    "see",
    "classes",
    "attribute",
    "comes",
    "dot",
    "classes",
    "leave",
    "class",
    "dot",
    "id",
    "yes",
    "attribute",
    "wonder",
    "get",
    "information",
    "google",
    "co",
    "lab",
    "loading",
    "get",
    "oh",
    "classes",
    "id",
    "x",
    "classes",
    "load",
    "image",
    "paths",
    "transform",
    "go",
    "back",
    "attributes",
    "paths",
    "transform",
    "classes",
    "class",
    "id",
    "x",
    "well",
    "load",
    "image",
    "coming",
    "code",
    "wrote",
    "custom",
    "data",
    "set",
    "class",
    "let",
    "keep",
    "pushing",
    "forward",
    "let",
    "look",
    "class",
    "id",
    "get",
    "wanted",
    "yes",
    "beautiful",
    "dictionary",
    "containing",
    "string",
    "names",
    "integer",
    "associations",
    "let",
    "check",
    "equality",
    "going",
    "check",
    "equality",
    "original",
    "image",
    "folder",
    "data",
    "set",
    "image",
    "folder",
    "custom",
    "data",
    "set",
    "kind",
    "already",
    "done",
    "let",
    "try",
    "let",
    "go",
    "print",
    "let",
    "go",
    "train",
    "data",
    "custom",
    "dot",
    "classes",
    "equal",
    "train",
    "oh",
    "want",
    "three",
    "equals",
    "train",
    "data",
    "original",
    "one",
    "classes",
    "also",
    "print",
    "let",
    "test",
    "data",
    "custom",
    "dot",
    "classes",
    "equal",
    "test",
    "data",
    "original",
    "one",
    "classes",
    "true",
    "true",
    "could",
    "try",
    "fact",
    "little",
    "exercise",
    "try",
    "compare",
    "others",
    "congratulations",
    "us",
    "replicated",
    "main",
    "functionality",
    "image",
    "folder",
    "data",
    "set",
    "class",
    "takeaways",
    "whatever",
    "data",
    "pytorch",
    "gives",
    "base",
    "data",
    "set",
    "class",
    "inherit",
    "write",
    "function",
    "class",
    "somehow",
    "interacts",
    "whatever",
    "data",
    "working",
    "case",
    "load",
    "image",
    "long",
    "override",
    "land",
    "method",
    "get",
    "item",
    "method",
    "return",
    "sort",
    "values",
    "well",
    "create",
    "data",
    "set",
    "loading",
    "function",
    "beautiful",
    "going",
    "help",
    "work",
    "custom",
    "data",
    "sets",
    "pytorch",
    "let",
    "keep",
    "pushing",
    "forward",
    "seen",
    "analytically",
    "custom",
    "data",
    "set",
    "quite",
    "similar",
    "original",
    "pytorch",
    "torch",
    "vision",
    "dot",
    "data",
    "sets",
    "image",
    "folder",
    "data",
    "set",
    "know",
    "like",
    "like",
    "visualize",
    "things",
    "let",
    "next",
    "video",
    "create",
    "function",
    "display",
    "random",
    "images",
    "trained",
    "data",
    "custom",
    "class",
    "time",
    "follow",
    "data",
    "explorer",
    "motto",
    "visualize",
    "visualize",
    "visualize",
    "let",
    "create",
    "another",
    "section",
    "going",
    "write",
    "title",
    "called",
    "create",
    "function",
    "display",
    "random",
    "images",
    "sure",
    "look",
    "different",
    "attributes",
    "custom",
    "data",
    "set",
    "see",
    "gives",
    "back",
    "list",
    "different",
    "class",
    "names",
    "see",
    "lengths",
    "similar",
    "original",
    "nothing",
    "quite",
    "like",
    "visualizing",
    "data",
    "let",
    "go",
    "going",
    "write",
    "function",
    "helper",
    "function",
    "step",
    "number",
    "one",
    "need",
    "take",
    "data",
    "set",
    "one",
    "data",
    "sets",
    "created",
    "whether",
    "trained",
    "data",
    "custom",
    "trained",
    "data",
    "number",
    "parameters",
    "class",
    "names",
    "many",
    "images",
    "visualize",
    "step",
    "number",
    "two",
    "prevent",
    "display",
    "getting",
    "hand",
    "let",
    "cap",
    "number",
    "images",
    "see",
    "look",
    "data",
    "set",
    "going",
    "thousands",
    "images",
    "want",
    "put",
    "number",
    "images",
    "look",
    "let",
    "make",
    "sure",
    "maximum",
    "enough",
    "set",
    "random",
    "seed",
    "reproducibility",
    "number",
    "four",
    "let",
    "get",
    "list",
    "random",
    "samples",
    "want",
    "random",
    "sample",
    "indexes",
    "get",
    "rid",
    "want",
    "target",
    "data",
    "set",
    "want",
    "take",
    "data",
    "set",
    "want",
    "count",
    "number",
    "images",
    "seeing",
    "want",
    "set",
    "random",
    "seed",
    "see",
    "much",
    "use",
    "randomness",
    "really",
    "get",
    "understanding",
    "data",
    "really",
    "really",
    "really",
    "love",
    "harnessing",
    "power",
    "randomness",
    "want",
    "get",
    "random",
    "sample",
    "indexes",
    "data",
    "set",
    "going",
    "set",
    "matplotlib",
    "plot",
    "want",
    "loop",
    "random",
    "sample",
    "images",
    "plot",
    "matplotlib",
    "side",
    "one",
    "step",
    "seven",
    "need",
    "make",
    "sure",
    "dimensions",
    "images",
    "line",
    "matplotlib",
    "matplotlib",
    "needs",
    "height",
    "width",
    "color",
    "channels",
    "right",
    "let",
    "take",
    "hey",
    "number",
    "one",
    "create",
    "function",
    "take",
    "data",
    "set",
    "going",
    "call",
    "def",
    "let",
    "call",
    "def",
    "display",
    "random",
    "images",
    "going",
    "one",
    "helper",
    "functions",
    "created",
    "type",
    "functions",
    "like",
    "let",
    "take",
    "data",
    "set",
    "torch",
    "utils",
    "type",
    "type",
    "data",
    "set",
    "going",
    "take",
    "classes",
    "going",
    "list",
    "different",
    "strings",
    "going",
    "class",
    "names",
    "whichever",
    "data",
    "set",
    "using",
    "going",
    "set",
    "equal",
    "none",
    "going",
    "take",
    "n",
    "number",
    "images",
    "like",
    "plot",
    "going",
    "set",
    "10",
    "default",
    "see",
    "10",
    "images",
    "time",
    "10",
    "random",
    "images",
    "want",
    "display",
    "shape",
    "let",
    "set",
    "equal",
    "true",
    "display",
    "shape",
    "images",
    "passing",
    "transform",
    "goes",
    "data",
    "set",
    "want",
    "see",
    "shape",
    "images",
    "make",
    "sure",
    "okay",
    "also",
    "let",
    "set",
    "seed",
    "going",
    "integer",
    "set",
    "none",
    "begin",
    "well",
    "okay",
    "step",
    "number",
    "two",
    "prevent",
    "display",
    "getting",
    "hand",
    "let",
    "cap",
    "number",
    "images",
    "see",
    "got",
    "n",
    "default",
    "going",
    "10",
    "let",
    "make",
    "sure",
    "stays",
    "adjust",
    "display",
    "n",
    "high",
    "n",
    "greater",
    "10",
    "let",
    "readjust",
    "let",
    "set",
    "n",
    "equal",
    "10",
    "display",
    "shape",
    "turn",
    "display",
    "shape",
    "10",
    "images",
    "display",
    "may",
    "get",
    "hand",
    "print",
    "display",
    "purposes",
    "larger",
    "10",
    "setting",
    "10",
    "removing",
    "shape",
    "display",
    "know",
    "experience",
    "cooking",
    "dish",
    "words",
    "written",
    "type",
    "code",
    "customize",
    "beautiful",
    "thing",
    "python",
    "pytorch",
    "customize",
    "display",
    "functions",
    "way",
    "see",
    "fit",
    "step",
    "number",
    "three",
    "set",
    "random",
    "seed",
    "reproducibility",
    "okay",
    "set",
    "seed",
    "seed",
    "let",
    "set",
    "random",
    "dot",
    "seed",
    "equal",
    "seed",
    "value",
    "keep",
    "keep",
    "going",
    "number",
    "four",
    "let",
    "get",
    "random",
    "sample",
    "indexes",
    "going",
    "get",
    "random",
    "sample",
    "indexes",
    "step",
    "number",
    "four",
    "got",
    "target",
    "data",
    "set",
    "want",
    "inspect",
    "want",
    "get",
    "random",
    "samples",
    "let",
    "create",
    "random",
    "samples",
    "idx",
    "list",
    "going",
    "randomly",
    "sample",
    "length",
    "data",
    "set",
    "sorry",
    "range",
    "length",
    "data",
    "set",
    "show",
    "means",
    "second",
    "k",
    "excuse",
    "got",
    "enough",
    "brackets",
    "always",
    "get",
    "confused",
    "brackets",
    "k",
    "going",
    "case",
    "want",
    "randomly",
    "sample",
    "10",
    "images",
    "length",
    "data",
    "set",
    "10",
    "indexes",
    "let",
    "look",
    "looks",
    "like",
    "put",
    "train",
    "data",
    "custom",
    "going",
    "take",
    "range",
    "length",
    "train",
    "data",
    "custom",
    "looked",
    "length",
    "zero",
    "255",
    "going",
    "get",
    "10",
    "indexes",
    "done",
    "correctly",
    "beautiful",
    "10",
    "random",
    "samples",
    "train",
    "data",
    "custom",
    "10",
    "random",
    "indexes",
    "step",
    "number",
    "five",
    "loop",
    "random",
    "sample",
    "images",
    "indexes",
    "let",
    "create",
    "indexes",
    "indexes",
    "plot",
    "matplotlib",
    "going",
    "give",
    "us",
    "list",
    "let",
    "go",
    "loop",
    "random",
    "indexes",
    "plot",
    "matplotlib",
    "beautiful",
    "tug",
    "sample",
    "enumerate",
    "let",
    "enumerate",
    "random",
    "random",
    "samples",
    "idx",
    "list",
    "going",
    "go",
    "tug",
    "image",
    "tug",
    "label",
    "samples",
    "target",
    "data",
    "set",
    "form",
    "tuples",
    "going",
    "get",
    "target",
    "image",
    "target",
    "label",
    "going",
    "data",
    "set",
    "tug",
    "sample",
    "take",
    "index",
    "might",
    "one",
    "values",
    "index",
    "zero",
    "index",
    "image",
    "go",
    "data",
    "set",
    "well",
    "take",
    "tug",
    "sample",
    "index",
    "index",
    "number",
    "one",
    "label",
    "target",
    "sample",
    "number",
    "seven",
    "oh",
    "excuse",
    "missed",
    "step",
    "number",
    "six",
    "catch",
    "number",
    "five",
    "setup",
    "plot",
    "quite",
    "easily",
    "going",
    "plot",
    "figure",
    "time",
    "iterate",
    "another",
    "sample",
    "going",
    "quite",
    "big",
    "figure",
    "set",
    "plot",
    "outside",
    "loop",
    "add",
    "plot",
    "original",
    "plot",
    "number",
    "seven",
    "make",
    "sure",
    "dimensions",
    "images",
    "line",
    "matplotlib",
    "recall",
    "default",
    "pytorch",
    "going",
    "turn",
    "image",
    "dimensions",
    "color",
    "channels",
    "first",
    "however",
    "matplotlib",
    "prefers",
    "color",
    "channels",
    "last",
    "let",
    "go",
    "adjust",
    "tensor",
    "dimensions",
    "plotting",
    "let",
    "go",
    "tag",
    "image",
    "let",
    "call",
    "tag",
    "image",
    "adjust",
    "equals",
    "tag",
    "image",
    "dot",
    "commute",
    "going",
    "alter",
    "order",
    "indexes",
    "going",
    "go",
    "color",
    "channels",
    "dimensions",
    "height",
    "width",
    "going",
    "change",
    "width",
    "could",
    "spell",
    "height",
    "width",
    "color",
    "channels",
    "beautiful",
    "one",
    "probably",
    "catch",
    "guard",
    "times",
    "seen",
    "couple",
    "times",
    "going",
    "keep",
    "going",
    "plot",
    "adjusted",
    "samples",
    "add",
    "subplot",
    "matplotlib",
    "plot",
    "want",
    "create",
    "want",
    "one",
    "row",
    "n",
    "images",
    "make",
    "lot",
    "sense",
    "visualize",
    "index",
    "going",
    "keep",
    "track",
    "plus",
    "one",
    "let",
    "keep",
    "going",
    "going",
    "go",
    "plot",
    "show",
    "going",
    "go",
    "tug",
    "image",
    "adjust",
    "going",
    "plot",
    "image",
    "let",
    "turn",
    "axis",
    "go",
    "classes",
    "variable",
    "exists",
    "list",
    "classes",
    "let",
    "adjust",
    "title",
    "plot",
    "particular",
    "index",
    "class",
    "list",
    "title",
    "equals",
    "f",
    "class",
    "going",
    "put",
    "classes",
    "going",
    "index",
    "target",
    "label",
    "index",
    "going",
    "come",
    "going",
    "new",
    "numerical",
    "format",
    "display",
    "shape",
    "let",
    "set",
    "title",
    "equal",
    "title",
    "plus",
    "going",
    "go",
    "new",
    "line",
    "shape",
    "going",
    "shape",
    "image",
    "tug",
    "image",
    "adjust",
    "dot",
    "shape",
    "set",
    "title",
    "plt",
    "dot",
    "title",
    "see",
    "display",
    "shape",
    "adjusting",
    "title",
    "variable",
    "created",
    "putting",
    "title",
    "onto",
    "plot",
    "let",
    "see",
    "goes",
    "quite",
    "beautiful",
    "function",
    "let",
    "pass",
    "one",
    "data",
    "sets",
    "see",
    "looks",
    "like",
    "let",
    "plot",
    "random",
    "images",
    "one",
    "start",
    "first",
    "let",
    "display",
    "random",
    "images",
    "image",
    "folder",
    "created",
    "data",
    "sets",
    "inbuilt",
    "pytorch",
    "image",
    "folder",
    "let",
    "go",
    "display",
    "random",
    "images",
    "function",
    "created",
    "going",
    "pass",
    "train",
    "data",
    "pass",
    "number",
    "images",
    "let",
    "look",
    "five",
    "classes",
    "going",
    "class",
    "names",
    "list",
    "different",
    "class",
    "names",
    "set",
    "seed",
    "want",
    "random",
    "set",
    "seed",
    "equal",
    "none",
    "oh",
    "look",
    "good",
    "original",
    "train",
    "data",
    "made",
    "image",
    "folder",
    "option",
    "number",
    "one",
    "option",
    "one",
    "go",
    "passed",
    "class",
    "name",
    "sushi",
    "resize",
    "64",
    "64",
    "three",
    "others",
    "different",
    "classes",
    "let",
    "set",
    "seed",
    "42",
    "see",
    "happens",
    "get",
    "images",
    "got",
    "sushi",
    "got",
    "pizza",
    "got",
    "pizza",
    "sushi",
    "pizza",
    "try",
    "different",
    "one",
    "go",
    "none",
    "get",
    "random",
    "images",
    "wonderful",
    "let",
    "write",
    "code",
    "time",
    "using",
    "train",
    "data",
    "custom",
    "data",
    "set",
    "display",
    "random",
    "images",
    "image",
    "folder",
    "custom",
    "data",
    "set",
    "one",
    "created",
    "display",
    "random",
    "images",
    "going",
    "pass",
    "train",
    "data",
    "custom",
    "data",
    "set",
    "oh",
    "exciting",
    "let",
    "set",
    "equal",
    "10",
    "see",
    "see",
    "far",
    "go",
    "plot",
    "maybe",
    "set",
    "20",
    "see",
    "code",
    "adjusting",
    "plot",
    "makes",
    "sense",
    "class",
    "names",
    "seed",
    "equals",
    "going",
    "put",
    "42",
    "time",
    "go",
    "display",
    "purposes",
    "larger",
    "10",
    "setting",
    "10",
    "removing",
    "shape",
    "display",
    "stake",
    "image",
    "pizza",
    "image",
    "pizza",
    "steak",
    "pizza",
    "pizza",
    "pizza",
    "pizza",
    "steak",
    "pizza",
    "turn",
    "random",
    "seed",
    "get",
    "another",
    "10",
    "random",
    "images",
    "beautiful",
    "look",
    "steak",
    "steak",
    "sushi",
    "pizza",
    "steak",
    "sushi",
    "class",
    "reading",
    "different",
    "things",
    "pizza",
    "pizza",
    "pizza",
    "pizza",
    "okay",
    "looks",
    "like",
    "custom",
    "data",
    "set",
    "working",
    "qualitative",
    "standpoint",
    "looking",
    "different",
    "images",
    "quantitative",
    "change",
    "five",
    "see",
    "looks",
    "like",
    "different",
    "shape",
    "yes",
    "shape",
    "wonderful",
    "okay",
    "got",
    "train",
    "data",
    "custom",
    "got",
    "train",
    "data",
    "made",
    "image",
    "folder",
    "premises",
    "remain",
    "built",
    "lot",
    "different",
    "ideas",
    "looking",
    "things",
    "different",
    "points",
    "view",
    "getting",
    "data",
    "folder",
    "structure",
    "tensor",
    "format",
    "still",
    "one",
    "step",
    "go",
    "data",
    "set",
    "data",
    "loader",
    "next",
    "video",
    "let",
    "see",
    "turn",
    "custom",
    "loaded",
    "images",
    "train",
    "data",
    "custom",
    "test",
    "data",
    "custom",
    "data",
    "loaders",
    "might",
    "want",
    "go",
    "ahead",
    "give",
    "try",
    "done",
    "turn",
    "loaded",
    "images",
    "data",
    "loaders",
    "going",
    "replicate",
    "thing",
    "option",
    "number",
    "two",
    "except",
    "time",
    "using",
    "custom",
    "data",
    "set",
    "see",
    "next",
    "video",
    "take",
    "good",
    "looking",
    "images",
    "even",
    "better",
    "custom",
    "data",
    "set",
    "got",
    "one",
    "step",
    "going",
    "turn",
    "data",
    "set",
    "data",
    "loader",
    "words",
    "going",
    "batchify",
    "images",
    "used",
    "model",
    "gave",
    "challenge",
    "trying",
    "last",
    "video",
    "hope",
    "gave",
    "go",
    "let",
    "see",
    "might",
    "look",
    "like",
    "going",
    "go",
    "let",
    "go",
    "call",
    "turn",
    "custom",
    "loaded",
    "images",
    "data",
    "loaders",
    "goes",
    "show",
    "write",
    "custom",
    "data",
    "set",
    "class",
    "still",
    "use",
    "pytorch",
    "data",
    "loader",
    "let",
    "go",
    "utils",
    "torch",
    "dot",
    "utils",
    "utils",
    "dot",
    "data",
    "import",
    "data",
    "loader",
    "get",
    "need",
    "completeness",
    "going",
    "set",
    "train",
    "data",
    "loader",
    "custom",
    "going",
    "create",
    "instance",
    "data",
    "loader",
    "inside",
    "going",
    "pass",
    "data",
    "set",
    "going",
    "train",
    "data",
    "custom",
    "going",
    "set",
    "universal",
    "parameter",
    "capitals",
    "batch",
    "size",
    "equals",
    "come",
    "set",
    "batch",
    "size",
    "going",
    "set",
    "equal",
    "words",
    "batch",
    "size",
    "parameter",
    "set",
    "set",
    "number",
    "workers",
    "well",
    "set",
    "zero",
    "let",
    "go",
    "see",
    "default",
    "actually",
    "torch",
    "utils",
    "data",
    "loader",
    "default",
    "number",
    "workers",
    "zero",
    "okay",
    "beautiful",
    "recall",
    "number",
    "workers",
    "going",
    "set",
    "many",
    "cores",
    "load",
    "data",
    "data",
    "loader",
    "generally",
    "higher",
    "better",
    "also",
    "experiment",
    "value",
    "see",
    "value",
    "suits",
    "model",
    "hardware",
    "best",
    "keep",
    "mind",
    "number",
    "workers",
    "going",
    "alter",
    "much",
    "compute",
    "hardware",
    "running",
    "code",
    "uses",
    "load",
    "data",
    "default",
    "set",
    "zero",
    "going",
    "shuffle",
    "training",
    "data",
    "wonderful",
    "let",
    "test",
    "data",
    "loader",
    "create",
    "test",
    "data",
    "loader",
    "custom",
    "going",
    "create",
    "new",
    "instance",
    "let",
    "make",
    "code",
    "cells",
    "data",
    "loader",
    "create",
    "data",
    "set",
    "pass",
    "data",
    "set",
    "parameter",
    "test",
    "data",
    "custom",
    "data",
    "sets",
    "created",
    "using",
    "custom",
    "data",
    "set",
    "class",
    "going",
    "set",
    "batch",
    "size",
    "equal",
    "batch",
    "size",
    "let",
    "set",
    "number",
    "workers",
    "equal",
    "zero",
    "previous",
    "video",
    "also",
    "set",
    "cpu",
    "count",
    "also",
    "set",
    "one",
    "hard",
    "code",
    "four",
    "depends",
    "hardware",
    "using",
    "like",
    "use",
    "opa",
    "os",
    "dot",
    "cpu",
    "count",
    "going",
    "shuffle",
    "test",
    "data",
    "false",
    "beautiful",
    "let",
    "look",
    "get",
    "train",
    "data",
    "loader",
    "custom",
    "test",
    "data",
    "loader",
    "custom",
    "actually",
    "going",
    "reset",
    "instead",
    "oos",
    "cpu",
    "count",
    "going",
    "put",
    "back",
    "zero",
    "got",
    "line",
    "one",
    "course",
    "numb",
    "workers",
    "could",
    "also",
    "set",
    "numb",
    "workers",
    "equals",
    "zero",
    "os",
    "dot",
    "cpu",
    "count",
    "could",
    "come",
    "set",
    "numb",
    "workers",
    "numb",
    "workers",
    "let",
    "look",
    "see",
    "works",
    "beautiful",
    "got",
    "two",
    "instances",
    "loader",
    "let",
    "get",
    "single",
    "sample",
    "train",
    "data",
    "loader",
    "make",
    "sure",
    "image",
    "shape",
    "batch",
    "size",
    "correct",
    "get",
    "image",
    "label",
    "custom",
    "data",
    "loader",
    "want",
    "image",
    "custom",
    "going",
    "go",
    "label",
    "custom",
    "equals",
    "next",
    "going",
    "iter",
    "train",
    "data",
    "loader",
    "custom",
    "let",
    "go",
    "print",
    "shapes",
    "want",
    "image",
    "custom",
    "dot",
    "shape",
    "label",
    "custom",
    "get",
    "shape",
    "beautiful",
    "go",
    "shape",
    "32",
    "batch",
    "size",
    "three",
    "color",
    "channels",
    "64",
    "64",
    "line",
    "line",
    "transform",
    "set",
    "way",
    "transform",
    "transform",
    "image",
    "may",
    "want",
    "change",
    "something",
    "different",
    "depending",
    "model",
    "using",
    "depending",
    "much",
    "data",
    "want",
    "comprised",
    "within",
    "image",
    "recall",
    "generally",
    "larger",
    "image",
    "size",
    "encodes",
    "information",
    "coming",
    "original",
    "image",
    "folder",
    "custom",
    "data",
    "set",
    "class",
    "look",
    "us",
    "go",
    "mean",
    "lot",
    "code",
    "fair",
    "bit",
    "code",
    "right",
    "could",
    "think",
    "like",
    "write",
    "data",
    "set",
    "continues",
    "format",
    "well",
    "use",
    "might",
    "put",
    "image",
    "folder",
    "custom",
    "helper",
    "function",
    "file",
    "data",
    "set",
    "dot",
    "pie",
    "something",
    "like",
    "could",
    "call",
    "future",
    "code",
    "instead",
    "rewriting",
    "time",
    "exactly",
    "pytorch",
    "done",
    "taught",
    "vision",
    "dot",
    "data",
    "sets",
    "dot",
    "image",
    "folder",
    "got",
    "shapes",
    "wanted",
    "change",
    "batch",
    "size",
    "change",
    "like",
    "remember",
    "good",
    "batch",
    "size",
    "also",
    "multiple",
    "eight",
    "going",
    "help",
    "computing",
    "batch",
    "size",
    "equals",
    "one",
    "get",
    "batch",
    "size",
    "equal",
    "one",
    "fair",
    "bit",
    "covered",
    "important",
    "thing",
    "loading",
    "data",
    "custom",
    "data",
    "set",
    "generally",
    "able",
    "load",
    "data",
    "existing",
    "data",
    "loading",
    "function",
    "data",
    "set",
    "function",
    "one",
    "torch",
    "domain",
    "libraries",
    "torch",
    "audio",
    "torch",
    "text",
    "torch",
    "vision",
    "torch",
    "rack",
    "later",
    "beta",
    "torch",
    "data",
    "need",
    "create",
    "custom",
    "one",
    "subclass",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "set",
    "add",
    "functionality",
    "let",
    "keep",
    "pushing",
    "forward",
    "previously",
    "touched",
    "little",
    "bit",
    "transforming",
    "data",
    "may",
    "heard",
    "say",
    "torch",
    "vision",
    "transforms",
    "used",
    "data",
    "augmentation",
    "documentation",
    "says",
    "data",
    "augmentation",
    "manipulating",
    "images",
    "way",
    "shape",
    "form",
    "artificially",
    "increase",
    "diversity",
    "training",
    "data",
    "set",
    "let",
    "look",
    "next",
    "video",
    "see",
    "last",
    "videos",
    "created",
    "functions",
    "classes",
    "load",
    "custom",
    "data",
    "set",
    "learned",
    "one",
    "biggest",
    "steps",
    "loading",
    "custom",
    "data",
    "set",
    "transforming",
    "data",
    "particularly",
    "turning",
    "target",
    "data",
    "tenses",
    "also",
    "brief",
    "look",
    "torch",
    "vision",
    "transforms",
    "module",
    "saw",
    "fair",
    "different",
    "ways",
    "transform",
    "data",
    "one",
    "ways",
    "transform",
    "image",
    "data",
    "augmentation",
    "went",
    "illustration",
    "transforms",
    "let",
    "look",
    "different",
    "ways",
    "got",
    "resize",
    "going",
    "change",
    "size",
    "original",
    "image",
    "got",
    "center",
    "crop",
    "crop",
    "got",
    "five",
    "crop",
    "got",
    "grayscale",
    "got",
    "random",
    "transforms",
    "got",
    "gaussian",
    "blur",
    "got",
    "random",
    "rotation",
    "random",
    "caffeine",
    "random",
    "crop",
    "could",
    "keep",
    "going",
    "fact",
    "encourage",
    "check",
    "different",
    "options",
    "oh",
    "auto",
    "augment",
    "wonderful",
    "random",
    "augment",
    "hinting",
    "data",
    "augmentation",
    "notice",
    "original",
    "image",
    "gets",
    "augmented",
    "different",
    "ways",
    "gets",
    "artificially",
    "changed",
    "gets",
    "rotated",
    "little",
    "gets",
    "dark",
    "little",
    "maybe",
    "brightened",
    "depending",
    "look",
    "gets",
    "shifted",
    "colors",
    "kind",
    "change",
    "process",
    "known",
    "data",
    "augmentation",
    "hinted",
    "going",
    "create",
    "another",
    "section",
    "number",
    "six",
    "forms",
    "transforms",
    "data",
    "augmentation",
    "could",
    "find",
    "data",
    "augmentation",
    "well",
    "could",
    "go",
    "data",
    "augmentation",
    "sure",
    "going",
    "plenty",
    "resources",
    "wikipedia",
    "go",
    "data",
    "augmentation",
    "data",
    "analysis",
    "techniques",
    "used",
    "increase",
    "amount",
    "data",
    "adding",
    "slightly",
    "modified",
    "copies",
    "already",
    "existing",
    "data",
    "newly",
    "created",
    "synthetic",
    "data",
    "existing",
    "data",
    "going",
    "write",
    "data",
    "augmentation",
    "process",
    "artificially",
    "adding",
    "diversity",
    "training",
    "data",
    "case",
    "image",
    "data",
    "may",
    "mean",
    "applying",
    "various",
    "image",
    "transformations",
    "training",
    "images",
    "saw",
    "whole",
    "bunch",
    "torch",
    "vision",
    "transformed",
    "package",
    "let",
    "look",
    "one",
    "type",
    "data",
    "augmentation",
    "particular",
    "trivial",
    "augment",
    "illustrate",
    "got",
    "slide",
    "ready",
    "go",
    "got",
    "data",
    "augmentation",
    "looking",
    "image",
    "different",
    "perspectives",
    "said",
    "artificially",
    "increase",
    "diversity",
    "data",
    "set",
    "imagine",
    "original",
    "images",
    "left",
    "wanted",
    "rotate",
    "could",
    "apply",
    "rotation",
    "transform",
    "wanted",
    "shift",
    "vertical",
    "horizontal",
    "axis",
    "could",
    "apply",
    "shift",
    "transform",
    "wanted",
    "zoom",
    "image",
    "could",
    "apply",
    "zoom",
    "transform",
    "many",
    "different",
    "types",
    "transforms",
    "got",
    "note",
    "many",
    "different",
    "kinds",
    "data",
    "augmentation",
    "cropping",
    "replacing",
    "shearing",
    "slide",
    "demonstrates",
    "like",
    "highlight",
    "another",
    "type",
    "data",
    "augmentation",
    "one",
    "used",
    "recently",
    "train",
    "pytorch",
    "torch",
    "vision",
    "image",
    "models",
    "state",
    "art",
    "levels",
    "let",
    "take",
    "look",
    "one",
    "particular",
    "type",
    "data",
    "augmentation",
    "used",
    "train",
    "pytorch",
    "vision",
    "models",
    "state",
    "art",
    "levels",
    "case",
    "sure",
    "might",
    "would",
    "like",
    "increase",
    "diversity",
    "training",
    "data",
    "images",
    "become",
    "harder",
    "model",
    "learn",
    "gets",
    "chance",
    "view",
    "image",
    "different",
    "perspectives",
    "use",
    "image",
    "classification",
    "model",
    "practice",
    "seen",
    "sort",
    "images",
    "many",
    "different",
    "angles",
    "hopefully",
    "learns",
    "patterns",
    "generalizable",
    "different",
    "angles",
    "practice",
    "hopefully",
    "results",
    "model",
    "generalizable",
    "unseen",
    "data",
    "go",
    "torch",
    "vision",
    "state",
    "art",
    "go",
    "recent",
    "blog",
    "post",
    "pytorch",
    "team",
    "train",
    "state",
    "art",
    "models",
    "want",
    "state",
    "art",
    "means",
    "best",
    "business",
    "otherwise",
    "known",
    "soda",
    "might",
    "see",
    "acronym",
    "quite",
    "often",
    "using",
    "torch",
    "visions",
    "latest",
    "primitives",
    "torch",
    "vision",
    "package",
    "using",
    "work",
    "vision",
    "data",
    "torch",
    "vision",
    "bunch",
    "primitives",
    "words",
    "functions",
    "help",
    "us",
    "train",
    "really",
    "good",
    "performing",
    "models",
    "blog",
    "post",
    "jump",
    "blog",
    "post",
    "scroll",
    "got",
    "improvements",
    "original",
    "resnet",
    "50",
    "model",
    "resnet",
    "50",
    "common",
    "computer",
    "vision",
    "architecture",
    "accuracy",
    "one",
    "well",
    "let",
    "say",
    "get",
    "boost",
    "previous",
    "results",
    "scroll",
    "type",
    "data",
    "augmentation",
    "add",
    "improvements",
    "used",
    "whole",
    "bunch",
    "extra",
    "curriculum",
    "encourage",
    "look",
    "improvements",
    "going",
    "get",
    "first",
    "go",
    "right",
    "blog",
    "posts",
    "like",
    "come",
    "time",
    "recipes",
    "continually",
    "changing",
    "even",
    "though",
    "showing",
    "may",
    "change",
    "future",
    "scroll",
    "see",
    "table",
    "showed",
    "us",
    "previous",
    "results",
    "look",
    "like",
    "oh",
    "baseline",
    "76",
    "little",
    "additions",
    "got",
    "right",
    "nearly",
    "nearly",
    "boost",
    "5",
    "accuracy",
    "pretty",
    "good",
    "going",
    "look",
    "trivial",
    "augment",
    "bunch",
    "different",
    "things",
    "learning",
    "rate",
    "optimization",
    "training",
    "longer",
    "ways",
    "improve",
    "model",
    "random",
    "erasing",
    "image",
    "data",
    "label",
    "smoothing",
    "add",
    "parameter",
    "loss",
    "functions",
    "cross",
    "entropy",
    "loss",
    "mix",
    "cut",
    "mix",
    "weight",
    "decay",
    "tuning",
    "fixed",
    "res",
    "mitigations",
    "exponential",
    "moving",
    "average",
    "ema",
    "inference",
    "resize",
    "tuning",
    "whole",
    "bunch",
    "different",
    "recipe",
    "items",
    "going",
    "focus",
    "going",
    "break",
    "let",
    "look",
    "trivial",
    "augment",
    "come",
    "let",
    "look",
    "trivial",
    "augment",
    "wanted",
    "look",
    "trivial",
    "augment",
    "find",
    "oh",
    "yes",
    "right",
    "trivial",
    "augment",
    "see",
    "pass",
    "image",
    "trivial",
    "augment",
    "going",
    "change",
    "different",
    "ways",
    "go",
    "let",
    "write",
    "let",
    "see",
    "action",
    "data",
    "import",
    "torch",
    "vision",
    "import",
    "transforms",
    "going",
    "create",
    "train",
    "transform",
    "equal",
    "transforms",
    "dot",
    "compose",
    "pass",
    "going",
    "similar",
    "done",
    "terms",
    "composing",
    "transform",
    "want",
    "well",
    "let",
    "say",
    "wanted",
    "resize",
    "one",
    "images",
    "image",
    "going",
    "transform",
    "let",
    "change",
    "size",
    "224224",
    "common",
    "size",
    "image",
    "classification",
    "going",
    "go",
    "transforms",
    "going",
    "pass",
    "trivial",
    "augment",
    "wide",
    "parameter",
    "number",
    "magnitude",
    "bins",
    "basically",
    "number",
    "0",
    "31",
    "31",
    "max",
    "intense",
    "want",
    "augmentation",
    "happen",
    "say",
    "put",
    "5",
    "augmentation",
    "would",
    "intensity",
    "0",
    "case",
    "maximum",
    "would",
    "intense",
    "put",
    "31",
    "going",
    "max",
    "intensity",
    "mean",
    "intensity",
    "say",
    "rotation",
    "go",
    "scale",
    "0",
    "31",
    "may",
    "10",
    "whereas",
    "31",
    "would",
    "completely",
    "rotating",
    "others",
    "right",
    "lower",
    "number",
    "less",
    "maximum",
    "bound",
    "applied",
    "transform",
    "go",
    "transforms",
    "dot",
    "tensor",
    "wonderful",
    "implemented",
    "trivial",
    "augment",
    "beautiful",
    "pytorch",
    "torch",
    "vision",
    "transforms",
    "library",
    "got",
    "trivial",
    "augment",
    "wide",
    "used",
    "trivial",
    "augment",
    "train",
    "latest",
    "state",
    "art",
    "vision",
    "models",
    "pytorch",
    "torch",
    "vision",
    "models",
    "library",
    "models",
    "repository",
    "wanted",
    "look",
    "trivial",
    "augment",
    "could",
    "find",
    "could",
    "search",
    "paper",
    "like",
    "read",
    "oh",
    "implemented",
    "actually",
    "would",
    "say",
    "let",
    "say",
    "trivial",
    "augment",
    "want",
    "say",
    "simple",
    "want",
    "downplay",
    "trivial",
    "augment",
    "leverages",
    "power",
    "randomness",
    "quite",
    "beautifully",
    "let",
    "read",
    "would",
    "rather",
    "try",
    "data",
    "visualize",
    "first",
    "test",
    "transform",
    "let",
    "go",
    "transforms",
    "compose",
    "might",
    "question",
    "transforms",
    "use",
    "data",
    "well",
    "million",
    "dollar",
    "question",
    "right",
    "thing",
    "asking",
    "model",
    "use",
    "data",
    "fair",
    "different",
    "answers",
    "best",
    "answer",
    "try",
    "see",
    "work",
    "people",
    "like",
    "done",
    "finding",
    "trivial",
    "augment",
    "worked",
    "well",
    "pytorch",
    "team",
    "try",
    "problems",
    "works",
    "well",
    "excellent",
    "work",
    "well",
    "well",
    "always",
    "excuse",
    "got",
    "spelling",
    "mistake",
    "work",
    "well",
    "well",
    "always",
    "set",
    "experiment",
    "try",
    "something",
    "else",
    "let",
    "test",
    "augmentation",
    "pipeline",
    "get",
    "image",
    "paths",
    "already",
    "done",
    "going",
    "anyway",
    "reiterate",
    "covered",
    "fair",
    "bit",
    "might",
    "rehash",
    "things",
    "going",
    "get",
    "list",
    "image",
    "path",
    "let",
    "show",
    "image",
    "path",
    "want",
    "get",
    "images",
    "within",
    "file",
    "go",
    "image",
    "path",
    "dot",
    "glob",
    "glob",
    "together",
    "files",
    "folders",
    "match",
    "pattern",
    "check",
    "get",
    "check",
    "first",
    "beautiful",
    "leverage",
    "function",
    "four",
    "plot",
    "random",
    "images",
    "plot",
    "random",
    "images",
    "pass",
    "plot",
    "transformed",
    "random",
    "transformed",
    "images",
    "want",
    "let",
    "see",
    "looks",
    "like",
    "goes",
    "trivial",
    "augment",
    "image",
    "paths",
    "equals",
    "image",
    "part",
    "list",
    "function",
    "created",
    "way",
    "transform",
    "equals",
    "train",
    "transform",
    "transform",
    "created",
    "contains",
    "trivial",
    "augment",
    "going",
    "put",
    "n",
    "equals",
    "three",
    "five",
    "images",
    "seed",
    "equals",
    "none",
    "plot",
    "oh",
    "sorry",
    "n",
    "equals",
    "three",
    "three",
    "images",
    "five",
    "beautiful",
    "set",
    "seed",
    "equals",
    "none",
    "way",
    "look",
    "got",
    "class",
    "pizza",
    "trivial",
    "augment",
    "resized",
    "quite",
    "sure",
    "transform",
    "per",
    "se",
    "maybe",
    "got",
    "little",
    "bit",
    "darker",
    "one",
    "looks",
    "like",
    "colors",
    "manipulated",
    "way",
    "shape",
    "form",
    "one",
    "looks",
    "like",
    "resized",
    "much",
    "happened",
    "one",
    "perspective",
    "go",
    "let",
    "look",
    "another",
    "three",
    "images",
    "trivial",
    "augment",
    "works",
    "said",
    "harnesses",
    "power",
    "randomness",
    "kind",
    "selects",
    "randomly",
    "augmentation",
    "types",
    "applies",
    "level",
    "intensity",
    "ones",
    "trivial",
    "augment",
    "going",
    "select",
    "summit",
    "random",
    "apply",
    "random",
    "intensity",
    "zero",
    "31",
    "set",
    "data",
    "course",
    "read",
    "little",
    "bit",
    "documentation",
    "sorry",
    "paper",
    "like",
    "see",
    "happening",
    "one",
    "looks",
    "like",
    "cut",
    "little",
    "bit",
    "one",
    "colors",
    "changed",
    "way",
    "shape",
    "form",
    "one",
    "darkened",
    "see",
    "artificially",
    "adding",
    "diversity",
    "training",
    "data",
    "set",
    "instead",
    "images",
    "one",
    "perspective",
    "like",
    "adding",
    "bunch",
    "different",
    "angles",
    "telling",
    "model",
    "hey",
    "got",
    "try",
    "still",
    "learn",
    "patterns",
    "even",
    "manipulated",
    "try",
    "one",
    "look",
    "one",
    "pretty",
    "manipulated",
    "still",
    "image",
    "stake",
    "trying",
    "get",
    "model",
    "still",
    "recognize",
    "image",
    "image",
    "stake",
    "even",
    "though",
    "manipulated",
    "bit",
    "work",
    "hey",
    "might",
    "might",
    "nature",
    "experimentation",
    "play",
    "around",
    "would",
    "encourage",
    "go",
    "transforms",
    "documentation",
    "like",
    "done",
    "illustrations",
    "change",
    "one",
    "trivial",
    "augment",
    "wine",
    "another",
    "type",
    "augmentation",
    "find",
    "see",
    "images",
    "randomly",
    "highlighted",
    "trivial",
    "augment",
    "pytorch",
    "team",
    "used",
    "recent",
    "blog",
    "post",
    "training",
    "recipe",
    "train",
    "vision",
    "models",
    "speaking",
    "training",
    "models",
    "let",
    "move",
    "forward",
    "got",
    "build",
    "first",
    "model",
    "section",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "pytorch",
    "team",
    "used",
    "trivial",
    "augment",
    "wide",
    "latest",
    "data",
    "augmentation",
    "time",
    "recording",
    "video",
    "train",
    "latest",
    "computer",
    "vision",
    "models",
    "within",
    "torch",
    "vision",
    "saw",
    "easily",
    "could",
    "apply",
    "trivial",
    "augment",
    "thanks",
    "torch",
    "vision",
    "dot",
    "transforms",
    "see",
    "one",
    "action",
    "highlight",
    "going",
    "look",
    "like",
    "much",
    "happened",
    "image",
    "augmented",
    "see",
    "one",
    "moved",
    "got",
    "black",
    "space",
    "one",
    "rotated",
    "little",
    "got",
    "black",
    "space",
    "time",
    "us",
    "build",
    "first",
    "computer",
    "vision",
    "model",
    "custom",
    "data",
    "set",
    "let",
    "get",
    "started",
    "going",
    "go",
    "model",
    "zero",
    "going",
    "reuse",
    "tiny",
    "vgg",
    "architecture",
    "covered",
    "computer",
    "vision",
    "section",
    "first",
    "experiment",
    "going",
    "going",
    "build",
    "baseline",
    "model",
    "zero",
    "going",
    "build",
    "without",
    "data",
    "augmentation",
    "rather",
    "use",
    "trivial",
    "augment",
    "got",
    "pytorch",
    "team",
    "used",
    "train",
    "computer",
    "vision",
    "models",
    "going",
    "start",
    "training",
    "computer",
    "vision",
    "model",
    "without",
    "data",
    "augmentation",
    "later",
    "try",
    "one",
    "see",
    "data",
    "augmentation",
    "see",
    "helps",
    "let",
    "put",
    "link",
    "cnn",
    "explainer",
    "model",
    "architecture",
    "covered",
    "depth",
    "last",
    "section",
    "going",
    "go",
    "spend",
    "much",
    "time",
    "know",
    "going",
    "input",
    "64",
    "64",
    "3",
    "multiple",
    "different",
    "layers",
    "convolutional",
    "layers",
    "relio",
    "layers",
    "max",
    "pool",
    "layers",
    "going",
    "output",
    "layer",
    "suits",
    "number",
    "classes",
    "case",
    "10",
    "different",
    "classes",
    "case",
    "three",
    "different",
    "classes",
    "one",
    "pizza",
    "steak",
    "sushi",
    "let",
    "replicate",
    "tiny",
    "vgg",
    "architecture",
    "cnn",
    "explainer",
    "website",
    "going",
    "good",
    "practice",
    "right",
    "going",
    "spend",
    "much",
    "time",
    "referencing",
    "architecture",
    "going",
    "spend",
    "time",
    "coding",
    "course",
    "train",
    "model",
    "well",
    "let",
    "go",
    "going",
    "create",
    "transforms",
    "loading",
    "data",
    "going",
    "load",
    "data",
    "model",
    "zero",
    "could",
    "course",
    "use",
    "variables",
    "already",
    "loaded",
    "going",
    "recreate",
    "practice",
    "let",
    "create",
    "simple",
    "transform",
    "whole",
    "premise",
    "loading",
    "data",
    "model",
    "zero",
    "want",
    "get",
    "data",
    "data",
    "folder",
    "pizza",
    "steak",
    "sushi",
    "training",
    "test",
    "folders",
    "respective",
    "folders",
    "want",
    "load",
    "images",
    "turn",
    "tenses",
    "done",
    "times",
    "one",
    "ways",
    "creating",
    "transform",
    "equals",
    "transforms",
    "dot",
    "compose",
    "going",
    "pass",
    "let",
    "resize",
    "transforms",
    "dot",
    "resize",
    "going",
    "resize",
    "images",
    "size",
    "tiny",
    "vgg",
    "architecture",
    "cnn",
    "explainer",
    "website",
    "64",
    "64",
    "three",
    "also",
    "going",
    "pass",
    "another",
    "transform",
    "tensor",
    "images",
    "get",
    "resized",
    "64",
    "get",
    "converted",
    "tenses",
    "particularly",
    "values",
    "within",
    "tensor",
    "going",
    "zero",
    "one",
    "transform",
    "going",
    "load",
    "data",
    "want",
    "pause",
    "video",
    "try",
    "load",
    "encourage",
    "try",
    "option",
    "one",
    "loading",
    "image",
    "data",
    "using",
    "image",
    "folder",
    "class",
    "turn",
    "data",
    "set",
    "image",
    "folder",
    "data",
    "set",
    "data",
    "loader",
    "batchify",
    "use",
    "pytorch",
    "model",
    "give",
    "shot",
    "otherwise",
    "let",
    "go",
    "ahead",
    "together",
    "one",
    "going",
    "load",
    "transform",
    "data",
    "done",
    "let",
    "rehash",
    "torch",
    "vision",
    "import",
    "data",
    "sets",
    "going",
    "create",
    "train",
    "data",
    "simple",
    "call",
    "simple",
    "going",
    "use",
    "first",
    "simple",
    "transform",
    "one",
    "data",
    "augmentation",
    "later",
    "another",
    "modeling",
    "experiment",
    "going",
    "create",
    "another",
    "transform",
    "one",
    "data",
    "augmentation",
    "let",
    "put",
    "data",
    "sets",
    "image",
    "folder",
    "let",
    "go",
    "route",
    "equals",
    "training",
    "directory",
    "transform",
    "going",
    "going",
    "simple",
    "transform",
    "got",
    "put",
    "test",
    "data",
    "simple",
    "going",
    "create",
    "data",
    "sets",
    "dot",
    "image",
    "folder",
    "going",
    "pass",
    "route",
    "test",
    "directory",
    "pass",
    "transform",
    "going",
    "simple",
    "transform",
    "performing",
    "transformation",
    "training",
    "data",
    "testing",
    "data",
    "next",
    "step",
    "well",
    "turn",
    "data",
    "sets",
    "data",
    "loaders",
    "let",
    "try",
    "first",
    "going",
    "import",
    "os",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "going",
    "import",
    "data",
    "loader",
    "going",
    "set",
    "batch",
    "size",
    "number",
    "workers",
    "let",
    "go",
    "batch",
    "size",
    "going",
    "use",
    "batch",
    "size",
    "32",
    "first",
    "model",
    "numb",
    "workers",
    "number",
    "excuse",
    "got",
    "typo",
    "classic",
    "number",
    "workers",
    "number",
    "cpu",
    "cores",
    "dedicate",
    "towards",
    "loading",
    "data",
    "let",
    "create",
    "data",
    "loaders",
    "going",
    "create",
    "train",
    "data",
    "loader",
    "simple",
    "equal",
    "data",
    "loader",
    "data",
    "set",
    "goes",
    "train",
    "data",
    "simple",
    "set",
    "batch",
    "size",
    "equal",
    "batch",
    "size",
    "parameter",
    "created",
    "hyper",
    "parameter",
    "recall",
    "hyper",
    "parameter",
    "something",
    "set",
    "would",
    "like",
    "shuffle",
    "training",
    "data",
    "going",
    "set",
    "numb",
    "workers",
    "equal",
    "numb",
    "workers",
    "case",
    "many",
    "calls",
    "google",
    "colab",
    "let",
    "run",
    "find",
    "many",
    "numb",
    "workers",
    "think",
    "going",
    "two",
    "cpus",
    "wonderful",
    "going",
    "thing",
    "test",
    "data",
    "loader",
    "test",
    "data",
    "loader",
    "simple",
    "going",
    "go",
    "data",
    "loader",
    "pass",
    "data",
    "set",
    "going",
    "test",
    "data",
    "simple",
    "going",
    "go",
    "batch",
    "size",
    "equals",
    "batch",
    "size",
    "going",
    "shuffle",
    "test",
    "data",
    "set",
    "numb",
    "workers",
    "set",
    "thing",
    "got",
    "beautiful",
    "hope",
    "gave",
    "shot",
    "see",
    "quickly",
    "get",
    "data",
    "loaded",
    "right",
    "format",
    "know",
    "spent",
    "lot",
    "time",
    "going",
    "steps",
    "multiple",
    "videos",
    "writing",
    "lots",
    "code",
    "quickly",
    "get",
    "set",
    "load",
    "data",
    "create",
    "simple",
    "transform",
    "load",
    "transform",
    "data",
    "time",
    "turn",
    "data",
    "sets",
    "data",
    "loaders",
    "like",
    "ready",
    "use",
    "data",
    "loaders",
    "model",
    "speaking",
    "models",
    "build",
    "tiny",
    "vgg",
    "architecture",
    "next",
    "video",
    "fact",
    "already",
    "done",
    "notebook",
    "number",
    "three",
    "want",
    "refer",
    "back",
    "model",
    "built",
    "right",
    "model",
    "number",
    "two",
    "want",
    "refer",
    "back",
    "section",
    "give",
    "go",
    "encourage",
    "otherwise",
    "build",
    "tiny",
    "vgg",
    "architecture",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "got",
    "set",
    "starting",
    "get",
    "ready",
    "model",
    "first",
    "custom",
    "data",
    "set",
    "issued",
    "challenge",
    "try",
    "replicate",
    "tiny",
    "vgg",
    "architecture",
    "cnn",
    "explainer",
    "website",
    "covered",
    "notebook",
    "number",
    "three",
    "let",
    "see",
    "fast",
    "together",
    "hey",
    "going",
    "write",
    "section",
    "seven",
    "point",
    "two",
    "know",
    "already",
    "coded",
    "good",
    "practice",
    "see",
    "like",
    "build",
    "pytorch",
    "models",
    "scratch",
    "create",
    "tiny",
    "vgg",
    "model",
    "class",
    "model",
    "going",
    "come",
    "previously",
    "created",
    "model",
    "would",
    "one",
    "big",
    "change",
    "model",
    "created",
    "section",
    "number",
    "three",
    "model",
    "section",
    "number",
    "three",
    "used",
    "black",
    "white",
    "images",
    "images",
    "going",
    "color",
    "images",
    "going",
    "three",
    "color",
    "channels",
    "rather",
    "one",
    "might",
    "little",
    "bit",
    "trick",
    "find",
    "shape",
    "later",
    "classifier",
    "layer",
    "let",
    "get",
    "started",
    "got",
    "class",
    "tiny",
    "vgg",
    "going",
    "inherit",
    "going",
    "model",
    "architecture",
    "copying",
    "tiny",
    "vgg",
    "cnn",
    "explainer",
    "remember",
    "quite",
    "common",
    "practice",
    "machine",
    "learning",
    "find",
    "model",
    "works",
    "problem",
    "similar",
    "copy",
    "try",
    "problem",
    "want",
    "two",
    "underscores",
    "going",
    "initialize",
    "class",
    "going",
    "give",
    "input",
    "shape",
    "int",
    "going",
    "say",
    "many",
    "hidden",
    "units",
    "want",
    "also",
    "int",
    "going",
    "output",
    "shape",
    "int",
    "well",
    "going",
    "return",
    "something",
    "none",
    "type",
    "none",
    "go",
    "initialize",
    "super",
    "dot",
    "underscore",
    "init",
    "beautiful",
    "let",
    "create",
    "first",
    "com",
    "block",
    "com",
    "block",
    "one",
    "recall",
    "section",
    "layers",
    "com",
    "block",
    "one",
    "let",
    "need",
    "com",
    "relu",
    "com",
    "relu",
    "max",
    "pool",
    "let",
    "try",
    "com",
    "channels",
    "going",
    "input",
    "shape",
    "model",
    "input",
    "shape",
    "parameter",
    "channels",
    "going",
    "number",
    "hidden",
    "units",
    "oh",
    "gon",
    "na",
    "put",
    "enter",
    "input",
    "shape",
    "hidden",
    "units",
    "getting",
    "let",
    "set",
    "kernel",
    "size",
    "three",
    "big",
    "convolving",
    "window",
    "image",
    "data",
    "stride",
    "one",
    "padding",
    "equals",
    "one",
    "well",
    "similar",
    "parameters",
    "cnn",
    "explainer",
    "website",
    "uses",
    "going",
    "go",
    "relu",
    "going",
    "go",
    "com",
    "want",
    "stress",
    "even",
    "someone",
    "else",
    "uses",
    "like",
    "certain",
    "values",
    "copy",
    "exactly",
    "keep",
    "mind",
    "try",
    "various",
    "values",
    "hyper",
    "parameters",
    "set",
    "hidden",
    "units",
    "channels",
    "equals",
    "hidden",
    "units",
    "well",
    "going",
    "go",
    "kernel",
    "size",
    "equals",
    "three",
    "stride",
    "equals",
    "one",
    "going",
    "put",
    "padding",
    "equals",
    "one",
    "well",
    "going",
    "another",
    "relu",
    "layer",
    "believe",
    "forgot",
    "comma",
    "another",
    "relu",
    "layer",
    "going",
    "finish",
    "n",
    "dot",
    "max",
    "pool",
    "2d",
    "going",
    "put",
    "kernel",
    "size",
    "equals",
    "two",
    "stride",
    "equals",
    "two",
    "wonderful",
    "oh",
    "way",
    "max",
    "pool",
    "2d",
    "default",
    "stride",
    "value",
    "kernel",
    "size",
    "let",
    "go",
    "well",
    "could",
    "replicate",
    "block",
    "block",
    "two",
    "copy",
    "already",
    "enough",
    "practice",
    "writing",
    "sort",
    "code",
    "going",
    "go",
    "comp",
    "block",
    "two",
    "need",
    "change",
    "input",
    "shape",
    "input",
    "shape",
    "block",
    "two",
    "going",
    "receive",
    "output",
    "shape",
    "need",
    "line",
    "going",
    "hidden",
    "units",
    "hidden",
    "units",
    "believe",
    "need",
    "change",
    "beautiful",
    "let",
    "create",
    "classifier",
    "layer",
    "classifier",
    "layer",
    "recall",
    "going",
    "output",
    "layer",
    "need",
    "point",
    "add",
    "linear",
    "layer",
    "going",
    "number",
    "outputs",
    "equal",
    "number",
    "classes",
    "working",
    "case",
    "number",
    "classes",
    "case",
    "custom",
    "data",
    "set",
    "three",
    "classes",
    "pizza",
    "steak",
    "sushi",
    "let",
    "create",
    "classifier",
    "layer",
    "end",
    "sequential",
    "going",
    "pass",
    "end",
    "dot",
    "flatten",
    "turn",
    "outputs",
    "convolutional",
    "blocks",
    "feature",
    "vector",
    "feature",
    "vector",
    "site",
    "going",
    "end",
    "dot",
    "linear",
    "end",
    "features",
    "remember",
    "trick",
    "calculating",
    "shape",
    "features",
    "going",
    "put",
    "hidden",
    "units",
    "time",
    "features",
    "going",
    "output",
    "shape",
    "put",
    "hidden",
    "units",
    "time",
    "quite",
    "yet",
    "know",
    "output",
    "shape",
    "operations",
    "going",
    "course",
    "could",
    "calculate",
    "hand",
    "looking",
    "formula",
    "input",
    "output",
    "shapes",
    "convolutional",
    "layers",
    "input",
    "output",
    "shapes",
    "prefer",
    "programmatically",
    "let",
    "errors",
    "tell",
    "wrong",
    "forward",
    "pass",
    "speaking",
    "forward",
    "pass",
    "let",
    "create",
    "forward",
    "method",
    "every",
    "time",
    "subclass",
    "end",
    "dot",
    "module",
    "override",
    "forward",
    "method",
    "done",
    "times",
    "see",
    "picking",
    "pace",
    "little",
    "bit",
    "got",
    "let",
    "pass",
    "conv",
    "block",
    "one",
    "going",
    "go",
    "x",
    "going",
    "print",
    "x",
    "dot",
    "shape",
    "going",
    "reassign",
    "x",
    "block",
    "two",
    "passing",
    "second",
    "block",
    "convolutional",
    "layers",
    "print",
    "x",
    "dot",
    "shape",
    "check",
    "shape",
    "model",
    "probably",
    "error",
    "input",
    "shape",
    "going",
    "line",
    "features",
    "hidden",
    "units",
    "passed",
    "output",
    "going",
    "comp",
    "block",
    "one",
    "comp",
    "block",
    "two",
    "flatten",
    "layer",
    "want",
    "feature",
    "vector",
    "go",
    "layer",
    "output",
    "layer",
    "features",
    "size",
    "output",
    "shape",
    "going",
    "return",
    "going",
    "print",
    "x",
    "dot",
    "shape",
    "want",
    "let",
    "one",
    "little",
    "secret",
    "well",
    "covered",
    "could",
    "rewrite",
    "entire",
    "forward",
    "method",
    "entire",
    "stack",
    "code",
    "going",
    "return",
    "self",
    "dot",
    "classifier",
    "going",
    "outside",
    "could",
    "pass",
    "comp",
    "block",
    "two",
    "comp",
    "block",
    "two",
    "self",
    "comp",
    "block",
    "one",
    "x",
    "inside",
    "essentially",
    "exact",
    "thing",
    "done",
    "except",
    "going",
    "benefits",
    "operator",
    "fusion",
    "topic",
    "beyond",
    "scope",
    "course",
    "essentially",
    "need",
    "know",
    "operator",
    "fusion",
    "behind",
    "scenes",
    "speeds",
    "gpu",
    "performs",
    "computations",
    "going",
    "happen",
    "one",
    "step",
    "rather",
    "reassigning",
    "x",
    "every",
    "time",
    "make",
    "computation",
    "layers",
    "spending",
    "time",
    "going",
    "computation",
    "back",
    "memory",
    "computation",
    "back",
    "memory",
    "whereas",
    "kind",
    "chunks",
    "together",
    "one",
    "hit",
    "like",
    "read",
    "encourage",
    "look",
    "blog",
    "post",
    "make",
    "gpus",
    "go",
    "bur",
    "first",
    "principles",
    "bur",
    "means",
    "fast",
    "love",
    "post",
    "right",
    "half",
    "satire",
    "half",
    "legitimately",
    "like",
    "gpu",
    "computer",
    "science",
    "go",
    "yeah",
    "want",
    "avoid",
    "want",
    "avoid",
    "transportation",
    "memory",
    "compute",
    "look",
    "might",
    "operator",
    "fusion",
    "go",
    "operator",
    "fusion",
    "important",
    "optimization",
    "deep",
    "learning",
    "compilers",
    "link",
    "making",
    "deep",
    "learning",
    "go",
    "bur",
    "first",
    "principles",
    "horace",
    "hare",
    "great",
    "blog",
    "post",
    "really",
    "like",
    "right",
    "like",
    "read",
    "also",
    "going",
    "extracurricular",
    "section",
    "course",
    "worry",
    "got",
    "model",
    "oh",
    "forget",
    "comma",
    "right",
    "course",
    "got",
    "another",
    "forgot",
    "another",
    "comma",
    "notice",
    "beautiful",
    "okay",
    "create",
    "model",
    "going",
    "torch",
    "instance",
    "tiny",
    "vgg",
    "see",
    "model",
    "holds",
    "let",
    "create",
    "model",
    "zero",
    "equals",
    "tiny",
    "vgg",
    "going",
    "pass",
    "input",
    "shape",
    "input",
    "shape",
    "going",
    "number",
    "color",
    "channels",
    "image",
    "number",
    "color",
    "channels",
    "image",
    "data",
    "three",
    "color",
    "images",
    "going",
    "put",
    "hidden",
    "units",
    "equals",
    "10",
    "number",
    "hidden",
    "units",
    "tiny",
    "vgg",
    "architecture",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "nine",
    "could",
    "put",
    "10",
    "could",
    "put",
    "100",
    "could",
    "put",
    "64",
    "good",
    "multiple",
    "eight",
    "let",
    "leave",
    "10",
    "output",
    "shape",
    "going",
    "going",
    "length",
    "class",
    "names",
    "want",
    "one",
    "hidden",
    "unit",
    "one",
    "output",
    "unit",
    "per",
    "class",
    "going",
    "send",
    "target",
    "device",
    "course",
    "cuda",
    "check",
    "model",
    "zero",
    "beautiful",
    "took",
    "seconds",
    "saw",
    "move",
    "gpu",
    "memory",
    "something",
    "keep",
    "mind",
    "build",
    "large",
    "neural",
    "networks",
    "want",
    "speed",
    "computation",
    "use",
    "operator",
    "fusion",
    "saw",
    "took",
    "seconds",
    "model",
    "move",
    "cpu",
    "default",
    "gpu",
    "got",
    "architecture",
    "course",
    "know",
    "potentially",
    "wrong",
    "would",
    "find",
    "well",
    "could",
    "find",
    "right",
    "hidden",
    "unit",
    "shape",
    "could",
    "find",
    "wrong",
    "passing",
    "dummy",
    "data",
    "model",
    "one",
    "favorite",
    "ways",
    "troubleshoot",
    "model",
    "let",
    "next",
    "video",
    "pass",
    "dummy",
    "data",
    "model",
    "see",
    "implemented",
    "forward",
    "pass",
    "correctly",
    "also",
    "check",
    "input",
    "output",
    "shapes",
    "layers",
    "see",
    "last",
    "video",
    "replicated",
    "tiny",
    "vgg",
    "architecture",
    "cnn",
    "explainer",
    "website",
    "similar",
    "model",
    "built",
    "section",
    "time",
    "using",
    "color",
    "images",
    "instead",
    "grayscale",
    "images",
    "quite",
    "bit",
    "faster",
    "previously",
    "already",
    "covered",
    "right",
    "experience",
    "building",
    "pilotage",
    "models",
    "scratch",
    "going",
    "pick",
    "pace",
    "build",
    "models",
    "let",
    "go",
    "try",
    "dummy",
    "forward",
    "pass",
    "check",
    "forward",
    "method",
    "working",
    "correctly",
    "input",
    "output",
    "shapes",
    "correct",
    "let",
    "create",
    "new",
    "heading",
    "try",
    "forward",
    "pass",
    "single",
    "image",
    "one",
    "favorite",
    "ways",
    "test",
    "model",
    "let",
    "first",
    "get",
    "single",
    "image",
    "get",
    "single",
    "image",
    "want",
    "image",
    "batch",
    "maybe",
    "get",
    "image",
    "batch",
    "get",
    "single",
    "image",
    "batch",
    "got",
    "images",
    "batches",
    "already",
    "image",
    "batch",
    "get",
    "label",
    "batch",
    "go",
    "next",
    "train",
    "data",
    "loader",
    "simple",
    "data",
    "loader",
    "working",
    "check",
    "image",
    "batch",
    "dot",
    "shape",
    "label",
    "batch",
    "dot",
    "shape",
    "wonderful",
    "let",
    "see",
    "happens",
    "try",
    "forward",
    "pass",
    "oh",
    "spelled",
    "single",
    "wrong",
    "try",
    "forward",
    "pass",
    "could",
    "try",
    "single",
    "image",
    "trying",
    "batch",
    "result",
    "similar",
    "results",
    "let",
    "go",
    "model",
    "zero",
    "going",
    "pass",
    "image",
    "batch",
    "see",
    "happens",
    "oh",
    "course",
    "get",
    "input",
    "type",
    "torch",
    "float",
    "tensor",
    "wait",
    "type",
    "torch",
    "cuda",
    "float",
    "tensor",
    "input",
    "got",
    "tensors",
    "different",
    "device",
    "right",
    "cpu",
    "image",
    "batch",
    "whereas",
    "model",
    "course",
    "target",
    "device",
    "seen",
    "error",
    "number",
    "times",
    "let",
    "see",
    "fixes",
    "oh",
    "get",
    "error",
    "kind",
    "expected",
    "type",
    "error",
    "got",
    "runtime",
    "error",
    "amount",
    "one",
    "mat",
    "two",
    "shapes",
    "multiplied",
    "looks",
    "like",
    "batch",
    "size",
    "2560",
    "hmm",
    "10",
    "well",
    "recall",
    "10",
    "number",
    "hidden",
    "units",
    "size",
    "10",
    "trying",
    "multiply",
    "matrix",
    "size",
    "size",
    "10",
    "got",
    "something",
    "going",
    "need",
    "get",
    "two",
    "numbers",
    "middle",
    "numbers",
    "satisfy",
    "rules",
    "matrix",
    "multiplication",
    "happens",
    "linear",
    "layer",
    "need",
    "get",
    "two",
    "numbers",
    "hint",
    "trick",
    "look",
    "previous",
    "layer",
    "batch",
    "size",
    "value",
    "come",
    "well",
    "could",
    "fact",
    "tensor",
    "size",
    "goes",
    "flatten",
    "layer",
    "recall",
    "layer",
    "printed",
    "shape",
    "conv",
    "block",
    "output",
    "conv",
    "block",
    "one",
    "shape",
    "output",
    "conv",
    "block",
    "two",
    "got",
    "number",
    "output",
    "conv",
    "block",
    "one",
    "output",
    "conv",
    "block",
    "two",
    "must",
    "input",
    "classifier",
    "layer",
    "go",
    "10",
    "times",
    "16",
    "times",
    "16",
    "get",
    "beautiful",
    "multiply",
    "hitting",
    "units",
    "10",
    "16",
    "16",
    "shape",
    "get",
    "let",
    "see",
    "works",
    "go",
    "times",
    "16",
    "times",
    "let",
    "see",
    "happens",
    "rerun",
    "model",
    "rerun",
    "image",
    "batch",
    "pass",
    "oh",
    "look",
    "model",
    "works",
    "shapes",
    "least",
    "line",
    "know",
    "works",
    "yet",
    "started",
    "training",
    "yet",
    "output",
    "size",
    "got",
    "output",
    "cuda",
    "device",
    "course",
    "got",
    "32",
    "samples",
    "three",
    "numbers",
    "going",
    "good",
    "random",
    "trained",
    "model",
    "yet",
    "initialized",
    "random",
    "weights",
    "got",
    "32",
    "batch",
    "worth",
    "random",
    "predictions",
    "32",
    "images",
    "see",
    "output",
    "shape",
    "three",
    "corresponds",
    "output",
    "shape",
    "set",
    "output",
    "shape",
    "equals",
    "length",
    "class",
    "names",
    "exactly",
    "number",
    "classes",
    "dealing",
    "think",
    "number",
    "little",
    "bit",
    "different",
    "cnn",
    "explainer",
    "end",
    "1313",
    "know",
    "think",
    "got",
    "one",
    "numbers",
    "wrong",
    "kernel",
    "size",
    "stride",
    "padding",
    "let",
    "look",
    "jump",
    "wanted",
    "truly",
    "replicate",
    "padding",
    "actually",
    "think",
    "padding",
    "go",
    "back",
    "see",
    "change",
    "zero",
    "change",
    "zero",
    "zero",
    "sure",
    "work",
    "way",
    "bad",
    "trying",
    "line",
    "shapes",
    "cnn",
    "explainer",
    "truly",
    "replicate",
    "output",
    "com",
    "block",
    "1",
    "working",
    "moment",
    "got",
    "let",
    "see",
    "removing",
    "padding",
    "convolutional",
    "layers",
    "lines",
    "shape",
    "cnn",
    "explainer",
    "going",
    "rerun",
    "rerun",
    "model",
    "set",
    "padding",
    "zero",
    "padding",
    "hyper",
    "parameters",
    "oh",
    "get",
    "another",
    "error",
    "get",
    "another",
    "shape",
    "error",
    "course",
    "got",
    "different",
    "shapes",
    "wow",
    "see",
    "often",
    "errors",
    "come",
    "trust",
    "spend",
    "lot",
    "time",
    "troubleshooting",
    "shape",
    "errors",
    "line",
    "shapes",
    "got",
    "equal",
    "let",
    "try",
    "beautiful",
    "shapes",
    "line",
    "cnn",
    "explainer",
    "got",
    "remember",
    "pytorch",
    "color",
    "channels",
    "first",
    "whereas",
    "color",
    "channels",
    "last",
    "yeah",
    "got",
    "output",
    "first",
    "com",
    "block",
    "lining",
    "correct",
    "second",
    "block",
    "good",
    "officially",
    "replicated",
    "cnn",
    "explainer",
    "model",
    "take",
    "value",
    "bring",
    "back",
    "remember",
    "hidden",
    "units",
    "going",
    "multiply",
    "could",
    "calculate",
    "shapes",
    "hand",
    "trick",
    "like",
    "let",
    "error",
    "codes",
    "give",
    "hint",
    "go",
    "boom",
    "go",
    "get",
    "working",
    "shape",
    "troubleshooting",
    "fly",
    "done",
    "single",
    "forward",
    "pass",
    "model",
    "kind",
    "verify",
    "data",
    "least",
    "flows",
    "next",
    "well",
    "like",
    "show",
    "another",
    "little",
    "package",
    "like",
    "use",
    "also",
    "look",
    "input",
    "output",
    "shapes",
    "model",
    "called",
    "torch",
    "info",
    "might",
    "want",
    "give",
    "shot",
    "go",
    "next",
    "video",
    "next",
    "video",
    "going",
    "see",
    "use",
    "torch",
    "info",
    "print",
    "summary",
    "model",
    "going",
    "get",
    "something",
    "like",
    "beautifully",
    "easy",
    "torch",
    "info",
    "use",
    "give",
    "shot",
    "install",
    "google",
    "colab",
    "run",
    "cell",
    "see",
    "get",
    "something",
    "similar",
    "output",
    "model",
    "zero",
    "see",
    "next",
    "video",
    "try",
    "together",
    "last",
    "video",
    "checked",
    "model",
    "forward",
    "pass",
    "single",
    "batch",
    "learned",
    "forward",
    "method",
    "far",
    "looks",
    "like",
    "intact",
    "get",
    "shape",
    "errors",
    "data",
    "moves",
    "model",
    "like",
    "introduce",
    "one",
    "favorite",
    "packages",
    "finding",
    "information",
    "pytorch",
    "model",
    "torch",
    "info",
    "let",
    "use",
    "torch",
    "info",
    "get",
    "idea",
    "shapes",
    "going",
    "model",
    "know",
    "much",
    "love",
    "things",
    "programmatic",
    "way",
    "well",
    "torch",
    "info",
    "used",
    "print",
    "statements",
    "find",
    "different",
    "shapes",
    "going",
    "model",
    "going",
    "comment",
    "forward",
    "method",
    "run",
    "later",
    "training",
    "get",
    "excessive",
    "printouts",
    "shapes",
    "let",
    "see",
    "torch",
    "info",
    "last",
    "video",
    "issued",
    "challenge",
    "give",
    "go",
    "quite",
    "straightforward",
    "use",
    "let",
    "see",
    "together",
    "type",
    "output",
    "looking",
    "tiny",
    "vgg",
    "model",
    "course",
    "could",
    "get",
    "type",
    "output",
    "almost",
    "pytorch",
    "model",
    "install",
    "first",
    "far",
    "know",
    "google",
    "colab",
    "come",
    "torch",
    "info",
    "default",
    "might",
    "well",
    "try",
    "future",
    "see",
    "works",
    "yeah",
    "get",
    "module",
    "google",
    "colab",
    "instance",
    "install",
    "problem",
    "let",
    "install",
    "torch",
    "info",
    "install",
    "torch",
    "info",
    "import",
    "available",
    "going",
    "try",
    "import",
    "torch",
    "info",
    "already",
    "installed",
    "import",
    "work",
    "try",
    "block",
    "fails",
    "going",
    "run",
    "pip",
    "install",
    "torch",
    "info",
    "import",
    "torch",
    "info",
    "going",
    "run",
    "torch",
    "info",
    "import",
    "summary",
    "works",
    "going",
    "get",
    "summary",
    "model",
    "going",
    "pass",
    "model",
    "zero",
    "put",
    "input",
    "size",
    "example",
    "size",
    "data",
    "flow",
    "model",
    "case",
    "let",
    "put",
    "input",
    "size",
    "1",
    "3",
    "64",
    "example",
    "putting",
    "batch",
    "one",
    "image",
    "could",
    "potentially",
    "put",
    "32",
    "wanted",
    "let",
    "put",
    "batch",
    "singular",
    "image",
    "course",
    "could",
    "change",
    "values",
    "wanted",
    "24",
    "might",
    "notice",
    "get",
    "right",
    "input",
    "size",
    "produces",
    "error",
    "go",
    "like",
    "got",
    "printed",
    "input",
    "sizes",
    "manually",
    "get",
    "error",
    "torch",
    "info",
    "behind",
    "scenes",
    "going",
    "going",
    "forward",
    "pass",
    "whichever",
    "model",
    "pass",
    "input",
    "size",
    "whichever",
    "input",
    "size",
    "give",
    "let",
    "put",
    "input",
    "size",
    "model",
    "built",
    "wonderful",
    "torch",
    "info",
    "gives",
    "us",
    "oh",
    "excuse",
    "comment",
    "printouts",
    "make",
    "sure",
    "commented",
    "printouts",
    "forward",
    "method",
    "20",
    "vgg",
    "class",
    "going",
    "run",
    "run",
    "run",
    "make",
    "sure",
    "everything",
    "still",
    "works",
    "run",
    "torch",
    "info",
    "go",
    "printouts",
    "model",
    "look",
    "beautiful",
    "love",
    "prints",
    "tiny",
    "vgg",
    "class",
    "see",
    "comprised",
    "three",
    "sequential",
    "blocks",
    "inside",
    "sequential",
    "blocks",
    "different",
    "combinations",
    "layers",
    "conv",
    "layers",
    "relu",
    "layers",
    "max",
    "pool",
    "layers",
    "final",
    "layer",
    "classification",
    "layer",
    "flatten",
    "linear",
    "layer",
    "see",
    "shapes",
    "changing",
    "throughout",
    "model",
    "data",
    "goes",
    "gets",
    "manipulated",
    "various",
    "layers",
    "line",
    "cnn",
    "explainer",
    "check",
    "last",
    "one",
    "already",
    "verified",
    "also",
    "get",
    "helpful",
    "information",
    "total",
    "params",
    "see",
    "layers",
    "different",
    "amount",
    "parameters",
    "learn",
    "recall",
    "parameter",
    "value",
    "weight",
    "bias",
    "term",
    "within",
    "layers",
    "starts",
    "random",
    "number",
    "whole",
    "goal",
    "deep",
    "learning",
    "adjust",
    "random",
    "numbers",
    "better",
    "represent",
    "data",
    "case",
    "8000",
    "total",
    "parameters",
    "actually",
    "quite",
    "small",
    "future",
    "probably",
    "play",
    "around",
    "models",
    "million",
    "parameters",
    "models",
    "starting",
    "many",
    "billions",
    "parameters",
    "also",
    "get",
    "information",
    "much",
    "model",
    "size",
    "would",
    "would",
    "helpful",
    "depending",
    "put",
    "model",
    "notice",
    "model",
    "gets",
    "larger",
    "layers",
    "parameters",
    "weights",
    "bias",
    "terms",
    "adjusted",
    "learn",
    "patterns",
    "data",
    "input",
    "size",
    "estimated",
    "total",
    "size",
    "would",
    "definitely",
    "get",
    "bigger",
    "well",
    "something",
    "keep",
    "mind",
    "size",
    "constraints",
    "terms",
    "storage",
    "future",
    "applications",
    "megabyte",
    "quite",
    "small",
    "might",
    "find",
    "models",
    "future",
    "get",
    "500",
    "megabytes",
    "maybe",
    "even",
    "gigabyte",
    "keep",
    "mind",
    "going",
    "forward",
    "crux",
    "torch",
    "info",
    "one",
    "favorite",
    "packages",
    "gives",
    "idea",
    "input",
    "output",
    "shapes",
    "layers",
    "use",
    "torch",
    "info",
    "wherever",
    "need",
    "work",
    "pytorch",
    "models",
    "sure",
    "pass",
    "right",
    "input",
    "size",
    "also",
    "use",
    "verify",
    "like",
    "input",
    "output",
    "shapes",
    "correct",
    "check",
    "big",
    "shout",
    "tyler",
    "yup",
    "everyone",
    "created",
    "torch",
    "info",
    "package",
    "next",
    "video",
    "let",
    "move",
    "towards",
    "training",
    "tiny",
    "vgg",
    "model",
    "going",
    "create",
    "training",
    "test",
    "functions",
    "want",
    "jump",
    "ahead",
    "already",
    "done",
    "encourage",
    "go",
    "back",
    "section",
    "functionalizing",
    "training",
    "test",
    "loops",
    "going",
    "build",
    "functions",
    "similar",
    "custom",
    "data",
    "set",
    "want",
    "replicate",
    "functions",
    "notebook",
    "give",
    "go",
    "otherwise",
    "see",
    "next",
    "video",
    "together",
    "go",
    "give",
    "shot",
    "try",
    "replicating",
    "train",
    "step",
    "test",
    "step",
    "function",
    "hope",
    "otherwise",
    "let",
    "video",
    "time",
    "going",
    "custom",
    "data",
    "sets",
    "find",
    "much",
    "anything",
    "changes",
    "created",
    "train",
    "test",
    "loop",
    "functions",
    "way",
    "generic",
    "want",
    "create",
    "train",
    "step",
    "function",
    "generic",
    "mean",
    "used",
    "almost",
    "model",
    "data",
    "loader",
    "train",
    "step",
    "takes",
    "model",
    "data",
    "loader",
    "trains",
    "model",
    "data",
    "loader",
    "also",
    "want",
    "create",
    "another",
    "function",
    "called",
    "test",
    "step",
    "takes",
    "model",
    "data",
    "loader",
    "things",
    "evaluates",
    "model",
    "data",
    "loader",
    "course",
    "train",
    "step",
    "test",
    "step",
    "respectively",
    "going",
    "take",
    "training",
    "data",
    "loader",
    "might",
    "make",
    "third",
    "heading",
    "outline",
    "looks",
    "nice",
    "beautiful",
    "section",
    "seven",
    "turning",
    "quite",
    "big",
    "section",
    "course",
    "want",
    "respectively",
    "taken",
    "data",
    "loader",
    "train",
    "takes",
    "train",
    "data",
    "loader",
    "test",
    "takes",
    "test",
    "data",
    "loader",
    "without",
    "ado",
    "let",
    "create",
    "train",
    "step",
    "function",
    "seen",
    "one",
    "computer",
    "vision",
    "section",
    "let",
    "see",
    "make",
    "need",
    "train",
    "step",
    "going",
    "take",
    "model",
    "torch",
    "dot",
    "module",
    "want",
    "also",
    "take",
    "data",
    "loader",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "loader",
    "going",
    "take",
    "loss",
    "function",
    "going",
    "torch",
    "dot",
    "module",
    "well",
    "going",
    "take",
    "optimizer",
    "going",
    "torch",
    "opt",
    "dot",
    "optimizer",
    "wonderful",
    "first",
    "thing",
    "training",
    "step",
    "well",
    "put",
    "model",
    "train",
    "mode",
    "let",
    "go",
    "model",
    "dot",
    "train",
    "shall",
    "next",
    "well",
    "let",
    "set",
    "evaluation",
    "metrics",
    "one",
    "loss",
    "one",
    "accuracy",
    "set",
    "train",
    "loss",
    "train",
    "accuracy",
    "values",
    "going",
    "accumulate",
    "per",
    "batch",
    "working",
    "batches",
    "got",
    "train",
    "loss",
    "train",
    "act",
    "equals",
    "zero",
    "zero",
    "loop",
    "data",
    "loader",
    "let",
    "write",
    "loop",
    "data",
    "loader",
    "loop",
    "batches",
    "batchified",
    "data",
    "loader",
    "batch",
    "x",
    "enumerate",
    "data",
    "loader",
    "want",
    "send",
    "data",
    "target",
    "device",
    "could",
    "even",
    "put",
    "device",
    "parameter",
    "device",
    "equals",
    "device",
    "set",
    "device",
    "default",
    "go",
    "x",
    "equals",
    "x",
    "dot",
    "two",
    "device",
    "dot",
    "two",
    "device",
    "beautiful",
    "well",
    "remember",
    "pie",
    "torch",
    "unofficial",
    "pie",
    "torch",
    "optimization",
    "song",
    "forward",
    "pass",
    "pred",
    "equals",
    "model",
    "om",
    "number",
    "two",
    "calculate",
    "last",
    "calculate",
    "loss",
    "let",
    "go",
    "loss",
    "equals",
    "loss",
    "function",
    "going",
    "pass",
    "pred",
    "done",
    "times",
    "little",
    "bit",
    "faster",
    "hope",
    "noticed",
    "things",
    "covered",
    "stepping",
    "pace",
    "bit",
    "might",
    "bit",
    "challenge",
    "right",
    "handle",
    "accumulating",
    "loss",
    "starting",
    "zero",
    "batch",
    "forward",
    "pass",
    "calculating",
    "loss",
    "adding",
    "overall",
    "train",
    "loss",
    "going",
    "optimize",
    "zero",
    "grad",
    "zero",
    "gradients",
    "optimizer",
    "new",
    "batch",
    "going",
    "perform",
    "back",
    "propagation",
    "loss",
    "backwards",
    "five",
    "optimize",
    "step",
    "step",
    "step",
    "wonderful",
    "look",
    "look",
    "us",
    "coding",
    "train",
    "loop",
    "minute",
    "let",
    "calculate",
    "accuracy",
    "accumulate",
    "calculate",
    "notice",
    "accuracy",
    "function",
    "accuracy",
    "quite",
    "straightforward",
    "metric",
    "calculate",
    "first",
    "get",
    "pred",
    "class",
    "going",
    "output",
    "model",
    "logits",
    "seen",
    "raw",
    "output",
    "model",
    "logits",
    "get",
    "class",
    "going",
    "take",
    "arg",
    "max",
    "torch",
    "dot",
    "softmax",
    "get",
    "prediction",
    "probabilities",
    "pred",
    "raw",
    "logits",
    "got",
    "across",
    "dimension",
    "one",
    "also",
    "across",
    "dimension",
    "one",
    "beautiful",
    "give",
    "us",
    "labels",
    "find",
    "wrong",
    "checking",
    "later",
    "going",
    "create",
    "accuracy",
    "taking",
    "pred",
    "class",
    "checking",
    "quality",
    "right",
    "labels",
    "going",
    "give",
    "us",
    "many",
    "values",
    "equal",
    "true",
    "want",
    "take",
    "sum",
    "take",
    "item",
    "single",
    "integer",
    "want",
    "divide",
    "length",
    "pred",
    "getting",
    "total",
    "number",
    "right",
    "dividing",
    "length",
    "samples",
    "formula",
    "accuracy",
    "come",
    "outside",
    "batch",
    "loop",
    "know",
    "got",
    "helpful",
    "line",
    "drawn",
    "go",
    "adjust",
    "metrics",
    "get",
    "average",
    "loss",
    "accuracy",
    "per",
    "batch",
    "going",
    "set",
    "train",
    "loss",
    "equal",
    "train",
    "loss",
    "divided",
    "length",
    "data",
    "loader",
    "number",
    "batches",
    "total",
    "train",
    "accuracy",
    "train",
    "act",
    "divided",
    "length",
    "data",
    "loader",
    "well",
    "going",
    "give",
    "us",
    "average",
    "loss",
    "average",
    "accuracy",
    "per",
    "epoch",
    "across",
    "batches",
    "train",
    "act",
    "pretty",
    "good",
    "looking",
    "function",
    "train",
    "step",
    "want",
    "take",
    "test",
    "step",
    "pause",
    "video",
    "give",
    "shot",
    "get",
    "great",
    "inspiration",
    "notebook",
    "otherwise",
    "going",
    "together",
    "three",
    "two",
    "one",
    "let",
    "test",
    "step",
    "create",
    "test",
    "step",
    "function",
    "want",
    "able",
    "call",
    "functions",
    "epoch",
    "loop",
    "way",
    "instead",
    "writing",
    "training",
    "test",
    "code",
    "multiple",
    "different",
    "models",
    "write",
    "call",
    "functions",
    "let",
    "create",
    "def",
    "test",
    "step",
    "going",
    "model",
    "going",
    "could",
    "type",
    "torch",
    "module",
    "going",
    "data",
    "loader",
    "torch",
    "utils",
    "dot",
    "data",
    "data",
    "loader",
    "capital",
    "l",
    "going",
    "pass",
    "loss",
    "function",
    "need",
    "optimizer",
    "test",
    "function",
    "trying",
    "optimize",
    "anything",
    "trying",
    "evaluate",
    "model",
    "training",
    "dataset",
    "let",
    "put",
    "device",
    "way",
    "change",
    "device",
    "need",
    "put",
    "model",
    "val",
    "mode",
    "going",
    "evaluating",
    "going",
    "testing",
    "set",
    "test",
    "loss",
    "test",
    "accuracy",
    "values",
    "test",
    "loss",
    "test",
    "act",
    "going",
    "make",
    "zero",
    "going",
    "accumulate",
    "per",
    "batch",
    "go",
    "batch",
    "let",
    "turn",
    "inference",
    "mode",
    "behind",
    "scenes",
    "going",
    "take",
    "care",
    "lot",
    "pie",
    "torch",
    "functionality",
    "need",
    "helpful",
    "training",
    "tracking",
    "gradients",
    "testing",
    "need",
    "loop",
    "data",
    "loader",
    "data",
    "batches",
    "going",
    "go",
    "batch",
    "x",
    "enumerate",
    "data",
    "loader",
    "notice",
    "actually",
    "use",
    "batch",
    "term",
    "probably",
    "wo",
    "use",
    "either",
    "like",
    "go",
    "case",
    "wanted",
    "use",
    "anyway",
    "send",
    "data",
    "target",
    "device",
    "going",
    "go",
    "x",
    "equals",
    "x",
    "dot",
    "two",
    "device",
    "dot",
    "two",
    "device",
    "beautiful",
    "evaluation",
    "step",
    "test",
    "step",
    "well",
    "course",
    "forward",
    "pass",
    "forward",
    "pass",
    "going",
    "let",
    "call",
    "test",
    "pred",
    "logits",
    "get",
    "raw",
    "outputs",
    "model",
    "calculate",
    "loss",
    "raw",
    "outputs",
    "calculate",
    "loss",
    "get",
    "loss",
    "equal",
    "loss",
    "function",
    "test",
    "pred",
    "logits",
    "versus",
    "going",
    "accumulate",
    "loss",
    "test",
    "loss",
    "plus",
    "equals",
    "loss",
    "dot",
    "item",
    "remember",
    "item",
    "gets",
    "single",
    "integer",
    "whatever",
    "term",
    "call",
    "going",
    "calculate",
    "accuracy",
    "exactly",
    "done",
    "training",
    "data",
    "set",
    "training",
    "step",
    "test",
    "pred",
    "labels",
    "going",
    "want",
    "highlight",
    "fact",
    "actually",
    "need",
    "take",
    "softmax",
    "could",
    "take",
    "argmax",
    "directly",
    "reason",
    "take",
    "softmax",
    "could",
    "could",
    "directly",
    "take",
    "argmax",
    "logits",
    "reason",
    "get",
    "softmax",
    "completeness",
    "wanted",
    "prediction",
    "probabilities",
    "could",
    "use",
    "torch",
    "dot",
    "softmax",
    "prediction",
    "logits",
    "100",
    "necessary",
    "get",
    "values",
    "test",
    "try",
    "without",
    "softmax",
    "see",
    "get",
    "results",
    "going",
    "go",
    "test",
    "accuracy",
    "plus",
    "equals",
    "create",
    "accuracy",
    "calculation",
    "fly",
    "test",
    "pred",
    "labels",
    "check",
    "equality",
    "get",
    "sum",
    "get",
    "item",
    "divide",
    "length",
    "test",
    "pred",
    "labels",
    "beautiful",
    "going",
    "give",
    "us",
    "accuracy",
    "per",
    "batch",
    "want",
    "adjust",
    "metrics",
    "get",
    "average",
    "loss",
    "accuracy",
    "per",
    "batch",
    "test",
    "loss",
    "equals",
    "test",
    "loss",
    "divided",
    "length",
    "data",
    "loader",
    "going",
    "go",
    "test",
    "ac",
    "equals",
    "test",
    "act",
    "divided",
    "length",
    "data",
    "loader",
    "finally",
    "going",
    "return",
    "test",
    "loss",
    "lost",
    "test",
    "accuracy",
    "look",
    "us",
    "go",
    "previous",
    "videos",
    "took",
    "us",
    "previous",
    "sections",
    "took",
    "us",
    "fairly",
    "long",
    "time",
    "done",
    "10",
    "minutes",
    "give",
    "pat",
    "back",
    "progress",
    "making",
    "let",
    "next",
    "video",
    "computer",
    "vision",
    "section",
    "well",
    "created",
    "create",
    "train",
    "function",
    "oh",
    "could",
    "let",
    "create",
    "function",
    "functionize",
    "want",
    "train",
    "model",
    "think",
    "actually",
    "deaf",
    "train",
    "done",
    "much",
    "sure",
    "done",
    "oh",
    "okay",
    "looks",
    "like",
    "might",
    "next",
    "video",
    "give",
    "challenge",
    "create",
    "function",
    "called",
    "train",
    "combines",
    "two",
    "functions",
    "loops",
    "epoch",
    "range",
    "like",
    "done",
    "previous",
    "notebook",
    "functionize",
    "step",
    "need",
    "take",
    "number",
    "epochs",
    "need",
    "take",
    "train",
    "data",
    "loader",
    "test",
    "data",
    "loader",
    "model",
    "loss",
    "function",
    "optimizer",
    "maybe",
    "device",
    "think",
    "pretty",
    "way",
    "steps",
    "need",
    "train",
    "give",
    "shot",
    "next",
    "video",
    "going",
    "create",
    "function",
    "combines",
    "train",
    "step",
    "test",
    "step",
    "train",
    "model",
    "see",
    "go",
    "last",
    "video",
    "issued",
    "challenge",
    "combine",
    "train",
    "step",
    "function",
    "well",
    "test",
    "step",
    "function",
    "together",
    "function",
    "could",
    "call",
    "one",
    "function",
    "calls",
    "train",
    "model",
    "evaluate",
    "course",
    "let",
    "together",
    "hope",
    "gave",
    "shot",
    "going",
    "create",
    "train",
    "function",
    "role",
    "function",
    "going",
    "said",
    "combine",
    "train",
    "step",
    "test",
    "step",
    "purpose",
    "right",
    "want",
    "rewrite",
    "code",
    "time",
    "want",
    "functionalizing",
    "many",
    "things",
    "possible",
    "import",
    "later",
    "wanted",
    "train",
    "models",
    "leverage",
    "code",
    "written",
    "long",
    "works",
    "let",
    "see",
    "going",
    "create",
    "train",
    "function",
    "going",
    "first",
    "import",
    "tqdm",
    "like",
    "get",
    "progress",
    "bar",
    "model",
    "training",
    "nothing",
    "quite",
    "like",
    "watching",
    "neural",
    "network",
    "train",
    "step",
    "number",
    "one",
    "need",
    "create",
    "train",
    "function",
    "takes",
    "various",
    "model",
    "parameters",
    "plus",
    "optimizer",
    "plus",
    "data",
    "loaders",
    "plus",
    "loss",
    "function",
    "whole",
    "bunch",
    "different",
    "things",
    "let",
    "create",
    "def",
    "train",
    "going",
    "pass",
    "model",
    "going",
    "torch",
    "dot",
    "module",
    "notice",
    "inputs",
    "going",
    "quite",
    "similar",
    "train",
    "step",
    "test",
    "step",
    "actually",
    "need",
    "also",
    "want",
    "train",
    "data",
    "loader",
    "training",
    "data",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "loader",
    "also",
    "want",
    "test",
    "data",
    "loader",
    "going",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "loader",
    "want",
    "optimizer",
    "optimizer",
    "used",
    "training",
    "data",
    "set",
    "okay",
    "take",
    "input",
    "miser",
    "want",
    "loss",
    "function",
    "generally",
    "used",
    "training",
    "testing",
    "step",
    "combining",
    "since",
    "working",
    "multi",
    "class",
    "classification",
    "going",
    "set",
    "loss",
    "function",
    "default",
    "n",
    "dot",
    "cross",
    "entropy",
    "loss",
    "going",
    "get",
    "epochs",
    "going",
    "set",
    "five",
    "train",
    "five",
    "epochs",
    "default",
    "finally",
    "going",
    "set",
    "device",
    "equal",
    "device",
    "get",
    "wrong",
    "right",
    "keep",
    "coding",
    "ignore",
    "little",
    "red",
    "lines",
    "stay",
    "around",
    "come",
    "back",
    "step",
    "number",
    "two",
    "going",
    "create",
    "step",
    "might",
    "seen",
    "going",
    "create",
    "empty",
    "results",
    "dictionary",
    "going",
    "help",
    "us",
    "track",
    "results",
    "recall",
    "previous",
    "notebook",
    "outputted",
    "model",
    "dictionary",
    "model",
    "went",
    "look",
    "model",
    "one",
    "results",
    "yeah",
    "got",
    "dictionary",
    "like",
    "like",
    "create",
    "one",
    "fly",
    "keep",
    "track",
    "result",
    "every",
    "epoch",
    "loss",
    "epoch",
    "number",
    "zero",
    "accuracy",
    "epoch",
    "number",
    "three",
    "show",
    "use",
    "dictionary",
    "update",
    "model",
    "trains",
    "results",
    "want",
    "keep",
    "track",
    "train",
    "loss",
    "going",
    "set",
    "equal",
    "empty",
    "list",
    "append",
    "also",
    "want",
    "keep",
    "track",
    "train",
    "accuracy",
    "set",
    "empty",
    "list",
    "well",
    "also",
    "want",
    "keep",
    "track",
    "test",
    "loss",
    "also",
    "want",
    "keep",
    "track",
    "test",
    "accuracy",
    "notice",
    "time",
    "track",
    "actually",
    "flexible",
    "functions",
    "also",
    "flexible",
    "gold",
    "standard",
    "anything",
    "means",
    "one",
    "way",
    "works",
    "probably",
    "find",
    "future",
    "need",
    "different",
    "functionality",
    "course",
    "code",
    "let",
    "loop",
    "epochs",
    "epoch",
    "tqdm",
    "let",
    "create",
    "range",
    "epochs",
    "set",
    "train",
    "loss",
    "missed",
    "comma",
    "somewhere",
    "type",
    "annotation",
    "supported",
    "type",
    "expression",
    "okay",
    "right",
    "leave",
    "going",
    "go",
    "train",
    "loss",
    "train",
    "act",
    "recall",
    "train",
    "step",
    "function",
    "created",
    "previous",
    "video",
    "train",
    "step",
    "returns",
    "train",
    "loss",
    "train",
    "act",
    "said",
    "want",
    "keep",
    "track",
    "throughout",
    "training",
    "going",
    "get",
    "train",
    "step",
    "epoch",
    "range",
    "epochs",
    "going",
    "pass",
    "model",
    "perform",
    "training",
    "step",
    "data",
    "loader",
    "course",
    "going",
    "train",
    "data",
    "loader",
    "loss",
    "function",
    "going",
    "loss",
    "function",
    "pass",
    "train",
    "function",
    "optimizer",
    "going",
    "optimizer",
    "device",
    "going",
    "device",
    "beautiful",
    "look",
    "performed",
    "training",
    "step",
    "five",
    "lines",
    "code",
    "let",
    "keep",
    "pushing",
    "forward",
    "telling",
    "us",
    "got",
    "whole",
    "bunch",
    "different",
    "things",
    "epox",
    "defined",
    "maybe",
    "get",
    "rid",
    "ca",
    "type",
    "annotation",
    "stop",
    "stop",
    "google",
    "colab",
    "getting",
    "angry",
    "us",
    "anymore",
    "going",
    "ignore",
    "epox",
    "anyway",
    "leave",
    "find",
    "error",
    "later",
    "test",
    "loss",
    "might",
    "able",
    "find",
    "test",
    "step",
    "going",
    "pass",
    "model",
    "going",
    "pass",
    "data",
    "loader",
    "going",
    "test",
    "data",
    "loader",
    "look",
    "us",
    "go",
    "grading",
    "training",
    "test",
    "step",
    "functions",
    "loss",
    "function",
    "need",
    "optimizer",
    "going",
    "pass",
    "device",
    "behind",
    "scenes",
    "functions",
    "going",
    "train",
    "test",
    "model",
    "cool",
    "still",
    "within",
    "loop",
    "important",
    "within",
    "loop",
    "going",
    "number",
    "four",
    "going",
    "print",
    "let",
    "print",
    "happening",
    "print",
    "happening",
    "go",
    "print",
    "fancy",
    "little",
    "print",
    "statement",
    "get",
    "epoch",
    "get",
    "train",
    "loss",
    "equal",
    "train",
    "loss",
    "get",
    "let",
    "go",
    "four",
    "decimal",
    "places",
    "get",
    "train",
    "accuracy",
    "going",
    "train",
    "act",
    "get",
    "four",
    "maybe",
    "three",
    "decimal",
    "four",
    "looks",
    "nice",
    "looks",
    "aesthetic",
    "go",
    "test",
    "loss",
    "get",
    "coming",
    "pass",
    "test",
    "loss",
    "get",
    "four",
    "decimal",
    "places",
    "well",
    "finally",
    "get",
    "test",
    "accuracy",
    "fairly",
    "long",
    "print",
    "statement",
    "right",
    "like",
    "see",
    "model",
    "training",
    "beautiful",
    "still",
    "within",
    "epoch",
    "want",
    "update",
    "results",
    "dictionary",
    "keep",
    "track",
    "model",
    "performed",
    "time",
    "let",
    "pass",
    "results",
    "want",
    "update",
    "train",
    "loss",
    "going",
    "append",
    "train",
    "loss",
    "value",
    "going",
    "expend",
    "list",
    "train",
    "loss",
    "value",
    "every",
    "epoch",
    "thing",
    "train",
    "accuracy",
    "append",
    "train",
    "act",
    "thing",
    "test",
    "loss",
    "dot",
    "append",
    "test",
    "loss",
    "finally",
    "thing",
    "test",
    "accuracy",
    "test",
    "accuracy",
    "pretty",
    "big",
    "function",
    "write",
    "code",
    "use",
    "multiple",
    "times",
    "later",
    "return",
    "field",
    "results",
    "end",
    "epoch",
    "outside",
    "epochs",
    "loop",
    "loop",
    "outside",
    "let",
    "return",
    "results",
    "probably",
    "got",
    "error",
    "somewhere",
    "might",
    "able",
    "spot",
    "okay",
    "train",
    "data",
    "loader",
    "get",
    "invalid",
    "syntax",
    "maybe",
    "comma",
    "issue",
    "whole",
    "time",
    "wonderful",
    "might",
    "seen",
    "completely",
    "missed",
    "train",
    "function",
    "train",
    "model",
    "train",
    "function",
    "course",
    "going",
    "call",
    "train",
    "step",
    "function",
    "test",
    "step",
    "function",
    "left",
    "well",
    "nothing",
    "less",
    "train",
    "evaluate",
    "model",
    "zero",
    "model",
    "way",
    "back",
    "next",
    "video",
    "leverage",
    "functions",
    "namely",
    "train",
    "function",
    "going",
    "call",
    "train",
    "step",
    "function",
    "test",
    "step",
    "function",
    "train",
    "model",
    "going",
    "encourage",
    "give",
    "go",
    "going",
    "go",
    "back",
    "workflow",
    "maybe",
    "maybe",
    "already",
    "know",
    "done",
    "got",
    "data",
    "ready",
    "turned",
    "tenses",
    "using",
    "combination",
    "functions",
    "built",
    "picked",
    "model",
    "built",
    "model",
    "tiny",
    "vgg",
    "architecture",
    "created",
    "loss",
    "function",
    "yet",
    "think",
    "optimizer",
    "think",
    "done",
    "yet",
    "definitely",
    "built",
    "training",
    "loop",
    "though",
    "using",
    "torch",
    "metrics",
    "using",
    "accuracy",
    "could",
    "use",
    "want",
    "improved",
    "experimentation",
    "yet",
    "going",
    "try",
    "later",
    "save",
    "reload",
    "model",
    "seen",
    "think",
    "picking",
    "loss",
    "function",
    "optimizer",
    "give",
    "shot",
    "next",
    "video",
    "going",
    "create",
    "loss",
    "function",
    "optimizer",
    "leverage",
    "functions",
    "spent",
    "last",
    "two",
    "videos",
    "creating",
    "train",
    "first",
    "model",
    "model",
    "zero",
    "custom",
    "data",
    "set",
    "super",
    "exciting",
    "see",
    "next",
    "video",
    "ready",
    "train",
    "evaluate",
    "model",
    "zero",
    "put",
    "hand",
    "definitely",
    "let",
    "together",
    "going",
    "start",
    "section",
    "going",
    "put",
    "train",
    "evaluate",
    "model",
    "zero",
    "baseline",
    "model",
    "custom",
    "data",
    "set",
    "refer",
    "back",
    "pytorch",
    "workflow",
    "issued",
    "challenge",
    "last",
    "video",
    "try",
    "create",
    "loss",
    "function",
    "optimizer",
    "hope",
    "gave",
    "go",
    "already",
    "built",
    "training",
    "loop",
    "going",
    "leverage",
    "training",
    "loop",
    "functions",
    "namely",
    "train",
    "train",
    "step",
    "test",
    "step",
    "need",
    "instantiate",
    "model",
    "choose",
    "loss",
    "function",
    "optimizer",
    "pass",
    "values",
    "training",
    "function",
    "let",
    "right",
    "exciting",
    "let",
    "set",
    "random",
    "seeds",
    "going",
    "set",
    "torch",
    "manual",
    "seed",
    "42",
    "torch",
    "cuda",
    "manual",
    "seed",
    "remember",
    "want",
    "highlight",
    "something",
    "read",
    "article",
    "day",
    "using",
    "random",
    "seeds",
    "reason",
    "using",
    "random",
    "seeds",
    "educational",
    "purposes",
    "try",
    "get",
    "numbers",
    "screen",
    "screen",
    "close",
    "possible",
    "practice",
    "quite",
    "often",
    "use",
    "random",
    "seeds",
    "time",
    "reason",
    "want",
    "models",
    "performance",
    "similar",
    "regardless",
    "random",
    "seed",
    "use",
    "keep",
    "mind",
    "going",
    "forward",
    "using",
    "random",
    "seeds",
    "exemplify",
    "get",
    "similar",
    "numbers",
    "page",
    "ideally",
    "matter",
    "random",
    "seed",
    "models",
    "would",
    "go",
    "direction",
    "want",
    "models",
    "eventually",
    "go",
    "going",
    "train",
    "five",
    "epochs",
    "let",
    "create",
    "recreate",
    "instance",
    "tiny",
    "vgg",
    "created",
    "tiny",
    "vgg",
    "class",
    "tiny",
    "vgg",
    "model",
    "zero",
    "going",
    "later",
    "got",
    "code",
    "one",
    "place",
    "tiny",
    "vgg",
    "input",
    "shape",
    "going",
    "number",
    "color",
    "channels",
    "target",
    "images",
    "dealing",
    "color",
    "images",
    "input",
    "shape",
    "three",
    "previously",
    "used",
    "input",
    "shape",
    "one",
    "deal",
    "grayscale",
    "images",
    "going",
    "set",
    "hidden",
    "units",
    "10",
    "line",
    "cnn",
    "explainer",
    "website",
    "output",
    "shape",
    "going",
    "number",
    "classes",
    "training",
    "data",
    "set",
    "course",
    "going",
    "send",
    "target",
    "model",
    "target",
    "device",
    "well",
    "set",
    "loss",
    "function",
    "optimizer",
    "loss",
    "function",
    "optimizer",
    "loss",
    "function",
    "going",
    "dealing",
    "multiclass",
    "classification",
    "cross",
    "entropy",
    "could",
    "spell",
    "cross",
    "entropy",
    "loss",
    "going",
    "optimizer",
    "time",
    "mix",
    "things",
    "try",
    "atom",
    "optimizer",
    "course",
    "optimizer",
    "one",
    "hyper",
    "parameters",
    "set",
    "model",
    "hyper",
    "parameter",
    "value",
    "set",
    "parameters",
    "want",
    "optimize",
    "model",
    "zero",
    "parameters",
    "going",
    "set",
    "learning",
    "rate",
    "recall",
    "tweet",
    "learning",
    "rate",
    "like",
    "believe",
    "see",
    "default",
    "learning",
    "rate",
    "atom",
    "yeah",
    "go",
    "adam",
    "default",
    "learning",
    "rate",
    "one",
    "power",
    "10",
    "negative",
    "three",
    "default",
    "learning",
    "rate",
    "adam",
    "said",
    "oftentimes",
    "different",
    "variables",
    "pytorch",
    "library",
    "optimizers",
    "good",
    "default",
    "values",
    "work",
    "across",
    "wide",
    "range",
    "problems",
    "going",
    "stick",
    "default",
    "want",
    "experiment",
    "different",
    "values",
    "let",
    "start",
    "timer",
    "want",
    "time",
    "models",
    "going",
    "import",
    "time",
    "want",
    "get",
    "default",
    "timer",
    "class",
    "going",
    "import",
    "timer",
    "type",
    "default",
    "timer",
    "start",
    "time",
    "going",
    "timer",
    "going",
    "put",
    "line",
    "sand",
    "start",
    "time",
    "particular",
    "line",
    "code",
    "going",
    "measure",
    "going",
    "train",
    "model",
    "zero",
    "using",
    "course",
    "train",
    "function",
    "let",
    "write",
    "model",
    "zero",
    "results",
    "wrote",
    "model",
    "one",
    "yet",
    "let",
    "go",
    "train",
    "model",
    "equals",
    "model",
    "zero",
    "training",
    "function",
    "wrote",
    "previous",
    "video",
    "train",
    "data",
    "going",
    "train",
    "data",
    "loader",
    "got",
    "train",
    "data",
    "loader",
    "simple",
    "using",
    "data",
    "augmentation",
    "model",
    "one",
    "test",
    "data",
    "loader",
    "going",
    "test",
    "data",
    "loader",
    "simple",
    "going",
    "set",
    "optimizer",
    "equal",
    "optimizer",
    "created",
    "friendly",
    "atom",
    "optimizer",
    "loss",
    "function",
    "going",
    "loss",
    "function",
    "created",
    "n",
    "cross",
    "entropy",
    "loss",
    "finally",
    "send",
    "epochs",
    "going",
    "num",
    "epochs",
    "set",
    "start",
    "video",
    "five",
    "course",
    "could",
    "train",
    "model",
    "longer",
    "wanted",
    "whole",
    "idea",
    "first",
    "start",
    "training",
    "model",
    "keep",
    "experiments",
    "quick",
    "training",
    "five",
    "maybe",
    "later",
    "train",
    "10",
    "20",
    "tweak",
    "learning",
    "rate",
    "whole",
    "bunch",
    "different",
    "things",
    "let",
    "go",
    "let",
    "end",
    "timer",
    "see",
    "long",
    "models",
    "took",
    "train",
    "timer",
    "print",
    "long",
    "took",
    "previous",
    "section",
    "created",
    "helper",
    "function",
    "going",
    "simplify",
    "section",
    "going",
    "print",
    "long",
    "training",
    "time",
    "total",
    "training",
    "time",
    "let",
    "go",
    "n",
    "time",
    "minus",
    "start",
    "time",
    "going",
    "go",
    "point",
    "take",
    "three",
    "decimal",
    "places",
    "hey",
    "seconds",
    "ready",
    "train",
    "first",
    "model",
    "first",
    "convolutional",
    "neural",
    "network",
    "custom",
    "data",
    "set",
    "pizza",
    "stake",
    "sushi",
    "images",
    "let",
    "ready",
    "three",
    "two",
    "one",
    "errors",
    "oh",
    "go",
    "okay",
    "trained",
    "data",
    "loader",
    "notice",
    "trained",
    "data",
    "taker",
    "input",
    "oh",
    "getting",
    "doc",
    "string",
    "oh",
    "go",
    "want",
    "trained",
    "data",
    "loader",
    "data",
    "loader",
    "believe",
    "let",
    "try",
    "beautiful",
    "oh",
    "look",
    "lovely",
    "progress",
    "bar",
    "okay",
    "model",
    "training",
    "quite",
    "fast",
    "okay",
    "right",
    "get",
    "get",
    "accuracy",
    "training",
    "data",
    "set",
    "40",
    "get",
    "accuracy",
    "test",
    "data",
    "set",
    "50",
    "telling",
    "us",
    "telling",
    "us",
    "50",
    "time",
    "model",
    "getting",
    "prediction",
    "correct",
    "got",
    "three",
    "classes",
    "even",
    "model",
    "guessing",
    "would",
    "get",
    "things",
    "right",
    "33",
    "time",
    "even",
    "guessed",
    "pizza",
    "every",
    "single",
    "time",
    "three",
    "classes",
    "guessed",
    "pizza",
    "every",
    "single",
    "time",
    "get",
    "baseline",
    "accuracy",
    "33",
    "model",
    "much",
    "better",
    "baseline",
    "accuracy",
    "course",
    "like",
    "number",
    "go",
    "higher",
    "maybe",
    "would",
    "trained",
    "longer",
    "let",
    "experiment",
    "like",
    "see",
    "different",
    "methods",
    "improving",
    "model",
    "recall",
    "back",
    "section",
    "number",
    "two",
    "improving",
    "model",
    "section",
    "improving",
    "model",
    "go",
    "things",
    "might",
    "want",
    "try",
    "improve",
    "model",
    "adding",
    "layers",
    "come",
    "back",
    "tiny",
    "vgg",
    "architecture",
    "right",
    "using",
    "two",
    "convolutional",
    "blocks",
    "perhaps",
    "wanted",
    "add",
    "convolutional",
    "block",
    "three",
    "also",
    "add",
    "hidden",
    "units",
    "right",
    "using",
    "10",
    "hidden",
    "units",
    "might",
    "want",
    "double",
    "see",
    "happens",
    "fitting",
    "longer",
    "spoke",
    "right",
    "fitting",
    "five",
    "epochs",
    "maybe",
    "wanted",
    "try",
    "double",
    "even",
    "double",
    "changing",
    "activation",
    "functions",
    "maybe",
    "relu",
    "ideal",
    "activation",
    "function",
    "specific",
    "use",
    "case",
    "change",
    "learning",
    "rate",
    "spoken",
    "right",
    "learning",
    "rate",
    "adam",
    "default",
    "perhaps",
    "better",
    "learning",
    "rate",
    "change",
    "loss",
    "function",
    "probably",
    "case",
    "going",
    "help",
    "much",
    "cross",
    "entropy",
    "loss",
    "pretty",
    "good",
    "loss",
    "multi",
    "class",
    "classification",
    "things",
    "could",
    "try",
    "first",
    "three",
    "especially",
    "could",
    "try",
    "quite",
    "quickly",
    "could",
    "try",
    "doubling",
    "layers",
    "could",
    "try",
    "adding",
    "hidden",
    "units",
    "could",
    "try",
    "fitting",
    "longer",
    "give",
    "shot",
    "next",
    "video",
    "going",
    "take",
    "model",
    "zero",
    "results",
    "dictionary",
    "least",
    "going",
    "plot",
    "loss",
    "curves",
    "good",
    "way",
    "inspect",
    "model",
    "training",
    "yes",
    "got",
    "values",
    "let",
    "plot",
    "next",
    "video",
    "see",
    "last",
    "video",
    "trained",
    "first",
    "convolutional",
    "neural",
    "network",
    "custom",
    "data",
    "proud",
    "small",
    "feat",
    "take",
    "data",
    "set",
    "whatever",
    "want",
    "train",
    "apply",
    "model",
    "however",
    "find",
    "perform",
    "well",
    "like",
    "also",
    "highlighted",
    "different",
    "things",
    "could",
    "try",
    "improve",
    "let",
    "plot",
    "models",
    "results",
    "using",
    "loss",
    "curve",
    "going",
    "write",
    "another",
    "heading",
    "go",
    "believe",
    "plot",
    "loss",
    "curves",
    "model",
    "zero",
    "loss",
    "curve",
    "going",
    "write",
    "loss",
    "curve",
    "way",
    "tracking",
    "models",
    "progress",
    "time",
    "looked",
    "google",
    "looked",
    "loss",
    "curves",
    "oh",
    "great",
    "guide",
    "way",
    "going",
    "link",
    "rather",
    "doubt",
    "code",
    "look",
    "guides",
    "yeah",
    "loss",
    "curves",
    "yeah",
    "loss",
    "time",
    "loss",
    "value",
    "left",
    "say",
    "steps",
    "epochs",
    "batches",
    "something",
    "like",
    "got",
    "whole",
    "bunch",
    "different",
    "loss",
    "curves",
    "essentially",
    "want",
    "go",
    "time",
    "idea",
    "loss",
    "curve",
    "let",
    "go",
    "back",
    "good",
    "guide",
    "different",
    "loss",
    "curves",
    "seen",
    "going",
    "go",
    "yet",
    "let",
    "focus",
    "plotting",
    "models",
    "loss",
    "curves",
    "inspect",
    "let",
    "get",
    "model",
    "keys",
    "get",
    "model",
    "zero",
    "results",
    "keys",
    "going",
    "type",
    "model",
    "zero",
    "results",
    "dot",
    "keys",
    "dictionary",
    "let",
    "see",
    "write",
    "code",
    "plot",
    "values",
    "yeah",
    "time",
    "one",
    "value",
    "train",
    "loss",
    "train",
    "act",
    "test",
    "loss",
    "test",
    "act",
    "every",
    "epoch",
    "course",
    "lists",
    "would",
    "longer",
    "train",
    "epochs",
    "let",
    "create",
    "function",
    "called",
    "def",
    "plot",
    "loss",
    "curves",
    "take",
    "results",
    "dictionary",
    "string",
    "list",
    "floats",
    "means",
    "results",
    "parameter",
    "taking",
    "dictionary",
    "string",
    "key",
    "contains",
    "list",
    "floats",
    "means",
    "let",
    "write",
    "doc",
    "string",
    "plots",
    "training",
    "curves",
    "results",
    "dictionary",
    "beautiful",
    "section",
    "workflow",
    "kind",
    "like",
    "kind",
    "something",
    "similar",
    "tensorboard",
    "let",
    "look",
    "want",
    "otherwise",
    "going",
    "see",
    "later",
    "really",
    "evaluating",
    "model",
    "let",
    "write",
    "plotting",
    "code",
    "going",
    "use",
    "map",
    "plot",
    "lib",
    "want",
    "get",
    "lost",
    "values",
    "results",
    "dictionary",
    "training",
    "test",
    "let",
    "set",
    "loss",
    "equal",
    "results",
    "train",
    "loss",
    "going",
    "loss",
    "training",
    "data",
    "set",
    "create",
    "test",
    "loss",
    "going",
    "well",
    "index",
    "results",
    "dictionary",
    "get",
    "test",
    "loss",
    "beautiful",
    "get",
    "accuracy",
    "get",
    "accuracy",
    "values",
    "results",
    "dictionary",
    "training",
    "test",
    "going",
    "go",
    "accuracy",
    "equals",
    "results",
    "training",
    "accuracy",
    "train",
    "act",
    "accuracy",
    "oh",
    "call",
    "test",
    "accuracy",
    "actually",
    "test",
    "accuracy",
    "equals",
    "results",
    "test",
    "act",
    "let",
    "create",
    "number",
    "epochs",
    "want",
    "figure",
    "many",
    "epochs",
    "counting",
    "length",
    "value",
    "figure",
    "many",
    "epochs",
    "set",
    "epochs",
    "equal",
    "range",
    "want",
    "plot",
    "time",
    "models",
    "results",
    "time",
    "whole",
    "idea",
    "loss",
    "curve",
    "get",
    "length",
    "results",
    "get",
    "range",
    "set",
    "plot",
    "let",
    "go",
    "plt",
    "dot",
    "figure",
    "set",
    "fig",
    "size",
    "equal",
    "something",
    "nice",
    "big",
    "going",
    "four",
    "plots",
    "want",
    "one",
    "maybe",
    "two",
    "plots",
    "one",
    "loss",
    "one",
    "accuracy",
    "go",
    "plot",
    "loss",
    "plt",
    "dot",
    "subplot",
    "going",
    "create",
    "one",
    "row",
    "two",
    "columns",
    "index",
    "number",
    "one",
    "want",
    "put",
    "plt",
    "dot",
    "plot",
    "going",
    "plot",
    "training",
    "loss",
    "get",
    "label",
    "train",
    "loss",
    "add",
    "another",
    "plot",
    "epochs",
    "test",
    "loss",
    "label",
    "going",
    "test",
    "loss",
    "add",
    "title",
    "loss",
    "plt",
    "let",
    "put",
    "label",
    "x",
    "epochs",
    "know",
    "many",
    "steps",
    "done",
    "plot",
    "loss",
    "curves",
    "uses",
    "steps",
    "going",
    "use",
    "epochs",
    "mean",
    "almost",
    "thing",
    "depends",
    "scale",
    "like",
    "see",
    "loss",
    "curves",
    "get",
    "legend",
    "well",
    "labels",
    "appear",
    "going",
    "plot",
    "accuracy",
    "plt",
    "dot",
    "subplot",
    "let",
    "go",
    "one",
    "two",
    "index",
    "number",
    "two",
    "plot",
    "going",
    "plt",
    "dot",
    "plot",
    "going",
    "go",
    "epochs",
    "accuracy",
    "label",
    "going",
    "train",
    "accuracy",
    "get",
    "next",
    "plot",
    "actually",
    "going",
    "plot",
    "put",
    "test",
    "accuracy",
    "way",
    "test",
    "accuracy",
    "training",
    "accuracy",
    "side",
    "side",
    "test",
    "accuracy",
    "train",
    "loss",
    "train",
    "sorry",
    "test",
    "loss",
    "give",
    "plot",
    "title",
    "plot",
    "going",
    "accuracy",
    "going",
    "give",
    "x",
    "label",
    "going",
    "epochs",
    "well",
    "finally",
    "get",
    "plot",
    "legend",
    "lot",
    "plotting",
    "code",
    "let",
    "see",
    "looks",
    "like",
    "hey",
    "done",
    "right",
    "able",
    "pass",
    "dictionary",
    "like",
    "see",
    "nice",
    "plots",
    "like",
    "let",
    "give",
    "go",
    "going",
    "call",
    "plot",
    "loss",
    "curves",
    "going",
    "pass",
    "model",
    "0",
    "results",
    "righty",
    "okay",
    "bad",
    "say",
    "well",
    "looking",
    "mainly",
    "trends",
    "trained",
    "model",
    "long",
    "quantitatively",
    "know",
    "model",
    "performed",
    "way",
    "like",
    "like",
    "accuracy",
    "train",
    "test",
    "data",
    "sets",
    "higher",
    "course",
    "accuracy",
    "going",
    "higher",
    "loss",
    "going",
    "come",
    "ideal",
    "trend",
    "loss",
    "curve",
    "go",
    "top",
    "left",
    "bottom",
    "right",
    "words",
    "loss",
    "going",
    "time",
    "trend",
    "right",
    "potentially",
    "train",
    "epochs",
    "encourage",
    "give",
    "go",
    "model",
    "loss",
    "might",
    "get",
    "lower",
    "accuracy",
    "also",
    "trending",
    "right",
    "way",
    "accuracy",
    "want",
    "go",
    "time",
    "train",
    "epochs",
    "curves",
    "may",
    "continue",
    "go",
    "may",
    "never",
    "really",
    "know",
    "right",
    "guess",
    "things",
    "try",
    "really",
    "know",
    "next",
    "video",
    "going",
    "look",
    "different",
    "forms",
    "loss",
    "curves",
    "encourage",
    "go",
    "guide",
    "interpreting",
    "loss",
    "curves",
    "feel",
    "like",
    "search",
    "loss",
    "curves",
    "going",
    "find",
    "google",
    "guide",
    "could",
    "search",
    "interpreting",
    "loss",
    "curves",
    "see",
    "many",
    "different",
    "ways",
    "loss",
    "curves",
    "interpreted",
    "ideal",
    "trend",
    "loss",
    "go",
    "time",
    "metrics",
    "like",
    "accuracy",
    "go",
    "time",
    "next",
    "video",
    "let",
    "cover",
    "different",
    "forms",
    "loss",
    "curves",
    "ideal",
    "loss",
    "curve",
    "looks",
    "like",
    "model",
    "underfitting",
    "looks",
    "like",
    "model",
    "overfitting",
    "like",
    "primer",
    "things",
    "read",
    "guide",
    "worry",
    "much",
    "sure",
    "happening",
    "going",
    "cover",
    "bit",
    "loss",
    "curves",
    "next",
    "video",
    "see",
    "last",
    "video",
    "looked",
    "model",
    "loss",
    "curves",
    "also",
    "accuracy",
    "curves",
    "loss",
    "curve",
    "way",
    "evaluate",
    "model",
    "performance",
    "time",
    "long",
    "training",
    "see",
    "google",
    "images",
    "loss",
    "curves",
    "see",
    "many",
    "different",
    "types",
    "loss",
    "curves",
    "come",
    "different",
    "shapes",
    "sizes",
    "many",
    "different",
    "ways",
    "interpret",
    "loss",
    "curves",
    "google",
    "testing",
    "debugging",
    "machine",
    "learning",
    "guide",
    "going",
    "set",
    "actually",
    "curriculum",
    "section",
    "number",
    "eight",
    "let",
    "look",
    "ideal",
    "loss",
    "curve",
    "look",
    "like",
    "link",
    "loss",
    "curve",
    "rewrite",
    "loss",
    "curve",
    "make",
    "space",
    "loss",
    "curve",
    "one",
    "helpful",
    "ways",
    "troubleshoot",
    "model",
    "trend",
    "loss",
    "curve",
    "want",
    "go",
    "time",
    "trend",
    "typically",
    "evaluation",
    "metric",
    "like",
    "accuracy",
    "want",
    "go",
    "time",
    "let",
    "go",
    "keynote",
    "loss",
    "curves",
    "way",
    "evaluate",
    "model",
    "performance",
    "time",
    "three",
    "main",
    "different",
    "forms",
    "loss",
    "curve",
    "face",
    "many",
    "different",
    "types",
    "mentioned",
    "interpreting",
    "loss",
    "curves",
    "sometimes",
    "get",
    "going",
    "place",
    "sometimes",
    "loss",
    "explode",
    "sometimes",
    "metrics",
    "contradictory",
    "sometimes",
    "testing",
    "loss",
    "higher",
    "training",
    "loss",
    "look",
    "sometimes",
    "model",
    "gets",
    "stuck",
    "words",
    "loss",
    "reduce",
    "let",
    "look",
    "loss",
    "curves",
    "case",
    "underfitting",
    "overfitting",
    "right",
    "goldilocks",
    "zone",
    "underfitting",
    "model",
    "loss",
    "training",
    "test",
    "data",
    "sets",
    "could",
    "lower",
    "case",
    "go",
    "back",
    "loss",
    "curves",
    "course",
    "want",
    "lower",
    "want",
    "accuracy",
    "higher",
    "perspective",
    "looks",
    "like",
    "model",
    "underfitting",
    "would",
    "probably",
    "want",
    "train",
    "longer",
    "say",
    "10",
    "20",
    "epochs",
    "see",
    "train",
    "continues",
    "keeps",
    "going",
    "may",
    "stop",
    "underfitting",
    "underfitting",
    "loss",
    "could",
    "lower",
    "inverse",
    "underfitting",
    "called",
    "overfitting",
    "two",
    "biggest",
    "problems",
    "machine",
    "learning",
    "trying",
    "underfitting",
    "words",
    "make",
    "loss",
    "lower",
    "also",
    "reduce",
    "overfitting",
    "active",
    "areas",
    "research",
    "always",
    "want",
    "model",
    "perform",
    "better",
    "also",
    "want",
    "perform",
    "pretty",
    "much",
    "training",
    "set",
    "test",
    "set",
    "overfitting",
    "would",
    "training",
    "loss",
    "lower",
    "testing",
    "loss",
    "would",
    "overfitting",
    "means",
    "overfitting",
    "model",
    "essentially",
    "learning",
    "training",
    "data",
    "well",
    "means",
    "loss",
    "goes",
    "training",
    "data",
    "set",
    "typically",
    "good",
    "thing",
    "however",
    "learning",
    "reflected",
    "testing",
    "data",
    "set",
    "model",
    "essentially",
    "memorizing",
    "patterns",
    "training",
    "data",
    "set",
    "generalize",
    "well",
    "test",
    "data",
    "set",
    "come",
    "right",
    "curve",
    "want",
    "ideally",
    "training",
    "loss",
    "reduce",
    "much",
    "test",
    "loss",
    "quite",
    "often",
    "find",
    "loss",
    "slightly",
    "lower",
    "training",
    "set",
    "test",
    "set",
    "model",
    "exposed",
    "training",
    "data",
    "never",
    "seen",
    "test",
    "data",
    "might",
    "little",
    "bit",
    "lower",
    "training",
    "data",
    "set",
    "test",
    "data",
    "set",
    "underfitting",
    "model",
    "loss",
    "could",
    "lower",
    "overfitting",
    "model",
    "learning",
    "training",
    "data",
    "well",
    "would",
    "equivalent",
    "say",
    "studying",
    "final",
    "exam",
    "memorize",
    "course",
    "materials",
    "training",
    "set",
    "came",
    "time",
    "final",
    "exam",
    "even",
    "memorize",
    "course",
    "materials",
    "could",
    "adapt",
    "skills",
    "questions",
    "seen",
    "final",
    "exam",
    "would",
    "test",
    "set",
    "overfitting",
    "train",
    "loss",
    "lower",
    "test",
    "loss",
    "right",
    "ideally",
    "probably",
    "wo",
    "see",
    "loss",
    "curves",
    "exact",
    "smooth",
    "mean",
    "might",
    "little",
    "bit",
    "jumpy",
    "ideally",
    "training",
    "loss",
    "test",
    "loss",
    "go",
    "similar",
    "rate",
    "course",
    "combinations",
    "like",
    "see",
    "check",
    "google",
    "loss",
    "curve",
    "guide",
    "check",
    "extra",
    "curriculum",
    "probably",
    "want",
    "know",
    "deal",
    "underfitting",
    "overfitting",
    "let",
    "look",
    "ways",
    "start",
    "overfitting",
    "want",
    "reduce",
    "overfitting",
    "words",
    "want",
    "model",
    "perform",
    "well",
    "training",
    "data",
    "set",
    "test",
    "data",
    "set",
    "one",
    "best",
    "ways",
    "reduce",
    "overfitting",
    "get",
    "data",
    "means",
    "training",
    "data",
    "set",
    "larger",
    "model",
    "exposed",
    "examples",
    "us",
    "theory",
    "always",
    "work",
    "come",
    "caveat",
    "right",
    "always",
    "work",
    "many",
    "things",
    "machine",
    "learning",
    "get",
    "data",
    "give",
    "model",
    "chance",
    "learn",
    "patterns",
    "generalizable",
    "patterns",
    "data",
    "set",
    "use",
    "data",
    "augmentation",
    "make",
    "models",
    "training",
    "data",
    "set",
    "harder",
    "learn",
    "seen",
    "examples",
    "data",
    "augmentation",
    "get",
    "better",
    "data",
    "data",
    "perhaps",
    "data",
    "using",
    "quality",
    "good",
    "enhance",
    "quality",
    "data",
    "set",
    "model",
    "may",
    "able",
    "learn",
    "better",
    "generalizable",
    "patterns",
    "turn",
    "reduce",
    "overfitting",
    "use",
    "transfer",
    "learning",
    "going",
    "cover",
    "later",
    "section",
    "course",
    "transfer",
    "learning",
    "taking",
    "one",
    "model",
    "works",
    "taking",
    "patterns",
    "learned",
    "applying",
    "data",
    "set",
    "example",
    "go",
    "torch",
    "vision",
    "models",
    "library",
    "many",
    "models",
    "torch",
    "vision",
    "models",
    "module",
    "already",
    "trained",
    "certain",
    "data",
    "set",
    "imagenet",
    "take",
    "weights",
    "patterns",
    "models",
    "learned",
    "work",
    "well",
    "imagenet",
    "data",
    "set",
    "millions",
    "different",
    "images",
    "adjust",
    "patterns",
    "problem",
    "oftentimes",
    "help",
    "overfitting",
    "still",
    "overfitting",
    "try",
    "simplify",
    "model",
    "usually",
    "means",
    "taking",
    "away",
    "things",
    "like",
    "extra",
    "layers",
    "taking",
    "away",
    "hidden",
    "units",
    "say",
    "10",
    "layers",
    "might",
    "reduce",
    "five",
    "layers",
    "theory",
    "behind",
    "well",
    "simplify",
    "model",
    "take",
    "away",
    "complexity",
    "model",
    "kind",
    "telling",
    "model",
    "hey",
    "use",
    "got",
    "going",
    "got",
    "five",
    "layers",
    "going",
    "make",
    "sure",
    "five",
    "layers",
    "work",
    "really",
    "well",
    "longer",
    "got",
    "hidden",
    "units",
    "say",
    "started",
    "100",
    "hidden",
    "units",
    "per",
    "layer",
    "might",
    "reduce",
    "50",
    "say",
    "hey",
    "100",
    "use",
    "50",
    "make",
    "patterns",
    "generalizable",
    "use",
    "learning",
    "rate",
    "decay",
    "learning",
    "rate",
    "much",
    "optimizer",
    "updates",
    "model",
    "weight",
    "every",
    "step",
    "learning",
    "rate",
    "decay",
    "decay",
    "learning",
    "rate",
    "time",
    "might",
    "look",
    "look",
    "go",
    "high",
    "torch",
    "learning",
    "rate",
    "scheduling",
    "means",
    "want",
    "decrease",
    "learning",
    "rate",
    "time",
    "know",
    "giving",
    "lot",
    "different",
    "things",
    "got",
    "keynote",
    "reference",
    "come",
    "across",
    "time",
    "learning",
    "rate",
    "scheduling",
    "might",
    "look",
    "schedule",
    "scheduler",
    "beautiful",
    "going",
    "adjust",
    "learning",
    "rate",
    "time",
    "example",
    "start",
    "model",
    "training",
    "might",
    "want",
    "higher",
    "learning",
    "rate",
    "model",
    "starts",
    "learn",
    "patterns",
    "might",
    "want",
    "reduce",
    "learning",
    "rate",
    "time",
    "model",
    "update",
    "patterns",
    "much",
    "later",
    "epochs",
    "concept",
    "learning",
    "rate",
    "scheduling",
    "closer",
    "get",
    "convergence",
    "lower",
    "might",
    "want",
    "set",
    "learning",
    "rate",
    "think",
    "like",
    "reaching",
    "coin",
    "back",
    "couch",
    "get",
    "image",
    "coin",
    "back",
    "couch",
    "images",
    "trying",
    "reach",
    "coin",
    "cushions",
    "closer",
    "get",
    "coin",
    "beginning",
    "might",
    "take",
    "big",
    "steps",
    "closer",
    "get",
    "coin",
    "smaller",
    "step",
    "might",
    "take",
    "pick",
    "coin",
    "take",
    "big",
    "step",
    "really",
    "close",
    "coin",
    "coin",
    "might",
    "fall",
    "couch",
    "thing",
    "learning",
    "rate",
    "decay",
    "start",
    "model",
    "training",
    "might",
    "take",
    "bigger",
    "steps",
    "model",
    "works",
    "way",
    "loss",
    "curve",
    "get",
    "closer",
    "closer",
    "ideal",
    "position",
    "loss",
    "curve",
    "might",
    "start",
    "lower",
    "lower",
    "learning",
    "rate",
    "get",
    "right",
    "close",
    "end",
    "pick",
    "coin",
    "words",
    "model",
    "converge",
    "finally",
    "use",
    "early",
    "stopping",
    "go",
    "image",
    "early",
    "stopping",
    "early",
    "stopping",
    "loss",
    "curves",
    "early",
    "stopping",
    "means",
    "stop",
    "yeah",
    "go",
    "heaps",
    "different",
    "guides",
    "early",
    "stopping",
    "pytorch",
    "beautiful",
    "means",
    "testing",
    "error",
    "starts",
    "go",
    "keep",
    "track",
    "model",
    "testing",
    "error",
    "stop",
    "model",
    "training",
    "save",
    "weight",
    "save",
    "patterns",
    "model",
    "loss",
    "lowest",
    "could",
    "set",
    "model",
    "train",
    "infinite",
    "amount",
    "training",
    "steps",
    "soon",
    "testing",
    "error",
    "starts",
    "increase",
    "say",
    "10",
    "steps",
    "row",
    "go",
    "back",
    "point",
    "go",
    "think",
    "model",
    "best",
    "testing",
    "error",
    "started",
    "increase",
    "going",
    "save",
    "model",
    "instead",
    "model",
    "concept",
    "early",
    "stopping",
    "dealing",
    "overfitting",
    "methods",
    "deal",
    "underfitting",
    "recall",
    "underfitting",
    "loss",
    "low",
    "like",
    "model",
    "fitting",
    "data",
    "well",
    "underfitting",
    "reduce",
    "underfitting",
    "add",
    "layers",
    "slash",
    "units",
    "model",
    "trying",
    "increase",
    "model",
    "ability",
    "learn",
    "adding",
    "layers",
    "units",
    "tweak",
    "learning",
    "rate",
    "perhaps",
    "learning",
    "rate",
    "high",
    "begin",
    "model",
    "learn",
    "well",
    "adjust",
    "learning",
    "rate",
    "like",
    "discussed",
    "reaching",
    "coin",
    "back",
    "couch",
    "model",
    "still",
    "underfitting",
    "train",
    "longer",
    "means",
    "giving",
    "model",
    "opportunities",
    "look",
    "data",
    "epochs",
    "means",
    "got",
    "looking",
    "training",
    "set",
    "trying",
    "learn",
    "patterns",
    "however",
    "might",
    "find",
    "try",
    "train",
    "long",
    "testing",
    "error",
    "start",
    "go",
    "model",
    "might",
    "start",
    "overfitting",
    "train",
    "long",
    "machine",
    "learning",
    "balance",
    "underfitting",
    "overfitting",
    "want",
    "model",
    "fit",
    "quite",
    "well",
    "great",
    "one",
    "want",
    "model",
    "start",
    "fitting",
    "quite",
    "well",
    "try",
    "reduce",
    "underfitting",
    "much",
    "might",
    "start",
    "overfit",
    "vice",
    "versa",
    "right",
    "try",
    "reduce",
    "overfitting",
    "much",
    "model",
    "might",
    "underfit",
    "one",
    "fun",
    "dances",
    "machine",
    "learning",
    "balance",
    "overfitting",
    "underfitting",
    "finally",
    "might",
    "use",
    "transfer",
    "learning",
    "transfer",
    "learning",
    "helps",
    "overfitting",
    "underfitting",
    "recall",
    "transfer",
    "learning",
    "using",
    "model",
    "learned",
    "patterns",
    "ron",
    "problem",
    "adjusting",
    "going",
    "see",
    "later",
    "course",
    "finally",
    "use",
    "less",
    "regularization",
    "regularization",
    "holding",
    "model",
    "back",
    "trying",
    "prevent",
    "overfitting",
    "much",
    "preventing",
    "overfitting",
    "words",
    "regularizing",
    "model",
    "might",
    "end",
    "underfitting",
    "go",
    "back",
    "look",
    "ideal",
    "curves",
    "underfitting",
    "try",
    "prevent",
    "underfitting",
    "much",
    "increasing",
    "model",
    "capability",
    "learn",
    "might",
    "end",
    "overfitting",
    "try",
    "prevent",
    "overfitting",
    "much",
    "might",
    "end",
    "underfitting",
    "going",
    "right",
    "section",
    "going",
    "balance",
    "two",
    "throughout",
    "entire",
    "machine",
    "learning",
    "career",
    "fact",
    "probably",
    "prevalent",
    "area",
    "research",
    "trying",
    "get",
    "models",
    "underfit",
    "also",
    "overfit",
    "keep",
    "mind",
    "loss",
    "curve",
    "great",
    "way",
    "evaluate",
    "model",
    "performance",
    "time",
    "lot",
    "loss",
    "curves",
    "try",
    "work",
    "whether",
    "model",
    "underfitting",
    "overfitting",
    "trying",
    "get",
    "right",
    "curve",
    "might",
    "get",
    "exactly",
    "want",
    "keep",
    "trying",
    "getting",
    "close",
    "said",
    "let",
    "build",
    "another",
    "model",
    "next",
    "video",
    "going",
    "try",
    "method",
    "try",
    "see",
    "use",
    "data",
    "augmentation",
    "prevent",
    "model",
    "overfitting",
    "although",
    "experiment",
    "sound",
    "like",
    "ideal",
    "one",
    "could",
    "right",
    "looks",
    "like",
    "model",
    "underfitting",
    "knowledge",
    "learned",
    "previous",
    "video",
    "prevent",
    "underfitting",
    "would",
    "increase",
    "model",
    "capability",
    "learning",
    "patterns",
    "training",
    "data",
    "set",
    "would",
    "train",
    "longer",
    "would",
    "add",
    "layers",
    "would",
    "add",
    "hidden",
    "units",
    "think",
    "start",
    "building",
    "another",
    "model",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "covered",
    "important",
    "concept",
    "loss",
    "curve",
    "give",
    "us",
    "information",
    "whether",
    "model",
    "underfitting",
    "words",
    "model",
    "loss",
    "could",
    "lower",
    "whether",
    "overfitting",
    "words",
    "training",
    "loss",
    "lower",
    "test",
    "loss",
    "far",
    "lower",
    "validation",
    "loss",
    "another",
    "thing",
    "note",
    "put",
    "training",
    "test",
    "sets",
    "could",
    "also",
    "validation",
    "data",
    "set",
    "right",
    "goldilocks",
    "zone",
    "training",
    "test",
    "loss",
    "quite",
    "similar",
    "time",
    "fair",
    "bit",
    "information",
    "last",
    "video",
    "wanted",
    "highlight",
    "get",
    "section",
    "04",
    "notebook",
    "working",
    "come",
    "come",
    "section",
    "8",
    "watch",
    "ideal",
    "loss",
    "curve",
    "look",
    "like",
    "got",
    "underfitting",
    "overfitting",
    "right",
    "deal",
    "overfitting",
    "got",
    "options",
    "got",
    "deal",
    "underfitting",
    "got",
    "options",
    "wanted",
    "look",
    "deal",
    "overfitting",
    "could",
    "find",
    "bunch",
    "resources",
    "deal",
    "underfitting",
    "could",
    "find",
    "bunch",
    "resources",
    "well",
    "fine",
    "line",
    "fine",
    "balance",
    "going",
    "experience",
    "throughout",
    "machine",
    "learning",
    "career",
    "time",
    "move",
    "going",
    "move",
    "creating",
    "another",
    "model",
    "tiny",
    "vgg",
    "data",
    "augmentation",
    "time",
    "go",
    "back",
    "slide",
    "data",
    "augmentation",
    "one",
    "way",
    "dealing",
    "overfitting",
    "probably",
    "ideal",
    "experiment",
    "could",
    "take",
    "model",
    "zero",
    "baseline",
    "model",
    "looks",
    "like",
    "underfitting",
    "data",
    "augmentation",
    "seen",
    "way",
    "manipulating",
    "images",
    "artificially",
    "increase",
    "diversity",
    "training",
    "data",
    "set",
    "without",
    "collecting",
    "data",
    "could",
    "take",
    "photos",
    "pizza",
    "sushi",
    "steak",
    "randomly",
    "rotate",
    "30",
    "degrees",
    "increase",
    "diversity",
    "forces",
    "model",
    "learn",
    "hopefully",
    "learn",
    "come",
    "caveat",
    "always",
    "silver",
    "bullet",
    "learn",
    "generalizable",
    "patterns",
    "spelled",
    "generalizable",
    "rather",
    "generalization",
    "similar",
    "thing",
    "let",
    "go",
    "let",
    "create",
    "start",
    "write",
    "let",
    "try",
    "another",
    "modeling",
    "experiment",
    "line",
    "pytorch",
    "workflow",
    "trying",
    "model",
    "trying",
    "another",
    "one",
    "trying",
    "another",
    "one",
    "time",
    "using",
    "model",
    "slight",
    "data",
    "augmentation",
    "oh",
    "maybe",
    "slight",
    "probably",
    "best",
    "word",
    "say",
    "data",
    "augmentation",
    "come",
    "going",
    "write",
    "section",
    "need",
    "first",
    "create",
    "transform",
    "data",
    "augmentation",
    "seen",
    "looks",
    "like",
    "going",
    "use",
    "trivial",
    "augment",
    "data",
    "augmentation",
    "create",
    "training",
    "transform",
    "saw",
    "previous",
    "video",
    "pytorch",
    "pytorch",
    "team",
    "recently",
    "used",
    "train",
    "computer",
    "vision",
    "models",
    "train",
    "transform",
    "trivial",
    "going",
    "call",
    "transform",
    "going",
    "torch",
    "vision",
    "import",
    "transforms",
    "done",
    "going",
    "anyway",
    "show",
    "using",
    "transforms",
    "going",
    "compose",
    "transform",
    "recall",
    "transforms",
    "help",
    "us",
    "manipulate",
    "data",
    "going",
    "transform",
    "images",
    "size",
    "64",
    "going",
    "set",
    "trivial",
    "augment",
    "transforms",
    "like",
    "trivial",
    "augment",
    "wide",
    "going",
    "set",
    "number",
    "magnitude",
    "bins",
    "31",
    "default",
    "means",
    "randomly",
    "use",
    "data",
    "augmentation",
    "one",
    "images",
    "applied",
    "magnitude",
    "0",
    "31",
    "also",
    "randomly",
    "selected",
    "lower",
    "five",
    "upper",
    "bound",
    "intensity",
    "much",
    "data",
    "augmentation",
    "applied",
    "certain",
    "image",
    "less",
    "set",
    "say",
    "final",
    "transform",
    "going",
    "tensor",
    "want",
    "images",
    "tensor",
    "format",
    "model",
    "going",
    "create",
    "test",
    "transform",
    "going",
    "call",
    "simple",
    "going",
    "transforms",
    "dot",
    "compose",
    "going",
    "oh",
    "put",
    "list",
    "going",
    "make",
    "space",
    "going",
    "transforms",
    "want",
    "resize",
    "image",
    "size",
    "equals",
    "64",
    "apply",
    "data",
    "augmentation",
    "test",
    "data",
    "set",
    "want",
    "evaluate",
    "models",
    "test",
    "data",
    "set",
    "models",
    "going",
    "learning",
    "generalizable",
    "patterns",
    "test",
    "data",
    "set",
    "focus",
    "data",
    "augmentations",
    "training",
    "data",
    "set",
    "readjusted",
    "want",
    "beautiful",
    "got",
    "transform",
    "ready",
    "let",
    "load",
    "data",
    "using",
    "transforms",
    "create",
    "train",
    "test",
    "data",
    "sets",
    "data",
    "loaders",
    "data",
    "augmentation",
    "done",
    "might",
    "want",
    "try",
    "pause",
    "video",
    "like",
    "test",
    "create",
    "data",
    "set",
    "data",
    "loader",
    "using",
    "transforms",
    "recall",
    "data",
    "set",
    "going",
    "creating",
    "data",
    "set",
    "pizza",
    "steak",
    "sushi",
    "train",
    "test",
    "folders",
    "data",
    "loader",
    "going",
    "batchifying",
    "data",
    "set",
    "let",
    "turn",
    "image",
    "folders",
    "data",
    "sets",
    "data",
    "sets",
    "beautiful",
    "going",
    "write",
    "train",
    "data",
    "augmented",
    "know",
    "augmented",
    "got",
    "similar",
    "variable",
    "names",
    "throughout",
    "notebook",
    "want",
    "clear",
    "possible",
    "going",
    "use",
    "import",
    "torch",
    "vision",
    "data",
    "sets",
    "seen",
    "image",
    "folder",
    "rather",
    "use",
    "custom",
    "class",
    "going",
    "use",
    "existing",
    "image",
    "folder",
    "class",
    "within",
    "torch",
    "vision",
    "data",
    "sets",
    "pass",
    "root",
    "get",
    "doc",
    "string",
    "root",
    "going",
    "equal",
    "trainer",
    "recall",
    "path",
    "training",
    "directory",
    "got",
    "saved",
    "going",
    "pass",
    "transform",
    "going",
    "train",
    "transform",
    "trivial",
    "training",
    "data",
    "going",
    "augmented",
    "thanks",
    "transform",
    "transforms",
    "trivial",
    "augment",
    "wide",
    "know",
    "find",
    "trivial",
    "augment",
    "wide",
    "course",
    "pie",
    "torch",
    "documentation",
    "searching",
    "transforms",
    "trivial",
    "augment",
    "wide",
    "spell",
    "wrong",
    "trivial",
    "oh",
    "train",
    "train",
    "transform",
    "spelled",
    "wrong",
    "course",
    "test",
    "data",
    "let",
    "create",
    "test",
    "data",
    "simple",
    "equals",
    "data",
    "sets",
    "dot",
    "image",
    "folder",
    "root",
    "going",
    "test",
    "directory",
    "transform",
    "going",
    "test",
    "transform",
    "simple",
    "beautiful",
    "let",
    "turn",
    "data",
    "sets",
    "data",
    "loaders",
    "turn",
    "data",
    "sets",
    "data",
    "loaders",
    "going",
    "import",
    "os",
    "going",
    "set",
    "batch",
    "size",
    "equal",
    "number",
    "workers",
    "going",
    "load",
    "data",
    "loaders",
    "going",
    "set",
    "os",
    "dot",
    "cpu",
    "count",
    "one",
    "worker",
    "per",
    "cpu",
    "machine",
    "going",
    "set",
    "torch",
    "manual",
    "seed",
    "42",
    "going",
    "shuffle",
    "training",
    "data",
    "train",
    "data",
    "loader",
    "going",
    "call",
    "augmented",
    "equals",
    "data",
    "loader",
    "want",
    "need",
    "import",
    "want",
    "show",
    "torch",
    "dot",
    "utils",
    "never",
    "enough",
    "practice",
    "right",
    "dot",
    "data",
    "let",
    "import",
    "data",
    "loader",
    "got",
    "data",
    "loader",
    "class",
    "let",
    "go",
    "train",
    "data",
    "augmented",
    "pass",
    "data",
    "set",
    "put",
    "parameter",
    "name",
    "completeness",
    "data",
    "set",
    "want",
    "set",
    "batch",
    "size",
    "equal",
    "batch",
    "size",
    "going",
    "set",
    "shuffle",
    "equal",
    "true",
    "going",
    "set",
    "num",
    "workers",
    "equal",
    "num",
    "workers",
    "beautiful",
    "let",
    "test",
    "data",
    "loader",
    "time",
    "test",
    "data",
    "loader",
    "going",
    "call",
    "test",
    "data",
    "loader",
    "simple",
    "using",
    "data",
    "augmentation",
    "test",
    "data",
    "set",
    "turning",
    "images",
    "test",
    "images",
    "tenses",
    "data",
    "set",
    "going",
    "test",
    "data",
    "simple",
    "going",
    "pass",
    "batch",
    "size",
    "equal",
    "batch",
    "size",
    "data",
    "loaders",
    "batch",
    "size",
    "going",
    "keep",
    "shuffle",
    "false",
    "num",
    "workers",
    "going",
    "set",
    "num",
    "workers",
    "look",
    "us",
    "go",
    "already",
    "got",
    "data",
    "set",
    "data",
    "loader",
    "time",
    "data",
    "loader",
    "going",
    "augmented",
    "training",
    "data",
    "set",
    "going",
    "nice",
    "simple",
    "test",
    "data",
    "set",
    "really",
    "similar",
    "data",
    "loader",
    "previous",
    "one",
    "made",
    "difference",
    "modeling",
    "experiment",
    "going",
    "adding",
    "data",
    "augmentation",
    "namely",
    "trivial",
    "augment",
    "wide",
    "said",
    "got",
    "data",
    "set",
    "got",
    "data",
    "loader",
    "next",
    "video",
    "let",
    "construct",
    "train",
    "model",
    "one",
    "fact",
    "might",
    "want",
    "give",
    "go",
    "use",
    "tiny",
    "vgg",
    "class",
    "make",
    "model",
    "one",
    "use",
    "train",
    "function",
    "train",
    "new",
    "tiny",
    "vgg",
    "instance",
    "training",
    "data",
    "loader",
    "augmented",
    "test",
    "data",
    "loader",
    "simple",
    "give",
    "go",
    "together",
    "next",
    "video",
    "see",
    "got",
    "data",
    "sets",
    "data",
    "loaders",
    "data",
    "augmentation",
    "ready",
    "let",
    "create",
    "another",
    "model",
    "going",
    "construct",
    "train",
    "model",
    "one",
    "time",
    "going",
    "write",
    "going",
    "going",
    "sorry",
    "time",
    "using",
    "model",
    "architecture",
    "changing",
    "data",
    "except",
    "time",
    "augmented",
    "training",
    "data",
    "like",
    "see",
    "performs",
    "compared",
    "model",
    "data",
    "augmentation",
    "baseline",
    "generally",
    "experiments",
    "start",
    "simple",
    "possible",
    "introduce",
    "complexity",
    "required",
    "create",
    "model",
    "one",
    "send",
    "target",
    "device",
    "target",
    "device",
    "helpful",
    "selves",
    "previously",
    "create",
    "manual",
    "seed",
    "create",
    "model",
    "one",
    "leveraging",
    "class",
    "created",
    "although",
    "built",
    "tiny",
    "vgg",
    "scratch",
    "video",
    "section",
    "sorry",
    "subsequent",
    "coding",
    "sessions",
    "built",
    "scratch",
    "know",
    "works",
    "recreate",
    "calling",
    "class",
    "passing",
    "different",
    "variables",
    "let",
    "get",
    "number",
    "classes",
    "train",
    "data",
    "augmented",
    "classes",
    "going",
    "send",
    "device",
    "inspect",
    "model",
    "one",
    "let",
    "look",
    "wonderful",
    "let",
    "keep",
    "going",
    "also",
    "leverage",
    "training",
    "function",
    "might",
    "tried",
    "let",
    "train",
    "model",
    "going",
    "put",
    "wonderful",
    "got",
    "model",
    "data",
    "loaders",
    "let",
    "create",
    "create",
    "loss",
    "function",
    "optimizer",
    "call",
    "upon",
    "train",
    "function",
    "created",
    "earlier",
    "train",
    "evaluate",
    "model",
    "beautiful",
    "going",
    "set",
    "random",
    "seeds",
    "torch",
    "dot",
    "manual",
    "seeds",
    "torch",
    "dot",
    "cuda",
    "going",
    "using",
    "cuda",
    "let",
    "set",
    "manual",
    "seed",
    "going",
    "set",
    "number",
    "epochs",
    "going",
    "keep",
    "many",
    "parameters",
    "set",
    "number",
    "epochs",
    "num",
    "epochs",
    "equals",
    "five",
    "could",
    "course",
    "train",
    "model",
    "longer",
    "really",
    "wanted",
    "increasing",
    "number",
    "epochs",
    "let",
    "set",
    "loss",
    "function",
    "loss",
    "fn",
    "equals",
    "nn",
    "cross",
    "entropy",
    "loss",
    "forget",
    "came",
    "mind",
    "loss",
    "function",
    "often",
    "well",
    "pytorch",
    "called",
    "criterion",
    "criterion",
    "trying",
    "reduce",
    "like",
    "call",
    "loss",
    "function",
    "going",
    "optimizer",
    "let",
    "use",
    "optimizer",
    "use",
    "torch",
    "dot",
    "opt",
    "dot",
    "atom",
    "recall",
    "sgd",
    "atom",
    "two",
    "popular",
    "optimizers",
    "model",
    "one",
    "dot",
    "parameters",
    "parameters",
    "going",
    "optimize",
    "going",
    "set",
    "learning",
    "rate",
    "zero",
    "zero",
    "one",
    "default",
    "atom",
    "optimizer",
    "pytorch",
    "going",
    "start",
    "timer",
    "time",
    "let",
    "import",
    "default",
    "timer",
    "timer",
    "go",
    "start",
    "time",
    "equals",
    "timer",
    "let",
    "go",
    "train",
    "model",
    "one",
    "well",
    "going",
    "get",
    "results",
    "dictionary",
    "model",
    "one",
    "results",
    "going",
    "call",
    "upon",
    "train",
    "function",
    "inside",
    "train",
    "function",
    "pass",
    "model",
    "parameter",
    "model",
    "one",
    "train",
    "data",
    "loader",
    "parameter",
    "going",
    "pass",
    "train",
    "data",
    "loader",
    "augmented",
    "augmented",
    "training",
    "data",
    "loader",
    "test",
    "data",
    "loader",
    "pass",
    "test",
    "data",
    "loader",
    "simple",
    "write",
    "optimizer",
    "atom",
    "optimizer",
    "loss",
    "function",
    "going",
    "n",
    "cross",
    "entropy",
    "loss",
    "created",
    "set",
    "number",
    "epochs",
    "going",
    "equal",
    "num",
    "epochs",
    "really",
    "wanted",
    "could",
    "set",
    "device",
    "equal",
    "device",
    "target",
    "device",
    "let",
    "end",
    "timer",
    "print",
    "long",
    "took",
    "took",
    "n",
    "time",
    "equals",
    "timer",
    "go",
    "print",
    "total",
    "training",
    "time",
    "model",
    "one",
    "going",
    "n",
    "time",
    "minus",
    "start",
    "time",
    "oh",
    "would",
    "help",
    "could",
    "spell",
    "get",
    "three",
    "decimal",
    "places",
    "seconds",
    "ready",
    "look",
    "quickly",
    "built",
    "training",
    "pipeline",
    "model",
    "one",
    "look",
    "big",
    "easily",
    "created",
    "go",
    "ask",
    "coding",
    "stuff",
    "let",
    "train",
    "second",
    "model",
    "first",
    "model",
    "using",
    "data",
    "augmentation",
    "ready",
    "three",
    "two",
    "one",
    "let",
    "go",
    "errors",
    "beautiful",
    "going",
    "nice",
    "quick",
    "oh",
    "seven",
    "seconds",
    "gpu",
    "currently",
    "keep",
    "mind",
    "using",
    "google",
    "colab",
    "pro",
    "get",
    "preference",
    "terms",
    "allocating",
    "faster",
    "gpu",
    "model",
    "training",
    "time",
    "may",
    "longer",
    "got",
    "depending",
    "gpu",
    "also",
    "may",
    "faster",
    "depending",
    "gpu",
    "get",
    "seven",
    "seconds",
    "looks",
    "like",
    "model",
    "data",
    "augmentation",
    "perform",
    "well",
    "model",
    "without",
    "data",
    "augmentation",
    "hmm",
    "long",
    "model",
    "without",
    "data",
    "augmentation",
    "take",
    "train",
    "oh",
    "seven",
    "seconds",
    "well",
    "got",
    "better",
    "results",
    "terms",
    "accuracy",
    "training",
    "test",
    "data",
    "sets",
    "model",
    "zero",
    "maybe",
    "data",
    "augmentation",
    "help",
    "case",
    "kind",
    "hinted",
    "loss",
    "already",
    "going",
    "really",
    "overfitting",
    "yet",
    "recall",
    "data",
    "augmentation",
    "way",
    "help",
    "overfitting",
    "generally",
    "maybe",
    "best",
    "step",
    "try",
    "improve",
    "model",
    "let",
    "nonetheless",
    "keep",
    "evaluating",
    "model",
    "next",
    "video",
    "going",
    "plot",
    "loss",
    "curves",
    "model",
    "one",
    "fact",
    "might",
    "want",
    "give",
    "go",
    "got",
    "function",
    "plot",
    "loss",
    "curves",
    "got",
    "results",
    "dictionary",
    "format",
    "try",
    "plot",
    "loss",
    "curves",
    "see",
    "see",
    "let",
    "together",
    "next",
    "video",
    "see",
    "last",
    "video",
    "really",
    "exciting",
    "thing",
    "training",
    "first",
    "model",
    "data",
    "augmentation",
    "also",
    "saw",
    "quantitatively",
    "looks",
    "like",
    "give",
    "us",
    "much",
    "improvement",
    "let",
    "keep",
    "evaluating",
    "model",
    "going",
    "make",
    "section",
    "recall",
    "one",
    "favorite",
    "ways",
    "one",
    "best",
    "ways",
    "favorite",
    "evaluate",
    "performance",
    "model",
    "time",
    "plot",
    "loss",
    "curves",
    "loss",
    "curve",
    "helps",
    "evaluate",
    "model",
    "performance",
    "time",
    "also",
    "give",
    "great",
    "visual",
    "representation",
    "visual",
    "way",
    "see",
    "model",
    "underfitting",
    "overfitting",
    "let",
    "plot",
    "loss",
    "curves",
    "model",
    "one",
    "results",
    "see",
    "happens",
    "using",
    "function",
    "created",
    "oh",
    "goodness",
    "going",
    "right",
    "direction",
    "looks",
    "like",
    "test",
    "loss",
    "going",
    "want",
    "go",
    "remember",
    "ideal",
    "direction",
    "loss",
    "curve",
    "go",
    "time",
    "loss",
    "measuring",
    "measuring",
    "wrong",
    "model",
    "accuracy",
    "curve",
    "looks",
    "like",
    "place",
    "well",
    "mean",
    "going",
    "kind",
    "maybe",
    "enough",
    "time",
    "measure",
    "things",
    "experiment",
    "could",
    "train",
    "models",
    "model",
    "zero",
    "model",
    "one",
    "epochs",
    "see",
    "loss",
    "curves",
    "flatten",
    "pose",
    "question",
    "model",
    "underfitting",
    "overfitting",
    "right",
    "want",
    "look",
    "loss",
    "curves",
    "right",
    "loss",
    "accuracy",
    "loss",
    "time",
    "want",
    "go",
    "model",
    "underfitting",
    "loss",
    "could",
    "lower",
    "also",
    "looks",
    "like",
    "overfitting",
    "well",
    "good",
    "job",
    "test",
    "loss",
    "far",
    "higher",
    "training",
    "loss",
    "go",
    "back",
    "section",
    "four",
    "book",
    "ideal",
    "loss",
    "curve",
    "look",
    "like",
    "like",
    "start",
    "thinking",
    "ways",
    "could",
    "deal",
    "overfitting",
    "model",
    "could",
    "get",
    "data",
    "could",
    "simplify",
    "could",
    "use",
    "transfer",
    "learning",
    "going",
    "see",
    "later",
    "might",
    "want",
    "jump",
    "ahead",
    "look",
    "dealing",
    "underfitting",
    "things",
    "could",
    "try",
    "model",
    "could",
    "add",
    "layers",
    "potentially",
    "another",
    "convolutional",
    "block",
    "could",
    "increase",
    "number",
    "hidden",
    "units",
    "per",
    "layer",
    "got",
    "currently",
    "10",
    "hidden",
    "units",
    "per",
    "layer",
    "maybe",
    "want",
    "increase",
    "64",
    "something",
    "like",
    "could",
    "train",
    "longer",
    "probably",
    "one",
    "easiest",
    "things",
    "try",
    "current",
    "training",
    "functions",
    "could",
    "train",
    "20",
    "epochs",
    "go",
    "reference",
    "try",
    "experiments",
    "see",
    "get",
    "loss",
    "curves",
    "towards",
    "ideal",
    "shape",
    "next",
    "video",
    "going",
    "keep",
    "pushing",
    "forward",
    "going",
    "compare",
    "model",
    "results",
    "done",
    "two",
    "experiments",
    "let",
    "see",
    "side",
    "side",
    "looked",
    "model",
    "results",
    "individually",
    "know",
    "could",
    "improved",
    "good",
    "way",
    "compare",
    "experiments",
    "compare",
    "model",
    "results",
    "side",
    "side",
    "going",
    "next",
    "video",
    "see",
    "compared",
    "models",
    "loss",
    "curves",
    "individually",
    "compare",
    "model",
    "results",
    "let",
    "look",
    "comparing",
    "model",
    "results",
    "going",
    "write",
    "little",
    "note",
    "evaluating",
    "modeling",
    "experiments",
    "important",
    "compare",
    "different",
    "ways",
    "different",
    "ways",
    "number",
    "one",
    "hard",
    "coding",
    "like",
    "done",
    "written",
    "functions",
    "written",
    "helper",
    "functions",
    "whatnot",
    "manually",
    "plotted",
    "things",
    "going",
    "write",
    "course",
    "tools",
    "pytorch",
    "plus",
    "tensorboard",
    "link",
    "pytorch",
    "tensorboard",
    "going",
    "see",
    "later",
    "section",
    "course",
    "tensorboard",
    "great",
    "resource",
    "tracking",
    "experiments",
    "like",
    "jump",
    "forward",
    "look",
    "pytorch",
    "documentation",
    "encourage",
    "another",
    "one",
    "favorite",
    "tools",
    "weights",
    "biases",
    "going",
    "involve",
    "code",
    "well",
    "help",
    "automatically",
    "tracking",
    "different",
    "experiments",
    "weights",
    "biases",
    "one",
    "favorite",
    "got",
    "platform",
    "experiments",
    "looking",
    "run",
    "multiple",
    "experiments",
    "set",
    "weights",
    "biases",
    "pretty",
    "easy",
    "track",
    "different",
    "model",
    "hub",
    "parameters",
    "pytorch",
    "go",
    "import",
    "weights",
    "biases",
    "start",
    "new",
    "run",
    "weights",
    "biases",
    "save",
    "learning",
    "rate",
    "value",
    "whatnot",
    "go",
    "data",
    "log",
    "everything",
    "course",
    "different",
    "tools",
    "going",
    "focus",
    "pure",
    "pytorch",
    "thought",
    "leave",
    "anyway",
    "going",
    "come",
    "across",
    "eventually",
    "mlflow",
    "another",
    "one",
    "favorites",
    "well",
    "ml",
    "tracking",
    "projects",
    "models",
    "registry",
    "sort",
    "stuff",
    "like",
    "look",
    "ways",
    "track",
    "experiments",
    "extensions",
    "going",
    "stick",
    "hard",
    "coding",
    "going",
    "simple",
    "possible",
    "begin",
    "wanted",
    "add",
    "tools",
    "later",
    "sure",
    "let",
    "create",
    "data",
    "frame",
    "model",
    "results",
    "model",
    "results",
    "recall",
    "form",
    "dictionaries",
    "model",
    "zero",
    "results",
    "see",
    "hard",
    "coding",
    "quite",
    "cumbersome",
    "imagine",
    "say",
    "10",
    "models",
    "even",
    "five",
    "models",
    "really",
    "write",
    "fair",
    "bit",
    "code",
    "dictionaries",
    "whatnot",
    "whereas",
    "tools",
    "help",
    "track",
    "everything",
    "automatically",
    "got",
    "data",
    "frame",
    "model",
    "zero",
    "results",
    "time",
    "number",
    "epochs",
    "notice",
    "training",
    "loss",
    "starts",
    "go",
    "testing",
    "loss",
    "also",
    "starts",
    "go",
    "accuracy",
    "training",
    "test",
    "data",
    "set",
    "starts",
    "go",
    "trends",
    "looking",
    "experiment",
    "could",
    "try",
    "would",
    "train",
    "model",
    "zero",
    "longer",
    "see",
    "improved",
    "currently",
    "interested",
    "comparing",
    "results",
    "let",
    "set",
    "plot",
    "want",
    "plot",
    "model",
    "zero",
    "results",
    "model",
    "one",
    "results",
    "plot",
    "need",
    "plot",
    "training",
    "loss",
    "need",
    "plot",
    "training",
    "accuracy",
    "test",
    "loss",
    "test",
    "accuracy",
    "want",
    "two",
    "separate",
    "lines",
    "one",
    "model",
    "zero",
    "one",
    "model",
    "one",
    "particular",
    "pattern",
    "would",
    "similar",
    "regardless",
    "10",
    "different",
    "experiments",
    "10",
    "different",
    "metrics",
    "wanted",
    "compare",
    "generally",
    "want",
    "plot",
    "make",
    "visual",
    "tools",
    "weights",
    "biases",
    "tensorboard",
    "ml",
    "flow",
    "help",
    "going",
    "get",
    "clean",
    "browser",
    "let",
    "set",
    "plot",
    "going",
    "use",
    "matplotlib",
    "going",
    "put",
    "figure",
    "going",
    "make",
    "quite",
    "large",
    "want",
    "four",
    "subplots",
    "one",
    "metrics",
    "want",
    "compare",
    "across",
    "different",
    "models",
    "let",
    "get",
    "number",
    "epochs",
    "epochs",
    "going",
    "length",
    "turn",
    "range",
    "actually",
    "range",
    "len",
    "model",
    "zero",
    "df",
    "going",
    "give",
    "us",
    "five",
    "beautiful",
    "range",
    "zero",
    "five",
    "let",
    "create",
    "plot",
    "train",
    "loss",
    "want",
    "compare",
    "train",
    "loss",
    "across",
    "model",
    "zero",
    "train",
    "loss",
    "across",
    "model",
    "one",
    "go",
    "plt",
    "dot",
    "subplot",
    "let",
    "create",
    "plot",
    "two",
    "rows",
    "two",
    "columns",
    "going",
    "index",
    "number",
    "one",
    "training",
    "loss",
    "go",
    "plt",
    "dot",
    "plot",
    "going",
    "put",
    "epochs",
    "model",
    "zero",
    "df",
    "inside",
    "going",
    "put",
    "train",
    "loss",
    "first",
    "metric",
    "going",
    "label",
    "model",
    "zero",
    "comparing",
    "train",
    "loss",
    "modeling",
    "experiments",
    "recall",
    "model",
    "zero",
    "baseline",
    "model",
    "tiny",
    "vgg",
    "without",
    "data",
    "augmentation",
    "tried",
    "model",
    "one",
    "model",
    "added",
    "data",
    "augmentation",
    "transform",
    "training",
    "data",
    "plt",
    "go",
    "x",
    "label",
    "used",
    "test",
    "data",
    "set",
    "plt",
    "dot",
    "legend",
    "let",
    "see",
    "looks",
    "like",
    "wonderful",
    "training",
    "loss",
    "across",
    "two",
    "different",
    "models",
    "notice",
    "model",
    "zero",
    "trending",
    "right",
    "way",
    "model",
    "one",
    "kind",
    "exploded",
    "epoch",
    "number",
    "would",
    "zero",
    "one",
    "two",
    "one",
    "depending",
    "counting",
    "let",
    "say",
    "epoch",
    "number",
    "two",
    "easier",
    "loss",
    "went",
    "started",
    "go",
    "back",
    "continued",
    "training",
    "models",
    "might",
    "notice",
    "overall",
    "trend",
    "loss",
    "going",
    "training",
    "data",
    "set",
    "exactly",
    "like",
    "let",
    "plot",
    "go",
    "test",
    "loss",
    "going",
    "go",
    "test",
    "loss",
    "going",
    "change",
    "believe",
    "hold",
    "control",
    "command",
    "maybe",
    "nope",
    "option",
    "mac",
    "keyboard",
    "yeah",
    "might",
    "different",
    "key",
    "windows",
    "press",
    "option",
    "get",
    "multi",
    "cursor",
    "going",
    "come",
    "back",
    "way",
    "backspace",
    "turn",
    "test",
    "loss",
    "wonderful",
    "going",
    "put",
    "test",
    "loss",
    "title",
    "need",
    "change",
    "index",
    "index",
    "one",
    "index",
    "two",
    "index",
    "three",
    "index",
    "four",
    "let",
    "see",
    "looks",
    "like",
    "get",
    "test",
    "loss",
    "beautiful",
    "get",
    "however",
    "noticed",
    "model",
    "one",
    "probably",
    "overfitting",
    "stage",
    "maybe",
    "data",
    "augmentation",
    "best",
    "change",
    "make",
    "model",
    "recall",
    "even",
    "make",
    "change",
    "model",
    "preventing",
    "overfitting",
    "underfitting",
    "wo",
    "always",
    "guarantee",
    "change",
    "takes",
    "model",
    "evaluation",
    "metrics",
    "right",
    "direction",
    "ideally",
    "loss",
    "going",
    "top",
    "left",
    "bottom",
    "right",
    "time",
    "looks",
    "like",
    "model",
    "zero",
    "winning",
    "moment",
    "loss",
    "front",
    "let",
    "plot",
    "accuracy",
    "training",
    "test",
    "going",
    "change",
    "train",
    "going",
    "put",
    "accuracy",
    "going",
    "index",
    "number",
    "three",
    "plot",
    "save",
    "yeah",
    "act",
    "wonderful",
    "going",
    "option",
    "click",
    "mac",
    "going",
    "train",
    "going",
    "accuracy",
    "change",
    "one",
    "accuracy",
    "going",
    "change",
    "accuracy",
    "going",
    "plot",
    "number",
    "four",
    "two",
    "rows",
    "two",
    "columns",
    "index",
    "number",
    "four",
    "going",
    "option",
    "click",
    "two",
    "cursors",
    "test",
    "act",
    "change",
    "test",
    "act",
    "going",
    "get",
    "rid",
    "legend",
    "takes",
    "little",
    "bit",
    "plot",
    "four",
    "graphs",
    "one",
    "hit",
    "wonderful",
    "comparing",
    "models",
    "see",
    "could",
    "potentially",
    "functionalize",
    "plot",
    "however",
    "many",
    "model",
    "results",
    "say",
    "another",
    "five",
    "models",
    "another",
    "five",
    "experiments",
    "actually",
    "many",
    "experiments",
    "problem",
    "might",
    "find",
    "sometimes",
    "dozen",
    "experiments",
    "single",
    "modeling",
    "problem",
    "maybe",
    "even",
    "graphs",
    "get",
    "pretty",
    "outlandish",
    "little",
    "lines",
    "going",
    "tools",
    "like",
    "tensorboard",
    "weights",
    "biases",
    "mlflow",
    "help",
    "look",
    "accuracy",
    "seems",
    "models",
    "heading",
    "right",
    "direction",
    "want",
    "go",
    "bottom",
    "left",
    "case",
    "accuracy",
    "test",
    "accuracy",
    "training",
    "oh",
    "excuse",
    "training",
    "accuracy",
    "messed",
    "catch",
    "one",
    "training",
    "accuracy",
    "heading",
    "right",
    "direction",
    "looks",
    "like",
    "model",
    "one",
    "yeah",
    "still",
    "overfitting",
    "results",
    "getting",
    "training",
    "data",
    "set",
    "coming",
    "testing",
    "data",
    "set",
    "really",
    "want",
    "models",
    "shine",
    "test",
    "data",
    "set",
    "metrics",
    "training",
    "data",
    "set",
    "good",
    "ideally",
    "want",
    "models",
    "perform",
    "well",
    "test",
    "data",
    "set",
    "data",
    "seen",
    "something",
    "keep",
    "mind",
    "whenever",
    "series",
    "modeling",
    "experiments",
    "always",
    "good",
    "evaluate",
    "individually",
    "evaluate",
    "way",
    "go",
    "back",
    "experiments",
    "see",
    "worked",
    "ask",
    "would",
    "models",
    "would",
    "probably",
    "train",
    "longer",
    "maybe",
    "add",
    "hidden",
    "units",
    "layers",
    "see",
    "results",
    "go",
    "give",
    "shot",
    "next",
    "video",
    "let",
    "see",
    "use",
    "trained",
    "models",
    "make",
    "prediction",
    "custom",
    "image",
    "food",
    "yes",
    "used",
    "custom",
    "data",
    "set",
    "pizza",
    "steak",
    "sushi",
    "images",
    "finished",
    "model",
    "training",
    "decided",
    "know",
    "good",
    "enough",
    "model",
    "deployed",
    "app",
    "like",
    "neutrify",
    "dot",
    "app",
    "food",
    "recognition",
    "app",
    "personally",
    "working",
    "wanted",
    "upload",
    "image",
    "classified",
    "pytorch",
    "model",
    "let",
    "give",
    "shot",
    "see",
    "use",
    "trained",
    "model",
    "predict",
    "image",
    "training",
    "data",
    "testing",
    "data",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "last",
    "video",
    "compared",
    "modeling",
    "experiments",
    "going",
    "move",
    "one",
    "exciting",
    "parts",
    "deep",
    "learning",
    "making",
    "prediction",
    "custom",
    "image",
    "although",
    "trained",
    "model",
    "custom",
    "data",
    "make",
    "prediction",
    "sample",
    "slash",
    "image",
    "case",
    "either",
    "training",
    "testing",
    "data",
    "set",
    "let",
    "say",
    "building",
    "food",
    "recognition",
    "app",
    "neutrify",
    "take",
    "photo",
    "food",
    "learn",
    "wanted",
    "use",
    "computer",
    "vision",
    "essentially",
    "turn",
    "foods",
    "qr",
    "codes",
    "show",
    "workflow",
    "upload",
    "image",
    "dad",
    "giving",
    "two",
    "thumbs",
    "delicious",
    "pizza",
    "neutrify",
    "predicted",
    "pizza",
    "beautiful",
    "macaronutrients",
    "get",
    "nutrition",
    "information",
    "time",
    "taken",
    "could",
    "replicate",
    "similar",
    "process",
    "using",
    "trained",
    "pytorch",
    "model",
    "going",
    "great",
    "results",
    "performance",
    "seen",
    "could",
    "improve",
    "models",
    "based",
    "accuracy",
    "based",
    "loss",
    "whatnot",
    "let",
    "see",
    "like",
    "workflow",
    "first",
    "thing",
    "going",
    "get",
    "custom",
    "image",
    "could",
    "upload",
    "one",
    "clicking",
    "upload",
    "button",
    "google",
    "colab",
    "choosing",
    "image",
    "importing",
    "like",
    "going",
    "programmatically",
    "seen",
    "let",
    "write",
    "code",
    "video",
    "download",
    "custom",
    "image",
    "going",
    "using",
    "requests",
    "like",
    "good",
    "cooking",
    "shows",
    "prepared",
    "custom",
    "image",
    "us",
    "custom",
    "image",
    "path",
    "could",
    "use",
    "process",
    "going",
    "go",
    "images",
    "pizza",
    "steak",
    "sushi",
    "wanted",
    "train",
    "model",
    "another",
    "set",
    "custom",
    "data",
    "workflow",
    "quite",
    "similar",
    "going",
    "download",
    "photo",
    "called",
    "pizza",
    "dad",
    "dad",
    "two",
    "big",
    "thumbs",
    "going",
    "download",
    "github",
    "image",
    "course",
    "github",
    "let",
    "write",
    "code",
    "download",
    "image",
    "already",
    "exist",
    "colab",
    "instance",
    "wanted",
    "upload",
    "single",
    "image",
    "could",
    "click",
    "button",
    "aware",
    "like",
    "data",
    "going",
    "disappear",
    "colab",
    "disconnects",
    "like",
    "write",
    "code",
    "upload",
    "every",
    "time",
    "custom",
    "image",
    "path",
    "file",
    "let",
    "open",
    "request",
    "open",
    "file",
    "going",
    "open",
    "custom",
    "image",
    "path",
    "right",
    "binary",
    "permissions",
    "f",
    "short",
    "file",
    "downloading",
    "image",
    "stored",
    "github",
    "downloading",
    "image",
    "downloading",
    "github",
    "general",
    "typically",
    "want",
    "raw",
    "link",
    "need",
    "use",
    "raw",
    "file",
    "link",
    "let",
    "write",
    "request",
    "equals",
    "request",
    "dot",
    "get",
    "go",
    "pytorch",
    "deep",
    "learning",
    "repo",
    "go",
    "believe",
    "might",
    "extras",
    "extras",
    "going",
    "images",
    "would",
    "make",
    "lot",
    "sense",
    "would",
    "daniel",
    "let",
    "get",
    "pizza",
    "dad",
    "look",
    "pytorch",
    "deep",
    "learning",
    "images",
    "pizza",
    "dad",
    "big",
    "version",
    "image",
    "click",
    "download",
    "going",
    "give",
    "us",
    "raw",
    "link",
    "yeah",
    "go",
    "image",
    "hey",
    "dad",
    "pizza",
    "delicious",
    "looks",
    "like",
    "let",
    "see",
    "model",
    "get",
    "right",
    "think",
    "course",
    "want",
    "model",
    "predict",
    "pizza",
    "image",
    "got",
    "pizza",
    "custom",
    "image",
    "path",
    "going",
    "download",
    "put",
    "raw",
    "url",
    "notice",
    "raw",
    "github",
    "user",
    "content",
    "course",
    "github",
    "going",
    "go",
    "f",
    "dot",
    "right",
    "file",
    "write",
    "request",
    "content",
    "content",
    "request",
    "words",
    "raw",
    "file",
    "github",
    "similar",
    "workflow",
    "getting",
    "another",
    "image",
    "somewhere",
    "else",
    "internet",
    "else",
    "already",
    "downloaded",
    "let",
    "download",
    "print",
    "f",
    "custom",
    "image",
    "path",
    "already",
    "exists",
    "skipping",
    "download",
    "let",
    "see",
    "works",
    "run",
    "code",
    "downloading",
    "data",
    "four",
    "pizza",
    "dad",
    "dot",
    "jpeg",
    "go",
    "refresh",
    "go",
    "beautiful",
    "data",
    "custom",
    "image",
    "sorry",
    "data",
    "folder",
    "click",
    "inside",
    "google",
    "colab",
    "beautiful",
    "got",
    "big",
    "nice",
    "big",
    "image",
    "nice",
    "big",
    "pizza",
    "going",
    "writing",
    "code",
    "next",
    "videos",
    "exact",
    "process",
    "import",
    "custom",
    "data",
    "set",
    "custom",
    "image",
    "still",
    "still",
    "turn",
    "tenses",
    "pass",
    "model",
    "let",
    "see",
    "looks",
    "like",
    "next",
    "videos",
    "one",
    "exciting",
    "parts",
    "building",
    "dev",
    "learning",
    "models",
    "predicting",
    "custom",
    "data",
    "case",
    "custom",
    "image",
    "photo",
    "dad",
    "eating",
    "pizza",
    "course",
    "training",
    "computer",
    "vision",
    "model",
    "pizza",
    "steak",
    "sushi",
    "hopefully",
    "ideal",
    "result",
    "model",
    "predict",
    "image",
    "pizza",
    "let",
    "keep",
    "going",
    "let",
    "figure",
    "get",
    "image",
    "custom",
    "image",
    "singular",
    "image",
    "tensor",
    "form",
    "loading",
    "custom",
    "image",
    "pytorch",
    "creating",
    "another",
    "section",
    "going",
    "write",
    "make",
    "sure",
    "custom",
    "image",
    "format",
    "data",
    "model",
    "trained",
    "namely",
    "tensor",
    "form",
    "data",
    "type",
    "torch",
    "float",
    "shape",
    "64",
    "64",
    "three",
    "might",
    "need",
    "change",
    "shape",
    "image",
    "need",
    "make",
    "sure",
    "right",
    "device",
    "command",
    "mm",
    "beautiful",
    "let",
    "see",
    "looks",
    "like",
    "hey",
    "going",
    "import",
    "torch",
    "vision",
    "package",
    "use",
    "load",
    "data",
    "depend",
    "domain",
    "let",
    "open",
    "torch",
    "vision",
    "documentation",
    "go",
    "models",
    "okay",
    "working",
    "text",
    "might",
    "want",
    "look",
    "input",
    "output",
    "functions",
    "loading",
    "functions",
    "torch",
    "audio",
    "thing",
    "torch",
    "vision",
    "working",
    "let",
    "click",
    "torch",
    "vision",
    "want",
    "look",
    "reading",
    "writing",
    "images",
    "videos",
    "want",
    "read",
    "image",
    "right",
    "got",
    "custom",
    "image",
    "want",
    "read",
    "part",
    "extracurricular",
    "way",
    "go",
    "least",
    "10",
    "minutes",
    "spend",
    "hour",
    "going",
    "torch",
    "vision",
    "could",
    "across",
    "ones",
    "really",
    "help",
    "familiarize",
    "functions",
    "pytorch",
    "domain",
    "libraries",
    "want",
    "look",
    "options",
    "video",
    "working",
    "video",
    "options",
    "images",
    "want",
    "want",
    "read",
    "image",
    "got",
    "things",
    "decode",
    "image",
    "oh",
    "skipped",
    "one",
    "write",
    "jpeg",
    "wanted",
    "encode",
    "png",
    "let",
    "jump",
    "one",
    "read",
    "image",
    "read",
    "jpeg",
    "png",
    "rgb",
    "grayscale",
    "tensor",
    "want",
    "optionally",
    "converts",
    "image",
    "desired",
    "format",
    "values",
    "output",
    "tensor",
    "int",
    "eight",
    "okay",
    "beautiful",
    "let",
    "see",
    "looks",
    "like",
    "okay",
    "mode",
    "read",
    "mode",
    "used",
    "optionally",
    "converting",
    "image",
    "let",
    "see",
    "going",
    "copy",
    "write",
    "read",
    "image",
    "pytorch",
    "using",
    "go",
    "let",
    "see",
    "looks",
    "like",
    "practice",
    "read",
    "custom",
    "image",
    "ca",
    "explain",
    "much",
    "love",
    "using",
    "deep",
    "learning",
    "models",
    "predict",
    "custom",
    "data",
    "custom",
    "image",
    "going",
    "call",
    "int",
    "eight",
    "read",
    "documentation",
    "reads",
    "int",
    "eight",
    "format",
    "let",
    "look",
    "looks",
    "like",
    "rather",
    "talking",
    "torch",
    "read",
    "image",
    "target",
    "image",
    "path",
    "well",
    "got",
    "custom",
    "image",
    "path",
    "like",
    "things",
    "programmatically",
    "collab",
    "notebook",
    "reset",
    "could",
    "run",
    "cell",
    "get",
    "custom",
    "image",
    "got",
    "custom",
    "image",
    "int",
    "eight",
    "let",
    "see",
    "looks",
    "like",
    "oh",
    "get",
    "wrong",
    "unable",
    "cast",
    "python",
    "instance",
    "oh",
    "need",
    "string",
    "expected",
    "value",
    "type",
    "string",
    "found",
    "posix",
    "path",
    "path",
    "needs",
    "string",
    "okay",
    "look",
    "custom",
    "image",
    "path",
    "get",
    "wrong",
    "oh",
    "got",
    "posix",
    "path",
    "let",
    "convert",
    "custom",
    "image",
    "path",
    "string",
    "see",
    "happens",
    "look",
    "image",
    "integer",
    "form",
    "wonder",
    "plotable",
    "let",
    "go",
    "plt",
    "dot",
    "show",
    "custom",
    "image",
    "int",
    "eight",
    "maybe",
    "get",
    "dimensionality",
    "problem",
    "valid",
    "shape",
    "okay",
    "let",
    "permute",
    "permute",
    "go",
    "one",
    "two",
    "zero",
    "going",
    "plot",
    "fairly",
    "big",
    "image",
    "go",
    "two",
    "thumbs",
    "look",
    "us",
    "power",
    "torch",
    "owe",
    "stands",
    "input",
    "output",
    "able",
    "read",
    "custom",
    "image",
    "get",
    "metadata",
    "let",
    "go",
    "print",
    "actually",
    "keep",
    "fun",
    "plot",
    "let",
    "find",
    "shape",
    "data",
    "data",
    "type",
    "yeah",
    "got",
    "tensor",
    "format",
    "int",
    "eight",
    "right",
    "might",
    "convert",
    "float",
    "want",
    "find",
    "shape",
    "need",
    "make",
    "sure",
    "predicting",
    "custom",
    "image",
    "data",
    "predicting",
    "custom",
    "image",
    "needs",
    "device",
    "model",
    "let",
    "print",
    "info",
    "print",
    "let",
    "go",
    "custom",
    "image",
    "tensor",
    "going",
    "new",
    "line",
    "go",
    "custom",
    "image",
    "int",
    "eight",
    "wonderful",
    "let",
    "go",
    "custom",
    "image",
    "shape",
    "get",
    "shape",
    "parameter",
    "custom",
    "image",
    "shape",
    "attribute",
    "sorry",
    "also",
    "want",
    "know",
    "data",
    "type",
    "custom",
    "image",
    "data",
    "type",
    "kind",
    "inkling",
    "documentation",
    "said",
    "would",
    "int",
    "eight",
    "int",
    "eight",
    "go",
    "type",
    "let",
    "look",
    "image",
    "tensor",
    "quite",
    "big",
    "image",
    "custom",
    "image",
    "shape",
    "model",
    "trained",
    "model",
    "trained",
    "images",
    "64",
    "image",
    "encodes",
    "lot",
    "information",
    "model",
    "trained",
    "going",
    "change",
    "shape",
    "pass",
    "model",
    "got",
    "image",
    "data",
    "type",
    "tensor",
    "data",
    "type",
    "torch",
    "int",
    "eight",
    "maybe",
    "going",
    "errors",
    "us",
    "later",
    "want",
    "go",
    "ahead",
    "see",
    "resize",
    "tensor",
    "64",
    "64",
    "using",
    "torch",
    "transform",
    "torch",
    "vision",
    "transform",
    "encourage",
    "try",
    "know",
    "change",
    "torch",
    "tensor",
    "int",
    "eight",
    "torch",
    "float",
    "32",
    "give",
    "shot",
    "well",
    "let",
    "try",
    "make",
    "prediction",
    "image",
    "next",
    "video",
    "see",
    "last",
    "video",
    "loaded",
    "custom",
    "image",
    "got",
    "two",
    "big",
    "thumbs",
    "dad",
    "turned",
    "tensor",
    "got",
    "custom",
    "image",
    "tensor",
    "quite",
    "big",
    "though",
    "looked",
    "things",
    "pass",
    "model",
    "need",
    "make",
    "sure",
    "data",
    "type",
    "torch",
    "float",
    "32",
    "shape",
    "64",
    "64",
    "3",
    "right",
    "device",
    "let",
    "make",
    "another",
    "section",
    "go",
    "call",
    "making",
    "prediction",
    "custom",
    "image",
    "pie",
    "torch",
    "model",
    "trained",
    "pie",
    "torch",
    "model",
    "albeit",
    "models",
    "quite",
    "level",
    "would",
    "like",
    "yet",
    "think",
    "important",
    "see",
    "like",
    "make",
    "prediction",
    "end",
    "end",
    "custom",
    "data",
    "fun",
    "part",
    "right",
    "try",
    "make",
    "prediction",
    "image",
    "want",
    "highlight",
    "something",
    "importance",
    "different",
    "data",
    "types",
    "shapes",
    "whatnot",
    "devices",
    "three",
    "biggest",
    "errors",
    "deep",
    "learning",
    "let",
    "see",
    "happens",
    "try",
    "predict",
    "int",
    "eight",
    "format",
    "go",
    "model",
    "one",
    "dot",
    "eval",
    "torch",
    "dot",
    "inference",
    "mode",
    "let",
    "make",
    "prediction",
    "pass",
    "model",
    "one",
    "could",
    "use",
    "model",
    "zero",
    "wanted",
    "performing",
    "pretty",
    "poorly",
    "anyway",
    "let",
    "send",
    "device",
    "see",
    "happens",
    "oh",
    "get",
    "wrong",
    "runtime",
    "error",
    "input",
    "type",
    "ah",
    "got",
    "int",
    "eight",
    "one",
    "first",
    "errors",
    "talked",
    "need",
    "make",
    "sure",
    "custom",
    "data",
    "data",
    "type",
    "model",
    "originally",
    "trained",
    "got",
    "torch",
    "cuda",
    "float",
    "tensor",
    "got",
    "issue",
    "got",
    "eight",
    "image",
    "data",
    "image",
    "tensor",
    "trying",
    "predicted",
    "model",
    "data",
    "type",
    "torch",
    "cuda",
    "float",
    "tensor",
    "let",
    "try",
    "fix",
    "loading",
    "custom",
    "image",
    "convert",
    "torch",
    "dot",
    "float",
    "one",
    "ways",
    "recreate",
    "custom",
    "image",
    "tensor",
    "going",
    "use",
    "torch",
    "vision",
    "dot",
    "io",
    "dot",
    "read",
    "image",
    "fully",
    "reload",
    "image",
    "going",
    "anyway",
    "completeness",
    "little",
    "bit",
    "practice",
    "going",
    "set",
    "type",
    "type",
    "method",
    "torch",
    "float",
    "let",
    "see",
    "happens",
    "go",
    "custom",
    "image",
    "let",
    "see",
    "looks",
    "like",
    "wonder",
    "model",
    "work",
    "let",
    "try",
    "bring",
    "copy",
    "make",
    "prediction",
    "custom",
    "image",
    "dot",
    "two",
    "device",
    "image",
    "torch",
    "float",
    "32",
    "let",
    "see",
    "happens",
    "oh",
    "get",
    "issue",
    "oh",
    "goodness",
    "big",
    "matrix",
    "feeling",
    "might",
    "image",
    "custom",
    "image",
    "shape",
    "far",
    "large",
    "custom",
    "image",
    "dot",
    "shape",
    "get",
    "oh",
    "gosh",
    "4000",
    "notice",
    "well",
    "values",
    "zero",
    "one",
    "whereas",
    "previous",
    "images",
    "image",
    "go",
    "model",
    "trained",
    "zero",
    "one",
    "could",
    "get",
    "values",
    "zero",
    "one",
    "well",
    "one",
    "ways",
    "dividing",
    "would",
    "divide",
    "255",
    "well",
    "standard",
    "image",
    "format",
    "store",
    "image",
    "tensor",
    "values",
    "values",
    "zero",
    "255",
    "red",
    "green",
    "blue",
    "color",
    "channels",
    "want",
    "scale",
    "meant",
    "zero",
    "255",
    "wanted",
    "scale",
    "values",
    "zero",
    "one",
    "divide",
    "maximum",
    "value",
    "let",
    "see",
    "happens",
    "okay",
    "get",
    "image",
    "values",
    "zero",
    "one",
    "plot",
    "image",
    "plt",
    "dot",
    "show",
    "let",
    "plot",
    "custom",
    "image",
    "got",
    "permute",
    "works",
    "nicely",
    "mapplotlib",
    "get",
    "beautiful",
    "get",
    "image",
    "right",
    "still",
    "quite",
    "big",
    "look",
    "got",
    "pixel",
    "height",
    "image",
    "height",
    "almost",
    "4000",
    "pixels",
    "width",
    "3000",
    "pixels",
    "need",
    "adjustments",
    "let",
    "keep",
    "going",
    "got",
    "custom",
    "image",
    "device",
    "got",
    "error",
    "shape",
    "error",
    "transform",
    "image",
    "shape",
    "might",
    "already",
    "tried",
    "well",
    "let",
    "create",
    "transform",
    "pipeline",
    "transform",
    "image",
    "shape",
    "create",
    "transform",
    "pipeline",
    "composition",
    "resize",
    "image",
    "remember",
    "trying",
    "trying",
    "get",
    "model",
    "predict",
    "type",
    "data",
    "trained",
    "let",
    "go",
    "custom",
    "image",
    "transform",
    "transforms",
    "dot",
    "compose",
    "going",
    "since",
    "image",
    "already",
    "tensor",
    "let",
    "transforms",
    "dot",
    "resize",
    "set",
    "size",
    "shape",
    "model",
    "trained",
    "size",
    "let",
    "go",
    "torch",
    "vision",
    "rewrite",
    "already",
    "imported",
    "want",
    "highlight",
    "using",
    "transforms",
    "package",
    "run",
    "go",
    "got",
    "transform",
    "pipeline",
    "let",
    "see",
    "happens",
    "transform",
    "target",
    "image",
    "transform",
    "target",
    "image",
    "happens",
    "custom",
    "image",
    "transformed",
    "love",
    "printing",
    "inputs",
    "outputs",
    "different",
    "pipelines",
    "let",
    "pass",
    "custom",
    "image",
    "imported",
    "custom",
    "image",
    "transform",
    "custom",
    "image",
    "recall",
    "shape",
    "quite",
    "large",
    "going",
    "pass",
    "transformation",
    "pipeline",
    "let",
    "print",
    "shapes",
    "let",
    "go",
    "original",
    "shape",
    "go",
    "custom",
    "image",
    "dot",
    "shape",
    "go",
    "print",
    "transformed",
    "shape",
    "going",
    "custom",
    "image",
    "underscore",
    "transformed",
    "dot",
    "shape",
    "let",
    "see",
    "transformation",
    "oh",
    "would",
    "look",
    "good",
    "gone",
    "quite",
    "large",
    "image",
    "transformed",
    "image",
    "going",
    "squished",
    "squashed",
    "little",
    "happens",
    "let",
    "see",
    "happens",
    "plot",
    "transformed",
    "image",
    "gone",
    "4000",
    "pixels",
    "height",
    "gone",
    "3000",
    "pixels",
    "height",
    "model",
    "going",
    "see",
    "let",
    "go",
    "custom",
    "image",
    "transformed",
    "going",
    "permute",
    "okay",
    "quite",
    "pixelated",
    "see",
    "might",
    "affect",
    "accuracy",
    "model",
    "gone",
    "custom",
    "image",
    "going",
    "oh",
    "yeah",
    "need",
    "plot",
    "dot",
    "image",
    "gone",
    "high",
    "definition",
    "image",
    "image",
    "far",
    "lower",
    "quality",
    "kind",
    "see",
    "still",
    "pizza",
    "know",
    "pizza",
    "keep",
    "mind",
    "going",
    "forward",
    "another",
    "way",
    "could",
    "potentially",
    "improve",
    "model",
    "performance",
    "increased",
    "size",
    "training",
    "image",
    "data",
    "instead",
    "64",
    "64",
    "might",
    "want",
    "upgrade",
    "models",
    "capability",
    "deal",
    "images",
    "224",
    "look",
    "looks",
    "like",
    "224",
    "wow",
    "looks",
    "lot",
    "better",
    "64",
    "something",
    "might",
    "want",
    "try",
    "later",
    "going",
    "stick",
    "line",
    "cnn",
    "explainer",
    "model",
    "try",
    "make",
    "another",
    "prediction",
    "since",
    "transformed",
    "image",
    "size",
    "data",
    "model",
    "trained",
    "torch",
    "inference",
    "mode",
    "let",
    "go",
    "custom",
    "image",
    "pred",
    "equals",
    "model",
    "one",
    "custom",
    "image",
    "underscore",
    "transformed",
    "work",
    "oh",
    "goodness",
    "still",
    "working",
    "expected",
    "tensors",
    "device",
    "course",
    "forgot",
    "let",
    "go",
    "device",
    "actually",
    "let",
    "leave",
    "error",
    "copy",
    "code",
    "let",
    "put",
    "custom",
    "image",
    "transform",
    "back",
    "right",
    "device",
    "see",
    "finally",
    "get",
    "prediction",
    "happen",
    "model",
    "oh",
    "still",
    "get",
    "error",
    "oh",
    "goodness",
    "going",
    "oh",
    "need",
    "add",
    "batch",
    "size",
    "gon",
    "na",
    "write",
    "error",
    "batch",
    "size",
    "error",
    "image",
    "right",
    "device",
    "let",
    "try",
    "need",
    "add",
    "batch",
    "size",
    "image",
    "look",
    "custom",
    "image",
    "transformed",
    "dot",
    "shape",
    "recall",
    "images",
    "passed",
    "model",
    "batch",
    "dimension",
    "another",
    "place",
    "get",
    "shape",
    "mismatch",
    "issues",
    "model",
    "going",
    "neural",
    "network",
    "lot",
    "tensor",
    "manipulation",
    "dimensions",
    "line",
    "want",
    "perform",
    "matrix",
    "multiplication",
    "rules",
    "play",
    "rules",
    "matrix",
    "multiplication",
    "fail",
    "let",
    "fix",
    "adding",
    "batch",
    "dimension",
    "going",
    "custom",
    "image",
    "transformed",
    "let",
    "unsqueeze",
    "first",
    "dimension",
    "check",
    "shape",
    "go",
    "add",
    "single",
    "batch",
    "want",
    "make",
    "prediction",
    "single",
    "custom",
    "image",
    "want",
    "pass",
    "model",
    "image",
    "batch",
    "one",
    "sample",
    "let",
    "finally",
    "see",
    "work",
    "let",
    "comment",
    "maybe",
    "try",
    "anyway",
    "work",
    "added",
    "batch",
    "size",
    "see",
    "steps",
    "far",
    "going",
    "unsqueeze",
    "unsqueeze",
    "zero",
    "dimension",
    "add",
    "batch",
    "size",
    "oh",
    "error",
    "oh",
    "goodness",
    "error",
    "look",
    "yes",
    "want",
    "get",
    "prediction",
    "load",
    "raw",
    "outputs",
    "model",
    "get",
    "load",
    "value",
    "custom",
    "classes",
    "could",
    "pizza",
    "could",
    "steak",
    "could",
    "sushi",
    "depending",
    "order",
    "classes",
    "let",
    "look",
    "class",
    "idx",
    "get",
    "class",
    "names",
    "beautiful",
    "pizza",
    "steak",
    "sushi",
    "still",
    "got",
    "ways",
    "go",
    "convert",
    "want",
    "highlight",
    "done",
    "note",
    "make",
    "prediction",
    "custom",
    "image",
    "something",
    "keep",
    "mind",
    "almost",
    "custom",
    "data",
    "needs",
    "formatted",
    "way",
    "model",
    "trained",
    "load",
    "image",
    "turn",
    "tensor",
    "make",
    "sure",
    "image",
    "data",
    "type",
    "model",
    "torch",
    "float",
    "make",
    "sure",
    "image",
    "shape",
    "data",
    "model",
    "trained",
    "64",
    "64",
    "three",
    "batch",
    "size",
    "one",
    "three",
    "64",
    "excuse",
    "actually",
    "way",
    "around",
    "color",
    "channels",
    "first",
    "dealing",
    "pie",
    "torch",
    "finally",
    "make",
    "sure",
    "image",
    "device",
    "model",
    "three",
    "big",
    "ones",
    "talked",
    "much",
    "data",
    "type",
    "data",
    "type",
    "mismatch",
    "result",
    "bunch",
    "issues",
    "shape",
    "mismatch",
    "result",
    "bunch",
    "issues",
    "device",
    "mismatch",
    "also",
    "result",
    "bunch",
    "issues",
    "want",
    "highlighted",
    "learn",
    "pie",
    "resource",
    "putting",
    "things",
    "together",
    "oh",
    "yeah",
    "main",
    "takeaway",
    "section",
    "sorry",
    "predicting",
    "custom",
    "data",
    "trained",
    "model",
    "possible",
    "long",
    "format",
    "data",
    "similar",
    "format",
    "model",
    "trained",
    "make",
    "sure",
    "take",
    "care",
    "three",
    "big",
    "pie",
    "torch",
    "deep",
    "learning",
    "errors",
    "wrong",
    "data",
    "types",
    "wrong",
    "data",
    "shapes",
    "wrong",
    "devices",
    "regardless",
    "whether",
    "images",
    "audio",
    "text",
    "three",
    "follow",
    "around",
    "keep",
    "mind",
    "got",
    "code",
    "predict",
    "custom",
    "images",
    "kind",
    "place",
    "got",
    "10",
    "coding",
    "cells",
    "make",
    "prediction",
    "custom",
    "image",
    "functionize",
    "see",
    "works",
    "pizza",
    "dad",
    "image",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "well",
    "way",
    "making",
    "custom",
    "predictions",
    "custom",
    "image",
    "data",
    "let",
    "keep",
    "pushing",
    "forward",
    "last",
    "video",
    "finished",
    "getting",
    "raw",
    "model",
    "logits",
    "raw",
    "outputs",
    "model",
    "let",
    "see",
    "convert",
    "logits",
    "prediction",
    "labels",
    "let",
    "write",
    "code",
    "convert",
    "logits",
    "prediction",
    "labels",
    "let",
    "go",
    "convert",
    "logits",
    "let",
    "first",
    "convert",
    "prediction",
    "probabilities",
    "probabilities",
    "let",
    "go",
    "custom",
    "image",
    "pred",
    "probes",
    "equals",
    "torch",
    "dot",
    "softmax",
    "convert",
    "custom",
    "image",
    "pred",
    "across",
    "first",
    "dimension",
    "first",
    "dimension",
    "tensor",
    "inner",
    "brackets",
    "course",
    "little",
    "section",
    "let",
    "see",
    "look",
    "like",
    "prediction",
    "probabilities",
    "wonderful",
    "notice",
    "quite",
    "spread",
    "ideal",
    "ideally",
    "like",
    "model",
    "assign",
    "fairly",
    "large",
    "prediction",
    "probability",
    "target",
    "class",
    "right",
    "target",
    "class",
    "however",
    "since",
    "model",
    "trained",
    "actually",
    "performing",
    "well",
    "prediction",
    "probabilities",
    "quite",
    "spread",
    "across",
    "classes",
    "nonetheless",
    "highlighting",
    "like",
    "predict",
    "custom",
    "data",
    "let",
    "convert",
    "prediction",
    "probabilities",
    "prediction",
    "labels",
    "notice",
    "used",
    "softmax",
    "working",
    "multi",
    "class",
    "classification",
    "data",
    "get",
    "custom",
    "image",
    "pred",
    "labels",
    "integers",
    "taking",
    "argmax",
    "prediction",
    "probabilities",
    "custom",
    "image",
    "pred",
    "probes",
    "across",
    "first",
    "dimension",
    "well",
    "let",
    "go",
    "custom",
    "image",
    "pred",
    "labels",
    "let",
    "see",
    "look",
    "like",
    "zero",
    "index",
    "highest",
    "value",
    "index",
    "number",
    "zero",
    "notice",
    "still",
    "coded",
    "device",
    "would",
    "happen",
    "try",
    "index",
    "class",
    "names",
    "custom",
    "image",
    "pred",
    "labels",
    "maybe",
    "need",
    "plural",
    "oh",
    "go",
    "get",
    "pizza",
    "might",
    "also",
    "change",
    "cpu",
    "later",
    "otherwise",
    "might",
    "run",
    "errors",
    "aware",
    "notice",
    "put",
    "cpu",
    "get",
    "pizza",
    "got",
    "correct",
    "prediction",
    "good",
    "guessing",
    "opinion",
    "kind",
    "spread",
    "ideally",
    "value",
    "would",
    "higher",
    "maybe",
    "something",
    "like",
    "pizza",
    "dad",
    "image",
    "nonetheless",
    "model",
    "getting",
    "two",
    "thumbs",
    "even",
    "64",
    "64",
    "image",
    "lot",
    "code",
    "written",
    "let",
    "functionize",
    "pass",
    "file",
    "path",
    "get",
    "custom",
    "prediction",
    "putting",
    "custom",
    "image",
    "prediction",
    "together",
    "let",
    "go",
    "building",
    "function",
    "want",
    "ideal",
    "outcome",
    "let",
    "plot",
    "image",
    "well",
    "ideal",
    "outcome",
    "function",
    "plot",
    "pass",
    "image",
    "path",
    "model",
    "predict",
    "image",
    "plot",
    "image",
    "plus",
    "prediction",
    "ideal",
    "outcome",
    "think",
    "going",
    "issue",
    "challenge",
    "give",
    "go",
    "put",
    "code",
    "together",
    "import",
    "image",
    "process",
    "whatnot",
    "know",
    "said",
    "going",
    "build",
    "function",
    "video",
    "going",
    "say",
    "next",
    "video",
    "like",
    "give",
    "go",
    "start",
    "way",
    "back",
    "import",
    "image",
    "via",
    "torture",
    "read",
    "image",
    "format",
    "using",
    "done",
    "change",
    "data",
    "type",
    "change",
    "shape",
    "change",
    "device",
    "plot",
    "image",
    "prediction",
    "title",
    "give",
    "go",
    "together",
    "next",
    "video",
    "go",
    "realized",
    "typo",
    "previous",
    "cell",
    "right",
    "give",
    "shot",
    "put",
    "together",
    "custom",
    "image",
    "prediction",
    "function",
    "format",
    "love",
    "okay",
    "let",
    "keep",
    "going",
    "let",
    "see",
    "might",
    "look",
    "like",
    "many",
    "different",
    "ways",
    "could",
    "one",
    "ways",
    "thought",
    "want",
    "function",
    "going",
    "pred",
    "plot",
    "target",
    "image",
    "wanted",
    "take",
    "torch",
    "model",
    "going",
    "ideally",
    "trained",
    "model",
    "wanted",
    "also",
    "take",
    "image",
    "path",
    "string",
    "take",
    "class",
    "names",
    "list",
    "index",
    "get",
    "prediction",
    "label",
    "string",
    "format",
    "let",
    "put",
    "list",
    "strings",
    "default",
    "equal",
    "none",
    "case",
    "wanted",
    "prediction",
    "wants",
    "take",
    "transform",
    "pass",
    "form",
    "transform",
    "transform",
    "image",
    "going",
    "take",
    "device",
    "default",
    "target",
    "device",
    "let",
    "write",
    "little",
    "doc",
    "string",
    "makes",
    "prediction",
    "target",
    "image",
    "trained",
    "model",
    "plots",
    "image",
    "prediction",
    "beautiful",
    "first",
    "let",
    "load",
    "image",
    "load",
    "image",
    "like",
    "torch",
    "vision",
    "target",
    "image",
    "equals",
    "torch",
    "dot",
    "read",
    "image",
    "go",
    "string",
    "image",
    "path",
    "image",
    "path",
    "convert",
    "string",
    "case",
    "get",
    "passed",
    "string",
    "let",
    "change",
    "type",
    "torch",
    "float",
    "want",
    "make",
    "sure",
    "custom",
    "image",
    "custom",
    "data",
    "type",
    "trained",
    "model",
    "let",
    "divide",
    "image",
    "pixel",
    "values",
    "255",
    "get",
    "zero",
    "get",
    "zero",
    "one",
    "range",
    "target",
    "image",
    "equals",
    "target",
    "image",
    "divided",
    "could",
    "also",
    "one",
    "step",
    "put",
    "let",
    "know",
    "hey",
    "read",
    "image",
    "imports",
    "image",
    "data",
    "zero",
    "model",
    "prefers",
    "numbers",
    "zero",
    "one",
    "let",
    "scale",
    "want",
    "transform",
    "data",
    "necessary",
    "case",
    "wo",
    "always",
    "want",
    "function",
    "pretty",
    "generic",
    "predomplot",
    "image",
    "transform",
    "exists",
    "let",
    "set",
    "target",
    "image",
    "transform",
    "pass",
    "transform",
    "wonderful",
    "transform",
    "going",
    "get",
    "left",
    "well",
    "let",
    "make",
    "sure",
    "model",
    "target",
    "device",
    "might",
    "default",
    "passing",
    "device",
    "parameter",
    "may",
    "well",
    "make",
    "sure",
    "model",
    "make",
    "prediction",
    "let",
    "turn",
    "vowel",
    "slash",
    "inference",
    "mode",
    "make",
    "prediction",
    "model",
    "model",
    "call",
    "vowel",
    "mode",
    "torch",
    "dot",
    "inference",
    "mode",
    "making",
    "prediction",
    "want",
    "turn",
    "model",
    "inference",
    "mode",
    "put",
    "inference",
    "mode",
    "context",
    "let",
    "add",
    "extra",
    "dimension",
    "image",
    "let",
    "go",
    "target",
    "image",
    "could",
    "step",
    "actually",
    "going",
    "kind",
    "remembering",
    "things",
    "fly",
    "need",
    "adding",
    "let",
    "write",
    "batch",
    "dimension",
    "e",
    "g",
    "model",
    "predict",
    "batches",
    "one",
    "x",
    "image",
    "unsqueezing",
    "add",
    "extra",
    "dimension",
    "zero",
    "dimension",
    "space",
    "like",
    "previous",
    "video",
    "let",
    "make",
    "prediction",
    "image",
    "extra",
    "dimension",
    "otherwise",
    "extra",
    "dimension",
    "saw",
    "get",
    "shape",
    "issue",
    "right",
    "target",
    "image",
    "pred",
    "remember",
    "going",
    "raw",
    "model",
    "outputs",
    "raw",
    "logit",
    "outputs",
    "going",
    "target",
    "image",
    "pred",
    "yeah",
    "believe",
    "need",
    "prediction",
    "oh",
    "wait",
    "one",
    "thing",
    "two",
    "device",
    "also",
    "make",
    "sure",
    "target",
    "image",
    "right",
    "device",
    "beautiful",
    "fair",
    "steps",
    "nothing",
    "ca",
    "handle",
    "really",
    "replicating",
    "done",
    "batches",
    "images",
    "want",
    "make",
    "sure",
    "someone",
    "passed",
    "image",
    "pred",
    "plot",
    "image",
    "function",
    "got",
    "functionality",
    "handle",
    "image",
    "get",
    "oh",
    "want",
    "target",
    "image",
    "device",
    "catch",
    "error",
    "let",
    "keep",
    "going",
    "let",
    "convert",
    "logits",
    "models",
    "raw",
    "logits",
    "let",
    "convert",
    "prediction",
    "probabilities",
    "exciting",
    "getting",
    "close",
    "making",
    "function",
    "predict",
    "custom",
    "data",
    "set",
    "target",
    "image",
    "pred",
    "probes",
    "going",
    "torch",
    "dot",
    "softmax",
    "pass",
    "target",
    "image",
    "pred",
    "want",
    "get",
    "softmax",
    "first",
    "dimension",
    "let",
    "convert",
    "prediction",
    "probabilities",
    "get",
    "line",
    "want",
    "convert",
    "prediction",
    "labels",
    "let",
    "get",
    "target",
    "image",
    "pred",
    "labels",
    "labels",
    "equals",
    "torch",
    "dot",
    "argmax",
    "want",
    "get",
    "argmax",
    "words",
    "index",
    "maximum",
    "value",
    "pred",
    "probes",
    "first",
    "dimension",
    "well",
    "return",
    "well",
    "really",
    "need",
    "return",
    "anything",
    "want",
    "create",
    "plot",
    "let",
    "plot",
    "image",
    "alongside",
    "prediction",
    "prediction",
    "probability",
    "beautiful",
    "plot",
    "dot",
    "show",
    "going",
    "pass",
    "going",
    "pass",
    "target",
    "image",
    "squeeze",
    "believe",
    "added",
    "extra",
    "dimension",
    "squeeze",
    "remove",
    "batch",
    "size",
    "still",
    "permute",
    "map",
    "plot",
    "lib",
    "likes",
    "images",
    "format",
    "color",
    "channels",
    "last",
    "one",
    "two",
    "zero",
    "remove",
    "batch",
    "dimension",
    "rearrange",
    "shape",
    "hc",
    "hwc",
    "color",
    "channels",
    "last",
    "class",
    "names",
    "parameter",
    "exists",
    "passed",
    "list",
    "class",
    "names",
    "function",
    "really",
    "replicating",
    "everything",
    "done",
    "past",
    "10",
    "cells",
    "way",
    "right",
    "back",
    "replicating",
    "stuff",
    "one",
    "function",
    "pretty",
    "large",
    "function",
    "written",
    "pass",
    "images",
    "much",
    "like",
    "class",
    "names",
    "exist",
    "let",
    "set",
    "title",
    "showcase",
    "class",
    "name",
    "pred",
    "going",
    "class",
    "names",
    "let",
    "index",
    "pred",
    "image",
    "target",
    "image",
    "pred",
    "label",
    "put",
    "cpu",
    "using",
    "title",
    "map",
    "plot",
    "lib",
    "map",
    "plot",
    "lib",
    "handle",
    "things",
    "gpu",
    "put",
    "cpu",
    "believe",
    "enough",
    "let",
    "add",
    "little",
    "line",
    "oh",
    "missed",
    "something",
    "outside",
    "bracket",
    "wonderful",
    "let",
    "add",
    "prediction",
    "probability",
    "always",
    "fun",
    "see",
    "want",
    "target",
    "image",
    "pred",
    "probs",
    "want",
    "get",
    "maximum",
    "pred",
    "problem",
    "also",
    "put",
    "cpu",
    "think",
    "might",
    "get",
    "three",
    "decimal",
    "places",
    "saying",
    "oh",
    "pred",
    "labels",
    "need",
    "need",
    "non",
    "plural",
    "beautiful",
    "class",
    "names",
    "exist",
    "let",
    "set",
    "title",
    "equal",
    "f",
    "f",
    "string",
    "go",
    "pred",
    "target",
    "image",
    "pred",
    "label",
    "google",
    "colab",
    "still",
    "telling",
    "wrong",
    "target",
    "image",
    "pred",
    "label",
    "oh",
    "still",
    "got",
    "thing",
    "caught",
    "coding",
    "bit",
    "fast",
    "pass",
    "prob",
    "could",
    "even",
    "copy",
    "beautiful",
    "let",
    "set",
    "title",
    "title",
    "turn",
    "axes",
    "plt",
    "axes",
    "false",
    "fair",
    "bit",
    "code",
    "going",
    "super",
    "exciting",
    "moment",
    "let",
    "see",
    "looks",
    "like",
    "pass",
    "target",
    "image",
    "target",
    "model",
    "class",
    "names",
    "transform",
    "ready",
    "got",
    "transform",
    "ready",
    "way",
    "back",
    "custom",
    "image",
    "transform",
    "going",
    "resize",
    "image",
    "let",
    "see",
    "oh",
    "file",
    "updated",
    "remotely",
    "another",
    "tab",
    "sometimes",
    "happens",
    "usually",
    "google",
    "colab",
    "sorts",
    "right",
    "affect",
    "code",
    "pred",
    "custom",
    "image",
    "ready",
    "save",
    "failed",
    "would",
    "like",
    "override",
    "yes",
    "would",
    "might",
    "see",
    "google",
    "colab",
    "usually",
    "fixes",
    "go",
    "save",
    "successfully",
    "pred",
    "plot",
    "image",
    "going",
    "say",
    "google",
    "colab",
    "fail",
    "predict",
    "custom",
    "data",
    "using",
    "model",
    "trained",
    "custom",
    "data",
    "image",
    "part",
    "let",
    "pass",
    "custom",
    "image",
    "path",
    "going",
    "path",
    "pizza",
    "dad",
    "image",
    "let",
    "go",
    "class",
    "names",
    "equals",
    "class",
    "names",
    "pizza",
    "steak",
    "sushi",
    "pass",
    "transform",
    "convert",
    "image",
    "right",
    "shape",
    "size",
    "custom",
    "image",
    "transform",
    "finally",
    "target",
    "device",
    "going",
    "device",
    "ready",
    "let",
    "make",
    "prediction",
    "custom",
    "data",
    "one",
    "favorite",
    "things",
    "one",
    "fun",
    "things",
    "building",
    "deep",
    "learning",
    "models",
    "three",
    "two",
    "one",
    "go",
    "oh",
    "get",
    "wrong",
    "cpu",
    "okay",
    "close",
    "yet",
    "far",
    "attribute",
    "cpu",
    "oh",
    "maybe",
    "need",
    "put",
    "cpu",
    "got",
    "square",
    "bracket",
    "wrong",
    "needed",
    "change",
    "needed",
    "going",
    "potentially",
    "gpu",
    "tag",
    "image",
    "pred",
    "label",
    "need",
    "put",
    "cpu",
    "need",
    "going",
    "title",
    "map",
    "plot",
    "lib",
    "plot",
    "map",
    "plot",
    "lib",
    "interface",
    "well",
    "data",
    "gpu",
    "let",
    "try",
    "three",
    "two",
    "one",
    "running",
    "oh",
    "look",
    "prediction",
    "custom",
    "image",
    "gets",
    "right",
    "two",
    "thumbs",
    "plan",
    "model",
    "performing",
    "actually",
    "quite",
    "poorly",
    "good",
    "guess",
    "might",
    "want",
    "try",
    "image",
    "fact",
    "please",
    "share",
    "would",
    "love",
    "see",
    "could",
    "potentially",
    "try",
    "another",
    "model",
    "see",
    "happens",
    "steak",
    "okay",
    "go",
    "even",
    "though",
    "model",
    "one",
    "performs",
    "worse",
    "quantitatively",
    "performs",
    "better",
    "qualitatively",
    "power",
    "visualize",
    "visualize",
    "visualize",
    "use",
    "model",
    "zero",
    "also",
    "performing",
    "well",
    "gets",
    "wrong",
    "prediction",
    "probability",
    "high",
    "either",
    "talked",
    "couple",
    "different",
    "ways",
    "improve",
    "models",
    "even",
    "got",
    "way",
    "make",
    "predictions",
    "custom",
    "images",
    "give",
    "shot",
    "love",
    "see",
    "custom",
    "predictions",
    "upload",
    "image",
    "want",
    "download",
    "google",
    "colab",
    "using",
    "code",
    "used",
    "come",
    "fairly",
    "long",
    "way",
    "feel",
    "like",
    "covered",
    "enough",
    "custom",
    "data",
    "sets",
    "let",
    "summarize",
    "covered",
    "next",
    "video",
    "got",
    "bunch",
    "exercises",
    "extra",
    "curriculum",
    "exciting",
    "stuff",
    "see",
    "next",
    "video",
    "last",
    "video",
    "exciting",
    "thing",
    "making",
    "prediction",
    "custom",
    "image",
    "although",
    "quite",
    "pixelated",
    "although",
    "models",
    "performance",
    "quantitatively",
    "turn",
    "good",
    "qualitatively",
    "happened",
    "work",
    "course",
    "fair",
    "ways",
    "could",
    "improve",
    "models",
    "performance",
    "main",
    "takeaway",
    "bunch",
    "pre",
    "processing",
    "make",
    "sure",
    "custom",
    "image",
    "format",
    "model",
    "expected",
    "quite",
    "lot",
    "behind",
    "scenes",
    "nutrify",
    "upload",
    "image",
    "gets",
    "pre",
    "processed",
    "similar",
    "way",
    "go",
    "image",
    "classification",
    "model",
    "output",
    "label",
    "like",
    "let",
    "get",
    "summarize",
    "got",
    "colorful",
    "slide",
    "already",
    "covered",
    "predicting",
    "custom",
    "data",
    "three",
    "things",
    "make",
    "sure",
    "regardless",
    "whether",
    "using",
    "images",
    "text",
    "audio",
    "make",
    "sure",
    "data",
    "right",
    "data",
    "type",
    "case",
    "torch",
    "float",
    "make",
    "sure",
    "data",
    "device",
    "model",
    "put",
    "custom",
    "image",
    "gpu",
    "model",
    "also",
    "lived",
    "make",
    "sure",
    "data",
    "correct",
    "shape",
    "original",
    "shape",
    "64",
    "64",
    "actually",
    "reversed",
    "color",
    "channels",
    "first",
    "principle",
    "remains",
    "add",
    "batch",
    "dimension",
    "rearrange",
    "needed",
    "case",
    "used",
    "images",
    "shape",
    "batches",
    "first",
    "color",
    "channels",
    "first",
    "height",
    "width",
    "depending",
    "problem",
    "depend",
    "shape",
    "depending",
    "device",
    "using",
    "depend",
    "data",
    "model",
    "lives",
    "depending",
    "data",
    "type",
    "using",
    "depend",
    "using",
    "torch",
    "float",
    "32",
    "something",
    "else",
    "let",
    "summarize",
    "go",
    "main",
    "takeaways",
    "read",
    "big",
    "ones",
    "pie",
    "torch",
    "many",
    "built",
    "functions",
    "deal",
    "kinds",
    "data",
    "vision",
    "text",
    "audio",
    "recommendation",
    "systems",
    "look",
    "pie",
    "torch",
    "docs",
    "going",
    "become",
    "familiar",
    "time",
    "got",
    "torch",
    "audio",
    "data",
    "torch",
    "text",
    "torch",
    "vision",
    "practiced",
    "got",
    "whole",
    "bunch",
    "things",
    "transforming",
    "augmenting",
    "images",
    "data",
    "sets",
    "utilities",
    "operators",
    "torch",
    "data",
    "currently",
    "beta",
    "something",
    "aware",
    "later",
    "prototype",
    "library",
    "right",
    "time",
    "watch",
    "might",
    "available",
    "another",
    "way",
    "loading",
    "data",
    "aware",
    "later",
    "come",
    "back",
    "applied",
    "watch",
    "built",
    "data",
    "loading",
    "functions",
    "suit",
    "requirements",
    "write",
    "custom",
    "data",
    "set",
    "classes",
    "subclassing",
    "torch",
    "dot",
    "utils",
    "dot",
    "data",
    "dot",
    "data",
    "set",
    "saw",
    "way",
    "back",
    "option",
    "number",
    "two",
    "option",
    "two",
    "go",
    "loading",
    "image",
    "data",
    "custom",
    "data",
    "set",
    "wrote",
    "plenty",
    "code",
    "lot",
    "machine",
    "learning",
    "dealing",
    "balance",
    "overfitting",
    "underfitting",
    "got",
    "whole",
    "section",
    "book",
    "check",
    "ideal",
    "loss",
    "curve",
    "look",
    "like",
    "deal",
    "overfitting",
    "deal",
    "underfitting",
    "fine",
    "line",
    "much",
    "research",
    "machine",
    "learning",
    "actually",
    "dedicated",
    "towards",
    "balance",
    "three",
    "big",
    "things",
    "aware",
    "predicting",
    "custom",
    "data",
    "wrong",
    "data",
    "types",
    "wrong",
    "data",
    "shapes",
    "wrong",
    "devices",
    "follow",
    "around",
    "said",
    "saw",
    "practice",
    "get",
    "custom",
    "image",
    "ready",
    "trained",
    "model",
    "exercises",
    "like",
    "link",
    "go",
    "loan",
    "section",
    "number",
    "four",
    "exercises",
    "course",
    "extra",
    "curriculum",
    "lot",
    "things",
    "mentioned",
    "throughout",
    "course",
    "would",
    "good",
    "resource",
    "check",
    "contained",
    "exercises",
    "time",
    "shine",
    "time",
    "practice",
    "let",
    "go",
    "back",
    "notebook",
    "scroll",
    "right",
    "bottom",
    "look",
    "much",
    "code",
    "written",
    "goodness",
    "exercises",
    "exercises",
    "extra",
    "curriculum",
    "see",
    "turn",
    "markdown",
    "wonderful",
    "go",
    "got",
    "couple",
    "resources",
    "exercise",
    "template",
    "notebook",
    "number",
    "four",
    "example",
    "solutions",
    "notebook",
    "number",
    "four",
    "working",
    "course",
    "encourage",
    "go",
    "pytorch",
    "custom",
    "data",
    "sets",
    "exercises",
    "template",
    "first",
    "try",
    "fill",
    "code",
    "got",
    "questions",
    "got",
    "dummy",
    "code",
    "got",
    "comments",
    "give",
    "go",
    "go",
    "use",
    "book",
    "resource",
    "reference",
    "use",
    "code",
    "written",
    "use",
    "documentation",
    "whatever",
    "want",
    "try",
    "go",
    "get",
    "stuck",
    "somewhere",
    "look",
    "example",
    "solution",
    "created",
    "pytorch",
    "custom",
    "data",
    "sets",
    "exercise",
    "solutions",
    "aware",
    "one",
    "way",
    "things",
    "necessarily",
    "best",
    "way",
    "reference",
    "writing",
    "would",
    "actually",
    "live",
    "walkthroughs",
    "solutions",
    "errors",
    "youtube",
    "go",
    "video",
    "going",
    "mute",
    "live",
    "streaming",
    "whole",
    "thing",
    "writing",
    "bunch",
    "pytorch",
    "code",
    "keep",
    "going",
    "see",
    "writing",
    "solutions",
    "running",
    "errors",
    "trying",
    "different",
    "things",
    "et",
    "cetera",
    "et",
    "cetera",
    "youtube",
    "check",
    "time",
    "feel",
    "like",
    "covered",
    "enough",
    "exercises",
    "oh",
    "way",
    "extras",
    "exercises",
    "tab",
    "pytorch",
    "deep",
    "learning",
    "repo",
    "extras",
    "exercises",
    "solutions",
    "contained",
    "far",
    "covered",
    "lot",
    "look",
    "pytorch",
    "custom",
    "data",
    "sets",
    "see",
    "next",
    "section",
    "holy",
    "smokes",
    "lot",
    "pytorch",
    "code",
    "still",
    "hungry",
    "five",
    "chapters",
    "available",
    "cover",
    "transfer",
    "learning",
    "favorite",
    "topic",
    "pytorch",
    "model",
    "experiment",
    "tracking",
    "pytorch",
    "paper",
    "replicating",
    "pytorch",
    "model",
    "deployment",
    "get",
    "model",
    "hands",
    "others",
    "like",
    "learn",
    "video",
    "style",
    "videos",
    "chapters",
    "available",
    "zero",
    "otherwise",
    "happy",
    "machine",
    "learning",
    "see",
    "next",
    "time"
  ],
  "keywords": [
    "course",
    "machine",
    "learning",
    "deep",
    "using",
    "pytorch",
    "written",
    "python",
    "learn",
    "writing",
    "code",
    "run",
    "experiment",
    "daniel",
    "popular",
    "watch",
    "whole",
    "thing",
    "one",
    "welcome",
    "video",
    "quite",
    "big",
    "come",
    "well",
    "right",
    "place",
    "focused",
    "got",
    "three",
    "six",
    "coding",
    "going",
    "cover",
    "bunch",
    "important",
    "get",
    "leave",
    "comment",
    "github",
    "page",
    "able",
    "find",
    "materials",
    "book",
    "version",
    "finish",
    "hey",
    "would",
    "still",
    "like",
    "mean",
    "ca",
    "really",
    "words",
    "length",
    "five",
    "available",
    "everything",
    "model",
    "videos",
    "go",
    "zero",
    "enough",
    "see",
    "inside",
    "name",
    "good",
    "exciting",
    "bit",
    "fun",
    "torch",
    "let",
    "might",
    "already",
    "sense",
    "much",
    "need",
    "know",
    "rather",
    "getting",
    "things",
    "happen",
    "define",
    "second",
    "data",
    "almost",
    "anything",
    "images",
    "text",
    "numbers",
    "audio",
    "love",
    "patterns",
    "computer",
    "part",
    "algorithm",
    "building",
    "want",
    "behind",
    "scenes",
    "extra",
    "resources",
    "however",
    "lots",
    "keep",
    "break",
    "little",
    "versus",
    "seen",
    "something",
    "similar",
    "put",
    "pretty",
    "could",
    "typically",
    "within",
    "known",
    "another",
    "called",
    "working",
    "use",
    "lot",
    "different",
    "truth",
    "kind",
    "two",
    "terms",
    "yes",
    "ml",
    "form",
    "highly",
    "encourage",
    "work",
    "fundamentals",
    "probably",
    "understand",
    "anyway",
    "say",
    "write",
    "favorite",
    "inputs",
    "beautiful",
    "rules",
    "add",
    "simple",
    "actually",
    "great",
    "step",
    "takes",
    "outputs",
    "hand",
    "ideal",
    "figure",
    "output",
    "input",
    "also",
    "features",
    "labels",
    "label",
    "wanted",
    "difference",
    "far",
    "sort",
    "algorithms",
    "throughout",
    "next",
    "question",
    "think",
    "back",
    "saw",
    "last",
    "covered",
    "time",
    "practice",
    "left",
    "reason",
    "draw",
    "line",
    "better",
    "said",
    "problem",
    "trying",
    "build",
    "car",
    "learned",
    "done",
    "maybe",
    "20",
    "100",
    "give",
    "every",
    "single",
    "rule",
    "turn",
    "stop",
    "fast",
    "somewhere",
    "fair",
    "help",
    "even",
    "try",
    "way",
    "long",
    "convert",
    "except",
    "us",
    "always",
    "used",
    "google",
    "number",
    "based",
    "map",
    "steps",
    "recommend",
    "read",
    "check",
    "otherwise",
    "mind",
    "saying",
    "start",
    "look",
    "basically",
    "looking",
    "problems",
    "remember",
    "list",
    "shape",
    "may",
    "state",
    "whatever",
    "new",
    "update",
    "trained",
    "adjust",
    "future",
    "sure",
    "large",
    "set",
    "example",
    "food",
    "search",
    "looked",
    "take",
    "photos",
    "sets",
    "tell",
    "looks",
    "specific",
    "change",
    "open",
    "exactly",
    "yet",
    "weights",
    "later",
    "times",
    "models",
    "10",
    "parameters",
    "sometimes",
    "four",
    "option",
    "means",
    "predict",
    "making",
    "whereas",
    "errors",
    "finally",
    "usually",
    "amount",
    "results",
    "without",
    "wrote",
    "research",
    "stuff",
    "ways",
    "make",
    "testing",
    "talking",
    "general",
    "confusing",
    "best",
    "type",
    "gradient",
    "nice",
    "oh",
    "nine",
    "many",
    "photo",
    "image",
    "tensor",
    "neural",
    "network",
    "random",
    "networks",
    "common",
    "vector",
    "often",
    "layers",
    "layer",
    "convolutional",
    "couple",
    "dot",
    "information",
    "depending",
    "represent",
    "ones",
    "confusion",
    "talk",
    "challenge",
    "together",
    "blue",
    "file",
    "call",
    "numerical",
    "representation",
    "square",
    "brackets",
    "matrix",
    "pass",
    "multiple",
    "hidden",
    "perform",
    "operations",
    "dots",
    "cnn",
    "refer",
    "feature",
    "fact",
    "word",
    "comes",
    "across",
    "weight",
    "whether",
    "case",
    "ideally",
    "goes",
    "units",
    "slash",
    "vision",
    "prediction",
    "probabilities",
    "architecture",
    "linear",
    "straight",
    "lines",
    "nonlinear",
    "functions",
    "function",
    "discussed",
    "first",
    "raw",
    "dog",
    "names",
    "necessarily",
    "show",
    "okay",
    "taking",
    "gets",
    "focus",
    "types",
    "though",
    "essentially",
    "idea",
    "issue",
    "shot",
    "found",
    "worry",
    "experiments",
    "smaller",
    "click",
    "hmm",
    "built",
    "took",
    "object",
    "around",
    "train",
    "certain",
    "excuse",
    "please",
    "close",
    "classification",
    "regression",
    "predicting",
    "x",
    "class",
    "multi",
    "direction",
    "started",
    "website",
    "gpus",
    "access",
    "power",
    "stack",
    "tenses",
    "generally",
    "tesla",
    "original",
    "move",
    "jump",
    "colab",
    "added",
    "hardware",
    "space",
    "top",
    "wrong",
    "gpu",
    "cuda",
    "computing",
    "running",
    "tensors",
    "towards",
    "small",
    "premise",
    "block",
    "create",
    "created",
    "stands",
    "module",
    "curriculum",
    "previous",
    "error",
    "goodness",
    "format",
    "section",
    "reference",
    "performing",
    "creating",
    "fit",
    "predictions",
    "evaluate",
    "save",
    "load",
    "custom",
    "true",
    "workflow",
    "entire",
    "ready",
    "pick",
    "point",
    "loss",
    "optimizer",
    "training",
    "loop",
    "value",
    "improve",
    "notice",
    "order",
    "happens",
    "visualize",
    "link",
    "exercises",
    "bottom",
    "else",
    "red",
    "yeah",
    "aware",
    "progress",
    "wo",
    "title",
    "import",
    "n",
    "helpful",
    "happening",
    "nothing",
    "issues",
    "notebook",
    "documentation",
    "slightly",
    "view",
    "main",
    "print",
    "runtime",
    "faster",
    "compute",
    "cell",
    "cells",
    "numpy",
    "lib",
    "plt",
    "gone",
    "possible",
    "wonderful",
    "plus",
    "believe",
    "instance",
    "rid",
    "split",
    "fashion",
    "side",
    "copy",
    "equals",
    "seven",
    "nn",
    "works",
    "dimensions",
    "none",
    "rerun",
    "longer",
    "calculate",
    "integer",
    "int",
    "dimension",
    "index",
    "zeroth",
    "eight",
    "bracket",
    "instead",
    "end",
    "size",
    "variable",
    "concept",
    "doc",
    "string",
    "pie",
    "height",
    "width",
    "color",
    "channels",
    "green",
    "pizza",
    "values",
    "method",
    "parameter",
    "default",
    "multiply",
    "target",
    "float",
    "range",
    "particular",
    "replicate",
    "made",
    "begin",
    "32",
    "16",
    "device",
    "final",
    "requires",
    "grad",
    "precision",
    "measure",
    "memory",
    "less",
    "64",
    "forward",
    "note",
    "shapes",
    "multiplication",
    "cpu",
    "agnostic",
    "track",
    "gradients",
    "action",
    "f",
    "element",
    "operation",
    "negative",
    "options",
    "test",
    "exact",
    "recall",
    "larger",
    "visual",
    "equal",
    "return",
    "match",
    "sorry",
    "b",
    "exist",
    "correct",
    "increase",
    "passing",
    "max",
    "maximum",
    "activation",
    "coded",
    "squeeze",
    "v",
    "unsqueeze",
    "saved",
    "saving",
    "download",
    "uses",
    "array",
    "library",
    "functionality",
    "randomness",
    "reduce",
    "seed",
    "compare",
    "manual",
    "c",
    "computation",
    "transform",
    "self",
    "folder",
    "optimize",
    "dictionary",
    "loading",
    "evaluating",
    "inference",
    "plot",
    "lower",
    "higher",
    "formula",
    "bias",
    "50",
    "samples",
    "high",
    "classes",
    "subclass",
    "descent",
    "propagation",
    "loader",
    "metrics",
    "mode",
    "hopefully",
    "preds",
    "potentially",
    "randomly",
    "libraries",
    "binary",
    "cross",
    "entropy",
    "rate",
    "hyper",
    "song",
    "epochs",
    "epoch",
    "pred",
    "curve",
    "2d",
    "curves",
    "evaluation",
    "batch",
    "accuracy",
    "previously",
    "send",
    "deck",
    "paths",
    "directory",
    "path",
    "loaded",
    "matplotlib",
    "sushi",
    "steak",
    "nonlinearity",
    "batches",
    "28",
    "circle",
    "relu",
    "per",
    "sigmoid",
    "softmax",
    "sequential",
    "sample",
    "extend",
    "bce",
    "logits",
    "logit",
    "act",
    "helper",
    "blob",
    "kernel",
    "transforms",
    "utils",
    "mnist",
    "loaders",
    "baseline",
    "overfitting",
    "flatten",
    "timer",
    "explainer",
    "conv",
    "pool",
    "tiny",
    "vgg",
    "augmentation",
    "augment",
    "trivial",
    "underfitting"
  ]
}