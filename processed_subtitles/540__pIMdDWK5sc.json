{
  "text": "hi everyone i'm patrick from the\nassembly ai team and today we learn\nabout generative adversarial networks or\nshort gans\nso you might have seen this popular\nexample where gans generate fake images\nof humans and they look incredibly real\ngans are indeed really powerful and are\none of the most fascinating ideas in\ndeep learning in recent years so today\nwe have a quick look at the theory\nbehind gans and then we code one from\nscratch using pytorch so let's get\nstarted alright so let's look at the\ntheory first and i promise that this\nwon't be too difficult because the idea\nis actually brilliant it's simple but\nsuper powerful\nso gans learn to generate new data with\nthe same statistics as the training set\nand gans consist of two networks playing\nan adversarial game against each other\nthat's why the name is generative\nadversarial networks\nso the goal is to generate fake data\nthat is as close as possible to the\ntraining data\nand then we have these two networks that\nplay a game against each other so how\ndoes this work exactly\nso the two networks are called the\ngenerator and the discriminator\nand the generator produces fake data and\ntries to trick the discriminator\nand the discriminator inspects the fake\ndata and then determines if it's real or\nfake so this is like a detective and\nthen they play against each other so\nthis is basically the training so first\nthey are initialized randomly and then\nthey are trained simultaneously and this\nmeans we have to minimize two losses so\nwe also use two optimizer\nand we use the binary cross entropy loss\nso i'm not going into detail here about\nthe loss formula but i will link another\nresource below the video if you want to\nlearn more about this and yeah this is\nbasically the whole concept and now\nbefore we jump to the code let's look at\nan example\nso later we use the mnist data set\nso now the generator tries to generate\nmnist images so these digits from zero\nto nine and then tries to trick the\ndiscriminator and then they play against\neach other and both sides get better and\nbetter and in the beginning they don't\nknow anything so they are randomly\ninitialized so the generator just\nproduces noise so random data that might\nlook like this and then the\ndiscriminator looks at this and can also\nlook at real data and compare it and\nthen it might easily say yeah this is\nfake but then the learning or the\ntraining continues and then the\ngenerator comes up with new data and\nthis might look something like this and\nthen again the discriminator looks at it\nand it can simply still say yes still\nfake but then at some point the\ngenerator gets better and better and\nthen the discriminator might be tricked\nand say yeah this is actually real data\nso obviously in this example the data is\nstill not perfect but of course also the\ndiscriminator is still not perfect so it\nalso has to improve and then this\ncontinues and both sides get better and\nbetter and eventually we get a or should\nget a generated data that should not be\neasily distinguishable from the original\ndata and yeah this is how it works so\nnow let's jump to the code and see how\nwe can implement this alright so here\ni'm in a google collab and i already\nprepared some code for the start but\nlater we will write the rest of the code\ntogether so i recommend that you just\ncopy this collab into your own folder\nand then follow me here\nso i will put the link in the\ndescription\nand the first thing you should do is set\nthe runtime so here we can click on\nchange runtime type and select a gpu\nthen the training will be much faster\nand then the first thing we do is we say\npip install pi torch lightning because\nwe also use pi torch lightning here to\nmake the code a little bit shorter\nand i also want to mention that there is\nan official gan tutorial on the pytorch\nlightning website that is pretty similar\nto my code but in this case they just\nuse normal linear layers\nand in our case we use cnns so it's a\nlittle bit different but yeah this is\nalso a great resource that you should\ncheck out\nso yeah we install this here then here\nwe do all the imports we need so torch\nand torch vision and the different\nmodules then we also use pi matplotlib\nand pi torch lightning and then here we\nset some parameters for example a random\nseed the batch size we check if we have\ngpus and if we have\nmultiple cpu cores\nand then the first thing we do is we\ncreate a data module that inherits from\npytorch lightning dot lightning data\nmodule so this is responsible to\ncreate the data loaders for us for the\ntraining validation and test set\nand the way this works is that we have\nto implement the init function so here\nwe simply define the parameters and we\nalso define the transformations\nso here we convert from images to a\ntensor and then we normalize it so this\nis the mean and the standard deviation\nfrom the mnist images then we have to\nimplement the prepared data function so\nhere we call the built in mnist data and\nwe can set download equals true and then\nthis will download the data and one time\nwe say training equals true for the\ntraining set and then one time training\nequals false for the test set\nthen we also\nimplement the setup function so here we\nfurther split the training data into\ntraining and validation set by applying\nthis random split function\nand yeah for the test set we simply also\ncreate the mnist data by saying training\nequals false and in all steps we already\napply the transformations and then we\ncan implement these three functions the\ntrain data loader the validation data\nloader and the test data data loader and\nthey are all pretty simple so they just\nreturn the data loader with the\ncorresponding\ndata set and then we can also set the\nbatch size and the number of workers so\nyeah this is what we have to do to get\nthe data loaders\nand then we have to implement the two\nnetworks so the discriminator and the\ngenerator\nand they are both vanilla\npie charge modules so we can mix and\nmatch pie charge and pie torch lightning\nso for the discriminator we create a\nclass that inherits from nn.module and\nlike i said the discriminator is like a\ndetective so it has the\ntask to detect if it's fake or no fake\nso in the end we only need one output\nthat must be between 0 and 1\nand we only have to implement the init\nfunction and the forward function and\nbasically here you could do whatever we\nwant but in the end we only want one\noutput that is between 0 and one so you\ncould just use linear layers but in this\nexample i want to show you how we can\nuse c and n's\nso we use two convolutional 2d layers\nthen we also use a dropout layer in the\n2d case and in the end we use two linear\nlayers\nand yeah we always have to be careful\nthat we have the correct input and\noutput sizes and then at the very end we\nneed one output\nso yeah in the init function we simply\ncreate all the layers and then in the\nforward function we apply all layers\nand we also apply max pooling and\nactivation functions so here the relu\nactivation function so yeah first the\nfirst convolution with max pool and relu\nthen the second convolution with a drop\nout max pool and relu\nthen we reshape the data so we can feed\nit to the fully connected layers then we\napply the first fully connected layer so\nthe linear layer with an actuation\nfunction a drop out and the second\nfully connected layer and then in the\nvery end we apply the sigmoid function\nso this takes care that the output is\nbetween 0 and 1.\nso this is our discriminator and now the\ngenerator basically works the other way\naround so we also create a class that\ninherits from nn.module\nand here this gets as a parameter the\nnumber of latent dimensions\nso this is a scalar value and basically\nfrom this value we upsample to a output\nthat is in the shape that the original\nimages is so 1 by 28 by 28 and the\nvalues are between -1 and 1 here so here\nwe also use a linear layer then we use a\nconvolution transpose 2d layer so two\ntimes actually and then in the end we\napply a normal convolution to put it\nback again into this shape and then in\nthe forward pass we apply all these\nsteps so first we apply the linear layer\nwith a relu activation then we reshape\nthen we apply the convolution transposed\nlayer this will up sample the data to be\nin this shape then again a activation\nfunction then the second convolution\ntransposed layer this will put it into\nthis shape then again a actuation\nfunction and at the very end a normal\nconvolution which will put it back into\nthis shape and then no activation at the\nend\nso yeah this are these are the two\nuh networks that we need and now we put\nthem together in one class so let's do\nthis together so here we want to create\none class that we call again and this\ninherits from pytorch lightning dot\nlightning module and then we have to\nimplement a few functions so that it\nworks with pytorch lightning so first of\nall we need the init function with self\nand then we also give it the number of\nlatent dimensions and this should be 100\nby default then we also give it a\nlearning rate and by default this is\nzero zero two so you might actually want\nto play around with this a little bit so\nit's very the learning is very sensitive\nto the learning rate\nso it's very important that you have\nfind a good one here so yeah by default\njust try this one then the first thing\nwe do we call the super initializer so\nsuper in it like this\nand then here we store\nor first save the hyper parameters so we\nsay self dot\nsafe\nhyper parameters\nthis will\nmake it accessible everywhere later\nand then we create our two networks so\nwe say\nself.generator equals the\ngenerator network and this gets one\nparameter this is the\nlatent dimension equals\nand now we can access the hyper\nparameter by saying self dot\nhp\nparams. and now we can use latent\ndim so this is the same um name that we\nuse here for the parameter name\nso this is our generator and then we do\nthe same thing for discriminator so we\ninitialize a discriminator by saying\nthis is a\nthis criminator and this doesn't get any\ninputs and then we create a\nrandom noise that we want to use later\nto test the images so we say self dot\nvalidation\nc\nequals torch dot rant n\nand then it should be\nsix images and we use\nself.hp\nparams.latent\nunderscore\ndim for the\nsecond size\nand yeah this is the init function\nthen we need to implement the forward\nfunction which gets self and the input\ntensor so we call this c\nand the forward pass in the gan is\nactually just the generator so here we\ncan say\nreturn self dot\njenna raider and this gets the\ninput the tensor as a input so this is\nthe forward pass\nthen let's create a function to define\nthe loss so we could call this\nadversarial underscore loss\nalso with self and then it gets y hat so\nthe predicted labels and the actual\nlabels\nand here we as i said in the beginning\nwe use the binary cross entropy so this\nis just a one one liner so we say return\nf dot\nbinary\ncross\nentropy\nand then here we feed it with y hat and\ny\nthen we need the training steps so\ndefine training step so this is actually\na function from pi charge lightning that\nwe have to implement and then everything\nlater\nwill be taken care of for us\nso this gets self then it also gets a\nbatch\nthen it gets the batch index and it also\ngets the\noptimizer index\nand for now let's just say pass so we do\nthis as last step then the next function\nwe have to implement from pi torch\nlightning is the configure\noptimizer\nfunction and this only gets self\nand it's actually called configure\noptimizer s optimizers\nand here we can access the learning rate\nby saying lr equals self.h\nparams.lr\nand then\nlr\nand then we\ncreate two optimizers one for the\ngenerator and one for the discriminators\nso we say opt g\nequals\nand here we say torch dot\noptim\ndot\nadam for example which is always a good\nchoice\nand then we want to optimize self dot\ngenerator dot parameters\nand we also give it the learning rate lr\nequals l r\nthen we copy and paste this and do the\nsame thing for the optimizer for the\ndiscriminator so opt d\nand here we optimize\nself.discriminator.parameters\nand we use the same learning rate and\nthen we return those so we say return\nand then as a list\nopt underscore g and opt underscore d\nand as second parameter we return a\nempty list so this would be if you use a\nscheduler as well and then there is one\nmore function that we want to implement\nfor the pytorch lightning module so this\nis called the on\nepoc\nend\nfunction this also only gets self and\nhere after each epoch i simply want to\nplot a few\nuh generated images to see how our fake\ndata looks like so here i call\nself.plot\nimages and this is a function that i\nsimply copy and paste for you\nso let me copy and paste this in here\nso this will be\none\nfunction\nand what we do here is here we use this\nvalidation c that we created up here so\nthis is a random noise with six images\nand this number of latent dimensions\nso here we use this and then we also use\nthis type s function this will basically\nmove it to the gpu or not if we don't\nuse a gpu\nthen we call self.c\nso self.c will\nexecute the forward pass so this will\nexecute the generator so this will\ngenerate some images and then we put it\nback to the cpu\nand then here i have a\nsmall code snippet to generate some\nimages with matplotlib and then plot\nthis as a grid\nso yeah this is what we do after each\nepoch and now the only thing left to do\nis to implement the training steps so\nlet's do this so here um first of all\nhere i noticed i have a typo so this is\njust self.h params\nand yeah now let's do this so here first\nwe want to unpack the batch so we call\nthis the real images and then just an\nunderscore because we don't need the\nlabels and this comes from the batch\nand then again we create a sample\nnoise data so here we call this c\nequals torch dot\nrand n\nand now it gets the shape of the real\nm e m g s and then the first dimension\nzero\nand a second um\nnumber it gets the number of latent dims\nso self.h\nparams.latent\n[Music]\nand\nthen we again have to move it to the gpu\nif we use one so we say c equals c dot\ntype underscore s and then we want to\nuse the same type as the image tensor\nand then we\nmake a\nif statement for both optimizers\nso in the first um case we want to\ntrain the\ngenerator so the first optimizer opt g\nso here we check if now\noptimizer underscore index\nequals equals zero\nthen we train the generator and here i\nwill write the formula for you so here\nwe want to maximize\nthe lock off and here we use the\ndiscriminator and then the generator\nof\nc and c will be\nfake images\nso yeah again i will put a\nreference below the video where you can\nread more about the loss function but\nyeah this is basically the formula\ntogether with this binary cross entropy\nloss\nso\nlet's do this so first we want to\ncall the generator with c\nso we\nand this will be the fake images so here\nwe can call self dot c\nso again self dot c will\nexecute the forward pass which will\nexecute the generator so this is this\npart\nthen we need the discriminator and this\nwill be the y hat so the predicted\nlabels so here we call self dot this\ncriminator and we put in the fake\nimages and now as real wise so as real\nlabels we say y\nequals and this will be a tensor of\nsimply once\nand here again we say real\nimages dot\nsize\nzero and as second thing we just say one\nand then again we have to move it to the\ngpu so again we can use um\ntype s so here we say y equals\ny type as the real images\nand then we create the or calculate the\nloss so we call this g\nloss equals self dot\nadversarial\nloss so the function\nand this gets y hat and y\nand then we return this so pytorch\nlightning knows what to do with this\nloss\nso\nwe create a lock\ndictionary and this is a dictionary with\nthe value\nwith the key\ng loss and we put in the\ng loss in here and then we return a\nanother full dictionary that this that\nhas this included so first of all we\nneed to have the\nkey loss in here and the loss will be\nthe\ng loss\nthen we can also\nsay the progress\nunderscore bar\ngets also the\nlock\ndictionary and we and for logging if we\nwant to use this later for example in\nthe tensorboard then we can also use\nthis key and this also gets the log\ndictionary\nso\nyeah this is what we have to do for the\ngenerator and then we do a similar\nthings for the um\ndiscriminator so if optimizer\nindex equals equals one\nand let's first write a\ncomment here\nso here we want to\ntrain the discriminator and now again i\nwill\nsay what we have to maximize so here we\nwant to maximize the lock\nof\nd of\nx and x will be the original images plus\nand then here the\nlock off\none\nminus\nd\nof\ng\nof\nc\nso\num\nlet's go over this and then it might\nbecome clearer what this means so here\nwe want to measure the discriminator's\nability to classify a real from\ngenerated images\nso we want to check how well can it\nlabel as real so how\nwell can it label as real and then also\nhow well can it label the generated\nimages as fake\nand these are essentially those two\nparts\nso for the first one um the code is very\nsimilar to this one\nso\nwe create y hat real\nequals and then we again\nself dot\nthis criminator and now we don't put in\nfake images but here we put in\nreal the real images\nand this is the y head reel and the\nnormal y reel so the actual\nlabels\nin this case it's also just torch dot\nonce like we use here and then again we\nhave to move it to the gpu if we use one\nso why\nreal equals y real type is real images\nand then we calculate the real loss\nequals\nself\ndot\nadversarial\nloss of\nwhy\nhad\nreal and y hat so this is the first part\nso this is very similar to here and now\nwe need this part\num so\nhere y\num real\nand now how well can it label as fake so\nhere we call this y\nhead\nfake and then again self dot\nthis criminator\nof\nself with\nc\nand now we have to be careful so\num\nself dot c will basically generate fake\nimages\nand\nwe already did this here and we want to\nuse the same ones here\nbut\nwhen we\nexecute this then this will do\ncalculations on the graph and we don't\nwant to do this twice so that's why here\nwe call c self c dot d touch so this\nwill create a new tensor that is\ndetached from the computational graph so\nbe careful here\nand yeah this is our y head fake and\nthen the y fake equals\nand this is very similar to here so then\nagain we want to move this to the\ngpu so y\nfake equals y\nfake\nand\nnow we have to be careful so now this\npart says 1 minus this\nso\nactually if we have a look at the binary\ncross entropy formula\nthen we find out that if we put in zeros\nhere\nthen we actually end up with this\nformula\nso here we put in torch dot zeros so\nyeah again i recommend that you just\ndouble check this for yourself and then\nwe also calculate the fake loss again\nwith self dot\nadversarial\nloss and here we put in y\nhead\nfake and why\nfake and then the total loss is the\naverage so we call this the d\nloss so the discriminator loss equals\nand here we say\nreal\nloss\nplus the fake loss\ndivided by two and then the same as we\ndo here we create a log\ndictionary\nand return this and in this case we call\nthis d loss and use d loss here and also\nhere\nand now this is everything that we need\nfor the training step\nso let's\ncreate a new code cell and now we want\nto\nset everything up so we create a data\nmodule by saying this is the\nmnist data module that we created\nthen we create a model so this is our\ngan\nthen again um\nand now before i want to train this i\nactually want to call this plot images\nonce so that we see that in the\nbeginning we simply plot noise\nso let's say model dot\nplot images\nand then let's run everything and see if\nit works until here and yes so here we\nhave the first generated images and as\nwe can see this is just noise\nand now we want to\ntrain our gans so we call a we create a\npytorch lightning dot trainer\nand here we can specify the maximum\nnumber of epochs equals let's use 20\nand then we can also use the gpus equals\nthe\nmax\num what would we call it in the\nbeginning\nmax no available\ngpus\nso let's put this also into our trainer\nand then we simply say trainer dot fit\nthe model\nand we also put in the data module and\nthen we\nrun this and when we execute this you\nmight see that after each epoch we\nshould print the images so let's do this\nand we get a\ntype error so in this part i forgot to\ncall shape so here in the training step\nwe must say real images dot\nshape and then zero so now again we have\nto run this cell\nthen\ndown here we again run this cell let's\nagain plot this this should be the same\nrandom noise and now let's run the\ntrainer again\nand training is done so now we can\nscroll down and see after the first\nepoch so this is actually yeah after\nepoch zero we get this images so it\nstill looks like random noise\nepoch one we have this images then this\nit still looks like noise but then after\nepoch 4 it starts getting better and\nit's starting to look like images so now\nlet's scroll down a little bit\nand yeah so here it still\nlooks a bit noisy\nbut for example this might be a three\nthis might be a four this might be a\nzero so yeah it's starting to\nget into shape and yeah our code works\nand yeah i actually recommend you that\nyou play around with the hyper\nparameters a little bit and also maybe\nincrease the maximum epochs\nand then test this for yourself again\nthe call up will be the link to the call\nup will be in the description below and\nyeah i hope you enjoyed this video if\nyou did so then please hit the like\nbutton and consider subscribing to the\nchannel and then i hope to see you next\ntime bye\n",
  "words": [
    "hi",
    "everyone",
    "patrick",
    "assembly",
    "ai",
    "team",
    "today",
    "learn",
    "generative",
    "adversarial",
    "networks",
    "short",
    "gans",
    "might",
    "seen",
    "popular",
    "example",
    "gans",
    "generate",
    "fake",
    "images",
    "humans",
    "look",
    "incredibly",
    "real",
    "gans",
    "indeed",
    "really",
    "powerful",
    "one",
    "fascinating",
    "ideas",
    "deep",
    "learning",
    "recent",
    "years",
    "today",
    "quick",
    "look",
    "theory",
    "behind",
    "gans",
    "code",
    "one",
    "scratch",
    "using",
    "pytorch",
    "let",
    "get",
    "started",
    "alright",
    "let",
    "look",
    "theory",
    "first",
    "promise",
    "wo",
    "difficult",
    "idea",
    "actually",
    "brilliant",
    "simple",
    "super",
    "powerful",
    "gans",
    "learn",
    "generate",
    "new",
    "data",
    "statistics",
    "training",
    "set",
    "gans",
    "consist",
    "two",
    "networks",
    "playing",
    "adversarial",
    "game",
    "name",
    "generative",
    "adversarial",
    "networks",
    "goal",
    "generate",
    "fake",
    "data",
    "close",
    "possible",
    "training",
    "data",
    "two",
    "networks",
    "play",
    "game",
    "work",
    "exactly",
    "two",
    "networks",
    "called",
    "generator",
    "discriminator",
    "generator",
    "produces",
    "fake",
    "data",
    "tries",
    "trick",
    "discriminator",
    "discriminator",
    "inspects",
    "fake",
    "data",
    "determines",
    "real",
    "fake",
    "like",
    "detective",
    "play",
    "basically",
    "training",
    "first",
    "initialized",
    "randomly",
    "trained",
    "simultaneously",
    "means",
    "minimize",
    "two",
    "losses",
    "also",
    "use",
    "two",
    "optimizer",
    "use",
    "binary",
    "cross",
    "entropy",
    "loss",
    "going",
    "detail",
    "loss",
    "formula",
    "link",
    "another",
    "resource",
    "video",
    "want",
    "learn",
    "yeah",
    "basically",
    "whole",
    "concept",
    "jump",
    "code",
    "let",
    "look",
    "example",
    "later",
    "use",
    "mnist",
    "data",
    "set",
    "generator",
    "tries",
    "generate",
    "mnist",
    "images",
    "digits",
    "zero",
    "nine",
    "tries",
    "trick",
    "discriminator",
    "play",
    "sides",
    "get",
    "better",
    "better",
    "beginning",
    "know",
    "anything",
    "randomly",
    "initialized",
    "generator",
    "produces",
    "noise",
    "random",
    "data",
    "might",
    "look",
    "like",
    "discriminator",
    "looks",
    "also",
    "look",
    "real",
    "data",
    "compare",
    "might",
    "easily",
    "say",
    "yeah",
    "fake",
    "learning",
    "training",
    "continues",
    "generator",
    "comes",
    "new",
    "data",
    "might",
    "look",
    "something",
    "like",
    "discriminator",
    "looks",
    "simply",
    "still",
    "say",
    "yes",
    "still",
    "fake",
    "point",
    "generator",
    "gets",
    "better",
    "better",
    "discriminator",
    "might",
    "tricked",
    "say",
    "yeah",
    "actually",
    "real",
    "data",
    "obviously",
    "example",
    "data",
    "still",
    "perfect",
    "course",
    "also",
    "discriminator",
    "still",
    "perfect",
    "also",
    "improve",
    "continues",
    "sides",
    "get",
    "better",
    "better",
    "eventually",
    "get",
    "get",
    "generated",
    "data",
    "easily",
    "distinguishable",
    "original",
    "data",
    "yeah",
    "works",
    "let",
    "jump",
    "code",
    "see",
    "implement",
    "alright",
    "google",
    "collab",
    "already",
    "prepared",
    "code",
    "start",
    "later",
    "write",
    "rest",
    "code",
    "together",
    "recommend",
    "copy",
    "collab",
    "folder",
    "follow",
    "put",
    "link",
    "description",
    "first",
    "thing",
    "set",
    "runtime",
    "click",
    "change",
    "runtime",
    "type",
    "select",
    "gpu",
    "training",
    "much",
    "faster",
    "first",
    "thing",
    "say",
    "pip",
    "install",
    "pi",
    "torch",
    "lightning",
    "also",
    "use",
    "pi",
    "torch",
    "lightning",
    "make",
    "code",
    "little",
    "bit",
    "shorter",
    "also",
    "want",
    "mention",
    "official",
    "gan",
    "tutorial",
    "pytorch",
    "lightning",
    "website",
    "pretty",
    "similar",
    "code",
    "case",
    "use",
    "normal",
    "linear",
    "layers",
    "case",
    "use",
    "cnns",
    "little",
    "bit",
    "different",
    "yeah",
    "also",
    "great",
    "resource",
    "check",
    "yeah",
    "install",
    "imports",
    "need",
    "torch",
    "torch",
    "vision",
    "different",
    "modules",
    "also",
    "use",
    "pi",
    "matplotlib",
    "pi",
    "torch",
    "lightning",
    "set",
    "parameters",
    "example",
    "random",
    "seed",
    "batch",
    "size",
    "check",
    "gpus",
    "multiple",
    "cpu",
    "cores",
    "first",
    "thing",
    "create",
    "data",
    "module",
    "inherits",
    "pytorch",
    "lightning",
    "dot",
    "lightning",
    "data",
    "module",
    "responsible",
    "create",
    "data",
    "loaders",
    "us",
    "training",
    "validation",
    "test",
    "set",
    "way",
    "works",
    "implement",
    "init",
    "function",
    "simply",
    "define",
    "parameters",
    "also",
    "define",
    "transformations",
    "convert",
    "images",
    "tensor",
    "normalize",
    "mean",
    "standard",
    "deviation",
    "mnist",
    "images",
    "implement",
    "prepared",
    "data",
    "function",
    "call",
    "built",
    "mnist",
    "data",
    "set",
    "download",
    "equals",
    "true",
    "download",
    "data",
    "one",
    "time",
    "say",
    "training",
    "equals",
    "true",
    "training",
    "set",
    "one",
    "time",
    "training",
    "equals",
    "false",
    "test",
    "set",
    "also",
    "implement",
    "setup",
    "function",
    "split",
    "training",
    "data",
    "training",
    "validation",
    "set",
    "applying",
    "random",
    "split",
    "function",
    "yeah",
    "test",
    "set",
    "simply",
    "also",
    "create",
    "mnist",
    "data",
    "saying",
    "training",
    "equals",
    "false",
    "steps",
    "already",
    "apply",
    "transformations",
    "implement",
    "three",
    "functions",
    "train",
    "data",
    "loader",
    "validation",
    "data",
    "loader",
    "test",
    "data",
    "data",
    "loader",
    "pretty",
    "simple",
    "return",
    "data",
    "loader",
    "corresponding",
    "data",
    "set",
    "also",
    "set",
    "batch",
    "size",
    "number",
    "workers",
    "yeah",
    "get",
    "data",
    "loaders",
    "implement",
    "two",
    "networks",
    "discriminator",
    "generator",
    "vanilla",
    "pie",
    "charge",
    "modules",
    "mix",
    "match",
    "pie",
    "charge",
    "pie",
    "torch",
    "lightning",
    "discriminator",
    "create",
    "class",
    "inherits",
    "like",
    "said",
    "discriminator",
    "like",
    "detective",
    "task",
    "detect",
    "fake",
    "fake",
    "end",
    "need",
    "one",
    "output",
    "must",
    "0",
    "1",
    "implement",
    "init",
    "function",
    "forward",
    "function",
    "basically",
    "could",
    "whatever",
    "want",
    "end",
    "want",
    "one",
    "output",
    "0",
    "one",
    "could",
    "use",
    "linear",
    "layers",
    "example",
    "want",
    "show",
    "use",
    "c",
    "n",
    "use",
    "two",
    "convolutional",
    "2d",
    "layers",
    "also",
    "use",
    "dropout",
    "layer",
    "2d",
    "case",
    "end",
    "use",
    "two",
    "linear",
    "layers",
    "yeah",
    "always",
    "careful",
    "correct",
    "input",
    "output",
    "sizes",
    "end",
    "need",
    "one",
    "output",
    "yeah",
    "init",
    "function",
    "simply",
    "create",
    "layers",
    "forward",
    "function",
    "apply",
    "layers",
    "also",
    "apply",
    "max",
    "pooling",
    "activation",
    "functions",
    "relu",
    "activation",
    "function",
    "yeah",
    "first",
    "first",
    "convolution",
    "max",
    "pool",
    "relu",
    "second",
    "convolution",
    "drop",
    "max",
    "pool",
    "relu",
    "reshape",
    "data",
    "feed",
    "fully",
    "connected",
    "layers",
    "apply",
    "first",
    "fully",
    "connected",
    "layer",
    "linear",
    "layer",
    "actuation",
    "function",
    "drop",
    "second",
    "fully",
    "connected",
    "layer",
    "end",
    "apply",
    "sigmoid",
    "function",
    "takes",
    "care",
    "output",
    "0",
    "discriminator",
    "generator",
    "basically",
    "works",
    "way",
    "around",
    "also",
    "create",
    "class",
    "inherits",
    "gets",
    "parameter",
    "number",
    "latent",
    "dimensions",
    "scalar",
    "value",
    "basically",
    "value",
    "upsample",
    "output",
    "shape",
    "original",
    "images",
    "1",
    "28",
    "28",
    "values",
    "1",
    "also",
    "use",
    "linear",
    "layer",
    "use",
    "convolution",
    "transpose",
    "2d",
    "layer",
    "two",
    "times",
    "actually",
    "end",
    "apply",
    "normal",
    "convolution",
    "put",
    "back",
    "shape",
    "forward",
    "pass",
    "apply",
    "steps",
    "first",
    "apply",
    "linear",
    "layer",
    "relu",
    "activation",
    "reshape",
    "apply",
    "convolution",
    "transposed",
    "layer",
    "sample",
    "data",
    "shape",
    "activation",
    "function",
    "second",
    "convolution",
    "transposed",
    "layer",
    "put",
    "shape",
    "actuation",
    "function",
    "end",
    "normal",
    "convolution",
    "put",
    "back",
    "shape",
    "activation",
    "end",
    "yeah",
    "two",
    "uh",
    "networks",
    "need",
    "put",
    "together",
    "one",
    "class",
    "let",
    "together",
    "want",
    "create",
    "one",
    "class",
    "call",
    "inherits",
    "pytorch",
    "lightning",
    "dot",
    "lightning",
    "module",
    "implement",
    "functions",
    "works",
    "pytorch",
    "lightning",
    "first",
    "need",
    "init",
    "function",
    "self",
    "also",
    "give",
    "number",
    "latent",
    "dimensions",
    "100",
    "default",
    "also",
    "give",
    "learning",
    "rate",
    "default",
    "zero",
    "zero",
    "two",
    "might",
    "actually",
    "want",
    "play",
    "around",
    "little",
    "bit",
    "learning",
    "sensitive",
    "learning",
    "rate",
    "important",
    "find",
    "good",
    "one",
    "yeah",
    "default",
    "try",
    "one",
    "first",
    "thing",
    "call",
    "super",
    "initializer",
    "super",
    "like",
    "store",
    "first",
    "save",
    "hyper",
    "parameters",
    "say",
    "self",
    "dot",
    "safe",
    "hyper",
    "parameters",
    "make",
    "accessible",
    "everywhere",
    "later",
    "create",
    "two",
    "networks",
    "say",
    "equals",
    "generator",
    "network",
    "gets",
    "one",
    "parameter",
    "latent",
    "dimension",
    "equals",
    "access",
    "hyper",
    "parameter",
    "saying",
    "self",
    "dot",
    "hp",
    "params",
    "use",
    "latent",
    "dim",
    "um",
    "name",
    "use",
    "parameter",
    "name",
    "generator",
    "thing",
    "discriminator",
    "initialize",
    "discriminator",
    "saying",
    "criminator",
    "get",
    "inputs",
    "create",
    "random",
    "noise",
    "want",
    "use",
    "later",
    "test",
    "images",
    "say",
    "self",
    "dot",
    "validation",
    "c",
    "equals",
    "torch",
    "dot",
    "rant",
    "n",
    "six",
    "images",
    "use",
    "underscore",
    "dim",
    "second",
    "size",
    "yeah",
    "init",
    "function",
    "need",
    "implement",
    "forward",
    "function",
    "gets",
    "self",
    "input",
    "tensor",
    "call",
    "c",
    "forward",
    "pass",
    "gan",
    "actually",
    "generator",
    "say",
    "return",
    "self",
    "dot",
    "jenna",
    "raider",
    "gets",
    "input",
    "tensor",
    "input",
    "forward",
    "pass",
    "let",
    "create",
    "function",
    "define",
    "loss",
    "could",
    "call",
    "adversarial",
    "underscore",
    "loss",
    "also",
    "self",
    "gets",
    "hat",
    "predicted",
    "labels",
    "actual",
    "labels",
    "said",
    "beginning",
    "use",
    "binary",
    "cross",
    "entropy",
    "one",
    "one",
    "liner",
    "say",
    "return",
    "f",
    "dot",
    "binary",
    "cross",
    "entropy",
    "feed",
    "hat",
    "need",
    "training",
    "steps",
    "define",
    "training",
    "step",
    "actually",
    "function",
    "pi",
    "charge",
    "lightning",
    "implement",
    "everything",
    "later",
    "taken",
    "care",
    "us",
    "gets",
    "self",
    "also",
    "gets",
    "batch",
    "gets",
    "batch",
    "index",
    "also",
    "gets",
    "optimizer",
    "index",
    "let",
    "say",
    "pass",
    "last",
    "step",
    "next",
    "function",
    "implement",
    "pi",
    "torch",
    "lightning",
    "configure",
    "optimizer",
    "function",
    "gets",
    "self",
    "actually",
    "called",
    "configure",
    "optimizer",
    "optimizers",
    "access",
    "learning",
    "rate",
    "saying",
    "lr",
    "equals",
    "lr",
    "create",
    "two",
    "optimizers",
    "one",
    "generator",
    "one",
    "discriminators",
    "say",
    "opt",
    "g",
    "equals",
    "say",
    "torch",
    "dot",
    "optim",
    "dot",
    "adam",
    "example",
    "always",
    "good",
    "choice",
    "want",
    "optimize",
    "self",
    "dot",
    "generator",
    "dot",
    "parameters",
    "also",
    "give",
    "learning",
    "rate",
    "lr",
    "equals",
    "l",
    "r",
    "copy",
    "paste",
    "thing",
    "optimizer",
    "discriminator",
    "opt",
    "optimize",
    "use",
    "learning",
    "rate",
    "return",
    "say",
    "return",
    "list",
    "opt",
    "underscore",
    "g",
    "opt",
    "underscore",
    "second",
    "parameter",
    "return",
    "empty",
    "list",
    "would",
    "use",
    "scheduler",
    "well",
    "one",
    "function",
    "want",
    "implement",
    "pytorch",
    "lightning",
    "module",
    "called",
    "epoc",
    "end",
    "function",
    "also",
    "gets",
    "self",
    "epoch",
    "simply",
    "want",
    "plot",
    "uh",
    "generated",
    "images",
    "see",
    "fake",
    "data",
    "looks",
    "like",
    "call",
    "images",
    "function",
    "simply",
    "copy",
    "paste",
    "let",
    "copy",
    "paste",
    "one",
    "function",
    "use",
    "validation",
    "c",
    "created",
    "random",
    "noise",
    "six",
    "images",
    "number",
    "latent",
    "dimensions",
    "use",
    "also",
    "use",
    "type",
    "function",
    "basically",
    "move",
    "gpu",
    "use",
    "gpu",
    "call",
    "execute",
    "forward",
    "pass",
    "execute",
    "generator",
    "generate",
    "images",
    "put",
    "back",
    "cpu",
    "small",
    "code",
    "snippet",
    "generate",
    "images",
    "matplotlib",
    "plot",
    "grid",
    "yeah",
    "epoch",
    "thing",
    "left",
    "implement",
    "training",
    "steps",
    "let",
    "um",
    "first",
    "noticed",
    "typo",
    "params",
    "yeah",
    "let",
    "first",
    "want",
    "unpack",
    "batch",
    "call",
    "real",
    "images",
    "underscore",
    "need",
    "labels",
    "comes",
    "batch",
    "create",
    "sample",
    "noise",
    "data",
    "call",
    "c",
    "equals",
    "torch",
    "dot",
    "rand",
    "n",
    "gets",
    "shape",
    "real",
    "e",
    "g",
    "first",
    "dimension",
    "zero",
    "second",
    "um",
    "number",
    "gets",
    "number",
    "latent",
    "dims",
    "music",
    "move",
    "gpu",
    "use",
    "one",
    "say",
    "c",
    "equals",
    "c",
    "dot",
    "type",
    "underscore",
    "want",
    "use",
    "type",
    "image",
    "tensor",
    "make",
    "statement",
    "optimizers",
    "first",
    "um",
    "case",
    "want",
    "train",
    "generator",
    "first",
    "optimizer",
    "opt",
    "g",
    "check",
    "optimizer",
    "underscore",
    "index",
    "equals",
    "equals",
    "zero",
    "train",
    "generator",
    "write",
    "formula",
    "want",
    "maximize",
    "lock",
    "use",
    "discriminator",
    "generator",
    "c",
    "c",
    "fake",
    "images",
    "yeah",
    "put",
    "reference",
    "video",
    "read",
    "loss",
    "function",
    "yeah",
    "basically",
    "formula",
    "together",
    "binary",
    "cross",
    "entropy",
    "loss",
    "let",
    "first",
    "want",
    "call",
    "generator",
    "c",
    "fake",
    "images",
    "call",
    "self",
    "dot",
    "c",
    "self",
    "dot",
    "c",
    "execute",
    "forward",
    "pass",
    "execute",
    "generator",
    "part",
    "need",
    "discriminator",
    "hat",
    "predicted",
    "labels",
    "call",
    "self",
    "dot",
    "criminator",
    "put",
    "fake",
    "images",
    "real",
    "wise",
    "real",
    "labels",
    "say",
    "equals",
    "tensor",
    "simply",
    "say",
    "real",
    "images",
    "dot",
    "size",
    "zero",
    "second",
    "thing",
    "say",
    "one",
    "move",
    "gpu",
    "use",
    "um",
    "type",
    "say",
    "equals",
    "type",
    "real",
    "images",
    "create",
    "calculate",
    "loss",
    "call",
    "g",
    "loss",
    "equals",
    "self",
    "dot",
    "adversarial",
    "loss",
    "function",
    "gets",
    "hat",
    "return",
    "pytorch",
    "lightning",
    "knows",
    "loss",
    "create",
    "lock",
    "dictionary",
    "dictionary",
    "value",
    "key",
    "g",
    "loss",
    "put",
    "g",
    "loss",
    "return",
    "another",
    "full",
    "dictionary",
    "included",
    "first",
    "need",
    "key",
    "loss",
    "loss",
    "g",
    "loss",
    "also",
    "say",
    "progress",
    "underscore",
    "bar",
    "gets",
    "also",
    "lock",
    "dictionary",
    "logging",
    "want",
    "use",
    "later",
    "example",
    "tensorboard",
    "also",
    "use",
    "key",
    "also",
    "gets",
    "log",
    "dictionary",
    "yeah",
    "generator",
    "similar",
    "things",
    "um",
    "discriminator",
    "optimizer",
    "index",
    "equals",
    "equals",
    "one",
    "let",
    "first",
    "write",
    "comment",
    "want",
    "train",
    "discriminator",
    "say",
    "maximize",
    "want",
    "maximize",
    "lock",
    "x",
    "x",
    "original",
    "images",
    "plus",
    "lock",
    "one",
    "minus",
    "g",
    "c",
    "um",
    "let",
    "go",
    "might",
    "become",
    "clearer",
    "means",
    "want",
    "measure",
    "discriminator",
    "ability",
    "classify",
    "real",
    "generated",
    "images",
    "want",
    "check",
    "well",
    "label",
    "real",
    "well",
    "label",
    "real",
    "also",
    "well",
    "label",
    "generated",
    "images",
    "fake",
    "essentially",
    "two",
    "parts",
    "first",
    "one",
    "um",
    "code",
    "similar",
    "one",
    "create",
    "hat",
    "real",
    "equals",
    "self",
    "dot",
    "criminator",
    "put",
    "fake",
    "images",
    "put",
    "real",
    "real",
    "images",
    "head",
    "reel",
    "normal",
    "reel",
    "actual",
    "labels",
    "case",
    "also",
    "torch",
    "dot",
    "like",
    "use",
    "move",
    "gpu",
    "use",
    "one",
    "real",
    "equals",
    "real",
    "type",
    "real",
    "images",
    "calculate",
    "real",
    "loss",
    "equals",
    "self",
    "dot",
    "adversarial",
    "loss",
    "real",
    "hat",
    "first",
    "part",
    "similar",
    "need",
    "part",
    "um",
    "um",
    "real",
    "well",
    "label",
    "fake",
    "call",
    "head",
    "fake",
    "self",
    "dot",
    "criminator",
    "self",
    "c",
    "careful",
    "um",
    "self",
    "dot",
    "c",
    "basically",
    "generate",
    "fake",
    "images",
    "already",
    "want",
    "use",
    "ones",
    "execute",
    "calculations",
    "graph",
    "want",
    "twice",
    "call",
    "c",
    "self",
    "c",
    "dot",
    "touch",
    "create",
    "new",
    "tensor",
    "detached",
    "computational",
    "graph",
    "careful",
    "yeah",
    "head",
    "fake",
    "fake",
    "equals",
    "similar",
    "want",
    "move",
    "gpu",
    "fake",
    "equals",
    "fake",
    "careful",
    "part",
    "says",
    "1",
    "minus",
    "actually",
    "look",
    "binary",
    "cross",
    "entropy",
    "formula",
    "find",
    "put",
    "zeros",
    "actually",
    "end",
    "formula",
    "put",
    "torch",
    "dot",
    "zeros",
    "yeah",
    "recommend",
    "double",
    "check",
    "also",
    "calculate",
    "fake",
    "loss",
    "self",
    "dot",
    "adversarial",
    "loss",
    "put",
    "head",
    "fake",
    "fake",
    "total",
    "loss",
    "average",
    "call",
    "loss",
    "discriminator",
    "loss",
    "equals",
    "say",
    "real",
    "loss",
    "plus",
    "fake",
    "loss",
    "divided",
    "two",
    "create",
    "log",
    "dictionary",
    "return",
    "case",
    "call",
    "loss",
    "use",
    "loss",
    "also",
    "everything",
    "need",
    "training",
    "step",
    "let",
    "create",
    "new",
    "code",
    "cell",
    "want",
    "set",
    "everything",
    "create",
    "data",
    "module",
    "saying",
    "mnist",
    "data",
    "module",
    "created",
    "create",
    "model",
    "gan",
    "um",
    "want",
    "train",
    "actually",
    "want",
    "call",
    "plot",
    "images",
    "see",
    "beginning",
    "simply",
    "plot",
    "noise",
    "let",
    "say",
    "model",
    "dot",
    "plot",
    "images",
    "let",
    "run",
    "everything",
    "see",
    "works",
    "yes",
    "first",
    "generated",
    "images",
    "see",
    "noise",
    "want",
    "train",
    "gans",
    "call",
    "create",
    "pytorch",
    "lightning",
    "dot",
    "trainer",
    "specify",
    "maximum",
    "number",
    "epochs",
    "equals",
    "let",
    "use",
    "20",
    "also",
    "use",
    "gpus",
    "equals",
    "max",
    "um",
    "would",
    "call",
    "beginning",
    "max",
    "available",
    "gpus",
    "let",
    "put",
    "also",
    "trainer",
    "simply",
    "say",
    "trainer",
    "dot",
    "fit",
    "model",
    "also",
    "put",
    "data",
    "module",
    "run",
    "execute",
    "might",
    "see",
    "epoch",
    "print",
    "images",
    "let",
    "get",
    "type",
    "error",
    "part",
    "forgot",
    "call",
    "shape",
    "training",
    "step",
    "must",
    "say",
    "real",
    "images",
    "dot",
    "shape",
    "zero",
    "run",
    "cell",
    "run",
    "cell",
    "let",
    "plot",
    "random",
    "noise",
    "let",
    "run",
    "trainer",
    "training",
    "done",
    "scroll",
    "see",
    "first",
    "epoch",
    "actually",
    "yeah",
    "epoch",
    "zero",
    "get",
    "images",
    "still",
    "looks",
    "like",
    "random",
    "noise",
    "epoch",
    "one",
    "images",
    "still",
    "looks",
    "like",
    "noise",
    "epoch",
    "4",
    "starts",
    "getting",
    "better",
    "starting",
    "look",
    "like",
    "images",
    "let",
    "scroll",
    "little",
    "bit",
    "yeah",
    "still",
    "looks",
    "bit",
    "noisy",
    "example",
    "might",
    "three",
    "might",
    "four",
    "might",
    "zero",
    "yeah",
    "starting",
    "get",
    "shape",
    "yeah",
    "code",
    "works",
    "yeah",
    "actually",
    "recommend",
    "play",
    "around",
    "hyper",
    "parameters",
    "little",
    "bit",
    "also",
    "maybe",
    "increase",
    "maximum",
    "epochs",
    "test",
    "call",
    "link",
    "call",
    "description",
    "yeah",
    "hope",
    "enjoyed",
    "video",
    "please",
    "hit",
    "like",
    "button",
    "consider",
    "subscribing",
    "channel",
    "hope",
    "see",
    "next",
    "time",
    "bye"
  ],
  "keywords": [
    "adversarial",
    "networks",
    "gans",
    "might",
    "example",
    "generate",
    "fake",
    "images",
    "look",
    "real",
    "one",
    "learning",
    "code",
    "pytorch",
    "let",
    "get",
    "first",
    "actually",
    "data",
    "training",
    "set",
    "two",
    "play",
    "generator",
    "discriminator",
    "like",
    "basically",
    "also",
    "use",
    "optimizer",
    "binary",
    "cross",
    "entropy",
    "loss",
    "formula",
    "want",
    "yeah",
    "later",
    "mnist",
    "zero",
    "better",
    "noise",
    "random",
    "looks",
    "say",
    "simply",
    "still",
    "gets",
    "generated",
    "works",
    "see",
    "implement",
    "put",
    "thing",
    "type",
    "gpu",
    "pi",
    "torch",
    "lightning",
    "little",
    "bit",
    "similar",
    "case",
    "linear",
    "layers",
    "check",
    "need",
    "parameters",
    "batch",
    "create",
    "module",
    "dot",
    "validation",
    "test",
    "init",
    "function",
    "tensor",
    "call",
    "equals",
    "saying",
    "apply",
    "train",
    "return",
    "number",
    "end",
    "output",
    "forward",
    "c",
    "layer",
    "max",
    "activation",
    "convolution",
    "second",
    "parameter",
    "latent",
    "shape",
    "pass",
    "self",
    "rate",
    "um",
    "underscore",
    "hat",
    "labels",
    "opt",
    "g",
    "well",
    "epoch",
    "plot",
    "move",
    "execute",
    "lock",
    "part",
    "dictionary",
    "run"
  ]
}