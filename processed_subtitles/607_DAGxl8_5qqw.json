{
  "text": "welcome back to my channel in this video\nI'm going to discuss the cuts of\ndimensionality so the cuts of\ndimensionality is a problem if you're\nbuilding a machine learning model and\nyou want to add more Dimensions or\nfeatures into your uh data you might\nthink that adding more dimensions and\nfeatures into your data will increase\nthe performance of your model but\nparadoxically adding more Dimensions to\nyour data will make your model less\neffective this is because of the problem\ncalled the cuts of dimensionality so in\nthis video I will discuss about the\nlocal versus global model how locality\nloses its meaning in higher Dimension\nthe boundary problems because of higher\ndimensional data sets data sparcity\nproblem and practical implications and\nsome mitigation tools let's get started\nlocal versus global problem so first\nlet's understand how different model\nperceive data local models like KNN K\nnearest neighbors make predictions based\non nearby data point\nthey focus on the local structure of the\ndata so let's see how KNN Works let's\nassume that you have two classes two\ncluster Class A and class P and you have\na new data point depending on the\nneighboring points in this case k is\nequal to 5 you look at how many\nneighboring points belongs to uh class P\nand a in this scenario you can see three\ndata points belongs to class B and two\nbelongs to class A so you classified the\nnew point uh should goes into class B\nand this this is why KNN is a local\nmodel because it's making predictions\nbased on nearby\n[Music]\npoints so this is how KNN Works let's\nsee how linear regression works unlike\nKNN linear regression is a global model\nbecause it considers the full set of\ndata points in order to come up with the\nbest fit line and once you have the best\nfit line you can do use it to do\ninference so given a new data point you\ncan use the best fit line to find the\nvalue of y so this way linear regression\nis considered as a global model because\nit's making the predictions based on the\ncontributions from all the data\n[Music]\npoints The Mirage of local in higher\ndimension in higher Dimensions our\nintuitive sense of local becomes\ndistorted look at this formula this\nformula shows you this distance from\nCenter to capture the fraction of volume\nin the space r is the fraction of volume\nand P is the number of\nDimensions imagine working in a 10\ndimensional space so to capture just 1%\nof the data using let's say KNN you\nwould need to include about 63% of each\nDimensions\nrange to capture 10% of the data you\nwill need to include 80% of each\nDimensions what was once a small\nneighborhood now spans almost the entire\nspace so the third point is why the\nboundary problems become important in\nhigher Dimensions let's have a look in\nhigher Dimension spaces data points tend\nto be near to the edges rather than the\ncenter for example if you have a 10\ndimensional sphere with 500 uniformly\ndistributed points the closest point to\nthe center is more than halfway to the\nboundary this presents a challenge\nbecause the K&N model needs to extract\naate rather than interpolate in order to\nmake the predictions and what about the\nglobal models like linear regression\nmodels well the face something called\nthe H biases as the bulk of the data is\ncured towards the boundary let's talk\nabout the data sparcity problem in\nhigher Dimensions the sampling density\nof data sets is represented by n^ 1 / P\nassuming you're building a prediction\nmodel with one dimensions and you have\n100 data points and now you add more\ndimension and and and you want to build\na model with 10 features so 10\ndimensional model to make a prediction\nin order to achieve the same density of\ndata points compared to like one 1D\nproblem you would need 100 to the power\n10 data points that means one followed\nby 20 zeros that's a lot of data points\nif you add more dimensions and do not\nincrease the data points then the data\nwill become more and more sparse in\nhigher Dimensions making your model less\neffective so how do you deal with this\nproblem well to deal with this problem\nthere are some methods you can apply the\nfirst thing you need to do is understand\nyour problem understand your data and\nunderstand the features of your model\nfeature selection is very important do\nnot just add more features in order to\nmake a complex model but think about it\nand choose the features which make more\nsense another solution is the feature\nengineering you can build more like less\nnumber of features with the given set of\nfeatures to reduce the dimensionality\nanother way is to use techniques uh such\nas PCA principal component analysis or\nTSN these methods are mostly used for\ndimensionality reduction and so they\nvery useful for visualization\nvisualizing higher dimensional data into\nlower Dimensions but they can also be\nused to reduce the dimensions of the\ndata of course you you um lose some\ninformation when you project the data\nset from higher Dimension to lower\nDimension but that's a trade of uh\nlosing a little bit of information with\nuh with with reducing Dimensions so\nthese are the the the methods to\nmitigate the problem of cuts of time\ndimensionality so in a sense while\nadding more dimensions and features into\nyour model is tempting because you might\nthink that you're adding more\ninformation\nparadoxically it makes your model less\neffective due to the data sparcity\nproblem and the data and the boundary\nproblem that we disc that we have\ndiscussed in this video so next time if\nyou're building a machine learning model\nmake sure you choose the right features\nand you use techniques like\ndimensionality reduction to reduce a\nnumber of Dimensions if you like this\nvideo don't forget to like And subscribe\nand share your thoughts in the comments\ndown below um until next time take care\nbye\n",
  "words": [
    "welcome",
    "back",
    "channel",
    "video",
    "going",
    "discuss",
    "cuts",
    "dimensionality",
    "cuts",
    "dimensionality",
    "problem",
    "building",
    "machine",
    "learning",
    "model",
    "want",
    "add",
    "dimensions",
    "features",
    "uh",
    "data",
    "might",
    "think",
    "adding",
    "dimensions",
    "features",
    "data",
    "increase",
    "performance",
    "model",
    "paradoxically",
    "adding",
    "dimensions",
    "data",
    "make",
    "model",
    "less",
    "effective",
    "problem",
    "called",
    "cuts",
    "dimensionality",
    "video",
    "discuss",
    "local",
    "versus",
    "global",
    "model",
    "locality",
    "loses",
    "meaning",
    "higher",
    "dimension",
    "boundary",
    "problems",
    "higher",
    "dimensional",
    "data",
    "sets",
    "data",
    "sparcity",
    "problem",
    "practical",
    "implications",
    "mitigation",
    "tools",
    "let",
    "get",
    "started",
    "local",
    "versus",
    "global",
    "problem",
    "first",
    "let",
    "understand",
    "different",
    "model",
    "perceive",
    "data",
    "local",
    "models",
    "like",
    "knn",
    "k",
    "nearest",
    "neighbors",
    "make",
    "predictions",
    "based",
    "nearby",
    "data",
    "point",
    "focus",
    "local",
    "structure",
    "data",
    "let",
    "see",
    "knn",
    "works",
    "let",
    "assume",
    "two",
    "classes",
    "two",
    "cluster",
    "class",
    "class",
    "p",
    "new",
    "data",
    "point",
    "depending",
    "neighboring",
    "points",
    "case",
    "k",
    "equal",
    "5",
    "look",
    "many",
    "neighboring",
    "points",
    "belongs",
    "uh",
    "class",
    "p",
    "scenario",
    "see",
    "three",
    "data",
    "points",
    "belongs",
    "class",
    "b",
    "two",
    "belongs",
    "class",
    "classified",
    "new",
    "point",
    "uh",
    "goes",
    "class",
    "b",
    "knn",
    "local",
    "model",
    "making",
    "predictions",
    "based",
    "nearby",
    "music",
    "points",
    "knn",
    "works",
    "let",
    "see",
    "linear",
    "regression",
    "works",
    "unlike",
    "knn",
    "linear",
    "regression",
    "global",
    "model",
    "considers",
    "full",
    "set",
    "data",
    "points",
    "order",
    "come",
    "best",
    "fit",
    "line",
    "best",
    "fit",
    "line",
    "use",
    "inference",
    "given",
    "new",
    "data",
    "point",
    "use",
    "best",
    "fit",
    "line",
    "find",
    "value",
    "way",
    "linear",
    "regression",
    "considered",
    "global",
    "model",
    "making",
    "predictions",
    "based",
    "contributions",
    "data",
    "music",
    "points",
    "mirage",
    "local",
    "higher",
    "dimension",
    "higher",
    "dimensions",
    "intuitive",
    "sense",
    "local",
    "becomes",
    "distorted",
    "look",
    "formula",
    "formula",
    "shows",
    "distance",
    "center",
    "capture",
    "fraction",
    "volume",
    "space",
    "r",
    "fraction",
    "volume",
    "p",
    "number",
    "dimensions",
    "imagine",
    "working",
    "10",
    "dimensional",
    "space",
    "capture",
    "1",
    "data",
    "using",
    "let",
    "say",
    "knn",
    "would",
    "need",
    "include",
    "63",
    "dimensions",
    "range",
    "capture",
    "10",
    "data",
    "need",
    "include",
    "80",
    "dimensions",
    "small",
    "neighborhood",
    "spans",
    "almost",
    "entire",
    "space",
    "third",
    "point",
    "boundary",
    "problems",
    "become",
    "important",
    "higher",
    "dimensions",
    "let",
    "look",
    "higher",
    "dimension",
    "spaces",
    "data",
    "points",
    "tend",
    "near",
    "edges",
    "rather",
    "center",
    "example",
    "10",
    "dimensional",
    "sphere",
    "500",
    "uniformly",
    "distributed",
    "points",
    "closest",
    "point",
    "center",
    "halfway",
    "boundary",
    "presents",
    "challenge",
    "k",
    "n",
    "model",
    "needs",
    "extract",
    "aate",
    "rather",
    "interpolate",
    "order",
    "make",
    "predictions",
    "global",
    "models",
    "like",
    "linear",
    "regression",
    "models",
    "well",
    "face",
    "something",
    "called",
    "h",
    "biases",
    "bulk",
    "data",
    "cured",
    "towards",
    "boundary",
    "let",
    "talk",
    "data",
    "sparcity",
    "problem",
    "higher",
    "dimensions",
    "sampling",
    "density",
    "data",
    "sets",
    "represented",
    "1",
    "p",
    "assuming",
    "building",
    "prediction",
    "model",
    "one",
    "dimensions",
    "100",
    "data",
    "points",
    "add",
    "dimension",
    "want",
    "build",
    "model",
    "10",
    "features",
    "10",
    "dimensional",
    "model",
    "make",
    "prediction",
    "order",
    "achieve",
    "density",
    "data",
    "points",
    "compared",
    "like",
    "one",
    "1d",
    "problem",
    "would",
    "need",
    "100",
    "power",
    "10",
    "data",
    "points",
    "means",
    "one",
    "followed",
    "20",
    "zeros",
    "lot",
    "data",
    "points",
    "add",
    "dimensions",
    "increase",
    "data",
    "points",
    "data",
    "become",
    "sparse",
    "higher",
    "dimensions",
    "making",
    "model",
    "less",
    "effective",
    "deal",
    "problem",
    "well",
    "deal",
    "problem",
    "methods",
    "apply",
    "first",
    "thing",
    "need",
    "understand",
    "problem",
    "understand",
    "data",
    "understand",
    "features",
    "model",
    "feature",
    "selection",
    "important",
    "add",
    "features",
    "order",
    "make",
    "complex",
    "model",
    "think",
    "choose",
    "features",
    "make",
    "sense",
    "another",
    "solution",
    "feature",
    "engineering",
    "build",
    "like",
    "less",
    "number",
    "features",
    "given",
    "set",
    "features",
    "reduce",
    "dimensionality",
    "another",
    "way",
    "use",
    "techniques",
    "uh",
    "pca",
    "principal",
    "component",
    "analysis",
    "tsn",
    "methods",
    "mostly",
    "used",
    "dimensionality",
    "reduction",
    "useful",
    "visualization",
    "visualizing",
    "higher",
    "dimensional",
    "data",
    "lower",
    "dimensions",
    "also",
    "used",
    "reduce",
    "dimensions",
    "data",
    "course",
    "um",
    "lose",
    "information",
    "project",
    "data",
    "set",
    "higher",
    "dimension",
    "lower",
    "dimension",
    "trade",
    "uh",
    "losing",
    "little",
    "bit",
    "information",
    "uh",
    "reducing",
    "dimensions",
    "methods",
    "mitigate",
    "problem",
    "cuts",
    "time",
    "dimensionality",
    "sense",
    "adding",
    "dimensions",
    "features",
    "model",
    "tempting",
    "might",
    "think",
    "adding",
    "information",
    "paradoxically",
    "makes",
    "model",
    "less",
    "effective",
    "due",
    "data",
    "sparcity",
    "problem",
    "data",
    "boundary",
    "problem",
    "disc",
    "discussed",
    "video",
    "next",
    "time",
    "building",
    "machine",
    "learning",
    "model",
    "make",
    "sure",
    "choose",
    "right",
    "features",
    "use",
    "techniques",
    "like",
    "dimensionality",
    "reduction",
    "reduce",
    "number",
    "dimensions",
    "like",
    "video",
    "forget",
    "like",
    "subscribe",
    "share",
    "thoughts",
    "comments",
    "um",
    "next",
    "time",
    "take",
    "care",
    "bye"
  ],
  "keywords": [
    "video",
    "cuts",
    "dimensionality",
    "problem",
    "building",
    "model",
    "add",
    "dimensions",
    "features",
    "uh",
    "data",
    "think",
    "adding",
    "make",
    "less",
    "effective",
    "local",
    "global",
    "higher",
    "dimension",
    "boundary",
    "dimensional",
    "sparcity",
    "let",
    "understand",
    "models",
    "like",
    "knn",
    "k",
    "predictions",
    "based",
    "point",
    "see",
    "works",
    "two",
    "class",
    "p",
    "new",
    "points",
    "look",
    "belongs",
    "making",
    "linear",
    "regression",
    "set",
    "order",
    "best",
    "fit",
    "line",
    "use",
    "sense",
    "center",
    "capture",
    "space",
    "number",
    "10",
    "need",
    "one",
    "methods",
    "reduce",
    "information",
    "time"
  ]
}