{
  "text": "So I've been an AI researcher\nfor over a decade.\nAnd a couple of months ago,\nI got the weirdest email of my career.\nA random stranger wrote to me\nsaying that my work in AI\nis going to end humanity.\nNow I get it,  AI, it's so hot right now.\n(Laughter)\nIt's in the headlines\npretty much every day,\nsometimes because of really cool things\nlike discovering new\nmolecules for medicine\nor that dope Pope\nin the white puffer coat.\nBut other times the headlines\nhave been really dark,\nlike that chatbot telling that guy\nthat he should divorce his wife\nor that AI meal planner app\nproposing a crowd pleasing recipe\nfeaturing chlorine gas.\nAnd in the background,\nwe've heard a lot of talk\nabout doomsday scenarios,\nexistential risk and the singularity,\nwith letters being written\nand events being organized\nto make sure that doesn't happen.\nNow I'm a researcher who studies\nAI's impacts on society,\nand I don't know what's going\nto happen in 10 or 20 years,\nand nobody really does.\nBut what I do know is that there's some\npretty nasty things going on right now,\nbecause AI doesn't exist in a vacuum.\nIt is part of society, and it has impacts\non people and the planet.\nAI models can contribute\nto climate change.\nTheir training data uses art\nand books created by artists\nand authors without their consent.\nAnd its deployment can discriminate\nagainst entire communities.\nBut we need to start tracking its impacts.\nWe need to start being transparent\nand disclosing them and creating tools\nso that people understand AI better,\nso that hopefully future\ngenerations of AI models\nare going to be more\ntrustworthy, sustainable,\nmaybe less likely to kill us,\nif that's what you're into.\nBut let's start with sustainability,\nbecause that cloud that AI models live on\nis actually made out of metal, plastic,\nand powered by vast amounts of energy.\nAnd each time you query an AI model,\nit comes with a cost to the planet.\nLast year, I was part\nof the BigScience initiative,\nwhich brought together\na thousand researchers\nfrom all over the world to create Bloom,\nthe first open large language\nmodel, like ChatGPT,\nbut with an emphasis on ethics,\ntransparency and consent.\nAnd the study I led that looked\nat Bloom's environmental impacts\nfound that just training it\nused as much energy\nas 30 homes in a whole year\nand emitted 25 tons of carbon dioxide,\nwhich is like driving your car\nfive times around the planet\njust so somebody can use this model\nto tell a knock-knock joke.\nAnd this might not seem like a lot,\nbut other similar large language models,\nlike GPT-3,\nemit 20 times more carbon.\nBut the thing is, tech companies\naren't measuring this stuff.\nThey're not disclosing it.\nAnd so this is probably\nonly the tip of the iceberg,\neven if it is a melting one.\nAnd in recent years we've seen\nAI models balloon in size\nbecause the current trend in AI\nis \"bigger is better.\"\nBut please don't get me started\non why that's the case.\nIn any case, we've seen large\nlanguage models in particular\ngrow 2,000 times in size\nover the last five years.\nAnd of course, their environmental\ncosts are rising as well.\nThe most recent work I led,\nfound that switching out a smaller,\nmore efficient model\nfor a larger language model\nemits 14 times more carbon\nfor the same task.\nLike telling that knock-knock joke.\nAnd as we're putting in these models\ninto cell phones and search engines\nand smart fridges and speakers,\nthe environmental costs\nare really piling up quickly.\nSo instead of focusing on some\nfuture existential risks,\nlet's talk about current tangible impacts\nand tools we can create to measure\nand mitigate these impacts.\nI helped create CodeCarbon,\na tool that runs in parallel\nto AI training code\nthat estimates the amount\nof energy it consumes\nand the amount of carbon it emits.\nAnd using a tool like this can help us\nmake informed choices,\nlike choosing one model over the other\nbecause it's more sustainable,\nor deploying AI models\non renewable energy,\nwhich can drastically reduce\ntheir emissions.\nBut let's talk about other things\nbecause there's other impacts of AI\napart from sustainability.\nFor example, it's been really\nhard for artists and authors\nto prove that their life's work\nhas been used for training AI models\nwithout their consent.\nAnd if you want to sue someone,\nyou tend to need proof, right?\nSo Spawning.ai, an organization\nthat was founded by artists,\ncreated this really cool tool\ncalled “Have I Been Trained?”\nAnd it lets you search\nthese massive data sets\nto see what they have on you.\nNow, I admit it, I was curious.\nI searched LAION-5B,\nwhich is this huge data set\nof images and text,\nto see if any images of me were in there.\nNow those two first images,\nthat's me from events I've spoken at.\nBut the rest of the images,\nnone of those are me.\nThey're probably of other\nwomen named Sasha\nwho put photographs of\nthemselves up on the internet.\nAnd this can probably explain why,\nwhen I query an image generation model\nto generate a photograph\nof a woman named Sasha,\nmore often than not\nI get images of bikini models.\nSometimes they have two arms,\nsometimes they have three arms,\nbut they rarely have any clothes on.\nAnd while it can be interesting\nfor people like you and me\nto search these data sets,\nfor artists like Karla Ortiz,\nthis provides crucial evidence\nthat her life's work, her artwork,\nwas used for training AI models\nwithout her consent,\nand she and two artists\nused this as evidence\nto file a class action lawsuit\nagainst AI companies\nfor copyright infringement.\nAnd most recently --\n(Applause)\nAnd most recently Spawning.ai\npartnered up with Hugging Face,\nthe company where I work at,\nto create opt-in and opt-out mechanisms\nfor creating these data sets.\nBecause artwork created by humans\nshouldn’t be an all-you-can-eat buffet\nfor training AI language models.\n(Applause)\nThe very last thing I want\nto talk about is bias.\nYou probably hear about this a lot.\nFormally speaking, it's when AI models\nencode patterns and beliefs\nthat can represent stereotypes\nor racism and sexism.\nOne of my heroes, Dr. Joy Buolamwini,\nexperienced this firsthand\nwhen she realized that AI systems\nwouldn't even detect her face\nunless she was wearing\na white-colored mask.\nDigging deeper, she found\nthat common facial recognition systems\nwere vastly worse for women of color\ncompared to white men.\nAnd when biased models like this\nare deployed in law enforcement settings,\nthis can result in false accusations,\neven wrongful imprisonment,\nwhich we've seen happen\nto multiple people in recent months.\nFor example, Porcha Woodruff\nwas wrongfully accused of carjacking\nat eight months pregnant\nbecause an AI system\nwrongfully identified her.\nBut sadly, these systems are black boxes,\nand even their creators can't say exactly\nwhy they work the way they do.\nAnd for example, for image\ngeneration systems,\nif they're used in contexts\nlike generating a forensic sketch\nbased on a description of a perpetrator,\nthey take all those biases\nand they spit them back out\nfor terms like dangerous criminal,\nterrorists or gang member,\nwhich of course is super dangerous\nwhen these tools are deployed in society.\nAnd so in order to understand\nthese tools better,\nI created this tool called\nthe Stable Bias Explorer,\nwhich lets you explore the bias\nof image generation models\nthrough the lens of professions.\nSo try to picture\na scientist in your mind.\nDon't look at me.\nWhat do you see?\nA lot of the same thing, right?\nMen in glasses and lab coats.\nAnd none of them look like me.\nAnd the thing is,\nis that we looked at all these\ndifferent image generation models\nand found a lot of the same thing:\nsignificant representation\nof whiteness and masculinity\nacross all 150 professions\nthat we looked at,\neven if compared to the real world,\nthe US Labor Bureau of Statistics.\nThese models show lawyers as men,\nand CEOs as men,\nalmost 100 percent of the time,\neven though we all know\nnot all of them are white and male.\nAnd sadly, my tool hasn't been used\nto write legislation yet.\nBut I recently presented it\nat a UN event about gender bias\nas an example of how we can make tools\nfor people from all walks of life,\neven those who don't know how to code,\nto engage with and better understand AI\nbecause we use professions,\nbut you can use any terms\nthat are of interest to you.\nAnd as these models are being deployed,\nare being woven into the very\nfabric of our societies,\nour cell phones, our social media feeds,\neven our justice systems\nand our economies have AI in them.\nAnd it's really important\nthat AI stays accessible\nso that we know both how it works\nand when it doesn't work.\nAnd there's no single solution\nfor really complex things like bias\nor copyright or climate change.\nBut by creating tools\nto measure AI's impact,\nwe can start getting an idea\nof how bad they are\nand start addressing them as we go.\nStart creating guardrails\nto protect society and the planet.\nAnd once we have this information,\ncompanies can use it in order to say,\nOK, we're going to choose this model\nbecause it's more sustainable,\nthis model because it respects copyright.\nLegislators who really need\ninformation to write laws,\ncan use these tools to develop\nnew regulation mechanisms\nor governance for AI\nas it gets deployed into society.\nAnd users like you and me\ncan use this information\nto choose AI models that we can trust,\nnot to misrepresent us\nand not to misuse our data.\nBut what did I reply to that email\nthat said that my work\nis going to destroy humanity?\nI said that focusing\non AI's future existential risks\nis a distraction from its current,\nvery tangible impacts\nand the work we should be doing\nright now, or even yesterday,\nfor reducing these impacts.\nBecause yes, AI is moving quickly,\nbut it's not a done deal.\nWe're building the road as we walk it,\nand we can collectively decide\nwhat direction we want to go in together.\nThank you.\n(Applause)\n",
  "words": [
    "ai",
    "researcher",
    "decade",
    "couple",
    "months",
    "ago",
    "got",
    "weirdest",
    "email",
    "career",
    "random",
    "stranger",
    "wrote",
    "saying",
    "work",
    "ai",
    "going",
    "end",
    "humanity",
    "get",
    "ai",
    "hot",
    "right",
    "laughter",
    "headlines",
    "pretty",
    "much",
    "every",
    "day",
    "sometimes",
    "really",
    "cool",
    "things",
    "like",
    "discovering",
    "new",
    "molecules",
    "medicine",
    "dope",
    "pope",
    "white",
    "puffer",
    "coat",
    "times",
    "headlines",
    "really",
    "dark",
    "like",
    "chatbot",
    "telling",
    "guy",
    "divorce",
    "wife",
    "ai",
    "meal",
    "planner",
    "app",
    "proposing",
    "crowd",
    "pleasing",
    "recipe",
    "featuring",
    "chlorine",
    "gas",
    "background",
    "heard",
    "lot",
    "talk",
    "doomsday",
    "scenarios",
    "existential",
    "risk",
    "singularity",
    "letters",
    "written",
    "events",
    "organized",
    "make",
    "sure",
    "happen",
    "researcher",
    "studies",
    "ai",
    "impacts",
    "society",
    "know",
    "going",
    "happen",
    "10",
    "20",
    "years",
    "nobody",
    "really",
    "know",
    "pretty",
    "nasty",
    "things",
    "going",
    "right",
    "ai",
    "exist",
    "vacuum",
    "part",
    "society",
    "impacts",
    "people",
    "planet",
    "ai",
    "models",
    "contribute",
    "climate",
    "change",
    "training",
    "data",
    "uses",
    "art",
    "books",
    "created",
    "artists",
    "authors",
    "without",
    "consent",
    "deployment",
    "discriminate",
    "entire",
    "communities",
    "need",
    "start",
    "tracking",
    "impacts",
    "need",
    "start",
    "transparent",
    "disclosing",
    "creating",
    "tools",
    "people",
    "understand",
    "ai",
    "better",
    "hopefully",
    "future",
    "generations",
    "ai",
    "models",
    "going",
    "trustworthy",
    "sustainable",
    "maybe",
    "less",
    "likely",
    "kill",
    "us",
    "let",
    "start",
    "sustainability",
    "cloud",
    "ai",
    "models",
    "live",
    "actually",
    "made",
    "metal",
    "plastic",
    "powered",
    "vast",
    "amounts",
    "energy",
    "time",
    "query",
    "ai",
    "model",
    "comes",
    "cost",
    "planet",
    "last",
    "year",
    "part",
    "bigscience",
    "initiative",
    "brought",
    "together",
    "thousand",
    "researchers",
    "world",
    "create",
    "bloom",
    "first",
    "open",
    "large",
    "language",
    "model",
    "like",
    "chatgpt",
    "emphasis",
    "ethics",
    "transparency",
    "consent",
    "study",
    "led",
    "looked",
    "bloom",
    "environmental",
    "impacts",
    "found",
    "training",
    "used",
    "much",
    "energy",
    "30",
    "homes",
    "whole",
    "year",
    "emitted",
    "25",
    "tons",
    "carbon",
    "dioxide",
    "like",
    "driving",
    "car",
    "five",
    "times",
    "around",
    "planet",
    "somebody",
    "use",
    "model",
    "tell",
    "joke",
    "might",
    "seem",
    "like",
    "lot",
    "similar",
    "large",
    "language",
    "models",
    "like",
    "emit",
    "20",
    "times",
    "carbon",
    "thing",
    "tech",
    "companies",
    "measuring",
    "stuff",
    "disclosing",
    "probably",
    "tip",
    "iceberg",
    "even",
    "melting",
    "one",
    "recent",
    "years",
    "seen",
    "ai",
    "models",
    "balloon",
    "size",
    "current",
    "trend",
    "ai",
    "bigger",
    "better",
    "please",
    "get",
    "started",
    "case",
    "case",
    "seen",
    "large",
    "language",
    "models",
    "particular",
    "grow",
    "times",
    "size",
    "last",
    "five",
    "years",
    "course",
    "environmental",
    "costs",
    "rising",
    "well",
    "recent",
    "work",
    "led",
    "found",
    "switching",
    "smaller",
    "efficient",
    "model",
    "larger",
    "language",
    "model",
    "emits",
    "14",
    "times",
    "carbon",
    "task",
    "like",
    "telling",
    "joke",
    "putting",
    "models",
    "cell",
    "phones",
    "search",
    "engines",
    "smart",
    "fridges",
    "speakers",
    "environmental",
    "costs",
    "really",
    "piling",
    "quickly",
    "instead",
    "focusing",
    "future",
    "existential",
    "risks",
    "let",
    "talk",
    "current",
    "tangible",
    "impacts",
    "tools",
    "create",
    "measure",
    "mitigate",
    "impacts",
    "helped",
    "create",
    "codecarbon",
    "tool",
    "runs",
    "parallel",
    "ai",
    "training",
    "code",
    "estimates",
    "amount",
    "energy",
    "consumes",
    "amount",
    "carbon",
    "emits",
    "using",
    "tool",
    "like",
    "help",
    "us",
    "make",
    "informed",
    "choices",
    "like",
    "choosing",
    "one",
    "model",
    "sustainable",
    "deploying",
    "ai",
    "models",
    "renewable",
    "energy",
    "drastically",
    "reduce",
    "emissions",
    "let",
    "talk",
    "things",
    "impacts",
    "ai",
    "apart",
    "sustainability",
    "example",
    "really",
    "hard",
    "artists",
    "authors",
    "prove",
    "life",
    "work",
    "used",
    "training",
    "ai",
    "models",
    "without",
    "consent",
    "want",
    "sue",
    "someone",
    "tend",
    "need",
    "proof",
    "right",
    "organization",
    "founded",
    "artists",
    "created",
    "really",
    "cool",
    "tool",
    "called",
    "trained",
    "lets",
    "search",
    "massive",
    "data",
    "sets",
    "see",
    "admit",
    "curious",
    "searched",
    "huge",
    "data",
    "set",
    "images",
    "text",
    "see",
    "images",
    "two",
    "first",
    "images",
    "events",
    "spoken",
    "rest",
    "images",
    "none",
    "probably",
    "women",
    "named",
    "sasha",
    "put",
    "photographs",
    "internet",
    "probably",
    "explain",
    "query",
    "image",
    "generation",
    "model",
    "generate",
    "photograph",
    "woman",
    "named",
    "sasha",
    "often",
    "get",
    "images",
    "bikini",
    "models",
    "sometimes",
    "two",
    "arms",
    "sometimes",
    "three",
    "arms",
    "rarely",
    "clothes",
    "interesting",
    "people",
    "like",
    "search",
    "data",
    "sets",
    "artists",
    "like",
    "karla",
    "ortiz",
    "provides",
    "crucial",
    "evidence",
    "life",
    "work",
    "artwork",
    "used",
    "training",
    "ai",
    "models",
    "without",
    "consent",
    "two",
    "artists",
    "used",
    "evidence",
    "file",
    "class",
    "action",
    "lawsuit",
    "ai",
    "companies",
    "copyright",
    "infringement",
    "recently",
    "applause",
    "recently",
    "partnered",
    "hugging",
    "face",
    "company",
    "work",
    "create",
    "mechanisms",
    "creating",
    "data",
    "sets",
    "artwork",
    "created",
    "humans",
    "buffet",
    "training",
    "ai",
    "language",
    "models",
    "applause",
    "last",
    "thing",
    "want",
    "talk",
    "bias",
    "probably",
    "hear",
    "lot",
    "formally",
    "speaking",
    "ai",
    "models",
    "encode",
    "patterns",
    "beliefs",
    "represent",
    "stereotypes",
    "racism",
    "sexism",
    "one",
    "heroes",
    "joy",
    "buolamwini",
    "experienced",
    "firsthand",
    "realized",
    "ai",
    "systems",
    "would",
    "even",
    "detect",
    "face",
    "unless",
    "wearing",
    "mask",
    "digging",
    "deeper",
    "found",
    "common",
    "facial",
    "recognition",
    "systems",
    "vastly",
    "worse",
    "women",
    "color",
    "compared",
    "white",
    "men",
    "biased",
    "models",
    "like",
    "deployed",
    "law",
    "enforcement",
    "settings",
    "result",
    "false",
    "accusations",
    "even",
    "wrongful",
    "imprisonment",
    "seen",
    "happen",
    "multiple",
    "people",
    "recent",
    "months",
    "example",
    "porcha",
    "woodruff",
    "wrongfully",
    "accused",
    "carjacking",
    "eight",
    "months",
    "pregnant",
    "ai",
    "system",
    "wrongfully",
    "identified",
    "sadly",
    "systems",
    "black",
    "boxes",
    "even",
    "creators",
    "ca",
    "say",
    "exactly",
    "work",
    "way",
    "example",
    "image",
    "generation",
    "systems",
    "used",
    "contexts",
    "like",
    "generating",
    "forensic",
    "sketch",
    "based",
    "description",
    "perpetrator",
    "take",
    "biases",
    "spit",
    "back",
    "terms",
    "like",
    "dangerous",
    "criminal",
    "terrorists",
    "gang",
    "member",
    "course",
    "super",
    "dangerous",
    "tools",
    "deployed",
    "society",
    "order",
    "understand",
    "tools",
    "better",
    "created",
    "tool",
    "called",
    "stable",
    "bias",
    "explorer",
    "lets",
    "explore",
    "bias",
    "image",
    "generation",
    "models",
    "lens",
    "professions",
    "try",
    "picture",
    "scientist",
    "mind",
    "look",
    "see",
    "lot",
    "thing",
    "right",
    "men",
    "glasses",
    "lab",
    "coats",
    "none",
    "look",
    "like",
    "thing",
    "looked",
    "different",
    "image",
    "generation",
    "models",
    "found",
    "lot",
    "thing",
    "significant",
    "representation",
    "whiteness",
    "masculinity",
    "across",
    "150",
    "professions",
    "looked",
    "even",
    "compared",
    "real",
    "world",
    "us",
    "labor",
    "bureau",
    "statistics",
    "models",
    "show",
    "lawyers",
    "men",
    "ceos",
    "men",
    "almost",
    "100",
    "percent",
    "time",
    "even",
    "though",
    "know",
    "white",
    "male",
    "sadly",
    "tool",
    "used",
    "write",
    "legislation",
    "yet",
    "recently",
    "presented",
    "un",
    "event",
    "gender",
    "bias",
    "example",
    "make",
    "tools",
    "people",
    "walks",
    "life",
    "even",
    "know",
    "code",
    "engage",
    "better",
    "understand",
    "ai",
    "use",
    "professions",
    "use",
    "terms",
    "interest",
    "models",
    "deployed",
    "woven",
    "fabric",
    "societies",
    "cell",
    "phones",
    "social",
    "media",
    "feeds",
    "even",
    "justice",
    "systems",
    "economies",
    "ai",
    "really",
    "important",
    "ai",
    "stays",
    "accessible",
    "know",
    "works",
    "work",
    "single",
    "solution",
    "really",
    "complex",
    "things",
    "like",
    "bias",
    "copyright",
    "climate",
    "change",
    "creating",
    "tools",
    "measure",
    "ai",
    "impact",
    "start",
    "getting",
    "idea",
    "bad",
    "start",
    "addressing",
    "go",
    "start",
    "creating",
    "guardrails",
    "protect",
    "society",
    "planet",
    "information",
    "companies",
    "use",
    "order",
    "say",
    "ok",
    "going",
    "choose",
    "model",
    "sustainable",
    "model",
    "respects",
    "copyright",
    "legislators",
    "really",
    "need",
    "information",
    "write",
    "laws",
    "use",
    "tools",
    "develop",
    "new",
    "regulation",
    "mechanisms",
    "governance",
    "ai",
    "gets",
    "deployed",
    "society",
    "users",
    "like",
    "use",
    "information",
    "choose",
    "ai",
    "models",
    "trust",
    "misrepresent",
    "us",
    "misuse",
    "data",
    "reply",
    "email",
    "said",
    "work",
    "going",
    "destroy",
    "humanity",
    "said",
    "focusing",
    "ai",
    "future",
    "existential",
    "risks",
    "distraction",
    "current",
    "tangible",
    "impacts",
    "work",
    "right",
    "even",
    "yesterday",
    "reducing",
    "impacts",
    "yes",
    "ai",
    "moving",
    "quickly",
    "done",
    "deal",
    "building",
    "road",
    "walk",
    "collectively",
    "decide",
    "direction",
    "want",
    "go",
    "together",
    "thank",
    "applause"
  ],
  "keywords": [
    "ai",
    "researcher",
    "months",
    "email",
    "work",
    "going",
    "humanity",
    "get",
    "right",
    "headlines",
    "pretty",
    "much",
    "sometimes",
    "really",
    "cool",
    "things",
    "like",
    "new",
    "white",
    "times",
    "telling",
    "lot",
    "talk",
    "existential",
    "events",
    "make",
    "happen",
    "impacts",
    "society",
    "know",
    "20",
    "years",
    "part",
    "people",
    "planet",
    "models",
    "climate",
    "change",
    "training",
    "data",
    "created",
    "artists",
    "authors",
    "without",
    "consent",
    "need",
    "start",
    "disclosing",
    "creating",
    "tools",
    "understand",
    "better",
    "future",
    "sustainable",
    "us",
    "let",
    "sustainability",
    "energy",
    "time",
    "query",
    "model",
    "last",
    "year",
    "together",
    "world",
    "create",
    "bloom",
    "first",
    "large",
    "language",
    "led",
    "looked",
    "environmental",
    "found",
    "used",
    "carbon",
    "five",
    "use",
    "joke",
    "thing",
    "companies",
    "probably",
    "even",
    "one",
    "recent",
    "seen",
    "size",
    "current",
    "case",
    "course",
    "costs",
    "emits",
    "cell",
    "phones",
    "search",
    "quickly",
    "focusing",
    "risks",
    "tangible",
    "measure",
    "tool",
    "code",
    "amount",
    "example",
    "life",
    "want",
    "called",
    "lets",
    "sets",
    "see",
    "images",
    "two",
    "none",
    "women",
    "named",
    "sasha",
    "image",
    "generation",
    "arms",
    "evidence",
    "artwork",
    "copyright",
    "recently",
    "applause",
    "face",
    "mechanisms",
    "bias",
    "systems",
    "compared",
    "men",
    "deployed",
    "wrongfully",
    "sadly",
    "say",
    "terms",
    "dangerous",
    "order",
    "professions",
    "look",
    "write",
    "go",
    "information",
    "choose",
    "said"
  ]
}