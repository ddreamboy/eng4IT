{
  "text": "welcome to this lecture about the\nmathematical details behind the\nprincipal component analysis\nin the previous video we covered the\nbasics of pca\nin this video we'll see how we can\nperform a pca analysis\nby using simple linear algebra in order\nto understand the math behind pca\ni will here show the math based on the\neigen decomposition of the covariance\nmatrix\nhowever note that there are other\nmethods such as the singular\nvalue decomposition to compute the pca\nwhich will not be discussed here\nto explain how the pca works we will use\nthe following example data\nwe'll use pca to combine the two blood\npressure variables\ninto just one variable based on data\nfrom six\nindividuals for example person number\none has diastolic blood pressure of 78\nand it sees total blood pressure of 126\nwhereas spouse number two has the\nhistoric blood pressure of 80 and its\nhistorical blood pressure\n128 and so on\nfor this data set it seems to be a\nstrong positive correlation\nbetween the two variables\nto compute a pca we can perform the\nfollowing steps\nwhere we first center our data\nthen we calculate the covariance matrix\non our center data\nnext we compute the eigenvalues and\neigenvectors of the covariance matrix\nfinally we order the eigenvectors and\ncalculate the principal components\nwhich is our combine or transform data\nset\nusually one starch to center or\nstandardize the data\nin the first step of the pca analyzes\nin this case we will only center the\ndata\nwhich means that we subtract all the\nvalues of each variable\nby its corresponding mean\nwe therefore subtract the means historic\nblood pressure\nfrom the individual observations\ncentering the systolic blood pressure\nresults in the following values which\ntell how far away\nthe original values are from the mean\nwe then do the same calculations for the\ndiastolic blood pressure\nwhich has a mean value of 82. we can\nsummarize the center data in the\nfollowing table\nwhen we center the data it means that we\ncenter the data points\naround the origin centering the data\naround the origin\nwill help us later when we rotate the\ndata\nafter we have centered the data we'll\nhave the following values\nwhich can be plotted like this where the\nx-axis now represents the\ncenter-sisterly blood pressure\nwhereas the y-axis represents the\ncentered diastolic blood pressure\nnext we calculate the covariance matrix\nbased on the center data\nnote that we have got the same values in\nthe covariance matrix\nif instead would have used original data\nsince the variance does not change when\nwe center the data\nremember that the main diagonal of the\ncovariance matrix\nincludes the variance of each variable\nthe sample variance of the centered\nsystolic blood pressure\nis calculated like this\nwhen we calculate the variance of the\ncentered data the calculations\nbecome a bit simpler since the mean of\nthe centered data\nis always equal to zero\nto calculate the sample variance of the\ncentered data\nwe therefore simply sum the squared\nvalues\nand divide by the sample size -1\nthen we calculate the variance of the\ndiastolic blood pressure by using the\nsame equation\nfinally we calculate the covariance\nwhich is the measure\nof how much the two variables spread\ntogether\nthe sample covariance is calculated by\nmultiplying the centered values\nof the two variables\nfor example we multiply the centered\nvalues for person number one\nand add that to the product of the\ncentered values for person number two\nand so on\nfinally we divide the sum of the\nproducts by the sample size minus one\nwe see that the spread in the dystolic\nblood pressure\nis a bit higher compared to the spread\nin the systolic blood pressure\nthe covariance is somewhere between\nthese two values\nnext we calculate the eigenvalues of the\ncovariance matrix\nfor more details watch the lecture about\neigenvalues and eigenvectors\nwe substitute a by the covariance matrix\nand this term by lambda times the\nidentity matrix\nwhich has the same number of rows and\ncolumns as the covariance matrix\nsubtracting these two matrices results\nin the following matrix\nnext we calculate the determinant of\nthis matrix\nwhich is the product of this diagonal\nminus the product of this diagonal\nafter some simplifications we have the\nfollowing quadratic equation\nquadratic equations like this can be\nsolved in different ways\nwhich will not be discussed here\nhowever if you plot how the left hand\nside changes\nas a function of different values of\nlambda we see that the left-hand side\nis equal to zero when lambda is equal to\neither\nabout 0.32 or\n12.08\nthis means that if we set lambda to\neither 0.32\nor 12.08 the left-hand side of this\nequation will become equal to zero\nor close to zero due to running effects\nin this example\nthese two values represent our\neigenvalues\nof the covariance matrix\nnext we calculate the corresponding\neigenvectors\nto these two eigenvalues\nwe'll start by calculating the\neigenvector of the covariance matrix\nwith the corresponding eigenvalue 12.08\nto calculate the eigenvectors of the\ncovariance matrix\nwe use the following equation that we\nhave discussed in the previous video\nabout eigenvectors\nwe plug in the covariance matrix\nand one of the two eigenvalues\nif you multiply the covariance matrix by\nthis column vector\nand multiply the eigenvalue by the same\nvector\nwe'll get the following system of\nequations\nwe now move these two terms to the right\nhand side\nafter some simplifications we have the\nfollowing system of equations\nsolving for y in the two equations\nresults\nin that y is equal to 1.37 x\nfor example if we set x equal to 1\ny is equal to 1.37\nthis vector is therefore an eigenvector\nof the covariance matrix\nwe can illustrate this vector in the\nplot like this\nby drawing an error from the origin to\nthe coordinates\n1 and 1.37\nwe'll now normalize this vector to unit\nlength\nwhich means that it should have a length\nof 1.\nwatch the lecture about the eigenvectors\nand eigenvalues to see\nhow one can normalize the eigenvector to\nunit length\nafter normalization this vector\nrepresents one\nout of two eigenvectors of the\ncovariance matrix\nto find the second eigenvector we do the\nsame calculations as before\nbased on the second eigenvalue\nafter some calculations this vector\nrepresents our second eigenvector\nwith unit length\nsince the covariance matrix is a\nsymmetric matrix\nthe eigenvectors will be orthogonal\nwhich means that the angle between them\nis 90 degrees\nnext we order the eigenvectors based on\ntheir corresponding eigenvalues\nwhere the eigenvector with the largest\neigenvalue\nbecomes our first eigenvector\nsince this eigenvector has the largest\neigenvalue\nit will represent our first eigenvector\nwe therefore rename this vector so it is\ncalled v1\ninstead of v2\nlet's put these two eigenvectors\ntogether into a matrix that we called v\nwhere the first column represents the\nfirst eigenvector\nwith the highest eigenvalue and the\nsecond column\nrepresents our second eigenvector\nwe'll now use this matrix to transform\nour original\ncenter data so that the two variables\nare completely uncorrelated\nwe define a matrix d which includes our\ncentered data\nnext we multiply our data matrix d\nby matrix v which includes our\neigenvectors as columns\nthen we get the new matrix with the\ntransformed data\nthis transformed data is called\nprincipal component scores\nor just scores which represent the\noriginal center data\nin the principal component space\nwhen we go from our original data matrix\nto the transformed data\nthis can be seen like we rotate the data\nclockwise\nuntil the two eigenvectors point in the\nsame direction\nas the x and y axis of the plot\nthe rotated data now looks like this\nnote that the labels of the axis have\nnow been changed\nto principal component 1 and 2.\nlet's call the two columns of the\ntransform data pc1\nand pc2 if we plot this data\nwhere we label the x-axis as pc1 and the\ny-axis as pc2\nwe will get the following plot which\nrepresents the arrhythmia plot after the\nrotation\nsince we plot the principal component\nscores this kind of plot is called a\nscore plot\nlet's compare the center data\nwith the transform data\nthe variance of the sistorial blood\npressure is 4.4\nwhereas the variance of the diastolic\nblood pressure is 8.\nthis is the covariance matrix of the\ndata\nwe see that the covariance is 5.6\nwhich tells us that there is a positive\ncorrelation between the two variables\nwhen transformed the data using pca the\nfirst variable\ncalled pc1 has a variance of 12.08\nwhereas pc2 has only a variance of 0.32\nthis means that almost all variance is\nkept in the first principal component\nif we divide the variance of the first\nprincipal component\nby the total variance\nwe see that the first principal\ncomponent captures 97.4 percent\nof the total variance\nnote that the variances of the principal\ncomponents\ncorrespond to the two eigenvalues we\ncalculated earlier\nthus the eigenvalues of the covariance\nmatrix represent\nthe variances of the principal\ncomponents\nwhen we started the covariance matrix of\nour transformed data\nwe see that the covariance between pc1\nand pc2\nis equal to 0 which means that pc1 and\npc2\nare completely uncorrelated\nwe also see that the variance of pc1 and\npc2\ncorrespond to the eigenvalues associated\nto the first\nand the second eigenvector\nnote that the total variance of pc1 and\npc2 is about 12.4\nwhich corresponds to the total variance\nof the original variables\nremember that when we multiplied our\ncenter data\nby the matrix that includes the\neigenvectors as columns\nwe got the transformed data\nthis is the same as using the following\nequation to calculate the principal\ncomponents\nthat we saw in the previous video\nwhere the weights for the first\nprincipal component comes from the first\neigenvector\nfor the highest eigenvalue\nwhereas the weights for the second\nprincipal component comes from the\nsecond eigenvector\nfor example if we calculate the\ncorresponding score for person number\nsix\nwe multiply the centered blood pressure\nvalues of person number six\nby the corresponding weights\nby adding these products we would get\nthe principal component\nscore of 5.\nnote that we can also use the following\nequation to calculate for example the\nfirst principal component\nwhere we replace the variable for the\ncenter data by the variable of the\noriginal data minus\nits corresponding mean\nfor example if we would use the original\nblood pressure values for person number\nsix\nwe would subtract the mean from the\ncorresponding systolic and the historic\nblood pressures\nwhich would give us the same values as\nfor the center data note that the\ngeneral aim\nof using pca is to reduce the\ndimensionality in the data\nin other words we like to reduce the\nnumber of variables we have\nhowever so far we have not reduced\nnumber of variables\nsince we have the same number of\nprincipal components\nas the number of variables we started\nwith\nsince the first principal component\ncaptures almost all variants\nwhich can be interpreted as distorts\nalmost all information about the two\nvariables\nwe can simply delete the second\nprincipal component\nbecause it includes almost no\ninformation\nas we have seen previously by using the\nfollowing equation\nwe can combine the two variables into\njust one variable\nin a way that maximize the variance of\nthe linear combination\nthe weights can be interpreted as how\nmuch each variable\ncontributes to the principal component\nsince the absolute value of the weight\nfor the diastolic blood pressure\nis higher than that for the systolic\nblood pressure\npca put more weight on the diastolic\nblood pressure\nwhen the two variables are combined\nbefore we end this lecture we'll see why\nthe first eigenvector\npoints in the same direction as the data\nthis is our previous eigenvector and if\nwe extend it by multiplying by\nfor example 6\nwe can draw this vector note that this\nis also an eigenvector of the covariance\nmatrix\nsince it has the same direction as the\neigenvector with the unit length\nthis is our covariance matrix\nlet's take an arbitrary vector with the\ncoordinates negative two\nand three if we multiply the covariance\nmatrix by this vector\nwe will get a new vector that has\nchanged direction\nwe see the covariance matrix transform\nthe vector\nso it now moves in the direction closer\nto the eigenvector\nnote that we here do not plot the full\nlength of the vector since it cannot fit\nthe screen\nthe importance is its direction\nif we multiply the covariance matrix by\nthis new vector\nwe'll get a new vector again this new\nvector will have more or less the same\ndirection\nas the eigenvector\nlet's take another example vector with\nthe coordinates 4\nand 1.\nmultiplying the covariance matrix by\nthis arbitrary vector\nwill result in the following vector\nwe see that the covariance matrix will\nagain rotate this vector\nso that it has a similar direction as\nthe eigenvector\nwe can therefore conclude that the\nvalues in the covariance matrix\nrotate vectors towards the eigenvector\nwhich points in a direction where the\ndata has a maximal variance\nthis was the end of this video about the\nfundamental math behind pca\nin the next video we'll focus on how to\ninterpret the output from the principal\ncomponent analysis\n",
  "words": [
    "welcome",
    "lecture",
    "mathematical",
    "details",
    "behind",
    "principal",
    "component",
    "analysis",
    "previous",
    "video",
    "covered",
    "basics",
    "pca",
    "video",
    "see",
    "perform",
    "pca",
    "analysis",
    "using",
    "simple",
    "linear",
    "algebra",
    "order",
    "understand",
    "math",
    "behind",
    "pca",
    "show",
    "math",
    "based",
    "eigen",
    "decomposition",
    "covariance",
    "matrix",
    "however",
    "note",
    "methods",
    "singular",
    "value",
    "decomposition",
    "compute",
    "pca",
    "discussed",
    "explain",
    "pca",
    "works",
    "use",
    "following",
    "example",
    "data",
    "use",
    "pca",
    "combine",
    "two",
    "blood",
    "pressure",
    "variables",
    "one",
    "variable",
    "based",
    "data",
    "six",
    "individuals",
    "example",
    "person",
    "number",
    "one",
    "diastolic",
    "blood",
    "pressure",
    "78",
    "sees",
    "total",
    "blood",
    "pressure",
    "126",
    "whereas",
    "spouse",
    "number",
    "two",
    "historic",
    "blood",
    "pressure",
    "80",
    "historical",
    "blood",
    "pressure",
    "128",
    "data",
    "set",
    "seems",
    "strong",
    "positive",
    "correlation",
    "two",
    "variables",
    "compute",
    "pca",
    "perform",
    "following",
    "steps",
    "first",
    "center",
    "data",
    "calculate",
    "covariance",
    "matrix",
    "center",
    "data",
    "next",
    "compute",
    "eigenvalues",
    "eigenvectors",
    "covariance",
    "matrix",
    "finally",
    "order",
    "eigenvectors",
    "calculate",
    "principal",
    "components",
    "combine",
    "transform",
    "data",
    "set",
    "usually",
    "one",
    "starch",
    "center",
    "standardize",
    "data",
    "first",
    "step",
    "pca",
    "analyzes",
    "case",
    "center",
    "data",
    "means",
    "subtract",
    "values",
    "variable",
    "corresponding",
    "mean",
    "therefore",
    "subtract",
    "means",
    "historic",
    "blood",
    "pressure",
    "individual",
    "observations",
    "centering",
    "systolic",
    "blood",
    "pressure",
    "results",
    "following",
    "values",
    "tell",
    "far",
    "away",
    "original",
    "values",
    "mean",
    "calculations",
    "diastolic",
    "blood",
    "pressure",
    "mean",
    "value",
    "summarize",
    "center",
    "data",
    "following",
    "table",
    "center",
    "data",
    "means",
    "center",
    "data",
    "points",
    "around",
    "origin",
    "centering",
    "data",
    "around",
    "origin",
    "help",
    "us",
    "later",
    "rotate",
    "data",
    "centered",
    "data",
    "following",
    "values",
    "plotted",
    "like",
    "represents",
    "blood",
    "pressure",
    "whereas",
    "represents",
    "centered",
    "diastolic",
    "blood",
    "pressure",
    "next",
    "calculate",
    "covariance",
    "matrix",
    "based",
    "center",
    "data",
    "note",
    "got",
    "values",
    "covariance",
    "matrix",
    "instead",
    "would",
    "used",
    "original",
    "data",
    "since",
    "variance",
    "change",
    "center",
    "data",
    "remember",
    "main",
    "diagonal",
    "covariance",
    "matrix",
    "includes",
    "variance",
    "variable",
    "sample",
    "variance",
    "centered",
    "systolic",
    "blood",
    "pressure",
    "calculated",
    "like",
    "calculate",
    "variance",
    "centered",
    "data",
    "calculations",
    "become",
    "bit",
    "simpler",
    "since",
    "mean",
    "centered",
    "data",
    "always",
    "equal",
    "zero",
    "calculate",
    "sample",
    "variance",
    "centered",
    "data",
    "therefore",
    "simply",
    "sum",
    "squared",
    "values",
    "divide",
    "sample",
    "size",
    "calculate",
    "variance",
    "diastolic",
    "blood",
    "pressure",
    "using",
    "equation",
    "finally",
    "calculate",
    "covariance",
    "measure",
    "much",
    "two",
    "variables",
    "spread",
    "together",
    "sample",
    "covariance",
    "calculated",
    "multiplying",
    "centered",
    "values",
    "two",
    "variables",
    "example",
    "multiply",
    "centered",
    "values",
    "person",
    "number",
    "one",
    "add",
    "product",
    "centered",
    "values",
    "person",
    "number",
    "two",
    "finally",
    "divide",
    "sum",
    "products",
    "sample",
    "size",
    "minus",
    "one",
    "see",
    "spread",
    "dystolic",
    "blood",
    "pressure",
    "bit",
    "higher",
    "compared",
    "spread",
    "systolic",
    "blood",
    "pressure",
    "covariance",
    "somewhere",
    "two",
    "values",
    "next",
    "calculate",
    "eigenvalues",
    "covariance",
    "matrix",
    "details",
    "watch",
    "lecture",
    "eigenvalues",
    "eigenvectors",
    "substitute",
    "covariance",
    "matrix",
    "term",
    "lambda",
    "times",
    "identity",
    "matrix",
    "number",
    "rows",
    "columns",
    "covariance",
    "matrix",
    "subtracting",
    "two",
    "matrices",
    "results",
    "following",
    "matrix",
    "next",
    "calculate",
    "determinant",
    "matrix",
    "product",
    "diagonal",
    "minus",
    "product",
    "diagonal",
    "simplifications",
    "following",
    "quadratic",
    "equation",
    "quadratic",
    "equations",
    "like",
    "solved",
    "different",
    "ways",
    "discussed",
    "however",
    "plot",
    "left",
    "hand",
    "side",
    "changes",
    "function",
    "different",
    "values",
    "lambda",
    "see",
    "side",
    "equal",
    "zero",
    "lambda",
    "equal",
    "either",
    "means",
    "set",
    "lambda",
    "either",
    "side",
    "equation",
    "become",
    "equal",
    "zero",
    "close",
    "zero",
    "due",
    "running",
    "effects",
    "example",
    "two",
    "values",
    "represent",
    "eigenvalues",
    "covariance",
    "matrix",
    "next",
    "calculate",
    "corresponding",
    "eigenvectors",
    "two",
    "eigenvalues",
    "start",
    "calculating",
    "eigenvector",
    "covariance",
    "matrix",
    "corresponding",
    "eigenvalue",
    "calculate",
    "eigenvectors",
    "covariance",
    "matrix",
    "use",
    "following",
    "equation",
    "discussed",
    "previous",
    "video",
    "eigenvectors",
    "plug",
    "covariance",
    "matrix",
    "one",
    "two",
    "eigenvalues",
    "multiply",
    "covariance",
    "matrix",
    "column",
    "vector",
    "multiply",
    "eigenvalue",
    "vector",
    "get",
    "following",
    "system",
    "equations",
    "move",
    "two",
    "terms",
    "right",
    "hand",
    "side",
    "simplifications",
    "following",
    "system",
    "equations",
    "solving",
    "two",
    "equations",
    "results",
    "equal",
    "x",
    "example",
    "set",
    "x",
    "equal",
    "1",
    "equal",
    "vector",
    "therefore",
    "eigenvector",
    "covariance",
    "matrix",
    "illustrate",
    "vector",
    "plot",
    "like",
    "drawing",
    "error",
    "origin",
    "coordinates",
    "1",
    "normalize",
    "vector",
    "unit",
    "length",
    "means",
    "length",
    "watch",
    "lecture",
    "eigenvectors",
    "eigenvalues",
    "see",
    "one",
    "normalize",
    "eigenvector",
    "unit",
    "length",
    "normalization",
    "vector",
    "represents",
    "one",
    "two",
    "eigenvectors",
    "covariance",
    "matrix",
    "find",
    "second",
    "eigenvector",
    "calculations",
    "based",
    "second",
    "eigenvalue",
    "calculations",
    "vector",
    "represents",
    "second",
    "eigenvector",
    "unit",
    "length",
    "since",
    "covariance",
    "matrix",
    "symmetric",
    "matrix",
    "eigenvectors",
    "orthogonal",
    "means",
    "angle",
    "90",
    "degrees",
    "next",
    "order",
    "eigenvectors",
    "based",
    "corresponding",
    "eigenvalues",
    "eigenvector",
    "largest",
    "eigenvalue",
    "becomes",
    "first",
    "eigenvector",
    "since",
    "eigenvector",
    "largest",
    "eigenvalue",
    "represent",
    "first",
    "eigenvector",
    "therefore",
    "rename",
    "vector",
    "called",
    "v1",
    "instead",
    "v2",
    "let",
    "put",
    "two",
    "eigenvectors",
    "together",
    "matrix",
    "called",
    "v",
    "first",
    "column",
    "represents",
    "first",
    "eigenvector",
    "highest",
    "eigenvalue",
    "second",
    "column",
    "represents",
    "second",
    "eigenvector",
    "use",
    "matrix",
    "transform",
    "original",
    "center",
    "data",
    "two",
    "variables",
    "completely",
    "uncorrelated",
    "define",
    "matrix",
    "includes",
    "centered",
    "data",
    "next",
    "multiply",
    "data",
    "matrix",
    "matrix",
    "v",
    "includes",
    "eigenvectors",
    "columns",
    "get",
    "new",
    "matrix",
    "transformed",
    "data",
    "transformed",
    "data",
    "called",
    "principal",
    "component",
    "scores",
    "scores",
    "represent",
    "original",
    "center",
    "data",
    "principal",
    "component",
    "space",
    "go",
    "original",
    "data",
    "matrix",
    "transformed",
    "data",
    "seen",
    "like",
    "rotate",
    "data",
    "clockwise",
    "two",
    "eigenvectors",
    "point",
    "direction",
    "x",
    "axis",
    "plot",
    "rotated",
    "data",
    "looks",
    "like",
    "note",
    "labels",
    "axis",
    "changed",
    "principal",
    "component",
    "1",
    "let",
    "call",
    "two",
    "columns",
    "transform",
    "data",
    "pc1",
    "pc2",
    "plot",
    "data",
    "label",
    "pc1",
    "pc2",
    "get",
    "following",
    "plot",
    "represents",
    "arrhythmia",
    "plot",
    "rotation",
    "since",
    "plot",
    "principal",
    "component",
    "scores",
    "kind",
    "plot",
    "called",
    "score",
    "plot",
    "let",
    "compare",
    "center",
    "data",
    "transform",
    "data",
    "variance",
    "sistorial",
    "blood",
    "pressure",
    "whereas",
    "variance",
    "diastolic",
    "blood",
    "pressure",
    "covariance",
    "matrix",
    "data",
    "see",
    "covariance",
    "tells",
    "us",
    "positive",
    "correlation",
    "two",
    "variables",
    "transformed",
    "data",
    "using",
    "pca",
    "first",
    "variable",
    "called",
    "pc1",
    "variance",
    "whereas",
    "pc2",
    "variance",
    "means",
    "almost",
    "variance",
    "kept",
    "first",
    "principal",
    "component",
    "divide",
    "variance",
    "first",
    "principal",
    "component",
    "total",
    "variance",
    "see",
    "first",
    "principal",
    "component",
    "captures",
    "percent",
    "total",
    "variance",
    "note",
    "variances",
    "principal",
    "components",
    "correspond",
    "two",
    "eigenvalues",
    "calculated",
    "earlier",
    "thus",
    "eigenvalues",
    "covariance",
    "matrix",
    "represent",
    "variances",
    "principal",
    "components",
    "started",
    "covariance",
    "matrix",
    "transformed",
    "data",
    "see",
    "covariance",
    "pc1",
    "pc2",
    "equal",
    "0",
    "means",
    "pc1",
    "pc2",
    "completely",
    "uncorrelated",
    "also",
    "see",
    "variance",
    "pc1",
    "pc2",
    "correspond",
    "eigenvalues",
    "associated",
    "first",
    "second",
    "eigenvector",
    "note",
    "total",
    "variance",
    "pc1",
    "pc2",
    "corresponds",
    "total",
    "variance",
    "original",
    "variables",
    "remember",
    "multiplied",
    "center",
    "data",
    "matrix",
    "includes",
    "eigenvectors",
    "columns",
    "got",
    "transformed",
    "data",
    "using",
    "following",
    "equation",
    "calculate",
    "principal",
    "components",
    "saw",
    "previous",
    "video",
    "weights",
    "first",
    "principal",
    "component",
    "comes",
    "first",
    "eigenvector",
    "highest",
    "eigenvalue",
    "whereas",
    "weights",
    "second",
    "principal",
    "component",
    "comes",
    "second",
    "eigenvector",
    "example",
    "calculate",
    "corresponding",
    "score",
    "person",
    "number",
    "six",
    "multiply",
    "centered",
    "blood",
    "pressure",
    "values",
    "person",
    "number",
    "six",
    "corresponding",
    "weights",
    "adding",
    "products",
    "would",
    "get",
    "principal",
    "component",
    "score",
    "note",
    "also",
    "use",
    "following",
    "equation",
    "calculate",
    "example",
    "first",
    "principal",
    "component",
    "replace",
    "variable",
    "center",
    "data",
    "variable",
    "original",
    "data",
    "minus",
    "corresponding",
    "mean",
    "example",
    "would",
    "use",
    "original",
    "blood",
    "pressure",
    "values",
    "person",
    "number",
    "six",
    "would",
    "subtract",
    "mean",
    "corresponding",
    "systolic",
    "historic",
    "blood",
    "pressures",
    "would",
    "give",
    "us",
    "values",
    "center",
    "data",
    "note",
    "general",
    "aim",
    "using",
    "pca",
    "reduce",
    "dimensionality",
    "data",
    "words",
    "like",
    "reduce",
    "number",
    "variables",
    "however",
    "far",
    "reduced",
    "number",
    "variables",
    "since",
    "number",
    "principal",
    "components",
    "number",
    "variables",
    "started",
    "since",
    "first",
    "principal",
    "component",
    "captures",
    "almost",
    "variants",
    "interpreted",
    "distorts",
    "almost",
    "information",
    "two",
    "variables",
    "simply",
    "delete",
    "second",
    "principal",
    "component",
    "includes",
    "almost",
    "information",
    "seen",
    "previously",
    "using",
    "following",
    "equation",
    "combine",
    "two",
    "variables",
    "one",
    "variable",
    "way",
    "maximize",
    "variance",
    "linear",
    "combination",
    "weights",
    "interpreted",
    "much",
    "variable",
    "contributes",
    "principal",
    "component",
    "since",
    "absolute",
    "value",
    "weight",
    "diastolic",
    "blood",
    "pressure",
    "higher",
    "systolic",
    "blood",
    "pressure",
    "pca",
    "put",
    "weight",
    "diastolic",
    "blood",
    "pressure",
    "two",
    "variables",
    "combined",
    "end",
    "lecture",
    "see",
    "first",
    "eigenvector",
    "points",
    "direction",
    "data",
    "previous",
    "eigenvector",
    "extend",
    "multiplying",
    "example",
    "6",
    "draw",
    "vector",
    "note",
    "also",
    "eigenvector",
    "covariance",
    "matrix",
    "since",
    "direction",
    "eigenvector",
    "unit",
    "length",
    "covariance",
    "matrix",
    "let",
    "take",
    "arbitrary",
    "vector",
    "coordinates",
    "negative",
    "two",
    "three",
    "multiply",
    "covariance",
    "matrix",
    "vector",
    "get",
    "new",
    "vector",
    "changed",
    "direction",
    "see",
    "covariance",
    "matrix",
    "transform",
    "vector",
    "moves",
    "direction",
    "closer",
    "eigenvector",
    "note",
    "plot",
    "full",
    "length",
    "vector",
    "since",
    "fit",
    "screen",
    "importance",
    "direction",
    "multiply",
    "covariance",
    "matrix",
    "new",
    "vector",
    "get",
    "new",
    "vector",
    "new",
    "vector",
    "less",
    "direction",
    "eigenvector",
    "let",
    "take",
    "another",
    "example",
    "vector",
    "coordinates",
    "4",
    "multiplying",
    "covariance",
    "matrix",
    "arbitrary",
    "vector",
    "result",
    "following",
    "vector",
    "see",
    "covariance",
    "matrix",
    "rotate",
    "vector",
    "similar",
    "direction",
    "eigenvector",
    "therefore",
    "conclude",
    "values",
    "covariance",
    "matrix",
    "rotate",
    "vectors",
    "towards",
    "eigenvector",
    "points",
    "direction",
    "data",
    "maximal",
    "variance",
    "end",
    "video",
    "fundamental",
    "math",
    "behind",
    "pca",
    "next",
    "video",
    "focus",
    "interpret",
    "output",
    "principal",
    "component",
    "analysis"
  ],
  "keywords": [
    "lecture",
    "principal",
    "component",
    "previous",
    "video",
    "pca",
    "see",
    "using",
    "based",
    "covariance",
    "matrix",
    "note",
    "use",
    "following",
    "example",
    "data",
    "two",
    "blood",
    "pressure",
    "variables",
    "one",
    "variable",
    "six",
    "person",
    "number",
    "diastolic",
    "total",
    "whereas",
    "set",
    "first",
    "center",
    "calculate",
    "next",
    "eigenvalues",
    "eigenvectors",
    "components",
    "transform",
    "means",
    "values",
    "corresponding",
    "mean",
    "therefore",
    "systolic",
    "original",
    "calculations",
    "rotate",
    "centered",
    "like",
    "represents",
    "would",
    "since",
    "variance",
    "includes",
    "sample",
    "equal",
    "zero",
    "equation",
    "multiply",
    "lambda",
    "columns",
    "equations",
    "plot",
    "side",
    "represent",
    "eigenvector",
    "eigenvalue",
    "vector",
    "get",
    "unit",
    "length",
    "second",
    "called",
    "let",
    "new",
    "transformed",
    "direction",
    "pc1",
    "pc2",
    "almost",
    "weights"
  ]
}