{
  "text": "one of the most powerful ideas in deep\nlearning is that sometimes we can take\nthe knowledge the new network has\nlearned from one toss and apply that\nknowledge to a separate task so for\nexample maybe kind of a new network\nlearn to recognize objects like cats and\nthen use that knowledge or use part of\nthat knowledge to help you do a better\njob reading x-ray scans this is called\ntransfer learning let's take a look\nlet's say you've trained in your network\non image recognition so you first take a\nneural network and train it on XY pairs\nwhere X is an image and Y is some object\nin the image as a cat or a dog or bird\nor something else if you want to take\nthis new network and a gap or we say\ntransfer what is learn to a different\ntasks such as radiology diagnosis or\nmeaning really reading x-ray scans what\nyou can do is take this loss output\nlayer of the neural network and just\ndelete that and delete also the waste\nfeeding into that loss output layer and\ncreate a new set of randomly initialized\nways just for the last layer and at that\nnow output radiology diagnosis so to be\nconcrete during the first phase of\ntraining when you're trading on an image\nrecognition task you train all of the\nusual parameters within your network all\nthe ways all the layers and you have\nsomething that now learns to make image\nrecognition predictions having trained\nthat neural network what you now do to\nimplement transfer learning is swap in a\nnew data set X Y where now these are\nradiology images and why are the\ndiagnosis you want to predict and what\nyou do is initialize the last layers\nways is called a WL + BL random being\nand now we train the neural network on\nthis new data set on the new radiology\ndataset\nwe have a couple options of\nregions in your network with radiology\ndata you might if you have a small\nradiology data set you might want to\njust retrain the weights of the last\nlayer just W LPL and keep the rest of\nparameters fixed if you have an up data\nyou could also retrain all the layers of\nthe rest of the neural network and the\nrule of thumb is maybe a bit of a small\ndata set then just retrain the one loss\nlayer at the output layer or maybe even\nlost one or two layers there's a lot of\ndata then maybe you can retrain all the\nparameters in the networks and if you\nretrain all the parameters in your\nnetwork then this initial phase of\ntraining on image recognition is\nsometimes called pre training because\nyou're using image recognition data to\npre initialize or really pre train the\nweights of the neural network and then\nif you are updating all the ways\nafterwards and trading on the radiology\ndata sometimes that's called fine tuning\nso if you share the words pre training\nand fine tuning in a deep learning\ncontext this is what they mean when they\nrefer to pre training and fine tuning\nways in a transfer learning cost and\nwhat you've done in this example is\nyou've taken knowledge learn from image\nrecognition and applied it or\ntransferred it to radiology diagnosis\nand the reason this can be helpful is\nthat a lot of the low-level features\nsuch as detecting edges that encourage\ndetecting positive objects learning from\nthat from a very enlarged image\nrecognition data set might help your\nlearning algorithm do better in\nradiology diagnosis it's just learned a\nlot about the structure and the nature\nof how images look like and some of that\nknowledge will be useful so having learn\nto recognize images it might have\nlearned enough about you know just what\nparts of different images look like that\nthat knowledge about lines dots curves\nand so on may be small parts of objects\nthat knowledge could help your radiology\ndiagnosis Network learn a bit faster or\nlearn what less data here's another\nexample let's say that you've trained a\nspeech recognition\nso now X is inputs of audio or your\nsnippers and Y is some in transcript so\nyou're trained in speech recognition\nsystem to output your transcripts and\nlet's say that you now want to build a\nwaik word or a trigger word detection\nsystem so recall that they wake whether\nthe trigger words are the words we say\nin order to wake up speech control\ndevices in the houses such as saying\nAlexis and we're going Amazon echo or\nokay Google to waken Google device or a\nseries with an Apple device or saying\nthey hope I do to wake up up my to\ndevice so in order to do this you might\ntake out the last layer of the neural\nnetwork again and create a new output\nnote but sometimes another thing you\ncould do is actually create not just a\nsingle new output but actually create\nseveral new layers to your neural\nnetwork to try to predict the labels Y\nfor your wake word detection problem\nthen again depending on how much data\nyou have you might just retrain the new\nlayers of the network or maybe you could\nbe trained you're even more layers of\nthis neural network so when does\ntransfer learning makes sense transfer\nlearning makes sense when you have a lot\nof data for the problem you're\ntransferring from and usually relatively\nless data for the problem you're\ntransferring to so for example let's say\nyou have a million examples for your\nimage recognition tasks so that's a lot\nof data to learn a lot of low-level\nfeatures or to learn a lot of useful\nfeatures in the earlier layers in your\nnetwork but for the radiology tasks\nmaybe you have only 100 examples so\nyou're very low data for the radiology\ndiagnosis problem you have only 100\nx-ray scans so lot of knowledge you\nlearn from image recognition can be\ntransferred and can really help you get\ngoing with radiology recognition even if\nyou don't have an all the data for\nradiology\nor speech recognition maybe you've\ntrained the speech recognition system on\n$10,000 of data so you have learned a\nlot about what human voices sounds like\nfrom that $10,000 of data which really\nis a lot but for your trigger word\ndetection maybe you have only one hour\nof data so that's not raw data to figure\nout parameters so in this case a lot of\nwhat you learn about what human voices\nsound like what are components of human\nspeech and so on that can be really\nhelpful but building a good wake word\ndetector even though you have a\nrelatively small data center he's a much\nsmaller data set for the weak word\ndetection task so both of these cases\nare transferring from a problem with a\nlot of data to a problem with relatively\nlittle data one case where transfer\nlearning would not make sense is if the\nopposite was true so if you had a\nhundred images for image recognition and\nyou had a hundred images for radiology\ndiagnosis or even you're a thousand\nimages really for radiology diagnosis\none would think about it is that to do\nwell on radiology diagnosis assuming\nwhat you really want to do well on is\nradiology diagnosis having radiology\nimages is much more valuable than having\ncat-and-dog and so on images so each\nexample here is much more valuable than\neach example there at least for the\npurpose of building a good radiology\nsystem so if you already have more data\nfor radiology is not that likely that\nhaving 100 images or your random objects\nof cats and dogs and calls and so on\nwould be that helpful because the value\nof one example of images from your\nEnglish recognition terms of cats and\ndogs is just less valuable than one\nexample of an x-ray image for the task\nof building a good radiology system so\nthis would be one example where transfer\nlearning well it might not hurt but I\nwouldn't expect it to give you any\nmeaningful gain either and similarly if\nyou built a speech recognition system on\n10oz of data and you actually have 10\nhours or maybe even more say 50 hours of\ndata for week word detection you know it\nwon't merely not hurt maybe it won't\nhurt to include that 10 hours of data to\ndo transfer\nbut you just couldn't expect to get a\nmeaningful game\nso to summarize when does transfer\nlearning make sense if you're trying to\nlearn from some task a and transfer some\nof the knowledge to sometimes be then\ntransfer learning make sense when toss a\nand B have the same input X in the first\nexample a and B both images as input in\nthe second example both had audio codes\nas input it tends to make sense when you\nhave one more data for toss a then toss\nB all this is under the assumption that\nwhat you really want to do well on this\ntoss video and because data for tossed B\nis more valuable for toss B usually you\njust need a lot more data for toss a\nbecause do each example from toss a is\njust less valuable photos B in each\nexample for toss B and then finally\ntransfer learning will tend to make more\nsense if you suspect that low-level\nfeatures from toss a could be helpful\nfor learning times B and in both of the\nearlier examples maybe learning image\nrecognition teaches you a number about\nimages to hover radiology diagnosis and\nmaybe learning speech recognition\nteaches you about human speech to help\nyou with trigger words on record\ndetection so to summarize transfer\nlearning tends to be most useful if\nyou're trying to do well on sometimes be\nusually a problem where you're\nrelatively little data so for example in\nradiology you know difficult to get that\nmany x-ray scans to build a good\nradiology diagnosis system so in that\ncase you might find it related by\ndifferent tasks such as image\nrecognition where you can get maybe\nmillion images and learn a lot of\nlow-level features from that so that you\ncan then try to do well on toss be on\nyour radiology task despite not having\ndamage data for it\nwhen transfer learning makes sense it\ndoes help the performance of your\nlearning algorithms significantly but\nI've also seen sometimes seen transfer\nlearning applied in settings where toss\na actually has less data than toss B and\nin those cases you kind of don't expect\nto see much of a game\nso that's it's a transfer learning where\nyou learn from one toss and try to\ntransfer to a different task there's\nanother version of learning from\nmultiple toss which is called\nmultitasking which is when you try to\nlearn from multiple tasks at the same\ntime rather than learning from one and\nthen sequentially or after that trying\nto transfer to a different task so in\nthe next video let's discuss\nmultitasking\n",
  "words": [
    "one",
    "powerful",
    "ideas",
    "deep",
    "learning",
    "sometimes",
    "take",
    "knowledge",
    "new",
    "network",
    "learned",
    "one",
    "toss",
    "apply",
    "knowledge",
    "separate",
    "task",
    "example",
    "maybe",
    "kind",
    "new",
    "network",
    "learn",
    "recognize",
    "objects",
    "like",
    "cats",
    "use",
    "knowledge",
    "use",
    "part",
    "knowledge",
    "help",
    "better",
    "job",
    "reading",
    "scans",
    "called",
    "transfer",
    "learning",
    "let",
    "take",
    "look",
    "let",
    "say",
    "trained",
    "network",
    "image",
    "recognition",
    "first",
    "take",
    "neural",
    "network",
    "train",
    "xy",
    "pairs",
    "x",
    "image",
    "object",
    "image",
    "cat",
    "dog",
    "bird",
    "something",
    "else",
    "want",
    "take",
    "new",
    "network",
    "gap",
    "say",
    "transfer",
    "learn",
    "different",
    "tasks",
    "radiology",
    "diagnosis",
    "meaning",
    "really",
    "reading",
    "scans",
    "take",
    "loss",
    "output",
    "layer",
    "neural",
    "network",
    "delete",
    "delete",
    "also",
    "waste",
    "feeding",
    "loss",
    "output",
    "layer",
    "create",
    "new",
    "set",
    "randomly",
    "initialized",
    "ways",
    "last",
    "layer",
    "output",
    "radiology",
    "diagnosis",
    "concrete",
    "first",
    "phase",
    "training",
    "trading",
    "image",
    "recognition",
    "task",
    "train",
    "usual",
    "parameters",
    "within",
    "network",
    "ways",
    "layers",
    "something",
    "learns",
    "make",
    "image",
    "recognition",
    "predictions",
    "trained",
    "neural",
    "network",
    "implement",
    "transfer",
    "learning",
    "swap",
    "new",
    "data",
    "set",
    "x",
    "radiology",
    "images",
    "diagnosis",
    "want",
    "predict",
    "initialize",
    "last",
    "layers",
    "ways",
    "called",
    "wl",
    "bl",
    "random",
    "train",
    "neural",
    "network",
    "new",
    "data",
    "set",
    "new",
    "radiology",
    "dataset",
    "couple",
    "options",
    "regions",
    "network",
    "radiology",
    "data",
    "might",
    "small",
    "radiology",
    "data",
    "set",
    "might",
    "want",
    "retrain",
    "weights",
    "last",
    "layer",
    "w",
    "lpl",
    "keep",
    "rest",
    "parameters",
    "fixed",
    "data",
    "could",
    "also",
    "retrain",
    "layers",
    "rest",
    "neural",
    "network",
    "rule",
    "thumb",
    "maybe",
    "bit",
    "small",
    "data",
    "set",
    "retrain",
    "one",
    "loss",
    "layer",
    "output",
    "layer",
    "maybe",
    "even",
    "lost",
    "one",
    "two",
    "layers",
    "lot",
    "data",
    "maybe",
    "retrain",
    "parameters",
    "networks",
    "retrain",
    "parameters",
    "network",
    "initial",
    "phase",
    "training",
    "image",
    "recognition",
    "sometimes",
    "called",
    "pre",
    "training",
    "using",
    "image",
    "recognition",
    "data",
    "pre",
    "initialize",
    "really",
    "pre",
    "train",
    "weights",
    "neural",
    "network",
    "updating",
    "ways",
    "afterwards",
    "trading",
    "radiology",
    "data",
    "sometimes",
    "called",
    "fine",
    "tuning",
    "share",
    "words",
    "pre",
    "training",
    "fine",
    "tuning",
    "deep",
    "learning",
    "context",
    "mean",
    "refer",
    "pre",
    "training",
    "fine",
    "tuning",
    "ways",
    "transfer",
    "learning",
    "cost",
    "done",
    "example",
    "taken",
    "knowledge",
    "learn",
    "image",
    "recognition",
    "applied",
    "transferred",
    "radiology",
    "diagnosis",
    "reason",
    "helpful",
    "lot",
    "features",
    "detecting",
    "edges",
    "encourage",
    "detecting",
    "positive",
    "objects",
    "learning",
    "enlarged",
    "image",
    "recognition",
    "data",
    "set",
    "might",
    "help",
    "learning",
    "algorithm",
    "better",
    "radiology",
    "diagnosis",
    "learned",
    "lot",
    "structure",
    "nature",
    "images",
    "look",
    "like",
    "knowledge",
    "useful",
    "learn",
    "recognize",
    "images",
    "might",
    "learned",
    "enough",
    "know",
    "parts",
    "different",
    "images",
    "look",
    "like",
    "knowledge",
    "lines",
    "dots",
    "curves",
    "may",
    "small",
    "parts",
    "objects",
    "knowledge",
    "could",
    "help",
    "radiology",
    "diagnosis",
    "network",
    "learn",
    "bit",
    "faster",
    "learn",
    "less",
    "data",
    "another",
    "example",
    "let",
    "say",
    "trained",
    "speech",
    "recognition",
    "x",
    "inputs",
    "audio",
    "snippers",
    "transcript",
    "trained",
    "speech",
    "recognition",
    "system",
    "output",
    "transcripts",
    "let",
    "say",
    "want",
    "build",
    "waik",
    "word",
    "trigger",
    "word",
    "detection",
    "system",
    "recall",
    "wake",
    "whether",
    "trigger",
    "words",
    "words",
    "say",
    "order",
    "wake",
    "speech",
    "control",
    "devices",
    "houses",
    "saying",
    "alexis",
    "going",
    "amazon",
    "echo",
    "okay",
    "google",
    "waken",
    "google",
    "device",
    "series",
    "apple",
    "device",
    "saying",
    "hope",
    "wake",
    "device",
    "order",
    "might",
    "take",
    "last",
    "layer",
    "neural",
    "network",
    "create",
    "new",
    "output",
    "note",
    "sometimes",
    "another",
    "thing",
    "could",
    "actually",
    "create",
    "single",
    "new",
    "output",
    "actually",
    "create",
    "several",
    "new",
    "layers",
    "neural",
    "network",
    "try",
    "predict",
    "labels",
    "wake",
    "word",
    "detection",
    "problem",
    "depending",
    "much",
    "data",
    "might",
    "retrain",
    "new",
    "layers",
    "network",
    "maybe",
    "could",
    "trained",
    "even",
    "layers",
    "neural",
    "network",
    "transfer",
    "learning",
    "makes",
    "sense",
    "transfer",
    "learning",
    "makes",
    "sense",
    "lot",
    "data",
    "problem",
    "transferring",
    "usually",
    "relatively",
    "less",
    "data",
    "problem",
    "transferring",
    "example",
    "let",
    "say",
    "million",
    "examples",
    "image",
    "recognition",
    "tasks",
    "lot",
    "data",
    "learn",
    "lot",
    "features",
    "learn",
    "lot",
    "useful",
    "features",
    "earlier",
    "layers",
    "network",
    "radiology",
    "tasks",
    "maybe",
    "100",
    "examples",
    "low",
    "data",
    "radiology",
    "diagnosis",
    "problem",
    "100",
    "scans",
    "lot",
    "knowledge",
    "learn",
    "image",
    "recognition",
    "transferred",
    "really",
    "help",
    "get",
    "going",
    "radiology",
    "recognition",
    "even",
    "data",
    "radiology",
    "speech",
    "recognition",
    "maybe",
    "trained",
    "speech",
    "recognition",
    "system",
    "data",
    "learned",
    "lot",
    "human",
    "voices",
    "sounds",
    "like",
    "data",
    "really",
    "lot",
    "trigger",
    "word",
    "detection",
    "maybe",
    "one",
    "hour",
    "data",
    "raw",
    "data",
    "figure",
    "parameters",
    "case",
    "lot",
    "learn",
    "human",
    "voices",
    "sound",
    "like",
    "components",
    "human",
    "speech",
    "really",
    "helpful",
    "building",
    "good",
    "wake",
    "word",
    "detector",
    "even",
    "though",
    "relatively",
    "small",
    "data",
    "center",
    "much",
    "smaller",
    "data",
    "set",
    "weak",
    "word",
    "detection",
    "task",
    "cases",
    "transferring",
    "problem",
    "lot",
    "data",
    "problem",
    "relatively",
    "little",
    "data",
    "one",
    "case",
    "transfer",
    "learning",
    "would",
    "make",
    "sense",
    "opposite",
    "true",
    "hundred",
    "images",
    "image",
    "recognition",
    "hundred",
    "images",
    "radiology",
    "diagnosis",
    "even",
    "thousand",
    "images",
    "really",
    "radiology",
    "diagnosis",
    "one",
    "would",
    "think",
    "well",
    "radiology",
    "diagnosis",
    "assuming",
    "really",
    "want",
    "well",
    "radiology",
    "diagnosis",
    "radiology",
    "images",
    "much",
    "valuable",
    "images",
    "example",
    "much",
    "valuable",
    "example",
    "least",
    "purpose",
    "building",
    "good",
    "radiology",
    "system",
    "already",
    "data",
    "radiology",
    "likely",
    "100",
    "images",
    "random",
    "objects",
    "cats",
    "dogs",
    "calls",
    "would",
    "helpful",
    "value",
    "one",
    "example",
    "images",
    "english",
    "recognition",
    "terms",
    "cats",
    "dogs",
    "less",
    "valuable",
    "one",
    "example",
    "image",
    "task",
    "building",
    "good",
    "radiology",
    "system",
    "would",
    "one",
    "example",
    "transfer",
    "learning",
    "well",
    "might",
    "hurt",
    "would",
    "expect",
    "give",
    "meaningful",
    "gain",
    "either",
    "similarly",
    "built",
    "speech",
    "recognition",
    "system",
    "10oz",
    "data",
    "actually",
    "10",
    "hours",
    "maybe",
    "even",
    "say",
    "50",
    "hours",
    "data",
    "week",
    "word",
    "detection",
    "know",
    "wo",
    "merely",
    "hurt",
    "maybe",
    "wo",
    "hurt",
    "include",
    "10",
    "hours",
    "data",
    "transfer",
    "could",
    "expect",
    "get",
    "meaningful",
    "game",
    "summarize",
    "transfer",
    "learning",
    "make",
    "sense",
    "trying",
    "learn",
    "task",
    "transfer",
    "knowledge",
    "sometimes",
    "transfer",
    "learning",
    "make",
    "sense",
    "toss",
    "b",
    "input",
    "x",
    "first",
    "example",
    "b",
    "images",
    "input",
    "second",
    "example",
    "audio",
    "codes",
    "input",
    "tends",
    "make",
    "sense",
    "one",
    "data",
    "toss",
    "toss",
    "b",
    "assumption",
    "really",
    "want",
    "well",
    "toss",
    "video",
    "data",
    "tossed",
    "b",
    "valuable",
    "toss",
    "b",
    "usually",
    "need",
    "lot",
    "data",
    "toss",
    "example",
    "toss",
    "less",
    "valuable",
    "photos",
    "b",
    "example",
    "toss",
    "b",
    "finally",
    "transfer",
    "learning",
    "tend",
    "make",
    "sense",
    "suspect",
    "features",
    "toss",
    "could",
    "helpful",
    "learning",
    "times",
    "b",
    "earlier",
    "examples",
    "maybe",
    "learning",
    "image",
    "recognition",
    "teaches",
    "number",
    "images",
    "hover",
    "radiology",
    "diagnosis",
    "maybe",
    "learning",
    "speech",
    "recognition",
    "teaches",
    "human",
    "speech",
    "help",
    "trigger",
    "words",
    "record",
    "detection",
    "summarize",
    "transfer",
    "learning",
    "tends",
    "useful",
    "trying",
    "well",
    "sometimes",
    "usually",
    "problem",
    "relatively",
    "little",
    "data",
    "example",
    "radiology",
    "know",
    "difficult",
    "get",
    "many",
    "scans",
    "build",
    "good",
    "radiology",
    "diagnosis",
    "system",
    "case",
    "might",
    "find",
    "related",
    "different",
    "tasks",
    "image",
    "recognition",
    "get",
    "maybe",
    "million",
    "images",
    "learn",
    "lot",
    "features",
    "try",
    "well",
    "toss",
    "radiology",
    "task",
    "despite",
    "damage",
    "data",
    "transfer",
    "learning",
    "makes",
    "sense",
    "help",
    "performance",
    "learning",
    "algorithms",
    "significantly",
    "also",
    "seen",
    "sometimes",
    "seen",
    "transfer",
    "learning",
    "applied",
    "settings",
    "toss",
    "actually",
    "less",
    "data",
    "toss",
    "b",
    "cases",
    "kind",
    "expect",
    "see",
    "much",
    "game",
    "transfer",
    "learning",
    "learn",
    "one",
    "toss",
    "try",
    "transfer",
    "different",
    "task",
    "another",
    "version",
    "learning",
    "multiple",
    "toss",
    "called",
    "multitasking",
    "try",
    "learn",
    "multiple",
    "tasks",
    "time",
    "rather",
    "learning",
    "one",
    "sequentially",
    "trying",
    "transfer",
    "different",
    "task",
    "next",
    "video",
    "let",
    "discuss",
    "multitasking"
  ],
  "keywords": [
    "one",
    "learning",
    "sometimes",
    "take",
    "knowledge",
    "new",
    "network",
    "learned",
    "toss",
    "task",
    "example",
    "maybe",
    "learn",
    "objects",
    "like",
    "help",
    "scans",
    "called",
    "transfer",
    "let",
    "say",
    "trained",
    "image",
    "recognition",
    "neural",
    "train",
    "x",
    "want",
    "different",
    "tasks",
    "radiology",
    "diagnosis",
    "really",
    "output",
    "layer",
    "create",
    "set",
    "ways",
    "last",
    "training",
    "parameters",
    "layers",
    "make",
    "data",
    "images",
    "might",
    "small",
    "retrain",
    "could",
    "even",
    "lot",
    "pre",
    "words",
    "helpful",
    "features",
    "less",
    "speech",
    "system",
    "word",
    "trigger",
    "detection",
    "wake",
    "actually",
    "try",
    "problem",
    "much",
    "sense",
    "relatively",
    "get",
    "human",
    "good",
    "would",
    "well",
    "valuable",
    "b"
  ]
}