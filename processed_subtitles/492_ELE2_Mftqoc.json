{
  "text": "welcome to the reinforcement learning\njump start series i'm your host phil\ntabor if you don't know me i'm a\nphysicist and former semiconductor\nengineer turned machine learning\npractitioner\nin this series of tutorials you're going\nto learn everything you need to know to\nget started with reinforcement learning\nyou don't need any prior exposure all\nyou really need is some basic\nfamiliarity with python\nas far as requirements for this course\nthey are pretty light you will need the\nopen ai gym because we're going to be\ntaking advantage of that rather\nextensively you'll also need the atari\nextension for that so we can play games\nlike breakout space invaders you'll also\nneed the box 2d extension so we can do\nthe new lander environment\nand beyond that you will need the\ntensorflow library as well as pytorch\nand i'm going to have tutorials in both\ntensorflow and pi torch with a bit of a\nstronger emphasis on tensorflow\ni'm going to teach the course in\nsomewhat of a top-down fashion meaning\nwe're going to get to the really\nimportant and exciting new stuff like\ndeep q learning and policy gradient\nmethods first\nafter that we'll kind of back up and\ntake a look at things like sarsa double\nq learning and we'll even get into how\nto make your own reinforcement learning\nenvironments when we code up our own\ngrid world and then solve it with\nregular q learning\nif you missed something in the code\ndon't worry i keep all this code on my\ngithub which i will link in the\npin comment down below i'll also link\nthe relevant timestamps for all the\nmaterial in case you want to jump around\nbecause maybe some topics interest you\nmore or you want to get some additional\nbackground information from the\nexplainer videos\nquestions comments leave them down below\ni will address all of them let's get to\nit\nin this video you're going to learn\neverything you need to know to implement\nq-learning from scratch\nyou don't need any prior exposure to\nq-learning you don't even really need\nmuch familiarity with reinforcement\nlearning you get everything you need in\nthis video\nif you're new here i'm phil and i'm here\nto help you get started with machine\nlearning i upload three videos a week so\nmake sure to subscribe so you don't miss\nout\nimagine you've just gotten the\nrecognition you deserve in the form of\noffers for a machine learning\nengineering position from google\nfacebook and amazon\nall three are offering you a boatload of\nmoney and your dreams are big balling or\ninterrupted by the realization that\nstarting salary is just well the\nstarting salary you've got friends at\neach of the three companies so you reach\nout to find out about the promotion\nschedules with each\nfacebook offers two hundred fifty\nthousand dollars to start with a ten\npercent raise after three years but with\na forty percent probability that you'll\nquit\ngoogle offers two hundred thousand\ndollars to start with the twenty percent\nraise after three years but with only a\n25 probability that you'll quit\namazon offers 350 000 to start with a\nfive percent raise after five years with\na sixty percent chance that you'll end\nup washing out\nso what should you take\nall three of our big money but future\nraises are far from certain\nthis is the sort of problem\nreinforcement learning is designed to\nsolve\nhow can an agent maximize long-term\nrewards in environments with\nuncertainties\nlearning is a powerful solution because\nit lets agents learn from the\nenvironment in real time and quickly\nlearn novel strategies for mastering the\ntask at hand\nq learning works by mapping pairs of\nstates and actions to the future rewards\nthe agent expects to receive\nit decides which actions to take based\non a strategy called epsilon greedy\naction selection\nbasically the agent spends some time\ntaking random actions to explore the\nenvironment and the remainder of the\ntime selecting actions with the highest\nknown expected feature rewards\nepsilon refers to the fraction of the\ntime the agent spends exploring and it's\na model hyperparameter between 0 and 1.\nyou can gradually decrease epsilon over\ntime to some finite value so that your\nagent eventually converges on a mostly\ngreedy strategy\nyou probably don't want to set epsilon\nat zero exactly since it's important to\nalways be testing the agent's model of\nthe environment after selecting and\ntaking some action the agent gets its\nreward from the environment\nwhat sets q-learning apart from many\nreinforcement learning algorithms is\nthat it performs its learning operation\nafter each time step instead of at the\nend of each episode as is the case with\npolicy gradient methods at this point\nit's important to make a distinction\ntraditional q learning works by\nliterally keeping a table of state and\naction pairs\nif you're implementing this in python\nyou could use a dictionary with state\nand action tuples as keys\nthis is only feasible in environments\nwith a limited number of discrete states\nand actions\nhere the agent doesn't need to keep\ntrack of its history since it can just\nupdate the table in place as it plays\nfor the game the way the agent updates\nhis memories by taking the difference in\nexpected returns\nbetween the actions it took with the\naction that had the highest possible\nfuture returns this ends up biasing the\nagent's estimates over time towards the\nactions that end up producing the best\npossible outcomes when we're dealing\nwith environments that have a huge\nnumber of states or state space that is\ncontinuous then we really can't use a\ntable\nin that case we have to use deep neural\nnetworks to take these observations of\nthe environment and turn them into\ndiscrete outputs that correspond to the\nvalue of each action this is called deep\nq learning the reason we have to use\nneural networks is that they are\nuniversal function approximators\nit turns out the deep neural nets can\napproximate any continuous function\nwhich is precisely what we have the\nrelationship between states actions and\nfeature returns is a function that the\nagent wants to learn so it can maximize\nits future rewards deepq learning agents\nhave a memory of the states they saw the\nactions they took and the rewards they\nreceived\nduring each learning step the agent\nsamples a subset of this memory to feed\nthese states through its neural network\nand compute the values of the actions it\ntook just like with regular q learning\nthe agent also computes the values for\nthe maximal actions and uses the\ndifference between the two as its loss\nfunction to update the weights of the\nneural network\nso let's talk implementation\nin practice we end up with two deep\nneural networks\none network called the evaluation\nnetwork is to evaluate the current state\nand see which action to take and another\nnetwork called the target network that\nis used to calculate the value of\nmaximal actions during the learning step\nthe reasoning for why you need two\nnetworks is a little complicated but\nbasically it boils down to eliminating\nbias in the estimates of the values of\nthe actions\nthe weight of the target network are\nperiodically updated with the weights of\nthe evaluation network so that the\nestimates of the maximal actions can get\nmore accurate over time if you're\ndealing with an environment that gives\npixel images just like in the atari\nlibrary from the openai gym then you\nwill need to use a convolutional neural\nnetwork to perform feature extraction on\nthe images\nthe output from the convolutional\nnetwork is flattened and then fed into a\ndense neural network to approximate the\nvalues of each action for your agent if\nthe environment has movement as most do\nthen you have an additional problem to\nsolve\nif you take a look at this image can you\ntell which way the ball or paddle is\nmoving\nit's pretty much impossible for you to\nget a sense of motion from just a single\nimage and this limitation applies to the\ndeep q learning agent as well\nthis means you'll need a way of stacking\nframes to give the agent a sense of\nmotion\nso to be clear this means that the\nconvolutional neural network takes in\nthe batch of stacked images as input\nrather than a single image\nchoosing an action is reasonably\nstraightforward generate a random number\nand if it's less than the epsilon\nparameter pick an action at random\nif it's greater than the agent's epsilon\nthen feed the set of stacked frames\nthrough the evaluation network to get\nthe values for all the actions in the\ncurrent state\nfind the maximal action and take it once\nyou get the new state back from the\nenvironment add it to the end of your\nstacked frames and store the stacked\nframes actions and rewards in the\nagent's memory then perform the learning\noperation by sampling the agent's memory\nit's really important to get a\nnon-sequential random sampling of the\nmemory so that you avoid getting trapped\nin one little corner of parameter space\nas long as you keep track of the state\ntransitions actions and rewards in the\nsame way you should be pretty safe\nfeed that random batch of data through\nthe evaluation and target networks and\nthe compute the loss function to perform\nyour loss minimization step for the\nneural network that's really all there\nis to deep cue learning a couple neural\nnetworks a memory to keep track of\nstates and lots of gpu horsepower to\nhandle the training\nspeaking of which of course you'll need\nto pick a framework preferably one that\nlets you use a gpu for the learning\npytorch and tensorflow are both great\nchoices and both support model\ncheckpointing\nthis will be critical if you have other\nstuff to do and can't dedicate a day or\nso for model training\nthat's it for now make sure to share the\nvideo if you found it helpful and\nsubscribe so you don't miss any future\nreinforcement learning content i'll see\nyou in the next video\nin this tutorial you're going to learn\nhow to use deep q learning to teach an\nagent to play breakout in the tensorflow\nframework you don't need to know\nanything about deep q learning you don't\neven need to know anything about\ntensorflow you just have to follow along\nlet's get started\nif you're new to the channel i'm phil a\nphysicist and former semiconductor\nengineer turned machine learning\npractitioner here at machine learning\nwith phil we do deep reinforcement\nlearning and artificial intelligence\ntutorials three times a week so if\nyou're into that kind of thing hit that\nsubscribe button let's get to the video\nso if you're not familiar with deep q\nlearning the basic idea is that the\nagent uses a convolutional neural\nnetwork to turn the set of images from\nthe game into a set of feature vectors\nthose are fed into a fully connected\nlayer to determine the value of each of\nthe actions given some set of states in\nthis case a set of states is just going\nto be a stack of frames because we want\nthe agent to have a sense of motion so\nas we go along we'll be stacking up\nframes passing them into our network and\nasking the network hey what is the value\nof either of the actions move left move\nright or fire a ball we're going to\nsplit this into two classes one of which\nwill house the deep q networks and the\nother will house the agent and in the\nagent class we're going to have other\nstuff that we'll get to later\nlet's go ahead and start with our\nimports we'll need os to handle model\nsaving\nwe'll need numpy to handle some basic\nrandom functions\nand of course tensorflow to build our\nagent\nso we'll start with our deepq network\nthe initializer is pretty\nstraightforward we're going to take the\nlearning rate number of actions\nthe\nname of the network that is important\nbecause we're gonna have two networks\none to select an action one to tell us\nthe value of an action\nmore on that later\nthe number of dimensions in the first\nfully connected layer\nthe input dimensions of our environment\nso\nfor the atari gym\nsorry the atari library of the open ai\ngym all of the images have 210 by 160\nresolution and we're going to pass in a\nset of frames to give the agent a sense\nof motion we're going to pass in four\nframes in particular so it's going to be\n210 by default 210 160 by 4. we're going\nto do some cropping later on we'll get\nto that in a minute\nwe also need a directory to save our\nmodel\nso the next thing we need is the\ntensorflow session this is what\ninstantiates everything into the graph\nand each network wants to have its own\nthen we'll call the build network\nfunction to add everything to the graph\nonce you've added everything to the\ngraph you have to initialize it very\nimportant tensorflow will complain if\nyou don't do that so best to do that now\nand the way you do that is by calling\nthe tf global variables initializer\nfunction\nother thing we need is a way of saving\nour models as we go along and this is\ncritical because this deep queue network\ntakes forever to train i let it train\nfor about 10 hours and it averages a\nscore of two to three points\nper set of uh whatever number of lies it\ngets so\nit's going to have to train for quite\nsome time so we're going to want to be\nable to save it as we go along because\nwe have other stuff to do right\nand of course you want a way of saving\nyour checkpoint files\nnext thing we need is a way of keeping\ntrack of the parameters for each\nparticular network\nand you do that like this\nso what this will do is tell tensorflow\nthat we want to keep track of all of the\ntrainable variables for the network that\ncorresponds to whatever the name of this\nparticular network is we use this later\nwhen we copy one network to another\nnext let's build our network\nso we're gonna encase everything in a\nscope that is based on the the network's\nname\nwe're going to have placeholder\nvariables that tell us the inputs to our\nmodel we're going to want to input the\nstack of images from the\natari game we want to input the actions\nthat the agent took as well as the\ntarget value for the q network we'll get\nto that in a minute\nand this convention of naming\nnaming placeholders and\nlayers you're going to see repeated\nthroughout the tensorflow library the\nreason is that it it makes debugging\neasier if you get an error it will tell\nyou the variable or layer that caused\nthe error very handy\nand so you can probably tell by the\nshape here that we are going to do a one\nhot encoding of the actions\nand the same thing for the q target so\nthe\nconvention of using none as the first\nparameter in the shape\nallows you to train a batch of stuff and\nthat's important because in virtually\nevery deep learning application you want\nto pass in a batch of information right\nin this case we're going to be passing\nin batches of stacked frames\nso we'll get to that in a moment next\nthing we have to do is start to build\nour\nscroll down a little bit and start\nbuilding our convolutional neural\nnetwork so let's start building our\nlayers\nthe first one will have 32 filters\nkernel size of 8x8\nstrides of 4 and a name of conf 1.\nthe other thing we need is an\ninitializer\nnow\nwe are going to use the initializer that\nthe deepmind team used in their paper\nreason is that we want to learn from the\nexperts and may as well do what they do\nif it's going to make our life\nsignificantly easier\nand that's going to be a variance\nscaling initializer the scale of 2.\nand then we want to activate that with a\nrelu function\nthat's right it's con1 activated\nso our next layer is\npretty similar\nit'll take the activated output of the\nfirst layer\nas input\nthat'll take 64 filters\nif you're not familiar with what a\nfilter is you can check out my video on\nconvolutional neural networks\na kernel size of in this case four by\nfour\nstrides of two\nname of conf two\nand the\nwe can go ahead and\ncopy that initializer\nwhy not\nso that is our second convolutional\nlayer and we're gonna do something\nsimilar for the third of course\nand that will take\nconf2 activated\n128 filters\ntwo by sorry a three by three kernel\ngood grief a stride of one and a name of\nconf 3\nand the same initializer\nand of course we want to activate it as\nwell so the next step is once we have\nthe outputs of our convolutional net\nneural network we want to flatten all of\nthem and pass them through a dense\nnetwork to get our q values or the\nvalues of each state action pair let's\ndo that now\nthat's where our fc1 dimms come in\nand\nwe need a\nvalue activation\nand oops we will do\nthe same initializer for the dense layer\nso next up we need to determine the q\nvalues\nq and q learning just refers to the\nvalue of a state action pair\nit's just the nomenclature\nand this will be the output of our\nneural network\nand of course we want to have one output\nfor each action\nand this gets the same initializer\nnow we're not activating that yet uh we\nwant to\njust get the linear values sorry the\nlinear activation of the output of our\nnetwork\nso the next thing we need is the actual\nvalue\nof q\nfor each action\nand remember actions is a placeholder\nand next thing we need for every neural\nnetwork is a loss function\nso we just want to have the squared\ndifference between\nthe\nq value of the network outputs and\nsomething called the q target the q\ntarget let's get to that now so the\nthe way q learning works is that at each\ntime step it's a form of temporal\ndifference learning so every time step\nit learns and it says hey i took some\naction what was the maximal action i\ncould have taken and then it takes the\ndelta between whatever action it took\nand the maximal action and uses that to\nupdate the\nthe neural network as its loss function\nso our training operation\nis just a form of gradient descent uh\natom optimizer in this case\nuh learning rate and you want to\nminimize that loss function\nlet's give this more room\nso that is almost it so that is our\nnetwork so the next thing we need is a\nway of saving files right\nand save and loading them as well\nthe reason we want this is as we said\nthese models take a notoriously long\ntime to train\nand so we may want to start and stop as\nwe go along\nand what this will do is it will look in\nthe checkpoint file and load up the\ngraph from that file\nand save it\nand load it into the graph of the\ncurrent session\nand we're going to save frequently as we\ntrain something like every 10 games\nand all this function does is it takes\nthe current session and opposite to a\nfile\npretty handy so that is our deep q\nnetwork what this does again is it takes\na batch of images from the environment\nin this case breakout passes it through\nconvolutional neural network to do the\nfeature selection that passes it through\na fully connected layer to determine the\nvalue of each given action and then uses\nthe the maximum value of the next action\nto determine its loss function and\nperform training on that network network\nvia back propagation\nnext up we need an agent that includes\neverything else all of the learnings all\nthe memories all that good stuff\nso it's going to take something called\nalpha that is the learning rate\ngamma that's a discount factor a hyper\nparameter of our model\nthe memory size number of actions and\nepsilon that determines how often it\ntakes a random action\na batch size\na\nparameter that tells us how often we\nwant to replace our target network\nset of input dimms\nwe use the same as before 210 160 by\nfour\none moment my cat is whining we need the\ndirectory to save the q next network\nand we will need a directory to save the\nq evaluation\nand what this\nas i said we'll have two networks one\nthat tells us the action to take the one\nthat tells us the value of that action\nso let's go ahead and start our\ninitializer\nso when we take random actions\nwe will need to know the action space\nwhich is just the set of all possible\nactions\nand we need to know the number of\nactions\nwe need our discount factor gamma this\ntells the agent how much it wants to\ndiscount future rewards\nthe memory size which tells us how many\ntransitions to store in memory\nof course our epsilon\nand epsilon greedy\nand then we need our network to tell the\nagent the value of the next action\nso we'll pass in the alpha learning rate\nnumber of actions\ninput dimms\nthe\nname\nand the checkpoint directory\nokay so now we have our two networks the\nnext thing we need is a memory\nso q learning works by saving the state\naction reward and new state\ntransitions in its memory we're also\ngoing to save the terminal flags that\ntell the agent whether or not the game\nis done\nthat'll go into the calculation of our\nreward when we do the learning function\nso we need a state memory\njust a numpy array of zeros\nwe shape mem size and input dimms\nand so this will save a\nset of\nfour transitions four frames stacked\nfour frames by number of memories\nand we also need an action memory\nand this will handle the one this will\nstore the one hot encoding of our\nactions\nnow\nthat is just one dimensional\nthis will just store the agent's memory\nof the rewards\nand then we need the terminal memory\nso this just saves\nthe memory of the\ndone flex\nand to save ram\nwe'll save that one as int8\nand you know what we can do the same\nthing with the\nactions\nand this is important because we're\ngoing to be saving 25 000 or so\ntransitions on my pc this consumes about\n47 gigabytes 48 gigabytes of ram i have\n96 so it fits\nuh if you have less you're gonna need a\nsignificantly smaller memory size just\nsomething to keep in mind\nso next thing we need to do is to store\nthose transitions in memory\nand this is of course pretty stateful\nstraightforward so we need the old state\nthe action the reward the new state\nlet's just call that state underscore\nand a terminal flag so that'll just be\nan integer zero or one\nso the agent has some fixed memory signs\nwe want to fill up to that memory and\nthen when we exceed it we just want to\ngo back to the beginning and start\noverriding it so the index\nis just going to be mem counter which is\nthe\nsomething i forgot let's put that up\nhere\nso that will be the\ncounter that keeps track of the\nnumber of memories that it has stored\nso\nfor our actions we need to do the one\nhot encoding\nand when we pass in the action it'll\njust be an integer\nso making an array of zeros and setting\nthe index of the action you took to one\nis a one hot encoding\nand of course you want to increment the\nmemory counter\nso the next thing we need is a way of\nchoosing actions so deep q learning\nrelies on what is called epsilon greedy\nso epsilon is a parameter that tells it\nhow often to choose a random action\nwe're going to dk epsilon over time the\nagent will start out acting purely\nrandomly for many many hundreds of games\nand eventually the random factor will\nstart decreasing over time and the agent\nwill take more and more greedy actions\nthe greedy action is choosing the action\nthat has the highest value of the next\nstate\nso this takes the state as input\nwe need a random number from the numpy\nrandom number generator\nand then we'll select an action at\nrandom from the agent's action space\nif we are going to take a greedy action\nthen we need to actually find out what\nour next highest lead highest valued\naction is\nso we need to use our evaluation network\nto run the\nq eval\ndot q values\ntensor\nusing a feed dict of the\nsorry the\ni can't talk and type at the same time\nof the current state as the q evaluation\nnetwork input\nand then of course if you want the\nmaximum action you just need\nnumpy.arcmax of actions\nand when you're done just return the\naction\nso now we come to the meat of the\nproblem we have to handle the learning\nso learning has many parts to it the\nbasic idea is first thing we're going to\ndo is check to see if we want to update\nthe value of our target network and if\nit's time to do that we're going to go\nahead and do that\nthe next thing we're going to do is\nselect a batch of random memories the\nmost important thing here is that these\nmemories are non-sequential if you\nchoose sequential memories then the\nagent will get trapped in some little\nnook and cranny a parameter space\nand what you'll get is oscillations and\nperformance over time to actually have\nrobust learning you want to select\ndifferent transitions over the entirety\nof the memory so\nthat's how you handle memory batching\nand sampling and then you have to\ncalculate the\nvalue of the\ncurrent action as well as the next\nmaximum action and then you plug that\ninto the bellman equation for the q\nlearning algorithm and run your update\nfunction on your loss so let's go ahead\nand do that\nso\nfirst thing we want to do is see if it's\ntime to replace our\nnetwork target network\nif it is\ngo ahead and do that and we'll write the\nupdate graph function momentarily\nnext thing we want to do is find out\nwhere our memory ends\nless than\nmem size\nelse\nthis will allow us to randomly sample a\nsubset of the memory\nand this will give us a\nrandom choice in the range maximum\nof size batch size\nso next we need our state batches\nthese are just sorry these are just the\nstate transitions\nwe will need the\nactions we took and remember we store\nthese as a one-hot encoding so we need\nto go back to a\nwe need to go back to a\ninteger encoding so\nwe need to handle that\nand the simplest way to do that is just\nto do a numpy dot operation to just\nmultiply two vectors together\nso next we need to calculate the values\nof the\ncurrent set of states as well as these\nset of next states\nsorry sorry with qe valve\nand the next so this will take the set\nof next states the transit the\ntransitions\nthe next thing we want to do is\ncopy the qeval network\nbecause we want the loss for all of the\nnon-optimal actions to be zero\nnext thing we need to do is calculate\nthe\nvalue of q target\nso for all of these states in the batch\nfor the actions we actually took\nuh plus this quantity here\nso the\nmaximum value of the next state\nmultiplied by this quantity terminal\nbatch so the reason is that if we if the\nnext state is the end of the episode you\njust want to have the reward whereas if\nit is not a terminal state the next\nstate then you want to actually take\ninto account the\ndiscounted future rewards\nso next we need to feed all of this\nthrough our neural network\nthrough our training operation\nwe need the\nactions which is the action we actually\ntook\nand we also need the target values\nwhich we just calculated\nso the next thing we need to do is to\nhandle the prospect of decreasing\nepsilon over time remember epsilon is\nthe random factor that tells the agent\nhow often to take a random action the\ngoal of learning is to eventually take\nthe best possible actions so you want to\ndecrease that epsilon over time so we\nhandle that by allowing the agent to\nplay\nsome number of\nmoves randomly so we'll say 100 000\nmoves randomly\nand we want to dictate some minimum\nvalue because you never wanted to do\npurely greedy actions because you never\nknow if your estimates are off so you\nalways want to be exploring to make sure\nestimates are not off\nand\nyou can decrease epsilon over time any\nnumber of ways you can do it linearly\nthat's what deepmind did you can use\nsquare roots you can use any number of\nthings i'm just going to do\nthis we're going to multiply it by some\nfraction of one\na bunch of nines that'll give you a\nreally slow decrease of epsilon over\ntime so the agent takes a lot of random\nactions and does a lot of exploration\nsorry i flipped my sign there\ni thought that didn't look right so yeah\nif it tries to drop below 0.01 we're\ngoing to go ahead and set it there\nokay so that is the learning function\nnext up we have to handle the functions\nto save the models\nand this will just call the save\ncheckpoint\nfunction\nfor the\nrespective networks\nnext function we need is a way of\nupdating our graph so what we want to do\nis we want to copy the\nevaluation network to the target network\nand this is actually a little bit tricky\nso what you want are the target\nparameters\nthis is why we saved them earlier\nand you want to perform a copy operation\non them\nnow\nthe reason this is non-trivial is\nbecause you have to pass in a session\nand the decision of which session to use\nis non-trivial so you have to use the\nsession for the values that you're\ntrying to copy from not copy 2. so if\nyou had q next you would get\nan error\nwhich took me a little bit to figure out\nso that is our agent class next up we\nhave to code a\na\nmain function so of course we have to\nstart again with our imports\ni'm going to import jim\nand we want to import a network\nwe will also need numpy to handle the\nreshaping of the observation we're going\nto truncate it to make the workload on\nthe neural the gpu a little bit lower so\nimport numpy\nas np\nif you want to save the\nthe uh games you can actually\nuse that with uh you can do that with\nthe wrappers so i won't put that in here\nbut i will put that in my github version\nso you can just do a git pull on this\nand you will have the way of\nsaving the games\nso first thing we have to do is\npre-process our observations\nand the reason you want to do this is\nbecause you don't need all of the image\nwe don't need the score and we also\ndon't need color we just need one\nchannel\nso i've already figured it out if you\ntake row 30 onward and all the columns\nthen you will get\na good image\nand you want to reshape it like so the\nnext thing you have to handle\nis a way of stacking the frames this is\nbecause the agent can't get a sense of\nmotion by looking at only one picture\nright nobody can get a sense of motion\nfrom looking at one picture worse yet\nthe openai\natari library returns a sequence of\nframes\nwhere it could be a random number\nbetween two three or four so to get a\nsense of motion we have to actually\nstack a set of frames and we're going to\nhandle that with this function\nso we'll just keep a running stack of\nframes the current frame to save\nand the buffer size which just tells you\nhow many frames to save so at the top of\nthe episode you're not going to have\nanything to save right there will be no\nstacked frames\nso you want to initialize it\nso it'll be the buffer size by the shape\nof each\nindividual frame\nnext you want to iterate over that\nand say\neach row which corresponds to each image\nin your stack gets assigned to a frame\nso otherwise it's not the beginning of\nthe episode and you want to\npop off the bottom observation\nshift the set of frames down and append\nthe new observation to the end so\ninstead of one two three four it'll be\ntwo three four and then frame five so\nlet's do that\nequals\nsorry zero two buffer size minus one\nso this will shift everything down\nand this will append the current frame\nto the\nend of the stack\nnext we have to do a reshape\nand this is basically to make everything\nplay nicely with the neural network\nall right now we're ready for our main\nfunction\nlet's go ahead and save\nscroll down\ngood grief\nbreakout v0 is the name of the\nenvironment\nthis is just a flag to determine if we\nwant to load a checkpoint\nsorry so yeah epsilon starts out at 1.0\nso the agent takes purely random actions\nour learning rate alpha\nwill be something small like zero zero\nzero two five\nand we've reshaped our input so it needs\nto be 180 instead of 210\n180 by 160 by four because we're going\nto stack four frames\nthe breakout library sorry the breakout\ngame has\nthree actions\nand a memory size of 25 000 which as i\nsaid takes about 48 gigabytes of ram so\ngo ahead and scale that based on however\nmuch you have if you have 16 gigs go\nahead and reduce it by down to something\nlike six or seven thousand\nso the batch size tells us how many\nbatches of memories to include for our\ntraining we'll use 32.\nif low checkpoint is true then we want\nto load the models\nnext thing we need is a way of keeping\ntrack of the scores\nwe will need a\nparameter for the number of games\nstick with 200 to start with a stack\nsize of four\nand an initial score of zero\nso\nthe memory is originally initialized\nwith a bunch of zeros\nthat is perfectly acceptable but another\noption something else we can do is we\ncan overwrite those zeros with actual\ngameplay sampled from the environment so\nwhy don't we do that so\nand the actions are just chosen randomly\nright it's just to give the agent some\nidea of what is going on in the\nenvironment\nso you want to\nreset at the top of every episode\nyou want to pre-process that observation\nyou want to\ngo ahead and stack the frames\nand then player episode\nso there's a bit of a\nquirk here the\nthe agent saves its actions as zero one\nor two but the actions in the\nenvironment are actually one two and\nthree so we have to sample from that\ninterval and then\nadd one take the action subtract one and\nthen save the the transition in the\nmemory just a bit of a kluge\nnot a big problem\nso then we want to\nadd that to our stack\ngo ahead and subtract off one from the\naction\nand store that transition in memory\nand then finally set the old observation\nto be the new one\nlet's go ahead and use a string print\nstatement\nokay\nnow that we've loaded up our agent with\nrandom memories\nwe can actually go ahead and play the\ngame\none thing i like to do is print out the\nrunning average of the last 10 games so\nthat we get an idea of if the agent is\nlearning over not over time or not you\ndo it like this\nand then just print that out\nand i also like to know if epsilon is\nstill\none or if it has actually started to\ndecrease over time\nand we also want to save the models\nevery 10 games\nand if on any other game we just want to\nprint out the episode number\nand the score\nso we can actually scroll up here\ncopy this\nso we're not done\nwhoops\nthere we go\nso instead of a random choice it is\nagent dot choose action\nand that takes in the observation\noh but we did forget to do the same\nthing here\ngrab those\nand\nput them there so the top of every\nepisode we reset the environment\npre-process the observation and stack\nthe frames quite critical\nso\nwe still have to do the same thing with\nadding and subtracting one we want to\nsave the transitions and the only other\ndifference is that we want to learn at\nevery step\nat the end of the episode\nwe want to\ngo ahead and append our score so that\nway we can keep track of our average\nand if you want to get fancy you can go\nahead and add in a plotting function so\nthat when this is done learning you can\nplot the learning over time and you\nshould see an upward slope over time and\nif you want to plot the epsilons you\nshould see a gradual downward slope as\nwell so i'm gonna leave it here because\nthe model is still training on my\ncomputer it's run about three thousand\niterations three thousand games that is\nand it at best it gets two to three\npoints per set of five lives so it's\nreally going to need a couple days of\ntraining to get up to anything\nresembling competitive gameplay but once\nit's done i'll upload another video that\nexplains how q learning works in depth\nand in detail and i'll include the\nperformance from this particular model\nin that video so you can check it out\nthen feel free to run this yourself when\ni finish the model i will go ahead and\nupload the trained version to my github\nso you are free to download it if you\ndon't have\nany hard any decent gpus to train this\nwith go ahead leave a comment down below\nsubscribe i will see you all in the next\nvideo\nwelcome back everybody in this series of\nvideos we're going to code up a deep q\nnetwork in pytorch to play space\ninvaders from the atari open ai gym\nlibrary in this first video we're going\nto code the convolutional neural network\nclass in the second video we'll code at\nthe aging class itself and in the third\nwe'll get to coding the main loop and\nseeing how it all performs let's get to\nit\nso if you're not familiar with the deep\nq network i would advise you to check\nout my earlier video bite size machine\nlearning what is a deep q network it's a\nquick little explainer video that gives\nyou the gist of it within about four and\na half minutes uh if you already know\nthen great we're ready to rock and roll\nbut if you don't\ni'll give you the brief too long didn't\nread so basic idea is that we want to\nreduce some rather large state space\ninto something more manageable so we're\ngoing to use a convolutional neural\nnetwork\nto reduce the representation of our\nestate space into something more\nmanageable and that'll be fed into a\nfully connected neural network to\ncompute the q values in other words the\naction values\nfor any particular given state\nwe're going to leverage two different\nnetworks a network that is going to tell\nus the value of the current state as\nwell as a network to tell us the value\nof the successor states this is going to\nbe used to calculate the values of the\ntarget and which is the purely greedy\naction as well as the behavioral network\nwhich is the current predicted state and\nthe difference between those two is used\nin the loss function for our stochastic\ngradient descent or root mean squared\npropagator\nso let's go ahead and get started um\nwe're going to start as usual\nby doing our imports\nand\nthere are quite a few\nin pi torch\ngreat library\none thing i really like about it is that\nit's highly expressive you don't have a\nwhole lot of boilerplate code as you do\nin something like tensorflow tensorflow\nis enormously powerful uh and if you\nwanna do\num\ncross app type of software then that's\ngoing to be really your only choice\nfor our purposes this is going to be\nprecise you want to use\nand of course numpy\nso\nwe're going to define a class for our\ndeep q network and that'll derive from\nan end module\nthis is kind of how\nuh\npytorch handles everything you want to\nderive your class from the module\nuh the base module uh so that way you\nget access to all of the goodies\nand we'll pass in our learning rate\nalpha for our\nstochastic gradient descent algorithm\nall right so\nthe network is comprised of three\nconvolutional layers and two fully\nconnected layers\nso the first one is just going to be a\ntwo-dimensional convolution that's going\nto take one input channel the reason is\nthat the\nagent doesn't really care about color so\nwe're going to go ahead and make things\neasier reduce our computational\nrequirements by going to a grayscale\nrepresentation\nwe'll take\n32 filters with an 8 by 8 kernel\na stride of 4 and a padding of\n1.\nsecond layer\nis going to be pretty similar\nthis one of course will take 32 channels\nin give 64 channels out\nand do a 4x4 kernel with a stride of two\nand our third convolutional layer is\ngoing to be of course two-dimensional as\nwell takes in 64\noutputs 128 with a 3x3\nkernel\nuh that's it for the convolutions hey if\nyou don't know how convolutional neural\nnetwork works i'm going to go ahead and\nlink a video here that will give you the\nuh\nthe the basic idea of of how those work\nand if you would rather see how uh how\nthis stuff works in text form i don't\nknow if i mentioned this earlier but\nthere is an associated blog article my\nwebsite neuronet dot ai yeah i bought\nthat domain i'll link it in the\ndescription go ahead and check it out\nplease\nso our fully connected layer is the\nfollowing\nand we're going to have to do a little\nbit of magic here so\nafter running this stuff a bunch of\ntimes\nuh i know the dimensionality of the\nimages we're going to of the\nconvolutions we're going to get out and\nwhat we want to know is\nwe're going to be taking a\nset of filters uh you know convolved\nimages that have had filters applied to\nthem 128 to be exact and we want to\nunroll them into a single flattened\nimage right to feed into our neural\nnetwork so that's going to be 128\nchannels that are 19 by 8 in size and\nthat's a magic number i got from just by\nrunning the code in trial and error\nand then our output layer is going to\ntake in those 512\nunits and output 6 y6 that's because we\nhave a total of 6 actions in the\ngame of space invaders you have a total\nof 6 actions which are the subset of the\nfull 20 available in the atari library\nbasically the agent can do nothing it\ncan move left it can move right it can\nshoot while standing still and it can\nmove left and right and shoot\nwhen we get to the main video i'll go\nahead and show all those actions\nwe also need an optimizer\nequals optim\nrms prop\nand we want to tell it to take the\nparameters of our object for the weights\nand our learning rate is going to be\nalpha that we've passed in\nour loss function is going to be a mean\nsquare error loss\nand the components of that will be the\ntarget and the current predicted q\nfunctions and of course the target is\njust the greedy value right because q\nlearning is an off policy method that\nuses\na\nepsilon greedy behavioral policy to\nupdate the purely greedy target policy\nanother thing we have to do in\npi torch is tell it which device we're\nusing so t.device\nand that'll be cuda zero\nif t dot cuda is\navailable\nelse\ncpu\nand what this is telling it is that if\nthe gpu is available go ahead and use it\nwhich is of course always preferable\notherwise just use the cpu\nand we\nalso have to tell it\nto actually\nsend the network to the device\num maybe i'm using 0.4 i don't know if\nin 1.0 that's actually required but as\nof the time i'm coding this up it's\nrequired so\nuh something we have to live with\nokay so this is the sum and the whole of\nour network\nonly thing where that remains to do is\nto feed forward our observation\nand that will take\nthe\nopposite no not observation observation\nnot only can i not type i cannot speak\nas well\ni suck at life\ni don't know why you guys listen to me\nso what we want to do is\nthe observation vector uh is the\nwe're going to use a sequence of frames\nand those frames are the reduced images\nfrom the screen so when we get to the\nthird video on\num\n[Music]\non actually playing the game you'll see\nthat you don't need the full frame to\nactually figure out what's going on you\ncan actually truncate it to get a really\ngood idea\nin particular the agent can only move\nleft and right a certain\namount so we go ahead and truncate the\nsides uh you don't need the score at the\ntop because we're keeping track of the\nscore ourselves you don't need some of\nthe bottom so you can actually truncate\nthe image a little bit to reduce your\nmemory requirements and we're going to\nconvert it into a grayscale so we get\nrid of two we're just going to average\nthe three channels to make them into one\nand we also have to pass in a sequence\nof frames right because if i only show\nyou one frame of video game you can't\ntell what's going on you don't get a\nsense of motion from a single frame\nright you need at least two to get a\nsense of motion uh we'll be using three\ni believe in the original implementation\nof this algorithm for um\nfor deepmind they used four we're going\nto go ahead and use three just to be\ndifferent\num\ni haven't i suspect that's a\nhyperparameter it's not something i've\nplayed with i encourage you to do that\noh another thing is that\nwe must train this on a gpu if you try\nto do it on your cpu it's going to take\n7 000 years so\nuh if you do not have access to a gpu go\nahead and leave a comment below and i\nwill find a way to get my pre-trained\nmodel to you so you can go ahead and\nplay around with this\ni have a 1080 ti and not not top of line\nanymore but it's still quite good for\nthis particular apps application\nso\nback to the topic we have to\nconvert our sequence of frames our obser\nobservation to a tensor and we\nalso want to send that to the device uh\nthis is a peculiarity of pi torch as far\nas i'm aware i'm not the world's leading\nexpert but as far as i understand it you\nhave to tell it to send the network as\nwell as the variables to the device as\nwell\nnot a big deal just something to be\naware of\nnext thing we have to do is resize it so\nwhen you are given a sequence of frames\nin the open ai gym and really any other\nformat of image is going to be\nheight and width by channels whereas if\nyou look up here\nthis actually expects the channels to\ncome first\nand so we have to reshape the array to\nto accommodate that\nso\npytorch's built-in function for that is\nview\nwe want a minus one to handle any number\nof stacked frames\none channel in the beginning and we're\ngoing to pass in 185 by 195 image\n[Music]\nthat doesn't seem right actually sorry\nit's not 195 i'm like the original image\nis 210 by 160 it can't be 180 595\ntotal brain fart there it's 185 by 95\nokay so we have\ntaken in our sequence of frames we've\nconverted it into a tensor and flat and\nconverted it into the proper proper\nshape for our network the next thing we\nwant to do is\nactivate it and feed it forward so\nwe call the first convolutional layer on\nour observation\nand we use the value function to\nactivate it\nand we\nstore that in the new variable\nobservation\nand then\nwe learn how to type and then we do the\nsame thing with that output\npass it through the second convolutional\nlayer\nand then we do the same thing again we\npass it to the third convolutional layer\num\nand we're almost home free so the next\nthing we have to do\nis\nwe are outputting a 128 by 19 by eight\noh\nset of arrays\nthe next thing we have to do is\noh by the way that noise is my pomodoro\ntimer if you're not using the pomodoro\ntechnique i highly recommend it\nit's an enormous productivity hack but\nanyway\nnext thing we have to do is\ntake our sequence of\nconvolved images and flatten them to\nfeed into our\nfully connected neural network so we\nagain use the view function\nuh to do that not minus one for whatever\nnumber of frames we pass in\n19 by eight is what we get out and i\nfound that out through experimentation\nand we know it's 128 channels because we\ndictate that\nhere right here 128 output channels\nso boom we've flattened it and the only\nthing that remains is to feed it forward\nand we'll use a value activation\nfunction there\nfc1\nfully connected layer one\nand then\nit's not observation but it's actions\nself dot\nfc2 observation so we'll feed it through\nthe final\nfully convolved layer and store that as\na variable called actions and go ahead\nand\nreturn it\nand the reason we're doing that is\nbecause\nwhat we're getting out is a q value for\neach of our actions and we want to pass\nthat back now keep in mind that we're\npassing in a sequence of frames and so\nwe're going to get back is a\nmatrix it's not going to be a single\narray of six values it's going to be six\nvalues times whatever number of rows of\nimages you pass in so if we pass in\nthree images it's going to have three\nrows and six columns and that'll be\nimportant later when we actually get to\nchoosing the actions\nspeaking of which i'm going to cut it\nshort here we've already i've already\nrambled for about 12-13 minutes uh in\npart two we're going to take a look at\ncoding of the agent class i have\nstructured it this way because the agent\nactually has two networks so it made\nsense to kind of stick the network in\nits own class\nwe'll get to coding up the agent's init\nfunction how to handle its memory how to\nstore transitions how to choose actions\nand how to actually implement the\nlearning function that's a much longer\nproject so we'll stick that in its own\nvideo and then in part three we're going\nto get to actually coding up the main\nloop and seeing how it performs hey if\nyou liked the video make sure to like\nthe video hey if you don't like it go\nahead the thumbs down i don't really\ncare let me know what you think\nquestions comments leave them down below\nif you made it this far please consider\nsubscribing i look forward to seeing all\nof you in the next video\nwelcome back everybody in the previous\nvideo we got to coding the convolutional\nneural network class for our deep q\nlearning agent that's going to play\nspace invaders if you haven't seen that\nyet go ahead and click the link to go\nahead and watch that first otherwise you\nprobably won't know what's going on\nif you're if you're the type of person\nthat would prefer to have a\nwritten set of instructions go ahead and\nclick the link down below i'll link to\nthe associated blog article for this\nparticular tutorial series\nwhen we last left off we just finished\nreturning the set of actions which is\nthe set of q values for our sequence of\nframes\nso of course in this video we're going\nto go ahead and get started\ncoding up the agent class which is where\nall the magic is going to happen\nuh oh and of course as always i have\nleft the code for this in my github\nunder the youtube directory it gets its\nown directory because there's a few\ndifferent files here\ni'll link that below as well and if you\naren't following me on github you\nprobably should because that's where i\npost all of my stuff\nokay next up we have the\nagent class right\nand this is just going to derive from\nthe base object class nothing fancy here\nwe need a basic init function\nand this is going to take gamma which is\nour\ndiscount factor so the agent\nhas a choice of how to value future\nrewards\nin general gets discounted by some value\nbecause a reward now is worth more than\na reward in the future\njust like with us we need epsilon for\nepsilon greedy action selection\nthe alpha for the learning rate\nthe max memory size\na\nvariable to keep track of how low we\nwant epsilon to go\nsomething to keep track of how long we\nreplace of how often we're going to\nreplace our target network i'll get to\nthat in a moment\nfor in a few minutes\nand the action space and that's just a\nlist\nof variables from zero through five\nthose correspond to all the possible\nactions for our agent\nand you just set these\nto the appropriate variables\nuh\nbeing careful not to turn on your caps\nlock key\nso what these all are are just hyper\nparameters for our agent we don't need\nto store alpha because we're just going\nto pass it into the network\nand then never touch it again\nstoring the action space allows us to\naccommodate the epsilon greedy action\nselection later\nthe mems size\nis\nused for\nefficiency purposes so we're going to\nkeep track of state action reward\ntransitions you don't want to store an\ninfinite number of them you only want to\nstore a subset\nyou don't need to store all of them\nanyway there's no real practical benefit\nsince we're just sampling a subset\nanyway so\nwe just use some rather large\nmemory um to keep track of all of the\nstate transitions\nthat we care about\nkeep track of how many steps\nand the learn step counter\nthat is\nto keep track of how many times the\nagent has called the learn function that\nis used for target network replacement\nif you're not familiar with it target\nnetwork replacement is when you swap the\nparameters from the evaluation network\nto the\ntarget network\nmy experience has been that it doesn't\nactually help things so i'm not going to\ndo it i'm going to code it in because\nit's\nan important part of the topic but in\nthis particular case i haven't found it\nto be helpful\nbut i haven't played with it too much i\nsaw that it does quite well without it\nso why break it\nso i'm going to store the memory as a\nlist and the reason i'll use a list\ninstead of\na numpy array is because the\nassociated cost of stacking numpy rays\nis really really high so it's much much\nfaster to store a list of lists and then\nconvert to a numpy array when you learn\nand that's much faster than keeping\ntrack of a set of numpy arrays and just\nstacking them the stack operation is\nincredibly computationally prohibitive\nso\nfor something like this that's already\ncomputationally expensive doesn't make\nany sense\n[Music]\nand keep track of the total number of\nmemory stored so that way you don't\noverwrite the array\nwe want to keep track of how often we\nwant to replace the target network\nand then we need our two networks qe val\nand that is just\npassing the alpha that is just the\nagent's estimate of the current set of\nstates\nand q next is the agent's estimate of\nthe successor set of states so recall in\ndeep q learning\nwe calculate the value the max value of\nthe successor state as our greedy action\nthat's our our actual target policy and\nour behavior policy that we used to\ngenerate data is epsilon greedy\nthat's it for our constructor\nthe next thing we have to worry about is\nstoring memory transitions\nso\ntransition\nso we are interested in the current\nstate\nthe action taken\nthe reward received\nand the resulting state\nbecause those are the things that that\nallow the agent to learn so\nwe have to make sure that we\naren't over our memory so if the\nmem\ncounters less than self.mem size\nthen just go ahead and append that\nas a list\nand again we're doing this because\nit is much cheaper computationally to\nappend a list than it is to actually\nstack a numpy array\nif we have\nfilled up our memory then we want to\nuh overwrite the position in memory that\nis uh determined by the modulus\ndot mem size so this will guarantee we\nare bounded by zero all the way up to\nour mem size\nand of course this is a list of lists\nit's just an action reward state\nunderscore\nand we want to increment the memory\ncounter\npretty simple huh pretty straightforward\nnext up we have the\nfunction to choose an action and this\nwill take the observation\nand again just to remind you as we\ndiscussed in the first video we're\npassing in a sequence of observations\nbecause we want to capture some\ninformation about the temporal\ndependence of what's going on again with\none frame you can't tell if the aliens\nare moving left or right and so you\ndon't know if you should move left or\nright\num of course you know which way your\nbullets go you know which way there is\ngo but\nas far as movement is concerned you need\nat least one frame\nso\nas always\nwe're going to be using\nnumpy to calculate a random number for\nus to our epsilon greedy action\nselection\nand you want to get the\nvalue\nof all of the actions for the current\nset of states itself\nq eval dot forward\nso what we're doing now is forward\npropagating that stack of frames through\nthe neural network the convolutional\nneural network and the fully connected\nlayer to get the value\nof each of each of the actions given\nthat you're in some set of states\ndenoted by the observation\nso if rain is less than one minus\nepsilon\nthen\nyou want to take the\narg max write the maximum action\nand recall that we have stored the\nactions as a matrix they're returned as\na matrix because you have\nthe number of rows that correspond to\nthe number of frames you pass in and\neach of the columns correspond to each\nof the six actions\nso you want to take the\nfirst\nfirst axis\nand if you're taking a random action\nthen you just choose something at random\nfrom the action space list\nand\ngo ahead and increment your steps\nand return the action that you've chosen\nthe way i the reason i use one minus\nepsilon is um\n[Music]\nyou can use the probability epsilon\nyou can use probability one minus\nepsilon this is really more of a soft uh\nsoft epsilon kind of strategy\nrather than purely greedy but it doesn't\nreally matter this is going to give you\nthe probability of choosing the maximum\naction\nof epsilon plus epsilon over six because\nthey're\nof course the greedy action is the\nsubset of all actions so there's a one\nover six probability that when you take\nthe\nquote-unquote non-greedy action you'll\nactually end up getting the greedy\naction\nall right\nnext thing we have to worry about is how\nthe agent is going to learn and this is\nreally the meat of everything so\nwe are doing batch learning so you want\nto pass in a batch size\nand\nthe reason we're doing batch learning is\num\n[Music]\na number of different reasons so you\nwant to break correlations\nuh state transitions you you\nit's even okay if the the batch overlaps\ndifferent episodes what you want to get\nis a good sub sampling\nof\nthe\noverall\nparameter space so that way you don't\nget trapped in a local minima so you\nrandomly sample these\nstate transitions through your memory\notherwise you could end up if you replay\nthe whole memory you could end up\ngetting trapped in some local minimum\nit's a way to improve the efficiency of\nthe algorithm to converge to a purely\noptimal strategy excuse me\nand thursday\nall right\nfirst thing we have to do since we're\nusing batch learning\nis uh\nwe have to zero out our gradients and\nwhat this means is that\nthe gradients can accumulate from step\nto step the network like pi torch\nlibrary can keep track of it we don't\nwant that if you do that if you don't do\nthe zero grad then you'll end up with\num\nbasically accumulating for every single\nbatch and that's really full learning\nrather than batch learning\nnext thing we have to check for is\nif we're going to replace a target\nnetwork\nso if it's not none and\nif it's time to do it\nrelease now replace target\naccount equals zero then we want to\nactually replace our target network what\nwe do there is\nwe take advantage of the pi torch\nrepresentation of our network\nin that case it's just\nwe can convert our entire network into a\ndictionary which is really cool so\nitself\nqnext dot load state dict\nso it's going to load\nuh a state to the network from a\ndictionary\nwhich network the evaluation\nnetwork\nwhich we're going to convert to a state\ndictionary\nwe're not actually going to use this in\nour implementation but i include it here\nfor completeness\nuh next up uh we want to know\nuh we want to select a random sub-sample\nof the memory so\nwe want to make sure we don't go all the\nway past the end of our array\nso if\nour current memory counter plus batch\nsize is less than our total memory\nwell then we're free to go ahead and\nselect\nany point\nof our memory\nbecause we know we're not going to go\nbeyond it\nrange\notherwise if\nthere is an overlap then we want memstar\nto just be an int in the range\n[Music]\ngood grief\ni hate that\nminus one just to be safe\nuh okay so\nthat is how we choose where to start\njust a random number somewhere between\nzero and the max memory if we have\nenough leftover\nin the memory for the bat to accommodate\nthe batch otherwise subtract that off\nand select something from that subset\nthen we're going to go ahead and get\nthat mini batch\nand convert that to a\nnumpy array so\ni suspect this is not the most efficient\nway of doing this\nand the reason is that\nyou run into some difficulties in the\nway in which you pass things into the\ntorch library it has a certain set of\nexpectations that\nare kind of finicky um not to say it's\nbad it's just something that i wasn't\nuh expecting but\nit works nonetheless\nso what we want to do next is feed\nforward both of our networks we want to\nknow what is the value of our current\nstate and what is the value of the\nsuccessor state after we take the action\nwhatever action we're looking at in our\nbatch\nso that's just q eval forward\nwhat are we feeding forward\nhere's where we got some hackiness we\ngot to convert it into a list and the\nreason is that our\nmemory is a numpy array\nof numpy objects because the\nobservation vectors are numpy objects so\nif you don't convert it into a list\nthen you have a numpy array of nd array\nobjects and tensor\npi torch will complain\nwe don't want tensor we don't want pi\ntorch to complain so\nwe want to access all\nall rows\nright our entire batch and the state is\njust the zeroth element\nand you want\nall of the variable you want all of the\npixels\nso you need the second set of columns\nthere and\nwe want to send this to the device\nself dot\nq eval dot device\nand this ensures that the\nthis network this set of variables gets\nsent to our gpu as well\nthe next thing we need is the\nvalue of the successor states\nand that is just all the\nrows\nthird column\nand all of the\num all of the\nmembers of that array\n[Music]\nand here i've just\nuh\ni gotta quit using the visual studio\ncode this is annoying but um\nscroll up here so all i have done here\nis\num i'm putting it on qeval.device\nbecause the device for both the\nevaluation and the\nuh\nnext network are the same so i only have\none gpu if i had two gpus then this\nwould matter\nit doesn't matter so\nyou can just call it that\nnext thing we want to know is what is\nthe max action for our current for our\nnext state right because the\nupdate rule actually calculates takes\ninto account the purely greedy action\nfor the successor state uh maybe if i\ncan find an image of it i'll flash it\nhere on the screen to make life easy but\nnext thing we have to know is the max\naction\nand we use the arg max function for that\nand remember we want to take the first\ndimension because we're\nthe actions q next q predicted and q\nnext are actually our actions that's\nwhat we get from the feed forward\nand that is batch size times\nnumber of actions and we want the number\nof actions so that's first dimension\nand i am a little bit\nin or attentive here about sending it to\nthe device just to make sure nothing\ngets\noff the device because this stuff slows\nto a crawl if you run on the cpu\nwe need to get our rewards\nand that is uh obtained from our memory\nall the rows and the second element\ntoo many parentheses\nand\none thing we want is our loss function\nto be zero for every action except for\nthat max action so\nwe have q target equal q predicted\nbut we want q target for all of our our\nentire match but the max action\nto be rewards plus self dot gamma times\nthe actual value of that\naction\nso t.max just gives you the value of the\nmaximum element and argmax gives you the\nindex\nand you want to find the maximum action\nthe value of it\nso those are our target and predicted\nvalues that's what we use to update our\nloss function the next thing we have to\nhandle is the epsilon decrement so we\nwant the agent\nto gradually converge on a purely greedy\nstrategy for its\nbehavior policy\nand\nthe way you do that is by gradually\ndecreasing epsilon over time i don't\nlike to let it\nsimply go to\num start decreasing right away so i have\nsome set number of steps i let it run\nfirst\nand i use a linear decrease over time\nyou can use exponential\nquadratic you can use whatever form you\nwant\nit's not\nit's my knowledge it's not super\ncritical but i could be completely wrong\nthis seems to work as we'll see in the\nthird video\nso if we don't have enough left then\njust set it to epsilon end\nscroll up here\nand we're almost done so we have\neverything we need we have everything we\nneed to compute our loss which is this\nq target\nand q predicted\nthose are the values of q predicted is\nthe value of the current set of states\nand q target is related to q next right\nit's the q target is the max action\nfor the next successor state\nand so we're almost there so the loss\nit's just\nthe mean squared error loss if you\nrecall from the first video where we\ndefined the\nloss function\nand\nas is the\ntorch style you have to send it to the\ndevice\nand then we want to back propagate with\nloss backward\nand we just want to step\nperform one iteration and go ahead and\nincrement our learn step\ncounter\nand that's basically it so there was a\nlot of code there let's go back and take\na look really quick so first thing you\nwant to do is zero your gradients\nso that way we're doing actual batch\noptimization instead of full\noptimization\nthen we want to check to see if we're\ngonna if it's if we are going to replace\nthe target network and if it is time and\nif it is load the state dictionary from\nthe q eval on to the q next network\nnext up\ncalculate the start of the bat of the uh\nmemory sub sampling i'm making sure to\nget some subset of the array\ngo ahead and sample that batch of memory\nand convert it to a numpy array\noh pomodoro timer\nso if you guys\naren't using the pomodoro method\nto work i highly recommend it go look\nthat up if you don't know what it is but\nanyway\nconvert that to a numpy array and then\ngo ahead and feed forward the\nthe current state and the successor\nstate\nusing the memory sub sample\num making sure it is sent to your device\nnext thing we have to know is the\nmaximum action for the successor state\nand calculate the rewards that the agent\nwas given set the q target to q\npredicted because you want the loss for\nevery state except\nthe loss for every action except the max\naction to be zero\nand then update the value of q target\nfor the max action to be equal to\nrewards plus gamma times the actual\nvalue of that max action\nnext up\nmake sure that you're using some way of\ndecrementing epsilon over time such that\nit converges to some small value\nthat\nmakes the agent\nsettle on an mostly greedy strategy in\nthis clay in this case i'm using five\npercent of the time for a\nrandom action\nfinally go ahead and calculate the loss\nfunction back propagated\nstep your optimizer and increment your\nstep counter\nand that is all she wrote for the learn\nfunction and the aging class slightly\nmore code than in the network class but\nstill fairly\nwe have a typo there still fairly\nstraightforward\ni hope this has been informative and in\npart three we're going to go ahead and\ncode up the main loop and see how all\nthis performs i look forward to seeing\nyou in the next video\nany comments questions suggestions go\nahead and leave them below if you made\nit this far please consider subscribing\ni look forward to seeing you all in the\nnext video\nand welcome back everybody to part three\nof coding a deep q learning agent in the\nopen ai gym atari library\nuh in parts one and two we took a look\nat the\ndeep neural network class as well as the\naging class for our agent and in part\nthree we're going to finally put it all\ntogether into the main loop to play the\ngame and see how our agent does\nlet's get started\nso we begin as usual with our typical\nimports we're going to need the gym of\ncourse\nand we're going to import our\nour model class so from model we'll\nimport\ndeep\nsorry dq model\nand agent\nand\ni also have a utility function i'm not\ngoing to go over the code it's just a\ntrivial function to post the uh to print\nto plot\nthe\ndecaying epsilon and the running average\nof previous five scores\nfrom utils import\nplot learning\nand\nuh oh and by the way so\nif you haven't seen parts one and two go\nahead check those out\nand\nif you want the code for this please\ncheck out my github if you would like to\nsee a\nblog article that details all this in\ntext if you missed something in the\nspeech\nthen go ahead and click the link down\nbelow\nso it's giving me some issue\nuh oh it's not deep cue model this is\nthe the downside of talking and typing\nat the same time i'm not that talented\nso\nuh we want to go ahead and make our\nenvironment\nand that space invaders\nv0\nuh another thing to know is that there\nare implementations of the environment\nwhere instead of being passed back an\nimage of the screen you're passed back\nlike a ram image something like that\ni've never used it sounds kind of\ninteresting it might be something for\nyou to check out and leave a comment\ndown below if you've played with have\nany experience with it\nor if you think it sounds cool\nso we want to make our agent and i'm\ngoing to call it brain big brain\npinky in the brain baby gamma i have\n0.95\nan epsilon of 1.0\nand\ni'm using epsilon 1.0 because it starts\nout taking purely random actions and\nconverges on a mostly greedy strategy\nlearning rate of 0.03\nmax memory size\n5000 transitions and\nwe're not going to do any replacement\nso you may have noticed uh when we were\nbuilding our agent that the memory was\ninstantiated as an empty list\nif you're going to use numpy arrays one\nthing you would do is just create an\narray of zeros and the shape of you know\nyour images\nas well as the total number of memories\ni'm going to do something slightly\ndifferent so\none way to help agents learn is to\nactually have them watch videos of\nhumans playing and in fact the\nuh deepmind team taught\nalpha\nalpha zero go to play by showing a board\nconfigurations and saying which which\nplayer won so you it's perfectly\nlegitimate to leverage the experience of\nhumans i'm not going to play the game\nfor the agent but what i'm going to do\nis allow the agent to play the game at\ntotally random he's just going to play a\nset number of games to fill up its\nmemory uh using totally random actions\nso it's a bit of a hack but i kind of i\ndon't know to me it seems legitimate but\nsome people may have frowned upon it i\ndon't really care\nit's how i chose to solve the problem\nand it seems to work fairly well\nso brain dot mem size we have to\nreset your environment\nreset your done flag\nand play an episode\nso here i'll let you know the action so\nzero is no action\none is fire two is move right\nthree is move left\nfour is move right fire\nfive is move left fire so that's zero\nthrough five total of six actions\nchoose one at random\nenv.actionspace.sample\nif you want to verify that these do that\ngo ahead and code up a simple loop you\nknow while loop while not done\ntake action zero and render it and see\nwhat it does\nso next you want to go ahead\nand\ntake that action\nand the other thing i do is\nthe\num and i'm on the fence about doing this\ni haven't tested it but in my experience\nwith other environments\nin more basic algorithms that\nmy experience is that it makes sense to\npenalize the agent for losing\nso\nuh you don't want the agent will\nnaturally try to maximize its score but\nyou want it to know that losing is\nreally bad so\nif you're done and the\nand this may be something i change on\nthe github so if you go to the github\nand see this isn't there it means that i\ntested it and decided it was stupid and\nbut as of right now i think it's okay\ni'm always open to change my mind though\nso i want to let it know that\nlosing really really sucks\nand i want to store that transition\nand\nuh here's a bit of a magical part so\nas i said in the in the first video in\nthe convolutional neural network we want\nto reshape it down from three channels\ninto one because the\nthe asian doesn't really care about\ncolor it only cares about\nis an enemy there or not right and it\ncan get that information from black and\nwhite so i'm going to take the mean over\nthe three channels\nto get\ndown to a single channel and i'm also\ngoing to go ahead and truncate it and\nthe reason is that there isn't a whole\nlot of information around the borders of\nthe screen that the agent needs so we\ncan get away with reducing our memory\nrequirements\nwithout losing anything meaningful and\ni'm going to go ahead and flash in some\nimages here of what that looks like\nbut what i'm going to do is take the ops\nobservation vector and i'm going to take\n15 to\n230 to 125\nand the mean is performed over access to\nand we also want to store our action and\nreward\nuh you guys can't see that there so we\nstore the action and reward as well as\nlet's go ahead and copy this\nwe also want to copy\nwe also want to store\nthe successor state\noh good grief\nlife is so hard there we go\nand that's observation underscore which\nis which is the successor state\nand then\nset your observation to observation\nunderscore\nand then when you're done\njust let yourself know\nokay\nokay and we are almost there so next\nthing you want to do is keep track of\nthe scores you want to know how well the\nagent is doing\num\ni have this variable epsilon history oh\nuh keep track of the\nhistory of epsilons as it decreases over\ntime because we want to know the\nrelationship between the score and the\nepsilon\nand we'll run it for\n50 games\nand you'll take a batch size of 32\nmemories\nthe batch size is it an important hyper\nparameter but what you find is that\nusing a larger batch size may get you a\nlittle bit better performance it slows\ndown training tremendously so on my\nsystem with an i7 7820x 32 gigs of ram\n1080 ti batch size of 32\nmeans that 50 games is gonna run in\nabout half an hour and it takes quite a\nbit of time\nusing a larger batch size doesn't seem\nto produce a whole lot better\nperformance\nbut it certainly slows it down by more\nthan a factor of two so\nthere's non-linear scaling there\nand we want to know\nthat we're starting game\ni plus one with an epsilon of something\ndot uh say four significant figures\nright not epsilon\nand we want to go ahead and append\nthe\nagents epsilon at the beginning of the\nepisode\nreset our done flag\nand reset our environment\nand okay so the next thing we want to do\nis as i said\nwhy is this unhappy\ninvalid syntax\noh no i've offended it what have i done\noh forgot a comma\nthere we go so next thing we want to do\nis construct our sequence of frames as i\nsaid we're going to pass in some\nsequence of frames to allow it to get\nsome conception of movement in the\nsystem so\ni have a rather ugly way of doing this\nas usual\nbut the first thing i want to pass into\nit is the\nfirst\nthe first\nobservation vector from the beginning of\nthe game\nand i have broken something again\nwhat have i broken\nframes done by some\noh of course\nof course okay\nso the score for this episode is zero\num\ni want to keep track of the last action\nso\nthis is something i'm not sure about i\nmust confess to you guys the\ndocumentation on the open ai gym is\nrather lackluster what it says is that\neach action will be repeated for k\nframes where k is the set two three or\nfour\nuh so i guess it gets repeated some\nrandom number of times so\nsince i don't know how many times and i\nwant to keep passing in a consistent set\nof observation vectors of frames\ni will\ndo something hacky so i'll keep track of\nthe last action and i will only update\nmy action every third action so i want\nto pass in a sequence of three frames\nand repeat the action three times i'm\nkind of forcing the issue\nagain it seems to work i don't think\nit's the best implementation but this is\nyou know just my quick and dirty\nimplementation\nso\nif we have three frames\ngo ahead and choose an action based on\nthose and reset your frame variable\nto an empty list\notherwise\ngo ahead and do what you just did\nscroll down\nso then we want to go ahead and take\nthat action\nkeep track of our score\n[Music]\nand append\nour new\nobservation\nuh\ni'm just gonna\ncopy that\nyeah\ncopy that and then\num i am going to go ahead and tell it\nthat losing is very bad we don't like\nlosers\nand this al dot lives thing is the\nnumber of lives the agent has\nale is just the\num emulator in which the opening atar\nopenai jim's atari library is built\nand next we have to store a transition\ni'm going to copy that code precisely\nbecause i have fat fingers and\napparently screw things up so\ncopy that\nand then\n[Music]\nunderscore\nbrain.learn\nbatch size\nkeep track of our action\nand here you can put in a render if you\nwant to see what it's doing\num if not then\nat the end of the\nepisode end of the episode\nyou want to append a score\nwhich we're going to plot later\nand i like to print the score so i can\nkind of get an idea of how the agent's\ndoing\nand\nwe need to make a list of the\nx variable\nfor our plotting function\nagain for the plotting function\njust go ahead and check out my github\nfor that\nthat's the easiest way\nand\ni'm going to set a\nfile name\n[Music]\ni'm just gonna call it test for now\nplus string um\nnum game something like that\nuh\nplus\ndot png\nso we have a file name then we're going\nto plot that\nwe want to plot the scores the epsilon\nhistory and the file and pass in the\nfile name so that it saves it\nand that's all she wrote for the\nfor the main loop uh\ni've gone ahead and run that so i'm\ngoing to go ahead and flash in the\nresults here\nso as you can see the epsilon decreases\nsomewhat linearly over time not somewhat\ncompletely linearly and as it does so\nthe agent's performance gradually\nincreases over time and keep in mind\nhere i am plotting the previous uh the\naverage of the previous five games\nthe reason i do that is to account for\nsignificant variations in game to game\nplay right so the agent is always going\nto choose some proportion of random\nactions and that means it can randomly\nchoose to move left or right into the\nenemies bullets so there's always some\ngames that's going to get cut short\nso\nthe the general trend in the performance\nis up until the very end when it\nactually takes a bit of a dive and i've\nseen this over many many different\niterations i suspect this has to do with\nthe way that it is navigating through\nparameter space it'll find pretty good\npockets and then kind of shift\ninto a related\nother local minima which isn't quite as\ngood\nif you let it run long enough this will\neventually go back up\nbut it does have some oscillatory\nbehavior to it but you can see that it\nincreases its score quite dramatically\nand uh in this particular set of runs i\nsaw scores in excess of 700 points which\nis actually pretty good for an agent um\nso let's go ahead and take a look at\nwhat it looks like with the target\nnetwork replacement\nso here you can see a dramatically\ndifferent behavior so in this case uh\nepsilon decreases and the score actually\ndecreases over time\nand uh\nyou know actually i don't quite know why\nit does this so the\nthe oscillations down there are almost\ncertainly from the target network\nreplacements uh it could be a fluke but\ni have run this several times where i\nsee this type of behavior where uh with\nthe target network replacement it\ntotally takes a nosedive i don't think i\nscrewed up my implementation please\nleave a comment below if you saw\nsomething it looks off but as far as i\ncan tell it looks it's implemented\ncorrectly you just copy one state dick\nto another no real big mystery there uh\nbut that's why i choose to leave it off\nyou get a significant variation in\nperformance um\nmore of the stories to go ahead and\nleave the\ntarget network replacement off and uh\nthat's it for this series so we have\nmade an agent to play the atari uh atari\ngame of space invaders uh you by\nuh gradually decreasing epsilon over\ntime we get really good performance\nuh several hundred points in fact\nactually learns how to play the game\nquite well i'm probably going to go\nahead and spin in a video of it playing\nhere so you can see how it looks\num if this has been helpful to you\nplease consider subscribing go ahead and\nleave a comment below if you have one a\nquestion suggestion anything uh go ahead\ni answer and read all my comments um\nand uh go ahead and smash that like\nbutton guys so i hope to see you all in\nthe next video\nand uh take care\nin this video i'm going to tell you\neverything you need to know to start\nsolving reinforcement learning problems\nwith policy gradient methods\ni'm going to give you the algorithm and\nthe implementation details up front and\nthen we'll go into how it all works and\nwhy you would want to do it let's get to\nit\nso here's a basic idea behind policy\ngradient methods\na policy is just a probability\ndistribution the agent uses to pick\nactions so we use a deep neural network\nto approximate the agent's policy the\nnetwork takes observations of the\nenvironment as input and outputs actions\nselected according to a softmax\nactivation function\nnext generate an episode and keep track\nof the states actions and rewards in the\nagent's memory\nat the end of each episode go back\nthrough these states actions and rewards\nand compete and compute the discounted\nfuture returns at each time step use\nthose returns as weights and the actions\nthe agent took as labels to perform back\npropagation and update the weights of\nyour deep neural network\nand just repeat until you have a kick\nass agent simple yeah\nso now we know the what let's unpack how\nall this works and why it's something\nworth doing\nremember with reinforcement learning\nwe're trying to maximize the agent's\nperformance over time\nlet's say the agent's performance is\ncharacterized by some function j and\nit's a function of the weights theta of\nthe deep neural network\nso our update rule for theta is that the\nnew theta equals the old theta plus some\nlearning rate times the gradient of that\nperformance metric\nnote that we want to increase\nperformance over time so this is\ntechnically gradient ascent instead of\ngradient descent\nthe gradient of this performance metric\nis going to be proportional to some\noverstates for the amount of time we\nspend in any given state and a sum over\nactions for the value of the state\naction pairs and the gradient of the\npolicy where of course the policy is\njust the probability of taking each\naction given we're in some state\nthis is really an expectation value and\nafter a little manipulation we arrive at\nthe following expression\nwhen you plug that into the update rule\nfor theta you get this other expression\nthere are two important features here\nthis g sub t term is the discounted\nfeature returns we referenced in the\nopening and this gradient of the policy\ndivided by the policy is a vector that\ntells us the direction and policy space\nthat maximizes the chance that we repeat\nthe action a sub t\nwhen you multiply the two you get a\nvector that increases the probability of\ntaking actions with high expected future\nreturns\nthis is precisely how the agent learns\nover time and what makes policy gradient\nmethods so powerful\nthis is called the reinforce algorithm\nby the way\nif we think about this long enough some\nproblems start to appear\nfor one it doesn't seem very sample\nefficient\nat the top of each episode we reset the\nagent's memory so it effectively\ndiscards all its previous experience\naside from the new weights that\nparameterize its policy it's kind of\nstarting from scratch\nafter every time it learns\nworse yet if the agent has some big\nprobability of selecting any action in\nany given state how can we control the\nvariation between the episodes\nfor large state spaces aren't there way\ntoo many combinations to consider well\nthat's actually a non-trivial problem\nwith policy gradient methods and part of\nthe reason our agent wasn't so great at\nspace invaders\nobviously no reinforcement learning\nmethod is going to be perfect and we'll\nget to the solution to both of these\nproblems here in a minute\nbut first let's talk about why we would\nwant to use policy gradients at all\ngiven these shortcomings\nthe policy gradient method is a pretty\ndifferent approach to reinforcement\nlearning\nmany reinforcement learning algorithms\nlike deep q learning for instance rely\non estimating the value of a state or\nstate action pair\nin other words the agent wants to know\nhow valuable each state is so that its\nepsilon greedy policy can let it select\nthe action that leads to the most\nvaluable states\nthe agent repeats this process over and\nover occasionally choosing random\nactions to see if it's missing something\nthe intuition behind epsilon greedy\naction selection is really\nstraightforward\nfigure out what the best action is and\ntake it\nsometimes do other stuff to make sure\nyou're not wildly wrong\nokay that makes sense but this assumes\nthat you can accurately learn the action\nvalue function to begin with\nin many cases the value or action value\nfunction is incredibly complex and\nreally difficult to learn on realistic\ntime scales\nin some cases the optimal policy itself\nmay be much simpler and therefore easier\nto approximate\nthis means the policy gradient agent can\nlearn to beat certain environments much\nmore quickly than if it relied on an\nalgorithm like deep q learning\nanother thing that makes policy gradient\nmethods attractive is what if the\noptimal policy is actually deterministic\nin really simple environments with an\nobvious deterministic policy like a grid\nworld example keeping a finite epsilon\nmeans that you keep on exploring even\nafter you've found the best possible\nsolution\nobviously this is sub-optimal\nfor more complex environments the\noptimal policy may very well be\ndeterministic but perhaps it's not so\nobvious and you can't guess at it\nbeforehand\nin that case one could argue that deep q\nlearning would be great because you can\nalways decrease the exploration factor\nepsilon over time and allow the agent to\nsettle on a purely greedy strategy\nthis is certainly true but how can we\nknow how quickly to decrease epsilon\nthe beauty of policy gradients is that\neven though they are stochastic they can\napproach a deterministic policy over\ntime\nactions that are optimal will be\nselected more frequently and this will\ncreate a sort of momentum that drives\nthe agent towards that optimal\ndeterministic policy\nthis really isn't feasible in action\nvalue algorithms that rely on epsilon\ngreedy or its variations\nso what about a shortcomings as we said\nearlier there are really big variations\nbetween episodes since each time the\nagent visits the state it can choose a\ndifferent action which leads to\nradically different future returns\nthe agent also doesn't make very good\nuse of its prior experience since it\ndiscards them after each time it learns\nwhile they seem like show stoppers they\nhave some pretty straightforward\nsolutions\nto deal with the variance between\nepisodes we want to scale our rewards by\nsome baseline the simplest baseline to\nuse is the average reward from the\nepisode and we can further normalize the\ng factor by dividing by the standard\ndeviation of those rewards this helps\ncontrol the variance in the returns so\nthat we don't end up with wildly\ndifferent step sizes when we when we\nperform our update to the weights of the\ndeep neural network\ndealing with the sample and efficiency\nis even easier while it's possible to\nupdate the weights of the neural net\nafter each episode nothing says this has\nto be the case we can let the agent play\na batch of games so it has a chance to\nvisit his state more than once before we\nupdate the weights for our network this\nintroduces an additional hyper parameter\nwhich is the batch size for our updates\nbut the trade-off is that we end up with\na much faster convergence to a good\npolicy\nnow it may seem obvious but increasing\nthe batch size is what allowed me to go\nfrom no learning at all in space\ninvaders with policy gradients to\nsomething that actually learns how to\nimprove its gameplay\nso that's policy gradient learning in a\nnutshell we're using a deep neural\nnetwork to approximate the agent's\npolicy and then using gradient ascent to\nchoose actions that result in larger\nreturns\nit may be sample inefficient and have\nissues with scaling the returns but we\ncan deal with these problems to make\npolicy gradients competitive with other\nreinforcement learning algorithms like\ndq learning\nif you've made it this far check out the\nvideo where i implement policy gradients\nin tensorflow if you like the video make\nsure to like the video subscribe\ncomment down below and i'll see you in\nthe next video\nwhat's up everybody in this tutorial\nyou're going to learn how to land a\nspaceship on the moon using policy\ngradient methods you don't need to know\nanything about reinforcement learning\nyou don't need to know anything about\npolicy gradient methods you just have to\nfollow along let's get started\nso before we begin let's take a look at\nthe basic idea of what we want to\naccomplish\npolicy gradient methods work by\napproximating the policy of the agent\nthe policy is just the probability\ndistribution that the agent uses to\nselect actions so we're going to use a\ndeep neural network to approximate that\nprobability distribution and we're going\nto be feeding in the input observations\nfrom the environment and getting out a\nprobability distribution as an output\nthe agent learns by replaying its memory\nof the rewards it received during the\nepisode and calculating the discounted\nfuture rewards that followed each\nparticular time step those discounted\nfeature rewards act as weights in the\nupdate of our deep neural network so\nthat the agent assigns a higher\nprobability to actions whose\nfeature rewards are higher\nso we'll start with our imports\nand we're going to want os to handle\nfile operations\nnumpy to handle numpy type operations\nand of course tensorflow to develop our\nagent we're going to stick everything in\none class\nwhose initialize function takes the\nlearning rate\nthe discount factor gamma\nthe\nnumber of actions\nfor the lunar lander environment that's\njust for\nthe\nsize of the first\nhidden layer of the neural network we'll\ndefault that to 64.\nthe\nsize of the second hidden layer of the\ndeep neural network will also default\nthat to 64.\nwe also need the input dimms\nand in this case that is 8 so the\nobservation isn't a pixel image it is\njust a vector that represents the state\nof the environment\nwe also want a checkpoint directory\nand this will be useful for saving our\nmodel later\nso this action space will be what we use\nto\nselect actions later on\nand we're going to need a number of\nother\nadministrative type stuff so for\ninstance the\nstate memory\nis just what the agent will use to keep\ntrack of the states it visited\nlikewise for the action memory we want\nto keep track of the actions the agent\ntook\nand the rewards it received along the\nway\nwe also need the layer one size\nand the layer two size\nwhat else we okay so now we can move on\nto the administrative stuff with\ntensorflow so\ntensorflow handles everything in what\nare called sessions\nso we have to define one of those\nwe're gonna need a function to build the\nnetwork and when we return from the\nnetwork we're going to want to\ninitialize all of the variables so this\nbuild network function is only called\nonce it will load up all of the\nvariables and operations under the\ntensorflow graph and then we have to\ninitialize those\nvariables with some initial values which\nwill be done at random\nwe also need a way of saving the model\nbecause your pc may not be fast enough\nto run this in a you know short enough\namount of time so you can do this in\nchunks by saving and then reloading it\nas you have time\nwe also need a\nfile to save the checkpoints in\nso our next order of business\nis to actually construct this network\nso we don't need any inputs for that\nbut we do need a set of placeholders and\nthe placeholders serve as placeholders\nfor our inputs it just tells the\ntensorflow graph that hey we're going to\nbe passing in some variables we don't\nknow what they are yet we may not\nnecessarily even know their shape or\nsize\nbut we know what type they are and we\nwant to give them names so if something\ngoes wrong we can debug later\nso this input will just be the eight\nelement vector that represents the\nagent's observation of the environment\nand if you're not really familiar with\ntensorflow\nthis idiom of saying shape equals a list\nwhose first element is none\ntells tensorflow that we do not know the\nbatch size of the data we're going to be\nloading\nso the inputs could be\n10 states it could be 100 it could be a\nthousand it could be any number of\nstates and so that none just tells\ntensorflow hey we don't know what shape\nit's going to be so\nyou know just take whatever\nand this\nlabel is going to be the actions the\nagent took during the course of the\nepisode this will be used in calculating\nour loss function\nsimilarly\nwe have something called g\nand g is just the generic name\nfor the agents discounted future rewards\nfollowing each time step\nthis is what we will use to\nbias the agent's loss function towards\nincreasing the probability of actions\nwho\ngenerate the most returns over time\nso now we're going to construct our\nnetwork\nso\nthe first layer will just be the input\nand the number of it'll take of course\nnumber of input and input dimms on\ninput and then output\nthe\nplayer one size\nand the\nactivation is just going to be a value\nfunction\nand then we have to worry about\ninitializing this so when we call the\ntf.global variables initializer it's\ngoing to initialize all of the layers\nand variables with some values we can\ndictate how it does that and we can\nthink very carefully about it so\nif you've dealt with deep neural\nnetworks in some cases you can get\nvanishing or exploding gradients that\ncause the\nvalues the network predicts to you know\ngo to you know either really large or\nreally small values it produces junk\nessentially\nthere is a\nfunction that will initialize the values\nof all the layers such that\nthey are relatively comparable and we\nhave\na minimal risk of that happening\nand that's called a xavier initializer\nand i'm going to want to copy that\nbecause we're going to use that more\nthan once\nso of course the second layer\njust takes the first layer's input\nhas l2 size as the number of units\nwith a\nvalue activation\nand of course the same initializer\nand l3 will be the output of our network\nhowever\nwe don't want to activate it just yet so\nthis quantity is related to the\nprobability sorry the policy of the\nagent but the policy must have the\nuh property that the sum of the\nprobabilities of taking an action you\nknow the sum of the probabilities for\nall the actions must equal one and it's\nthe softmax function that has that\nproperty and we're going to separate it\nout so that we can just\nso that it is a little bit more clean in\nterms of the\nthe code a little bit more readable for\nus\nbut the self.actions variable is what\nwill actually\ncalculate the probabilities of selecting\nsome action and that is just the softmax\nof the\nl3\nand the name of that is probabilities\nfinally we need to well not finally but\nnext we need to calculate the\nthe loss function\nso\nwe'll stick that in a separate scope\nand uh what we need is the negative log\nprobability so\nexcuse me the\ncalculation involves the natural log of\nthe policy and you want to take the\ngradient of the natural log of something\nso we need a function that matches that\nproperty\nso we'll call it neg log\nprobability\nand that's the sparse\nsoft max cross entropy\nwith logits\ntry saying that five times fast\nso log x is just l three\nand this is important because this is\npart of the reason we separated out l\nthree in actions because when you're\npassing in\nthe logits you don't want it to already\nbe activated because the negative the\nsparse soft max cross entropy will\nhandle the\nsoftmax activation function\nand the labels will just be the label\nthat we pass in\nfrom the placeholder and then of course\nbe the actions the agent took\nand so the loss is then that quantity\nneglog probability\nmultiplied by the returns g\nnext we need the training operation\nand that of course is just our gradient\ndescent type algorithm in this case\nwe'll use the\natom optimizer\nthe learning rate of whatever we dictate\nin the constructor\nand we want to minimize that loss\nso that is the sum and hole of the deep\nneural network we need to we need to\ncode\nthe next question we have is how can we\nselect actions for the agent\nso\nthe\nagent is attempting to model its own\nprobability distribution for selecting\nactions so that means what we want to do\nis take a state as input pass it through\nthe network and get out that\nprobability distribution at the end\ngiven by the variable self.actions\nand then we can use numpy to select a\nrandom action according to that\nprobability distribution\nand we're just going to go ahead and\nreshape this\nbill it is\nso when you run an operation through the\ntensorflow graph you need to specify a\nfeed dictionary which gives the graph\nall of the input placeholder variables\nit's expecting so in this case it wants\nself.input\nand that takes state\nas input\nand it's going to return a tuple so\nwe're going to take the 0th element as\nso we can get the correct\nvalue that we want\nand then we can select an action\naccording to\nnumpy random choice\nfrom the action space using the\nprobabilities\nas our distribution\nand we just returned that action\nthe next problem we have to deal with is\nstoring an action\nsorry the transitions and of course\nwe'll store\nthe state action and reward\nand this will be useful when we learn\nand since we're using lists here we're\njust going to append the elements to the\nend of the list\nnext we come to the meat of the problem\nthe learning function\nand this doesn't require any input\nso the basic idea here is that we are\ngoing to convert these lists into numpy\narrays so we can more easily manipulate\nthem\nand then we're going to\niterate through the agent's memory of\nthe rewards it received\nand calculate the discounted sum\nof rewards that followed each time step\nso we're going to need two for loops and\na variable to keep track of the sum as\nwell as something to keep track of our\ndiscounting\nso let's deal with the memories first\nthat won't work will it\nthere we go\nso now\nwe'll instantiate our g factor\nand get in\nshape of reward memory\nwe will iterate over the\nlength of that memory\nwhich is just length of our episode\nand so for each time step\nwe want to calculate the sum\nof the rewards that follow that time\nstep\nis that correct yes\nnext we take the sum\nand discount it\nthen of course the discount\nis just the\ngamma to the k\nwhere k is our time step\nso then of course the\nrewards following the teeth the teeth\nteeth\nthe t time step is just g sum\nso that's the sum that's the weighted\ndiscounted rewards\nthe next thing we have to think about is\nthese rewards can vary a lot between\nepisodes so to promote stability in our\nalgorithm we want to scale these results\nby some number\nit turns out that a reasonable number to\nuse is the mean so we're going to\nsubtract off the mean of the rewards the\nagent received during the episode and\nthen divide by the standard deviation\nthis will give us some nice\nscaled and normalized numbers such that\nthe algorithm doesn't produce wacky\nresults\nand since we're dividing by the standard\ndeviation we have to account for the\npossibility that the standard deviation\ncould be zero\nhence the conditional statement\nnext we have to run our training\noperation\nso the underscore tells us we don't\nreally care about what it returns\nwe have to run the training operation\nthe feed dictionary\nthat'll take an input which is just our\nstate memory\nit will take the labels\nand that is just the action memory\nand it will take the\ng factor which is just the g we just\ncalculated\nnow at the end of the episode\nonce we finish learning we want to reset\nthe agent's memory so that rewards from\none episode don't spill over into\nanother\nso there are two more functions we need\nthese are administrative we need\na way to load the checkpoint\nand we'll go ahead and print out loading\ncheckpoint\njust so we know it's doing something\nand then we have the saver object and we\nwant to restore\na session\nfrom our checkpoint\nfile\nnext we want to save a checkpoint\nwe're not going to print here i take it\nback reason is that we're going to be\nsaving a lot and i don't want to print\nout a bunch of junk statements so\nand what these are doing is just either\ntaking the the current graph as it is\nright now and sticking it into a file or\nconversely taking the graph out of the\nfile loading it into the graph for the\ncurrent session\nso that is that\nnext we can move on to the main program\nto actually test our\nlander\nso we'll need jim\nand i didn't i don't know if i made it\nclear at the beginning but you'll also\nneed the box 2d dash pi environment so\ngo ahead and do pip install box 2d dash\npi if you don't already have that\nso we'll need to import our policy\ngradient agent\ni have this plot learning function i\nalways use you can find it on my github\nalong with this code of course\ni don't elaborate on it but basically\nwhat it does is it takes a sequence of\nrewards\nkeeps track of it performs a running\naverage of say the last 20 25 whatever\namount you want and spits that out to a\nfile\nwe also have\na way of saving the renderings of the\nenvironment so it runs much faster if\nyou don't actually render to the screen\nwhile it's training but you can save\nthose renderings to files in mp4 files\nso you can go back and watch them later\nit's how i produce the\nepisodes you saw at the beginning of\nthis video\nso first thing i'm going to do is\ninstantiate our agent\nand we'll use a learning rate of zero\nthree zeros and a five\nand i believe uh oh we'll need a gamma\nwe use something like 0.99\nand all\nand then all the other parameters we'll\njust leave at the defaults\nnext we need to make our environment\nand that's lunarlander v2\na way to keep track of the scores the\nagent received\nour initial score\nand\nnumber of episodes\nso i achieved uh pretty good results\nafter 2500 episodes so we can\nstart with that\nso the first thing we want to do is\niterate\nover\nour episodes\noh let me do this for you so\nbefore we do that um i'll comment this\nout but if you want to save the output\nyou do this env equals\nwrappers dot monitor\npass in the environment wherever you\nwant to save it lunar lander\nand then you want to use a lambda\nfunction\nto tell it\nto render on\nevery episode\nand this force equals true i believe\nthat just\ntells it to overwrite if there's already\ndata in the directory\nso at the top of every episode just\nprint out what episode number you're on\nand the\ncurrent score\nyou set your done flag\nand for subsequent episodes you'll want\nto reset the score\nreset your environment\nand play an episode\nfirst thing you want to do is select an\naction\nand that takes the observation as input\nso we need to get the new observation\nreward done flag and info by stepping\nthrough the environment\nonce that's done you want to store the\ntransition\nset the old observation to be the new\none so that you select an action based\non the new estate\nand keep track of the reward you\nreceived\nat the end of every episode we want to\nappend\nthe score to the score history\nand perform our learning operation\nyou also want to\nsave a checkpoint\nafter every operation ever after every\nlearning operation\nand then when you're done dictate some\nfile name\ndot png\nand call my super secret\nplot learning function\nthat just takes the score history\nfile name\nand a window that tells it over how many\ngames you want to take the running\naverage\nso now let's go to our terminal and see\nhow many mistakes i made one second\nall right\nso here we are let's go ahead and\ngive it a try\nso i made some kind of error in the\npolicy gradient agent let's swing back\nto that file and see where it is\none second\nso here's the model\nit does not have\ninput dimms that's because i forgot to\nkeep track of it\nsave\ngo back to our terminal and see how it\ndoes\nand i apparently called something i\nshould not have\nthis is line 27\nit says\nself.label placeholder got an unexpected\nkeyword argument named label\nah that's because\nit is called name\nthat's what happens when i try to type\nand talk at the same time\nlet me make sure i didn't call l1 size\nl2 size something different no i did not\nall right i will run that again\nso now it's unhappy about the\nchoice of oh of course\nso it's lunar\nthere's no dash in there\nget rid of that\ntypos galore tonight\noh really\noh really\nah it's store transitions\nyes\nso it's actually store transitions so\nlet's call it that\nand there we go\nperfect\nso now it's actually learning i'm not\ngoing to sit here and wait for this to\ndo 2500 games because i've already run\nthis once before that's how i got the\nkodi saw at the beginning\nso\ni'm going to go ahead and show you the\nplot that it produces now so you can see\nhow it actually learns so\nby the end it gets up to about an\naverage reward of around 200 and above\npoints that's considered solved if you\ncheck the documentation on the openai\ngym so congratulations we have solved\nthe lunar lander environment with the\npolicy gradient algorithm relatively\nsimple just a a deep neural network\nthat calculates the probability of the\nagent picking an action\nso i hope this has been helpful go ahead\nand leave a comment down below\nfeel free to take this code from my\ngithub fork it make it better do\nwhatever you want with it i look forward\nto seeing you all in the next video\nwelcome back everybody to a new\nreinforcement learning tutorial in\ntoday's episode we're going to teach an\nagent to play space invaders using the\npolicy gradient method let's get to it\nfor imports we start with the usual\nsuspects numpy and tensorflow\nwe start by initializing our class for\nthe policy gradient agent\nwe take the learning rate discount\nfactor number of actions number of\nlayers for the fully connected layer\nthe input shape channels\na directory for our checkpoints very\nimportant as well as a parameter to\ndictate which gpu we want tensorflow to\nuse if you only have one gpu or using\nthe cpu you don't need that parameter\nsave the relevant parameters and compute\nthe action space which is just a set of\nintegers\nwe want to subtract out the input\nheights\nand width from the input shapes and keep\ntrack of the number of channels for use\nlater\nour agent's memory will be comprised of\nthree lifts that keep track of the state\naction and rewards\nwe want a configuration which just tells\ntensorflow which gpu we want to use\nas well as our session graph that uses\nthat config we call the build network\nfunction\nand then use the tf global variables\ninitializer\nwe need to keep track of the saver and a\ncheckpoint file for saving and loading\nthe model later\nnow if you don't know anything about\npolicy gradient methods don't worry\nwe're going to cover everything you need\nto know as we go along the first thing\nwe're going to need is a convolutional\nneural network to handle image\npre-processing that'll be connected to a\nfully connected layer that allows the\nagent to estimate what action it wants\nto take in contrast to things like deep\nq learning policy gradient methods don't\nactually try to learn the action value\nor value functions rather policy\ngradients try to approximate the actual\npolicy of the agent\nlet's take a look at that\nfor our build network function we're\ngoing to construct a convolutional\nneural network with a few parameters\nan input placeholder that has shape\nbatch size by input height and width as\nwell as the number of channels\nwe will also need an input for the\nlabels which just correspond to the\nactions the agent takes that is shape\nbatch size\nand a factor g which is just the\ndiscounted future rewards following a\ngiven time step that is shape batch size\nour convolutional neural network is\ngoing to be pretty straightforward if\nyou've seen any of my videos you can\nkind of know what to expect the first\nlayer has 32 filters a kernel size of\neight by eight and a stride four\nand we're going to want to use an\ninitializer for this i found that the\nxavier initializer works quite well the\npurpose of it is to keep the\nto initialize all the parameters in such\na way that the\nuh they the network doesn't have one\nlayer with parameters significantly\nlarger than any other\nwe want to do batch normalization i\nmistyped the epsilon there should be one\nby ten to the minus five\nand you want to of course use a value\nactivation on the first convolutional\nlayer\nand that commvault that activated output\nserves as the input to the next layer\nwith 64 filters\nkernel size of 4x4 and a stride of 2 and\nagain we'll use the\nxavier initializer\nof course we also want to do batch\nnormalization on this layer and at this\npoint i think i actually get the correct\nepsilon for the\nbatch norm\nand of course we want to use a value\nactivation on that batch normed output\nas well\nour third convolutional layer is more of\nthe same 2d convolution\nwith 128 filters\nit will have a kernel size of 2x2 a\nstride of 1\nand we'll also use the same\ninitialization\nagain batch normalization\nwith an epsilon of 1 by 10 to the minus\n5.\nactivate with a value\nnext we have to take into account our\nfully connected layers so the first\nthing we need to do is flatten the\noutput of the convolutions\nbecause they come out as matrices we\nneed a list\ngo ahead and make the first fully\nconnected layer using fc1 as the number\nof units value activation\nand the second dense layer is going to\nhave units equal to the number of\nactions for the agent which in this case\nis just six\nnotice that we don't activate that but\nwe have a separate variable for the\nactions which is the activated output of\nthe network and we're going to use a\nsoft max so that way we get\nprobabilities that add up to one\nwe need to calculate the negative log\nprobability with a sparse softmax\ncross-entropy function with logits using\ndense two as our logits and\nlabels\nas of the actions as our labels the loss\nis just that quantity multiplied by the\nexpected future rewards\nand of course we want to reduce that\nquantity\nnow for the training operation we're\ngoing to use the rms prop optimizer with\na set of parameters i found these to\nwork quite well\nthis algorithm is pretty finicky so you\nmay have to play around the next thing\nwe have to do is\ncode up the action selection algorithm\nfor the agent in policy gradient methods\nwe're trying to actually approximate the\npolicy which means we're trying to\napproximate the distribution by which\nthe agent chooses actions given it's in\nsome state s so what we need is a way of\ncomputing those probabilities and then\nsampling them\nsampling the actions according to those\nprobabilities\nwe choose an action\nby taking in an observation reshaping it\nof course this will be a sequence of\nframes\nuh for high\nand you want to calculate the\nprobabilities associated for\neach action given that observation and\nthen you want to sample that probability\ndistribution\nusing the numpy random choice function\nnext up we have to take care of the\nagent's memory so we're going to store\nthe observation action and reward in the\nagent's list using a simple append\nfunction\nso one big problem we're going to have\nto solve is the fact that policy\ngradient methods are incredibly simple\nand efficient these monte carlo methods\nmeaning that at the end of every episode\nthe agent is learning so it throws away\nall of the\nexperience that required in prior\nepisodes so how do we deal with that\nwell one way to deal with that is to\nactually queue up a batch of episodes\nand learn based on that batch of\nexperiences\nthe trouble here is that when we take\nthe rewards that follow any given time\nstep we don't want to account for\nrewards following the current episode so\nwe don't want rewards from one episode\nspilling over into another so we have to\ntake care of that here\nnext up we handle the learning for the\nagent we want to\nwe want to convert the state action and\nreward memories into arrays so that we\ncan feed them into the numpad the excuse\nme tensorflow learning function\ninto the sorry the tensorflow graph\nwe have to start by reshaping the state\nmemory into something feasible and then\nwe can calculate the expected feature\nrewards starting from any given state so\nwhat we're going to do is iterate over\nthe entire memory\nand\ntake into account the rewards the agent\nreceives for all subsequent time steps\nwe also need to make sure that we're not\ngoing to take into account rewards from\nthe next episode\nnext up we have the scale the expected\nfeature rewards this is to reduce\nthe variance in the problem so let's\nmake sure we don't have really really\nlarge rewards so everything is just kind\nof scaled\nnext up we call the training operation\nwith an appropriate feed dict of the\nstate memory action memory as labels and\nthe g for our g variable\nfinally since we're done we're going to\nclear out the agent's memories\nfinally we just have some bookkeeping\nfunctions to load and save checkpoints\nyou just call the saver restore function\nthat loads the checkpoint file into the\nsession\nand the save checkpoint just dumps the\ncurrent session into the checkpoint file\nanother problem we have to solve is that\nthe agent doesn't get a sense of motion\nfrom only a single image right if i show\nyou a single image you don't know if the\naliens are moving left or right or\nreally you don't know which direction\nyou're moving so we have to pass in a\nsequence of frames to get a sense of\nmotion for our agent this is complicated\nby the fact that the open ai gym atari\nlibrary in particular returns a set of\nframes that are repeated so if you\nactually cycle through the observations\nover time you'll see that the frames\nchange or sorry don't change\nuh based on an interval of one two or\nthree so we have to capture enough\nframes to account for that fact as well\nas to get a overall sense of movement so\nthat means four is going to be our magic\nnumber for so for stacking frames\nnext we move into the main program we\nimport gym numpy\nour model\nas well as the\nplot learning function which is a simple\nutility you can find on my github as\nwell as the wrappers to record the\nagent's gameplay if you so choose we\nneed to pre-process the observation by\ntruncating it and taking the average\nnext up we stack the frames so at the\nbeginning of the episode stack frames\nwill be none so you want to initialize\nan empty array of zeros\nand iterate over that array and set each\nof those rows to be the current\nobservation\notherwise what you want to do is you\nwant to pop off the bottom observation\nshift everything down and put the fourth\nspot or the last spot to be the current\nobservation\ndown in the main function we have a\ncheckpoint flag if you want to load a\ncheckpoint\nwe want to initialize our agent with\nthis set of hyper parameters i found\nthese to work reasonably well you can\nplay around with them but it is\nincredibly finicky so\nyour mileage may vary we need a file\nname to save our plots\nwe'll also want to see if we want to\nload a checkpoint\nnext we initialize our environment space\ninvaders of course\nkeep track of the score history score\nnumber of episodes and our stack size of\nfour\none iterate over the number of episodes\nresetting the done flag at the top of\neach episode\nwe also want to keep track of the\nrunning average score from the previous\n20 games just so we got an idea if it's\nactually learning\nevery 20 games we're gonna print out the\nuh episode score and average score\notherwise every other every other\nepisode we're just going to print out\nthe episode number and the score\nreset the environment and of course you\nhave to pre-process that observation\nand then go ahead and set your stacked\nframes to none because we're the top of\nthe episode and then call the stack\nframes function so that we get four of\nthe initial observation\nset the score to zero\nand start iterating over the episode so\nhis first step is to choose an action\nbased on that set of stacked frames\ngo ahead and take that action and get\nyour new state action and reward\ngo ahead and pre-process that\nobservation\nso that way you can stack it on the\nstack of frames\nnext up you have to take care of the\nagent's memory by storing that\ntransition\nand finally you can increment your score\nsave the score at the end of the episode\nand every 10 games are going to handle\nlearning and saving a checkpoint and\nwhen you're all done go ahead and plot\nthe learning\nnow the agent is done we can take a look\nat the actual plot it produces over time\nthis is how you know an agent is\nactually learning what you'll see is\nthat there is some increase in the\naverage reward over time you'll see\noscillations up and down and that's\nperfectly normal but what you want is an\noverall upward trend now for this\nparticular set of algorithms it is\nnotoriously finicky with respect to\nlearning rates i didn't spend a huge\namount of time tuning them or playing\nwith them i just wanted to get something\ngood enough to show you guys how it\nworks and turn it over to your capable\nhands for fine tuning but what you do\nsee is a definite increase over time as\nthe agent's average reward improves by\nabout 100 points or so that isn't going\nto win any awards but it is definitely a\nclear and unequivocal sign of learning\nso there you have it that was policy\ngradients in the space invaders\nenvironment from the open ai gym i hope\nyou learned something make sure to check\nout this code on my github you can fork\nit you can copy it you can do whatever\nyou want with it\nmake sure to subscribe leave a comment\ndown below if you found this helpful i\nlook forward to seeing you all in the\nnext video\nwelcome back everybody to neuralnet.ai i\nam your host phil tabor\npreviously a subscriber asked me hey\nphil how do i create my own\nreinforcement learning environment\ni said well that's a great question i\ndon't have time to answer it in the\ncomments but i can make a video so here\nwe are\nwhat we're going to do in the next two\nvideos is create our own open ai gym\ncompliant reinforcement learning\nenvironment the grid world it's going to\nbe text based and if you're not familiar\nwith it the grid world is aptly named a\ngrid of size and by n where the agent\nstarts out in say the top left and\nthat's to navigate its way all the way\nto the bottom right\nthe twist on this is going to be that\nthere will be two magic squares that\ncause the agent to teleport across the\nboard the purpose of doing this is to\ncreate a shortcut to see if the agent\ncan actually learn the shortcut kind of\ninteresting\nthe agent receives a reward of -1 with\neach step except for the terminal step\nor receives a reward of zero\ntherefore the agent will attempt to\nmaximize its reward by minimizing the\nnumber of steps it takes to get off the\ngrid world\nwhat other things two other concepts we\nneed are the concept of the state space\nwhich is the set of all states minus the\nterminal state and the state space plus\nwhich is the set of all states including\nthe terminal state\nthis gives us a bit of a handy way to\nfind out the terminal state\nas well as to find out if we're\nattempting to make illegal moves\nit also follows the nomenclature and\nterminology from the sutton bardo book\nreinforcement learning\nwhich is an awesome resource you should\ndefinitely check out if you have not\nalready\nso in part one we're going to handle the\nenvironment and in part two we're going\nto get to the main loop and the agent\nfor which we will use q learning now\ndeep q learning because this is a very\nstraightforward environment we don't\nneed\na functional approximation we just need\nthe tabular representation of the\nagent's estimate of the action value\nfunction\nso if you're not familiar with q\nlearning i do have a couple videos on\nthe topic\none where\nthe q learning agent solved the card\npoll game as well as a an explainer type\nvideo that talks about what exactly\nq-learning is\nso let's go ahead and get started\nwe only have a couple dependencies we're\nnot doing anything on the gpu so just\nnumpy and matplotlib\nwe want to close everything up into a\nclass called grid world\nand our\ninitializer\nwill take the m and n which is the shape\nof the grid as well as the magic squares\nso we represent our grid\nas an array of\nzeros and shape m by n\nwe want\nwe want to learn to type uh we want to\nlearn to\nsorry we want to keep tr\nwe want to keep track of the m and the\nend\nfor\nhandy use later\nso let's go ahead and define our state\nspace\nand that's just going to be a list\ncomprehension\nfor all the states in the range\nself.m times self.n\nnow the as i said the state space does\nnot include the terminal state and the\nterminal state is the bottom right so we\nhave to go ahead and remove or pop off\nthat particular\nstate from the list\nuh next up so\nnow let's go ahead and\nagain learn to type\ngo ahead and define our\nstate space plus\nalso we need to know the\nthe way that the actions map up to their\nchange on the environment so we'll call\nthat\nthe action space a little bit of a\nmisnomer but we can live with it for now\nso moving up we'll translate the agent\nup\none row which is distance m\nand moving down will advance the agent's\nposition\ndownward by\nalso m\num\nmoving left we'll translate the agent\none step we'll decrement the agent's\nposition by one\nand moving right we'll\nincrease it by one\nwe also want to keep track of the set of\npossible actions\nyou could use the keys in the action\nspace dictionary but\nlet's go ahead and use a separate\nstructure\nand we'll use a list\nup down left and right the reason is\nthat the q learning agent\na q learning algorithm sorry\ncan it can choose actions at random so\nit is handy to have a list from which\nyou can choose randomly\nuh next we need to add the magic squares\nbecause that's a little bit more\ncomplicated than\nit may seem\nand finally when we initialize the grid\nwe want to set the agent to the top left\nposition\nlet's go ahead and add those magic\nsquares\nso\nof course we want to store that\nin our\nobject now there's a little bit of\nhokiness that i must explain so\nthe agent is represented by a zero when\nwe print out the grid to the terminal\nand uh md squares are represented by a\none\nand so excuse me and so that means we\nneed something other than 0 and 1 to\nrepresent these magic squares\ni want to when i render the environment\ni want to know where the entrance and\nwhere the exit is\nso we use different values for\nthe entrance and exit so that way we can\nrender it correctly\nso\njust by royal decree we set i which\nwould be the representation of the\nof the magic square in the grid world to\n2 to start\nand we're going to go ahead and iterate\nover the magic squares\nso now what we need to know\nis\nuh\nwhat position we are in\nso\nyou\nsorry it's the color is off indicating\nsomething is wrong i have screwed\nsomething up royally\nwhich i do not see\nbecause i am blind\nanyway so\nthe\nthe x position is just going to be the\nfloor of the\ncurrent square and the\nnumber of rows and y will be the\nmodulus of\nthe number of columns\nso then the grid we want to set that x\nand y position to i\nand since we want to have a different\nrepresentation for the entrance and exit\ngo ahead and set the\nincrement i by one recall that the\nmagic squares are is represented as a\ndictionary so we're iterating over the\nkeys and the values are the destinations\nso the keys are the source values are\ndestinations\nso\nnext we want to find out\nprecisely that\nwhat the destinations are\nset that in and then set the grid that\nsquare\nto i and then increment i again\nand i'm only going to do\ni'm only going to do\ntwo magic squares you can do any number\nbut in this case we're just gonna do two\nso\nokay so the next thing we need to know\nis if we are in the terminal state\nand\nas i said earlier the state space and\nstate space plus concepts give us a very\neasy way of doing that so\nlet's go ahead and take care of that so\nsince the state space plus is all the\nstates and the state space is all the\nstates\nminus the terminal state we know that\nthe difference between these two sets\nis the terminal state so\nstate in\nstate space\nplus\nand\nnot in\nthe state space\nhow does that look\nlet me scroll down a bit okay\nso\nnext up\nlet us\ngo ahead and get the agent row and\ncolumn\nand we're going to use the same logic as\nabove\nso next we want to set the state\nso that will take the new state as input\nand we're going to go ahead and assume\nthat the new state is allowed so the\nagent\nif it is along the left edge and\nattempts to move left it just receives a\nreward of minus one and doesn't actually\ndo anything\nlikewise if it's on the top row and\nattempts to move up it doesn't actually\ndo anything it just gets a reward of\nminus one for wasting its time\nso we want to get\nuh sorry the\nrow and column\nand set that space\nto zero because zero denotes an empty\nsquare\nand the agent position then is the new\nstate\nand again we want to get the new x a new\ny\nthere is a typo there let's fix that\nand then set that position\nto one because that is how we represent\nthe agent by royal decree\nso the next thing we have to know\nis if we're attempting to move off the\ngrid that's not allowed\nthe agent can only\nstay on the grid so let's take care of\nthat\nso we want to take the new and old\nstates as input\nand the first thing we want to know is\nif we're attempting to move off the grid\nworld entirely\nthat's ah\nso\ni hate these editors so\nuh if we are if the new state is not in\nthe new state space plus we are\nattempting to move off the grid so you\nreturn true\notherwise\nif the old state\nmodulus\nm equals zero\nand new state\nmodulus\num self.m equals self.m minus one\nthen we return true\nand\nfor brevity i could explain this but the\nvideo is running long already so for\nbrevity\nthe reason this is true is left as an\nexercise to the reader bet you didn't\nknow this was going to be like a college\ncourse\nso now\nuh basically what we're trying to do\nhere i'll just give you a hint what\nwe're trying to do here is determine if\nwe're trying to move off the grid either\nto the left or to the right we don't\nwant to wrap around so\nso if you're for instance if we have a\nnine by nine grid it goes from zero to\neight so then if you add one\nright you would get nine which would\nteleport you to the other row\nuh and the zeroth column you don't want\nthat what you want to do is waste spa\nways to move and receive a reward of\nminus one so that's what we're doing\nhere\num\nold state\nmodulus\ngood grief\nso if\nneither of those are true then you can\ngo ahead and return false\nmeaning you're not trying to move off\nthe grid\nso let's see can you see that you can so\nnext function we need is\na way to actually step\nso let's go ahead and do that let's say\nthe only thing we need is to take in the\naction\nso\nthe first thing you want to do\nis get the x and y again\nand here's where we're going to check to\nmake sure it's a legal move so the\nresulting state\nis then agent position\nplus the mapping so the agent position\nis whatever it is and recall that the\naction space\nis this dictionary here that maps the\nactions to the translations in the grid\nso we're doing down here then is saying\nthe new state is equal to the current\nstate plus whatever the\nresulting translation is for whatever\naction we're attempting to make\nso\nnext thing we need to know is are we on\na magic square\nand if we are\num\nand if we are then the um\nagent teleports to its new position\nokay\nso\nnext up we need to handle the reward\nso it's minus one if not\num\nis terminal\nstate\nso it's minus one if we haven't\ntransitioned into the terminal state\notherwise it is zero\nif we're not trying to move off the grid\nthen we can go ahead and set that state\nself.set state\nresulting state\nand then we're ready to go ahead and\nreturn\nso in the openai gym uh whenever you\ntake a step it returns the new state the\nreward\num\nwhether or not the game is over and some\ndebug information\nso we're gonna do the same thing\nresulting state reward\nand\nthe\nwhether or not it is the terminal state\nand our debug info is just going to be\nnone\nso if we are attempting to move off the\ngrid what do we want to do nothing\nso we want to return\nagent position\num\nand the\nreward\nand\nwhether or not it's terminal\nand the null debug info\nwe're almost there so\nnext thing we need to know is\nhow do we reset the grid because at the\nend of every episode we have to reset\nright\nfirst thing to do is set the agent\nposition to zero\nreset the grid to zeros\nand go ahead and add the magic squares\nback in\nand return the\nagent position which is of course zero\noh wow that's real close all right so\nnext up\none last function i swear just one more\ni promise all right\ni wouldn't lie to you\nall right next up\nwe want to provide a way of rendering\nbecause hey that's helpful for debug\ni like to print a whole big string of\nyou know dashes because it's party\n[Music]\nwe want to iterate over the grid\nfor column in row\ncolumn equals\nzero in other words if it's an empty\nsquare we're just going to print a dash\nand we're going to end it with a tab\nif the column is 1 meaning we have an\nagent there\nwe're going to print an x\nto denote the agent\nnow\nif the column is 2\nthen that is one of the entrances to\nour magic squares\nso print the\na in\nwith a\ntab delimiter a tab end\nif the column\nequals three\nyou can print a out\nand equals tab\num\nand if the column\nequals four\nthen we know we're at the\nother magic square entrance\nand finally if it's five then\nwe know we're at the\nother\nmagic squares exit\nafter each\nrow we want to print a new line\nand at the end we'll go ahead and print\nanother\nchunk of pretty dashes\noh\nah yeah that's it that is it okay so\nthat is it for our agent class that only\ntook how long 20 some minutes wow okay\ni hope you're still with me\nbasic idea here is to make your own\nenvironment you need an initialize a\nreset\na state space state space plus a way to\ndenote possible actions\na way to make sure the move is legal and\na way to actually affect that\nenvironment\nthe step function needs to return the\nnew position the reward\nwhether or not the state is the new\nstate is terminal as well as some debug\ninformation\nyou also need a way of resetting and\nprinting out your environment to the\nterminal\nso in part two we're actually going to\nfire this baby up with a q learning\nalgorithm and see how she do\nthat's actually quite exciting it well\nmoderately exciting anyway it actually\nlearns it does quite well and it does\nfind the magic square\nspoiler alert if you made it this far it\nfinds a magic square and gets out of the\nminimum number of moves required\num\nit's pretty cool to see so that will\ncome in the next video on vedna's day i\nhope to see you all then\nif you like the video make sure to leave\na thumbs up subscribe if you have not\nalready for more reinforcement learning\ncontent and i will see you all in the\nnext video\nwelcome back everybody to a new tutorial\nfrom neuralnet.ai i am your host phil\ntaber\nif you're new to the channel i'm a\nphysicist former semiconductor process\nengineer turned machine learning\npractitioner\nif you haven't subscribed yet go ahead\nand hit the the subscribe button so you\ndon't miss any future reinforcement\nlearning tutorials\nwhen we left off in our previous video\nwe just finished up the\nbulk of our open ai gym compliant\nreinforcement learning environment\ntoday we're going to go ahead and code\nup a q learning agent and the main loop\nof the program to see how it all\nperforms\nso let's go ahead and get started\nso the uh first thing we are going to\nneed is the\num\nis the\nmagic squares right\nif you recall the magic squares are the\nteleporters in the grid world that\neither advance the agent forward or\nbackward\nso the first one is going to be at\nposition 18 and dump out at position 54\nso it'll move it forward\nand the next one will be at let's say 63\nand dump out at position 14. so\nteleporter a will advance the agent\nthrough the grid world and teleporter b\nwill send it back to\nan earlier square\nso we need to create our grid world\nwe use a nine by nine grid and pass in\nthe magic squares we just created\nnext up we have to worry about the model\nhyper parameters so if you are not\nfamiliar with that let me give you a\nquick rundown these are the parameters\nthat control how fast the agent learns\nand how much it chooses to value the\npotential future rewards\nso the first parameter is alpha that is\nour learning rate\n0.1\na gamma of 1.0 tells us that the agent\nis going to be totally farsighted it\nwill count all future rewards equally\nan epsilon of 1.0 this is of course the\nepsilon for epsilon greedy action\nselection\nso it will start out\nbehaving pretty much randomly and\neventually converge on a purely greedy\nstrategy\nso q learning is a tabular method where\nyou have a table of state and action\npairs and you want to find the value of\nthose state action pairs so to construct\nthat we have to iterate over the set of\nstates and actions\nstate space plus\nand\nemv dot possible actions\nand you have to pick something for an\ninitial value it's really arbitrary but\nthe cool thing about picking zero is\nthat we're using something called\noptimistic initial values what this\nmeans is that since the agent takes or\nreceives a reward of minus one for every\nstep\nit can never have a reward of zero right\nbecause there's some distance between\nthe agent and the exit\nso by setting the\ninitial estimate at zero you actually\nencourage exploration of unexplored\nstates because if the agent takes a move\nit realizes oh i get a reward of -1\nthat's significantly worse than 0. let\nme try this other unexplored option so\nover time it will gradually explore all\nthe available actions for any given\nstate\nbecause it's been disappointed by all\nthe stuff it has previously tried just a\nfun little fact\nwe want to play\n50 000 games\nwe need a way of keeping track of our\nrewards\nnumpy array will do just fine yes\nso now let's iterate over the total\nnumber of games\nand um\nthief\ni like to print out a\nmarker to the terminal so that way i\nknow\nit's actually working\nso every five thousand games just print\nthat we're starting\nthe ith game\nat the top of every episode you want to\nreset your done flag you want to reset\nyour episode rewards\nso you don't accumulate rewards from\nepisode episode and of course you want\nto reset your environment\noh let me scroll down here\nthere we go\nyou want to reset your environment\njust as you would with any open ai gym\ntype problem\nnext up\nwe begin each episode so while not done\nwe want to take a\nrandom number\nfor our\nepsilon greedy action selection\nso\nwe're going to just make use of this max\naction before we define it\nq\nobservation\nand\nenv.possible actions\nand what that will do is\nuh we're going to write it here\nmomentarily but what it's going to do\nis it is going to\nfind the maximum action for a given\nstate\nso the random number is less than one\nminus epsilon we want to do that can you\nguys see that yep\notherwise\nwe want to take\na random sample of the action space so\nwe have to write these two functions\nlet's do that quite quickly\nso the\naction space sample is pretty\nstraightforward\nwe can just return a random choice from\nthe\nlist of possible actions and that's the\nreason we chose a list as that data\nstructure just to make it easy\nnext up we have the max action max\nyeah max action function\nbut that doesn't need to belong to the\nclass\nthat takes the q the state and the set\nof possible actions\nwe want to take a numpy array of the\nestimates agent\nsorry the agent's estimate of the\npresent value of the expected future\nrewards\nfor the stated stand in all possible\nactions\na in actions\nand then we want to find the maximum of\nthat\nand that's just an index so we want oop\nsorry that's just an index so we want to\nreturn\nthe action that that actually\ncorresponds to\nall right that's all well and good\nwe need more space\nlet's do a little bit more\nthere we go\nso next\nwe want to actually take the action\nso we get our new state observation\nunderscore reward done and info\nenvy.step action\nand next up uh we have to calculate the\nmaximal action for this new state so\nthat we can insert that into\nour\nupdate equation for the q function\nso let's do that\nand we're not worried about epsilon\ngreedy here because we're not actually\ntaking that action\nnext up we have to update rq function\nfor the current action and state\nand\nthat's where our alpha comes in\nward plus\nmake sure that is visible to you\nreward plus\nsome quantity\nwhich is gamma our discount factor times\nq\nobservation underscore action underscore\nso the new state and action\nminus q observation\naction\nlet me tab this over there we go\nnice and compliant with the pep style\nguides right\nmostly\nokay so this is the update equation for\nthe q function that will update the\nagent's estimate\nof the\nvalue of the current state and action\npair\nnext up\nwe just need to let the agent know that\nthe uh environment has changed states so\nyou set observation to observation\nunderscore\nand that is it for q learning in a\nnutshell folks that's really that\nstraightforward\nso the end of each episode we want to\ndecrease epsilon\nso that way the agent eventually settles\non a purely greedy strategy\nyou can do this a number of ways you can\ndo it you know with a square root\nfunction a log function\ni'm just going to do it linearly\nit's not that critical for something\nlike this\nso\nit's going to decrease it by 2 divided\nby num games every every game so about\nhalfway through it'll be purely greedy\nand at the end of every episode you want\nto make sure you're keeping track of the\ntotal rewards which is something i\nforgot up here\nyeah so one thing i did forget is to\nkeep track of the\ntotal reward for the episode don't\nforget that very important\nand at the end of all the episodes\nyou want\nto plot the total rewards\nand that is it for the coding portion oh\none other thing i take it back so let's\nscroll up here i do want to show you the\nenvironment so\nlet's just do that env dot render\nand the\npurpose of doing that is so that you can\nsee how many moves it takes the agent to\nescape from the grid world\nthat will tell us if it's doing good or\nnot right because there's a minimum\nminimum number of moves it takes to\nescape\nso i'm going to fire up the terminal and\ngo ahead and get that started\none second\nand here we are in the terminal\nlet's go ahead and run that\nand you can see here that we start at\nthe top left so it takes one\ntwo\nmoving to a out is free\nso 3\n4 5 6 7 8 9 10 11. sorry 11 and the 12th\nmove is free because it's the exit to\nthe maze sorry to the grid world so\ntotal reward of minus 11 is the best the\nagent can possibly do\nit's gone ahead and plotted so let's\ncheck that out and here is the plot and\nyou can see that indeed the agent starts\nout\nrather poorly\nexploring finding sub-optimal routes\nthrough the grid world but eventually\nand about halfway through here at um\n2500 or so sorry 25 000 you can see that\nit settles on at least a constant value\nlet's prove that it is\nthe maximum value of minus 11 and you\ncan see that\nit's 10.97 that is\nclose enough for government work so you\nsee that the agent\nis able to actually solve the maze sorry\nthe grid world i keep calling it amaze\nis able to solve the grid world using\nthe q learning algorithm now this isn't\nsurprising you know we would expect this\nuh what's novel here is that we have\ncreated our own reinforcement learning\nenvironment that uses a very similar\nformat to the open ai gym\nso anytime you want to create a new\nenvironment you can use\nyou can fire this video up and use the\nyou know set of code here\njust as a template for your own projects\ni'm going to put this up on my github\ni'll link that down below and i'm also\ngoing to write up a tutorial for uh in\ntext form and upload it to neuralnet.ai\ni don't know if all that done tonight\ni'll update the description with it\nthat's if you are a\nyou know if you consume text more easily\nthan than video then you can go ahead\nand check that out i hope this has been\nhelpful\nuh make sure to leave a comment\nsubscribe if you haven't already and i\nhope to see you all in the next video\nwelcome back data manglers thanks for\ntuning in for another episode from\nneuralnet.ai\nif you're new to the channel i'm phil\ntabor a physicist and former\nsemiconductor engineer turned machine\nlearning practitioner\ni'm on a mission to teach the next\ngeneration of data engineers so we can\nstay one step ahead of our robot\noverlords\nif you're not subscribed be sure to do\nthat now so you don't miss any future\nreinforcement learning content\nwe've touched on reinforcement learning\nmany times here on the channel\nas it represents our best chance at\ndeveloping something approximating\nartificial general intelligence\nwe've covered everything from monte\ncarlo methods to deep q q-learning to\npolicy gradient methods using both the\npi torch and tensorflow frameworks\nwhat we haven't discussed on this\nchannel is the what and the how of\nreinforcement learning\nthat oversight ends today\nright now\nokay maybe a few seconds from now but\neither way we're going to cover the\nessentials of reinforcement learning\nbut first let's take a quick step back\nyou're probably familiar with supervised\nlearning which has been successfully\napplied to fields like computer vision\nand linear regression\nhere we need mountains of data all\nclassified by hand just to train a\nneural network\nwhile this is proven quite effective it\nhas some pretty significant limitations\nhow do you get the data how do you label\nit\nthese barriers put many of the most\ninteresting problems in the realm of\nmega corporations and this does us the\nindividual practitioners no good\nto top it off it's not really\nintelligence\nyou and i don't have to see thousands of\nexamples of a thing to understand what\nthat thing is\nmost of us learn actively by doing\nsure we can shortcut the process by\nreading books or watching youtube videos\nbut ultimately we have to get our hands\ndirty to learn if we abstract out the\nimportant concepts here we see that the\nessential stuff is the environment that\nfacilitates our learning the actions\nthat affect that environment and the\nthing that does the learning the agent\nno jacket or labels required\nenter reinforcement learning this is our\nattempt to take those ingredients and\nincorporate them into artificial\nintelligence\nthe environment can be anything from\ntext-based environments like card games\nto classic atari games to real to the\nreal world\nat least if you're not afraid of skynet\nstarting an all-out nuclear war that is\nour ai interacts with this environment\nthrough some set of actions which is\nusually discrete move in some direction\nor fire at the enemy for instance\nthese actions in turn cause some\nobservable change in the environment\nmeaning the environment transitions from\none state to another\nso for example in the space invaders\nenvironment in the open ai gym\nattempting to move left caused the agent\nto move left with 100 probability\nthat need not be the case though\nin the frozen lake environment\nattempting to move left can result in\nthe agent moving right or up or down\neven\nso just keep that in mind that these\nstate transitions are probabilistic and\ntheir probabilities don't have to be one\nhundred percent merely their sum the\nmost important part of the environment\nis the reward or penalty the agent\nreceives\nif you take only one thing away from\nthis video it should be that the design\nof the reward is the most critical\ncomponent of creating effective\nreinforcement learning systems\nthis is because all reinforcement\nlearning algorithms seek to maximize the\nreward of the agent\nnothing more nothing less\nin fact this is where the real danger of\nai is\nit's not that it would be malicious but\nthat it would be ruthlessly rational\nthe classic example is the case of an\nartificial general intelligence whose\nreward is centered around how many paper\nclips it churns out sounds innocent\nright\nwell if you're a paper clip making bot\nand you figure out that humans consume a\nbunch of resources that you need to make\npaper clips\nthen those pesky humans are in the way\nof an orderly planetary scale office\nthat's problematic for all involved\nwhat this means is we must think long\nand hard about what we want to reward\nthe agent for and even introduce\npenalties for undertaking actions that\nendanger human safety at least and\nsystems that will see action in the real\nworld\nperhaps less dramatic although no less\nimportant are the implications for\nintroducing inefficiencies in your agent\nconsider the game of chess\nyou might be tempted to give the agent a\npenalty for losing pieces but this would\npotentially prevent the agent from\ndiscovering gambits where it sacrifices\na piece for a longer term positional\nadvantage\nthe alpha zero engine a chess playing\nartificial intelligence is notorious for\nthis\nit will sacrifice multiple pawns and yet\nstill dominate the best traditional\nchess engines we have to offer\n[Music]\nso we have the reward the actions and\nthe environment what are the agent\nitself\nthe agent is the part of the software\nthat keeps track of these state\ntransitions actions and rewards and\nlooks for patterns to maximize its total\nreward over time\nthe algorithm that dictates how the\nagent will act in any given situation or\nstate of the environment is called its\npolicy\nit is expressed as a probability of\nchoosing some action a\ngiven the environment is in some state s\nplease note these probabilities are not\nthe same as the state transition\nprobabilities\nthe mathematical relationship between\nstate transitions rewards and the policy\nis known as the bellman equation and it\ntells us the value meaning the expected\nfuture reward of a policy for some state\nof the environment\nreinforcement learning often though not\nalways means maximizing or solving that\nbellman equation more on that in future\nvideos\nthis desire to maximize reward leads to\na dilemma\nshould the agent maximize his short-term\nreward by exploiting the best-known\naction or should it be adventurous and\nchoose actions whose reward appears\nsmaller or maybe even unknown\nthis is known as the explore exploit\ndilemma and one popular solution is to\nchoose the best known action most of the\ntime and occasionally choose a\nsub-optimal action to see if there's\nsomething better out there\nthis is called an epsilon greedy policy\nwhen we think of reinforcement learning\nwe're often thinking about the algorithm\nthe agent uses to solve the bellman\nequation\nthese generally fall into two categories\nalgorithms that require a full model of\ntheir environment and algorithms that\ndon't\nwhat does this mean exactly to have a\nmodel of the environment\nas i said earlier actions cause the\nenvironment to transition from one state\nto another with some probability\nhaving a full model of the environment\nmeans knowing all the state transition\nprobabilities with certainty\nof course it's quite rare to know this\nbeforehand and so the algorithms that\nrequire a full model are of somewhat\nlimited utility\nthis class of algorithms is known as\ndynamic programming if we don't have a\nmodel or a model of the environment is\nincomplete we can't use dynamic\nprogramming\ninstead we have to rely on the family of\nmodel-free algorithms\none popular such algorithm is q learning\nor deep q learning which you studied on\nthis channel\nthese rely on keeping track of the state\ntransitions actions and rewards to learn\nthe model of the environment over time\nin the case of q-learning these\nparameters are saved in a table and in\nthe case of deep q learning the\nrelationships between them are expressed\nas an approximate functional\nrelationship which is learned by a deep\nneural network\nthat's really all there is at least at a\nhigh level\nso to recap\nreinforcement learning is a class of\nmachine learning algorithms that help an\nautonomous agent navigate a complex\nenvironment\nthe agent must be given a sequence of\nrewards or penalties to learn what is\nrequired of it the agent attempts to\nmaximize this reward over time or\nmathematical terms to solve the bellman\nequation\nthe algorithms that help the agent\nestimate future rewards fall into two\nclasses\nthose that require we know the state\ntransition probabilities for the\nenvironment beforehand and those that\ndon't\nsince knowing these probabilities is a\nrare luxury we often rely on model-free\nalgorithms like deep queue learning\nif you'd like to know more please check\nout some of the other videos on this\nchannel\ni hope this has been helpful please\nleave a comment a like and subscribe if\nyou haven't already look forward to\nseeing you all in the next video\nwelcome back to the free reinforcement\nlearning course from neuralnet.ai\ni'm your host phil tabor\nif you're not subscribed be sure to do\nthat now and hit the bell icon so you\nget notified for each new module in the\ncourse\nin module 1 we covered some essential\nconcepts in reinforcement learning so if\nyou haven't seen it go ahead and check\nit out now so this module makes more\nsense\nif you have seen it you may remember\nthat reinforcement learning basically\nboils down to an agent interacting with\nsome environment and receiving some\nrewards in the process\nthese rewards tell the agent what's good\nand bad and the agent uses some\nalgorithm to try to maximize rewards\nover time\nin practice what we get is a sequence of\ndecisions by the agent and each decision\ndoesn't just influence its immediate\nreward rather each decision influences\nall future rewards\nin mathematical terms we have a sequence\nof states actions and rewards that one\ncould call a decision process\nif each state in this process is purely\na function of the previous state and\naction of the agent then this process is\ncalled a markov decision process or mdp\nfor short\nthese are an idealized mathematical\nabstraction that we use to construct the\ntheory of reinforcement learning\nfor many problems this assumption can be\nbroken to various degrees\nhow much that really matters is often a\ncomplicated question and one we're just\ngoing to dodge for now\nregardless in most cases the assumption\nthat a process obeys the markov property\nis good enough and we can use all the\nresulting mathematics\nfor reinforcement learning problems\nby now i've said that a reinforcement\nlearning agent seeks to maximize rewards\nover time\nso how does this fit into a markov\ndecision process\nfrom the agent's perspective it receives\nsome sequence of rewards over time and\nthat sequence of rewards can be used to\nconstruct the expected return for the\nagent\nthen the return at some time step t\nis just the sum of the rewards that\nfollow\nall the way up to some final time\ncapital t\nthis final time step naturally\nintroduces the concept of episodes which\nare discrete periods of gameplay that\nare characterized by state transitions\nactions and rewards\nupon taking this final time step the\nagent enters some terminal state which\nis unique\nthis means that no matter how we end the\nepisode the terminal state is always the\nsame\nno future rewards follow after we reach\nthe terminal state so the agent's\nexpected reward for that terminal state\nis precisely\nzero\nwith a bit of creativity we call tasks\nthat can be broken into episodes\nepisodic tasks\nof course not all tasks are episodic\nmany are in fact continuous this is a\nbit of a problem since if the final time\nstep is at infinity the total reward\ncould also be infinite this makes the\nconcept of maximizing rewards\nmeaningless so we have to introduce an\nadditional concept\nthe fix and we use this for both\nepisodic and continuing tasks is the\nidea of discounting\nthis basically means the agent values\nfuture rewards less and less\nthis discounting follows a power law\nwhere each time step results in more and\nmore discounting\nthis hyperparameter gamma is called the\ndiscount rate and you've no doubt seen\nthis before in our videos on\nreinforcement learning\nif you use this form for the expected\nreturn and do some simple factoring you\nderive a really useful fact\nthere is a recursive relationship\nbetween rewards at subsequent time steps\nthis is something we'll exploit\nconstantly in reinforcement learning\nso we have an agent that is engaged in\nsome discrete processes receiving\nrewards and trying to maximize its\nexpected feature returns\nif you remember from the first lecture\nthe algorithm that determines how the\nagent is going to act is called its\npolicy\nsince the agent has a set of defined\nrules for how it's going to act in any\ngiven state\nit can use a sequence of states actions\nand rewards to figure out the value of\nany given state\nthe value of a state is the expected\nreturn when starting in that state and\nfollowing the policy\nit's given formally by the following\nequation\nin some problems like say q learning\nwe're more concerned with maximizing the\naction value function which tells the\nagent the value of taking some action\nwhile in some given state and following\nthe policy thereafter\n[Music]\nremember how i said we can exploit the\nrecursive relationship between\nsubsequent returns\nwell if we plug that into the expression\nfor the value function we actually\ndiscover that the value function itself\nis defined recursively\nthis is called the bellman equation from\nthe first module and this is the\nquantity many algorithms seek to\nmaximize\nthe bellman equation is really an\nexpectation value as it's a weighted\naverage of how likely each particular\nsequence of states actions and rewards\nis given the state transition\nprobabilities and the probability of the\nagent selecting that action\nmuch of the following material will\ninvolve coming up with various schemes\nto solve the bellman equation and evolve\nthe policy in such a way that the value\nfunction increases over time\nin the next module we'll take a look at\nthe explore exploit dilemma which is the\nexpression of the trade-off between long\nand short-term rewards\ni hope this has been helpful\nquestions comments suggestions leave\nthem below i read and answer all my\ncomments if you made it this far\nconsider subscribing so you get notified\nwhen the rest of the course drops i look\nforward to seeing you\nin the next video\nwelcome to module 3 of the free\nreinforcement learning course from\nneural net dot ai i'm your host phil\ntaper if you're not subscribed make sure\nto do that now so you don't miss the\nrest of the course\nin the previous video we learned about a\nspecial type of process called the\nmarkov decision process\nthere each state depends only on the\nprevious state and the action taken by\nthe agent\nthis leads to the recursive relationship\nbetween the agent's estimate of returns\nat successive time steps\nthis relationship extends to the agent's\nestimate of the value function which is\ngiven by the bellman equation\nas we covered in module 1 reinforcement\nlearning for the most part boils down to\nmaximizing this value function\nhowever it's not always so simple\nsurprise surprise\njust like you and i have trade-offs in\nreal life\nreinforcement learning agents are faced\nwith similar considerations\nshould the agent take the action that it\nknows will immediately provide the most\nreward or should it explore other\nactions to see if it can do better this\nconundrum is known as the explorer\nexploit dilemma and every reinforcement\nlearning algorithm has to deal with this\nfortunately there are many solutions and\nwe'll cover some of them here\none such solution is the idea of\noptimistic initial values\nwhen the agent starts playing the game\nit has to use some initial estimate for\nthe value or action value function\nthis estimate is totally arbitrary but\nif you know something about the reward\nstructure beforehand we can actually\ninitialize it in such a way as to\nencourage exploration\nsuppose we have an environment like our\ngrid world and the video on creating our\nown reinforcement learning environment\nin that environment the agent receives a\nreward of minus one for each step and so\nthe expected returns are always negative\nor zero no matter the state of the\nenvironment or the action the agent\ntakes\nso what would happen if we tell the\nagent that the value of all the state\naction pairs are positive or even zero\non the first move the agent picks some\naction randomly because all the actions\nlook identical\nit receives a reward of -1 and updates\nhis estimates accordingly\nso it's a bit disappointed it was\nexpecting chocolate cake and got a mud\npie\nthe next time it encounters that state\nit will take a different action because\nthe other actions have an estimate of\nzero reward for that state which is\nbetter than the negative reward it\nactually received this means that the\nagent ends up exploring all the state\naction pairs many times as each update\nmakes the agent's estimate more and more\naccurate\nwe never had to explicitly tell the\nagent to take exploratory actions\nbecause it's greed drobit to take\nexploratory actions after it became\ndisappointed with whatever action it\njust took\nagain this is called optimistic initial\nvalues\nanother feasible solution is to spend\nsome portion of the time choosing random\nactions and the majority of the time\nchoosing greedy actions this is called\nan epsilon greedy strategy and it's the\none we employ the most\nit's quite robust as we can change the\nrandom parameter over time so the agent\nconverges onto a nearly pure greedy\nstrategy\nthe proportion of the time the agent\nspends exploring is a hyper parameter of\nthe problem and we typically call it\nepsilon\none potential strategy is to start out\ncompletely randomly and then use some\ndecay function to gradually increase the\nproportion of greedy actions the agent\ntakes\nthe form of this function isn't\ncritically important it can be linear a\npower law or really any other function\nwhether or not the agent converges to a\npurely greedy strategy is going to\ndepend on the problem\nfor simple environments like the grid\nworld where we know the optimal solution\nbeforehand it makes quite a bit of sense\nconverged to a purely greedy strategy\nhowever with a game like space invaders\na popular environment from the open ai\ngym there are so many variables that\nit's hard to be sure the agent has\nsettled on the truly optimal strategy\nthe solution there is to leave epsilon\nat some small but finite value so the\nagent is occasionally taking exploratory\nactions to test its understanding of the\nenvironment\nall this discussion has made a very\nimportant assumption\nwe've assumed the agent only uses a\nsingle policy\nthe agent uses both the same policy to\nupdate his estimate of the value\nfunction as well as to generate actions\nthere's no rule this has to be the case\nin fact an agent can leverage two\npolicies\nit can use one policy to generate\nactions and then use the data that\ngenerates to update the value function\nfor some other policy\nthis is called off policy learning and\nthis is precisely what we use in\nq-learning\nthe agent uses some epsilon greedy\nstrategy to generate steps in the markov\nchain which is the sequence of state\naction rewards and resulting states and\nthen uses that data to update the\nestimate of the action value function\nfor the purely greedy action in effect\nwe're using an epsilon greedy strategy\nto update our estimate of the purely\ngreedy strategy\nneedless to say this works quite well\nand it's something we'll come back to in\nlater modules when we get to monte carlo\nmethods and temporal difference learning\nthat's it for now\nreinforcement learning agents seek to\nmaximize their total reward but face a\ndilemma of whether to maximize current\nreward or take exploratory steps with\nsuboptimal actions in the hope of\noptimizing long-term rewards\none solution is to bias the agent's\ninitial estimates in such a way that it\nencourages exploration before settling\non a purely greedy strategy\nanother is to spend some proportion of\nthe time exploring and the majority of\nthe time exploiting the best known\naction\nand finally the agent can leverage two\npolicies one to generate data and the\nother to update the estimate of the\naction value or value function\nin the next module we're going to get to\ndynamic programming class of model based\nreinforcement learning algorithms\nmake sure to subscribe so you don't miss\nthe remainder of this course and i look\nforward to seeing you in the next video\nwelcome back everybody to machine\nlearning with phil i am your host dr\nphil\nwhen we last touched on the open ai gym\nwe did q-learning to teach the cartpole\nrobot how to dance basically how to\nbalance the pole\nin this video we're going to take a look\nat a related algorithm called sarsa so\nthey're related in the sense that\nthey're both types of temporal\ndifference learning algorithms the\ndifference being that\nsarsa is an on policy method and q\nlearning is an off policy method\nhey appearance by the cat um\nif you if you don't know what that means\ni highly encourage you to check out my\ncourse reinforcement learning in motion\non manning publications i go in depth on\nall this stuff\nin that course uh enough plugging let's\nget back to it so the other cool thing\nis that it that sarsa as well as q\nlearning are model free meaning that you\ndo not need a complete model of your\nenvironment to actually get some\nlearning done and that's important\nbecause there's many cases in which you\ndon't know the full model of the\nenvironment what does that mean it means\nyou don't know the state transition\nprobabilities so if you're in some state\ns and take some action a what is the\nprobability you will end up in state s\nprime and get reward r those\nprobabilities are not completely known\nfor all problems and so\nalgorithms that that handle that\nuncertainty are critical for real-world\napplications\nanother neat thing is uh that this is a\nbootstrapped method meaning that it uses\nestimates to generate other estimates\nright so you don't need to know too much\nabout the system to get started you just\nmake some wild ass guesses and you get\nmoving let's take a look at the\nalgorithm\nso uh your first step is to initialize\nyour learning rate alpha\nuh and of course that's going to control\nthe rate of learning how quickly you\nmake adjustments to the q function uh\nthen you initialize the q function the q\nfunction is just the agent's estimate of\nits discounted future rewards\nstarting from a given state s and taking\nan action a and it may have some\nassumptions built in onto whether or not\nyou follow some particular policy or not\nbut that's a general gist\nso you need to initialize your state and\nchoose some initial action based on that\nstate using an epsilon greedy strategy\nfrom that function q\nthen you loop over the episode taking\nthe action getting your reward and your\nnew state s prime choose an action a\nprime as a function of that state s\nprime using epsilon greedy from your q\nfunction and then go ahead and update\nthe q function according to the update\nrule you see on the screen and then go\nahead and store your state prime into s\nand your a prime into a and loop until\nthe episode is done again in the course\ni go into many more details this is just\nquick and dirty a bit of a teaser video\nto get you guys\ninterested in the course and to give you\nsome useful information at the same time\nso with that being said let's go ahead\nand jump into the code i'm not going to\nbe doing typing on screen\nbut i will be showing you the relevant\ncode as we go along\nand boom we are back in the code editor\nso here i am using visual studio code um\neven on linux this is a great editor if\nyou're not using it i highly recommend\nit adam was a little bit buggy for me\nand of course sublime is now is nag ware\nso go ahead and give it a look if you\nhaven't already so\nwe need to define a function to\ntake the max action and that takes as\ninputs the q function as well as the\nstate and you're just converting\nthe um\nthe q function into an array and to a\nnumpy array uh for each action in that\nin that list\nand finding the arg max of that now\nrecall that in numpy the arg max takes\nthe\nreturns the first element of a max so if\nyou have two actions that are tied it'll\ngive you the first one so of course in\nthe cart poll example our action space\nis just moving left and right right if\nyou don't remember it's just a cart that\nslides along the x-axis\ntrying to keep a pole vertical\nof course this is a continuous space and\nthe q function is a discrete uh a\ndiscrete mathematical construct right so\nthe states are discrete numbers and so\nyou have to do a little trick here to\ndiscretize your space and so if you look\nin the documentation for the cartpole\nexample you'll find the limits on these\nvariables and you can use that to create\na linear space based out of it\nbased on those limits and divide it up\ninto 10 different buckets right so that\nway you get\nyou go from a continuous representation\nto a discrete representation of your\nstate space\nand then i define a small helper\nfunction here\nto get the state based on the\nobservation it just digitizes these\nit digitizes those linear spaces using\nthe observation that you pass in from\nthe open ai gym\nand it returns a four vector that is a\nthe buckets that correspond to the\nvalue of the\nelement of the observation\nthe main program we want to use\nsmall learning rate alpha 0.1\nfor a gamma something like 0.9 of course\nthe gamma is the discount factor it's\ndebatable whether or not you need it\nhere so\ndiscounting in general is used when you\ndon't know the\nwe you don't know for certain you're\ngoing to get some reward in the future\nso it doesn't make sense to give it a\n100 percent weight you could just as\neasily here use a 1.0\nbecause the state transition functions\nin the cardboard example are\ndeterministic as far as i'm aware some\nif i'm wrong please someone correct me\nand of course the epsilon for the\nepsilon greedy we're going to start out\nat 1.0\nyou'll see why here in a second\nand so you need to construct the set of\nstates which of course\njust corresponds to the\ninteger representations of our\ncontinuous space so you just have um\nranges from zero to zero to nine\nand you construct a four vector out of\nout of that right so you have zero zero\nzero one one one et cetera et cetera et\ncetera\nand initialize your q function here i'm\ngoing to initialize everything as\na zero right recall that we had to\nwe could initialize it arbitrarily but\nfor the terminal states you want that to\nbe zero because again the value of the\nterminal state is zero and a is two in a\nrange of two because we only have two\nactions move left move right\nwhoops\nalso i'm gonna run fifty thousand games\nif you have a slower computer you might\nwanna run fewer it takes quite a bit of\ntime to run and i'm going to track the\ntotal rewards as we go along\nso\njust a little helper line here to print\nout the the number of games you're\nplaying it's always good to know where\nyou are right you if it stops chugging\nalong you want to know if it's broken or\nactually doing something useful\nso you get your initial observation by\nresetting the environment get your state\nand calculate a random number and so you\ntake a maximum action if the random\nnumber is less than one minus epsilon so\nepsilon is starting out at one so if\nrandom is less than zero otherwise\nrandomly sample your action space\ndone flag defaults and your rewards for\nthe episode to zero\nthen you loop over the episode until\nyou're done\nand you go ahead and take the action a\ngetting your reward and the new\nobservation\nthe\nstate prime then is going to be the\nget state of the observation right these\nobservation is a four vector of\ncontinuous numbers that we have to\ntransform into a set of discrete\nintegers a four vector of discrete\nintegers then we go ahead and calculate\nanother random number and choose another\naction based upon that\nthen calculate sum up the total rewards\nand update the q function based on the\nupdate rule i gave you in the slides and\nof course set the state in action to the\nnew the s prime and a prime\nand after each episode you're going to\ndecrease epsilon because you want this\nyou don't want the epsilon to be\npermanently one right you want to\nencourage some amount of exploration and\nsome amount of exploitation so epsilon\nhas to be a function of time\nand just save your total rewards when\nit's all done it's going to go ahead and\nplot it out\nand you should see something similar to\nthe following i'm going to go ahead and\nrun that now\nand that is going to take a minute to\nrun and so here you have the output of\nthe source algorithm after running 50\n000 iterations so what you see is first\nof all a messy plot that's to be\nexpected with 50 000 games when you're\nplotting every single point but what you\nnotice immediately is that there is a\ngeneral trend upward and when epsilon\nreaches its minimum epsilon goes to zero\nand it does a fully exploitative\nstrategy the algorithm actually does a\nreally good job of hitting 200 moves\nmost of the time recall that 200 moves\nis the\num\n200 moves is the maximum number of steps\nfor the cart pull problem uh because\ngood algorithms can get it to to balance\nuh pretty much indefinitely so it would\nnever terminate so the open ai gym just\nterminates at 200 steps so anything\nclose to that is pretty good now one\nthing that's interesting is that it does\nhave a fair amount of variability it\ndoesn't actually\nbalance it 200 moves the entire time and\nthere are a number of reasons for this\nperhaps you can speculate below i invite\nyou to speculate my thought process is\nthat the\nthe way we have discretized this space\nisn't sufficient to characterize the\nproblem\nin such a way that the algorithm can\nlearn something completely and totally\nuseful so it just doesn't have enough\ninformation based on the ten thousand\nten of the four yeah ten thousand uh\nstates we've we've\ndiscretized it into\nuh and there could be other things that\nmatter you know uh you could have other\nfeatures for instance\ncombinations of velocities and positions\nthat matter so we could have under\nengineered the problem slightly but for\njust a quick little\nchunk of\n170 lines of code or so it's actually\nquite good\nso uh any questions be sure to leave\nthem below and hey if you've made it\nthis far and you haven't subscribed\nplease consider i'm going to be\nreleasing more and more content like\nthis i'm doing this full time now um\nand i look forward to seeing you all in\nthe next video oh and by the way in the\nnext video we're going to be taking a\nlook at double q learning uh which is\nyet another variation of these uh model\nfree bootstrap methods see you then\noh and one other thing if you want the\ncode for this i'll leave the code i'll\nleave the link to my github\nthis is code for my course reinforcement\nlearning and motion i'm just showcasing\nit here to show what you're going to\nlearn in the course so go ahead and\nclick the link in the description and\nit'll take you to my github where you\ncan find that code as well as all the\ncode from the course\nhope you like it see you guys in the\nnext video\nwelcome back everybody to machine\nlearning with phil i am your host dr\nphil\nin yesterday's video we took a look at\nsarsa in the open ai gym getting the\ncart pole to balance itself as promised\ntoday we are looking at the algorithm of\ndouble queue learning also in the\ncartpole openaigm environment\nso we touched on q-learning many many\nmonths ago and the basic idea is that\nq-learning is a\nmodel-free bootstrapped off-policy\nlearning algorithm what that means is\nmodel-free it does not need to know it\ndoes not need the complete state\ntransition dynamics of the environment\nto function it learns the game by\nplaying it bootstrapped in that it\ndoesn't need very many very much help\ngetting started it generates\nestimates using its initial estimates\nwhich are totally arbitrary except for\nthe\nterminal states off policy meaning that\nit is using a separate policy other than\nit is using a behavioral policy and a\ntarget policy to to both learn about the\nenvironment and generate behavior\nrespectively\nnow when you deal with problems that uh\nwhen you deal with algorithms that take\na maximizing approach to choosing\nactions you always get something called\nmaximization bias so say you have some\nset of states with many different\nactions such that the action value\nfunction for that state and all actions\nis zero\nthen the\nagent's estimate the q capital q of s a\ncan actually be\nwill actually have some uncertainty to\nit and that uncertainty is actually a\nspread in the values right and that\nspread causes it to have some amount of\npositive bias\nand the max of the true values is zero\nbut the max of the capital q the agent's\nestimate is positive hence you have a\npositive bias and that can often be a\nproblem in\nin reinforcement learning algorithms so\nthis happens because you're using the\nsame set of samples to max to determine\nthe maximizing action as well as the\nvalue of that action and one way to\nsolve this problem is to use two\nseparate q functions to determine the\nmax action and the value and you\nset up a relationship between them and\nthen you alternate between them as you\nplay the game so you're using one of\nthem to determine the max action one of\nthem to determine its value and you\nalternate between them so that you\neliminate the bias over time\nthat's double q learning in a nutshell\nthe algorithm is the following so you\ninitialize your alpha and your epsilon\nwhere alpha is your learning rate\nepsilon is what you use for epsilon\ngreedy you want to initialize the q1 and\nq2 functions for all states and actions\nin your state in action space of course\nthat's arbitrary except for the terminal\nstates which must have a value of zero\nand you loop over your set of episodes\nand you initialize your state and for\neach episode write each step within the\nepisode choose an action from uh using\nyour state using epsilon greedy strategy\nin the sum of q1 and q2 so you have the\ntwo separate q functions\nso if you're using single queue learning\nyou would take the\nmax action over just one queue but since\nyou're dealing with two you have to\naccount for that somehow right you could\ndo a max you could do a sum\nyou could do an average in this case\nwe're going to take the sum of the two q\nfunctions take that action get your\nreward observe the new state and then\nwith the 0.5 probability either update\nq1 or q2 according to this update rule\nhere and of course at the end of the\nstep go ahead and set your estate to the\nnew state and keep looping until the\ngame is done\nso clear as mud i hope so by the way if\nyou want more reinforcement learning\ncontent make sure to subscribe hit the\nbell icon so you get notified\nlet's get to it so next up we have our\ncode\nand here we are inside of our code\neditor again i'm using visual studio\ncode to take a look at our double queue\nlearning script i'm not going to be\ntyping into the terminal i think that's\nprobably a little bit annoying i'm just\ngoing to review the code as we go along\nif you have seen my video on the source\nalgorithm there's going to be a fair\namount of overlap because we're solving\nthe same set of problems over again the\nonly real difference is\nin that video we source it to calculate\nthe\naction value function and in this case\nwe're using double q learning\nagain we have a max action function what\nthis does is tells us the max action for\na given state to construct that you make\na numpy array out of a list that is for\na given state both actions and as we\nsaid in the video we're going to take\nthe sum of the q1 and q2 for a given\nstate for both actions\nyou want to take the arg max of that and\nrecall in numpy the arg max function\nif there is a tie returns the first\nelement so if the left and right actions\nboth have identical action value\nfunctions then it will return the left\naction consistently\nthat may or may not be a problem it's\njust something to be aware of\nand once again we have to discretize the\nspaces recall that the cart pull problem\nwhich is just the cart sliding along a\ntrack with a pole that is that must be\nmaintained vertically right\nin the cart pole example we have a\ncontinuous space the x and the theta can\nbe any number\nwithin a given range and likewise for\nthe velocities\nto deal with that we have a couple\noptions we could simply use neural\nnetworks to approximate those functions\nbut in this case we're going to use\na little trick to discretize the space\nso we're going to divide it up into 10\nequal chunks and any number that falls\nwithin a particular chunk will be\nassigned an integer so you'll go from a\ncontinuous to a\ndiscrete representation of your four\nvector the observation\nalong with that comes a get state\nobservat state\nalong with that comes a get state\nfunction\nthat\nyou pass in the observation\nand it just uses those\nuh digitized spaces excuse me just use\nthose linear spaces\nto use the numpy digitized function to\nget the integer representation\nof the respective elements of your\nobservation\ni've also added a function to plot the\nrunning average here i do this because\nin the sarsa video we end up with a\nlittle bit of a mess with 50 000 data\npoints this will plot a running average\nover the prior 100 games\nnext up we have to initialize our hyper\nparameters our learning rate of 0.1 this\njust controls the step size in the\nupdate equation the gamma is of course\nthe discount factor the agent uses in\nits estimates of the future rewards\nso i don't believe this should actually\nbe 0.9 i i left it here because it's not\nsuper critical as far as i'm concerned\nit should really be 1.0 and the reason\nis that\nthe purpose of discounting is to account\nfor uncertainties and future rewards if\nyou have some sequence of rewards with a\nprobability of receiving them then it\nmakes no sense to\ngive each of those rewards equal weight\nbecause you don't know if you're going\nto get them in the cart poll example the\nrewards are certain as far as i'm aware\nthe state transition probabilities are\none you know that if you move right\nyou're going to actually end up moving\nright you know deterministically where\nthe pole and the cart are going to move\nso it shouldn't be discounted as far as\ni'm concerned epsilon is just the\nepsilon factor for our for our epsilon\ngreedy\nalgorithm and\nthat's\npretty much it for hyperparameters of\nthe model next up we have to construct\nour state space so what this means oh\nbaby's unhappy\nthe state space is of course the\num the representation of the digitized\nspace so we're going to have\nfor the cart position you're going to\nhave 10 buckets the velocity is 10\nbuckets and likewise for the thetas\ntheta position and theta velocity so\nyou're going to have 10 to the four\npossible states so 10 000 states and\nthose are going to be numbered all the\nway from 0 0 0 to 99.99 that's all we're\ndoing here is we're constructing the set\nof states\nnext up we have to initialize our q\nfunctions recall that the initialization\nis arbitrary except for the terminal\nstate which must have a value of zero\nthe reason for this is that the\nterminal state by definition has a\nfuture value of zero because you stopped\nplaying the game right makes sense you\ncould initialize this randomly you could\ninitialize it with minus one plus one\ndoesn't really matter so long as the\nterminal state is zero for simplicity\ni'm initializing everything at zero\ni'm going to play a hundred thousand\ngames the reason is that this algorithm\neliminates bias but at the expense of\nconvergence speed so you have to let it\nrun a little bit longer\nuh an array for keeping track of the\ntotal rewards and we're gonna loop over\na hundred thousand games printing out\nevery five thousand games to let us know\nit's still running\nalways want to reset your done flag your\nrewards and reset the episode at the top\nand you're going to loop over the\nepisode getting your state\ncalculating a random number for your\nepsilon greedy strategy you're gonna set\nthe action to be the max action of q1\nand q2 if the random number is less than\none minus epsilon otherwise you're going\nto randomly sample your action space in\nany event you take that action get your\nnew state reward and done flag and go\nahead and tally up your reward and\nconvert that observation to a state s\nprime\nthen go ahead and calculate a separate\nrandom number the purpose of this random\nnumber is to determine which q function\nwe're going to update you know we're\ngoing to be using one to calculate\nwe're alternating between them because\nwe have to eliminate the\nmaximization bias right one is for\nfinding the max action one is for\nfinding the value of that action we\nalternate between episodes by way of\nthis random number\nin both cases you want to collect the\nyou want to calculate the max action\neither q1 or q2 and use the update rule\ni showed you in the slides to update the\nestimates for q1 and q2\nas you go\nat the end of the episode sorry at the\nend of the step excuse me you want to\nreset the\nold observation to the new one so that\nway you can get\nthe state up here and at the end of the\nepisode you want to go ahead and\ndecrease epsilon if you're not familiar\nwith this\nepsilon greedy is just a strategy for\ndealing with the explore exploit dilemma\nso an agent always has some estimate of\nthe future rewards based on its model of\nthe environment or its experience\nplaying the game if it's model free or\na model uh problem\nright it can either explore or exploit\nits best known actions so one way of\ndealing with the dilemma of how much\ntime should you spend exploring versus\nhow much time should you spend\nexploiting is to use something called\nepsilon greedy meaning that some\npercentage of the time you explore some\npercentage of the time you exploit\nand the way that you get it to settle on\na greedy strategy is to gradually\ndecrease that\nexploration parameter epsilon over time\nand that's what we're doing here\nand of course you want to keep track of\nthe total rewards for that episode and\nrecall in the current poll example\nthe agent gets a reward of positive one\nevery time the poll stays\nvertical so every move that it doesn't\nflop over it gets one point\nand at the end you're going to go ahead\nand plot your running averages\nso i'm going to go ahead and run that\nand that'll take a minute uh while it's\nrunning i want to ask you guys a\nquestion so what type of material do you\nwant to see\nfrom what i'm seeing in the data the\nthe reinforcement learning stuff is\nimmensely popular my other content not\nso much so i'm going to keep focusing on\nthis type of stuff but are you happy\nseeing the sutton bardo type\nintroductory material or do you want to\nsee more deep learning type material\nright there's a whole host of dozens of\ndeep reinforcement learning algorithms\nwe can cover\nbut i'm actually quite content to cover\nthis stuff because\ni believe that if you can't master the\nbasics then the deep learning stuff\nisn't going to make sense anyway right\nbecause you have the complexity of deep\nlearning on top of\nthe\ncomplexity of the reinforcement learning\nmaterial on top of it\nso if there's anything in particular you\nguys want to see make sure to leave a\ncomment below and hey if you haven't\nsubscribed and you happen to like\nreinforcement learning and machine\nlearning material please consider doing\nso if you like the video make sure to\nleave a thumbs up hey if you thought it\nsucked go ahead and leave a thumbs down\nand tell me why i'm happy to answer the\ncomments answer your objections and if\nyou guys have suggestions for\nimprovement i'm all ears\nand here we are it is finally finished\nwith all hundred thousand episodes\nand you can see here the running average\nover the course of those games\nas you would expect the agent begins to\nlearn fairly quickly\nbalancing the cart pull more and more\nand more by about 60 000 games it starts\nto hit the consistently hit the 200 move\nthreshold where it is able to balance\nthe cart pull all 200 moves of the game\nnow recall this was with a gamma of 1.0\ni'm going to go ahead and rerun this\nwith a gamma of 0.9 and see how it does\nso burn this image into your brain and\ni'm going to go ahead and check it out\nwith a gamma of 0.9 and see if we can do\nany better\nand we are back with the second run\nusing a gamma of 0.9 and you can see\nsomething quite interesting here\nso it actually\nonly kind of ever reaches the 200 mark\nuh\njust for a handful of games and then\nkind of stutters along actually\ndecreasing in performance as it goes\nalong so something funny is going on\nhere and to be frank i off the top of my\nhead i'm not entirely certain why so\ni invite you all to speculate however\nthe\nwhat's also interesting is that i this\nis the second time i'm recording this i\nrecorded it earlier and didn't scroll\ndown the code so you ended up staring at\nthe same chunk of stuff had to redo it\nand in that case i had a gamma of 0.9 as\nwell and it seemed to work just fine so\ni suspect there's some significant\nvariation here to do with the random\nnumber generator\num\nit could just all be due to that right\nthis is a complex space and it\nwanders around different portions this\ncould happen potentially because it\ndoesn't visit all areas of the parameter\nspace enough times to get a reasonable\nestimate of the samples and there may be\nsome type of bias on where it visits\nlater on in the course of the episodes\nalthough that sounds kind of unlikely to\nme but either way that is double q\nlearning you can see how the\nhyper parameters actually affect the\nmodel it seems to have a fairly large\neffect as you might expect\nand the next video we're going to be\ntaking a look at double sarsa so if you\nare not subscribed i ask you to please\nconsider doing so hit the notification\nicon so you can see when i release that\nvideo i look forward to seeing you all\nin the next video\nwell i hope that was helpful everyone so\nwhat did we learn we learned about\nq-learning policy gradient methods sarsa\ndouble q learning and even how to create\nour own reinforcement learning\nenvironments this is a very solid\nfoundation in the topic of reinforcement\nlearning and you're pretty well prepared\nto go out and explore more advanced\ntopics so what are those more advanced\ntopics so right now the forefront are\nthings like deep deterministic policy\ngradients which is as you might guess\nfrom the name a more\nadvanced version of policy gradient\nmethods they're also actor critic\nmethods\nuh behavioral cloning there's all sorts\nof more advanced topics out there that\nyou're now pretty well equipped to go\nexplore\nthese are particularly useful in\nenvironments where you have continuous\naction spaces so all the environments we\nstudied in this set of tutorials have a\ndiscrete action space meaning the agent\nonly moves\nor takes some discrete set of actions\nother environments such as the bipedal\nwalker\ncar racing things of that nature have\ncontinuous state spaces so excuse me\ncontinuous action spaces which require\ndifferent mechanisms to solve q learning\nreally can't handle it so you're now\nfree to go ahead and check that stuff\nout if you've made it this far please\nconsider subscribing to my channel\nmachine learning with phil and i hope\nthis is helpful for all of you leave a\ncomment down below and make sure to\nshare this and i'll see you all\nin the next video\n",
  "words": [
    "welcome",
    "reinforcement",
    "learning",
    "jump",
    "start",
    "series",
    "host",
    "phil",
    "tabor",
    "know",
    "physicist",
    "former",
    "semiconductor",
    "engineer",
    "turned",
    "machine",
    "learning",
    "practitioner",
    "series",
    "tutorials",
    "going",
    "learn",
    "everything",
    "need",
    "know",
    "get",
    "started",
    "reinforcement",
    "learning",
    "need",
    "prior",
    "exposure",
    "really",
    "need",
    "basic",
    "familiarity",
    "python",
    "far",
    "requirements",
    "course",
    "pretty",
    "light",
    "need",
    "open",
    "ai",
    "gym",
    "going",
    "taking",
    "advantage",
    "rather",
    "extensively",
    "also",
    "need",
    "atari",
    "extension",
    "play",
    "games",
    "like",
    "breakout",
    "space",
    "invaders",
    "also",
    "need",
    "box",
    "2d",
    "extension",
    "new",
    "lander",
    "environment",
    "beyond",
    "need",
    "tensorflow",
    "library",
    "well",
    "pytorch",
    "going",
    "tutorials",
    "tensorflow",
    "pi",
    "torch",
    "bit",
    "stronger",
    "emphasis",
    "tensorflow",
    "going",
    "teach",
    "course",
    "somewhat",
    "fashion",
    "meaning",
    "going",
    "get",
    "really",
    "important",
    "exciting",
    "new",
    "stuff",
    "like",
    "deep",
    "q",
    "learning",
    "policy",
    "gradient",
    "methods",
    "first",
    "kind",
    "back",
    "take",
    "look",
    "things",
    "like",
    "sarsa",
    "double",
    "q",
    "learning",
    "even",
    "get",
    "make",
    "reinforcement",
    "learning",
    "environments",
    "code",
    "grid",
    "world",
    "solve",
    "regular",
    "q",
    "learning",
    "missed",
    "something",
    "code",
    "worry",
    "keep",
    "code",
    "github",
    "link",
    "pin",
    "comment",
    "also",
    "link",
    "relevant",
    "timestamps",
    "material",
    "case",
    "want",
    "jump",
    "around",
    "maybe",
    "topics",
    "interest",
    "want",
    "get",
    "additional",
    "background",
    "information",
    "explainer",
    "videos",
    "questions",
    "comments",
    "leave",
    "address",
    "let",
    "get",
    "video",
    "going",
    "learn",
    "everything",
    "need",
    "know",
    "implement",
    "scratch",
    "need",
    "prior",
    "exposure",
    "even",
    "really",
    "need",
    "much",
    "familiarity",
    "reinforcement",
    "learning",
    "get",
    "everything",
    "need",
    "video",
    "new",
    "phil",
    "help",
    "get",
    "started",
    "machine",
    "learning",
    "upload",
    "three",
    "videos",
    "week",
    "make",
    "sure",
    "subscribe",
    "miss",
    "imagine",
    "gotten",
    "recognition",
    "deserve",
    "form",
    "offers",
    "machine",
    "learning",
    "engineering",
    "position",
    "google",
    "facebook",
    "amazon",
    "three",
    "offering",
    "boatload",
    "money",
    "dreams",
    "big",
    "balling",
    "interrupted",
    "realization",
    "starting",
    "salary",
    "well",
    "starting",
    "salary",
    "got",
    "friends",
    "three",
    "companies",
    "reach",
    "find",
    "promotion",
    "schedules",
    "facebook",
    "offers",
    "two",
    "hundred",
    "fifty",
    "thousand",
    "dollars",
    "start",
    "ten",
    "percent",
    "raise",
    "three",
    "years",
    "forty",
    "percent",
    "probability",
    "quit",
    "google",
    "offers",
    "two",
    "hundred",
    "thousand",
    "dollars",
    "start",
    "twenty",
    "percent",
    "raise",
    "three",
    "years",
    "25",
    "probability",
    "quit",
    "amazon",
    "offers",
    "350",
    "000",
    "start",
    "five",
    "percent",
    "raise",
    "five",
    "years",
    "sixty",
    "percent",
    "chance",
    "end",
    "washing",
    "take",
    "three",
    "big",
    "money",
    "future",
    "raises",
    "far",
    "certain",
    "sort",
    "problem",
    "reinforcement",
    "learning",
    "designed",
    "solve",
    "agent",
    "maximize",
    "rewards",
    "environments",
    "uncertainties",
    "learning",
    "powerful",
    "solution",
    "lets",
    "agents",
    "learn",
    "environment",
    "real",
    "time",
    "quickly",
    "learn",
    "novel",
    "strategies",
    "mastering",
    "task",
    "hand",
    "q",
    "learning",
    "works",
    "mapping",
    "pairs",
    "states",
    "actions",
    "future",
    "rewards",
    "agent",
    "expects",
    "receive",
    "decides",
    "actions",
    "take",
    "based",
    "strategy",
    "called",
    "epsilon",
    "greedy",
    "action",
    "selection",
    "basically",
    "agent",
    "spends",
    "time",
    "taking",
    "random",
    "actions",
    "explore",
    "environment",
    "remainder",
    "time",
    "selecting",
    "actions",
    "highest",
    "known",
    "expected",
    "feature",
    "rewards",
    "epsilon",
    "refers",
    "fraction",
    "time",
    "agent",
    "spends",
    "exploring",
    "model",
    "hyperparameter",
    "0",
    "gradually",
    "decrease",
    "epsilon",
    "time",
    "finite",
    "value",
    "agent",
    "eventually",
    "converges",
    "mostly",
    "greedy",
    "strategy",
    "probably",
    "want",
    "set",
    "epsilon",
    "zero",
    "exactly",
    "since",
    "important",
    "always",
    "testing",
    "agent",
    "model",
    "environment",
    "selecting",
    "taking",
    "action",
    "agent",
    "gets",
    "reward",
    "environment",
    "sets",
    "apart",
    "many",
    "reinforcement",
    "learning",
    "algorithms",
    "performs",
    "learning",
    "operation",
    "time",
    "step",
    "instead",
    "end",
    "episode",
    "case",
    "policy",
    "gradient",
    "methods",
    "point",
    "important",
    "make",
    "distinction",
    "traditional",
    "q",
    "learning",
    "works",
    "literally",
    "keeping",
    "table",
    "state",
    "action",
    "pairs",
    "implementing",
    "python",
    "could",
    "use",
    "dictionary",
    "state",
    "action",
    "tuples",
    "keys",
    "feasible",
    "environments",
    "limited",
    "number",
    "discrete",
    "states",
    "actions",
    "agent",
    "need",
    "keep",
    "track",
    "history",
    "since",
    "update",
    "table",
    "place",
    "plays",
    "game",
    "way",
    "agent",
    "updates",
    "memories",
    "taking",
    "difference",
    "expected",
    "returns",
    "actions",
    "took",
    "action",
    "highest",
    "possible",
    "future",
    "returns",
    "ends",
    "biasing",
    "agent",
    "estimates",
    "time",
    "towards",
    "actions",
    "end",
    "producing",
    "best",
    "possible",
    "outcomes",
    "dealing",
    "environments",
    "huge",
    "number",
    "states",
    "state",
    "space",
    "continuous",
    "really",
    "ca",
    "use",
    "table",
    "case",
    "use",
    "deep",
    "neural",
    "networks",
    "take",
    "observations",
    "environment",
    "turn",
    "discrete",
    "outputs",
    "correspond",
    "value",
    "action",
    "called",
    "deep",
    "q",
    "learning",
    "reason",
    "use",
    "neural",
    "networks",
    "universal",
    "function",
    "approximators",
    "turns",
    "deep",
    "neural",
    "nets",
    "approximate",
    "continuous",
    "function",
    "precisely",
    "relationship",
    "states",
    "actions",
    "feature",
    "returns",
    "function",
    "agent",
    "wants",
    "learn",
    "maximize",
    "future",
    "rewards",
    "deepq",
    "learning",
    "agents",
    "memory",
    "states",
    "saw",
    "actions",
    "took",
    "rewards",
    "received",
    "learning",
    "step",
    "agent",
    "samples",
    "subset",
    "memory",
    "feed",
    "states",
    "neural",
    "network",
    "compute",
    "values",
    "actions",
    "took",
    "like",
    "regular",
    "q",
    "learning",
    "agent",
    "also",
    "computes",
    "values",
    "maximal",
    "actions",
    "uses",
    "difference",
    "two",
    "loss",
    "function",
    "update",
    "weights",
    "neural",
    "network",
    "let",
    "talk",
    "implementation",
    "practice",
    "end",
    "two",
    "deep",
    "neural",
    "networks",
    "one",
    "network",
    "called",
    "evaluation",
    "network",
    "evaluate",
    "current",
    "state",
    "see",
    "action",
    "take",
    "another",
    "network",
    "called",
    "target",
    "network",
    "used",
    "calculate",
    "value",
    "maximal",
    "actions",
    "learning",
    "step",
    "reasoning",
    "need",
    "two",
    "networks",
    "little",
    "complicated",
    "basically",
    "boils",
    "eliminating",
    "bias",
    "estimates",
    "values",
    "actions",
    "weight",
    "target",
    "network",
    "periodically",
    "updated",
    "weights",
    "evaluation",
    "network",
    "estimates",
    "maximal",
    "actions",
    "get",
    "accurate",
    "time",
    "dealing",
    "environment",
    "gives",
    "pixel",
    "images",
    "like",
    "atari",
    "library",
    "openai",
    "gym",
    "need",
    "use",
    "convolutional",
    "neural",
    "network",
    "perform",
    "feature",
    "extraction",
    "images",
    "output",
    "convolutional",
    "network",
    "flattened",
    "fed",
    "dense",
    "neural",
    "network",
    "approximate",
    "values",
    "action",
    "agent",
    "environment",
    "movement",
    "additional",
    "problem",
    "solve",
    "take",
    "look",
    "image",
    "tell",
    "way",
    "ball",
    "paddle",
    "moving",
    "pretty",
    "much",
    "impossible",
    "get",
    "sense",
    "motion",
    "single",
    "image",
    "limitation",
    "applies",
    "deep",
    "q",
    "learning",
    "agent",
    "well",
    "means",
    "need",
    "way",
    "stacking",
    "frames",
    "give",
    "agent",
    "sense",
    "motion",
    "clear",
    "means",
    "convolutional",
    "neural",
    "network",
    "takes",
    "batch",
    "stacked",
    "images",
    "input",
    "rather",
    "single",
    "image",
    "choosing",
    "action",
    "reasonably",
    "straightforward",
    "generate",
    "random",
    "number",
    "less",
    "epsilon",
    "parameter",
    "pick",
    "action",
    "random",
    "greater",
    "agent",
    "epsilon",
    "feed",
    "set",
    "stacked",
    "frames",
    "evaluation",
    "network",
    "get",
    "values",
    "actions",
    "current",
    "state",
    "find",
    "maximal",
    "action",
    "take",
    "get",
    "new",
    "state",
    "back",
    "environment",
    "add",
    "end",
    "stacked",
    "frames",
    "store",
    "stacked",
    "frames",
    "actions",
    "rewards",
    "agent",
    "memory",
    "perform",
    "learning",
    "operation",
    "sampling",
    "agent",
    "memory",
    "really",
    "important",
    "get",
    "random",
    "sampling",
    "memory",
    "avoid",
    "getting",
    "trapped",
    "one",
    "little",
    "corner",
    "parameter",
    "space",
    "long",
    "keep",
    "track",
    "state",
    "transitions",
    "actions",
    "rewards",
    "way",
    "pretty",
    "safe",
    "feed",
    "random",
    "batch",
    "data",
    "evaluation",
    "target",
    "networks",
    "compute",
    "loss",
    "function",
    "perform",
    "loss",
    "minimization",
    "step",
    "neural",
    "network",
    "really",
    "deep",
    "cue",
    "learning",
    "couple",
    "neural",
    "networks",
    "memory",
    "keep",
    "track",
    "states",
    "lots",
    "gpu",
    "horsepower",
    "handle",
    "training",
    "speaking",
    "course",
    "need",
    "pick",
    "framework",
    "preferably",
    "one",
    "lets",
    "use",
    "gpu",
    "learning",
    "pytorch",
    "tensorflow",
    "great",
    "choices",
    "support",
    "model",
    "checkpointing",
    "critical",
    "stuff",
    "ca",
    "dedicate",
    "day",
    "model",
    "training",
    "make",
    "sure",
    "share",
    "video",
    "found",
    "helpful",
    "subscribe",
    "miss",
    "future",
    "reinforcement",
    "learning",
    "content",
    "see",
    "next",
    "video",
    "tutorial",
    "going",
    "learn",
    "use",
    "deep",
    "q",
    "learning",
    "teach",
    "agent",
    "play",
    "breakout",
    "tensorflow",
    "framework",
    "need",
    "know",
    "anything",
    "deep",
    "q",
    "learning",
    "even",
    "need",
    "know",
    "anything",
    "tensorflow",
    "follow",
    "along",
    "let",
    "get",
    "started",
    "new",
    "channel",
    "phil",
    "physicist",
    "former",
    "semiconductor",
    "engineer",
    "turned",
    "machine",
    "learning",
    "practitioner",
    "machine",
    "learning",
    "phil",
    "deep",
    "reinforcement",
    "learning",
    "artificial",
    "intelligence",
    "tutorials",
    "three",
    "times",
    "week",
    "kind",
    "thing",
    "hit",
    "subscribe",
    "button",
    "let",
    "get",
    "video",
    "familiar",
    "deep",
    "q",
    "learning",
    "basic",
    "idea",
    "agent",
    "uses",
    "convolutional",
    "neural",
    "network",
    "turn",
    "set",
    "images",
    "game",
    "set",
    "feature",
    "vectors",
    "fed",
    "fully",
    "connected",
    "layer",
    "determine",
    "value",
    "actions",
    "given",
    "set",
    "states",
    "case",
    "set",
    "states",
    "going",
    "stack",
    "frames",
    "want",
    "agent",
    "sense",
    "motion",
    "go",
    "along",
    "stacking",
    "frames",
    "passing",
    "network",
    "asking",
    "network",
    "hey",
    "value",
    "either",
    "actions",
    "move",
    "left",
    "move",
    "right",
    "fire",
    "ball",
    "going",
    "split",
    "two",
    "classes",
    "one",
    "house",
    "deep",
    "q",
    "networks",
    "house",
    "agent",
    "agent",
    "class",
    "going",
    "stuff",
    "get",
    "later",
    "let",
    "go",
    "ahead",
    "start",
    "imports",
    "need",
    "os",
    "handle",
    "model",
    "saving",
    "need",
    "numpy",
    "handle",
    "basic",
    "random",
    "functions",
    "course",
    "tensorflow",
    "build",
    "agent",
    "start",
    "deepq",
    "network",
    "initializer",
    "pretty",
    "straightforward",
    "going",
    "take",
    "learning",
    "rate",
    "number",
    "actions",
    "name",
    "network",
    "important",
    "gon",
    "na",
    "two",
    "networks",
    "one",
    "select",
    "action",
    "one",
    "tell",
    "us",
    "value",
    "action",
    "later",
    "number",
    "dimensions",
    "first",
    "fully",
    "connected",
    "layer",
    "input",
    "dimensions",
    "environment",
    "atari",
    "gym",
    "sorry",
    "atari",
    "library",
    "open",
    "ai",
    "gym",
    "images",
    "210",
    "160",
    "resolution",
    "going",
    "pass",
    "set",
    "frames",
    "give",
    "agent",
    "sense",
    "motion",
    "going",
    "pass",
    "four",
    "frames",
    "particular",
    "going",
    "210",
    "default",
    "210",
    "160",
    "going",
    "cropping",
    "later",
    "get",
    "minute",
    "also",
    "need",
    "directory",
    "save",
    "model",
    "next",
    "thing",
    "need",
    "tensorflow",
    "session",
    "instantiates",
    "everything",
    "graph",
    "network",
    "wants",
    "call",
    "build",
    "network",
    "function",
    "add",
    "everything",
    "graph",
    "added",
    "everything",
    "graph",
    "initialize",
    "important",
    "tensorflow",
    "complain",
    "best",
    "way",
    "calling",
    "tf",
    "global",
    "variables",
    "initializer",
    "function",
    "thing",
    "need",
    "way",
    "saving",
    "models",
    "go",
    "along",
    "critical",
    "deep",
    "queue",
    "network",
    "takes",
    "forever",
    "train",
    "let",
    "train",
    "10",
    "hours",
    "averages",
    "score",
    "two",
    "three",
    "points",
    "per",
    "set",
    "uh",
    "whatever",
    "number",
    "lies",
    "gets",
    "going",
    "train",
    "quite",
    "time",
    "going",
    "want",
    "able",
    "save",
    "go",
    "along",
    "stuff",
    "right",
    "course",
    "want",
    "way",
    "saving",
    "checkpoint",
    "files",
    "next",
    "thing",
    "need",
    "way",
    "keeping",
    "track",
    "parameters",
    "particular",
    "network",
    "like",
    "tell",
    "tensorflow",
    "want",
    "keep",
    "track",
    "trainable",
    "variables",
    "network",
    "corresponds",
    "whatever",
    "name",
    "particular",
    "network",
    "use",
    "later",
    "copy",
    "one",
    "network",
    "another",
    "next",
    "let",
    "build",
    "network",
    "gon",
    "na",
    "encase",
    "everything",
    "scope",
    "based",
    "network",
    "name",
    "going",
    "placeholder",
    "variables",
    "tell",
    "us",
    "inputs",
    "model",
    "going",
    "want",
    "input",
    "stack",
    "images",
    "atari",
    "game",
    "want",
    "input",
    "actions",
    "agent",
    "took",
    "well",
    "target",
    "value",
    "q",
    "network",
    "get",
    "minute",
    "convention",
    "naming",
    "naming",
    "placeholders",
    "layers",
    "going",
    "see",
    "repeated",
    "throughout",
    "tensorflow",
    "library",
    "reason",
    "makes",
    "debugging",
    "easier",
    "get",
    "error",
    "tell",
    "variable",
    "layer",
    "caused",
    "error",
    "handy",
    "probably",
    "tell",
    "shape",
    "going",
    "one",
    "hot",
    "encoding",
    "actions",
    "thing",
    "q",
    "target",
    "convention",
    "using",
    "none",
    "first",
    "parameter",
    "shape",
    "allows",
    "train",
    "batch",
    "stuff",
    "important",
    "virtually",
    "every",
    "deep",
    "learning",
    "application",
    "want",
    "pass",
    "batch",
    "information",
    "right",
    "case",
    "going",
    "passing",
    "batches",
    "stacked",
    "frames",
    "get",
    "moment",
    "next",
    "thing",
    "start",
    "build",
    "scroll",
    "little",
    "bit",
    "start",
    "building",
    "convolutional",
    "neural",
    "network",
    "let",
    "start",
    "building",
    "layers",
    "first",
    "one",
    "32",
    "filters",
    "kernel",
    "size",
    "8x8",
    "strides",
    "4",
    "name",
    "conf",
    "thing",
    "need",
    "initializer",
    "going",
    "use",
    "initializer",
    "deepmind",
    "team",
    "used",
    "paper",
    "reason",
    "want",
    "learn",
    "experts",
    "may",
    "well",
    "going",
    "make",
    "life",
    "significantly",
    "easier",
    "going",
    "variance",
    "scaling",
    "initializer",
    "scale",
    "want",
    "activate",
    "relu",
    "function",
    "right",
    "con1",
    "activated",
    "next",
    "layer",
    "pretty",
    "similar",
    "take",
    "activated",
    "output",
    "first",
    "layer",
    "input",
    "take",
    "64",
    "filters",
    "familiar",
    "filter",
    "check",
    "video",
    "convolutional",
    "neural",
    "networks",
    "kernel",
    "size",
    "case",
    "four",
    "four",
    "strides",
    "two",
    "name",
    "conf",
    "two",
    "go",
    "ahead",
    "copy",
    "initializer",
    "second",
    "convolutional",
    "layer",
    "gon",
    "na",
    "something",
    "similar",
    "third",
    "course",
    "take",
    "conf2",
    "activated",
    "128",
    "filters",
    "two",
    "sorry",
    "three",
    "three",
    "kernel",
    "good",
    "grief",
    "stride",
    "one",
    "name",
    "conf",
    "3",
    "initializer",
    "course",
    "want",
    "activate",
    "well",
    "next",
    "step",
    "outputs",
    "convolutional",
    "net",
    "neural",
    "network",
    "want",
    "flatten",
    "pass",
    "dense",
    "network",
    "get",
    "q",
    "values",
    "values",
    "state",
    "action",
    "pair",
    "let",
    "fc1",
    "dimms",
    "come",
    "need",
    "value",
    "activation",
    "oops",
    "initializer",
    "dense",
    "layer",
    "next",
    "need",
    "determine",
    "q",
    "values",
    "q",
    "q",
    "learning",
    "refers",
    "value",
    "state",
    "action",
    "pair",
    "nomenclature",
    "output",
    "neural",
    "network",
    "course",
    "want",
    "one",
    "output",
    "action",
    "gets",
    "initializer",
    "activating",
    "yet",
    "uh",
    "want",
    "get",
    "linear",
    "values",
    "sorry",
    "linear",
    "activation",
    "output",
    "network",
    "next",
    "thing",
    "need",
    "actual",
    "value",
    "q",
    "action",
    "remember",
    "actions",
    "placeholder",
    "next",
    "thing",
    "need",
    "every",
    "neural",
    "network",
    "loss",
    "function",
    "want",
    "squared",
    "difference",
    "q",
    "value",
    "network",
    "outputs",
    "something",
    "called",
    "q",
    "target",
    "q",
    "target",
    "let",
    "get",
    "way",
    "q",
    "learning",
    "works",
    "time",
    "step",
    "form",
    "temporal",
    "difference",
    "learning",
    "every",
    "time",
    "step",
    "learns",
    "says",
    "hey",
    "took",
    "action",
    "maximal",
    "action",
    "could",
    "taken",
    "takes",
    "delta",
    "whatever",
    "action",
    "took",
    "maximal",
    "action",
    "uses",
    "update",
    "neural",
    "network",
    "loss",
    "function",
    "training",
    "operation",
    "form",
    "gradient",
    "descent",
    "uh",
    "atom",
    "optimizer",
    "case",
    "uh",
    "learning",
    "rate",
    "want",
    "minimize",
    "loss",
    "function",
    "let",
    "give",
    "room",
    "almost",
    "network",
    "next",
    "thing",
    "need",
    "way",
    "saving",
    "files",
    "right",
    "save",
    "loading",
    "well",
    "reason",
    "want",
    "said",
    "models",
    "take",
    "notoriously",
    "long",
    "time",
    "train",
    "may",
    "want",
    "start",
    "stop",
    "go",
    "along",
    "look",
    "checkpoint",
    "file",
    "load",
    "graph",
    "file",
    "save",
    "load",
    "graph",
    "current",
    "session",
    "going",
    "save",
    "frequently",
    "train",
    "something",
    "like",
    "every",
    "10",
    "games",
    "function",
    "takes",
    "current",
    "session",
    "opposite",
    "file",
    "pretty",
    "handy",
    "deep",
    "q",
    "network",
    "takes",
    "batch",
    "images",
    "environment",
    "case",
    "breakout",
    "passes",
    "convolutional",
    "neural",
    "network",
    "feature",
    "selection",
    "passes",
    "fully",
    "connected",
    "layer",
    "determine",
    "value",
    "given",
    "action",
    "uses",
    "maximum",
    "value",
    "next",
    "action",
    "determine",
    "loss",
    "function",
    "perform",
    "training",
    "network",
    "network",
    "via",
    "back",
    "propagation",
    "next",
    "need",
    "agent",
    "includes",
    "everything",
    "else",
    "learnings",
    "memories",
    "good",
    "stuff",
    "going",
    "take",
    "something",
    "called",
    "alpha",
    "learning",
    "rate",
    "gamma",
    "discount",
    "factor",
    "hyper",
    "parameter",
    "model",
    "memory",
    "size",
    "number",
    "actions",
    "epsilon",
    "determines",
    "often",
    "takes",
    "random",
    "action",
    "batch",
    "size",
    "parameter",
    "tells",
    "us",
    "often",
    "want",
    "replace",
    "target",
    "network",
    "set",
    "input",
    "dimms",
    "use",
    "210",
    "160",
    "four",
    "one",
    "moment",
    "cat",
    "whining",
    "need",
    "directory",
    "save",
    "q",
    "next",
    "network",
    "need",
    "directory",
    "save",
    "q",
    "evaluation",
    "said",
    "two",
    "networks",
    "one",
    "tells",
    "us",
    "action",
    "take",
    "one",
    "tells",
    "us",
    "value",
    "action",
    "let",
    "go",
    "ahead",
    "start",
    "initializer",
    "take",
    "random",
    "actions",
    "need",
    "know",
    "action",
    "space",
    "set",
    "possible",
    "actions",
    "need",
    "know",
    "number",
    "actions",
    "need",
    "discount",
    "factor",
    "gamma",
    "tells",
    "agent",
    "much",
    "wants",
    "discount",
    "future",
    "rewards",
    "memory",
    "size",
    "tells",
    "us",
    "many",
    "transitions",
    "store",
    "memory",
    "course",
    "epsilon",
    "epsilon",
    "greedy",
    "need",
    "network",
    "tell",
    "agent",
    "value",
    "next",
    "action",
    "pass",
    "alpha",
    "learning",
    "rate",
    "number",
    "actions",
    "input",
    "dimms",
    "name",
    "checkpoint",
    "directory",
    "okay",
    "two",
    "networks",
    "next",
    "thing",
    "need",
    "memory",
    "q",
    "learning",
    "works",
    "saving",
    "state",
    "action",
    "reward",
    "new",
    "state",
    "transitions",
    "memory",
    "also",
    "going",
    "save",
    "terminal",
    "flags",
    "tell",
    "agent",
    "whether",
    "game",
    "done",
    "go",
    "calculation",
    "reward",
    "learning",
    "function",
    "need",
    "state",
    "memory",
    "numpy",
    "array",
    "zeros",
    "shape",
    "mem",
    "size",
    "input",
    "dimms",
    "save",
    "set",
    "four",
    "transitions",
    "four",
    "frames",
    "stacked",
    "four",
    "frames",
    "number",
    "memories",
    "also",
    "need",
    "action",
    "memory",
    "handle",
    "one",
    "store",
    "one",
    "hot",
    "encoding",
    "actions",
    "one",
    "dimensional",
    "store",
    "agent",
    "memory",
    "rewards",
    "need",
    "terminal",
    "memory",
    "saves",
    "memory",
    "done",
    "flex",
    "save",
    "ram",
    "save",
    "one",
    "int8",
    "know",
    "thing",
    "actions",
    "important",
    "going",
    "saving",
    "25",
    "000",
    "transitions",
    "pc",
    "consumes",
    "47",
    "gigabytes",
    "48",
    "gigabytes",
    "ram",
    "96",
    "fits",
    "uh",
    "less",
    "gon",
    "na",
    "need",
    "significantly",
    "smaller",
    "memory",
    "size",
    "something",
    "keep",
    "mind",
    "next",
    "thing",
    "need",
    "store",
    "transitions",
    "memory",
    "course",
    "pretty",
    "stateful",
    "straightforward",
    "need",
    "old",
    "state",
    "action",
    "reward",
    "new",
    "state",
    "let",
    "call",
    "state",
    "underscore",
    "terminal",
    "flag",
    "integer",
    "zero",
    "one",
    "agent",
    "fixed",
    "memory",
    "signs",
    "want",
    "fill",
    "memory",
    "exceed",
    "want",
    "go",
    "back",
    "beginning",
    "start",
    "overriding",
    "index",
    "going",
    "mem",
    "counter",
    "something",
    "forgot",
    "let",
    "put",
    "counter",
    "keeps",
    "track",
    "number",
    "memories",
    "stored",
    "actions",
    "need",
    "one",
    "hot",
    "encoding",
    "pass",
    "action",
    "integer",
    "making",
    "array",
    "zeros",
    "setting",
    "index",
    "action",
    "took",
    "one",
    "one",
    "hot",
    "encoding",
    "course",
    "want",
    "increment",
    "memory",
    "counter",
    "next",
    "thing",
    "need",
    "way",
    "choosing",
    "actions",
    "deep",
    "q",
    "learning",
    "relies",
    "called",
    "epsilon",
    "greedy",
    "epsilon",
    "parameter",
    "tells",
    "often",
    "choose",
    "random",
    "action",
    "going",
    "dk",
    "epsilon",
    "time",
    "agent",
    "start",
    "acting",
    "purely",
    "randomly",
    "many",
    "many",
    "hundreds",
    "games",
    "eventually",
    "random",
    "factor",
    "start",
    "decreasing",
    "time",
    "agent",
    "take",
    "greedy",
    "actions",
    "greedy",
    "action",
    "choosing",
    "action",
    "highest",
    "value",
    "next",
    "state",
    "takes",
    "state",
    "input",
    "need",
    "random",
    "number",
    "numpy",
    "random",
    "number",
    "generator",
    "select",
    "action",
    "random",
    "agent",
    "action",
    "space",
    "going",
    "take",
    "greedy",
    "action",
    "need",
    "actually",
    "find",
    "next",
    "highest",
    "lead",
    "highest",
    "valued",
    "action",
    "need",
    "use",
    "evaluation",
    "network",
    "run",
    "q",
    "eval",
    "dot",
    "q",
    "values",
    "tensor",
    "using",
    "feed",
    "dict",
    "sorry",
    "ca",
    "talk",
    "type",
    "time",
    "current",
    "state",
    "q",
    "evaluation",
    "network",
    "input",
    "course",
    "want",
    "maximum",
    "action",
    "need",
    "actions",
    "done",
    "return",
    "action",
    "come",
    "meat",
    "problem",
    "handle",
    "learning",
    "learning",
    "many",
    "parts",
    "basic",
    "idea",
    "first",
    "thing",
    "going",
    "check",
    "see",
    "want",
    "update",
    "value",
    "target",
    "network",
    "time",
    "going",
    "go",
    "ahead",
    "next",
    "thing",
    "going",
    "select",
    "batch",
    "random",
    "memories",
    "important",
    "thing",
    "memories",
    "choose",
    "sequential",
    "memories",
    "agent",
    "get",
    "trapped",
    "little",
    "nook",
    "cranny",
    "parameter",
    "space",
    "get",
    "oscillations",
    "performance",
    "time",
    "actually",
    "robust",
    "learning",
    "want",
    "select",
    "different",
    "transitions",
    "entirety",
    "memory",
    "handle",
    "memory",
    "batching",
    "sampling",
    "calculate",
    "value",
    "current",
    "action",
    "well",
    "next",
    "maximum",
    "action",
    "plug",
    "bellman",
    "equation",
    "q",
    "learning",
    "algorithm",
    "run",
    "update",
    "function",
    "loss",
    "let",
    "go",
    "ahead",
    "first",
    "thing",
    "want",
    "see",
    "time",
    "replace",
    "network",
    "target",
    "network",
    "go",
    "ahead",
    "write",
    "update",
    "graph",
    "function",
    "momentarily",
    "next",
    "thing",
    "want",
    "find",
    "memory",
    "ends",
    "less",
    "mem",
    "size",
    "else",
    "allow",
    "us",
    "randomly",
    "sample",
    "subset",
    "memory",
    "give",
    "us",
    "random",
    "choice",
    "range",
    "maximum",
    "size",
    "batch",
    "size",
    "next",
    "need",
    "state",
    "batches",
    "sorry",
    "state",
    "transitions",
    "need",
    "actions",
    "took",
    "remember",
    "store",
    "encoding",
    "need",
    "go",
    "back",
    "need",
    "go",
    "back",
    "integer",
    "encoding",
    "need",
    "handle",
    "simplest",
    "way",
    "numpy",
    "dot",
    "operation",
    "multiply",
    "two",
    "vectors",
    "together",
    "next",
    "need",
    "calculate",
    "values",
    "current",
    "set",
    "states",
    "well",
    "set",
    "next",
    "states",
    "sorry",
    "sorry",
    "qe",
    "valve",
    "next",
    "take",
    "set",
    "next",
    "states",
    "transit",
    "transitions",
    "next",
    "thing",
    "want",
    "copy",
    "qeval",
    "network",
    "want",
    "loss",
    "actions",
    "zero",
    "next",
    "thing",
    "need",
    "calculate",
    "value",
    "q",
    "target",
    "states",
    "batch",
    "actions",
    "actually",
    "took",
    "uh",
    "plus",
    "quantity",
    "maximum",
    "value",
    "next",
    "state",
    "multiplied",
    "quantity",
    "terminal",
    "batch",
    "reason",
    "next",
    "state",
    "end",
    "episode",
    "want",
    "reward",
    "whereas",
    "terminal",
    "state",
    "next",
    "state",
    "want",
    "actually",
    "take",
    "account",
    "discounted",
    "future",
    "rewards",
    "next",
    "need",
    "feed",
    "neural",
    "network",
    "training",
    "operation",
    "need",
    "actions",
    "action",
    "actually",
    "took",
    "also",
    "need",
    "target",
    "values",
    "calculated",
    "next",
    "thing",
    "need",
    "handle",
    "prospect",
    "decreasing",
    "epsilon",
    "time",
    "remember",
    "epsilon",
    "random",
    "factor",
    "tells",
    "agent",
    "often",
    "take",
    "random",
    "action",
    "goal",
    "learning",
    "eventually",
    "take",
    "best",
    "possible",
    "actions",
    "want",
    "decrease",
    "epsilon",
    "time",
    "handle",
    "allowing",
    "agent",
    "play",
    "number",
    "moves",
    "randomly",
    "say",
    "100",
    "000",
    "moves",
    "randomly",
    "want",
    "dictate",
    "minimum",
    "value",
    "never",
    "wanted",
    "purely",
    "greedy",
    "actions",
    "never",
    "know",
    "estimates",
    "always",
    "want",
    "exploring",
    "make",
    "sure",
    "estimates",
    "decrease",
    "epsilon",
    "time",
    "number",
    "ways",
    "linearly",
    "deepmind",
    "use",
    "square",
    "roots",
    "use",
    "number",
    "things",
    "going",
    "going",
    "multiply",
    "fraction",
    "one",
    "bunch",
    "nines",
    "give",
    "really",
    "slow",
    "decrease",
    "epsilon",
    "time",
    "agent",
    "takes",
    "lot",
    "random",
    "actions",
    "lot",
    "exploration",
    "sorry",
    "flipped",
    "sign",
    "thought",
    "look",
    "right",
    "yeah",
    "tries",
    "drop",
    "going",
    "go",
    "ahead",
    "set",
    "okay",
    "learning",
    "function",
    "next",
    "handle",
    "functions",
    "save",
    "models",
    "call",
    "save",
    "checkpoint",
    "function",
    "respective",
    "networks",
    "next",
    "function",
    "need",
    "way",
    "updating",
    "graph",
    "want",
    "want",
    "copy",
    "evaluation",
    "network",
    "target",
    "network",
    "actually",
    "little",
    "bit",
    "tricky",
    "want",
    "target",
    "parameters",
    "saved",
    "earlier",
    "want",
    "perform",
    "copy",
    "operation",
    "reason",
    "pass",
    "session",
    "decision",
    "session",
    "use",
    "use",
    "session",
    "values",
    "trying",
    "copy",
    "copy",
    "q",
    "next",
    "would",
    "get",
    "error",
    "took",
    "little",
    "bit",
    "figure",
    "agent",
    "class",
    "next",
    "code",
    "main",
    "function",
    "course",
    "start",
    "imports",
    "going",
    "import",
    "jim",
    "want",
    "import",
    "network",
    "also",
    "need",
    "numpy",
    "handle",
    "reshaping",
    "observation",
    "going",
    "truncate",
    "make",
    "workload",
    "neural",
    "gpu",
    "little",
    "bit",
    "lower",
    "import",
    "numpy",
    "np",
    "want",
    "save",
    "uh",
    "games",
    "actually",
    "use",
    "uh",
    "wrappers",
    "wo",
    "put",
    "put",
    "github",
    "version",
    "git",
    "pull",
    "way",
    "saving",
    "games",
    "first",
    "thing",
    "observations",
    "reason",
    "want",
    "need",
    "image",
    "need",
    "score",
    "also",
    "need",
    "color",
    "need",
    "one",
    "channel",
    "already",
    "figured",
    "take",
    "row",
    "30",
    "onward",
    "columns",
    "get",
    "good",
    "image",
    "want",
    "reshape",
    "like",
    "next",
    "thing",
    "handle",
    "way",
    "stacking",
    "frames",
    "agent",
    "ca",
    "get",
    "sense",
    "motion",
    "looking",
    "one",
    "picture",
    "right",
    "nobody",
    "get",
    "sense",
    "motion",
    "looking",
    "one",
    "picture",
    "worse",
    "yet",
    "openai",
    "atari",
    "library",
    "returns",
    "sequence",
    "frames",
    "could",
    "random",
    "number",
    "two",
    "three",
    "four",
    "get",
    "sense",
    "motion",
    "actually",
    "stack",
    "set",
    "frames",
    "going",
    "handle",
    "function",
    "keep",
    "running",
    "stack",
    "frames",
    "current",
    "frame",
    "save",
    "buffer",
    "size",
    "tells",
    "many",
    "frames",
    "save",
    "top",
    "episode",
    "going",
    "anything",
    "save",
    "right",
    "stacked",
    "frames",
    "want",
    "initialize",
    "buffer",
    "size",
    "shape",
    "individual",
    "frame",
    "next",
    "want",
    "iterate",
    "say",
    "row",
    "corresponds",
    "image",
    "stack",
    "gets",
    "assigned",
    "frame",
    "otherwise",
    "beginning",
    "episode",
    "want",
    "pop",
    "bottom",
    "observation",
    "shift",
    "set",
    "frames",
    "append",
    "new",
    "observation",
    "end",
    "instead",
    "one",
    "two",
    "three",
    "four",
    "two",
    "three",
    "four",
    "frame",
    "five",
    "let",
    "equals",
    "sorry",
    "zero",
    "two",
    "buffer",
    "size",
    "minus",
    "one",
    "shift",
    "everything",
    "append",
    "current",
    "frame",
    "end",
    "stack",
    "next",
    "reshape",
    "basically",
    "make",
    "everything",
    "play",
    "nicely",
    "neural",
    "network",
    "right",
    "ready",
    "main",
    "function",
    "let",
    "go",
    "ahead",
    "save",
    "scroll",
    "good",
    "grief",
    "breakout",
    "v0",
    "name",
    "environment",
    "flag",
    "determine",
    "want",
    "load",
    "checkpoint",
    "sorry",
    "yeah",
    "epsilon",
    "starts",
    "agent",
    "takes",
    "purely",
    "random",
    "actions",
    "learning",
    "rate",
    "alpha",
    "something",
    "small",
    "like",
    "zero",
    "zero",
    "zero",
    "two",
    "five",
    "reshaped",
    "input",
    "needs",
    "180",
    "instead",
    "210",
    "180",
    "160",
    "four",
    "going",
    "stack",
    "four",
    "frames",
    "breakout",
    "library",
    "sorry",
    "breakout",
    "game",
    "three",
    "actions",
    "memory",
    "size",
    "25",
    "000",
    "said",
    "takes",
    "48",
    "gigabytes",
    "ram",
    "go",
    "ahead",
    "scale",
    "based",
    "however",
    "much",
    "16",
    "gigs",
    "go",
    "ahead",
    "reduce",
    "something",
    "like",
    "six",
    "seven",
    "thousand",
    "batch",
    "size",
    "tells",
    "us",
    "many",
    "batches",
    "memories",
    "include",
    "training",
    "use",
    "low",
    "checkpoint",
    "true",
    "want",
    "load",
    "models",
    "next",
    "thing",
    "need",
    "way",
    "keeping",
    "track",
    "scores",
    "need",
    "parameter",
    "number",
    "games",
    "stick",
    "200",
    "start",
    "stack",
    "size",
    "four",
    "initial",
    "score",
    "zero",
    "memory",
    "originally",
    "initialized",
    "bunch",
    "zeros",
    "perfectly",
    "acceptable",
    "another",
    "option",
    "something",
    "else",
    "overwrite",
    "zeros",
    "actual",
    "gameplay",
    "sampled",
    "environment",
    "actions",
    "chosen",
    "randomly",
    "right",
    "give",
    "agent",
    "idea",
    "going",
    "environment",
    "want",
    "reset",
    "top",
    "every",
    "episode",
    "want",
    "observation",
    "want",
    "go",
    "ahead",
    "stack",
    "frames",
    "player",
    "episode",
    "bit",
    "quirk",
    "agent",
    "saves",
    "actions",
    "zero",
    "one",
    "two",
    "actions",
    "environment",
    "actually",
    "one",
    "two",
    "three",
    "sample",
    "interval",
    "add",
    "one",
    "take",
    "action",
    "subtract",
    "one",
    "save",
    "transition",
    "memory",
    "bit",
    "kluge",
    "big",
    "problem",
    "want",
    "add",
    "stack",
    "go",
    "ahead",
    "subtract",
    "one",
    "action",
    "store",
    "transition",
    "memory",
    "finally",
    "set",
    "old",
    "observation",
    "new",
    "one",
    "let",
    "go",
    "ahead",
    "use",
    "string",
    "print",
    "statement",
    "okay",
    "loaded",
    "agent",
    "random",
    "memories",
    "actually",
    "go",
    "ahead",
    "play",
    "game",
    "one",
    "thing",
    "like",
    "print",
    "running",
    "average",
    "last",
    "10",
    "games",
    "get",
    "idea",
    "agent",
    "learning",
    "time",
    "like",
    "print",
    "also",
    "like",
    "know",
    "epsilon",
    "still",
    "one",
    "actually",
    "started",
    "decrease",
    "time",
    "also",
    "want",
    "save",
    "models",
    "every",
    "10",
    "games",
    "game",
    "want",
    "print",
    "episode",
    "number",
    "score",
    "actually",
    "scroll",
    "copy",
    "done",
    "whoops",
    "go",
    "instead",
    "random",
    "choice",
    "agent",
    "dot",
    "choose",
    "action",
    "takes",
    "observation",
    "oh",
    "forget",
    "thing",
    "grab",
    "put",
    "top",
    "every",
    "episode",
    "reset",
    "environment",
    "observation",
    "stack",
    "frames",
    "quite",
    "critical",
    "still",
    "thing",
    "adding",
    "subtracting",
    "one",
    "want",
    "save",
    "transitions",
    "difference",
    "want",
    "learn",
    "every",
    "step",
    "end",
    "episode",
    "want",
    "go",
    "ahead",
    "append",
    "score",
    "way",
    "keep",
    "track",
    "average",
    "want",
    "get",
    "fancy",
    "go",
    "ahead",
    "add",
    "plotting",
    "function",
    "done",
    "learning",
    "plot",
    "learning",
    "time",
    "see",
    "upward",
    "slope",
    "time",
    "want",
    "plot",
    "epsilons",
    "see",
    "gradual",
    "downward",
    "slope",
    "well",
    "gon",
    "na",
    "leave",
    "model",
    "still",
    "training",
    "computer",
    "run",
    "three",
    "thousand",
    "iterations",
    "three",
    "thousand",
    "games",
    "best",
    "gets",
    "two",
    "three",
    "points",
    "per",
    "set",
    "five",
    "lives",
    "really",
    "going",
    "need",
    "couple",
    "days",
    "training",
    "get",
    "anything",
    "resembling",
    "competitive",
    "gameplay",
    "done",
    "upload",
    "another",
    "video",
    "explains",
    "q",
    "learning",
    "works",
    "depth",
    "detail",
    "include",
    "performance",
    "particular",
    "model",
    "video",
    "check",
    "feel",
    "free",
    "run",
    "finish",
    "model",
    "go",
    "ahead",
    "upload",
    "trained",
    "version",
    "github",
    "free",
    "download",
    "hard",
    "decent",
    "gpus",
    "train",
    "go",
    "ahead",
    "leave",
    "comment",
    "subscribe",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "everybody",
    "series",
    "videos",
    "going",
    "code",
    "deep",
    "q",
    "network",
    "pytorch",
    "play",
    "space",
    "invaders",
    "atari",
    "open",
    "ai",
    "gym",
    "library",
    "first",
    "video",
    "going",
    "code",
    "convolutional",
    "neural",
    "network",
    "class",
    "second",
    "video",
    "code",
    "aging",
    "class",
    "third",
    "get",
    "coding",
    "main",
    "loop",
    "seeing",
    "performs",
    "let",
    "get",
    "familiar",
    "deep",
    "q",
    "network",
    "would",
    "advise",
    "check",
    "earlier",
    "video",
    "bite",
    "size",
    "machine",
    "learning",
    "deep",
    "q",
    "network",
    "quick",
    "little",
    "explainer",
    "video",
    "gives",
    "gist",
    "within",
    "four",
    "half",
    "minutes",
    "uh",
    "already",
    "know",
    "great",
    "ready",
    "rock",
    "roll",
    "give",
    "brief",
    "long",
    "read",
    "basic",
    "idea",
    "want",
    "reduce",
    "rather",
    "large",
    "state",
    "space",
    "something",
    "manageable",
    "going",
    "use",
    "convolutional",
    "neural",
    "network",
    "reduce",
    "representation",
    "estate",
    "space",
    "something",
    "manageable",
    "fed",
    "fully",
    "connected",
    "neural",
    "network",
    "compute",
    "q",
    "values",
    "words",
    "action",
    "values",
    "particular",
    "given",
    "state",
    "going",
    "leverage",
    "two",
    "different",
    "networks",
    "network",
    "going",
    "tell",
    "us",
    "value",
    "current",
    "state",
    "well",
    "network",
    "tell",
    "us",
    "value",
    "successor",
    "states",
    "going",
    "used",
    "calculate",
    "values",
    "target",
    "purely",
    "greedy",
    "action",
    "well",
    "behavioral",
    "network",
    "current",
    "predicted",
    "state",
    "difference",
    "two",
    "used",
    "loss",
    "function",
    "stochastic",
    "gradient",
    "descent",
    "root",
    "mean",
    "squared",
    "propagator",
    "let",
    "go",
    "ahead",
    "get",
    "started",
    "um",
    "going",
    "start",
    "usual",
    "imports",
    "quite",
    "pi",
    "torch",
    "great",
    "library",
    "one",
    "thing",
    "really",
    "like",
    "highly",
    "expressive",
    "whole",
    "lot",
    "boilerplate",
    "code",
    "something",
    "like",
    "tensorflow",
    "tensorflow",
    "enormously",
    "powerful",
    "uh",
    "wan",
    "na",
    "um",
    "cross",
    "app",
    "type",
    "software",
    "going",
    "really",
    "choice",
    "purposes",
    "going",
    "precise",
    "want",
    "use",
    "course",
    "numpy",
    "going",
    "define",
    "class",
    "deep",
    "q",
    "network",
    "derive",
    "end",
    "module",
    "kind",
    "uh",
    "pytorch",
    "handles",
    "everything",
    "want",
    "derive",
    "class",
    "module",
    "uh",
    "base",
    "module",
    "uh",
    "way",
    "get",
    "access",
    "goodies",
    "pass",
    "learning",
    "rate",
    "alpha",
    "stochastic",
    "gradient",
    "descent",
    "algorithm",
    "right",
    "network",
    "comprised",
    "three",
    "convolutional",
    "layers",
    "two",
    "fully",
    "connected",
    "layers",
    "first",
    "one",
    "going",
    "convolution",
    "going",
    "take",
    "one",
    "input",
    "channel",
    "reason",
    "agent",
    "really",
    "care",
    "color",
    "going",
    "go",
    "ahead",
    "make",
    "things",
    "easier",
    "reduce",
    "computational",
    "requirements",
    "going",
    "grayscale",
    "representation",
    "take",
    "32",
    "filters",
    "8",
    "8",
    "kernel",
    "stride",
    "4",
    "padding",
    "second",
    "layer",
    "going",
    "pretty",
    "similar",
    "one",
    "course",
    "take",
    "32",
    "channels",
    "give",
    "64",
    "channels",
    "4x4",
    "kernel",
    "stride",
    "two",
    "third",
    "convolutional",
    "layer",
    "going",
    "course",
    "well",
    "takes",
    "64",
    "outputs",
    "128",
    "3x3",
    "kernel",
    "uh",
    "convolutions",
    "hey",
    "know",
    "convolutional",
    "neural",
    "network",
    "works",
    "going",
    "go",
    "ahead",
    "link",
    "video",
    "give",
    "uh",
    "basic",
    "idea",
    "work",
    "would",
    "rather",
    "see",
    "uh",
    "stuff",
    "works",
    "text",
    "form",
    "know",
    "mentioned",
    "earlier",
    "associated",
    "blog",
    "article",
    "website",
    "neuronet",
    "dot",
    "ai",
    "yeah",
    "bought",
    "domain",
    "link",
    "description",
    "go",
    "ahead",
    "check",
    "please",
    "fully",
    "connected",
    "layer",
    "following",
    "going",
    "little",
    "bit",
    "magic",
    "running",
    "stuff",
    "bunch",
    "times",
    "uh",
    "know",
    "dimensionality",
    "images",
    "going",
    "convolutions",
    "going",
    "get",
    "want",
    "know",
    "going",
    "taking",
    "set",
    "filters",
    "uh",
    "know",
    "convolved",
    "images",
    "filters",
    "applied",
    "128",
    "exact",
    "want",
    "unroll",
    "single",
    "flattened",
    "image",
    "right",
    "feed",
    "neural",
    "network",
    "going",
    "128",
    "channels",
    "19",
    "8",
    "size",
    "magic",
    "number",
    "got",
    "running",
    "code",
    "trial",
    "error",
    "output",
    "layer",
    "going",
    "take",
    "512",
    "units",
    "output",
    "6",
    "y6",
    "total",
    "6",
    "actions",
    "game",
    "space",
    "invaders",
    "total",
    "6",
    "actions",
    "subset",
    "full",
    "20",
    "available",
    "atari",
    "library",
    "basically",
    "agent",
    "nothing",
    "move",
    "left",
    "move",
    "right",
    "shoot",
    "standing",
    "still",
    "move",
    "left",
    "right",
    "shoot",
    "get",
    "main",
    "video",
    "go",
    "ahead",
    "show",
    "actions",
    "also",
    "need",
    "optimizer",
    "equals",
    "optim",
    "rms",
    "prop",
    "want",
    "tell",
    "take",
    "parameters",
    "object",
    "weights",
    "learning",
    "rate",
    "going",
    "alpha",
    "passed",
    "loss",
    "function",
    "going",
    "mean",
    "square",
    "error",
    "loss",
    "components",
    "target",
    "current",
    "predicted",
    "q",
    "functions",
    "course",
    "target",
    "greedy",
    "value",
    "right",
    "q",
    "learning",
    "policy",
    "method",
    "uses",
    "epsilon",
    "greedy",
    "behavioral",
    "policy",
    "update",
    "purely",
    "greedy",
    "target",
    "policy",
    "another",
    "thing",
    "pi",
    "torch",
    "tell",
    "device",
    "using",
    "cuda",
    "zero",
    "dot",
    "cuda",
    "available",
    "else",
    "cpu",
    "telling",
    "gpu",
    "available",
    "go",
    "ahead",
    "use",
    "course",
    "always",
    "preferable",
    "otherwise",
    "use",
    "cpu",
    "also",
    "tell",
    "actually",
    "send",
    "network",
    "device",
    "um",
    "maybe",
    "using",
    "know",
    "actually",
    "required",
    "time",
    "coding",
    "required",
    "uh",
    "something",
    "live",
    "okay",
    "sum",
    "whole",
    "network",
    "thing",
    "remains",
    "feed",
    "forward",
    "observation",
    "take",
    "opposite",
    "observation",
    "observation",
    "type",
    "speak",
    "well",
    "suck",
    "life",
    "know",
    "guys",
    "listen",
    "want",
    "observation",
    "vector",
    "uh",
    "going",
    "use",
    "sequence",
    "frames",
    "frames",
    "reduced",
    "images",
    "screen",
    "get",
    "third",
    "video",
    "um",
    "music",
    "actually",
    "playing",
    "game",
    "see",
    "need",
    "full",
    "frame",
    "actually",
    "figure",
    "going",
    "actually",
    "truncate",
    "get",
    "really",
    "good",
    "idea",
    "particular",
    "agent",
    "move",
    "left",
    "right",
    "certain",
    "amount",
    "go",
    "ahead",
    "truncate",
    "sides",
    "uh",
    "need",
    "score",
    "top",
    "keeping",
    "track",
    "score",
    "need",
    "bottom",
    "actually",
    "truncate",
    "image",
    "little",
    "bit",
    "reduce",
    "memory",
    "requirements",
    "going",
    "convert",
    "grayscale",
    "get",
    "rid",
    "two",
    "going",
    "average",
    "three",
    "channels",
    "make",
    "one",
    "also",
    "pass",
    "sequence",
    "frames",
    "right",
    "show",
    "one",
    "frame",
    "video",
    "game",
    "ca",
    "tell",
    "going",
    "get",
    "sense",
    "motion",
    "single",
    "frame",
    "right",
    "need",
    "least",
    "two",
    "get",
    "sense",
    "motion",
    "uh",
    "using",
    "three",
    "believe",
    "original",
    "implementation",
    "algorithm",
    "um",
    "deepmind",
    "used",
    "four",
    "going",
    "go",
    "ahead",
    "use",
    "three",
    "different",
    "um",
    "suspect",
    "hyperparameter",
    "something",
    "played",
    "encourage",
    "oh",
    "another",
    "thing",
    "must",
    "train",
    "gpu",
    "try",
    "cpu",
    "going",
    "take",
    "7",
    "000",
    "years",
    "uh",
    "access",
    "gpu",
    "go",
    "ahead",
    "leave",
    "comment",
    "find",
    "way",
    "get",
    "model",
    "go",
    "ahead",
    "play",
    "around",
    "1080",
    "ti",
    "top",
    "line",
    "anymore",
    "still",
    "quite",
    "good",
    "particular",
    "apps",
    "application",
    "back",
    "topic",
    "convert",
    "sequence",
    "frames",
    "obser",
    "observation",
    "tensor",
    "also",
    "want",
    "send",
    "device",
    "uh",
    "peculiarity",
    "pi",
    "torch",
    "far",
    "aware",
    "world",
    "leading",
    "expert",
    "far",
    "understand",
    "tell",
    "send",
    "network",
    "well",
    "variables",
    "device",
    "well",
    "big",
    "deal",
    "something",
    "aware",
    "next",
    "thing",
    "resize",
    "given",
    "sequence",
    "frames",
    "open",
    "ai",
    "gym",
    "really",
    "format",
    "image",
    "going",
    "height",
    "width",
    "channels",
    "whereas",
    "look",
    "actually",
    "expects",
    "channels",
    "come",
    "first",
    "reshape",
    "array",
    "accommodate",
    "pytorch",
    "function",
    "view",
    "want",
    "minus",
    "one",
    "handle",
    "number",
    "stacked",
    "frames",
    "one",
    "channel",
    "beginning",
    "going",
    "pass",
    "185",
    "195",
    "image",
    "music",
    "seem",
    "right",
    "actually",
    "sorry",
    "195",
    "like",
    "original",
    "image",
    "210",
    "160",
    "ca",
    "180",
    "595",
    "total",
    "brain",
    "fart",
    "185",
    "95",
    "okay",
    "taken",
    "sequence",
    "frames",
    "converted",
    "tensor",
    "flat",
    "converted",
    "proper",
    "proper",
    "shape",
    "network",
    "next",
    "thing",
    "want",
    "activate",
    "feed",
    "forward",
    "call",
    "first",
    "convolutional",
    "layer",
    "observation",
    "use",
    "value",
    "function",
    "activate",
    "store",
    "new",
    "variable",
    "observation",
    "learn",
    "type",
    "thing",
    "output",
    "pass",
    "second",
    "convolutional",
    "layer",
    "thing",
    "pass",
    "third",
    "convolutional",
    "layer",
    "um",
    "almost",
    "home",
    "free",
    "next",
    "thing",
    "outputting",
    "128",
    "19",
    "eight",
    "oh",
    "set",
    "arrays",
    "next",
    "thing",
    "oh",
    "way",
    "noise",
    "pomodoro",
    "timer",
    "using",
    "pomodoro",
    "technique",
    "highly",
    "recommend",
    "enormous",
    "productivity",
    "hack",
    "anyway",
    "next",
    "thing",
    "take",
    "sequence",
    "convolved",
    "images",
    "flatten",
    "feed",
    "fully",
    "connected",
    "neural",
    "network",
    "use",
    "view",
    "function",
    "uh",
    "minus",
    "one",
    "whatever",
    "number",
    "frames",
    "pass",
    "19",
    "eight",
    "get",
    "found",
    "experimentation",
    "know",
    "128",
    "channels",
    "dictate",
    "right",
    "128",
    "output",
    "channels",
    "boom",
    "flattened",
    "thing",
    "remains",
    "feed",
    "forward",
    "use",
    "value",
    "activation",
    "function",
    "fc1",
    "fully",
    "connected",
    "layer",
    "one",
    "observation",
    "actions",
    "self",
    "dot",
    "fc2",
    "observation",
    "feed",
    "final",
    "fully",
    "convolved",
    "layer",
    "store",
    "variable",
    "called",
    "actions",
    "go",
    "ahead",
    "return",
    "reason",
    "getting",
    "q",
    "value",
    "actions",
    "want",
    "pass",
    "back",
    "keep",
    "mind",
    "passing",
    "sequence",
    "frames",
    "going",
    "get",
    "back",
    "matrix",
    "going",
    "single",
    "array",
    "six",
    "values",
    "going",
    "six",
    "values",
    "times",
    "whatever",
    "number",
    "rows",
    "images",
    "pass",
    "pass",
    "three",
    "images",
    "going",
    "three",
    "rows",
    "six",
    "columns",
    "important",
    "later",
    "actually",
    "get",
    "choosing",
    "actions",
    "speaking",
    "going",
    "cut",
    "short",
    "already",
    "already",
    "rambled",
    "minutes",
    "uh",
    "part",
    "two",
    "going",
    "take",
    "look",
    "coding",
    "agent",
    "class",
    "structured",
    "way",
    "agent",
    "actually",
    "two",
    "networks",
    "made",
    "sense",
    "kind",
    "stick",
    "network",
    "class",
    "get",
    "coding",
    "agent",
    "init",
    "function",
    "handle",
    "memory",
    "store",
    "transitions",
    "choose",
    "actions",
    "actually",
    "implement",
    "learning",
    "function",
    "much",
    "longer",
    "project",
    "stick",
    "video",
    "part",
    "three",
    "going",
    "get",
    "actually",
    "coding",
    "main",
    "loop",
    "seeing",
    "performs",
    "hey",
    "liked",
    "video",
    "make",
    "sure",
    "like",
    "video",
    "hey",
    "like",
    "go",
    "ahead",
    "thumbs",
    "really",
    "care",
    "let",
    "know",
    "think",
    "questions",
    "comments",
    "leave",
    "made",
    "far",
    "please",
    "consider",
    "subscribing",
    "look",
    "forward",
    "seeing",
    "next",
    "video",
    "welcome",
    "back",
    "everybody",
    "previous",
    "video",
    "got",
    "coding",
    "convolutional",
    "neural",
    "network",
    "class",
    "deep",
    "q",
    "learning",
    "agent",
    "going",
    "play",
    "space",
    "invaders",
    "seen",
    "yet",
    "go",
    "ahead",
    "click",
    "link",
    "go",
    "ahead",
    "watch",
    "first",
    "otherwise",
    "probably",
    "wo",
    "know",
    "going",
    "type",
    "person",
    "would",
    "prefer",
    "written",
    "set",
    "instructions",
    "go",
    "ahead",
    "click",
    "link",
    "link",
    "associated",
    "blog",
    "article",
    "particular",
    "tutorial",
    "series",
    "last",
    "left",
    "finished",
    "returning",
    "set",
    "actions",
    "set",
    "q",
    "values",
    "sequence",
    "frames",
    "course",
    "video",
    "going",
    "go",
    "ahead",
    "get",
    "started",
    "coding",
    "agent",
    "class",
    "magic",
    "going",
    "happen",
    "uh",
    "oh",
    "course",
    "always",
    "left",
    "code",
    "github",
    "youtube",
    "directory",
    "gets",
    "directory",
    "different",
    "files",
    "link",
    "well",
    "following",
    "github",
    "probably",
    "post",
    "stuff",
    "okay",
    "next",
    "agent",
    "class",
    "right",
    "going",
    "derive",
    "base",
    "object",
    "class",
    "nothing",
    "fancy",
    "need",
    "basic",
    "init",
    "function",
    "going",
    "take",
    "gamma",
    "discount",
    "factor",
    "agent",
    "choice",
    "value",
    "future",
    "rewards",
    "general",
    "gets",
    "discounted",
    "value",
    "reward",
    "worth",
    "reward",
    "future",
    "like",
    "us",
    "need",
    "epsilon",
    "epsilon",
    "greedy",
    "action",
    "selection",
    "alpha",
    "learning",
    "rate",
    "max",
    "memory",
    "size",
    "variable",
    "keep",
    "track",
    "low",
    "want",
    "epsilon",
    "go",
    "something",
    "keep",
    "track",
    "long",
    "replace",
    "often",
    "going",
    "replace",
    "target",
    "network",
    "get",
    "moment",
    "minutes",
    "action",
    "space",
    "list",
    "variables",
    "zero",
    "five",
    "correspond",
    "possible",
    "actions",
    "agent",
    "set",
    "appropriate",
    "variables",
    "uh",
    "careful",
    "turn",
    "caps",
    "lock",
    "key",
    "hyper",
    "parameters",
    "agent",
    "need",
    "store",
    "alpha",
    "going",
    "pass",
    "network",
    "never",
    "touch",
    "storing",
    "action",
    "space",
    "allows",
    "us",
    "accommodate",
    "epsilon",
    "greedy",
    "action",
    "selection",
    "later",
    "mems",
    "size",
    "used",
    "efficiency",
    "purposes",
    "going",
    "keep",
    "track",
    "state",
    "action",
    "reward",
    "transitions",
    "want",
    "store",
    "infinite",
    "number",
    "want",
    "store",
    "subset",
    "need",
    "store",
    "anyway",
    "real",
    "practical",
    "benefit",
    "since",
    "sampling",
    "subset",
    "anyway",
    "use",
    "rather",
    "large",
    "memory",
    "um",
    "keep",
    "track",
    "state",
    "transitions",
    "care",
    "keep",
    "track",
    "many",
    "steps",
    "learn",
    "step",
    "counter",
    "keep",
    "track",
    "many",
    "times",
    "agent",
    "called",
    "learn",
    "function",
    "used",
    "target",
    "network",
    "replacement",
    "familiar",
    "target",
    "network",
    "replacement",
    "swap",
    "parameters",
    "evaluation",
    "network",
    "target",
    "network",
    "experience",
    "actually",
    "help",
    "things",
    "going",
    "going",
    "code",
    "important",
    "part",
    "topic",
    "particular",
    "case",
    "found",
    "helpful",
    "played",
    "much",
    "saw",
    "quite",
    "well",
    "without",
    "break",
    "going",
    "store",
    "memory",
    "list",
    "reason",
    "use",
    "list",
    "instead",
    "numpy",
    "array",
    "associated",
    "cost",
    "stacking",
    "numpy",
    "rays",
    "really",
    "really",
    "high",
    "much",
    "much",
    "faster",
    "store",
    "list",
    "lists",
    "convert",
    "numpy",
    "array",
    "learn",
    "much",
    "faster",
    "keeping",
    "track",
    "set",
    "numpy",
    "arrays",
    "stacking",
    "stack",
    "operation",
    "incredibly",
    "computationally",
    "prohibitive",
    "something",
    "like",
    "already",
    "computationally",
    "expensive",
    "make",
    "sense",
    "music",
    "keep",
    "track",
    "total",
    "number",
    "memory",
    "stored",
    "way",
    "overwrite",
    "array",
    "want",
    "keep",
    "track",
    "often",
    "want",
    "replace",
    "target",
    "network",
    "need",
    "two",
    "networks",
    "qe",
    "val",
    "passing",
    "alpha",
    "agent",
    "estimate",
    "current",
    "set",
    "states",
    "q",
    "next",
    "agent",
    "estimate",
    "successor",
    "set",
    "states",
    "recall",
    "deep",
    "q",
    "learning",
    "calculate",
    "value",
    "max",
    "value",
    "successor",
    "state",
    "greedy",
    "action",
    "actual",
    "target",
    "policy",
    "behavior",
    "policy",
    "used",
    "generate",
    "data",
    "epsilon",
    "greedy",
    "constructor",
    "next",
    "thing",
    "worry",
    "storing",
    "memory",
    "transitions",
    "transition",
    "interested",
    "current",
    "state",
    "action",
    "taken",
    "reward",
    "received",
    "resulting",
    "state",
    "things",
    "allow",
    "agent",
    "learn",
    "make",
    "sure",
    "memory",
    "mem",
    "counters",
    "less",
    "size",
    "go",
    "ahead",
    "append",
    "list",
    "much",
    "cheaper",
    "computationally",
    "append",
    "list",
    "actually",
    "stack",
    "numpy",
    "array",
    "filled",
    "memory",
    "want",
    "uh",
    "overwrite",
    "position",
    "memory",
    "uh",
    "determined",
    "modulus",
    "dot",
    "mem",
    "size",
    "guarantee",
    "bounded",
    "zero",
    "way",
    "mem",
    "size",
    "course",
    "list",
    "lists",
    "action",
    "reward",
    "state",
    "underscore",
    "want",
    "increment",
    "memory",
    "counter",
    "pretty",
    "simple",
    "huh",
    "pretty",
    "straightforward",
    "next",
    "function",
    "choose",
    "action",
    "take",
    "observation",
    "remind",
    "discussed",
    "first",
    "video",
    "passing",
    "sequence",
    "observations",
    "want",
    "capture",
    "information",
    "temporal",
    "dependence",
    "going",
    "one",
    "frame",
    "ca",
    "tell",
    "aliens",
    "moving",
    "left",
    "right",
    "know",
    "move",
    "left",
    "right",
    "um",
    "course",
    "know",
    "way",
    "bullets",
    "go",
    "know",
    "way",
    "go",
    "far",
    "movement",
    "concerned",
    "need",
    "least",
    "one",
    "frame",
    "always",
    "going",
    "using",
    "numpy",
    "calculate",
    "random",
    "number",
    "us",
    "epsilon",
    "greedy",
    "action",
    "selection",
    "want",
    "get",
    "value",
    "actions",
    "current",
    "set",
    "states",
    "q",
    "eval",
    "dot",
    "forward",
    "forward",
    "propagating",
    "stack",
    "frames",
    "neural",
    "network",
    "convolutional",
    "neural",
    "network",
    "fully",
    "connected",
    "layer",
    "get",
    "value",
    "actions",
    "given",
    "set",
    "states",
    "denoted",
    "observation",
    "rain",
    "less",
    "one",
    "minus",
    "epsilon",
    "want",
    "take",
    "arg",
    "max",
    "write",
    "maximum",
    "action",
    "recall",
    "stored",
    "actions",
    "matrix",
    "returned",
    "matrix",
    "number",
    "rows",
    "correspond",
    "number",
    "frames",
    "pass",
    "columns",
    "correspond",
    "six",
    "actions",
    "want",
    "take",
    "first",
    "first",
    "axis",
    "taking",
    "random",
    "action",
    "choose",
    "something",
    "random",
    "action",
    "space",
    "list",
    "go",
    "ahead",
    "increment",
    "steps",
    "return",
    "action",
    "chosen",
    "way",
    "reason",
    "use",
    "one",
    "minus",
    "epsilon",
    "um",
    "music",
    "use",
    "probability",
    "epsilon",
    "use",
    "probability",
    "one",
    "minus",
    "epsilon",
    "really",
    "soft",
    "uh",
    "soft",
    "epsilon",
    "kind",
    "strategy",
    "rather",
    "purely",
    "greedy",
    "really",
    "matter",
    "going",
    "give",
    "probability",
    "choosing",
    "maximum",
    "action",
    "epsilon",
    "plus",
    "epsilon",
    "six",
    "course",
    "greedy",
    "action",
    "subset",
    "actions",
    "one",
    "six",
    "probability",
    "take",
    "action",
    "actually",
    "end",
    "getting",
    "greedy",
    "action",
    "right",
    "next",
    "thing",
    "worry",
    "agent",
    "going",
    "learn",
    "really",
    "meat",
    "everything",
    "batch",
    "learning",
    "want",
    "pass",
    "batch",
    "size",
    "reason",
    "batch",
    "learning",
    "um",
    "music",
    "number",
    "different",
    "reasons",
    "want",
    "break",
    "correlations",
    "uh",
    "state",
    "transitions",
    "even",
    "okay",
    "batch",
    "overlaps",
    "different",
    "episodes",
    "want",
    "get",
    "good",
    "sub",
    "sampling",
    "overall",
    "parameter",
    "space",
    "way",
    "get",
    "trapped",
    "local",
    "minima",
    "randomly",
    "sample",
    "state",
    "transitions",
    "memory",
    "otherwise",
    "could",
    "end",
    "replay",
    "whole",
    "memory",
    "could",
    "end",
    "getting",
    "trapped",
    "local",
    "minimum",
    "way",
    "improve",
    "efficiency",
    "algorithm",
    "converge",
    "purely",
    "optimal",
    "strategy",
    "excuse",
    "thursday",
    "right",
    "first",
    "thing",
    "since",
    "using",
    "batch",
    "learning",
    "uh",
    "zero",
    "gradients",
    "means",
    "gradients",
    "accumulate",
    "step",
    "step",
    "network",
    "like",
    "pi",
    "torch",
    "library",
    "keep",
    "track",
    "want",
    "zero",
    "grad",
    "end",
    "um",
    "basically",
    "accumulating",
    "every",
    "single",
    "batch",
    "really",
    "full",
    "learning",
    "rather",
    "batch",
    "learning",
    "next",
    "thing",
    "check",
    "going",
    "replace",
    "target",
    "network",
    "none",
    "time",
    "release",
    "replace",
    "target",
    "account",
    "equals",
    "zero",
    "want",
    "actually",
    "replace",
    "target",
    "network",
    "take",
    "advantage",
    "pi",
    "torch",
    "representation",
    "network",
    "case",
    "convert",
    "entire",
    "network",
    "dictionary",
    "really",
    "cool",
    "qnext",
    "dot",
    "load",
    "state",
    "dict",
    "going",
    "load",
    "uh",
    "state",
    "network",
    "dictionary",
    "network",
    "evaluation",
    "network",
    "going",
    "convert",
    "state",
    "dictionary",
    "actually",
    "going",
    "use",
    "implementation",
    "include",
    "completeness",
    "uh",
    "next",
    "uh",
    "want",
    "know",
    "uh",
    "want",
    "select",
    "random",
    "memory",
    "want",
    "make",
    "sure",
    "go",
    "way",
    "past",
    "end",
    "array",
    "current",
    "memory",
    "counter",
    "plus",
    "batch",
    "size",
    "less",
    "total",
    "memory",
    "well",
    "free",
    "go",
    "ahead",
    "select",
    "point",
    "memory",
    "know",
    "going",
    "go",
    "beyond",
    "range",
    "otherwise",
    "overlap",
    "want",
    "memstar",
    "int",
    "range",
    "music",
    "good",
    "grief",
    "hate",
    "minus",
    "one",
    "safe",
    "uh",
    "okay",
    "choose",
    "start",
    "random",
    "number",
    "somewhere",
    "zero",
    "max",
    "memory",
    "enough",
    "leftover",
    "memory",
    "bat",
    "accommodate",
    "batch",
    "otherwise",
    "subtract",
    "select",
    "something",
    "subset",
    "going",
    "go",
    "ahead",
    "get",
    "mini",
    "batch",
    "convert",
    "numpy",
    "array",
    "suspect",
    "efficient",
    "way",
    "reason",
    "run",
    "difficulties",
    "way",
    "pass",
    "things",
    "torch",
    "library",
    "certain",
    "set",
    "expectations",
    "kind",
    "finicky",
    "um",
    "say",
    "bad",
    "something",
    "uh",
    "expecting",
    "works",
    "nonetheless",
    "want",
    "next",
    "feed",
    "forward",
    "networks",
    "want",
    "know",
    "value",
    "current",
    "state",
    "value",
    "successor",
    "state",
    "take",
    "action",
    "whatever",
    "action",
    "looking",
    "batch",
    "q",
    "eval",
    "forward",
    "feeding",
    "forward",
    "got",
    "hackiness",
    "got",
    "convert",
    "list",
    "reason",
    "memory",
    "numpy",
    "array",
    "numpy",
    "objects",
    "observation",
    "vectors",
    "numpy",
    "objects",
    "convert",
    "list",
    "numpy",
    "array",
    "nd",
    "array",
    "objects",
    "tensor",
    "pi",
    "torch",
    "complain",
    "want",
    "tensor",
    "want",
    "pi",
    "torch",
    "complain",
    "want",
    "access",
    "rows",
    "right",
    "entire",
    "batch",
    "state",
    "zeroth",
    "element",
    "want",
    "variable",
    "want",
    "pixels",
    "need",
    "second",
    "set",
    "columns",
    "want",
    "send",
    "device",
    "self",
    "dot",
    "q",
    "eval",
    "dot",
    "device",
    "ensures",
    "network",
    "set",
    "variables",
    "gets",
    "sent",
    "gpu",
    "well",
    "next",
    "thing",
    "need",
    "value",
    "successor",
    "states",
    "rows",
    "third",
    "column",
    "um",
    "members",
    "array",
    "music",
    "uh",
    "got",
    "ta",
    "quit",
    "using",
    "visual",
    "studio",
    "code",
    "annoying",
    "um",
    "scroll",
    "done",
    "um",
    "putting",
    "device",
    "evaluation",
    "uh",
    "next",
    "network",
    "one",
    "gpu",
    "two",
    "gpus",
    "would",
    "matter",
    "matter",
    "call",
    "next",
    "thing",
    "want",
    "know",
    "max",
    "action",
    "current",
    "next",
    "state",
    "right",
    "update",
    "rule",
    "actually",
    "calculates",
    "takes",
    "account",
    "purely",
    "greedy",
    "action",
    "successor",
    "state",
    "uh",
    "maybe",
    "find",
    "image",
    "flash",
    "screen",
    "make",
    "life",
    "easy",
    "next",
    "thing",
    "know",
    "max",
    "action",
    "use",
    "arg",
    "max",
    "function",
    "remember",
    "want",
    "take",
    "first",
    "dimension",
    "actions",
    "q",
    "next",
    "q",
    "predicted",
    "q",
    "next",
    "actually",
    "actions",
    "get",
    "feed",
    "forward",
    "batch",
    "size",
    "times",
    "number",
    "actions",
    "want",
    "number",
    "actions",
    "first",
    "dimension",
    "little",
    "bit",
    "attentive",
    "sending",
    "device",
    "make",
    "sure",
    "nothing",
    "gets",
    "device",
    "stuff",
    "slows",
    "crawl",
    "run",
    "cpu",
    "need",
    "get",
    "rewards",
    "uh",
    "obtained",
    "memory",
    "rows",
    "second",
    "element",
    "many",
    "parentheses",
    "one",
    "thing",
    "want",
    "loss",
    "function",
    "zero",
    "every",
    "action",
    "except",
    "max",
    "action",
    "q",
    "target",
    "equal",
    "q",
    "predicted",
    "want",
    "q",
    "target",
    "entire",
    "match",
    "max",
    "action",
    "rewards",
    "plus",
    "self",
    "dot",
    "gamma",
    "times",
    "actual",
    "value",
    "action",
    "gives",
    "value",
    "maximum",
    "element",
    "argmax",
    "gives",
    "index",
    "want",
    "find",
    "maximum",
    "action",
    "value",
    "target",
    "predicted",
    "values",
    "use",
    "update",
    "loss",
    "function",
    "next",
    "thing",
    "handle",
    "epsilon",
    "decrement",
    "want",
    "agent",
    "gradually",
    "converge",
    "purely",
    "greedy",
    "strategy",
    "behavior",
    "policy",
    "way",
    "gradually",
    "decreasing",
    "epsilon",
    "time",
    "like",
    "let",
    "simply",
    "go",
    "um",
    "start",
    "decreasing",
    "right",
    "away",
    "set",
    "number",
    "steps",
    "let",
    "run",
    "first",
    "use",
    "linear",
    "decrease",
    "time",
    "use",
    "exponential",
    "quadratic",
    "use",
    "whatever",
    "form",
    "want",
    "knowledge",
    "super",
    "critical",
    "could",
    "completely",
    "wrong",
    "seems",
    "work",
    "see",
    "third",
    "video",
    "enough",
    "left",
    "set",
    "epsilon",
    "end",
    "scroll",
    "almost",
    "done",
    "everything",
    "need",
    "everything",
    "need",
    "compute",
    "loss",
    "q",
    "target",
    "q",
    "predicted",
    "values",
    "q",
    "predicted",
    "value",
    "current",
    "set",
    "states",
    "q",
    "target",
    "related",
    "q",
    "next",
    "right",
    "q",
    "target",
    "max",
    "action",
    "next",
    "successor",
    "state",
    "almost",
    "loss",
    "mean",
    "squared",
    "error",
    "loss",
    "recall",
    "first",
    "video",
    "defined",
    "loss",
    "function",
    "torch",
    "style",
    "send",
    "device",
    "want",
    "back",
    "propagate",
    "loss",
    "backward",
    "want",
    "step",
    "perform",
    "one",
    "iteration",
    "go",
    "ahead",
    "increment",
    "learn",
    "step",
    "counter",
    "basically",
    "lot",
    "code",
    "let",
    "go",
    "back",
    "take",
    "look",
    "really",
    "quick",
    "first",
    "thing",
    "want",
    "zero",
    "gradients",
    "way",
    "actual",
    "batch",
    "optimization",
    "instead",
    "full",
    "optimization",
    "want",
    "check",
    "see",
    "gon",
    "na",
    "going",
    "replace",
    "target",
    "network",
    "time",
    "load",
    "state",
    "dictionary",
    "q",
    "eval",
    "q",
    "next",
    "network",
    "next",
    "calculate",
    "start",
    "bat",
    "uh",
    "memory",
    "sub",
    "sampling",
    "making",
    "sure",
    "get",
    "subset",
    "array",
    "go",
    "ahead",
    "sample",
    "batch",
    "memory",
    "convert",
    "numpy",
    "array",
    "oh",
    "pomodoro",
    "timer",
    "guys",
    "using",
    "pomodoro",
    "method",
    "work",
    "highly",
    "recommend",
    "go",
    "look",
    "know",
    "anyway",
    "convert",
    "numpy",
    "array",
    "go",
    "ahead",
    "feed",
    "forward",
    "current",
    "state",
    "successor",
    "state",
    "using",
    "memory",
    "sub",
    "sample",
    "um",
    "making",
    "sure",
    "sent",
    "device",
    "next",
    "thing",
    "know",
    "maximum",
    "action",
    "successor",
    "state",
    "calculate",
    "rewards",
    "agent",
    "given",
    "set",
    "q",
    "target",
    "q",
    "predicted",
    "want",
    "loss",
    "every",
    "state",
    "except",
    "loss",
    "every",
    "action",
    "except",
    "max",
    "action",
    "zero",
    "update",
    "value",
    "q",
    "target",
    "max",
    "action",
    "equal",
    "rewards",
    "plus",
    "gamma",
    "times",
    "actual",
    "value",
    "max",
    "action",
    "next",
    "make",
    "sure",
    "using",
    "way",
    "decrementing",
    "epsilon",
    "time",
    "converges",
    "small",
    "value",
    "makes",
    "agent",
    "settle",
    "mostly",
    "greedy",
    "strategy",
    "clay",
    "case",
    "using",
    "five",
    "percent",
    "time",
    "random",
    "action",
    "finally",
    "go",
    "ahead",
    "calculate",
    "loss",
    "function",
    "back",
    "propagated",
    "step",
    "optimizer",
    "increment",
    "step",
    "counter",
    "wrote",
    "learn",
    "function",
    "aging",
    "class",
    "slightly",
    "code",
    "network",
    "class",
    "still",
    "fairly",
    "typo",
    "still",
    "fairly",
    "straightforward",
    "hope",
    "informative",
    "part",
    "three",
    "going",
    "go",
    "ahead",
    "code",
    "main",
    "loop",
    "see",
    "performs",
    "look",
    "forward",
    "seeing",
    "next",
    "video",
    "comments",
    "questions",
    "suggestions",
    "go",
    "ahead",
    "leave",
    "made",
    "far",
    "please",
    "consider",
    "subscribing",
    "look",
    "forward",
    "seeing",
    "next",
    "video",
    "welcome",
    "back",
    "everybody",
    "part",
    "three",
    "coding",
    "deep",
    "q",
    "learning",
    "agent",
    "open",
    "ai",
    "gym",
    "atari",
    "library",
    "uh",
    "parts",
    "one",
    "two",
    "took",
    "look",
    "deep",
    "neural",
    "network",
    "class",
    "well",
    "aging",
    "class",
    "agent",
    "part",
    "three",
    "going",
    "finally",
    "put",
    "together",
    "main",
    "loop",
    "play",
    "game",
    "see",
    "agent",
    "let",
    "get",
    "started",
    "begin",
    "usual",
    "typical",
    "imports",
    "going",
    "need",
    "gym",
    "course",
    "going",
    "import",
    "model",
    "class",
    "model",
    "import",
    "deep",
    "sorry",
    "dq",
    "model",
    "agent",
    "also",
    "utility",
    "function",
    "going",
    "go",
    "code",
    "trivial",
    "function",
    "post",
    "uh",
    "print",
    "plot",
    "decaying",
    "epsilon",
    "running",
    "average",
    "previous",
    "five",
    "scores",
    "utils",
    "import",
    "plot",
    "learning",
    "uh",
    "oh",
    "way",
    "seen",
    "parts",
    "one",
    "two",
    "go",
    "ahead",
    "check",
    "want",
    "code",
    "please",
    "check",
    "github",
    "would",
    "like",
    "see",
    "blog",
    "article",
    "details",
    "text",
    "missed",
    "something",
    "speech",
    "go",
    "ahead",
    "click",
    "link",
    "giving",
    "issue",
    "uh",
    "oh",
    "deep",
    "cue",
    "model",
    "downside",
    "talking",
    "typing",
    "time",
    "talented",
    "uh",
    "want",
    "go",
    "ahead",
    "make",
    "environment",
    "space",
    "invaders",
    "v0",
    "uh",
    "another",
    "thing",
    "know",
    "implementations",
    "environment",
    "instead",
    "passed",
    "back",
    "image",
    "screen",
    "passed",
    "back",
    "like",
    "ram",
    "image",
    "something",
    "like",
    "never",
    "used",
    "sounds",
    "kind",
    "interesting",
    "might",
    "something",
    "check",
    "leave",
    "comment",
    "played",
    "experience",
    "think",
    "sounds",
    "cool",
    "want",
    "make",
    "agent",
    "going",
    "call",
    "brain",
    "big",
    "brain",
    "pinky",
    "brain",
    "baby",
    "gamma",
    "epsilon",
    "using",
    "epsilon",
    "starts",
    "taking",
    "purely",
    "random",
    "actions",
    "converges",
    "mostly",
    "greedy",
    "strategy",
    "learning",
    "rate",
    "max",
    "memory",
    "size",
    "5000",
    "transitions",
    "going",
    "replacement",
    "may",
    "noticed",
    "uh",
    "building",
    "agent",
    "memory",
    "instantiated",
    "empty",
    "list",
    "going",
    "use",
    "numpy",
    "arrays",
    "one",
    "thing",
    "would",
    "create",
    "array",
    "zeros",
    "shape",
    "know",
    "images",
    "well",
    "total",
    "number",
    "memories",
    "going",
    "something",
    "slightly",
    "different",
    "one",
    "way",
    "help",
    "agents",
    "learn",
    "actually",
    "watch",
    "videos",
    "humans",
    "playing",
    "fact",
    "uh",
    "deepmind",
    "team",
    "taught",
    "alpha",
    "alpha",
    "zero",
    "go",
    "play",
    "showing",
    "board",
    "configurations",
    "saying",
    "player",
    "perfectly",
    "legitimate",
    "leverage",
    "experience",
    "humans",
    "going",
    "play",
    "game",
    "agent",
    "going",
    "allow",
    "agent",
    "play",
    "game",
    "totally",
    "random",
    "going",
    "play",
    "set",
    "number",
    "games",
    "fill",
    "memory",
    "uh",
    "using",
    "totally",
    "random",
    "actions",
    "bit",
    "hack",
    "kind",
    "know",
    "seems",
    "legitimate",
    "people",
    "may",
    "frowned",
    "upon",
    "really",
    "care",
    "chose",
    "solve",
    "problem",
    "seems",
    "work",
    "fairly",
    "well",
    "brain",
    "dot",
    "mem",
    "size",
    "reset",
    "environment",
    "reset",
    "done",
    "flag",
    "play",
    "episode",
    "let",
    "know",
    "action",
    "zero",
    "action",
    "one",
    "fire",
    "two",
    "move",
    "right",
    "three",
    "move",
    "left",
    "four",
    "move",
    "right",
    "fire",
    "five",
    "move",
    "left",
    "fire",
    "zero",
    "five",
    "total",
    "six",
    "actions",
    "choose",
    "one",
    "random",
    "want",
    "verify",
    "go",
    "ahead",
    "code",
    "simple",
    "loop",
    "know",
    "loop",
    "done",
    "take",
    "action",
    "zero",
    "render",
    "see",
    "next",
    "want",
    "go",
    "ahead",
    "take",
    "action",
    "thing",
    "um",
    "fence",
    "tested",
    "experience",
    "environments",
    "basic",
    "algorithms",
    "experience",
    "makes",
    "sense",
    "penalize",
    "agent",
    "losing",
    "uh",
    "want",
    "agent",
    "naturally",
    "try",
    "maximize",
    "score",
    "want",
    "know",
    "losing",
    "really",
    "bad",
    "done",
    "may",
    "something",
    "change",
    "github",
    "go",
    "github",
    "see",
    "means",
    "tested",
    "decided",
    "stupid",
    "right",
    "think",
    "okay",
    "always",
    "open",
    "change",
    "mind",
    "though",
    "want",
    "let",
    "know",
    "losing",
    "really",
    "really",
    "sucks",
    "want",
    "store",
    "transition",
    "uh",
    "bit",
    "magical",
    "part",
    "said",
    "first",
    "video",
    "convolutional",
    "neural",
    "network",
    "want",
    "reshape",
    "three",
    "channels",
    "one",
    "asian",
    "really",
    "care",
    "color",
    "cares",
    "enemy",
    "right",
    "get",
    "information",
    "black",
    "white",
    "going",
    "take",
    "mean",
    "three",
    "channels",
    "get",
    "single",
    "channel",
    "also",
    "going",
    "go",
    "ahead",
    "truncate",
    "reason",
    "whole",
    "lot",
    "information",
    "around",
    "borders",
    "screen",
    "agent",
    "needs",
    "get",
    "away",
    "reducing",
    "memory",
    "requirements",
    "without",
    "losing",
    "anything",
    "meaningful",
    "going",
    "go",
    "ahead",
    "flash",
    "images",
    "looks",
    "like",
    "going",
    "take",
    "ops",
    "observation",
    "vector",
    "going",
    "take",
    "15",
    "230",
    "125",
    "mean",
    "performed",
    "access",
    "also",
    "want",
    "store",
    "action",
    "reward",
    "uh",
    "guys",
    "ca",
    "see",
    "store",
    "action",
    "reward",
    "well",
    "let",
    "go",
    "ahead",
    "copy",
    "also",
    "want",
    "copy",
    "also",
    "want",
    "store",
    "successor",
    "state",
    "oh",
    "good",
    "grief",
    "life",
    "hard",
    "go",
    "observation",
    "underscore",
    "successor",
    "state",
    "set",
    "observation",
    "observation",
    "underscore",
    "done",
    "let",
    "know",
    "okay",
    "okay",
    "almost",
    "next",
    "thing",
    "want",
    "keep",
    "track",
    "scores",
    "want",
    "know",
    "well",
    "agent",
    "um",
    "variable",
    "epsilon",
    "history",
    "oh",
    "uh",
    "keep",
    "track",
    "history",
    "epsilons",
    "decreases",
    "time",
    "want",
    "know",
    "relationship",
    "score",
    "epsilon",
    "run",
    "50",
    "games",
    "take",
    "batch",
    "size",
    "32",
    "memories",
    "batch",
    "size",
    "important",
    "hyper",
    "parameter",
    "find",
    "using",
    "larger",
    "batch",
    "size",
    "may",
    "get",
    "little",
    "bit",
    "better",
    "performance",
    "slows",
    "training",
    "tremendously",
    "system",
    "i7",
    "7820x",
    "32",
    "gigs",
    "ram",
    "1080",
    "ti",
    "batch",
    "size",
    "32",
    "means",
    "50",
    "games",
    "gon",
    "na",
    "run",
    "half",
    "hour",
    "takes",
    "quite",
    "bit",
    "time",
    "using",
    "larger",
    "batch",
    "size",
    "seem",
    "produce",
    "whole",
    "lot",
    "better",
    "performance",
    "certainly",
    "slows",
    "factor",
    "two",
    "scaling",
    "want",
    "know",
    "starting",
    "game",
    "plus",
    "one",
    "epsilon",
    "something",
    "dot",
    "uh",
    "say",
    "four",
    "significant",
    "figures",
    "right",
    "epsilon",
    "want",
    "go",
    "ahead",
    "append",
    "agents",
    "epsilon",
    "beginning",
    "episode",
    "reset",
    "done",
    "flag",
    "reset",
    "environment",
    "okay",
    "next",
    "thing",
    "want",
    "said",
    "unhappy",
    "invalid",
    "syntax",
    "oh",
    "offended",
    "done",
    "oh",
    "forgot",
    "comma",
    "go",
    "next",
    "thing",
    "want",
    "construct",
    "sequence",
    "frames",
    "said",
    "going",
    "pass",
    "sequence",
    "frames",
    "allow",
    "get",
    "conception",
    "movement",
    "system",
    "rather",
    "ugly",
    "way",
    "usual",
    "first",
    "thing",
    "want",
    "pass",
    "first",
    "first",
    "observation",
    "vector",
    "beginning",
    "game",
    "broken",
    "something",
    "broken",
    "frames",
    "done",
    "oh",
    "course",
    "course",
    "okay",
    "score",
    "episode",
    "zero",
    "um",
    "want",
    "keep",
    "track",
    "last",
    "action",
    "something",
    "sure",
    "must",
    "confess",
    "guys",
    "documentation",
    "open",
    "ai",
    "gym",
    "rather",
    "lackluster",
    "says",
    "action",
    "repeated",
    "k",
    "frames",
    "k",
    "set",
    "two",
    "three",
    "four",
    "uh",
    "guess",
    "gets",
    "repeated",
    "random",
    "number",
    "times",
    "since",
    "know",
    "many",
    "times",
    "want",
    "keep",
    "passing",
    "consistent",
    "set",
    "observation",
    "vectors",
    "frames",
    "something",
    "hacky",
    "keep",
    "track",
    "last",
    "action",
    "update",
    "action",
    "every",
    "third",
    "action",
    "want",
    "pass",
    "sequence",
    "three",
    "frames",
    "repeat",
    "action",
    "three",
    "times",
    "kind",
    "forcing",
    "issue",
    "seems",
    "work",
    "think",
    "best",
    "implementation",
    "know",
    "quick",
    "dirty",
    "implementation",
    "three",
    "frames",
    "go",
    "ahead",
    "choose",
    "action",
    "based",
    "reset",
    "frame",
    "variable",
    "empty",
    "list",
    "otherwise",
    "go",
    "ahead",
    "scroll",
    "want",
    "go",
    "ahead",
    "take",
    "action",
    "keep",
    "track",
    "score",
    "music",
    "append",
    "new",
    "observation",
    "uh",
    "gon",
    "na",
    "copy",
    "yeah",
    "copy",
    "um",
    "going",
    "go",
    "ahead",
    "tell",
    "losing",
    "bad",
    "like",
    "losers",
    "al",
    "dot",
    "lives",
    "thing",
    "number",
    "lives",
    "agent",
    "ale",
    "um",
    "emulator",
    "opening",
    "atar",
    "openai",
    "jim",
    "atari",
    "library",
    "built",
    "next",
    "store",
    "transition",
    "going",
    "copy",
    "code",
    "precisely",
    "fat",
    "fingers",
    "apparently",
    "screw",
    "things",
    "copy",
    "music",
    "underscore",
    "batch",
    "size",
    "keep",
    "track",
    "action",
    "put",
    "render",
    "want",
    "see",
    "um",
    "end",
    "episode",
    "end",
    "episode",
    "want",
    "append",
    "score",
    "going",
    "plot",
    "later",
    "like",
    "print",
    "score",
    "kind",
    "get",
    "idea",
    "agent",
    "need",
    "make",
    "list",
    "x",
    "variable",
    "plotting",
    "function",
    "plotting",
    "function",
    "go",
    "ahead",
    "check",
    "github",
    "easiest",
    "way",
    "going",
    "set",
    "file",
    "name",
    "music",
    "gon",
    "na",
    "call",
    "test",
    "plus",
    "string",
    "um",
    "num",
    "game",
    "something",
    "like",
    "uh",
    "plus",
    "dot",
    "png",
    "file",
    "name",
    "going",
    "plot",
    "want",
    "plot",
    "scores",
    "epsilon",
    "history",
    "file",
    "pass",
    "file",
    "name",
    "saves",
    "wrote",
    "main",
    "loop",
    "uh",
    "gone",
    "ahead",
    "run",
    "going",
    "go",
    "ahead",
    "flash",
    "results",
    "see",
    "epsilon",
    "decreases",
    "somewhat",
    "linearly",
    "time",
    "somewhat",
    "completely",
    "linearly",
    "agent",
    "performance",
    "gradually",
    "increases",
    "time",
    "keep",
    "mind",
    "plotting",
    "previous",
    "uh",
    "average",
    "previous",
    "five",
    "games",
    "reason",
    "account",
    "significant",
    "variations",
    "game",
    "game",
    "play",
    "right",
    "agent",
    "always",
    "going",
    "choose",
    "proportion",
    "random",
    "actions",
    "means",
    "randomly",
    "choose",
    "move",
    "left",
    "right",
    "enemies",
    "bullets",
    "always",
    "games",
    "going",
    "get",
    "cut",
    "short",
    "general",
    "trend",
    "performance",
    "end",
    "actually",
    "takes",
    "bit",
    "dive",
    "seen",
    "many",
    "many",
    "different",
    "iterations",
    "suspect",
    "way",
    "navigating",
    "parameter",
    "space",
    "find",
    "pretty",
    "good",
    "pockets",
    "kind",
    "shift",
    "related",
    "local",
    "minima",
    "quite",
    "good",
    "let",
    "run",
    "long",
    "enough",
    "eventually",
    "go",
    "back",
    "oscillatory",
    "behavior",
    "see",
    "increases",
    "score",
    "quite",
    "dramatically",
    "uh",
    "particular",
    "set",
    "runs",
    "saw",
    "scores",
    "excess",
    "700",
    "points",
    "actually",
    "pretty",
    "good",
    "agent",
    "um",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "looks",
    "like",
    "target",
    "network",
    "replacement",
    "see",
    "dramatically",
    "different",
    "behavior",
    "case",
    "uh",
    "epsilon",
    "decreases",
    "score",
    "actually",
    "decreases",
    "time",
    "uh",
    "know",
    "actually",
    "quite",
    "know",
    "oscillations",
    "almost",
    "certainly",
    "target",
    "network",
    "replacements",
    "uh",
    "could",
    "fluke",
    "run",
    "several",
    "times",
    "see",
    "type",
    "behavior",
    "uh",
    "target",
    "network",
    "replacement",
    "totally",
    "takes",
    "nosedive",
    "think",
    "screwed",
    "implementation",
    "please",
    "leave",
    "comment",
    "saw",
    "something",
    "looks",
    "far",
    "tell",
    "looks",
    "implemented",
    "correctly",
    "copy",
    "one",
    "state",
    "dick",
    "another",
    "real",
    "big",
    "mystery",
    "uh",
    "choose",
    "leave",
    "get",
    "significant",
    "variation",
    "performance",
    "um",
    "stories",
    "go",
    "ahead",
    "leave",
    "target",
    "network",
    "replacement",
    "uh",
    "series",
    "made",
    "agent",
    "play",
    "atari",
    "uh",
    "atari",
    "game",
    "space",
    "invaders",
    "uh",
    "uh",
    "gradually",
    "decreasing",
    "epsilon",
    "time",
    "get",
    "really",
    "good",
    "performance",
    "uh",
    "several",
    "hundred",
    "points",
    "fact",
    "actually",
    "learns",
    "play",
    "game",
    "quite",
    "well",
    "probably",
    "going",
    "go",
    "ahead",
    "spin",
    "video",
    "playing",
    "see",
    "looks",
    "um",
    "helpful",
    "please",
    "consider",
    "subscribing",
    "go",
    "ahead",
    "leave",
    "comment",
    "one",
    "question",
    "suggestion",
    "anything",
    "uh",
    "go",
    "ahead",
    "answer",
    "read",
    "comments",
    "um",
    "uh",
    "go",
    "ahead",
    "smash",
    "like",
    "button",
    "guys",
    "hope",
    "see",
    "next",
    "video",
    "uh",
    "take",
    "care",
    "video",
    "going",
    "tell",
    "everything",
    "need",
    "know",
    "start",
    "solving",
    "reinforcement",
    "learning",
    "problems",
    "policy",
    "gradient",
    "methods",
    "going",
    "give",
    "algorithm",
    "implementation",
    "details",
    "front",
    "go",
    "works",
    "would",
    "want",
    "let",
    "get",
    "basic",
    "idea",
    "behind",
    "policy",
    "gradient",
    "methods",
    "policy",
    "probability",
    "distribution",
    "agent",
    "uses",
    "pick",
    "actions",
    "use",
    "deep",
    "neural",
    "network",
    "approximate",
    "agent",
    "policy",
    "network",
    "takes",
    "observations",
    "environment",
    "input",
    "outputs",
    "actions",
    "selected",
    "according",
    "softmax",
    "activation",
    "function",
    "next",
    "generate",
    "episode",
    "keep",
    "track",
    "states",
    "actions",
    "rewards",
    "agent",
    "memory",
    "end",
    "episode",
    "go",
    "back",
    "states",
    "actions",
    "rewards",
    "compete",
    "compute",
    "discounted",
    "future",
    "returns",
    "time",
    "step",
    "use",
    "returns",
    "weights",
    "actions",
    "agent",
    "took",
    "labels",
    "perform",
    "back",
    "propagation",
    "update",
    "weights",
    "deep",
    "neural",
    "network",
    "repeat",
    "kick",
    "ass",
    "agent",
    "simple",
    "yeah",
    "know",
    "let",
    "unpack",
    "works",
    "something",
    "worth",
    "remember",
    "reinforcement",
    "learning",
    "trying",
    "maximize",
    "agent",
    "performance",
    "time",
    "let",
    "say",
    "agent",
    "performance",
    "characterized",
    "function",
    "j",
    "function",
    "weights",
    "theta",
    "deep",
    "neural",
    "network",
    "update",
    "rule",
    "theta",
    "new",
    "theta",
    "equals",
    "old",
    "theta",
    "plus",
    "learning",
    "rate",
    "times",
    "gradient",
    "performance",
    "metric",
    "note",
    "want",
    "increase",
    "performance",
    "time",
    "technically",
    "gradient",
    "ascent",
    "instead",
    "gradient",
    "descent",
    "gradient",
    "performance",
    "metric",
    "going",
    "proportional",
    "overstates",
    "amount",
    "time",
    "spend",
    "given",
    "state",
    "sum",
    "actions",
    "value",
    "state",
    "action",
    "pairs",
    "gradient",
    "policy",
    "course",
    "policy",
    "probability",
    "taking",
    "action",
    "given",
    "state",
    "really",
    "expectation",
    "value",
    "little",
    "manipulation",
    "arrive",
    "following",
    "expression",
    "plug",
    "update",
    "rule",
    "theta",
    "get",
    "expression",
    "two",
    "important",
    "features",
    "g",
    "sub",
    "term",
    "discounted",
    "feature",
    "returns",
    "referenced",
    "opening",
    "gradient",
    "policy",
    "divided",
    "policy",
    "vector",
    "tells",
    "us",
    "direction",
    "policy",
    "space",
    "maximizes",
    "chance",
    "repeat",
    "action",
    "sub",
    "multiply",
    "two",
    "get",
    "vector",
    "increases",
    "probability",
    "taking",
    "actions",
    "high",
    "expected",
    "future",
    "returns",
    "precisely",
    "agent",
    "learns",
    "time",
    "makes",
    "policy",
    "gradient",
    "methods",
    "powerful",
    "called",
    "reinforce",
    "algorithm",
    "way",
    "think",
    "long",
    "enough",
    "problems",
    "start",
    "appear",
    "one",
    "seem",
    "sample",
    "efficient",
    "top",
    "episode",
    "reset",
    "agent",
    "memory",
    "effectively",
    "discards",
    "previous",
    "experience",
    "aside",
    "new",
    "weights",
    "parameterize",
    "policy",
    "kind",
    "starting",
    "scratch",
    "every",
    "time",
    "learns",
    "worse",
    "yet",
    "agent",
    "big",
    "probability",
    "selecting",
    "action",
    "given",
    "state",
    "control",
    "variation",
    "episodes",
    "large",
    "state",
    "spaces",
    "way",
    "many",
    "combinations",
    "consider",
    "well",
    "actually",
    "problem",
    "policy",
    "gradient",
    "methods",
    "part",
    "reason",
    "agent",
    "great",
    "space",
    "invaders",
    "obviously",
    "reinforcement",
    "learning",
    "method",
    "going",
    "perfect",
    "get",
    "solution",
    "problems",
    "minute",
    "first",
    "let",
    "talk",
    "would",
    "want",
    "use",
    "policy",
    "gradients",
    "given",
    "shortcomings",
    "policy",
    "gradient",
    "method",
    "pretty",
    "different",
    "approach",
    "reinforcement",
    "learning",
    "many",
    "reinforcement",
    "learning",
    "algorithms",
    "like",
    "deep",
    "q",
    "learning",
    "instance",
    "rely",
    "estimating",
    "value",
    "state",
    "state",
    "action",
    "pair",
    "words",
    "agent",
    "wants",
    "know",
    "valuable",
    "state",
    "epsilon",
    "greedy",
    "policy",
    "let",
    "select",
    "action",
    "leads",
    "valuable",
    "states",
    "agent",
    "repeats",
    "process",
    "occasionally",
    "choosing",
    "random",
    "actions",
    "see",
    "missing",
    "something",
    "intuition",
    "behind",
    "epsilon",
    "greedy",
    "action",
    "selection",
    "really",
    "straightforward",
    "figure",
    "best",
    "action",
    "take",
    "sometimes",
    "stuff",
    "make",
    "sure",
    "wildly",
    "wrong",
    "okay",
    "makes",
    "sense",
    "assumes",
    "accurately",
    "learn",
    "action",
    "value",
    "function",
    "begin",
    "many",
    "cases",
    "value",
    "action",
    "value",
    "function",
    "incredibly",
    "complex",
    "really",
    "difficult",
    "learn",
    "realistic",
    "time",
    "scales",
    "cases",
    "optimal",
    "policy",
    "may",
    "much",
    "simpler",
    "therefore",
    "easier",
    "approximate",
    "means",
    "policy",
    "gradient",
    "agent",
    "learn",
    "beat",
    "certain",
    "environments",
    "much",
    "quickly",
    "relied",
    "algorithm",
    "like",
    "deep",
    "q",
    "learning",
    "another",
    "thing",
    "makes",
    "policy",
    "gradient",
    "methods",
    "attractive",
    "optimal",
    "policy",
    "actually",
    "deterministic",
    "really",
    "simple",
    "environments",
    "obvious",
    "deterministic",
    "policy",
    "like",
    "grid",
    "world",
    "example",
    "keeping",
    "finite",
    "epsilon",
    "means",
    "keep",
    "exploring",
    "even",
    "found",
    "best",
    "possible",
    "solution",
    "obviously",
    "complex",
    "environments",
    "optimal",
    "policy",
    "may",
    "well",
    "deterministic",
    "perhaps",
    "obvious",
    "ca",
    "guess",
    "beforehand",
    "case",
    "one",
    "could",
    "argue",
    "deep",
    "q",
    "learning",
    "would",
    "great",
    "always",
    "decrease",
    "exploration",
    "factor",
    "epsilon",
    "time",
    "allow",
    "agent",
    "settle",
    "purely",
    "greedy",
    "strategy",
    "certainly",
    "true",
    "know",
    "quickly",
    "decrease",
    "epsilon",
    "beauty",
    "policy",
    "gradients",
    "even",
    "though",
    "stochastic",
    "approach",
    "deterministic",
    "policy",
    "time",
    "actions",
    "optimal",
    "selected",
    "frequently",
    "create",
    "sort",
    "momentum",
    "drives",
    "agent",
    "towards",
    "optimal",
    "deterministic",
    "policy",
    "really",
    "feasible",
    "action",
    "value",
    "algorithms",
    "rely",
    "epsilon",
    "greedy",
    "variations",
    "shortcomings",
    "said",
    "earlier",
    "really",
    "big",
    "variations",
    "episodes",
    "since",
    "time",
    "agent",
    "visits",
    "state",
    "choose",
    "different",
    "action",
    "leads",
    "radically",
    "different",
    "future",
    "returns",
    "agent",
    "also",
    "make",
    "good",
    "use",
    "prior",
    "experience",
    "since",
    "discards",
    "time",
    "learns",
    "seem",
    "like",
    "show",
    "stoppers",
    "pretty",
    "straightforward",
    "solutions",
    "deal",
    "variance",
    "episodes",
    "want",
    "scale",
    "rewards",
    "baseline",
    "simplest",
    "baseline",
    "use",
    "average",
    "reward",
    "episode",
    "normalize",
    "g",
    "factor",
    "dividing",
    "standard",
    "deviation",
    "rewards",
    "helps",
    "control",
    "variance",
    "returns",
    "end",
    "wildly",
    "different",
    "step",
    "sizes",
    "perform",
    "update",
    "weights",
    "deep",
    "neural",
    "network",
    "dealing",
    "sample",
    "efficiency",
    "even",
    "easier",
    "possible",
    "update",
    "weights",
    "neural",
    "net",
    "episode",
    "nothing",
    "says",
    "case",
    "let",
    "agent",
    "play",
    "batch",
    "games",
    "chance",
    "visit",
    "state",
    "update",
    "weights",
    "network",
    "introduces",
    "additional",
    "hyper",
    "parameter",
    "batch",
    "size",
    "updates",
    "end",
    "much",
    "faster",
    "convergence",
    "good",
    "policy",
    "may",
    "seem",
    "obvious",
    "increasing",
    "batch",
    "size",
    "allowed",
    "go",
    "learning",
    "space",
    "invaders",
    "policy",
    "gradients",
    "something",
    "actually",
    "learns",
    "improve",
    "gameplay",
    "policy",
    "gradient",
    "learning",
    "nutshell",
    "using",
    "deep",
    "neural",
    "network",
    "approximate",
    "agent",
    "policy",
    "using",
    "gradient",
    "ascent",
    "choose",
    "actions",
    "result",
    "larger",
    "returns",
    "may",
    "sample",
    "inefficient",
    "issues",
    "scaling",
    "returns",
    "deal",
    "problems",
    "make",
    "policy",
    "gradients",
    "competitive",
    "reinforcement",
    "learning",
    "algorithms",
    "like",
    "dq",
    "learning",
    "made",
    "far",
    "check",
    "video",
    "implement",
    "policy",
    "gradients",
    "tensorflow",
    "like",
    "video",
    "make",
    "sure",
    "like",
    "video",
    "subscribe",
    "comment",
    "see",
    "next",
    "video",
    "everybody",
    "tutorial",
    "going",
    "learn",
    "land",
    "spaceship",
    "moon",
    "using",
    "policy",
    "gradient",
    "methods",
    "need",
    "know",
    "anything",
    "reinforcement",
    "learning",
    "need",
    "know",
    "anything",
    "policy",
    "gradient",
    "methods",
    "follow",
    "along",
    "let",
    "get",
    "started",
    "begin",
    "let",
    "take",
    "look",
    "basic",
    "idea",
    "want",
    "accomplish",
    "policy",
    "gradient",
    "methods",
    "work",
    "approximating",
    "policy",
    "agent",
    "policy",
    "probability",
    "distribution",
    "agent",
    "uses",
    "select",
    "actions",
    "going",
    "use",
    "deep",
    "neural",
    "network",
    "approximate",
    "probability",
    "distribution",
    "going",
    "feeding",
    "input",
    "observations",
    "environment",
    "getting",
    "probability",
    "distribution",
    "output",
    "agent",
    "learns",
    "replaying",
    "memory",
    "rewards",
    "received",
    "episode",
    "calculating",
    "discounted",
    "future",
    "rewards",
    "followed",
    "particular",
    "time",
    "step",
    "discounted",
    "feature",
    "rewards",
    "act",
    "weights",
    "update",
    "deep",
    "neural",
    "network",
    "agent",
    "assigns",
    "higher",
    "probability",
    "actions",
    "whose",
    "feature",
    "rewards",
    "higher",
    "start",
    "imports",
    "going",
    "want",
    "os",
    "handle",
    "file",
    "operations",
    "numpy",
    "handle",
    "numpy",
    "type",
    "operations",
    "course",
    "tensorflow",
    "develop",
    "agent",
    "going",
    "stick",
    "everything",
    "one",
    "class",
    "whose",
    "initialize",
    "function",
    "takes",
    "learning",
    "rate",
    "discount",
    "factor",
    "gamma",
    "number",
    "actions",
    "lunar",
    "lander",
    "environment",
    "size",
    "first",
    "hidden",
    "layer",
    "neural",
    "network",
    "default",
    "size",
    "second",
    "hidden",
    "layer",
    "deep",
    "neural",
    "network",
    "also",
    "default",
    "also",
    "need",
    "input",
    "dimms",
    "case",
    "8",
    "observation",
    "pixel",
    "image",
    "vector",
    "represents",
    "state",
    "environment",
    "also",
    "want",
    "checkpoint",
    "directory",
    "useful",
    "saving",
    "model",
    "later",
    "action",
    "space",
    "use",
    "select",
    "actions",
    "later",
    "going",
    "need",
    "number",
    "administrative",
    "type",
    "stuff",
    "instance",
    "state",
    "memory",
    "agent",
    "use",
    "keep",
    "track",
    "states",
    "visited",
    "likewise",
    "action",
    "memory",
    "want",
    "keep",
    "track",
    "actions",
    "agent",
    "took",
    "rewards",
    "received",
    "along",
    "way",
    "also",
    "need",
    "layer",
    "one",
    "size",
    "layer",
    "two",
    "size",
    "else",
    "okay",
    "move",
    "administrative",
    "stuff",
    "tensorflow",
    "tensorflow",
    "handles",
    "everything",
    "called",
    "sessions",
    "define",
    "one",
    "gon",
    "na",
    "need",
    "function",
    "build",
    "network",
    "return",
    "network",
    "going",
    "want",
    "initialize",
    "variables",
    "build",
    "network",
    "function",
    "called",
    "load",
    "variables",
    "operations",
    "tensorflow",
    "graph",
    "initialize",
    "variables",
    "initial",
    "values",
    "done",
    "random",
    "also",
    "need",
    "way",
    "saving",
    "model",
    "pc",
    "may",
    "fast",
    "enough",
    "run",
    "know",
    "short",
    "enough",
    "amount",
    "time",
    "chunks",
    "saving",
    "reloading",
    "time",
    "also",
    "need",
    "file",
    "save",
    "checkpoints",
    "next",
    "order",
    "business",
    "actually",
    "construct",
    "network",
    "need",
    "inputs",
    "need",
    "set",
    "placeholders",
    "placeholders",
    "serve",
    "placeholders",
    "inputs",
    "tells",
    "tensorflow",
    "graph",
    "hey",
    "going",
    "passing",
    "variables",
    "know",
    "yet",
    "may",
    "necessarily",
    "even",
    "know",
    "shape",
    "size",
    "know",
    "type",
    "want",
    "give",
    "names",
    "something",
    "goes",
    "wrong",
    "debug",
    "later",
    "input",
    "eight",
    "element",
    "vector",
    "represents",
    "agent",
    "observation",
    "environment",
    "really",
    "familiar",
    "tensorflow",
    "idiom",
    "saying",
    "shape",
    "equals",
    "list",
    "whose",
    "first",
    "element",
    "none",
    "tells",
    "tensorflow",
    "know",
    "batch",
    "size",
    "data",
    "going",
    "loading",
    "inputs",
    "could",
    "10",
    "states",
    "could",
    "100",
    "could",
    "thousand",
    "could",
    "number",
    "states",
    "none",
    "tells",
    "tensorflow",
    "hey",
    "know",
    "shape",
    "going",
    "know",
    "take",
    "whatever",
    "label",
    "going",
    "actions",
    "agent",
    "took",
    "course",
    "episode",
    "used",
    "calculating",
    "loss",
    "function",
    "similarly",
    "something",
    "called",
    "g",
    "g",
    "generic",
    "name",
    "agents",
    "discounted",
    "future",
    "rewards",
    "following",
    "time",
    "step",
    "use",
    "bias",
    "agent",
    "loss",
    "function",
    "towards",
    "increasing",
    "probability",
    "actions",
    "generate",
    "returns",
    "time",
    "going",
    "construct",
    "network",
    "first",
    "layer",
    "input",
    "number",
    "take",
    "course",
    "number",
    "input",
    "input",
    "dimms",
    "input",
    "output",
    "player",
    "one",
    "size",
    "activation",
    "going",
    "value",
    "function",
    "worry",
    "initializing",
    "call",
    "variables",
    "initializer",
    "going",
    "initialize",
    "layers",
    "variables",
    "values",
    "dictate",
    "think",
    "carefully",
    "dealt",
    "deep",
    "neural",
    "networks",
    "cases",
    "get",
    "vanishing",
    "exploding",
    "gradients",
    "cause",
    "values",
    "network",
    "predicts",
    "know",
    "go",
    "know",
    "either",
    "really",
    "large",
    "really",
    "small",
    "values",
    "produces",
    "junk",
    "essentially",
    "function",
    "initialize",
    "values",
    "layers",
    "relatively",
    "comparable",
    "minimal",
    "risk",
    "happening",
    "called",
    "xavier",
    "initializer",
    "going",
    "want",
    "copy",
    "going",
    "use",
    "course",
    "second",
    "layer",
    "takes",
    "first",
    "layer",
    "input",
    "l2",
    "size",
    "number",
    "units",
    "value",
    "activation",
    "course",
    "initializer",
    "l3",
    "output",
    "network",
    "however",
    "want",
    "activate",
    "yet",
    "quantity",
    "related",
    "probability",
    "sorry",
    "policy",
    "agent",
    "policy",
    "must",
    "uh",
    "property",
    "sum",
    "probabilities",
    "taking",
    "action",
    "know",
    "sum",
    "probabilities",
    "actions",
    "must",
    "equal",
    "one",
    "softmax",
    "function",
    "property",
    "going",
    "separate",
    "little",
    "bit",
    "clean",
    "terms",
    "code",
    "little",
    "bit",
    "readable",
    "us",
    "variable",
    "actually",
    "calculate",
    "probabilities",
    "selecting",
    "action",
    "softmax",
    "l3",
    "name",
    "probabilities",
    "finally",
    "need",
    "well",
    "finally",
    "next",
    "need",
    "calculate",
    "loss",
    "function",
    "stick",
    "separate",
    "scope",
    "uh",
    "need",
    "negative",
    "log",
    "probability",
    "excuse",
    "calculation",
    "involves",
    "natural",
    "log",
    "policy",
    "want",
    "take",
    "gradient",
    "natural",
    "log",
    "something",
    "need",
    "function",
    "matches",
    "property",
    "call",
    "neg",
    "log",
    "probability",
    "sparse",
    "soft",
    "max",
    "cross",
    "entropy",
    "logits",
    "try",
    "saying",
    "five",
    "times",
    "fast",
    "log",
    "x",
    "l",
    "three",
    "important",
    "part",
    "reason",
    "separated",
    "l",
    "three",
    "actions",
    "passing",
    "logits",
    "want",
    "already",
    "activated",
    "negative",
    "sparse",
    "soft",
    "max",
    "cross",
    "entropy",
    "handle",
    "softmax",
    "activation",
    "function",
    "labels",
    "label",
    "pass",
    "placeholder",
    "course",
    "actions",
    "agent",
    "took",
    "loss",
    "quantity",
    "neglog",
    "probability",
    "multiplied",
    "returns",
    "g",
    "next",
    "need",
    "training",
    "operation",
    "course",
    "gradient",
    "descent",
    "type",
    "algorithm",
    "case",
    "use",
    "atom",
    "optimizer",
    "learning",
    "rate",
    "whatever",
    "dictate",
    "constructor",
    "want",
    "minimize",
    "loss",
    "sum",
    "hole",
    "deep",
    "neural",
    "network",
    "need",
    "need",
    "code",
    "next",
    "question",
    "select",
    "actions",
    "agent",
    "agent",
    "attempting",
    "model",
    "probability",
    "distribution",
    "selecting",
    "actions",
    "means",
    "want",
    "take",
    "state",
    "input",
    "pass",
    "network",
    "get",
    "probability",
    "distribution",
    "end",
    "given",
    "variable",
    "use",
    "numpy",
    "select",
    "random",
    "action",
    "according",
    "probability",
    "distribution",
    "going",
    "go",
    "ahead",
    "reshape",
    "bill",
    "run",
    "operation",
    "tensorflow",
    "graph",
    "need",
    "specify",
    "feed",
    "dictionary",
    "gives",
    "graph",
    "input",
    "placeholder",
    "variables",
    "expecting",
    "case",
    "wants",
    "takes",
    "state",
    "input",
    "going",
    "return",
    "tuple",
    "going",
    "take",
    "0th",
    "element",
    "get",
    "correct",
    "value",
    "want",
    "select",
    "action",
    "according",
    "numpy",
    "random",
    "choice",
    "action",
    "space",
    "using",
    "probabilities",
    "distribution",
    "returned",
    "action",
    "next",
    "problem",
    "deal",
    "storing",
    "action",
    "sorry",
    "transitions",
    "course",
    "store",
    "state",
    "action",
    "reward",
    "useful",
    "learn",
    "since",
    "using",
    "lists",
    "going",
    "append",
    "elements",
    "end",
    "list",
    "next",
    "come",
    "meat",
    "problem",
    "learning",
    "function",
    "require",
    "input",
    "basic",
    "idea",
    "going",
    "convert",
    "lists",
    "numpy",
    "arrays",
    "easily",
    "manipulate",
    "going",
    "iterate",
    "agent",
    "memory",
    "rewards",
    "received",
    "calculate",
    "discounted",
    "sum",
    "rewards",
    "followed",
    "time",
    "step",
    "going",
    "need",
    "two",
    "loops",
    "variable",
    "keep",
    "track",
    "sum",
    "well",
    "something",
    "keep",
    "track",
    "discounting",
    "let",
    "deal",
    "memories",
    "first",
    "wo",
    "work",
    "go",
    "instantiate",
    "g",
    "factor",
    "get",
    "shape",
    "reward",
    "memory",
    "iterate",
    "length",
    "memory",
    "length",
    "episode",
    "time",
    "step",
    "want",
    "calculate",
    "sum",
    "rewards",
    "follow",
    "time",
    "step",
    "correct",
    "yes",
    "next",
    "take",
    "sum",
    "discount",
    "course",
    "discount",
    "gamma",
    "k",
    "k",
    "time",
    "step",
    "course",
    "rewards",
    "following",
    "teeth",
    "teeth",
    "teeth",
    "time",
    "step",
    "g",
    "sum",
    "sum",
    "weighted",
    "discounted",
    "rewards",
    "next",
    "thing",
    "think",
    "rewards",
    "vary",
    "lot",
    "episodes",
    "promote",
    "stability",
    "algorithm",
    "want",
    "scale",
    "results",
    "number",
    "turns",
    "reasonable",
    "number",
    "use",
    "mean",
    "going",
    "subtract",
    "mean",
    "rewards",
    "agent",
    "received",
    "episode",
    "divide",
    "standard",
    "deviation",
    "give",
    "us",
    "nice",
    "scaled",
    "normalized",
    "numbers",
    "algorithm",
    "produce",
    "wacky",
    "results",
    "since",
    "dividing",
    "standard",
    "deviation",
    "account",
    "possibility",
    "standard",
    "deviation",
    "could",
    "zero",
    "hence",
    "conditional",
    "statement",
    "next",
    "run",
    "training",
    "operation",
    "underscore",
    "tells",
    "us",
    "really",
    "care",
    "returns",
    "run",
    "training",
    "operation",
    "feed",
    "dictionary",
    "take",
    "input",
    "state",
    "memory",
    "take",
    "labels",
    "action",
    "memory",
    "take",
    "g",
    "factor",
    "g",
    "calculated",
    "end",
    "episode",
    "finish",
    "learning",
    "want",
    "reset",
    "agent",
    "memory",
    "rewards",
    "one",
    "episode",
    "spill",
    "another",
    "two",
    "functions",
    "need",
    "administrative",
    "need",
    "way",
    "load",
    "checkpoint",
    "go",
    "ahead",
    "print",
    "loading",
    "checkpoint",
    "know",
    "something",
    "saver",
    "object",
    "want",
    "restore",
    "session",
    "checkpoint",
    "file",
    "next",
    "want",
    "save",
    "checkpoint",
    "going",
    "print",
    "take",
    "back",
    "reason",
    "going",
    "saving",
    "lot",
    "want",
    "print",
    "bunch",
    "junk",
    "statements",
    "either",
    "taking",
    "current",
    "graph",
    "right",
    "sticking",
    "file",
    "conversely",
    "taking",
    "graph",
    "file",
    "loading",
    "graph",
    "current",
    "session",
    "next",
    "move",
    "main",
    "program",
    "actually",
    "test",
    "lander",
    "need",
    "jim",
    "know",
    "made",
    "clear",
    "beginning",
    "also",
    "need",
    "box",
    "2d",
    "dash",
    "pi",
    "environment",
    "go",
    "ahead",
    "pip",
    "install",
    "box",
    "2d",
    "dash",
    "pi",
    "already",
    "need",
    "import",
    "policy",
    "gradient",
    "agent",
    "plot",
    "learning",
    "function",
    "always",
    "use",
    "find",
    "github",
    "along",
    "code",
    "course",
    "elaborate",
    "basically",
    "takes",
    "sequence",
    "rewards",
    "keeps",
    "track",
    "performs",
    "running",
    "average",
    "say",
    "last",
    "20",
    "25",
    "whatever",
    "amount",
    "want",
    "spits",
    "file",
    "also",
    "way",
    "saving",
    "renderings",
    "environment",
    "runs",
    "much",
    "faster",
    "actually",
    "render",
    "screen",
    "training",
    "save",
    "renderings",
    "files",
    "mp4",
    "files",
    "go",
    "back",
    "watch",
    "later",
    "produce",
    "episodes",
    "saw",
    "beginning",
    "video",
    "first",
    "thing",
    "going",
    "instantiate",
    "agent",
    "use",
    "learning",
    "rate",
    "zero",
    "three",
    "zeros",
    "five",
    "believe",
    "uh",
    "oh",
    "need",
    "gamma",
    "use",
    "something",
    "like",
    "parameters",
    "leave",
    "defaults",
    "next",
    "need",
    "make",
    "environment",
    "lunarlander",
    "v2",
    "way",
    "keep",
    "track",
    "scores",
    "agent",
    "received",
    "initial",
    "score",
    "number",
    "episodes",
    "achieved",
    "uh",
    "pretty",
    "good",
    "results",
    "2500",
    "episodes",
    "start",
    "first",
    "thing",
    "want",
    "iterate",
    "episodes",
    "oh",
    "let",
    "um",
    "comment",
    "want",
    "save",
    "output",
    "env",
    "equals",
    "wrappers",
    "dot",
    "monitor",
    "pass",
    "environment",
    "wherever",
    "want",
    "save",
    "lunar",
    "lander",
    "want",
    "use",
    "lambda",
    "function",
    "tell",
    "render",
    "every",
    "episode",
    "force",
    "equals",
    "true",
    "believe",
    "tells",
    "overwrite",
    "already",
    "data",
    "directory",
    "top",
    "every",
    "episode",
    "print",
    "episode",
    "number",
    "current",
    "score",
    "set",
    "done",
    "flag",
    "subsequent",
    "episodes",
    "want",
    "reset",
    "score",
    "reset",
    "environment",
    "play",
    "episode",
    "first",
    "thing",
    "want",
    "select",
    "action",
    "takes",
    "observation",
    "input",
    "need",
    "get",
    "new",
    "observation",
    "reward",
    "done",
    "flag",
    "info",
    "stepping",
    "environment",
    "done",
    "want",
    "store",
    "transition",
    "set",
    "old",
    "observation",
    "new",
    "one",
    "select",
    "action",
    "based",
    "new",
    "estate",
    "keep",
    "track",
    "reward",
    "received",
    "end",
    "every",
    "episode",
    "want",
    "append",
    "score",
    "score",
    "history",
    "perform",
    "learning",
    "operation",
    "also",
    "want",
    "save",
    "checkpoint",
    "every",
    "operation",
    "ever",
    "every",
    "learning",
    "operation",
    "done",
    "dictate",
    "file",
    "name",
    "dot",
    "png",
    "call",
    "super",
    "secret",
    "plot",
    "learning",
    "function",
    "takes",
    "score",
    "history",
    "file",
    "name",
    "window",
    "tells",
    "many",
    "games",
    "want",
    "take",
    "running",
    "average",
    "let",
    "go",
    "terminal",
    "see",
    "many",
    "mistakes",
    "made",
    "one",
    "second",
    "right",
    "let",
    "go",
    "ahead",
    "give",
    "try",
    "made",
    "kind",
    "error",
    "policy",
    "gradient",
    "agent",
    "let",
    "swing",
    "back",
    "file",
    "see",
    "one",
    "second",
    "model",
    "input",
    "dimms",
    "forgot",
    "keep",
    "track",
    "save",
    "go",
    "back",
    "terminal",
    "see",
    "apparently",
    "called",
    "something",
    "line",
    "27",
    "says",
    "placeholder",
    "got",
    "unexpected",
    "keyword",
    "argument",
    "named",
    "label",
    "ah",
    "called",
    "name",
    "happens",
    "try",
    "type",
    "talk",
    "time",
    "let",
    "make",
    "sure",
    "call",
    "l1",
    "size",
    "l2",
    "size",
    "something",
    "different",
    "right",
    "run",
    "unhappy",
    "choice",
    "oh",
    "course",
    "lunar",
    "dash",
    "get",
    "rid",
    "typos",
    "galore",
    "tonight",
    "oh",
    "really",
    "oh",
    "really",
    "ah",
    "store",
    "transitions",
    "yes",
    "actually",
    "store",
    "transitions",
    "let",
    "call",
    "go",
    "perfect",
    "actually",
    "learning",
    "going",
    "sit",
    "wait",
    "2500",
    "games",
    "already",
    "run",
    "got",
    "kodi",
    "saw",
    "beginning",
    "going",
    "go",
    "ahead",
    "show",
    "plot",
    "produces",
    "see",
    "actually",
    "learns",
    "end",
    "gets",
    "average",
    "reward",
    "around",
    "200",
    "points",
    "considered",
    "solved",
    "check",
    "documentation",
    "openai",
    "gym",
    "congratulations",
    "solved",
    "lunar",
    "lander",
    "environment",
    "policy",
    "gradient",
    "algorithm",
    "relatively",
    "simple",
    "deep",
    "neural",
    "network",
    "calculates",
    "probability",
    "agent",
    "picking",
    "action",
    "hope",
    "helpful",
    "go",
    "ahead",
    "leave",
    "comment",
    "feel",
    "free",
    "take",
    "code",
    "github",
    "fork",
    "make",
    "better",
    "whatever",
    "want",
    "look",
    "forward",
    "seeing",
    "next",
    "video",
    "welcome",
    "back",
    "everybody",
    "new",
    "reinforcement",
    "learning",
    "tutorial",
    "today",
    "episode",
    "going",
    "teach",
    "agent",
    "play",
    "space",
    "invaders",
    "using",
    "policy",
    "gradient",
    "method",
    "let",
    "get",
    "imports",
    "start",
    "usual",
    "suspects",
    "numpy",
    "tensorflow",
    "start",
    "initializing",
    "class",
    "policy",
    "gradient",
    "agent",
    "take",
    "learning",
    "rate",
    "discount",
    "factor",
    "number",
    "actions",
    "number",
    "layers",
    "fully",
    "connected",
    "layer",
    "input",
    "shape",
    "channels",
    "directory",
    "checkpoints",
    "important",
    "well",
    "parameter",
    "dictate",
    "gpu",
    "want",
    "tensorflow",
    "use",
    "one",
    "gpu",
    "using",
    "cpu",
    "need",
    "parameter",
    "save",
    "relevant",
    "parameters",
    "compute",
    "action",
    "space",
    "set",
    "integers",
    "want",
    "subtract",
    "input",
    "heights",
    "width",
    "input",
    "shapes",
    "keep",
    "track",
    "number",
    "channels",
    "use",
    "later",
    "agent",
    "memory",
    "comprised",
    "three",
    "lifts",
    "keep",
    "track",
    "state",
    "action",
    "rewards",
    "want",
    "configuration",
    "tells",
    "tensorflow",
    "gpu",
    "want",
    "use",
    "well",
    "session",
    "graph",
    "uses",
    "config",
    "call",
    "build",
    "network",
    "function",
    "use",
    "tf",
    "global",
    "variables",
    "initializer",
    "need",
    "keep",
    "track",
    "saver",
    "checkpoint",
    "file",
    "saving",
    "loading",
    "model",
    "later",
    "know",
    "anything",
    "policy",
    "gradient",
    "methods",
    "worry",
    "going",
    "cover",
    "everything",
    "need",
    "know",
    "go",
    "along",
    "first",
    "thing",
    "going",
    "need",
    "convolutional",
    "neural",
    "network",
    "handle",
    "image",
    "connected",
    "fully",
    "connected",
    "layer",
    "allows",
    "agent",
    "estimate",
    "action",
    "wants",
    "take",
    "contrast",
    "things",
    "like",
    "deep",
    "q",
    "learning",
    "policy",
    "gradient",
    "methods",
    "actually",
    "try",
    "learn",
    "action",
    "value",
    "value",
    "functions",
    "rather",
    "policy",
    "gradients",
    "try",
    "approximate",
    "actual",
    "policy",
    "agent",
    "let",
    "take",
    "look",
    "build",
    "network",
    "function",
    "going",
    "construct",
    "convolutional",
    "neural",
    "network",
    "parameters",
    "input",
    "placeholder",
    "shape",
    "batch",
    "size",
    "input",
    "height",
    "width",
    "well",
    "number",
    "channels",
    "also",
    "need",
    "input",
    "labels",
    "correspond",
    "actions",
    "agent",
    "takes",
    "shape",
    "batch",
    "size",
    "factor",
    "g",
    "discounted",
    "future",
    "rewards",
    "following",
    "given",
    "time",
    "step",
    "shape",
    "batch",
    "size",
    "convolutional",
    "neural",
    "network",
    "going",
    "pretty",
    "straightforward",
    "seen",
    "videos",
    "kind",
    "know",
    "expect",
    "first",
    "layer",
    "32",
    "filters",
    "kernel",
    "size",
    "eight",
    "eight",
    "stride",
    "four",
    "going",
    "want",
    "use",
    "initializer",
    "found",
    "xavier",
    "initializer",
    "works",
    "quite",
    "well",
    "purpose",
    "keep",
    "initialize",
    "parameters",
    "way",
    "uh",
    "network",
    "one",
    "layer",
    "parameters",
    "significantly",
    "larger",
    "want",
    "batch",
    "normalization",
    "mistyped",
    "epsilon",
    "one",
    "ten",
    "minus",
    "five",
    "want",
    "course",
    "use",
    "value",
    "activation",
    "first",
    "convolutional",
    "layer",
    "commvault",
    "activated",
    "output",
    "serves",
    "input",
    "next",
    "layer",
    "64",
    "filters",
    "kernel",
    "size",
    "4x4",
    "stride",
    "2",
    "use",
    "xavier",
    "initializer",
    "course",
    "also",
    "want",
    "batch",
    "normalization",
    "layer",
    "point",
    "think",
    "actually",
    "get",
    "correct",
    "epsilon",
    "batch",
    "norm",
    "course",
    "want",
    "use",
    "value",
    "activation",
    "batch",
    "normed",
    "output",
    "well",
    "third",
    "convolutional",
    "layer",
    "2d",
    "convolution",
    "128",
    "filters",
    "kernel",
    "size",
    "2x2",
    "stride",
    "1",
    "also",
    "use",
    "initialization",
    "batch",
    "normalization",
    "epsilon",
    "1",
    "10",
    "minus",
    "activate",
    "value",
    "next",
    "take",
    "account",
    "fully",
    "connected",
    "layers",
    "first",
    "thing",
    "need",
    "flatten",
    "output",
    "convolutions",
    "come",
    "matrices",
    "need",
    "list",
    "go",
    "ahead",
    "make",
    "first",
    "fully",
    "connected",
    "layer",
    "using",
    "fc1",
    "number",
    "units",
    "value",
    "activation",
    "second",
    "dense",
    "layer",
    "going",
    "units",
    "equal",
    "number",
    "actions",
    "agent",
    "case",
    "six",
    "notice",
    "activate",
    "separate",
    "variable",
    "actions",
    "activated",
    "output",
    "network",
    "going",
    "use",
    "soft",
    "max",
    "way",
    "get",
    "probabilities",
    "add",
    "one",
    "need",
    "calculate",
    "negative",
    "log",
    "probability",
    "sparse",
    "softmax",
    "function",
    "logits",
    "using",
    "dense",
    "two",
    "logits",
    "labels",
    "actions",
    "labels",
    "loss",
    "quantity",
    "multiplied",
    "expected",
    "future",
    "rewards",
    "course",
    "want",
    "reduce",
    "quantity",
    "training",
    "operation",
    "going",
    "use",
    "rms",
    "prop",
    "optimizer",
    "set",
    "parameters",
    "found",
    "work",
    "quite",
    "well",
    "algorithm",
    "pretty",
    "finicky",
    "may",
    "play",
    "around",
    "next",
    "thing",
    "code",
    "action",
    "selection",
    "algorithm",
    "agent",
    "policy",
    "gradient",
    "methods",
    "trying",
    "actually",
    "approximate",
    "policy",
    "means",
    "trying",
    "approximate",
    "distribution",
    "agent",
    "chooses",
    "actions",
    "given",
    "state",
    "need",
    "way",
    "computing",
    "probabilities",
    "sampling",
    "sampling",
    "actions",
    "according",
    "probabilities",
    "choose",
    "action",
    "taking",
    "observation",
    "reshaping",
    "course",
    "sequence",
    "frames",
    "uh",
    "high",
    "want",
    "calculate",
    "probabilities",
    "associated",
    "action",
    "given",
    "observation",
    "want",
    "sample",
    "probability",
    "distribution",
    "using",
    "numpy",
    "random",
    "choice",
    "function",
    "next",
    "take",
    "care",
    "agent",
    "memory",
    "going",
    "store",
    "observation",
    "action",
    "reward",
    "agent",
    "list",
    "using",
    "simple",
    "append",
    "function",
    "one",
    "big",
    "problem",
    "going",
    "solve",
    "fact",
    "policy",
    "gradient",
    "methods",
    "incredibly",
    "simple",
    "efficient",
    "monte",
    "carlo",
    "methods",
    "meaning",
    "end",
    "every",
    "episode",
    "agent",
    "learning",
    "throws",
    "away",
    "experience",
    "required",
    "prior",
    "episodes",
    "deal",
    "well",
    "one",
    "way",
    "deal",
    "actually",
    "queue",
    "batch",
    "episodes",
    "learn",
    "based",
    "batch",
    "experiences",
    "trouble",
    "take",
    "rewards",
    "follow",
    "given",
    "time",
    "step",
    "want",
    "account",
    "rewards",
    "following",
    "current",
    "episode",
    "want",
    "rewards",
    "one",
    "episode",
    "spilling",
    "another",
    "take",
    "care",
    "next",
    "handle",
    "learning",
    "agent",
    "want",
    "want",
    "convert",
    "state",
    "action",
    "reward",
    "memories",
    "arrays",
    "feed",
    "numpad",
    "excuse",
    "tensorflow",
    "learning",
    "function",
    "sorry",
    "tensorflow",
    "graph",
    "start",
    "reshaping",
    "state",
    "memory",
    "something",
    "feasible",
    "calculate",
    "expected",
    "feature",
    "rewards",
    "starting",
    "given",
    "state",
    "going",
    "iterate",
    "entire",
    "memory",
    "take",
    "account",
    "rewards",
    "agent",
    "receives",
    "subsequent",
    "time",
    "steps",
    "also",
    "need",
    "make",
    "sure",
    "going",
    "take",
    "account",
    "rewards",
    "next",
    "episode",
    "next",
    "scale",
    "expected",
    "feature",
    "rewards",
    "reduce",
    "variance",
    "problem",
    "let",
    "make",
    "sure",
    "really",
    "really",
    "large",
    "rewards",
    "everything",
    "kind",
    "scaled",
    "next",
    "call",
    "training",
    "operation",
    "appropriate",
    "feed",
    "dict",
    "state",
    "memory",
    "action",
    "memory",
    "labels",
    "g",
    "g",
    "variable",
    "finally",
    "since",
    "done",
    "going",
    "clear",
    "agent",
    "memories",
    "finally",
    "bookkeeping",
    "functions",
    "load",
    "save",
    "checkpoints",
    "call",
    "saver",
    "restore",
    "function",
    "loads",
    "checkpoint",
    "file",
    "session",
    "save",
    "checkpoint",
    "dumps",
    "current",
    "session",
    "checkpoint",
    "file",
    "another",
    "problem",
    "solve",
    "agent",
    "get",
    "sense",
    "motion",
    "single",
    "image",
    "right",
    "show",
    "single",
    "image",
    "know",
    "aliens",
    "moving",
    "left",
    "right",
    "really",
    "know",
    "direction",
    "moving",
    "pass",
    "sequence",
    "frames",
    "get",
    "sense",
    "motion",
    "agent",
    "complicated",
    "fact",
    "open",
    "ai",
    "gym",
    "atari",
    "library",
    "particular",
    "returns",
    "set",
    "frames",
    "repeated",
    "actually",
    "cycle",
    "observations",
    "time",
    "see",
    "frames",
    "change",
    "sorry",
    "change",
    "uh",
    "based",
    "interval",
    "one",
    "two",
    "three",
    "capture",
    "enough",
    "frames",
    "account",
    "fact",
    "well",
    "get",
    "overall",
    "sense",
    "movement",
    "means",
    "four",
    "going",
    "magic",
    "number",
    "stacking",
    "frames",
    "next",
    "move",
    "main",
    "program",
    "import",
    "gym",
    "numpy",
    "model",
    "well",
    "plot",
    "learning",
    "function",
    "simple",
    "utility",
    "find",
    "github",
    "well",
    "wrappers",
    "record",
    "agent",
    "gameplay",
    "choose",
    "need",
    "observation",
    "truncating",
    "taking",
    "average",
    "next",
    "stack",
    "frames",
    "beginning",
    "episode",
    "stack",
    "frames",
    "none",
    "want",
    "initialize",
    "empty",
    "array",
    "zeros",
    "iterate",
    "array",
    "set",
    "rows",
    "current",
    "observation",
    "otherwise",
    "want",
    "want",
    "pop",
    "bottom",
    "observation",
    "shift",
    "everything",
    "put",
    "fourth",
    "spot",
    "last",
    "spot",
    "current",
    "observation",
    "main",
    "function",
    "checkpoint",
    "flag",
    "want",
    "load",
    "checkpoint",
    "want",
    "initialize",
    "agent",
    "set",
    "hyper",
    "parameters",
    "found",
    "work",
    "reasonably",
    "well",
    "play",
    "around",
    "incredibly",
    "finicky",
    "mileage",
    "may",
    "vary",
    "need",
    "file",
    "name",
    "save",
    "plots",
    "also",
    "want",
    "see",
    "want",
    "load",
    "checkpoint",
    "next",
    "initialize",
    "environment",
    "space",
    "invaders",
    "course",
    "keep",
    "track",
    "score",
    "history",
    "score",
    "number",
    "episodes",
    "stack",
    "size",
    "four",
    "one",
    "iterate",
    "number",
    "episodes",
    "resetting",
    "done",
    "flag",
    "top",
    "episode",
    "also",
    "want",
    "keep",
    "track",
    "running",
    "average",
    "score",
    "previous",
    "20",
    "games",
    "got",
    "idea",
    "actually",
    "learning",
    "every",
    "20",
    "games",
    "gon",
    "na",
    "print",
    "uh",
    "episode",
    "score",
    "average",
    "score",
    "otherwise",
    "every",
    "every",
    "episode",
    "going",
    "print",
    "episode",
    "number",
    "score",
    "reset",
    "environment",
    "course",
    "observation",
    "go",
    "ahead",
    "set",
    "stacked",
    "frames",
    "none",
    "top",
    "episode",
    "call",
    "stack",
    "frames",
    "function",
    "get",
    "four",
    "initial",
    "observation",
    "set",
    "score",
    "zero",
    "start",
    "iterating",
    "episode",
    "first",
    "step",
    "choose",
    "action",
    "based",
    "set",
    "stacked",
    "frames",
    "go",
    "ahead",
    "take",
    "action",
    "get",
    "new",
    "state",
    "action",
    "reward",
    "go",
    "ahead",
    "observation",
    "way",
    "stack",
    "stack",
    "frames",
    "next",
    "take",
    "care",
    "agent",
    "memory",
    "storing",
    "transition",
    "finally",
    "increment",
    "score",
    "save",
    "score",
    "end",
    "episode",
    "every",
    "10",
    "games",
    "going",
    "handle",
    "learning",
    "saving",
    "checkpoint",
    "done",
    "go",
    "ahead",
    "plot",
    "learning",
    "agent",
    "done",
    "take",
    "look",
    "actual",
    "plot",
    "produces",
    "time",
    "know",
    "agent",
    "actually",
    "learning",
    "see",
    "increase",
    "average",
    "reward",
    "time",
    "see",
    "oscillations",
    "perfectly",
    "normal",
    "want",
    "overall",
    "upward",
    "trend",
    "particular",
    "set",
    "algorithms",
    "notoriously",
    "finicky",
    "respect",
    "learning",
    "rates",
    "spend",
    "huge",
    "amount",
    "time",
    "tuning",
    "playing",
    "wanted",
    "get",
    "something",
    "good",
    "enough",
    "show",
    "guys",
    "works",
    "turn",
    "capable",
    "hands",
    "fine",
    "tuning",
    "see",
    "definite",
    "increase",
    "time",
    "agent",
    "average",
    "reward",
    "improves",
    "100",
    "points",
    "going",
    "win",
    "awards",
    "definitely",
    "clear",
    "unequivocal",
    "sign",
    "learning",
    "policy",
    "gradients",
    "space",
    "invaders",
    "environment",
    "open",
    "ai",
    "gym",
    "hope",
    "learned",
    "something",
    "make",
    "sure",
    "check",
    "code",
    "github",
    "fork",
    "copy",
    "whatever",
    "want",
    "make",
    "sure",
    "subscribe",
    "leave",
    "comment",
    "found",
    "helpful",
    "look",
    "forward",
    "seeing",
    "next",
    "video",
    "welcome",
    "back",
    "everybody",
    "host",
    "phil",
    "tabor",
    "previously",
    "subscriber",
    "asked",
    "hey",
    "phil",
    "create",
    "reinforcement",
    "learning",
    "environment",
    "said",
    "well",
    "great",
    "question",
    "time",
    "answer",
    "comments",
    "make",
    "video",
    "going",
    "next",
    "two",
    "videos",
    "create",
    "open",
    "ai",
    "gym",
    "compliant",
    "reinforcement",
    "learning",
    "environment",
    "grid",
    "world",
    "going",
    "text",
    "based",
    "familiar",
    "grid",
    "world",
    "aptly",
    "named",
    "grid",
    "size",
    "n",
    "agent",
    "starts",
    "say",
    "top",
    "left",
    "navigate",
    "way",
    "way",
    "bottom",
    "right",
    "twist",
    "going",
    "two",
    "magic",
    "squares",
    "cause",
    "agent",
    "teleport",
    "across",
    "board",
    "purpose",
    "create",
    "shortcut",
    "see",
    "agent",
    "actually",
    "learn",
    "shortcut",
    "kind",
    "interesting",
    "agent",
    "receives",
    "reward",
    "step",
    "except",
    "terminal",
    "step",
    "receives",
    "reward",
    "zero",
    "therefore",
    "agent",
    "attempt",
    "maximize",
    "reward",
    "minimizing",
    "number",
    "steps",
    "takes",
    "get",
    "grid",
    "world",
    "things",
    "two",
    "concepts",
    "need",
    "concept",
    "state",
    "space",
    "set",
    "states",
    "minus",
    "terminal",
    "state",
    "state",
    "space",
    "plus",
    "set",
    "states",
    "including",
    "terminal",
    "state",
    "gives",
    "us",
    "bit",
    "handy",
    "way",
    "find",
    "terminal",
    "state",
    "well",
    "find",
    "attempting",
    "make",
    "illegal",
    "moves",
    "also",
    "follows",
    "nomenclature",
    "terminology",
    "sutton",
    "bardo",
    "book",
    "reinforcement",
    "learning",
    "awesome",
    "resource",
    "definitely",
    "check",
    "already",
    "part",
    "one",
    "going",
    "handle",
    "environment",
    "part",
    "two",
    "going",
    "get",
    "main",
    "loop",
    "agent",
    "use",
    "q",
    "learning",
    "deep",
    "q",
    "learning",
    "straightforward",
    "environment",
    "need",
    "functional",
    "approximation",
    "need",
    "tabular",
    "representation",
    "agent",
    "estimate",
    "action",
    "value",
    "function",
    "familiar",
    "q",
    "learning",
    "couple",
    "videos",
    "topic",
    "one",
    "q",
    "learning",
    "agent",
    "solved",
    "card",
    "poll",
    "game",
    "well",
    "explainer",
    "type",
    "video",
    "talks",
    "exactly",
    "let",
    "go",
    "ahead",
    "get",
    "started",
    "couple",
    "dependencies",
    "anything",
    "gpu",
    "numpy",
    "matplotlib",
    "want",
    "close",
    "everything",
    "class",
    "called",
    "grid",
    "world",
    "initializer",
    "take",
    "n",
    "shape",
    "grid",
    "well",
    "magic",
    "squares",
    "represent",
    "grid",
    "array",
    "zeros",
    "shape",
    "n",
    "want",
    "want",
    "learn",
    "type",
    "uh",
    "want",
    "learn",
    "sorry",
    "want",
    "keep",
    "tr",
    "want",
    "keep",
    "track",
    "end",
    "handy",
    "use",
    "later",
    "let",
    "go",
    "ahead",
    "define",
    "state",
    "space",
    "going",
    "list",
    "comprehension",
    "states",
    "range",
    "times",
    "said",
    "state",
    "space",
    "include",
    "terminal",
    "state",
    "terminal",
    "state",
    "bottom",
    "right",
    "go",
    "ahead",
    "remove",
    "pop",
    "particular",
    "state",
    "list",
    "uh",
    "next",
    "let",
    "go",
    "ahead",
    "learn",
    "type",
    "go",
    "ahead",
    "define",
    "state",
    "space",
    "plus",
    "also",
    "need",
    "know",
    "way",
    "actions",
    "map",
    "change",
    "environment",
    "call",
    "action",
    "space",
    "little",
    "bit",
    "misnomer",
    "live",
    "moving",
    "translate",
    "agent",
    "one",
    "row",
    "distance",
    "moving",
    "advance",
    "agent",
    "position",
    "downward",
    "also",
    "um",
    "moving",
    "left",
    "translate",
    "agent",
    "one",
    "step",
    "decrement",
    "agent",
    "position",
    "one",
    "moving",
    "right",
    "increase",
    "one",
    "also",
    "want",
    "keep",
    "track",
    "set",
    "possible",
    "actions",
    "could",
    "use",
    "keys",
    "action",
    "space",
    "dictionary",
    "let",
    "go",
    "ahead",
    "use",
    "separate",
    "structure",
    "use",
    "list",
    "left",
    "right",
    "reason",
    "q",
    "learning",
    "agent",
    "q",
    "learning",
    "algorithm",
    "sorry",
    "choose",
    "actions",
    "random",
    "handy",
    "list",
    "choose",
    "randomly",
    "uh",
    "next",
    "need",
    "add",
    "magic",
    "squares",
    "little",
    "bit",
    "complicated",
    "may",
    "seem",
    "finally",
    "initialize",
    "grid",
    "want",
    "set",
    "agent",
    "top",
    "left",
    "position",
    "let",
    "go",
    "ahead",
    "add",
    "magic",
    "squares",
    "course",
    "want",
    "store",
    "object",
    "little",
    "bit",
    "hokiness",
    "must",
    "explain",
    "agent",
    "represented",
    "zero",
    "print",
    "grid",
    "terminal",
    "uh",
    "md",
    "squares",
    "represented",
    "one",
    "excuse",
    "means",
    "need",
    "something",
    "0",
    "1",
    "represent",
    "magic",
    "squares",
    "want",
    "render",
    "environment",
    "want",
    "know",
    "entrance",
    "exit",
    "use",
    "different",
    "values",
    "entrance",
    "exit",
    "way",
    "render",
    "correctly",
    "royal",
    "decree",
    "set",
    "would",
    "representation",
    "magic",
    "square",
    "grid",
    "world",
    "2",
    "start",
    "going",
    "go",
    "ahead",
    "iterate",
    "magic",
    "squares",
    "need",
    "know",
    "uh",
    "position",
    "sorry",
    "color",
    "indicating",
    "something",
    "wrong",
    "screwed",
    "something",
    "royally",
    "see",
    "blind",
    "anyway",
    "x",
    "position",
    "going",
    "floor",
    "current",
    "square",
    "number",
    "rows",
    "modulus",
    "number",
    "columns",
    "grid",
    "want",
    "set",
    "x",
    "position",
    "since",
    "want",
    "different",
    "representation",
    "entrance",
    "exit",
    "go",
    "ahead",
    "set",
    "increment",
    "one",
    "recall",
    "magic",
    "squares",
    "represented",
    "dictionary",
    "iterating",
    "keys",
    "values",
    "destinations",
    "keys",
    "source",
    "values",
    "destinations",
    "next",
    "want",
    "find",
    "precisely",
    "destinations",
    "set",
    "set",
    "grid",
    "square",
    "increment",
    "going",
    "going",
    "two",
    "magic",
    "squares",
    "number",
    "case",
    "gon",
    "na",
    "two",
    "okay",
    "next",
    "thing",
    "need",
    "know",
    "terminal",
    "state",
    "said",
    "earlier",
    "state",
    "space",
    "state",
    "space",
    "plus",
    "concepts",
    "give",
    "us",
    "easy",
    "way",
    "let",
    "go",
    "ahead",
    "take",
    "care",
    "since",
    "state",
    "space",
    "plus",
    "states",
    "state",
    "space",
    "states",
    "minus",
    "terminal",
    "state",
    "know",
    "difference",
    "two",
    "sets",
    "terminal",
    "state",
    "state",
    "state",
    "space",
    "plus",
    "state",
    "space",
    "look",
    "let",
    "scroll",
    "bit",
    "okay",
    "next",
    "let",
    "us",
    "go",
    "ahead",
    "get",
    "agent",
    "row",
    "column",
    "going",
    "use",
    "logic",
    "next",
    "want",
    "set",
    "state",
    "take",
    "new",
    "state",
    "input",
    "going",
    "go",
    "ahead",
    "assume",
    "new",
    "state",
    "allowed",
    "agent",
    "along",
    "left",
    "edge",
    "attempts",
    "move",
    "left",
    "receives",
    "reward",
    "minus",
    "one",
    "actually",
    "anything",
    "likewise",
    "top",
    "row",
    "attempts",
    "move",
    "actually",
    "anything",
    "gets",
    "reward",
    "minus",
    "one",
    "wasting",
    "time",
    "want",
    "get",
    "uh",
    "sorry",
    "row",
    "column",
    "set",
    "space",
    "zero",
    "zero",
    "denotes",
    "empty",
    "square",
    "agent",
    "position",
    "new",
    "state",
    "want",
    "get",
    "new",
    "x",
    "new",
    "typo",
    "let",
    "fix",
    "set",
    "position",
    "one",
    "represent",
    "agent",
    "royal",
    "decree",
    "next",
    "thing",
    "know",
    "attempting",
    "move",
    "grid",
    "allowed",
    "agent",
    "stay",
    "grid",
    "let",
    "take",
    "care",
    "want",
    "take",
    "new",
    "old",
    "states",
    "input",
    "first",
    "thing",
    "want",
    "know",
    "attempting",
    "move",
    "grid",
    "world",
    "entirely",
    "ah",
    "hate",
    "editors",
    "uh",
    "new",
    "state",
    "new",
    "state",
    "space",
    "plus",
    "attempting",
    "move",
    "grid",
    "return",
    "true",
    "otherwise",
    "old",
    "state",
    "modulus",
    "equals",
    "zero",
    "new",
    "state",
    "modulus",
    "um",
    "equals",
    "minus",
    "one",
    "return",
    "true",
    "brevity",
    "could",
    "explain",
    "video",
    "running",
    "long",
    "already",
    "brevity",
    "reason",
    "true",
    "left",
    "exercise",
    "reader",
    "bet",
    "know",
    "going",
    "like",
    "college",
    "course",
    "uh",
    "basically",
    "trying",
    "give",
    "hint",
    "trying",
    "determine",
    "trying",
    "move",
    "grid",
    "either",
    "left",
    "right",
    "want",
    "wrap",
    "around",
    "instance",
    "nine",
    "nine",
    "grid",
    "goes",
    "zero",
    "eight",
    "add",
    "one",
    "right",
    "would",
    "get",
    "nine",
    "would",
    "teleport",
    "row",
    "uh",
    "zeroth",
    "column",
    "want",
    "want",
    "waste",
    "spa",
    "ways",
    "move",
    "receive",
    "reward",
    "minus",
    "one",
    "um",
    "old",
    "state",
    "modulus",
    "good",
    "grief",
    "neither",
    "true",
    "go",
    "ahead",
    "return",
    "false",
    "meaning",
    "trying",
    "move",
    "grid",
    "let",
    "see",
    "see",
    "next",
    "function",
    "need",
    "way",
    "actually",
    "step",
    "let",
    "go",
    "ahead",
    "let",
    "say",
    "thing",
    "need",
    "take",
    "action",
    "first",
    "thing",
    "want",
    "get",
    "x",
    "going",
    "check",
    "make",
    "sure",
    "legal",
    "move",
    "resulting",
    "state",
    "agent",
    "position",
    "plus",
    "mapping",
    "agent",
    "position",
    "whatever",
    "recall",
    "action",
    "space",
    "dictionary",
    "maps",
    "actions",
    "translations",
    "grid",
    "saying",
    "new",
    "state",
    "equal",
    "current",
    "state",
    "plus",
    "whatever",
    "resulting",
    "translation",
    "whatever",
    "action",
    "attempting",
    "make",
    "next",
    "thing",
    "need",
    "know",
    "magic",
    "square",
    "um",
    "um",
    "agent",
    "teleports",
    "new",
    "position",
    "okay",
    "next",
    "need",
    "handle",
    "reward",
    "minus",
    "one",
    "um",
    "terminal",
    "state",
    "minus",
    "one",
    "transitioned",
    "terminal",
    "state",
    "otherwise",
    "zero",
    "trying",
    "move",
    "grid",
    "go",
    "ahead",
    "set",
    "state",
    "state",
    "resulting",
    "state",
    "ready",
    "go",
    "ahead",
    "return",
    "openai",
    "gym",
    "uh",
    "whenever",
    "take",
    "step",
    "returns",
    "new",
    "state",
    "reward",
    "um",
    "whether",
    "game",
    "debug",
    "information",
    "gon",
    "na",
    "thing",
    "resulting",
    "state",
    "reward",
    "whether",
    "terminal",
    "state",
    "debug",
    "info",
    "going",
    "none",
    "attempting",
    "move",
    "grid",
    "want",
    "nothing",
    "want",
    "return",
    "agent",
    "position",
    "um",
    "reward",
    "whether",
    "terminal",
    "null",
    "debug",
    "info",
    "almost",
    "next",
    "thing",
    "need",
    "know",
    "reset",
    "grid",
    "end",
    "every",
    "episode",
    "reset",
    "right",
    "first",
    "thing",
    "set",
    "agent",
    "position",
    "zero",
    "reset",
    "grid",
    "zeros",
    "go",
    "ahead",
    "add",
    "magic",
    "squares",
    "back",
    "return",
    "agent",
    "position",
    "course",
    "zero",
    "oh",
    "wow",
    "real",
    "close",
    "right",
    "next",
    "one",
    "last",
    "function",
    "swear",
    "one",
    "promise",
    "right",
    "would",
    "lie",
    "right",
    "next",
    "want",
    "provide",
    "way",
    "rendering",
    "hey",
    "helpful",
    "debug",
    "like",
    "print",
    "whole",
    "big",
    "string",
    "know",
    "dashes",
    "party",
    "music",
    "want",
    "iterate",
    "grid",
    "column",
    "row",
    "column",
    "equals",
    "zero",
    "words",
    "empty",
    "square",
    "going",
    "print",
    "dash",
    "going",
    "end",
    "tab",
    "column",
    "1",
    "meaning",
    "agent",
    "going",
    "print",
    "x",
    "denote",
    "agent",
    "column",
    "2",
    "one",
    "entrances",
    "magic",
    "squares",
    "print",
    "tab",
    "delimiter",
    "tab",
    "end",
    "column",
    "equals",
    "three",
    "print",
    "equals",
    "tab",
    "um",
    "column",
    "equals",
    "four",
    "know",
    "magic",
    "square",
    "entrance",
    "finally",
    "five",
    "know",
    "magic",
    "squares",
    "exit",
    "row",
    "want",
    "print",
    "new",
    "line",
    "end",
    "go",
    "ahead",
    "print",
    "another",
    "chunk",
    "pretty",
    "dashes",
    "oh",
    "ah",
    "yeah",
    "okay",
    "agent",
    "class",
    "took",
    "long",
    "20",
    "minutes",
    "wow",
    "okay",
    "hope",
    "still",
    "basic",
    "idea",
    "make",
    "environment",
    "need",
    "initialize",
    "reset",
    "state",
    "space",
    "state",
    "space",
    "plus",
    "way",
    "denote",
    "possible",
    "actions",
    "way",
    "make",
    "sure",
    "move",
    "legal",
    "way",
    "actually",
    "affect",
    "environment",
    "step",
    "function",
    "needs",
    "return",
    "new",
    "position",
    "reward",
    "whether",
    "state",
    "new",
    "state",
    "terminal",
    "well",
    "debug",
    "information",
    "also",
    "need",
    "way",
    "resetting",
    "printing",
    "environment",
    "terminal",
    "part",
    "two",
    "actually",
    "going",
    "fire",
    "baby",
    "q",
    "learning",
    "algorithm",
    "see",
    "actually",
    "quite",
    "exciting",
    "well",
    "moderately",
    "exciting",
    "anyway",
    "actually",
    "learns",
    "quite",
    "well",
    "find",
    "magic",
    "square",
    "spoiler",
    "alert",
    "made",
    "far",
    "finds",
    "magic",
    "square",
    "gets",
    "minimum",
    "number",
    "moves",
    "required",
    "um",
    "pretty",
    "cool",
    "see",
    "come",
    "next",
    "video",
    "vedna",
    "day",
    "hope",
    "see",
    "like",
    "video",
    "make",
    "sure",
    "leave",
    "thumbs",
    "subscribe",
    "already",
    "reinforcement",
    "learning",
    "content",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "everybody",
    "new",
    "tutorial",
    "host",
    "phil",
    "taber",
    "new",
    "channel",
    "physicist",
    "former",
    "semiconductor",
    "process",
    "engineer",
    "turned",
    "machine",
    "learning",
    "practitioner",
    "subscribed",
    "yet",
    "go",
    "ahead",
    "hit",
    "subscribe",
    "button",
    "miss",
    "future",
    "reinforcement",
    "learning",
    "tutorials",
    "left",
    "previous",
    "video",
    "finished",
    "bulk",
    "open",
    "ai",
    "gym",
    "compliant",
    "reinforcement",
    "learning",
    "environment",
    "today",
    "going",
    "go",
    "ahead",
    "code",
    "q",
    "learning",
    "agent",
    "main",
    "loop",
    "program",
    "see",
    "performs",
    "let",
    "go",
    "ahead",
    "get",
    "started",
    "uh",
    "first",
    "thing",
    "going",
    "need",
    "um",
    "magic",
    "squares",
    "right",
    "recall",
    "magic",
    "squares",
    "teleporters",
    "grid",
    "world",
    "either",
    "advance",
    "agent",
    "forward",
    "backward",
    "first",
    "one",
    "going",
    "position",
    "18",
    "dump",
    "position",
    "54",
    "move",
    "forward",
    "next",
    "one",
    "let",
    "say",
    "63",
    "dump",
    "position",
    "teleporter",
    "advance",
    "agent",
    "grid",
    "world",
    "teleporter",
    "b",
    "send",
    "back",
    "earlier",
    "square",
    "need",
    "create",
    "grid",
    "world",
    "use",
    "nine",
    "nine",
    "grid",
    "pass",
    "magic",
    "squares",
    "created",
    "next",
    "worry",
    "model",
    "hyper",
    "parameters",
    "familiar",
    "let",
    "give",
    "quick",
    "rundown",
    "parameters",
    "control",
    "fast",
    "agent",
    "learns",
    "much",
    "chooses",
    "value",
    "potential",
    "future",
    "rewards",
    "first",
    "parameter",
    "alpha",
    "learning",
    "rate",
    "gamma",
    "tells",
    "us",
    "agent",
    "going",
    "totally",
    "farsighted",
    "count",
    "future",
    "rewards",
    "equally",
    "epsilon",
    "course",
    "epsilon",
    "epsilon",
    "greedy",
    "action",
    "selection",
    "start",
    "behaving",
    "pretty",
    "much",
    "randomly",
    "eventually",
    "converge",
    "purely",
    "greedy",
    "strategy",
    "q",
    "learning",
    "tabular",
    "method",
    "table",
    "state",
    "action",
    "pairs",
    "want",
    "find",
    "value",
    "state",
    "action",
    "pairs",
    "construct",
    "iterate",
    "set",
    "states",
    "actions",
    "state",
    "space",
    "plus",
    "emv",
    "dot",
    "possible",
    "actions",
    "pick",
    "something",
    "initial",
    "value",
    "really",
    "arbitrary",
    "cool",
    "thing",
    "picking",
    "zero",
    "using",
    "something",
    "called",
    "optimistic",
    "initial",
    "values",
    "means",
    "since",
    "agent",
    "takes",
    "receives",
    "reward",
    "minus",
    "one",
    "every",
    "step",
    "never",
    "reward",
    "zero",
    "right",
    "distance",
    "agent",
    "exit",
    "setting",
    "initial",
    "estimate",
    "zero",
    "actually",
    "encourage",
    "exploration",
    "unexplored",
    "states",
    "agent",
    "takes",
    "move",
    "realizes",
    "oh",
    "get",
    "reward",
    "significantly",
    "worse",
    "let",
    "try",
    "unexplored",
    "option",
    "time",
    "gradually",
    "explore",
    "available",
    "actions",
    "given",
    "state",
    "disappointed",
    "stuff",
    "previously",
    "tried",
    "fun",
    "little",
    "fact",
    "want",
    "play",
    "50",
    "000",
    "games",
    "need",
    "way",
    "keeping",
    "track",
    "rewards",
    "numpy",
    "array",
    "fine",
    "yes",
    "let",
    "iterate",
    "total",
    "number",
    "games",
    "um",
    "thief",
    "like",
    "print",
    "marker",
    "terminal",
    "way",
    "know",
    "actually",
    "working",
    "every",
    "five",
    "thousand",
    "games",
    "print",
    "starting",
    "ith",
    "game",
    "top",
    "every",
    "episode",
    "want",
    "reset",
    "done",
    "flag",
    "want",
    "reset",
    "episode",
    "rewards",
    "accumulate",
    "rewards",
    "episode",
    "episode",
    "course",
    "want",
    "reset",
    "environment",
    "oh",
    "let",
    "scroll",
    "go",
    "want",
    "reset",
    "environment",
    "would",
    "open",
    "ai",
    "gym",
    "type",
    "problem",
    "next",
    "begin",
    "episode",
    "done",
    "want",
    "take",
    "random",
    "number",
    "epsilon",
    "greedy",
    "action",
    "selection",
    "going",
    "make",
    "use",
    "max",
    "action",
    "define",
    "q",
    "observation",
    "actions",
    "uh",
    "going",
    "write",
    "momentarily",
    "going",
    "going",
    "find",
    "maximum",
    "action",
    "given",
    "state",
    "random",
    "number",
    "less",
    "one",
    "minus",
    "epsilon",
    "want",
    "guys",
    "see",
    "yep",
    "otherwise",
    "want",
    "take",
    "random",
    "sample",
    "action",
    "space",
    "write",
    "two",
    "functions",
    "let",
    "quite",
    "quickly",
    "action",
    "space",
    "sample",
    "pretty",
    "straightforward",
    "return",
    "random",
    "choice",
    "list",
    "possible",
    "actions",
    "reason",
    "chose",
    "list",
    "data",
    "structure",
    "make",
    "easy",
    "next",
    "max",
    "action",
    "max",
    "yeah",
    "max",
    "action",
    "function",
    "need",
    "belong",
    "class",
    "takes",
    "q",
    "state",
    "set",
    "possible",
    "actions",
    "want",
    "take",
    "numpy",
    "array",
    "estimates",
    "agent",
    "sorry",
    "agent",
    "estimate",
    "present",
    "value",
    "expected",
    "future",
    "rewards",
    "stated",
    "stand",
    "possible",
    "actions",
    "actions",
    "want",
    "find",
    "maximum",
    "index",
    "want",
    "oop",
    "sorry",
    "index",
    "want",
    "return",
    "action",
    "actually",
    "corresponds",
    "right",
    "well",
    "good",
    "need",
    "space",
    "let",
    "little",
    "bit",
    "go",
    "next",
    "want",
    "actually",
    "take",
    "action",
    "get",
    "new",
    "state",
    "observation",
    "underscore",
    "reward",
    "done",
    "info",
    "action",
    "next",
    "uh",
    "calculate",
    "maximal",
    "action",
    "new",
    "state",
    "insert",
    "update",
    "equation",
    "q",
    "function",
    "let",
    "worried",
    "epsilon",
    "greedy",
    "actually",
    "taking",
    "action",
    "next",
    "update",
    "rq",
    "function",
    "current",
    "action",
    "state",
    "alpha",
    "comes",
    "ward",
    "plus",
    "make",
    "sure",
    "visible",
    "reward",
    "plus",
    "quantity",
    "gamma",
    "discount",
    "factor",
    "times",
    "q",
    "observation",
    "underscore",
    "action",
    "underscore",
    "new",
    "state",
    "action",
    "minus",
    "q",
    "observation",
    "action",
    "let",
    "tab",
    "go",
    "nice",
    "compliant",
    "pep",
    "style",
    "guides",
    "right",
    "mostly",
    "okay",
    "update",
    "equation",
    "q",
    "function",
    "update",
    "agent",
    "estimate",
    "value",
    "current",
    "state",
    "action",
    "pair",
    "next",
    "need",
    "let",
    "agent",
    "know",
    "uh",
    "environment",
    "changed",
    "states",
    "set",
    "observation",
    "observation",
    "underscore",
    "q",
    "learning",
    "nutshell",
    "folks",
    "really",
    "straightforward",
    "end",
    "episode",
    "want",
    "decrease",
    "epsilon",
    "way",
    "agent",
    "eventually",
    "settles",
    "purely",
    "greedy",
    "strategy",
    "number",
    "ways",
    "know",
    "square",
    "root",
    "function",
    "log",
    "function",
    "going",
    "linearly",
    "critical",
    "something",
    "like",
    "going",
    "decrease",
    "2",
    "divided",
    "num",
    "games",
    "every",
    "every",
    "game",
    "halfway",
    "purely",
    "greedy",
    "end",
    "every",
    "episode",
    "want",
    "make",
    "sure",
    "keeping",
    "track",
    "total",
    "rewards",
    "something",
    "forgot",
    "yeah",
    "one",
    "thing",
    "forget",
    "keep",
    "track",
    "total",
    "reward",
    "episode",
    "forget",
    "important",
    "end",
    "episodes",
    "want",
    "plot",
    "total",
    "rewards",
    "coding",
    "portion",
    "oh",
    "one",
    "thing",
    "take",
    "back",
    "let",
    "scroll",
    "want",
    "show",
    "environment",
    "let",
    "env",
    "dot",
    "render",
    "purpose",
    "see",
    "many",
    "moves",
    "takes",
    "agent",
    "escape",
    "grid",
    "world",
    "tell",
    "us",
    "good",
    "right",
    "minimum",
    "minimum",
    "number",
    "moves",
    "takes",
    "escape",
    "going",
    "fire",
    "terminal",
    "go",
    "ahead",
    "get",
    "started",
    "one",
    "second",
    "terminal",
    "let",
    "go",
    "ahead",
    "run",
    "see",
    "start",
    "top",
    "left",
    "takes",
    "one",
    "two",
    "moving",
    "free",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "10",
    "sorry",
    "11",
    "12th",
    "move",
    "free",
    "exit",
    "maze",
    "sorry",
    "grid",
    "world",
    "total",
    "reward",
    "minus",
    "11",
    "best",
    "agent",
    "possibly",
    "gone",
    "ahead",
    "plotted",
    "let",
    "check",
    "plot",
    "see",
    "indeed",
    "agent",
    "starts",
    "rather",
    "poorly",
    "exploring",
    "finding",
    "routes",
    "grid",
    "world",
    "eventually",
    "halfway",
    "um",
    "2500",
    "sorry",
    "25",
    "000",
    "see",
    "settles",
    "least",
    "constant",
    "value",
    "let",
    "prove",
    "maximum",
    "value",
    "minus",
    "11",
    "see",
    "close",
    "enough",
    "government",
    "work",
    "see",
    "agent",
    "able",
    "actually",
    "solve",
    "maze",
    "sorry",
    "grid",
    "world",
    "keep",
    "calling",
    "amaze",
    "able",
    "solve",
    "grid",
    "world",
    "using",
    "q",
    "learning",
    "algorithm",
    "surprising",
    "know",
    "would",
    "expect",
    "uh",
    "novel",
    "created",
    "reinforcement",
    "learning",
    "environment",
    "uses",
    "similar",
    "format",
    "open",
    "ai",
    "gym",
    "anytime",
    "want",
    "create",
    "new",
    "environment",
    "use",
    "fire",
    "video",
    "use",
    "know",
    "set",
    "code",
    "template",
    "projects",
    "going",
    "put",
    "github",
    "link",
    "also",
    "going",
    "write",
    "tutorial",
    "uh",
    "text",
    "form",
    "upload",
    "know",
    "done",
    "tonight",
    "update",
    "description",
    "know",
    "consume",
    "text",
    "easily",
    "video",
    "go",
    "ahead",
    "check",
    "hope",
    "helpful",
    "uh",
    "make",
    "sure",
    "leave",
    "comment",
    "subscribe",
    "already",
    "hope",
    "see",
    "next",
    "video",
    "welcome",
    "back",
    "data",
    "manglers",
    "thanks",
    "tuning",
    "another",
    "episode",
    "new",
    "channel",
    "phil",
    "tabor",
    "physicist",
    "former",
    "semiconductor",
    "engineer",
    "turned",
    "machine",
    "learning",
    "practitioner",
    "mission",
    "teach",
    "next",
    "generation",
    "data",
    "engineers",
    "stay",
    "one",
    "step",
    "ahead",
    "robot",
    "overlords",
    "subscribed",
    "sure",
    "miss",
    "future",
    "reinforcement",
    "learning",
    "content",
    "touched",
    "reinforcement",
    "learning",
    "many",
    "times",
    "channel",
    "represents",
    "best",
    "chance",
    "developing",
    "something",
    "approximating",
    "artificial",
    "general",
    "intelligence",
    "covered",
    "everything",
    "monte",
    "carlo",
    "methods",
    "deep",
    "q",
    "policy",
    "gradient",
    "methods",
    "using",
    "pi",
    "torch",
    "tensorflow",
    "frameworks",
    "discussed",
    "channel",
    "reinforcement",
    "learning",
    "oversight",
    "ends",
    "today",
    "right",
    "okay",
    "maybe",
    "seconds",
    "either",
    "way",
    "going",
    "cover",
    "essentials",
    "reinforcement",
    "learning",
    "first",
    "let",
    "take",
    "quick",
    "step",
    "back",
    "probably",
    "familiar",
    "supervised",
    "learning",
    "successfully",
    "applied",
    "fields",
    "like",
    "computer",
    "vision",
    "linear",
    "regression",
    "need",
    "mountains",
    "data",
    "classified",
    "hand",
    "train",
    "neural",
    "network",
    "proven",
    "quite",
    "effective",
    "pretty",
    "significant",
    "limitations",
    "get",
    "data",
    "label",
    "barriers",
    "put",
    "many",
    "interesting",
    "problems",
    "realm",
    "mega",
    "corporations",
    "us",
    "individual",
    "practitioners",
    "good",
    "top",
    "really",
    "intelligence",
    "see",
    "thousands",
    "examples",
    "thing",
    "understand",
    "thing",
    "us",
    "learn",
    "actively",
    "sure",
    "shortcut",
    "process",
    "reading",
    "books",
    "watching",
    "youtube",
    "videos",
    "ultimately",
    "get",
    "hands",
    "dirty",
    "learn",
    "abstract",
    "important",
    "concepts",
    "see",
    "essential",
    "stuff",
    "environment",
    "facilitates",
    "learning",
    "actions",
    "affect",
    "environment",
    "thing",
    "learning",
    "agent",
    "jacket",
    "labels",
    "required",
    "enter",
    "reinforcement",
    "learning",
    "attempt",
    "take",
    "ingredients",
    "incorporate",
    "artificial",
    "intelligence",
    "environment",
    "anything",
    "environments",
    "like",
    "card",
    "games",
    "classic",
    "atari",
    "games",
    "real",
    "real",
    "world",
    "least",
    "afraid",
    "skynet",
    "starting",
    "nuclear",
    "war",
    "ai",
    "interacts",
    "environment",
    "set",
    "actions",
    "usually",
    "discrete",
    "move",
    "direction",
    "fire",
    "enemy",
    "instance",
    "actions",
    "turn",
    "cause",
    "observable",
    "change",
    "environment",
    "meaning",
    "environment",
    "transitions",
    "one",
    "state",
    "another",
    "example",
    "space",
    "invaders",
    "environment",
    "open",
    "ai",
    "gym",
    "attempting",
    "move",
    "left",
    "caused",
    "agent",
    "move",
    "left",
    "100",
    "probability",
    "need",
    "case",
    "though",
    "frozen",
    "lake",
    "environment",
    "attempting",
    "move",
    "left",
    "result",
    "agent",
    "moving",
    "right",
    "even",
    "keep",
    "mind",
    "state",
    "transitions",
    "probabilistic",
    "probabilities",
    "one",
    "hundred",
    "percent",
    "merely",
    "sum",
    "important",
    "part",
    "environment",
    "reward",
    "penalty",
    "agent",
    "receives",
    "take",
    "one",
    "thing",
    "away",
    "video",
    "design",
    "reward",
    "critical",
    "component",
    "creating",
    "effective",
    "reinforcement",
    "learning",
    "systems",
    "reinforcement",
    "learning",
    "algorithms",
    "seek",
    "maximize",
    "reward",
    "agent",
    "nothing",
    "nothing",
    "less",
    "fact",
    "real",
    "danger",
    "ai",
    "would",
    "malicious",
    "would",
    "ruthlessly",
    "rational",
    "classic",
    "example",
    "case",
    "artificial",
    "general",
    "intelligence",
    "whose",
    "reward",
    "centered",
    "around",
    "many",
    "paper",
    "clips",
    "churns",
    "sounds",
    "innocent",
    "right",
    "well",
    "paper",
    "clip",
    "making",
    "bot",
    "figure",
    "humans",
    "consume",
    "bunch",
    "resources",
    "need",
    "make",
    "paper",
    "clips",
    "pesky",
    "humans",
    "way",
    "orderly",
    "planetary",
    "scale",
    "office",
    "problematic",
    "involved",
    "means",
    "must",
    "think",
    "long",
    "hard",
    "want",
    "reward",
    "agent",
    "even",
    "introduce",
    "penalties",
    "undertaking",
    "actions",
    "endanger",
    "human",
    "safety",
    "least",
    "systems",
    "see",
    "action",
    "real",
    "world",
    "perhaps",
    "less",
    "dramatic",
    "although",
    "less",
    "important",
    "implications",
    "introducing",
    "inefficiencies",
    "agent",
    "consider",
    "game",
    "chess",
    "might",
    "tempted",
    "give",
    "agent",
    "penalty",
    "losing",
    "pieces",
    "would",
    "potentially",
    "prevent",
    "agent",
    "discovering",
    "gambits",
    "sacrifices",
    "piece",
    "longer",
    "term",
    "positional",
    "advantage",
    "alpha",
    "zero",
    "engine",
    "chess",
    "playing",
    "artificial",
    "intelligence",
    "notorious",
    "sacrifice",
    "multiple",
    "pawns",
    "yet",
    "still",
    "dominate",
    "best",
    "traditional",
    "chess",
    "engines",
    "offer",
    "music",
    "reward",
    "actions",
    "environment",
    "agent",
    "agent",
    "part",
    "software",
    "keeps",
    "track",
    "state",
    "transitions",
    "actions",
    "rewards",
    "looks",
    "patterns",
    "maximize",
    "total",
    "reward",
    "time",
    "algorithm",
    "dictates",
    "agent",
    "act",
    "given",
    "situation",
    "state",
    "environment",
    "called",
    "policy",
    "expressed",
    "probability",
    "choosing",
    "action",
    "given",
    "environment",
    "state",
    "please",
    "note",
    "probabilities",
    "state",
    "transition",
    "probabilities",
    "mathematical",
    "relationship",
    "state",
    "transitions",
    "rewards",
    "policy",
    "known",
    "bellman",
    "equation",
    "tells",
    "us",
    "value",
    "meaning",
    "expected",
    "future",
    "reward",
    "policy",
    "state",
    "environment",
    "reinforcement",
    "learning",
    "often",
    "though",
    "always",
    "means",
    "maximizing",
    "solving",
    "bellman",
    "equation",
    "future",
    "videos",
    "desire",
    "maximize",
    "reward",
    "leads",
    "dilemma",
    "agent",
    "maximize",
    "reward",
    "exploiting",
    "action",
    "adventurous",
    "choose",
    "actions",
    "whose",
    "reward",
    "appears",
    "smaller",
    "maybe",
    "even",
    "unknown",
    "known",
    "explore",
    "exploit",
    "dilemma",
    "one",
    "popular",
    "solution",
    "choose",
    "best",
    "known",
    "action",
    "time",
    "occasionally",
    "choose",
    "action",
    "see",
    "something",
    "better",
    "called",
    "epsilon",
    "greedy",
    "policy",
    "think",
    "reinforcement",
    "learning",
    "often",
    "thinking",
    "algorithm",
    "agent",
    "uses",
    "solve",
    "bellman",
    "equation",
    "generally",
    "fall",
    "two",
    "categories",
    "algorithms",
    "require",
    "full",
    "model",
    "environment",
    "algorithms",
    "mean",
    "exactly",
    "model",
    "environment",
    "said",
    "earlier",
    "actions",
    "cause",
    "environment",
    "transition",
    "one",
    "state",
    "another",
    "probability",
    "full",
    "model",
    "environment",
    "means",
    "knowing",
    "state",
    "transition",
    "probabilities",
    "certainty",
    "course",
    "quite",
    "rare",
    "know",
    "beforehand",
    "algorithms",
    "require",
    "full",
    "model",
    "somewhat",
    "limited",
    "utility",
    "class",
    "algorithms",
    "known",
    "dynamic",
    "programming",
    "model",
    "model",
    "environment",
    "incomplete",
    "ca",
    "use",
    "dynamic",
    "programming",
    "instead",
    "rely",
    "family",
    "algorithms",
    "one",
    "popular",
    "algorithm",
    "q",
    "learning",
    "deep",
    "q",
    "learning",
    "studied",
    "channel",
    "rely",
    "keeping",
    "track",
    "state",
    "transitions",
    "actions",
    "rewards",
    "learn",
    "model",
    "environment",
    "time",
    "case",
    "parameters",
    "saved",
    "table",
    "case",
    "deep",
    "q",
    "learning",
    "relationships",
    "expressed",
    "approximate",
    "functional",
    "relationship",
    "learned",
    "deep",
    "neural",
    "network",
    "really",
    "least",
    "high",
    "level",
    "recap",
    "reinforcement",
    "learning",
    "class",
    "machine",
    "learning",
    "algorithms",
    "help",
    "autonomous",
    "agent",
    "navigate",
    "complex",
    "environment",
    "agent",
    "must",
    "given",
    "sequence",
    "rewards",
    "penalties",
    "learn",
    "required",
    "agent",
    "attempts",
    "maximize",
    "reward",
    "time",
    "mathematical",
    "terms",
    "solve",
    "bellman",
    "equation",
    "algorithms",
    "help",
    "agent",
    "estimate",
    "future",
    "rewards",
    "fall",
    "two",
    "classes",
    "require",
    "know",
    "state",
    "transition",
    "probabilities",
    "environment",
    "beforehand",
    "since",
    "knowing",
    "probabilities",
    "rare",
    "luxury",
    "often",
    "rely",
    "algorithms",
    "like",
    "deep",
    "queue",
    "learning",
    "like",
    "know",
    "please",
    "check",
    "videos",
    "channel",
    "hope",
    "helpful",
    "please",
    "leave",
    "comment",
    "like",
    "subscribe",
    "already",
    "look",
    "forward",
    "seeing",
    "next",
    "video",
    "welcome",
    "back",
    "free",
    "reinforcement",
    "learning",
    "course",
    "host",
    "phil",
    "tabor",
    "subscribed",
    "sure",
    "hit",
    "bell",
    "icon",
    "get",
    "notified",
    "new",
    "module",
    "course",
    "module",
    "1",
    "covered",
    "essential",
    "concepts",
    "reinforcement",
    "learning",
    "seen",
    "go",
    "ahead",
    "check",
    "module",
    "makes",
    "sense",
    "seen",
    "may",
    "remember",
    "reinforcement",
    "learning",
    "basically",
    "boils",
    "agent",
    "interacting",
    "environment",
    "receiving",
    "rewards",
    "process",
    "rewards",
    "tell",
    "agent",
    "good",
    "bad",
    "agent",
    "uses",
    "algorithm",
    "try",
    "maximize",
    "rewards",
    "time",
    "practice",
    "get",
    "sequence",
    "decisions",
    "agent",
    "decision",
    "influence",
    "immediate",
    "reward",
    "rather",
    "decision",
    "influences",
    "future",
    "rewards",
    "mathematical",
    "terms",
    "sequence",
    "states",
    "actions",
    "rewards",
    "one",
    "could",
    "call",
    "decision",
    "process",
    "state",
    "process",
    "purely",
    "function",
    "previous",
    "state",
    "action",
    "agent",
    "process",
    "called",
    "markov",
    "decision",
    "process",
    "mdp",
    "short",
    "idealized",
    "mathematical",
    "abstraction",
    "use",
    "construct",
    "theory",
    "reinforcement",
    "learning",
    "many",
    "problems",
    "assumption",
    "broken",
    "various",
    "degrees",
    "much",
    "really",
    "matters",
    "often",
    "complicated",
    "question",
    "one",
    "going",
    "dodge",
    "regardless",
    "cases",
    "assumption",
    "process",
    "obeys",
    "markov",
    "property",
    "good",
    "enough",
    "use",
    "resulting",
    "mathematics",
    "reinforcement",
    "learning",
    "problems",
    "said",
    "reinforcement",
    "learning",
    "agent",
    "seeks",
    "maximize",
    "rewards",
    "time",
    "fit",
    "markov",
    "decision",
    "process",
    "agent",
    "perspective",
    "receives",
    "sequence",
    "rewards",
    "time",
    "sequence",
    "rewards",
    "used",
    "construct",
    "expected",
    "return",
    "agent",
    "return",
    "time",
    "step",
    "sum",
    "rewards",
    "follow",
    "way",
    "final",
    "time",
    "capital",
    "final",
    "time",
    "step",
    "naturally",
    "introduces",
    "concept",
    "episodes",
    "discrete",
    "periods",
    "gameplay",
    "characterized",
    "state",
    "transitions",
    "actions",
    "rewards",
    "upon",
    "taking",
    "final",
    "time",
    "step",
    "agent",
    "enters",
    "terminal",
    "state",
    "unique",
    "means",
    "matter",
    "end",
    "episode",
    "terminal",
    "state",
    "always",
    "future",
    "rewards",
    "follow",
    "reach",
    "terminal",
    "state",
    "agent",
    "expected",
    "reward",
    "terminal",
    "state",
    "precisely",
    "zero",
    "bit",
    "creativity",
    "call",
    "tasks",
    "broken",
    "episodes",
    "episodic",
    "tasks",
    "course",
    "tasks",
    "episodic",
    "many",
    "fact",
    "continuous",
    "bit",
    "problem",
    "since",
    "final",
    "time",
    "step",
    "infinity",
    "total",
    "reward",
    "could",
    "also",
    "infinite",
    "makes",
    "concept",
    "maximizing",
    "rewards",
    "meaningless",
    "introduce",
    "additional",
    "concept",
    "fix",
    "use",
    "episodic",
    "continuing",
    "tasks",
    "idea",
    "discounting",
    "basically",
    "means",
    "agent",
    "values",
    "future",
    "rewards",
    "less",
    "less",
    "discounting",
    "follows",
    "power",
    "law",
    "time",
    "step",
    "results",
    "discounting",
    "hyperparameter",
    "gamma",
    "called",
    "discount",
    "rate",
    "doubt",
    "seen",
    "videos",
    "reinforcement",
    "learning",
    "use",
    "form",
    "expected",
    "return",
    "simple",
    "factoring",
    "derive",
    "really",
    "useful",
    "fact",
    "recursive",
    "relationship",
    "rewards",
    "subsequent",
    "time",
    "steps",
    "something",
    "exploit",
    "constantly",
    "reinforcement",
    "learning",
    "agent",
    "engaged",
    "discrete",
    "processes",
    "receiving",
    "rewards",
    "trying",
    "maximize",
    "expected",
    "feature",
    "returns",
    "remember",
    "first",
    "lecture",
    "algorithm",
    "determines",
    "agent",
    "going",
    "act",
    "called",
    "policy",
    "since",
    "agent",
    "set",
    "defined",
    "rules",
    "going",
    "act",
    "given",
    "state",
    "use",
    "sequence",
    "states",
    "actions",
    "rewards",
    "figure",
    "value",
    "given",
    "state",
    "value",
    "state",
    "expected",
    "return",
    "starting",
    "state",
    "following",
    "policy",
    "given",
    "formally",
    "following",
    "equation",
    "problems",
    "like",
    "say",
    "q",
    "learning",
    "concerned",
    "maximizing",
    "action",
    "value",
    "function",
    "tells",
    "agent",
    "value",
    "taking",
    "action",
    "given",
    "state",
    "following",
    "policy",
    "thereafter",
    "music",
    "remember",
    "said",
    "exploit",
    "recursive",
    "relationship",
    "subsequent",
    "returns",
    "well",
    "plug",
    "expression",
    "value",
    "function",
    "actually",
    "discover",
    "value",
    "function",
    "defined",
    "recursively",
    "called",
    "bellman",
    "equation",
    "first",
    "module",
    "quantity",
    "many",
    "algorithms",
    "seek",
    "maximize",
    "bellman",
    "equation",
    "really",
    "expectation",
    "value",
    "weighted",
    "average",
    "likely",
    "particular",
    "sequence",
    "states",
    "actions",
    "rewards",
    "given",
    "state",
    "transition",
    "probabilities",
    "probability",
    "agent",
    "selecting",
    "action",
    "much",
    "following",
    "material",
    "involve",
    "coming",
    "various",
    "schemes",
    "solve",
    "bellman",
    "equation",
    "evolve",
    "policy",
    "way",
    "value",
    "function",
    "increases",
    "time",
    "next",
    "module",
    "take",
    "look",
    "explore",
    "exploit",
    "dilemma",
    "expression",
    "long",
    "rewards",
    "hope",
    "helpful",
    "questions",
    "comments",
    "suggestions",
    "leave",
    "read",
    "answer",
    "comments",
    "made",
    "far",
    "consider",
    "subscribing",
    "get",
    "notified",
    "rest",
    "course",
    "drops",
    "look",
    "forward",
    "seeing",
    "next",
    "video",
    "welcome",
    "module",
    "3",
    "free",
    "reinforcement",
    "learning",
    "course",
    "neural",
    "net",
    "dot",
    "ai",
    "host",
    "phil",
    "taper",
    "subscribed",
    "make",
    "sure",
    "miss",
    "rest",
    "course",
    "previous",
    "video",
    "learned",
    "special",
    "type",
    "process",
    "called",
    "markov",
    "decision",
    "process",
    "state",
    "depends",
    "previous",
    "state",
    "action",
    "taken",
    "agent",
    "leads",
    "recursive",
    "relationship",
    "agent",
    "estimate",
    "returns",
    "successive",
    "time",
    "steps",
    "relationship",
    "extends",
    "agent",
    "estimate",
    "value",
    "function",
    "given",
    "bellman",
    "equation",
    "covered",
    "module",
    "1",
    "reinforcement",
    "learning",
    "part",
    "boils",
    "maximizing",
    "value",
    "function",
    "however",
    "always",
    "simple",
    "surprise",
    "surprise",
    "like",
    "real",
    "life",
    "reinforcement",
    "learning",
    "agents",
    "faced",
    "similar",
    "considerations",
    "agent",
    "take",
    "action",
    "knows",
    "immediately",
    "provide",
    "reward",
    "explore",
    "actions",
    "see",
    "better",
    "conundrum",
    "known",
    "explorer",
    "exploit",
    "dilemma",
    "every",
    "reinforcement",
    "learning",
    "algorithm",
    "deal",
    "fortunately",
    "many",
    "solutions",
    "cover",
    "one",
    "solution",
    "idea",
    "optimistic",
    "initial",
    "values",
    "agent",
    "starts",
    "playing",
    "game",
    "use",
    "initial",
    "estimate",
    "value",
    "action",
    "value",
    "function",
    "estimate",
    "totally",
    "arbitrary",
    "know",
    "something",
    "reward",
    "structure",
    "beforehand",
    "actually",
    "initialize",
    "way",
    "encourage",
    "exploration",
    "suppose",
    "environment",
    "like",
    "grid",
    "world",
    "video",
    "creating",
    "reinforcement",
    "learning",
    "environment",
    "environment",
    "agent",
    "receives",
    "reward",
    "minus",
    "one",
    "step",
    "expected",
    "returns",
    "always",
    "negative",
    "zero",
    "matter",
    "state",
    "environment",
    "action",
    "agent",
    "takes",
    "would",
    "happen",
    "tell",
    "agent",
    "value",
    "state",
    "action",
    "pairs",
    "positive",
    "even",
    "zero",
    "first",
    "move",
    "agent",
    "picks",
    "action",
    "randomly",
    "actions",
    "look",
    "identical",
    "receives",
    "reward",
    "updates",
    "estimates",
    "accordingly",
    "bit",
    "disappointed",
    "expecting",
    "chocolate",
    "cake",
    "got",
    "mud",
    "pie",
    "next",
    "time",
    "encounters",
    "state",
    "take",
    "different",
    "action",
    "actions",
    "estimate",
    "zero",
    "reward",
    "state",
    "better",
    "negative",
    "reward",
    "actually",
    "received",
    "means",
    "agent",
    "ends",
    "exploring",
    "state",
    "action",
    "pairs",
    "many",
    "times",
    "update",
    "makes",
    "agent",
    "estimate",
    "accurate",
    "never",
    "explicitly",
    "tell",
    "agent",
    "take",
    "exploratory",
    "actions",
    "greed",
    "drobit",
    "take",
    "exploratory",
    "actions",
    "became",
    "disappointed",
    "whatever",
    "action",
    "took",
    "called",
    "optimistic",
    "initial",
    "values",
    "another",
    "feasible",
    "solution",
    "spend",
    "portion",
    "time",
    "choosing",
    "random",
    "actions",
    "majority",
    "time",
    "choosing",
    "greedy",
    "actions",
    "called",
    "epsilon",
    "greedy",
    "strategy",
    "one",
    "employ",
    "quite",
    "robust",
    "change",
    "random",
    "parameter",
    "time",
    "agent",
    "converges",
    "onto",
    "nearly",
    "pure",
    "greedy",
    "strategy",
    "proportion",
    "time",
    "agent",
    "spends",
    "exploring",
    "hyper",
    "parameter",
    "problem",
    "typically",
    "call",
    "epsilon",
    "one",
    "potential",
    "strategy",
    "start",
    "completely",
    "randomly",
    "use",
    "decay",
    "function",
    "gradually",
    "increase",
    "proportion",
    "greedy",
    "actions",
    "agent",
    "takes",
    "form",
    "function",
    "critically",
    "important",
    "linear",
    "power",
    "law",
    "really",
    "function",
    "whether",
    "agent",
    "converges",
    "purely",
    "greedy",
    "strategy",
    "going",
    "depend",
    "problem",
    "simple",
    "environments",
    "like",
    "grid",
    "world",
    "know",
    "optimal",
    "solution",
    "beforehand",
    "makes",
    "quite",
    "bit",
    "sense",
    "converged",
    "purely",
    "greedy",
    "strategy",
    "however",
    "game",
    "like",
    "space",
    "invaders",
    "popular",
    "environment",
    "open",
    "ai",
    "gym",
    "many",
    "variables",
    "hard",
    "sure",
    "agent",
    "settled",
    "truly",
    "optimal",
    "strategy",
    "solution",
    "leave",
    "epsilon",
    "small",
    "finite",
    "value",
    "agent",
    "occasionally",
    "taking",
    "exploratory",
    "actions",
    "test",
    "understanding",
    "environment",
    "discussion",
    "made",
    "important",
    "assumption",
    "assumed",
    "agent",
    "uses",
    "single",
    "policy",
    "agent",
    "uses",
    "policy",
    "update",
    "estimate",
    "value",
    "function",
    "well",
    "generate",
    "actions",
    "rule",
    "case",
    "fact",
    "agent",
    "leverage",
    "two",
    "policies",
    "use",
    "one",
    "policy",
    "generate",
    "actions",
    "use",
    "data",
    "generates",
    "update",
    "value",
    "function",
    "policy",
    "called",
    "policy",
    "learning",
    "precisely",
    "use",
    "agent",
    "uses",
    "epsilon",
    "greedy",
    "strategy",
    "generate",
    "steps",
    "markov",
    "chain",
    "sequence",
    "state",
    "action",
    "rewards",
    "resulting",
    "states",
    "uses",
    "data",
    "update",
    "estimate",
    "action",
    "value",
    "function",
    "purely",
    "greedy",
    "action",
    "effect",
    "using",
    "epsilon",
    "greedy",
    "strategy",
    "update",
    "estimate",
    "purely",
    "greedy",
    "strategy",
    "needless",
    "say",
    "works",
    "quite",
    "well",
    "something",
    "come",
    "back",
    "later",
    "modules",
    "get",
    "monte",
    "carlo",
    "methods",
    "temporal",
    "difference",
    "learning",
    "reinforcement",
    "learning",
    "agents",
    "seek",
    "maximize",
    "total",
    "reward",
    "face",
    "dilemma",
    "whether",
    "maximize",
    "current",
    "reward",
    "take",
    "exploratory",
    "steps",
    "suboptimal",
    "actions",
    "hope",
    "optimizing",
    "rewards",
    "one",
    "solution",
    "bias",
    "agent",
    "initial",
    "estimates",
    "way",
    "encourages",
    "exploration",
    "settling",
    "purely",
    "greedy",
    "strategy",
    "another",
    "spend",
    "proportion",
    "time",
    "exploring",
    "majority",
    "time",
    "exploiting",
    "best",
    "known",
    "action",
    "finally",
    "agent",
    "leverage",
    "two",
    "policies",
    "one",
    "generate",
    "data",
    "update",
    "estimate",
    "action",
    "value",
    "value",
    "function",
    "next",
    "module",
    "going",
    "get",
    "dynamic",
    "programming",
    "class",
    "model",
    "based",
    "reinforcement",
    "learning",
    "algorithms",
    "make",
    "sure",
    "subscribe",
    "miss",
    "remainder",
    "course",
    "look",
    "forward",
    "seeing",
    "next",
    "video",
    "welcome",
    "back",
    "everybody",
    "machine",
    "learning",
    "phil",
    "host",
    "dr",
    "phil",
    "last",
    "touched",
    "open",
    "ai",
    "gym",
    "teach",
    "cartpole",
    "robot",
    "dance",
    "basically",
    "balance",
    "pole",
    "video",
    "going",
    "take",
    "look",
    "related",
    "algorithm",
    "called",
    "sarsa",
    "related",
    "sense",
    "types",
    "temporal",
    "difference",
    "learning",
    "algorithms",
    "difference",
    "sarsa",
    "policy",
    "method",
    "q",
    "learning",
    "policy",
    "method",
    "hey",
    "appearance",
    "cat",
    "um",
    "know",
    "means",
    "highly",
    "encourage",
    "check",
    "course",
    "reinforcement",
    "learning",
    "motion",
    "manning",
    "publications",
    "go",
    "depth",
    "stuff",
    "course",
    "uh",
    "enough",
    "plugging",
    "let",
    "get",
    "back",
    "cool",
    "thing",
    "sarsa",
    "well",
    "q",
    "learning",
    "model",
    "free",
    "meaning",
    "need",
    "complete",
    "model",
    "environment",
    "actually",
    "get",
    "learning",
    "done",
    "important",
    "many",
    "cases",
    "know",
    "full",
    "model",
    "environment",
    "mean",
    "means",
    "know",
    "state",
    "transition",
    "probabilities",
    "state",
    "take",
    "action",
    "probability",
    "end",
    "state",
    "prime",
    "get",
    "reward",
    "r",
    "probabilities",
    "completely",
    "known",
    "problems",
    "algorithms",
    "handle",
    "uncertainty",
    "critical",
    "applications",
    "another",
    "neat",
    "thing",
    "uh",
    "bootstrapped",
    "method",
    "meaning",
    "uses",
    "estimates",
    "generate",
    "estimates",
    "right",
    "need",
    "know",
    "much",
    "system",
    "get",
    "started",
    "make",
    "wild",
    "ass",
    "guesses",
    "get",
    "moving",
    "let",
    "take",
    "look",
    "algorithm",
    "uh",
    "first",
    "step",
    "initialize",
    "learning",
    "rate",
    "alpha",
    "uh",
    "course",
    "going",
    "control",
    "rate",
    "learning",
    "quickly",
    "make",
    "adjustments",
    "q",
    "function",
    "uh",
    "initialize",
    "q",
    "function",
    "q",
    "function",
    "agent",
    "estimate",
    "discounted",
    "future",
    "rewards",
    "starting",
    "given",
    "state",
    "taking",
    "action",
    "may",
    "assumptions",
    "built",
    "onto",
    "whether",
    "follow",
    "particular",
    "policy",
    "general",
    "gist",
    "need",
    "initialize",
    "state",
    "choose",
    "initial",
    "action",
    "based",
    "state",
    "using",
    "epsilon",
    "greedy",
    "strategy",
    "function",
    "q",
    "loop",
    "episode",
    "taking",
    "action",
    "getting",
    "reward",
    "new",
    "state",
    "prime",
    "choose",
    "action",
    "prime",
    "function",
    "state",
    "prime",
    "using",
    "epsilon",
    "greedy",
    "q",
    "function",
    "go",
    "ahead",
    "update",
    "q",
    "function",
    "according",
    "update",
    "rule",
    "see",
    "screen",
    "go",
    "ahead",
    "store",
    "state",
    "prime",
    "prime",
    "loop",
    "episode",
    "done",
    "course",
    "go",
    "many",
    "details",
    "quick",
    "dirty",
    "bit",
    "teaser",
    "video",
    "get",
    "guys",
    "interested",
    "course",
    "give",
    "useful",
    "information",
    "time",
    "said",
    "let",
    "go",
    "ahead",
    "jump",
    "code",
    "going",
    "typing",
    "screen",
    "showing",
    "relevant",
    "code",
    "go",
    "along",
    "boom",
    "back",
    "code",
    "editor",
    "using",
    "visual",
    "studio",
    "code",
    "um",
    "even",
    "linux",
    "great",
    "editor",
    "using",
    "highly",
    "recommend",
    "adam",
    "little",
    "bit",
    "buggy",
    "course",
    "sublime",
    "nag",
    "ware",
    "go",
    "ahead",
    "give",
    "look",
    "already",
    "need",
    "define",
    "function",
    "take",
    "max",
    "action",
    "takes",
    "inputs",
    "q",
    "function",
    "well",
    "state",
    "converting",
    "um",
    "q",
    "function",
    "array",
    "numpy",
    "array",
    "uh",
    "action",
    "list",
    "finding",
    "arg",
    "max",
    "recall",
    "numpy",
    "arg",
    "max",
    "takes",
    "returns",
    "first",
    "element",
    "max",
    "two",
    "actions",
    "tied",
    "give",
    "first",
    "one",
    "course",
    "cart",
    "poll",
    "example",
    "action",
    "space",
    "moving",
    "left",
    "right",
    "right",
    "remember",
    "cart",
    "slides",
    "along",
    "trying",
    "keep",
    "pole",
    "vertical",
    "course",
    "continuous",
    "space",
    "q",
    "function",
    "discrete",
    "uh",
    "discrete",
    "mathematical",
    "construct",
    "right",
    "states",
    "discrete",
    "numbers",
    "little",
    "trick",
    "discretize",
    "space",
    "look",
    "documentation",
    "cartpole",
    "example",
    "find",
    "limits",
    "variables",
    "use",
    "create",
    "linear",
    "space",
    "based",
    "based",
    "limits",
    "divide",
    "10",
    "different",
    "buckets",
    "right",
    "way",
    "get",
    "go",
    "continuous",
    "representation",
    "discrete",
    "representation",
    "state",
    "space",
    "define",
    "small",
    "helper",
    "function",
    "get",
    "state",
    "based",
    "observation",
    "digitizes",
    "digitizes",
    "linear",
    "spaces",
    "using",
    "observation",
    "pass",
    "open",
    "ai",
    "gym",
    "returns",
    "four",
    "vector",
    "buckets",
    "correspond",
    "value",
    "element",
    "observation",
    "main",
    "program",
    "want",
    "use",
    "small",
    "learning",
    "rate",
    "alpha",
    "gamma",
    "something",
    "like",
    "course",
    "gamma",
    "discount",
    "factor",
    "debatable",
    "whether",
    "need",
    "discounting",
    "general",
    "used",
    "know",
    "know",
    "certain",
    "going",
    "get",
    "reward",
    "future",
    "make",
    "sense",
    "give",
    "100",
    "percent",
    "weight",
    "could",
    "easily",
    "use",
    "state",
    "transition",
    "functions",
    "cardboard",
    "example",
    "deterministic",
    "far",
    "aware",
    "wrong",
    "please",
    "someone",
    "correct",
    "course",
    "epsilon",
    "epsilon",
    "greedy",
    "going",
    "start",
    "see",
    "second",
    "need",
    "construct",
    "set",
    "states",
    "course",
    "corresponds",
    "integer",
    "representations",
    "continuous",
    "space",
    "um",
    "ranges",
    "zero",
    "zero",
    "nine",
    "construct",
    "four",
    "vector",
    "right",
    "zero",
    "zero",
    "zero",
    "one",
    "one",
    "one",
    "et",
    "cetera",
    "et",
    "cetera",
    "et",
    "cetera",
    "initialize",
    "q",
    "function",
    "going",
    "initialize",
    "everything",
    "zero",
    "right",
    "recall",
    "could",
    "initialize",
    "arbitrarily",
    "terminal",
    "states",
    "want",
    "zero",
    "value",
    "terminal",
    "state",
    "zero",
    "two",
    "range",
    "two",
    "two",
    "actions",
    "move",
    "left",
    "move",
    "right",
    "whoops",
    "also",
    "gon",
    "na",
    "run",
    "fifty",
    "thousand",
    "games",
    "slower",
    "computer",
    "might",
    "wan",
    "na",
    "run",
    "fewer",
    "takes",
    "quite",
    "bit",
    "time",
    "run",
    "going",
    "track",
    "total",
    "rewards",
    "go",
    "along",
    "little",
    "helper",
    "line",
    "print",
    "number",
    "games",
    "playing",
    "always",
    "good",
    "know",
    "right",
    "stops",
    "chugging",
    "along",
    "want",
    "know",
    "broken",
    "actually",
    "something",
    "useful",
    "get",
    "initial",
    "observation",
    "resetting",
    "environment",
    "get",
    "state",
    "calculate",
    "random",
    "number",
    "take",
    "maximum",
    "action",
    "random",
    "number",
    "less",
    "one",
    "minus",
    "epsilon",
    "epsilon",
    "starting",
    "one",
    "random",
    "less",
    "zero",
    "otherwise",
    "randomly",
    "sample",
    "action",
    "space",
    "done",
    "flag",
    "defaults",
    "rewards",
    "episode",
    "zero",
    "loop",
    "episode",
    "done",
    "go",
    "ahead",
    "take",
    "action",
    "getting",
    "reward",
    "new",
    "observation",
    "state",
    "prime",
    "going",
    "get",
    "state",
    "observation",
    "right",
    "observation",
    "four",
    "vector",
    "continuous",
    "numbers",
    "transform",
    "set",
    "discrete",
    "integers",
    "four",
    "vector",
    "discrete",
    "integers",
    "go",
    "ahead",
    "calculate",
    "another",
    "random",
    "number",
    "choose",
    "another",
    "action",
    "based",
    "upon",
    "calculate",
    "sum",
    "total",
    "rewards",
    "update",
    "q",
    "function",
    "based",
    "update",
    "rule",
    "gave",
    "slides",
    "course",
    "set",
    "state",
    "action",
    "new",
    "prime",
    "prime",
    "episode",
    "going",
    "decrease",
    "epsilon",
    "want",
    "want",
    "epsilon",
    "permanently",
    "one",
    "right",
    "want",
    "encourage",
    "amount",
    "exploration",
    "amount",
    "exploitation",
    "epsilon",
    "function",
    "time",
    "save",
    "total",
    "rewards",
    "done",
    "going",
    "go",
    "ahead",
    "plot",
    "see",
    "something",
    "similar",
    "following",
    "going",
    "go",
    "ahead",
    "run",
    "going",
    "take",
    "minute",
    "run",
    "output",
    "source",
    "algorithm",
    "running",
    "50",
    "000",
    "iterations",
    "see",
    "first",
    "messy",
    "plot",
    "expected",
    "50",
    "000",
    "games",
    "plotting",
    "every",
    "single",
    "point",
    "notice",
    "immediately",
    "general",
    "trend",
    "upward",
    "epsilon",
    "reaches",
    "minimum",
    "epsilon",
    "goes",
    "zero",
    "fully",
    "exploitative",
    "strategy",
    "algorithm",
    "actually",
    "really",
    "good",
    "job",
    "hitting",
    "200",
    "moves",
    "time",
    "recall",
    "200",
    "moves",
    "um",
    "200",
    "moves",
    "maximum",
    "number",
    "steps",
    "cart",
    "pull",
    "problem",
    "uh",
    "good",
    "algorithms",
    "get",
    "balance",
    "uh",
    "pretty",
    "much",
    "indefinitely",
    "would",
    "never",
    "terminate",
    "open",
    "ai",
    "gym",
    "terminates",
    "200",
    "steps",
    "anything",
    "close",
    "pretty",
    "good",
    "one",
    "thing",
    "interesting",
    "fair",
    "amount",
    "variability",
    "actually",
    "balance",
    "200",
    "moves",
    "entire",
    "time",
    "number",
    "reasons",
    "perhaps",
    "speculate",
    "invite",
    "speculate",
    "thought",
    "process",
    "way",
    "discretized",
    "space",
    "sufficient",
    "characterize",
    "problem",
    "way",
    "algorithm",
    "learn",
    "something",
    "completely",
    "totally",
    "useful",
    "enough",
    "information",
    "based",
    "ten",
    "thousand",
    "ten",
    "four",
    "yeah",
    "ten",
    "thousand",
    "uh",
    "states",
    "discretized",
    "uh",
    "could",
    "things",
    "matter",
    "know",
    "uh",
    "could",
    "features",
    "instance",
    "combinations",
    "velocities",
    "positions",
    "matter",
    "could",
    "engineered",
    "problem",
    "slightly",
    "quick",
    "little",
    "chunk",
    "170",
    "lines",
    "code",
    "actually",
    "quite",
    "good",
    "uh",
    "questions",
    "sure",
    "leave",
    "hey",
    "made",
    "far",
    "subscribed",
    "please",
    "consider",
    "going",
    "releasing",
    "content",
    "like",
    "full",
    "time",
    "um",
    "look",
    "forward",
    "seeing",
    "next",
    "video",
    "oh",
    "way",
    "next",
    "video",
    "going",
    "taking",
    "look",
    "double",
    "q",
    "learning",
    "uh",
    "yet",
    "another",
    "variation",
    "uh",
    "model",
    "free",
    "bootstrap",
    "methods",
    "see",
    "oh",
    "one",
    "thing",
    "want",
    "code",
    "leave",
    "code",
    "leave",
    "link",
    "github",
    "code",
    "course",
    "reinforcement",
    "learning",
    "motion",
    "showcasing",
    "show",
    "going",
    "learn",
    "course",
    "go",
    "ahead",
    "click",
    "link",
    "description",
    "take",
    "github",
    "find",
    "code",
    "well",
    "code",
    "course",
    "hope",
    "like",
    "see",
    "guys",
    "next",
    "video",
    "welcome",
    "back",
    "everybody",
    "machine",
    "learning",
    "phil",
    "host",
    "dr",
    "phil",
    "yesterday",
    "video",
    "took",
    "look",
    "sarsa",
    "open",
    "ai",
    "gym",
    "getting",
    "cart",
    "pole",
    "balance",
    "promised",
    "today",
    "looking",
    "algorithm",
    "double",
    "queue",
    "learning",
    "also",
    "cartpole",
    "openaigm",
    "environment",
    "touched",
    "many",
    "many",
    "months",
    "ago",
    "basic",
    "idea",
    "bootstrapped",
    "learning",
    "algorithm",
    "means",
    "need",
    "know",
    "need",
    "complete",
    "state",
    "transition",
    "dynamics",
    "environment",
    "function",
    "learns",
    "game",
    "playing",
    "bootstrapped",
    "need",
    "many",
    "much",
    "help",
    "getting",
    "started",
    "generates",
    "estimates",
    "using",
    "initial",
    "estimates",
    "totally",
    "arbitrary",
    "except",
    "terminal",
    "states",
    "policy",
    "meaning",
    "using",
    "separate",
    "policy",
    "using",
    "behavioral",
    "policy",
    "target",
    "policy",
    "learn",
    "environment",
    "generate",
    "behavior",
    "respectively",
    "deal",
    "problems",
    "uh",
    "deal",
    "algorithms",
    "take",
    "maximizing",
    "approach",
    "choosing",
    "actions",
    "always",
    "get",
    "something",
    "called",
    "maximization",
    "bias",
    "say",
    "set",
    "states",
    "many",
    "different",
    "actions",
    "action",
    "value",
    "function",
    "state",
    "actions",
    "zero",
    "agent",
    "estimate",
    "q",
    "capital",
    "q",
    "actually",
    "actually",
    "uncertainty",
    "uncertainty",
    "actually",
    "spread",
    "values",
    "right",
    "spread",
    "causes",
    "amount",
    "positive",
    "bias",
    "max",
    "true",
    "values",
    "zero",
    "max",
    "capital",
    "q",
    "agent",
    "estimate",
    "positive",
    "hence",
    "positive",
    "bias",
    "often",
    "problem",
    "reinforcement",
    "learning",
    "algorithms",
    "happens",
    "using",
    "set",
    "samples",
    "max",
    "determine",
    "maximizing",
    "action",
    "well",
    "value",
    "action",
    "one",
    "way",
    "solve",
    "problem",
    "use",
    "two",
    "separate",
    "q",
    "functions",
    "determine",
    "max",
    "action",
    "value",
    "set",
    "relationship",
    "alternate",
    "play",
    "game",
    "using",
    "one",
    "determine",
    "max",
    "action",
    "one",
    "determine",
    "value",
    "alternate",
    "eliminate",
    "bias",
    "time",
    "double",
    "q",
    "learning",
    "nutshell",
    "algorithm",
    "following",
    "initialize",
    "alpha",
    "epsilon",
    "alpha",
    "learning",
    "rate",
    "epsilon",
    "use",
    "epsilon",
    "greedy",
    "want",
    "initialize",
    "q1",
    "q2",
    "functions",
    "states",
    "actions",
    "state",
    "action",
    "space",
    "course",
    "arbitrary",
    "except",
    "terminal",
    "states",
    "must",
    "value",
    "zero",
    "loop",
    "set",
    "episodes",
    "initialize",
    "state",
    "episode",
    "write",
    "step",
    "within",
    "episode",
    "choose",
    "action",
    "uh",
    "using",
    "state",
    "using",
    "epsilon",
    "greedy",
    "strategy",
    "sum",
    "q1",
    "q2",
    "two",
    "separate",
    "q",
    "functions",
    "using",
    "single",
    "queue",
    "learning",
    "would",
    "take",
    "max",
    "action",
    "one",
    "queue",
    "since",
    "dealing",
    "two",
    "account",
    "somehow",
    "right",
    "could",
    "max",
    "could",
    "sum",
    "could",
    "average",
    "case",
    "going",
    "take",
    "sum",
    "two",
    "q",
    "functions",
    "take",
    "action",
    "get",
    "reward",
    "observe",
    "new",
    "state",
    "probability",
    "either",
    "update",
    "q1",
    "q2",
    "according",
    "update",
    "rule",
    "course",
    "end",
    "step",
    "go",
    "ahead",
    "set",
    "estate",
    "new",
    "state",
    "keep",
    "looping",
    "game",
    "done",
    "clear",
    "mud",
    "hope",
    "way",
    "want",
    "reinforcement",
    "learning",
    "content",
    "make",
    "sure",
    "subscribe",
    "hit",
    "bell",
    "icon",
    "get",
    "notified",
    "let",
    "get",
    "next",
    "code",
    "inside",
    "code",
    "editor",
    "using",
    "visual",
    "studio",
    "code",
    "take",
    "look",
    "double",
    "queue",
    "learning",
    "script",
    "going",
    "typing",
    "terminal",
    "think",
    "probably",
    "little",
    "bit",
    "annoying",
    "going",
    "review",
    "code",
    "go",
    "along",
    "seen",
    "video",
    "source",
    "algorithm",
    "going",
    "fair",
    "amount",
    "overlap",
    "solving",
    "set",
    "problems",
    "real",
    "difference",
    "video",
    "source",
    "calculate",
    "action",
    "value",
    "function",
    "case",
    "using",
    "double",
    "q",
    "learning",
    "max",
    "action",
    "function",
    "tells",
    "us",
    "max",
    "action",
    "given",
    "state",
    "construct",
    "make",
    "numpy",
    "array",
    "list",
    "given",
    "state",
    "actions",
    "said",
    "video",
    "going",
    "take",
    "sum",
    "q1",
    "q2",
    "given",
    "state",
    "actions",
    "want",
    "take",
    "arg",
    "max",
    "recall",
    "numpy",
    "arg",
    "max",
    "function",
    "tie",
    "returns",
    "first",
    "element",
    "left",
    "right",
    "actions",
    "identical",
    "action",
    "value",
    "functions",
    "return",
    "left",
    "action",
    "consistently",
    "may",
    "may",
    "problem",
    "something",
    "aware",
    "discretize",
    "spaces",
    "recall",
    "cart",
    "pull",
    "problem",
    "cart",
    "sliding",
    "along",
    "track",
    "pole",
    "must",
    "maintained",
    "vertically",
    "right",
    "cart",
    "pole",
    "example",
    "continuous",
    "space",
    "x",
    "theta",
    "number",
    "within",
    "given",
    "range",
    "likewise",
    "velocities",
    "deal",
    "couple",
    "options",
    "could",
    "simply",
    "use",
    "neural",
    "networks",
    "approximate",
    "functions",
    "case",
    "going",
    "use",
    "little",
    "trick",
    "discretize",
    "space",
    "going",
    "divide",
    "10",
    "equal",
    "chunks",
    "number",
    "falls",
    "within",
    "particular",
    "chunk",
    "assigned",
    "integer",
    "go",
    "continuous",
    "discrete",
    "representation",
    "four",
    "vector",
    "observation",
    "along",
    "comes",
    "get",
    "state",
    "observat",
    "state",
    "along",
    "comes",
    "get",
    "state",
    "function",
    "pass",
    "observation",
    "uses",
    "uh",
    "digitized",
    "spaces",
    "excuse",
    "use",
    "linear",
    "spaces",
    "use",
    "numpy",
    "digitized",
    "function",
    "get",
    "integer",
    "representation",
    "respective",
    "elements",
    "observation",
    "also",
    "added",
    "function",
    "plot",
    "running",
    "average",
    "sarsa",
    "video",
    "end",
    "little",
    "bit",
    "mess",
    "50",
    "000",
    "data",
    "points",
    "plot",
    "running",
    "average",
    "prior",
    "100",
    "games",
    "next",
    "initialize",
    "hyper",
    "parameters",
    "learning",
    "rate",
    "controls",
    "step",
    "size",
    "update",
    "equation",
    "gamma",
    "course",
    "discount",
    "factor",
    "agent",
    "uses",
    "estimates",
    "future",
    "rewards",
    "believe",
    "actually",
    "left",
    "super",
    "critical",
    "far",
    "concerned",
    "really",
    "reason",
    "purpose",
    "discounting",
    "account",
    "uncertainties",
    "future",
    "rewards",
    "sequence",
    "rewards",
    "probability",
    "receiving",
    "makes",
    "sense",
    "give",
    "rewards",
    "equal",
    "weight",
    "know",
    "going",
    "get",
    "cart",
    "poll",
    "example",
    "rewards",
    "certain",
    "far",
    "aware",
    "state",
    "transition",
    "probabilities",
    "one",
    "know",
    "move",
    "right",
    "going",
    "actually",
    "end",
    "moving",
    "right",
    "know",
    "deterministically",
    "pole",
    "cart",
    "going",
    "move",
    "discounted",
    "far",
    "concerned",
    "epsilon",
    "epsilon",
    "factor",
    "epsilon",
    "greedy",
    "algorithm",
    "pretty",
    "much",
    "hyperparameters",
    "model",
    "next",
    "construct",
    "state",
    "space",
    "means",
    "oh",
    "baby",
    "unhappy",
    "state",
    "space",
    "course",
    "um",
    "representation",
    "digitized",
    "space",
    "going",
    "cart",
    "position",
    "going",
    "10",
    "buckets",
    "velocity",
    "10",
    "buckets",
    "likewise",
    "thetas",
    "theta",
    "position",
    "theta",
    "velocity",
    "going",
    "10",
    "four",
    "possible",
    "states",
    "10",
    "000",
    "states",
    "going",
    "numbered",
    "way",
    "0",
    "0",
    "0",
    "constructing",
    "set",
    "states",
    "next",
    "initialize",
    "q",
    "functions",
    "recall",
    "initialization",
    "arbitrary",
    "except",
    "terminal",
    "state",
    "must",
    "value",
    "zero",
    "reason",
    "terminal",
    "state",
    "definition",
    "future",
    "value",
    "zero",
    "stopped",
    "playing",
    "game",
    "right",
    "makes",
    "sense",
    "could",
    "initialize",
    "randomly",
    "could",
    "initialize",
    "minus",
    "one",
    "plus",
    "one",
    "really",
    "matter",
    "long",
    "terminal",
    "state",
    "zero",
    "simplicity",
    "initializing",
    "everything",
    "zero",
    "going",
    "play",
    "hundred",
    "thousand",
    "games",
    "reason",
    "algorithm",
    "eliminates",
    "bias",
    "expense",
    "convergence",
    "speed",
    "let",
    "run",
    "little",
    "bit",
    "longer",
    "uh",
    "array",
    "keeping",
    "track",
    "total",
    "rewards",
    "gon",
    "na",
    "loop",
    "hundred",
    "thousand",
    "games",
    "printing",
    "every",
    "five",
    "thousand",
    "games",
    "let",
    "us",
    "know",
    "still",
    "running",
    "always",
    "want",
    "reset",
    "done",
    "flag",
    "rewards",
    "reset",
    "episode",
    "top",
    "going",
    "loop",
    "episode",
    "getting",
    "state",
    "calculating",
    "random",
    "number",
    "epsilon",
    "greedy",
    "strategy",
    "gon",
    "na",
    "set",
    "action",
    "max",
    "action",
    "q1",
    "q2",
    "random",
    "number",
    "less",
    "one",
    "minus",
    "epsilon",
    "otherwise",
    "going",
    "randomly",
    "sample",
    "action",
    "space",
    "event",
    "take",
    "action",
    "get",
    "new",
    "state",
    "reward",
    "done",
    "flag",
    "go",
    "ahead",
    "tally",
    "reward",
    "convert",
    "observation",
    "state",
    "prime",
    "go",
    "ahead",
    "calculate",
    "separate",
    "random",
    "number",
    "purpose",
    "random",
    "number",
    "determine",
    "q",
    "function",
    "going",
    "update",
    "know",
    "going",
    "using",
    "one",
    "calculate",
    "alternating",
    "eliminate",
    "maximization",
    "bias",
    "right",
    "one",
    "finding",
    "max",
    "action",
    "one",
    "finding",
    "value",
    "action",
    "alternate",
    "episodes",
    "way",
    "random",
    "number",
    "cases",
    "want",
    "collect",
    "want",
    "calculate",
    "max",
    "action",
    "either",
    "q1",
    "q2",
    "use",
    "update",
    "rule",
    "showed",
    "slides",
    "update",
    "estimates",
    "q1",
    "q2",
    "go",
    "end",
    "episode",
    "sorry",
    "end",
    "step",
    "excuse",
    "want",
    "reset",
    "old",
    "observation",
    "new",
    "one",
    "way",
    "get",
    "state",
    "end",
    "episode",
    "want",
    "go",
    "ahead",
    "decrease",
    "epsilon",
    "familiar",
    "epsilon",
    "greedy",
    "strategy",
    "dealing",
    "explore",
    "exploit",
    "dilemma",
    "agent",
    "always",
    "estimate",
    "future",
    "rewards",
    "based",
    "model",
    "environment",
    "experience",
    "playing",
    "game",
    "model",
    "free",
    "model",
    "uh",
    "problem",
    "right",
    "either",
    "explore",
    "exploit",
    "best",
    "known",
    "actions",
    "one",
    "way",
    "dealing",
    "dilemma",
    "much",
    "time",
    "spend",
    "exploring",
    "versus",
    "much",
    "time",
    "spend",
    "exploiting",
    "use",
    "something",
    "called",
    "epsilon",
    "greedy",
    "meaning",
    "percentage",
    "time",
    "explore",
    "percentage",
    "time",
    "exploit",
    "way",
    "get",
    "settle",
    "greedy",
    "strategy",
    "gradually",
    "decrease",
    "exploration",
    "parameter",
    "epsilon",
    "time",
    "course",
    "want",
    "keep",
    "track",
    "total",
    "rewards",
    "episode",
    "recall",
    "current",
    "poll",
    "example",
    "agent",
    "gets",
    "reward",
    "positive",
    "one",
    "every",
    "time",
    "poll",
    "stays",
    "vertical",
    "every",
    "move",
    "flop",
    "gets",
    "one",
    "point",
    "end",
    "going",
    "go",
    "ahead",
    "plot",
    "running",
    "averages",
    "going",
    "go",
    "ahead",
    "run",
    "take",
    "minute",
    "uh",
    "running",
    "want",
    "ask",
    "guys",
    "question",
    "type",
    "material",
    "want",
    "see",
    "seeing",
    "data",
    "reinforcement",
    "learning",
    "stuff",
    "immensely",
    "popular",
    "content",
    "much",
    "going",
    "keep",
    "focusing",
    "type",
    "stuff",
    "happy",
    "seeing",
    "sutton",
    "bardo",
    "type",
    "introductory",
    "material",
    "want",
    "see",
    "deep",
    "learning",
    "type",
    "material",
    "right",
    "whole",
    "host",
    "dozens",
    "deep",
    "reinforcement",
    "learning",
    "algorithms",
    "cover",
    "actually",
    "quite",
    "content",
    "cover",
    "stuff",
    "believe",
    "ca",
    "master",
    "basics",
    "deep",
    "learning",
    "stuff",
    "going",
    "make",
    "sense",
    "anyway",
    "right",
    "complexity",
    "deep",
    "learning",
    "top",
    "complexity",
    "reinforcement",
    "learning",
    "material",
    "top",
    "anything",
    "particular",
    "guys",
    "want",
    "see",
    "make",
    "sure",
    "leave",
    "comment",
    "hey",
    "subscribed",
    "happen",
    "like",
    "reinforcement",
    "learning",
    "machine",
    "learning",
    "material",
    "please",
    "consider",
    "like",
    "video",
    "make",
    "sure",
    "leave",
    "thumbs",
    "hey",
    "thought",
    "sucked",
    "go",
    "ahead",
    "leave",
    "thumbs",
    "tell",
    "happy",
    "answer",
    "comments",
    "answer",
    "objections",
    "guys",
    "suggestions",
    "improvement",
    "ears",
    "finally",
    "finished",
    "hundred",
    "thousand",
    "episodes",
    "see",
    "running",
    "average",
    "course",
    "games",
    "would",
    "expect",
    "agent",
    "begins",
    "learn",
    "fairly",
    "quickly",
    "balancing",
    "cart",
    "pull",
    "60",
    "000",
    "games",
    "starts",
    "hit",
    "consistently",
    "hit",
    "200",
    "move",
    "threshold",
    "able",
    "balance",
    "cart",
    "pull",
    "200",
    "moves",
    "game",
    "recall",
    "gamma",
    "going",
    "go",
    "ahead",
    "rerun",
    "gamma",
    "see",
    "burn",
    "image",
    "brain",
    "going",
    "go",
    "ahead",
    "check",
    "gamma",
    "see",
    "better",
    "back",
    "second",
    "run",
    "using",
    "gamma",
    "see",
    "something",
    "quite",
    "interesting",
    "actually",
    "kind",
    "ever",
    "reaches",
    "200",
    "mark",
    "uh",
    "handful",
    "games",
    "kind",
    "stutters",
    "along",
    "actually",
    "decreasing",
    "performance",
    "goes",
    "along",
    "something",
    "funny",
    "going",
    "frank",
    "top",
    "head",
    "entirely",
    "certain",
    "invite",
    "speculate",
    "however",
    "also",
    "interesting",
    "second",
    "time",
    "recording",
    "recorded",
    "earlier",
    "scroll",
    "code",
    "ended",
    "staring",
    "chunk",
    "stuff",
    "redo",
    "case",
    "gamma",
    "well",
    "seemed",
    "work",
    "fine",
    "suspect",
    "significant",
    "variation",
    "random",
    "number",
    "generator",
    "um",
    "could",
    "due",
    "right",
    "complex",
    "space",
    "wanders",
    "around",
    "different",
    "portions",
    "could",
    "happen",
    "potentially",
    "visit",
    "areas",
    "parameter",
    "space",
    "enough",
    "times",
    "get",
    "reasonable",
    "estimate",
    "samples",
    "may",
    "type",
    "bias",
    "visits",
    "later",
    "course",
    "episodes",
    "although",
    "sounds",
    "kind",
    "unlikely",
    "either",
    "way",
    "double",
    "q",
    "learning",
    "see",
    "hyper",
    "parameters",
    "actually",
    "affect",
    "model",
    "seems",
    "fairly",
    "large",
    "effect",
    "might",
    "expect",
    "next",
    "video",
    "going",
    "taking",
    "look",
    "double",
    "sarsa",
    "subscribed",
    "ask",
    "please",
    "consider",
    "hit",
    "notification",
    "icon",
    "see",
    "release",
    "video",
    "look",
    "forward",
    "seeing",
    "next",
    "video",
    "well",
    "hope",
    "helpful",
    "everyone",
    "learn",
    "learned",
    "policy",
    "gradient",
    "methods",
    "sarsa",
    "double",
    "q",
    "learning",
    "even",
    "create",
    "reinforcement",
    "learning",
    "environments",
    "solid",
    "foundation",
    "topic",
    "reinforcement",
    "learning",
    "pretty",
    "well",
    "prepared",
    "go",
    "explore",
    "advanced",
    "topics",
    "advanced",
    "topics",
    "right",
    "forefront",
    "things",
    "like",
    "deep",
    "deterministic",
    "policy",
    "gradients",
    "might",
    "guess",
    "name",
    "advanced",
    "version",
    "policy",
    "gradient",
    "methods",
    "also",
    "actor",
    "critic",
    "methods",
    "uh",
    "behavioral",
    "cloning",
    "sorts",
    "advanced",
    "topics",
    "pretty",
    "well",
    "equipped",
    "go",
    "explore",
    "particularly",
    "useful",
    "environments",
    "continuous",
    "action",
    "spaces",
    "environments",
    "studied",
    "set",
    "tutorials",
    "discrete",
    "action",
    "space",
    "meaning",
    "agent",
    "moves",
    "takes",
    "discrete",
    "set",
    "actions",
    "environments",
    "bipedal",
    "walker",
    "car",
    "racing",
    "things",
    "nature",
    "continuous",
    "state",
    "spaces",
    "excuse",
    "continuous",
    "action",
    "spaces",
    "require",
    "different",
    "mechanisms",
    "solve",
    "q",
    "learning",
    "really",
    "ca",
    "handle",
    "free",
    "go",
    "ahead",
    "check",
    "stuff",
    "made",
    "far",
    "please",
    "consider",
    "subscribing",
    "channel",
    "machine",
    "learning",
    "phil",
    "hope",
    "helpful",
    "leave",
    "comment",
    "make",
    "sure",
    "share",
    "see",
    "next",
    "video"
  ],
  "keywords": [
    "welcome",
    "reinforcement",
    "learning",
    "start",
    "host",
    "phil",
    "know",
    "machine",
    "going",
    "learn",
    "everything",
    "need",
    "get",
    "started",
    "really",
    "basic",
    "far",
    "course",
    "pretty",
    "open",
    "ai",
    "gym",
    "taking",
    "rather",
    "also",
    "atari",
    "play",
    "games",
    "like",
    "space",
    "invaders",
    "new",
    "environment",
    "tensorflow",
    "library",
    "well",
    "pi",
    "torch",
    "bit",
    "meaning",
    "important",
    "stuff",
    "deep",
    "q",
    "policy",
    "gradient",
    "methods",
    "first",
    "kind",
    "back",
    "take",
    "look",
    "things",
    "sarsa",
    "double",
    "even",
    "make",
    "environments",
    "code",
    "grid",
    "world",
    "solve",
    "something",
    "keep",
    "github",
    "link",
    "comment",
    "case",
    "want",
    "around",
    "information",
    "videos",
    "comments",
    "leave",
    "let",
    "video",
    "much",
    "three",
    "sure",
    "subscribe",
    "form",
    "position",
    "big",
    "starting",
    "got",
    "find",
    "two",
    "thousand",
    "percent",
    "probability",
    "000",
    "five",
    "end",
    "future",
    "problem",
    "agent",
    "maximize",
    "rewards",
    "solution",
    "real",
    "time",
    "works",
    "states",
    "actions",
    "based",
    "strategy",
    "called",
    "epsilon",
    "greedy",
    "action",
    "selection",
    "basically",
    "random",
    "explore",
    "known",
    "expected",
    "feature",
    "exploring",
    "model",
    "gradually",
    "decrease",
    "value",
    "set",
    "zero",
    "since",
    "always",
    "gets",
    "reward",
    "many",
    "algorithms",
    "operation",
    "step",
    "instead",
    "episode",
    "keeping",
    "state",
    "could",
    "use",
    "dictionary",
    "number",
    "discrete",
    "track",
    "update",
    "game",
    "way",
    "memories",
    "difference",
    "returns",
    "took",
    "possible",
    "estimates",
    "best",
    "continuous",
    "ca",
    "neural",
    "networks",
    "reason",
    "function",
    "approximate",
    "relationship",
    "memory",
    "received",
    "subset",
    "feed",
    "network",
    "values",
    "uses",
    "loss",
    "weights",
    "one",
    "evaluation",
    "current",
    "see",
    "another",
    "target",
    "used",
    "calculate",
    "little",
    "bias",
    "images",
    "convolutional",
    "perform",
    "output",
    "image",
    "tell",
    "moving",
    "sense",
    "motion",
    "single",
    "means",
    "frames",
    "give",
    "takes",
    "batch",
    "stacked",
    "input",
    "choosing",
    "straightforward",
    "generate",
    "less",
    "parameter",
    "add",
    "store",
    "sampling",
    "getting",
    "long",
    "transitions",
    "data",
    "gpu",
    "handle",
    "training",
    "critical",
    "found",
    "helpful",
    "next",
    "anything",
    "along",
    "channel",
    "times",
    "thing",
    "familiar",
    "idea",
    "fully",
    "connected",
    "layer",
    "determine",
    "given",
    "stack",
    "go",
    "passing",
    "hey",
    "either",
    "move",
    "left",
    "right",
    "fire",
    "class",
    "later",
    "ahead",
    "saving",
    "numpy",
    "functions",
    "build",
    "initializer",
    "rate",
    "name",
    "gon",
    "na",
    "select",
    "us",
    "sorry",
    "pass",
    "four",
    "particular",
    "directory",
    "save",
    "session",
    "graph",
    "call",
    "initialize",
    "variables",
    "train",
    "10",
    "score",
    "uh",
    "whatever",
    "quite",
    "checkpoint",
    "parameters",
    "copy",
    "layers",
    "makes",
    "variable",
    "shape",
    "using",
    "every",
    "scroll",
    "filters",
    "kernel",
    "size",
    "may",
    "check",
    "second",
    "third",
    "128",
    "good",
    "activation",
    "yet",
    "linear",
    "actual",
    "remember",
    "learns",
    "said",
    "file",
    "load",
    "maximum",
    "alpha",
    "gamma",
    "discount",
    "factor",
    "hyper",
    "often",
    "tells",
    "replace",
    "okay",
    "terminal",
    "whether",
    "done",
    "array",
    "zeros",
    "old",
    "underscore",
    "flag",
    "beginning",
    "counter",
    "put",
    "increment",
    "choose",
    "purely",
    "randomly",
    "actually",
    "run",
    "dot",
    "type",
    "return",
    "performance",
    "different",
    "bellman",
    "equation",
    "algorithm",
    "sample",
    "choice",
    "plus",
    "quantity",
    "account",
    "discounted",
    "moves",
    "say",
    "square",
    "lot",
    "yeah",
    "earlier",
    "trying",
    "would",
    "main",
    "import",
    "observation",
    "already",
    "row",
    "sequence",
    "running",
    "frame",
    "top",
    "iterate",
    "otherwise",
    "append",
    "equals",
    "minus",
    "six",
    "true",
    "200",
    "initial",
    "reset",
    "transition",
    "finally",
    "print",
    "average",
    "last",
    "still",
    "oh",
    "plot",
    "free",
    "everybody",
    "coding",
    "loop",
    "seeing",
    "representation",
    "successor",
    "predicted",
    "mean",
    "um",
    "module",
    "care",
    "channels",
    "work",
    "please",
    "following",
    "magic",
    "total",
    "full",
    "show",
    "method",
    "device",
    "sum",
    "forward",
    "guys",
    "vector",
    "music",
    "playing",
    "amount",
    "convert",
    "must",
    "try",
    "deal",
    "rows",
    "part",
    "made",
    "think",
    "consider",
    "previous",
    "seen",
    "max",
    "list",
    "steps",
    "experience",
    "estimate",
    "recall",
    "simple",
    "matter",
    "episodes",
    "optimal",
    "gradients",
    "enough",
    "element",
    "column",
    "rule",
    "hope",
    "create",
    "fact",
    "construct",
    "x",
    "problems",
    "distribution",
    "labels",
    "theta",
    "g",
    "spaces",
    "process",
    "example",
    "probabilities",
    "separate",
    "attempting",
    "receives",
    "squares",
    "exploit",
    "prime",
    "cart"
  ]
}