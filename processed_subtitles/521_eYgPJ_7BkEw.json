{
  "text": "hi today we're looking at fix match\nsimplifying semi-supervised learning\nwith consistency and confidence by cukes\non David birth Berthelot and others of\nGoogle research so this paper concerns\nsemi-supervised learning so what does\nsemi-supervised learning mean in\nsemi-supervised learning you have a data\nset of labeled samples so right you have\nthis data set of X's and corresponding Y\nlabels but this data set sometimes is\nvery small now you have a much bigger\ndata set of unlabeled examples just X's\nwith no labels right so you don't know\nwhat the labels of the of the unlabeled\nexamples are but what you would like to\ndo is you would like to use this really\nlarge data set in order to help you with\nlearning the association between the\ndata points and the labels so for\nexample in this case you would have\nsomething like like an image\nclassification data set and I'm gonna\ntake the example here of medical data so\nyou have a pictures of lungs let's draw\na long here that is an ugly long you\nhave pictures of lungs and whether or\nnot they are they have like a tumor in\nthem right so medical data is very hard\nto get especially labeled medical data\nbecause you need first of all you need\nthe data itself but then you also need\nlike like one at least one but ideally\nlike three radiologists to look at\nwhether or not this is a good or a bad\nimage and label it so it's usually very\nexpensive to collect that data but you\nmight have plenty of unlabeled data\nright you might just be able to go who\nyou're through through some database and\nfind like anonymized undiagnosed long\nscans somewhere lying around the same\nwith image like other images\nso labeling images is pretty human\nintensive but the internet contains like\na whole bunch of unlabeled images so the\ntask of semi-supervised learning is how\ndo you use this unlabeled data set in\norder to make your classification on the\nlabel data set easier and fix match\ncombines two approaches to this in a\nsmart way namely the consistency and\nconfidence approach right so what does\nwhat does well it will jump right into\ninto the method so basically what you\nwant to do is you want to say my loss\nthat I optimized right this is my loss\nconsists of two parts namely a\nsupervised loss which is your classic\nclassification loss right plus an\nunsupervised loss right and then you\nhave like some sort of a trade-off\nparameter in front now your supervised\nloss here this is where this is just the\nthe cross-entropy\nlet's call it h between your predicted\nlabels and your the actual true labels\nright and the predicted labels say they\ncan be you know kind of a distribution\nover labels now the magic of course is\nhere in the unsupervised loss and this\nunsupervised loss this is what's\ndescribed here in this part right so the\nunsupervised loss is going to be this\nage between P and Q and we'll see what P\nand Q is so if for the unsupervised loss\nyou two of course want to start with an\nunlabeled example then you have the same\nsample go into two different pipelines\nin the first pipeline up here what you\ndo is you so-called weekly augmented and\nhere we're dealing with images so we\nhave to talk about image augmentation so\nimage augmentation has long been used in\nsupervised learning to kind of give you\nmore it's kind of a cheat to give you\nmore training data so if you have an\nimage right of let's say\nfamous cat you can obtain met more\ntraining data if you for example by\nrandom cropping so you can random crop\nlet's say we just take this bottom right\ncorner here and then we enlarge it to\nthe original size right then it is still\nsort of a cat but it's just a part of a\ncat right but usually that helps because\nyou you say okay um my image data set is\njust pictures of animals right it's\nentirely conceivable that someone held\nthe camera like this or like this right\nso technically in terms of generalizing\nto a test set these both data points\nshould be valid so I'm just gonna add\nboth to my training data so you can see\nhow from one training data point you can\nget many training data points just by\ndoing this cropping what you can also do\nis you can flip it left right right you\njust in swap the pixels left right and\nusually the these kind of um so a a a\ncat that has a little dark spot here is\nstill a cat when it has too little dark\nspot over there right but to your\nclassifier those are two different\nsamples so you can do many of those\nthings and they have to kind of\naugmentations they have what they call\nweakly augmented and strongly augmented\nright so in the weakly augmented\npipeline I think they just they crop and\nthey they shift and they rotate or\nsomething like this so you can see here\nthis this horsey here it is something\nlike it's cropped here about then it is\nturned slightly to the left and then\nyeah I think that's it so they crop they\nrotate and then they also flip\nhorizontally at random in like 50\npercent of the time so these are what's\ncalled weekly augmented the goal here is\njust to kind of obtain a bit more\ntraining data alright so you run this\nthrough your model through your\nclassification model as you would\na regular sample and you get a\nprediction now from your prediction you\ncan take the highest prediction here and\nthat is going to be your pseudo label so\nthis is P of Y this is your distribution\nthat you estimate right so and this and\nthis if you just take the max this is\ngoing to be your Y hat right and this is\nwhat they call a pseudo label sorry\nyou'll see why it is called a pseudo\nlabel so the other pipeline here is the\nstrong augmentation pipeline now in weak\naugmentation we just wanted to get\nsomewhere training it in strong\naugmentation now the goal is to really\nscrew up that picture to the point where\nit's still you know you could recognize\nit in the same class but you can see\nhere the augmentations they go wild so\nyou play around with the color with the\nhue you play around with the light\nintensity right with the contrast you\ncan do many many things you can see this\nthis image looks basically nothing like\nthis image buddied you can still kind of\nrecognize it as a horse but the strongly\naugmented data is much more distorted\nthan the weakly augmented data and\nthat's the point so also you send the\nstrongly augmented data through the\nmodel and again you get a prediction\nright and now is that the trick is you\ntake the label from here and you you\ntake that as if it were the true label\nright you take that as if it were the\ntrue label and you form a loss from this\nprediction being the model prediction as\nif this thing here that also comes from\nthe model as if that was the true label\nright that's why it's called a pseudo\nlabel because it is a label that you\nproduce from the model itself now of\ncourse if these were to be the same\npicture it would be kind of pointless\nright that's why you see there\nneeds to be a weekly and a strongly\naugmented pipeline I am pretty sure ammo\nif you want a more basic version of this\nmake this just clean\nso no augmentation and make this augment\nit right that's that's how you can think\nof it the fact that there is weak and\nhere strong augmentation I think is just\na your classic trick to get more\ntraining data but in essence you can\nthink of it as this is here the clean\nthing you just want to produce a label\nand then you want the that an Augmented\nversion of the image has the same label\nnow you can think of it shortly what\ndoes this model learn if you just have\nthis you remember I think the important\nthing is always to remember that there\nare two components here right there is\nfirst the supervised loss this is the\nimportant one ultimately because we have\nthe true labels right and then second\nthere is the unsupervised loss which is\njust an auxiliary loss that is supposed\nto just kind of tune our model to the\nnature of the data right so don't forget\nthat this this down here just concerns\nthe unsupervised part of that loss so if\nyou think what does the model actually\nlearn when whenever you train it like\nthis it basically learns to revert this\nstrong augmentation right say basically\nsells hey model whenever I give you a\nweek\naugmented image and I distort it heavily\nright whenever I give you an image and\nthat distort it heavily I want the label\nto be the same so the model basically\nlearns that whatever the image the\nwhatever the image the model at the end\nof the trend will be able to basically\nmap any strongly augmented picture to\nthe same class as a weekly augmented\npicture if it comes from the same source\nright so\nthe model basically learned to ignore\nthese kinds of augmentations that's what\nthis loss over here does it basically\nsays these sorts of augmentations these\nsorts of distortions of images please\nignore those because I always want you\nto output the same label here in the\nprediction here as if I had not\ndistorted or just weakly distorted the\nimage so that's that's what you have to\nkeep in mind that this this loss is\ndesigned to make the model not\ndistinguish between differently\naugmented versions of the same image and\ninterestingly that really seems to help\nwith the with the supervised loss right\nmy kind of hypothesis is is that all\nthese methods what they're kind of\ntrying to do is to just tune the neural\nnetwork to the let's say the orders of\nmagnitude of the of the input data and\nalso to the kinds of augmentations that\nthe humans come up with and that's a\nvery important point\nso the Ottoman tations and here we said\nyou know it's it's kind of a rotation\nand the crop the kind of augmentation\nreally seemed to play a role\nso this paper finds that on C 410 where\nthe state of the art I believe is\nsomething like ninety six ninety seven\npercent accuracy on C for ten with just\ntwo hundred and fifty labeled examples\nright now the usual data set size is\nabout fifty thousand it goes to ninety\nfour point three four point nine percent\nso almost 95 percent accuracy with the\nstate of the art being like ninety seven\nthis is incredible with just two hundred\nand fifty labeled examples crazy right\nand it with only four labels per class\nit gets eighty eight point six percent\nso that's just forty images with labels\nthey get a\n8.6 percent of of the of accuracy\ncompared to the 97 percent that you get\nwith like 50,000 images that is pretty\npretty cool right simply by having all\nother images not labeled but pseudo\nlabeled and consistency regularized\nright so the the two to two things that\nare combined by fixed match again or\nconsistency regularization which\nbasically it means that the model should\noutput similar predictions when fed\nperturbed versions of the same image\nright this they they're really\nforthcoming that they are not the ones\nwho invented this they just combine the\nconsistency regularization with the\npseudo labeling now the pseudo labeling\nthey have also not invented the pseudo\nlabeling leverages the idea that we\nshould use the model itself to obtain\nartificial labels for unlabeled data\nwe've seen a lot of papers in the last\nfew months or years where it's like the\nteacher teaches the student and then the\nstudent teaches the teacher model again\nand so on so that they simply combine\nthe two methods in a clever way they\nhave one last thing that is not in this\ndrawing namely they only use the pseudo\nlabel they have a break right here and\nthey only use the pseudo label if if the\nconfidence if this P of Y here is above\na certain threshold so they don't take\nall the pseudo labels but they only take\nthe labels where the model is fairly\nsure about right so they haven't\nactually an ablation study where they\nshow that this is reasonably reasonably\nimportant and if you go down here where\nthey say ablation or is it ablation\nablation study oh yeah something I also\nfind cool if you just give one image per\nclass this one image per class ten\nimages that are labeled\nit still gets like 78 percent accuracy I\nthink the images are chosen as good\nrepresentations of their class but still\none image per class pretty pretty cool\nan important part of this is the\nablation study where they say okay we\nwant to tease apart why this algorithm\nwhy this on semi-supervised learning\ntechnique works so well and they find\nseveral important factors they find for\nexample that they're all mentation\nstrategy is extremely important so how\nthey augment the images is very\nimportant you see here the error of this\n4.8% and the 250 label split if you\nchange up the if you change up the the\naugmentation strategies your error gets\nhigher right and so they say we use this\ncutout and we measure the effect of cut\nout we find that both cut out and seek T\naugment are required to obtain the best\nperformance removing either results in\nin a comparable increase in error rate\nwe've seen before for example they went\nthey went from there some 93 sorry 93\npoint something percent to ninety four\npoint something percent from the\nprevious state-of-the-art\nsemi-supervised learning and here they\nfind that simply changing the\naugmentation strategy changes the error\nby more than a percent so you can just\nsee this in context of of what's\nimportant here right\nthey say again the ratio of unlabeled\ndata seems pretty important we observe a\nsignificant decrease in error rates by\nlosing using a large amounts of\nunlabeled data right then the optimizer\nand learning\nschedule seems to be very important as\nwell in that they use this they say STD\nwith momentum works much better than\nAdam and then they use this decreasing\nlearning rate schedule this cosine\nlearning rate schedule so there seem to\nbe a lot of things a lot of hyper\nparameters that are fairly important\nhere and you can see that the gains are\nsubstantial sometimes but they aren't\nlike through the roof substantial where\nyou can make a good argument that it is\nunclear how much really comes from this\nclever combination that fit fix match\nproposes and how much also just comes\nfrom whether or not you set the hyper\nparameters correctly and exactly how\nmuch computation are you able to throw\nat selecting your selecting your hyper\nparameters so that that seems to be a\nbit of a a bit of a pain point for me\nthey also say we find that tuning the\nweight decay is exceptionally important\nfor low label regimes right choosing a\nvalue that is just one order of\nmagnitude larger or smaller than optimal\ncan cost ten percentage points or more\nand so that all of that seems to me that\nthis this kind of research where you're\nyou're nibbling for half or single\npercentage points in accuracy while a\nsingle misstep in a choice of hyper\nparameter might cost you ten times that\ngain is is a bit sketchy now I recognize\nthey get numbers like no one else has\ngotten before but where exactly the\ngains come from and if the gains really\ncome from this architecture or actually\njust more from throwing computer\nat it I don't know all right with that I\nhope you enjoyed this and I invite you\nto check out the paper bye-bye\n",
  "words": [
    "hi",
    "today",
    "looking",
    "fix",
    "match",
    "simplifying",
    "learning",
    "consistency",
    "confidence",
    "cukes",
    "david",
    "birth",
    "berthelot",
    "others",
    "google",
    "research",
    "paper",
    "concerns",
    "learning",
    "learning",
    "mean",
    "learning",
    "data",
    "set",
    "labeled",
    "samples",
    "right",
    "data",
    "set",
    "x",
    "corresponding",
    "labels",
    "data",
    "set",
    "sometimes",
    "small",
    "much",
    "bigger",
    "data",
    "set",
    "unlabeled",
    "examples",
    "x",
    "labels",
    "right",
    "know",
    "labels",
    "unlabeled",
    "examples",
    "would",
    "like",
    "would",
    "like",
    "use",
    "really",
    "large",
    "data",
    "set",
    "order",
    "help",
    "learning",
    "association",
    "data",
    "points",
    "labels",
    "example",
    "case",
    "would",
    "something",
    "like",
    "like",
    "image",
    "classification",
    "data",
    "set",
    "gon",
    "na",
    "take",
    "example",
    "medical",
    "data",
    "pictures",
    "lungs",
    "let",
    "draw",
    "long",
    "ugly",
    "long",
    "pictures",
    "lungs",
    "whether",
    "like",
    "tumor",
    "right",
    "medical",
    "data",
    "hard",
    "get",
    "especially",
    "labeled",
    "medical",
    "data",
    "need",
    "first",
    "need",
    "data",
    "also",
    "need",
    "like",
    "like",
    "one",
    "least",
    "one",
    "ideally",
    "like",
    "three",
    "radiologists",
    "look",
    "whether",
    "good",
    "bad",
    "image",
    "label",
    "usually",
    "expensive",
    "collect",
    "data",
    "might",
    "plenty",
    "unlabeled",
    "data",
    "right",
    "might",
    "able",
    "go",
    "database",
    "find",
    "like",
    "anonymized",
    "undiagnosed",
    "long",
    "scans",
    "somewhere",
    "lying",
    "around",
    "image",
    "like",
    "images",
    "labeling",
    "images",
    "pretty",
    "human",
    "intensive",
    "internet",
    "contains",
    "like",
    "whole",
    "bunch",
    "unlabeled",
    "images",
    "task",
    "learning",
    "use",
    "unlabeled",
    "data",
    "set",
    "order",
    "make",
    "classification",
    "label",
    "data",
    "set",
    "easier",
    "fix",
    "match",
    "combines",
    "two",
    "approaches",
    "smart",
    "way",
    "namely",
    "consistency",
    "confidence",
    "approach",
    "right",
    "well",
    "jump",
    "right",
    "method",
    "basically",
    "want",
    "want",
    "say",
    "loss",
    "optimized",
    "right",
    "loss",
    "consists",
    "two",
    "parts",
    "namely",
    "supervised",
    "loss",
    "classic",
    "classification",
    "loss",
    "right",
    "plus",
    "unsupervised",
    "loss",
    "right",
    "like",
    "sort",
    "parameter",
    "front",
    "supervised",
    "loss",
    "let",
    "call",
    "h",
    "predicted",
    "labels",
    "actual",
    "true",
    "labels",
    "right",
    "predicted",
    "labels",
    "say",
    "know",
    "kind",
    "distribution",
    "labels",
    "magic",
    "course",
    "unsupervised",
    "loss",
    "unsupervised",
    "loss",
    "described",
    "part",
    "right",
    "unsupervised",
    "loss",
    "going",
    "age",
    "p",
    "q",
    "see",
    "p",
    "q",
    "unsupervised",
    "loss",
    "two",
    "course",
    "want",
    "start",
    "unlabeled",
    "example",
    "sample",
    "go",
    "two",
    "different",
    "pipelines",
    "first",
    "pipeline",
    "weekly",
    "augmented",
    "dealing",
    "images",
    "talk",
    "image",
    "augmentation",
    "image",
    "augmentation",
    "long",
    "used",
    "supervised",
    "learning",
    "kind",
    "give",
    "kind",
    "cheat",
    "give",
    "training",
    "data",
    "image",
    "right",
    "let",
    "say",
    "famous",
    "cat",
    "obtain",
    "met",
    "training",
    "data",
    "example",
    "random",
    "cropping",
    "random",
    "crop",
    "let",
    "say",
    "take",
    "bottom",
    "right",
    "corner",
    "enlarge",
    "original",
    "size",
    "right",
    "still",
    "sort",
    "cat",
    "part",
    "cat",
    "right",
    "usually",
    "helps",
    "say",
    "okay",
    "um",
    "image",
    "data",
    "set",
    "pictures",
    "animals",
    "right",
    "entirely",
    "conceivable",
    "someone",
    "held",
    "camera",
    "like",
    "like",
    "right",
    "technically",
    "terms",
    "generalizing",
    "test",
    "set",
    "data",
    "points",
    "valid",
    "gon",
    "na",
    "add",
    "training",
    "data",
    "see",
    "one",
    "training",
    "data",
    "point",
    "get",
    "many",
    "training",
    "data",
    "points",
    "cropping",
    "also",
    "flip",
    "left",
    "right",
    "right",
    "swap",
    "pixels",
    "left",
    "right",
    "usually",
    "kind",
    "um",
    "cat",
    "little",
    "dark",
    "spot",
    "still",
    "cat",
    "little",
    "dark",
    "spot",
    "right",
    "classifier",
    "two",
    "different",
    "samples",
    "many",
    "things",
    "kind",
    "augmentations",
    "call",
    "weakly",
    "augmented",
    "strongly",
    "augmented",
    "right",
    "weakly",
    "augmented",
    "pipeline",
    "think",
    "crop",
    "shift",
    "rotate",
    "something",
    "like",
    "see",
    "horsey",
    "something",
    "like",
    "cropped",
    "turned",
    "slightly",
    "left",
    "yeah",
    "think",
    "crop",
    "rotate",
    "also",
    "flip",
    "horizontally",
    "random",
    "like",
    "50",
    "percent",
    "time",
    "called",
    "weekly",
    "augmented",
    "goal",
    "kind",
    "obtain",
    "bit",
    "training",
    "data",
    "alright",
    "run",
    "model",
    "classification",
    "model",
    "would",
    "regular",
    "sample",
    "get",
    "prediction",
    "prediction",
    "take",
    "highest",
    "prediction",
    "going",
    "pseudo",
    "label",
    "p",
    "distribution",
    "estimate",
    "right",
    "take",
    "max",
    "going",
    "hat",
    "right",
    "call",
    "pseudo",
    "label",
    "sorry",
    "see",
    "called",
    "pseudo",
    "label",
    "pipeline",
    "strong",
    "augmentation",
    "pipeline",
    "weak",
    "augmentation",
    "wanted",
    "get",
    "somewhere",
    "training",
    "strong",
    "augmentation",
    "goal",
    "really",
    "screw",
    "picture",
    "point",
    "still",
    "know",
    "could",
    "recognize",
    "class",
    "see",
    "augmentations",
    "go",
    "wild",
    "play",
    "around",
    "color",
    "hue",
    "play",
    "around",
    "light",
    "intensity",
    "right",
    "contrast",
    "many",
    "many",
    "things",
    "see",
    "image",
    "looks",
    "basically",
    "nothing",
    "like",
    "image",
    "buddied",
    "still",
    "kind",
    "recognize",
    "horse",
    "strongly",
    "augmented",
    "data",
    "much",
    "distorted",
    "weakly",
    "augmented",
    "data",
    "point",
    "also",
    "send",
    "strongly",
    "augmented",
    "data",
    "model",
    "get",
    "prediction",
    "right",
    "trick",
    "take",
    "label",
    "take",
    "true",
    "label",
    "right",
    "take",
    "true",
    "label",
    "form",
    "loss",
    "prediction",
    "model",
    "prediction",
    "thing",
    "also",
    "comes",
    "model",
    "true",
    "label",
    "right",
    "called",
    "pseudo",
    "label",
    "label",
    "produce",
    "model",
    "course",
    "picture",
    "would",
    "kind",
    "pointless",
    "right",
    "see",
    "needs",
    "weekly",
    "strongly",
    "augmented",
    "pipeline",
    "pretty",
    "sure",
    "ammo",
    "want",
    "basic",
    "version",
    "make",
    "clean",
    "augmentation",
    "make",
    "augment",
    "right",
    "think",
    "fact",
    "weak",
    "strong",
    "augmentation",
    "think",
    "classic",
    "trick",
    "get",
    "training",
    "data",
    "essence",
    "think",
    "clean",
    "thing",
    "want",
    "produce",
    "label",
    "want",
    "augmented",
    "version",
    "image",
    "label",
    "think",
    "shortly",
    "model",
    "learn",
    "remember",
    "think",
    "important",
    "thing",
    "always",
    "remember",
    "two",
    "components",
    "right",
    "first",
    "supervised",
    "loss",
    "important",
    "one",
    "ultimately",
    "true",
    "labels",
    "right",
    "second",
    "unsupervised",
    "loss",
    "auxiliary",
    "loss",
    "supposed",
    "kind",
    "tune",
    "model",
    "nature",
    "data",
    "right",
    "forget",
    "concerns",
    "unsupervised",
    "part",
    "loss",
    "think",
    "model",
    "actually",
    "learn",
    "whenever",
    "train",
    "like",
    "basically",
    "learns",
    "revert",
    "strong",
    "augmentation",
    "right",
    "say",
    "basically",
    "sells",
    "hey",
    "model",
    "whenever",
    "give",
    "week",
    "augmented",
    "image",
    "distort",
    "heavily",
    "right",
    "whenever",
    "give",
    "image",
    "distort",
    "heavily",
    "want",
    "label",
    "model",
    "basically",
    "learns",
    "whatever",
    "image",
    "whatever",
    "image",
    "model",
    "end",
    "trend",
    "able",
    "basically",
    "map",
    "strongly",
    "augmented",
    "picture",
    "class",
    "weekly",
    "augmented",
    "picture",
    "comes",
    "source",
    "right",
    "model",
    "basically",
    "learned",
    "ignore",
    "kinds",
    "augmentations",
    "loss",
    "basically",
    "says",
    "sorts",
    "augmentations",
    "sorts",
    "distortions",
    "images",
    "please",
    "ignore",
    "always",
    "want",
    "output",
    "label",
    "prediction",
    "distorted",
    "weakly",
    "distorted",
    "image",
    "keep",
    "mind",
    "loss",
    "designed",
    "make",
    "model",
    "distinguish",
    "differently",
    "augmented",
    "versions",
    "image",
    "interestingly",
    "really",
    "seems",
    "help",
    "supervised",
    "loss",
    "right",
    "kind",
    "hypothesis",
    "methods",
    "kind",
    "trying",
    "tune",
    "neural",
    "network",
    "let",
    "say",
    "orders",
    "magnitude",
    "input",
    "data",
    "also",
    "kinds",
    "augmentations",
    "humans",
    "come",
    "important",
    "point",
    "ottoman",
    "tations",
    "said",
    "know",
    "kind",
    "rotation",
    "crop",
    "kind",
    "augmentation",
    "really",
    "seemed",
    "play",
    "role",
    "paper",
    "finds",
    "c",
    "410",
    "state",
    "art",
    "believe",
    "something",
    "like",
    "ninety",
    "six",
    "ninety",
    "seven",
    "percent",
    "accuracy",
    "c",
    "ten",
    "two",
    "hundred",
    "fifty",
    "labeled",
    "examples",
    "right",
    "usual",
    "data",
    "set",
    "size",
    "fifty",
    "thousand",
    "goes",
    "ninety",
    "four",
    "point",
    "three",
    "four",
    "point",
    "nine",
    "percent",
    "almost",
    "95",
    "percent",
    "accuracy",
    "state",
    "art",
    "like",
    "ninety",
    "seven",
    "incredible",
    "two",
    "hundred",
    "fifty",
    "labeled",
    "examples",
    "crazy",
    "right",
    "four",
    "labels",
    "per",
    "class",
    "gets",
    "eighty",
    "eight",
    "point",
    "six",
    "percent",
    "forty",
    "images",
    "labels",
    "get",
    "percent",
    "accuracy",
    "compared",
    "97",
    "percent",
    "get",
    "like",
    "images",
    "pretty",
    "pretty",
    "cool",
    "right",
    "simply",
    "images",
    "labeled",
    "pseudo",
    "labeled",
    "consistency",
    "regularized",
    "right",
    "two",
    "two",
    "things",
    "combined",
    "fixed",
    "match",
    "consistency",
    "regularization",
    "basically",
    "means",
    "model",
    "output",
    "similar",
    "predictions",
    "fed",
    "perturbed",
    "versions",
    "image",
    "right",
    "really",
    "forthcoming",
    "ones",
    "invented",
    "combine",
    "consistency",
    "regularization",
    "pseudo",
    "labeling",
    "pseudo",
    "labeling",
    "also",
    "invented",
    "pseudo",
    "labeling",
    "leverages",
    "idea",
    "use",
    "model",
    "obtain",
    "artificial",
    "labels",
    "unlabeled",
    "data",
    "seen",
    "lot",
    "papers",
    "last",
    "months",
    "years",
    "like",
    "teacher",
    "teaches",
    "student",
    "student",
    "teaches",
    "teacher",
    "model",
    "simply",
    "combine",
    "two",
    "methods",
    "clever",
    "way",
    "one",
    "last",
    "thing",
    "drawing",
    "namely",
    "use",
    "pseudo",
    "label",
    "break",
    "right",
    "use",
    "pseudo",
    "label",
    "confidence",
    "p",
    "certain",
    "threshold",
    "take",
    "pseudo",
    "labels",
    "take",
    "labels",
    "model",
    "fairly",
    "sure",
    "right",
    "actually",
    "ablation",
    "study",
    "show",
    "reasonably",
    "reasonably",
    "important",
    "go",
    "say",
    "ablation",
    "ablation",
    "ablation",
    "study",
    "oh",
    "yeah",
    "something",
    "also",
    "find",
    "cool",
    "give",
    "one",
    "image",
    "per",
    "class",
    "one",
    "image",
    "per",
    "class",
    "ten",
    "images",
    "labeled",
    "still",
    "gets",
    "like",
    "78",
    "percent",
    "accuracy",
    "think",
    "images",
    "chosen",
    "good",
    "representations",
    "class",
    "still",
    "one",
    "image",
    "per",
    "class",
    "pretty",
    "pretty",
    "cool",
    "important",
    "part",
    "ablation",
    "study",
    "say",
    "okay",
    "want",
    "tease",
    "apart",
    "algorithm",
    "learning",
    "technique",
    "works",
    "well",
    "find",
    "several",
    "important",
    "factors",
    "find",
    "example",
    "mentation",
    "strategy",
    "extremely",
    "important",
    "augment",
    "images",
    "important",
    "see",
    "error",
    "250",
    "label",
    "split",
    "change",
    "change",
    "augmentation",
    "strategies",
    "error",
    "gets",
    "higher",
    "right",
    "say",
    "use",
    "cutout",
    "measure",
    "effect",
    "cut",
    "find",
    "cut",
    "seek",
    "augment",
    "required",
    "obtain",
    "best",
    "performance",
    "removing",
    "either",
    "results",
    "comparable",
    "increase",
    "error",
    "rate",
    "seen",
    "example",
    "went",
    "went",
    "93",
    "sorry",
    "93",
    "point",
    "something",
    "percent",
    "ninety",
    "four",
    "point",
    "something",
    "percent",
    "previous",
    "learning",
    "find",
    "simply",
    "changing",
    "augmentation",
    "strategy",
    "changes",
    "error",
    "percent",
    "see",
    "context",
    "important",
    "right",
    "say",
    "ratio",
    "unlabeled",
    "data",
    "seems",
    "pretty",
    "important",
    "observe",
    "significant",
    "decrease",
    "error",
    "rates",
    "losing",
    "using",
    "large",
    "amounts",
    "unlabeled",
    "data",
    "right",
    "optimizer",
    "learning",
    "schedule",
    "seems",
    "important",
    "well",
    "use",
    "say",
    "std",
    "momentum",
    "works",
    "much",
    "better",
    "adam",
    "use",
    "decreasing",
    "learning",
    "rate",
    "schedule",
    "cosine",
    "learning",
    "rate",
    "schedule",
    "seem",
    "lot",
    "things",
    "lot",
    "hyper",
    "parameters",
    "fairly",
    "important",
    "see",
    "gains",
    "substantial",
    "sometimes",
    "like",
    "roof",
    "substantial",
    "make",
    "good",
    "argument",
    "unclear",
    "much",
    "really",
    "comes",
    "clever",
    "combination",
    "fit",
    "fix",
    "match",
    "proposes",
    "much",
    "also",
    "comes",
    "whether",
    "set",
    "hyper",
    "parameters",
    "correctly",
    "exactly",
    "much",
    "computation",
    "able",
    "throw",
    "selecting",
    "selecting",
    "hyper",
    "parameters",
    "seems",
    "bit",
    "bit",
    "pain",
    "point",
    "also",
    "say",
    "find",
    "tuning",
    "weight",
    "decay",
    "exceptionally",
    "important",
    "low",
    "label",
    "regimes",
    "right",
    "choosing",
    "value",
    "one",
    "order",
    "magnitude",
    "larger",
    "smaller",
    "optimal",
    "cost",
    "ten",
    "percentage",
    "points",
    "seems",
    "kind",
    "research",
    "nibbling",
    "half",
    "single",
    "percentage",
    "points",
    "accuracy",
    "single",
    "misstep",
    "choice",
    "hyper",
    "parameter",
    "might",
    "cost",
    "ten",
    "times",
    "gain",
    "bit",
    "sketchy",
    "recognize",
    "get",
    "numbers",
    "like",
    "one",
    "else",
    "gotten",
    "exactly",
    "gains",
    "come",
    "gains",
    "really",
    "come",
    "architecture",
    "actually",
    "throwing",
    "computer",
    "know",
    "right",
    "hope",
    "enjoyed",
    "invite",
    "check",
    "paper"
  ],
  "keywords": [
    "fix",
    "match",
    "learning",
    "consistency",
    "confidence",
    "paper",
    "data",
    "set",
    "labeled",
    "right",
    "labels",
    "much",
    "unlabeled",
    "examples",
    "know",
    "would",
    "like",
    "use",
    "really",
    "order",
    "points",
    "example",
    "something",
    "image",
    "classification",
    "take",
    "medical",
    "pictures",
    "let",
    "long",
    "whether",
    "get",
    "need",
    "first",
    "also",
    "one",
    "good",
    "label",
    "usually",
    "might",
    "able",
    "go",
    "find",
    "around",
    "images",
    "labeling",
    "pretty",
    "make",
    "two",
    "namely",
    "well",
    "basically",
    "want",
    "say",
    "loss",
    "supervised",
    "unsupervised",
    "call",
    "true",
    "kind",
    "course",
    "part",
    "going",
    "p",
    "see",
    "pipeline",
    "weekly",
    "augmented",
    "augmentation",
    "give",
    "training",
    "cat",
    "obtain",
    "random",
    "crop",
    "still",
    "point",
    "many",
    "left",
    "things",
    "augmentations",
    "weakly",
    "strongly",
    "think",
    "percent",
    "called",
    "bit",
    "model",
    "prediction",
    "pseudo",
    "strong",
    "picture",
    "recognize",
    "class",
    "play",
    "distorted",
    "thing",
    "comes",
    "augment",
    "important",
    "actually",
    "whenever",
    "seems",
    "come",
    "ninety",
    "accuracy",
    "ten",
    "fifty",
    "four",
    "per",
    "gets",
    "cool",
    "simply",
    "lot",
    "ablation",
    "study",
    "error",
    "rate",
    "schedule",
    "hyper",
    "parameters",
    "gains"
  ]
}