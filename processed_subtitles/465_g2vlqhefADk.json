{
  "text": "Today I realized that it's been a long time\nsince I made the 'how to design a convolutional\nneural network' video.\nA lot has changed since then, but also a lot\nhas stayed the same.\nSo, in this video, we will talk about how\nto design a neural network in 2020, covering\nsome of the useful techniques that came out\nor popularized between 2018 and 2020.\nAt the end of the video, I will also go through\nsome of our recent papers and explain how\nmy colleagues and I designed neural networks\nfor constrained environments.\nAlright, let's get started!\nMy first advice is still the same.\nYou don't really need to spend too much time\ntrying to design a neural network.\nYou can pick something that worked for a similar\nproblem and use it.\nBut what if your model needs to have some\nspecial properties that mainstream models\ndon't provide?\nWhat if you have some unique concerns that\nothers typically disregard or haven't researched\nyet, such as minimizing the hardware footprint\nof a model or dealing with very large multispectral\nimages.\nIn many of those cases, you still don't have\nto start from scratch.\nThere are many good practices and useful design\npatterns that you can use in your model architecture.\nThat's what we will be covering in this video.\nLet's first talk about efficient model architectures.\nDespite the recent advances in automated network\narchitecture search, hand-designed models\nare still relevant, especially when it comes\nto designing efficient models.\nFor example, the ShuffleNetV2 paper argues\nthat automatically found network architectures\nare much slower in practice even when they\nrequire a smaller number of operations to\nrun.\nTheir paper reports that MobileNetV2, a hand-designed\nmodel, is much faster than the NasNet-A, which\nwas a result of an automated network architecture\nsearch process.\nThis is because the speed of a model depends\nnot only on the number of floating-point operations\nbut also on memory access costs and platform\ncharacteristics.\nKeeping those in mind, the ShuffleNetV2 paper\nproposes some guidelines to design efficient\nmodel architectures, optimized for inference\nspeed.\nOne of their guidelines is based on the observation\nthat equal channel width at both ends of a\nlayer minimizes memory access costs.\nSo it's a good practice not to change the\nnumber of input and output channels too frequently.\nModels that make use of bottlenecks and expand-and-contract\nmodules such as SqueezeNet and MobileNet V2\nviolate this guideline.\nIt doesn't mean you should never use them,\nthough.\nSpeaking of bottlenecks, something I observed\nis that extremely narrow bottlenecks also\nhurt training stability.\nA few dead neurons and the entire model collapses.\nSo, if you really need to use layers having\nvery few filters, such as 8 or smaller, using\nlinear activation instead of ReLU at the end\nof those layers would at least prevent dead\nneurons.\nMobileNetV2 also does something similar by\nusing ReLU activations at the expansion layers\nwhile keeping the bottlenecks linear.\nAnother guideline is that network fragmentation\nreduces the degree of parallelism.\nUsing a lot of small operations instead of\na few large ones decreases efficiency since\nthey are not very GPU-friendly, and they introduce\nextra overheads.\nThis is a well-known phenomenon and is one\nof the reasons why some automatically designed\nmodels run slower.\nNetwork architecture search algorithms may\nresult in heavily fragmented architectures\nwhen accuracy and the number of operations\nare the only search criteria.\nThe paper also points out is that element-wise\noperations have a non-negligible cost.\nPoint-wise operations such as ReLU and 1x1\nconvolutions have a small number of floating-point\noperations, but their memory access cost is\nnon-negligible.\nTherefore, one shouldn't consider them free\nin model architecture design.\nAs you may know, my work mostly focuses on\nmodels that operate on image data, and I haven't\nbeen working on anything natural language\nprocessing related for the past few years.\nHowever, it's hard not to see how successful\nthe Transformer model has been in the field\nof natural language processing.\nThis new type of architecture relies on what's\ncalled `attention mechanisms.'\nAt a very high level, an attention mechanism\ntells a model where to look; what parts of\nthe input signal are more relevant.\nYou can think of it as a module that generates\na weight vector given a query.\nFor example, to resolve what 'it' refers to\nin \"This video is very interesting.\nI liked it.\", an attention vector would put\na higher weight on the words \"this\" and \"video.\"\nThis type of attention mechanism is called\nself-attention.\nIt's straightforward to see how attention\nmechanisms help in this example, but can we\napply this type of self-attention also to\nmodels that deal with computer vision problems?\nYes, we can.\nWe don't have words and sentences in images,\nbut we certainly can design mechanisms to\nshift the attention towards particular spatial\nlocations or feature maps.\nFor example, this paper, titled \"Squeeze-and-Excitation\nNetworks,\" proposes a self-attention mechanism\nthat assigns weights to feature maps based\non their relevance for a given input.\nThe way they do it is quite simple.\nFor a given layer, they first squeeze the\nfeature maps into a global description vector\nby averaging over the spatial axes.\nThis is basically a global average pooling\noperator.\nThen, they feed that information into a mini\nfully-connected neural network that outputs\nan attention vector.\nFinally, they take those weights in the attention\nvector and multiply them with their corresponding\nfeature maps at the input.\nThis process essentially puts the attention\non more relevant feature maps by recalibrating\ntheir channels.\nThis method can easily be applied to many\ntypes of convolutional neural network architectures,\nsuch as ResNet.\nMobileNetV3 also makes use of similar attention\nmodules, combining manual design practices\nwith an automated network architecture search\napproach.\nFor many computer vision tasks, it seems that\nit's a good strategy to use self-attention\nmechanisms in convolutional neural network\narchitectures.\nAt the beginning of the video, I mentioned\nusing a well-known model architecture off-the-shelf\nwould be sufficient in many cases.\nBut if you are trying to solve a problem that\nhas some specific requirements or limitations,\nthen you may need to make some task-specific\ndesign choices to adapt your model to a targeted\napplication.\nLet's go through two such examples.\nThe first one is a pixel-wise segmentation\nmodel that I designed to handle very large\ninput images.\nThe goal was to create surface water maps\ngiven satellite imagery.\nThis is a typical semantic segmentation task\nthat any mainstream pixel-wise prediction\nmodel would be expected to perform well.\nHowever, satellite imagery can be much larger,\nlike orders of magnitude larger, than images\nthat the mainstream models are designed to\ndeal with.\nSo, I needed to make some design choices to\nprocess large inputs in one shot, without\ndividing them into tiles, given a certain\nmemory budget.\nIt's a common practice to double the number\nof feature maps whenever you downscale the\ninput by two and vice versa.\nIn this setting, higher-resolution layers\nget much more memory allocation than the coarser\nscale ones.\nBecause downscaling an image by two in both\nspatial axes while doubling the number of\nchannels still reduces the overall feature\nmap volume by two.\nOne design choice I made was to quadruple\nthe number of channels whenever the feature\nmaps are spatially downscaled so that the\nmodel uses constant memory throughout the\nnetwork and layers at different levels of\nabstraction get their fair share of memory\nallocation.\nThis approach also has some downsides, but\nfor this particular task, it worked very well.\nI published a paper on this in the IEEE Geoscience\nand Remote Sensing Letters very recently.\nThere's a lot more in the paper.\nCheck it out to learn more about it.\nLet's move on to the second custom model design\nexample, in which our goal was to minimize\nthe latency and hardware footprint of our\nmodel.\nWe used several tricks to do that.\nThe main innovation in our model was the use\nof 3-way separable FIR-IIR filters for the\npurposes of line buffer minimization, and\nI'll explain what that means.\nThe concept of separable convolutions is nothing\nnew.\nMany efficient model architectures use depthwise\nseparable convolutions.\nHowever, as I mentioned in one of my earlier\nvideos, 3-way separable convolutions having\nvertical, horizontal, and depthwise components\nare fairly uncommon.\nThere is a reason for that.\nConvolutional neural networks typically use\nvery small kernel sizes, such as 3x3.\nTherefore, breaking down a spatial convolution\ninto column and row convolutions doesn't really\nsave much.\nI'm guessing that's also why popular models\ndon't use depthwise separable convolutions\nin the first layer, although I haven't seen\nit stated explicitly in the papers.\nSince the input is usually a 3-channel RGB\nimage, the number of channels is too few for\ndepthwise separability to be worth it.\nIn our case, spatial separability was very\nuseful for factorizing the hardware cost.\nThe cost of vertical convolutions was disproportionally\nhigh because of the number of lines that need\nto be buffered.\nThe hardware acquired images line-by-line.\nTherefore, a column convolution required more\nelements to be buffered.\nFor example, a 5x1 convolution would need\n4 lines of data to be buffered, whereas a\n1x5 convolution would need only 4 elements\nto be buffered.\nWe addressed this problem by replacing the\nvertical convolutions in a 3-way separable\nconvolution layer with infinite impulse response\nfilters.\nYou can think of those IIR filters as recurrent\nneural network modules that summarize pixels\nin the vertical direction.\nUnlike fixed-window convolutions, our separable\nFIR-IIR filters start processing their input\nas soon as the pixels arrive, without having\nto buffer the lines that would be spanned\nby a fixed-sized window.\nThis reduces latency and the size of the line\nbuffers, leading to significant savings in\nsilicon area.\nYou can find our paper in the description\nbelow.\nMy final tip in this video is not to follow\nany advice or guideline religiously, including\nmy own.\nThings change fast, especially in this field.\nGuidelines, rules of thumb, and design patterns\nare practical, but they don't always work\nwell.\nThings change, our understanding of things\nchange.\nSo it's better to keep an open mind.\nAlright, that's all for today.\nI hope you liked it.\nLinks are in the description, as always.\nSubscribe for more videos.\nHappy belated new year.\nThanks for watching, stay tuned, and see you\nnext time.\n",
  "words": [
    "today",
    "realized",
    "long",
    "time",
    "since",
    "made",
    "design",
    "convolutional",
    "neural",
    "network",
    "video",
    "lot",
    "changed",
    "since",
    "also",
    "lot",
    "stayed",
    "video",
    "talk",
    "design",
    "neural",
    "network",
    "2020",
    "covering",
    "useful",
    "techniques",
    "came",
    "popularized",
    "2018",
    "end",
    "video",
    "also",
    "go",
    "recent",
    "papers",
    "explain",
    "colleagues",
    "designed",
    "neural",
    "networks",
    "constrained",
    "environments",
    "alright",
    "let",
    "get",
    "started",
    "first",
    "advice",
    "still",
    "really",
    "need",
    "spend",
    "much",
    "time",
    "trying",
    "design",
    "neural",
    "network",
    "pick",
    "something",
    "worked",
    "similar",
    "problem",
    "use",
    "model",
    "needs",
    "special",
    "properties",
    "mainstream",
    "models",
    "provide",
    "unique",
    "concerns",
    "others",
    "typically",
    "disregard",
    "researched",
    "yet",
    "minimizing",
    "hardware",
    "footprint",
    "model",
    "dealing",
    "large",
    "multispectral",
    "images",
    "many",
    "cases",
    "still",
    "start",
    "scratch",
    "many",
    "good",
    "practices",
    "useful",
    "design",
    "patterns",
    "use",
    "model",
    "architecture",
    "covering",
    "video",
    "let",
    "first",
    "talk",
    "efficient",
    "model",
    "architectures",
    "despite",
    "recent",
    "advances",
    "automated",
    "network",
    "architecture",
    "search",
    "models",
    "still",
    "relevant",
    "especially",
    "comes",
    "designing",
    "efficient",
    "models",
    "example",
    "shufflenetv2",
    "paper",
    "argues",
    "automatically",
    "found",
    "network",
    "architectures",
    "much",
    "slower",
    "practice",
    "even",
    "require",
    "smaller",
    "number",
    "operations",
    "run",
    "paper",
    "reports",
    "mobilenetv2",
    "model",
    "much",
    "faster",
    "result",
    "automated",
    "network",
    "architecture",
    "search",
    "process",
    "speed",
    "model",
    "depends",
    "number",
    "operations",
    "also",
    "memory",
    "access",
    "costs",
    "platform",
    "characteristics",
    "keeping",
    "mind",
    "shufflenetv2",
    "paper",
    "proposes",
    "guidelines",
    "design",
    "efficient",
    "model",
    "architectures",
    "optimized",
    "inference",
    "speed",
    "one",
    "guidelines",
    "based",
    "observation",
    "equal",
    "channel",
    "width",
    "ends",
    "layer",
    "minimizes",
    "memory",
    "access",
    "costs",
    "good",
    "practice",
    "change",
    "number",
    "input",
    "output",
    "channels",
    "frequently",
    "models",
    "make",
    "use",
    "bottlenecks",
    "modules",
    "squeezenet",
    "mobilenet",
    "v2",
    "violate",
    "guideline",
    "mean",
    "never",
    "use",
    "though",
    "speaking",
    "bottlenecks",
    "something",
    "observed",
    "extremely",
    "narrow",
    "bottlenecks",
    "also",
    "hurt",
    "training",
    "stability",
    "dead",
    "neurons",
    "entire",
    "model",
    "collapses",
    "really",
    "need",
    "use",
    "layers",
    "filters",
    "8",
    "smaller",
    "using",
    "linear",
    "activation",
    "instead",
    "relu",
    "end",
    "layers",
    "would",
    "least",
    "prevent",
    "dead",
    "neurons",
    "mobilenetv2",
    "also",
    "something",
    "similar",
    "using",
    "relu",
    "activations",
    "expansion",
    "layers",
    "keeping",
    "bottlenecks",
    "linear",
    "another",
    "guideline",
    "network",
    "fragmentation",
    "reduces",
    "degree",
    "parallelism",
    "using",
    "lot",
    "small",
    "operations",
    "instead",
    "large",
    "ones",
    "decreases",
    "efficiency",
    "since",
    "introduce",
    "extra",
    "overheads",
    "phenomenon",
    "one",
    "reasons",
    "automatically",
    "designed",
    "models",
    "run",
    "slower",
    "network",
    "architecture",
    "search",
    "algorithms",
    "may",
    "result",
    "heavily",
    "fragmented",
    "architectures",
    "accuracy",
    "number",
    "operations",
    "search",
    "criteria",
    "paper",
    "also",
    "points",
    "operations",
    "cost",
    "operations",
    "relu",
    "1x1",
    "convolutions",
    "small",
    "number",
    "operations",
    "memory",
    "access",
    "cost",
    "therefore",
    "one",
    "consider",
    "free",
    "model",
    "architecture",
    "design",
    "may",
    "know",
    "work",
    "mostly",
    "focuses",
    "models",
    "operate",
    "image",
    "data",
    "working",
    "anything",
    "natural",
    "language",
    "processing",
    "related",
    "past",
    "years",
    "however",
    "hard",
    "see",
    "successful",
    "transformer",
    "model",
    "field",
    "natural",
    "language",
    "processing",
    "new",
    "type",
    "architecture",
    "relies",
    "called",
    "attention",
    "mechanisms",
    "high",
    "level",
    "attention",
    "mechanism",
    "tells",
    "model",
    "look",
    "parts",
    "input",
    "signal",
    "relevant",
    "think",
    "module",
    "generates",
    "weight",
    "vector",
    "given",
    "query",
    "example",
    "resolve",
    "refers",
    "video",
    "interesting",
    "liked",
    "attention",
    "vector",
    "would",
    "put",
    "higher",
    "weight",
    "words",
    "video",
    "type",
    "attention",
    "mechanism",
    "called",
    "straightforward",
    "see",
    "attention",
    "mechanisms",
    "help",
    "example",
    "apply",
    "type",
    "also",
    "models",
    "deal",
    "computer",
    "vision",
    "problems",
    "yes",
    "words",
    "sentences",
    "images",
    "certainly",
    "design",
    "mechanisms",
    "shift",
    "attention",
    "towards",
    "particular",
    "spatial",
    "locations",
    "feature",
    "maps",
    "example",
    "paper",
    "titled",
    "networks",
    "proposes",
    "mechanism",
    "assigns",
    "weights",
    "feature",
    "maps",
    "based",
    "relevance",
    "given",
    "input",
    "way",
    "quite",
    "simple",
    "given",
    "layer",
    "first",
    "squeeze",
    "feature",
    "maps",
    "global",
    "description",
    "vector",
    "averaging",
    "spatial",
    "axes",
    "basically",
    "global",
    "average",
    "pooling",
    "operator",
    "feed",
    "information",
    "mini",
    "neural",
    "network",
    "outputs",
    "attention",
    "vector",
    "finally",
    "take",
    "weights",
    "attention",
    "vector",
    "multiply",
    "corresponding",
    "feature",
    "maps",
    "input",
    "process",
    "essentially",
    "puts",
    "attention",
    "relevant",
    "feature",
    "maps",
    "recalibrating",
    "channels",
    "method",
    "easily",
    "applied",
    "many",
    "types",
    "convolutional",
    "neural",
    "network",
    "architectures",
    "resnet",
    "mobilenetv3",
    "also",
    "makes",
    "use",
    "similar",
    "attention",
    "modules",
    "combining",
    "manual",
    "design",
    "practices",
    "automated",
    "network",
    "architecture",
    "search",
    "approach",
    "many",
    "computer",
    "vision",
    "tasks",
    "seems",
    "good",
    "strategy",
    "use",
    "mechanisms",
    "convolutional",
    "neural",
    "network",
    "architectures",
    "beginning",
    "video",
    "mentioned",
    "using",
    "model",
    "architecture",
    "would",
    "sufficient",
    "many",
    "cases",
    "trying",
    "solve",
    "problem",
    "specific",
    "requirements",
    "limitations",
    "may",
    "need",
    "make",
    "design",
    "choices",
    "adapt",
    "model",
    "targeted",
    "application",
    "let",
    "go",
    "two",
    "examples",
    "first",
    "one",
    "segmentation",
    "model",
    "designed",
    "handle",
    "large",
    "input",
    "images",
    "goal",
    "create",
    "surface",
    "water",
    "maps",
    "given",
    "satellite",
    "imagery",
    "typical",
    "semantic",
    "segmentation",
    "task",
    "mainstream",
    "prediction",
    "model",
    "would",
    "expected",
    "perform",
    "well",
    "however",
    "satellite",
    "imagery",
    "much",
    "larger",
    "like",
    "orders",
    "magnitude",
    "larger",
    "images",
    "mainstream",
    "models",
    "designed",
    "deal",
    "needed",
    "make",
    "design",
    "choices",
    "process",
    "large",
    "inputs",
    "one",
    "shot",
    "without",
    "dividing",
    "tiles",
    "given",
    "certain",
    "memory",
    "budget",
    "common",
    "practice",
    "double",
    "number",
    "feature",
    "maps",
    "whenever",
    "downscale",
    "input",
    "two",
    "vice",
    "versa",
    "setting",
    "layers",
    "get",
    "much",
    "memory",
    "allocation",
    "coarser",
    "scale",
    "ones",
    "downscaling",
    "image",
    "two",
    "spatial",
    "axes",
    "doubling",
    "number",
    "channels",
    "still",
    "reduces",
    "overall",
    "feature",
    "map",
    "volume",
    "two",
    "one",
    "design",
    "choice",
    "made",
    "quadruple",
    "number",
    "channels",
    "whenever",
    "feature",
    "maps",
    "spatially",
    "downscaled",
    "model",
    "uses",
    "constant",
    "memory",
    "throughout",
    "network",
    "layers",
    "different",
    "levels",
    "abstraction",
    "get",
    "fair",
    "share",
    "memory",
    "allocation",
    "approach",
    "also",
    "downsides",
    "particular",
    "task",
    "worked",
    "well",
    "published",
    "paper",
    "ieee",
    "geoscience",
    "remote",
    "sensing",
    "letters",
    "recently",
    "lot",
    "paper",
    "check",
    "learn",
    "let",
    "move",
    "second",
    "custom",
    "model",
    "design",
    "example",
    "goal",
    "minimize",
    "latency",
    "hardware",
    "footprint",
    "model",
    "used",
    "several",
    "tricks",
    "main",
    "innovation",
    "model",
    "use",
    "separable",
    "filters",
    "purposes",
    "line",
    "buffer",
    "minimization",
    "explain",
    "means",
    "concept",
    "separable",
    "convolutions",
    "nothing",
    "new",
    "many",
    "efficient",
    "model",
    "architectures",
    "use",
    "depthwise",
    "separable",
    "convolutions",
    "however",
    "mentioned",
    "one",
    "earlier",
    "videos",
    "separable",
    "convolutions",
    "vertical",
    "horizontal",
    "depthwise",
    "components",
    "fairly",
    "uncommon",
    "reason",
    "convolutional",
    "neural",
    "networks",
    "typically",
    "use",
    "small",
    "kernel",
    "sizes",
    "3x3",
    "therefore",
    "breaking",
    "spatial",
    "convolution",
    "column",
    "row",
    "convolutions",
    "really",
    "save",
    "much",
    "guessing",
    "also",
    "popular",
    "models",
    "use",
    "depthwise",
    "separable",
    "convolutions",
    "first",
    "layer",
    "although",
    "seen",
    "stated",
    "explicitly",
    "papers",
    "since",
    "input",
    "usually",
    "rgb",
    "image",
    "number",
    "channels",
    "depthwise",
    "separability",
    "worth",
    "case",
    "spatial",
    "separability",
    "useful",
    "factorizing",
    "hardware",
    "cost",
    "cost",
    "vertical",
    "convolutions",
    "disproportionally",
    "high",
    "number",
    "lines",
    "need",
    "buffered",
    "hardware",
    "acquired",
    "images",
    "therefore",
    "column",
    "convolution",
    "required",
    "elements",
    "buffered",
    "example",
    "5x1",
    "convolution",
    "would",
    "need",
    "4",
    "lines",
    "data",
    "buffered",
    "whereas",
    "1x5",
    "convolution",
    "would",
    "need",
    "4",
    "elements",
    "buffered",
    "addressed",
    "problem",
    "replacing",
    "vertical",
    "convolutions",
    "separable",
    "convolution",
    "layer",
    "infinite",
    "impulse",
    "response",
    "filters",
    "think",
    "iir",
    "filters",
    "recurrent",
    "neural",
    "network",
    "modules",
    "summarize",
    "pixels",
    "vertical",
    "direction",
    "unlike",
    "convolutions",
    "separable",
    "filters",
    "start",
    "processing",
    "input",
    "soon",
    "pixels",
    "arrive",
    "without",
    "buffer",
    "lines",
    "would",
    "spanned",
    "window",
    "reduces",
    "latency",
    "size",
    "line",
    "buffers",
    "leading",
    "significant",
    "savings",
    "silicon",
    "area",
    "find",
    "paper",
    "description",
    "final",
    "tip",
    "video",
    "follow",
    "advice",
    "guideline",
    "religiously",
    "including",
    "things",
    "change",
    "fast",
    "especially",
    "field",
    "guidelines",
    "rules",
    "thumb",
    "design",
    "patterns",
    "practical",
    "always",
    "work",
    "well",
    "things",
    "change",
    "understanding",
    "things",
    "change",
    "better",
    "keep",
    "open",
    "mind",
    "alright",
    "today",
    "hope",
    "liked",
    "links",
    "description",
    "always",
    "subscribe",
    "videos",
    "happy",
    "belated",
    "new",
    "year",
    "thanks",
    "watching",
    "stay",
    "tuned",
    "see",
    "next",
    "time"
  ],
  "keywords": [
    "today",
    "time",
    "since",
    "made",
    "design",
    "convolutional",
    "neural",
    "network",
    "video",
    "lot",
    "also",
    "talk",
    "covering",
    "useful",
    "end",
    "go",
    "recent",
    "papers",
    "explain",
    "designed",
    "networks",
    "alright",
    "let",
    "get",
    "first",
    "advice",
    "still",
    "really",
    "need",
    "much",
    "trying",
    "something",
    "worked",
    "similar",
    "problem",
    "use",
    "model",
    "mainstream",
    "models",
    "typically",
    "hardware",
    "footprint",
    "large",
    "images",
    "many",
    "cases",
    "start",
    "good",
    "practices",
    "patterns",
    "architecture",
    "efficient",
    "architectures",
    "automated",
    "search",
    "relevant",
    "especially",
    "example",
    "shufflenetv2",
    "paper",
    "automatically",
    "slower",
    "practice",
    "smaller",
    "number",
    "operations",
    "run",
    "mobilenetv2",
    "result",
    "process",
    "speed",
    "memory",
    "access",
    "costs",
    "keeping",
    "mind",
    "proposes",
    "guidelines",
    "one",
    "based",
    "layer",
    "change",
    "input",
    "channels",
    "make",
    "bottlenecks",
    "modules",
    "guideline",
    "dead",
    "neurons",
    "layers",
    "filters",
    "using",
    "linear",
    "instead",
    "relu",
    "would",
    "reduces",
    "small",
    "ones",
    "may",
    "cost",
    "convolutions",
    "therefore",
    "work",
    "image",
    "data",
    "natural",
    "language",
    "processing",
    "however",
    "see",
    "field",
    "new",
    "type",
    "called",
    "attention",
    "mechanisms",
    "high",
    "mechanism",
    "think",
    "weight",
    "vector",
    "given",
    "liked",
    "words",
    "deal",
    "computer",
    "vision",
    "particular",
    "spatial",
    "feature",
    "maps",
    "weights",
    "global",
    "description",
    "axes",
    "approach",
    "mentioned",
    "choices",
    "two",
    "segmentation",
    "goal",
    "satellite",
    "imagery",
    "task",
    "well",
    "larger",
    "without",
    "whenever",
    "allocation",
    "latency",
    "separable",
    "line",
    "buffer",
    "depthwise",
    "videos",
    "vertical",
    "convolution",
    "column",
    "separability",
    "lines",
    "buffered",
    "elements",
    "4",
    "pixels",
    "things",
    "always"
  ]
}