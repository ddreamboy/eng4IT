{
  "text": "gpt3 has gotten a lot of attention\nrecently\nso i got curious and i want to dig a\nlittle bit deeper and understand what\nexactly\ngpt3 is how it works what it's doing\nbut also how it compares to other nlp\nmodels that i use on a pretty consistent\nand regular basis\ni'd like to share some of what i've\nlearned with you all and hopefully by\nthe end of it you'll also have a sense\nof\nhow exactly gpt3 works what it's good at\nwhat some of its limitations are\nand how it compares to alternative\nmodels and there's been a lot of models\nover the last couple of years and this\nis a good thing\nit's a good thing because we've seen a\npretty radical change in state of the\nart and natural language processing\nbut all of these models bert roberta\ngpt xlnet they all have a few things in\ncommon\nfirst of all they're all language models\nthat means at their core\nthey're looking at raw text and trying\nto learn something\njust by looking at the raw text because\npresumably the words that we use in the\nway that we arrange words\nconveys meaning so if we can train a\nmodel machine learning model to extract\nthe meaning from the words\nand the grammar then well that would be\na good thing and it turned out to work\nfor quite well\nnow the other thing that all these\nmodels have in common is that they're\nall based on transformers\nnow transformer is the the specific uh\nneural network architecture that has\nbeen implemented\nin all of these models starting from\nbird\nthrough gpt and all the ones in between\nand the idea of transformers is that\nthey use this\nthing called attention so the model\nlooks at sentences\nand it plays these games with sentences\nby looking and paying attention\nto different parts of the sentence to\ntry to understand and extract\nmeaning and insight from text so\nalthough all these models have these\nthings in common\nthere are two general families that we\ncan\nslot these models into on the one hand\nwe have the bert and the roberta and the\nalbert family of models which for\nobvious reasons i'll call the burnt\nfamily\nthese are bi-directional encoders and\nthere's a fundamental difference\nsubtle but fundamental difference\nbetween the bert family of\ntransformers the burt family models and\ngpt and xlnet and this other family\nand the second family that gpt falls\ninto they're called auto regressive\ndecoders\nnow the best way to see the difference\nis graphically\nso here's a picture which i've taken\nfrom a blog that's linked at the bottom\nit's a very good blog\nto understand how these models work i i\nhighly recommend you go there and visit\nthat\nthat blog but from a high level models\nlike burt and the burt family\nthey're shown on the left and these are\nbi-directional encoders which means\nthat when you give bert a piece of text\nbert will look at that entire piece of\ntext from left to right from right to\nleft\nand as it's looking at any one\nparticular word it will look at the\nsurrounding words for context\nso if i take the example sentence i'm\ngoing to throw a ball to the outfielder\nwell bert will take that sentence and if\ni focus in on the word\nball well when it's trying to encode or\nembed that word\nit will know that i am throwing the ball\nand it will look at the other side of\nthe word and also get that context so it\nwill know that i am going to throw that\nball to an\noutfielder now let's contrast that\nsituation\nwith the situation on the right and the\nsituation on the right this is the gbt\nstyle of family\nif you give a gpt or similar models that\nsame sentence it's not going to look at\nall the context to the left and right of\nthe word ball\nit's going to start at the beginning and\nsay i am going to throw a ball\nand then gbt tries to predict what's\ngoing to come next that's its\nlanguage learning task all it's doing is\ntrying to predict what word comes next\nand it turns out that's a pretty good\nway to learn\na language and to extract information\nfrom language\nthat's all gpt is trying to do now of\ncourse there's some downsides so\nif for that example sentence when i get\nto the word ball\ngbt doesn't know whether i'm going to\nthrow the ball to an outfielder or to a\nwide receiver or to my daughter in the\nbackyard\nit tries to guess these things and you\nknow it does a pretty good job\nbut on the other hand this is where you\nsee a lot of attention\nno pun intended around gpt3 is that it's\nreally good\nat writing it's really good at\ngenerating text because it was trained\nto guess\nthe next word and so if you give gpt a\nsentence like i'm going to throw the\nball well gpt has seen enough examples\nthat it can\nfill in the blanks it can fill in the\nrest of the sentence fill in the rest of\nthat sentence\nin several ways that makes sense and so\nagain to reiterate that's why\na lot of the attention around gbt3 has\nbeen its uncanny ability to to write\nto write things like uh news articles\neven code\nand this is pretty impressive and it's\npretty remarkable\nnow the big thing with gpt3 apart from\nits impressive\nperformance at at writing and generating\ntext\nis its size and the authors write\neach increase in model size has brought\nimprovements in text synthesis\nand or downstream nlp tasks so in other\nwords looking back at the recent history\nof transformer models\nthey've noticed that we add more\nparameters make the models bigger they\ntend to work\nbetter so effectively gpt3 is an\nexperiment\nto see if we can extrapolate off the end\nof that\ncurve and make a really big model\nand we'll see if that really big model\ndoes really well\nhow far can we take this size argument\nso to give you a sense of how big big is\nin the context of gpt3\nlet's look at a few examples i'll start\nwith bert bert had\n340 million or so parameters\nwhich is a lot of parameters and you\nknow a parameter is the the\nmost granular part of a model it's the\nsmallest bit of a model\nmore parameters the bigger model the\nmore capacity it has to learn\nnow let's look at roberta which was an\nextension of\nbird but roughly the same size about 355\nmillion or so parameters\nnow if we look ahead to gpt2 gpt2 was\nquite a bit bigger over 700 million\nparameters\nsimilar to that was t5 again between 700\nand 800 million parameters\nthese are big models already but how big\ndid gpt3 go\nhuge gpe t3 clocks in\nat 175 billion\nparameters it's a big model\nnow to show this on a slightly more\nequitable chart\nwe can go to a log scale so we're\nlooking at the number of parameters log\nnumber of parameter\nparameters on the vertical scale now but\neven here you see that gpt3 is about\nthree orders of magnitude larger\nthan the models that came before it now\nthese numbers\nit's hard to get an intuitive sense of\nhow big 175 billion is\nbut kind of an interesting thought\nexperiment is well what if i take\none parameter and equate that to a unit\ni'm more familiar with\nlike one second so if one parameter in\none of these models equates to one\nsecond\nthen both bert and roberta come out to\nbe roughly\n10 years of time equivalent\nthe next larger models next two larger\nmodels gpt2 and t5\nif one parameter represents one second\nthen those models come out to be about\n24\n25 years but now with gpt3 with 175\nbillion parameters\nthat would be over 5 000 years\nif one parameter equaled one second it's\na lot bigger\ngpd3 in fact takes supposedly 320\ngigabytes of memory\nand it costs 12 million dollars to train\nso gpd3 is a very large model but what\ndoes it actually\ndo and how is it different than the bird\nfamily now a few slides ago i showed you\nkind of the structural difference\nbetween gpt and the bird family of\ntransformers\nbut what i'd like to get into now is how\nit's\nfunctionally different perhaps even\nphilosophically or conceptually\ndifferent\nand i think one interesting way to\nunpack this question\nis simply through the titles of the\npapers that came out describing these\nmodels\nso with burp the title of the paper was\nbert pre-training of deep bi-directional\ntransformers for language understanding\ni think the key word that i want to\nfocus on here is pre-training\nthat's always been bert's mo you you\nstart off by\npre-training a generic model so this is\na transformer that understands\nlanguage well it has smart embeddings\nfor words\nbut it's not useful for anything yet you\nhave to start off with that pre-trained\nmodel but then\ntake it further and fine-tune it to do\nsome specific tasks\nso for example you might take a\npre-trained bird model but then\nfine-tune it\nto do text classification or fine-tune\nit to do question answering\nyou might fine-tune it to do sentiment\nanalysis or\nnamed entity extraction\nthe idea though is to starting with a\npre-trained generic model and\nspecializing that model in any number of\ndirections\ngpt 2 and 3 seem to have a slightly\ndifferent objective in mind however so\nthe gpt-2 paper\nsaid was titled language models are\nunsupervised\nmulti-task learners so if you\nnotice the emphasis is different i think\nthe operative word here is multi-task\nlearners\ntheir approach was to posit that a\nlanguage\nmodel could be pre-trained\nbut also be good at performing multiple\ntasks\nmore or less out of the box they were\nnot interested\nin fine-tuning gpt to be really good at\none thing or another thing\nat least not necessarily so but they\nwere more interested in the fact that\nthis could be good at several things\nwithout fine tuning and that thread was\ncarried on to gpt 3.\nthe title of the gpt3 paper was language\nmodels are few shot learners\nand so we'll get into a few shot what\nwhat few shot learning means\nbut the idea is that you have a\npre-trained model that is\nright out of the box right off the bat\ngood at\nseveral different tasks how does it do\nthis\none way to describe this is to break the\ntraining into two different parts\nthe top part which is shown in purple\nand is called the outer loop in this\ndiagram by the way is out of the gpt3\npaper\nis kind of your standard language\npre-training bert does this and gpt-3\ndoes this\nand this is learning via gradient\ndescent during unsupervised pre-training\nthis is where you're just learning the\nmodel with burt and the burt family it\ntries to fill in blanks\nthat's the game it plays to learn the\nlanguage with gpt3 as we mentioned\nearlier is trying to predict the next\nword\nbut it's still unsupervised pre-training\nit's not necessarily\ntask specific training these models\nafter completing the outer loop they're\nnot really good at anything\nin particular right well gpt\n2 and 3's position was that even\nstopping with that pre-training\nwe can follow it on with something\ncalled an inner loop\nand this is not done during training the\ninner loop is at run time\nis when you have your pre-trained model\ncan you take a model and\nshow it a few examples of what you would\nlike it to do\nand then have it learn on the fly to do\nthese different things\nso this is specific to gpt bert again as\ni said does not take this approach\nlet me show you a few more concrete\nexamples to help clarify this\nan approach like bert and the bert\nfamily of models\nif we want to translate a phrase from\nenglish to french\nwe'll have to show bert multiple\nexamples of translation from\nenglish to french and after every\nexample every batch of examples\nbert will update its model parameters\nthrough the gradient update\nand so as we're going through these\nexamples and bert\nis learning it's updating the model and\nthis model is becoming\nan english to french translation\nspecialist\nall the parameters in the model are\nadjusting themselves specifically to\nthis task\nand it can actually get pretty good at\nthat task that's burnt\ngpt 2 and 3 as i mentioned have a\ndifferent approach\nwe'll start with zero shot learning the\nidea here is can i take a gpt-3 model\nthat has just been pre-trained not\nspecialized like bert\nbut can i give it some task and just\ntell it what i want to do\nat inference time at runtime so i'm\ngoing to say hey gpt3\ni want to translate english to french so\nhere's cheese blank\nand i want gbt3 to be smart enough out\nof the box to\nknow what i wanted to do by reading what\nthe task in the input\nand then doing that automatically this\nis called zero shot learning because\ngpt3 has never seen an example of\nenglish to french translation\ni'm just telling you what i want to do\nat the input and asking it to do it\nautomatically contrast this with one\nshot learning\nwith one shot it's the same idea i'm\ngoing to take the gpt3 model and say\nat input not during training time but at\ninput time hey take this uh translate\nenglish to french\nand i'm gonna give you one example\nhere's your one shot to learn what i\nwant you to do\nsea otter maps two i'm not going to try\nto read french but there it is\nnow after gpt3 sees that one example i\nwant to\nfill in the blank in my real example\nwhich is translating cheese to french\ncan gpt3 do that if it does that's an\nexample\na successful example of one-shot\nlearning again\ni'm going to say this a couple of times\nthis is the big difference gpg3 is not\nfine-tuned to translate english to\nfrench i'm telling it what i want to do\nafter it's trained in this case giving\nit one example and expecting it to take\nit from there\nnow in reality it might take more than\none example\nso uh you'll hear the exam you'll hear\nthe phrase few shot learning\nit's the same idea at input time at\nruntime at inference time i'm going to\nsay hey gbt3 translate english to france\nfrench but now i'm going to give you a\ncouple of examples so here you see three\nexamples of english to french\ntranslation and then on the last one\ni give it the english and i ask gpt3 to\nfill in the blank it turns out\nit's pretty good at this this is a chart\nthat's\nyou know i took it out of the gpt3 paper\nthere's a lot of these kind of charts i\npicked this one just as a typical\nexample\nbut what you see is along the the bottom\naxis\nyou have the size of the model in\nbillions of parameters you know again\ntesting this\ntheory of bigger is better when it comes\nto model performance\nand so let's say all the way out here on\nthe right which is 175 billion that's\nthe largest model they trained\nthese horizontal lines at the bottom you\nsee random guessing up here you see a\ncouple of burp models and up here\nyou see kind of state of the art and\nhuman performance and all of these are\nmeasured on the super glue score which\nis an\nwhich is an nlp benchmark\nso the blue line is zero shot i don't\nshow gpth3 in examples\nand i just see how well it can do on\nthis variety of nlp tasks that's\nmeasured by super glue\nand surprisingly it does well not\nhorribly better than random guessing\nbut what's really interesting is if i\ngive it one example i give it one shot\nor a few shots which in this case is i\ngive it 32 examples\nand then i ask it to take it from there\ngpt3 does\nquite well it's competitive actually it\nbeats a little\nbit fine-tuned burnt\nand again just to reiterate this in this\ncase that fine-tuned bert was\nspecifically trained\nto do those tasks as well as it could\ngpt-3 is just a generic\npre-trained big model and then give it a\nfew examples\nand then ask to take it from there so\nthe fact that it's competitive with a\nfew shots\nis interesting and impressive\nso what tasks were actually tested\nthere's a variety of them\nyou can read about this in the paper\nit's a very well written paper\nbut gpt3 was tested on things like\nsentence and paragraph completion which\nis\nyou know you would think it would be\ngood at because that's what it was\ntrained on as well as trained how to do\nbut it was also asked to do things like\nquestion answering\nuh translation as we've seen pronoun\nreference resolution which can be\nyou know quite tricky and then some you\ncan read the list here and there's also\nsynthetic demonstrations that i think\nare\nfun and interesting so to give you an\nexample of one of the synthetic\ndemonstrations i think highlights\nuh how potentially impressive gbt3 is\nthis is an example from the paper\nbut we're going to give gpt3 a new word\na made up word a fictional word but\nwe're going to ask gpth3 to use it in a\nsentence\nso again how do i tell gpt3 that i\nwanted to do that well i have to give it\nan example first\nso in gray here is the example i give to\ngpt3\ni'm going to say hey gpd3 a watpu is a\nsmall furry animal native to tanzania an\nexample of a sentence that uses the word\nthe word wapu is we were traveling in\nafrica and we saw these very cute wapoos\nnow i continue and say now to do a far\nduddle\nmeans to jump up and down really fast an\nexample of a sentences that uses\nthe word the word far duddle is and then\ni asked gbt3 to give me an example of\nthis made-up word that i just defined\nfor it on the fly and shebt3 says\none day when i was playing tag with my\nlittle sister she got really excited and\nshe started doing these crazy far\ndoubles\nthat's pretty cool i i told it i gave it\nthe definition of this word\nat runtime and it and it gave me a very\ncoherent\nand reasonable sentence that uses the\nword correctly\nso this is an example of a couple of\nthings gpt3 was it good at it's pretty\ngood text generation\nand it's also which is one of the main\narguments of the paper\ngood at learning just from examples\nwithout fine-tuning\nso the 12 million dollar question is is\ngpt-3 a child prodigy\nor simply a parlor trick now the chop\nprodigy analogy i saw it online it's not\noriginal to me i saw on a blog somewhere\nbut the idea is\nyou know for example i'll play this game\nwith my daughter where i'll say\nfive times ten is fifty six times ten is\nsixty seven times ten is seventy\nso what's eight times ten and i wanna\nsee if she can pick up the pattern and\nextrapolate off of it\nthat's basically what gpt3 is doing\nwhich is impressive\non the other hand is this just kind of a\nneat trick\nmost of the examples that we see online\nare of gpt3\nwriting it's generating text and it\nturns it you know it is good at that it\nwas trained to generate text it was\ntrained to write text so for example\ni give it the headline of an article and\nit completes that article\nnow not all the articles it writes are\ngoing to make sense and in fact you\ndon't have to look\ntoo closely to find some holes in logic\nor just see some bad examples where\nthings don't make sense\non the other hand how often do i need a\n175 billion parameter model to write a\nnews article for me\nall things considered there's still a\nfew practical considerations\nregardless of which side of this\nyou know choice that you come down on\none of them is\nobviously the issue of size and the\nauthors acknowledge this in their paper\nthey say a limitation associated with\nmodels at the scale of gpt-3 regardless\nof what you're trying to do\nis that they're both expensive and\ninconvenient to perform inference on\nso yeah with a model this size i'm going\nto need\nmultiple servers i'm going to have to\ndistribute the model across multiple\ngpus it's going to be a headache to run\njobs on and not to mention just the\nexpense of\neither buying or renting that hardware\nand it's a huge model so it's going to\ntake a long time to even run data\nthrough it and so\nit's it's big but that makes it all so\nunwieldy\nand the last thing that i would mention\nand i think this is\none of the big issues for me i do think\ngpt3 is very impressive but\nis it what i need and the answer is\nprobably no\nin my day-to-day work i want a model\nthat's really good\nusually at doing one particular thing if\ni want to do text classification\ni want to take a model and adapt it to\nthe domain that i'm working in\nand have that model be the best text\nclassification model for that domain\nthat i can come up with\nand well that's bert or at least the\nbert family of models\ni can fine tune it on the specific\nlanguage domain specific language that\ni'm interested in\nand have it be really good at the\nspecific text classification tasks that\ni'm interested in\ni don't need an all-purpose model and in\nfact going to something like gpt3\ni would have to tell it i would have to\nrepeat the instructions i have to give\nexamples of text classification\nevery time that i wanted to to classify\ntext and it wouldn't be as good as a\nspecial purpose model built just for\nthat\ncase and again the authors acknowledge\nthis in the paper\nyou know they say that you know it's\nreally impossible to have\nat least at this point uh a language\nsystem that's\nthe best at everything now on the other\nhand although gpt3 may not be a useful\nmodel\nfor me on a day-to-day basis i still\nthink that gb83 is very interesting and\ni think it's an important\nvery important uh piece of work that was\nperformed\nit's it's asking the question of what\nhappens if we build a really\nbig model where is that edge point have\nwe detected it yet\ni don't know but i think this is an\nimportant step in learning more about\nnatural language processing and learning\nmore about\nyou know transformer models and what\nthey're capable of i think there's a lot\nhere to learn\nat an academic level at an intellectual\nlevel even if it's not\na day-to-day tool that i would use so\nthat's it for me guys i hope that you\nhave learned something\nif you're still watching uh if you have\nany comments or feedback i'd love to\nhear from you and\ni hope it was useful or informative\nthanks\nyou\n",
  "words": [
    "gpt3",
    "gotten",
    "lot",
    "attention",
    "recently",
    "got",
    "curious",
    "want",
    "dig",
    "little",
    "bit",
    "deeper",
    "understand",
    "exactly",
    "gpt3",
    "works",
    "also",
    "compares",
    "nlp",
    "models",
    "use",
    "pretty",
    "consistent",
    "regular",
    "basis",
    "like",
    "share",
    "learned",
    "hopefully",
    "end",
    "also",
    "sense",
    "exactly",
    "gpt3",
    "works",
    "good",
    "limitations",
    "compares",
    "alternative",
    "models",
    "lot",
    "models",
    "last",
    "couple",
    "years",
    "good",
    "thing",
    "good",
    "thing",
    "seen",
    "pretty",
    "radical",
    "change",
    "state",
    "art",
    "natural",
    "language",
    "processing",
    "models",
    "bert",
    "roberta",
    "gpt",
    "xlnet",
    "things",
    "common",
    "first",
    "language",
    "models",
    "means",
    "core",
    "looking",
    "raw",
    "text",
    "trying",
    "learn",
    "something",
    "looking",
    "raw",
    "text",
    "presumably",
    "words",
    "use",
    "way",
    "arrange",
    "words",
    "conveys",
    "meaning",
    "train",
    "model",
    "machine",
    "learning",
    "model",
    "extract",
    "meaning",
    "words",
    "grammar",
    "well",
    "would",
    "good",
    "thing",
    "turned",
    "work",
    "quite",
    "well",
    "thing",
    "models",
    "common",
    "based",
    "transformers",
    "transformer",
    "specific",
    "uh",
    "neural",
    "network",
    "architecture",
    "implemented",
    "models",
    "starting",
    "bird",
    "gpt",
    "ones",
    "idea",
    "transformers",
    "use",
    "thing",
    "called",
    "attention",
    "model",
    "looks",
    "sentences",
    "plays",
    "games",
    "sentences",
    "looking",
    "paying",
    "attention",
    "different",
    "parts",
    "sentence",
    "try",
    "understand",
    "extract",
    "meaning",
    "insight",
    "text",
    "although",
    "models",
    "things",
    "common",
    "two",
    "general",
    "families",
    "slot",
    "models",
    "one",
    "hand",
    "bert",
    "roberta",
    "albert",
    "family",
    "models",
    "obvious",
    "reasons",
    "call",
    "burnt",
    "family",
    "encoders",
    "fundamental",
    "difference",
    "subtle",
    "fundamental",
    "difference",
    "bert",
    "family",
    "transformers",
    "burt",
    "family",
    "models",
    "gpt",
    "xlnet",
    "family",
    "second",
    "family",
    "gpt",
    "falls",
    "called",
    "auto",
    "regressive",
    "decoders",
    "best",
    "way",
    "see",
    "difference",
    "graphically",
    "picture",
    "taken",
    "blog",
    "linked",
    "bottom",
    "good",
    "blog",
    "understand",
    "models",
    "work",
    "highly",
    "recommend",
    "go",
    "visit",
    "blog",
    "high",
    "level",
    "models",
    "like",
    "burt",
    "burt",
    "family",
    "shown",
    "left",
    "encoders",
    "means",
    "give",
    "bert",
    "piece",
    "text",
    "bert",
    "look",
    "entire",
    "piece",
    "text",
    "left",
    "right",
    "right",
    "left",
    "looking",
    "one",
    "particular",
    "word",
    "look",
    "surrounding",
    "words",
    "context",
    "take",
    "example",
    "sentence",
    "going",
    "throw",
    "ball",
    "outfielder",
    "well",
    "bert",
    "take",
    "sentence",
    "focus",
    "word",
    "ball",
    "well",
    "trying",
    "encode",
    "embed",
    "word",
    "know",
    "throwing",
    "ball",
    "look",
    "side",
    "word",
    "also",
    "get",
    "context",
    "know",
    "going",
    "throw",
    "ball",
    "outfielder",
    "let",
    "contrast",
    "situation",
    "situation",
    "right",
    "situation",
    "right",
    "gbt",
    "style",
    "family",
    "give",
    "gpt",
    "similar",
    "models",
    "sentence",
    "going",
    "look",
    "context",
    "left",
    "right",
    "word",
    "ball",
    "going",
    "start",
    "beginning",
    "say",
    "going",
    "throw",
    "ball",
    "gbt",
    "tries",
    "predict",
    "going",
    "come",
    "next",
    "language",
    "learning",
    "task",
    "trying",
    "predict",
    "word",
    "comes",
    "next",
    "turns",
    "pretty",
    "good",
    "way",
    "learn",
    "language",
    "extract",
    "information",
    "language",
    "gpt",
    "trying",
    "course",
    "downsides",
    "example",
    "sentence",
    "get",
    "word",
    "ball",
    "gbt",
    "know",
    "whether",
    "going",
    "throw",
    "ball",
    "outfielder",
    "wide",
    "receiver",
    "daughter",
    "backyard",
    "tries",
    "guess",
    "things",
    "know",
    "pretty",
    "good",
    "job",
    "hand",
    "see",
    "lot",
    "attention",
    "pun",
    "intended",
    "around",
    "gpt3",
    "really",
    "good",
    "writing",
    "really",
    "good",
    "generating",
    "text",
    "trained",
    "guess",
    "next",
    "word",
    "give",
    "gpt",
    "sentence",
    "like",
    "going",
    "throw",
    "ball",
    "well",
    "gpt",
    "seen",
    "enough",
    "examples",
    "fill",
    "blanks",
    "fill",
    "rest",
    "sentence",
    "fill",
    "rest",
    "sentence",
    "several",
    "ways",
    "makes",
    "sense",
    "reiterate",
    "lot",
    "attention",
    "around",
    "gbt3",
    "uncanny",
    "ability",
    "write",
    "write",
    "things",
    "like",
    "uh",
    "news",
    "articles",
    "even",
    "code",
    "pretty",
    "impressive",
    "pretty",
    "remarkable",
    "big",
    "thing",
    "gpt3",
    "apart",
    "impressive",
    "performance",
    "writing",
    "generating",
    "text",
    "size",
    "authors",
    "write",
    "increase",
    "model",
    "size",
    "brought",
    "improvements",
    "text",
    "synthesis",
    "downstream",
    "nlp",
    "tasks",
    "words",
    "looking",
    "back",
    "recent",
    "history",
    "transformer",
    "models",
    "noticed",
    "add",
    "parameters",
    "make",
    "models",
    "bigger",
    "tend",
    "work",
    "better",
    "effectively",
    "gpt3",
    "experiment",
    "see",
    "extrapolate",
    "end",
    "curve",
    "make",
    "really",
    "big",
    "model",
    "see",
    "really",
    "big",
    "model",
    "really",
    "well",
    "far",
    "take",
    "size",
    "argument",
    "give",
    "sense",
    "big",
    "big",
    "context",
    "gpt3",
    "let",
    "look",
    "examples",
    "start",
    "bert",
    "bert",
    "340",
    "million",
    "parameters",
    "lot",
    "parameters",
    "know",
    "parameter",
    "granular",
    "part",
    "model",
    "smallest",
    "bit",
    "model",
    "parameters",
    "bigger",
    "model",
    "capacity",
    "learn",
    "let",
    "look",
    "roberta",
    "extension",
    "bird",
    "roughly",
    "size",
    "355",
    "million",
    "parameters",
    "look",
    "ahead",
    "gpt2",
    "gpt2",
    "quite",
    "bit",
    "bigger",
    "700",
    "million",
    "parameters",
    "similar",
    "t5",
    "700",
    "800",
    "million",
    "parameters",
    "big",
    "models",
    "already",
    "big",
    "gpt3",
    "go",
    "huge",
    "gpe",
    "t3",
    "clocks",
    "175",
    "billion",
    "parameters",
    "big",
    "model",
    "show",
    "slightly",
    "equitable",
    "chart",
    "go",
    "log",
    "scale",
    "looking",
    "number",
    "parameters",
    "log",
    "number",
    "parameter",
    "parameters",
    "vertical",
    "scale",
    "even",
    "see",
    "gpt3",
    "three",
    "orders",
    "magnitude",
    "larger",
    "models",
    "came",
    "numbers",
    "hard",
    "get",
    "intuitive",
    "sense",
    "big",
    "175",
    "billion",
    "kind",
    "interesting",
    "thought",
    "experiment",
    "well",
    "take",
    "one",
    "parameter",
    "equate",
    "unit",
    "familiar",
    "like",
    "one",
    "second",
    "one",
    "parameter",
    "one",
    "models",
    "equates",
    "one",
    "second",
    "bert",
    "roberta",
    "come",
    "roughly",
    "10",
    "years",
    "time",
    "equivalent",
    "next",
    "larger",
    "models",
    "next",
    "two",
    "larger",
    "models",
    "gpt2",
    "t5",
    "one",
    "parameter",
    "represents",
    "one",
    "second",
    "models",
    "come",
    "24",
    "25",
    "years",
    "gpt3",
    "175",
    "billion",
    "parameters",
    "would",
    "5",
    "000",
    "years",
    "one",
    "parameter",
    "equaled",
    "one",
    "second",
    "lot",
    "bigger",
    "gpd3",
    "fact",
    "takes",
    "supposedly",
    "320",
    "gigabytes",
    "memory",
    "costs",
    "12",
    "million",
    "dollars",
    "train",
    "gpd3",
    "large",
    "model",
    "actually",
    "different",
    "bird",
    "family",
    "slides",
    "ago",
    "showed",
    "kind",
    "structural",
    "difference",
    "gpt",
    "bird",
    "family",
    "transformers",
    "like",
    "get",
    "functionally",
    "different",
    "perhaps",
    "even",
    "philosophically",
    "conceptually",
    "different",
    "think",
    "one",
    "interesting",
    "way",
    "unpack",
    "question",
    "simply",
    "titles",
    "papers",
    "came",
    "describing",
    "models",
    "burp",
    "title",
    "paper",
    "bert",
    "deep",
    "transformers",
    "language",
    "understanding",
    "think",
    "key",
    "word",
    "want",
    "focus",
    "always",
    "bert",
    "mo",
    "start",
    "generic",
    "model",
    "transformer",
    "understands",
    "language",
    "well",
    "smart",
    "embeddings",
    "words",
    "useful",
    "anything",
    "yet",
    "start",
    "model",
    "take",
    "specific",
    "tasks",
    "example",
    "might",
    "take",
    "bird",
    "model",
    "text",
    "classification",
    "question",
    "answering",
    "might",
    "sentiment",
    "analysis",
    "named",
    "entity",
    "extraction",
    "idea",
    "though",
    "starting",
    "generic",
    "model",
    "specializing",
    "model",
    "number",
    "directions",
    "gpt",
    "2",
    "3",
    "seem",
    "slightly",
    "different",
    "objective",
    "mind",
    "however",
    "paper",
    "said",
    "titled",
    "language",
    "models",
    "unsupervised",
    "learners",
    "notice",
    "emphasis",
    "different",
    "think",
    "operative",
    "word",
    "learners",
    "approach",
    "posit",
    "language",
    "model",
    "could",
    "also",
    "good",
    "performing",
    "multiple",
    "tasks",
    "less",
    "box",
    "interested",
    "gpt",
    "really",
    "good",
    "one",
    "thing",
    "another",
    "thing",
    "least",
    "necessarily",
    "interested",
    "fact",
    "could",
    "good",
    "several",
    "things",
    "without",
    "fine",
    "tuning",
    "thread",
    "carried",
    "gpt",
    "title",
    "gpt3",
    "paper",
    "language",
    "models",
    "shot",
    "learners",
    "get",
    "shot",
    "shot",
    "learning",
    "means",
    "idea",
    "model",
    "right",
    "box",
    "right",
    "bat",
    "good",
    "several",
    "different",
    "tasks",
    "one",
    "way",
    "describe",
    "break",
    "training",
    "two",
    "different",
    "parts",
    "top",
    "part",
    "shown",
    "purple",
    "called",
    "outer",
    "loop",
    "diagram",
    "way",
    "gpt3",
    "paper",
    "kind",
    "standard",
    "language",
    "bert",
    "learning",
    "via",
    "gradient",
    "descent",
    "unsupervised",
    "learning",
    "model",
    "burt",
    "burt",
    "family",
    "tries",
    "fill",
    "blanks",
    "game",
    "plays",
    "learn",
    "language",
    "gpt3",
    "mentioned",
    "earlier",
    "trying",
    "predict",
    "next",
    "word",
    "still",
    "unsupervised",
    "necessarily",
    "task",
    "specific",
    "training",
    "models",
    "completing",
    "outer",
    "loop",
    "really",
    "good",
    "anything",
    "particular",
    "right",
    "well",
    "gpt",
    "2",
    "3",
    "position",
    "even",
    "stopping",
    "follow",
    "something",
    "called",
    "inner",
    "loop",
    "done",
    "training",
    "inner",
    "loop",
    "run",
    "time",
    "model",
    "take",
    "model",
    "show",
    "examples",
    "would",
    "like",
    "learn",
    "fly",
    "different",
    "things",
    "specific",
    "gpt",
    "bert",
    "said",
    "take",
    "approach",
    "let",
    "show",
    "concrete",
    "examples",
    "help",
    "clarify",
    "approach",
    "like",
    "bert",
    "bert",
    "family",
    "models",
    "want",
    "translate",
    "phrase",
    "english",
    "french",
    "show",
    "bert",
    "multiple",
    "examples",
    "translation",
    "english",
    "french",
    "every",
    "example",
    "every",
    "batch",
    "examples",
    "bert",
    "update",
    "model",
    "parameters",
    "gradient",
    "update",
    "going",
    "examples",
    "bert",
    "learning",
    "updating",
    "model",
    "model",
    "becoming",
    "english",
    "french",
    "translation",
    "specialist",
    "parameters",
    "model",
    "adjusting",
    "specifically",
    "task",
    "actually",
    "get",
    "pretty",
    "good",
    "task",
    "burnt",
    "gpt",
    "2",
    "3",
    "mentioned",
    "different",
    "approach",
    "start",
    "zero",
    "shot",
    "learning",
    "idea",
    "take",
    "model",
    "specialized",
    "like",
    "bert",
    "give",
    "task",
    "tell",
    "want",
    "inference",
    "time",
    "runtime",
    "going",
    "say",
    "hey",
    "gpt3",
    "want",
    "translate",
    "english",
    "french",
    "cheese",
    "blank",
    "want",
    "gbt3",
    "smart",
    "enough",
    "box",
    "know",
    "wanted",
    "reading",
    "task",
    "input",
    "automatically",
    "called",
    "zero",
    "shot",
    "learning",
    "gpt3",
    "never",
    "seen",
    "example",
    "english",
    "french",
    "translation",
    "telling",
    "want",
    "input",
    "asking",
    "automatically",
    "contrast",
    "one",
    "shot",
    "learning",
    "one",
    "shot",
    "idea",
    "going",
    "take",
    "gpt3",
    "model",
    "say",
    "input",
    "training",
    "time",
    "input",
    "time",
    "hey",
    "take",
    "uh",
    "translate",
    "english",
    "french",
    "gon",
    "na",
    "give",
    "one",
    "example",
    "one",
    "shot",
    "learn",
    "want",
    "sea",
    "otter",
    "maps",
    "two",
    "going",
    "try",
    "read",
    "french",
    "gpt3",
    "sees",
    "one",
    "example",
    "want",
    "fill",
    "blank",
    "real",
    "example",
    "translating",
    "cheese",
    "french",
    "gpt3",
    "example",
    "successful",
    "example",
    "learning",
    "going",
    "say",
    "couple",
    "times",
    "big",
    "difference",
    "gpg3",
    "translate",
    "english",
    "french",
    "telling",
    "want",
    "trained",
    "case",
    "giving",
    "one",
    "example",
    "expecting",
    "take",
    "reality",
    "might",
    "take",
    "one",
    "example",
    "uh",
    "hear",
    "exam",
    "hear",
    "phrase",
    "shot",
    "learning",
    "idea",
    "input",
    "time",
    "runtime",
    "inference",
    "time",
    "going",
    "say",
    "hey",
    "gbt3",
    "translate",
    "english",
    "france",
    "french",
    "going",
    "give",
    "couple",
    "examples",
    "see",
    "three",
    "examples",
    "english",
    "french",
    "translation",
    "last",
    "one",
    "give",
    "english",
    "ask",
    "gpt3",
    "fill",
    "blank",
    "turns",
    "pretty",
    "good",
    "chart",
    "know",
    "took",
    "gpt3",
    "paper",
    "lot",
    "kind",
    "charts",
    "picked",
    "one",
    "typical",
    "example",
    "see",
    "along",
    "bottom",
    "axis",
    "size",
    "model",
    "billions",
    "parameters",
    "know",
    "testing",
    "theory",
    "bigger",
    "better",
    "comes",
    "model",
    "performance",
    "let",
    "say",
    "way",
    "right",
    "175",
    "billion",
    "largest",
    "model",
    "trained",
    "horizontal",
    "lines",
    "bottom",
    "see",
    "random",
    "guessing",
    "see",
    "couple",
    "burp",
    "models",
    "see",
    "kind",
    "state",
    "art",
    "human",
    "performance",
    "measured",
    "super",
    "glue",
    "score",
    "nlp",
    "benchmark",
    "blue",
    "line",
    "zero",
    "shot",
    "show",
    "gpth3",
    "examples",
    "see",
    "well",
    "variety",
    "nlp",
    "tasks",
    "measured",
    "super",
    "glue",
    "surprisingly",
    "well",
    "horribly",
    "better",
    "random",
    "guessing",
    "really",
    "interesting",
    "give",
    "one",
    "example",
    "give",
    "one",
    "shot",
    "shots",
    "case",
    "give",
    "32",
    "examples",
    "ask",
    "take",
    "gpt3",
    "quite",
    "well",
    "competitive",
    "actually",
    "beats",
    "little",
    "bit",
    "burnt",
    "reiterate",
    "case",
    "bert",
    "specifically",
    "trained",
    "tasks",
    "well",
    "could",
    "generic",
    "big",
    "model",
    "give",
    "examples",
    "ask",
    "take",
    "fact",
    "competitive",
    "shots",
    "interesting",
    "impressive",
    "tasks",
    "actually",
    "tested",
    "variety",
    "read",
    "paper",
    "well",
    "written",
    "paper",
    "gpt3",
    "tested",
    "things",
    "like",
    "sentence",
    "paragraph",
    "completion",
    "know",
    "would",
    "think",
    "would",
    "good",
    "trained",
    "well",
    "trained",
    "also",
    "asked",
    "things",
    "like",
    "question",
    "answering",
    "uh",
    "translation",
    "seen",
    "pronoun",
    "reference",
    "resolution",
    "know",
    "quite",
    "tricky",
    "read",
    "list",
    "also",
    "synthetic",
    "demonstrations",
    "think",
    "fun",
    "interesting",
    "give",
    "example",
    "one",
    "synthetic",
    "demonstrations",
    "think",
    "highlights",
    "uh",
    "potentially",
    "impressive",
    "gbt3",
    "example",
    "paper",
    "going",
    "give",
    "gpt3",
    "new",
    "word",
    "made",
    "word",
    "fictional",
    "word",
    "going",
    "ask",
    "gpth3",
    "use",
    "sentence",
    "tell",
    "gpt3",
    "wanted",
    "well",
    "give",
    "example",
    "first",
    "gray",
    "example",
    "give",
    "gpt3",
    "going",
    "say",
    "hey",
    "gpd3",
    "watpu",
    "small",
    "furry",
    "animal",
    "native",
    "tanzania",
    "example",
    "sentence",
    "uses",
    "word",
    "word",
    "wapu",
    "traveling",
    "africa",
    "saw",
    "cute",
    "wapoos",
    "continue",
    "say",
    "far",
    "duddle",
    "means",
    "jump",
    "really",
    "fast",
    "example",
    "sentences",
    "uses",
    "word",
    "word",
    "far",
    "duddle",
    "asked",
    "gbt3",
    "give",
    "example",
    "word",
    "defined",
    "fly",
    "shebt3",
    "says",
    "one",
    "day",
    "playing",
    "tag",
    "little",
    "sister",
    "got",
    "really",
    "excited",
    "started",
    "crazy",
    "far",
    "doubles",
    "pretty",
    "cool",
    "told",
    "gave",
    "definition",
    "word",
    "runtime",
    "gave",
    "coherent",
    "reasonable",
    "sentence",
    "uses",
    "word",
    "correctly",
    "example",
    "couple",
    "things",
    "gpt3",
    "good",
    "pretty",
    "good",
    "text",
    "generation",
    "also",
    "one",
    "main",
    "arguments",
    "paper",
    "good",
    "learning",
    "examples",
    "without",
    "12",
    "million",
    "dollar",
    "question",
    "child",
    "prodigy",
    "simply",
    "parlor",
    "trick",
    "chop",
    "prodigy",
    "analogy",
    "saw",
    "online",
    "original",
    "saw",
    "blog",
    "somewhere",
    "idea",
    "know",
    "example",
    "play",
    "game",
    "daughter",
    "say",
    "five",
    "times",
    "ten",
    "fifty",
    "six",
    "times",
    "ten",
    "sixty",
    "seven",
    "times",
    "ten",
    "seventy",
    "eight",
    "times",
    "ten",
    "wan",
    "na",
    "see",
    "pick",
    "pattern",
    "extrapolate",
    "basically",
    "gpt3",
    "impressive",
    "hand",
    "kind",
    "neat",
    "trick",
    "examples",
    "see",
    "online",
    "gpt3",
    "writing",
    "generating",
    "text",
    "turns",
    "know",
    "good",
    "trained",
    "generate",
    "text",
    "trained",
    "write",
    "text",
    "example",
    "give",
    "headline",
    "article",
    "completes",
    "article",
    "articles",
    "writes",
    "going",
    "make",
    "sense",
    "fact",
    "look",
    "closely",
    "find",
    "holes",
    "logic",
    "see",
    "bad",
    "examples",
    "things",
    "make",
    "sense",
    "hand",
    "often",
    "need",
    "175",
    "billion",
    "parameter",
    "model",
    "write",
    "news",
    "article",
    "things",
    "considered",
    "still",
    "practical",
    "considerations",
    "regardless",
    "side",
    "know",
    "choice",
    "come",
    "one",
    "obviously",
    "issue",
    "size",
    "authors",
    "acknowledge",
    "paper",
    "say",
    "limitation",
    "associated",
    "models",
    "scale",
    "regardless",
    "trying",
    "expensive",
    "inconvenient",
    "perform",
    "inference",
    "yeah",
    "model",
    "size",
    "going",
    "need",
    "multiple",
    "servers",
    "going",
    "distribute",
    "model",
    "across",
    "multiple",
    "gpus",
    "going",
    "headache",
    "run",
    "jobs",
    "mention",
    "expense",
    "either",
    "buying",
    "renting",
    "hardware",
    "huge",
    "model",
    "going",
    "take",
    "long",
    "time",
    "even",
    "run",
    "data",
    "big",
    "makes",
    "unwieldy",
    "last",
    "thing",
    "would",
    "mention",
    "think",
    "one",
    "big",
    "issues",
    "think",
    "gpt3",
    "impressive",
    "need",
    "answer",
    "probably",
    "work",
    "want",
    "model",
    "really",
    "good",
    "usually",
    "one",
    "particular",
    "thing",
    "want",
    "text",
    "classification",
    "want",
    "take",
    "model",
    "adapt",
    "domain",
    "working",
    "model",
    "best",
    "text",
    "classification",
    "model",
    "domain",
    "come",
    "well",
    "bert",
    "least",
    "bert",
    "family",
    "models",
    "fine",
    "tune",
    "specific",
    "language",
    "domain",
    "specific",
    "language",
    "interested",
    "really",
    "good",
    "specific",
    "text",
    "classification",
    "tasks",
    "interested",
    "need",
    "model",
    "fact",
    "going",
    "something",
    "like",
    "gpt3",
    "would",
    "tell",
    "would",
    "repeat",
    "instructions",
    "give",
    "examples",
    "text",
    "classification",
    "every",
    "time",
    "wanted",
    "classify",
    "text",
    "would",
    "good",
    "special",
    "purpose",
    "model",
    "built",
    "case",
    "authors",
    "acknowledge",
    "paper",
    "know",
    "say",
    "know",
    "really",
    "impossible",
    "least",
    "point",
    "uh",
    "language",
    "system",
    "best",
    "everything",
    "hand",
    "although",
    "gpt3",
    "may",
    "useful",
    "model",
    "basis",
    "still",
    "think",
    "gb83",
    "interesting",
    "think",
    "important",
    "important",
    "uh",
    "piece",
    "work",
    "performed",
    "asking",
    "question",
    "happens",
    "build",
    "really",
    "big",
    "model",
    "edge",
    "point",
    "detected",
    "yet",
    "know",
    "think",
    "important",
    "step",
    "learning",
    "natural",
    "language",
    "processing",
    "learning",
    "know",
    "transformer",
    "models",
    "capable",
    "think",
    "lot",
    "learn",
    "academic",
    "level",
    "intellectual",
    "level",
    "even",
    "tool",
    "would",
    "use",
    "guys",
    "hope",
    "learned",
    "something",
    "still",
    "watching",
    "uh",
    "comments",
    "feedback",
    "love",
    "hear",
    "hope",
    "useful",
    "informative",
    "thanks"
  ],
  "keywords": [
    "gpt3",
    "lot",
    "attention",
    "want",
    "little",
    "bit",
    "understand",
    "also",
    "nlp",
    "models",
    "use",
    "pretty",
    "like",
    "sense",
    "good",
    "last",
    "couple",
    "years",
    "thing",
    "seen",
    "language",
    "bert",
    "roberta",
    "gpt",
    "things",
    "common",
    "means",
    "looking",
    "text",
    "trying",
    "learn",
    "something",
    "words",
    "way",
    "meaning",
    "model",
    "learning",
    "extract",
    "well",
    "would",
    "work",
    "quite",
    "transformers",
    "transformer",
    "specific",
    "uh",
    "bird",
    "idea",
    "called",
    "sentences",
    "different",
    "sentence",
    "two",
    "one",
    "hand",
    "family",
    "burnt",
    "difference",
    "burt",
    "second",
    "best",
    "see",
    "blog",
    "bottom",
    "go",
    "level",
    "left",
    "give",
    "piece",
    "look",
    "right",
    "particular",
    "word",
    "context",
    "take",
    "example",
    "going",
    "throw",
    "ball",
    "outfielder",
    "know",
    "get",
    "let",
    "situation",
    "gbt",
    "start",
    "say",
    "tries",
    "predict",
    "come",
    "next",
    "task",
    "turns",
    "really",
    "writing",
    "generating",
    "trained",
    "examples",
    "fill",
    "several",
    "gbt3",
    "write",
    "even",
    "impressive",
    "big",
    "performance",
    "size",
    "authors",
    "tasks",
    "parameters",
    "make",
    "bigger",
    "better",
    "far",
    "million",
    "parameter",
    "gpt2",
    "175",
    "billion",
    "show",
    "scale",
    "number",
    "larger",
    "kind",
    "interesting",
    "time",
    "gpd3",
    "fact",
    "actually",
    "think",
    "question",
    "paper",
    "generic",
    "useful",
    "might",
    "classification",
    "2",
    "3",
    "unsupervised",
    "learners",
    "approach",
    "could",
    "multiple",
    "box",
    "interested",
    "least",
    "shot",
    "training",
    "loop",
    "still",
    "run",
    "translate",
    "english",
    "french",
    "translation",
    "every",
    "zero",
    "tell",
    "inference",
    "runtime",
    "hey",
    "blank",
    "wanted",
    "input",
    "read",
    "times",
    "case",
    "hear",
    "ask",
    "uses",
    "saw",
    "ten",
    "article",
    "need",
    "domain",
    "important"
  ]
}