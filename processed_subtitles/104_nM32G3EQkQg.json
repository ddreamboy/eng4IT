{
  "text": "hello guys welcome back in this video\nlet us see in detail about\ntokenization now let's have a question\nwhy do we need this tokenization so the\nrequirement of this tokenization is\nlet's say I want to perform any task\nlike summarizing the text question and\nanswering from the model I'll provide a\ncontext and I'd like to ask some\nquestions to the model to get the\nresponse related to the question so like\nthat for question and\nanswering and if You observe for\nsentiment analysis\nalso and so on we can able to\nperform multiple\ntasks by using any one of the NLP model\nby using any one of the natural language\nprocessing model we can able to perform\nthese tasks by using any one of the NLP\nmodel then in order to perform any one\nof the task based on my requirement we\nneed to pass our data or\ntext or our content to our model right\nyes so while passing what type of data\nour model is expecting that type of data\nwe need to\npass any model will only accept\nnumerical representations only as an\ninput so now in order to perform these\ntasks in real time by using any one of\nthe NLP model I am having a requirement\nto convert my input to numerical\nrepresentation because my input how it\nwill be actually in the form of sentence\nthat is nothing but text right yes\nlet's take an example so let's say I\nwant to analyze the sentiment of the\ntext by passing the input then here so\nwhat is the first task I need to do I\nneed to pass my input right yes so how\ncan I able to pass my input to any one\nof the natural language processing model\nso that we're going to understood in\nthis video so what is the text so let\nlet's take a sentence\nas let's take an example sentence that\nis let's say I am a developer I'm a\ndeveloper or I want to become a\ndeveloper I want to become a developer\nlike this I want to send this input I\nwant to send or I want to provide this\nsentence to any one of the ml model then\nhow can I able to provide then can I\nsend this sentence directly to my model\nno if I send directly this sentence to\nmy model then my model do not able to\nunderstand that sentence because it only\nknow the numerical representations and\nit only understood these numerical\nrepresentations whatever the sentence\nthat I have sent in the form of a text\nthat our model will not\nunderstood and here one more important\npoint is if I send any numerical\nrepresentation instead of the sentence\nthen my model need to understood that\nparticular numerical representation\nrepresenting this\nsentence how can my model able to\nunderstood that numerical representation\nrepresents the\nsentence so first initially that data\nneed to be there right yes\nso in order to understood a numerical\nrepresentation belongs to particular\nword or particular sentence or\nparticular character first that data\nthat vocabulary data need to be stored\nin that model right yes so let's say if\nI assign one to I and if I assign two to\nthis whole sentence like want the\nsentence\nthen I know that so this one represents\nthis I this two represents this W and\nspace before it how actually like that\nin vocabulary way how our model will\nactually store that data or in what\nvocabulary our model is using in order\nto identify our sentence from numerical\nwords so let's first that so that we can\nable to further convert the sentence\ninto that numerical representation and\nthen we can able to pass that data to\nour model so let's first observe what\nvocabulary a model is using in order to\nunderstood numerical in order to\nunderstood our sentence based on the\nnumerical representation so how can you\nhow can we able to get that vocabulary\nsir we already have a library that\nLibrary is\nTransformers so in this Library actually\nwe\nhave we\nhave tokenizers that is Auto\ntokenization auto tokenizer is there you\ncan see suggesting yeah this Auto\ntokenizer is already present in this\nTransformers Library by using this Auto\ntokenizer Library by using this Auto\ntokenizer you can able\nto actually tokenize a sentence that is\nyou can able to convert this sentence\ninto numerical\nrepresentation based on the model that\nyou\nuse you can use this tokenizer that is\nauto tokenizer to convert this to\nrespective numerical representation\nbecause each and every model have their\nown tokenizers\nso so before passing any sentence input\nto our model we need to convert our\nsentence to\ntokenization by using that particular\ntokenizer only because that tokenizer\nwill convert our sentence into numerical\nrepresentation so don't worry we'll\nunderstood how the vocabulary stored for\nthat particular model in the tokenizer\nas well\nby using this Auto tokenizer you can\nable to load a tokenizer or you can say\nyou can able to load any model related\ntokenizer let's say I want to load a\ntokenizer related\nto B model then I can able to use this\nthe this Auto tokenizer you can able to\nuse Auto tokenizer Dot from pre-trained\nfrom pre-trained Models you can able to\nimport a tokenizer let's say I want to\nuse but\nbase case I want to use this board base\ncased model to create a\ntokenizer so let's say I created a\ntokenizer that is I want to load a\ntokenizer here you can see for this\nmodel for this model by using this Auto\ntokenizer and Dot from pre trained I\nloaded this model related tokenizer into\nthis\nvariable now in order to understood this\nin better way let's print this tokenizer\nand let's see what it is\nprinting what it is printing instead of\nthis so actually I'm one bit out\nnow I want to run it to check what how\nmy tokenizer is loaded so how my\ntokenizer is\nrepresented now you can see so it's\nrunning yeah currently now it is loading\nthe tokenizer yeah you can see so this\nis is how it will be\nloaded so it contains our tokenizer\ncontains these many things like so what\nis the\nmodel which model tokenizer it is so\nthat model is there and how many\nvocabularies are there so what is\nvocabulary vocabulary is nothing but\neach word related or each token related\nnumerical representation we call word or\nsentence or\nsentence as a token here so in case of\nmodel we call it as a token so for each\ntoken numerical representation we will\nstore in in the form of a vocabulary so\nlike this how many vocabulary size is\nthere\n28996 so this actually store these many\ndata of\nvocabulary this also specifying me the\nmax length this represent what is the\nmax length of tokens that we can able to\npass to our\nmodel and so it's actually F so that's\nwhy it is true and padding in right side\nyou can able to apply padding so what is\nthe major requirement of this padding\nsir the major requirement of this\npadding is whenever you are comparing\nthe similarity between two words or two\nsentences during that scenario the\nnumber of tokens for sentence one and\nsentence two may not\nmatch one sentence may have large number\nof\ntokens compared to other sentence during\nthat scenario you use this padding to\njust match the length of the both\nsentences before processing that data by\nusing our model that padding actually\nhappens in right side\nand if you see the other things like\ntruncation side is also right side only\nas well as special\ntokens it contains this model actually\ncontains special tokens like unknown\ntokens are represented by using Unk and\nseparate tokens are represented by using\nSCP that means if there are two\nsentences then they are separated by\nusing this SCP token and padding so just\nnow we discussed in order to provide\npadding to a sentence so that padding\nthat extra padding is provided by using\nthis pad\ntokens and so\nwhile let's say if I provide this\nsentence then actually after tokenizing\nthe sentence it actually contains the\nfirst the first element is this class\ntoken and the last element is this\nseparate token in order to separate the\nsentence this this becomes the last and\nin order to tell this is the starting of\nthe sentence this token is added and\ncoming to mask so this is the mask token\nwe can actually use it to train a\nmodel now we have understood how a\ntokenizer is loaded now what I'm\nexpecting is I want to see the\nvocabulary so what is the vocabulary or\nyou can say how actually you have seen\nright so how what is the vocabulary size\nof this model this model vocabulary size\nis this so what is vocabulary vocabulary\nis nothing but for particular word what\nis the numerical representation so that\nis the vocabulary so in order to\nidentify a word based on numerical\nrepresentation it also need a vocabulary\nright yes so that we going to see\nnow how can we able to see you can see\nwe created this we loaded this tokenizer\nright so by using this tokenizer you can\nable to see that tokenizer do oap so\nthis is\nthe this is the thing you can able to\nuse or you can say this is the method\nyou can able to use to see the\nvocabulary\nof that particular model now I'm just\nrerunning it again\nto see the\nvocabulary how yeah you can see multiple\nvocabularies are there for particular\nword\nparticular numerical representation is\nassigned already now I want to see that\nin a meaningful\nmanner I want to see these all in a\nstructured format for one what is the\nword for two what is the word so like\nthat I want to see in a structured\nformat for that in order to represent\nthis data in a very structured format I\nwant to import\npandas so let's\nimport\npandas yeah let's import\npandas as\nPD I think this library was not\ninstalled yeah so let's install it\nmeanwhile we\ninstall pandas so this used this command\nis used to install the library so\nmeanwhile let's write let's continue the\ncommands so I want to first I want to\nconvert this vocabulary or you can say\nthis dictionary into P data frame so how\ncan I able to convert this dictionary\ninto Data frame so okab data frame in\nthis variable I want to store that data\nframe so in order to create this data\nframe I want want to use pandas PD do\ndata frame what is that pd. data frame\nthis you can able to use in order to\nconvert your data or you can say our\ndata our vocabulary data\ninto Data frame and\nhere we need to provide a dictionary\nformat data or let let's provide the\nsame thing\nor let's provide our own\nlike what is\nthe key and what is the value I want to\nstore for particular\nvariable or else you can able to\ndirectly provide this vocabulary so it's\nactual dictionary so you can able to\nprovide and let's see the data frame how\nit looks like so let's print it so that\nwe can able to get the data frame\nvocabulary data frame of this particular\nmodel so like this for every model so\nthis related to what model this related\nto B base Case Model so like this for\nevery model we have\ntokenizers yeah I think we got some\nerror in line number 10 that is at here\nso let let's make it in the form of a\ndictionary to pass it so first I want to\nstore tokens that is nothing but\nwordss so what are the wordss wordss are\nnothing but the first keys right yes so\nokab do\nkeys so these keys are nothing but words\nas well as the second one is I want the\ntoken\nIDs so words as well as token IDs are\nstored right so okab\ndot values you can able to use values to\nget all the values of the dictionary and\nto get all the keys of the dictionary\nyou can able to use it so let's rerun it\nto\nsee to see this data\nin this dictionary format\ndata yeah now you can able to observe\ntotally these many rows are there that\nis a\n28,990 and you can see how the data is\nrepresented yeah looks good right yes\nbut the token IDs are not in arranged\norder like0 to ending I want to I want\nto sort this data based on a ascending\nor like like zero from zero to ending\nthat is 28 9 96 like that I want order\norder way I want to see so in order to\nsee an order way let's sort the\ndata based on this token ID so for\nsorting this so vocabulary data frame\ndot\nsort sort values I want to sort values\nby\nSo based on what we are sorting based on\nthis uh yeah token\nID based on this token ID I want to sort\nso that's why I'm copying this and\nproviding that here and so I want to set\nindex as well so here you can see index\nI'm getting some random index values I\nwant to set that index as the same that\nis this token ID I want to set this as\nindex so I'm providing the token ID\nand again at last I want to store this\nsorted data in this vocabulary data\nframe itself so I'm storing it so let's\nsee in a sorted order from zero to\nending yeah you can observe so first\nyeah this is the sorted order so from 0\nto it was started and 200 yeah something\nit was there so up till there it was\nthere now you can observe Z to token is\nprovided for padding so let's say if we\nwant to provide padding for a sentence\nthen what it will provide it will\nprovide zero it will provide a zero for\nthat extra\nplaces in a sentence at the place of\npadding and one and you can see so\nmultiple are\nthere so let's\nprint since lot of values are there so\nthat's why the reason we are unable to\nsee that data so let's download the data\nin the form of CSV and see that data so\nwab data frame dot to CSV so this is the\nthis is the method you can able to use\nin order to load your\ndata in data Frame data into your local\nmachine and you need to provide the\nCSV file related name like data. CSV\nor let's provide okab data. CSV and do\nyou want index\nthen you can provide or else you can\nprovide currently I don't want any index\nso that's why I'm providing false so\nlet's rerun it so that my data frame got\nconverted to CSV format and it will gets\ndownloaded into\nmy yeah it will get downloaded into my\nlocal machine you can see this is the\nokay okay\nfolder so this is how this this is the\nCSV file that is downloaded so let's\nopen this CSV file to see the\ndata since our index is nothing but the\ntoken ID we need this so that's why I'm\nI want to I want this as well so that's\nwhy I'm providing true and rerunning it\nthen let's see how how the vocabulary of\nthis particular model will be so like\nthis for every model there will be some\nvocabul\npermissions denied so let's provide\none let's provide another file name and\nlet's try\nto do this again yeah you can see so\nit's get downloaded so if you open it\nthen you can able to see the token ID\nand the data corresponding to it right\nyes so the whole data is there if You\nobserve so first few\nor you can say first some data will be\nrepresented for T padding\nand you can see for classing separation\nand MTH so like that for different\nlabels\nfirst approximately yeah exactly one5\ntokens or you can say one5 tokens are\ngiven for these type of data then after\nyou can see special symbols got\nstarted and up to 120 we have special\nsymbols and numbers are\nstored from 121 to 130 and then again\nspecial symbols came and then you can\nsee AB CDs that is nothing but your\nletters our characters are characters\nindication starting from\n138 then yeah so then special symbols\nare there so like that so for each and\nevery for each and every you can see for\ndifferent languages different symbols\nare there those are also stored here so\nsince it's a language model multiple\nlanguages also got stored and you can\nsee words word related word related\ntoken is also there so like this based\non token and token ID this model already\nhave based on the token and token ID\nthis model already have this a tokenizer\nso where is the tokenizer yeah yeah here\nso this tokenizer whatever the tokenizer\nthat we created from that model that\nwill con that will convert our sentence\nthat will convert our sentence into\nnumerical representation particular\ncorresponding numerical representation\nso before converting our sentence into\ncorresponding numerical representation\nso this tokenizer this tokenizer will\nactually split or divide our sentence\ninto\ntokens where those every tokens will be\npresent here so by using that tokens it\nwill represent the corresponding token\nIDs so let's first\nconvert this sentence into two\ntokens to observe how this tokenizer is\nconverting the sentence into\ntokens so for that let's comment this\nvocabulary representation I don't want\nthis vocabulary right now okay now I\nwant to see how this\ntokenizer will split my\nsentence so for that let's see so I want\ntokens right yes so by using that\ntokenizer you can to\nuse tokenize you can able to use\ntokenize you can able to use this\ntokenize of this\nsentence so this can able to tokenize\nyour sentence and then let's see the\ntokens how it will convert the sentence\nor you can say how it will tokenize the\nsentence tokenization is nothing but\nsplitting the sentence toiz is nothing\nbut splitting the sentence you can see\nhow it is splitted I is taking as one\nword want is taken as one word two is\ntaken as one word become is taken as one\nword a is taken as one word developer is\ntaken as one word and this dot is taken\nas another\nword in this way actually it can able to\ntokenize the sentence so like this every\ntokenizer related to every model will\ntokenize the\nsentence based on what based on the\nvocabulary token toer vocabulary it has\nso actually this modal tokenizer will\nhave the meaning related to this word\nthat is the numerical representation\nrelated to this word so that's why the\nsplitted like\nthis now let's tokenize that is these\nare tokens right yes so now I want to\ntokenize these tokens nothing but\nwhatever the sentence I have that I want\nto tokenize and see\nhow my tokenizer will converting this\nsentence into numerical representation\nso for that so what we doing we are\nactually converting or you can say we're\ngetting that tokens IDs right yes so we\nare tokenizing nothing but converting\nthe tokens into token IDs so by using\nthe tokenizer only you can able to do\nthat by using the same tokenizer I'm\nusing encode function and providing the\nsame\nsentence and let's print these token IDs\nas well after this tokens let's print\nthis token ID what we get after encoding\nthat\nsentence I'm rerunning this to see these\ntokens as well as the token IDs you can\nsee the token as well as token ID token\ntoken ID But If You observe so totally 1\n2 3 4 5 6 7 7even tokens are there\ncoming to token IDs 1 2 3 4 5 6 7 8 n so\nwhy two extra tokens came in this token\nIDs other than the sentence why two\nextra tokens will came the reason behind\nthis is the first token represents that\nclass and this last token represents the\nseparation nothing but the ending of the\nsentence so let's see that as well\nin order to see these two tokens related\nwordss or these two tokens related\nrepresenting I want this vocabulary so I\nI'm using this vocabulary and I also\nrequire these data\nframes so I'm just uh removing or\nuncommenting these two\nand I want to print right so I want to\nprint\nwhat is\nthe what is this numerical\nrepresentation represents in this data\nframe so for that I'm just using the\nprint\nstatement okab data frame dot\nitem location or\nitem load of this one1 nothing but this\ntoken I provided and I'm providing the\nanother one as well\nthat is nothing but this\none2 one2 now I want to rerun this to\nsee so what these two tokens will\nactually represented in this data\nframe so I think some error\ncame yeah PD is not there so that's the\nerror so let's rerun\nit we this importing p s PD also we need\nto un comment so that we forgot it yeah\nyou can see so what does this represents\nthis\nrepresents CLS what does this one or two\ntoken represents this represents this\nSCP so like this the starting and ending\ntokens will be represented in a sentence\nso that's why the two get extra and all\nthe middle tokens represent the\ncorresponding tokens the corresponding\ntokens in the tokenized sentence\nso now we observed and we\nanalyzed how any model vocabulary will\nlooks like by using this by using this\ntokenizer doab and we also analyzed how\na tokenizer how a tokenizer will\ntokenizes the sentence by using this how\nit breaks the sentence and we also\nunderstood the encoding or nothing but\ntoken IDs of the sentence of the\nsentence we also identify the token IDs\nof the\nsentence and we also observed why two\nextra tokens got added to our sentence\nand what they are nothing but they are\nspecifying the tokens like CLS as well\nas this SCP to represent the start as\nwell as ending of the sentence\nnow based on these token IDs based on\nthese token IDs whatever generated I\nwant to decode it that is I want to see\nthe I want to see back that is these\ntokens I want to see back so can I able\nto do that yes we can able to do that so\njust a decode method you can able to use\nso these are the token IDs right now I'm\njust printing I'm just commenting the\nbelow because uh this vocabul vocabul\nrelated code we have already seen now\nlet's see how to decode\nthis so to\ndecode to decode these\ntokens so token IDs let's say you you\nhave this encoded sentence that is these\ntokens so by using these token IDs let's\nsay I want to decode how we can able to\ndecode that so in order to decode you\nsimply write the sentence that is these\ntoken IDs\ndot this tokenizer do decode so by using\nthe tokenizer right you you are decoding\nyes this tokenizer do\ndecode of so what you want to decode\nthis token IDs this tokens related IDs I\nwant to decode and then I want to print\nwhatever the response that I'm\ngetting to\nsee how what is the decoded\ntokens so\nas per your observation the first token\nrepresents the CLS and uh you can\nsee so somewhat error I'm no\nsome yeah these all are warnings you\njust ignore them you can see so the\nfirst one represents class and the last\none represents this separation and this\nis what the sentence have provided so by\nusing the token IDs also you can able to\ngenerate the\nsentence so from starting from first\ncharacter to nothing but starting from\nthe first sentence to last sentence I\nwant I don't want the first as well as\nlast up to there I want so like that\nthat's why I'm providing this slicing\nand let's print it to get only the\nsentence no these tokens are not there\nright so let's print the sentence\nlet's print this sentence yeah you can\nsee we got this so again we are running\nit to see how the\nsentence will\nbe you can see this is the decoded\nsentence so like this actually we\nrepresent we encode the sentence to send\nthe sentence to our\nmodel one more important point you not\nonly pass these token IDs apart from\nthese token IDs you also pass two more\nparameters how to generate those so to\ngenerate those\nso how to generate\nthose you can generate those by using\ntokenizer by using the tokenizer itself\nyou can able to generate tokenizer of\nwhatever the sentence that you want to\npass that sentence you will provide so\nthis will generate\nyou this will generate you three things\nso let's see what three things it will\ngenerate and let's understood what are\nthe three\nthings first one is these token IDs you\ncan see input IDs it will generated that\ninput ID are nothing but my tokens\nrelated IDs okay then what these token\ntype IDs these token type IDs represents\nthe type of sentence what is the the\ninput\ntype is the input is prompt or is the\ninput is question so like that in order\nto provide the variation between\ndifferent\nsentences so this actually uses this\ntoken type IDs okay in order to tell the\ndifferentiation between input inputs so\nthese token IDs are been given okay so\nwhat about these attention Mass so I\nalready told you these attention Mass\nare nothing but in order to specify is\nthere any\npadding is there any padding so in order\nto specify those paddings these\nattention masks are there so in order to\nobserve the padding as well let's let's\ntake one more sentence as an\nexample to provide two sentences to this\nmodel so let's say this is the sentence\none and this sentence one is nothing but\nI a\ndeveloper I am a developer you can see\nthese two will generate different length\ntokens right yes these two sentences\nwill generate me different length\ntokens comma\nsentence one I'm passing now for the\ntokenizer I'm passing the sentence as\nwell as sentence one that is nothing but\nthese two sentences I'm passing\nand padding so here in order to balance\nthe length nothing\nbut for the first for the first sentence\nit will generate these many tokens for\nthe second sentence might these many\ntokens won't be generated you can print\nit as well if you want so yeah let's\nprint it to make it\nclear for the sentence one also I'm\ntaking tokens one and printing that so\nlet's rerun it so that you can able to\nobserve how the output will be you can\nsee this yes so for this the number of\ntokens are these many for this the\nnumber of tokens are these many right\nyes so now here do I applied any padding\nno so that's why these extra two bits\nare not taken and here all the padding\nbits are one now let's say I want to\napply the padding in order to balance\nthe two sentences I want to apply\npadding equals to\ntrue so I want to apply the\npadding and let's rerun it to see this\nattention mask to see this attention\nmask I applied the padding you can see\nextra two zeros got added that means two\nextra tokens are there in the first\nsentence you can see this is the first\nsentence\nrelated attention mask paddings and\nthese are the attention mask paddings\nrelated to Second sentence that is\nsentence one here you can see two zeros\nare added nothing but two extra padding\nbits got added here whereas in\npreviously two extra paddings are not\nadded so that's why there is no padding\nhere right yes so nothing but two zeros\nare there these two represents the pad\nin previously before padding there is no\ntwo extra zeros after padding now two\nextra zeros came so that's\nthe so in order to represent that\npadding this attention mask is used so\nthis whole data nothing\nbut this all data will pass as tokenized\ndata to our embeddings or to our\nmodel so if we recap it\nthen from where we imported the\ntokenizer we imported the tokenizer from\nTransformers Library by using this Auto\nTransformers and we provided the model\nfor which model we want to create this\ntokenizer and we have taken a sample\nsentence and we tokenized that sentence\nas well to see how the token\nrepresentation of the sentence as well\nand then we we previously seen what is\nthe vocabulary that is nothing but how\nthis model is remembering this model is\nidentifying what word represent what\nnumerical representation so that\nvocabulary we have Sav we have saved in\nthis vocab data one file and we have\nseen that vocabulary how it will be and\nthen we just\nsimply tokenized our sentence that is\nnothing but\nwe taking a sample sentence and we\ntokenized that sentence by using using\nthis tokenizer tokenized function then\nafter we encoded that sentence nothing\nbut we generated that token IDs from the\ntokens and we also understood what are\nthe other parameters will pass to the\nmodel before before we sending the data\nyou can see we provided the tokenized\ninputs nothing but input IDs and we are\npassing these token type IDs nothing but\nin order to represent what is that token\ntype\nso in order to represent different\ninputs nothing but if you take an\nexample then question and answering or\nyou can say context you know\nto differentiate between different types\nof\ninputs this token type IDs are created\nand this attention Mass actually to\nrepresent padding so is there any extra\npadding happened or not so that to\nrepresent that you can see extra zeros\nare there so that representing by using\nthis attention Mass so that's it all\nabout this tokenization in\ndepth analysis so I hope you understood\nthis video and you understood some\nmore important and detailed Concepts and\nYou observe that detailed observation of\nthis\ntokenization how a model how any model\nwill understood the numerical\nrepresentation that we create by using\nthis\ntokenizers and how actually the\nvocabulary of this token tokenizer will\nbe for any\nmodel and what we will pass and what we\nwill pass as an input to our tokenizer\nyou can see this data will pass as an\ninput to our tokenizer and you also\nobserved what are the major components\nin\nit so that's it all about this video I\nhope you understood this\nvideo and you got some clear conceptual\nknowledge if you feel this video is\nuseful then please like it so that it\nwill be very supporting to\nme and type your comment so that I can\nable to understood your\nfeeling and don't forget to subscribe to\nmy channel to watch such informative\nvideos thanks for watching again see you\nback in the next video with another\ninteresting topic until then bye-bye\nguys\n",
  "words": [
    "hello",
    "guys",
    "welcome",
    "back",
    "video",
    "let",
    "us",
    "see",
    "detail",
    "tokenization",
    "let",
    "question",
    "need",
    "tokenization",
    "requirement",
    "tokenization",
    "let",
    "say",
    "want",
    "perform",
    "task",
    "like",
    "summarizing",
    "text",
    "question",
    "answering",
    "model",
    "provide",
    "context",
    "like",
    "ask",
    "questions",
    "model",
    "get",
    "response",
    "related",
    "question",
    "like",
    "question",
    "answering",
    "observe",
    "sentiment",
    "analysis",
    "also",
    "able",
    "perform",
    "multiple",
    "tasks",
    "using",
    "one",
    "nlp",
    "model",
    "using",
    "one",
    "natural",
    "language",
    "processing",
    "model",
    "able",
    "perform",
    "tasks",
    "using",
    "one",
    "nlp",
    "model",
    "order",
    "perform",
    "one",
    "task",
    "based",
    "requirement",
    "need",
    "pass",
    "data",
    "text",
    "content",
    "model",
    "right",
    "yes",
    "passing",
    "type",
    "data",
    "model",
    "expecting",
    "type",
    "data",
    "need",
    "pass",
    "model",
    "accept",
    "numerical",
    "representations",
    "input",
    "order",
    "perform",
    "tasks",
    "real",
    "time",
    "using",
    "one",
    "nlp",
    "model",
    "requirement",
    "convert",
    "input",
    "numerical",
    "representation",
    "input",
    "actually",
    "form",
    "sentence",
    "nothing",
    "text",
    "right",
    "yes",
    "let",
    "take",
    "example",
    "let",
    "say",
    "want",
    "analyze",
    "sentiment",
    "text",
    "passing",
    "input",
    "first",
    "task",
    "need",
    "need",
    "pass",
    "input",
    "right",
    "yes",
    "able",
    "pass",
    "input",
    "one",
    "natural",
    "language",
    "processing",
    "model",
    "going",
    "understood",
    "video",
    "text",
    "let",
    "let",
    "take",
    "sentence",
    "let",
    "take",
    "example",
    "sentence",
    "let",
    "say",
    "developer",
    "developer",
    "want",
    "become",
    "developer",
    "want",
    "become",
    "developer",
    "like",
    "want",
    "send",
    "input",
    "want",
    "send",
    "want",
    "provide",
    "sentence",
    "one",
    "ml",
    "model",
    "able",
    "provide",
    "send",
    "sentence",
    "directly",
    "model",
    "send",
    "directly",
    "sentence",
    "model",
    "model",
    "able",
    "understand",
    "sentence",
    "know",
    "numerical",
    "representations",
    "understood",
    "numerical",
    "representations",
    "whatever",
    "sentence",
    "sent",
    "form",
    "text",
    "model",
    "understood",
    "one",
    "important",
    "point",
    "send",
    "numerical",
    "representation",
    "instead",
    "sentence",
    "model",
    "need",
    "understood",
    "particular",
    "numerical",
    "representation",
    "representing",
    "sentence",
    "model",
    "able",
    "understood",
    "numerical",
    "representation",
    "represents",
    "sentence",
    "first",
    "initially",
    "data",
    "need",
    "right",
    "yes",
    "order",
    "understood",
    "numerical",
    "representation",
    "belongs",
    "particular",
    "word",
    "particular",
    "sentence",
    "particular",
    "character",
    "first",
    "data",
    "vocabulary",
    "data",
    "need",
    "stored",
    "model",
    "right",
    "yes",
    "let",
    "say",
    "assign",
    "one",
    "assign",
    "two",
    "whole",
    "sentence",
    "like",
    "want",
    "sentence",
    "know",
    "one",
    "represents",
    "two",
    "represents",
    "w",
    "space",
    "actually",
    "like",
    "vocabulary",
    "way",
    "model",
    "actually",
    "store",
    "data",
    "vocabulary",
    "model",
    "using",
    "order",
    "identify",
    "sentence",
    "numerical",
    "words",
    "let",
    "first",
    "able",
    "convert",
    "sentence",
    "numerical",
    "representation",
    "able",
    "pass",
    "data",
    "model",
    "let",
    "first",
    "observe",
    "vocabulary",
    "model",
    "using",
    "order",
    "understood",
    "numerical",
    "order",
    "understood",
    "sentence",
    "based",
    "numerical",
    "representation",
    "able",
    "get",
    "vocabulary",
    "sir",
    "already",
    "library",
    "library",
    "transformers",
    "library",
    "actually",
    "tokenizers",
    "auto",
    "tokenization",
    "auto",
    "tokenizer",
    "see",
    "suggesting",
    "yeah",
    "auto",
    "tokenizer",
    "already",
    "present",
    "transformers",
    "library",
    "using",
    "auto",
    "tokenizer",
    "library",
    "using",
    "auto",
    "tokenizer",
    "able",
    "actually",
    "tokenize",
    "sentence",
    "able",
    "convert",
    "sentence",
    "numerical",
    "representation",
    "based",
    "model",
    "use",
    "use",
    "tokenizer",
    "auto",
    "tokenizer",
    "convert",
    "respective",
    "numerical",
    "representation",
    "every",
    "model",
    "tokenizers",
    "passing",
    "sentence",
    "input",
    "model",
    "need",
    "convert",
    "sentence",
    "tokenization",
    "using",
    "particular",
    "tokenizer",
    "tokenizer",
    "convert",
    "sentence",
    "numerical",
    "representation",
    "worry",
    "understood",
    "vocabulary",
    "stored",
    "particular",
    "model",
    "tokenizer",
    "well",
    "using",
    "auto",
    "tokenizer",
    "able",
    "load",
    "tokenizer",
    "say",
    "able",
    "load",
    "model",
    "related",
    "tokenizer",
    "let",
    "say",
    "want",
    "load",
    "tokenizer",
    "related",
    "b",
    "model",
    "able",
    "use",
    "auto",
    "tokenizer",
    "able",
    "use",
    "auto",
    "tokenizer",
    "dot",
    "models",
    "able",
    "import",
    "tokenizer",
    "let",
    "say",
    "want",
    "use",
    "base",
    "case",
    "want",
    "use",
    "board",
    "base",
    "cased",
    "model",
    "create",
    "tokenizer",
    "let",
    "say",
    "created",
    "tokenizer",
    "want",
    "load",
    "tokenizer",
    "see",
    "model",
    "model",
    "using",
    "auto",
    "tokenizer",
    "dot",
    "pre",
    "trained",
    "loaded",
    "model",
    "related",
    "tokenizer",
    "variable",
    "order",
    "understood",
    "better",
    "way",
    "let",
    "print",
    "tokenizer",
    "let",
    "see",
    "printing",
    "printing",
    "instead",
    "actually",
    "one",
    "bit",
    "want",
    "run",
    "check",
    "tokenizer",
    "loaded",
    "tokenizer",
    "represented",
    "see",
    "running",
    "yeah",
    "currently",
    "loading",
    "tokenizer",
    "yeah",
    "see",
    "loaded",
    "contains",
    "tokenizer",
    "contains",
    "many",
    "things",
    "like",
    "model",
    "model",
    "tokenizer",
    "model",
    "many",
    "vocabularies",
    "vocabulary",
    "vocabulary",
    "nothing",
    "word",
    "related",
    "token",
    "related",
    "numerical",
    "representation",
    "call",
    "word",
    "sentence",
    "sentence",
    "token",
    "case",
    "model",
    "call",
    "token",
    "token",
    "numerical",
    "representation",
    "store",
    "form",
    "vocabulary",
    "like",
    "many",
    "vocabulary",
    "size",
    "28996",
    "actually",
    "store",
    "many",
    "data",
    "vocabulary",
    "also",
    "specifying",
    "max",
    "length",
    "represent",
    "max",
    "length",
    "tokens",
    "able",
    "pass",
    "model",
    "actually",
    "f",
    "true",
    "padding",
    "right",
    "side",
    "able",
    "apply",
    "padding",
    "major",
    "requirement",
    "padding",
    "sir",
    "major",
    "requirement",
    "padding",
    "whenever",
    "comparing",
    "similarity",
    "two",
    "words",
    "two",
    "sentences",
    "scenario",
    "number",
    "tokens",
    "sentence",
    "one",
    "sentence",
    "two",
    "may",
    "match",
    "one",
    "sentence",
    "may",
    "large",
    "number",
    "tokens",
    "compared",
    "sentence",
    "scenario",
    "use",
    "padding",
    "match",
    "length",
    "sentences",
    "processing",
    "data",
    "using",
    "model",
    "padding",
    "actually",
    "happens",
    "right",
    "side",
    "see",
    "things",
    "like",
    "truncation",
    "side",
    "also",
    "right",
    "side",
    "well",
    "special",
    "tokens",
    "contains",
    "model",
    "actually",
    "contains",
    "special",
    "tokens",
    "like",
    "unknown",
    "tokens",
    "represented",
    "using",
    "unk",
    "separate",
    "tokens",
    "represented",
    "using",
    "scp",
    "means",
    "two",
    "sentences",
    "separated",
    "using",
    "scp",
    "token",
    "padding",
    "discussed",
    "order",
    "provide",
    "padding",
    "sentence",
    "padding",
    "extra",
    "padding",
    "provided",
    "using",
    "pad",
    "tokens",
    "let",
    "say",
    "provide",
    "sentence",
    "actually",
    "tokenizing",
    "sentence",
    "actually",
    "contains",
    "first",
    "first",
    "element",
    "class",
    "token",
    "last",
    "element",
    "separate",
    "token",
    "order",
    "separate",
    "sentence",
    "becomes",
    "last",
    "order",
    "tell",
    "starting",
    "sentence",
    "token",
    "added",
    "coming",
    "mask",
    "mask",
    "token",
    "actually",
    "use",
    "train",
    "model",
    "understood",
    "tokenizer",
    "loaded",
    "expecting",
    "want",
    "see",
    "vocabulary",
    "vocabulary",
    "say",
    "actually",
    "seen",
    "right",
    "vocabulary",
    "size",
    "model",
    "model",
    "vocabulary",
    "size",
    "vocabulary",
    "vocabulary",
    "nothing",
    "particular",
    "word",
    "numerical",
    "representation",
    "vocabulary",
    "order",
    "identify",
    "word",
    "based",
    "numerical",
    "representation",
    "also",
    "need",
    "vocabulary",
    "right",
    "yes",
    "going",
    "see",
    "able",
    "see",
    "see",
    "created",
    "loaded",
    "tokenizer",
    "right",
    "using",
    "tokenizer",
    "able",
    "see",
    "tokenizer",
    "oap",
    "thing",
    "able",
    "use",
    "say",
    "method",
    "able",
    "use",
    "see",
    "vocabulary",
    "particular",
    "model",
    "rerunning",
    "see",
    "vocabulary",
    "yeah",
    "see",
    "multiple",
    "vocabularies",
    "particular",
    "word",
    "particular",
    "numerical",
    "representation",
    "assigned",
    "already",
    "want",
    "see",
    "meaningful",
    "manner",
    "want",
    "see",
    "structured",
    "format",
    "one",
    "word",
    "two",
    "word",
    "like",
    "want",
    "see",
    "structured",
    "format",
    "order",
    "represent",
    "data",
    "structured",
    "format",
    "want",
    "import",
    "pandas",
    "let",
    "import",
    "pandas",
    "yeah",
    "let",
    "import",
    "pandas",
    "pd",
    "think",
    "library",
    "installed",
    "yeah",
    "let",
    "install",
    "meanwhile",
    "install",
    "pandas",
    "used",
    "command",
    "used",
    "install",
    "library",
    "meanwhile",
    "let",
    "write",
    "let",
    "continue",
    "commands",
    "want",
    "first",
    "want",
    "convert",
    "vocabulary",
    "say",
    "dictionary",
    "p",
    "data",
    "frame",
    "able",
    "convert",
    "dictionary",
    "data",
    "frame",
    "okab",
    "data",
    "frame",
    "variable",
    "want",
    "store",
    "data",
    "frame",
    "order",
    "create",
    "data",
    "frame",
    "want",
    "want",
    "use",
    "pandas",
    "pd",
    "data",
    "frame",
    "pd",
    "data",
    "frame",
    "able",
    "use",
    "order",
    "convert",
    "data",
    "say",
    "data",
    "vocabulary",
    "data",
    "data",
    "frame",
    "need",
    "provide",
    "dictionary",
    "format",
    "data",
    "let",
    "let",
    "provide",
    "thing",
    "let",
    "provide",
    "like",
    "key",
    "value",
    "want",
    "store",
    "particular",
    "variable",
    "else",
    "able",
    "directly",
    "provide",
    "vocabulary",
    "actual",
    "dictionary",
    "able",
    "provide",
    "let",
    "see",
    "data",
    "frame",
    "looks",
    "like",
    "let",
    "print",
    "able",
    "get",
    "data",
    "frame",
    "vocabulary",
    "data",
    "frame",
    "particular",
    "model",
    "like",
    "every",
    "model",
    "related",
    "model",
    "related",
    "b",
    "base",
    "case",
    "model",
    "like",
    "every",
    "model",
    "tokenizers",
    "yeah",
    "think",
    "got",
    "error",
    "line",
    "number",
    "10",
    "let",
    "let",
    "make",
    "form",
    "dictionary",
    "pass",
    "first",
    "want",
    "store",
    "tokens",
    "nothing",
    "wordss",
    "wordss",
    "wordss",
    "nothing",
    "first",
    "keys",
    "right",
    "yes",
    "okab",
    "keys",
    "keys",
    "nothing",
    "words",
    "well",
    "second",
    "one",
    "want",
    "token",
    "ids",
    "words",
    "well",
    "token",
    "ids",
    "stored",
    "right",
    "okab",
    "dot",
    "values",
    "able",
    "use",
    "values",
    "get",
    "values",
    "dictionary",
    "get",
    "keys",
    "dictionary",
    "able",
    "use",
    "let",
    "rerun",
    "see",
    "see",
    "data",
    "dictionary",
    "format",
    "data",
    "yeah",
    "able",
    "observe",
    "totally",
    "many",
    "rows",
    "see",
    "data",
    "represented",
    "yeah",
    "looks",
    "good",
    "right",
    "yes",
    "token",
    "ids",
    "arranged",
    "order",
    "like0",
    "ending",
    "want",
    "want",
    "sort",
    "data",
    "based",
    "ascending",
    "like",
    "like",
    "zero",
    "zero",
    "ending",
    "28",
    "9",
    "96",
    "like",
    "want",
    "order",
    "order",
    "way",
    "want",
    "see",
    "order",
    "see",
    "order",
    "way",
    "let",
    "sort",
    "data",
    "based",
    "token",
    "id",
    "sorting",
    "vocabulary",
    "data",
    "frame",
    "dot",
    "sort",
    "sort",
    "values",
    "want",
    "sort",
    "values",
    "based",
    "sorting",
    "based",
    "uh",
    "yeah",
    "token",
    "id",
    "based",
    "token",
    "id",
    "want",
    "sort",
    "copying",
    "providing",
    "want",
    "set",
    "index",
    "well",
    "see",
    "index",
    "getting",
    "random",
    "index",
    "values",
    "want",
    "set",
    "index",
    "token",
    "id",
    "want",
    "set",
    "index",
    "providing",
    "token",
    "id",
    "last",
    "want",
    "store",
    "sorted",
    "data",
    "vocabulary",
    "data",
    "frame",
    "storing",
    "let",
    "see",
    "sorted",
    "order",
    "zero",
    "ending",
    "yeah",
    "observe",
    "first",
    "yeah",
    "sorted",
    "order",
    "0",
    "started",
    "200",
    "yeah",
    "something",
    "till",
    "observe",
    "z",
    "token",
    "provided",
    "padding",
    "let",
    "say",
    "want",
    "provide",
    "padding",
    "sentence",
    "provide",
    "provide",
    "zero",
    "provide",
    "zero",
    "extra",
    "places",
    "sentence",
    "place",
    "padding",
    "one",
    "see",
    "multiple",
    "let",
    "print",
    "since",
    "lot",
    "values",
    "reason",
    "unable",
    "see",
    "data",
    "let",
    "download",
    "data",
    "form",
    "csv",
    "see",
    "data",
    "wab",
    "data",
    "frame",
    "dot",
    "csv",
    "method",
    "able",
    "use",
    "order",
    "load",
    "data",
    "data",
    "frame",
    "data",
    "local",
    "machine",
    "need",
    "provide",
    "csv",
    "file",
    "related",
    "name",
    "like",
    "data",
    "csv",
    "let",
    "provide",
    "okab",
    "data",
    "csv",
    "want",
    "index",
    "provide",
    "else",
    "provide",
    "currently",
    "want",
    "index",
    "providing",
    "false",
    "let",
    "rerun",
    "data",
    "frame",
    "got",
    "converted",
    "csv",
    "format",
    "gets",
    "downloaded",
    "yeah",
    "get",
    "downloaded",
    "local",
    "machine",
    "see",
    "okay",
    "okay",
    "folder",
    "csv",
    "file",
    "downloaded",
    "let",
    "open",
    "csv",
    "file",
    "see",
    "data",
    "since",
    "index",
    "nothing",
    "token",
    "id",
    "need",
    "want",
    "want",
    "well",
    "providing",
    "true",
    "rerunning",
    "let",
    "see",
    "vocabulary",
    "particular",
    "model",
    "like",
    "every",
    "model",
    "vocabul",
    "permissions",
    "denied",
    "let",
    "provide",
    "one",
    "let",
    "provide",
    "another",
    "file",
    "name",
    "let",
    "try",
    "yeah",
    "see",
    "get",
    "downloaded",
    "open",
    "able",
    "see",
    "token",
    "id",
    "data",
    "corresponding",
    "right",
    "yes",
    "whole",
    "data",
    "observe",
    "first",
    "say",
    "first",
    "data",
    "represented",
    "padding",
    "see",
    "classing",
    "separation",
    "mth",
    "like",
    "different",
    "labels",
    "first",
    "approximately",
    "yeah",
    "exactly",
    "one5",
    "tokens",
    "say",
    "one5",
    "tokens",
    "given",
    "type",
    "data",
    "see",
    "special",
    "symbols",
    "got",
    "started",
    "120",
    "special",
    "symbols",
    "numbers",
    "stored",
    "121",
    "130",
    "special",
    "symbols",
    "came",
    "see",
    "ab",
    "cds",
    "nothing",
    "letters",
    "characters",
    "characters",
    "indication",
    "starting",
    "138",
    "yeah",
    "special",
    "symbols",
    "like",
    "every",
    "every",
    "see",
    "different",
    "languages",
    "different",
    "symbols",
    "also",
    "stored",
    "since",
    "language",
    "model",
    "multiple",
    "languages",
    "also",
    "got",
    "stored",
    "see",
    "words",
    "word",
    "related",
    "word",
    "related",
    "token",
    "also",
    "like",
    "based",
    "token",
    "token",
    "id",
    "model",
    "already",
    "based",
    "token",
    "token",
    "id",
    "model",
    "already",
    "tokenizer",
    "tokenizer",
    "yeah",
    "yeah",
    "tokenizer",
    "whatever",
    "tokenizer",
    "created",
    "model",
    "con",
    "convert",
    "sentence",
    "convert",
    "sentence",
    "numerical",
    "representation",
    "particular",
    "corresponding",
    "numerical",
    "representation",
    "converting",
    "sentence",
    "corresponding",
    "numerical",
    "representation",
    "tokenizer",
    "tokenizer",
    "actually",
    "split",
    "divide",
    "sentence",
    "tokens",
    "every",
    "tokens",
    "present",
    "using",
    "tokens",
    "represent",
    "corresponding",
    "token",
    "ids",
    "let",
    "first",
    "convert",
    "sentence",
    "two",
    "tokens",
    "observe",
    "tokenizer",
    "converting",
    "sentence",
    "tokens",
    "let",
    "comment",
    "vocabulary",
    "representation",
    "want",
    "vocabulary",
    "right",
    "okay",
    "want",
    "see",
    "tokenizer",
    "split",
    "sentence",
    "let",
    "see",
    "want",
    "tokens",
    "right",
    "yes",
    "using",
    "tokenizer",
    "use",
    "tokenize",
    "able",
    "use",
    "tokenize",
    "able",
    "use",
    "tokenize",
    "sentence",
    "able",
    "tokenize",
    "sentence",
    "let",
    "see",
    "tokens",
    "convert",
    "sentence",
    "say",
    "tokenize",
    "sentence",
    "tokenization",
    "nothing",
    "splitting",
    "sentence",
    "toiz",
    "nothing",
    "splitting",
    "sentence",
    "see",
    "splitted",
    "taking",
    "one",
    "word",
    "want",
    "taken",
    "one",
    "word",
    "two",
    "taken",
    "one",
    "word",
    "become",
    "taken",
    "one",
    "word",
    "taken",
    "one",
    "word",
    "developer",
    "taken",
    "one",
    "word",
    "dot",
    "taken",
    "another",
    "word",
    "way",
    "actually",
    "able",
    "tokenize",
    "sentence",
    "like",
    "every",
    "tokenizer",
    "related",
    "every",
    "model",
    "tokenize",
    "sentence",
    "based",
    "based",
    "vocabulary",
    "token",
    "toer",
    "vocabulary",
    "actually",
    "modal",
    "tokenizer",
    "meaning",
    "related",
    "word",
    "numerical",
    "representation",
    "related",
    "word",
    "splitted",
    "like",
    "let",
    "tokenize",
    "tokens",
    "right",
    "yes",
    "want",
    "tokenize",
    "tokens",
    "nothing",
    "whatever",
    "sentence",
    "want",
    "tokenize",
    "see",
    "tokenizer",
    "converting",
    "sentence",
    "numerical",
    "representation",
    "actually",
    "converting",
    "say",
    "getting",
    "tokens",
    "ids",
    "right",
    "yes",
    "tokenizing",
    "nothing",
    "converting",
    "tokens",
    "token",
    "ids",
    "using",
    "tokenizer",
    "able",
    "using",
    "tokenizer",
    "using",
    "encode",
    "function",
    "providing",
    "sentence",
    "let",
    "print",
    "token",
    "ids",
    "well",
    "tokens",
    "let",
    "print",
    "token",
    "id",
    "get",
    "encoding",
    "sentence",
    "rerunning",
    "see",
    "tokens",
    "well",
    "token",
    "ids",
    "see",
    "token",
    "well",
    "token",
    "id",
    "token",
    "token",
    "id",
    "observe",
    "totally",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "7even",
    "tokens",
    "coming",
    "token",
    "ids",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "n",
    "two",
    "extra",
    "tokens",
    "came",
    "token",
    "ids",
    "sentence",
    "two",
    "extra",
    "tokens",
    "came",
    "reason",
    "behind",
    "first",
    "token",
    "represents",
    "class",
    "last",
    "token",
    "represents",
    "separation",
    "nothing",
    "ending",
    "sentence",
    "let",
    "see",
    "well",
    "order",
    "see",
    "two",
    "tokens",
    "related",
    "wordss",
    "two",
    "tokens",
    "related",
    "representing",
    "want",
    "vocabulary",
    "using",
    "vocabulary",
    "also",
    "require",
    "data",
    "frames",
    "uh",
    "removing",
    "uncommenting",
    "two",
    "want",
    "print",
    "right",
    "want",
    "print",
    "numerical",
    "representation",
    "represents",
    "data",
    "frame",
    "using",
    "print",
    "statement",
    "okab",
    "data",
    "frame",
    "dot",
    "item",
    "location",
    "item",
    "load",
    "one1",
    "nothing",
    "token",
    "provided",
    "providing",
    "another",
    "one",
    "well",
    "nothing",
    "one2",
    "one2",
    "want",
    "rerun",
    "see",
    "two",
    "tokens",
    "actually",
    "represented",
    "data",
    "frame",
    "think",
    "error",
    "came",
    "yeah",
    "pd",
    "error",
    "let",
    "rerun",
    "importing",
    "p",
    "pd",
    "also",
    "need",
    "un",
    "comment",
    "forgot",
    "yeah",
    "see",
    "represents",
    "represents",
    "cls",
    "one",
    "two",
    "token",
    "represents",
    "represents",
    "scp",
    "like",
    "starting",
    "ending",
    "tokens",
    "represented",
    "sentence",
    "two",
    "get",
    "extra",
    "middle",
    "tokens",
    "represent",
    "corresponding",
    "tokens",
    "corresponding",
    "tokens",
    "tokenized",
    "sentence",
    "observed",
    "analyzed",
    "model",
    "vocabulary",
    "looks",
    "like",
    "using",
    "using",
    "tokenizer",
    "doab",
    "also",
    "analyzed",
    "tokenizer",
    "tokenizer",
    "tokenizes",
    "sentence",
    "using",
    "breaks",
    "sentence",
    "also",
    "understood",
    "encoding",
    "nothing",
    "token",
    "ids",
    "sentence",
    "sentence",
    "also",
    "identify",
    "token",
    "ids",
    "sentence",
    "also",
    "observed",
    "two",
    "extra",
    "tokens",
    "got",
    "added",
    "sentence",
    "nothing",
    "specifying",
    "tokens",
    "like",
    "cls",
    "well",
    "scp",
    "represent",
    "start",
    "well",
    "ending",
    "sentence",
    "based",
    "token",
    "ids",
    "based",
    "token",
    "ids",
    "whatever",
    "generated",
    "want",
    "decode",
    "want",
    "see",
    "want",
    "see",
    "back",
    "tokens",
    "want",
    "see",
    "back",
    "able",
    "yes",
    "able",
    "decode",
    "method",
    "able",
    "use",
    "token",
    "ids",
    "right",
    "printing",
    "commenting",
    "uh",
    "vocabul",
    "vocabul",
    "related",
    "code",
    "already",
    "seen",
    "let",
    "see",
    "decode",
    "decode",
    "decode",
    "tokens",
    "token",
    "ids",
    "let",
    "say",
    "encoded",
    "sentence",
    "tokens",
    "using",
    "token",
    "ids",
    "let",
    "say",
    "want",
    "decode",
    "able",
    "decode",
    "order",
    "decode",
    "simply",
    "write",
    "sentence",
    "token",
    "ids",
    "dot",
    "tokenizer",
    "decode",
    "using",
    "tokenizer",
    "right",
    "decoding",
    "yes",
    "tokenizer",
    "decode",
    "want",
    "decode",
    "token",
    "ids",
    "tokens",
    "related",
    "ids",
    "want",
    "decode",
    "want",
    "print",
    "whatever",
    "response",
    "getting",
    "see",
    "decoded",
    "tokens",
    "per",
    "observation",
    "first",
    "token",
    "represents",
    "cls",
    "uh",
    "see",
    "somewhat",
    "error",
    "yeah",
    "warnings",
    "ignore",
    "see",
    "first",
    "one",
    "represents",
    "class",
    "last",
    "one",
    "represents",
    "separation",
    "sentence",
    "provided",
    "using",
    "token",
    "ids",
    "also",
    "able",
    "generate",
    "sentence",
    "starting",
    "first",
    "character",
    "nothing",
    "starting",
    "first",
    "sentence",
    "last",
    "sentence",
    "want",
    "want",
    "first",
    "well",
    "last",
    "want",
    "like",
    "providing",
    "slicing",
    "let",
    "print",
    "get",
    "sentence",
    "tokens",
    "right",
    "let",
    "print",
    "sentence",
    "let",
    "print",
    "sentence",
    "yeah",
    "see",
    "got",
    "running",
    "see",
    "sentence",
    "see",
    "decoded",
    "sentence",
    "like",
    "actually",
    "represent",
    "encode",
    "sentence",
    "send",
    "sentence",
    "model",
    "one",
    "important",
    "point",
    "pass",
    "token",
    "ids",
    "apart",
    "token",
    "ids",
    "also",
    "pass",
    "two",
    "parameters",
    "generate",
    "generate",
    "generate",
    "generate",
    "using",
    "tokenizer",
    "using",
    "tokenizer",
    "able",
    "generate",
    "tokenizer",
    "whatever",
    "sentence",
    "want",
    "pass",
    "sentence",
    "provide",
    "generate",
    "generate",
    "three",
    "things",
    "let",
    "see",
    "three",
    "things",
    "generate",
    "let",
    "understood",
    "three",
    "things",
    "first",
    "one",
    "token",
    "ids",
    "see",
    "input",
    "ids",
    "generated",
    "input",
    "id",
    "nothing",
    "tokens",
    "related",
    "ids",
    "okay",
    "token",
    "type",
    "ids",
    "token",
    "type",
    "ids",
    "represents",
    "type",
    "sentence",
    "input",
    "type",
    "input",
    "prompt",
    "input",
    "question",
    "like",
    "order",
    "provide",
    "variation",
    "different",
    "sentences",
    "actually",
    "uses",
    "token",
    "type",
    "ids",
    "okay",
    "order",
    "tell",
    "differentiation",
    "input",
    "inputs",
    "token",
    "ids",
    "given",
    "okay",
    "attention",
    "mass",
    "already",
    "told",
    "attention",
    "mass",
    "nothing",
    "order",
    "specify",
    "padding",
    "padding",
    "order",
    "specify",
    "paddings",
    "attention",
    "masks",
    "order",
    "observe",
    "padding",
    "well",
    "let",
    "let",
    "take",
    "one",
    "sentence",
    "example",
    "provide",
    "two",
    "sentences",
    "model",
    "let",
    "say",
    "sentence",
    "one",
    "sentence",
    "one",
    "nothing",
    "developer",
    "developer",
    "see",
    "two",
    "generate",
    "different",
    "length",
    "tokens",
    "right",
    "yes",
    "two",
    "sentences",
    "generate",
    "different",
    "length",
    "tokens",
    "comma",
    "sentence",
    "one",
    "passing",
    "tokenizer",
    "passing",
    "sentence",
    "well",
    "sentence",
    "one",
    "nothing",
    "two",
    "sentences",
    "passing",
    "padding",
    "order",
    "balance",
    "length",
    "nothing",
    "first",
    "first",
    "sentence",
    "generate",
    "many",
    "tokens",
    "second",
    "sentence",
    "might",
    "many",
    "tokens",
    "wo",
    "generated",
    "print",
    "well",
    "want",
    "yeah",
    "let",
    "print",
    "make",
    "clear",
    "sentence",
    "one",
    "also",
    "taking",
    "tokens",
    "one",
    "printing",
    "let",
    "rerun",
    "able",
    "observe",
    "output",
    "see",
    "yes",
    "number",
    "tokens",
    "many",
    "number",
    "tokens",
    "many",
    "right",
    "yes",
    "applied",
    "padding",
    "extra",
    "two",
    "bits",
    "taken",
    "padding",
    "bits",
    "one",
    "let",
    "say",
    "want",
    "apply",
    "padding",
    "order",
    "balance",
    "two",
    "sentences",
    "want",
    "apply",
    "padding",
    "equals",
    "true",
    "want",
    "apply",
    "padding",
    "let",
    "rerun",
    "see",
    "attention",
    "mask",
    "see",
    "attention",
    "mask",
    "applied",
    "padding",
    "see",
    "extra",
    "two",
    "zeros",
    "got",
    "added",
    "means",
    "two",
    "extra",
    "tokens",
    "first",
    "sentence",
    "see",
    "first",
    "sentence",
    "related",
    "attention",
    "mask",
    "paddings",
    "attention",
    "mask",
    "paddings",
    "related",
    "second",
    "sentence",
    "sentence",
    "one",
    "see",
    "two",
    "zeros",
    "added",
    "nothing",
    "two",
    "extra",
    "padding",
    "bits",
    "got",
    "added",
    "whereas",
    "previously",
    "two",
    "extra",
    "paddings",
    "added",
    "padding",
    "right",
    "yes",
    "nothing",
    "two",
    "zeros",
    "two",
    "represents",
    "pad",
    "previously",
    "padding",
    "two",
    "extra",
    "zeros",
    "padding",
    "two",
    "extra",
    "zeros",
    "came",
    "order",
    "represent",
    "padding",
    "attention",
    "mask",
    "used",
    "whole",
    "data",
    "nothing",
    "data",
    "pass",
    "tokenized",
    "data",
    "embeddings",
    "model",
    "recap",
    "imported",
    "tokenizer",
    "imported",
    "tokenizer",
    "transformers",
    "library",
    "using",
    "auto",
    "transformers",
    "provided",
    "model",
    "model",
    "want",
    "create",
    "tokenizer",
    "taken",
    "sample",
    "sentence",
    "tokenized",
    "sentence",
    "well",
    "see",
    "token",
    "representation",
    "sentence",
    "well",
    "previously",
    "seen",
    "vocabulary",
    "nothing",
    "model",
    "remembering",
    "model",
    "identifying",
    "word",
    "represent",
    "numerical",
    "representation",
    "vocabulary",
    "sav",
    "saved",
    "vocab",
    "data",
    "one",
    "file",
    "seen",
    "vocabulary",
    "simply",
    "tokenized",
    "sentence",
    "nothing",
    "taking",
    "sample",
    "sentence",
    "tokenized",
    "sentence",
    "using",
    "using",
    "tokenizer",
    "tokenized",
    "function",
    "encoded",
    "sentence",
    "nothing",
    "generated",
    "token",
    "ids",
    "tokens",
    "also",
    "understood",
    "parameters",
    "pass",
    "model",
    "sending",
    "data",
    "see",
    "provided",
    "tokenized",
    "inputs",
    "nothing",
    "input",
    "ids",
    "passing",
    "token",
    "type",
    "ids",
    "nothing",
    "order",
    "represent",
    "token",
    "type",
    "order",
    "represent",
    "different",
    "inputs",
    "nothing",
    "take",
    "example",
    "question",
    "answering",
    "say",
    "context",
    "know",
    "differentiate",
    "different",
    "types",
    "inputs",
    "token",
    "type",
    "ids",
    "created",
    "attention",
    "mass",
    "actually",
    "represent",
    "padding",
    "extra",
    "padding",
    "happened",
    "represent",
    "see",
    "extra",
    "zeros",
    "representing",
    "using",
    "attention",
    "mass",
    "tokenization",
    "depth",
    "analysis",
    "hope",
    "understood",
    "video",
    "understood",
    "important",
    "detailed",
    "concepts",
    "observe",
    "detailed",
    "observation",
    "tokenization",
    "model",
    "model",
    "understood",
    "numerical",
    "representation",
    "create",
    "using",
    "tokenizers",
    "actually",
    "vocabulary",
    "token",
    "tokenizer",
    "model",
    "pass",
    "pass",
    "input",
    "tokenizer",
    "see",
    "data",
    "pass",
    "input",
    "tokenizer",
    "also",
    "observed",
    "major",
    "components",
    "video",
    "hope",
    "understood",
    "video",
    "got",
    "clear",
    "conceptual",
    "knowledge",
    "feel",
    "video",
    "useful",
    "please",
    "like",
    "supporting",
    "type",
    "comment",
    "able",
    "understood",
    "feeling",
    "forget",
    "subscribe",
    "channel",
    "watch",
    "informative",
    "videos",
    "thanks",
    "watching",
    "see",
    "back",
    "next",
    "video",
    "another",
    "interesting",
    "topic",
    "guys"
  ],
  "keywords": [
    "video",
    "let",
    "see",
    "tokenization",
    "question",
    "need",
    "say",
    "want",
    "like",
    "text",
    "model",
    "provide",
    "get",
    "related",
    "observe",
    "also",
    "able",
    "using",
    "one",
    "order",
    "based",
    "pass",
    "data",
    "right",
    "yes",
    "passing",
    "type",
    "numerical",
    "input",
    "convert",
    "representation",
    "actually",
    "sentence",
    "nothing",
    "first",
    "understood",
    "developer",
    "send",
    "whatever",
    "particular",
    "represents",
    "word",
    "vocabulary",
    "stored",
    "two",
    "store",
    "already",
    "library",
    "auto",
    "tokenizer",
    "yeah",
    "tokenize",
    "use",
    "every",
    "well",
    "load",
    "dot",
    "print",
    "represented",
    "many",
    "token",
    "length",
    "represent",
    "tokens",
    "padding",
    "sentences",
    "special",
    "extra",
    "provided",
    "last",
    "added",
    "mask",
    "format",
    "dictionary",
    "frame",
    "got",
    "ids",
    "values",
    "rerun",
    "ending",
    "sort",
    "id",
    "providing",
    "index",
    "csv",
    "okay",
    "corresponding",
    "different",
    "taken",
    "tokenized",
    "decode",
    "generate",
    "attention",
    "zeros"
  ]
}