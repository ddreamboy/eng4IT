{
  "text": "This course from Harvard University explores the concepts and algorithms at the foundation of modern\nartificial intelligence, diving into the ideas that give rise to technologies like game-playing\nengines, handwriting recognition, and machine translation. You'll gain exposure to the theory\nbehind graph search algorithms, classification, optimization, reinforcement learning,\nand other topics in artificial intelligence and machine learning. Brian Yu teaches this course.\nHello, world. This is CS50, and this is an introduction to artificial intelligence with\nPython with CS50's own Brian Yu. This course picks up where CS50 itself leaves off and explores the\nconcepts and algorithms at the foundation of modern AI.\nWe'll start with a look at how AI can search for solutions to problems,\nwhether those problems are learning how to play a game or trying\nto find driving directions to a destination.\nWe'll then look at how AI can represent information, both knowledge that our AI\nis certain about, but also information and events about which our AI might be uncertain,\nlearning how to represent that information, but more importantly,\nhow to use that information to draw inferences and new conclusions as well.\nWe'll explore how AI can solve various types of optimization problems,\ntrying to maximize profits or minimize costs or satisfy some other constraints\nbefore turning our attention to the fast-growing field of machine learning,\nwhere we won't tell our AI exactly how to solve a problem, but instead,\ngive our AI access to data and experiences\nso that our AI can learn on its own how to perform these tasks.\nIn particular, we'll look at neural networks, one of the most popular tools\nin modern machine learning, inspired by the way that human brains learn and reason as well\nbefore finally taking a look at the world of natural language processing\nso that it's not just us humans learning to learn how artificial intelligence is\nable to speak, but also AI learning how to understand and interpret human language as well.\nWe'll explore these ideas and algorithms, and along the way,\ngive you the opportunity to build your own AI programs to implement all of this and more.\nThis is CS50.\nAll right.\nWelcome, everyone, to an introduction to artificial intelligence with Python.\nMy name is Brian Yu, and in this class, we'll\nexplore some of the ideas and techniques and algorithms\nthat are at the foundation of artificial intelligence.\nNow, artificial intelligence covers a wide variety of types of techniques.\nAnytime you see a computer do something that\nappears to be intelligent or rational in some way,\nlike recognizing someone's face in a photo,\nor being able to play a game better than people can,\nor being able to understand human language when we talk to our phones\nand they understand what we mean and are able to respond back to us,\nthese are all examples of AI, or artificial intelligence.\nAnd in this class, we'll explore some of the ideas that make that AI possible.\nSo we'll begin our conversations with search, the problem of we have an AI,\nand we would like the AI to be able to search for solutions to some kind of problem,\nno matter what that problem might be.\nWhether it's trying to get driving directions from point A to point B,\nor trying to figure out how to play a game, given a tic-tac-toe game,\nfor example, figuring out what move it ought to make.\nAfter that, we'll take a look at knowledge.\nIdeally, we want our AI to be able to know information,\nto be able to represent that information, and more importantly,\nto be able to draw inferences from that information,\nto be able to use the information it knows and draw additional conclusions.\nSo we'll talk about how AI can be programmed in order to do just that.\nThen we'll explore the topic of uncertainty,\ntalking about ideas of what happens if a computer isn't sure about a fact,\nbut maybe is only sure with a certain probability.\nSo we'll talk about some of the ideas behind probability,\nand how computers can begin to deal with uncertain events\nin order to be a little bit more intelligent in that sense as well.\nAfter that, we'll turn our attention to optimization,\nproblems of when the computer is trying to optimize for some sort of goal,\nespecially in a situation where there might be multiple ways\nthat a computer might solve a problem, but we're looking for a better way,\nor potentially the best way, if that's at all possible.\nThen we'll take a look at machine learning, or learning more generally,\nand looking at how, when we have access to data,\nour computers can be programmed to be quite intelligent by learning from data\nand learning from experience, being able to perform a task better and better\nbased on greater access to data.\nSo your email, for example, where your email inbox somehow knows\nwhich of your emails are good emails and which of your emails are spam.\nThese are all examples of computers being able to learn from past experiences\nand past data.\nWe'll take a look, too, at how computers are\nable to draw inspiration from human intelligence,\nlooking at the structure of the human brain,\nand how neural networks can be a computer analog to that sort of idea,\nand how, by taking advantage of a certain type of structure of a computer program,\nwe can write neural networks that are able to perform tasks very, very\neffectively.\nAnd then finally, we'll turn our attention to language, not programming\nlanguages, but human languages that we speak every day.\nAnd taking a look at the challenges that come about as a computer tries\nto understand natural language, and how it is some of the natural language\nprocessing that occurs in modern artificial intelligence can actually\nwork.\nBut today, we'll begin our conversation with search, this problem\nof trying to figure out what to do when we have some sort of situation\nthat the computer is in, some sort of environment that an agent is in,\nso to speak, and we would like for that agent\nto be able to somehow look for a solution to that problem.\nNow, these problems can come in any number of different types of formats.\nOne example, for instance, might be something\nlike this classic 15 puzzle with the sliding tiles that you might have seen.\nWhere you're trying to slide the tiles in order\nto make sure that all the numbers line up in order.\nThis is an example of what you might call a search problem.\nThe 15 puzzle begins in an initially mixed up state,\nand we need some way of finding moves to make in order\nto return the puzzle to its solved state.\nBut there are similar problems that you can frame in other ways.\nTrying to find your way through a maze, for example,\nis another example of a search problem.\nYou begin in one place, you have some goal of where you're trying to get to,\nand you need to figure out the correct sequence of actions that will take you\nfrom that initial state to the goal.\nAnd while this is a little bit abstract, any time\nwe talk about maze solving in this class,\nyou can translate it to something a little more real world.\nSomething like driving directions.\nIf you ever wonder how Google Maps is able to figure out what is the best way\nfor you to get from point A to point B, and what turns to make at what time,\ndepending on traffic, for example, it's often some sort of search algorithm.\nYou have an AI that is trying to get from an initial position\nto some sort of goal by taking some sequence of actions.\nSo we'll start our conversations today by thinking\nabout these types of search problems and what\ngoes in to solving a search problem like this in order for an AI\nto be able to find a good solution.\nIn order to do so, though, we're going to need\nto introduce a little bit of terminology, some of which I've already used.\nBut the first term we'll need to think about is an agent.\nAn agent is just some entity that perceives its environment.\nIt somehow is able to perceive the things around it\nand act on that environment in some way.\nSo in the case of the driving directions,\nyour agent might be some representation of a car that\nis trying to figure out what actions to take in order\nto arrive at a destination.\nIn the case of the 15 puzzle with the sliding tiles,\nthe agent might be the AI or the person that\nis trying to solve that puzzle to try and figure out what tiles to move\nin order to get to that solution.\nNext, we introduce the idea of a state.\nA state is just some configuration of the agent in its environment.\nSo in the 15 puzzle, for example, any state might be any one of these three,\nfor example. A state is just some configuration of the tiles.\nAnd each of these states is different and is\ngoing to require a slightly different solution.\nA different sequence of actions will be needed in each one of these\nin order to get from this initial state to the goal, which\nis where we're trying to get.\nSo the initial state, then, what is that?\nThe initial state is just the state where the agent begins.\nIt is one such state where we're going to start from.\nAnd this is going to be the starting point for our search algorithm,\nso to speak.\nWe're going to begin with this initial state\nand then start to reason about it, to think about what actions might we\napply to that initial state in order to figure out how to get from the beginning\nto the end, from the initial position to whatever our goal happens to be.\nAnd how do we make our way from that initial position to the goal?\nWell, ultimately, it's via taking actions.\nActions are just choices that we can make in any given state.\nAnd in AI, we're always going to try to formalize these ideas a little bit\nmore precisely, such that we could program them a little bit more\nmathematically, so to speak.\nSo this will be a recurring theme.\nAnd we can more precisely define actions as a function.\nWe're going to effectively define a function called actions that takes an\ninput, s, where s is going to be some state that exists inside of our environment.\nAnd actions of s is going to take the state as input and return as output\nthe set of all actions that can be executed in that state.\nAnd so it's possible that some actions are only valid in certain states\nand not in other states.\nAnd we'll see examples of that soon, too.\nSo in the case of the 15 puzzle, for example,\nthere are generally going to be four possible actions that we can do most of\nthe time.\nWe can slide a tile to the right, slide a tile to the left, slide a tile up,\nor slide a tile down, for example.\nAnd those are going to be the actions that are available to us.\nSo somehow our AI, our program, needs some encoding\nof the state, which is often going to be in some numerical format,\nand some encoding of these actions.\nBut it also needs some encoding of the relationship between these things.\nHow do the states and actions relate to one another?\nAnd in order to do that, we'll introduce to our AI a transition model, which\nwill be a description of what state we get after we perform some available\naction in some other state.\nAnd again, we can be a little bit more precise about this,\ndefine this transition model a little bit more formally, again, as a function.\nThe function is going to be a function called result that this time takes two\ninputs.\nInput number one is s, some state.\nAnd input number two is a, some action.\nAnd the output of this function result is it\nis going to give us the state that we get after we perform action a in state s.\nSo let's take a look at an example to see more precisely what this actually means.\nHere is an example of a state, of the 15 puzzle, for example.\nAnd here is an example of an action, sliding a tile to the right.\nWhat happens if we pass these as inputs to the result function?\nAgain, the result function takes this board, this state, as its first input.\nAnd it takes an action as a second input.\nAnd of course, here, I'm describing things visually\nso that you can see visually what the state is and what the action is.\nIn a computer, you might represent one of these actions\nas just some number that represents the action.\nOr if you're familiar with enums that allow\nyou to enumerate multiple possibilities,\nit might be something like that.\nAnd this state might just be represented\nas an array or two-dimensional array of all of these numbers that exist.\nBut here, we're going to show it visually just so you can see it.\nBut when we take this state and this action,\npass it into the result function, the output is a new state.\nThe state we get after we take a tile and slide it to the right,\nand this is the state we get as a result.\nIf we had a different action and a different state, for example,\nand pass that into the result function, we'd\nget a different answer altogether.\nSo the result function needs to take care\nof figuring out how to take a state and take an action and get what results.\nAnd this is going to be our transition model that\ndescribes how it is that states and actions are related to each other.\nIf we take this transition model and think about it more generally\nand across the entire problem, we can form what we might call a state space.\nThe set of all of the states we can get from the initial state\nvia any sequence of actions, by taking 0 or 1 or 2 or more actions in addition\nto that, so we could draw a diagram that looks something like this, where\nevery state is represented here by a game board, and there are arrows\nthat connect every state to every other state we can get to from that state.\nAnd the state space is much larger than what you see just here.\nThis is just a sample of what the state space might actually look like.\nAnd in general, across many search problems,\nwhether they're this particular 15 puzzle or driving directions or something else,\nthe state space is going to look something like this.\nWe have individual states and arrows that are connecting them.\nAnd oftentimes, just for simplicity, we'll\nsimplify our representation of this entire thing as a graph, some sequence\nof nodes and edges that connect nodes.\nBut you can think of this more abstract representation\nas the exact same idea.\nEach of these little circles or nodes is going\nto represent one of the states inside of our problem.\nAnd the arrows here represent the actions\nthat we can take in any particular state, taking us\nfrom one particular state to another state, for example.\nAll right.\nSo now we have this idea of nodes that are representing these states,\nactions that can take us from one state to another,\nand a transition model that defines what happens after we\ntake a particular action.\nSo the next step we need to figure out is how\nwe know when the AI is done solving the problem.\nThe AI needs some way to know when it gets to the goal that it's found the goal.\nSo the next thing we'll need to encode into our artificial intelligence\nis a goal test, some way to determine whether a given state is a goal state.\nIn the case of something like driving directions, it might be pretty easy.\nIf you're in a state that corresponds to whatever the user typed\nin as their intended destination, well, then you know you're in a goal state.\nIn the 15 puzzle, it might be checking the numbers\nto make sure they're all in ascending order.\nBut the AI needs some way to encode whether or not\nany state they happen to be in is a goal.\nAnd some problems might have one goal, like a maze\nwhere you have one initial position and one ending position,\nand that's the goal.\nIn other more complex problems, you might imagine\nthat there are multiple possible goals.\nThat there are multiple ways to solve a problem,\nand we might not care which one the computer finds,\nas long as it does find a particular goal.\nHowever, sometimes the computer doesn't just care about finding a goal,\nbut finding a goal well, or one with a low cost.\nAnd it's for that reason that the last piece of terminology\nthat we'll use to define these search problems\nis something called a path cost.\nYou might imagine that in the case of driving directions,\nit would be pretty annoying if I said I wanted directions from point A\nto point B, and the route that Google Maps gave me\nwas a long route with lots of detours that were unnecessary that took longer\nthan it should have for me to get to that destination.\nAnd it's for that reason that when we're formulating search problems,\nwe'll often give every path some sort of numerical cost,\nsome number telling us how expensive it is to take this particular option,\nand then tell our AI that instead of just finding\na solution, some way of getting from the initial state to the goal,\nwe'd really like to find one that minimizes this path cost.\nThat is, less expensive, or takes less time,\nor minimizes some other numerical value.\nWe can represent this graphically if we take a look at this graph again,\nand imagine that each of these arrows, each of these actions\nthat we can take from one state to another state,\nhas some sort of number associated with it.\nThat number being the path cost of this particular action,\nwhere some of the costs for any particular action\nmight be more expensive than the cost for some other action, for example.\nAlthough this will only happen in some sorts of problems.\nIn other problems, we can simplify the diagram\nand just assume that the cost of any particular action is the same.\nAnd this is probably the case in something like the 15 puzzle,\nfor example, where it doesn't really make a difference\nwhether I'm moving right or moving left.\nThe only thing that matters is the total number\nof steps that I have to take to get from point A to point B.\nAnd each of those steps is of equal cost.\nWe can just assume it's of some constant cost like one.\nAnd so this now forms the basis for what we might consider to be a search problem.\nA search problem has some sort of initial state, some place where we begin,\nsome sort of action that we can take or multiple actions\nthat we can take in any given state.\nAnd it has a transition model.\nSome way of defining what happens when we go from one state\nand take one action, what state do we end up with as a result.\nIn addition to that, we need some goal test\nto know whether or not we've reached a goal.\nAnd then we need a path cost function that\ntells us for any particular path, by following some sequence of actions,\nhow expensive is that path.\nWhat does its cost in terms of money or time or some other resource\nthat we are trying to minimize our usage of.\nAnd the goal ultimately is to find a solution.\nWhere a solution in this case is just some sequence of actions\nthat will take us from the initial state to the goal state.\nAnd ideally, we'd like to find not just any solution\nbut the optimal solution, which is a solution that\nhas the lowest path cost among all of the possible solutions.\nAnd in some cases, there might be multiple optimal solutions.\nBut an optimal solution just means that there\nis no way that we could have done better in terms of finding that solution.\nSo now we've defined the problem.\nAnd now we need to begin to figure out how it\nis that we're going to solve this kind of search problem.\nAnd in order to do so, you'll probably imagine\nthat our computer is going to need to represent a whole bunch of data\nabout this particular problem.\nWe need to represent data about where we are in the problem.\nAnd we might need to be considering multiple different options at once.\nAnd oftentimes, when we're trying to package a whole bunch of data\nrelated to a state together, we'll do so using a data structure\nthat we're going to call a node.\nA node is a data structure that is just going\nto keep track of a variety of different values.\nAnd specifically, in the case of a search problem,\nit's going to keep track of these four values in particular.\nEvery node is going to keep track of a state, the state we're currently on.\nAnd every node is also going to keep track of a parent.\nA parent being the state before us or the node\nthat we used in order to get to this current state.\nAnd this is going to be relevant because eventually, once we reach the goal node,\nonce we get to the end, we want to know what sequence of actions\nwe use in order to get to that goal.\nAnd the way we'll know that is by looking at these parents\nto keep track of what led us to the goal and what led us to that state\nand what led us to the state before that, so on and so forth,\nbacktracking our way to the beginning so that we\nknow the entire sequence of actions we needed in order\nto get from the beginning to the end.\nThe node is also going to keep track of what action we took in order\nto get from the parent to the current state.\nAnd the node is also going to keep track of a path cost.\nIn other words, it's going to keep track of the number\nthat represents how long it took to get from the initial state\nto the state that we currently happen to be at.\nAnd we'll see why this is relevant as we\nstart to talk about some of the optimizations\nthat we can make in terms of these search problems more generally.\nSo this is the data structure that we're going to use in order to solve\nthe problem.\nAnd now let's talk about the approach.\nHow might we actually begin to solve the problem?\nWell, as you might imagine, what we're going to do\nis we're going to start at one particular state,\nand we're just going to explore from there.\nThe intuition is that from a given state,\nwe have multiple options that we could take,\nand we're going to explore those options.\nAnd once we explore those options, we'll\nfind that more options than that are going to make themselves available.\nAnd we're going to consider all of the available options\nto be stored inside of a single data structure that we'll call the frontier.\nThe frontier is going to represent all of the things\nthat we could explore next that we haven't yet explored or visited.\nSo in our approach, we're going to begin the search algorithm\nby starting with a frontier that just contains one state.\nThe frontier is going to contain the initial state,\nbecause at the beginning, that's the only state we know about.\nThat is the only state that exists.\nAnd then our search algorithm is effectively going to follow a loop.\nWe're going to repeat some process again and again and again.\nThe first thing we're going to do is if the frontier is empty,\nthen there's no solution.\nAnd we can report that there is no way to get to the goal.\nAnd that's certainly possible.\nThere are certain types of problems that an AI might try to explore\nand realize that there is no way to solve that problem.\nAnd that's useful information for humans to know as well.\nSo if ever the frontier is empty, that means there's nothing left to explore.\nAnd we haven't yet found a solution, so there is no solution.\nThere's nothing left to explore.\nOtherwise, what we'll do is we'll remove a node from the frontier.\nSo right now at the beginning, the frontier just contains one node\nrepresenting the initial state.\nBut over time, the frontier might grow.\nIt might contain multiple states.\nAnd so here, we're just going to remove a single node from that frontier.\nIf that node happens to be a goal, then we found a solution.\nSo we remove a node from the frontier and ask ourselves, is this the goal?\nAnd we do that by applying the goal test that we talked about earlier,\nasking if we're at the destination.\nOr asking if all the numbers of the 15 puzzle happen to be in order.\nSo if the node contains the goal, we found a solution.\nGreat.\nWe're done.\nAnd otherwise, what we'll need to do is we'll need to expand the node.\nAnd this is a term of art in artificial intelligence.\nTo expand the node just means to look at all of the neighbors of that node.\nIn other words, consider all of the possible actions\nthat I could take from the state that this node is representing\nand what nodes could I get to from there.\nWe're going to take all of those nodes, the next nodes\nthat I can get to from this current one I'm looking at,\nand add those to the frontier.\nAnd then we'll repeat this process.\nSo at a very high level, the idea is we start\nwith a frontier that contains the initial state.\nAnd we're constantly removing a node from the frontier,\nlooking at where we can get to next and adding those nodes to the frontier,\nrepeating this process over and over until either we\nremove a node from the frontier and it contains a goal,\nmeaning we've solved the problem, or we run into a situation\nwhere the frontier is empty, at which point we're left with no solution.\nSo let's actually try and take the pseudocode,\nput it into practice by taking a look at an example of a sample search problem.\nSo right here, I have a sample graph.\nA is connected to B via this action.\nB is connected to nodes C and D. C is connected to E. D is connected to F.\nAnd what I'd like to do is have my AI find a path from A to E.\nWe want to get from this initial state to this goal state.\nSo how are we going to do that?\nWell, we're going to start with a frontier that contains the initial state.\nThis is going to represent our frontier.\nSo our frontier initially will just contain\nA, that initial state where we're going to begin.\nAnd now we'll repeat this process.\nIf the frontier is empty, no solution.\nThat's not a problem, because the frontier is not empty.\nSo we'll remove a node from the frontier as the one to consider next.\nThere's only one node in the frontier.\nSo we'll go ahead and remove it from the frontier.\nBut now A, this initial node, this is the node we're currently considering.\nWe follow the next step.\nWe ask ourselves, is this node the goal?\nNo, it's not.\nA is not the goal.\nE is the goal.\nSo we don't return the solution.\nSo instead, we go to this last step, expand the node,\nand add the resulting nodes to the frontier.\nWhat does that mean?\nWell, it means take this state A and consider where we could get to next.\nAnd after A, what we could get to next is only B.\nSo that's what we get when we expand A. We find B.\nAnd we add B to the frontier.\nAnd now B is in the frontier.\nAnd we repeat the process again.\nWe say, all right, the frontier is not empty.\nSo let's remove B from the frontier.\nB is now the node that we're considering.\nWe ask ourselves, is B the goal?\nNo, it's not.\nSo we go ahead and expand B and add its resulting nodes to the frontier.\nWhat happens when we expand B?\nIn other words, what nodes can we get to from B?\nWell, we can get to C and D. So we'll go ahead and add C and D\nfrom the frontier.\nAnd now we have two nodes in the frontier, C and D.\nAnd we repeat the process again.\nWe remove a node from the frontier.\nFor now, I'll do so arbitrarily just by picking C.\nWe'll see why later, how choosing which node you remove from the frontier\nis actually quite an important part of the algorithm.\nBut for now, I'll arbitrarily remove C, say it's not the goal.\nSo we'll add E, the next one, to the frontier.\nThen let's say I remove E from the frontier.\nAnd now I check I'm currently looking at state E. Is it a goal state?\nIt is, because I'm trying to find a path from A to E. So I would return the goal.\nAnd that now would be the solution, that I'm now able to return the solution.\nAnd I have found a path from A to E.\nSo this is the general idea, the general approach of this search algorithm,\nto follow these steps, constantly removing nodes from the frontier,\nuntil we're able to find a solution.\nSo the next question you might reasonably ask is, what could go wrong here?\nWhat are the potential problems with an approach like this?\nAnd here's one example of a problem that could arise from this sort of approach.\nImagine this same graph, same as before, with one change.\nThe change being now, instead of just an arrow from A to B,\nwe also have an arrow from B to A, meaning we can go in both directions.\nAnd this is true in something like the 15 puzzle, where when I slide a tile\nto the right, I could then slide a tile to the left\nto get back to the original position.\nI could go back and forth between A and B.\nAnd that's what these double arrows symbolize,\nthe idea that from one state, I can get to another, and then I can get back.\nAnd that's true in many search problems.\nWhat's going to happen if I try to apply the same approach now?\nWell, I'll begin with A, same as before.\nAnd I'll remove A from the frontier.\nAnd then I'll consider where I can get to from A.\nAnd after A, the only place I can get to is B. So B goes into the frontier.\nThen I'll say, all right, let's take a look at B.\nThat's the only thing left in the frontier.\nWhere can I get to from B?\nBefore, it was just C and D. But now, because of that reverse arrow,\nI can get to A or C or D. So all three, A, C, and D, all of those\nnow go into the frontier.\nThey are places I can get to from B. And now I remove one from the frontier.\nAnd maybe I'm unlucky, and maybe I pick A. And now I'm looking at A again.\nAnd I consider, where can I get to from A?\nAnd from A, well, I can get to B. And now we start to see the problem.\nBut if I'm not careful, I go from A to B, and then back to A, and then to B again.\nAnd I could be going in this infinite loop, where I never make any progress,\nbecause I'm constantly just going back and forth between two states\nthat I've already seen.\nSo what is the solution to this?\nWe need some way to deal with this problem.\nAnd the way that we can deal with this problem\nis by somehow keeping track of what we've already explored.\nAnd the logic is going to be, well, if we've already explored the state,\nthere's no reason to go back to it.\nOnce we've explored a state, don't go back to it.\nDon't bother adding it to the frontier.\nThere's no need to.\nSo here's going to be our revised approach, a better way\nto approach this sort of search problem.\nAnd it's going to look very similar, just with a couple of modifications.\nWe'll start with a frontier that contains the initial state, same as before.\nBut now we'll start with another data structure, which\nwill just be a set of nodes that we've already explored.\nSo what are the states we've explored?\nInitially, it's empty.\nWe have an empty explored set.\nAnd now we repeat.\nIf the frontier is empty, no solution, same as before.\nWe remove a node from the frontier.\nWe check to see if it's a goal state, return the solution.\nNone of this is any different so far.\nBut now what we're going to do is we're going to add the node\nto the explored state.\nSo if it happens to be the case that we remove a node from the frontier\nand it's not the goal, we'll add it to the explored set\nso that we know we've already explored it.\nWe don't need to go back to it again if it happens to come up later.\nAnd then the final step, we expand the node\nand we add the resulting nodes to the frontier.\nBut before, we just always added the resulting nodes to the frontier.\nWe're going to be a little clever about it this time.\nWe're only going to add the nodes to the frontier\nif they aren't already in the frontier and if they aren't already\nin the explored set.\nSo we'll check both the frontier and the explored set,\nmake sure that the node isn't already in one of those two.\nAnd so long as it isn't, then we'll go ahead and add it to the frontier,\nbut not otherwise.\nAnd so that revised approach is ultimately\nwhat's going to help make sure that we don't go back and forth between two\nnodes.\nNow, the one point that I've kind of glossed over here so far\nis this step here, removing a node from the frontier.\nBefore, I just chose arbitrarily.\nLike, let's just remove a node and that's it.\nBut it turns out it's actually quite important how\nwe decide to structure our frontier, how we add and how we remove our nodes.\nThe frontier is a data structure and we need\nto make a choice about in what order are we\ngoing to be removing elements.\nAnd one of the simplest data structures for adding and removing elements\nis something called a stack.\nAnd a stack is a data structure that is a last in, first out data type, which\nmeans the last thing that I add to the frontier\nis going to be the first thing that I remove from the frontier.\nSo the most recent thing to go into the stack or the frontier in this case\nis going to be the node that I explore.\nSo let's see what happens if I apply this stack-based approach to something\nlike this problem, finding a path from A to E. What's going to happen?\nWell, again, we'll start with A and we'll say, all right,\nlet's go ahead and look at A first.\nAnd then notice this time, we've added A to the explored set.\nA is something we've now explored.\nWe have this data structure that's keeping track.\nWe then say from A, we can get to B. And all right, from B, what can we do?\nWell, from B, we can explore B and get to both C and D.\nSo we added C and then D. So now,\nwhen we explore a node, we're going to treat the frontier as a stack,\nlast in, first out.\nD was the last one to come in.\nSo we'll go ahead and explore that next and say, all right,\nwhere can we get to from D?\nWell, we can get to F. And so all right, we'll put F into the frontier.\nAnd now, because the frontier is a stack,\nF is the most recent thing that's gone in the stack.\nSo F is what we'll explore next.\nWe'll explore F and say, all right, where can we get to from F?\nWell, we can't get anywhere, so nothing gets added to the frontier.\nSo now, what was the new most recent thing added to the frontier?\nWell, it's now C, the only thing left in the frontier.\nWe'll explore that from which we can see, all right, from C, we can get to E.\nSo E goes into the frontier.\nAnd then we say, all right, let's look at E. And E is now the solution.\nAnd now, we've solved the problem.\nSo when we treat the frontier like a stack, a last in,\nfirst out data structure, that's the result we get.\nWe go from A to B to D to F. And then we sort of backed up and went down to C\nand then E.\nAnd it's important to get a visual sense for how this algorithm is working.\nWe went very deep in this search tree, so to speak,\nall the way until the bottom where we hit a dead end.\nAnd then we effectively backed up and explored this other route\nthat we didn't try before.\nAnd it's this going very deep in the search tree idea,\nthis way the algorithm ends up working when we use a stack\nthat we call this version of the algorithm depth first search.\nDepth first search is the search algorithm\nwhere we always explore the deepest node in the frontier.\nWe keep going deeper and deeper through our search tree.\nAnd then if we hit a dead end, we back up and we try something else instead.\nBut depth first search is just one of the possible search options\nthat we could use.\nIt turns out that there's another algorithm called breadth first search,\nwhich behaves very similarly to depth first search with one difference.\nInstead of always exploring the deepest node in the search tree,\nthe way the depth first search does, breadth first search\nis always going to explore the shallowest node in the frontier.\nSo what does that mean?\nWell, it means that instead of using a stack which depth first search or DFS\nused, where the most recent item added to the frontier\nis the one we'll explore next, in breadth first search or BFS,\nwe'll instead use a queue, where a queue is a first in first out data type,\nwhere the very first thing we add to the frontier\nis the first one we'll explore and they effectively form a line or a queue,\nwhere the earlier you arrive in the frontier, the earlier you get explored.\nSo what would that mean for the same exact problem,\nfinding a path from A to E?\nWell, we start with A, same as before, then we'll go ahead and have explored A\nand say, where can we get to from A?\nWell, from A, we can get to B, same as before.\nFrom B, same as before, we can get to C and D.\nSo C and D get added to the frontier.\nThis time, though, we added C to the frontier before D.\nSo we'll explore C first.\nSo C gets explored.\nAnd from C, where can we get to?\nWell, we can get to E. So E gets added to the frontier.\nBut because D was explored before E, we'll look at D next.\nSo we'll explore D and say, where can we get to from D?\nWe can get to F. And only then will we say, all right, now we can get to E.\nAnd so what breadth first search or BFS did is we started here,\nwe looked at both C and D, and then we looked at E.\nEffectively, we're looking at things one away from the initial state,\nthen two away from the initial state, and only then,\nthings that are three away from the initial state, unlike depth first search,\nwhich just went as deep as possible into the search tree\nuntil it hit a dead end and then ultimately had to back up.\nSo these now are two different search algorithms\nthat we could apply in order to try and solve a problem.\nAnd let's take a look at how these would actually work in practice\nwith something like maze solving, for example.\nSo here's an example of a maze.\nThese empty cells represent places where our agent can move.\nThese darkened gray cells represent walls that the agent can't pass through.\nAnd ultimately, our agent, our AI, is going to try to find a way\nto get from position A to position B via some sequence of actions,\nwhere those actions are left, right, up, and down.\nWhat will depth first search do in this case?\nWell, depth first search will just follow one path.\nIf it reaches a fork in the road where it has multiple different options,\ndepth first search is just, in this case, going to choose one.\nThat doesn't a real preference.\nBut it's going to keep following one until it hits a dead end.\nAnd when it hits a dead end, depth first search effectively\ngoes back to the last decision point and tries the other path,\nfully exhausting this entire path.\nAnd when it realizes that, OK, the goal is not here,\nthen it turns its attention to this path.\nIt goes as deep as possible.\nWhen it hits a dead end, it backs up and then tries this other path,\nkeeps going as deep as possible down one particular path.\nAnd when it realizes that that's a dead end, then it'll back up,\nand then ultimately find its way to the goal.\nAnd maybe you got lucky, and maybe you made a different choice earlier on.\nBut ultimately, this is how depth first search is going to work.\nIt's going to keep following until it hits a dead end.\nAnd when it hits a dead end, it backs up and looks for a different solution.\nAnd so one thing you might reasonably ask is,\nis this algorithm always going to work?\nWill it always actually find a way to get from the initial state?\nTo the goal.\nAnd it turns out that as long as our maze is finite,\nas long as there are only finitely many spaces where we can travel,\nthen, yes, depth first search is going to find a solution.\nBecause eventually, it'll just explore everything.\nIf the maze happens to be infinite and there's an infinite state space,\nwhich does exist in certain types of problems,\nthen it's a slightly different story.\nBut as long as our maze has finitely many squares,\nwe're going to find a solution.\nThe next question, though, that we want to ask is,\nis it going to be a good solution?\nIs it the optimal solution that we can find?\nAnd the answer there is not necessarily.\nAnd let's take a look at an example of that.\nIn this maze, for example, we're again trying to find our way from A to B.\nAnd you notice here there are multiple possible solutions.\nWe could go this way or we could go up in order to make our way from A to B.\nNow, if we're lucky, depth first search will choose this way and get to B.\nBut there's no reason necessarily why depth first search\nwould choose between going up or going to the right.\nIt's sort of an arbitrary decision point because both\nare going to be added to the frontier.\nAnd ultimately, if we get unlucky, depth first search\nmight choose to explore this path first because it's just a random choice\nat this point.\nIt'll explore, explore, explore.\nAnd it'll eventually find the goal, this particular path,\nwhen in actuality there was a better path.\nThere was a more optimal solution that used fewer steps,\nassuming we're measuring the cost of a solution based on the number of steps\nthat we need to take.\nSo depth first search, if we're unlucky,\nmight end up not finding the best solution when a better solution is\navailable.\nSo that's DFS, depth first search.\nHow does BFS, or breadth first search, compare?\nHow would it work in this particular situation?\nWell, the algorithm is going to look very different visually\nin terms of how BFS explores.\nBecause BFS looks at shallower nodes first, the idea is going to be,\nBFS will first look at all of the nodes that are one away from the initial state.\nLook here and look here, for example, just\nat the two nodes that are immediately next to this initial state.\nThen it'll explore nodes that are two away,\nlooking at this state and that state, for example.\nThen it'll explore nodes that are three away, this state and that state.\nWhereas depth first search just picked one path and kept following it,\nbreadth first search, on the other hand,\nis taking the option of exploring all of the possible paths\nas kind of at the same time bouncing back between them,\nlooking deeper and deeper at each one, but making sure\nto explore the shallower ones or the ones that\nare closer to the initial state earlier.\nSo we'll keep following this pattern, looking at things that are four away,\nlooking at things that are five away, looking at things that are six away,\nuntil eventually we make our way to the goal.\nAnd in this case, it's true we had to explore some states that ultimately\ndidn't lead us anywhere, but the path that we found to the goal\nwas the optimal path.\nThis is the shortest way that we could get to the goal.\nAnd so what might happen then in a larger maze?\nWell, let's take a look at something like this\nand how breadth first search is going to behave.\nWell, breadth first search, again, we'll just keep following the states\nuntil it receives a decision point.\nIt could go either left or right.\nAnd while DFS just picked one and kept following that until it hit a dead end,\nBFS, on the other hand, will explore both.\nIt'll say look at this node, then this node,\nand it'll look at this node, then that node.\nSo on and so forth.\nAnd when it hits a decision point here, rather than pick one left or two\nright and explore that path, it'll again explore both,\nalternating between them, going deeper and deeper.\nWe'll explore here, and then maybe here and here, and then keep going.\nExplore here and slowly make our way, you can visually\nsee, further and further out.\nOnce we get to this decision point, we'll explore both up and down\nuntil ultimately we make our way to the goal.\nAnd what you'll notice is, yes, breadth first search\ndid find our way from A to B by following this particular path,\nbut it needed to explore a lot of states in order to do so.\nAnd so we see some trade offs here between DFS and BFS,\nthat in DFS, there may be some cases where there is some memory savings\nas compared to a breadth first approach, where breadth first search in this case\nhad to explore a lot of states.\nBut maybe that won't always be the case.\nSo now let's actually turn our attention to some code\nand look at the code that we could actually\nwrite in order to implement something like depth first search or breadth\nfirst search in the context of solving a maze, for example.\nSo I'll go ahead and go into my terminal.\nAnd what I have here inside of maze.py is an implementation\nof this same idea of maze solving.\nI've defined a class called node that in this case\nis keeping track of the state, the parent, in other words,\nthe state before the state, and the action.\nIn this case, we're not keeping track of the path cost\nbecause we can calculate the cost of the path at the end\nafter we found our way from the initial state to the goal.\nIn addition to this, I've defined a class called a stack frontier.\nAnd if unfamiliar with a class, a class is a way for me\nto define a way to generate objects in Python.\nIt refers to an idea of object oriented programming, where the idea here\nis that I would like to create an object that is\nable to store all of my frontier data.\nAnd I would like to have functions, otherwise known\nas methods, on that object that I can use to manipulate the object.\nAnd so what's going on here, if unfamiliar with the syntax,\nis I have a function that initially creates a frontier that I'm\ngoing to represent using a list.\nAnd initially, my frontier is represented by the empty list.\nThere's nothing in my frontier to begin with.\nI have an add function that adds something to the frontier\nas by appending it to the end of the list.\nI have a function that checks if the frontier contains\na particular state.\nI have an empty function that checks if the frontier is empty.\nIf the frontier is empty, that just means the length of the frontier is 0.\nAnd then I have a function for removing something from the frontier.\nI can't remove something from the frontier if the frontier is empty,\nso I check for that first.\nBut otherwise, if the frontier isn't empty,\nrecall that I'm implementing this frontier as a stack, a last in first\nout data structure, which means the last thing I add to the frontier,\nin other words, the last thing in the list, is the item\nthat I should remove from this frontier.\nSo what you'll see here is I have removed the last item of a list.\nAnd if you index into a Python list with negative 1,\nthat gets you the last item in the list.\nSince 0 is the first item, negative 1 kind of wraps around\nand gets you to the last item in the list.\nSo we give that the node.\nWe call that node.\nWe update the frontier here on line 28 to say,\ngo ahead and remove that node that you just removed from the frontier.\nAnd then we return the node as a result.\nSo this class here effectively implements the idea of a frontier.\nIt gives me a way to add something to a frontier\nand a way to remove something from the frontier as a stack.\nI've also, just for good measure, implemented\nan alternative version of the same thing called a queue frontier, which\nin parentheses you'll see here, it inherits from a stack frontier,\nmeaning it's going to do all the same things that the stack frontier did,\nexcept the way we remove a node from the frontier\nis going to be slightly different.\nInstead of removing from the end of the list the way we would in a stack,\nwe're instead going to remove from the beginning of the list.\nSelf.frontier 0 will get me the first node in the frontier, the first one\nthat was added, and that is going to be the one\nthat we return in the case of a queue.\nThen under here, I have a definition of a class called maze.\nThis is going to handle the process of taking a sequence, a maze-like text\nfile, and figuring out how to solve it.\nSo it will take as input a text file that looks something like this,\nfor example, where we see hash marks that are here representing walls,\nand I have the character A representing the starting position\nand the character B representing the ending position.\nAnd you can take a look at the code for parsing this text file right now.\nThat's the less interesting part.\nThe more interesting part is this solve function here,\nthe solve function is going to figure out\nhow to actually get from point A to point B.\nAnd here we see an implementation of the exact same idea\nwe saw from a moment ago.\nWe're going to keep track of how many states we've explored,\njust so we can report that data later.\nBut I start with a node that represents just the start state.\nAnd I start with a frontier that, in this case, is a stack frontier.\nAnd given that I'm treating my frontier as a stack,\nyou might imagine that the algorithm I'm using here is now depth-first search,\nbecause depth-first search, or DFS, uses a stack as its data structure.\nAnd initially, this frontier is just going to contain the start state.\nWe initialize an explored set that initially is empty.\nThere's nothing we've explored so far.\nAnd now here's our loop, that notion of repeating something again and again.\nFirst, we check if the frontier is empty by calling that empty function\nthat we saw the implementation of a moment ago.\nAnd if the frontier is indeed empty, we'll\ngo ahead and raise an exception, or a Python error, to say,\nsorry, there is no solution to this problem.\nOtherwise, we'll go ahead and remove a node from the frontier\nas by calling frontier.remove and update the number of states we've explored,\nbecause now we've explored one additional state.\nSo we say self.numexplored plus equals 1, adding 1\nto the number of states we've explored.\nOnce we remove a node from the frontier,\nrecall that the next step is to see whether or not\nit's the goal, the goal test.\nAnd in the case of the maze, the goal is pretty easy.\nI check to see whether the state of the node is equal to the goal.\nInitially, when I set up the maze, I set up\nthis value called goal, which is a property of the maze,\nso I can just check to see if the node is actually the goal.\nAnd if it is the goal, then what I want to do\nis backtrack my way towards figuring out what actions I took in order\nto get to this goal.\nAnd how do I do that?\nWe'll recall that every node stores its parent, the node that came before it\nthat we used to get to this node, and also the action used in order to get\nthere.\nSo I can create this loop where I'm constantly just looking\nat the parent of every node and keeping track for all of the parents\nwhat action I took to get from the parent to this current node.\nSo this loop is going to keep repeating this process\nof looking through all of the parent nodes\nuntil we get back to the initial state, which\nhas no parent, where node.parent is going to be equal to none.\nAs I do so, I'm going to be building up the list of all of the actions\nthat I'm following and the list of all the cells that are part of the solution.\nBut I'll reverse them because when I build it up,\ngoing from the goal back to the initial state\nand building the sequence of actions from the goal to the initial state,\nbut I want to reverse them in order to get the sequence of actions\nfrom the initial state to the goal.\nAnd that is ultimately going to be the solution.\nSo all of that happens if the current state is equal to the goal.\nAnd otherwise, if it's not the goal, well,\nthen I'll go ahead and add this state to the explored set to say,\nI've explored this state now.\nNo need to go back to it if I come across it in the future.\nAnd then this logic here implements the idea of adding neighbors to the frontier.\nI'm saying, look at all of my neighbors, and I\nimplemented a function called neighbors that you can take a look at.\nAnd for each of those neighbors, I'm going to check,\nis the state already in the frontier?\nIs the state already in the explored set?\nAnd if it's not in either of those, then I'll go ahead and add this new child\nnode, this new node, to the frontier.\nSo there's a fair amount of syntax here,\nbut the key here is not to understand all the nuances of the syntax.\nSo feel free to take a closer look at this file on your own\nto get a sense for how it is working.\nBut the key is to see how this is an implementation\nof the same pseudocode, the same idea that we were describing a moment ago\non the screen when we were looking at the steps\nthat we might follow in order to solve this kind of search problem.\nSo now let's actually see this in action.\nI'll go ahead and run maze.py on maze1.txt, for example.\nAnd what we'll see is here, we have a printout\nof what the maze initially looked like.\nAnd then here down below is after we've solved it.\nWe had to explore 11 states in order to do it,\nand we found a path from A to B. And in this program,\nI just happened to generate a graphical representation of this as well.\nSo I can open up maze.png, which is generated\nby this program, that shows you where in the darker color here are the walls,\nred is the initial state, green is the goal,\nand yellow is the path that was followed.\nWe found a path from the initial state to the goal.\nBut now let's take a look at a more sophisticated maze\nto see what might happen instead.\nLet's look now at maze2.txt.\nWe're now here.\nWe have a much larger maze.\nAgain, we're trying to find our way from point A to point B.\nBut now you'll imagine that depth-first search might not be so lucky.\nIt might not get the goal on the first try.\nIt might have to follow one path, then backtrack and explore something else\na little bit later.\nSo let's try this.\nWe'll run python maze.py of maze2.txt, this time trying on this other maze.\nAnd now, depth-first search is able to find a solution.\nHere, as indicated by the stars, is a way to get from A to B.\nAnd we can represent this visually by opening up this maze.\nHere's what that maze looks like, and highlighted in yellow\nis the path that was found from the initial state to the goal.\nBut how many states did we have to explore before we found that path?\nWell, recall that in my program, I was keeping\ntrack of the number of states that we've explored so far.\nAnd so I can go back to the terminal and see that, all right,\nin order to solve this problem, we had to explore 399 different states.\nAnd in fact, if I make one small modification of the program\nand tell the program at the end when we output this image,\nI added an argument called show explored.\nAnd if I set show explored equal to true and rerun this program,\npython maze.py, running it on maze2, and then I open the maze, what you'll see\nhere is highlighted in red are all of the states\nthat had to be explored to get from the initial state to the goal.\nDepth-first search, or DFS, didn't find its way to the goal right away.\nIt made a choice to first explore this direction.\nAnd when it explored this direction, it had\nto follow every conceivable path all the way to the very end,\neven this long and winding one, in order to realize that, you know what?\nThat's a dead end.\nAnd instead, the program needed to backtrack.\nAfter going this direction, it must have gone this direction.\nIt got lucky here by just not choosing this path,\nbut it got unlucky here, exploring this direction, exploring a bunch of states\nit didn't need to, and then likewise exploring\nall of this top part of the graph when it probably\ndidn't need to do that either.\nSo all in all, depth-first search here really not performing optimally,\nor probably exploring more states than it needs to.\nIt finds an optimal solution, the best path to the goal,\nbut the number of states needed to explore in order to do so,\nthe number of steps I had to take, that was much higher.\nSo let's compare.\nHow would breadth-first search, or BFS, do on this exact same maze instead?\nAnd in order to do so, it's a very easy change.\nThe algorithm for DFS and BFS is identical with the exception\nof what data structure we use to represent the frontier,\nthat in DFS, I used a stack frontier, last in, first out,\nwhereas in BFS, I'm going to use a queue frontier, first in, first out,\nwhere the first thing I add to the frontier is the first thing that I\nremove.\nSo I'll go back to the terminal, rerun this program on the same maze,\nand now you'll see that the number of states\nwe had to explore was only 77 as compared to almost 400\nwhen we used depth-first search.\nAnd we can see exactly why.\nWe can see what happened if we open up maze.png now and take a look.\nAgain, yellow highlight is the solution that breadth-first search found,\nwhich incidentally is the same solution that depth-first search found.\nThey're both finding the best solution.\nBut notice all the white unexplored cells.\nThere was much fewer states that needed to be explored in order\nto make our way to the goal because breadth-first search operates\na little more shallowly.\nIt's exploring things that are close to the initial state\nwithout exploring things that are further away.\nSo if the goal is not too far away, then breadth-first search\ncan actually behave quite effectively on a maze that\nlooks a little something like this.\nNow, in this case, both BFS and DFS ended up finding the same solution,\nbut that won't always be the case.\nAnd in fact, let's take a look at one more example.\nFor instance, maze3.txt.\nIn maze3.txt, notice that here there are multiple ways\nthat you could get from A to B. It's a relatively small maze,\nbut let's look at what happens.\nIf I use, and I'll go ahead and turn off show explored\nso we just see the solution.\nIf I use BFS, breadth-first search, to solve maze3.txt,\nwell, then we find a solution, and if I open up the maze,\nhere is the solution that we found.\nIt is the optimal one.\nWith just four steps, we can get from the initial state\nto what the goal happens to be.\nBut what happens if we tried to use depth-first search or DFS instead?\nWell, again, I'll go back up to my Q frontier, where Q frontier means\nthat we're using breadth-first search, and I'll change it to a stack frontier,\nwhich means that now we'll be using depth-first search.\nI'll rerun pythonmaze.py, and now you'll see that we find the solution,\nbut it is not the optimal solution.\nThis instead is what our algorithm finds,\nand maybe depth-first search would have found the solution.\nIt's possible, but it's not guaranteed that if we just\nhappen to be unlucky, if we choose this state instead of that state,\nthen depth-first search might find a longer route\nto get from the initial state to the goal.\nSo we do see some trade-offs here, where depth-first search might not\nfind the optimal solution.\nSo at that point, it seems like breadth-first search is pretty good.\nIs that the best we can do, where it's going to find us the optimal solution,\nand we don't have to worry about situations\nwhere we might end up finding a longer path to the solution\nthan what actually exists?\nWhere the goal is far away from the initial state,\nand we might have to take lots of steps in order\nto get from the initial state to the goal, what ended up happening\nis that this algorithm, BFS, ended up exploring basically the entire graph,\nhaving to go through the entire maze in order\nto find its way from the initial state to the goal state.\nWhat we'd ultimately like is for our algorithm\nto be a little bit more intelligent.\nAnd now what would it mean for our algorithm to be a little bit more\nintelligent in this case?\nWell, let's look back to where breadth-first search might\nhave been able to make a different decision\nand consider human intuition in this process as well.\nWhat might a human do when solving this maze\nthat is different than what BFS ultimately chose to do?\nWell, the very first decision point that BFS made was right here,\nwhen it made five steps and ended up in a position\nwhere it had a fork in the row.\nIt could either go left or it could go right.\nIn these initial couple steps, there was no choice.\nThere was only one action that could be taken from each of those states.\nAnd so the search algorithm did the only thing\nthat any search algorithm could do, which is keep following that state\nafter the next state.\nBut this decision point is where things get a little bit interesting.\nDepth-first search, that very first search algorithm we looked at,\nchose to say, let's pick one path and exhaust that path.\nSee if anything that way has the goal.\nAnd if not, then let's try the other way.\nDepth-first search took the alternative approach of saying,\nyou know what, let's explore things that are shallow, close to us first.\nLook left and right, then back left and back right, so on and so forth,\nalternating between our options in the hopes of finding something nearby.\nBut ultimately, what might a human do if confronted\nwith a situation like this of go left or go right?\nWell, a human might visually see that, all right, I'm\ntrying to get to state b, which is way up there,\nand going right just feels like it's closer to the goal.\nIt feels like going right should be better than going left\nbecause I'm making progress towards getting to that goal.\nNow, of course, there are a couple of assumptions that I'm making here.\nI'm making the assumption that we can represent this grid\nas like a two-dimensional grid where I know the coordinates of everything.\nI know that a is in coordinate 0, 0, and b is in some other coordinate pair,\nand I know what coordinate I'm at now.\nSo I can calculate that, yeah, going this way, that is closer to the goal.\nAnd that might be a reasonable assumption for some types of search problems,\nbut maybe not in others.\nBut for now, we'll go ahead and assume that,\nthat I know what my current coordinate pair is,\nand I know the coordinate, x, y, of the goal that I'm trying to get to.\nAnd in this situation, I'd like an algorithm\nthat is a little bit more intelligent, that somehow knows\nthat I should be making progress towards the goal,\nand this is probably the way to do that because in a maze,\nmoving in the coordinate direction of the goal\nis usually, though not always, a good thing.\nAnd so here we draw a distinction between two different types\nof search algorithms, uninformed search and informed search.\nUninformed search algorithms are algorithms like DFS and BFS,\nthe two algorithms that we just looked at, which\nare search strategies that don't use any problem-specific knowledge\nto be able to solve the problem.\nDFS and BFS didn't really care about the structure of the maze\nor anything about the way that a maze is in order to solve the problem.\nThey just look at the actions available and choose from those actions,\nand it doesn't matter whether it's a maze or some other problem,\nthe solution or the way that it tries to solve the problem\nis really fundamentally going to be the same.\nWhat we're going to take a look at now is an improvement\nupon uninformed search.\nWe're going to take a look at informed search.\nInformed search are going to be search strategies\nthat use knowledge specific to the problem\nto be able to better find a solution.\nAnd in the case of a maze, this problem-specific knowledge\nis something like if I'm in a square that is geographically closer to the goal,\nthat is better than being in a square that is geographically further away.\nAnd this is something we can only know by thinking about this problem\nand reasoning about what knowledge might be helpful for our AI agent\nto know a little something about.\nThere are a number of different types of informed search.\nSpecifically, first, we're going to look at a particular type of search\nalgorithm called greedy best-first search.\nGreedy best-first search, often abbreviated G-BFS,\nis a search algorithm that instead of expanding the deepest node like DFS\nor the shallowest node like BFS, this algorithm\nis always going to expand the node that it thinks is closest to the goal.\nNow, the search algorithm isn't going to know for sure\nwhether it is the closest thing to the goal.\nBecause if we knew what was closest to the goal all the time,\nthen we would already have a solution.\nThe knowledge of what is close to the goal,\nwe could just follow those steps in order to get from the initial position\nto the solution.\nBut if we don't know the solution, meaning\nwe don't know exactly what's closest to the goal,\ninstead we can use an estimate of what's closest to the goal,\notherwise known as a heuristic, just some way of estimating whether or not\nwe're close to the goal.\nAnd we'll do so using a heuristic function conventionally\ncalled h of n that takes a status input and returns\nour estimate of how close we are to the goal.\nSo what might this heuristic function actually\nlook like in the case of a maze solving algorithm?\nWhere we're trying to solve a maze, what does the heuristic look like?\nWell, the heuristic needs to answer a question\nbetween these two cells, C and D, which one is better?\nWhich one would I rather be in if I'm trying to find my way to the goal?\nWell, any human could probably look at this and tell you,\nyou know what, D looks like it's better.\nEven if the maze is convoluted and you haven't thought about all the walls,\nD is probably better.\nAnd why is D better?\nWell, because if you ignore the wall, so let's just pretend\nthe walls don't exist for a moment and relax the problem, so to speak,\nD, just in terms of coordinate pairs, is closer to this goal.\nIt's fewer steps that I wouldn't take to get to the goal as compared to C,\neven if you ignore the walls.\nIf you just know the xy-coordinate of C and the xy-coordinate of the goal,\nand likewise you know the xy-coordinate of D,\nyou can calculate the D just geographically.\nIgnoring the walls looks like it's better.\nAnd so this is the heuristic function that we're going to use.\nAnd it's something called the Manhattan distance,\none specific type of heuristic, where the heuristic is how many squares\nvertically and horizontally and then left to right,\nso not allowing myself to go diagonally, just either up or right\nor left or down.\nHow many steps do I need to take to get from each of these cells to the goal?\nWell, as it turns out, D is much closer.\nThere are fewer steps.\nIt only needs to take six steps in order to get to that goal.\nAgain, here, ignoring the walls.\nWe've relaxed the problem a little bit.\nWe're just concerned with if you do the math\nto subtract the x values from each other and the y values from each other,\nwhat is our estimate of how far we are away?\nWe can estimate the D is closer to the goal than C is.\nAnd so now we have an approach.\nWe have a way of picking which node to remove from the frontier.\nAnd at each stage in our algorithm, we're\ngoing to remove a node from the frontier.\nWe're going to explore the node if it has the smallest\nvalue for this heuristic function, if it has the smallest\nManhattan distance to the goal.\nAnd so what would this actually look like?\nWell, let me first label this graph, label this maze,\nwith a number representing the value of this heuristic function,\nthe value of the Manhattan distance from any of these cells.\nSo from this cell, for example, we're one away from the goal.\nFrom this cell, we're two away from the goal, three away, four away.\nHere, we're five away because we have to go one to the right\nand then four up.\nFrom somewhere like here, the Manhattan distance is two.\nWe're only two squares away from the goal geographically,\neven though in practice, we're going to have to take a longer path.\nBut we don't know that yet.\nThe heuristic is just some easy way to estimate\nhow far we are away from the goal.\nAnd maybe our heuristic is overly optimistic.\nIt thinks that, yeah, we're only two steps away.\nWhen in practice, when you consider the walls, it might be more steps.\nSo the important thing here is that the heuristic isn't a guarantee of how\nmany steps it's going to take.\nIt is estimating.\nIt's an attempt at trying to approximate.\nAnd it does seem generally the case that the squares that\nlook closer to the goal have smaller values for the heuristic function\nthan squares that are further away.\nSo now, using greedy best-first search, what might this algorithm actually do?\nWell, again, for these first five steps, there's not much of a choice.\nWe start at this initial state a, and we say, all right,\nwe have to explore these five states.\nBut now we have a decision point.\nNow we have a choice between going left and going right.\nAnd before, when DFS and BFS would just pick arbitrarily,\nbecause it just depends on the order you throw these two nodes into the frontier,\nand we didn't specify what order you put them into the frontier,\nonly the order you take them out, here we can look at 13 and 11\nand say that, all right, this square is a distance of 11 away from the goal\naccording to our heuristic, according to our estimate.\nAnd this one, we estimate to be 13 away from the goal.\nSo between those two options, between these two choices,\nI'd rather have the 11.\nI'd rather be 11 steps away from the goal, so I'll go to the right.\nWe're able to make an informed decision, because we know a little something\nmore about this problem.\nSo then we keep following, 10, 9, 8.\nBetween the two 7s, we don't really have much of a way to know between those.\nSo then we do just have to make an arbitrary choice.\nAnd you know what, maybe we choose wrong.\nBut that's OK, because now we can still say, all right, let's try this 7.\nWe say 7, 6, we have to make this choice,\neven though it increases the value of the heuristic function.\nBut now we have another decision point, between 6 and 8, and between those two.\nAnd really, we're also considering this 13, but that's much higher.\nBetween 6, 8, and 13, well, the 6 is the smallest value,\nso we'd rather take the 6.\nWe're able to make an informed decision that going this way to the right\nis probably better than going down.\nSo we turn this way, we go to 5.\nAnd now we find a decision point where we'll actually\nmake a decision that we might not want to make,\nbut there's unfortunately not too much of a way around this.\nWe see 4 and 6.\n4 looks closer to the goal, right?\nIt's going up, and the goal is further up.\nSo we end up taking that route, which ultimately leads us to a dead end.\nBut that's OK, because we can still say, all right, now let's try the 6.\nAnd now follow this route that will ultimately lead us to the goal.\nAnd so this now is how greedy best-for-search\nmight try to approach this problem by saying,\nwhenever we have a decision between multiple nodes that we could explore,\nlet's explore the node that has the smallest value of h of n,\nthis heuristic function that is estimating how far I have to go.\nAnd it just so happens that in this case, we end up\ndoing better in terms of the number of states we needed to explore\nthan BFS needed to.\nBFS explored all of this section and all of that section,\nbut we were able to eliminate that by taking advantage of this heuristic,\nthis knowledge about how close we are to the goal or some estimate of that idea.\nSo this seems much better.\nSo wouldn't we always prefer an algorithm like this over an algorithm\nlike breadth-first search?\nWell, maybe one thing to take into consideration\nis that we need to come up with a good heuristic, how good the heuristic is,\nis going to affect how good this algorithm is.\nAnd coming up with a good heuristic can oftentimes be challenging.\nBut the other thing to consider is to ask the question,\njust as we did with the prior two algorithms, is this algorithm optimal?\nWill it always find the shortest path from the initial state to the goal?\nAnd to answer that question, let's take a look at this example for a moment.\nTake a look at this example.\nAgain, we're trying to get from A to B. And again,\nI've labeled each of the cells with their Manhattan distance from the goal.\nThe number of squares up and to the right,\nyou would need to travel in order to get from that square to the goal.\nAnd let's think about, would greedy best-first search\nthat always picks the smallest number end up finding the optimal solution?\nWhat is the shortest solution?\nAnd would this algorithm find it?\nAnd the important thing to realize is that right here is the decision point.\nWe're estimated to be 12 away from the goal.\nAnd we have two choices.\nWe can go to the left, which we estimate to be 13 away from the goal.\nOr we can go up, where we estimate it to be 11 away from the goal.\nAnd between those two, greedy best-first search\nis going to say the 11 looks better than the 13.\nAnd in doing so, greedy best-first search will end up\nfinding this path to the goal.\nBut it turns out this path is not optimal.\nThere is a way to get to the goal using fewer steps.\nAnd it's actually this way, this way that ultimately involved fewer steps,\neven though it meant at this moment choosing the worst option between the two\nor what we estimated to be the worst option based on the heuristics.\nAnd so this is what we mean by this is a greedy algorithm.\nIt's making the best decision locally.\nAt this decision point, it looks like it's better to go here\nthan it is to go to the 13.\nBut in the big picture, it's not necessarily optimal.\nThat it might find a solution when in actuality,\nthere was a better solution available.\nSo we would like some way to solve this problem.\nWe like the idea of this heuristic, of being\nable to estimate the path, the distance between us and the goal.\nAnd that helps us to be able to make better decisions\nand to eliminate having to search through entire parts of this state space.\nBut we would like to modify the algorithm so that we can achieve optimality,\nso that it can be optimal.\nAnd what is the way to do this?\nWhat is the intuition here?\nWell, let's take a look at this problem.\nIn this initial problem, greedy best research\nfound us this solution here, this long path.\nAnd the reason why it wasn't great is because, yes, the heuristic numbers\nwent down pretty low.\nBut later on, they started to build back up.\nThey built back 8, 9, 10, 11, all the way up to 12 in this case.\nAnd so how might we go about trying to improve this algorithm?\nWell, one thing that we might realize is that if we go all the way\nthrough this algorithm, through this path, and we end up going to the 12,\nand we've had to take this many steps, who knows how many steps that is,\njust to get to this 12, we could have also, as an alternative,\ntaken much fewer steps, just six steps, and ended up at this 13 here.\nAnd yes, 13 is more than 12, so it looks like it's not as good.\nBut it required far fewer steps.\nIt only took six steps to get to this 13 versus many more steps\nto get to this 12.\nAnd while greedy best research says, oh, well, 12 is better than 13,\nso pick the 12, we might more intelligently say,\nI'd rather be somewhere that heuristically looks\nlike it takes slightly longer if I can get there much more quickly.\nAnd we're going to encode that idea, this general idea,\ninto a more formal algorithm known as A star search.\nA star search is going to solve this problem\nby instead of just considering the heuristic,\nalso considering how long it took us to get to any particular state.\nSo the distinction is greedy best for search.\nIf I am in a state right now, the only thing I care about\nis, what is the estimated distance, the heuristic value,\nbetween me and the goal?\nWhereas A star search will take into consideration\ntwo pieces of information.\nIt'll take into consideration, how far do I estimate I am from the goal?\nBut also, how far did I have to travel in order to get here?\nBecause that is relevant, too.\nSo we'll search algorithms by expanding the node\nwith the lowest value of g of n plus h of n.\nh of n is that same heuristic that we were talking about a moment ago that's\ngoing to vary based on the problem.\nBut g of n is going to be the cost to reach the node, how many steps\nI had to take, in this case, to get to my current position.\nSo what does that search algorithm look like in practice?\nWell, let's take a look.\nAgain, we've got the same maze.\nAnd again, I've labeled them with their Manhattan distance.\nThis value is the h of n value, the heuristic\nestimate of how far each of these squares is away from the goal.\nBut now, as we begin to explore states, we\ncare not just about this heuristic value, but also about g of n,\nthe number of steps I had to take in order to get there.\nAnd I care about summing those two numbers together.\nSo what does that look like?\nOn this very first step, I have taken one step.\nAnd now I am estimated to be 16 steps away from the goal.\nSo the total value here is 17.\nThen I take one more step.\nI've now taken two steps.\nAnd I estimate myself to be 15 away from the goal, again, a total value of 17.\nNow I've taken three steps.\nAnd I'm estimated to be 14 away from the goal, so on and so forth.\nFour steps, an estimate of 13.\nFive steps, estimate of 12.\nAnd now here's a decision point.\nI could either be six steps away from the goal with a heuristic of 13\nfor a total of 19, or I could be six steps away\nfrom the goal with a heuristic of 11 with an estimate of 17 for the total.\nSo between 19 and 17, I'd rather take the 17, the 6 plus 11.\nSo so far, no different than what we saw before.\nWe're still taking this option because it appears to be better.\nAnd I keep taking this option because it appears to be better.\nBut it's right about here that things get a little bit different.\nNow I could be 15 steps away from the goal with an estimated distance of 6.\nSo 15 plus 6, total value of 21.\nAlternatively, I could be six steps away from the goal,\nbecause this is five steps away, so this is six steps away,\nwith a total value of 13 as my estimate.\nSo 6 plus 13, that's 19.\nSo here, we would evaluate g of n plus h of n to be 19, 6 plus 13.\nWhereas here, we would be 15 plus 6, or 21.\nAnd so the intuition is 19 less than 21, pick here.\nBut the idea is ultimately I'd rather be having taken fewer steps, get to a 13,\nthan having taken 15 steps and be at a 6, because it\nmeans I've had to take more steps in order to get there.\nMaybe there's a better path this way.\nSo instead, we'll explore this route.\nNow if we go one more, this is seven steps plus 14 is 21.\nSo between those two, it's sort of a toss-up.\nWe might end up exploring that one anyways.\nBut after that, as these numbers start to get bigger in the heuristic values,\nand these heuristic values start to get smaller,\nyou'll find that we'll actually keep exploring down this path.\nAnd you can do the math to see that at every decision point,\nA star search is going to make a choice based\non the sum of how many steps it took me to get to my current position,\nand then how far I estimate I am from the goal.\nSo while we did have to explore some of these states,\nthe ultimate solution we found was, in fact, an optimal solution.\nIt did find us the quickest possible way to get from the initial state\nto the goal.\nAnd it turns out that A star is an optimal search algorithm\nunder certain conditions.\nSo the conditions are H of n, my heuristic, needs to be admissible.\nWhat does it mean for a heuristic to be admissible?\nWell, a heuristic is admissible if it never overestimates the true cost.\nH of n always needs to either get it exactly right\nin terms of how far away I am, or it needs to underestimate.\nSo we saw an example from before where the heuristic value was much smaller\nthan the actual cost it would take.\nThat's totally fine, but the heuristic value should never overestimate.\nIt should never think that I'm further away from the goal than I actually am.\nAnd meanwhile, to make a stronger statement, H of n also needs to be\nconsistent.\nAnd what does it mean for it to be consistent?\nMathematically, it means that for every node, which we'll call n,\nand successor, the node after me, that I'll\ncall n prime, where it takes a cost of C to make that step,\nthe heuristic value of n needs to be less than or equal to the heuristic\nvalue of n prime plus the cost.\nSo it's a lot of math, but in words what that ultimately means\nis that if I am here at this state right now,\nthe heuristic value from me to the goal shouldn't\nbe more than the heuristic value of my successor,\nthe next place I could go to, plus however much\nit would cost me to just make that step from one step to the next step.\nAnd so this is just making sure that my heuristic is consistent between all\nof these steps that I might take.\nSo as long as this is true, then A star search\nis going to find me an optimal solution.\nAnd this is where much of the challenge of solving these search problems\ncan sometimes come in, that A star search is an algorithm that is known\nand you could write the code fairly easily,\nbut it's choosing the heuristic.\nIt can be the interesting challenge.\nThe better the heuristic is, the better I'll\nbe able to solve the problem in the fewer states that I'll have to explore.\nAnd I need to make sure that the heuristic satisfies\nthese particular constraints.\nSo all in all, these are some of the examples of search algorithms\nthat might work, and certainly there are many more than just this.\nA star, for example, does have a tendency to use quite a bit of memory.\nSo there are alternative approaches to A star\nthat ultimately use less memory than this version of A star\nhappens to use, and there are other search algorithms\nthat are optimized for other cases as well.\nBut now so far, we've only been looking at search algorithms\nwhere there is one agent.\nI am trying to find a solution to a problem.\nI am trying to navigate my way through a maze.\nI am trying to solve a 15 puzzle.\nI am trying to find driving directions from point A to point B.\nSometimes in search situations, though, we'll\nenter an adversarial situation, where I am an agent trying\nto make intelligent decisions.\nAnd there's someone else who is fighting against me, so to speak,\nthat has opposite objectives, someone where\nI am trying to succeed, someone else that wants me to fail.\nAnd this is most popular in something like a game, a game like Tic Tac Toe,\nwhere we've got this 3 by 3 grid, and x and o take turns,\neither writing an x or an o in any one of these squares.\nAnd the goal is to get three x's in a row if you're the x player,\nor three o's in a row if you're the o player.\nAnd computers have gotten quite good at playing games,\nTic Tac Toe very easily, but even more complex games.\nAnd so you might imagine, what does an intelligent decision in a game\nlook like?\nSo maybe x makes an initial move in the middle, and o plays up here.\nWhat does an intelligent move for x now become?\nWhere should you move if you were x?\nAnd it turns out there are a couple of possibilities.\nBut if an AI is playing this game optimally,\nthen the AI might play somewhere like the upper right,\nwhere in this situation, o has the opposite objective of x.\nx is trying to win the game to get three in a row diagonally here.\nAnd o is trying to stop that objective, opposite of the objective.\nAnd so o is going to place here to try to block.\nBut now, x has a pretty clever move.\nx can make a move like this, where now x has two possible ways\nthat x can win the game.\nx could win the game by getting three in a row across here.\nOr x could win the game by getting three in a row vertically this way.\nSo it doesn't matter where o makes their next move.\no could play here, for example, blocking the three in a row horizontally.\nBut then x is going to win the game by getting a three in a row vertically.\nAnd so there's a fair amount of reasoning that's\ngoing on here in order for the computer to be able to solve a problem.\nAnd it's similar in spirit to the problems we've looked at so far.\nThere are actions.\nThere's some sort of state of the board and some transition\nfrom one action to the next.\nBut it's different in the sense that this is now\nnot just a classical search problem, but an adversarial search problem.\nThat I am at the x player trying to find the best moves to make,\nbut I know that there is some adversary that is trying to stop me.\nSo we need some sort of algorithm to deal with these adversarial type of search\nsituations.\nAnd the algorithm we're going to take a look at\nis an algorithm called Minimax, which works very well\nfor these deterministic games where there are two players.\nIt can work for other types of games as well.\nBut we'll look right now at games where I make a move,\nthen my opponent makes a move.\nAnd I am trying to win, and my opponent is trying to win also.\nOr in other words, my opponent is trying to get me to lose.\nAnd so what do we need in order to make this algorithm work?\nWell, any time we try and translate this human concept of playing a game,\nwinning and losing to a computer, we want to translate it\nin terms that the computer can understand.\nAnd ultimately, the computer really just understands the numbers.\nAnd so we want some way of translating a game of x's and o's on a grid\nto something numerical, something the computer can understand.\nThe computer doesn't normally understand notions of win or lose.\nBut it does understand the concept of bigger and smaller.\nAnd so what we might do is we might take each of the possible ways\nthat a tic-tac-toe game can unfold and assign a value or a utility\nto each one of those possible ways.\nAnd in a tic-tac-toe game, and in many types of games,\nthere are three possible outcomes.\nThe outcomes are o wins, x wins, or nobody wins.\nSo player one wins, player two wins, or nobody wins.\nAnd for now, let's go ahead and assign each of these possible outcomes\na different value.\nWe'll say o winning, that'll have a value of negative 1.\nNobody winning, that'll have a value of 0.\nAnd x winning, that will have a value of 1.\nSo we've just assigned numbers to each of these three possible outcomes.\nAnd now we have two players, we have the x player and the o player.\nAnd we're going to go ahead and call the x player the max player.\nAnd we'll call the o player the min player.\nAnd the reason why is because in the min and max algorithm,\nthe max player, which in this case is x, is aiming to maximize the score.\nThese are the possible options for the score, negative 1, 0, and 1.\nx wants to maximize the score, meaning if at all possible,\nx would like this situation, where x wins the game,\nand we give it a score of 1.\nBut if this isn't possible, if x needs to choose between these two options,\nnegative 1, meaning o winning, or 0, meaning nobody winning,\nx would rather that nobody wins, score of 0,\nthan a score of negative 1, o winning.\nSo this notion of winning and losing and tying\nhas been reduced mathematically to just this idea of try and maximize the score.\nThe x player always wants the score to be bigger.\nAnd on the flip side, the min player, in this case o,\nis aiming to minimize the score.\nThe o player wants the score to be as small as possible.\nSo now we've taken this game of x's and o's and winning and losing\nand turned it into something mathematical,\nsomething where x is trying to maximize the score,\no is trying to minimize the score.\nLet's now look at all of the parts of the game\nthat we need in order to encode it in an AI\nso that an AI can play a game like tic-tac-toe.\nSo the game is going to need a couple of things.\nWe'll need some sort of initial state that will, in this case, call s0,\nwhich is how the game begins, like an empty tic-tac-toe board, for example.\nWe'll also need a function called player, where the player function\nis going to take as input a state here represented by s.\nAnd the output of the player function is going to be which player's turn is it.\nWe need to be able to give a tic-tac-toe board to the computer,\nrun it through a function, and that function tells us whose turn it is.\nWe'll need some notion of actions that we can take.\nWe'll see examples of that in just a moment.\nWe need some notion of a transition model, same as before.\nIf I have a state and I take an action, I\nneed to know what results as a consequence of it.\nI need some way of knowing when the game is over.\nSo this is equivalent to kind of like a goal test,\nbut I need some terminal test, some way to check\nto see if a state is a terminal state, where a terminal state means the game is\nover.\nIn a classic game of tic-tac-toe, a terminal state\nmeans either someone has gotten three in a row\nor all of the squares of the tic-tac-toe board are filled.\nEither of those conditions make it a terminal state.\nIn a game of chess, it might be something like when there is checkmate\nor if checkmate is no longer possible, that that becomes a terminal state.\nAnd then finally, we'll need a utility function, a function that takes a state\nand gives us a numerical value for that terminal state, some way of saying\nif x wins the game, that has a value of 1.\nIf o is won the game, that has a value of negative 1.\nIf nobody has won the game, that has a value of 0.\nSo let's take a look at each of these in turn.\nThe initial state, we can just represent in tic-tac-toe as the empty game board.\nThis is where we begin.\nIt's the place from which we begin this search.\nAnd again, I'll be representing these things visually,\nbut you can imagine this really just being like an array\nor a two-dimensional array of all of these possible squares.\nThen we need the player function that, again, takes a state\nand tells us whose turn it is.\nAssuming x makes the first move, if I have an empty game board,\nthen my player function is going to return x.\nAnd if I have a game board where x has made a move,\nthen my player function is going to return o.\nThe player function takes a tic-tac-toe game board\nand tells us whose turn it is.\nNext up, we'll consider the actions function.\nThe actions function, much like it did in classical search,\ntakes a state and gives us the set of all of the possible actions\nwe can take in that state.\nSo let's imagine it's o is turned to move in a game board that looks like this.\nWhat happens when we pass it into the actions function?\nSo the actions function takes this state of the game as input,\nand the output is a set of possible actions.\nIt's a set of I could move in the upper left\nor I could move in the bottom middle.\nSo those are the two possible action choices\nthat I have when I begin in this particular state.\nNow, just as before, when we had states and actions,\nwe need some sort of transition model to tell us\nwhen we take this action in the state, what is the new state that we get.\nAnd here, we define that using the result function\nthat takes a state as input as well as an action.\nAnd when we apply the result function to this state,\nsaying that let's let o move in this upper left corner,\nthe new state we get is this resulting state where o is in the upper left\ncorner.\nAnd now, this seems obvious to someone who knows how to play tic-tac-toe.\nOf course, you play in the upper left corner.\nThat's the board you get.\nBut all of this information needs to be encoded into the AI.\nThe AI doesn't know how to play tic-tac-toe until you\ntell the AI how the rules of tic-tac-toe work.\nAnd this function, defining this function here,\nallows us to tell the AI how this game actually works\nand how actions actually affect the outcome of the game.\nSo the AI needs to know how the game works.\nThe AI also needs to know when the game is over,\nas by defining a function called terminal that takes as input a state s,\nsuch that if we take a game that is not yet over,\npass it into the terminal function, the output is false.\nThe game is not over.\nBut if we take a game that is over because x has gotten three in a row\nalong that diagonal, pass that into the terminal function,\nthen the output is going to be true because the game now is, in fact, over.\nAnd finally, we've told the AI how the game works\nin terms of what moves can be made and what happens when you make those moves.\nWe've told the AI when the game is over.\nNow we need to tell the AI what the value of each of those states is.\nAnd we do that by defining this utility function that takes a state s\nand tells us the score or the utility of that state.\nSo again, we said that if x wins the game, that utility is a value of 1,\nwhereas if o wins the game, then the utility of that is negative 1.\nAnd the AI needs to know, for each of these terminal states\nwhere the game is over, what is the utility of that state?\nSo if I give you a game board like this where the game is, in fact, over,\nand I ask the AI to tell me what the value of that state is, it could do so.\nThe value of the state is 1.\nWhere things get interesting, though, is if the game is not yet over.\nLet's imagine a game board like this, where in the middle of the game,\nit's o's turn to make a move.\nSo how do we know it's o's turn to make a move?\nWe can calculate that using the player function.\nWe can say player of s, pass in the state, o is the answer.\nSo we know it's o's turn to move.\nAnd now, what is the value of this board and what action should o take?\nWell, that's going to depend.\nWe have to do some calculation here.\nAnd this is where the minimax algorithm really comes in.\nRecall that x is trying to maximize the score, which\nmeans that o is trying to minimize the score.\nSo o would like to minimize the total value\nthat we get at the end of the game.\nAnd because this game isn't over yet, we don't really\nknow just yet what the value of this game board is.\nWe have to do some calculation in order to figure that out.\nAnd so how do we do that kind of calculation?\nWell, in order to do so, we're going to consider,\njust as we might in a classical search situation,\nwhat actions could happen next and what states will that take us to.\nAnd it turns out that in this position, there are only two open squares,\nwhich means there are only two open places where o can make a move.\no could either make a move in the upper left\nor o can make a move in the bottom middle.\nAnd minimax doesn't know right out of the box which of those moves\nis going to be better.\nSo it's going to consider both.\nBut now, we sort of run into the same situation.\nNow, I have two more game boards, neither of which is over.\nWhat happens next?\nAnd now, it's in this sense that minimax is\nwhat we'll call a recursive algorithm.\nIt's going to now repeat the exact same process,\nalthough now considering it from the opposite perspective.\nIt's as if I am now going to put myself, if I am the o player,\nI'm going to put myself in my opponent's shoes, my opponent as the x player,\nand consider what would my opponent do if they were in this position?\nWhat would my opponent do, the x player, if they were in that position?\nAnd what would then happen?\nWell, the other player, my opponent, the x player,\nis trying to maximize the score, whereas I\nam trying to minimize the score as the o player.\nSo x is trying to find the maximum possible value that they can get.\nAnd so what's going to happen?\nWell, from this board position, x only has one choice.\nx is going to play here, and they're going to get three in a row.\nAnd we know that that board, x winning, that has a value of 1.\nIf x wins the game, the value of that game board is 1.\nAnd so from this position, if this state can only ever\nlead to this state, it's the only possible option,\nand this state has a value of 1, then the maximum possible value\nthat the x player can get from this game board is also 1.\nFrom here, the only place we can get is to a game with a value of 1,\nso this game board also has a value of 1.\nNow we consider this one over here.\nWhat's going to happen now?\nWell, x needs to make a move.\nThe only move x can make is in the upper left, so x will go there.\nAnd in this game, no one wins the game.\nNobody has three in a row.\nAnd so the value of that game board is 0.\nNobody is 1.\nAnd so again, by the same logic, if from this board position\nthe only place we can get to is a board where the value is 0,\nthen this state must also have a value of 0.\nAnd now here comes the choice part, the idea of trying to minimize.\nI, as the o player, now know that if I make this choice moving in the upper\nleft, that is going to result in a game with a value of 1,\nassuming everyone plays optimally.\nAnd if I instead play in the lower middle,\nchoose this fork in the road, that is going\nto result in a game board with a value of 0.\nI have two options.\nI have a 1 and a 0 to choose from, and I need to pick.\nAnd as the min player, I would rather choose\nthe option with the minimum value.\nSo whenever a player has multiple choices,\nthe min player will choose the option with the smallest value.\nThe max player will choose the option with the largest value.\nBetween the 1 and the 0, the 0 is smaller,\nmeaning I'd rather tie the game than lose the game.\nAnd so this game board will say also has a value of 0,\nbecause if I am playing optimally, I will pick this fork in the road.\nI'll place my o here to block x's 3 in a row, x will move in the upper left,\nand the game will be over, and no one will have won the game.\nSo this is now the logic of minimax, to consider all of the possible options\nthat I can take, all of the actions that I can take,\nand then to put myself in my opponent's shoes.\nI decide what move I'm going to make now by considering\nwhat move my opponent will make on the next turn.\nAnd to do that, I consider what move I would make on the turn after that,\nso on and so forth, until I get all the way down\nto the end of the game, to one of these so-called terminal states.\nIn fact, this very decision point, where I am trying to decide as the o player\nwhat to make a decision about, might have just\nbeen a part of the logic that the x player, my opponent, was using,\nthe move before me.\nThis might be part of some larger tree, where\nx is trying to make a move in this situation,\nand needs to pick between three different options in order\nto make a decision about what to happen.\nAnd the further and further away we are from the end of the game,\nthe deeper this tree has to go.\nBecause every level in this tree is going to correspond to one move,\none move or action that I take, one move or action\nthat my opponent takes, in order to decide what happens.\nAnd in fact, it turns out that if I am the x player in this position,\nand I recursively do the logic, and see I have a choice, three choices,\nin fact, one of which leads to a value of 0.\nIf I play here, and if everyone plays optimally, the game will be a tie.\nIf I play here, then o is going to win, and I'll lose playing optimally.\nOr here, where I, the x player, can win, well between a score of 0,\nand negative 1, and 1, I'd rather pick the board with a value of 1,\nbecause that's the maximum value I can get.\nAnd so this board would also have a maximum value of 1.\nAnd so this tree can get very, very deep, especially as the game\nstarts to have more and more moves.\nAnd this logic works not just for tic-tac-toe,\nbut any of these sorts of games, where I make a move,\nmy opponent makes a move, and ultimately, we\nhave these adversarial objectives.\nAnd we can simplify the diagram into a diagram that looks like this.\nThis is a more abstract version of the minimax tree,\nwhere these are each states, but I'm no longer representing them\nas exactly like tic-tac-toe boards.\nThis is just representing some generic game that might be tic-tac-toe,\nmight be some other game altogether.\nAny of these green arrows that are pointing up,\nthat represents a maximizing state.\nI would like the score to be as big as possible.\nAnd any of these red arrows pointing down,\nthose are minimizing states, where the player is the min player,\nand they are trying to make the score as small as possible.\nSo if you imagine in this situation, I am the maximizing player, this player\nhere, and I have three choices.\nOne choice gives me a score of 5, one choice gives me a score of 3,\nand one choice gives me a score of 9.\nWell, then between those three choices, my best option\nis to choose this 9 over here, the score that\nmaximizes my options out of all the three options.\nAnd so I can give this state a value of 9, because among my three options,\nthat is the best choice that I have available to me.\nSo that's my decision now.\nYou imagine it's like one move away from the end of the game.\nBut then you could also ask a reasonable question,\nwhat might my opponent do two moves away from the end of the game?\nMy opponent is the minimizing player.\nThey are trying to make the score as small as possible.\nImagine what would have happened if they had to pick which choice to make.\nOne choice leads us to this state, where I, the maximizing player,\nam going to opt for 9, the biggest score that I can get.\nAnd 1 leads to this state, where I, the maximizing player,\nwould choose 8, which is then the largest score that I can get.\nNow the minimizing player, forced to choose between a 9 or an 8,\nis going to choose the smallest possible score,\nwhich in this case is an 8.\nAnd that is then how this process would unfold,\nthat the minimizing player in this case considers both of their options,\nand then all of the options that would happen as a result of that.\nSo this now is a general picture of what the minimax algorithm looks like.\nLet's now try to formalize it using a little bit of pseudocode.\nSo what exactly is happening in the minimax algorithm?\nWell, given a state s, we need to decide what to happen.\nThe max player, if it's max's player's turn,\nthen max is going to pick an action a in actions of s.\nRecall that actions is a function that takes a state\nand gives me back all of the possible actions that I can take.\nIt tells me all of the moves that are possible.\nThe max player is going to specifically pick an action a in this set of actions\nthat gives me the highest value of min value of result of s and a.\nSo what does that mean?\nWell, it means that I want to make the option that\ngives me the highest score of all of the actions a.\nBut what score is that going to have?\nTo calculate that, I need to know what my opponent, the min player,\nis going to do if they try to minimize the value of the state that results.\nSo we say, what state results after I take this action?\nAnd what happens when the min player tries to minimize the value of that state?\nI consider that for all of my possible options.\nAnd after I've considered that for all of my possible options,\nI pick the action a that has the highest value.\nLikewise, the min player is going to do the same thing but backwards.\nThey're also going to consider what are all of the possible actions they\ncan take if it's their turn.\nAnd they're going to pick the action a that\nhas the smallest possible value of all the options.\nAnd the way they know what the smallest possible value of all the options\nis is by considering what the max player is going to do by saying,\nwhat's the result of applying this action to the current state?\nAnd then what would the max player try to do?\nWhat value would the max player calculate for that particular state?\nSo everyone makes their decision based on trying\nto estimate what the other person would do.\nAnd now we need to turn our attention to these two functions, max value\nand min value.\nHow do you actually calculate the value of a state\nif you're trying to maximize its value?\nAnd how do you calculate the value of a state\nif you're trying to minimize the value?\nIf you can do that, then we have an entire implementation\nof this min and max algorithm.\nSo let's try it.\nLet's try and implement this max value function that takes a state\nand returns as output the value of that state\nif I'm trying to maximize the value of the state.\nWell, the first thing I can check for is to see if the game is over.\nBecause if the game is over, in other words,\nif the state is a terminal state, then this is easy.\nI already have this utility function that tells me\nwhat the value of the board is.\nIf the game is over, I just check, did x win, did o win, is it a tie?\nAnd this utility function just knows what the value of the state is.\nWhat's trickier is if the game isn't over.\nBecause then I need to do this recursive reasoning about thinking,\nwhat is my opponent going to do on the next move?\nAnd I want to calculate the value of this state.\nAnd I want the value of the state to be as high as possible.\nAnd I'll keep track of that value in a variable called v.\nAnd if I want the value to be as high as possible,\nI need to give v an initial value.\nAnd initially, I'll just go ahead and set it to be as low as possible.\nBecause I don't know what options are available to me yet.\nSo initially, I'll set v equal to negative infinity, which\nseems a little bit strange.\nBut the idea here is I want the value initially\nto be as low as possible.\nBecause as I consider my actions, I'm always\ngoing to try and do better than v. And if I set v to negative infinity,\nI know I can always do better than that.\nSo now I consider my actions.\nAnd this is going to be some kind of loop\nwhere for every action in actions of state,\nrecall actions as a function that takes my state\nand gives me all the possible actions that I can use in that state.\nSo for each one of those actions, I want to compare it to v and say,\nall right, v is going to be equal to the maximum of v and this expression.\nSo what is this expression?\nWell, first it is get the result of taking the action in the state\nand then get the min value of that.\nIn other words, let's say I want to find out from that state\nwhat is the best that the min player can do because they're\ngoing to try and minimize the score.\nSo whatever the resulting score is of the min value of that state,\ncompare it to my current best value and just pick the maximum of those two\nbecause I am trying to maximize the value.\nIn short, what these three lines of code are doing\nare going through all of my possible actions and asking the question,\nhow do I maximize the score given what my opponent is going to try to do?\nAfter this entire loop, I can just return v\nand that is now the value of that particular state.\nAnd for the min player, it's the exact opposite of this,\nthe same logic just backwards.\nTo calculate the minimum value of a state,\nfirst we check if it's a terminal state.\nIf it is, we return its utility.\nOtherwise, we're going to now try to minimize the value of the state\ngiven all of my possible actions.\nSo I need an initial value for v, the value of the state.\nAnd initially, I'll set it to infinity because I\nknow I can always get something less than infinity.\nSo by starting with v equals infinity, I make sure that the very first action\nI find, that will be less than this value of v.\nAnd then I do the same thing, loop over all of my possible actions.\nAnd for each of the results that we could get when the max player makes\ntheir decision, let's take the minimum of that and the current value of v.\nSo after all is said and done, I get the smallest possible value of v\nthat I then return back to the user.\nSo that, in effect, is the pseudocode for Minimax.\nThat is how we take a gain and figure out what the best move to make\nis by recursively using these max value and min value functions,\nwhere max value calls min value, min value calls max value back and forth,\nall the way until we reach a terminal state, at which point\nour algorithm can simply return the utility of that particular state.\nSo what you might imagine is that this is going to start to be a long process,\nespecially as games start to get more complex,\nas we start to add more moves and more possible options and games that\nmight last quite a bit longer.\nSo the next question to ask is, what sort of optimizations can we make here?\nHow can we do better in order to use less space or take less time\nto be able to solve this kind of problem?\nAnd we'll take a look at a couple of possible optimizations.\nBut for one, we'll take a look at this example.\nAgain, returning to these up arrows and down arrows,\nlet's imagine that I now am the max player, this green arrow.\nI am trying to make this score as high as possible.\nAnd this is an easy game where there are just two moves.\nI make a move, one of these three options.\nAnd then my opponent makes a move, one of these three options,\nbased on what move I make.\nAnd as a result, we get some value.\nLet's look at the order in which I do these calculations\nand figure out if there are any optimizations I\nmight be able to make to this calculation process.\nI'm going to have to look at these states one at a time.\nSo let's say I start here on the left and say, all right,\nnow I'm going to consider, what will the min player, my opponent,\ntry to do here?\nWell, the min player is going to look at all three of their possible actions\nand look at their value, because these are terminal states.\nThey're the end of the game.\nAnd so they'll see, all right, this node is a value of four, value of eight,\nvalue of five.\nAnd the min player is going to say, well, all right,\nbetween these three options, four, eight, and five, I'll take the smallest one.\nI'll take the four.\nSo this state now has a value of four.\nThen I, as the max player, say, all right, if I take this action,\nit will have a value of four.\nThat's the best that I can do, because min player\nis going to try and minimize my score.\nSo now what if I take this option?\nWe'll explore this next.\nAnd now explore what the min player would do if I choose this action.\nAnd the min player is going to say, all right, what are the three options?\nThe min player has options between nine, three, and seven.\nAnd so three is the smallest among nine, three, and seven.\nSo we'll go ahead and say this state has a value of three.\nSo now I, as the max player, I have now explored two of my three options.\nI know that one of my options will guarantee me a score of four, at least.\nAnd one of my options will guarantee me a score of three.\nAnd now I consider my third option and say, all right, what happens here?\nSame exact logic.\nThe min player is going to look at these three states, two, four, and six.\nI'll say the minimum possible option is two.\nSo the min player wants the two.\nNow I, as the max player, have calculated all of the information\nby looking two layers deep, by looking at all of these nodes.\nAnd I can now say, between the four, the three, and the two, you know what?\nI'd rather take the four.\nBecause if I choose this option, if my opponent plays optimally,\nthey will try and get me to the four.\nBut that's the best I can do.\nI can't guarantee a higher score.\nBecause if I pick either of these two options, I might get a three\nor I might get a two.\nAnd it's true that down here is a nine.\nAnd that's the highest score out of any of the scores.\nSo I might be tempted to say, you know what?\nMaybe I should take this option because I might get the nine.\nBut if the min player is playing intelligently,\nif they're making the best moves at each possible option\nthey have when they get to make a choice, I'll be left with a three.\nWhereas I could better, playing optimally,\nhave guaranteed that I would get the four.\nSo that is, in effect, the logic that I would use as a min and max player\ntrying to maximize my score from that node there.\nBut it turns out they took quite a bit of computation for me to figure that out.\nI had to reason through all of these nodes\nin order to draw this conclusion.\nAnd this is for a pretty simple game where I have three choices,\nmy opponent has three choices, and then the game's over.\nSo what I'd like to do is come up with some way to optimize this.\nMaybe I don't need to do all of this calculation\nto still reach the conclusion that, you know what, this action to the left,\nthat's the best that I could do.\nLet's go ahead and try again and try to be a little more intelligent about how\nI go about doing this.\nSo first, I start the exact same way.\nI don't know what to do initially, so I just\nhave to consider one of the options and consider what the min player might do.\nMin has three options, four, eight, and five.\nAnd between those three options, min says four is the best they can do\nbecause they want to try to minimize the score.\nNow I, the max player, will consider my second option,\nmaking this move here, and considering what my opponent would do in response.\nWhat will the min player do?\nWell, the min player is going to, from that state, look at their options.\nAnd I would say, all right, nine is an option, three is an option.\nAnd if I am doing the math from this initial state,\ndoing all this calculation, when I see a three,\nthat should immediately be a red flag for me.\nBecause when I see a three down here at this state,\nI know that the value of this state is going to be at most three.\nIt's going to be three or something less than three,\neven though I haven't yet looked at this last action or even further actions\nif there were more actions that could be taken here.\nHow do I know that?\nWell, I know that the min player is going to try to minimize my score.\nAnd if they see a three, the only way this\ncould be something other than a three is if this remaining thing\nthat I haven't yet looked at is less than three, which\nmeans there is no way for this value to be anything more than three\nbecause the min player can already guarantee a three\nand they are trying to minimize my score.\nSo what does that tell me?\nWell, it tells me that if I choose this action,\nmy score is going to be three or maybe even less than three if I'm unlucky.\nBut I already know that this action will guarantee me a four.\nAnd so given that I know that this action guarantees me a score of four\nand this action means I can't do better than three,\nif I'm trying to maximize my options, there\nis no need for me to consider this triangle here.\nThere is no value, no number that could go here\nthat would change my mind between these two options.\nI'm always going to opt for this path that gets me a four as opposed\nto this path where the best I can do is a three if my opponent plays optimally.\nAnd this is going to be true for all the future states that I look at too.\nThat if I look over here at what min player might do over here,\nif I see that this state is a two, I know that this state is at most a two\nbecause the only way this value could be something other than two\nis if one of these remaining states is less than a two\nand so the min player would opt for that instead.\nSo even without looking at these remaining states,\nI as the maximizing player can know that choosing this path to the left\nis going to be better than choosing either of those two paths to the right\nbecause this one can't be better than three.\nThis one can't be better than two.\nAnd so four in this case is the best that I can do.\nSo in order to do this cut, and I can say now\nthat this state has a value of four.\nSo in order to do this type of calculation,\nI was doing a little bit more bookkeeping, keeping track of things,\nkeeping track all the time of what is the best that I can do,\nwhat is the worst that I can do, and for each of these states\nsaying, all right, well, if I already know that I can get a four,\nthen if the best I can do at this state is a three,\nno reason for me to consider it, I can effectively prune this leaf\nand anything below it from the tree.\nAnd it's for that reason this approach, this optimization to minimax,\nis called alpha, beta pruning.\nAlpha and beta stand for these two values\nthat you'll have to keep track of of the best you can do so far\nand the worst you can do so far.\nAnd pruning is the idea of if I have a big, long, deep search tree,\nI might be able to search it more efficiently\nif I don't need to search through everything,\nif I can remove some of the nodes to try and optimize the way that I\nlook through this entire search space.\nSo alpha, beta pruning can definitely save us a lot of time\nas we go about the search process by making our searches more efficient.\nBut even then, it's still not great as games get more complex.\nTic-tac-toe, fortunately, is a relatively simple game.\nAnd we might reasonably ask a question like,\nhow many total possible tic-tac-toe games are there?\nYou can think about it.\nYou can try and estimate how many moves are there at any given point,\nhow many moves long can the game last.\nIt turns out there are about 255,000 possible tic-tac-toe games\nthat can be played.\nBut compare that to a more complex game, something\nlike a game of chess, for example.\nFar more pieces, far more moves, games that last much longer.\nHow many total possible chess games could there be?\nIt turns out that after just four moves each, four moves by the white player,\nfour moves by the black player, that there are\n288 billion possible chess games that can result from that situation,\nafter just four moves each.\nAnd going even further, if you look at entire chess games\nand how many possible chess games there could be as a result there,\nthere are more than 10 to the 29,000 possible chess games,\nfar more chess games than could ever be considered.\nAnd this is a pretty big problem for the Minimax algorithm,\nbecause the Minimax algorithm starts with an initial state,\nconsiders all the possible actions, and all the possible actions\nafter that, all the way until we get to the end of the game.\nAnd that's going to be a problem if the computer is going\nto need to look through this many states, which is far more than any computer\ncould ever do in any reasonable amount of time.\nSo what do we do in order to solve this problem?\nInstead of looking through all these states which\nis totally intractable for a computer, we need some better approach.\nAnd it turns out that better approach generally takes the form of something\ncalled depth-limited Minimax, where normally Minimax\nis depth-unlimited.\nWe just keep going layer after layer, move after move,\nuntil we get to the end of the game.\nDepth-limited Minimax is instead going to say,\nyou know what, after a certain number of moves, maybe I'll look 10 moves ahead,\nmaybe I'll look 12 moves ahead, but after that point,\nI'm going to stop and not consider additional moves that\nmight come after that, just because it would be computationally intractable\nto consider all of those possible options.\nBut what do we do after we get 10 or 12 moves deep\nwhen we arrive at a situation where the game's not over?\nMinimax still needs a way to assign a score to that game board or game\nstate to figure out what its current value is, which is easy to do\nif the game is over, but not so easy to do if the game is not yet over.\nSo in order to do that, we need to add one additional feature\nto depth-limited Minimax called an evaluation function, which\nis just some function that is going to estimate the expected utility\nof a game from a given state.\nSo in a game like chess, if you imagine that a game value of 1\nmeans white wins, negative 1 means black wins, 0 means it's a draw,\nthen you might imagine that a score of 0.8\nmeans white is very likely to win, though certainly not guaranteed.\nAnd you would have an evaluation function\nthat estimates how good the game state happens to be.\nAnd depending on how good that evaluation function is,\nthat is ultimately what's going to constrain how good the AI is.\nThe better the AI is at estimating how good or how bad\nany particular game state is, the better the AI\nis going to be able to play that game.\nIf the evaluation function is worse and not as good as it estimating\nwhat the expected utility is, then it's going to be a whole lot harder.\nAnd you can imagine trying to come up with these evaluation functions.\nIn chess, for example, you might write an evaluation function\nbased on how many pieces you have as compared\nto how many pieces your opponent has, because each one has a value.\nAnd your evaluation function probably needs\nto be a little bit more complicated than that\nto consider other possible situations that might arise as well.\nAnd there are many other variants on Minimax that add additional features\nin order to help it perform better under these larger, more computationally\nuntractable situations where we couldn't possibly\nexplore all of the possible moves.\nSo we need to figure out how to use evaluation functions and other techniques\nto be able to play these games ultimately better.\nBut this now was a look at this kind of adversarial search, these search\nproblems where we have situations where I am trying\nto play against some sort of opponent.\nAnd these search problems show up all over the place\nthroughout artificial intelligence.\nWe've been talking a lot today about more classical search problems,\nlike trying to find directions from one location to another.\nBut any time an AI is faced with trying to make a decision,\nlike what do I do now in order to do something that is rational,\nor do something that is intelligent, or trying to play a game,\nlike figuring out what move to make, these sort of algorithms\ncan really come in handy.\nIt turns out that for tic-tac-toe, the solution is pretty simple\nbecause it's a small game.\nXKCD has famously put together a web comic\nwhere he will tell you exactly what move to make as the optimal move to make\nno matter what your opponent happens to do.\nThis type of thing is not quite as possible for a much larger game\nlike Checkers or Chess, for example, where chess is totally computationally\nuntractable for most computers to be able to explore all the possible states.\nSo we really need our AI to be far more intelligent about how\nthey go about trying to deal with these problems\nand how they go about taking this environment that they find themselves in\nand ultimately searching for one of these solutions.\nSo this, then, was a look at search in artificial intelligence.\nNext time, we'll take a look at knowledge,\nthinking about how it is that our AIs are able to know information, reason\nabout that information, and draw conclusions, all in our look at AI\nand the principles behind it.\nWe'll see you next time.\n[\"AIMS INTRO MUSIC\"]\nAll right, welcome back, everyone, to an introduction\nto artificial intelligence with Python.\nLast time, we took a look at search problems, in particular,\nwhere we have AI agents that are trying to solve some sort of problem\nby taking actions in some sort of environment,\nwhether that environment is trying to take actions by playing moves in a game\nor whether those actions are something like trying\nto figure out where to make turns in order to get driving directions\nfrom point A to point B. This time, we're\ngoing to turn our attention more generally to just this idea of knowledge,\nthe idea that a lot of intelligence is based on knowledge,\nespecially if we think about human intelligence.\nPeople know information.\nWe know facts about the world.\nAnd using that information that we know, we're\nable to draw conclusions, reason about the information\nthat we know in order to figure out how to do something\nor figure out some other piece of information\nthat we conclude based on the information we already have available to us.\nWhat we'd like to focus on now is the ability\nto take this idea of knowledge and being able to reason based on knowledge\nand apply those ideas to artificial intelligence.\nIn particular, we're going to be building what\nare known as knowledge-based agents, agents that\nare able to reason and act by representing knowledge internally.\nSomehow inside of our AI, they have some understanding\nof what it means to know something.\nAnd ideally, they have some algorithms or some techniques\nthey can use based on that knowledge that they know in order to figure out\nthe solution to a problem or figure out some additional piece of information\nthat can be helpful in some sense.\nSo what do we mean by reasoning based on knowledge\nto be able to draw conclusions?\nWell, let's look at a simple example drawn from the world of Harry Potter.\nWe take one sentence that we know to be true.\nImagine if it didn't rain, then Harry visited Hagrid today.\nSo one fact that we might know about the world.\nAnd then we take another fact.\nHarry visited Hagrid or Dumbledore today, but not both.\nSo it tells us something about the world, that Harry either visited\nHagrid but not Dumbledore, or Harry visited Dumbledore but not Hagrid.\nAnd now we have a third piece of information about the world\nthat Harry visited Dumbledore today.\nSo we now have three pieces of information now, three facts.\nInside of a knowledge base, so to speak, information that we know.\nAnd now we, as humans, can try and reason about this\nand figure out, based on this information, what additional information\ncan we begin to conclude?\nAnd well, looking at these last two statements,\nHarry either visited Hagrid or Dumbledore but not both,\nand we know that Harry visited Dumbledore today, well,\nthen it's pretty reasonable that we could draw the conclusion that,\nyou know what, Harry must not have visited Hagrid today.\nBecause based on a combination of these two statements,\nwe can draw this inference, so to speak, a conclusion that Harry did not\nvisit Hagrid today.\nBut it turns out we can even do a little bit better than that,\nget some more information by taking a look at this first statement\nand reasoning about that.\nThis first statement says, if it didn't rain,\nthen Harry visited Hagrid today.\nSo what does that mean?\nIn all cases where it didn't rain, then we know that Harry visited Hagrid.\nBut if we also know now that Harry did not visit Hagrid,\nthen that tells us something about our initial premise\nthat we were thinking about.\nIn particular, it tells us that it did rain today, because we can reason,\nif it didn't rain, that Harry would have visited Hagrid.\nBut we know for a fact that Harry did not visit Hagrid today.\nSo it's this kind of reason, this sort of logical reasoning,\nwhere we use logic based on the information\nthat we know in order to take information and reach conclusions that\nis going to be the focus of what we're going to be talking about today.\nHow can we make our artificial intelligence\nlogical so that they can perform the same kinds of deduction,\nthe same kinds of reasoning that we've been doing so far?\nOf course, humans reason about logic generally\nin terms of human language.\nThat I just now was speaking in English, talking in English about these\nsentences and trying to reason through how it\nis that they relate to one another.\nWe're going to need to be a little bit more formal when\nwe turn our attention to computers and being\nable to encode this notion of logic and truthhood and falsehood\ninside of a machine.\nSo we're going to need to introduce a few more terms and a few symbols that\nwill help us reason through this idea of logic\ninside of an artificial intelligence.\nAnd we'll begin with the idea of a sentence.\nNow, a sentence in a natural language like English\nis just something that I'm saying, like what I'm saying right now.\nIn the context of AI, though, a sentence is just an assertion about the world\nin what we're going to call a knowledge representation language,\nsome way of representing knowledge inside of our computers.\nAnd the way that we're going to spend most of today reasoning about knowledge\nis through a type of logic known as propositional logic.\nThere are a number of different types of logic, some of which we'll touch on.\nBut propositional logic is based on a logic of propositions,\nor just statements about the world.\nAnd so we begin in propositional logic with a notion of propositional symbols.\nWe will have certain symbols that are oftentimes just letters,\nsomething like P or Q or R, where each of those symbols\nis going to represent some fact or sentence about the world.\nSo P, for example, might represent the fact that it is raining.\nAnd so P is going to be a symbol that represents that idea.\nAnd Q, for example, might represent Harry visited Hagrid today.\nEach of these propositional symbols represents some sentence\nor some fact about the world.\nBut in addition to just having individual facts about the world,\nwe want some way to connect these propositional symbols together\nin order to reason more complexly about other facts that\nmight exist inside of the world in which we're reasoning.\nSo in order to do that, we'll need to introduce some additional symbols\nthat are known as logical connectives.\nNow, there are a number of these logical connectives.\nBut five of the most important, and the ones we're going to focus on today,\nare these five up here, each represented by a logical symbol.\nNot is represented by this symbol here, and is represented\nas sort of an upside down V, or is represented by a V shape.\nImplication, and we'll talk about what that means in just a moment,\nis represented by an arrow.\nAnd biconditional, again, we'll talk about what that means in a moment,\nis represented by these double arrows.\nBut these five logical connectives are the main ones\nwe're going to be focusing on in terms of thinking about how\nit is that a computer can reason about facts\nand draw conclusions based on the facts that it knows.\nBut in order to get there, we need to take\na look at each of these logical connectives\nand build up an understanding for what it is that they actually mean.\nSo let's go ahead and begin with the not symbol, so this not symbol here.\nAnd what we're going to show for each of these logical connectives\nis what we're going to call a truth table, a table that\ndemonstrates what this word not means when we attach it\nto a propositional symbol or any sentence inside of our logical language.\nAnd so the truth table for not is shown right here.\nIf P, some propositional symbol, or some other sentence even, is false,\nthen not P is true.\nAnd if P is true, then not P is false.\nSo you can imagine that placing this not symbol\nin front of some sentence of propositional logic\njust says the opposite of that.\nSo if, for example, P represented it is raining,\nthen not P would represent the idea that it is not raining.\nAnd as you might expect, if P is false, meaning if the sentence,\nit is raining, is false, well then the sentence not P must be true.\nThe sentence that it is not raining is therefore true.\nSo not, you can imagine, just takes whatever is in P and it inverts it.\nIt turns false into true and true into false,\nmuch analogously to what the English word not means,\njust taking whatever comes after it and inverting it to mean the opposite.\nNext up, and also very English-like, is this idea\nof and represented by this upside-down V shape or this point shape.\nAnd as opposed to just taking a single argument the way not does,\nwe have P and we have not P. And is going to combine two different sentences\nin propositional logic together.\nSo I might have one sentence P and another sentence Q,\nand I want to combine them together to say P and Q.\nAnd the general logic for what P and Q means\nis it means that both of its operands are true.\nP is true and also Q is true.\nAnd so here's what that truth table looks like.\nThis time we have two variables, P and Q. And when we have two variables, each\nof which can be in two possible states, true or false,\nthat leads to two squared or four possible combinations\nof truth and falsehood.\nSo we have P is false and Q is false.\nWe have P is false and Q is true.\nP is true and Q is false.\nAnd then P and Q both are true.\nAnd those are the only four possibilities for what P and Q could mean.\nAnd in each of those situations, this third column here, P and Q,\nis telling us a little bit about what it actually means for P and Q to be true.\nAnd we see that the only case where P and Q is true is in this fourth row\nhere, where P happens to be true, Q also happens to be true.\nAnd in all other situations, P and Q is going to evaluate to false.\nSo this, again, is much in line with what our intuition of and might mean.\nIf I say P and Q, I probably mean that I expect both P and Q to be true.\nNext up, also potentially consistent with what we mean,\nis this word or, represented by this V shape, sort of an upside down and symbol.\nAnd or, as the name might suggest, is true if either of its arguments\nare true, as long as P is true or Q is true, then P or Q is going to be true.\nWhich means the only time that P or Q is false\nis if both of its operands are false.\nIf P is false and Q is false, then P or Q is going to be false.\nBut in all other cases, at least one of the operands is true.\nMaybe they're both true, in which case P or Q is going to evaluate to true.\nNow, this is mostly consistent with the way\nthat most people might use the word or, in the sense of speaking the word\nor in normal English, though there is sometimes when we might say\nor, where we mean P or Q, but not both, where we mean, sort of,\nit can only be one or the other.\nIt's important to note that this symbol here, this or,\nmeans P or Q or both, that those are totally OK.\nAs long as either or both of them are true,\nthen the or is going to evaluate to be true, as well.\nIt's only in the case where all of the operands\nare false that P or Q ultimately evaluates to false, as well.\nIn logic, there's another symbol known as the exclusive or,\nwhich encodes this idea of exclusivity of one or the other, but not both.\nBut we're not going to be focusing on that today.\nWhenever we talk about or, we're always talking about either or both,\nin this case, as represented by this truth table here.\nSo that now is not an and an or.\nAnd next up is what we might call implication,\nas denoted by this arrow symbol.\nSo we have P and Q. And this sentence here will generally\nread as P implies Q.\nAnd what P implies Q means is that if P is true, then Q is also true.\nSo I might say something like, if it is raining, then I will be indoors.\nMeaning, it is raining implies I will be indoors,\nas the logical sentence that I'm saying there.\nAnd the truth table for this can sometimes be a little bit tricky.\nSo obviously, if P is true and Q is true, then P implies Q. That's true.\nThat definitely makes sense.\nAnd it should also stand to reason that when P is true and Q is false,\nthen P implies Q is false.\nBecause if I said to you, if it is raining, then I will be out indoors.\nAnd it is raining, but I'm not indoors?\nWell, then it would seem to be that my original statement was not true.\nP implies Q means that if P is true, then Q also needs to be true.\nAnd if it's not, well, then the statement is false.\nWhat's also worth noting, though, is what happens when P is false.\nWhen P is false, the implication makes no claim at all.\nIf I say something like, if it is raining, then I will be indoors.\nAnd it turns out it's not raining.\nThen in that case, I am not making any statement\nas to whether or not I will be indoors or not.\nP implies Q just means that if P is true, Q must be true.\nBut if P is not true, then we make no claim about whether or not Q\nis true at all.\nSo in either case, if P is false, it doesn't matter what Q is.\nWhether it's false or true, we're not making any claim about Q whatsoever.\nWe can still evaluate the implication to true.\nThe only way that the implication is ever false\nis if our premise, P, is true, but the conclusion that we're drawing Q\nhappens to be false.\nSo in that case, we would say P does not imply Q in that case.\nFinally, the last connective that we'll discuss is this bi-conditional.\nYou can think of a bi-conditional as a condition\nthat goes in both directions.\nSo originally, when I said something like, if it is raining,\nthen I will be indoors.\nI didn't say what would happen if it wasn't raining.\nMaybe I'll be indoors, maybe I'll be outdoors.\nThis bi-conditional, you can read as an if and only if.\nSo I can say, I will be indoors if and only if it is raining,\nmeaning if it is raining, then I will be indoors.\nAnd if I am indoors, it's reasonable to conclude that it is also raining.\nSo this bi-conditional is only true when P and Q are the same.\nSo if P is true and Q is true, then this bi-conditional is also true.\nP implies Q, but also the reverse is true.\nQ also implies P. So if P and Q both happen to be false,\nwe would still say it's true.\nBut in any of these other two situations,\nthis P if and only if Q is going to ultimately evaluate to false.\nSo a lot of trues and falses going on there,\nbut these five basic logical connectives\nare going to form the core of the language of propositional logic,\nthe language that we're going to use in order to describe ideas,\nand the language that we're going to use in order\nto reason about those ideas in order to draw conclusions.\nSo let's now take a look at some of the additional terms\nthat we'll need to know about in order to go about trying\nto form this language of propositional logic\nand writing AI that's actually able to understand this sort of logic.\nThe next thing we're going to need is the notion of what\nis actually true about the world.\nWe have a whole bunch of propositional symbols, P and Q and R and maybe others,\nbut we need some way of knowing what actually is true in the world.\nIs P true or false?\nIs Q true or false?\nSo on and so forth.\nAnd to do that, we'll introduce the notion of a model.\nA model just assigns a truth value, where a truth value is either true\nor false, to every propositional symbol.\nIn other words, it's creating what we might call a possible world.\nSo let me give an example.\nIf, for example, I have two propositional symbols, P is it is raining\nand Q is it is a Tuesday, a model just takes each of these two symbols\nand assigns a truth value to them, either true or false.\nSo here's a sample model.\nIn this model, in other words, in this possible world,\nit is possible that P is true, meaning it is raining, and Q is false,\nmeaning it is not a Tuesday.\nBut there are other possible worlds or other models as well.\nThere is some model where both of these variables are true,\nsome model where both of these variables are false.\nIn fact, if there are n variables that are propositional symbols like this\nthat are either true or false, then the number of possible models\nis 2 to the n, because each of these possible models,\npossible variables within my model, could be set to either true or false\nif I don't know any information about it.\nSo now that I have the symbols and the connectives\nthat I'm going to need in order to construct these parts of knowledge,\nwe need some way to represent that knowledge.\nAnd to do so, we're going to allow our AI access\nto what we'll call a knowledge base.\nAnd a knowledge base is really just a set of sentences\nthat our AI knows to be true.\nSome set of sentences in propositional logic\nthat are things that our AI knows about the world.\nAnd so we might tell our AI some information, information about a situation\nthat it finds itself in, or a situation about a problem\nthat it happens to be trying to solve.\nAnd we would give that information to the AI\nthat the AI would store inside of its knowledge base.\nAnd what happens next is the AI would like\nto use that information in the knowledge base\nto be able to draw conclusions about the rest of the world.\nAnd what do those conclusions look like?\nWell, to understand those conclusions, we'll\nneed to introduce one more idea, one more symbol.\nAnd that is the notion of entailment.\nSo this sentence here, with this double turnstile in these Greek letters,\nthis is the Greek letter alpha and the Greek letter beta.\nAnd we read this as alpha entails beta.\nAnd alpha and beta here are just sentences in propositional logic.\nAnd what this means is that alpha entails beta\nmeans that in every model, in other words,\nin every possible world in which sentence alpha is true,\nthen sentence beta is also true.\nSo if something entails something else, if alpha entails beta,\nit means that if I know alpha to be true, then beta must therefore also\nbe true.\nSo if my alpha is something like I know that it is a Tuesday in January,\nthen a reasonable beta might be something like I know that it is January.\nBecause in all worlds where it is a Tuesday in January,\nI know for sure that it must be January, just by definition.\nThis first statement or sentence about the world\nentails the second statement.\nAnd we can reasonably use deduction based on that first sentence\nto figure out that the second sentence is, in fact, true as well.\nAnd ultimately, it's this idea of entailment\nthat we're going to try and encode into our computer.\nWe want our AI agent to be able to figure out\nwhat the possible entailments are.\nWe want our AI to be able to take these three sentences, sentences like,\nif it didn't rain, Harry visited Hagrid.\nThat Harry visited Hagrid or Dumbledore, but not both.\nAnd that Harry visited Dumbledore.\nAnd just using that information, we'd like our AI\nto be able to infer or figure out that using these three sentences inside\nof a knowledge base, we can draw some conclusions.\nIn particular, we can draw the conclusions here that, one,\nHarry did not visit Hagrid today.\nAnd we can draw the entailment, too, that it did, in fact, rain today.\nAnd this process is known as inference.\nAnd that's what we're going to be focusing on today,\nthis process of deriving new sentences from old ones,\nthat I give you these three sentences, you put them\nin the knowledge base in, say, the AI.\nAnd the AI is able to use some sort of inference algorithm\nto figure out that these two sentences must also be true.\nAnd that is how we define inference.\nSo let's take a look at an inference example\nto see how we might actually go about inferring things in a human sense\nbefore we take a more algorithmic approach\nto see how we could encode this idea of inference in AI.\nAnd we'll see there are a number of ways that we can actually achieve this.\nSo again, we'll deal with a couple of propositional symbols.\nWe'll deal with P, Q, and R. P is it is a Tuesday.\nQ is it is raining.\nAnd R is Harry will go for a run, three propositional symbols\nthat we are just defining to mean this.\nWe're not saying anything yet about whether they're true or false.\nWe're just defining what they are.\nNow, we'll give ourselves or an AI access to a knowledge base,\nabbreviated to KB, the knowledge that we know about the world.\nWe know this statement.\nAll right.\nSo let's try to parse it.\nThe parentheses here are just used for precedent,\nso we can see what associates with what.\nBut you would read this as P and not Q implies R.\nAll right.\nSo what does that mean?\nLet's put it piece by piece.\nP is it is a Tuesday.\nQ is it is raining, so not Q is it is not raining,\nand implies R is Harry will go for a run.\nSo the way to read this entire sentence in human natural language\nat least is if it is a Tuesday and it is not raining,\nthen Harry will go for a run.\nSo if it is a Tuesday and it is not raining,\nthen Harry will go for a run.\nAnd that is now inside of our knowledge base.\nAnd let's now imagine that our knowledge base has\ntwo other pieces of information as well.\nIt has information that P is true, that it is a Tuesday.\nAnd we also have the information not Q, that it is not raining,\nthat this sentence Q, it is raining, happens to be false.\nAnd those are the three sentences that we have access to.\nP and not Q implies R, P and not Q. Using that information,\nwe should be able to draw some inferences.\nP and not Q is only true if both P and not Q are true.\nAll right, we know that P is true and we know that not Q is true.\nSo we know that this whole expression is true.\nAnd the definition of implication is if this whole thing on the left\nis true, then this thing on the right must also be true.\nSo if we know that P and not Q is true, then R must be true as well.\nSo the inference we should be able to draw from all of this\nis that R is true and we know that Harry will go for a run\nby taking this knowledge inside of our knowledge base\nand being able to reason based on that idea.\nAnd so this ultimately is the beginning of what\nwe might consider to be some sort of inference algorithm,\nsome process that we can use to try and figure out\nwhether or not we can draw some conclusion.\nAnd ultimately, what these inference algorithms are going to answer\nis the central question about entailment.\nGiven some query about the world, something\nwe're wondering about the world, and we'll call that query alpha,\nthe question we want to ask using these inference algorithms\nis does KB, our knowledge base, entail alpha?\nIn other words, using only the information\nwe know inside of our knowledge base, the knowledge that we have access to,\ncan we conclude that this sentence alpha is true?\nAnd that's ultimately what we would like to do.\nSo how can we do that?\nHow can we go about writing an algorithm that\ncan look at this knowledge base and figure out whether or not this query\nalpha is actually true?\nWell, it turns out there are a couple of different algorithms for doing so.\nAnd one of the simplest, perhaps, is known as model checking.\nNow, remember that a model is just some assignment\nof all of the propositional symbols inside of our language to a truth\nvalue, true or false.\nAnd you can think of a model as a possible world,\nthat there are many possible worlds where different things might\nbe true or false, and we can enumerate all of them.\nAnd the model checking algorithm does exactly that.\nSo what does our model checking algorithm do?\nWell, if we wanted to determine if our knowledge base entails\nsome query alpha, then we are going to enumerate all possible models.\nIn other words, consider all possible values of true and false\nfor our variables, all possible states in which our world can be in.\nAnd if in every model where our knowledge base is true,\nalpha is also true, then we know that the knowledge base entails alpha.\nSo let's take a closer look at that sentence\nand try and figure out what it actually means.\nIf we know that in every model, in other words, in every possible world,\nno matter what assignment of true and false to variables you give,\nif we know that whenever our knowledge is true, what\nwe know to be true is true, that this query alpha is also true,\nwell, then it stands to reason that as long as our knowledge base is true,\nthen alpha must also be true.\nAnd so this is going to form the foundation of our model checking\nalgorithm.\nWe're going to enumerate all of the possible worlds\nand ask ourselves whenever the knowledge base is true, is alpha true?\nAnd if that's the case, then we know alpha to be true.\nAnd otherwise, there is no entailment.\nOur knowledge base does not entail alpha.\nAll right.\nSo this is a little bit abstract, but let's\ntake a look at an example to try and put real propositional symbols\nto this idea.\nSo again, we'll work with the same example.\nP is it is a Tuesday, Q is it is raining, R as Harry will go for a run.\nOur knowledge base contains these pieces of information.\nP and not Q implies R. We also know P.\nIt is a Tuesday and not Q. It is not raining.\nAnd our query, our alpha in this case, the thing we want to ask is R.\nWe want to know, is it guaranteed?\nIs it entailed that Harry will go for a run?\nSo the first step is to enumerate all of the possible models.\nWe have three propositional symbols here, P, Q, and R,\nwhich means we have 2 to the third power, or eight possible models.\nAll false, false, false true, false true, false, false true, true, et cetera.\nEight possible ways you could assign true and false to all of these models.\nAnd we might ask in each one of them, is the knowledge base true?\nHere are the set of things that we know.\nIn which of these worlds could this knowledge base possibly apply to?\nIn which world is this knowledge base true?\nWell, in the knowledge base, for example, we know P.\nWe know it is a Tuesday, which means we know that these four first four rows\nwhere P is false, none of those are going to be true\nor are going to work for this particular knowledge base.\nOur knowledge base is not true in those worlds.\nLikewise, we also know not Q. We know that it is not raining.\nSo any of these models where Q is true, like these two and these two here,\nthose aren't going to work either because we know that Q is not true.\nAnd finally, we also know that P and not Q implies R,\nwhich means that when P is true or P is true here and Q is false,\nQ is false in these two, then R must be true.\nAnd if ever P is true, Q is false, but R is also false,\nwell, that doesn't satisfy this implication here.\nThat implication does not hold true under those situations.\nSo we could say that for our knowledge base,\nwe can conclude under which of these possible worlds\nis our knowledge base true and under which of the possible worlds\nis our knowledge base false.\nAnd it turns out there is only one possible world\nwhere our knowledge base is actually true.\nIn some cases, there might be multiple possible worlds\nwhere the knowledge base is true.\nBut in this case, it just so happens that there's only one, one possible world\nwhere we can definitively say something about our knowledge base.\nAnd in this case, we would look at the query.\nThe query of R is R true, R is true, and so as a result,\nwe can draw that conclusion.\nAnd so this is this idea of model check-in.\nEnumerate all the possible models and look in those possible models\nto see whether or not, if our knowledge base is true,\nis the query in question true as well.\nSo let's now take a look at how we might actually go about writing this\nin a programming language like Python.\nTake a look at some actual code that would\nencode this notion of propositional symbols and logic\nand these connectives like and and or and not and implication and so forth\nand see what that code might actually look like.\nSo I've written in advance a logic library that's\nmore detailed than we need to worry about entirely today.\nBut the important thing is that we have one class for every type\nof logical symbol or connective that we might have.\nSo we just have one class for logical symbols, for example,\nwhere every symbol is going to represent and store\nsome name for that particular symbol.\nAnd we also have a class for not that takes an operand.\nSo we might say not one symbol to say something is not true\nor some other sentence is not true.\nWe have one for and, one for or, so on and so forth.\nAnd I'll just demonstrate how this works.\nAnd you can take a look at the actual logic.py later on.\nBut I'll go ahead and call this file harry.py.\nWe're going to store information about this world of Harry Potter,\nfor example.\nSo I'll go ahead and import from my logic module.\nI'll import everything.\nAnd in this library, in order to create a symbol, you use capital S symbol.\nAnd I'll create a symbol for rain, to mean it is raining, for example.\nAnd I'll create a symbol for Hagrid, to mean Harry visited Hagrid,\nis what this symbol is going to mean.\nSo this symbol means it is raining.\nThis symbol means Harry visited Hagrid.\nAnd I'll add another symbol called Dumbledore for Harry visited Dumbledore.\nNow, I'd like to save these symbols so that I can use them later\nas I do some logical analysis.\nSo I'll go ahead and save each one of them inside of a variable.\nSo like rain, Hagrid, and Dumbledore, so you could call the variables anything.\nAnd now that I have these logical symbols,\nI can use logical connectives to combine them together.\nSo for example, if I have a sentence like and rain and Hagrid,\nfor example, which is not necessarily true, but just for demonstration,\nI can now try and print out sentence.formula, which\nis a function I wrote that takes a sentence in propositional logic\nand just prints it out so that we, the programmers,\ncan now see this in order to get an understanding for how it actually works.\nSo if I run python harry.py, what we'll see\nis this sentence in propositional logic, rain and Hagrid.\nThis is the logical representation of what we have here in our Python program\nof saying and whose arguments are rain and Hagrid.\nSo we're saying rain and Hagrid by encoding that idea.\nAnd this is quite common in Python object-oriented programming,\nwhere you have a number of different classes,\nand you pass arguments into them in order to create a new and object,\nfor example, in order to represent this idea.\nBut now what I'd like to do is somehow encode the knowledge\nthat I have about the world in order to solve\nthat problem from the beginning of class, where\nwe talked about trying to figure out who Harry visited\nand trying to figure out if it's raining or if it's not raining.\nAnd so what knowledge do I have?\nI'll go ahead and create a new variable called knowledge.\nAnd what do I know?\nWell, I know the very first sentence that we talked about\nwas the idea that if it is not raining, then Harry will visit Hagrid.\nSo all right, how do I encode the idea that it is not raining?\nWell, I can use not and then the rain symbol.\nSo here's me saying that it is not raining.\nAnd now the implication is that if it is not raining,\nthen Harry visited Hagrid.\nSo I'll wrap this inside of an implication to say,\nif it is not raining, this first argument to the implication\nwill then Harry visited Hagrid.\nSo I'm saying implication, the premise is that it's not raining.\nAnd if it is not raining, then Harry visited Hagrid.\nAnd I can print out knowledge.formula to see the logical formula\nequivalent of that same idea.\nSo I run Python of harry.py.\nAnd this is the logical formula that we see\nas a result, which is a text-based version of what\nwe were looking at before, that if it is not raining,\nthen that implies that Harry visited Hagrid.\nBut there was additional information that we had access to as well.\nIn this case, we had access to the fact that Harry visited either Hagrid\nor Dumbledore.\nSo how do I encode that?\nWell, this means that in my knowledge, I've really\ngot multiple pieces of knowledge going on.\nI know one thing and another thing and another thing.\nSo I'll go ahead and wrap all of my knowledge inside of an and.\nAnd I'll move things on to new lines just for good measure.\nBut I know multiple things.\nSo I'm saying knowledge is an and of multiple different sentences.\nI know multiple different sentences to be true.\nOne such sentence that I know to be true is this implication,\nthat if it is not raining, then Harry visited Hagrid.\nAnother such sentence that I know to be true is or Hagrid Dumbledore.\nIn other words, Hagrid or Dumbledore is true,\nbecause I know that Harry visited Hagrid or Dumbledore.\nBut I know more than that, actually.\nThat initial sentence from before said that Harry visited Hagrid or Dumbledore,\nbut not both.\nSo now I want a sentence that will encode the idea that Harry didn't\nvisit both Hagrid and Dumbledore.\nWell, the notion of Harry visiting Hagrid and Dumbledore\nwould be represented like this, and of Hagrid and Dumbledore.\nAnd if that is not true, if I want to say not that,\nthen I'll just wrap this whole thing inside of a not.\nSo now these three lines, line 8 says that if it is not raining,\nthen Harry visited Hagrid.\nLine 9 says Harry visited Hagrid or Dumbledore.\nAnd line 10 says Harry didn't visit both Hagrid and Dumbledore,\nthat it is not true that both the Hagrid symbol and the Dumbledore\nsymbol are true.\nOnly one of them can be true.\nAnd finally, the last piece of information that I knew\nwas the fact that Harry visited Dumbledore.\nSo these now are the pieces of knowledge that I know, one sentence\nand another sentence and another and another.\nAnd I can print out what I know just to see it a little bit more visually.\nAnd here now is a logical representation of the information\nthat my computer is now internally representing\nusing these various different Python objects.\nAnd again, take a look at logic.py if you want to take a look at how exactly\nit's implementing this, but no need to worry too much about all of the details\nthere.\nWe're here saying that if it is not raining, then Harry visited Hagrid.\nWe're saying that Hagrid or Dumbledore is true.\nAnd we're saying it is not the case that Hagrid and Dumbledore is true,\nthat they're not both true.\nAnd we also know that Dumbledore is true.\nSo this long logical sentence represents our knowledge base.\nIt is the thing that we know.\nAnd now what we'd like to do is we'd like to use model checking\nto ask a query, to ask a question like, based on this information,\ndo I know whether or not it's raining?\nAnd we as humans were able to logic our way through it and figure out that,\nall right, based on these sentences, we can conclude this and that\nto figure out that, yes, it must have been raining.\nBut now we'd like for the computer to do that as well.\nSo let's take a look at the model checking algorithm\nthat is going to follow that same pattern\nthat we drew out in pseudocode a moment ago.\nSo I've defined a function here in logic.py\nthat you can take a look at called model check.\nModel check takes two arguments, the knowledge that I already know,\nand the query.\nAnd the idea is, in order to do model checking,\nI need to enumerate all of the possible models.\nAnd for each of the possible models, I need to ask myself,\nis the knowledge base true?\nAnd is the query true?\nSo the first thing I need to do is somehow\nenumerate all of the possible models, meaning\nfor all possible symbols that exist, I need\nto assign true and false to each one of them\nand see whether or not it's still true.\nAnd so here is the way we're going to do that.\nWe're going to start.\nSo I've defined another helper function internally\nthat we'll get to in just a moment.\nBut this function starts by getting all of the symbols in both the knowledge\nand the query, by figuring out what symbols am I dealing with.\nIn this case, the symbols I'm dealing with are rain and Hagrid and Dumbledore,\nbut there might be other symbols depending on the problem.\nAnd we'll take a look soon at some examples of situations\nwhere ultimately we're going to need some additional symbols in order\nto represent the problem.\nAnd then we're going to run this check all function, which\nis a helper function that's basically going to recursively call itself\nchecking every possible configuration of propositional symbols.\nSo we start out by looking at this check all function.\nAnd what do we do?\nSo if not symbols means if we finish assigning all of the symbols.\nWe've assigned every symbol a value.\nSo far we haven't done that, but if we ever do, then we check.\nIn this model, is the knowledge true?\nThat's what this line is saying.\nIf we evaluate the knowledge propositional logic formula\nusing the model's assignment of truth values, is the knowledge true?\nIf the knowledge is true, then we should return true only if the query is true.\nBecause if the knowledge is true, we want the query\nto be true as well in order for there to be entailment.\nOtherwise, we don't know that there otherwise there won't be an entailment\nif there's ever a situation where what we know in our knowledge is true,\nbut the query, the thing we're asking, happens to be false.\nSo this line here is checking that same idea\nthat in all worlds where the knowledge is true, the query must also be true.\nOtherwise, we can just return true because if the knowledge isn't true,\nthen we don't care.\nThis is equivalent to when we were enumerating\nthis table from a moment ago.\nIn all situations where the knowledge base wasn't true, all of these seven\nrows here, we didn't care whether or not our query was true or not.\nWe only care to check whether the query is true\nwhen the knowledge base is actually true, which was just this green highlighted\nrow right there.\nSo that logic is encoded using that statement there.\nAnd otherwise, if we haven't assigned symbols yet,\nwhich we haven't seen anything yet, then the first thing we do\nis pop one of the symbols.\nI make a copy of the symbols first just to save an existing copy.\nBut I pop one symbol off of the remaining symbols\nso that I just pick one symbol at random.\nAnd I create one copy of the model where that symbol is true.\nAnd I create a second copy of the model where that symbol is false.\nSo I now have two copies of the model, one where the symbol is true\nand one where the symbol is false.\nAnd I need to make sure that this entailment holds in both of those models.\nSo I recursively check all on the model where the statement is true\nand check all on the model where the statement is false.\nSo again, you can take a look at that function\nto try to get a sense for how exactly this logic is working.\nBut in effect, what it's doing is recursively\ncalling this check all function again and again and again.\nAnd on every level of the recursion, we're\nsaying let's pick a new symbol that we haven't yet assigned,\nassign it to true and assign it to false,\nand then check to make sure that the entailment holds in both cases.\nBecause ultimately, I need to check every possible world.\nI need to take every combination of symbols\nand try every combination of true and false\nin order to figure out whether the entailment relation actually holds.\nSo that function we've written for you.\nBut in order to use that function inside of harry.py,\nwhat I'll write is something like this.\nI would like to model check based on the knowledge.\nAnd then I provide as a second argument what the query is,\nwhat the thing I want to ask is.\nAnd what I want to ask in this case is, is it raining?\nSo model check again takes two arguments.\nThe first argument is the information that I know, this knowledge,\nwhich in this case is this information that was given to me at the beginning.\nAnd the second argument, rain, is encoding the idea of the query.\nWhat am I asking?\nI would like to ask, based on this knowledge,\ndo I know for sure that it is raining?\nAnd I can try and print out the result of that.\nAnd when I run this program, I see that the answer is true.\nThat based on this information, I can conclusively\nsay that it is raining, because using this model checking algorithm,\nwe were able to check that in every world where this knowledge is true,\nit is raining.\nIn other words, there is no world where this knowledge is true,\nand it is not raining.\nSo you can conclude that it is, in fact, raining.\nAnd this sort of logic can be applied to a number\nof different types of problems, that if confronted with a problem where\nsome sort of logical deduction can be used in order to try to solve it,\nyou might try thinking about what propositional symbols you might\nneed in order to represent that information,\nand what statements and propositional logic\nyou might use in order to encode that information which you know.\nAnd this process of trying to take a problem\nand figure out what propositional symbols to use in order\nto encode that idea, or how to represent it logically,\nis known as knowledge engineering.\nThat software engineers and AI engineers will take a problem\nand try and figure out how to distill it down\ninto knowledge that is representable by a computer.\nAnd if we can take any general purpose problem, some problem\nthat we find in the human world, and turn it\ninto a problem that computers know how to solve\nas by using any number of different variables, well,\nthen we can take a computer that is able to do something\nlike model checking or some other inference algorithm\nand actually figure out how to solve that problem.\nSo now we'll take a look at two or three examples of knowledge engineering\nand practice, of taking some problem and figuring out\nhow we can apply logical symbols and use logical formulas\nto be able to encode that idea.\nAnd we'll start with a very popular board game in the US and the UK\nknown as Clue.\nNow, in the game of Clue, there's a number of different factors\nthat are going on.\nBut the basic premise of the game, if you've never played it before,\nis that there are a number of different people.\nFor now, we'll just use three, Colonel Mustard, Professor Plumb,\nand Miss Scarlet.\nThere are a number of different rooms, like a ballroom, a kitchen,\nand a library.\nAnd there are a number of different weapons, a knife, a revolver, and a wrench.\nAnd three of these, one person, one room, and one weapon,\nis the solution to the mystery, the murderer and what room they were in\nand what weapon they happened to use.\nAnd what happens at the beginning of the game\nis that all these cards are randomly shuffled together.\nAnd three of them, one person, one room, and one weapon,\nare placed into a sealed envelope that we don't know.\nAnd we would like to figure out, using some sort of logical process,\nwhat's inside the envelope, which person, which room, and which weapon.\nAnd we do so by looking at some, but not all, of these cards here,\nby looking at these cards to try and figure out what might be going on.\nAnd so this is a very popular game.\nBut let's now try and formalize it and see\nif we could train a computer to be able to play this game by reasoning\nthrough it logically.\nSo in order to do this, we'll begin by thinking about what\npropositional symbols we're ultimately going to need.\nRemember, again, that propositional symbols are just some symbol,\nsome variable, that can be either true or false in the world.\nAnd so in this case, the propositional symbols\nare really just going to correspond to each of the possible things that\ncould be inside the envelope.\nMustard is a propositional symbol that, in this case,\nwill just be true if Colonel Mustard is inside the envelope,\nif he is the murderer, and false otherwise.\nAnd likewise for Plum, for Professor Plum, and Scarlet, for Miss Scarlet.\nAnd likewise for each of the rooms and for each of the weapons.\nWe have one propositional symbol for each of these ideas.\nThen using those propositional symbols, we\ncan begin to create logical sentences, create knowledge\nthat we know about the world.\nSo for example, we know that someone is the murderer,\nthat one of the three people is, in fact, the murderer.\nAnd how would we encode that?\nWell, we don't know for sure who the murderer is.\nBut we know it is one person or the second person or the third person.\nSo I could say something like this.\nMustard or Plum or Scarlet.\nAnd this piece of knowledge encodes that one of these three people\nis the murderer.\nWe don't know which, but one of these three things must be true.\nWhat other information do we know?\nWell, we know that, for example, one of the rooms\nmust have been the room in the envelope.\nThe crime was committed either in the ballroom or the kitchen or the library.\nAgain, right now, we don't know which.\nBut this is knowledge we know at the outset,\nknowledge that one of these three must be inside the envelope.\nAnd likewise, we can say the same thing about the weapon,\nthat it was either the knife or the revolver or the wrench,\nthat one of those weapons must have been the weapon of choice\nand therefore the weapon in the envelope.\nAnd then as the game progresses, the gameplay\nworks by people get various different cards.\nAnd using those cards, you can deduce information.\nThat if someone gives you a card, for example,\nI have the Professor Plum card in my hand,\nthen I know the Professor Plum card can't be inside the envelope.\nI know that Professor Plum is not the criminal,\nso I know a piece of information like not Plum, for example.\nI know that Professor Plum has to be false.\nThis propositional symbol is not true.\nAnd sometimes I might not know for sure that a particular card is not\nin the middle, but sometimes someone will make a guess\nand I'll know that one of three possibilities is not true.\nSomeone will guess Colonel Mustard in the library with the revolver\nor something to that effect.\nAnd in that case, a card might be revealed that I don't see.\nBut if it is a card and it is either Colonel Mustard or the revolver\nor the library, then I know that at least one of them\ncan't be in the middle.\nSo I know something like it is either not Mustard\nor it is not the library or it is not the revolver.\nNow maybe multiple of these are not true,\nbut I know that at least one of Mustard, Library, and Revolver\nmust, in fact, be false.\nAnd so this now is a propositional logic representation\nof this game of Clue, a way of encoding the knowledge that we\nknow inside this game using propositional logic\nthat a computer algorithm, something like model checking\nthat we saw a moment ago, can actually look at and understand.\nSo let's now take a look at some code to see\nhow this algorithm might actually work in practice.\nAll right, so I'm now going to open up a file called Clue.py, which\nI've started already.\nAnd what we'll see here is I've defined a couple of things.\nTo find some symbols initially, notice I\nhave a symbol for Colonel Mustard, a symbol for Professor Plum,\na symbol for Miss Scarlett, all of which\nI've put inside of this list of characters.\nI have a symbol for Ballroom and Kitchen and Library\ninside of a list of rooms.\nAnd then I have symbols for Knife and Revolver and Wrench.\nThese are my weapons.\nAnd so all of these characters and rooms and weapons altogether,\nthose are my symbols.\nAnd now I also have this check knowledge function.\nAnd what the check knowledge function does is it takes my knowledge\nand it's going to try and draw conclusions about what I know.\nSo for example, we'll loop over all of the possible symbols\nand we'll check, do I know that that symbol is true?\nAnd a symbol is going to be something like Professor Plum\nor the Knife or the Library.\nAnd if I know that it is true, in other words,\nI know that it must be the card in the envelope,\nthen I'm going to print out using a function called\ncprint, which prints things in color.\nI'm going to print out the word yes, and I'm\ngoing to print that in green, just to make it very clear to us.\nIf we're not sure that the symbol is true,\nmaybe I can check to see if I'm sure that the symbol is not true.\nLike if I know for sure that it is not Professor Plum, for example.\nAnd I do that by running model check again,\nthis time checking if my knowledge is not the symbol,\nif I know for sure that the symbol is not true.\nAnd if I don't know for sure that the symbol is not true,\nbecause I say if not model check, meaning I'm not sure that the symbol is\nfalse, well, then I'll go ahead and print out maybe next to the symbol.\nBecause maybe the symbol is true, maybe it's not, I don't actually know.\nSo what knowledge do I actually have?\nWell, let's try and represent my knowledge now.\nSo my knowledge is, I know a couple of things, so I'll put them in an and.\nAnd I know that one of the three people must be the criminal.\nSo I know or mustard, plum, scarlet.\nThis is my way of encoding that it is either Colonel Mustard or Professor\nPlum or Miss Scarlet.\nI know that it must have happened in one of the rooms.\nSo I know or ballroom, kitchen, library, for example.\nAnd I know that one of the weapons must have been used as well.\nSo I know or knife, revolver, wrench.\nSo that might be my initial knowledge, that I\nknow that it must have been one of the people,\nI know it must have been in one of the rooms,\nand I know that it must have been one of the weapons.\nAnd I can see what that knowledge looks like as a formula\nby printing out knowledge.formula.\nSo I'll run python clue.py.\nAnd here now is the information that I know in logical format.\nI know that it is Colonel Mustard or Professor Plum or Miss Scarlet.\nAnd I know that it is the ballroom, the kitchen, or the library.\nAnd I know that it is the knife, the revolver, or the wrench.\nBut I don't know much more than that.\nI can't really draw any firm conclusions.\nAnd in fact, we can see that if I try and do,\nlet me go ahead and run my knowledge check function on my knowledge.\nKnowledge check is this function that I, or check knowledge rather,\nis this function that I just wrote that looks over all of the symbols\nand tries to see what conclusions I can actually\ndraw about any of the symbols.\nSo I'll go ahead and run clue.py and see what it is that I know.\nAnd it seems that I don't really know anything for sure.\nI have all three people are maybes, all three of the rooms are maybes,\nall three of the weapons are maybes.\nI don't really know anything for certain just yet.\nBut now let me try and add some additional information\nand see if additional information, additional knowledge,\ncan help us to logically reason our way through this process.\nAnd we are just going to provide the information.\nOur AI is going to take care of doing the inference\nand figuring out what conclusions it's able to draw.\nSo I start with some cards.\nAnd those cards tell me something.\nSo if I have the kernel mustard card, for example,\nI know that the mustard symbol must be false.\nIn other words, mustard is not the one in the envelope,\nis not the criminal.\nSo I can say, knowledge supports something called,\nevery and in this library supports dot add,\nwhich is a way of adding knowledge or adding\nan additional logical sentence to an and clause.\nSo I can say, knowledge dot add, not mustard.\nI happen to know, because I have the mustard card,\nthat kernel mustard is not the suspect.\nAnd maybe I have a couple of other cards too.\nMaybe I also have a card for the kitchen.\nSo I know it's not the kitchen.\nAnd maybe I have another card that says that it is not the revolver.\nSo I have three cards, kernel mustard, the kitchen, and the revolver.\nAnd I encode that into my AI this way by saying, it's not kernel mustard,\nit's not the kitchen, and it's not the revolver.\nAnd I know those to be true.\nSo now, when I rerun clue.py, we'll see that I've\nbeen able to eliminate some possibilities.\nBefore, I wasn't sure if it was the knife or the revolver or the wrench.\nIf a knife was maybe, a revolver was maybe, wrench is maybe.\nNow I'm down to just the knife and the wrench.\nBetween those two, I don't know which one it is.\nThey're both maybes.\nBut I've been able to eliminate the revolver, which\nis one that I know to be false, because I have the revolver card.\nAnd so additional information might be acquired\nover the course of this game.\nAnd we would represent that just by adding knowledge to our knowledge set\nor knowledge base that we've been building here.\nSo if, for example, we additionally got the information\nthat someone made a guess, someone guessed like Miss Scarlet\nin the library with the wrench.\nAnd we know that a card was revealed, which\nmeans that one of those three cards, either Miss Scarlet\nor the library or the wrench, one of those at minimum\nmust not be inside of the envelope.\nSo I could add some knowledge, say knowledge.add.\nAnd I'm going to add an or clause, because I don't know for sure which one\nit's not, but I know one of them is not in the envelope.\nSo it's either not Scarlet, or it's not the library,\nand or supports multiple arguments.\nI can say it's also or not the wrench.\nSo at least one of those needs a Scarlet library and wrench.\nAt least one of those needs to be false.\nI don't know which, though.\nMaybe it's multiple.\nMaybe it's just one, but at least one I know needs to hold.\nAnd so now if I rerun clue.py, I don't actually\nhave any additional information just yet.\nNothing I can say conclusively.\nI still know that maybe it's Professor Plum, maybe it's Miss Scarlet.\nI haven't eliminated any options.\nBut let's imagine that I get some more information,\nthat someone shows me the Professor Plum card, for example.\nSo I say, all right, let's go back here, knowledge.add, not Plum.\nSo I have the Professor Plum card.\nI know the Professor Plum is not in the middle.\nI rerun clue.py.\nAnd right now, I'm able to draw some conclusions.\nNow I've been able to eliminate Professor Plum,\nand the only person it could left remaining be is Miss Scarlet.\nSo I know, yes, Miss Scarlet, this variable must be true.\nAnd I've been able to infer that based on the information I already had.\nNow between the ballroom and the library and the knife and the wrench,\nfor those two, I'm still not sure.\nSo let's add one more piece of information.\nLet's say that I know that it's not the ballroom.\nSomeone has shown me the ballroom card, so I know it's not the ballroom.\nWhich means at this point, I should be able to conclude that it's the library.\nLet's see.\nI'll say knowledge.add, not the ballroom.\nAnd we'll go ahead and run that.\nAnd it turns out that after all of this, not only can I conclude that I\nknow that it's the library, but I also know that the weapon was the knife.\nAnd that might have been an inference that was a little bit trickier, something\nI wouldn't have realized immediately, but the AI,\nvia this model checking algorithm, is able to draw that conclusion,\nthat we know for sure that it must be Miss Scarlet in the library with the knife.\nAnd how did we know that?\nWell, we know it from this or clause up here,\nthat we know that it's either not Scarlet, or it's not the library,\nor it's not the wrench.\nAnd given that we know that it is Miss Scarlet,\nand we know that it is the library, then the only remaining option for the weapon\nis that it is not the wrench, which means that it must be the knife.\nSo we as humans now can go back and reason through that,\neven though it might not have been immediately clear.\nAnd that's one of the advantages of using an AI or some sort of algorithm\nin order to do this, is that the computer can exhaust all of these possibilities\nand try and figure out what the solution actually should be.\nAnd so for that reason, it's often helpful to be\nable to represent knowledge in this way.\nKnowledge engineering, some situation where\nwe can use a computer to be able to represent knowledge\nand draw conclusions based on that knowledge.\nAnd any time we can translate something into propositional logic symbols\nlike this, this type of approach can be useful.\nSo you might be familiar with logic puzzles,\nwhere you have to puzzle your way through trying to figure something out.\nThis is what a classic logic puzzle might look like.\nSomething like Gilderoy, Minerva, Pomona, and Horace each\nbelong to a different one of the four houses, Gryffindor, Hufflepuff, Ravenclaw,\nand Slytherin.\nAnd then we have some information.\nThe Gilderoy belongs to Gryffindor or Ravenclaw, Pomona\ndoes not belong in Slytherin, and Minerva does belong to Gryffindor.\nSo we have a couple pieces of information.\nAnd using that information, we need to be\nable to draw some conclusions about which person should\nbe assigned to which house.\nAnd again, we can use the exact same idea to try and implement this notion.\nSo we need some propositional symbols.\nAnd in this case, the propositional symbols\nare going to get a little more complex, although we'll\nsee ways to make this a little bit cleaner later on.\nBut we'll need 16 propositional symbols, one for each person and house.\nSo we need to say, remember, every propositional symbol\nis either true or false.\nSo Gilderoy Gryffindor is either true or false.\nEither he's in Gryffindor or he is not.\nLikewise, Gilderoy Hufflepuff also true or false.\nEither it is true or it's false.\nAnd that's true for every combination of person and house\nthat we could come up with.\nWe have some sort of propositional symbol for each one of those.\nUsing this type of knowledge, we can then\nbegin to think about what types of logical sentences\nwe can say about the puzzle.\nThat if we know what will before even think about the information we were\ngiven, we can think about the premise of the problem,\nthat every person is assigned to a different house.\nSo what does that tell us?\nWell, it tells us sentences like this.\nIt tells us like Pomona Slytherin implies not Pomona Hufflepuff.\nSomething like if Pomona is in Slytherin,\nthen we know that Pomona is not in Hufflepuff.\nAnd we know this for all four people and for all combinations of houses,\nthat no matter what person you pick, if they're in one house,\nthen they're not in some other house.\nSo I'll probably have a whole bunch of knowledge statements\nthat are of this form, that if we know Pomona is in Slytherin,\nthen we know Pomona is not in Hufflepuff.\nWe were also given the information that each person\nis in a different house.\nSo I also have pieces of knowledge that look something like this.\nMinerva Ravenclaw implies not Gilderoy Ravenclaw.\nIf they're all in different houses, then if Minerva is in Ravenclaw,\nthen we know the Gilderoy is not in Ravenclaw as well.\nAnd I have a whole bunch of similar sentences\nlike this that are expressing that idea for other people and other houses\nas well.\nAnd so in addition to sentences of these form,\nI also have the knowledge that was given to me.\nInformation like Gilderoy was in Gryffindor or in Ravenclaw\nthat would be represented like this, Gilderoy Gryffindor or Gilderoy\nRavenclaw.\nAnd then using these sorts of sentences,\nI can begin to draw some conclusions about the world.\nSo let's see an example of this.\nWe'll go ahead and actually try and implement this logic puzzle\nto see if we can figure out what the answer is.\nI'll go ahead and open up puzzle.py, where I've already\nstarted to implement this sort of idea.\nI've defined a list of people and a list of houses.\nAnd I've so far created one symbol for every person and for every house.\nThat's what this double four loop is doing, looping over all people,\nlooping over all houses, creating a new symbol for each of them.\nAnd then I've added some information.\nI know that every person belongs to a house,\nso I've added the information for every person that person Gryffindor\nor person Hufflepuff or person Ravenclaw or person Slytherin,\nthat one of those four things must be true.\nEvery person belongs to a house.\nWhat other information do I know?\nI also know that only one house per person,\nso no person belongs to multiple houses.\nSo how does this work?\nWell, this is going to be true for all people.\nSo I'll loop over every person.\nAnd then I need to loop over all different pairs of houses.\nThe idea is I want to encode the idea that if Minerva is in Gryffindor,\nthen Minerva can't be in Ravenclaw.\nSo I'll loop over all houses, each one.\nAnd I'll loop over all houses again, h2.\nAnd as long as they're different, h1 not equal to h2,\nthen I'll add to my knowledge base this piece of information.\nThat implication, in other words, an if then, if the person is in h1,\nthen I know that they are not in house h2.\nSo these lines here are encoding the notion that for every person,\nif they belong to house one, then they are not in house two.\nAnd the other piece of logic we need to encode\nis the idea that every house can only have one person.\nIn other words, if Pomona is in Hufflepuff,\nthen nobody else is allowed to be in Hufflepuff either.\nAnd that's the same logic, but sort of backwards.\nI loop over all of the houses and loop over all different pairs of people.\nSo I loop over people once, loop over people again,\nand only do this when the people are different, p1 not equal to p2.\nAnd I add the knowledge that if, as given by the implication,\nif person one belongs to the house, then it\nis not the case that person two belongs to the same house.\nSo here I'm just encoding the knowledge that\nrepresents the problem's constraints.\nI know that everyone's in a different house.\nI know that any person can only belong to one house.\nAnd I can now take my knowledge and try and print out the information\nthat I happen to know.\nSo I'll go ahead and print out knowledge.formula,\njust to see this in action, and I'll go ahead and skip this for now.\nBut we'll come back to this in a second.\nLet's print out the knowledge that I know by running Python puzzle.py.\nIt's a lot of information, a lot that I have to scroll through,\nbecause there are 16 different variables all going on.\nBut the basic idea, if we scroll up to the very top,\nis I see my initial information.\nGilderoy is either in Gryffindor, or Gilderoy is in Hufflepuff,\nor Gilderoy is in Ravenclaw, or Gilderoy is in Slytherin,\nand then way more information as well.\nSo this is quite messy, more than we really want to be looking at.\nAnd soon, too, we'll see ways of representing\nthis a little bit more nicely using logic.\nBut for now, we can just say these are the variables\nthat we're dealing with.\nAnd now we'd like to add some information.\nSo the information we're going to add is Gilderoy is in Gryffindor,\nor he is in Ravenclaw.\nSo that knowledge was given to us.\nSo I'll go ahead and say knowledge.add.\nAnd I know that either or Gilderoy Gryffindor or Gilderoy Ravenclaw.\nOne of those two things must be true.\nI also know that Pomona was not in Slytherin,\nso I can say knowledge.add not this symbol, not the Pomona-Slytherin\nsymbol.\nAnd then I can add the knowledge that Minerva is in Gryffindor\nby adding the symbol Minerva Gryffindor.\nSo those are the pieces of knowledge that I know.\nAnd this loop here at the bottom just loops over all of my symbols,\nchecks to see if the knowledge entails that symbol\nby calling this model check function again.\nAnd if it does, if we know the symbol is true, we print out the symbol.\nSo now I can run Python, puzzle.py, and Python\nis going to solve this puzzle for me.\nWe're able to conclude that Gilderoy belongs to Ravenclaw,\nPomona belongs to Hufflepuff, Minerva to Gryffindor, and Horace to Slytherin\njust by encoding this knowledge inside the computer,\nalthough it was quite tedious to do in this case.\nAnd as a result, we were able to get the conclusion from that as well.\nAnd you can imagine this being applied to many sorts\nof different deductive situations.\nSo not only these situations where we're trying\nto deal with Harry Potter characters in this puzzle,\nbut if you've ever played games like Mastermind, where\nyou're trying to figure out which order different colors go in\nand trying to make predictions about it, I\ncould tell you, for example, let's play a simplified version of Mastermind\nwhere there are four colors, red, blue, green, and yellow,\nand they're in some order, but I'm not telling you what order.\nYou just have to make a guess, and I'll tell you\nof red, blue, green, and yellow how many of the four\nyou got in the right position.\nSo a simplified version of this game, you\nmight make a guess like red, blue, green, yellow,\nand I would tell you something like two of those four\nare in the correct position, but the other two are not.\nAnd then you could reasonably make a guess and say, all right,\nlook at this, blue, red, green, yellow.\nTry switching two of them around, and this time maybe I tell you,\nyou know what, none of those are in the correct position.\nAnd the question then is, all right, what is the correct order\nof these four colors?\nAnd we as humans could begin to reason this through.\nAll right, well, if none of these were correct,\nbut two of these were correct, well, it must have been\nbecause I switched the red and the blue, which means red and blue here\nmust be correct, which means green and yellow are probably not correct.\nYou can begin to do this sort of deductive reasoning.\nAnd we can also equivalently try and take this\nand encode it inside of our computer as well.\nAnd it's going to be very similar to the logic puzzle\nthat we just did a moment ago.\nSo I won't spend too much time on this code because it is fairly similar.\nBut again, we have a whole bunch of colors\nand four different positions in which those colors can be.\nAnd then we have some additional knowledge.\nAnd I encode all of that knowledge.\nAnd you can take a look at this code on your own time.\nBut I just want to demonstrate that when we run this code,\nrun python mastermind.py and run and see what we get,\nwe ultimately are able to compute red 0 in the 0 position,\nblue in the 1 position, yellow in the 2 position,\nand green in the 3 position as the ordering of those symbols.\nNow, ultimately, what you might have noticed\nis this process was taking quite a long time.\nAnd in fact, model checking is not a particularly efficient algorithm, right?\nWhat I need to do in order to model check\nis take all of my possible different variables\nand enumerate all of the possibilities that they could be in.\nIf I have n variables, I have 2 to the n possible worlds\nthat I need to be looking through in order\nto perform this model checking algorithm.\nAnd this is probably not tractable, especially\nas we start to get to much larger and larger sets of data\nwhere you have many, many more variables that are at play.\nRight here, we only have a relatively small number of variables.\nSo this sort of approach can actually work.\nBut as the number of variables increases, model checking\nbecomes less and less good of a way of trying\nto solve these sorts of problems.\nSo while it might have been OK for something like Mastermind\nto conclude that this is indeed the correct sequence where all four\nare in the correct position, what we'd like to do\nis come up with some better ways to be able to make inferences rather than\njust enumerate all of the possibilities.\nAnd to do so, what we'll transition to next\nis the idea of inference rules, some sort of rules\nthat we can apply to take knowledge that already exists\nand translate it into new forms of knowledge.\nAnd the general way we'll structure an inference rule\nis by having a horizontal line here.\nAnything above the line is going to represent a premise, something\nthat we know to be true.\nAnd then anything below the line will be the conclusion\nthat we can arrive at after we apply the logic from the inference rule\nthat we're going to demonstrate.\nSo we'll do some of these inference rules\nby demonstrating them in English first, but then translating them\ninto the world of propositional logic so you\ncan see what those inference rules actually look like.\nSo for example, let's imagine that I have access\nto two pieces of information.\nI know, for example, that if it is raining,\nthen Harry is inside, for example.\nAnd let's say I also know it is raining.\nThen most of us could reasonably then look at this information\nand conclude that, all right, Harry must be inside.\nThis inference rule is known as modus ponens,\nand it's phrased more formally in logic as this.\nIf we know that alpha implies beta, in other words, if alpha, then beta,\nand we also know that alpha is true, then we\nshould be able to conclude that beta is also true.\nWe can apply this inference rule to take these two pieces of information\nand generate this new piece of information.\nNotice that this is a totally different approach from the model checking\napproach, where the approach was look at all of the possible worlds\nand see what's true in each of these worlds.\nHere, we're not dealing with any specific world.\nWe're just dealing with the knowledge that we know\nand what conclusions we can arrive at based on that knowledge.\nThat I know that A implies B, and I know A, and the conclusion is B.\nAnd this should seem like a relatively obvious rule.\nBut of course, if alpha, then beta, and we know alpha,\nthen we should be able to conclude that beta is also true.\nAnd that's going to be true for many, but maybe even\nall of the inference rules that we'll take a look at.\nYou should be able to look at them and say,\nyeah, of course that's going to be true.\nBut it's putting these all together, figuring out the right combination\nof inference rules that can be applied that ultimately\nis going to allow us to generate interesting knowledge inside of our AI.\nSo that's modus ponensis application of implication,\nthat if we know alpha and we know that alpha implies beta,\nthen we can conclude beta.\nLet's take a look at another example.\nFairly straightforward, something like Harry is friends with Ron and Hermione.\nBased on that information, we can reasonably\nconclude Harry is friends with Hermione.\nThat must also be true.\nAnd this inference rule is known as and elimination.\nAnd what and elimination says is that if we have a situation where alpha\nand beta are both true, I have information alpha and beta,\nwell then, just alpha is true.\nOr likewise, just beta is true.\nThat if I know that both parts are true, then one of those parts\nmust also be true.\nAgain, something obvious from the point of view of human intuition,\nbut a computer needs to be told this kind of information.\nTo be able to apply the inference rule, we\nneed to tell the computer that this is an inference rule that you can apply,\nso the computer has access to it and is able to use it\nin order to translate information from one form to another.\nIn addition to that, let's take a look at another example of an inference\nrule, something like it is not true that Harry did not pass the test.\nBit of a tricky sentence to parse.\nI'll read it again.\nIt is not true, or it is false, that Harry did not pass the test.\nWell, if it is false that Harry did not pass the test,\nthen the only reasonable conclusion is that Harry did pass the test.\nAnd so this, instead of being and elimination,\nis what we call double negation elimination.\nThat if we have two negatives inside of our premise,\nthen we can just remove them altogether.\nThey cancel each other out.\nOne turns true to false, and the other one turns false back into true.\nPhrased a little bit more formally, we say\nthat if the premise is not alpha, then the conclusion\nwe can draw is just alpha.\nWe can say that alpha is true.\nWe'll take a look at a couple more of these.\nIf I have it is raining, then Harry is inside.\nHow do I reframe this?\nWell, this one is a little bit trickier.\nBut if I know if it is raining, then Harry is inside,\nthen I conclude one of two things must be true.\nEither it is not raining, or Harry is inside.\nNow, this one's trickier.\nSo let's think about it a little bit.\nThis first premise here, if it is raining, then Harry is inside,\nis saying that if I know that it is raining, then Harry must be inside.\nSo what is the other possible case?\nWell, if Harry is not inside, then I know that it must not be raining.\nSo one of those two situations must be true.\nEither it's not raining, or it is raining, in which case Harry is inside.\nSo the conclusion I can draw is either it is not raining,\nor it is raining, so therefore, Harry is inside.\nAnd so this is a way to translate if-then statements into or statements.\nAnd this is known as implication elimination.\nAnd this is similar to what we actually did in the beginning\nwhen we were first looking at those very first sentences\nabout Harry and Hagrid and Dumbledore.\nAnd phrased a little bit more formally, this\nsays that if I have the implication, alpha implies beta,\nthat I can draw the conclusion that either not alpha or beta,\nbecause there are only two possibilities.\nEither alpha is true or alpha is not true.\nSo one of those possibilities is alpha is not true.\nBut if alpha is true, well, then we can draw the conclusion\nthat beta must be true.\nSo either alpha is not true or alpha is true, in which case beta is also true.\nSo this is one way to turn an implication into just a statement about or.\nIn addition to eliminating implications,\nwe can also eliminate biconditionals as well.\nSo let's take an English example, something like,\nit is raining if and only if Harry is inside.\nAnd this if and only if really sounds like that biconditional,\nthat double arrow sign that we saw in propositional logic not too long ago.\nAnd what does this actually mean if we were to translate this?\nWell, this means that if it is raining, then Harry is inside.\nAnd if Harry is inside, then it is raining,\nthat this implication goes both ways.\nAnd this is what we would call biconditional elimination,\nthat I can take a biconditional, a if and only if b,\nand translate that into something like this, a implies b, and b implies a.\nSo many of these inference rules are taking logic that uses certain symbols\nand turning them into different symbols, taking an implication\nand turning it into an or, or taking a biconditional\nand turning it into implication.\nAnd another example of it would be something like this.\nIt is not true that both Harry and Ron passed the test.\nWell, all right, how do we translate that?\nWhat does that mean?\nWell, if it is not true that both of them passed the test, well,\nthen the reasonable conclusion we might draw\nis that at least one of them didn't pass the test.\nSo the conclusion is either Harry did not pass the test\nor Ron did not pass the test, or both.\nThis is not an exclusive or.\nBut if it is true that it is not true that both Harry and Ron passed the test,\nwell, then either Harry didn't pass the test or Ron didn't pass the test.\nAnd this type of law is one of De Morgan's laws.\nQuite famous in logic where the idea is that we can turn an and into an or.\nWe can say we can take this and that both Harry and Ron passed the test\nand turn it into an or by moving the nots around.\nSo if it is not true that Harry and Ron passed the test,\nwell, then either Harry did not pass the test\nor Ron did not pass the test either.\nAnd the way we frame that more formally using logic is to say this.\nIf it is not true that alpha and beta, well, then either not alpha or not beta.\nThe way I like to think about this is that if you\nhave a negation in front of an and expression,\nyou move the negation inwards, so to speak,\nmoving the negation into each of these individual sentences\nand then flip the and into an or.\nSo the negation moves inwards and the and flips into an or.\nSo I go from not a and b to not a or not b.\nAnd there's actually a reverse of De Morgan's law\nthat goes in the other direction for something like this.\nIf I say it is not true that Harry or Ron passed the test,\nmeaning neither of them passed the test, well, then the conclusion I can draw\nis that Harry did not pass the test and Ron did not pass the test.\nSo in this case, instead of turning an and into an or,\nwe're turning an or into an and.\nBut the idea is the same.\nAnd this, again, is another example of De Morgan's laws.\nAnd the way that works is that if I have not a or b this time,\nthe same logic is going to apply.\nI'm going to move the negation inwards.\nAnd I'm going to flip this time, flip the or into an and.\nSo if not a or b, meaning it is not true that a or b or alpha or beta,\nthen I can say not alpha and not beta, moving the negation inwards\nin order to make that conclusion.\nSo those are De Morgan's laws and a couple other inference rules\nthat are worth just taking a look at.\nOne is the distributive law that works this way.\nSo if I have alpha and beta or gamma, well, then much in the same way\nthat you can use in math, use distributive laws to distribute\noperands like addition and multiplication,\nI can do a similar thing here, where I can say if alpha and beta or gamma,\nthen I can say something like alpha and beta or alpha and gamma,\nthat I've been able to distribute this and sign throughout this expression.\nSo this is an example of the distributive property\nor the distributive law as applied to logic in much the same way\nthat you would distribute a multiplication over the addition\nof something, for example.\nThis works the other way too.\nSo if, for example, I have alpha or beta and gamma,\nI can distribute the or throughout the expression.\nI can say alpha or beta and alpha or gamma.\nSo the distributive law works in that way too.\nAnd it's helpful if I want to take an or and move it into the expression.\nAnd we'll see an example soon of why it is that we might actually\ncare to do something like that.\nAll right, so now we've seen a lot of different inference rules.\nAnd the question now is, how can we use those inference rules to actually try\nand draw some conclusions, to actually try and prove something about entailment,\nproving that given some initial knowledge base,\nwe would like to find some way to prove that a query is true?\nWell, one way to think about it is actually\nto think back to what we talked about last time\nwhen we talked about search problems.\nRecall again that search problems have some sort of initial state.\nThey have actions that you can take from one state to another\nas defined by a transition model that tells you\nhow to get from one state to another.\nWe talked about testing to see if you were at a goal.\nAnd then some path cost function to see how many steps\ndid you have to take or how costly was the solution that you found.\nNow that we have these inference rules that\ntake some set of sentences in propositional logic\nand get us some new set of sentences in propositional logic,\nwe can actually treat those sentences or those sets of sentences\nas states inside of a search problem.\nSo if we want to prove that some query is true,\nprove that some logical theorem is true,\nwe can treat theorem proving as a form of a search problem.\nI can say that we begin in some initial state, where\nthat initial state is the knowledge base that I begin with,\nthe set of all of the sentences that I know to be true.\nWhat actions are available to me?\nWell, the actions are any of the inference rules\nthat I can apply at any given time.\nThe transition model just tells me after I apply the inference rule,\nhere is the new set of all of the knowledge\nthat I have, which will be the old set of knowledge,\nplus some additional inference that I've been able to draw,\nmuch as in the same way we saw what we got when we applied those inference\nrules and got some sort of conclusion.\nThat conclusion gets added to our knowledge base,\nand our transition model will encode that.\nWhat is the goal test?\nWell, our goal test is checking to see if we\nhave proved the statement we're trying to prove,\nif the thing we're trying to prove is inside of our knowledge base.\nAnd the path cost function, the thing we're trying to minimize,\nis maybe the number of inference rules that we needed to use,\nthe number of steps, so to speak, inside of our proof.\nAnd so here we've been able to apply the same types of ideas\nthat we saw last time with search problems\nto something like trying to prove something about knowledge\nby taking our knowledge and framing it in terms\nthat we can understand as a search problem with an initial state,\nwith actions, with a transition model.\nSo this shows a couple of things, one being how versatile search problems\nare, that they can be the same types of algorithms\nthat we use to solve a maze or figure out\nhow to get from point A to point B inside of driving directions,\nfor example, can also be used as a theorem proving\nmethod of taking some sort of starting knowledge base\nand trying to prove something about that knowledge.\nSo this, yet again, is a second way, in addition to model checking,\nto try and prove that certain statements are true.\nBut it turns out there's yet another way that we can try and apply inference.\nAnd we'll talk about this now, which is not the only way, but certainly one\nof the most common, which is known as resolution.\nAnd resolution is based on another inference rule\nthat we'll take a look at now, quite a powerful inference rule that\nwill let us prove anything that can be proven about a knowledge base.\nAnd it's based on this basic idea.\nLet's say I know that either Ron is in the Great Hall\nor Hermione is in the library.\nAnd let's say I also know that Ron is not in the Great Hall.\nBased on those two pieces of information, what can I conclude?\nWell, I could pretty reasonably conclude that Hermione\nmust be in the library.\nHow do I know that?\nWell, it's because these two statements, these two\nwhat we'll call complementary literals, literals that complement each other,\nthey're opposites of each other, seem to conflict with each other.\nThis sentence tells us that either Ron is in the Great Hall\nor Hermione is in the library.\nSo if we know that Ron is not in the Great Hall,\nthat conflicts with this one, which means Hermione must be in the library.\nAnd this we can frame as a more general rule\nknown as the unit resolution rule, a rule that says that if we have p or q\nand we also know not p, well then from that we can reasonably conclude q.\nThat if p or q are true and we know that p is not true,\nthe only possibility is for q to then be true.\nAnd this, it turns out, is quite a powerful inference rule\nin terms of what it can do, in part because we can quickly\nstart to generalize this rule.\nThis q right here doesn't need to just be a single propositional symbol.\nIt could be multiple, all chained together in a single clause,\nas we'll call it.\nSo if I had something like p or q1 or q2 or q3, so on and so forth, up until qn,\nso I had n different other variables, and I have not p,\nwell then what happens when these two complement each other\nis that these two clauses resolve, so to speak,\nto produce a new clause that is just q1 or q2 all the way up to qn.\nAnd in an or, the order of the arguments in the or doesn't actually matter.\nThe p doesn't need to be the first thing.\nIt could have been in the middle.\nBut the idea here is that if I have p in one clause and not\np in the other clause, well then I know that one of these remaining things\nmust be true.\nI've resolved them in order to produce a new clause.\nBut it turns out we can generalize this idea even further, in fact,\nand display even more power that we can have with this resolution rule.\nSo let's take another example.\nLet's say, for instance, that I know the same piece of information\nthat either Ron is in the Great Hall or Hermione is in the library.\nAnd the second piece of information I know\nis that Ron is not in the Great Hall or Harry is sleeping.\nSo it's not just a single piece of information.\nI have two different clauses.\nAnd we'll define clauses more precisely in just a moment.\nWhat do I know here?\nWell again, for any propositional symbol like Ron is in the Great Hall,\nthere are only two possibilities.\nEither Ron is in the Great Hall, in which case, based on resolution,\nwe know that Harry must be sleeping, or Ron is not in the Great Hall,\nin which case we know based on the same rule\nthat Hermione must be in the library.\nBased on those two things in combination,\nI can say based on these two premises that I\ncan conclude that either Hermione is in the library or Harry is sleeping.\nSo again, because these two conflict with each other,\nI know that one of these two must be true.\nAnd you can take a closer look and try and reason through that logic.\nMake sure you convince yourself that you believe this conclusion.\nStated more generally, we can name this resolution rule\nby saying that if we know p or q is true,\nand we also know that not p or r is true,\nwe resolve these two clauses together to get a new clause, q or r,\nthat either q or r must be true.\nAnd again, much as in the last case, q and r\ndon't need to just be single propositional symbols.\nIt could be multiple symbols.\nSo if I had a rule that had p or q1 or q2 or q3, so on and so forth,\nup until qn, where n is just some number.\nAnd likewise, I had not p or r1 or r2, so on and so forth, up until rm,\nwhere m, again, is just some other number.\nI can resolve these two clauses together to get one of these must be true,\nq1 or q2 up until qn or r1 or r2 up until rm.\nAnd this is just a generalization of that same rule we saw before.\nEach of these things here are what we're going to call a clause,\nwhere a clause is formally defined as a disjunction of literals,\nwhere a disjunction means it's a bunch of things that are connected with or.\nDisjunction means things connected with or.\nConjunction, meanwhile, is things connected with and.\nAnd a literal is either a propositional symbol\nor the opposite of a propositional symbol.\nSo it's something like p or q or not p or not q.\nThose are all propositional symbols or not of the propositional symbols.\nAnd we call those literals.\nAnd so a clause is just something like this, p or q or r, for example.\nMeanwhile, what this gives us an ability to do\nis it gives us an ability to turn logic, any logical sentence,\ninto something called conjunctive normal form.\nA conjunctive normal form sentence is a logical sentence\nthat is a conjunction of clauses.\nRecall, again, conjunction means things are connected to one another using and.\nAnd so a conjunction of clauses means it is an and of individual clauses,\neach of which has ors in it.\nSo something like this, a or b or c, and d or not e, and f or g.\nEverything in parentheses is one clause.\nAll of the clauses are connected to each other using an and.\nAnd everything in the clause is separated using an or.\nAnd this is just a standard form that we can translate a logical sentence\ninto that just makes it easy to work with and easy to manipulate.\nAnd it turns out that we can take any sentence in logic\nand turn it into conjunctive normal form just\nby applying some inference rules and transformations to it.\nSo we'll take a look at how we can actually do that.\nSo what is the process for taking a logical formula\nand converting it into conjunctive normal form, otherwise known as c and f?\nWell, the process looks a little something like this.\nWe need to take all of the symbols that are not\npart of conjunctive normal form.\nThe bi-conditionals and the implications and so forth,\nand turn them into something that is more closely like conjunctive normal\nform.\nSo the first step will be to eliminate bi-conditionals,\nthose if and only if double arrows.\nAnd we know how to eliminate bi-conditionals\nbecause we saw there was an inference rule to do just that.\nAny time I have an expression like alpha if and only if beta,\nI can turn that into alpha implies beta and beta implies alpha\nbased on that inference rule we saw before.\nLikewise, in addition to eliminating bi-conditionals,\nI can eliminate implications as well, the if then arrows.\nAnd I can do that using the same inference rule we saw before too,\ntaking alpha implies beta and turning that into not alpha or beta\nbecause that is logically equivalent to this first thing here.\nThen we can move knots inwards because we don't\nwant knots on the outsides of our expressions.\nConjunctive normal form requires that it's just claws and claws\nand claws and claws.\nAny knots need to be immediately next to propositional symbols.\nBut we can move those knots around using De Morgan's laws\nby taking something like not A and B and turn it into not A or not B,\nfor example, using De Morgan's laws to manipulate that.\nAnd after that, all we'll be left with are ands and ors.\nAnd those are easy to deal with.\nWe can use the distributive law to distribute the ors\nso that the ors end up on the inside of the expression, so to speak,\nand the ands end up on the outside.\nSo this is the general pattern for how we'll take a formula\nand convert it into conjunctive normal form.\nAnd let's now take a look at an example of how we would do this\nand explore then why it is that we would want to do something like this.\nHere's how we can do it.\nLet's take this formula, for example.\nP or Q implies R. And I'd like to convert this into conjunctive normal form,\nwhere it's all ands of clauses, and every clause is a disjunctive clause.\nIt's ors together.\nSo what's the first thing I need to do?\nWell, this is an implication.\nSo let me go ahead and remove that implication.\nUsing the implication inference rule, I can turn P or Q into P or Q implies R\ninto not P or Q or R. So that's the first step.\nI've gotten rid of the implication.\nAnd next, I can get rid of the not on the outside of this expression, too.\nI can move the nots inwards so they're closer to the literals themselves\nby using De Morgan's laws.\nAnd De Morgan's law says that not P or Q is equivalent to not P and not Q.\nAgain, here, just applying the inference rules\nthat we've already seen in order to translate these statements.\nAnd now, I have two things that are separated by an or,\nwhere this thing on the inside is an and.\nWhat I'd really like to move the ors so the ors are on the inside,\nbecause conjunctive normal form means I need clause and clause\nand clause and clause.\nAnd so to do that, I can use the distributive law.\nIf I have not P and not Q or R, I can distribute the or R to both of these\nto get not P or R and not Q or R using the distributive law.\nAnd this now here at the bottom is in conjunctive normal form.\nIt is a conjunction and and of disjunctions of clauses\nthat just are separated by ors.\nSo this process can be used by any formula to take a logical sentence\nand turn it into this conjunctive normal form, where\nI have clause and clause and clause and clause and clause and so on.\nSo why is this helpful?\nWhy do we even care about taking all these sentences\nand converting them into this form?\nIt's because once they're in this form where we have these clauses,\nthese clauses are the inputs to the resolution inference rule\nthat we saw a moment ago, that if I have two clauses where there's\nsomething that conflicts or something complementary\nbetween those two clauses, I can resolve them\nto get a new clause, to draw a new conclusion.\nAnd we call this process inference by resolution,\nusing the resolution rule to draw some sort of inference.\nAnd it's based on the same idea, that if I have P or Q, this clause,\nand I have not P or R, that I can resolve these two clauses together\nto get Q or R as the resulting clause, a new piece of information\nthat I didn't have before.\nNow, a couple of key points that are worth noting about this\nbefore we talk about the actual algorithm.\nOne thing is that, let's imagine we have P or Q or S,\nand I also have not P or R or S. The resolution rule\nsays that because this P conflicts with this not P,\nwe would resolve to put everything else together to get Q or S or R or S.\nBut it turns out that this double S is redundant, or S here and or S there.\nIt doesn't change the meaning of the sentence.\nSo in resolution, when we do this resolution process,\nwe'll usually also do a process known as factoring,\nwhere we take any duplicate variables that show up\nand just eliminate them.\nSo Q or S or R or S just becomes Q or R or S. The S only needs to appear once,\nno need to include it multiple times.\nNow, one final question worth considering\nis what happens if I try to resolve P and not P together?\nIf I know that P is true and I know that not P is true,\nwell, resolution says I can merge these clauses together\nand look at everything else.\nWell, in this case, there is nothing else,\nso I'm left with what we might call the empty clause.\nI'm left with nothing.\nAnd the empty clause is always false.\nThe empty clause is equivalent to just being false.\nAnd that's pretty reasonable because it's impossible for both P and not P\nto both hold at the same time.\nP is either true or it's not true, which\nmeans that if P is true, then this must be false.\nAnd if this is true, then this must be false.\nThere is no way for both of these to hold at the same time.\nSo if ever I try and resolve these two, it's a contradiction,\nand I'll end up getting this empty clause where the empty clause I\ncan call equivalent to false.\nAnd this idea that if I resolve these two contradictory terms,\nI get the empty clause, this is the basis for our inference\nby resolution algorithm.\nHere's how we're going to perform inference by resolution\nat a very high level.\nWe want to prove that our knowledge base entails some query alpha,\nthat based on the knowledge we have, we can prove conclusively\nthat alpha is going to be true.\nHow are we going to do that?\nWell, in order to do that, we're going to try\nto prove that if we know the knowledge and not alpha,\nthat that would be a contradiction.\nAnd this is a common technique in computer science\nmore generally, this idea of proving something by contradiction.\nIf I want to prove that something is true,\nI can do so by first assuming that it is false\nand showing that it would be contradictory,\nshowing that it leads to some contradiction.\nAnd if the thing I'm trying to prove, if when I assume it's false,\nleads to a contradiction, then it must be true.\nAnd that's the logical approach or the idea behind a proof by contradiction.\nAnd that's what we're going to do here.\nWe want to prove that this query alpha is true.\nSo we're going to assume that it's not true.\nWe're going to assume not alpha.\nAnd we're going to try and prove that it's a contradiction.\nIf we do get a contradiction, well, then we\nknow that our knowledge entails the query alpha.\nIf we don't get a contradiction, there is no entailment.\nThis is this idea of a proof by contradiction\nof assuming the opposite of what you're trying to prove.\nAnd if you can demonstrate that that's a contradiction,\nthen what you're proving must be true.\nBut more formally, how do we actually do this?\nHow do we check that knowledge base and not alpha\nis going to lead to a contradiction?\nWell, here is where resolution comes into play.\nTo determine if our knowledge base entails some query alpha,\nwe're going to convert knowledge base and not alpha\nto conjunctive normal form, that form where\nwe have a whole bunch of clauses that are all anded together.\nAnd when we have these individual clauses,\nnow we can keep checking to see if we can use resolution\nto produce a new clause.\nWe can take any pair of clauses and check,\nis there some literal that is the opposite of each other\nor complementary to each other in both of them?\nFor example, I have a p in one clause and a not p in another clause.\nOr an r in one clause and a not r in another clause.\nIf ever I have that situation where once I\nconvert to conjunctive normal form and I have a whole bunch of clauses,\nI see two clauses that I can resolve to produce a new clause, then I'll do so.\nThis process occurs in a loop.\nI'm going to keep checking to see if I can use resolution\nto produce a new clause and keep using those new clauses\nto try to generate more new clauses after that.\nNow, it just so may happen that eventually we\nmay produce the empty clause, the clause we were talking about before.\nIf I resolve p and not p together, that produces the empty clause\nand the empty clause we know to be false.\nBecause we know that there's no way for both p and not p\nto both simultaneously be true.\nSo if ever we produce the empty clause, then we have a contradiction.\nAnd if we have a contradiction, that's exactly what we were trying\nto do in a fruit by contradiction.\nIf we have a contradiction, then we know that our knowledge base\nmust entail this query alpha.\nAnd we know that alpha must be true.\nAnd it turns out, and we won't go into the proof here,\nbut you can show that otherwise, if you don't produce the empty clause,\nthen there is no entailment.\nIf we run into a situation where there are no more new clauses to add,\nwe've done all the resolution that we can do,\nand yet we still haven't produced the empty clause,\nthen there is no entailment in this case.\nAnd this now is the resolution algorithm.\nAnd it's very abstract looking, especially this idea of like,\nwhat does it even mean to have the empty clause?\nSo let's take a look at an example, actually\ntry and prove some entailment by using this inference by resolution process.\nSo here's our question.\nWe have this knowledge base.\nHere is the knowledge that we know, A or B, and not B or C, and not C.\nAnd we want to know if all of this entails A.\nSo this is our knowledge base here, this whole log thing.\nAnd our query alpha is just this propositional symbol, A.\nSo what do we do?\nWell, first, we want to prove by contradiction.\nSo we want to first assume that A is false,\nand see if that leads to some sort of contradiction.\nSo here is what we're going to start with, A or B, and not B or C, and not C.\nThis is our knowledge base.\nAnd we're going to assume not A. We're going\nto assume that the thing we're trying to prove is, in fact, false.\nAnd so this is now in conjunctive normal form,\nand I have four different clauses.\nI have A or B. I have not B or C. I have not C, and I have not A.\nAnd now, I can begin to just pick two clauses that I can resolve,\nand apply the resolution rule to them.\nAnd so looking at these four clauses, I see, all right, these two clauses\nare ones I can resolve.\nI can resolve them because there are complementary literals\nthat show up in them.\nThere's a C here, and a not C here.\nSo just looking at these two clauses, if I know that not B or C is true,\nand I know that C is not true, well, then I\ncan resolve these two clauses to say, all right, not B, that must be true.\nI can generate this new clause as a new piece of information\nthat I now know to be true.\nAnd all right, now I can repeat this process, do the process again.\nCan I use resolution again to get some new conclusion?\nWell, it turns out I can.\nI can use that new clause I just generated, along with this one here.\nThere are complementary literals.\nThis B is complementary to, or conflicts with, this not B over here.\nAnd so if I know that A or B is true, and I know that B is not true,\nwell, then the only remaining possibility is that A must be true.\nSo now we have A. That is a new clause that I've been able to generate.\nAnd now, I can do this one more time.\nI'm looking for two clauses that can be resolved,\nand you might programmatically do this by just looping\nover all possible pairs of clauses and checking\nfor complementary literals in each.\nAnd here, I can say, all right, I found two clauses, not A and A,\nthat conflict with each other.\nAnd when I resolve these two together, well,\nthis is the same as when we were resolving P and not P from before.\nWhen I resolve these two clauses together, I get rid of the As,\nand I'm left with the empty clause.\nAnd the empty clause we know to be false, which means we have a contradiction,\nwhich means we can safely say that this whole knowledge base does entail A.\nThat if this sentence is true, that we know that A for sure is also true.\nSo this now, using inference by resolution,\nis an entirely different way to take some statement\nand try and prove that it is, in fact, true.\nInstead of enumerating all of the possible worlds\nthat we might be in in order to try to figure out in which cases\nis the knowledge base true and in which cases are query true,\ninstead we use this resolution algorithm to say,\nlet's keep trying to figure out what conclusions we can draw\nand see if we reach a contradiction.\nAnd if we reach a contradiction, then that\ntells us something about whether our knowledge actually\nentails the query or not.\nAnd it turns out there are many different algorithms that\ncan be used for inference.\nWhat we've just looked at here are just a couple of them.\nAnd in fact, all of this is just based on one particular type of logic.\nIt's based on propositional logic, where we have these individual symbols\nand we connect them using and and or and not and implies and by conditionals.\nBut propositional logic is not the only kind of logic that exists.\nAnd in fact, we see that there are limitations\nthat exist in propositional logic, especially\nas we saw in examples like with the mastermind example\nor with the example with the logic puzzle where\nwe had different Hogwarts house people that belong to different houses\nand we were trying to figure out who belonged to which houses.\nThere were a lot of different propositional symbols that we needed\nin order to represent some fairly basic ideas.\nSo now is the final topic that we'll take a look at just before we end class\ntoday is one final type of logic different from propositional logic\nknown as first order logic, which is a little bit more powerful than\npropositional logic and is going to make it easier for us\nto express certain types of ideas.\nIn propositional logic, if we think back to that puzzle\nwith the people in the Hogwarts houses, we had a whole bunch of symbols.\nAnd every symbol could only be true or false.\nWe had a symbol for Minerva Gryffindor, which was either true of Minerva\nwithin Gryffindor and false otherwise, and likewise\nfor Minerva Hufflepuff and Minerva Ravenclaw and Minerva Slytherin\nand so forth.\nBut this was starting to get quite redundant.\nWe wanted some way to be able to express that there\nis a relationship between these propositional symbols,\nthat Minerva shows up in all of them.\nAnd also, I would have liked to have not have had so many different symbols\nto represent what really was a fairly straightforward problem.\nSo first order logic will give us a different way\nof trying to deal with this idea by giving us two different types of symbols.\nWe're going to have constant symbols that are going to represent objects\nlike people or houses.\nAnd then predicate symbols, which you can think of as relations or functions\nthat take an input and evaluate them to true or false, for example,\nthat tell us whether or not some property of some constant\nor some pair of constants or multiple constants actually holds.\nSo we'll see an example of that in just a moment.\nFor now, in this same problem, our constant symbols\nmight be objects, things like people or houses.\nSo Minerva, Pomona, Horace, Gilderoy, those are all constant symbols,\nas are my four houses, Gryffindor, Hufflepuff, Ravenclaw, and Slytherin.\nPredicates, meanwhile, these predicate symbols\nare going to be properties that might hold true or false\nof these individual constants.\nSo person might hold true of Minerva, but it\nwould be false for Gryffindor because Gryffindor is not a person.\nAnd house is going to hold true for Ravenclaw,\nbut it's not going to hold true for Horace, for example,\nbecause Horace is a person.\nAnd belongs to, meanwhile, is going to be some relation that\nis going to relate people to their houses.\nAnd it's going to only tell me when someone belongs to a house or does not.\nSo let's take a look at some examples of what a sentence in first order logic\nmight actually look like.\nA sentence might look like something like this.\nPerson Minerva, with Minerva in parentheses, and person being a predicate\nsymbol, Minerva being a constant symbol.\nThis sentence in first order logic effectively\nmeans Minerva is a person, or the person property applies to the Minerva object.\nSo if I want to say something like Minerva is a person,\nhere is how I express that idea using first order logic.\nMeanwhile, I can say something like, house Gryffindor,\nto likewise express the idea that Gryffindor is a house.\nI can do that this way.\nAnd all of the same logical connectives that we\nsaw in propositional logic, those are going to work here too.\nAnd or implication by conditional not.\nIn fact, I can use not to say something like, not house Minerva.\nAnd this sentence in first order logic means something like,\nMinerva is not a house.\nIt is not true that the house property applies to Minerva.\nMeanwhile, in addition to some of these predicate symbols\nthat just take a single argument, some of our predicate symbols\nare going to express binary relations, relations\nbetween two of its arguments.\nSo I could say something like, belongs to, and then two inputs, Minerva\nand Gryffindor, to express the idea that Minerva belongs to Gryffindor.\nAnd so now here's the key difference, or one of the key differences,\nbetween this and propositional logic.\nIn propositional logic, I needed one symbol for Minerva Gryffindor,\nand one symbol for Minerva Hufflepuff, and one\nsymbol for all the other people's Gryffindor and Hufflepuff variables.\nIn this case, I just need one symbol for each of my people,\nand one symbol for each of my houses.\nAnd then I can express as a predicate something like, belongs to,\nand say, belongs to Minerva Gryffindor, to express the idea that Minerva\nbelongs to Gryffindor House.\nSo already we can see that first order logic is quite expressive in being\nable to express these sorts of sentences using the existing constant symbols\nand predicates that already exist, while minimizing the number of new symbols\nthat I need to create.\nI can just use eight symbols for people for houses,\ninstead of 16 symbols for every possible combination of each.\nBut first order logic gives us a couple of additional features\nthat we can use to express even more complex ideas.\nAnd these more additional features are generally known as quantifiers.\nAnd there are two main quantifiers in first order logic,\nthe first of which is universal quantification.\nUniversal quantification lets me express an idea\nlike something is going to be true for all values of a variable.\nLike for all values of x, some statement is going to hold true.\nSo what might a sentence in universal quantification look like?\nWell, we're going to use this upside down a to mean for all.\nSo upside down ax means for all values of x, where x is any object,\nthis is going to hold true.\nBelongs to x Gryffindor implies not belongs to x Hufflepuff.\nSo let's try and parse this out.\nThis means that for all values of x, if this holds true,\nif x belongs to Gryffindor, then this does not hold true.\nx does not belong to Hufflepuff.\nSo translated into English, this sentence\nis saying something like for all objects x, if x belongs to Gryffindor,\nthen x does not belong to Hufflepuff, for example.\nOr a phrase even more simply, anyone in Gryffindor\nis not in Hufflepuff, simplified way of saying the same thing.\nSo this universal quantification lets us express\nan idea like something is going to hold true for all values\nof a particular variable.\nIn addition to universal quantification though,\nwe also have existential quantification.\nWhereas universal quantification said that something\nis going to be true for all values of a variable,\nexistential quantification says that some expression is going\nto be true for some value of a variable, at least one value of the variable.\nSo let's take a look at a sample sentence using existential quantification.\nOne such sentence looks like this.\nThere exists an x.\nThis backwards e stands for exists.\nAnd here we're saying there exists an x such that house x and belongs\nto Minerva x.\nIn other words, there exists some object x where x is a house\nand Minerva belongs to x.\nOr phrased a little more succinctly in English,\nI'm here just saying Minerva belongs to a house.\nThere's some object that is a house and Minerva belongs to a house.\nAnd combining this universal and existential quantification,\nwe can create far more sophisticated logical statements\nthan we were able to just using propositional logic.\nI could combine these to say something like this.\nFor all x, person x implies there exists\na y such that house y and belongs to xy.\nAll right.\nSo a lot of stuff going on there, a lot of symbols.\nLet's try and parse it out and just understand what it's saying.\nHere we're saying that for all values of x, if x is a person,\nthen this is true.\nSo in other words, I'm saying for all people,\nand we call that person x, this statement is going to be true.\nWhat statement is true of all people?\nWell, there exists a y that is a house, so there exists some house,\nand x belongs to y.\nIn other words, I'm saying that for all people out there,\nthere exists some house such that x, the person, belongs to y, the house.\nThis is phrased more succinctly.\nI'm saying that every person belongs to a house, that for all x,\nif x is a person, then there exists a house that x belongs to.\nAnd so we can now express a lot more powerful ideas using this idea now\nof first order logic.\nAnd it turns out there are many other kinds of logic out there.\nThere's second order logic and other higher order logic,\neach of which allows us to express more and more complex ideas.\nBut all of it, in this case, is really in pursuit\nof the same goal, which is the representation of knowledge.\nWe want our AI agents to be able to know information,\nto represent that information, whether that's\nusing propositional logic or first order logic or some other logic,\nand then be able to reason based on that, to be able to draw conclusions,\nmake inferences, figure out whether there's\nsome sort of entailment relationship, as by using some sort of inference\nalgorithm, something like inference by resolution or model checking\nor any number of these other algorithms that we can use in order\nto take information that we know and translate it to additional conclusions.\nSo all of this has helped us to create AI that\nis able to represent information about what it knows and what it doesn't know.\nNext time, though, we'll take a look at how we can make our AI even more\npowerful by not just encoding information that we know for sure to be true\nand not to be true, but also to take a look at uncertainty,\nto look at what happens if AI thinks that something might be probable\nor maybe not very probable or somewhere in between those two extremes,\nall in the pursuit of trying to build our intelligent systems\nto be even more intelligent.\nWe'll see you next time.\nThank you.\nAll right, welcome back, everyone, to an introduction\nto artificial intelligence with Python.\nAnd last time, we took a look at how it is that AI inside of our computers\ncan represent knowledge.\nWe represented that knowledge in the form of logical sentences\nin a variety of different logical languages.\nAnd the idea was we wanted our AI to be able to represent knowledge\nor information and somehow use those pieces of information\nto be able to derive new pieces of information by inference,\nto be able to take some information and deduce\nsome additional conclusions based on the information\nthat it already knew for sure.\nBut in reality, when we think about computers and we think about AI,\nvery rarely are our machines going to be able to know things for sure.\nOftentimes, there's going to be some amount of uncertainty\nin the information that our AIs or our computers are dealing with,\nwhere it might believe something with some probability,\nas we'll soon discuss what probability is all about and what it means,\nbut not entirely for certain.\nAnd we want to use the information that it has some knowledge about,\neven if it doesn't have perfect knowledge,\nto still be able to make inferences, still be able to draw conclusions.\nSo you might imagine, for example, in the context of a robot that\nhas some sensors and is exploring some environment,\nit might not know exactly where it is or exactly what's around it,\nbut it does have access to some data that can allow it\nto draw inferences with some probability.\nThere's some likelihood that one thing is true or another.\nOr you can imagine in context where there is a little bit more\nrandomness and uncertainty, something like predicting the weather,\nwhere you might not be able to know for sure what tomorrow's weather is\nwith 100% certainty, but you can probably infer with some probability\nwhat tomorrow's weather is going to be based on maybe today's weather\nand yesterday's weather and other data that you might have access\nto as well.\nAnd so oftentimes, we can distill this in terms of just possible events\nthat might happen and what the likelihood of those events are.\nThis comes a lot in games, for example, where there is an element of chance\ninside of those games.\nSo you imagine rolling a dice.\nYou're not sure exactly what the die roll is going to be,\nbut you know it's going to be one of these possibilities from 1 to 6,\nfor example.\nAnd so here now, we introduce the idea of probability theory.\nAnd what we'll take a look at today is beginning\nby looking at the mathematical foundations of probability theory,\ngetting an understanding for some of the key concepts within probability,\nand then diving into how we can use probability and the ideas\nthat we look at mathematically to represent some ideas in terms of models\nthat we can put into our computers in order to program an AI that\nis able to use information about probability to draw inferences,\nto make some judgments about the world with some probability\nor likelihood of being true.\nSo probability ultimately boils down to this idea\nthat there are possible worlds that we're here representing\nusing this little Greek letter omega.\nAnd the idea of a possible world is that when I roll a die,\nthere are six possible worlds that could result from it.\nI could roll a 1, or a 2, or a 3, or a 4, or a 5, or a 6.\nAnd each of those are a possible world.\nAnd each of those possible worlds has some probability of being true,\nthe probability that I do roll a 1, or a 2, or a 3, or something else.\nAnd we represent that probability like this, using the capital letter P.\nAnd then in parentheses, what it is that we want the probability of.\nSo this right here would be the probability of some possible world\nas represented by the little letter omega.\nNow, there are a couple of basic axioms of probability\nthat become relevant as we consider how we deal with probability\nand how we think about it.\nFirst and foremost, every probability value\nmust range between 0 and 1 inclusive.\nSo the smallest value any probability can have is the number 0,\nwhich is an impossible event.\nSomething like I roll a die, and the die is a 7 is the roll that I get.\nIf the die only has numbers 1 through 6, the event that I roll a 7\nis impossible, so it would have probability 0.\nAnd on the other end of the spectrum, probability\ncan range all the way up to the positive number 1,\nmeaning an event is certain to happen, that I roll a die\nand the number is less than 10, for example.\nThat is an event that is guaranteed to happen if the only sides on my die\nare 1 through 6, for instance.\nAnd then they can range through any real number in between these two values.\nWhere, generally speaking, a higher value for the probability\nmeans an event is more likely to take place,\nand a lower value for the probability means the event is less\nlikely to take place.\nAnd the other key rule for probability looks a little bit like this.\nThis sigma notation, if you haven't seen it before,\nrefers to summation, the idea that we're going\nto be adding up a whole sequence of values.\nAnd this sigma notation is going to come up a couple of times today,\nbecause as we deal with probability, oftentimes we're\nadding up a whole bunch of individual values or individual probabilities\nto get some other value.\nSo we'll see this come up a couple of times.\nBut what this notation means is that if I sum up\nall of the possible worlds omega that are in big omega, which\nrepresents the set of all the possible worlds,\nmeaning I take for all of the worlds in the set of possible worlds\nand add up all of their probabilities, what I ultimately get is the number 1.\nSo if I take all the possible worlds, add up\nwhat each of their probabilities is, I should get the number 1 at the end,\nmeaning all probabilities just need to sum to 1.\nSo for example, if I take dice, for example,\nand if you imagine I have a fair die with numbers 1 through 6\nand I roll the die, each one of these rolls\nhas an equal probability of taking place.\nAnd the probability is 1 over 6, for example.\nSo each of these probabilities is between 0 and 1, 0 meaning impossible\nand 1 meaning for certain.\nAnd if you add up all of these probabilities\nfor all of the possible worlds, you get the number 1.\nAnd we can represent any one of those probabilities like this.\nThe probability that we roll the number 2, for example,\nis just 1 over 6.\nEvery six times we roll the die, we'd expect that one time, for instance,\nthe die might come up as a 2.\nIts probability is not certain, but it's a little more than nothing,\nfor instance.\nAnd so this is all fairly straightforward for just a single die.\nBut things get more interesting as our models of the world\nget a little bit more complex.\nLet's imagine now that we're not just dealing with a single die,\nbut we have two dice, for example.\nI have a red die here and a blue die there,\nand I care not just about what the individual roll is,\nbut I care about the sum of the two rolls.\nIn this case, the sum of the two rolls is the number 3.\nHow do I begin to now reason about what does the probability look like\nif instead of having one die, I now have two dice?\nWell, what we might imagine is that we could first consider\nwhat are all of the possible worlds.\nAnd in this case, all of the possible worlds\nare just every combination of the red and blue die that I could come up with.\nFor the red die, it could be a 1 or a 2 or a 3 or a 4 or a 5 or a 6.\nAnd for each of those possibilities, the blue die, likewise,\ncould also be either 1 or 2 or 3 or 4 or 5 or 6.\nAnd it just so happens that in this particular case,\neach of these possible combinations is equally likely.\nEqually likely are all of these various different possible worlds.\nThat's not always going to be the case.\nIf you imagine more complex models that we could try to build and things\nthat we could try to represent in the real world,\nit's probably not going to be the case that every single possible world is\nalways equally likely.\nBut in the case of fair dice, where in any given die roll,\nany one number has just as good a chance of coming up as any other number,\nwe can consider all of these possible worlds to be equally likely.\nBut even though all of the possible worlds are equally likely,\nthat doesn't necessarily mean that their sums are equally likely.\nSo if we consider what the sum is of all of these two, so 1 plus 1,\nthat's a 2.\n2 plus 1 is a 3.\nAnd consider for each of these possible pairs of numbers\nwhat their sum ultimately is, we can notice that there are some patterns\nhere, where it's not entirely the case that every number comes up\nequally likely.\nIf you consider 7, for example, what's the probability that when I roll two\ndice, their sum is 7?\nThere are several ways this can happen.\nThere are six possible worlds where the sum is 7.\nIt could be a 1 and a 6, or a 2 and a 5, or a 3 and a 4, a 4 and a 3,\nand so forth.\nBut if you instead consider what's the probability that I roll two dice,\nand the sum of those two die rolls is 12, for example,\nwe're looking at this diagram, there's only one possible world in which that\ncan happen.\nAnd that's the possible world where both the red die and the blue die\nboth come up as sixes to give us a sum total of 12.\nSo based on just taking a look at this diagram,\nwe see that some of these probabilities are likely different.\nThe probability that the sum is a 7 must be greater than the probability\nthat the sum is a 12.\nAnd we can represent that even more formally by saying, OK, the probability\nthat we sum to 12 is 1 out of 36.\nOut of the 36 equally likely possible worlds,\n6 squared because we have six options for the red die and six\noptions for the blue die, out of those 36 options,\nonly one of them sums to 12.\nWhereas on the other hand, the probability\nthat if we take two dice rolls and they sum up to the number 7, well,\nout of those 36 possible worlds, there were six worlds where the sum was 7.\nAnd so we get 6 over 36, which we can simplify as a fraction to just 1\nover 6.\nSo here now, we're able to represent these different ideas\nof probability, representing some events that might be more likely\nand then other events that are less likely as well.\nAnd these sorts of judgments, where we're figuring out just in the abstract\nwhat is the probability that this thing takes place,\nare generally known as unconditional probabilities.\nSome degree of belief we have in some proposition,\nsome fact about the world, in the absence of any other evidence.\nWithout knowing any additional information, if I roll a die,\nwhat's the chance it comes up as a 2?\nOr if I roll two dice, what's the chance that the sum of those two die\nrolls is a 7?\nBut usually when we're thinking about probability, especially when\nwe're thinking about training in AI to intelligently\nbe able to know something about the world\nand make predictions based on that information,\nit's not unconditional probability that our AI is dealing with,\nbut rather conditional probability, probability\nwhere rather than having no original knowledge,\nwe have some initial knowledge about the world\nand how the world actually works.\nSo conditional probability is the degree of belief in a proposition\ngiven some evidence that has already been revealed to us.\nSo what does this look like?\nWell, it looks like this in terms of notation.\nWe're going to represent conditional probability as probability of A\nand then this vertical bar and then B. And the way to read this\nis the thing on the left-hand side of the vertical bar\nis what we want the probability of.\nHere now, I want the probability that A is true,\nthat it is the real world, that it is the event that actually does take place.\nAnd then on the right side of the vertical bar is our evidence,\nthe information that we already know for certain about the world.\nFor example, that B is true.\nSo the way to read this entire expression\nis what is the probability of A given B, the probability that A is true,\ngiven that we already know that B is true.\nAnd this type of judgment, conditional probability,\nthe probability of one thing given some other fact,\ncomes up quite a lot when we think about the types of calculations\nwe might want our AI to be able to do.\nFor example, we might care about the probability of rain today\ngiven that we know that it rained yesterday.\nWe could think about the probability of rain today just in the abstract.\nWhat is the chance that today it rains?\nBut usually, we have some additional evidence.\nI know for certain that it rained yesterday.\nAnd so I would like to calculate the probability that it rains today\ngiven that I know that it rained yesterday.\nOr you might imagine that I want to know the probability that my optimal\nroute to my destination changes given the current traffic condition.\nSo whether or not traffic conditions change,\nthat might change the probability that this route is actually the optimal route.\nOr you might imagine in a medical context,\nI want to know the probability that a patient has a particular disease given\nsome results of some tests that have been performed on that patient.\nAnd I have some evidence, the results of that test,\nand I would like to know the probability that a patient has\na particular disease.\nSo this notion of conditional probability comes up everywhere.\nSo we begin to think about what we would like to reason about,\nbut being able to reason a little more intelligently\nby taking into account evidence that we already have.\nWe're more able to get an accurate result for what is the likelihood\nthat someone has this disease if we know this evidence, the results of the test,\nas opposed to if we were just calculating the unconditional probability of saying,\nwhat is the probability they have the disease without any evidence\nto try and back up our result one way or the other.\nSo now that we've got this idea of what conditional probability is,\nthe next question we have to ask is, all right,\nhow do we calculate conditional probability?\nHow do we figure out mathematically, if I have an expression like this,\nhow do I get a number from that?\nWhat does conditional probability actually mean?\nWell, the formula for conditional probability\nlooks a little something like this.\nThe probability of a given b, the probability that a is true,\ngiven that we know that b is true, is equal to this fraction,\nthe probability that a and b are true, divided by just the probability\nthat b is true.\nAnd the way to intuitively try to think about this\nis that if I want to know the probability that a is true, given\nthat b is true, well, I want to consider all the ways they could both be true out\nof the only worlds that I care about are the worlds where b is already true.\nI can sort of ignore all the cases where b isn't true,\nbecause those aren't relevant to my ultimate computation.\nThey're not relevant to what it is that I want to get information about.\nSo let's take a look at an example.\nLet's go back to that example of rolling two dice and the idea\nthat those two dice might sum up to the number 12.\nWe discussed earlier that the unconditional probability\nthat if I roll two dice and they sum to 12 is 1 out of 36,\nbecause out of the 36 possible worlds that I might care about,\nin only one of them is the sum of those two dice 12.\nIt's only when red is 6 and blue is also 6.\nBut let's say now that I have some additional information.\nI now want to know what is the probability that the two dice sum to 12,\ngiven that I know that the red die was a 6.\nSo I already have some evidence.\nI already know the red die is a 6.\nI don't know what the blue die is.\nThat information isn't given to me in this expression.\nBut given the fact that I know that the red die rolled a 6,\nwhat is the probability that we sum to 12?\nAnd so we can begin to do the math using that expression from before.\nHere, again, are all of the possibilities,\nall of the possible combinations of red die being 1 through 6\nand blue die being 1 through 6.\nAnd I might consider first, all right, what\nis the probability of my evidence, my B variable, where I want to know,\nwhat is the probability that the red die is a 6?\nWell, the probability that the red die is a 6 is just 1 out of 6.\nSo these 1 out of 6 options are really the only worlds\nthat I care about here now.\nAll the rest of them are irrelevant to my calculation,\nbecause I already have this evidence that the red die was a 6,\nso I don't need to care about all of the other possibilities that could result.\nSo now, in addition to the fact that the red die rolled as a 6\nand the probability of that, the other piece of information\nI need to know in order to calculate this conditional probability\nis the probability that both of my variables, A and B, are true.\nThe probability that both the red die is a 6, and they all sum to 12.\nSo what is the probability that both of these things happen?\nWell, it only happens in one possible case in 1 out of these 36 cases,\nand it's the case where both the red and the blue die are equal to 6.\nThis is a piece of information that we already knew.\nAnd so this probability is equal to 1 over 36.\nAnd so to get the conditional probability that the sum is 12,\ngiven that I know that the red dice is equal to 6,\nwell, I just divide these two values together,\nand 1 over 36 divided by 1 over 6 gives us this probability of 1 over 6.\nGiven that I know that the red die rolled a value of 6,\nthe probability that the sum of the two dice is 12 is also 1 over 6.\nAnd that probably makes intuitive sense to you, too,\nbecause if the red die is a 6, the only way for me to get to a 12\nis if the blue die also rolls a 6, and we\nknow that the probability of the blue die rolling a 6 is 1 over 6.\nSo in this case, the conditional probability seems fairly straightforward.\nBut this idea of calculating a conditional probability\nby looking at the probability that both of these events take place\nis an idea that's going to come up again and again.\nThis is the definition now of conditional probability.\nAnd we're going to use that definition as we\nthink about probability more generally to be\nable to draw conclusions about the world.\nThis, again, is that formula.\nThe probability of A given B is equal to the probability\nthat A and B take place divided by the probability of B.\nAnd you'll see this formula sometimes written in a couple of different ways.\nYou could imagine algebraically multiplying both sides of this equation\nby probability of B to get rid of the fraction,\nand you'll get an expression like this.\nThe probability of A and B, which is this expression over here,\nis just the probability of B times the probability of A given B.\nOr you could represent this equivalently since A and B in this expression\nare interchangeable.\nA and B is the same thing as B and A. You could imagine also\nrepresenting the probability of A and B as the probability of A\ntimes the probability of B given A, just switching all of the A's and B's.\nThese three are all equivalent ways of trying\nto represent what joint probability means.\nAnd so you'll sometimes see all of these equations,\nand they might be useful to you as you begin to reason about probability\nand to think about what values might be taking place in the real world.\nNow, sometimes when we deal with probability,\nwe don't just care about a Boolean event like did this happen\nor did this not happen.\nSometimes we might want the ability to represent variable values\nin a probability space where some variable might take\non multiple different possible values.\nAnd in probability, we call a variable in probability theory\na random variable.\nA random variable in probability is just some variable in probability theory\nthat has some domain of values that it can take on.\nSo what do I mean by this?\nWell, what I mean is I might have a random variable that is just\ncalled roll, for example, that has six possible values.\nRoll is my variable, and the possible values, the domain of values\nthat it can take on are 1, 2, 3, 4, 5, and 6.\nAnd I might like to know the probability of each.\nIn this case, they happen to all be the same.\nBut in other random variables, that might not be the case.\nFor example, I might have a random variable\nto represent the weather, for example, where the domain of values\nit could take on are things like sun or cloudy or rainy or windy or snowy.\nAnd each of those might have a different probability.\nAnd I care about knowing what is the probability that the weather equals\nsun or that the weather equals clouds, for instance.\nAnd I might like to do some mathematical calculations\nbased on that information.\nOther random variables might be something like traffic.\nWhat are the odds that there is no traffic or light traffic or heavy traffic?\nTraffic, in this case, is my random variable.\nAnd the values that that random variable can take on are here.\nIt's either none or light or heavy.\nAnd I, the person doing these calculations,\nI, the person encoding these random variables into my computer,\nneed to make the decision as to what these possible values actually are.\nYou might imagine, for example, for a flight.\nIf I care about whether or not I make it or do a flight on time,\nmy flight has a couple of possible values that it could take on.\nMy flight could be on time.\nMy flight could be delayed.\nMy flight could be canceled.\nSo flight, in this case, is my random variable.\nAnd these are the values that it can take on.\nAnd often, I want to know something about the probability\nthat my random variable takes on each of those possible values.\nAnd this is what we then call a probability distribution.\nA probability distribution takes a random variable\nand gives me the probability for each of the possible values in its domain.\nSo in the case of this flight, for example, my probability distribution\nmight look something like this.\nMy probability distribution says the probability\nthat the random variable flight is equal to the value on time is 0.6.\nOr otherwise, put into more English human-friendly terms,\nthe likelihood that my flight is on time is 60%, for example.\nAnd in this case, the probability that my flight is delayed is 30%.\nThe probability that my flight is canceled is 10% or 0.1.\nAnd if you sum up all of these possible values,\nthe sum is going to be 1, right?\nIf you take all of the possible worlds, here\nare my three possible worlds for the value of the random variable flight,\nadd them all up together, the result needs\nto be the number 1 per that axiom of probability theory\nthat we've discussed before.\nSo this now is one way of representing this probability\ndistribution for the random variable flight.\nSometimes you'll see it represented a little bit more concisely\nthat this is pretty verbose for really just trying\nto express three possible values.\nAnd so often, you'll instead see the same notation\nrepresenting using a vector.\nAnd all a vector is is a sequence of values.\nAs opposed to just a single value, I might have multiple values.\nAnd so I could extend instead, represent this idea this way.\nBold p, so a larger p, generally meaning the probability distribution\nof this variable flight is equal to this vector represented in angle brackets.\nThe probability distribution is 0.6, 0.3, and 0.1.\nAnd I would just have to know that this probability distribution is\nin order of on time or delayed and canceled\nto know how to interpret this vector.\nTo mean the first value in the vector is the probability\nthat my flight is on time.\nThe second value in the vector is the probability that my flight is delayed.\nAnd the third value in the vector is the probability\nthat my flight is canceled.\nAnd so this is just an alternate way of representing this idea,\na little more verbosely.\nBut oftentimes, you'll see us just talk about a probability distribution\nover a random variable.\nAnd whenever we talk about that, what we're really doing\nis trying to figure out the probabilities of each of the possible values\nthat that random variable can take on.\nBut this notation is just a little bit more succinct,\neven though it can sometimes be a little confusing,\ndepending on the context in which you see it.\nSo we'll start to look at examples where we use this sort of notation\nto describe probability and to describe events that might take place.\nA couple of other important ideas to know with regards to probability theory.\nOne is this idea of independence.\nAnd independence refers to the idea that the knowledge of one event\ndoesn't influence the probability of another event.\nSo for example, in the context of my two dice rolls,\nwhere I had the red die and the blue die, the probability\nthat I roll the red die and the blue die,\nthose two events, red die and blue die, are independent.\nKnowing the result of the red die doesn't change\nthe probabilities for the blue die.\nIt doesn't give me any additional information\nabout what the value of the blue die is ultimately going to be.\nBut that's not always going to be the case.\nYou might imagine that in the case of weather, something\nlike clouds and rain, those are probably not independent.\nBut if it is cloudy, that might increase the probability that later\nin the day it's going to rain.\nSo some information informs some other event or some other random variable.\nSo independence refers to the idea that one event doesn't influence the other.\nAnd if they're not independent, then there might be some relationship.\nSo mathematically, formally, what does independence actually mean?\nWell, recall this formula from before, that the probability of A and B\nis the probability of A times the probability of B given A.\nAnd the more intuitive way to think about this\nis that to know how likely it is that A and B happen,\nwell, let's first figure out the likelihood that A happens.\nAnd then given that we know that A happens,\nlet's figure out the likelihood that B happens\nand multiply those two things together.\nBut if A and B were independent, meaning knowing A\ndoesn't change anything about the likelihood that B is true,\nwell, then the probability of B given A, meaning the probability that B is true,\ngiven that I know A is true, well, that I know A is true\nshouldn't really make a difference if these two things are independent,\nthat A shouldn't influence B at all.\nSo the probability of B given A is really just the probability of B.\nIf it is true that A and B are independent.\nAnd so this right here is one example of a definition\nfor what it means for A and B to be independent.\nThe probability of A and B is just the probability\nof A times the probability of B. Anytime you find two events A and B\nwhere this relationship holds, then you can say that A and B are independent.\nSo an example of that might be the dice that we were taking a look at before.\nHere, if I wanted the probability of red being a 6 and blue being a 6,\nwell, that's just the probability that red is a 6 multiplied\nby the probability that blue is a 6.\nIt's both equal to 1 over 36.\nSo I can say that these two events are independent.\nWhat wouldn't be independent, for example, would be an example.\nSo this, for example, has a probability of 1 over 36,\nas we talked about before.\nBut what wouldn't be independent would be a case like this,\nthe probability that the red die rolls a 6 and the red die rolls a 4.\nIf you just naively took, OK, red die 6, red die 4,\nwell, if I'm only rolling the die once, you\nmight imagine the naive approach is to say, well, each of these\nhas a probability of 1 over 6.\nSo multiply them together, and the probability is 1 over 36.\nBut of course, if you're only rolling the red die once,\nthere's no way you could get two different values for the red die.\nIt couldn't both be a 6 and a 4.\nSo the probability should be 0.\nBut if you were to multiply probability of red 6 times\nprobability of red 4, well, that would equal 1 over 36.\nBut of course, that's not true.\nBecause we know that there is no way, probability 0,\nthat when we roll the red die once, we get both a 6 and a 4,\nbecause only one of those possibilities can actually be the result.\nAnd so we can say that the event that red roll is 6\nand the event that red roll is 4, those two events are not independent.\nIf I know that the red roll is a 6, I know that the red roll cannot possibly\nbe a 4, so these things are not independent.\nAnd instead, if I wanted to calculate the probability,\nI would need to use this conditional probability\nas the regular definition of the probability of two events taking place.\nAnd the probability of this now, well, the probability\nof the red roll being a 6, that's 1 over 6.\nBut what's the probability that the roll is a 4 given that the roll is a 6?\nWell, this is just 0, because there's no way for the red roll to be a 4,\ngiven that we already know the red roll is a 6.\nAnd so the value, if we do add all that multiplication, is we get the number 0.\nSo this idea of conditional probability is going to come up again and again,\nespecially as we begin to reason about multiple different random variables\nthat might be interacting with each other in some way.\nAnd this gets us to one of the most important rules\nin probability theory, which is known as Bayes rule.\nAnd it turns out that just using the information we've already\nlearned about probability and just applying a little bit of algebra,\nwe can actually derive Bayes rule for ourselves.\nBut it's a very important rule when it comes to inference\nand thinking about probability in the context of what\nit is that a computer can do or what a mathematician could\ndo by having access to information about probability.\nSo let's go back to these equations to be able to derive Bayes rule ourselves.\nWe know the probability of A and B, the likelihood that A and B take place,\nis the likelihood of B, and then the likelihood of A,\ngiven that we know that B is already true.\nAnd likewise, the probability of A given A and B\nis the probability of A times the probability of B,\ngiven that we know that A is already true.\nThis is sort of a symmetric relationship where\nit doesn't matter the order of A and B and B and A mean the same thing.\nAnd so in these equations, we can just swap out A and B\nto be able to represent the exact same idea.\nSo we know that these two equations are already true.\nWe've seen that already.\nAnd now let's just do a little bit of algebraic manipulation of this stuff.\nBoth of these expressions on the right-hand side\nare equal to the probability of A and B. So what I can do\nis take these two expressions on the right-hand side\nand just set them equal to each other.\nIf they're both equal to the probability of A and B,\nthen they both must be equal to each other.\nSo probability of A times probability of B given A\nis equal to the probability of B times the probability of A given B.\nAnd now all we're going to do is do a little bit of division.\nI'm going to divide both sides by P of A. And now I get what is Bayes' rule.\nThe probability of B given A is equal to the probability of B\ntimes the probability of A given B divided by the probability of A.\nAnd sometimes in Bayes' rule, you'll see the order\nof these two arguments switched.\nSo instead of B times A given B, it'll be A given B times B.\nThat ultimately doesn't matter because in multiplication,\nyou can switch the order of the two things you're multiplying,\nand it doesn't change the result. But this here right now\nis the most common formulation of Bayes' rule.\nThe probability of B given A is equal to the probability of A given\nB times the probability of B divided by the probability of A.\nAnd this rule, it turns out, is really important\nwhen it comes to trying to infer things about the world,\nbecause it means you can express one conditional probability,\nthe conditional probability of B given A, using knowledge\nabout the probability of A given B, using the reverse\nof that conditional probability.\nSo let's first do a little bit of an example with this,\njust to see how we might use it, and then explore\nwhat this means a little bit more generally.\nSo we're going to construct a situation where I have some information.\nThere are two events that I care about, the idea\nthat it's cloudy in the morning and the idea\nthat it is rainy in the afternoon.\nThose are two different possible events that could take place,\ncloudy in the morning, or the AM, rainy in the PM.\nAnd what I care about is, given clouds in the morning,\nwhat is the probability of rain in the afternoon?\nA reasonable question I might ask, in the morning,\nI look outside, or an AI's camera looks outside\nand sees that there are clouds in the morning.\nAnd we want to conclude, we want to figure out what is the probability\nthat in the afternoon, there is going to be rain.\nOf course, in the abstract, we don't have access\nto this kind of information, but we can use data\nto begin to try and figure this out.\nSo let's imagine now that I have access to some pieces of information.\nI have access to the idea that 80% of rainy afternoons\nstart out with a cloudy morning.\nAnd you might imagine that I could have gathered this data just\nby looking at data over a sequence of time,\nthat I know that 80% of the time when it's raining in the afternoon,\nit was cloudy that morning.\nI also know that 40% of days have cloudy mornings.\nAnd I also know that 10% of days have rainy afternoons.\nAnd now using this information, I would like to figure out,\ngiven clouds in the morning, what is the probability\nthat it rains in the afternoon?\nI want to know the probability of afternoon rain given morning clouds.\nAnd I can do that, in particular, using this fact, the probability of,\nso if I know that 80% of rainy afternoons start with cloudy mornings,\nthen I know the probability of cloudy mornings given rainy afternoons.\nSo using sort of the reverse conditional probability,\nI can figure that out.\nExpressed in terms of Bayes rule, this is what that would look like.\nProbability of rain given clouds is the probability of clouds given rain\ntimes the probability of rain divided by the probability of clouds.\nHere I'm just substituting in for the values of a and b\nfrom that equation of Bayes rule from before.\nAnd then I can just do the math.\nI have this information.\nI know that 80% of the time, if it was raining,\nthen there were clouds in the morning.\nSo 0.8 here.\nProbability of rain is 0.1, because 10% of days were rainy,\nand 40% of days were cloudy.\nI do the math, and I can figure out the answer is 0.2.\nSo the probability that it rains in the afternoon,\ngiven that it was cloudy in the morning, is 0.2 in this case.\nAnd this now is an application of Bayes rule,\nthe idea that using one conditional probability,\nwe can get the reverse conditional probability.\nAnd this is often useful when one of the conditional probabilities\nmight be easier for us to know about or easier for us to have data about.\nAnd using that information, we can calculate\nthe other conditional probability.\nSo what does this look like?\nWell, it means that knowing the probability of cloudy mornings\ngiven rainy afternoons, we can calculate the probability\nof rainy afternoons given cloudy mornings.\nOr, for example, more generally, if we know the probability\nof some visible effect, some effect that we can see and observe,\ngiven some unknown cause that we're not sure about,\nwell, then we can calculate the probability of that unknown cause\ngiven the visible effect.\nSo what might that look like?\nWell, in the context of medicine, for example,\nI might know the probability of some medical test result given a disease.\nLike, I know that if someone has a disease,\nthen x% of the time the medical test result will show up as this,\nfor instance.\nAnd using that information, then I can calculate, all right,\nwhat is the probability that given I know the medical test result, what\nis the likelihood that someone has the disease?\nThis is the piece of information that is usually easier to know,\neasier to immediately have access to data for.\nAnd this is the information that I actually want to calculate.\nOr I might want to know, for example, if I\nknow that some probability of counterfeit bills\nhave blurry text around the edges, because counterfeit printers aren't\nnearly as good at printing text precisely.\nSo I have some information about, given that something\nis a counterfeit bill, like x% of counterfeit bills\nhave blurry text, for example.\nAnd using that information, then I can calculate some piece of information\nthat I might want to know, like, given that I know there's blurry text\non a bill, what is the probability that that bill is counterfeit?\nSo given one conditional probability, I can\ncalculate the other conditional probability as well.\nAnd so now we've taken a look at a couple of different types of probability.\nAnd we've looked at unconditional probability,\nwhere I just look at what is the probability of this event occurring,\ngiven no additional evidence that I might have access to.\nAnd we've also looked at conditional probability,\nwhere I have some sort of evidence, and I\nwould like to, using that evidence, be able to calculate some other\nprobability as well.\nAnd the other kind of probability that will be important for us to think about\nis joint probability.\nAnd this is when we're considering the likelihood\nof multiple different events simultaneously.\nAnd so what do we mean by this?\nFor example, I might have probability distributions\nthat look a little something like this.\nLike, oh, I want to know the probability distribution of clouds\nin the morning.\nAnd that distribution looks like this.\n40% of the time, C, which is my random variable here,\nis equal to it's cloudy.\nAnd 60% of the time, it's not cloudy.\nSo here is just a simple probability distribution\nthat is effectively telling me that 40% of the time, it's cloudy.\nI might also have a probability distribution for rain in the afternoon,\nwhere 10% of the time, or with probability 0.1,\nit is raining in the afternoon.\nAnd with probability 0.9, it is not raining in the afternoon.\nAnd using just these two pieces of information,\nI don't actually have a whole lot of information\nabout how these two variables relate to each other.\nBut I could if I had access to their joint probability,\nmeaning for every combination of these two things,\nmeaning morning cloudy and afternoon rain, morning cloudy and afternoon not\nrain, morning not cloudy and afternoon rain,\nand morning not cloudy and afternoon not raining,\nif I had access to values for each of those four,\nI'd have more information.\nSo information that'd be organized in a table like this,\nand this, rather than just a probability distribution,\nis a joint probability distribution.\nIt tells me the probability distribution of each\nof the possible combinations of values that these random variables can take on.\nSo if I want to know what is the probability that on any given day\nit is both cloudy and rainy, well, I would say, all right,\nwe're looking at cases where it is cloudy and cases where it is raining.\nAnd the intersection of those two, that row in that column, is 0.08.\nSo that is the probability that it is both cloudy and rainy using\nthat information.\nAnd using this conditional probability table,\nusing this joint probability table, I can\nbegin to draw other pieces of information about things like conditional\nprobability.\nSo I might ask a question like, what is the probability distribution of clouds\ngiven that I know that it is raining?\nMeaning I know for sure that it's raining.\nTell me the probability distribution over whether it's cloudy or not,\ngiven that I know already that it is, in fact, raining.\nAnd here I'm using C to stand for that random variable.\nI'm looking for a distribution, meaning the answer to this\nis not going to be a single value.\nIt's going to be two values, a vector of two values,\nwhere the first value is probability of clouds,\nthe second value is probability that it is not cloudy,\nbut the sum of those two values is going to be 1.\nBecause when you add up the probabilities of all of the possible worlds,\nthe result that you get must be the number 1.\nAnd well, what do we know about how to calculate a conditional probability?\nWell, we know that the probability of A given B\nis the probability of A and B divided by the probability of B.\nSo what does this mean?\nWell, it means that I can calculate the probability of clouds\ngiven that it's raining as the probability of clouds and raining\ndivided by the probability of rain.\nAnd this comma here for the probability distribution\nof clouds and rain, this comma sort of stands in for the word and.\nYou'll sort of see in the logical operator and and the comma\nused interchangeably.\nThis means the probability distribution over the clouds\nand knowing the fact that it is raining divided\nby the probability of rain.\nAnd the interesting thing to note here and what\nwe'll often do in order to simplify our mathematics\nis that dividing by the probability of rain,\nthe probability of rain here is just some numerical constant.\nIt is some number.\nDividing by probability of rain is just dividing by some constant,\nor in other words, multiplying by the inverse of that constant.\nAnd it turns out that oftentimes we can just not\nworry about what the exact value of this is\nand just know that it is, in fact, a constant value.\nAnd we'll see why in a moment.\nSo instead of expressing this as this joint probability divided\nby the probability of rain, sometimes we'll\njust represent it as alpha times the numerator here,\nthe probability distribution of C, this variable,\nand that we know that it is raining, for instance.\nSo all we've done here is said this value of 1 over the probability of rain,\nthat's really just a constant we're going to divide by or equivalently\nmultiply by the inverse of at the end.\nWe'll just call it alpha for now and deal with it a little bit later.\nBut the key idea here now, and this is an idea that's going to come up again,\nis that the conditional distribution of C given rain\nis proportional to, meaning just some factor multiplied\nby the joint probability of C and rain being true.\nAnd so how do we figure this out?\nWell, this is going to be the probability that it\nis cloudy given that it's raining, which is 0.08,\nand the probability that it's not cloudy given\nthat it's raining, which is 0.02.\nAnd so we get alpha times here now is that probability distribution.\n0.08 is clouds and rain.\n0.02 is not cloudy and rain.\nBut of course, 0.08 and 0.02 don't sum up to the number 1.\nAnd we know that in a probability distribution,\nif you consider all of the possible values,\nthey must sum up to a probability of 1.\nAnd so we know that we just need to figure out\nsome constant to normalize, so to speak, these values, something\nwe can multiply or divide by to get it so that all these probabilities sum up\nto 1, and it turns out that if we multiply both numbers by 10,\nthen we can get that result of 0.8 and 0.2.\nThe proportions are still equivalent, but now 0.8 plus 0.2,\nthose sum up to the number 1.\nSo take a look at this and see if you can understand step by step\nhow it is we're getting from one point to another.\nThe key idea here is that by using the joint probabilities,\nthese probabilities that it is both cloudy and rainy\nand that it is not cloudy and rainy, I can take that information\nand figure out the conditional probability given that it's raining.\nWhat is the chance that it's cloudy versus not cloudy?\nJust by multiplying by some normalization constant, so to speak.\nAnd this is what a computer can begin to use\nto be able to interact with these various different types of probabilities.\nAnd it turns out there are a number of other probability rules\nthat are going to be useful to us as we begin\nto explore how we can actually use this information to encode\ninto our computers some more complex analysis that we might want to do\nabout probability and distributions and random variables\nthat we might be interacting with.\nSo here are a couple of those important probability rules.\nOne of the simplest rules is just this negation rule.\nWhat is the probability of not event A?\nSo A is an event that has some probability,\nand I would like to know what is the probability that A does not occur.\nAnd it turns out it's just 1 minus P of A, which makes sense.\nBecause if those are the two possible cases, either A happens or A\ndoesn't happen, then when you add up those two cases, you must get 1,\nwhich means that P of not A must just be 1 minus P of A.\nBecause P of A and P of not A must sum up to the number 1.\nThey must include all of the possible cases.\nWe've seen an expression for calculating the probability of A and B.\nWe might also reasonably want to calculate the probability of A or B.\nWhat is the probability that one thing happens or another thing happens?\nSo for example, I might want to calculate what is the probability\nthat if I roll two dice, a red die and a blue die, what is the likelihood\nthat A is a 6 or B is a 6, like one or the other?\nAnd what you might imagine you could do, and the wrong way to approach it,\nwould be just to say, all right, well, A comes up as a 6 with the red die\ncomes up as a 6 with probability 1 over 6.\nThe same for the blue die, it's also 1 over 6.\nAdd them together, and you get 2 over 6, otherwise known as 1 third.\nBut this suffers from a problem of over counting,\nthat we've double counted the case, where both A and B, both the red die\nand the blue die, both come up as a 6-roll.\nAnd I've counted that instance twice.\nSo to resolve this, the actual expression for calculating the probability of A\nor B uses what we call the inclusion-exclusion formula.\nSo I take the probability of A, add it to the probability of B.\nThat's all same as before.\nBut then I need to exclude the cases that I've double counted.\nSo I subtract from that the probability of A and B.\nAnd that gets me the result for A or B. I consider all the cases where A is true\nand all the cases where B is true.\nAnd if you imagine this is like a Venn diagram of cases where A is true,\ncases where B is true, I just need to subtract out the middle\nto get rid of the cases that I have overcounted by double counting them\ninside of both of these individual expressions.\nOne other rule that's going to be quite helpful\nis a rule called marginalization.\nSo marginalization is answering the question\nof how do I figure out the probability of A using some other variable\nthat I might have access to, like B?\nEven if I don't know additional information about it,\nI know that B, some event, can have two possible states, either B\nhappens or B doesn't happen, assuming it's a Boolean, true or false.\nAnd well, what that means is that for me to be\nable to calculate the probability of A, there are only two cases.\nEither A happens and B happens, or A happens and B doesn't happen.\nAnd those are two disjoint, meaning they can't both happen together.\nEither B happens or B doesn't happen.\nThey're disjoint or separate cases.\nAnd so I can figure out the probability of A\njust by adding up those two cases.\nThe probability that A is true is the probability that A and B is true,\nplus the probability that A is true and B isn't true.\nSo by marginalizing, I've looked at the two possible cases\nthat might take place, either B happens or B doesn't happen.\nAnd in either of those cases, I look at what's\nthe probability that A happens.\nAnd if I add those together, well, then I get the probability\nthat A happens as a whole.\nSo take a look at that rule.\nIt doesn't matter what B is or how it's related to A.\nSo long as I know these joint distributions,\nI can figure out the overall probability of A.\nAnd this can be a useful way if I have a joint distribution,\nlike the joint distribution of A and B, to just figure out\nsome unconditional probability, like the probability of A.\nAnd we'll see examples of this soon as well.\nNow, sometimes these might not just be random,\nmight not just be variables that are events that are like they happened\nor they didn't happen, like B is here.\nThey might be some broader probability distribution\nwhere there are multiple possible values.\nAnd so here, in order to use this marginalization rule,\nI need to sum up not just over B and not B,\nbut for all of the possible values that the other random variable could take\non.\nAnd so here, we'll see a version of this rule for random variables.\nAnd it's going to include that summation notation\nto indicate that I'm summing up, adding up a whole bunch of individual values.\nSo here's the rule.\nLooks a lot more complicated, but it's actually\nthe equivalent exactly the same rule.\nWhat I'm saying here is that if I have two random variables, one called x\nand one called y, well, the probability that x is equal to some value x sub i,\nthis is just some value that this variable takes on.\nHow do I figure it out?\nWell, I'm going to sum up over j, where j is going\nto range over all of the possible values that y can take on.\nWell, let's look at the probability that x equals xi and y equals yj.\nSo the exact same rule, the only difference here\nis now I'm summing up over all of the possible values\nthat y can take on, saying let's add up all of those possible cases\nand look at this joint distribution, this joint probability,\nthat x takes on the value I care about, given all of the possible values for y.\nAnd if I add all those up, then I can get\nthis unconditional probability of what x is equal to,\nwhether or not x is equal to some value x sub i.\nSo let's take a look at this rule, because it\ndoes look a little bit complicated.\nLet's try and put a concrete example to it.\nHere again is that same joint distribution from before.\nI have cloud, not cloudy, rainy, not rainy.\nAnd maybe I want to access some variable.\nI want to know what is the probability that it is cloudy.\nWell, marginalization says that if I have this joint distribution\nand I want to know what is the probability that it is cloudy,\nwell, I need to consider the other variable, the variable that's not here,\nthe idea that it's rainy.\nAnd I consider the two cases, either it's raining or it's not raining.\nAnd I just sum up the values for each of those possibilities.\nIn other words, the probability that it is cloudy\nis equal to the sum of the probability that it's cloudy and it's rainy\nand the probability that it's cloudy and it is not raining.\nAnd so these now are values that I have access to.\nThese are values that are just inside of this joint probability table.\nWhat is the probability that it is both cloudy and rainy?\nWell, it's just the intersection of these two here, which is 0.08.\nAnd the probability that it's cloudy and not raining is, all right,\nhere's cloudy, here's not raining.\nIt's 0.32.\nSo it's 0.08 plus 0.32, which just gives us equal to 0.4.\nThat is the unconditional probability that it is, in fact, cloudy.\nAnd so marginalization gives us a way to go from these joint distributions\nto just some individual probability that I might care about.\nAnd you'll see a little bit later why it is that we care about that\nand why that's actually useful to us as we begin\ndoing some of these calculations.\nLast rule we'll take a look at before transitioning\nto something a little bit different is this rule of conditioning,\nvery similar to the marginalization rule.\nBut it says that, again, if I have two events, a and b,\nbut instead of having access to their joint probabilities,\nI have access to their conditional probabilities,\nhow they relate to each other.\nWell, again, if I want to know the probability that a happens,\nand I know that there's some other variable b, either b happens or b\ndoesn't happen, and so I can say that the probability of a\nis the probability of a given b times the probability of b, meaning b happened.\nAnd given that I know b happened, what's the likelihood that a happened?\nAnd then I consider the other case, that b didn't happen.\nSo here's the probability that b didn't happen.\nAnd here's the probability that a happens,\ngiven that I know that b didn't happen.\nAnd this is really the equivalent rule just\nusing conditional probability instead of joint probability,\nwhere I'm saying let's look at both of these two cases and condition on b.\nLook at the case where b happens, and look at the case where b doesn't happen,\nand look at what probabilities I get as a result.\nAnd just as in the case of marginalization,\nwhere there was an equivalent rule for random variables\nthat could take on multiple possible values in a domain of possible values,\nhere, too, conditioning has the same equivalent rule.\nAgain, there's a summation to mean I'm summing over\nall of the possible values that some random variable y could take on.\nBut if I want to know what is the probability that x takes on this value,\nthen I'm going to sum up over all the values j that y could take on,\nand say, all right, what's the chance that y takes on that value yj?\nAnd multiply it by the conditional probability\nthat x takes on this value, given that y took on that value yj.\nSo equivalent rule just using conditional probabilities\ninstead of joint probabilities.\nAnd using the equation we know about joint probabilities,\nwe can translate between these two.\nSo all right, we've seen a whole lot of mathematics,\nand we've just laid the foundation for mathematics.\nAnd no need to worry if you haven't seen probability in too much detail\nup until this point.\nThese are the foundations of the ideas that are going to come up\nas we begin to explore how we can now take these ideas from probability\nand begin to apply them to represent something inside of our computer,\nsomething inside of the AI agent we're trying to design that\nis able to represent information and probabilities\nand the likelihoods between various different events.\nSo there are a number of different probabilistic models\nthat we can generate, but the first of the models\nwe're going to talk about are what are known as Bayesian networks.\nAnd a Bayesian network is just going to be some network of random variables,\nconnected random variables that are going to represent\nthe dependence between these random variables.\nThe odds are most random variables in this world\nare not independent from each other, but there's\nsome relationship between things that are happening that we care about.\nIf it is rainy today, that might increase the likelihood\nthat my flight or my train gets delayed, for example.\nThere are some dependence between these random variables,\nand a Bayesian network is going to be able to capture those dependencies.\nSo what is a Bayesian network?\nWhat is its actual structure, and how does it work?\nWell, a Bayesian network is going to be a directed graph.\nAnd again, we've seen directed graphs before.\nThey are individual nodes with arrows or edges\nthat connect one node to another node pointing in a particular direction.\nAnd so this directed graph is going to have nodes\nas well, where each node in this directed graph\nis going to represent a random variable, something like the weather,\nor something like whether my train was on time or delayed.\nAnd we're going to have an arrow from a node x to a node y\nto mean that x is a parent of y.\nSo that'll be our notation.\nIf there's an arrow from x to y, x is going to be considered a parent of y.\nAnd the reason that's important is because each of these nodes\nis going to have a probability distribution that we're\ngoing to store along with it, which is the distribution of x\ngiven some evidence, given the parents of x.\nSo the way to more intuitively think about this\nis the parents seem to be thought of as sort of causes for some effect\nthat we're going to observe.\nAnd so let's take a look at an actual example of a Bayesian network\nand think about the types of logic that might be involved\nin reasoning about that network.\nLet's imagine for a moment that I have an appointment out of town,\nand I need to take a train in order to get to that appointment.\nSo what are the things I might care about?\nWell, I care about getting to my appointment on time.\nWhether I make it to my appointment and I'm able to attend it\nor I miss the appointment.\nAnd you might imagine that that's influenced by the train,\nthat the train is either on time or it's delayed, for example.\nBut that train itself is also influenced.\nWhether the train is on time or not depends maybe on the rain.\nIs there no rain?\nIs it light rain?\nIs there heavy rain?\nAnd it might also be influenced by other variables too.\nIt might be influenced as well by whether or not\nthere's maintenance on the train track, for example.\nIf there is maintenance on the train track,\nthat probably increases the likelihood that my train is delayed.\nAnd so we can represent all of these ideas\nusing a Bayesian network that looks a little something like this.\nHere I have four nodes representing four random variables\nthat I would like to keep track of.\nI have one random variable called rain that\ncan take on three possible values in its domain, either none or light\nor heavy, for no rain, light rain, or heavy rain.\nI have a variable called maintenance for whether or not\nthere is maintenance on the train track, which\nit has two possible values, just either yes or no.\nEither there is maintenance or there's no maintenance happening on the track.\nThen I have a random variable for the train indicating whether or not\nthe train was on time or not.\nThat random variable has two possible values in its domain.\nThe train is either on time or the train is delayed.\nAnd then finally, I have a random variable\nfor whether I make it to my appointment.\nFor my appointment down here, I have a random variable\ncalled appointment that itself has two possible values, attend and miss.\nAnd so here are the possible values.\nHere are my four nodes, each of which represents a random variable, each\nof which has a domain of possible values that it can take on.\nAnd the arrows, the edges pointing from one node to another,\nencode some notion of dependence inside of this graph,\nthat whether I make it to my appointment or not\nis dependent upon whether the train is on time or delayed.\nAnd whether the train is on time or delayed\nis dependent on two things given by the two arrows pointing at this node.\nIt is dependent on whether or not there was maintenance on the train track.\nAnd it is also dependent upon whether or not it was raining\nor whether it is raining.\nAnd just to make things a little complicated,\nlet's say as well that whether or not there is maintenance on the track,\nthis too might be influenced by the rain.\nThat if there's heavier rain, well, maybe it's\nless likely that it's going to be maintenance on the train track that day\nbecause they're more likely to want to do maintenance on the track on days\nwhen it's not raining, for example.\nAnd so these nodes might have different relationships between them.\nBut the idea is that we can come up with a probability distribution\nfor any of these nodes based only upon its parents.\nAnd so let's look node by node at what this probability distribution might\nactually look like.\nAnd we'll go ahead and begin with this root node, this rain node here,\nwhich is at the top, and has no arrows pointing into it, which\nmeans its probability distribution is not\ngoing to be a conditional distribution.\nIt's not based on anything.\nI just have some probability distribution over the possible values\nfor the rain random variable.\nAnd that distribution might look a little something like this.\nNone, light and heavy, each have a possible value.\nHere I'm saying the likelihood of no rain is 0.7, of light rain is 0.2,\nof heavy rain is 0.1, for example.\nSo here is a probability distribution for this root node in this Bayesian\nnetwork.\nAnd let's now consider the next node in the network, maintenance.\nTrack maintenance is yes or no.\nAnd the general idea of what this distribution is going to encode,\nat least in this story, is the idea that the heavier the rain is,\nthe less likely it is that there's going to be maintenance on the track.\nBecause the people that are doing maintenance on the track probably\nwant to wait until a day when it's not as rainy in order\nto do the track maintenance, for example.\nAnd so what might that probability distribution look like?\nWell, this now is going to be a conditional probability distribution,\nthat here are the three possible values for the rain random variable, which\nI'm here just going to abbreviate to R, either no rain, light rain,\nor heavy rain.\nAnd for each of those possible values, either there\nis yes track maintenance or no track maintenance.\nAnd those have probabilities associated with them.\nThat I see here that if it is not raining,\nthen there is a probability of 0.4 that there's track maintenance\nand a probability of 0.6 that there isn't.\nBut if there's heavy rain, then here the chance\nthat there is track maintenance is 0.1 and the chance\nthat there is not track maintenance is 0.9.\nEach of these rows is going to sum up to 1.\nBecause each of these represent different values\nof whether or not it's raining, the three possible values\nthat that random variable can take on.\nAnd each is associated with its own probability distribution\nthat is ultimately all going to add up to the number 1.\nSo that there is our distribution for this random variable called maintenance,\nabout whether or not there is maintenance on the train track.\nAnd now let's consider the next variable.\nHere we have a node inside of our Bayesian network called train\nthat has two possible values, on time and delayed.\nAnd this node is going to be dependent upon the two nodes that\nare pointing towards it, that whether or not\nthe train is on time or delayed depends on whether or not\nthere is track maintenance.\nAnd it depends on whether or not there is rain,\nthat heavier rain probably means more likely that my train is delayed.\nAnd if there is track maintenance, that also probably\nmeans it's more likely that my train is delayed as well.\nAnd so you could construct a larger probability distribution,\na conditional probability distribution, that instead\nof conditioning on just one variable, as was the case here,\nis now conditioning on two variables, conditioning\nboth on rain represented by r and on maintenance represented by yes.\nAgain, each of these rows has two values that sum up to the number 1,\none for whether the train is on time, one for whether the train is delayed.\nAnd here I can say something like, all right,\nif I know there was light rain and track maintenance, well, OK,\nthat would be r is light and m is yes.\nWell, then there is a probability of 0.6 that my train is on time,\nand a probability of 0.4 the train is delayed.\nAnd you can imagine gathering this data just\nby looking at real world data, looking at data about, all right,\nif I knew that it was light rain and there was track maintenance,\nhow often was a train delayed or not delayed?\nAnd you could begin to construct this thing.\nThe interesting thing is intelligently, being\nable to try to figure out how might you go about ordering these things,\nwhat things might influence other nodes inside of this Bayesian network.\nAnd the last thing I care about is whether or not I make it to my appointment.\nSo did I attend or miss the appointment?\nAnd ultimately, whether I attend or miss the appointment,\nit is influenced by track maintenance, because it's indirectly this idea that,\nall right, if there is track maintenance,\nwell, then my train might more likely be delayed.\nAnd if my train is more likely to be delayed,\nthen I'm more likely to miss my appointment.\nBut what we encode in this Bayesian network\nare just what we might consider to be more direct relationships.\nSo the train has a direct influence on the appointment.\nAnd given that I know whether the train is on time or delayed,\nknowing whether there's track maintenance isn't\ngoing to give me any additional information that I didn't already have.\nThat if I know train, these other nodes that are up above\nisn't really going to influence the result.\nAnd so here we might represent it using another conditional probability\ndistribution that looks a little something like this.\nThe train can take on two possible values.\nEither my train is on time or my train is delayed.\nAnd for each of those two possible values,\nI have a distribution for what are the odds that I'm\nable to attend the meeting and what are the odds that I missed the meeting.\nAnd obviously, if my train is on time, I'm\nmuch more likely to be able to attend the meeting\nthan if my train is delayed, in which case I'm more likely to miss that\nmeeting.\nSo all of these nodes put all together here represent this Bayesian network,\nthis network of random variables whose values I ultimately care about,\nand that have some sort of relationship between them,\nsome sort of dependence where these arrows from one node to another\nindicate some dependence, that I can calculate\nthe probability of some node given the parents that happen to exist there.\nSo now that we've been able to describe the structure of this Bayesian\nnetwork and the relationships between each of these nodes\nby associating each of the nodes in the network with a probability\ndistribution, whether that's an unconditional probability distribution\nin the case of this root node here, like rain,\nand a conditional probability distribution in the case\nof all of the other nodes whose probabilities are\ndependent upon the values of their parents,\nwe can begin to do some computation and calculation using\nthe information inside of that table.\nSo let's imagine, for example, that I just\nwanted to compute something simple like the probability of light rain.\nHow would I get the probability of light rain?\nWell, light rain, rain here is a root node.\nAnd so if I wanted to calculate that probability,\nI could just look at the probability distribution for rain\nand extract from it the probability of light rains, just a single value\nthat I already have access to.\nBut we could also imagine wanting to compute more complex joint\nprobabilities, like the probability that there is light rain and also\nno track maintenance.\nThis is a joint probability of two values, light rain and no track\nmaintenance.\nAnd the way I might do that is first by starting by saying, all right,\nwell, let me get the probability of light rain.\nBut now I also want the probability of no track maintenance.\nBut of course, this node is dependent upon the value of rain.\nSo what I really want is the probability of no track maintenance,\ngiven that I know that there was light rain.\nAnd so the expression for calculating this idea that the probability of light\nrain and no track maintenance is really just the probability of light rain\nand the probability that there is no track maintenance,\ngiven that I know that there already is light rain.\nSo I take the unconditional probability of light rain,\nmultiply it by the conditional probability of no track maintenance,\ngiven that I know there is light rain.\nAnd you can continue to do this again and again for every variable\nthat you want to add into this joint probability\nthat I might want to calculate.\nIf I wanted to know the probability of light rain and no track maintenance\nand a delayed train, well, that's going to be the probability of light rain,\nmultiplied by the probability of no track maintenance, given light rain,\nmultiplied by the probability of a delayed train, given light rain\nand no track maintenance.\nBecause whether the train is on time or delayed\nis dependent upon both of these other two variables.\nAnd so I have two pieces of evidence that go\ninto the calculation of that conditional probability.\nAnd each of these three values is just a value\nthat I can look up by looking at one of these individual probability\ndistributions that is encoded into my Bayesian network.\nAnd if I wanted a joint probability over all four of the variables,\nsomething like the probability of light rain and no track maintenance\nand a delayed train and I miss my appointment,\nwell, that's going to be multiplying four different values, one\nfrom each of these individual nodes.\nIt's going to be the probability of light rain,\nthen of no track maintenance given light rain, then of a delayed train,\ngiven light rain and no track maintenance.\nAnd then finally, for this node here, for whether I\nmake it to my appointment or not, it's not\ndependent upon these two variables, given\nthat I know whether or not the train is on time.\nI only need to care about the conditional probability\nthat I miss my train, or that I miss my appointment,\ngiven that the train happens to be delayed.\nAnd so that's represented here by four probabilities, each of which\nis located inside of one of these probability distributions\nfor each of the nodes, all multiplied together.\nAnd so I can take a variable like that and figure out\nwhat the joint probability is by multiplying\na whole bunch of these individual probabilities from the Bayesian network.\nBut of course, just as with last time, where what I really wanted to do\nwas to be able to get new pieces of information,\nhere, too, this is what we're going to want to do with our Bayesian network.\nIn the context of knowledge, we talked about the problem of inference.\nGiven things that I know to be true, can I draw conclusions,\nmake deductions about other facts about the world that I also know to be true?\nAnd what we're going to do now is apply the same sort of idea to probability.\nUsing information about which I have some knowledge,\nwhether some evidence or some probabilities,\ncan I figure out not other variables for certain,\nbut can I figure out the probabilities of other variables\ntaking on particular values?\nAnd so here, we introduce the problem of inference in a probabilistic setting,\nin a case where variables might not necessarily be true for sure,\nbut they might be random variables that take on different values\nwith some probability.\nSo how do we formally define what exactly this inference problem actually\nis?\nWell, the inference problem has a couple of parts to it.\nWe have some query, some variable x that we\nwant to compute the distribution for.\nMaybe I want the probability that I miss my train,\nor I want the probability that there is track maintenance,\nsomething that I want information about.\nAnd then I have some evidence variables.\nMaybe it's just one piece of evidence.\nMaybe it's multiple pieces of evidence.\nBut I've observed certain variables for some sort of event.\nSo for example, I might have observed that it is raining.\nThis is evidence that I have.\nI know that there is light rain, or I know that there is heavy rain.\nAnd that is evidence I have.\nAnd using that evidence, I want to know what is the probability\nthat my train is delayed, for example.\nAnd that is a query that I might want to ask based on this evidence.\nSo I have a query, some variable.\nEvidence, which are some other variables that I\nhave observed inside of my Bayesian network.\nAnd of course, that does leave some hidden variables.\nWhy?\nThese are variables that are not evidence variables and not query variables.\nSo you might imagine in the case where I know whether or not it's raining,\nand I want to know whether my train is going to be delayed or not,\nthe hidden variable, the thing I don't have access to,\nis something like, is there maintenance on the track?\nOr am I going to make or not make my appointment, for example?\nThese are variables that I don't have access to.\nThey're hidden because they're not things I observed,\nand they're also not the query, the thing that I'm asking.\nAnd so ultimately, what we want to calculate\nis I want to know the probability distribution of x given\ne, the event that I observed.\nSo given that I observed some event, I observed that it is raining,\nI would like to know what is the distribution over the possible values\nof the train random variable.\nIs it on time?\nIs it delayed?\nWhat's the likelihood it's going to be there?\nAnd it turns out we can do this calculation just\nusing a lot of the probability rules that we've already seen in action.\nAnd ultimately, we're going to take a look at the math\nat a little bit of a high level, at an abstract level.\nBut ultimately, we can allow computers and programming libraries\nthat already exist to begin to do some of this math for us.\nBut it's good to get a general sense for what's actually happening\nwhen this inference process takes place.\nLet's imagine, for example, that I want to compute the probability\ndistribution of the appointment random variable given some evidence,\ngiven that I know that there was light rain\nand no track maintenance.\nSo there's my evidence, these two variables that I observe the values of.\nI observe the value of rain.\nI know there's light rain.\nAnd I know that there is no track maintenance going on today.\nAnd what I care about knowing, my query, is this random variable appointment.\nI want to know the distribution of this random variable appointment,\nlike what is the chance that I'm able to attend my appointment?\nWhat is the chance that I miss my appointment given this evidence?\nAnd the hidden variable, the information that I don't have access to,\nis this variable train.\nThis is information that is not part of the evidence\nthat I see, not something that I observe.\nBut it is also not the query that I'm asking for.\nAnd so what might this inference procedure look like?\nWell, if you recall back from when we were defining conditional probability\nand doing math with conditional probabilities,\nwe know that a conditional probability is proportional to the joint\nprobability.\nAnd we remembered this by recalling that the probability of A given\nB is just some constant factor alpha multiplied by the probability of A\nand B. That constant factor alpha turns out\nto be like dividing over the probability of B.\nBut the important thing is that it's just some constant multiplied\nby the joint distribution, the probability\nthat all of these individual things happen.\nSo in this case, I can take the probability of the appointment random\nvariable given light rain and no track maintenance\nand say that is just going to be proportional, some constant alpha,\nmultiplied by the joint probability, the probability\nof a particular value for the appointment random variable\nand light rain and no track maintenance.\nWell, all right, how do I calculate this, probability of appointment\nand light rain and no track maintenance, when what I really care about\nis knowing I need all four of these values\nto be able to calculate a joint distribution across everything\nbecause in a particular appointment depends upon the value of train?\nWell, in order to do that, here I can begin to use that marginalization\ntrick, that there are only two ways I can get\nany configuration of an appointment, light rain, and no track maintenance.\nEither this particular setting of variables\nhappens and the train is on time, or this particular setting of variables\nhappens and the train is delayed.\nThose are two possible cases that I would want to consider.\nAnd if I add those two cases up, well, then I\nget the result just by adding up all of the possibilities\nfor the hidden variable or variables that there are multiple.\nBut since there's only one hidden variable here, train, all I need to do\nis iterate over all the possible values for that hidden variable train\nand add up their probabilities.\nSo this probability expression here becomes probability distribution\nover appointment, light, no rain, and train is on time,\nand the probability distribution over the appointment, light rain,\nno track maintenance, and that the train is delayed, for example.\nSo I take both of the possible values for train, go ahead and add them up.\nThese are just joint probabilities that we saw earlier,\nhow to calculate just by going parent, parent, parent, parent,\nand calculating those probabilities and multiplying them together.\nAnd then you'll need to normalize them at the end,\nspeaking at a high level, to make sure that everything adds up to the number 1.\nSo the formula for how you do this in a process known as inference by enumeration\nlooks a little bit complicated, but ultimately it looks like this.\nAnd let's now try to distill what it is that all of these symbols actually mean.\nLet's start here.\nWhat I care about knowing is the probability of x, my query variable,\ngiven some sort of evidence.\nWhat do I know about conditional probabilities?\nWell, a conditional probability is proportional to the joint probability.\nSo it is some alpha, some normalizing constant,\nmultiplied by this joint probability of x and evidence.\nAnd how do I calculate that?\nWell, to do that, I'm going to marginalize\nover all of the hidden variables, all the variables\nthat I don't directly observe the values for.\nI'm basically going to iterate over all of the possibilities\nthat it could happen and just sum them all up.\nAnd so I can translate this into a sum over all y,\nwhich ranges over all the possible hidden variables and the values\nthat they could take on, and adds up all of those possible individual\nprobabilities.\nAnd that is going to allow me to do this process of inference by enumeration.\nNow, ultimately, it's pretty annoying if we as humans\nhave to do all this math for ourselves.\nBut turns out this is where computers and AI can be particularly helpful,\nthat we can program a computer to understand a Bayesian network,\nto be able to understand these inference procedures,\nand to be able to do these calculations.\nAnd using the information you've seen here,\nyou could implement a Bayesian network from scratch yourself.\nBut turns out there are a lot of libraries, especially written in Python,\nthat allow us to make it easier to do this sort of probabilistic inference,\nto be able to take a Bayesian network and do these sorts of calculations,\nso that you don't need to know and understand all of the underlying math,\nthough it's helpful to have a general sense for how it works.\nBut you just need to be able to describe the structure of the network\nand make queries in order to be able to produce the result.\nAnd so let's take a look at an example of that right now.\nIt turns out that there are a lot of possible libraries\nthat exist in Python for doing this sort of inference.\nIt doesn't matter too much which specific library you use.\nThey all behave in fairly similar ways.\nBut the library I'm going to use here is one known as pomegranate.\nAnd here inside of model.py, I have defined a Bayesian network,\njust using the structure and the syntax that the pomegranate library expects.\nAnd what I'm effectively doing is just, in Python,\ncreating nodes to represent each of the nodes of the Bayesian network\nthat you saw me describe a moment ago.\nSo here on line four, after I've imported pomegranate,\nI'm defining a variable called rain that is going\nto represent a node inside of my Bayesian network.\nIt's going to be a node that follows this distribution, where\nthere are three possible values, none for no rain, light for light rain,\nheavy for heavy rain.\nAnd these are the probabilities of each of those taking place.\n0.7 is the likelihood of no rain, 0.2 for light rain, 0.1 for heavy rain.\nThen after that, we go to the next variable,\nthe variable for track maintenance, for example,\nwhich is dependent upon that rain variable.\nAnd this, instead of being an unconditional distribution,\nis a conditional distribution, as indicated\nby a conditional probability table here.\nAnd the idea is that I'm following this is conditional\non the distribution of rain.\nSo if there is no rain, then the chance that there is, yes, track maintenance\nis 0.4.\nIf there's no rain, the chance that there is no track maintenance is 0.6.\nLikewise, for light rain, I have a distribution.\nFor heavy rain, I have a distribution as well.\nBut I'm effectively encoding the same information\nyou saw represented graphically a moment ago.\nBut I'm telling this Python program that the maintenance node\nobeys this particular conditional probability distribution.\nAnd we do the same thing for the other random variables as well.\nTrain was a node inside my distribution that\nwas a conditional probability table with two parents.\nIt was dependent not only on rain, but also on track maintenance.\nAnd so here I'm saying something like, given\nthat there is no rain and, yes, track maintenance,\nthe probability that my train is on time is 0.8.\nAnd the probability that it's delayed is 0.2.\nAnd likewise, I can do the same thing for all\nof the other possible values of the parents of the train node\ninside of my Bayesian network by saying, for all of those possible values,\nhere is the distribution that the train node should follow.\nThen I do the same thing for an appointment\nbased on the distribution of the variable train.\nThen at the end, what I do is actually construct this network\nby describing what the states of the network are\nand by adding edges between the dependent nodes.\nSo I create a new Bayesian network, add states to it, one for rain,\none for maintenance, one for the train, one for the appointment.\nAnd then I add edges connecting the related pieces.\nRain has an arrow to maintenance because rain influences track maintenance.\nRain also influences the train.\nMaintenance also influences the train.\nAnd train influences whether I make it to my appointment\nand bake just finalizes the model and does some additional computation.\nSo the specific syntax of this is not really the important part.\nPomegranate just happens to be one of several different libraries\nthat can all be used for similar purposes.\nAnd you could describe and define a library for yourself\nthat implemented similar things.\nBut the key idea here is that someone can design a library\nfor a general Bayesian network that has nodes that are based upon its parents.\nAnd then all a programmer needs to do using one of those libraries\nis to define what those nodes and what those probability distributions are.\nAnd we can begin to do some interesting logic based on it.\nSo let's try doing that conditional or joint probability calculation\nthat we saw us do by hand before by going into likelihood.py, where\nhere I'm importing the model that I just defined a moment ago.\nAnd here I'd just like to calculate model.probability, which\ncalculates the probability for a given observation.\nAnd I'd like to calculate the probability of no rain, no track maintenance,\nmy train is on time, and I'm able to attend the meeting.\nSo sort of the optimal scenario that there is no rain and no maintenance\non the track, my train is on time, and I'm able to attend the meeting.\nWhat is the probability that all of that actually happens?\nAnd I can calculate that using the library and just print out its probability.\nAnd so I'll go ahead and run python of likelihood.py.\nAnd I see that, OK, the probability is about 0.34.\nSo about a third of the time, everything goes right for me in this case.\nNo rain, no track maintenance, train is on time,\nand I'm able to attend the meeting.\nBut I could experiment with this, try and calculate other probabilities as well.\nWhat's the probability that everything goes right up until the train,\nbut I still miss my meeting?\nSo no rain, no track maintenance, train is on time,\nbut I miss the appointment.\nLet's calculate that probability.\nAnd all right, that has a probability of about 0.04.\nSo about 4% of the time, the train will be on time,\nthere won't be any rain, no track maintenance,\nand yet I'll still miss the meeting.\nAnd so this is really just an implementation\nof the calculation of the joint probabilities that we did before.\nWhat this library is likely doing is first figuring out\nthe probability of no rain, then figuring out\nthe probability of no track maintenance given no rain,\nthen the probability that my train is on time given both of these values,\nand then the probability that I miss my appointment given that I\nknow that the train was on time.\nSo this, again, is the calculation of that joint probability.\nAnd turns out we can also begin to have our computer solve inference problems\nas well, to begin to infer, based on information, evidence that we see,\nwhat is the likelihood of other variables also being true.\nSo let's go into inference.py, for example.\nWe're here, I'm again importing that exact same model from before,\nimporting all the nodes and all the edges\nand the probability distribution that is encoded there as well.\nAnd now there's a function for doing some sort of prediction.\nAnd here, into this model, I pass in the evidence that I observe.\nSo here, I've encoded into this Python program the evidence\nthat I have observed.\nI have observed the fact that the train is delayed.\nAnd that is the value for one of the four random variables\ninside of this Bayesian network.\nAnd using that information, I would like to be able to draw inspiration\nand figure out inferences about the values\nof the other random variables that are inside of my Bayesian network.\nI would like to make predictions about everything else.\nSo all of the actual computational logic is happening in just these three lines,\nwhere I'm making this call to this prediction.\nDown below, I'm just iterating over all of the states and all the predictions\nand just printing them out so that we can visually see what the results are.\nBut let's find out, given the train is delayed,\nwhat can I predict about the values of the other random variables?\nLet's go ahead and run python inference.py.\nI run that, and all right, here is the result that I get.\nGiven the fact that I know that the train is delayed,\nthis is evidence that I have observed.\nWell, given that there is a 45% chance or a 46% chance\nthat there was no rain, a 31% chance there was light rain,\na 23% chance there was heavy rain, I can see a probability distribution\nof a track maintenance and a probability distribution\nover whether I'm able to attend or miss my appointment.\nNow, we know that whether I attend or miss the appointment,\nthat is only dependent upon the train being delayed or not delayed.\nIt shouldn't depend on anything else.\nSo let's imagine, for example, that I knew that there was heavy rain.\nThat shouldn't affect the distribution for making the appointment.\nAnd indeed, if I go up here and add some evidence,\nsay that I know that the value of rain is heavy.\nThat is evidence that I now have access to.\nI now have two pieces of evidence.\nI know that the rain is heavy, and I know that my train is delayed.\nI can calculate the probability by running this inference procedure again\nand seeing the result. I know that the rain is heavy.\nI know my train is delayed.\nThe probability distribution for track maintenance changed.\nGiven that I know that there's heavy rain,\nnow it's more likely that there is no track maintenance, 88%,\nas opposed to 64% from here before.\nAnd now, what is the probability that I make the appointment?\nWell, that's the same as before.\nIt's still going to be attend the appointment with probability 0.6,\nmissed the appointment with probability 0.4,\nbecause it was only dependent upon whether or not\nmy train was on time or delayed.\nAnd so this here is implementing that idea of that inference algorithm\nto be able to figure out, based on the evidence that I have,\nwhat can we infer about the values of the other variables that exist as well.\nSo inference by enumeration is one way of doing this inference procedure,\njust looping over all of the values the hidden variables could take on\nand figuring out what the probability is.\nNow, it turns out this is not particularly efficient.\nAnd there are definitely optimizations you can make by avoiding repeated work.\nIf you're calculating the same sort of probability multiple times,\nthere are ways of optimizing the program to avoid\nhaving to recalculate the same probabilities again and again.\nBut even then, as the number of variables get large,\nas the number of possible values of variables could take on, get large,\nwe're going to start to have to do a lot of computation,\na lot of calculation, to be able to do this inference.\nAnd at that point, it might start to get unreasonable,\nin terms of the amount of time that it would take\nto be able to do this sort of exact inference.\nAnd it's for that reason that oftentimes, when\nit comes towards probability and things we're not entirely sure about,\nwe don't always care about doing exact inference\nand knowing exactly what the probability is.\nBut if we can approximate the inference procedure,\ndo some sort of approximate inference, that that can be pretty good as well.\nThat if I don't know the exact probability,\nbut I have a general sense for the probability\nthat I can get increasingly accurate with more time,\nthat that's probably pretty good, especially\nif I can get that to happen even faster.\nSo how could I do approximate inference inside of a Bayesian network?\nWell, one method is through a procedure known as sampling.\nIn the process of sampling, I'm going to take\na sample of all of the variables inside of this Bayesian network here.\nAnd how am I going to sample?\nWell, I'm going to sample one of the values from each of these nodes\naccording to their probability distribution.\nSo how might I take a sample of all these nodes?\nWell, I'll start at the root.\nI'll start with rain.\nHere's the distribution for rain.\nAnd I'll go ahead and, using a random number generator or something like it,\nrandomly pick one of these three values.\nI'll pick none with probability 0.7, light with probability 0.2,\nand heavy with probability 0.1.\nSo I'll randomly just pick one of them according to that distribution.\nAnd maybe in this case, I pick none, for example.\nThen I do the same thing for the other variable.\nMaintenance also has a probability distribution.\nAnd I'm going to sample.\nNow, there are three probability distributions here.\nBut I'm only going to sample from this first row here,\nbecause I've observed already in my sample that the value of rain is none.\nSo given that rain is none, I'm going to sample from this distribution to say,\nall right, what should the value of maintenance be?\nAnd in this case, maintenance is going to be, let's just say yes,\nwhich happens 40% of the time in the event that there is no rain, for example.\nAnd we'll sample all of the rest of the nodes in this way as well,\nthat I want to sample from the train distribution.\nAnd I'll sample from this first row here, where there is no rain,\nbut there is track maintenance.\nAnd I'll sample 80% of the time.\nI'll say the train is on time.\n20% of the time, I'll say the train is delayed.\nAnd finally, we'll do the same thing for whether I make it to my appointment\nor not.\nDid I attend or miss the appointment?\nWe'll sample based on this distribution and maybe say\nthat in this case, I attend the appointment, which\nhappens 90% of the time when the train is actually on time.\nSo by going through these nodes, I can very quickly just do some sampling\nand get a sample of the possible values that could come up\nfrom going through this entire Bayesian network\naccording to those probability distributions.\nAnd where this becomes powerful is if I do this not once,\nbut I do this thousands or tens of thousands of times\nand generate a whole bunch of samples all using this distribution.\nI get different samples.\nMaybe some of them are the same.\nBut I get a value for each of the possible variables that could come up.\nAnd so then if I'm ever faced with a question,\na question like, what is the probability that the train is on time,\nyou could do an exact inference procedure.\nThis is no different than the inference problem we had before\nwhere I could just marginalize, look at all the possible other values\nof the variables, and do the computation of inference by enumeration\nto find out this probability exactly.\nBut I could also, if I don't care about the exact probability,\njust sample it, approximate it to get close.\nAnd this is a powerful tool in AI where we don't need to be right 100%\nof the time or we don't need to be exactly right.\nIf we just need to be right with some probability,\nwe can often do so more effectively, more efficiently.\nAnd so if here now are all of those possible samples,\nI'll highlight the ones where the train is on time.\nI'm ignoring the ones where the train is delayed.\nAnd in this case, there's like six out of eight of the samples\nhave the train is arriving on time.\nAnd so maybe in this case, I can say that in six out of eight cases,\nthat's the likelihood that the train is on time.\nAnd with eight samples, that might not be a great prediction.\nBut if I had thousands upon thousands of samples,\nthen this could be a much better inference procedure\nto be able to do these sorts of calculations.\nSo this is a direct sampling method to just do a bunch of samples\nand then figure out what the probability of some event is.\nNow, this from before was an unconditional probability.\nWhat is the probability that the train is on time?\nAnd I did that by looking at all the samples and figuring out, right,\nhere are the ones where the train is on time.\nBut sometimes what I want to calculate is not an unconditional probability,\nbut rather a conditional probability, something\nlike what is the probability that there is light rain,\ngiven that the train is on time, something to that effect.\nAnd to do that kind of calculation, well, what I might do\nis here are all the samples that I have.\nAnd I want to calculate a probability distribution,\ngiven that I know that the train is on time.\nSo to be able to do that, I can kind of look\nat the two cases where the train was delayed and ignore or reject them,\nsort of exclude them from the possible samples that I'm considering.\nAnd now I want to look at these remaining cases where the train is on time.\nHere are the cases where there is light rain.\nAnd I say, OK, these are two out of the six possible cases.\nThat can give me an approximation for the probability of light rain,\ngiven the fact that I know the train was on time.\nAnd I did that in almost exactly the same way,\njust by adding an additional step, by saying that, all right,\nwhen I take each sample, let me reject all of the samples that\ndon't match my evidence and only consider\nthe samples that do match what it is that I have in my evidence\nthat I want to make some sort of calculation about.\nAnd it turns out, using the libraries that we've had for Bayesian networks,\nwe can begin to implement this same sort of idea,\nlike implement rejection sampling, which is what this method is called,\nto be able to figure out some probability, not via direct inference,\nbut instead by sampling.\nSo what I have here is a program called sample.py.\nImports the exact same model.\nAnd what I define first is a program to generate a sample.\nAnd the way I generate a sample is just by looping over all of the states.\nThe states need to be in some sort of order\nto make sure I'm looping in the correct order.\nBut effectively, if it is a conditional distribution,\nI'm going to sample based on the parents.\nAnd otherwise, I'm just going to directly sample\nthe variable, like rain, which has no parents.\nIt's just an unconditional distribution and keep\ntrack of all those parent samples and return the final sample.\nThe exact syntax of this, again, not particularly important.\nIt just happens to be part of the implementation details\nof this particular library.\nThe interesting logic is down below.\nNow that I have the ability to generate a sample,\nif I want to know the distribution of the appointment random variable,\ngiven that the train is delayed, well, then I\ncan begin to do calculations like this.\nLet me take 10,000 samples and assemble all my results\nin this list called data.\nI'll go ahead and loop n times, in this case, 10,000 times.\nI'll generate a sample.\nAnd I want to know the distribution of appointment,\ngiven that the train is delayed.\nSo according to rejection sampling, I'm only\ngoing to consider samples where the train is delayed.\nIf the train is not delayed, I'm not going to consider those values at all.\nSo I'm going to say, all right, if I take the sample,\nlook at the value of the train random variable, if the train is delayed,\nwell, let me go ahead and add to my data\nthat I'm collecting the value of the appointment random variable\nthat it took on in this particular sample.\nSo I'm only considering the samples where the train is delayed.\nAnd for each of those samples, considering what the value of appointment\nis, and then at the end, I'm using a Python class called\ncounter, which quickly counts up all the values inside of a data set.\nSo I can take this list of data and figure out\nhow many times was my appointment made and how many times was my appointment\nmissed.\nAnd so this here, with just a couple lines of code,\nis an implementation of rejection sampling.\nAnd I can run it by going ahead and running Python sample.py.\nAnd when I do that, here is the result I get.\nThis is the result of the counter.\n1,251 times, I was able to attend the meeting.\nAnd 856 times, I was able to miss the meeting.\nAnd you can imagine, by doing more and more samples,\nI'll be able to get a better and better, more accurate result.\nAnd this is a randomized process.\nIt's going to be an approximation of the probability.\nIf I run it a different time, you'll notice the numbers are similar, 12,\n72, and 905.\nBut they're not identical because there's some randomization, some likelihood\nthat things might be higher or lower.\nAnd so this is why we generally want to try and use more samples so that we\ncan have a greater amount of confidence in our result,\nbe more sure about the result that we're getting of whether or not\nit accurately reflects or represents the actual underlying probabilities that\nare inherent inside of this distribution.\nAnd so this, then, was an instance of rejection sampling.\nAnd it turns out there are a number of other sampling methods\nthat you could use to begin to try to sample.\nOne problem that rejection sampling has is\nthat if the evidence you're looking for is a fairly unlikely event,\nwell, you're going to be rejecting a lot of samples.\nLike if I'm looking for the probability of x given some evidence e,\nif e is very unlikely to occur, like occurs maybe one every 1,000 times,\nthen I'm only going to be considering 1 out of every 1,000 samples that I do,\nwhich is a pretty inefficient method for trying to do this sort of calculation.\nI'm throwing away a lot of samples.\nAnd it takes computational effort to be able to generate those samples.\nSo I'd like to not have to do something like that.\nSo there are other sampling methods that can try and address this.\nOne such sampling method is called likelihood weighting.\nIn likelihood weighting, we follow a slightly different procedure.\nAnd the goal is to avoid needing to throw out samples\nthat didn't match the evidence.\nAnd so what we'll do is we'll start by fixing the values for the evidence\nvariables.\nRather than sample everything, we're going\nto fix the values of the evidence variables and not sample those.\nThen we're going to sample all the other non-evidence variables\nin the same way, just using the Bayesian network looking\nat the probability distributions, sampling all the non-evidence variables.\nBut then what we need to do is weight each sample by its likelihood.\nIf our evidence is really unlikely, we want\nto make sure that we've taken into account how likely was the evidence\nto actually show up in the sample.\nIf I have a sample where the evidence was much more\nlikely to show up than another sample, then I\nwant to weight the more likely one higher.\nSo we're going to weight each sample by its likelihood, where likelihood is just\ndefined as the probability of all the evidence.\nGiven all the evidence we have, what is the probability\nthat it would happen in that particular sample?\nSo before, all of our samples were weighted equally.\nThey all had a weight of 1 when we were calculating\nthe overall average.\nIn this case, we're going to weight each sample,\nmultiply each sample by its likelihood in order\nto get the more accurate distribution.\nSo what would this look like?\nWell, if I ask the same question, what is the probability of light rain,\ngiven that the train is on time, when I do the sampling procedure\nand start by trying to sample, I'm going to start by fixing the evidence\nvariable.\nI'm already going to have in my sample the train is on time.\nThat way, I don't have to throw out anything.\nI'm only sampling things where I know the value of the variables that\nare my evidence are what I expect them to be.\nSo I'll go ahead and sample from rain.\nAnd maybe this time, I sample light rain instead of no rain.\nThen I'll sample from track maintenance and say,\nmaybe, yes, there's track maintenance.\nThen for train, well, I've already fixed it in place.\nTrain was an evidence variable.\nSo I'm not going to bother sampling again.\nI'll just go ahead and move on.\nI'll move on to appointment and go ahead and sample from appointment as well.\nSo now I've generated a sample.\nI've generated a sample by fixing this evidence variable\nand sampling the other three.\nAnd the last step is now weighting the sample.\nHow much weight should it have?\nAnd the weight is based on how probable is it\nthat the train was actually on time, this evidence actually happened,\ngiven the values of these other variables, light rain and the fact\nthat, yes, there was track maintenance.\nWell, to do that, I can just go back to the train variable\nand say, all right, if there was light rain and track maintenance,\nthe likelihood of my evidence, the likelihood that my train was on time,\nis 0.6.\nAnd so this particular sample would have a weight of 0.6.\nAnd I could repeat the sampling procedure again and again.\nEach time every sample would be given a weight\naccording to the probability of the evidence that I see associated with it.\nAnd there are other sampling methods that exist as well,\nbut all of them are designed to try and get it the same idea,\nto approximate the inference procedure of figuring out the value of a variable.\nSo we've now dealt with probability as it\npertains to particular variables that have these discrete values.\nBut what we haven't really considered is how values might change over time.\nThat we've considered something like a variable for rain,\nwhere rain can take on values of none or light rain or heavy rain.\nBut in practice, usually when we consider values for variables like rain,\nwe like to consider it for over time, how do the values of these variables\nchange?\nWhat do we do with when we're dealing with uncertainty\nover a period of time, which can come up in the context of weather,\nfor example, if I have sunny days and I have rainy days.\nAnd I'd like to know not just what is the probability that it's raining now,\nbut what is the probability that it rains tomorrow,\nor the day after that, or the day after that.\nAnd so to do this, we're going to introduce\na slightly different kind of model.\nBut here, we're going to have a random variable, not just one for the weather,\nbut for every possible time step.\nAnd you can define time step however you like.\nA simple way is just to use days as your time step.\nAnd so we can define a variable called x sub t, which\nis going to be the weather at time t.\nSo x sub 0 might be the weather on day 0.\nx sub 1 might be the weather on day 1, so on and so forth.\nx sub 2 is the weather on day 2.\nBut as you can imagine, if we start to do this\nover longer and longer periods of time, there's\nan incredible amount of data that might go into this.\nIf you're keeping track of data about the weather for a year,\nnow suddenly you might be trying to predict the weather tomorrow,\ngiven 365 days of previous pieces of evidence.\nAnd that's a lot of evidence to have to deal with and manipulate and calculate.\nProbably nobody knows what the exact conditional probability distribution\nis for all of those combinations of variables.\nAnd so when we're trying to do this inference inside of a computer,\nwhen we're trying to reasonably do this sort of analysis,\nit's helpful to make some simplifying assumptions,\nsome assumptions about the problem that we can just assume are true,\nto make our lives a little bit easier.\nEven if they're not totally accurate assumptions,\nif they're close to accurate or approximate, they're usually pretty good.\nAnd the assumption we're going to make is called the Markov assumption, which\nis the assumption that the current state depends only\non a finite fixed number of previous states.\nSo the current day's weather depends not on all the previous day's weather\nfor the rest of all of history, but the current day's weather\nI can predict just based on yesterday's weather,\nor just based on the last two days weather, or the last three days weather.\nBut oftentimes, we're going to deal with just the one previous state\nthat helps to predict this current state.\nAnd by putting a whole bunch of these random variables together,\nusing this Markov assumption, we can create what's called a Markov chain,\nwhere a Markov chain is just some sequence of random variables\nwhere each of the variables distribution follows that Markov assumption.\nAnd so we'll do an example of this where the Markov assumption is,\nI can predict the weather.\nIs it sunny or rainy?\nAnd we'll just consider those two possibilities for now,\neven though there are other types of weather.\nBut I can predict each day's weather just on the prior day's weather,\nusing today's weather, I can come up with a probability distribution\nfor tomorrow's weather.\nAnd here's what this weather might look like.\nIt's formatted in terms of a matrix, as you might describe it,\nas rows and columns of values, where on the left-hand side,\nI have today's weather, represented by the variable x sub t.\nAnd over here in the columns, I have tomorrow's weather,\nrepresented by the variable x sub t plus 1, t plus 1 day's weather instead.\nAnd what this matrix is saying is, if today is sunny,\nwell, then it's more likely than not that tomorrow is also sunny.\nOftentimes, the weather stays consistent for multiple days in a row.\nAnd for example, let's say that if today is sunny,\nour model says that tomorrow, with probability 0.8, it will also be sunny.\nAnd with probability 0.2, it will be raining.\nAnd likewise, if today is raining, then it's more likely than not\nthat tomorrow is also raining.\nWith probability 0.7, it'll be raining. With probability 0.3, it will be sunny.\nSo this matrix, this description of how it is we transition from one state\nto the next state is what we're going to call the transition model.\nAnd using the transition model, you can begin\nto construct this Markov chain by just predicting,\ngiven today's weather, what's the likelihood of tomorrow's weather\nhappening.\nAnd you can imagine doing a similar sampling procedure,\nwhere you take this information, you sample what tomorrow's weather is\ngoing to be.\nUsing that, you sample the next day's weather.\nAnd the result of that is you can form this Markov chain of like x0,\ntime and time, day zero is sunny, the next day is sunny,\nmaybe the next day it changes to raining, then raining, then raining.\nAnd the pattern that this Markov chain follows,\ngiven the distribution that we had access to, this transition model here,\nis that when it's sunny, it tends to stay sunny for a little while.\nThe next couple of days tend to be sunny too.\nAnd when it's raining, it tends to be raining as well.\nAnd so you get a Markov chain that looks like this,\nand you can do analysis on this.\nYou can say, given that today is raining, what is the probability\nthat tomorrow is raining?\nOr you can begin to ask probability questions\nlike, what is the probability of this sequence of five values, sun, sun,\nrain, rain, rain, and answer those sorts of questions too.\nAnd it turns out there are, again, many Python libraries\nfor interacting with models like this of probabilities\nthat have distributions and random variables that\nare based on previous variables according to this Markov assumption.\nAnd pomegranate2 has ways of dealing with these sorts of variables.\nSo I'll go ahead and go into the chain directory,\nwhere I have some information about Markov chains.\nAnd here, I've defined a file called model.py,\nwhere I've defined in a very similar syntax.\nAnd again, the exact syntax doesn't matter so much as the idea\nthat I'm encoding this information into a Python program\nso that the program has access to these distributions.\nI've here defined some starting distribution.\nSo every Markov model begins at some point in time,\nand I need to give it some starting distribution.\nAnd so we'll just say, you know at the start, you can pick 50-50 between sunny\nand rainy.\nWe'll say it's sunny 50% of the time, rainy 50% of the time.\nAnd then down below, I've here defined the transition model,\nhow it is that I transition from one day to the next.\nAnd here, I've encoded that exact same matrix from before,\nthat if it was sunny today, then with probability 0.8,\nit will be sunny tomorrow.\nAnd it'll be rainy tomorrow with probability 0.2.\nAnd I likewise have another distribution for if it was raining today instead.\nAnd so that alone defines the Markov model.\nYou can begin to answer questions using that model.\nBut one thing I'll just do is sample from the Markov chain.\nIt turns out there is a method built into this Markov chain library\nthat allows me to sample 50 states from the chain,\nbasically just simulating like 50 instances of weather.\nAnd so let me go ahead and run this.\nPython model.py.\nAnd when I run it, what I get is that it's\ngoing to sample from this Markov chain 50 states, 50 days worth of weather\nthat it's just going to randomly sample.\nAnd you can imagine sampling many times to be able to get more data,\nto be able to do more analysis.\nBut here, for example, it's sunny two days in a row,\nrainy a whole bunch of days in a row before it changes back to sun.\nAnd so you get this model that follows the distribution\nthat we originally described, that follows the distribution of sunny days\ntend to lead to more sunny days.\nRainy days tend to lead to more rainy days.\nAnd that then is a Markov model.\nAnd Markov models rely on us knowing the values\nof these individual states.\nI know that today is sunny or that today is raining.\nAnd using that information, I can draw some sort of inference\nabout what tomorrow is going to be like.\nBut in practice, this often isn't the case.\nIt often isn't the case that I know for certain what\nthe exact state of the world is.\nOftentimes, the state of the world is exactly unknown.\nBut I'm able to somehow sense some information about that state,\nthat a robot or an AI doesn't have exact knowledge\nabout the world around it.\nBut it has some sort of sensor, whether that sensor is a camera\nor sensors that detect distance or just a microphone that is sensing audio,\nfor example.\nIt is sensing data.\nAnd using that data, that data is somehow related\nto the state of the world, even if it doesn't actually know,\nour AI doesn't know, what the underlying true state of the world\nactually is.\nAnd for that, we need to get into the world of sensor models,\nthe way of describing how it is that we translate\nwhat the hidden state, the underlying true state of the world,\nis with what the observation, what it is that the AI knows or the AI has\naccess to, actually is.\nAnd so for example, a hidden state might be a robot's position.\nIf a robot is exploring new uncharted territory,\nthe robot likely doesn't know exactly where it is.\nBut it does have an observation.\nIt has robot sensor data, where it can sense how far away\nare possible obstacles around it.\nAnd using that information, using the observed information that it has,\nit can infer something about the hidden state.\nBecause what the true hidden state is influences those observations.\nWhatever the robot's true position is affects or has some effect\nupon what the sensor data of the robot is able to collect is,\neven if the robot doesn't actually know for certain what its true position is.\nLikewise, if you think about a voice recognition or a speech recognition\nprogram that listens to you and is able to respond to you, something\nlike Alexa or what Apple and Google are doing with their voice recognition\nas well, that you might imagine that the hidden state, the underlying state,\nis what words are actually spoken.\nThe true nature of the world contains you saying\na particular sequence of words, but your phone or your smart home device\ndoesn't know for sure exactly what words you said.\nThe only observation that the AI has access to is some audio waveforms.\nAnd those audio waveforms are, of course, dependent upon this hidden state.\nAnd you can infer, based on those audio waveforms,\nwhat the words spoken likely were.\nBut you might not know with 100% certainty what that hidden state actually\nis.\nAnd it might be a task to try and predict, given this observation,\ngiven these audio waveforms, can you figure out what the actual words spoken\nare.\nAnd likewise, you might imagine on a website, true user engagement.\nMight be information you don't directly have access to.\nBut you can observe data, like website or app analytics,\nabout how often was this button clicked or how often are people interacting\nwith a page in a particular way.\nAnd you can use that to infer things about your users as well.\nSo this type of problem comes up all the time\nwhen we're dealing with AI and trying to infer things about the world.\nThat often AI doesn't really know the hidden true state of the world.\nAll the AI has access to is some observation\nthat is related to the hidden true state.\nBut it's not direct.\nThere might be some noise there.\nThe audio waveform might have some additional noise\nthat might be difficult to parse.\nThe sensor data might not be exactly correct.\nThere's some noise that might not allow you to conclude with certainty what\nthe hidden state is, but can allow you to infer what it might be.\nAnd so the simple example we'll take a look at here\nis imagining the hidden state as the weather, whether it's sunny or rainy\nor not.\nAnd imagine you are programming an AI inside of a building that maybe has\naccess to just a camera to inside the building.\nAnd all you have access to is an observation\nas to whether or not employees are bringing\nan umbrella into the building or not.\nYou can detect whether it's an umbrella or not.\nAnd so you might have an observation as to whether or not\nan umbrella is brought into the building or not.\nAnd using that information, you want to predict whether it's sunny or rainy,\neven if you don't know what the underlying weather is.\nSo the underlying weather might be sunny or rainy.\nAnd if it's raining, obviously people are more likely to bring an umbrella.\nAnd so whether or not people bring an umbrella, your observation,\ntells you something about the hidden state.\nAnd of course, this is a bit of a contrived example,\nbut the idea here is to think about this more broadly in terms of more\ngenerally, any time you observe something,\nit having to do with some underlying hidden state.\nAnd so to try and model this type of idea where\nwe have these hidden states and observations,\nrather than just use a Markov model, which has state, state, state, state,\neach of which is connected by that transition matrix that we described\nbefore, we're going to use what we call a hidden Markov model.\nVery similar to a Markov model, but this is going\nto allow us to model a system that has hidden states\nthat we don't directly observe, along with some observed event\nthat we do actually see.\nAnd so in addition to that transition model that we still\nneed of saying, given the underlying state of the world,\nif it's sunny or rainy, what's the probability of tomorrow's weather?\nWe also need another model that, given some state,\nis going to give us an observation of green, yes, someone brings\nan umbrella into the office, or red, no, nobody brings umbrellas into the office.\nAnd so the observation might be that if it's sunny,\nthen odds are nobody is going to bring an umbrella to the office.\nBut maybe some people are just being cautious,\nand they do bring an umbrella to the office anyways.\nAnd if it's raining, then with much higher probability,\nthen people are going to bring umbrellas into the office.\nBut maybe if the rain was unexpected, people didn't bring an umbrella.\nAnd so it might have some other probability as well.\nAnd so using the observations, you can begin\nto predict with reasonable likelihood what the underlying state is,\neven if you don't actually get to observe the underlying state,\nif you don't get to see what the hidden state is actually equal to.\nThis here we'll often call the sensor model.\nIt's also often called the emission probabilities,\nbecause the state, the underlying state, emits some sort of emission\nthat you then observe.\nAnd so that can be another way of describing that same idea.\nAnd the sensor Markov assumption that we're going to use\nis this assumption that the evidence variable, the thing we observe,\nthe emission that gets produced, depends only on the corresponding state,\nmeaning it can predict whether or not people will bring umbrellas or not\nentirely dependent just on whether it is sunny or rainy today.\nOf course, again, this assumption might not hold in practice,\nthat in practice, it might depend whether or not\npeople bring umbrellas, might depend not just on today's weather,\nbut also on yesterday's weather and the day before.\nBut for simplification purposes, it can be helpful to apply this sort\nof assumption just to allow us to be able to reason\nabout these probabilities a little more easily.\nAnd if we're able to approximate it, we can still often get a very good answer.\nAnd so what these hidden Markov models end up looking like\nis a little something like this, where now, rather than just have\none chain of states, like sun, sun, rain, rain, rain,\nwe instead have this upper level, which is the underlying state of the world.\nIs it sunny or is it rainy?\nAnd those are connected by that transition matrix we described before.\nBut each of these states produces an emission,\nproduces an observation that I see, that on this day, it was sunny\nand people didn't bring umbrellas.\nAnd on this day, it was sunny, but people did bring umbrellas.\nAnd on this day, it was raining and people did bring umbrellas,\nand so on and so forth.\nAnd so each of these underlying states represented\nby x sub t for x sub 1, 0, 1, 2, so on and so forth,\nproduces some sort of observation or emission,\nwhich is what the e stands for, e sub 0, e sub 1, e sub 2, so on and so forth.\nAnd so this, too, is a way of trying to represent this idea.\nAnd what you want to think about is that these underlying states are\nthe true nature of the world, the robot's position as it moves over time,\nand that produces some sort of sensor data that might be observed,\nor what people are actually saying and using the emission data of what\naudio waveforms do you detect in order to process that data\nand try and figure it out.\nAnd there are a number of possible tasks that you might want to do\ngiven this kind of information.\nAnd one of the simplest is trying to infer something\nabout the future or the past or about these sort of hidden states that\nmight exist.\nAnd so the tasks that you'll often see, and we're not\ngoing to go into the mathematics of these tasks,\nbut they're all based on the same idea of conditional probabilities\nand using the probability distributions we\nhave to draw these sorts of conclusions.\nOne task is called filtering, which is given observations from the start\nuntil now, calculate the distribution for the current state,\nmeaning given information about from the beginning of time until now,\non which days do people bring an umbrella or not bring an umbrella,\ncan I calculate the probability of the current state that today,\nis it sunny or is it raining?\nAnother task that might be possible is prediction,\nwhich is looking towards the future.\nGiven observations about people bringing umbrellas\nfrom the beginning of when we started counting time until now,\ncan I figure out the distribution that tomorrow is it sunny or is it\nraining?\nAnd you can also go backwards as well by a smoothing,\nwhere I can say given observations from start until now,\ncalculate the distributions for some past state.\nLike I know that today people brought umbrellas and tomorrow people\nbrought umbrellas.\nAnd so given two days worth of data of people bringing umbrellas,\nwhat's the probability that yesterday it was raining?\nAnd that I know that people brought umbrellas today,\nthat might inform that decision as well.\nIt might influence those probabilities.\nAnd there's also a most likely explanation task,\nin addition to other tasks that might exist as well, which\nis combining some of these given observations from the start up\nuntil now, figuring out the most likely sequence of states.\nAnd this is what we're going to take a look at now, this idea that if I\nhave all these observations, umbrella, no umbrella, umbrella, no umbrella,\ncan I calculate the most likely states of sun, rain, sun, rain, and whatnot\nthat actually represented the true weather that\nwould produce these observations?\nAnd this is quite common when you're trying to do something like voice\nrecognition, for example, that you have these emissions of the audio waveforms,\nand you would like to calculate based on all of the observations\nthat you have, what is the most likely sequence of actual words, or syllables,\nor sounds that the user actually made when they were speaking\nto this particular device, or other tasks that might come up in that context\nas well.\nAnd so we can try this out by going ahead and going into the HMM directory,\nHMM for Hidden Markov Model.\nAnd here, what I've done is I've defined a model where this model first defines\nmy possible state, sun, and rain, along with their emission probabilities,\nthe observation model, or the emission model, where here, given\nthat I know that it's sunny, the probability\nthat I see people bring an umbrella is 0.2,\nthe probability of no umbrella is 0.8.\nAnd likewise, if it's raining, then people\nare more likely to bring an umbrella.\nUmbrella has probability 0.9, no umbrella has probability 0.1.\nSo the actual underlying hidden states, those states are sun and rain,\nbut the things that I observe, the observations that I can see,\nare either umbrella or no umbrella as the things that I observe as a result.\nSo this then, I also need to add to it a transition matrix, same as before,\nsaying that if today is sunny, then tomorrow is more likely to be sunny.\nAnd if today is rainy, then tomorrow is more likely to be raining.\nAs of before, I give it some starting probabilities,\nsaying at first, 50-50 chance for whether it's sunny or rainy.\nAnd then I can create the model based on that information.\nAgain, the exact syntax of this is not so important,\nso much as it is the data that I am now encoding into a program,\nsuch that now I can begin to do some inference.\nSo I can give my program, for example, a list of observations,\numbrella, umbrella, no umbrella, umbrella, umbrella, so on and so forth,\nno umbrella, no umbrella.\nAnd I would like to calculate, I would like to figure out the most likely\nexplanation for these observations.\nWhat is likely is whether rain, rain, is this rain,\nor is it more likely that this was actually sunny,\nand then it switched back to it being rainy?\nAnd that's an interesting question.\nWe might not be sure, because it might just\nbe that it just so happened on this rainy day,\npeople decided not to bring an umbrella.\nOr it could be that it switched from rainy to sunny back to rainy,\nwhich doesn't seem too likely, but it certainly could happen.\nAnd using the data we give to the hidden Markov model,\nour model can begin to predict these answers, can begin to figure it out.\nSo we're going to go ahead and just predict these observations.\nAnd then for each of those predictions, go ahead and print out\nwhat the prediction is.\nAnd this library just so happens to have a function called\npredict that does this prediction process for me.\nSo I'll run python sequence.py.\nAnd the result I get is this.\nThis is the prediction based on the observations\nof what all of those states are likely to be.\nAnd it's likely to be rain and rain.\nIn this case, it thinks that what most likely happened\nis that it was sunny for a day and then went back to being rainy.\nBut in different situations, if it was rainy for longer maybe,\nor if the probabilities were slightly different,\nyou might imagine that it's more likely that it was rainy all the way through.\nAnd it just so happened on one rainy day, people decided not to bring umbrellas.\nAnd so here, too, Python libraries can begin\nto allow for the sort of inference procedure.\nAnd by taking what we know and by putting it\nin terms of these tasks that already exist,\nthese general tasks that work with hidden Markov models,\nthen any time we can take an idea and formulate it as a hidden Markov model,\nformulate it as something that has hidden states\nand observed emissions that result from those states,\nthen we can take advantage of these algorithms\nthat are known to exist for trying to do this sort of inference.\nSo now we've seen a couple of ways that AI can begin to deal with uncertainty.\nWe've taken a look at probability and how we can use probability\nto describe numerically things that are likely or more likely or less\nlikely to happen than other events or other variables.\nAnd using that information, we can begin to construct\nthese standard types of models, things like Bayesian networks and Markov\nchains and hidden Markov models that all allow us to be able to describe\nhow particular events relate to other events\nor how the values of particular variables relate to other variables,\nnot for certain, but with some sort of probability distribution.\nAnd by formulating things in terms of these models that already exist,\nwe can take advantage of Python libraries that\nimplement these sort of models already and allow us just\nto be able to use them to produce some sort of resulting effect.\nSo all of this then allows our AI to begin\nto deal with these sort of uncertain problems\nso that our AI doesn't need to know things for certain\nbut can infer based on information it doesn't know.\nNext time, we'll take a look at additional types of problems\nthat we can solve by taking advantage of AI-related algorithms,\neven beyond the world of the types of problems we've already explored.\nWe'll see you next time.\nOK.\nWelcome back, everyone, to an introduction to artificial intelligence\nwith Python.\nAnd now, so far, we've taken a look at a couple\nof different types of problems.\nWe've seen classical search problems where\nwe're trying to get from an initial state to a goal\nby figuring out some optimal path.\nWe've taken a look at adversarial search where\nwe have a game-playing agent that is trying to make the best move.\nWe've seen knowledge-based problems where we're trying to use logic\nand inference to be able to figure out and draw\nsome additional conclusions.\nAnd we've seen some probabilistic models as well where we might not\nhave certain information about the world,\nbut we want to use the knowledge about probabilities that we do have\nto be able to draw some conclusions.\nToday, we're going to turn our attention to another category of problems\ngenerally known as optimization problems, where optimization is really\nall about choosing the best option from a set of possible options.\nAnd we've already seen optimization in some contexts,\nlike game-playing, where we're trying to create an AI that\nchooses the best move out of a set of possible moves.\nBut what we'll take a look at today is a category of types of problems\nand algorithms to solve them that can be used\nin order to deal with a broader range of potential optimization problems.\nAnd the first of the algorithms that we'll take a look at\nis known as a local search.\nAnd local search differs from search algorithms\nwe've seen before in the sense that the search algorithms we've\nlooked at so far, which are things like breadth-first search or A-star search,\nfor example, generally maintain a whole bunch of different paths\nthat we're simultaneously exploring, and we're\nlooking at a bunch of different paths at once trying\nto find our way to the solution.\nOn the other hand, in local search, this is going\nto be a search algorithm that's really just going to maintain a single node,\nlooking at a single state.\nAnd we'll generally run this algorithm by maintaining that single node\nand then moving ourselves to one of the neighboring nodes\nthroughout this search process.\nAnd this is generally useful in context not like these problems, which\nwe've seen before, like a maze-solving situation where\nwe're trying to find our way from the initial state to the goal\nby following some path.\nBut local search is most applicable when we really\ndon't care about the path at all, and all we care about\nis what the solution is.\nAnd in the case of solving a maze, the solution was always obvious.\nYou could point to the solution.\nYou know exactly what the goal is, and the real question\nis, what is the path to get there?\nBut local search is going to come up in cases\nwhere figuring out exactly what the solution is,\nexactly what the goal looks like, is actually the heart of the challenge.\nAnd to give an example of one of these kinds of problems,\nwe'll consider a scenario where we have two types of buildings,\nfor example.\nWe have houses and hospitals.\nAnd our goal might be in a world that's formatted as this grid,\nwhere we have a whole bunch of houses, a house here, house here,\ntwo houses over there, maybe we want to try and find a way\nto place two hospitals on this map.\nSo maybe a hospital here and a hospital there.\nAnd the problem now is we want to place two hospitals on the map,\nbut we want to do so with some sort of objective.\nAnd our objective in this case is to try and minimize\nthe distance of any of the houses from a hospital.\nSo you might imagine, all right, what's the distance\nfrom each of the houses to their nearest hospital?\nThere are a number of ways we could calculate that distance.\nBut one way is using a heuristic we've looked at before,\nwhich is the Manhattan distance, this idea of how many rows\nand columns would you have to move inside of this grid layout in order\nto get to a hospital, for example.\nAnd it turns out, if you take each of these four houses\nand figure out, all right, how close are they to their nearest hospital,\nyou get something like this, where this house is three away from a hospital,\nthis house is six away, and these two houses are each four away.\nAnd if you add all those numbers up together,\nyou get a total cost of 17, for example.\nSo for this particular configuration of hospitals, a hospital here\nand a hospital there, that state, we might say,\nhas a cost of 17.\nAnd the goal of this problem now that we would\nlike to apply a search algorithm to figure out\nis, can you solve this problem to find a way to minimize that cost?\nMinimize the total amount if you sum up all of the distances\nfrom all the houses to the nearest hospital.\nHow can we minimize that final value?\nAnd if we think about this problem a little bit more abstractly,\nabstracting away from this specific problem\nand thinking more generally about problems like it,\nyou can often formulate these problems by thinking about them\nas a state-space landscape, as we'll soon call it.\nHere in this diagram of a state-space landscape,\neach of these vertical bars represents a particular state\nthat our world could be in.\nSo for example, each of these vertical bars\nrepresents a particular configuration of two hospitals.\nAnd the height of this vertical bar is generally\ngoing to represent some function of that state, some value of that state.\nSo maybe in this case, the height of the vertical bar\nrepresents what is the cost of this particular configuration\nof hospitals in terms of what is the sum total of all the distances\nfrom all of the houses to their nearest hospital.\nAnd generally speaking, when we have a state-space landscape,\nwe want to do one of two things.\nWe might be trying to maximize the value of this function,\ntrying to find a global maximum, so to speak, of this state-space landscape,\na single state whose value is higher than all of the other states\nthat we could possibly choose from.\nAnd generally in this case, when we're trying to find a global maximum,\nwe'll call the function that we're trying to optimize\nsome objective function, some function that\nmeasures for any given state how good is that state,\nsuch that we can take any state, pass it into the objective function,\nand get a value for how good that state is.\nAnd ultimately, what our goal is is to find one of these states\nthat has the highest possible value for that objective function.\nAn equivalent but reversed problem is the problem\nof finding a global minimum, some state that has a value\nafter you pass it into this function that is lower than all of the other\npossible values that we might choose from.\nAnd generally speaking, when we're trying to find a global minimum,\nwe call the function that we're calculating a cost function.\nGenerally, each state has some sort of cost,\nwhether that cost is a monetary cost, or a time cost,\nor in the case of the houses and hospitals,\nwe've been looking at just now, a distance cost in terms\nof how far away each of the houses is from a hospital.\nAnd we're trying to minimize the cost, find\nthe state that has the lowest possible value of that cost.\nSo these are the general types of ideas we\nmight be trying to go for within a state space landscape,\ntrying to find a global maximum, or trying to find a global minimum.\nAnd how exactly do we do that?\nWe'll recall that in local search, we generally\noperate this algorithm by maintaining just a single state,\njust some current state represented inside of some node,\nmaybe inside of a data structure, where we're\nkeeping track of where we are currently.\nAnd then ultimately, what we're going to do is from that state,\nmove to one of its neighbor states.\nSo in this case, represented in this one-dimensional space\nby just the state immediately to the left or to the right of it.\nBut for any different problem, you might define\nwhat it means for there to be a neighbor of a particular state.\nIn the case of a hospital, for example, that we were just looking at,\na neighbor might be moving one hospital one space to the left\nor to the right or up or down.\nSome state that is close to our current state, but slightly different,\nand as a result, might have a slightly different value\nin terms of its objective function or in terms of its cost function.\nSo this is going to be our general strategy in local search,\nto be able to take a state, maintaining some current node,\nand move where we're looking at in the state space landscape\nin order to try to find a global maximum or a global minimum somehow.\nAnd perhaps the simplest of algorithms that we\ncould use to implement this idea of local search\nis an algorithm known as hill climbing.\nAnd the basic idea of hill climbing is, let's\nsay I'm trying to maximize the value of my state.\nI'm trying to figure out where the global maximum is.\nI'm going to start at a state.\nAnd generally, what hill climbing is going to do\nis it's going to consider the neighbors of that state,\nthat from this state, all right, I could go left or I could go right,\nand this neighbor happens to be higher and this neighbor happens to be lower.\nAnd in hill climbing, if I'm trying to maximize the value,\nI'll generally pick the highest one I can between the state\nto the left and right of me.\nThis one is higher.\nSo I'll go ahead and move myself to consider that state instead.\nAnd then I'll repeat this process, continually looking at all of my neighbors\nand picking the highest neighbor, doing the same thing,\nlooking at my neighbors, picking the highest of my neighbors,\nuntil I get to a point like right here, where I consider both of my neighbors\nand both of my neighbors have a lower value than I do.\nThis current state has a value that is higher than any of its neighbors.\nAnd at that point, the algorithm terminates.\nAnd I can say, all right, here I have now found the solution.\nAnd the same thing works in exactly the opposite way\nfor trying to find a global minimum.\nBut the algorithm is fundamentally the same.\nIf I'm trying to find a global minimum and say my current state starts here,\nI'll continually look at my neighbors, pick the lowest value\nthat I possibly can, until I eventually, hopefully,\nfind that global minimum, a point at which when\nI look at both of my neighbors, they each have a higher value.\nAnd I'm trying to minimize the total score or cost or value\nthat I get as a result of calculating some sort of cost function.\nSo we can formulate this graphical idea in terms of pseudocode.\nAnd the pseudocode for hill climbing might look like this.\nWe define some function called hill climb that\ntakes as input the problem that we're trying to solve.\nAnd generally, we're going to start in some sort of initial state.\nSo I'll start with a variable called current\nthat is keeping track of my initial state, like an initial configuration\nof hospitals.\nAnd maybe some problems lend themselves to an initial state,\nsome place where you begin.\nIn other cases, maybe not, in which case we might just randomly\ngenerate some initial state, just by choosing two locations for hospitals\nat random, for example, and figuring out from there\nhow we might be able to improve.\nBut that initial state, we're going to store inside of current.\nAnd now, here comes our loop, some repetitive process\nwe're going to do again and again until the algorithm terminates.\nAnd what we're going to do is first say, let's\nfigure out all of the neighbors of the current state.\nFrom my state, what are all of the neighboring\nstates for some definition of what it means to be a neighbor?\nAnd I'll go ahead and choose the highest value of all of those neighbors\nand save it inside of this variable called neighbor.\nSo keep track of the highest-valued neighbor.\nThis is in the case where I'm trying to maximize the value.\nIn the case where I'm trying to minimize the value,\nyou might imagine here, you'll pick the neighbor\nwith the lowest possible value.\nBut these ideas are really fundamentally interchangeable.\nAnd it's possible, in some cases, there might be multiple neighbors\nthat each have an equally high value or an equally low value\nin the minimizing case.\nAnd in that case, we can just choose randomly from among them.\nChoose one of them and save it inside of this variable neighbor.\nAnd then the key question to ask is, is this neighbor better\nthan my current state?\nAnd if the neighbor, the best neighbor that I was able to find,\nis not better than my current state, well, then the algorithm is over.\nAnd I'll just go ahead and return the current state.\nIf none of my neighbors are better, then I may as well stay where I am,\nis the general logic of the hill climbing algorithm.\nBut otherwise, if the neighbor is better, then I may as well\nmove to that neighbor.\nSo you might imagine setting current equal to neighbor, where the general idea\nis if I'm at a current state and I see a neighbor that is better than me,\nthen I'll go ahead and move there.\nAnd then I'll repeat the process, continually moving to a better neighbor\nuntil I reach a point at which none of my neighbors are better than I am.\nAnd at that point, we'd say the algorithm can just terminate there.\nSo let's take a look at a real example of this\nwith these houses and hospitals.\nSo we've seen now that if we put the hospitals in these two locations,\nthat has a total cost of 17.\nAnd now we need to define, if we're going to implement this hill climbing\nalgorithm, what it means to take this particular configuration\nof hospitals, this particular state, and get a neighbor of that state.\nAnd a simple definition of neighbor might be just,\nlet's pick one of the hospitals and move it by one square, the left or right\nor up or down, for example.\nAnd that would mean we have six possible neighbors\nfrom this particular configuration.\nWe could take this hospital and move it to any of these three possible squares,\nor we take this hospital and move it to any of those three possible squares.\nAnd each of those would generate a neighbor.\nAnd what I might do is say, all right, here's\nthe locations and the distances between each of the houses\nand their nearest hospital.\nLet me consider all of the neighbors and see if any of them\ncan do better than a cost of 17.\nAnd it turns out there are a couple of ways that we could do that.\nAnd it doesn't matter if we randomly choose\namong all the ways that are the best.\nBut one such possible way is by taking a look at this hospital here\nand considering the directions in which it might move.\nIf we hold this hospital constant, if we take this hospital\nand move it one square up, for example, that doesn't really help us.\nIt gets closer to the house up here, but it gets further away\nfrom the house down here.\nAnd it doesn't really change anything for the two houses\nalong the left-hand side.\nBut if we take this hospital on the right and move it one square down,\nit's the opposite problem.\nIt gets further away from the house up above,\nand it gets closer to the house down below.\nThe real idea, the goal should be to be able to take this hospital\nand move it one square to the left.\nBy moving it one square to the left, we move it closer\nto both of these houses on the right without changing anything\nabout the houses on the left.\nFor them, this hospital is still the closer one, so they aren't affected.\nSo we're able to improve the situation by picking a neighbor that\nresults in a decrease in our total cost.\nAnd so we might do that.\nMove ourselves from this current state to a neighbor\nby just taking that hospital and moving it.\nAnd at this point, there's not a whole lot\nthat can be done with this hospital.\nBut there's still other optimizations we can make, other neighbors\nwe can move to that are going to have a better value.\nIf we consider this hospital, for example,\nwe might imagine that right now it's a bit far up,\nthat both of these houses are a little bit lower.\nSo we might be able to do better by taking this hospital\nand moving it one square down, moving it down so that now instead\nof a cost of 15, we're down to a cost of 13\nfor this particular configuration.\nAnd we can do even better by taking the hospital\nand moving it one square to the left.\nNow instead of a cost of 13, we have a cost of 11,\nbecause this house is one away from the hospital.\nThis one is four away.\nThis one is three away.\nAnd this one is also three away.\nSo we've been able to do much better than that initial cost\nthat we had using the initial configuration.\nJust by taking every state and asking ourselves the question,\ncan we do better by just making small incremental changes,\nmoving to a neighbor, moving to a neighbor,\nand moving to a neighbor after that?\nAnd now at this point, we can potentially see that at this point,\nthe algorithm is going to terminate.\nThere's actually no neighbor we can move to\nthat is going to improve the situation, get us a cost that is less than 11.\nBecause if we take this hospital and move it upper to the right,\nwell, that's going to make it further away.\nIf we take it and move it down, that doesn't really change the situation.\nIt gets further away from this house but closer to that house.\nAnd likewise, the same story was true for this hospital.\nAny neighbor we move it to, up, left, down, or right,\nis either going to make it further away from the houses and increase the cost,\nor it's going to have no effect on the cost whatsoever.\nAnd so the question we might now ask is, is this the best we could do?\nIs this the best placement of the hospitals we could possibly have?\nAnd it turns out the answer is no, because there's a better way\nthat we could place these hospitals.\nAnd in particular, there are a number of ways you could do this.\nBut one of the ways is by taking this hospital here\nand moving it to this square, for example, moving it diagonally\nby one square, which was not part of our definition of neighbor.\nWe could only move left, right, up, or down.\nBut this is, in fact, better.\nIt has a total cost of 9.\nIt is now closer to both of these houses.\nAnd as a result, the total cost is less.\nBut we weren't able to find it, because in order to get there,\nwe had to go through a state that actually wasn't any better than the current\nstate that we had been on previously.\nAnd so this appears to be a limitation, or a concern you might have\nas you go about trying to implement a hill climbing algorithm,\nis that it might not always give you the optimal solution.\nIf we're trying to maximize the value of any particular state,\nwe're trying to find the global maximum, a concern\nmight be that we could get stuck at one of the local maxima,\nhighlighted here in blue, where a local maxima is any state whose value is\nhigher than any of its neighbors.\nIf we ever find ourselves at one of these two states\nwhen we're trying to maximize the value of the state,\nwe're not going to make any changes.\nWe're not going to move left or right.\nWe're not going to move left here, because those states are worse.\nBut yet, we haven't found the global optimum.\nWe haven't done as best as we could do.\nAnd likewise, in the case of the hospitals, what we're ultimately\ntrying to do is find a global minimum, find a value that\nis lower than all of the others.\nBut we have the potential to get stuck at one of the local minima,\nany of these states whose value is lower than all of its neighbors,\nbut still not as low as the local minima.\nAnd so the takeaway here is that it's not always\ngoing to be the case that when we run this naive hill climbing algorithm,\nthat we're always going to find the optimal solution.\nThere are things that could go wrong.\nIf we started here, for example, and tried to maximize our value as much\nas possible, we might move to the highest possible neighbor,\nmove to the highest possible neighbor, move to the highest possible neighbor,\nand stop, and never realize that there's actually a better state way over there\nthat we could have gone to instead.\nAnd other problems you might imagine just by taking a look at this state\nspace landscape are these various different types of plateaus,\nsomething like this flat local maximum here,\nwhere all six of these states each have the exact same value.\nAnd so in the case of the algorithm we showed before,\nnone of the neighbors are better, so we might just\nget stuck at this flat local maximum.\nAnd even if you allowed yourself to move to one of the neighbors,\nit wouldn't be clear which neighbor you would ultimately move to,\nand you could get stuck here as well.\nAnd there's another one over here.\nThis one is called a shoulder.\nIt's not really a local maximum, because there's still\nplaces where we can go higher, not a local minimum, because we can go lower.\nSo we can still make progress, but it's still this flat area,\nwhere if you have a local search algorithm,\nthere's potential to get lost here, unable to make some upward or downward\nprogress, depending on whether we're trying to maximize or minimize it,\nand therefore another potential for us to be\nable to find a solution that might not actually be the optimal solution.\nAnd so because of this potential, the potential that hill climbing\nhas to not always find us the optimal result,\nit turns out there are a number of different varieties and variations\non the hill climbing algorithm that help to solve the problem better\ndepending on the context, and depending on the specific type of problem,\nsome of these variants might be more applicable than others.\nWhat we've taken a look at so far is a version of hill climbing\ngenerally called steepest ascent hill climbing,\nwhere the idea of steepest ascent hill climbing\nis we are going to choose the highest valued neighbor,\nin the case where we're trying to maximize or the lowest valued neighbor\nin cases where we're trying to minimize.\nBut generally speaking, if I have five neighbors\nand they're all better than my current state,\nI will pick the best one of those five.\nNow, sometimes that might work pretty well.\nIt's sort of a greedy approach of trying to take the best operation\nat any particular time step, but it might not always work.\nThere might be cases where actually I want\nto choose an option that is slightly better than me,\nbut maybe not the best one because that later on might\nlead to a better outcome ultimately.\nSo there are other variants that we might consider\nof this basic hill climbing algorithm.\nOne is known as stochastic hill climbing.\nAnd in this case, we choose randomly from all of our higher value neighbors.\nSo if I'm at my current state and there are five neighbors that\nare all better than I am, rather than choosing the best one,\nas steep as the set would do, stochastic will just choose\nrandomly from one of them, thinking that if it's better, then it's better.\nAnd maybe there's a potential to make forward progress,\neven if it is not locally the best option I could possibly choose.\nFirst choice hill climbing ends up just choosing the very first highest\nvalued neighbor that it follows, behaving on a similar idea,\nrather than consider all of the neighbors.\nAs soon as we find a neighbor that is better than our current state,\nwe'll go ahead and move there.\nThere may be some efficiency improvements there\nand maybe has the potential to find a solution\nthat the other strategies weren't able to find.\nAnd with all of these variants, we still suffer from the same potential risk,\nthis risk that we might end up at a local minimum or a local maximum.\nAnd we can reduce that risk by repeating the process multiple times.\nSo one variant of hill climbing is random restart hill climbing,\nwhere the general idea is we'll conduct hill climbing multiple times.\nIf we apply steepest descent hill climbing, for example,\nwe'll start at some random state, try and figure out\nhow to solve the problem and figure out what\nis the local maximum or local minimum we get to.\nAnd then we'll just randomly restart and try again,\nchoose a new starting configuration, try and figure out\nwhat the local maximum or minimum is, and do this some number of times.\nAnd then after we've done it some number of times,\nwe can pick the best one out of all of the ones that we've taken a look at.\nSo there's another option we have access to as well.\nAnd then, although I said that generally local search will usually\njust keep track of a single node and then move to one of its neighbors,\nthere are variants of hill climbing that are known as local beam searches,\nwhere rather than keep track of just one current best state,\nwe're keeping track of k highest valued neighbors, such that rather than\nstarting at one random initial configuration,\nI might start with 3 or 4 or 5, randomly generate all the neighbors,\nand then pick the 3 or 4 or 5 best of all of the neighbors that I find,\nand continually repeat this process, with the idea\nbeing that now I have more options that I'm considering,\nmore ways that I could potentially navigate myself\nto the optimal solution that might exist for a particular problem.\nSo let's now take a look at some actual code that\ncan implement some of these kinds of ideas, something\nlike steepest ascent hill climbing, for example,\nfor trying to solve this hospital problem.\nSo I'm going to go ahead and go into my hospitals directory, where\nI've actually set up the basic framework for solving this type of problem.\nI'll go ahead and go into hospitals.py, and we'll\ntake a look at the code we've created here.\nI've defined a class that is going to represent the state space.\nSo the space has a height, and a width, and also some number of hospitals.\nSo you can configure how big is your map, how many hospitals should go here.\nWe have a function for adding a new house to the state space,\nand then some functions that are going to get\nme all of the available spaces for if I want to randomly place hospitals\nin particular locations.\nAnd here now is the hill climbing algorithm.\nSo what are we going to do in the hill climbing algorithm?\nWell, we're going to start by randomly initializing\nwhere the hospitals are going to go.\nWe don't know where the hospitals should actually be,\nso let's just randomly place them.\nSo here I'm running a loop for each of the hospitals that I have.\nI'm going to go ahead and add a new hospital at some random location.\nSo I basically get all of the available spaces,\nand I randomly choose one of them as where\nI would like to add this particular hospital.\nI have some logging output and generating some images,\nwhich we'll take a look at a little bit later.\nBut here is the key idea.\nSo I'm going to just keep repeating this algorithm.\nI could specify a maximum of how many times I want it to run,\nor I could just run it up until it hits a local maximum or local minimum.\nAnd now we'll basically consider all of the hospitals\nthat could potentially move.\nSo consider each of the two hospitals or more hospitals\nif they're more than that.\nAnd consider all of the places where that hospital could move to,\nsome neighbor of that hospital that we can move the neighbor to.\nAnd then see, is this going to be better than where we were currently?\nSo if it is going to be better, then we'll\ngo ahead and update our best neighbor and keep\ntrack of this new best neighbor that we found.\nAnd then afterwards, we can ask ourselves the question,\nif best neighbor cost is greater than or equal\nto the cost of the current set of hospitals,\nmeaning if the cost of our best neighbor is greater than the current cost,\nmeaning our best neighbor is worse than our current state,\nwell, then we shouldn't make any changes at all.\nAnd we should just go ahead and return the current set of hospitals.\nBut otherwise, we can update our hospitals\nin order to change them to one of the best neighbors.\nAnd if there are multiple that are all equivalent,\nI'm here using random.choice to say go ahead and choose one randomly.\nSo this is really just a Python implementation of that same idea\nthat we were just talking about, this idea of taking a current state,\nsome current set of hospitals, generating all of the neighbors,\nlooking at all of the ways we could take one hospital\nand move it one square to the left or right or up or down,\nand then figuring out, based on all of that information, which\nis the best neighbor or the set of all the best neighbors,\nand then choosing from one of those.\nAnd each time, we go ahead and generate an image in order to do that.\nAnd so now what we're doing is if we look down at the bottom,\nI'm going to randomly generate a space with height 10 and width 20.\nAnd I'll say go ahead and put three hospitals somewhere in the space.\nI'll randomly generate 15 houses that I just go ahead\nand add in random locations.\nAnd now I'm going to run this hill climbing algorithm in order\nto try and figure out where we should place those hospitals.\nSo we'll go ahead and run this program by running Python hospitals.\nAnd we see that we started.\nOur initial state had a cost of 72, but we\nwere able to continually find neighbors that were able to decrease that cost,\ndecrease to 69, 66, 63, so on and so forth, all the way down to 53,\nas the best neighbor we were able to ultimately find.\nAnd we can take a look at what that looked like\nby just opening up these files.\nSo here, for example, was the initial configuration.\nWe randomly selected a location for each of these 15 different houses\nand then randomly selected locations for one, two, three hospitals\nthat were just located somewhere inside of the state space.\nAnd if you add up all the distances from each of the houses\nto their nearest hospital, you get a total cost of about 72.\nAnd so now the question is, what neighbors can we move to\nthat improve the situation?\nAnd it looks like the first one the algorithm found\nwas by taking this house that was over there on the right\nand just moving it to the left.\nAnd that probably makes sense because if you\nlook at the houses in that general area, really these five houses look like\nthey're probably the ones that are going to be closest to this hospital over here.\nMoving it to the left decreases the total distance, at least\nto most of these houses, though it does increase that distance for one of them.\nAnd so we're able to make these improvements to the situation\nby continually finding ways that we can move these hospitals around\nuntil we eventually settle at this particular state that\nhas a cost of 53, where we figured out a position for each of the hospitals.\nAnd now none of the neighbors that we could move to\nare actually going to improve the situation.\nWe can take this hospital and this hospital and that hospital\nand look at each of the neighbors.\nAnd none of those are going to be better than this particular configuration.\nAnd again, that's not to say that this is the best we could do.\nThere might be some other configuration of hospitals\nthat is a global minimum.\nAnd this might just be a local minimum that is the best of all of its neighbors,\nbut maybe not the best in the entire possible state space.\nAnd you could search through the entire state space\nby considering all of the possible configurations for hospitals.\nBut ultimately, that's going to be very time intensive,\nespecially as our state space gets bigger and there\nmight be more and more possible states.\nIt's going to take quite a long time to look through all of them.\nAnd so being able to use these sort of local search algorithms\ncan often be quite good for trying to find the best solution we can do.\nAnd especially if we don't care about doing the best possible\nand we just care about doing pretty good and finding\na pretty good placement of those hospitals,\nthen these methods can be particularly powerful.\nBut of course, we can try and mitigate some of this concern\nby instead of using hill climbing to use random restart,\nthis idea of rather than just hill climb one time,\nwe can hill climb multiple times and say,\ntry hill climbing a whole bunch of times on the exact same map\nand figure out what is the best one that we've been able to find.\nAnd so I've here implemented a function for random restart\nthat restarts some maximum number of times.\nAnd what we're going to do is repeat that number of times this process of just\ngo ahead and run the hill climbing algorithm,\nfigure out what the cost is of getting from all the houses to the hospitals,\nand then figure out is this better than we've done so far.\nSo I can try this exact same idea where instead of running hill climbing,\nI'll go ahead and run random restart.\nAnd I'll randomly restart maybe 20 times, for example.\nAnd we'll go ahead and now I'll remove all the images\nand then rerun the program.\nAnd now we started by finding a original state.\nWhen we initially ran hill climbing, the best cost\nwe were able to find was 56.\nEach of these iterations is a different iteration of the hill climbing\nalgorithm.\nWe're running hill climbing not one time, but 20 times here,\neach time going until we find a local minimum in this case.\nAnd we look and see each time did we do better\nthan we did the best time we've done so far.\nSo we went from 56 to 46.\nThis one was greater, so we ignored it.\nThis one was 41, which was less, so we went ahead and kept that one.\nAnd for all of the remaining 16 times that we\ntried to implement hill climbing and we tried to run the hill climbing\nalgorithm, we couldn't do any better than that 41.\nAgain, maybe there is a way to do better that we just didn't find,\nbut it looks like that way ended up being a pretty good solution\nto the problem.\nThat was attempt number three, starting from counting at zero.\nSo we can take a look at that, open up number three.\nAnd this was the state that happened to have a cost of 41,\nthat after running the hill climbing algorithm\non some particular random initial configuration of hospitals,\nthis is what we found was the local minimum in terms\nof trying to minimize the cost.\nAnd it looks like we did pretty well.\nThis hospital is pretty close to this region.\nThis one is pretty close to these houses here.\nThis hospital looks about as good as we can do\nfor trying to capture those houses over on that side.\nAnd so these sorts of algorithms can be quite useful\nfor trying to solve these problems.\nBut the real problem with many of these different types of hill climbing,\nsteepest of sense, stochastic, first choice, and so forth,\nis that they never make a move that makes our situation worse.\nThey're always going to take ourselves in our current state,\nlook at the neighbors, and consider can we do better than our current state\nand move to one of those neighbors.\nWhich of those neighbors we choose might vary among these various different\ntypes of algorithms, but we never go from a current position\nto a position that is worse than our current position.\nAnd ultimately, that's what we're going to need to do\nif we want to be able to find a global maximum or a global minimum.\nBecause sometimes if we get stuck, we want\nto find some way of dislodging ourselves from our local maximum\nor local minimum in order to find the global maximum or the global minimum\nor increase the probability that we do find it.\nAnd so the most popular technique for trying\nto approach the problem from that angle is a technique known\nas simulated annealing, simulated because it's modeling\nafter a real physical process of annealing, where you can think about this\nin terms of physics, a physical situation where\nyou have some system of particles.\nAnd you might imagine that when you heat up\na particular physical system, there's a lot of energy there.\nThings are moving around quite randomly.\nBut over time, as the system cools down, it eventually\nsettles into some final position.\nAnd that's going to be the general idea of simulated annealing.\nWe're going to simulate that process of some high temperature system where\nthings are moving around randomly quite frequently,\nbut over time decreasing that temperature until we eventually\nsettle at our ultimate solution.\nAnd the idea is going to be if we have some state space landscape that\nlooks like this and we begin at its initial state here,\nif we're looking for a global maximum and we're\ntrying to maximize the value of the state,\nour traditional hill climbing algorithms would just take the state\nand look at the two neighbor ones and always\npick the one that is going to increase the value of the state.\nBut if we want some chance of being able to find the global maximum,\nwe can't always make good moves.\nWe have to sometimes make bad moves and allow ourselves\nto make a move in a direction that actually seems for now\nto make our situation worse such that later we\ncan find our way up to that global maximum in terms\nof trying to solve that problem.\nOf course, once we get up to this global maximum,\nonce we've done a whole lot of the searching,\nthen we probably don't want to be moving to states\nthat are worse than our current state.\nAnd so this is where this metaphor for annealing\nstarts to come in, where we want to start making more random moves\nand over time start to make fewer of those random moves based\non a particular temperature schedule.\nSo the basic outline looks something like this.\nEarly on in simulated annealing, we have a higher temperature state.\nAnd what we mean by a higher temperature state\nis that we are more likely to accept neighbors that\nare worse than our current state.\nWe might look at our neighbors.\nAnd if one of our neighbors is worse than the current state,\nespecially if it's not all that much worse,\nif it's pretty close but just slightly worse,\nthen we might be more likely to accept that and go ahead\nand move to that neighbor anyways.\nBut later on as we run simulated annealing,\nwe're going to decrease that temperature.\nAnd at a lower temperature, we're going to be less likely to accept neighbors\nthat are worse than our current state.\nNow to formalize this and put a little bit of pseudocode to it,\nhere is what that algorithm might look like.\nWe have a function called simulated annealing\nthat takes as input the problem we're trying to solve\nand also potentially some maximum number of times\nwe might want to run the simulated annealing process, how many different\nneighbors we're going to try and look for.\nAnd that value is going to vary based on the problem you're trying to solve.\nWe'll, again, start with some current state\nthat will be equal to the initial state of the problem.\nBut now we need to repeat this process over and over\nfor max number of times.\nRepeat some process some number of times where we're first\ngoing to calculate a temperature.\nAnd this temperature function takes the current time t\nstarting at 1 going all the way up to max\nand then gives us some temperature that we can use in our computation,\nwhere the idea is that this temperature is going to be higher early on\nand it's going to be lower later on.\nSo there are a number of ways this temperature function could often work.\nOne of the simplest ways is just to say it\nis like the proportion of time that we still have remaining.\nOut of max units of time, how much time do we have remaining?\nYou start off with a lot of that time remaining.\nAnd as time goes on, the temperature is going to decrease\nbecause you have less and less of that remaining time still available to you.\nSo we calculate a temperature for the current time.\nAnd then we pick a random neighbor of the current state.\nNo longer are we going to be picking the best neighbor that we possibly can\nor just one of the better neighbors that we can.\nWe're going to pick a random neighbor.\nIt might be better.\nIt might be worse.\nBut we're going to calculate that.\nWe're going to calculate delta E, E for energy in this case,\nwhich is just how much better is the neighbor than the current state.\nSo if delta E is positive, that means the neighbor\nis better than our current state.\nIf delta E is negative, that means the neighbor\nis worse than our current state.\nAnd so we can then have a condition that looks like this.\nIf delta E is greater than 0, that means the neighbor state\nis better than our current state.\nAnd if ever that situation arises, we'll just go ahead and update current\nto be that neighbor.\nSame as before, move where we are currently to be the neighbor\nbecause the neighbor is better than our current state.\nWe'll go ahead and accept that.\nBut now the difference is that whereas before, we never,\never wanted to take a move that made our situation worse,\nnow we sometimes want to make a move that is actually\ngoing to make our situation worse because sometimes we're\ngoing to need to dislodge ourselves from a local minimum or local maximum\nto increase the probability that we're able to find the global minimum\nor the global maximum a little bit later.\nAnd so how do we do that?\nHow do we decide to sometimes accept some state that might actually be worse?\nWell, we're going to accept a worse state with some probability.\nAnd that probability needs to be based on a couple of factors.\nIt needs to be based in part on the temperature,\nwhere if the temperature is higher, we're more likely to move to a worse\nneighbor.\nAnd if the temperature is lower, we're less likely to move to a worse neighbor.\nBut it also, to some degree, should be based on delta E.\nIf the neighbor is much worse than the current state,\nwe probably want to be less likely to choose that\nthan if the neighbor is just a little bit worse than the current state.\nSo again, there are a couple of ways you could calculate this.\nBut it turns out one of the most popular is just\nto calculate E to the power of delta E over T, where E is just a constant.\nDelta E over T are based on delta E and T here.\nWe calculate that value.\nAnd that'll be some value between 0 and 1.\nAnd that is the probability with which we should just say, all right,\nlet's go ahead and move to that neighbor.\nAnd it turns out that if you do the math for this value,\nwhen delta E is such that the neighbor is not\nthat much worse than the current state, that's\ngoing to be more likely that we're going to go ahead and move to that state.\nAnd likewise, when the temperature is lower,\nwe're going to be less likely to move to that neighboring state as well.\nSo now this is the big picture for simulated annealing,\nthis process of taking the problem and going ahead and generating\nrandom neighbors will always move to a neighbor\nif it's better than our current state.\nBut even if the neighbor is worse than our current state,\nwe'll sometimes move there depending on how much worse it is\nand also based on the temperature.\nAnd as a result, the hope, the goal of this whole process\nis that as we begin to try and find our way to the global maximum\nor the global minimum, we can dislodge ourselves\nif we ever get stuck at a local maximum or local minimum\nin order to eventually make our way to exploring\nthe part of the state space that is going to be the best.\nAnd then as the temperature decreases, eventually we settle there\nwithout moving around too much from what we've\nfound to be the globally best thing that we can do thus far.\nSo at the very end, we just return whatever the current state happens to be.\nAnd that is the conclusion of this algorithm.\nWe've been able to figure out what the solution is.\nAnd these types of algorithms have a lot of different applications.\nAny time you can take a problem and formulate it\nas something where you can explore a particular configuration\nand then ask, are any of the neighbors better\nthan this current configuration and have some way of measuring that,\nthen there is an applicable case for these hill climbing, simulated annealing\ntypes of algorithms.\nSo sometimes it can be for facility location type problems,\nlike for when you're trying to plan a city and figure out\nwhere the hospitals should be.\nBut there are definitely other applications as well.\nAnd one of the most famous problems in computer science\nis the traveling salesman problem.\nTraveling salesman problem generally is formulated like this.\nI have a whole bunch of cities here indicated by these dots.\nAnd what I'd like to do is find some route that\ntakes me through all of the cities and ends up back where I started.\nSo some route that starts here, goes through all these cities,\nand ends up back where I originally started.\nAnd what I might like to do is minimize the total distance\nthat I have to travel or the total cost of taking this entire path.\nAnd you can imagine this is a problem that's very applicable in situations\nlike when delivery companies are trying to deliver things\nto a whole bunch of different houses, they\nwant to figure out, how do I get from the warehouse\nto all these various different houses and get back again,\nall using as minimal time and distance and energy as possible.\nSo you might want to try to solve these sorts of problems.\nBut it turns out that solving this particular kind of problem\nis very computationally difficult.\nIt is a very computationally expensive task to be able to figure it out.\nThis falls under the category of what are known as NP-complete problems,\nproblems that there is no known efficient way to try and solve\nthese sorts of problems.\nAnd so what we ultimately have to do is come up with some approximation,\nsome ways of trying to find a good solution, even if we're not\ngoing to find the globally best solution that we possibly can,\nat least not in a feasible or tractable amount of time.\nAnd so what we could do is take the traveling salesman problem\nand try to formulate it using local search and ask a question like, all right,\nI can pick some state, some configuration, some route between all\nof these nodes.\nAnd I can measure the cost of that state, figure out what the distance is.\nAnd I might now want to try to minimize that cost as much as possible.\nAnd then the only question now is, what does it\nmean to have a neighbor of this state?\nWhat does it mean to take this particular route\nand have some neighboring route that is close to it but slightly different\nand such that it might have a different total distance?\nAnd there are a number of different definitions\nfor what a neighbor of a traveling salesman configuration might look like.\nBut one way is just to say, a neighbor is\nwhat happens if we pick two of these edges between nodes\nand switch them effectively.\nSo for example, I might pick these two edges here,\nthese two that just happened across this node goes here, this node goes there,\nand go ahead and switch them.\nAnd what that process will generally look like\nis removing both of these edges from the graph, taking this node,\nand connecting it to the node it wasn't connected to.\nSo connecting it up here instead.\nWe'll need to take these arrows that were originally\ngoing this way and reverse them, so move them going the other way,\nand then just fill in that last remaining blank,\nadd an arrow that goes in that direction instead.\nSo by taking two edges and just switching them,\nI have been able to consider one possible neighbor\nof this particular configuration.\nAnd it looks like this neighbor is actually better.\nIt looks like this probably travels a shorter distance in order\nto get through all the cities through this route\nthan the current state did.\nAnd so you could imagine implementing this idea inside of a hill climbing\nor simulated annealing algorithm, where we repeat this process\nto try and take a state of this traveling salesman problem,\nlook at all the neighbors, and then move to the neighbors if they're better,\nor maybe even move to the neighbors if they're worse,\nuntil we eventually settle upon some best solution\nthat we've been able to find.\nAnd it turns out that these types of approximation algorithms,\neven if they don't always find the very best solution,\ncan often do pretty well at trying to find solutions that are helpful too.\nSo that then was a look at local search, a particular category of algorithms\nthat can be used for solving a particular type of problem,\nwhere we don't really care about the path to the solution.\nI didn't care about the steps I took to decide\nwhere the hospitals should go.\nI just cared about the solution itself.\nI just care about where the hospitals should be,\nor what the route through the traveling salesman journey really ought to be.\nAnother type of algorithm that might come up\nare known as these categories of linear programming types of problems.\nAnd linear programming often comes up in the context\nwhere we're trying to optimize for some mathematical function.\nBut oftentimes, linear programming will come up\nwhen we might have real numbered values.\nSo it's not just discrete fixed values that we might have,\nbut any decimal values that we might want to be able to calculate.\nAnd so linear programming is a family of types of problems\nwhere we might have a situation that looks like this, where\nthe goal of linear programming is to minimize a cost function.\nAnd you can invert the numbers and say try and maximize it,\nbut often we'll frame it as trying to minimize a cost function that\nhas some number of variables, x1, x2, x3, all the way up to xn,\njust some number of variables that are involved,\nthings that I want to know the values to.\nAnd this cost function might have coefficients\nin front of those variables.\nAnd this is what we would call a linear equation,\nwhere we just have all of these variables that might be multiplied\nby a coefficient and then add it together.\nWe're not going to square anything or cube anything,\nbecause that'll give us different types of equations.\nWith linear programming, we're just dealing with linear equations\nin addition to linear constraints, where a constraint is going\nto look something like if we sum up this particular equation that\nis just some linear combination of all of these variables,\nit is less than or equal to some bound b.\nAnd we might have a whole number of these various different constraints\nthat we might place onto our linear programming exercise.\nAnd likewise, just as we can have constraints that are saying this linear\nequation is less than or equal to some bound b,\nit might also be equal to something.\nThat if you want some sum of some combination of variables\nto be equal to a value, you can specify that.\nAnd we can also maybe specify that each variable has lower and upper bounds,\nthat it needs to be a positive number, for example,\nor it needs to be a number that is less than 50, for example.\nAnd there are a number of other choices that we\ncan make there for defining what the bounds of a variable are.\nBut it turns out that if you can take a problem\nand formulate it in these terms, formulate the problem as your goal\nis to minimize a cost function, and you're\nminimizing that cost function subject to particular constraints,\nsubjects to equations that are of the form like this of some sequence\nof variables is less than a bound or is equal to some particular value,\nthen there are a number of algorithms that already\nexist for solving these sorts of problems.\nSo let's go ahead and take a look at an example.\nHere's an example of a problem that might come up\nin the world of linear programming.\nOften, this is going to come up when we're\ntrying to optimize for something.\nAnd we want to be able to do some calculations,\nand we have constraints on what we're trying to optimize.\nAnd so it might be something like this.\nIn the context of a factory, we have two machines, x1 and x2.\nx1 costs $50 an hour to run.\nx2 costs $80 an hour to run.\nAnd our goal, what we're trying to do, our objective,\nis to minimize the total cost.\nSo that's what we'd like to do.\nBut we need to do so subject to certain constraints.\nSo there might be a labor constraint that x1\nrequires five units of labor per hour, x2 requires two units of labor per hour,\nand we have a total of 20 units of labor that we have to spend.\nSo this is a constraint.\nWe have no more than 20 units of labor that we can spend,\nand we have to spend it across x1 and x2, each of which\nrequires a different amount of labor.\nAnd we might also have a constraint like this\nthat tells us x1 is going to produce 10 units of output per hour,\nx2 is going to produce 12 units of output per hour,\nand the company needs 90 units of output.\nSo we have some goal, something we need to achieve.\nWe need to achieve 90 units of output, but there are some constraints\nthat x1 can only produce 10 units of output per hour,\nx2 produces 12 units of output per hour.\nThese types of problems come up quite frequently,\nand you can start to notice patterns in these types of problems,\nproblems where I am trying to optimize for some goal, minimizing cost,\nmaximizing output, maximizing profits, or something like that.\nAnd there are constraints that are placed on that process.\nAnd so now we just need to formulate this problem\nin terms of linear equations.\nSo let's start with this first point.\nTwo machines, x1 and x2, x costs $50 an hour, x2 costs $80 an hour.\nHere we can come up with an objective function that might look like this.\nThis is our cost function, rather.\n50 times x1 plus 80 times x2, where x1 is going\nto be a variable representing how many hours do we run machine x1 for,\nx2 is going to be a variable representing how many hours\nare we running machine x2 for.\nAnd what we're trying to minimize is this cost function, which\nis just how much it costs to run each of these machines per hour summed up.\nThis is an example of a linear equation, just some combination\nof these variables plus coefficients that are placed in front of them.\nAnd I would like to minimize that total value.\nBut I need to do so subject to these constraints.\nx1 requires 50 units of labor per hour, x2 requires 2,\nand we have a total of 20 units of labor to spend.\nAnd so that gives us a constraint of this form.\n5 times x1 plus 2 times x2 is less than or equal to 20.\n20 is the total number of units of labor we have to spend.\nAnd that's spent across x1 and x2, each of which\nrequires a different number of units of labor per hour, for example.\nAnd finally, we have this constraint here.\nx1 produces 10 units of output per hour, x2 produces 12,\nand we need 90 units of output.\nAnd so this might look something like this.\nThat 10x1 plus 12x2, this is amount of output per hour,\nit needs to be at least 90.\nWe can do better or great, but it needs to be at least 90.\nAnd if you recall from my formulation before,\nI said that generally speaking in linear programming,\nwe deal with equals constraints or less than or equal to constraints.\nSo we have a greater than or equal to sign here.\nThat's not a problem.\nWhenever we have a greater than or equal to sign,\nwe can just multiply the equation by negative 1,\nand that'll flip it around to a less than or equals negative 90,\nfor example, instead of a greater than or equal to 90.\nAnd that's going to be an equivalent expression\nthat we can use to represent this problem.\nSo now that we have this cost function and these constraints\nthat it's subject to, it turns out there are a number of algorithms\nthat can be used in order to solve these types of problems.\nAnd these problems go a little bit more into geometry and linear algebra\nthan we're really going to get into.\nBut the most popular of these types of algorithms\nare simplex, which was one of the first algorithms discovered\nfor trying to solve linear programs.\nAnd later on, a class of interior point algorithms\ncan be used to solve this type of problem as well.\nThe key is not to understand exactly how these algorithms work,\nbut to realize that these algorithms exist for efficiently finding solutions\nany time we have a problem of this particular form.\nAnd so we can take a look, for example, at the production directory here,\nwhere here I have a file called production.py, where here I'm\nusing scipy, which was the library for a lot of science-related functions\nwithin Python.\nAnd I can go ahead and just run this optimization function\nin order to run a linear program.\n.linprog here is going to try and solve this linear program for me,\nwhere I provide to this expression, to this function call,\nall of the data about my linear program.\nSo it needs to be in a particular format, which\nmight be a little confusing at first.\nBut this first argument to scipy.optimize.linprogramming\nis the cost function, which is in this case just an array or a list that\nhas 50 and 80, because my original cost function was 50 times x1 plus 80\ntimes x2.\nSo I just tell Python, 50 and 80, those are the coefficients\nthat I am now trying to optimize for.\nAnd then I provide all of the constraints.\nSo the constraints, and I wrote them up above in comments,\nis the constraint 1 is 5x1 plus 2x2 is less than or equal to 20.\nAnd constraint 2 is negative 10x1 plus negative 12x2\nis less than or equal to negative 90.\nAnd so scipy expects these constraints to be in a particular format.\nIt first expects me to provide all of the coefficients\nfor the upper bound equations, ub just for upper bound,\nwhere the coefficients of the first equation\nare 5 and 2, because we have 5x1 and 2x2.\nAnd the coefficients for the second equation\nare negative 10 and negative 12, because I have negative 10x1 plus negative 12x2.\nAnd then here, we provide it as a separate argument,\njust to keep things separate, what the actual bound is.\nWhat is the upper bound for each of these constraints?\nWell, for the first constraint, the upper bound is 20.\nThat was constraint number 1.\nAnd then for constraint number 2, the upper bound is 90.\nSo a bit of a cryptic way of representing it.\nIt's not quite as simple as just writing the mathematical equations.\nWhat really is being expected here are all of the coefficients\nand all of the numbers that are in these equations\nby first providing the coefficients for the cost function,\nthen providing all the coefficients for the inequality constraints,\nand then providing all of the upper bounds for those inequality constraints.\nAnd once all of that information is there,\nthen we can run any of these interior point algorithms or the simplex algorithm.\nEven if you don't understand how it works,\nyou can just run the function and figure out what the result should be.\nAnd here, I said if the result is a success,\nwe were able to solve this problem.\nGo ahead and print out what the value of x1 and x2 should be.\nOtherwise, go ahead and print out no solution.\nAnd so if I run this program by running python production.py,\nit takes a second to calculate.\nBut then we see here is what the optimal solution should be.\nx1 should run for 1.5 hours.\nx2 should run for 6.25 hours.\nAnd we were able to do this by just formulating the problem\nas a linear equation that we were trying to optimize,\nsome cost that we were trying to minimize,\nand then some constraints that were placed on that.\nAnd many, many problems fall into this category of problems\nthat you can solve if you can just figure out how to use equations\nand use these constraints to represent that general idea.\nAnd that's a theme that's going to come up a couple of times today,\nwhere we want to be able to take some problem\nand reduce it down to some problem we know\nhow to solve in order to begin to find a solution\nand to use existing methods that we can use in order\nto find a solution more effectively or more efficiently.\nAnd it turns out that these types of problems, where we have constraints,\nshow up in other ways too.\nAnd there's an entire class of problems that's more generally just known\nas constraint satisfaction problems.\nAnd we're going to now take a look at how you might formulate a constraint\nsatisfaction problem and how you might go about solving a constraint\nsatisfaction problem.\nBut the basic idea of a constraint satisfaction problem\nis we have some number of variables that need to take on some values.\nAnd we need to figure out what values each of those variables should take on.\nBut those variables are subject to particular constraints\nthat are going to limit what values those variables can actually take on.\nSo let's take a look at a real world example, for example.\nLet's look at exam scheduling, that I have\nfour students here, students 1, 2, 3, and 4.\nEach of them is taking some number of different classes.\nClasses here are going to be represented by letters.\nSo student 1 is enrolled in courses A, B, and C. Student 2\nis enrolled in courses B, D, and E, so on and so forth.\nAnd now, say university, for example, is trying\nto schedule exams for all of these courses.\nBut there are only three exam slots on Monday, Tuesday, and Wednesday.\nAnd we have to schedule an exam for each of these courses.\nBut the constraint now, the constraint we\nhave to deal with with the scheduling, is\nthat we don't want anyone to have to take two exams on the same day.\nWe would like to try and minimize that or eliminate it if at all possible.\nSo how do we begin to represent this idea?\nHow do we structure this in a way that a computer with an AI algorithm\ncan begin to try and solve the problem?\nWell, let's in particular just look at these classes that we might take\nand represent each of the courses as some node inside of a graph.\nAnd what we'll do is we'll create an edge between two nodes in this graph\nif there is a constraint between those two nodes.\nSo what does this mean?\nWell, we can start with student 1, who's enrolled in courses A, B, and C.\nWhat that means is that A and B can't have an exam at the same time.\nA and C can't have an exam at the same time.\nAnd B and C also can't have an exam at the same time.\nAnd I can represent that in this graph by just drawing edges.\nOne edge between A and B, one between B and C,\nand then one between C and A. And that encodes now the idea\nthat between those nodes, there is a constraint.\nAnd in particular, the constraint happens to be\nthat these two can't be equal to each other,\nthough there are other types of constraints that are possible,\ndepending on the type of problem that you're trying to solve.\nAnd then we can do the same thing for each of the other students.\nSo for student 2, who's enrolled in courses B, D, and E,\nwell, that means B, D, and E, those all need\nto have edges that connect each other as well.\nStudent 3 is enrolled in courses C, E, and F. So we'll go ahead\nand take C, E, and F and connect those by drawing edges between them too.\nAnd then finally, student 4 is enrolled in courses E, F, and G.\nAnd we can represent that by drawing edges between E, F, and G,\nalthough E and F already had an edge between them.\nWe don't need another one, because this constraint\nis just encoding the idea that course E and course F cannot have\nan exam on the same day.\nSo this then is what we might call the constraint graph.\nThere's some graphical representation of all of my variables,\nso to speak, and the constraints between those possible variables.\nWhere in this particular case, each of the constraints\nrepresents an inequality constraint, that an edge between B and D\nmeans whatever value the variable B takes on cannot be the value\nthat the variable D takes on as well.\nSo what then actually is a constraint satisfaction problem?\nWell, a constraint satisfaction problem is just some set of variables, x1\nall the way through xn, some set of domains for each of those variables.\nSo every variable needs to take on some values.\nMaybe every variable has the same domain,\nbut maybe each variable has a slightly different domain.\nAnd then there's a set of constraints, and we'll just call a set C,\nthat is some constraints that are placed upon these variables,\nlike x1 is not equal to x2.\nBut there could be other forms too, like maybe x1 equals x2 plus 1\nif these variables are taking on numerical values in their domain,\nfor example.\nThe types of constraints are going to vary based on the types of problems.\nAnd constraint satisfaction shows up all over the place as well,\nin any situation where we have variables that\nare subject to particular constraints.\nSo one popular game is Sudoku, for example, this 9 by 9 grid\nwhere you need to fill in numbers in each of these cells,\nbut you want to make sure there's never a duplicate number in any row,\nor in any column, or in any grid of 3 by 3 cells, for example.\nSo what might this look like as a constraint satisfaction problem?\nWell, my variables are all of the empty squares in the puzzle.\nSo represented here is just like an x comma y coordinate, for example,\nas all of the squares where I need to plug in a value,\nwhere I don't know what value it should take on.\nThe domain is just going to be all of the numbers from 1 through 9,\nany value that I could fill in to one of these cells.\nSo that is going to be the domain for each of these variables.\nAnd then the constraints are going to be of the form,\nlike this cell can't be equal to this cell, can't be equal to this cell,\ncan't be, and all of these need to be different, for example,\nand same for all of the rows, and the columns, and the 3 by 3 squares as well.\nSo those constraints are going to enforce what values are actually allowed.\nAnd we can formulate the same idea in the case of this exam scheduling\nproblem, where the variables we have are the different courses, a up through g.\nThe domain for each of these variables is going to be Monday, Tuesday,\nand Wednesday.\nThose are the possible values each of the variables can take on,\nthat in this case just represent when is the exam for that class.\nAnd then the constraints are of this form, a is not equal to b,\na is not equal to c, meaning a and b can't have an exam on the same day,\na and c can't have an exam on the same day.\nOr more formally, these two variables cannot take on the same value\nwithin their domain.\nSo that then is this formulation of a constraint satisfaction problem\nthat we can begin to use to try and solve this problem.\nAnd constraints can come in a number of different forms.\nThere are hard constraints, which are constraints\nthat must be satisfied for a correct solution.\nSo something like in the Sudoku puzzle, you cannot have this cell\nand this cell that are in the same row take on the same value.\nThat is a hard constraint.\nBut problems can also have soft constraints,\nwhere these are constraints that express some notion of preference,\nthat maybe a and b can't have an exam on the same day,\nbut maybe someone has a preference that a's exam is earlier than b's exam.\nIt doesn't need to be the case with some expression\nthat some solution is better than another solution.\nAnd in that case, you might formulate the problem\nas trying to optimize for maximizing people's preferences.\nYou want people's preferences to be satisfied as much as possible.\nIn this case, though, we'll mostly just deal with hard constraints,\nconstraints that must be met in order to have a correct solution to the problem.\nSo we want to figure out some assignment of these variables\nto their particular values that is ultimately\ngoing to give us a solution to the problem\nby allowing us to assign some day to each of the classes\nsuch that we don't have any conflicts between classes.\nSo it turns out that we can classify the constraints\nin a constraint satisfaction problem into a number of different categories.\nThe first of those categories are perhaps the simplest\nof the types of constraints, which are known as unary constraints,\nwhere unary constraint is a constraint that just involves a single variable.\nFor example, a unary constraint might be something like,\na does not equal Monday, meaning Course A cannot have its exam on Monday.\nIf for some reason the instructor for the course\nisn't available on Monday, you might have a constraint in your problem\nthat looks like this, something that just has a single variable a in it,\nand maybe says a is not equal to Monday, or a is equal to something,\nor in the case of numbers greater than or less than something,\na constraint that just has one variable, we consider to be a unary constraint.\nAnd this is in contrast to something like a binary constraint, which\nis a constraint that involves two variables, for example.\nSo this would be a constraint like the ones we were looking at before.\nSomething like a does not equal b is an example of a binary constraint,\nbecause it is a constraint that has two variables involved in it, a and b.\nAnd we represented that using some arc or some edge that\nconnects variable a to variable b.\nAnd using this knowledge of, OK, what is a unary constraint?\nWhat is a binary constraint?\nThere are different types of things we can\nsay about a particular constraint satisfaction problem.\nAnd one thing we can say is we can try and make the problem node consistent.\nSo what does node consistency mean?\nNode consistency means that we have all of the values\nin a variable's domain satisfying that variable's unary constraints.\nSo for each of the variables inside of our constraint satisfaction problem,\nif all of the values satisfy the unary constraints\nfor that particular variable, we can say that the entire problem is node\nconsistent, or we can even say that a particular variable is\nnode consistent if we just want to make one node consistent within itself.\nSo what does that actually look like?\nLet's look at now a simplified example, where\ninstead of having a whole bunch of different classes,\nwe just have two classes, a and b, each of which\nhas an exam on either Monday or Tuesday or Wednesday.\nSo this is the domain for the variable a,\nand this is the domain for the variable b.\nAnd now let's imagine we have these constraints, a not equal to Monday,\nb not equal to Tuesday, b not equal to Monday, a not equal to b.\nSo those are the constraints that we have on this particular problem.\nAnd what we can now try to do is enforce node consistency.\nAnd node consistency just means we make sure\nthat all of the values for any variable's domain satisfy its unary constraints.\nAnd so we could start by trying to make node a node consistent.\nIs it consistent?\nDoes every value inside of a's domain satisfy its unary constraints?\nWell, initially, we'll see that Monday does not satisfy a's unary constraints,\nbecause we have a constraint, a unary constraint here,\nthat a is not equal to Monday.\nBut Monday is still in a's domain.\nAnd so this is something that is not node consistent,\nbecause we have Monday in the domain.\nBut this is not a valid value for this particular node.\nAnd so how do we make this node consistent?\nWell, to make the node consistent, what we'll do\nis we'll just go ahead and remove Monday from a's domain.\nNow a can only be on Tuesday or Wednesday,\nbecause we had this constraint that said a is not equal to Monday.\nAnd at this point now, a is node consistent.\nFor each of the values that a can take on, Tuesday and Wednesday,\nthere is no constraint that is a unary constraint that conflicts with that idea.\nThere is no constraint that says that a can't be Tuesday.\nThere is no unary constraint that says that a cannot be on Wednesday.\nAnd so now we can turn our attention to b.\nb also has a domain, Monday, Tuesday, and Wednesday.\nAnd we can begin to see whether those variables satisfy\nthe unary constraints as well.\nWell, here is a unary constraint, b is not equal to Tuesday.\nAnd that does not appear to be satisfied by this domain of Monday, Tuesday,\nand Wednesday, because Tuesday, this possible value\nthat the variable b could take on is not consistent with this unary constraint,\nthat b is not equal to Tuesday.\nSo to solve that problem, we'll go ahead and remove Tuesday from b's domain.\nNow b's domain only contains Monday and Wednesday.\nBut as it turns out, there's yet another unary constraint\nthat we placed on the variable b, which is here.\nb is not equal to Monday.\nAnd that means that this value, Monday, inside of b's domain,\nis not consistent with b's unary constraints,\nbecause we have a constraint that says the b cannot be Monday.\nAnd so we can remove Monday from b's domain.\nAnd now we've made it through all of the unary constraints.\nWe've not yet considered this constraint, which is a binary constraint.\nBut we've considered all of the unary constraints,\nall of the constraints that involve just a single variable.\nAnd we've made sure that every node is consistent with those unary constraints.\nSo we can say that now we have enforced node consistency,\nthat for each of these possible nodes, we can pick any of these values\nin the domain.\nAnd there won't be a unary constraint that is violated as a result of it.\nSo node consistency is fairly easy to enforce.\nWe just take each node, make sure the values in the domain\nsatisfy the unary constraints.\nWhere things get a little bit more interesting\nis when we consider different types of consistency,\nsomething like arc consistency, for example.\nAnd arc consistency refers to when all of the values in a variable's domain\nsatisfy the variable's binary constraints.\nSo when we're looking at trying to make a arc consistent,\nwe're no longer just considering the unary constraints that involve a.\nWe're trying to consider all of the binary constraints\nthat involve a as well.\nSo any edge that connects a to another variable\ninside of that constraint graph that we were taking a look at before.\nPut a little bit more formally, arc consistency.\nAnd arc really is just another word for an edge\nthat connects two of these nodes inside of our constraint graph.\nWe can define arc consistency a little more precisely like this.\nIn order to make some variable x arc consistent with respect\nto some other variable y, we need to remove any element from x's domain\nto make sure that every choice for x, every choice in x's domain,\nhas a possible choice for y.\nSo put another way, if I have a variable x\nand I want to make x an arc consistent, then\nI'm going to look at all of the possible values that x can take on\nand make sure that for all of those possible values,\nthere is still some choice that I can make for y,\nif there's some arc between x and y, to make sure\nthat y has a possible option that I can choose as well.\nSo let's look at an example of that going back to this example from before.\nWe enforced node consistency already by saying\nthat a can only be on Tuesday or Wednesday\nbecause we knew that a could not be on Monday.\nAnd we also said that b's only domain only\nconsists of Wednesday because we know that b does not equal Tuesday\nand also b does not equal Monday.\nSo now let's begin to consider arc consistency.\nLet's try and make a arc consistent with b.\nAnd what that means is to make a arc consistent with respect to b\nmeans that for any choice we make in a's domain,\nthere is some choice we can make in b's domain that is going to be consistent.\nAnd we can try that.\nFor a, we can choose Tuesday as a possible value for a.\nIf I choose Tuesday for a, is there a value\nfor b that satisfies the binary constraint?\nWell, yes, b Wednesday would satisfy this constraint\nthat a does not equal b because Tuesday does not equal Wednesday.\nHowever, if we chose Wednesday for a, well, then\nthere is no choice in b's domain that satisfies this binary constraint.\nThere is no way I can choose something for b that satisfies a does not equal b\nbecause I know b must be Wednesday.\nAnd so if ever I run into a situation like this\nwhere I see that here is a possible value for a such\nthat there is no choice of value for b that satisfies the binary constraint,\nwell, then this is not arc consistent.\nAnd to make it arc consistent, I would need to take Wednesday\nand remove it from a's domain.\nBecause Wednesday was not going to be a possible choice I can make for a\nbecause it wasn't consistent with this binary constraint for b.\nThere was no way I could choose Wednesday for a\nand still have an available solution by choosing something for b as well.\nSo here now, I've been able to enforce arc consistency.\nAnd in doing so, I've actually solved this entire problem,\nthat given these constraints where a and b can have exams on either Monday\nor Tuesday or Wednesday, the only solution, as it would appear,\nis that a's exam must be on Tuesday and b's exam must be on Wednesday.\nAnd that is the only option available to me.\nSo if we want to apply our consistency to a larger graph,\nnot just looking at one particular pair of our consistency,\nthere are ways we can do that too.\nAnd we can begin to formalize what the pseudocode would look like\nfor trying to write an algorithm that enforces arc consistency.\nAnd we'll start by defining a function called revise.\nRevise is going to take as input a CSP, otherwise\nknown as a constraint satisfaction problem,\nand also two variables, x and y.\nAnd what revise is going to do is it is going\nto make x arc consistent with respect to y,\nmeaning remove anything from x's domain that\ndoesn't allow for a possible option for y.\nHow does this work?\nWell, we'll go ahead and first keep track of whether or not\nwe've made a revision.\nRevise is ultimately going to return true or false.\nIt'll return true in the event that we did make a revision to x's domain.\nIt'll return false if we didn't make any change to x's domain.\nAnd we'll see in a moment why that's going to be helpful.\nBut we start by saying revised equals false.\nWe haven't made any changes.\nThen we'll say, all right, let's go ahead and loop over all\nof the possible values in x's domain.\nSo loop over x's domain for each little x in x's domain.\nI want to make sure that for each of those choices,\nI have some available choice in y that satisfies the binary constraints that\nare defined inside of my CSP, inside of my constraint\nsatisfaction problem.\nSo if ever it's the case that there is no value y in y's domain that\nsatisfies the constraint for x and y, well, if that's the case,\nthat means that this value x shouldn't be in x's domain.\nSo we'll go ahead and delete x from x's domain.\nAnd I'll set revised equal to true because I did change x's domain.\nI changed x's domain by removing little x.\nAnd I removed little x because it wasn't art consistent.\nThere was no way I could choose a value for y\nthat would satisfy this xy constraint.\nSo in this case, we'll go ahead and set revised equal true.\nAnd we'll do this again and again for every value in x's domain.\nSometimes it might be fine.\nIn other cases, it might not allow for a possible choice for y,\nin which case we need to remove this value from x's domain.\nAnd at the end, we just return revised to indicate whether or not\nwe actually made a change.\nSo this function, then, this revised function\nis effectively an implementation of what you saw me do graphically a moment ago.\nAnd it makes one variable, x, arc consistent with another variable,\nin this case, y.\nBut generally speaking, when we want to enforce our consistency,\nwe'll often want to enforce our consistency not just for a single arc,\nbut for the entire constraint satisfaction problem.\nAnd it turns out there's an algorithm to do that as well.\nAnd that algorithm is known as AC3.\nAC3 takes a constraint satisfaction problem.\nAnd it enforces our consistency across the entire problem.\nHow does it do that?\nWell, it's going to basically maintain a queue or basically just a line\nof all of the arcs that it needs to make consistent.\nAnd over time, we might remove things from that queue\nas we begin dealing with our consistency.\nAnd we might need to add things to that queue as well\nif there are more things we need to make arc consistent.\nSo we'll go ahead and start with a queue that\ncontains all of the arcs in the constraint satisfaction problem,\nall of the edges that connect two nodes that\nhave some sort of binary constraint between them.\nAnd now, as long as the queue is non-empty, there is work to be done.\nThe queue is all of the things that we need to make arc consistent.\nSo as long as the queue is non-empty, there's still things we have to do.\nWhat do we have to do?\nWell, we'll start by de-queuing from the queue,\nremove something from the queue.\nAnd strictly speaking, it doesn't need to be a queue,\nbut a queue is a traditional way of doing this.\nWe'll de-queue from the queue, and that'll give us an arc, x and y,\nthese two variables where I would like to make x arc consistent with y.\nSo how do we make x arc consistent with y?\nWell, we can go ahead and just use that revise function\nthat we talked about a moment ago.\nWe called the revise function, passing as input the constraint satisfaction\nproblem, and also these variables x and y,\nbecause I want to make x arc consistent with y.\nIn other words, remove any values from x's domain\nthat don't leave an available option for y.\nAnd recall, what does revised return?\nWell, it returns true if we actually made a change,\nif we removed something from x's domain, because there\nwasn't an available option for y, for example.\nAnd it returns false if we didn't make any change to x's domain at all.\nAnd it turns out if revised returns false, if we didn't make any changes,\nwell, then there's not a whole lot more work\nto be done here for this arc.\nWe can just move ahead to the next arc that's in the queue.\nBut if we did make a change, if we did reduce x's domain\nby removing values from x's domain, well, then what we might realize\nis that this creates potential problems later on,\nthat it might mean that some arc that was arc consistent with x,\nthat node might no longer be arc consistent with x,\nbecause while there used to be an option that we could choose for x,\nnow there might not be, because now we might have removed something\nfrom x that was necessary for some other arc to be arc consistent.\nAnd so if ever we did revise x's domain,\nwe're going to need to add some things to the queue, some additional arcs\nthat we might want to check.\nHow do we do that?\nWell, first thing we want to check is to make sure that x's domain is not 0.\nIf x's domain is 0, that means there are no available options for x at all.\nAnd that means that there's no way you can solve the constraint satisfaction\nproblem.\nIf we've removed everything from x's domain,\nwe'll go ahead and just return false here to indicate there's\nno way to solve the problem, because there's nothing left in x's domain.\nBut otherwise, if there are things left in x's domain,\nbut fewer things than before, well, then what we'll do\nis we'll loop over each variable z that is in all of x's neighbors,\nexcept for y, y we already handled.\nBut we'll consider all of x's other's neighbors and ask ourselves,\nall right, will that arc from each of those z's to x,\nthat arc might no longer be arc consistent,\nbecause while for each z, there might have been a possible option\nwe could choose for x to correspond with each of z's possible values,\nnow there might not be, because we removed some elements from x's domain.\nAnd so what we'll do here is we'll go ahead and enqueue,\nadding something to the queue, this arc zx for all of those neighbors z.\nSo we need to add back some arcs to the queue\nin order to continue to enforce arc consistency.\nAt the very end, if we make it through all this process,\nthen we can return true.\nBut this now is AC3, this algorithm for enforcing arc consistency\non a constraint satisfaction problem.\nAnd the big idea is really just keep track of all of the arcs\nthat we might need to make arc consistent,\nmake it arc consistent by calling the revise function.\nAnd if we did revise it, then there are some new arcs\nthat might need to be added to the queue in order\nto make sure that everything is still arc consistent, even\nafter we've removed some of the elements from a particular variable's\ndomain.\nSo what then would happen if we tried to enforce arc consistency\non a graph like this, on a graph where each of these variables\nhas a domain of Monday, Tuesday, and Wednesday?\nWell, it turns out that by enforcing arc consistency on this graph,\nwell, it can solve some types of problems.\nNothing actually changes here.\nFor any particular arc, just considering two variables,\nthere's always a way for me to just, for any of the choices\nI make for one of them, make a choice for the other one,\nbecause there are three options, and I just need the two\nto be different from each other.\nSo this is actually quite easy to just take an arc\nand just declare that it is arc consistent,\nbecause if I pick Monday for D, then I just\npick something that isn't Monday for B. In arc consistency,\nwe only consider consistency between a binary constraint between two nodes,\nand we're not really considering all of the rest of the nodes yet.\nSo just using AC3, the enforcement of arc consistency,\nthat can sometimes have the effect of reducing domains\nto make it easier to find solutions, but it will not always actually\nsolve the problem.\nWe might still need to somehow search to try and find a solution.\nAnd we can use classical traditional search algorithms to try to do so.\nYou'll recall that a search problem generally consists of these parts.\nWe have some initial state, some actions, a transition model\nthat takes me from one state to another state,\na goal test to tell me have I satisfied my objective correctly,\nand then some path cost function, because in the case of like maze solving,\nI was trying to get to my goal as quickly as possible.\nSo you could formulate a CSP, or a constraint satisfaction problem,\nas one of these types of search problems.\nThe initial state will just be an empty assignment,\nwhere an assignment is just a way for me to assign any particular variable\nto any particular value.\nSo if an empty assignment is no variables that are assigned to any values\nyet, then the action I can take is adding some new variable equals value\npair to that assignment, saying for this assignment,\nlet me add a new value for this variable.\nAnd the transition model just defines what happens when you take that action.\nYou get a new assignment that has that variable equal to that value inside\nof it.\nThe goal test is just checking to make sure all the variables have been assigned\nand making sure all the constraints have been satisfied.\nAnd the path cost function is sort of irrelevant.\nI don't really care about what the path really is.\nI just care about finding some assignment that actually satisfies\nall of the constraints.\nSo really, all the paths have the same cost.\nI don't really care about the path to the goal.\nI just care about the solution itself, much as we've talked about now before.\nThe problem here, though, is that if we just implement this naive search\nalgorithm just by implementing like breadth-first search or depth-first\nsearch, this is going to be very, very inefficient.\nAnd there are ways we can take advantage of efficiencies\nin the structure of a constraint satisfaction problem itself.\nAnd one of the key ideas is that we can really just order these variables.\nAnd it doesn't matter what order we assign variables in.\nThe assignment a equals 2 and then b equals 8\nis identical to the assignment of b equals 8 and then a equals 2.\nSwitching the order doesn't really change anything\nabout the fundamental nature of that assignment.\nAnd so there are some ways that we can try and revise\nthis idea of a search algorithm to apply it specifically\nfor a problem like a constraint satisfaction problem.\nAnd it turns out the search algorithm we'll generally\nuse when talking about constraint satisfaction problems\nis something known as backtracking search.\nAnd the big idea of backtracking search is we'll\ngo ahead and make assignments from variables to values.\nAnd if ever we get stuck, we arrive at a place\nwhere there is no way we can make any forward progress while still\npreserving the constraints that we need to enforce,\nwe'll go ahead and backtrack and try something else instead.\nSo the very basic sketch of what backtracking search looks like\nis it looks like this.\nFunction called backtrack that takes as input an assignment\nand a constraint satisfaction problem.\nSo initially, we don't have any assigned variables.\nSo when we begin backtracking search, this assignment\nis just going to be the empty assignment with no variables inside of it.\nBut we'll see later this is going to be a recursive function.\nSo backtrack takes as input the assignment and the problem.\nIf the assignment is complete, meaning all of the variables have been assigned,\nwe just return that assignment.\nThat, of course, won't be true initially,\nbecause we start with an empty assignment.\nBut over time, we might add things to that assignment.\nSo if ever the assignment actually is complete, then we're done.\nThen just go ahead and return that assignment.\nBut otherwise, there is some work to be done.\nSo what we'll need to do is select an unassigned variable\nfor this particular problem.\nSo we need to take the problem, look at the variables that have already\nbeen assigned, and pick a variable that has not yet been assigned.\nAnd I'll go ahead and take that variable.\nAnd then I need to consider all of the values in that variable's domain.\nSo we'll go ahead and call this domain values function.\nWe'll talk a little more about that later, that takes a variable\nand just gives me back an ordered list of all of the values in its domain.\nSo I've taken a random unselected variable.\nI'm going to loop over all of the possible values.\nAnd the idea is, let me just try all of these values\nas possible values for the variable.\nSo if the value is consistent with the assignment so far,\nit doesn't violate any of the constraints,\nwell then let's go ahead and add variable equals value to the assignment\nbecause it's so far consistent.\nAnd now let's recursively call backtrack to try and make\nthe rest of the assignments also consistent.\nSo I'll go ahead and call backtrack on this new assignment\nthat I've added the variable equals value to.\nAnd now I recursively call backtrack and see what the result is.\nAnd if the result isn't a failure, well then let me just return that result.\nAnd otherwise, what else could happen?\nWell, if it turns out the result was a failure, well then\nthat means this value was probably a bad choice\nfor this particular variable because when I assigned\nthis variable equal to that value, eventually down the road\nI ran into a situation where I violated constraints.\nThere was nothing more I could do.\nSo now I'll remove variable equals value from the assignment,\neffectively backtracking to say, all right, that value didn't work.\nLet's try another value instead.\nAnd then at the very end, if we were never\nable to return a complete assignment, we'll just go ahead and return failure\nbecause that means that none of the values worked for this particular\nvariable.\nThis now is the idea for backtracking search,\nto take each of the variables, try values for them,\nand recursively try backtracking search, see if we can make progress.\nAnd if ever we run into a dead end, we run\ninto a situation where there is no possible value we can choose\nthat satisfies the constraints, we return failure.\nAnd that propagates up, and eventually we\nmake a different choice by going back and trying something else instead.\nSo let's put this algorithm into practice.\nLet's actually try and use backtracking search to solve this problem now,\nwhere I need to figure out how to assign each of these courses\nto an exam slot on Monday or Tuesday or Wednesday in such a way\nthat it satisfies these constraints, that each of these edges\nmean those two classes cannot have an exam on the same day.\nSo I can start by just starting at a node.\nIt doesn't really matter which I start with,\nbut in this case, I'll just start with A.\nAnd I'll ask the question, all right, let me loop over the values in the domain.\nAnd maybe in this case, I'll just start with Monday and say, all right,\nlet's go ahead and assign A to Monday.\nWe'll just go and order Monday, Tuesday, Wednesday.\nAnd now let's consider node B. So I've made an assignment to A,\nso I recursively call backtrack with this new part of the assignment.\nAnd now I'm looking to pick another unassigned variable like B.\nAnd I'll say, all right, maybe I'll start with Monday,\nbecause that's the very first value in B's domain.\nAnd I ask, all right, does Monday violate any constraints?\nAnd it turns out, yes, it does.\nIt violates this constraint here between A and B,\nbecause A and B are now both on Monday, and that doesn't work,\nbecause B can't be on the same day as A. So that doesn't work.\nSo we might instead try Tuesday, try the next value in B's domain.\nAnd is that consistent with the assignment so far?\nWell, yeah, B, Tuesday, A, Monday, that is consistent so far,\nbecause they're not on the same day.\nSo that's good.\nNow we can recursively call backtrack.\nTry again.\nPick another unassigned variable, something like D, and say, all right,\nlet's go through its possible values.\nIs Monday consistent with this assignment?\nWell, yes, it is.\nB and D are on different days, Monday versus Tuesday.\nAnd A and B are also on different days, Monday versus Tuesday.\nSo that's fine so far, too.\nWe'll go ahead and try again.\nMaybe we'll go to this variable here, E. Say, can we make that consistent?\nLet's go through the possible values.\nWe've recursively called backtrack.\nWe might start with Monday and say, all right, that's not consistent,\nbecause D and E now have exams on the same day.\nSo we might try Tuesday instead, going to the next one.\nAsk, is that consistent?\nWell, no, it's not, because B and E, those have exams on the same day.\nAnd so we try, all right, is Wednesday consistent?\nAnd in turn, it's like, all right, yes, it is.\nWednesday is consistent, because D and E now\nhave exams on different days.\nB and E now have exams on different days.\nAll seems to be well so far.\nI recursively call backtrack, select another unassigned variable,\nwe'll say maybe choose C this time, and say, all right,\nlet's try the values that C could take on.\nLet's start with Monday.\nAnd it turns out that's not consistent, because now A and C both\nhave exams on the same day.\nSo I try Tuesday and say, that's not consistent either,\nbecause B and C now have exams on the same day.\nAnd then I say, all right, let's go ahead and try Wednesday.\nBut that's not consistent either, because C and E each have\nexams on the same day too.\nSo now we've gone through all the possible values for C, Monday, Tuesday,\nand Wednesday.\nAnd none of them are consistent.\nThere is no way we can have a consistent assignment.\nBacktrack, in this case, will return a failure.\nAnd so then we'd say, all right, we have to backtrack back to here.\nWell, now for E, we've tried all of Monday, Tuesday, and Wednesday.\nAnd none of those work, because Wednesday, which seemed to work,\nturned out to be a failure.\nSo that means there's no possible way we can assign E.\nSo that's a failure too.\nWe have to go back up to D, which means that Monday assignment to D,\nthat must be wrong.\nWe must try something else.\nSo we can try, all right, what if instead of Monday, we try Tuesday?\nTuesday, it turns out, is not consistent,\nbecause B and D now have an exam on the same day.\nBut Wednesday, as it turns out, works.\nAnd now we can begin to mix and forward progress again.\nWe go back to E and say, all right, which of these values works?\nMonday turns out to work by not violating any constraints.\nThen we go up to C now.\nMonday doesn't work, because it violates a constraint.\nViolates two, actually.\nTuesday doesn't work, because it violates a constraint as well.\nBut Wednesday does work.\nThen we can go to the next variable, F, and say, all right, does Monday work?\nWe'll know.\nIt violates a constraint.\nBut Tuesday does work.\nAnd then finally, we can look at the last variable, G,\nrecursively calling backtrack one more time.\nMonday is inconsistent.\nThat violates a constraint.\nTuesday also violates a constraint.\nBut Wednesday, that doesn't violate a constraint.\nAnd so now at this point, we recursively call backtrack one last time.\nWe now have a satisfactory assignment of all of the variables.\nAnd at this point, we can say that we are now done.\nWe have now been able to successfully assign a variable or a value\nto each one of these variables in such a way\nthat we're not violating any constraints.\nWe're going to go ahead and have classes A and E have their exams on Monday.\nClasses B and F can have their exams on Tuesday.\nAnd classes C, D, and G can have their exams on Wednesday.\nAnd there's no violated constraints that might come up there.\nSo that then was a graphical look at how this might work.\nLet's now take a look at some code we could use to actually try\nand solve this problem as well.\nSo here I'll go ahead and go into the scheduling directory.\nWe're here now.\nWe'll start by looking at schedule0.py.\nWe're here.\nI define a list of variables, A, B, C, D, E, F, G.\nThose are all different classes.\nThen underneath that, I define my list of constraints.\nSo constraint A and B. That is a constraint\nbecause they can't be on the same day.\nLikewise, A and C, B and C, so on and so forth,\nenforcing those exact same constraints.\nAnd here then is what the backtracking function might look like.\nFirst, if the assignment is complete, if I've\nmade an assignment of every variable to a value,\ngo ahead and just return that assignment.\nThen we'll select an unassigned variable from that assignment.\nThen for each of the possible values in the domain, Monday, Tuesday,\nWednesday, let's go ahead and create a new assignment that\nassigns the variable to that value.\nI'll call this consistent function, which I'll show you in a moment,\nthat just checks to make sure this new assignment is consistent.\nBut if it is consistent, we'll go ahead and call backtrack\nto go ahead and continue trying to run backtracking search.\nAnd as long as the result is not none, meaning it wasn't a failure,\nwe can go ahead and return that result.\nBut if we make it through all the values and nothing works, then it is a failure.\nThere's no solution.\nWe go ahead and return none here.\nWhat do these functions do?\nSelect unassigned variable is just going to choose a variable not yet assigned.\nSo it's going to loop over all the variables.\nAnd if it's not already assigned, we'll go ahead and just return that variable.\nAnd what does the consistent function do?\nWell, the consistent function goes through all the constraints.\nAnd if we have a situation where we've assigned both of those values\nto variables, but they are the same, well,\nthen that is a violation of the constraint, in which case we'll return false.\nBut if nothing is inconsistent, then the assignment is consistent\nand will return true.\nAnd then all the program does is it calls backtrack\non an empty assignment, an empty dictionary that has no variable assigned\nand no values yet, save that inside a solution,\nand then print out that solution.\nSo by running this now, I can run Python schedule0.py.\nAnd what I get as a result of that is an assignment\nof all these variables to values.\nAnd it turns out we assign a to Monday as we would expect, b to Tuesday,\nc to Wednesday, exactly the same type of thing\nwe were talking about before, an assignment of each of these variables\nto values that doesn't violate any constraints.\nAnd I had to do a fair amount of work in order\nto implement this idea myself.\nI had to write the backtrack function that went ahead\nand went through this process of recursively trying\nto do this backtracking search.\nBut it turns out the constraint satisfaction problems are so popular\nthat there exist many libraries that already implement this type of idea.\nAgain, as with before, the specific library\nis not as important as the fact that libraries do exist.\nThis is just one example of a Python constraint library,\nwhere now, rather than having to do all the work from scratch\ninside of schedule1.py, I'm just taking advantage\nof a library that implements a lot of these ideas already.\nSo here, I create a new problem, add variables to it\nwith particular domains.\nI add a whole bunch of these individual constraints,\nwhere I call addConstraint and pass in a function describing\nwhat the constraint is.\nAnd the constraint basically says the function that takes two variables, x\nand y, and makes sure that x is not equal to y,\nenforcing the idea that these two classes cannot have exams on the same day.\nAnd then, for any constraint satisfaction problem,\nI can call getSolutions to get all the solutions to that problem.\nAnd then, for each of those solutions, print out\nwhat that solution happens to be.\nAnd if I run python schedule1.py, and now see,\nthere are actually a number of different solutions\nthat can be used to solve the problem.\nThere are, in fact, six different solutions, assignments of variables\nto values that will give me a satisfactory answer to this constraint\nsatisfaction problem.\nSo this then was an implementation of a very basic backtracking search method,\nwhere really we just went through each of the variables,\npicked one that wasn't assigned, tried the possible values\nthe variable could take on.\nAnd then, if it worked, if it didn't violate any constraints,\nthen we kept trying other variables.\nAnd if ever we hit a dead end, we had to backtrack.\nBut ultimately, we might be able to be a little bit more\nintelligent about how we do this in order\nto improve the efficiency of how we solve these sorts of problems.\nAnd one thing we might imagine trying to do\nis going back to this idea of inference, using the knowledge we\nknow to be able to draw conclusions in order\nto make the rest of the problem solving process a little bit easier.\nAnd let's now go back to where we got stuck in this problem the first time.\nWhen we were solving this constraint satisfaction problem, we dealt with B.\nAnd then we went on to D. And we went ahead and just assigned D to Monday,\nbecause that seemed to work with the assignment so far.\nIt didn't violate any constraints.\nBut it turned out that later on that choice turned out to be a bad one,\nthat that choice wasn't consistent with the rest of the values\nthat we could take on here.\nAnd the question is, is there anything we\ncould do to avoid getting into a situation like this,\navoid trying to go down a path that's ultimately not going to lead anywhere\nby taking advantage of knowledge that we have initially?\nAnd it turns out we do have that kind of knowledge.\nWe can look at just the structure of this graph so far.\nAnd we can say that right now C's domain, for example,\ncontains values Monday, Tuesday, and Wednesday.\nAnd based on those values, we can say that this graph is not arc consistent.\nRecall that arc consistency is all about making sure\nthat for every possible value for a particular node,\nthat there is some other value that we are able to choose.\nAnd as we can see here, Monday and Tuesday\nare not going to be possible values that we can choose for C.\nThey're not going to be consistent with a node like B, for example,\nbecause B is equal to Tuesday, which means that C cannot be Tuesday.\nAnd because A is equal to Monday, C also cannot be Monday.\nSo using that information, by making C arc consistent with A and B,\nwe could remove Monday and Tuesday from C's domain\nand just leave C with Wednesday, for example.\nAnd if we continued to try and enforce arc consistency,\nwe'd see there are some other conclusions we can draw as well.\nWe see that B's only option is Tuesday and C's only option is Wednesday.\nAnd so if we want to make E arc consistent,\nwell, E can't be Tuesday, because that wouldn't be arc consistent with B.\nAnd E can't be Wednesday, because that wouldn't be arc consistent with C.\nSo we can go ahead and say E and just set that equal to Monday, for example.\nAnd then we can begin to do this process again and again,\nthat in order to make D arc consistent with B and E,\nthen D would have to be Wednesday.\nThat's the only possible option.\nAnd likewise, we can make the same judgments for F and G as well.\nAnd it turns out that without having to do any additional search,\njust by enforcing arc consistency, we were\nable to actually figure out what the assignment of all the variables\nshould be without needing to backtrack at all.\nAnd the way we did that is by interleaving this search process\nand the inference step, by this step of trying to enforce arc consistency.\nAnd the algorithm to do this is often called just the maintaining arc\nconsistency algorithm, which just enforces arc consistency every time\nwe make a new assignment of a value to an existing variable.\nSo sometimes we can enforce our consistency using that AC3 algorithm\nat the very beginning of the problem before we even begin searching\nin order to limit the domain of the variables\nin order to make it easier to search.\nBut we can also take advantage of the interleaving\nof enforcing our consistency with search such that every time in the search\nprocess we make a new assignment, we go ahead and enforce arc consistency\nas well to make sure that we're just eliminating\npossible values from domains whenever possible.\nAnd how do we do this?\nWell, this is really equivalent to just every time\nwe make a new assignment to a variable x.\nWe'll go ahead and call our AC3 algorithm,\nthis algorithm that enforces arc consistency on a constraint satisfaction\nproblem.\nAnd we go ahead and call that, starting it\nwith a Q, not of all of the arcs, which we did originally,\nbut just of all of the arcs that we want to make arc consistent with x,\nthis thing that we have just made an assignment to.\nSo all arcs yx, where y is a neighbor of x, something\nthat shares a constraint with x, for example.\nAnd by maintaining arc consistency in the backtracking search process,\nwe can ultimately make our search process a little bit more efficient.\nAnd so this is the revised version of this backtrack function.\nSame as before, the changes here are highlighted in yellow.\nEvery time we add a new variable equals value to our assignment,\nwe'll go ahead and run this inference procedure, which\nmight do a number of different things.\nBut one thing it could do is call the maintaining arc consistency\nalgorithm to make sure we're able to enforce arc consistency on the problem.\nAnd we might be able to draw new inferences as a result of that process.\nGet new guarantees of this variable needs to be equal to that value,\nfor example.\nThat might happen one time.\nIt might happen many times.\nAnd so long as those inferences are not a failure,\nas long as they don't lead to a situation where there is no possible way\nto make forward progress, well, then we can go ahead and add those inferences,\nthose new knowledge, that new pieces of knowledge\nI know about what variables should be assigned to what values,\nI can add those to the assignment in order to more quickly make forward\nprogress by taking advantage of information that I can just deduce,\ninformation I know based on the rest of the structure\nof the constraint satisfaction problem.\nAnd the only other change I'll need to make now\nis if it turns out this value doesn't work, well, then down here,\nI'll go ahead and need to remove not only variable equals value,\nbut also any of those inferences that I made,\nremove that from the assignment as well.\nSo here, then, we're often able to solve the problem by backtracking less\nthan we might originally have needed to, just\nby taking advantage of the fact that every time we\nmake a new assignment of one variable to one value,\nthat might reduce the domains of other variables as well.\nAnd we can use that information to begin to more quickly draw conclusions\nin order to try and solve the problem more efficiently as well.\nAnd it turns out there are other heuristics\nwe can use to try and improve the efficiency of our search process\nas well.\nAnd it really boils down to a couple of these functions\nthat I've talked about, but we haven't really\ntalked about how they're working.\nAnd one of them is this function here, select unassigned variable,\nwhere we're selecting some variable in the constraint satisfaction problem\nthat has not yet been assigned.\nSo far, I've sort of just been selecting variables randomly,\njust like picking one variable and one unassigned variable in order\nto decide, all right, this is the variable\nthat we're going to assign next, and then going from there.\nBut it turns out that by being a little bit intelligent,\nby following certain heuristics, we might be\nable to make the search process much more efficient just\nby choosing very carefully which variable we should explore next.\nSo some of those heuristics include the minimum remaining values,\nor MRV heuristic, which generally says that if I\nhave a choice between which variable I should select,\nI should select the variable with the smallest domain,\nthe variable that has the fewest number of remaining values left.\nWith the idea being, if there are only two remaining values left,\nwell, I may as well prune one of them very quickly in order\nto get to the other, because one of those two has got to be the solution,\nif a solution does exist.\nSometimes minimum remaining values might not give a conclusive result\nif all the nodes have the same number of remaining values, for example.\nAnd in that case, another heuristic that can be helpful to look at\nis the degree heuristic.\nThe degree of a node is the number of nodes that are attached to that node,\nthe number of nodes that are constrained by that particular node.\nAnd if you imagine which variable should I choose,\nshould I choose a variable that has a high degree that\nis connected to a lot of different things,\nor a variable with a low degree that is not\nconnected to a lot of different things, well,\nit can often make sense to choose the variable that\nhas the highest degree that is connected to the most other nodes\nas the thing you would search first.\nWhy is that the case?\nWell, it's because by choosing a variable with a high degree,\nthat is immediately going to constrain the rest of the variables more,\nand it's more likely to be able to eliminate large sections of the state\nspace that you don't need to search through at all.\nSo what could this actually look like?\nLet's go back to this search problem here.\nIn this particular case, I've made an assignment here.\nI've made an assignment here.\nAnd the question is, what should I look at next?\nAnd according to the minimum remaining values heuristic,\nwhat I should choose is the variable that has the fewest\nremaining possible values.\nAnd in this case, that's this node here, node\nC, that only has one variable left in this domain, which in this case\nis Wednesday, which is a very reasonable choice of a next assignment\nto make, because I know it's the only option, for example.\nI know that the only possible option for C is Wednesday,\nso I may as well make that assignment and then potentially explore\nthe rest of the space after that.\nBut meanwhile, at the very start of the problem,\nwhen I didn't have any knowledge of what nodes should have what values yet,\nI still had to pick what node should be the first one that I try and assign\na value to.\nAnd I arbitrarily just chose the one at the top, node A originally.\nBut we can be more intelligent about that.\nWe can look at this particular graph.\nAll of them have domains of the same size, domain of size 3.\nSo minimum remaining values doesn't really help us there.\nBut we might notice that node E has the highest degree.\nIt is connected to the most things.\nAnd so perhaps it makes sense to begin our search,\nrather than starting at node A at the very top,\nstart with the node with the highest degree.\nStart by searching from node E, because from there,\nthat's going to much more easily allow us to enforce\nthe constraints that are nearby, eliminating\nlarge portions of the search space that I might not need to search through.\nAnd in fact, by starting with E, we can immediately then assign other variables.\nAnd following that, we can actually assign the rest of the variables\nwithout needing to do any backtracking at all,\neven if I'm not using this inference procedure.\nJust by starting with a node that has a high degree,\nthat is going to very quickly restrict the possible values\nthat other nodes can take on.\nSo that then is how we can go about selecting\nan unassigned variable in a particular order.\nRather than randomly picking a variable, if we're\na little bit intelligent about how we choose it,\nwe can make our search process much, much more efficient\nby making sure we don't have to search through portions of the search space\nthat ultimately aren't going to matter.\nThe other variable we haven't really talked about,\nthe other function here, is this domain values function.\nThis domain values function that takes a variable\nand gives me back a sequence of all of the values\ninside of that variable's domain.\nThe naive way to approach it is what we did before,\nwhich is just go in order, go Monday, then Tuesday, then Wednesday.\nBut the problem is that going in that order\nmight not be the most efficient order to search in,\nthat sometimes it might be more efficient to choose values\nthat are likely to be solutions first and then go to other values.\nNow, how do you assess whether a value is\nlikelier to lead to a solution or less likely to lead to a solution?\nWell, one thing you can take a look at is how many constraints get added,\nhow many things get removed from domains as you\nmake this new assignment of a variable to this particular value.\nAnd the heuristic we can use here is the least constraining value heuristic,\nwhich is the idea that we should return variables in order\nbased on the number of choices that are ruled out for neighboring values.\nAnd I want to start with the least constraining value, the value that\nrules out the fewest possible options.\nAnd the idea there is that if all I care about doing\nis finding a solution, if I start with a value that\nrules out a lot of other choices, I'm ruling out a lot of possibilities\nthat maybe is going to make it less likely that this particular choice\nleads to a solution.\nWhereas on the other hand, if I have a variable\nand I start by choosing a value that doesn't rule out very much,\nwell, then I still have a lot of space where there might be a solution\nthat I could ultimately find.\nAnd this might seem a little bit counterintuitive and a little bit at odds\nwith what we were talking about before, where I said,\nwhen you're picking a variable, you should\npick the variable that is going to have the fewest possible values remaining.\nBut here, I want to pick the value for the variable\nthat is the least constraining.\nBut the general idea is that when I am picking a variable,\nI would like to prune large portions of the search space\nby just choosing a variable that is going to allow me to quickly eliminate\npossible options.\nWhereas here, within a particular variable,\nas I'm considering values that that variable could take on,\nI would like to just find a solution.\nAnd so what I want to do is ultimately choose\na value that still leaves open the possibility of me finding a solution\nto be as likely as possible.\nBy not ruling out many options, I leave open the possibility\nthat I can still find a solution without needing\nto go back later and backtrack.\nSo an example of that might be in this particular situation here,\nif I'm trying to choose a variable for a value for node C here,\nthat C is equal to either Tuesday or Wednesday.\nWe know it can't be Monday because it conflicts with this domain here,\nwhere we already know that A is Monday, so C must be Tuesday or Wednesday.\nAnd the question is, should I try Tuesday first,\nor should I try Wednesday first?\nAnd if I try Tuesday, what gets ruled out?\nWell, one option gets ruled out here, a second option gets ruled out here,\nand a third option gets ruled out here.\nSo choosing Tuesday would rule out three possible options.\nAnd what about choosing Wednesday?\nWell, choosing Wednesday would rule out one option here,\nand it would rule out one option there.\nAnd so I have two choices.\nI can choose Tuesday that rules out three options,\nor Wednesday that rules out two options.\nAnd according to the least constraining value heuristic,\nwhat I should probably do is go ahead and choose Wednesday,\nthe one that rules out the fewest number of possible options,\nleaving open as many chances as possible for me\nto eventually find the solution inside of the state space.\nAnd ultimately, if you continue this process,\nwe will find the solution, an assignment of variables, two values,\nthat allows us to give each of these exams, each of these classes,\nan exam date that doesn't conflict with anyone\nthat happens to be enrolled in two classes at the same time.\nSo the big takeaway now with all of this is\nthat there are a number of different ways we can formulate a problem.\nThe ways we've looked at today are we can formulate a problem\nas a local search problem, a problem where we're looking at a current node\nand moving to a neighbor based on whether that neighbor is better\nor worse than the current node that we are looking at.\nWe looked at formulating problems as linear programs,\nwhere just by putting things in terms of equations and constraints,\nwe're able to solve problems a little bit more efficiently.\nAnd we saw formulating a problem as a constraint satisfaction problem,\ncreating this graph of all of the constraints\nthat connect two variables that have some constraint between them,\nand using that information to be able to figure out\nwhat the solution should be.\nAnd so the takeaway of all of this now is\nthat if we have some problem in artificial intelligence\nthat we would like to use AI to be able to solve them,\nwhether that's trying to figure out where hospitals should be\nor trying to solve the traveling salesman problem,\ntrying to optimize productions and costs and whatnot,\nor trying to figure out how to satisfy certain constraints,\nwhether that's in a Sudoku puzzle, or whether that's\nin trying to figure out how to schedule exams for a university,\nor any number of a wide variety of types of problems,\nif we can formulate that problem as one of these sorts of problems,\nthen we can use these known algorithms, these algorithms\nfor enforcing art consistency and backtracking search,\nthese hill climbing and simulated annealing algorithms,\nthese simplex algorithms and interior point algorithms that\ncan be used to solve linear programs, that we\ncan use those techniques to begin to solve a whole wide variety of problems\nall in this world of optimization inside of artificial intelligence.\nThis was an introduction to artificial intelligence with Python for today.\nWe will see you next time.\n[\"\nAll right.\nWelcome back, everyone, to an introduction\nto artificial intelligence with Python.\nNow, so far in this class, we've used AI to solve\na number of different problems, giving AI instructions\nfor how to search for a solution, or how to satisfy certain constraints in order\nto find its way from some input point to some output point\nin order to solve some sort of problem.\nToday, we're going to turn to the world of learning,\nin particular the idea of machine learning, which generally refers\nto the idea where we are not going to give the computer explicit instructions\nfor how to perform a task, but rather we are going to give the computer access\nto information in the form of data, or patterns that it can learn from,\nand let the computer try and figure out what those patterns are,\ntry and understand that data to be able to perform a task on its own.\nNow, machine learning comes in a number of different forms,\nand it's a very wide field.\nSo today, we'll explore some of the foundational algorithms and ideas\nthat are behind a lot of the different areas within machine learning.\nAnd one of the most popular is the idea of supervised machine learning,\nor just supervised learning.\nAnd supervised learning is a particular type of task.\nIt refers to the task where we give the computer access\nto a data set, where that data set consists of input-output pairs.\nAnd what we would like the computer to do\nis we would like our AI to be able to figure out\nsome function that maps inputs to outputs.\nSo we have a whole bunch of data that generally consists\nof some kind of input, some evidence, some information\nthat the computer will have access to.\nAnd we would like the computer, based on that input information,\nto predict what some output is going to be.\nAnd we'll give it some data so that the computer can train its model on\nand begin to understand how it is that this information works\nand how it is that the inputs and outputs relate to each other.\nBut ultimately, we hope that our computer\nwill be able to figure out some function that, given those inputs,\nis able to get those outputs.\nThere are a couple of different tasks within supervised learning.\nThe one we'll focus on and start with is known as classification.\nAnd classification is the problem where, if I give you a whole bunch of inputs,\nyou need to figure out some way to map those inputs into discrete categories,\nwhere you can decide what those categories are,\nand it's the job of the computer to predict what those categories are\ngoing to be.\nSo that might be, for example, I give you information\nabout a bank note, like a US dollar, and I'm asking you to predict for me,\ndoes it belong to the category of authentic bank notes,\nor does it belong to the category of counterfeit bank notes?\nYou need to categorize the input, and we want\nto train the computer to figure out some function\nto be able to do that calculation.\nAnother example might be the case of weather,\nsomeone we've talked about a little bit so far in this class,\nwhere we would like to predict on a given day,\nis it going to rain on that day?\nIs it going to be cloudy on that day?\nAnd before we've seen how we could do this, if we really give the computer\nall the exact probabilities for if these are the conditions,\nwhat's the probability of rain?\nOftentimes, we don't have access to that information, though.\nBut what we do have access to is a whole bunch of data.\nSo if we wanted to be able to predict something like,\nis it going to rain or is it not going to rain,\nwe would give the computer historical information about days\nwhen it was raining and days when it was not raining\nand ask the computer to look for patterns in that data.\nSo what might that data look like?\nWell, we could structure that data in a table like this.\nThis might be what our table looks like, where for any particular day,\ngoing back, we have information about that day's humidity,\nthat day's air pressure, and then importantly, we have a label,\nsomething where the human has said that on this particular day,\nit was raining or it was not raining.\nSo you could fill in this table with a whole bunch of data.\nAnd what makes this what we would call a supervised learning exercise\nis that a human has gone in and labeled each of these data points,\nsaid that on this day, when these were the values for the humidity and pressure,\nthat day was a rainy day and this day was a not rainy day.\nAnd what we would like the computer to be able to do then\nis to be able to figure out, given these inputs, given the humidity\nand the pressure, can the computer predict what label\nshould be associated with that day?\nDoes that day look more like it's going to be a day that rains\nor does it look more like a day when it's not going to rain?\nPut a little bit more mathematically, you can think of this as a function\nthat takes two inputs, the inputs being the data points\nthat our computer will have access to, things like humidity and pressure.\nSo we could write a function f that takes\nas input both humidity and pressure.\nAnd then the output is going to be what category\nwe would ascribe to these particular input points, what label\nwe would associate with that input.\nSo we've seen a couple of example data points here,\nwhere given this value for humidity and this value for pressure,\nwe predict, is it going to rain or is it not going to rain?\nAnd that's information that we just gathered from the world.\nWe measured on various different days what the humidity and pressure were.\nWe observed whether or not we saw rain or no rain on that particular day.\nAnd this function f is what we would like to approximate.\nNow, the computer and we humans don't really\nknow exactly how this function f works.\nIt's probably quite a complex function.\nSo what we're going to do instead is attempt to estimate it.\nWe would like to come up with a hypothesis function.\nh, which is going to try to approximate what f does.\nWe want to come up with some function h that will also take the same inputs\nand will also produce an output, rain or no rain.\nAnd ideally, we'd like these two functions to agree as much as possible.\nSo the goal then of the supervised learning classification tasks\nis going to be to figure out, what does that function h look like?\nHow can we begin to estimate, given all of this information, all of this data,\nwhat category or what label should be assigned to a particular data point?\nSo where could you begin doing this?\nWell, a reasonable thing to do, especially in this situation,\nI have two numerical values, is I could try\nto plot this on a graph that has two axes, an x-axis and a y-axis.\nAnd in this case, we're just going to be using two numerical values as input.\nBut these same types of ideas scale as you add more and more inputs as well.\nWe'll be plotting things in two dimensions.\nBut as we soon see, you could add more inputs\nand just imagine things in multiple dimensions.\nAnd while we humans have trouble conceptualizing anything really\nbeyond three dimensions, at least visually,\na computer has no problem with trying to imagine things\nin many, many more dimensions, that for a computer,\neach dimension is just some separate number that it is keeping track of.\nSo it wouldn't be unreasonable for a computer to think in 10 dimensions\nor 100 dimensions to be able to try to solve a problem.\nBut for now, we've got two inputs.\nSo we'll graph things along two axes, an x-axis, which will here\nrepresent humidity, and a y-axis, which here represents pressure.\nAnd what we might do is say, let's take all of the days\nthat were raining and just try to plot them on this graph\nand see where they fall on this graph.\nAnd here might be all of the rainy days, where each rainy day is\none of these blue dots here that corresponds\nto a particular value for humidity and a particular value for pressure.\nAnd then I might do the same thing with the days that were not rainy.\nSo take all the not rainy days, figure out\nwhat their values were for each of these two inputs,\nand go ahead and plot them on this graph as well.\nAnd I've here plotted them in red.\nSo blue here stands for a rainy day.\nRed here stands for a not rainy day.\nAnd this then is the input that my computer\nhas access to all of this input.\nAnd what I would like the computer to be able to do\nis to train a model such that if I'm ever presented with a new input that\ndoesn't have a label associated with it, something like this white dot here,\nI would like to predict, given those values for each of the two inputs,\nshould we classify it as a blue dot, a rainy day,\nor should we classify it as a red dot, a not rainy day?\nAnd if you're just looking at this picture graphically, trying to say,\nall right, this white dot, does it look like it belongs to the blue category,\nor does it look like it belongs to the red category,\nI think most people would agree that it probably belongs to the blue category.\nAnd why is that?\nWell, it looks like it's close to other blue dots.\nAnd that's not a very formal notion, but it's a notion\nthat we'll formalize in just a moment.\nThat because it seems to be close to this blue dot here,\nnothing else is closer to it, then we might\nsay that it should be categorized as blue.\nIt should fall into that category of, I think\nthat day is going to be a rainy day based on that input.\nMight not be totally accurate, but it's a pretty good guess.\nAnd this type of algorithm is actually a very popular and common machine\nlearning algorithm known as nearest neighbor classification.\nIt's an algorithm for solving these classification-type problems.\nAnd in nearest neighbor classification, it's going to perform this algorithm.\nWhat it will do is, given an input, it will\nchoose the class of the nearest data point to that input.\nBy class, we just here mean category, like rain or no rain,\ncounterfeit or not counterfeit.\nAnd we choose the category or the class based on the nearest data point.\nSo given all that data, we just looked at,\nis the nearest data point a blue point or is it a red point?\nAnd depending on the answer to that question,\nwe were able to make some sort of judgment.\nWe were able to say something like, we think it's going to be blue\nor we think it's going to be red.\nSo likewise, we could apply this to other data points\nthat we encounter as well.\nIf suddenly this data point comes about, well, its nearest data is red.\nSo we would go ahead and classify this as a red point, not raining.\nThings get a little bit trickier, though, when you look at a point\nlike this white point over here and you ask the same sort of question.\nShould it belong to the category of blue points, the rainy days?\nOr should it belong to the category of red points, the not rainy days?\nNow, nearest neighbor classification would say the way you solve this problem\nis look at which point is nearest to that point.\nYou look at this nearest point and say it's red.\nIt's a not rainy day.\nAnd therefore, according to nearest neighbor classification,\nI would say that this unlabeled point, well, that should also be red.\nIt should also be classified as a not rainy day.\nBut your intuition might think that that's a reasonable judgment to make,\nthat it's the closest thing is a not rainy day.\nSo may as well guess that it's a not rainy day.\nBut it's probably also reasonable to look at the bigger picture of things\nto say, yes, it is true that the nearest point to it was a red point.\nBut it's surrounded by a whole bunch of other blue points.\nSo looking at the bigger picture, there's potentially\nan argument to be made that this point should actually be blue.\nAnd with only this data, we actually don't know for sure.\nWe are given some input, something we're trying to predict.\nAnd we don't necessarily know what the output is going to be.\nSo in this case, which one is correct is difficult to say.\nBut oftentimes, considering more than just a single neighbor,\nconsidering multiple neighbors can sometimes give us a better result.\nAnd so there's a variant on the nearest neighbor classification algorithm\nthat is known as the K nearest neighbor classification algorithm,\nwhere K is some parameter, some number that we choose,\nfor how many neighbors are we going to look at.\nSo one nearest neighbor classification is what we saw before.\nJust pick the one nearest neighbor and use that category.\nBut with K nearest neighbor classification,\nwhere K might be 3, or 5, or 7, to say look at the 3, or 5, or 7 closest\nneighbors, closest data points to that point, works a little bit differently.\nThis algorithm, we'll give it an input.\nChoose the most common class out of the K nearest data points to that input.\nSo if we look at the five nearest points, and three of them say it's raining,\nand two of them say it's not raining, we'll\ngo with the three instead of the two, because each one effectively\ngets one vote towards what they believe the category ought to be.\nAnd ultimately, you choose the category that has the most votes\nas a consequence of that.\nSo K nearest neighbor classification, fairly straightforward one\nto understand intuitively.\nYou just look at the neighbors and figure out what the answer might be.\nAnd it turns out this can work very, very well\nfor solving a whole variety of different types of classification problems.\nBut not every model is going to work under every situation.\nAnd so one of the things we'll take a look at today, especially\nin the context of supervised machine learning,\nis that there are a number of different approaches to machine learning,\na number of different algorithms that we can apply,\nall solving the same type of problem, all solving some kind of classification\nproblem where we want to take inputs and organize it\ninto different categories.\nAnd no one algorithm is necessarily always\ngoing to be better than some other algorithm.\nThey each have their trade-offs.\nAnd maybe depending on the data, one type of algorithm\nis going to be better suited to trying to model\nthat information than some other algorithm.\nAnd so this is what a lot of machine learning research ends up being about,\nthat when you're trying to apply machine learning techniques,\nyou're often looking not just at one particular algorithm,\nbut trying multiple different algorithms,\ntrying to see what is going to give you the best results for trying\nto predict some function that maps inputs to outputs.\nSo what then are the drawbacks of K nearest neighbor classification?\nWell, there are a couple.\nOne might be that in a naive approach, at least, it could be fairly slow\nto have to go through and measure the distance between a point\nand every single one of these points that exist here.\nNow, there are ways of trying to get around that.\nThere are data structures that can help to make it more quickly\nto be able to find these neighbors.\nThere are also techniques you can use to try and prune some of this data,\nremove some of the data points so that you're only\nleft with the relevant data points just to make it a little bit easier.\nBut ultimately, what we might like to do is come up\nwith another way of trying to do this classification.\nAnd one way of trying to do the classification\nwas looking at what are the neighboring points.\nBut another way might be to try to look at all of the data\nand see if we can come up with some decision boundary, some boundary that\nwill separate the rainy days from the not rainy days.\nAnd in the case of two dimensions, we can do that by drawing a line,\nfor example.\nSo what we might want to try to do is just find some line,\nfind some separator that divides the rainy days, the blue points over here,\nfrom the not rainy days, the red points over there.\nWe're now trying a different approach in contrast\nwith the nearest neighbor approach, which just\nlooked at local data around the input data point that we cared about.\nNow what we're doing is trying to use a technique known as linear regression\nto find some sort of line that will separate the two halves from each other.\nNow sometimes it'll actually be possible to come up\nwith some line that perfectly separates all the rainy days\nfrom the not rainy days.\nRealistically, though, this is probably cleaner\nthan many data sets will actually be.\nOftentimes, data is messier.\nThere are outliers.\nThere's random noise that happens inside of a particular system.\nAnd what we'd like to do is still be able to figure out\nwhat a line might look like.\nSo in practice, the data will not always be linearly separable.\nOr linearly separable refers to some data set\nwhere I could draw a line just to separate the two halves of it perfectly.\nInstead, you might have a situation like this,\nwhere there are some rainy points that are on this side of the line\nand some not rainy points that are on that side of the line.\nAnd there may not be a line that perfectly separates\nwhat path of the inputs from the other half,\nthat perfectly separates all the rainy days from the not rainy days.\nBut we can still say that this line does a pretty good job.\nAnd we'll try to formalize a little bit later\nwhat we mean when we say something like this line does a pretty good job\nof trying to make that prediction.\nBut for now, let's just say we're looking for a line that\ndoes as good of a job as we can at trying to separate one category of things\nfrom another category of things.\nSo let's now try to formalize this a little bit more mathematically.\nWe want to come up with some sort of function, some way we can define this\nline.\nAnd our inputs are things like humidity and pressure in this case.\nSo our inputs we might call x1 is going to represent humidity,\nand x2 is going to represent pressure.\nThese are inputs that we are going to provide to our machine learning\nalgorithm.\nAnd given those inputs, we would like for our model\nto be able to predict some sort of output.\nAnd we are going to predict that using our hypothesis function, which\nwe called h.\nOur hypothesis function is going to take as input x1 and x2, humidity\nand pressure in this case.\nAnd you can imagine if we didn't just have two inputs,\nwe had three or four or five inputs or more,\nwe could have this hypothesis function take all of those as input.\nAnd we'll see examples of that a little bit later as well.\nAnd now the question is, what does this hypothesis function do?\nWell, it really just needs to measure, is this data point\non one side of the boundary, or is it on the other side of the boundary?\nAnd how do we formalize that boundary?\nWell, the boundary is generally going to be\na linear combination of these input variables,\nat least in this particular case.\nSo what we're trying to do when we say linear combination\nis take each of these inputs and multiply them\nby some number that we're going to have to figure out.\nWe'll generally call that number a weight for how important\nshould these variables be in trying to determine the answer.\nSo we'll weight each of these variables with some weight,\nand we might add a constant to it just to try and make\nthe function a little bit different.\nAnd the result, we just need to compare.\nIs it greater than 0, or is it less than 0 to say,\ndoes it belong on one side of the line or the other side of the line?\nSo what that mathematical expression might look like is this.\nWe would take each of my variables, x1 and x2, multiply them by some weight.\nI don't yet know what that weight is, but it's\ngoing to be some number, weight 1 and weight 2.\nAnd maybe we just want to add some other weight 0 to it,\nbecause the function might require us to shift the entire value up or down\nby a certain amount.\nAnd then we just compare.\nIf we do all this math, is it greater than or equal to 0?\nIf so, we might categorize that data point as a rainy day.\nAnd otherwise, we might say, no rain.\nSo the key here, then, is that this expression\nis how we are going to calculate whether it's a rainy day or not.\nWe're going to do a bunch of math where we take each of the variables,\nmultiply them by a weight, maybe add an extra weight to it,\nsee if the result is greater than or equal to 0.\nAnd using that result of that expression,\nwe're able to determine whether it's raining or not raining.\nThis expression here is in this case going to refer to just some line.\nIf you were to plot that graphically, it would just be some line.\nAnd what the line actually looks like depends upon these weights.\nx1 and x2 are the inputs, but these weights\nare really what determine the shape of that line, the slope of that line,\nand what that line actually looks like.\nSo we then would like to figure out what these weights should be.\nWe can choose whatever weights we want, but we\nwant to choose weights in such a way that if you pass in a rainy day's\nhumidity and pressure, then you end up with a result that\nis greater than or equal to 0.\nAnd we would like it such that if we passed into our hypothesis\nfunction a not rainy day's inputs, then the output that we get\nshould be not raining.\nSo before we get there, let's try and formalize this a little bit more\nmathematically just to get a sense for how it is that you'll often see this\nif you ever go further into supervised machine learning\nand explore this idea.\nOne thing is that generally for these categories,\nwe'll sometimes just use the names of the categories like rain and not rain.\nOften mathematically, if we're trying to do comparisons between these things,\nit's easier just to deal in the world of numbers.\nSo we could just say 1 and 0, 1 for raining, 0 for not raining.\nSo we do all this math.\nAnd if the result is greater than or equal to 0,\nwe'll go ahead and say our hypothesis function outputs 1, meaning raining.\nAnd otherwise, it outputs 0, meaning not raining.\nAnd oftentimes, this type of expression will instead\nexpress using vector mathematics.\nAnd all a vector is, if you're not familiar with the term,\nis it refers to a sequence of numerical values.\nYou could represent that in Python using a list of numerical values\nor a tuple with numerical values.\nAnd here, we have a couple of sequences of numerical values.\nOne of our vectors, one of our sequences of numerical values,\nare all of these individual weights, w0, w1, and w2.\nSo we could construct what we'll call a weight vector,\nand we'll see why this is useful in a moment,\ncalled w, generally represented using a boldface w, that\nis just a sequence of these three weights, weight 0, weight 1,\nand weight 2.\nAnd to be able to calculate, based on those weights,\nwhether we think a day is raining or not raining,\nwe're going to multiply each of those weights by one of our input variables.\nThat w2, this weight, is going to be multiplied by input variable x2.\nw1 is going to be multiplied by input variable x1.\nAnd w0, well, it's not being multiplied by anything.\nBut to make sure the vectors are the same length,\nand we'll see why that's useful in just a second,\nwe'll just go ahead and say w0 is being multiplied by 1.\nBecause you can multiply by something by 1,\nand you end up getting the exact same number.\nSo in addition to the weight vector w, we'll\nalso have an input vector that we'll call x that has three values, 1,\nagain, because we're just multiplying w0 by 1 eventually, and then x1 and x2.\nSo here, then, we've represented two distinct vectors, a vector of weights\nthat we need to somehow learn.\nThe goal of our machine learning algorithm\nis to learn what this weight vector is supposed to be.\nWe could choose any arbitrary set of numbers,\nand it would produce a function that tries to predict rain or not rain,\nbut it probably wouldn't be very good.\nWhat we want to do is come up with a good choice of these weights\nso that we're able to do the accurate predictions.\nAnd then this input vector represents a particular input\nto the function, a data point for which we would like to estimate,\nis that day a rainy day, or is that day a not rainy day?\nAnd so that's going to vary just depending\non what input is provided to our function, what\nit is that we are trying to estimate.\nAnd then to do the calculation, we want to calculate this expression here,\nand it turns out that expression is what we would call the dot product\nof these two vectors.\nThe dot product of two vectors just means taking each of the terms\nin the vectors and multiplying them together, w0 multiply it by 1,\nw1 multiply it by x1, w2 multiply it by x2,\nand that's why these vectors need to be the same length.\nAnd then we just add all of the results together.\nSo the dot product of w and x, our weight vector and our input vector,\nthat's just going to be w0 times 1, or just w0,\nplus w1 times x1, multiplying these two terms together,\nplus w2 times x2, multiplying those terms together.\nSo we have our weight vector, which we need to figure out.\nWe need our machine learning algorithm to figure out\nwhat the weights should be.\nWe have the input vector representing the data point\nthat we're trying to predict a category for, predict a label for.\nAnd we're able to do that calculation by taking this dot product, which\nyou'll often see represented in vector form.\nBut if you haven't seen vectors before, you\ncan think of it as identical to just this mathematical expression,\njust doing the multiplication, adding the results together,\nand then seeing whether the result is greater than or equal to 0 or not.\nThis expression here is identical to the expression\nthat we're calculating to see whether or not\nthat answer is greater than or equal to 0 in this case.\nAnd so for that reason, you'll often see the hypothesis function\nwritten as something like this, a simpler representation where\nthe hypothesis takes as input some input vector x, some humidity\nand pressure for some day.\nAnd we want to predict an output like rain or no rain or 1 or 0\nif we choose to represent things numerically.\nAnd the way we do that is by taking the dot product of the weights\nand our input.\nIf it's greater than or equal to 0, we'll go ahead and say the output is 1.\nOtherwise, the output is going to be 0.\nAnd this hypothesis, we say, is parameterized by the weights.\nDepending on what weights we choose, we'll\nend up getting a different hypothesis.\nIf we choose the weights randomly, we're probably\nnot going to get a very good hypothesis function.\nWe'll get a 1 or a 0.\nBut it's probably not accurately going to reflect\nwhether we think a day is going to be rainy or not rainy.\nBut if we choose the weights right, we can often\ndo a pretty good job of trying to estimate whether we think\nthe output of the function should be a 1 or a 0.\nAnd so the question, then, is how to figure out\nwhat these weights should be, how to be able to tune those parameters.\nAnd there are a number of ways you can do that.\nOne of the most common is known as the perceptron learning rule.\nAnd we'll see more of this later.\nBut the idea of the perceptron learning rule,\nand we're not going to get too deep into the mathematics,\nwe'll mostly just introduce it more conceptually,\nis to say that given some data point that we would like to learn from,\nsome data point that has an input x and an output y, where\ny is like 1 for rain or 0 for not rain, then we're going to update the weights.\nAnd we'll look at the formula in just a moment.\nBut the big picture idea is that we can start with random weights,\nbut then learn from the data.\nTake the data points one at a time.\nAnd for each one of the data points, figure out, all right,\nwhat parameters do we need to change inside of the weights\nin order to better match that input point.\nAnd so that is the value of having access to a lot of data\nin the supervised machine learning algorithm,\nis that you take each of the data points and maybe look at them multiple times\nand constantly try and figure out whether you\nneed to shift your weights in order to better create some weight vector that\nis able to correctly or more accurately try to estimate what the output should\nbe, whether we think it's going to be raining\nor whether we think it's not going to be raining.\nSo what does that weight update look like?\nWithout going into too much of the mathematics,\nwe're going to update each of the weights to be the result of the original\nweight plus some additional expression.\nAnd to understand this expression, y, well,\ny is what the actual output is.\nAnd hypothesis of x, the input, that's going to be what we thought the input\nwas.\nAnd so I can replace this by saying what the actual value was minus what\nour estimate was.\nAnd based on the difference between the actual value and what our estimate was,\nwe might want to change our hypothesis, change the way\nthat we do that estimation.\nIf the actual value and the estimate were the same thing,\nmeaning we were correctly able to predict what category\nthis data point belonged to, well, then actual value minus estimate,\nthat's just going to be 0, which means this whole term on the right-hand side\ngoes to be 0, and the weight doesn't change.\nWeight i, where i is like weight 1 or weight 2 or weight 0,\nweight i just stays at weight i.\nAnd none of the weights change if we were able to correctly predict\nwhat category the input belonged to.\nBut if our hypothesis didn't correctly predict what category the input\nbelonged to, well, then maybe then we need to make some changes, adjust\nthe weights so that we're better able to predict this kind of data\npoint in the future.\nAnd what is the way we might do that?\nWell, if the actual value was bigger than the estimate, then,\nand for now we'll go ahead and assume that these x's are positive values,\nthen if the actual value was bigger than the estimate,\nwell, that means we need to increase the weight in order\nto make it such that the output is bigger,\nand therefore we're more likely to get to the right actual value.\nAnd so if the actual value is bigger than the estimate,\nthen actual value minus estimate, that'll be a positive number.\nAnd so you imagine we're just adding some positive number to the weight\njust to increase it ever so slightly.\nAnd likewise, the inverse case is true, that if the actual value\nwas less than the estimate, the actual value was 0,\nbut we estimated 1, meaning it actually was not raining,\nbut we predicted it was going to be raining.\nWell, then we want to decrease the value of the weight,\nbecause then in that case, we want to try and lower\nthe total value of computing that dot product in order\nto make it less likely that we would predict that it would actually\nbe raining.\nSo no need to get too deep into the mathematics of that,\nbut the general idea is that every time we encounter some data point,\nwe can adjust these weights accordingly to try and make\nthe weights better line up with the actual data that we have access to.\nAnd you can repeat this process with data point after data point\nuntil eventually, hopefully, your algorithm\nconverges to some set of weights that do a pretty good job of trying\nto figure out whether a day is going to be rainy or not raining.\nAnd just as a final point about this particular equation,\nthis value alpha here is generally what we'll call the learning rate.\nIt's just some parameter, some number we choose\nfor how quickly we're actually going to be updating these weight values.\nSo that if alpha is bigger, then we're going\nto update these weight values by a lot.\nAnd if alpha is smaller, then we'll update the weight values by less.\nAnd you can choose a value of alpha.\nDepending on the problem, different values\nmight suit the situation better or worse than others.\nSo after all of that, after we've done this training process of take\nall this data and using this learning rule,\nlook at all the pieces of data and use each piece of data as an indication\nto us of do the weights stay the same, do we increase the weights,\ndo we decrease the weights, and if so, by how much?\nWhat you end up with is effectively a threshold function.\nAnd we can look at what the threshold function looks like like this.\nOn the x-axis here, we have the output of that function,\ntaking the weights, taking the dot product of it with the input.\nAnd on the y-axis, we have what the output is going to be,\n0, which in this case represented not raining,\nand 1, which in this case represented raining.\nAnd the way that our hypothesis function works is it calculates this value.\nAnd if it's greater than 0 or greater than some threshold value,\nthen we declare that it's a rainy day.\nAnd otherwise, we declare that it's a not rainy day.\nAnd this then graphically is what that function looks like,\nthat initially when the value of this dot product is small, it's not raining,\nit's not raining, it's not raining.\nBut as soon as it crosses that threshold,\nwe suddenly say, OK, now it's raining, now it's raining, now it's raining.\nAnd the way to interpret this kind of representation\nis that anything on this side of the line, that\nwould be the category of data points where we say, yes, it's raining.\nAnything that falls on this side of the line\nare the data points where we would say, it's not raining.\nAnd again, we want to choose some value for the weights\nthat results in a function that does a pretty good job of trying\nto do this estimation.\nBut one tricky thing with this type of hard threshold\nis that it only leaves two possible outcomes.\nWe plug in some data as input.\nAnd the output we get is raining or not raining.\nAnd there's no room for anywhere in between.\nAnd maybe that's what you want.\nMaybe all you want is given some data point,\nyou would like to be able to classify it into one or two or more\nof these various different categories.\nBut it might also be the case that you care about knowing\nhow strong that prediction is, for example.\nSo if we go back to this instance here, where we have rainy days\non this side of the line, not rainy days on that side of the line,\nyou might imagine that let's look now at these two white data points.\nThis data point here that we would like to predict a label or a category for.\nAnd this data point over here that we would also\nlike to predict a label or a category for.\nIt seems likely that you could pretty confidently\nsay that this data point, that should be a rainy day.\nSeems close to the other rainy days if we're\ngoing by the nearest neighbor strategy.\nIt's on this side of the line if we're going by the strategy of just saying,\nwhich side of the line does it fall on by figuring out\nwhat those weights should be.\nAnd if we're using the line strategy of just which side of the line\ndoes it fall on, which side of this decision boundary,\nwell, we'd also say that this point here is also a rainy day\nbecause it falls on the side of the line that corresponds to rainy days.\nBut it's likely that even in this case, we\nwould know that we don't feel nearly as confident about this data\npoint on the left as compared to this data point on the right.\nThat for this one on the right, we can feel very confident\nthat yes, it's a rainy day.\nThis one, it's pretty close to the line if we're judging just by distance.\nAnd so you might be less sure.\nBut our threshold function doesn't allow for a notion of less sure\nor more sure about something.\nIt's what we would call a hard threshold.\nIt's once you've crossed this line, then immediately we say,\nyes, this is going to be a rainy day.\nAnywhere before it, we're going to say it's not a rainy day.\nAnd that may not be helpful in a number of cases.\nOne, this is not a particularly easy function to deal with.\nAs you get deeper into the world of machine learning\nand are trying to do things like taking derivatives of these curves\nwith this type of function makes things challenging.\nBut the other challenge is that we don't really\nhave any notion of gradation between things.\nWe don't have a notion of yes, this is a very strong belief\nthat it's going to be raining as opposed to it's probably more likely than not\nthat it's going to be raining, but maybe not totally sure about that either.\nSo what we can do by taking advantage of a technique known\nas logistic regression is instead of using this hard threshold\ntype of function, we can use instead a logistic function, something\nwe might call a soft threshold.\nAnd that's going to transform this into looking something\na little more like this, something that more nicely curves.\nAnd as a result, the possible output values are no longer just 0 and 1,\n0 for not raining, 1 for raining.\nBut you can actually get any real numbered value between 0 and 1.\nBut if you're way over on this side, then you get a value of 0.\nOK, it's not going to be raining, and we're pretty sure about that.\nAnd if you're over on this side, you get a value of 1.\nAnd yes, we're very sure that it's going to be raining.\nBut in between, you could get some real numbered value,\nwhere a value like 0.7 might mean we think it's going to rain.\nIt's more probable that it's going to rain than not based on the data.\nBut we're not as confident as some of the other data points might be.\nSo one of the advantages of the soft threshold\nis that it allows us to have an output that could be some real number that\npotentially reflects some sort of probability, the likelihood that we\nthink that this particular data point belongs to that particular category.\nAnd there are some other nice mathematical properties of that as well.\nSo that then is two different approaches to trying\nto solve this type of classification problem.\nOne is this nearest neighbor type of approach,\nwhere you just take a data point and look at the data points that are nearby\nto try and estimate what category we think it belongs to.\nAnd the other approach is the approach of saying, all right,\nlet's just try and use linear regression,\nfigure out what these weights should be, adjust the weights in order\nto figure out what line or what decision boundary is going\nto best separate these two categories.\nIt turns out that another popular approach, a very popular approach\nif you just have a data set and you want to start\ntrying to do some learning on it, is what we call the support vector machine.\nAnd we're not going to go too much into the mathematics of the support vector\nmachine, but we'll at least explore it graphically to see what it is\nthat it looks like.\nAnd the idea or the motivation behind the support vector machine\nis the idea that there are actually a lot of different lines\nthat we could draw, a lot of different decision boundaries\nthat we could draw to separate two groups.\nSo for example, I had the red data points over here\nand the blue data points over here.\nOne possible line I could draw is a line like this,\nthat this line here would separate the red points from the blue points.\nAnd it does so perfectly.\nAll the red points are on one side of the line.\nAll the blue points are on the other side of the line.\nBut this should probably make you a little bit nervous.\nIf you come up with a model and the model comes up\nwith a line that looks like this.\nAnd the reason why is that you worry about how well\nit's going to generalize to other data points that are not necessarily\nin the data set that we have access to.\nFor example, if there was a point that fell like right here,\nfor example, on the right side of the line, well, then based on that,\nwe might want to guess that it is, in fact, a red point,\nbut it falls on the side of the line where instead we\nwould estimate that it's a blue point instead.\nAnd so based on that, this line is probably not a great choice\njust because it is so close to these various data points.\nWe might instead prefer like a diagonal line\nthat just goes diagonally through the data set like we've seen before.\nBut there too, there's a lot of diagonal lines that we could draw as well.\nFor example, I could draw this diagonal line here, which also successfully\nseparates all the red points from all of the blue points.\nFrom the perspective of something like just trying\nto figure out some setting of weights that allows\nus to predict the correct output, this line\nwill predict the correct output for this particular set of data\nevery single time because the red points are on one side,\nthe blue points are on the other.\nBut yet again, you should probably be a little nervous\nbecause this line is so close to these red points,\neven though we're able to correctly predict on the input data,\nif there was a point that fell somewhere in this general area,\nour algorithm, this model, would say that, yeah, we think it's a blue point,\nwhen in actuality, it might belong to the red category instead\njust because it looks like it's close to the other red points.\nWhat we really want to be able to say, given this data, how can you generalize\nthis as best as possible, is to come up with a line like this that\nseems like the intuitive line to draw.\nAnd the reason why it's intuitive is because it\nseems to be as far apart as possible from the red data and the blue data.\nSo that if we generalize a little bit and assume\nthat maybe we have some points that are different from the input\nbut still slightly further away, we can still\nsay that something on this side probably red, something on that side\nprobably blue, and we can make those judgments that way.\nAnd that is what support vector machines are designed to do.\nThey're designed to try and find what we call the maximum margin separator,\nwhere the maximum margin separator is just\nsome boundary that maximizes the distance between the groups of points\nrather than come up with some boundary that's\nvery close to one set or the other, where in the case\nbefore, we wouldn't have cared.\nAs long as we're categorizing the input well, that seems all we need to do.\nThe support vector machine will try and find this maximum margin separator,\nsome way of trying to maximize that particular distance.\nAnd it does so by finding what we call the support vectors, which\nare the vectors that are closest to the line,\nand trying to maximize the distance between the line\nand those particular points.\nAnd it works that way in two dimensions.\nIt also works in higher dimensions, where we're not\nlooking for some line that separates the two data points,\nbut instead looking for what we generally call a hyperplane,\nsome decision boundary, effectively, that separates one set of data\nfrom the other set of data.\nAnd this ability of support vector machines\nto work in higher dimensions actually has a number of other applications\nas well.\nBut one is that it helpfully deals with cases\nwhere data may not be linearly separable.\nSo we talked about linear separability before,\nthis idea that you can take data and just draw a line or some linear\ncombination of the inputs that allows us to perfectly separate\nthe two sets from each other.\nThere are some data sets that are not linearly separable.\nAnd some were even two.\nYou would not be able to find a good line at all\nthat would try to do that kind of separation.\nSomething like this, for example.\nOr if you imagine here are the red points and the blue points\naround it.\nIf you try to find a line that divides the red points from the blue points,\nit's actually going to be difficult, if not impossible,\nto do that any line you choose, well, if you draw a line here,\nthen you ignore all of these blue points that should actually\nbe blue and not red.\nAnywhere else you draw a line, there's going to be a lot of error,\na lot of mistakes, a lot of what we'll soon\ncall loss to that line that you draw, a lot of points\nthat you're going to categorize incorrectly.\nWhat we really want is to be able to find a better decision boundary that\nmay not be just a straight line through this two dimensional space.\nAnd what support vector machines can do is\nthey can begin to operate in higher dimensions\nand be able to find some other decision boundary,\nlike the circle in this case, that actually\nis able to separate one of these sets of data\nfrom the other set of data a lot better.\nSo oftentimes in data sets where the data is not linearly separable,\nsupport vector machines by working in higher dimensions\ncan actually figure out a way to solve that kind of problem effectively.\nSo that then, three different approaches to trying\nto solve these sorts of problems.\nWe've seen support vector machines.\nWe've seen trying to use linear regression and the perceptron learning\nrule to be able to figure out how to categorize inputs and outputs.\nWe've seen the nearest neighbor approach.\nNo one necessarily better than any other again.\nIt's going to depend on the data set, the information you have access to.\nIt's going to depend on what the function looks like that you're ultimately\ntrying to predict.\nAnd this is where a lot of research and experimentation\ncan be involved in trying to figure out how it\nis to best perform that kind of estimation.\nBut classification is only one of the tasks\nthat you might encounter in supervised machine learning.\nBecause in classification, what we're trying to predict\nis some discrete category.\nWe're trying to predict red or blue, rain or not rain,\nauthentic or counterfeit.\nBut sometimes what we want to predict is a real numbered value.\nAnd for that, we have a related problem, not classification,\nbut instead known as regression.\nAnd regression is the supervised learning problem\nwhere we try and learn a function mapping inputs to outputs same as before.\nBut instead of the outputs being discrete categories, things\nlike rain or not rain, in a regression problem,\nthe output values are generally continuous values, some real number\nthat we would like to predict.\nThis happens all the time as well.\nYou might imagine that a company might take this approach\nif it's trying to figure out, for instance, what\nthe effect of its advertising is.\nHow do advertising dollars spent translate\ninto sales for the company's product, for example?\nAnd so they might like to try to predict some function that\ntakes as input the amount of money spent on advertising.\nAnd here, we're just going to use one input.\nBut again, you could scale this up to many more inputs as well\nif you have a lot of different kinds of data you have access to.\nAnd the goal is to learn a function that given this amount of spending\non advertising, we're going to get this amount in sales.\nAnd you might judge, based on having access to a whole bunch of data,\nlike for every past month, here is how much we spent on advertising,\nand here is what sales were.\nAnd we would like to predict some sort of hypothesis function\nthat, again, given the amount spent on advertising,\nwe can predict, in this case, some real number, some number estimate\nof how much sales we expect that company to do in this month\nor in this quarter or whatever unit of time\nwe're choosing to measure things in.\nAnd so again, the approach to solving this type of problem,\nwe could try using a linear regression type approach where we take this data\nand we just plot it.\nOn the x-axis, we have advertising dollars spent.\nOn the y-axis, we have sales.\nAnd we might just want to try and draw a line that\ndoes a pretty good job of trying to estimate\nthis relationship between advertising and sales.\nAnd in this case, unlike before, we're not\ntrying to separate the data points into discrete categories.\nBut instead, in this case, we're just trying\nto find a line that approximates this relationship between advertising\nand sales so that if we want to figure out what the estimated sales are\nfor a particular advertising budget, you just look it up in this line,\nfigure out for this amount of advertising,\nwe would have this amount of sales and just try\nand make the estimate that way.\nAnd so you can try and come up with a line, again,\nfiguring out how to modify the weights using various different techniques\nto try and make it so that this line fits as well as possible.\nSo with all of these approaches, then, to trying to solve machine learning\nstyle problems, the question becomes, how do we evaluate these approaches?\nHow do we evaluate the various different hypotheses\nthat we could come up with?\nBecause each of these algorithms will give us some sort of hypothesis,\nsome function that maps inputs to outputs,\nand we want to know, how well does that function work?\nAnd you can think of evaluating these hypotheses\nand trying to get a better hypothesis as kind of like an optimization problem.\nIn an optimization problem, as you recall from before,\nwe were either trying to maximize some objective function\nby trying to find a global maximum, or we\nwere trying to minimize some cost function by trying to find some global\nminimum.\nAnd in the case of evaluating these hypotheses, one thing we might say\nis that this cost function, the thing we're trying to minimize,\nwe might be trying to minimize what we would call a loss function.\nAnd what a loss function is, is it is a function\nthat is going to estimate for us how poorly our function performs.\nMore formally, it's like a loss of utility\nby whenever we predict something that is wrong, that is a loss of utility.\nThat's going to add to the output of our loss function.\nAnd you could come up with any loss function\nthat you want, just some mathematical way of estimating,\ngiven each of these data points, given what the actual output is,\nand given what our projected output is, our estimate,\nyou could calculate some sort of numerical loss for it.\nBut there are a couple of popular loss functions\nthat are worth discussing, just so that you've seen them before.\nWhen it comes to discrete categories, things like rain or not rain,\ncounterfeit or not counterfeit, one approaches the 0, 1 loss function.\nAnd the way that works is for each of the data points,\nour loss function takes as input what the actual output is,\nlike whether it was actually raining or not raining,\nand takes our prediction into account.\nDid we predict, given this data point, that it was raining or not raining?\nAnd if the actual value equals the prediction, well, then the 0, 1 loss\nfunction will just say the loss is 0.\nThere was no loss of utility, because we were able to predict correctly.\nAnd otherwise, if the actual value was not the same thing\nas what we predicted, well, then in that case, our loss is 1.\nWe lost something, lost some utility, because what we predicted\nwas the output of the function, was not what it actually was.\nAnd the goal, then, in a situation like this\nwould be to come up with some hypothesis that minimizes\nthe total empirical loss, the total amount that we've lost,\nif you add up for all these data points what the actual output is\nand what your hypothesis would have predicted.\nSo in this case, for example, if we go back to classifying days as raining\nor not raining, and we came up with this decision boundary,\nhow would we evaluate this decision boundary?\nHow much better is it than drawing the line here or drawing the line there?\nWell, we could take each of the input data points,\nand each input data point has a label, whether it was raining\nor whether it was not raining.\nAnd we could compare it to the prediction,\nwhether we predicted it would be raining or not raining,\nand assign it a numerical value as a result.\nSo for example, these points over here, they were all rainy days,\nand we predicted they would be raining, because they\nfall on the bottom side of the line.\nSo they have a loss of 0, nothing lost from those situations.\nAnd likewise, same is true for some of these points over here,\nwhere it was not raining and we predicted it would not be raining either.\nWhere we do have loss are points like this point here and that point there,\nwhere we predicted that it would not be raining,\nbut in actuality, it's a blue point.\nIt was raining.\nOr likewise here, we predicted that it would be raining,\nbut in actuality, it's a red point.\nIt was not raining.\nAnd so as a result, we miscategorized these data points\nthat we were trying to train on.\nAnd as a result, there is some loss here.\nOne loss here, there, here, and there, for a total loss of 4,\nfor example, in this case.\nAnd that might be how we would estimate or how we would say\nthat this line is better than a line that goes somewhere else\nor a line that's further down, because this line might minimize the loss.\nSo there is no way to do better than just these four points of loss\nif you're just drawing a straight line through our space.\nSo the 0, 1 loss function checks.\nDid we get it right?\nDid we get it wrong?\nIf we got it right, the loss is 0, nothing lost.\nIf we got it wrong, then our loss function for that data point says 1.\nAnd we add up all of those losses across all of our data points\nto get some sort of empirical loss, how much we\nhave lost across all of these original data points\nthat our algorithm had access to.\nThere are other forms of loss as well that work especially well when\nwe deal with more real valued cases, cases\nlike the mapping between advertising budget and amount\nthat we do in sales, for example.\nBecause in that case, you care not just that you get the number exactly right,\nbut you care how close you were to the actual value.\nIf the actual value is you did like $2,800 in sales\nand you predicted that you would do $2,900 in sales,\nmaybe that's pretty good.\nThat's much better than if you had predicted you'd do $1,000 in sales,\nfor example.\nAnd so we would like our loss function to be\nable to take that into account as well, take into account not just\nwhether the actual value and the expected value are exactly the same,\nbut also take into account how far apart they were.\nAnd so for that one approach is what we call L1 loss.\nL1 loss doesn't just look at whether actual and predicted\nare equal to each other, but we take the absolute value\nof the actual value minus the predicted value.\nIn other words, we just ask how far apart were the actual and predicted\nvalues, and we sum that up across all of the data points\nto be able to get what our answer ultimately is.\nSo what might this actually look like for our data set?\nWell, if we go back to this representation\nwhere we had advertising along the x-axis, sales along the y-axis,\nour line was our prediction, our estimate for any given\namount of advertising, what we predicted sales was going to be.\nAnd our L1 loss is just how far apart vertically along the sales axis\nour prediction was from each of the data points.\nSo we could figure out exactly how far apart\nour prediction was from each of the data points\nand figure out as a result of that what our loss is overall\nfor this particular hypothesis just by adding up\nall of these various different individual losses for each of these data\npoints.\nAnd our goal then is to try and minimize that loss,\nto try and come up with some line that minimizes what the utility loss is\nby judging how far away our estimate amount of sales\nis from the actual amount of sales.\nAnd turns out there are other loss functions as well.\nOne that's quite popular is the L2 loss.\nThe L2 loss, instead of just using the absolute value,\nlike how far away the actual value is from the predicted value,\nit uses the square of actual minus predicted.\nSo how far apart are the actual and predicted value?\nAnd it squares that value, effectively penalizing much more harshly anything\nthat is a worse prediction.\nSo you imagine if you have two data points\nthat you predict as being one value away from their actual value,\nas opposed to one data point that you predict as being two away\nfrom its actual value, the L2 loss function\nwill more harshly penalize that one that is two away,\nbecause it's going to square, however, much the differences\nbetween the actual value and the predicted value.\nAnd depending on the situation, you might\nwant to choose a loss function depending on what you care about minimizing.\nIf you really care about minimizing the error on more outlier cases,\nthen you might want to consider something like this.\nBut if you've got a lot of outliers, and you don't necessarily\ncare about modeling them, then maybe an L1 loss function is preferable.\nBut there are trade-offs here that you need to decide,\nbased on a particular set of data.\nBut what you do run the risk of with any of these loss functions,\nwith anything that we're trying to do, is a problem known as overfitting.\nAnd overfitting is a big problem that you can encounter in machine learning,\nwhich happens anytime a model fits too closely with a data set,\nand as a result, fails to generalize.\nWe would like our model to be able to accurately predict\ndata and inputs and output pairs for the data that we have access to.\nBut the reason we wanted to do so is because we\nwant our model to generalize well to data that we haven't seen before.\nI would like to take data from the past year\nof whether it was raining or not raining,\nand use that data to generalize it towards the future.\nSay, in the future, is it going to be raining or not raining?\nOr if I have a whole bunch of data on what counterfeit and not counterfeit\nUS dollar bills look like in the past when people have encountered them,\nI'd like to train a computer to be able to, in the future,\ngeneralize to other dollar bills that I might see as well.\nAnd the problem with overfitting is that if you try and tie yourself\ntoo closely to the data set that you're training your model on,\nyou can end up not generalizing very well.\nSo what does this look like?\nWell, we might imagine the rainy day and not rainy day\nexample again from here, where the blue points indicate rainy days\nand the red points indicate not rainy days.\nAnd we decided that we felt pretty comfortable with drawing a line\nlike this as the decision boundary between rainy days and not rainy days.\nSo we can pretty comfortably say that points on this side\nmore likely to be rainy days, points on that side more\nlikely to be not rainy days.\nBut the loss, the empirical loss, isn't zero in this particular case\nbecause we didn't categorize everything perfectly.\nThere was this one outlier, this one day that it wasn't raining,\nbut yet our model still predicts that it is raining.\nBut that doesn't necessarily mean our model is bad.\nIt just means the model isn't 100% accurate.\nIf you really wanted to try and find a hypothesis that\nresulted in minimizing the loss, you could come up\nwith a different decision boundary.\nIt wouldn't be a line, but it would look something like this.\nThis decision boundary does separate all of the red points\nfrom all of the blue points because the red points fall\non this side of this decision boundary, the blue points\nfall on the other side of the decision boundary.\nBut this, we would probably argue, is not as good of a prediction.\nEven though it seems to be more accurate based\non all of the available training data that we\nhave for training this machine learning model,\nwe might say that it's probably not going to generalize well.\nThat if there were other data points like here and there,\nwe might still want to consider those to be rainy days\nbecause we think this was probably just an outlier.\nSo if the only thing you care about is minimizing the loss on the data\nyou have available to you, you run the risk of overfitting.\nAnd this can happen in the classification case.\nIt can also happen in the regression case,\nthat here we predicted what we thought was a pretty good line relating\nadvertising to sales, trying to predict what sales were going\nto be for a given amount of advertising.\nBut I could come up with a line that does a better job of predicting\nthe training data, and it would be something that looks like this,\njust connecting all of the various different data points.\nAnd now there is no loss at all.\nNow I've perfectly predicted, given any advertising, what sales are.\nAnd for all the data available to me, it's going to be accurate.\nBut it's probably not going to generalize very well.\nI have overfit my model on the training data that is available to me.\nAnd so in general, we want to avoid overfitting.\nWe'd like strategies to make sure that we haven't overfit our model\nto a particular data set.\nAnd there are a number of ways that you could try to do this.\nOne way is by examining what it is that we're optimizing for.\nIn an optimization problem, all we do is we say, there is some cost,\nand I want to minimize that cost.\nAnd so far, we've defined that cost function, the cost of a hypothesis,\njust as being equal to the empirical loss of that hypothesis,\nlike how far away are the actual data points, the outputs,\naway from what I predicted them to be based on that particular hypothesis.\nAnd if all you're trying to do is minimize cost, meaning minimizing\nthe loss in this case, then the result is going to be that you might overfit,\nthat to minimize cost, you're going to try and find a way to perfectly match\nall the input data.\nAnd that might happen as a result of overfitting\non that particular input data.\nSo in order to address this, you could add something to the cost function.\nWhat counts as cost will not just loss, but also\nsome measure of the complexity of the hypothesis.\nThe word the complexity of the hypothesis is something\nthat you would need to define for how complicated does our line look.\nThis is sort of an Occam's razor-style approach\nwhere we want to give preference to a simpler decision boundary,\nlike a straight line, for example, some simpler curve, as opposed\nto something far more complex that might represent the training data better\nbut might not generalize as well.\nWe'll generally say that a simpler solution is probably the better solution\nand probably the one that is more likely to generalize well to other inputs.\nSo we measure what the loss is, but we also measure the complexity.\nAnd now that all gets taken into account when we consider the overall cost,\nthat yes, something might have less loss if it better predicts the training\ndata, but if it's much more complex, it still\nmight not be the best option that we have.\nAnd we need to come up with some balance between loss and complexity.\nAnd for that reason, you'll often see this represented\nas multiplying the complexity by some parameter that we have to choose,\nparameter lambda in this case, where we're saying if lambda is a greater\nvalue, then we really want to penalize more complex hypotheses.\nWhereas if lambda is smaller, we're going to penalize more complex hypotheses\na little bit, and it's up to the machine learning programmer\nto decide where they want to set that value of lambda\nfor how much do I want to penalize a more complex hypothesis that\nmight fit the data a little better.\nAnd again, there's no one right answer to a lot of these things,\nbut depending on the data set, depending on the data you have available to you\nand the problem you're trying to solve, your choice of these parameters\nmay vary, and you may need to experiment a little bit\nto figure out what the right choice of that is ultimately going to be.\nThis process, then, of considering not only loss,\nbut also some measure of the complexity is known as regularization.\nRegularization is the process of penalizing a hypothesis that\nis more complex in order to favor a simpler hypothesis that is more\nlikely to generalize well, more likely to be\nable to apply to other situations that are dealing with other input points\nunlike the ones that we've necessarily seen before.\nSo oftentimes, you'll see us add some regularizing term\nto what we're trying to minimize in order to avoid this problem of overfitting.\nNow, another way of making sure we don't overfit\nis to run some experiments and to see whether or not\nwe are able to generalize our model that we've created to other data sets\nas well.\nAnd it's for that reason that oftentimes when you're\ndoing a machine learning experiment, when you've got some data\nand you want to try and come up with some function that predicts,\ngiven some input, what the output is going to be,\nyou don't necessarily want to do your training on all of the data\nyou have available to you that you could employ\na method known as holdout cross-validation,\nwhere in holdout cross-validation, we split up our data.\nWe split up our data into a training set and a testing set.\nThe training set is the set of data that we're\ngoing to use to train our machine learning model.\nAnd the testing set is the set of data that we're\ngoing to use in order to test to see how well our machine learning\nmodel actually performed.\nSo the learning happens on the training set.\nWe figure out what the parameters should be.\nWe figure out what the right model is.\nAnd then we see, all right, now that we've trained the model,\nwe'll see how well it does at predicting things\ninside of the testing set, some set of data that we haven't seen before.\nAnd the hope then is that we're going to be\nable to predict the testing set pretty well\nif we're able to generalize based on the training\ndata that's available to us.\nIf we've overfit the training data, though,\nand we're not able to generalize, well, then when we look at the testing set,\nit's likely going to be the case that we're not\ngoing to predict things in the testing set nearly as effectively.\nSo this is one method of cross-validation,\nvalidating to make sure that the work we have done\nis actually going to generalize to other data sets as well.\nAnd there are other statistical techniques we can use as well.\nOne of the downsides of this just hold out cross-validation\nis if you say I just split it 50-50, I train using 50% of the data\nand test using the other 50%, or you could choose other percentages as well,\nis that there is a fair amount of data that I am now not using to train,\nthat I might be able to get a better model as a result, for example.\nSo one approach is known as k-fold cross-validation.\nIn k-fold cross-validation, rather than just divide things into two sets\nand run one experiment, we divide things into k different sets.\nSo maybe I divide things up into 10 different sets\nand then run 10 different experiments.\nSo if I split up my data into 10 different sets of data,\nthen what I'll do is each time for each of my 10 experiments,\nI will hold out one of those sets of data, where I'll say,\nlet me train my model on these nine sets,\nand then test to see how well it predicts on set number 10.\nAnd then pick another set of nine sets to train on,\nand then test it on the other one that I held out,\nwhere each time I train the model on everything\nminus the one set that I'm holding out, and then\ntest to see how well our model performs on the test that I did hold out.\nAnd what you end up getting is 10 different results,\n10 different answers for how accurately our model worked.\nAnd oftentimes, you could just take the average of those 10\nto get an approximation for how well we think our model performs overall.\nBut the key idea is separating the training data from the testing data,\nbecause you want to test your model on data\nthat is different from what you trained the model on.\nBecause the training, you want to avoid overfitting.\nYou want to be able to generalize.\nAnd the way you test whether you're able to generalize\nis by looking at some data that you haven't seen before\nand seeing how well we're actually able to perform.\nAnd so if we want to actually implement any of these techniques\ninside of a programming language like Python, number of ways we could do that.\nWe could write this from scratch on our own,\nbut there are libraries out there that allow\nus to take advantage of existing implementations of these algorithms,\nthat we can use the same types of algorithms\nin a lot of different situations.\nAnd so there's a library, very popular one, known as Scikit-learn,\nwhich allows us in Python to be able to very quickly get\nset up with a lot of these different machine learning models.\nThis library has already written an algorithm\nfor nearest neighbor classification, for doing perceptron learning,\nfor doing a bunch of other types of inference and supervised learning\nthat we haven't yet talked about.\nBut using it, we can begin to try actually testing how these methods work\nand how accurately they perform.\nSo let's go ahead and take a look at one approach\nto trying to solve this type of problem.\nAll right, so I'm first going to pull up banknotes.csv, which\nis a whole bunch of data provided by UC Irvine, which\nis information about various different banknotes\nthat people took pictures of various different banknotes\nand measured various different properties of those banknotes.\nAnd in particular, some human categorized each of those banknotes\nas either a counterfeit banknote or as not counterfeit.\nAnd so what you're looking at here is each row represents one banknote.\nThis is formatted as a CSV spreadsheet, where just comma separated values\nseparating each of these various different fields.\nWe have four different input values for each of these data points,\njust information, some measurement that was made on the banknote.\nAnd what those measurements exactly are aren't as important as the fact\nthat we do have access to this data.\nBut more importantly, we have access for each of these data points\nto a label, where 0 indicates something like this was not a counterfeit bill,\nmeaning it was an authentic bill.\nAnd a data point labeled 1 means that it is a counterfeit bill,\nat least according to the human researcher who labeled this particular data.\nSo we have a whole bunch of data representing\na whole bunch of different data points, each of which\nhas these various different measurements that\nwere made on that particular bill, and each of which\nhas an output value, 0 or 1, 0 meaning it was a genuine bill, 1 meaning\nit was a counterfeit bill.\nAnd what we would like to do is use supervised learning\nto begin to predict or model some sort of function that\ncan take these four values as input and predict what the output would be.\nWe want our learning algorithm to find some sort of pattern\nthat is able to predict based on these measurements, something\nthat you could measure just by taking a photo of a bill,\npredict whether that bill is authentic or whether that bill is counterfeit.\nAnd so how can we do that?\nWell, I'm first going to open up banknote0.py\nand see how it is that we do this.\nI'm first importing a lot of things from Scikit-learn,\nbut importantly, I'm going to set my model equal to the perceptron model,\nwhich is one of those models that we talked about before.\nWe're just going to try and figure out some setting of weights\nthat is able to divide our data into two different groups.\nThen I'm going to go ahead and read data in for my file from banknotes.csv.\nAnd basically, for every row, I'm going to separate that row\ninto the first four values of that row, which is the evidence for that row.\nAnd then the label, where if the final column in that row is a 0,\nthe label is authentic.\nAnd otherwise, it's going to be counterfeit.\nSo I'm effectively reading data in from the CSV file,\ndividing into a whole bunch of rows where each row has some evidence,\nthose four input values that are going to be inputs to my hypothesis function.\nAnd then the label, the output, whether it is authentic or counterfeit,\nthat is the thing that I am then trying to predict.\nSo the next step is that I would like to split up my data set\ninto a training set and a testing set, some set of data\nthat I would like to train my machine learning model on,\nand some set of data that I would like to use to test that model,\nsee how well it performed.\nSo what I'll do is I'll go ahead and figure out length of the data,\nhow many data points do I have.\nI'll go ahead and take half of them, save that number as a number called holdout.\nThat is how many items I'm going to hold out for my data set\nto save for the testing phase.\nI'll randomly shuffle the data so it's in some random order.\nAnd then I'll say my testing set will be all of the data up to the holdout.\nSo I'll take holdout many data items, and that will be my testing set.\nMy training data will be everything else, the information\nthat I'm going to train my model on.\nAnd then I'll say I need to divide my training data into two different sets.\nI need to divide it into my x values, where x here represents the inputs.\nSo the x values, the x values that I'm going to train on,\nare basically for every row in my training set,\nI'm going to get the evidence for that row, those four values,\nwhere it's basically a vector of four numbers, where\nthat is going to be all of the input.\nAnd then I need the y values.\nWhat are the outputs that I want to learn from,\nthe labels that belong to each of these various different input points?\nWell, that's going to be the same thing for each row in the training data.\nBut this time, I take that row and get what its label is,\nwhether it is authentic or counterfeit.\nSo I end up with one list of all of these vectors of my input data,\nand one list, which follows the same order,\nbut is all of the labels that correspond with each of those vectors.\nAnd then to train my model, which in this case is just this perceptron model,\nI just call model.fit, pass in the training data,\nand what the labels for those training data are.\nAnd scikit-learn will take care of fitting the model,\nwill do the entire algorithm for me.\nAnd then when it's done, I can then test to see how well that model performed.\nSo I can say, let me get all of these input vectors\nfor what I want to test on.\nSo for each row in my testing data set, go ahead and get the evidence.\nAnd the y values, those are what the actual values were\nfor each of the rows in the testing data set, what the actual label is.\nBut then I'm going to generate some predictions.\nI'm going to use this model and try and predict,\nbased on the testing vectors, I want to predict what the output is.\nAnd my goal then is to now compare y testing with predictions.\nI want to see how well my predictions, based on the model,\nactually reflect what the y values were, what the output is,\nthat were actually labeled.\nBecause I now have this label data, I can assess how well the algorithm worked.\nAnd so now I can just compute how well we did.\nI'm going to, this zip function basically just lets\nme look through two different lists, one by one at the same time.\nSo for each actual value and for each predicted value,\nif the actual is the same thing as what I predicted,\nI'll go ahead and increment the counter by one.\nOtherwise, I'll increment my incorrect counter by one.\nAnd so at the end, I can print out, here are the results,\nhere's how many I got right, here's how many I got wrong,\nand here was my overall accuracy, for example.\nSo I can go ahead and run this.\nI can run python banknote0.py.\nAnd it's going to train on half the data set\nand then test on half the data set.\nAnd here are the results for my perceptron model.\nIn this case, it correctly was able to classify 679 bills as correctly\neither authentic or counterfeit and incorrectly classified seven of them\nfor an overall accuracy of close to 99% accurate.\nSo on this particular data set, using this perceptron model,\nwe were able to predict very well what the output was going to be.\nAnd we can try different models, too, that scikit-learn\nmakes it very easy just to swap out one model for another model.\nSo instead of the perceptron model, I can use the support vector machine\nusing the SVC, otherwise known as a support vector classifier,\nusing a support vector machine to classify things\ninto two different groups.\nAnd now see, all right, how well does this perform?\nAnd all right, this time, we were able to correctly predict 682\nand incorrectly predicted four for accuracy of 99.4%.\nAnd we could even try the k-neighbors classifier as the model instead.\nAnd this takes a parameter, n neighbors, for how many neighbors\ndo you want to look at?\nLet's just look at one neighbor, the one nearest neighbor,\nand use that to predict.\nGo ahead and run this as well.\nAnd it looks like, based on the k-neighbors classifier,\nlooking at just one neighbor, we were able to correctly classify\n685 data points, incorrectly classified one.\nMaybe let's try three neighbors instead, instead of just using one neighbor.\nDo more of a k-nearest neighbors approach,\nwhere I look at the three nearest neighbors and see how that performs.\nAnd that one, in this case, seems to have gotten 100% of all of the predictions\ncorrectly described as either authentic banknotes\nor as counterfeit banknotes.\nAnd we could run these experiments multiple times,\nbecause I'm randomly reorganizing the data every time.\nWe're technically training these on slightly different data sets.\nAnd so you might want to run multiple experiments to really see\nhow well they're actually going to perform.\nBut in short, they all perform very well.\nAnd while some of them perform slightly better than others here,\nthat might not always be the case for every data set.\nBut you can begin to test now by very quickly putting together\nthese machine learning models using Scikit-learn\nto be able to train on some training set and then\ntest on some testing set as well.\nAnd this splitting up into training groups and testing groups and testing\nhappens so often that Scikit-learn has functions built in for trying to do it.\nI did it all by hand just now.\nBut if we take a look at banknotes one, we\ntake advantage of some other features that exist in Scikit-learn,\nwhere we can really simplify a lot of our logic,\nthat there is a function built into Scikit-learn called train test split,\nwhich will automatically split data into a training group and a testing group.\nI just have to say what proportion should be in the testing group, something\nlike 0.5, half the data inside the testing group.\nThen I can fit the model on the training data,\nmake the predictions on the testing data, and then just count up.\nAnd Scikit-learn has some nice methods for just counting up\nhow many times our testing data match the predictions,\nhow many times our testing data didn't match the predictions.\nSo very quickly, you can write programs with not all that many lines of code.\nIt's maybe like 40 lines of code to get through all of these predictions.\nAnd then as a result, see how well we're able to do.\nSo these types of libraries can allow us, without really knowing\nthe implementation details of these algorithms,\nto be able to use the algorithms in a very practical way\nto be able to solve these types of problems.\nSo that then was supervised learning, this task\nof given a whole set of data, some input output pairs,\nwe would like to learn some function that maps those inputs to those outputs.\nBut turns out there are other forms of learning as well.\nAnd another popular type of machine learning, especially nowadays,\nis known as reinforcement learning.\nAnd the idea of reinforcement learning is rather than just\nbeing given a whole data set at the beginning of input output pairs,\nreinforcement learning is all about learning from experience.\nIn reinforcement learning, our agent, whether it's\nlike a physical robot that's trying to make actions in the world\nor just some virtual agent that is a program running somewhere,\nour agent is going to be given a set of rewards or punishments\nin the form of numerical values.\nBut you can think of them as reward or punishment.\nAnd based on that, it learns what actions to take in the future,\nthat our agent, our AI, will be put in some sort of environment.\nIt will make some actions.\nAnd based on the actions that it makes, it learns something.\nIt either gets a reward when it does something well,\nit gets a punishment when it does something poorly,\nand it learns what to do or what not to do in the future\nbased on those individual experiences.\nAnd so what this will often look like is it will often\nstart with some agent, some AI, which might, again, be a physical robot,\nif you're imagining a physical robot moving around,\nbut it can also just be a program.\nAnd our agent is situated in their environment,\nwhere the environment is where they're going to make their actions,\nand it's what's going to give them rewards or punishments\nfor various actions that they're in.\nSo for example, the environment is going to start off\nby putting our agent inside of a state.\nOur agent has some state that, in a game,\nmight be the state of the game that the agent is playing.\nIn a world that the agent is exploring might\nbe some position inside of a grid representing the world\nthat they're exploring.\nBut the agent is in some sort of state.\nAnd in that state, the agent needs to choose to take an action.\nThe agent likely has multiple actions they can choose from,\nbut they pick an action.\nSo they take an action in a particular state.\nAnd as a result of that, the agent will generally\nget two things in response as we model them.\nThe agent gets a new state that they find themselves in.\nAfter being in this state, taking one action,\nthey end up in some other state.\nAnd they're also given some sort of numerical reward,\npositive meaning reward, meaning it was a good thing,\nnegative generally meaning they did something bad,\nthey received some sort of punishment.\nAnd that is all the information the agent has.\nIt's told what state it's in.\nIt makes some sort of action.\nAnd based on that, it ends up in another state.\nAnd it ends up getting some particular reward.\nAnd it needs to learn, based on that information, what actions\nto begin to take in the future.\nAnd so you could imagine generalizing this to a lot\nof different situations.\nThis is oftentimes how you train if you've ever seen those robots that\nare now able to walk around the way humans do.\nIt would be quite difficult to program the robot in exactly the right way\nto get it to walk the way humans do.\nYou could instead train it through reinforcement learning,\ngive it some sort of numerical reward every time it does something good,\nlike take steps forward, and punish it every time it does something\nbad, like fall over, and then let the AI just\nlearn based on that sequence of rewards, based\non trying to take various different actions.\nYou can begin to have the agent learn what to do in the future\nand what not to do.\nSo in order to begin to formalize this, the first thing we need to do\nis formalize this notion of what we mean about states and actions and rewards,\nlike what does this world look like?\nAnd oftentimes, we'll formulate this world\nas what's known as a Markov decision process, similar in spirit\nto Markov chains, which you might recall from before.\nBut a Markov decision process is a model that we\ncan use for decision making, for an agent trying\nto make decisions in its environment.\nAnd it's a model that allows us to represent the various different states\nthat an agent can be in, the various different actions that they can take,\nand also what the reward is for taking one action as opposed to another action.\nSo what then does it actually look like?\nWell, if you recall a Markov chain from before,\na Markov chain looked a little something like this,\nwhere we had a whole bunch of these individual states,\nand each state immediately transitioned to another state\nbased on some probability distribution.\nWe saw this in the context of the weather before, where if it was sunny,\nwe said with some probability, it'll be sunny the next day.\nWith some other probability, it'll be rainy, for example.\nBut we could also imagine generalizing this.\nIt's not just sun and rain anymore.\nWe just have these states, where one state leads to another state\naccording to some probability distribution.\nBut in this original model, there was no agent\nthat had any control over this process.\nIt was just entirely probability based, where with some probability,\nwe moved to this next state.\nBut maybe it's going to be some other state with some other probability.\nWhat we'll now have is the ability for the agent in this state\nto choose from a set of actions, where maybe instead of just one path\nforward, they have three different choices of actions that each lead up\ndown different paths.\nAnd even this is a bit of an oversimplification,\nbecause in each of these states, you might imagine more branching points\nwhere there are more decisions that can be taken as well.\nSo we've extended the Markov chain to say that from a state,\nyou now have available action choices.\nAnd each of those actions might be associated\nwith its own probability distribution of going to various different states.\nThen in addition, we'll add another extension,\nwhere any time you move from a state, taking an action,\ngoing into this other state, we can associate a reward with that outcome,\nsaying either r is positive, meaning some positive reward,\nor r is negative, meaning there was some sort of punishment.\nAnd this then is what we'll consider to be a Markov decision process.\nThat a Markov decision process has some initial set\nof states, of states in the world that we can be in.\nWe have some set of actions that, given a state,\nI can say, what are the actions that are available to me in that state,\nan action that I can choose from?\nThen we have some transition model.\nThe transition model before just said that, given my current state,\nwhat is the probability that I end up in that next state or this other state?\nThe transition model now has effectively two things we're conditioning on.\nWe're saying, given that I'm in this state and that I take this action,\nwhat's the probability that I end up in this next state?\nNow maybe we live in a very deterministic world in this Markov decision process.\nWe're given a state and given an action.\nWe know for sure what next state we'll end up in.\nBut maybe there's some randomness in the world\nthat when you take in a state and you take an action,\nyou might not always end up in the exact same state.\nThere might be some probabilities involved there as well.\nThe Markov decision process can handle both of those possible cases.\nAnd then finally, we have a reward function, generally called r,\nthat in this case says, what is the reward for being in this state,\ntaking this action, and then getting to s prime this next state?\nSo I'm in this original state.\nI take this action.\nI get to this next state.\nWhat is the reward for doing that process?\nAnd you can add up these rewards every time you take an action\nto get the total amount of rewards that an agent might\nget from interacting in a particular environment\nmodeled using this Markov decision process.\nSo what might this actually look like in practice?\nWell, let's just create a little simulated world here\nwhere I have this agent that is just trying to navigate its way.\nThis agent is this yellow dot here, like a robot in the world,\ntrying to navigate its way through this grid.\nAnd ultimately, it's trying to find its way to the goal.\nAnd if it gets to the green goal, then it's going to get some sort of reward.\nBut then we might also have some red squares that are places\nwhere you get some sort of punishment, some bad place where we don't want\nthe agent to go.\nAnd if it ends up in the red square, then our agent\nis going to get some sort of punishment as a result of that.\nBut the agent originally doesn't know all of these details.\nIt doesn't know that these states are associated with punishments.\nBut maybe it does know that this state is associated with a reward.\nMaybe it doesn't.\nBut it just needs to sort of interact with the environment\nto try and figure out what to do and what not to do.\nSo the first thing the agent might do is,\ngiven no additional information, if it doesn't know what the punishments are,\nit doesn't know where the rewards are, it just might try and take an action.\nAnd it takes an action and ends up realizing\nthat it got some sort of punishment.\nAnd so what does it learn from that experience?\nWell, it might learn that when you're in this state in the future,\ndon't take the action move to the right, that that is a bad action to take.\nThat in the future, if you ever find yourself back in the state,\ndon't take this action of going to the right\nwhen you're in this particular state, because that leads to punishment.\nThat might be the intuition at least.\nAnd so you could try doing other actions.\nYou move up, all right, that didn't lead to any immediate rewards.\nMaybe try something else.\nThen maybe try something else.\nAnd all right, now you found that you got another punishment.\nAnd so you learn something from that experience.\nSo the next time you do this whole process,\nyou know that if you ever end up in this square,\nyou shouldn't take the down action, because being in this state\nand taking that action ultimately leads to some sort of punishment,\na negative reward, in other words.\nAnd this process repeats.\nYou might imagine just letting our agent explore the world,\nlearning over time what states tend to correspond with poor actions,\nlearning over time what states correspond with poor actions,\nuntil eventually, if it tries enough things randomly,\nit might find that eventually when you get to this state,\nif you take the up action in this state, it\nmight find that you actually get a reward from that.\nAnd what it can learn from that is that if you're in this state,\nyou should take the up action, because that leads to a reward.\nAnd over time, you can also learn that if you're in this state,\nyou should take the left action, because that leads to this state that also\nlets you eventually get to the reward.\nSo you begin to learn over time not only which actions\nare good in particular states, but also which actions are bad,\nsuch that once you know some sequence of good actions that\nleads you to some sort of reward, our agent can just follow those\ninstructions, follow the experience that it has learned.\nWe didn't tell the agent what the goal was.\nWe didn't tell the agent where the punishments were.\nBut the agent can begin to learn from this experience\nand learn to begin to perform these sorts of tasks better in the future.\nAnd so let's now try to formalize this idea, formalize the idea\nthat we would like to be able to learn in this state taking this action,\nis that a good thing or a bad thing?\nThere are lots of different models for reinforcement learning.\nWe're just going to look at one of them today.\nAnd the one that we're going to look at is a method known as Q-learning.\nAnd what Q-learning is all about is about learning\na function, a function Q, that takes inputs S and A, where S is a state\nand A is an action that you take in that state.\nAnd what this Q function is going to do is it is going to estimate the value.\nHow much reward will I get from taking this action in this state?\nOriginally, we don't know what this Q function should be.\nBut over time, based on experience, based on trying things out\nand seeing what the result is, I would like to try and learn\nwhat Q of SA is for any particular state and any particular action\nthat I might take in that state.\nSo what is the approach?\nWell, the approach originally is we'll start with Q SA equal to 0 for all\nstates S and for all actions A. That initially,\nbefore I've ever started anything, before I've had any experiences,\nI don't know the value of taking any action in any given state.\nSo I'm going to assume that the value is just 0 all across the board.\nBut then as I interact with the world, as I experience rewards or punishments,\nor maybe I go to a cell where I don't get either reward or a punishment,\nI want to somehow update my estimate of Q SA.\nI want to continually update my estimate of Q SA\nbased on the experiences and rewards and punishments that I've received,\nsuch that in the future, my knowledge of what actions are good\nand what states will be better.\nSo when we take an action and receive some sort of reward,\nI want to estimate the new value of Q SA.\nAnd I estimate that based on a couple of different things.\nI estimate it based on the reward that I'm getting from taking this action\nand getting into the next state.\nBut assuming the situation isn't over, assuming there are still\nfuture actions that I might take as well,\nI also need to take into account the expected future rewards.\nThat if you imagine an agent interacting with the environment,\nthen sometimes you'll take an action and get a reward,\nbut then you can keep taking more actions and get more rewards,\nthat these both are relevant, both the current reward\nI'm getting from this current step and also my future reward.\nAnd it might be the case that I'll want to take a step that\ndoesn't immediately lead to a reward, because later on down the line,\nI know it will lead to more rewards as well.\nSo there's a balancing act between current rewards\nthat the agent experiences and future rewards\nthat the agent experiences as well.\nAnd then we need to update QSA.\nSo we estimate the value of QSA based on the current reward\nand the expected future rewards.\nAnd then we need to update this Q function\nto take into account this new estimate.\nNow, we already, as we go through this process,\nwe'll already have an estimate for what we think the value is.\nNow we have a new estimate, and then somehow we\nneed to combine these two estimates together,\nand we'll look at more formal ways that we can actually begin to do that.\nSo to actually show you what this formula looks like,\nhere is the approach we'll take with Q learning.\nWe're going to, again, start with Q of S and A being equal to 0 for all states.\nAnd then every time we take an action A in state S and observer reward R,\nwe're going to update our value, our estimate, for Q of SA.\nAnd the idea is that we're going to figure out\nwhat the new value estimate is minus what our existing value estimate is.\nAnd so we have some preconceived notion for what the value is\nfor taking this action in this state.\nMaybe our expectation is we currently think the value is 10.\nBut then we're going to estimate what we now think it's going to be.\nMaybe the new value estimate is something like 20.\nSo there's a delta of 10 that our new value estimate\nis 10 points higher than what our current value estimate happens to be.\nAnd so we have a couple of options here.\nWe need to decide how much we want to adjust\nour current expectation of what the value is\nof taking this action in this particular state.\nAnd what that difference is, how much we add or subtract\nfrom our existing notion of how much do we expect the value to be,\nis dependent on this parameter alpha, also called a learning rate.\nAnd alpha represents, in effect, how much we value new information\ncompared to how much we value old information.\nAn alpha value of 1 means we really value new information.\nBut if we have a new estimate, then it doesn't\nmatter what our old estimate is.\nWe're only going to consider our new estimate\nbecause we always just want to take into consideration our new information.\nSo the way that works is that if you imagine alpha being 1,\nwell, then we're taking the old value of QSA\nand then adding 1 times the new value minus the old value.\nAnd that just leaves us with the new value.\nSo when alpha is 1, all we take into consideration\nis what our new estimate happens to be.\nBut over time, as we go through a lot of experiences,\nwe already have some existing information.\nWe might have tried taking this action nine times already.\nAnd now we just tried it a 10th time.\nAnd we don't only want to consider this 10th experience.\nI also want to consider the fact that my prior nine experiences, those\nwere meaningful, too.\nAnd that's data I don't necessarily want to lose.\nAnd so this alpha controls that decision,\ncontrols how important is the new information.\n0 would mean ignore all the new information.\nJust keep this Q value the same.\n1 means replace the old information entirely with the new information.\nAnd somewhere in between, keep some sort of balance between these two values.\nWe can put this equation a little bit more formally as well.\nThe old value estimate is our old estimate\nfor what the value is of taking this action in a particular state.\nThat's just Q of SNA.\nSo we have it once here, and we're going to add something to it.\nWe're going to add alpha times the new value estimate\nminus the old value estimate.\nBut the old value estimate, we just look up by calling this Q function.\nAnd what then is the new value estimate?\nBased on this experience we have just taken,\nwhat is our new estimate for the value of taking\nthis action in this particular state?\nWell, it's going to be composed of two parts.\nIt's going to be composed of what reward did I just\nget from taking this action in this state.\nAnd then it's going to be, what can I expect my future rewards\nto be from this point forward?\nSo it's going to be R, some reward I'm getting right now,\nplus whatever I estimate I'm going to get in the future.\nAnd how do I estimate what I'm going to get in the future?\nWell, it's a bit of another call to this Q function.\nIt's going to be take the maximum across all possible actions\nI could take next and say, all right, of all of these possible actions\nI could take, which one is going to have the highest reward?\nAnd so this then looks a little bit complicated.\nThis is going to be our notion for how we're\ngoing to perform this kind of update.\nI have some estimate, some old estimate, for what the value is\nof taking this action in this state.\nAnd I'm going to update it based on new information\nthat I experience some reward.\nI predict what my future reward is going to be.\nAnd using that I update what I estimate the reward will\nbe for taking this action in this particular state.\nAnd there are other additions you might make to this algorithm as well.\nSometimes it might not be the case that future rewards\nyou want to wait equally to current rewards.\nMaybe you want an agent that values reward now over reward later.\nAnd so sometimes you can even add another term in here, some other parameter,\nwhere you discount future rewards and say future rewards are not\nas valuable as rewards immediately.\nThat getting reward in the current time step\nis better than waiting a year and getting rewards later.\nBut that's something up to the programmer\nto decide what that parameter ought to be.\nBut the big picture idea of this entire formula\nis to say that every time we experience some new reward,\nwe take that into account.\nWe update our estimate of how good is this action.\nAnd then in the future, we can make decisions based on that algorithm.\nOnce we have some good estimate for every state and for every action,\nwhat the value is of taking that action, then we\ncan do something like implement a greedy decision making policy.\nThat if I am in a state and I want to know what action\nshould I take in that state, well, then I\nconsider for all of my possible actions, what is the value of QSA?\nWhat is my estimated value of taking that action in that state?\nAnd I will just pick the action that has the highest value\nafter I evaluate that expression.\nSo I pick the action that has the highest value.\nAnd based on that, that tells me what action I should take.\nAt any given state that I'm in, I can just greedily say across all my actions,\nthis action gives me the highest expected value.\nAnd so I'll go ahead and choose that action as the action that I take as well.\nBut there is a downside to this kind of approach.\nAnd then downside comes up in a situation like this,\nwhere we know that there is some solution that gets me to the reward.\nAnd our agent has been able to figure that out.\nBut it might not necessarily be the best way or the fastest way.\nIf the agent is allowed to explore a little bit more,\nit might find that it can get the reward faster\nby taking some other route instead, by going through this particular path\nthat is a faster way to get to that ultimate goal.\nAnd maybe we would like for the agent to be able to figure that out as well.\nBut if the agent always takes the actions that it knows to be best,\nwell, when it gets to this particular square,\nit doesn't know that this is a good action because it's never really tried it.\nBut it knows that going down eventually leads its way to this reward.\nSo it might learn in the future that it should just always take this route\nand it's never going to explore and go along that route instead.\nSo in reinforcement learning, there is this tension\nbetween exploration and exploitation.\nAnd exploitation generally refers to using knowledge that the AI already has.\nThe AI already knows that this is a move that leads to reward.\nSo we'll go ahead and use that move.\nAnd exploration is all about exploring other actions\nthat we may not have explored as thoroughly before\nbecause maybe one of these actions, even if I don't know anything about it,\nmight lead to better rewards faster or to more rewards in the future.\nAnd so an agent that only ever exploits information and never explores\nmight be able to get reward, but it might not maximize its rewards\nbecause it doesn't know what other possibilities are out there,\npossibilities that we only know about by taking advantage of exploration.\nAnd so how can we try and address this?\nWell, one possible solution is known as the Epsilon greedy algorithm,\nwhere we set Epsilon equal to how often we want to just make a random move,\nwhere occasionally we will just make a random move in order to say,\nlet's try to explore and see what happens.\nAnd then the logic of the algorithm will be with probability 1 minus Epsilon,\nchoose the estimated best move.\nIn a greedy case, we'd always choose the best move.\nBut in Epsilon greedy, we're most of the time\ngoing to choose the best move or sometimes going to choose the best move.\nBut sometimes with probability Epsilon, we're\ngoing to choose a random move instead.\nSo every time we're faced with the ability to take an action,\nsometimes we're going to choose the best move.\nSometimes we're just going to choose a random move.\nSo this type of algorithm can be quite powerful in a reinforcement learning\ncontext by not always just choosing the best possible move right now,\nbut sometimes, especially early on, allowing yourself\nto make random moves that allow you to explore various different possible\nstates and actions more, and maybe over time,\nyou might decrease your value of Epsilon.\nMore and more often, choosing the best move\nafter you're more confident that you've explored\nwhat all of the possibilities actually are.\nSo we can put this into practice.\nAnd one very common application of reinforcement learning\nis in game playing, that if you want to teach an agent how to play a game,\nyou just let the agent play the game a whole bunch.\nAnd then the reward signal happens at the end of the game.\nWhen the game is over, if our AI won the game,\nit gets a reward of like 1, for example.\nAnd if it lost the game, it gets a reward of negative 1.\nAnd from that, it begins to learn what actions are good\nand what actions are bad.\nYou don't have to tell the AI what's good and what's bad,\nbut the AI figures it out based on that reward.\nWinning the game is some signal, losing the game is some signal,\nand based on all of that, it begins to figure out\nwhat decisions it should actually make.\nSo one very simple game, which you may have played before, is a game called\nNim.\nAnd in the game of Nim, you've got a whole bunch of objects\nin a whole bunch of different piles, where here I've\nrepresented each pile as an individual row.\nSo you've got one object in the first pile,\nthree in the second pile, five in the third pile, seven in the fourth pile.\nAnd the game of Nim is a two player game\nwhere players take turns removing objects from piles.\nAnd the rule is that on any given turn, you\nwere allowed to remove as many objects as you want from any one of these piles,\nany one of these rows.\nYou have to remove at least one object, but you\nremove as many as you want from exactly one of the piles.\nAnd whoever takes the last object loses.\nSo player one might remove four from this pile here.\nPlayer two might remove four from this pile here.\nSo now we've got four piles left, one, three, one, and three.\nPlayer one might remove the entirety of the second pile.\nPlayer two, if they're being strategic, might remove two from the third pile.\nNow we've got three piles left, each with one object left.\nPlayer one might remove one from one pile.\nPlayer two removes one from the other pile.\nAnd now player one is left with choosing this one object from the last pile,\nat which point player one loses the game.\nSo fairly simple game.\nPiles of objects, any turn you choose how many objects\nto remove from a pile, whoever removes the last object loses.\nAnd this is the type of game you could encode into an AI fairly easily,\nbecause the states are really just four numbers.\nEvery state is just how many objects in each of the four piles.\nAnd the actions are things like, how many\nam I going to remove from each one of these individual piles?\nAnd the reward happens at the end, that if you\nwere the player that had to remove the last object,\nthen you get some sort of punishment.\nBut if you were not, and the other player\nhad to remove the last object, well, then you get some sort of reward.\nSo we could actually try and show a demonstration of this,\nthat I've implemented an AI to play the game of Nim.\nAll right, so here, what we're going to do is create an AI\nas a result of training the AI on some number of games,\nthat the AI is going to play against itself, where the idea is the AI will\nplay games against itself, learn from each of those experiences,\nand learn what to do in the future.\nAnd then I, the human, will play against the AI.\nSo initially, we'll say train zero times,\nmeaning we're not going to let the AI play any practice games against itself\nin order to learn from its experiences.\nWe're just going to see how well it plays.\nAnd it looks like there are four piles.\nI can choose how many I remove from any one of the piles.\nSo maybe from pile three, I will remove five objects, for example.\nSo now, AI chose to take one item from pile zero.\nSo I'm left with these piles now, for example.\nAnd so here, I could choose maybe to say, I\nwould like to remove from pile two, I'll remove all five of them,\nfor example.\nAnd so AI chose to take two away from pile one.\nNow I'm left with one pile that has one object, one pile that has two objects.\nSo from pile three, I will remove two objects.\nAnd now I've left the AI with no choice but to take that last one.\nAnd so the game is over, and I was able to win.\nBut I did so because the AI was really just playing randomly.\nIt didn't have any prior experience that it was using in order\nto make these sorts of judgments.\nNow let me let the AI train itself on 10,000 games.\nI'm going to let the AI play 10,000 games of nim against itself.\nEvery time it wins or loses, it's going to learn from that experience\nand learn in the future what to do and what not to do.\nSo here then, I'll go ahead and run this again.\nAnd now you see the AI running through a whole bunch of training games,\n10,000 training games against itself.\nAnd now it's going to let me make these sorts of decisions.\nSo now I'm going to play against the AI.\nMaybe I'll remove one from pile three.\nAnd the AI took everything from pile three, so I'm left with three piles.\nI'll go ahead and from pile two maybe remove three items.\nAnd the AI removes one item from pile zero.\nI'm left with two piles, each of which has two items in it.\nI'll remove one from pile one, I guess.\nAnd the AI took two from pile two, leaving me with no choice\nbut to take one away from pile one.\nSo it seems like after playing 10,000 games of nim against itself,\nthe AI has learned something about what states and what actions tend to be good\nand has begun to learn some sort of pattern for how\nto predict what actions are going to be good\nand what actions are going to be bad in any given state.\nSo reinforcement learning can be a very powerful technique\nfor achieving these sorts of game-playing agents, agents\nthat are able to play a game well just by learning from experience,\nwhether that's playing against other people\nor by playing against itself and learning from those experiences as well.\nNow, nim is a bit of an easy game to use reinforcement learning for\nbecause there are so few states.\nThere are only states that are as many as how many different objects\nare in each of these various different piles.\nYou might imagine that it's going to be harder if you think of a game like chess\nor games where there are many, many more states and many, many more actions\nthat you can imagine taking, where it's not\ngoing to be as easy to learn for every state and for every action\nwhat the value is going to be.\nSo oftentimes in that case, we can't necessarily\nlearn exactly what the value is for every state and for every action,\nbut we can approximate it.\nSo much as we saw with minimax, so we could use a depth-limiting approach\nto stop calculating at a certain point in time,\nwe can do a similar type of approximation known\nas function approximation in a reinforcement learning context\nwhere instead of learning a value of q for every state and every action,\nwe just have some function that estimates what the value is\nfor taking this action in this particular state that\nmight be based on various different features of the state\nthat the agent happens to be in, where you might have\nto choose what those features actually are.\nBut you can begin to learn some patterns that generalize beyond one\nspecific state and one specific action that you can begin to learn\nif certain features tend to be good things or bad things.\nReinforcement learning can allow you, using a very similar mechanism,\nto generalize beyond one particular state and say,\nif this other state looks kind of like this state,\nthen maybe the similar types of actions that worked in one state\nwill also work in another state as well.\nAnd so this type of approach can be quite helpful\nas you begin to deal with reinforcement learning that\nexist in larger and larger state spaces where it's just not feasible\nto explore all of the possible states that could actually exist.\nSo there, then, are two of the main categories of reinforcement learning.\nSupervised learning, where you have labeled input and output pairs,\nand reinforcement learning, where an agent learns from rewards or punishments\nthat it receives.\nThe third major category of machine learning\nthat we'll just touch on briefly is known as unsupervised learning.\nAnd unsupervised learning happens when we have data\nwithout any additional feedback, without labels,\nthat in the supervised learning case, all of our data had labels.\nWe labeled the data point with whether that was a rainy day or not rainy day.\nAnd using those labels, we were able to infer what the pattern was.\nOr we labeled data as a counterfeit banknote or not a counterfeit.\nAnd using those labels, we were able to draw inferences and patterns\nto figure out what does a banknote look like versus not.\nIn unsupervised learning, we don't have any access to any of those labels.\nBut we still would like to learn some of those patterns.\nAnd one of the tasks that you might want to perform in unsupervised learning\nis something like clustering, where clustering is just\nthe task of, given some set of objects, organize it\ninto distinct clusters, groups of objects that are similar to one another.\nAnd there's lots of applications for clustering.\nIt comes up in genetic research, where you might have\na whole bunch of different genes and you want to cluster them into similar genes\nif you're trying to analyze them across a population or across species.\nIt comes up in an image if you want to take all the pixels of an image,\ncluster them into different parts of the image.\nComes a lot up in market research if you want to divide your consumers\ninto different groups so you know which groups to target with certain types\nof product advertisements, for example, and a number of other contexts\nas well in which clustering can be very applicable.\nOne technique for clustering is an algorithm known as k-means clustering.\nAnd what k-means clustering is going to do\nis it is going to divide all of our data points into k different clusters.\nAnd it's going to do so by repeating this process of assigning points\nto clusters and then moving around those clusters at centers.\nWe're going to define a cluster by its center, the middle of the cluster,\nand then assign points to that cluster based on which\ncenter is closest to that point.\nAnd I'll show you an example of that now.\nHere, for example, I have a whole bunch of unlabeled data,\njust various data points that are in some sort of graphical space.\nAnd I would like to group them into various different clusters.\nBut I don't know how to do that originally.\nAnd let's say I want to assign like three clusters to this group.\nAnd you have to choose how many clusters you want in k-means clustering\nthat you could try multiple and see how well those values perform.\nBut I'll start just by randomly picking some places\nto put the centers of those clusters.\nMaybe I have a blue cluster, a red cluster, and a green cluster.\nAnd I'm going to start with the centers of those clusters\njust being in these three locations here.\nAnd what k-means clustering tells us to do\nis once I have the centers of the clusters,\nassign every point to a cluster based on which cluster center it is closest to.\nSo we end up with something like this, where all of these points\nare closer to the blue cluster center than any other cluster center.\nAll of these points here are closer to the green cluster\ncenter than any other cluster center.\nAnd then these two points plus these points over here,\nthose are all closest to the red cluster center instead.\nSo here then is one possible assignment of all these points\nto three different clusters.\nBut it's not great that it seems like in this red cluster,\nthese points are kind of far apart.\nIn this green cluster, these points are kind of far apart.\nIt might not be my ideal choice of how I would cluster\nthese various different data points.\nBut k-means clustering is an iterative process\nthat after I do this, there is a next step, which\nis that after I've assigned all of the points to the cluster center\nthat it is nearest to, we are going to re-center the clusters,\nmeaning take the cluster centers, these diamond shapes here,\nand move them to the middle, or the average,\neffectively, of all of the points that are in that cluster.\nSo we'll take this blue point, this blue center,\nand go ahead and move it to the middle or to the center of all\nof the points that were assigned to the blue cluster,\nmoving it slightly to the right in this case.\nAnd we'll do the same thing for red.\nWe'll move the cluster center to the middle of all of these points,\nweighted by how many points there are.\nThere are more points over here, so the red center ends up\nmoving a little bit further that way.\nAnd likewise, for the green center, there are many more points\non this side of the green center.\nSo the green center ends up being pulled a little bit further\nin this direction.\nSo we re-center all of the clusters, and then we repeat the process.\nWe go ahead and now reassign all of the points to the cluster center\nthat they are now closest to.\nAnd now that we've moved around the cluster centers,\nthese cluster assignments might change.\nThat this point originally was closer to the red cluster center,\nbut now it's actually closer to the blue cluster center.\nSame goes for this point as well.\nAnd these three points that were originally closer to the green cluster\ncenter are now closer to the red cluster center instead.\nSo we can reassign what colors or which clusters each of these data points\nbelongs to, and then repeat the process again,\nmoving each of these cluster means and the middles of the clusterism\nto the mean, the average, of all of the other points that happen to be there,\nand repeat the process again.\nGo ahead and assign each of the points to the cluster\nthat they are closest to.\nSo once we reach a point where we've assigned all the points to clusters\nto the cluster that they are nearest to, and nothing changed,\nwe've reached a sort of equilibrium in this situation,\nwhere no points are changing their allegiance.\nAnd as a result, we can declare this algorithm is now over.\nAnd we now have some assignment of each of these points\ninto three different clusters.\nAnd it looks like we did a pretty good job of trying\nto identify which points are more similar to one another\nthan they are to points in other groups.\nSo we have the green cluster down here, this blue cluster here,\nand then this red cluster over there as well.\nAnd we did so without any access to some labels\nto tell us what these various different clusters were.\nWe just used an algorithm in an unsupervised sense\nwithout any of those labels to figure out which points\nbelonged to which categories.\nAnd again, lots of applications for this type of clustering technique.\nAnd there are many more algorithms in each of these various different fields\nwithin machine learning, supervised and reinforcement and unsupervised.\nBut those are many of the big picture foundational ideas\nthat underlie a lot of these techniques, where these are the problems\nthat we're trying to solve.\nAnd we try and solve those problems using\na number of different methods of trying to take data and learn\npatterns in that data, whether that's trying\nto find neighboring data points that are similar\nor trying to minimize some sort of loss function\nor any number of other techniques that allow us to begin to try\nto solve these sorts of problems.\nThat then was a look at some of the principles\nthat are at the foundation of modern machine learning,\nthis ability to take data and learn from that data\nso that the computer can perform a task even\nif they haven't explicitly been given instructions\nin order to do so.\nNext time, we'll continue this conversation about machine learning,\nlooking at other techniques we can use for solving these sorts of problems.\nWe'll see you then.\nAll right, welcome back, everyone, to an introduction\nto artificial intelligence with Python.\nNow, last time, we took a look at machine learning,\na set of techniques that computers can use in order to take a set of data\nand learn some patterns inside of that data,\nlearn how to perform a task even if we the programmers didn't\ngive the computer explicit instructions for how to perform that task.\nToday, we transition to one of the most popular techniques and tools\nwithin machine learning, that of neural networks.\nAnd neural networks were inspired as early as the 1940s\nby researchers who were thinking about how it is that humans learn,\nstudying neuroscience in the human brain and trying\nto see whether or not we could apply those same ideas to computers\nas well and model computer learning off of human learning.\nSo how is the brain structured?\nWell, very simply put, the brain consists of a whole bunch of neurons.\nAnd those neurons are connected to one another\nand communicate with one another in some way.\nIn particular, if you think about the structure of a biological neural\nnetwork, something like this, there are a couple of key properties\nthat scientists observed.\nOne was that these neurons are connected to each other\nand receive electrical signals from one another,\nthat one neuron can propagate electrical signals to another neuron.\nAnd another point is that neurons process those input signals\nand then can be activated, that a neuron becomes activated at a certain point\nand then can propagate further signals onto neurons in the future.\nAnd so the question then became, could we\ntake this biological idea of how it is that humans learn with brains\nand with neurons and apply that to a machine as well,\nin effect designing an artificial neural network, or an ANN,\nwhich will be a mathematical model for learning\nthat is inspired by these biological neural networks?\nAnd what artificial neural networks will allow us to do\nis they will first be able to model some sort of mathematical function.\nEvery time you look at a neural network, which\nwe'll see more of later today, each one of them\nis really just some mathematical function that\nis mapping certain inputs to particular outputs based\non the structure of the network, that depending on where we place\nparticular units inside of this neural network,\nthat's going to determine how it is that the network is going to function.\nAnd in particular, artificial neural networks\nare going to lend themselves to a way that we can learn what the network's\nparameters should be.\nWe'll see more on that in just a moment.\nBut in effect, we want a model such that it\nis easy for us to be able to write some code that\nallows for the network to be able to figure out\nhow to model the right mathematical function given\na particular set of input data.\nSo in order to create our artificial neural network,\ninstead of using biological neurons, we're just\ngoing to use what we're going to call units, units inside of a neural\nnetwork, which we can represent kind of like a node in a graph, which\nwill here be represented just by a blue circle like this.\nAnd these artificial units, these artificial neurons,\ncan be connected to one another.\nSo here, for instance, we have two units that\nare connected by this edge inside of this graph, effectively.\nAnd so what we're going to do now is think\nof this idea as some sort of mapping from inputs to outputs.\nSo we have one unit that is connected to another unit\nthat we might think of this side of the input and that side of the output.\nAnd what we're trying to do then is to figure out\nhow to solve a problem, how to model some sort of mathematical function.\nAnd this might take the form of something\nwe saw last time, which was something like we have certain inputs,\nlike variables x1 and x2.\nAnd given those inputs, we want to perform some sort of task,\na task like predicting whether or not it's going to rain.\nAnd ideally, we'd like some way, given these inputs, x1 and x2,\nwhich stand for some sort of variables to do with the weather,\nwe would like to be able to predict, in this case, a Boolean classification.\nIs it going to rain, or is it not going to rain?\nAnd we did this last time by way of a mathematical function.\nWe defined some function, h, for our hypothesis function,\nthat took as input x1 and x2, the two inputs that we cared about processing,\nin order to determine whether we thought it was going to rain\nor whether we thought it was not going to rain.\nThe question then becomes, what does this hypothesis function\ndo in order to make that determination?\nAnd we decided last time to use a linear combination of these input variables\nto determine what the output should be.\nSo our hypothesis function was equal to something like this.\nWeight 0 plus weight 1 times x1 plus weight 2 times x2.\nSo what's going on here is that x1 and x2, those are input variables,\nthe inputs to this hypothesis function.\nAnd each of those input variables is being multiplied\nby some weight, which is just some number.\nSo x1 is being multiplied by weight 1, x2 is being multiplied by weight 2.\nAnd we have this additional weight, weight 0,\nthat doesn't get multiplied by an input variable at all,\nthat just serves to either move the function up\nor move the function's value down.\nYou can think of this as either a weight that's just\nmultiplied by some dummy value, like the number 1.\nIt's multiplied by 1, and so it's not multiplied by anything.\nOr sometimes, you'll see in the literature,\npeople call this variable weight 0 a bias,\nso that you can think of these variables as slightly different.\nWe have weights that are multiplied by the input,\nand we separately add some bias to the result as well.\nYou'll hear both of those terminologies used\nwhen people talk about neural networks and machine learning.\nSo in effect, what we've done here is that in order\nto define a hypothesis function, we just need to decide and figure out\nwhat these weights should be to determine\nwhat values to multiply by our inputs to get some sort of result.\nOf course, at the end of this, what we need to do\nis make some sort of classification, like rainy or not rainy.\nAnd to do that, we use some sort of function\nthat defines some sort of threshold.\nAnd so we saw, for instance, the step function,\nwhich is defined as 1 if the result of multiplying the weights by the inputs\nis at least 0, otherwise it's 0.\nAnd you can think of this line down the middle\nas kind of like a dotted line.\nEffectively, it stays at 0 all the way up to one point,\nand then the function steps or jumps up to 1.\nSo it's 0 before it reaches some threshold,\nand then it's 1 after it reaches a particular threshold.\nAnd so this was one way we could define what\nwill come to call an activation function, a function that\ndetermines when it is that this output becomes active, changes to 1\ninstead of being a 0.\nBut we also saw that if we didn't just want a purely binary classification,\nwe didn't want purely 1 or 0, but we wanted\nto allow for some in-between real numbered values,\nwe could use a different function.\nAnd there are a number of choices, but the one that we looked at\nwas the logistic sigmoid function that has sort of an s-shaped curve,\nwhere we could represent this as a probability that\nmay be somewhere in between the probability of rain\nor something like 0.5.\nMaybe a little bit later, the probability of rain is 0.8.\nAnd so rather than just have a binary classification of 0 or 1,\nwe could allow for numbers that are in between as well.\nAnd it turns out there are many other different types of activation\nfunctions, where an activation function just\ntakes the output of multiplying the weights together and adding that bias,\nand then figuring out what the actual output should be.\nAnother popular one is the rectified linear unit, otherwise known as ReLU.\nAnd the way that works is that it just takes its input\nand takes the maximum of that input and 0.\nSo if it's positive, it remains unchanged.\nBut if it's 0, if it's negative, it goes ahead and levels out at 0.\nAnd there are other activation functions that we could choose as well.\nBut in short, each of these activation functions,\nyou can just think of as a function that gets applied\nto the result of all of this computation.\nWe take some function g and apply it to the result of all of that calculation.\nAnd this then is what we saw last time, the way\nof defining some hypothesis function that takes in inputs,\ncalculate some linear combination of those inputs,\nand then passes it through some sort of activation function to get our output.\nAnd this actually turns out to be the model for the simplest of neural\nnetworks, that we're going to instead represent this mathematical idea\ngraphically by using a structure like this.\nHere then is a neural network that has two inputs.\nWe can think of this as x1 and this as x2.\nAnd then one output, which you can think of as classifying whether or not\nwe think it's going to rain or not rain, for example,\nin this particular instance.\nAnd so how exactly does this model work?\nWell, each of these two inputs represents one of our input variables,\nx1 and x2.\nAnd notice that these inputs are connected to this output via these edges,\nwhich are going to be defined by their weights.\nSo these edges each have a weight associated with them, weight 1 and weight\n2.\nAnd then this output unit, what it's going to do\nis it is going to calculate an output based on those inputs\nand based on those weights.\nThis output unit is going to multiply all the inputs by their weights,\nadd in this bias term, which you can think of as an extra w0 term\nthat gets added into it, and then we pass it through an activation function.\nSo this then is just a graphical way of representing the same idea\nwe saw last time just mathematically.\nAnd we're going to call this a very simple neural network.\nAnd we'd like for this neural network to be\nable to learn how to calculate some function,\nthat we want some function for the neural network to learn.\nAnd the neural network is going to learn what should the values of w0,\nw1, and w2 be?\nWhat should the activation function be in order\nto get the result that we would expect?\nSo we can actually take a look at an example of this.\nWhat then is a very simple function that we might calculate?\nWell, if we recall back from when we were looking at propositional logic,\none of the simplest functions we looked at\nwas something like the or function that takes two inputs, x and y,\nand outputs 1, otherwise known as true, if either one of the inputs\nor both of them are 1, and outputs of 0 if both of the inputs are 0 or false.\nSo this then is the or function.\nAnd this was the truth table for the or function,\nthat as long as either of the inputs are 1, the output of the function is 1,\nand the only case where the output is 0 is where both of the inputs are 0.\nSo the question is, how could we take this and train a neural network\nto be able to learn this particular function?\nWhat would those weights look like?\nWell, we could do something like this.\nHere's our neural network.\nAnd I'll propose that in order to calculate the or function,\nwe're going to use a value of 1 for each of the weights.\nAnd we'll use a bias of negative 1.\nAnd then we'll just use this step function as our activation function.\nHow then does this work?\nWell, if I wanted to calculate something like 0 or 0,\nwhich we know to be 0 because false or false is false, then what are we going\nto do?\nWell, our output unit is going to calculate this input multiplied\nby the weight, 0 times 1, that's 0.\nSame thing here, 0 times 1, that's 0.\nAnd we'll add to that the bias minus 1.\nSo that'll give us a result of negative 1.\nIf we plot that on our activation function, negative 1 is here.\nIt's before the threshold, which means either 0 or 1.\nIt's only 1 after the threshold.\nSince negative 1 is before the threshold,\nthe output that this unit provides is going to be 0.\nAnd that's what we would expect it to be, that 0 or 0 should be 0.\nWhat if instead we had had 1 or 0, where this is the number 1?\nWell, in this case, in order to calculate what the output is going to be,\nwe again have to do this weighted sum, 1 times 1, that's 1.\n0 times 1, that's 0.\nSum of that so far is 1.\nAdd negative 1 to that.\nWell, then the output is 0.\nAnd if we plot 0 on the step function, 0 ends up being here.\nIt's just at the threshold.\nAnd so the output here is going to be 1, because the output of 1 or 0,\nthat's 1.\nSo that's what we would expect as well.\nAnd just for one more example, if I had 1 or 1, what would the result be?\nWell, 1 times 1 is 1.\n1 times 1 is 1.\nThe sum of those is 2.\nI add the bias term to that.\nI get the number 1.\n1 plotted on this graph is way over there.\nThat's well beyond the threshold.\nAnd so this output is going to be 1 as well.\nThe output is always 0 or 1, depending on whether or not\nwe're past the threshold.\nAnd this neural network then models the OR function, a very simple function,\ndefinitely.\nBut it still is able to model it correctly.\nIf I give it the inputs, it will tell me what x1 or x2 happens to be.\nAnd you could imagine trying to do this for other functions as well.\nA function like the AND function, for instance, that takes two inputs\nand calculates whether both x and y are true.\nSo if x is 1 and y is 1, then the output of x and y is 1.\nBut in all the other cases, the output is 0.\nHow could we model that inside of a neural network as well?\nWell, it turns out we could do it in the same way,\nexcept instead of negative 1 as the bias,\nwe can use negative 2 as the bias instead.\nWhat does that end up looking like?\nWell, if I had 1 and 1, that should be 1, because 1 true and true\nis equal to true.\nWell, I take 1 times 1, that's 1.\n1 times 1 is 1.\nI get a total sum of 2 so far.\nNow I add the bias of negative 2, and I get the value 0.\nAnd 0, when I plot it on the activation function,\nis just past that threshold, and so the output is going to be 1.\nBut if I had any other input, for example, like 1 and 0,\nwell, the weighted sum of these is 1 plus 0 is going to be 1.\nMinus 2 is going to give us negative 1, and negative 1\nis not past that threshold, and so the output is going to be 0.\nSo those then are some very simple functions\nthat we can model using a neural network that has two inputs and one output,\nwhere our goal is to be able to figure out what those weights should be\nin order to determine what the output should be.\nAnd you could imagine generalizing this to calculate more complex functions\nas well, that maybe, given the humidity and the pressure,\nwe want to calculate what's the probability that it's going to rain,\nfor example.\nOr we might want to do a regression-style problem.\nWe're given some amount of advertising, and given what month it is maybe,\nwe want to predict what our expected sales are\ngoing to be for that particular month.\nSo you could imagine these inputs and outputs being different as well.\nAnd it turns out that in some problems, we're not just\ngoing to have two inputs, and the nice thing about these neural networks\nis that we can compose multiple units together,\nmake our networks more complex just by adding more units\ninto this particular neural network.\nSo the network we've been looking at has two inputs and one output.\nBut we could just as easily say, let's go ahead and have three inputs in there,\nor have even more inputs, where we could arbitrarily\ndecide however many inputs there are to our problem, all going\nto be calculating some sort of output that we care about figuring out\nthe value of.\nHow then does the math work for figuring out that output?\nWell, it's going to work in a very similar way.\nIn the case of two inputs, we had two weights indicated by these edges,\nand we multiplied the weights by the numbers, adding this bias term.\nAnd we'll do the same thing in the other cases as well.\nIf I have three inputs, you'll imagine multiplying\neach of these three inputs by each of these weights.\nIf I had five inputs instead, we're going to do the same thing.\nHere I'm saying sum up from 1 to 5, xi multiplied by weight i.\nSo take each of the five input variables, multiply them\nby their corresponding weight, and then add the bias to that.\nSo this would be a case where there are five inputs into this neural network,\nfor example.\nBut there could be more, arbitrarily many nodes\nthat we want inside of this neural network, where each time we're just\ngoing to sum up all of those input variables multiplied by their weight\nand then add the bias term at the very end.\nAnd so this allows us to be able to represent problems\nthat have even more inputs just by growing the size of our neural network.\nNow, the next question we might ask is a question about how it\nis that we train these neural networks.\nIn the case of the or function and the and function,\nthey were simple enough functions that I could just tell you,\nlike here, what the weights should be.\nAnd you could probably reason through it yourself\nwhat the weights should be in order to calculate the output that you want.\nBut in general, with functions like predicting sales\nor predicting whether or not it's going to rain,\nthese are much trickier functions to be able to figure out.\nWe would like the computer to have some mechanism\nof calculating what it is that the weights should be,\nhow it is to set the weights so that our neural network is\nable to accurately model the function that we\ncare about trying to estimate.\nAnd it turns out that the strategy for doing this,\ninspired by the domain of calculus, is a technique called gradient descent.\nAnd what gradient descent is, it is an algorithm\nfor minimizing loss when you're training a neural network.\nAnd recall that loss refers to how bad our hypothesis\nfunction happens to be, that we can define certain loss functions.\nAnd we saw some examples of loss functions last time that just give us\na number for any particular hypothesis, saying,\nhow poorly does it model the data?\nHow many examples does it get wrong?\nHow are they worse or less bad as compared to other hypothesis functions\nthat we might define?\nAnd this loss function is just a mathematical function.\nAnd when you have a mathematical function,\nin calculus what you could do is calculate\nsomething known as the gradient, which you can think of as like a slope.\nIt's the direction the loss function is moving at any particular point.\nAnd what it's going to tell us is, in which direction\nshould we be moving these weights in order to minimize the amount of loss?\nAnd so generally speaking, we won't get into the calculus of it.\nBut the high level idea for gradient descent\nis going to look something like this.\nIf we want to train a neural network, we'll go ahead and start just\nby choosing the weights randomly.\nJust pick random weights for all of the weights in the neural network.\nAnd then we'll use the input data that we have access\nto in order to train the network, in order\nto figure out what the weights should actually be.\nSo we'll repeat this process again and again.\nThe first step is we're going to calculate the gradient based\non all of the data points.\nSo we'll look at all the data and figure out\nwhat the gradient is at the place where we currently\nare for the current setting of the weights, which\nmeans in which direction should we move the weights in order\nto minimize the total amount of loss, in order to make our solution better.\nAnd once we've calculated that gradient, which direction\nwe should move in the loss function, well,\nthen we can just update those weights according to the gradient.\nTake a small step in the direction of those weights\nin order to try to make our solution a little bit better.\nAnd the size of the step that we take, that's going to vary.\nAnd you can choose that when you're training a particular neural network.\nBut in short, the idea is going to be take all the data points,\nfigure out based on those data points in what direction\nthe weights should move, and then move the weights one small step\nin that direction.\nAnd if you repeat that process over and over again,\nadjusting the weights a little bit at a time based on all the data points,\neventually you should end up with a pretty good solution\nto trying to solve this sort of problem.\nAt least that's what we would hope to happen.\nNow, if you look at this algorithm, a good question\nto ask anytime you're analyzing an algorithm\nis what is going to be the expensive part of doing the calculation?\nWhat's going to take a lot of work to try to figure out?\nWhat is going to be expensive to calculate?\nAnd in particular, in the case of gradient descent,\nthe really expensive part is this all data points part right here,\nhaving to take all of the data points and using all of those data points\nfigure out what the gradient is at this particular setting of all\nof the weights.\nBecause odds are in a big machine learning problem\nwhere you're trying to solve a big problem with a lot of data,\nyou have a lot of data points in order to calculate.\nAnd figuring out the gradient based on all of those data points\nis going to be expensive.\nAnd you'll have to do it many times.\nYou'll likely repeat this process again and again and again,\ngoing through all the data points, taking one small step over and over\nas you try and figure out what the optimal setting of those weights\nhappens to be.\nIt turns out that we would ideally like to be\nable to train our neural networks faster,\nto be able to more quickly converge to some sort of solution that\nis going to be a good solution to the problem.\nSo in that case, there are alternatives to just standard gradient descent,\nwhich looks at all of the data points at once.\nWe can employ a method like stochastic gradient descent,\nwhich will randomly just choose one data point at a time\nto calculate the gradient based on, instead of calculating it\nbased on all of the data points.\nSo the idea there is that we have some setting of the weights.\nWe pick a data point.\nAnd based on that one data point, we figure out in which direction\nshould we move all of the weights and move the weights in that small\ndirection, then take another data point and do that again\nand repeat this process again and again,\nmaybe looking at each of the data points multiple times,\nbut each time only using one data point to calculate the gradient,\nto calculate which direction we should move in.\nNow, just using one data point instead of all of the data points\nprobably gives us a less accurate estimate of what the gradient actually\nis.\nBut on the plus side, it's going to be much faster\nto be able to calculate, that we can much more quickly calculate\nwhat the gradient is based on one data point,\ninstead of calculating based on all of the data points\nand having to do all of that computational work again and again.\nSo there are trade-offs here between looking at all of the data points\nand just looking at one data point.\nAnd it turns out that a middle ground that is also quite popular\nis a technique called mini-batch gradient descent, where the idea there\nis instead of looking at all of the data versus just a single point,\nwe instead divide our data set up into small batches, groups of data points,\nwhere you can decide how big a particular batch is.\nBut in short, you're just going to look at a small number of points\nat any given time, hopefully getting a more accurate estimate of the gradient,\nbut also not requiring all of the computational effort needed\nto look at every single one of these data points.\nSo gradient descent, then, is this technique\nthat we can use in order to train these neural networks,\nin order to figure out what the setting of all of these weights\nshould be if we want some way to try and get\nan accurate notion of how it is that this function should work,\nsome way of modeling how to transform the inputs into particular outputs.\nNow, so far, the networks that we've taken a look at\nhave all been structured similar to this.\nWe have some number of inputs, maybe two or three or five or more.\nAnd then we have one output that is just predicting like rain or no rain\nor just predicting one particular value.\nBut often in machine learning problems, we\ndon't just care about one output.\nWe might care about an output that has multiple different values\nassociated with it.\nSo in the same way that we could take a neural network\nand add units to the input layer, we can likewise add inputs or add outputs\nto the output layer as well.\nInstead of just one output, you could imagine we have two outputs,\nor we could have four outputs, for example,\nwhere in each case, as we add more inputs or add more outputs,\nif we want to keep this network fully connected between these two layers,\nwe just need to add more weights, that now each of these input nodes\nhas four weights associated with each of the four outputs.\nAnd that's true for each of these various different input nodes.\nSo as we add nodes, we add more weights in order\nto make sure that each of the inputs can somehow\nbe connected to each of the outputs so that each output\nvalue can be calculated based on what the value of the input happens to be.\nSo what might a case be where we want multiple different output values?\nWell, you might consider that in the case of weather predicting,\nfor example, we might not just care whether it's raining or not raining.\nThere might be multiple different categories of weather\nthat we would like to categorize the weather into.\nWith just a single output variable, we can do a binary classification,\nlike rain or no rain, for instance, 1 or 0.\nBut it doesn't allow us to do much more than that.\nWith multiple output variables, I might be\nable to use each one to predict something a little different.\nMaybe I want to categorize the weather into one of four different categories,\nsomething like is it going to be raining or sunny or cloudy or snowy.\nAnd I now have four output variables that\ncan be used to represent maybe the probability that it is\nrainy as opposed to sunny as opposed to cloudy or as opposed to snowy.\nHow then would this neural network work?\nWell, we have some input variables that represent some data\nthat we have collected about the weather.\nEach of those inputs gets multiplied by each of these various different weights.\nWe have more multiplications to do, but these\nare fairly quick mathematical operations to perform.\nAnd then what we get is after passing them\nthrough some sort of activation function in the outputs,\nwe end up getting some sort of number, where that number, you might imagine,\nyou could interpret as a probability, like a probability that it is one\ncategory as opposed to another category.\nSo here we're saying that based on the inputs,\nwe think there is a 10% chance that it's raining, a 60% chance that it's sunny,\na 20% chance of cloudy, a 10% chance that it's snowy.\nAnd given that output, if these represent a probability distribution,\nwell, then you could just pick whichever one has the highest value,\nin this case, sunny, and say that, well, most likely, we\nthink that this categorization of inputs means that the output should be snowy\nor should be sunny.\nAnd that is what we would expect the weather to be in this particular instance.\nAnd so this allows us to do these sort of multi-class classifications,\nwhere instead of just having a binary classification, 1 or 0,\nwe can have as many different categories as we want.\nAnd we can have our neural network output these probabilities\nover which categories are more likely than other categories.\nAnd using that data, we're able to draw some sort of inference\non what it is that we should do.\nSo this was sort of the idea of supervised machine learning.\nI can give this neural network a whole bunch of data,\na whole bunch of input data corresponding to some label, some output data,\nlike we know that it was raining on this day,\nwe know that it was sunny on that day.\nAnd using all of that data, the algorithm\ncan use gradient descent to figure out what all of the weights\nshould be in order to create some sort of model that hopefully allows us\na way to predict what we think the weather is going to be.\nBut neural networks have a lot of other applications as well.\nYou could imagine applying the same sort of idea to a reinforcement learning\nsort of example as well, where you remember that in reinforcement\nlearning, what we wanted to do is train some sort of agent\nto learn what action to take, depending on what state\nthey currently happen to be in.\nSo depending on the current state of the world,\nwe wanted the agent to pick from one of the available actions\nthat is available to them.\nAnd you might model that by having each of these input variables\nrepresent some information about the state, some data about what state\nour agent is currently in.\nAnd then the output, for example, could be each\nof the various different actions that our agent could take,\naction 1, 2, 3, and 4.\nAnd you might imagine that this network would work in the same way,\nbut based on these particular inputs, we go ahead and calculate values\nfor each of these outputs.\nAnd those outputs could model which action is better than other actions.\nAnd we could just choose, based on looking at those outputs,\nwhich action we should take.\nAnd so these neural networks are very broadly applicable,\nthat all they're really doing is modeling some mathematical function.\nSo anything that we can frame as a mathematical function,\nsomething like classifying inputs into various different categories\nor figuring out based on some input state what action we should take,\nthese are all mathematical functions that we could attempt to model\nby taking advantage of this neural network structure,\nand in particular, taking advantage of this technique, gradient descent,\nthat we can use in order to figure out what the weights should\nbe in order to do this sort of calculation.\nNow, how is it that you would go about training a neural network that\nhas multiple outputs instead of just one?\nWell, with just a single output, we could see what the output for that value\nshould be, and then you update all of the weights that corresponded to it.\nAnd when we have multiple outputs, at least in this particular case,\nwe can really think of this as four separate neural networks,\nthat really we just have one network here that has these three inputs\ncorresponding with these three weights corresponding to this one output value.\nAnd the same thing is true for this output value.\nThis output value effectively defines yet another neural network\nthat has these same three inputs, but a different set of weights\nthat correspond to this output.\nAnd likewise, this output has its own set of weights as well,\nand same thing for the fourth output too.\nAnd so if you wanted to train a neural network that had four outputs instead\nof just one, in this case where the inputs are directly\nconnected to the outputs, you could really\nthink of this as just training four independent neural networks.\nWe know what the outputs for each of these four\nshould be based on our input data, and using that data,\nwe can begin to figure out what all of these individual weights should be.\nAnd maybe there's an additional step at the end\nto make sure that we turn these values into a probability distribution such\nthat we can interpret which one is better than another\nor more likely than another as a category or something like that.\nSo this then seems like it does a pretty good job of taking inputs\nand trying to predict what outputs should be.\nAnd we'll see some real examples of this in just a moment as well.\nBut it's important then to think about what the limitations\nof this sort of approach is, of just taking some linear combination\nof inputs and passing it into some sort of activation function.\nAnd it turns out that when we do this in the case of binary classification,\ntrying to predict does it belong to one category or another,\nwe can only predict things that are linearly separable.\nBecause we're taking a linear combination of inputs\nand using that to define some decision boundary or threshold,\nthen what we get is a situation where if we have this set of data,\nwe can predict a line that separates linearly the red points from the blue\npoints, but a single unit that is making a binary classification, otherwise\nknown as a perceptron, can't deal with a situation like this, where we've\nseen this type of situation before, where there is no straight line that\njust goes straight through the data that will divide the red points away\nfrom the blue points.\nIt's a more complex decision boundary.\nThe decision boundary somehow needs to capture the things inside of this\ncircle.\nAnd there isn't really a line that will allow us to deal with that.\nSo this is the limitation of the perceptron,\nthese units that just make these binary decisions based on their inputs,\nthat a single perceptron is only capable of learning\na linearly separable decision boundary.\nAll it can do is define a line.\nAnd sure, it can give us probabilities based\non how close to that decision boundary we are,\nbut it can only really decide based on a linear decision boundary.\nAnd so this doesn't seem like it's going to generalize well\nto situations where real world data is involved,\nbecause real world data often isn't linearly separable.\nIt often isn't the case that we can just draw a line through the data\nand be able to divide it up into multiple groups.\nSo what then is the solution to this?\nWell, what was proposed was the idea of a multilayer neural network,\nthat so far all of the neural networks we've seen\nhave had a set of inputs and a set of outputs,\nand the inputs are connected to those outputs.\nBut in a multilayer neural network, this is going\nto be an artificial neural network that has an input layer still.\nIt has an output layer, but also has one or more hidden layers in between.\nOther layers of artificial neurons or units\nthat are going to calculate their own values as well.\nSo instead of a neural network that looks like this with three inputs\nand one output, you might imagine in the middle\nhere injecting a hidden layer, something like this.\nThis is a hidden layer that has four nodes.\nYou could choose how many nodes or units end up going into the hidden layer.\nYou can have multiple hidden layers as well.\nAnd so now each of these inputs isn't directly connected to the output.\nEach of the inputs is connected to this hidden layer.\nAnd then all of the nodes in the hidden layer, those\nare connected to the one output.\nAnd so this is just another step that we can\ntake towards calculating more complex functions.\nEach of these hidden units will calculate its output value,\notherwise known as its activation, based on a linear combination\nof all the inputs.\nAnd once we have values for all of these nodes,\nas opposed to this just being the output, we do the same thing again.\nCalculate the output for this node based on multiplying\neach of the values for these units by their weights as well.\nSo in effect, the way this works is that we start with inputs.\nThey get multiplied by weights in order to calculate values for the hidden nodes.\nThose get multiplied by weights in order to figure out\nwhat the ultimate output is going to be.\nAnd the advantage of layering things like this\nis it gives us an ability to model more complex functions,\nthat instead of just having a single decision boundary, a single line\ndividing the red points from the blue points, each of these hidden nodes\ncan learn a different decision boundary.\nAnd we can combine those decision boundaries\nto figure out what the ultimate output is going to be.\nAnd as we begin to imagine more complex situations,\nyou could imagine each of these nodes learning some useful property\nor learning some useful feature of all of the inputs\nand us somehow learning how to combine those features together\nin order to get the output that we actually want.\nNow, the natural question when we begin to look at this now\nis to ask the question of, how do we train a neural network that\nhas hidden layers inside of it?\nAnd this turns out to initially be a bit of a tricky question,\nbecause the input data that we are given is we\nare given values for all of the inputs, and we're\ngiven what the value of the output should be, what the category is,\nfor example.\nBut the input data doesn't tell us what the values for all of these nodes\nshould be.\nSo we don't know how far off each of these nodes actually\nis because we're only given data for the inputs and the outputs.\nThe reason this is called the hidden layer\nis because the data that is made available to us\ndoesn't tell us what the values for all of these intermediate nodes\nshould actually be.\nAnd so the strategy people came up with was\nto say that if you know what the error or the losses on the output node,\nwell, then based on what these weights are,\nif one of these weights is higher than another,\nyou can calculate an estimate for how much\nthe error from this node was due to this part of the hidden node,\nor this part of the hidden layer, or this part of the hidden layer,\nbased on the values of these weights, in effect saying\nthat based on the error from the output, I can back propagate the error\nand figure out an estimate for what the error is for each of these nodes\nin the hidden layer as well.\nAnd there's some more calculus here that we won't get into the details of,\nbut the idea of this algorithm is known as back propagation.\nIt's an algorithm for training a neural network\nwith multiple different hidden layers.\nAnd the idea for this, the pseudocode for it,\nwill again be if we want to run gradient descent with back propagation.\nWe'll start with a random choice of weights, as we did before.\nAnd now we'll go ahead and repeat the training process again and again.\nBut what we're going to do each time is now\nwe're going to calculate the error for the output layer first.\nWe know the output and what it should be,\nand we know what we calculated so we can figure out what the error there is.\nBut then we're going to repeat for every layer,\nstarting with the output layer, moving back into the hidden layer,\nthen the hidden layer before that if there are multiple hidden layers,\ngoing back all the way to the very first hidden layer,\nassuming there are multiple, we're going to propagate the error back one layer.\nWhatever the error was from the output, figure out\nwhat the error should be a layer before that\nbased on what the values of those weights are.\nAnd then we can update those weights.\nSo graphically, the way you might think about this\nis that we first start with the output.\nWe know what the output should be.\nWe know what output we calculated.\nAnd based on that, we can figure out, all right,\nhow do we need to update those weights?\nBackpropagating the error to these nodes.\nAnd using that, we can figure out how we should update these weights.\nAnd you might imagine if there are multiple layers,\nwe could repeat this process again and again\nto begin to figure out how all of these weights should be updated.\nAnd this backpropagation algorithm is really\nthe key algorithm that makes neural networks possible.\nIt makes it possible to take these multi-level structures\nand be able to train those structures depending\non what the values of these weights are in order\nto figure out how it is that we should go about updating those weights in\norder to create some function that is able to minimize\nthe total amount of loss, to figure out some good setting of the weights\nthat will take the inputs and translate it into the output that we expect.\nAnd this works, as we said, not just for a single hidden layer.\nBut you can imagine multiple hidden layers, where each hidden layer we just\ndefine however many nodes we want, where each of the nodes in one layer,\nwe can connect to the nodes in the next layer,\ndefining more and more complex networks that\nare able to model more and more complex types of functions.\nAnd so this type of network is what we might call a deep neural network,\npart of a larger family of deep learning algorithms,\nif you've ever heard that term.\nAnd all deep learning is about is it's using multiple layers\nto be able to predict and be able to model higher level\nfeatures inside of the input, to be able to figure out\nwhat the output should be.\nAnd so a deep neural network is just a neural network\nthat has multiple of these hidden layers,\nwhere we start at the input, calculate values for this layer,\nthen this layer, then this layer, and then ultimately get an output.\nAnd this allows us to be able to model more and more sophisticated types\nof functions, that each of these layers can calculate something\na little bit different, and we can combine that information\nto figure out what the output should be.\nOf course, as with any situation of machine learning,\nas we begin to make our models more and more complex,\nto model more and more complex functions, the risk we run\nis something like overfitting.\nAnd we talked about overfitting last time in the context of overfitting\nbased on when we were training our models to be\nable to learn some sort of decision boundary,\nwhere overfitting happens when we fit too closely to the training data.\nAnd as a result, we don't generalize well to other situations as well.\nAnd one of the risks we run with a far more complex neural network that\nhas many, many different nodes is that we might overfit based on the input\ndata.\nWe might grow over reliant on certain nodes\nto calculate things just purely based on the input data that\ndoesn't allow us to generalize very well to the output.\nAnd there are a number of strategies for dealing with overfitting.\nBut one of the most popular in the context of neural networks\nis a technique known as dropout.\nAnd what dropout does is it, when we're training the neural network,\nwhat we'll do in dropout is temporarily remove units,\ntemporarily remove these artificial neurons from our network chosen at\nrandom.\nAnd the goal here is to prevent over-reliance on certain units.\nWhat generally happens in overfitting is that we\nbegin to over-rely on certain units inside the neural network\nto be able to tell us how to interpret the input data.\nWhat dropout will do is randomly remove some of these units\nin order to reduce the chance that we over-rely on certain units\nto make our neural network more robust, to be able to handle the situations\neven when we just drop out particular neurons entirely.\nSo the way that might work is we have a network like this.\nAnd as we're training it, when we go about trying\nto update the weights the first time, we'll just randomly pick\nsome percentage of the nodes to drop out of the network.\nIt's as if those nodes aren't there at all.\nIt's as if the weights associated with those nodes aren't there at all.\nAnd we'll train it this way.\nThen the next time we update the weights, we'll pick a different set\nand just go ahead and train that way.\nAnd then again, randomly choose and train with other nodes\nthat have been dropped out as well.\nAnd the goal of that is that after the training process,\nif you train by dropping out random nodes inside of this neural network,\nyou hopefully end up with a network that's a little bit more robust,\nthat doesn't rely too heavily on any one particular node,\nbut more generally learns how to approximate a function in general.\nSo that then is a look at some of these techniques\nthat we can use in order to implement a neural network,\nto get at the idea of taking this input, passing it\nthrough these various different layers in order to produce some sort of output.\nAnd what we'd like to do now is take those ideas and put them into code.\nAnd to do that, there are a number of different machine learning libraries,\nneural network libraries that we can use that allow us to get access\nto someone's implementation of back propagation and all of these hidden\nlayers.\nAnd one of the most popular, developed by Google, is known as TensorFlow,\na library that we can use for quickly creating neural networks and modeling\nthem and running them on some sample data to see what the output is going\nto be.\nAnd before we actually start writing code,\nwe'll go ahead and take a look at TensorFlow's playground, which\nwill be an opportunity for us just to play around with this idea of neural\nnetworks in different layers, just to get a sense for what\nit is that we can do by taking advantage of neural networks.\nSo let's go ahead and go into TensorFlow's playground, which\nyou can go to by visiting that URL from before.\nAnd what we're going to do now is we're going to try and learn the decision\nboundary for this particular output.\nI want to learn to separate the orange points from the blue points.\nAnd I'd like to learn some sort of setting of weights inside of a neural\nnetwork that will be able to separate those from each other.\nThe features we have access to, our input data,\nare the x value and the y value, so the two values along each of the two axes.\nAnd what I'll do now is I can set particular parameters,\nlike what activation function I would like to use.\nAnd I'll just go ahead and press play and see what happens.\nAnd what happens here is that you'll see that just\nby using these two input features, the x value and the y value,\nwith no hidden layers, just take the input, x and y values,\nand figure out what the decision boundary is.\nOur neural network learns pretty quickly that in order\nto divide these two points, we should just use this line.\nThis line acts as a decision boundary that\nseparates this group of points from that group of points,\nand it does it very well.\nYou can see up here what the loss is.\nThe training loss is 0, meaning we were able to perfectly model separating\nthese two points from each other inside of our training data.\nSo this was a fairly simple case of trying\nto apply a neural network because the data is very clean.\nIt's very nicely linearly separable.\nWe could just draw a line that separates all of those points from each other.\nLet's now consider a more complex case.\nSo I'll go ahead and pause the simulation,\nand we'll go ahead and look at this data set here.\nThis data set is a little bit more complex now.\nIn this data set, we still have blue and orange points\nthat we'd like to separate from each other.\nBut there's no single line that we can draw\nthat is going to be able to figure out how to separate the blue from the orange,\nbecause the blue is located in these two quadrants,\nand the orange is located here and here.\nIt's a more complex function to be able to learn.\nSo let's see what happens.\nIf we just try and predict based on those inputs, the x and y coordinates,\nwhat the output should be, I'll press Play.\nAnd what you'll notice is that we're not really\nable to draw much of a conclusion, that we're not\nable to very cleanly see how we should divide the orange points from the blue\npoints, and you don't see a very clean separation there.\nSo it seems like we don't have enough sophistication inside of our network\nto be able to model something that is that complex.\nWe need a better model for this neural network.\nAnd I'll do that by adding a hidden layer.\nSo now I have a hidden layer that has two neurons inside of it.\nSo I have two inputs that then go to two neurons\ninside of a hidden layer that then go to our output.\nAnd now I'll press Play.\nAnd what you'll notice here is that we're able to do slightly better.\nWe're able to now say, all right, these points are definitely blue.\nThese points are definitely orange.\nWe're still struggling a little bit with these points up here, though.\nAnd what we can do is we can see for each of these hidden neurons,\nwhat is it exactly that these hidden neurons are doing?\nEach hidden neuron is learning its own decision boundary.\nAnd we can see what that boundary is.\nThis first neuron is learning, all right,\nthis line that seems to separate some of the blue points\nfrom the rest of the points.\nThis other hidden neuron is learning another line\nthat seems to be separating the orange points in the lower right\nfrom the rest of the points.\nSo that's why we're able to figure out these two areas in the bottom region.\nBut we're still not able to perfectly classify all of the points.\nSo let's go ahead and add another neuron.\nNow we've got three neurons inside of our hidden layer\nand see what we're able to learn now.\nAll right, well, now we seem to be doing a better job.\nBy learning three different decision boundaries, which\neach of the three neurons inside of our hidden layer,\nwe're able to much better figure out how to separate these blue points\nfrom the orange points.\nAnd we can see what each of these hidden neurons is learning.\nEach one is learning a slightly different decision boundary.\nAnd then we're combining those decision boundaries together\nto figure out what the overall output should be.\nAnd then we can try it one more time by adding a fourth neuron there\nand try learning that.\nAnd it seems like now we can do even better at trying\nto separate the blue points from the orange points.\nBut we were only able to do this by adding a hidden layer,\nby adding some layer that is learning some other boundaries\nand combining those boundaries to determine the output.\nAnd the strength, the size and thickness of these lines\nindicate how high these weights are, how important each of these inputs\nis for making this sort of calculation.\nAnd we can do maybe one more simulation.\nLet's go ahead and try this on a data set that looks like this.\nGo ahead and get rid of the hidden layer.\nHere now we're trying to separate the blue points from the orange points\nwhere all the blue points are located, again,\ninside of a circle effectively.\nSo we're not going to be able to learn a line.\nNotice I press Play.\nAnd we're really not able to draw any sort of classification at all\nbecause there is no line that cleanly separates the blue points\nfrom the orange points.\nSo let's try to solve this by introducing a hidden layer.\nI'll go ahead and press Play.\nAnd all right, with two neurons in a hidden layer,\nwe're able to do a little better because we effectively\nlearned two different decision boundaries.\nWe learned this line here.\nAnd we learned this line on the right-hand side.\nAnd right now we're just saying, all right, well, if it's in between,\nwe'll call it blue.\nAnd if it's outside, we'll call it orange.\nSo not great, but certainly better than before,\nthat we're learning one decision boundary and another.\nAnd based on those, we can figure out what the output should be.\nBut let's now go ahead and add a third neuron and see what happens now.\nI go ahead and train it.\nAnd now, using three different decision boundaries\nthat are learned by each of these hidden neurons,\nwe're able to much more accurately model this distinction\nbetween blue points and orange points.\nWe're able to figure out maybe with these three decision boundaries,\ncombining them together, you can imagine figuring out\nwhat the output should be and how to make that sort of classification.\nAnd so the goal here is just to get a sense for having more neurons\nin these hidden layers allows us to learn more structure in the data,\nallows us to figure out what the relevant and important decision\nboundaries are.\nAnd then using this backpropagation algorithm,\nwe're able to figure out what the values of these weights should be\nin order to train this network to be able to classify one category of points\naway from another category of points instead.\nAnd this is ultimately what we're going to be trying\nto do whenever we're training a neural network.\nSo let's go ahead and actually see an example of this.\nYou'll recall from last time that we had this banknotes file\nthat included information about counterfeit banknotes as opposed\nto authentic banknotes, where I had four different values for each banknote\nand then a categorization of whether that banknote is considered\nto be authentic or a counterfeit note.\nAnd what I wanted to do was, based on that input information,\nfigure out some function that could calculate\nbased on the input information what category it belonged to.\nAnd what I've written here in banknotes.py\nis a neural network that will learn just that, a network that\nlearns based on all of the input whether or not\nwe should categorize a banknote as authentic or as counterfeit.\nThe first step is the same as what we saw from last time.\nI'm really just reading the data in and getting it\ninto an appropriate format.\nAnd so this is where more of the writing Python code on your own\ncomes in, in terms of manipulating this data,\nmassaging the data into a format that will be understood\nby a machine learning library like scikit-learn or like TensorFlow.\nAnd so here I separate it into a training and a testing set.\nAnd now what I'm doing down below is I'm creating a neural network.\nHere I'm using TF, which stands for TensorFlow.\nUp above, I said import TensorFlow as TF, TF just an abbreviation that we'll\noften use so we don't need to write out TensorFlow\nevery time we want to use anything inside of the library.\nI'm using TF.keras.\nKeras is an API, a set of functions that we\ncan use in order to manipulate neural networks inside of TensorFlow.\nAnd it turns out there are other machine learning libraries\nthat also use the Keras API.\nBut here I'm saying, all right, go ahead and give me\na model that is a sequential model, a sequential neural network,\nmeaning one layer after another.\nAnd now I'm going to add to that model what layers\nI want inside of my neural network.\nSo here I'm saying model.add.\nGo ahead and add a dense layer.\nAnd when we say a dense layer, we mean a layer that is just each\nof the nodes inside of the layer is going to be connected\nto each of the nodes from the previous layer.\nSo we have a densely connected layer.\nThis layer is going to have eight units inside of it.\nSo it's going to be a hidden layer inside of a neural network\nwith eight different units, eight artificial neurons, each of which\nmight learn something different.\nAnd I just sort of chose eight arbitrarily.\nYou could choose a different number of hidden nodes inside of the layer.\nAnd as we saw before, depending on the number of units\nthere are inside of your hidden layer, more units\nmeans you can learn more complex functions.\nSo maybe you can more accurately model the training data.\nBut it comes at the cost.\nMore units means more weights that you need to figure out how to update.\nSo it might be more expensive to do that calculation.\nAnd you also run the risk of overfitting on the data.\nIf you have too many units and you learn to just\noverfit on the training data, that's not good either.\nSo there is a balance.\nAnd there's often a testing process where you'll train on some data\nand maybe validate how well you're doing on a separate set of data,\noften called a validation set, to see, all right, which setting of parameters.\nHow many layers should I have?\nHow many units should be in each layer?\nWhich one of those performs the best on the validation set?\nSo you can do some testing to figure out what these hyper parameters, so called,\nshould be equal to.\nNext, I specify what the input shape is.\nMeaning, all right, what does my input look like?\nMy input has four values.\nAnd so the input shape is just four, because we have four inputs.\nAnd then I specify what the activation function is.\nAnd the activation function, again, we can choose.\nThere are a number of different activation functions.\nHere I'm using relu, which you might recall from earlier.\nAnd then I'll add an output layer.\nSo I have my hidden layer.\nNow I'm adding one more layer that will just have one unit,\nbecause all I want to do is predict something\nlike counterfeit build or authentic build.\nSo I just need a single unit.\nAnd the activation function I'm going to use here\nis that sigmoid activation function, which, again,\nwas that S-shaped curve that just gave us a probability of what\nis the probability that this is a counterfeit build,\nas opposed to an authentic build.\nSo that, then, is the structure of my neural network,\na sequential neural network that has one hidden layer with eight units inside\nof it, and then one output layer that just has a single unit inside of it.\nAnd I can choose how many units there are.\nI can choose the activation function.\nThen I'm going to compile this model.\nTensorFlow gives you a choice of how you would like to optimize the weights.\nThere are various different algorithms for doing that.\nWhat type of loss function you want to use.\nAgain, many different options for doing that.\nAnd then how I want to evaluate my model, well, I care about accuracy.\nI care about how many of my points am I able to classify correctly\nversus not correctly as counterfeit or not counterfeit.\nAnd I would like it to report to me how accurate my model is performing.\nThen, now that I've defined that model, I\ncall model.fit to say go ahead and train the model.\nTrain it on all the training data plus all of the training labels.\nSo labels for each of those pieces of training data.\nAnd I'm saying run it for 20 epics, meaning go ahead and go\nthrough each of these training points 20 times, effectively.\nGo through the data 20 times and keep trying to update the weights.\nIf I did it for more, I could train for even longer\nand maybe get a more accurate result.\nBut then after I fit it on all the data, I'll go ahead and just test it.\nI'll evaluate my model using model.evaluate built into TensorFlow\nthat is just going to tell me how well do I perform on the testing data.\nSo ultimately, this is just going to give me some numbers that tell me\nhow well we did in this particular case.\nSo now what I'm going to do is go into banknotes and go ahead and run\nbanknotes.py.\nAnd what's going to happen now is it's going to read in all of that training\ndata.\nIt's going to generate a neural network with all my inputs,\nmy eight hidden units inside my layer, and then an output unit.\nAnd now what it's doing is it's training.\nIt's training 20 times.\nAnd each time you can see how my accuracy is increasing on my training data.\nIt starts off the very first time not very accurate,\nthough better than random, something like 79% of the time.\nIt's able to accurately classify one bill from another.\nBut as I keep training, notice this accuracy value\nimproves and improves and improves until after I've trained through all\nthe data points 20 times, it looks like my accuracy is above 99% on the training\ndata.\nAnd here's where I tested it on a whole bunch of testing data.\nAnd it looks like in this case, I was also like 99.8% accurate.\nSo just using that, I was able to generate a neural network that\ncan detect counterfeit bills from authentic bills based on this input\ndata 99.8% of the time, at least based on this particular testing data.\nAnd I might want to test it with more data as well,\njust to be confident about that.\nBut this is really the value of using a machine learning library like TensorFlow.\nAnd there are others available for Python and other languages as well.\nBut all I have to do is define the structure of the network\nand define the data that I'm going to pass into the network.\nAnd then TensorFlow runs the backpropagation algorithm\nfor learning what all of those weights should be,\nfor figuring out how to train this neural network to be\nable to accurately, as accurately as possible,\nfigure out what the output values should be there as well.\nAnd so this then was a look at what it is that neural networks can do just\nusing these sequences of layer after layer after layer.\nAnd you can begin to imagine applying these to much more general problems.\nAnd one big problem in computing and artificial intelligence\nmore generally is the problem of computer vision.\nComputer vision is all about computational methods\nfor analyzing and understanding images.\nYou might have pictures that you want the computer to figure out\nhow to deal with, how to process those images\nand figure out how to produce some sort of useful result out of this.\nYou've seen this in the context of social media websites\nthat are able to look at a photo that contains a whole bunch of faces.\nAnd it's able to figure out what's a picture of whom\nand label those and tag them with appropriate people.\nThis is becoming increasingly relevant as we\nbegin to discuss self-driving cars, that these cars now have cameras.\nAnd we would like for the computer to have some sort of algorithm\nthat looks at the image and figures out what color is the light, what cars\nare around us and in what direction, for example.\nAnd so computer vision is all about taking an image and figuring out\nwhat sort of computation, what sort of calculation\nwe can do with that image.\nIt's also relevant in the context of something like handwriting recognition.\nThis, what you're looking at, is an example of the MNIST data set.\nIt's a big data set just of handwritten digits\nthat we could use to ideally try and figure out\nhow to predict, given someone's handwriting, given a photo of a digit\nthat they have drawn, can you predict whether it's a 0, 1, 2, 3, 4, 5, 6, 7, 8,\nor 9, for example.\nSo this sort of handwriting recognition is yet another task\nthat we might want to use computer vision tasks and tools\nto be able to apply it towards.\nThis might be a task that we might care about.\nSo how, then, can we use neural networks to be\nable to solve a problem like this?\nWell, neural networks rely upon some sort of input\nwhere that input is just numerical data.\nWe have a whole bunch of units where each one of them\njust represents some sort of number.\nAnd so in the context of something like handwriting recognition\nor in the context of just an image, you might imagine that an image is really\njust a grid of pixels, grid of dots where each dot has some sort of color.\nAnd in the context of something like handwriting recognition,\nyou might imagine that if you just fill in each of these dots in a particular\nway, you can generate a 2 or an 8, for example,\nbased on which dots happen to be shaded in and which dots are not.\nAnd we can represent each of these pixel values just using numbers.\nSo for a particular pixel, for example, 0 might represent entirely black.\nDepending on how you're representing color,\nit's often common to represent color values on a 0 to 255 range\nso that you can represent a color using 8 bits for a particular value,\nlike how much white is in the image.\nSo 0 might represent all black.\n255 might represent entirely white as a pixel.\nAnd somewhere in between might represent some shade of gray, for example.\nBut you might imagine not just having a single slider that\ndetermines how much white is in the image,\nbut if you had a color image, you might imagine\nthree different numerical values, a red, green, and blue value,\nwhere the red value controls how much red is in the image.\nWe have one value for controlling how much green is in the pixel\nand one value for how much blue is in the pixel as well.\nAnd depending on how it is that you set these values of red, green, and blue,\nyou can get a different color.\nAnd so any pixel can really be represented, in this case,\nby three numerical values, a red value, a green value, and a blue value.\nAnd if you take a whole bunch of these pixels, assemble them together\ninside of a grid of pixels, then you really\njust have a whole bunch of numerical values\nthat you can use in order to perform some sort of prediction task.\nAnd so what you might imagine doing is using the same techniques\nwe talked about before, just design a neural network\nwith a lot of inputs, that for each of the pixels,\nwe might have one or three different inputs\nin the case of a color image, a different input that\nis just connected to a deep neural network, for example.\nAnd this deep neural network might take all of the pixels\ninside of the image of what digit a person drew.\nAnd the output might be like 10 neurons that\nclassify it as a 0, or a 1, or a 2, or a 3,\nor just tells us in some way what that digit happens to be.\nNow, there are a couple of drawbacks to this approach.\nThe first drawback to the approach is just the size of this input array,\nthat we have a whole bunch of inputs.\nIf we have a big image that has a lot of different channels,\nwe're looking at a lot of inputs, and therefore a lot of weights\nthat we have to calculate.\nAnd a second problem is the fact that by flattening everything\ninto just this structure of all the pixels,\nwe've lost access to a lot of the information\nabout the structure of the image that's relevant,\nthat really, when a person looks at an image,\nthey're looking at particular features of the image.\nThey're looking at curves.\nThey're looking at shapes.\nThey're looking at what things can you identify\nin different regions of the image, and maybe put those things together\nin order to get a better picture of what the overall image is about.\nAnd by just turning it into pixel values for each of the pixels,\nsure, you might be able to learn that structure,\nbut it might be challenging in order to do so.\nIt might be helpful to take advantage of the fact\nthat you can use properties of the image itself, the fact\nthat it's structured in a particular way, to be\nable to improve the way that we learn based on that image too.\nSo in order to figure out how we can train our neural networks to better\nbe able to deal with images, we'll introduce a couple of ideas,\na couple of algorithms that we can apply that\nallow us to take the image and extract some useful information out\nof that image.\nAnd the first idea we'll introduce is the notion of image convolution.\nAnd what image convolution is all about is it's about filtering an image,\nsort of extracting useful or relevant features out of the image.\nAnd the way we do that is by applying a particular filter that\nbasically adds the value for every pixel with the values\nfor all of the neighboring pixels to it, according\nto some sort of kernel matrix, which we'll see in a moment,\nis going to allow us to weight these pixels in various different ways.\nAnd the goal of image convolution, then, is\nto extract some sort of interesting or useful features out of an image,\nto be able to take a pixel and, based on its neighboring pixels,\nmaybe predict some sort of valuable information.\nSomething like taking a pixel and looking at its neighboring pixels,\nyou might be able to predict whether or not\nthere's some sort of curve inside the image,\nor whether it's forming the outline of a particular line or a shape,\nfor example.\nAnd that might be useful if you're trying to use\nall of these various different features to combine them\nto say something meaningful about an image as a whole.\nSo how, then, does image convolution work?\nWell, we start with a kernel matrix.\nAnd the kernel matrix looks something like this.\nAnd the idea of this is that, given a pixel that will be the middle pixel,\nwe're going to multiply each of the neighboring pixels\nby these values in order to get some sort of result\nby summing up all the numbers together.\nSo if I take this kernel, which you can think of as a filter\nthat I'm going to apply to the image, and let's say that I take this image.\nThis is a 4 by 4 image.\nWe'll think of it as just a black and white image,\nwhere each one is just a single pixel value.\nSo somewhere between 0 and 255, for example.\nSo we have a whole bunch of individual pixel values like this.\nAnd what I'd like to do is apply this kernel, this filter, so to speak,\nto this image.\nAnd the way I'll do that is, all right, the kernel is 3 by 3.\nYou can imagine a 5 by 5 kernel or a larger kernel, too.\nAnd I'll take it and just first apply it to the first 3\nby 3 section of the image.\nAnd what I'll do is I'll take each of these pixel values,\nmultiply it by its corresponding value in the filter matrix,\nand add all of the results together.\nSo here, for example, I'll say 10 times 0, plus 20 times negative 1,\nplus 30 times 0, so on and so forth, doing all of this calculation.\nAnd at the end, if I take all these values,\nmultiply them by their corresponding value in the kernel,\nadd the results together, for this particular set of 9 pixels,\nI get the value of 10, for example.\nAnd then what I'll do is I'll slide this 3 by 3 grid, effectively, over.\nI'll slide the kernel by 1 to look at the next 3 by 3 section.\nHere, I'm just sliding it over by 1 pixel.\nBut you might imagine a different stride length,\nor maybe I jump by multiple pixels at a time if you really wanted to.\nYou have different options here.\nBut here, I'm just sliding over, looking at the next 3 by 3 section.\nAnd I'll do the same math, 20 times 0, plus 30 times negative 1,\nplus 40 times 0, plus 20 times negative 1, so on and so forth, plus 30 times 5.\nAnd what I end up getting is the number 20.\nThen you can imagine shifting over to this one, doing the same thing,\ncalculating the number 40, for example, and then doing the same thing here,\nand calculating a value there as well.\nAnd so what we have now is what we'll call a feature map.\nWe have taken this kernel, applied it to each\nof these various different regions, and what we get\nis some representation of a filtered version of that image.\nAnd so to give a more concrete example of why\nit is that this kind of thing could be useful,\nlet's take this kernel matrix, for example, which is quite a famous one,\nthat has an 8 in the middle, and then all of the neighboring pixels\nget a negative 1.\nAnd let's imagine we wanted to apply that to a 3\nby 3 part of an image that looks like this, where all the values are the same.\nThey're all 20, for instance.\nWell, in this case, if you do 20 times 8, and then subtract 20, subtract 20,\nsubtract 20 for each of the eight neighbors, well, the result of that\nis you just get that expression, which comes out to be 0.\nYou multiplied 20 by 8, but then you subtracted\n20 eight times, according to that particular kernel.\nThe result of all that is just 0.\nSo the takeaway here is that when a lot of the pixels are the same value,\nwe end up getting a value close to 0.\nIf, though, we had something like this, 20 is along this first row,\nthen 50 is in the second row, and 50 is in the third row, well,\nthen when you do this, because it's the same kind of math, 20 times negative 1,\n20 times negative 1, so on and so forth, then I get a higher value,\na value like 90 in this particular case.\nAnd so the more general idea here is that by applying this kernel, negative 1s,\n8 in the middle, and then negative 1s, what I get\nis when this middle value is very different from the neighboring values,\nlike 50 is greater than these 20s, then you'll\nend up with a value higher than 0.\nIf this number is higher than its neighbors,\nyou end up getting a bigger output.\nBut if this value is the same as all of its neighbors,\nthen you get a lower output, something like 0.\nAnd it turns out that this sort of filter can therefore\nbe used in something like detecting edges in an image.\nOr I want to detect the boundaries between various different objects\ninside of an image.\nI might use a filter like this, which is able to tell\nwhether the value of this pixel is different\nfrom the values of the neighboring pixel,\nif it's greater than the values of the pixels that happen to surround it.\nAnd so we can use this in terms of image filtering.\nAnd so I'll show you an example of that.\nI have here in filter.py a file that uses Python's image library,\nor PIL, to do some image filtering.\nI go ahead and open an image.\nAnd then all I'm going to do is apply a kernel to that image.\nIt's going to be a 3 by 3 kernel, same kind of kernel we saw before.\nAnd here is the kernel.\nThis is just a list representation of the same matrix\nthat I showed you a moment ago.\nIt's negative 1, negative 1, negative 1.\nThe second row is negative 1, 8, negative 1.\nAnd the third row is all negative 1s.\nAnd then at the end, I'm going to go ahead and show the filtered image.\nSo if, for example, I go into convolution directory\nand I open up an image, like bridge.png, this\nis what an input image might look like, just an image of a bridge over a river.\nNow I'm going to go ahead and run this filter program on the bridge.\nAnd what I get is this image here.\nJust by taking the original image and applying that filter\nto each 3 by 3 grid, I've extracted all of the boundaries,\nall of the edges inside the image that separate one part of the image\nfrom another.\nSo here I've got a representation of boundaries\nbetween particular parts of the image.\nAnd you might imagine that if a machine learning algorithm is\ntrying to learn what an image is of, a filter like this could be pretty useful.\nMaybe the machine learning algorithm doesn't\ncare about all of the details of the image.\nIt just cares about certain useful features.\nIt cares about particular shapes that are\nable to help it determine that based on the image,\nthis is going to be a bridge, for example.\nAnd so this type of idea of image convolution\ncan allow us to apply filters to images that allow us to extract useful results\nout of those images, taking an image and extracting its edges, for example.\nAnd you might imagine many other filters that\ncould be applied to an image that are able to extract particular values as\nwell.\nAnd a filter might have separate kernels for the red values, the green values,\nand the blue values that are all summed together at the end,\nsuch that you could have particular filters looking for,\nis there red in this part of the image?\nAre there green in other parts of the image?\nYou can begin to assemble these relevant and useful filters\nthat are able to do these calculations as well.\nSo that then was the idea of image convolution,\napplying some sort of filter to an image to be\nable to extract some useful features out of that image.\nBut all the while, these images are still pretty big.\nThere's a lot of pixels involved in the image.\nAnd realistically speaking, if you've got a really big image,\nthat poses a couple of problems.\nOne, it means a lot of input going into the neural network.\nBut two, it also means that we really have\nto care about what's in each particular pixel.\nWhereas realistically, we often, if you're looking at an image,\nyou don't care whether something is in one particular pixel versus the pixel\nimmediately to the right of it.\nThey're pretty close together.\nYou really just care about whether there's a particular feature\nin some region of the image.\nAnd maybe you don't care about exactly which pixel it happens to be in.\nAnd so there's a technique we can use known as pooling.\nAnd what pooling is, is it means reducing the size of an input\nby sampling from regions inside of the input.\nSo we're going to take a big image and turn it into a smaller image\nby using pooling.\nAnd in particular, one of the most popular types of pooling\nis called max pooling.\nAnd what max pooling does is it pools just\nby choosing the maximum value in a particular region.\nSo for example, let's imagine I had this 4 by 4 image.\nBut I wanted to reduce its dimensions.\nI wanted to make it a smaller image so that I have fewer inputs to work with.\nWell, what I could do is I could apply a 2 by 2 max pool,\nwhere the idea would be that I'm going to first look at this 2 by 2 region\nand say, what is the maximum value in that region?\nWell, it's the number 50.\nSo we'll go ahead and just use the number 50.\nAnd then we'll look at this 2 by 2 region.\nWhat is the maximum value here?\nIt's 110, so that's going to be my value.\nLikewise here, the maximum value looks like 20.\nGo ahead and put that there.\nThen for this last region, the maximum value was 40.\nSo we'll go ahead and use that.\nAnd what I have now is a smaller representation\nof this same original image that I obtained just\nby picking the maximum value from each of these regions.\nSo again, the advantages here are now I only\nhave to deal with a 2 by 2 input instead of a 4 by 4.\nAnd you can imagine shrinking the size of an image even more.\nBut in addition to that, I'm now able to make my analysis\nindependent of whether a particular value was in this pixel or this pixel.\nI don't care if the 50 was here or here.\nAs long as it was generally in this region,\nI'll still get access to that value.\nSo it makes our algorithms a little bit more robust as well.\nSo that then is pooling, taking the size of the image,\nreducing it a little bit by just sampling from particular regions\ninside of the image.\nAnd now we can put all of these ideas together, pooling, image convolution,\nand neural networks all together into another type of neural network\ncalled a convolutional neural network, or a CNN, which\nis a neural network that uses this convolution step usually\nin the context of analyzing an image, for example.\nAnd so the way that a convolutional neural network works\nis that we start with some sort of input image, some grid of pixels.\nBut rather than immediately put that into the neural network layers\nthat we've seen before, we'll start by applying a convolution step,\nwhere the convolution step involves applying\nsome number of different image filters to our original image\nin order to get what we call a feature map, the result of applying\nsome filter to an image.\nAnd we could do this once, but in general, we'll do this multiple times,\ngetting a whole bunch of different feature maps, each of which\nmight extract some different relevant feature out of the image,\nsome different important characteristic of the image\nthat we might care about using in order to calculate\nwhat the result should be.\nAnd in the same way that when we train neural networks,\nwe can train neural networks to learn the weights between particular units\ninside of the neural networks, we can also train neural networks\nto learn what those filters should be, what\nthe values of the filters should be in order\nto get the most useful, most relevant information out of the original image\njust by figuring out what setting of those filter values,\nthe values inside of that kernel, results in minimizing the loss function,\nminimizing how poorly our hypothesis actually\nperforms in figuring out the classification of a particular image,\nfor example.\nSo we first apply this convolution step, get a whole bunch\nof these various different feature maps.\nBut these feature maps are quite large.\nThere's a lot of pixel values that happen to be here.\nAnd so a logical next step to take is a pooling step,\nwhere we reduce the size of these images by using max pooling,\nfor example, extracting the maximum value from any particular region.\nThere are other pooling methods that exist as well,\ndepending on the situation.\nYou could use something like average pooling,\nwhere instead of taking the maximum value from a region,\nyou take the average value from a region, which has its uses as well.\nBut in effect, what pooling will do is it will take these feature maps\nand reduce their dimensions so that we end up\nwith smaller grids with fewer pixels.\nAnd this then is going to be easier for us to deal with.\nIt's going to mean fewer inputs that we have to worry about.\nAnd it's also going to mean we're more resilient,\nmore robust against potential movements of particular values,\njust by one pixel, when ultimately we really\ndon't care about those one-pixel differences that\nmight arise in the original image.\nAnd now, after we've done this pooling step,\nnow we have a whole bunch of values that we can then flatten out and just put\ninto a more traditional neural network.\nSo we go ahead and flatten it, and then we\nend up with a traditional neural network that\nhas one input for each of these values in each of these resulting feature\nmaps after we do the convolution and after we do the pooling step.\nAnd so this then is the general structure of a convolutional network.\nWe begin with the image, apply convolution, apply pooling,\nflatten the results, and then put that into a more traditional neural\nnetwork that might itself have hidden layers.\nYou can have deep convolutional networks that\nhave hidden layers in between this flattened layer and the eventual output\nto be able to calculate various different features of those values.\nBut this then can help us to be able to use convolution and pooling\nto use our knowledge about the structure of an image\nto be able to get better results, to be able to train our networks faster\nin order to better capture particular parts of the image.\nAnd there's no reason necessarily why you can only use these steps once.\nIn fact, in practice, you'll often use convolution and pooling\nmultiple times in multiple different steps.\nSee, what you might imagine doing is starting with an image,\nfirst applying convolution to get a whole bunch of maps,\nthen applying pooling, then applying convolution again,\nbecause these maps are still pretty big.\nYou can apply convolution to try and extract relevant features out\nof this result. Then take those results, apply pooling\nin order to reduce their dimensions, and then take that\nand feed it into a neural network that maybe has fewer inputs.\nSo here I have two different convolution and pooling steps.\nI do convolution and pooling once, and then I do convolution and pooling\na second time, each time extracting useful features\nfrom the layer before it, each time using pooling\nto reduce the dimensions of what you're ultimately looking at.\nAnd the goal now of this sort of model is that in each of these steps,\nyou can begin to learn different types of features of the original image.\nThat maybe in the first step, you learn very low level features.\nJust learn and look for features like edges and curves and shapes,\nbecause based on pixels and their neighboring values, you can figure out,\nall right, what are the edges?\nWhat are the curves?\nWhat are the various different shapes that might be present there?\nBut then once you have a mapping that just represents\nwhere the edges and curves and shapes happen to be,\nyou can imagine applying the same sort of process again\nto begin to look for higher level features, look for objects,\nmaybe look for people's eyes and facial recognition, for example.\nMaybe look for more complex shapes like the curves on a particular number\nif you're trying to recognize a digit in a handwriting recognition sort\nof scenario.\nAnd then after all of that, now that you have these results that\nrepresent these higher level features, you\ncan pass them into a neural network, which is really just a deep neural\nnetwork that looks like this, where you might imagine\nmaking a binary classification or classifying into multiple categories\nor performing various different tasks on this sort of model.\nSo convolutional neural networks can be quite powerful and quite popular\nwhen it comes towards trying to analyze images.\nWe don't strictly need them.\nWe could have just used a vanilla neural network\nthat just operates with layer after layer, as we've seen before.\nBut these convolutional neural networks can be quite helpful,\nin particular, because of the way they model\nthe way a human might look at an image, that instead of a human looking\nat every single pixel simultaneously and trying to convolve all of them\nby multiplying them together, you might imagine\nthat what convolution is really doing is looking\nat various different regions of the image\nand extracting relevant information and features out\nof those parts of the image, the same way\nthat a human might have visual receptors that\nare looking at particular parts of what they see\nand using those combining them to figure out\nwhat meaning they can draw from all of those various different inputs.\nAnd so you might imagine applying this to a situation\nlike handwriting recognition.\nSo we'll go ahead and see an example of that now,\nwhere I'll go ahead and open up handwriting.py.\nAgain, what we do here is we first import TensorFlow.\nAnd then TensorFlow, it turns out, has a few data sets\nthat are built into the library that you can just immediately access.\nAnd one of the most famous data sets in machine learning\nis the MNIST data set, which is just a data\nset of a whole bunch of samples of people's handwritten digits.\nI showed you a slide of that a little while ago.\nAnd what we can do is just immediately access\nthat data set which is built into the library\nso that if I want to do something like train\non a whole bunch of handwritten digits, I can just use the data set\nthat is provided to me.\nOf course, if I had my own data set of handwritten images,\nI can apply the same idea.\nI'd first just need to take those images and turn them\ninto an array of pixels, because that's the way that these\nare going to be formatted.\nThey're going to be formatted as, effectively,\nan array of individual pixels.\nNow there's a bit of reshaping I need to do,\njust turning the data into a format that I\ncan put into my convolutional neural network.\nSo this is doing things like taking all the values\nand dividing them by 255.\nIf you remember, these color values tend to range from 0 to 255.\nSo I can divide them by 255 just to put them\ninto 0 to 1 range, which might be a little bit easier to train on.\nAnd then doing various other modifications to the data\njust to get it into a nice usable format.\nBut here's the interesting and important part.\nHere is where I create the convolutional neural network, the CNN,\nwhere here I'm saying, go ahead and use a sequential model.\nAnd before I could use model.add to say add a layer, add a layer, add a layer,\nanother way I could define it is just by passing as input\nto this sequential neural network a list of all of the layers that I want.\nAnd so here, the very first layer in my model is a convolution layer,\nwhere I'm first going to apply convolution to my image.\nI'm going to use 13 different filters.\nSo my model is going to learn 32, rather, 32 different filters\nthat I would like to learn on the input image, where each filter is going\nto be a 3 by 3 kernel.\nSo we saw those 3 by 3 kernels before, where\nwe could multiply each value in a 3 by 3 grid by a value,\nmultiply it, and add all the results together.\nSo here, I'm going to learn 32 different of these 3 by 3 filters.\nI can, again, specify my activation function.\nAnd I specify what my input shape is.\nMy input shape in the banknotes case was just 4.\nI had 4 inputs.\nMy input shape here is going to be 28, 28, 1,\nbecause for each of these handwritten digits,\nit turns out that the MNIST data set organizes their data.\nEach image is a 28 by 28 pixel grid.\nSo we're going to have a 28 by 28 pixel grid.\nAnd each one of those images only has one channel value.\nThese handwritten digits are just black and white.\nSo there's just a single color value representing\nhow much black or how much white.\nYou might imagine that in a color image, if you\nwere doing this sort of thing, you might have three different channels,\na red, a green, and a blue channel, for example.\nBut in the case of just handwriting recognition,\nrecognizing a digit, we're just going to use a single value for,\nlike, shaded in or not shaded in.\nAnd it might range, but it's just a single color value.\nAnd that, then, is the very first layer of our neural network,\na convolutional layer that will take the input\nand learn a whole bunch of different filters\nthat we can apply to the input to extract meaningful features.\nNext step is going to be a max pooling layer, also built right\ninto TensorFlow, where this is going to be a layer that\nis going to use a pool size of 2 by 2, meaning\nwe're going to look at 2 by 2 regions inside of the image\nand just extract the maximum value.\nAgain, we've seen why this can be helpful.\nIt'll help to reduce the size of our input.\nAnd once we've done that, we'll go ahead and flatten all of the units\njust into a single layer that we can then\npass into the rest of the neural network.\nAnd now, here's the rest of the neural network.\nHere, I'm saying, let's add a hidden layer to my neural network\nwith 128 units, so a whole bunch of hidden units\ninside of the hidden layer.\nAnd just to prevent overfitting, I can add a dropout to that.\nSay, you know what, when you're training, randomly dropout half\nof the nodes from this hidden layer just to make sure\nwe don't become over-reliant on any particular node,\nwe begin to really generalize and stop ourselves from overfitting.\nSo TensorFlow allows us, just by adding a single line,\nto add dropout into our model as well, such that when it's training,\nit will perform this dropout step in order\nto help make sure that we don't overfit on this particular data.\nAnd then finally, I add an output layer.\nThe output layer is going to have 10 units, one for each category\nthat I would like to classify digits into, so 0 through 9,\n10 different categories.\nAnd the activation function I'm going to use here\nis called the softmax activation function.\nAnd in short, what the softmax activation function is going to do\nis it's going to take the output and turn it\ninto a probability distribution.\nSo ultimately, it's going to tell me, what\ndid we estimate the probability is that this\nis a 2 versus a 3 versus a 4.\nAnd so it will turn it into that probability distribution for me.\nNext up, I'll go ahead and compile my model\nand fit it on all of my training data.\nAnd then I can evaluate how well the neural network performs.\nAnd then I've added to my Python program,\nif I've provided a command line argument like the name of a file,\nI'm going to go ahead and save the model to a file.\nAnd so this can be quite useful too.\nOnce you've done the training step, which could take some time in terms\nof taking all the time, going through the data,\nrunning back propagation with gradient descent to be able to say, all right,\nhow should we adjust the weight to this particular model?\nYou end up calculating values for these weights,\ncalculating values for these filters.\nYou'd like to remember that information so you can use it later.\nAnd so TensorFlow allows us to just save a model to a file,\nsuch that later, if we want to use the model we've learned,\nuse the weights that we've learned to make some sort of new prediction,\nwe can just use the model that already exists.\nSo what we're doing here is after we've done all the calculation,\nwe go ahead and save the model to a file, such\nthat we can use it a little bit later.\nSo for example, if I go into digits, I'm going to run handwriting.py.\nI won't save it this time.\nWe'll just run it and go ahead and see what happens.\nWhat will happen is we need to go through the model in order\nto train on all of these samples of handwritten digits.\nThe MNIST data set gives us thousands and thousands\nof sample handwritten digits in the same format\nthat we can use in order to train.\nAnd so now what you're seeing is this training process.\nAnd unlike the banknotes case, where there was much fewer data points,\nthe data was very, very simple, here this data is more complex\nand this training process takes time.\nAnd so this is another one of those cases where when training neural networks,\nthis is why computational power is so important that oftentimes you\nsee people wanting to use sophisticated GPUs in order\nto more efficiently be able to do this sort of neural network training.\nIt also speaks to the reason why more data can be helpful.\nThe more sample data points you have, the better\nyou can begin to do this training.\nSo here we're going through 60,000 different samples of handwritten digits.\nAnd I said we're going to go through them 10 times.\nWe're going to go through the data set 10 times, training each time,\nhopefully improving upon our weights with every time\nwe run through this data set.\nAnd we can see over here on the right what the accuracy is each time\nwe go ahead and run this model, that the first time it\nlooks like we got an accuracy of about 92% of the digits\ncorrect based on this training set.\nWe increased that to 96% or 97%.\nAnd every time we run this, we're going to see hopefully the accuracy\nimprove as we continue to try and use that gradient descent,\nthat process of trying to run the algorithm,\nto minimize the loss that we get in order to more accurately\npredict what the output should be.\nAnd what this process is doing is it's learning not only the weights,\nbut it's learning the features to use, the kernel matrix\nto use when performing that convolution step.\nBecause this is a convolutional neural network,\nwhere I'm first performing those convolutions\nand then doing the more traditional neural network structure,\nthis is going to learn all of those individual steps as well.\nAnd so here we see the TensorFlow provides me with some very nice output,\ntelling me about how many seconds are left with each of these training\nruns that allows me to see just how well we're doing.\nSo we'll go ahead and see how this network performs.\nIt looks like we've gone through the data set seven times.\nWe're going through it an eighth time now.\nAnd at this point, the accuracy is pretty high.\nWe saw we went from 92% up to 97%.\nNow it looks like 98%.\nAnd at this point, it seems like things are starting to level out.\nIt's probably a limit to how accurate we can ultimately be\nwithout running the risk of overfitting.\nOf course, with enough nodes, you would just\nmemorize the input and overfit upon them.\nBut we'd like to avoid doing that.\nAnd Dropout will help us with this.\nBut now we see we're almost done finishing our training step.\nWe're at 55,000.\nAll right, we finished training.\nAnd now it's going to go ahead and test for us on 10,000 samples.\nAnd it looks like on the testing set, we were at 98.8% accurate.\nSo we ended up doing pretty well, it seems,\non this testing set to see how accurately can we\npredict these handwritten digits.\nAnd so what we could do then is actually test it out.\nI've written a program called Recognition.py using PyGame.\nIf you pass it a model that's been trained,\nand I pre-trained an example model using this input data, what we can do\nis see whether or not we've been able to train\nthis convolutional neural network to be able to predict handwriting,\nfor example.\nSo I can try, just like drawing a handwritten digit.\nI'll go ahead and draw the number 2, for example.\nSo there's my number 2.\nAgain, this is messy.\nIf you tried to imagine, how would you write a program with just ifs\nand thens to be able to do this sort of calculation,\nit would be tricky to do so.\nBut here I'll press Classify, and all right,\nit seems I was able to correctly classify that what I drew was the number 2.\nI'll go ahead and reset it, try it again.\nWe'll draw an 8, for example.\nSo here is an 8.\nPress Classify.\nAnd all right, it predicts that the digit that I drew was an 8.\nAnd the key here is this really begins to show the power of what\nthe neural network is doing, somehow looking\nat various different features of these different pixels,\nfiguring out what the relevant features are,\nand figuring out how to combine them to get a classification.\nAnd this would be a difficult task to provide explicit instructions\nto the computer on how to do, to use a whole bunch of ifs ands\nto process all these pixel values to figure out\nwhat the handwritten digit is.\nEveryone's going to draw their 8s a little bit differently.\nIf I drew the 8 again, it would look a little bit different.\nAnd yet, ideally, we want to train a network to be robust enough\nso that it begins to learn these patterns on its own.\nAll I said was, here is the structure of the network,\nand here is the data on which to train the network.\nAnd the network learning algorithm just tries\nto figure out what is the optimal set of weights, what\nis the optimal set of filters to use them in order\nto be able to accurately classify a digit into one category or another.\nJust going to show the power of these sorts of convolutional neural\nnetworks.\nAnd so that then was a look at how we can use convolutional neural networks\nto begin to solve problems with regards to computer vision,\nthe ability to take an image and begin to analyze it.\nSo this is the type of analysis you might imagine\nthat's happening in self-driving cars that\nare able to figure out what filters to apply to an image\nto understand what it is that the computer is looking at,\nor the same type of idea that might be applied\nto facial recognition and social media to be\nable to determine how to recognize faces in an image as well.\nYou can imagine a neural network that instead of classifying\ninto one of 10 different digits could instead classify like,\nis this person A or is this person B, trying\nto tell those people apart just based on convolution.\nAnd so now what we'll take a look at is yet another type of neural network\nthat can be quite popular for certain types of tasks.\nBut to do so, we'll try to generalize and think about our neural network\na little bit more abstractly.\nThat here we have a sample deep neural network\nwhere we have this input layer, a whole bunch of different hidden layers\nthat are performing certain types of calculations,\nand then an output layer here that just generates some sort of output\nthat we care about calculating.\nBut we could imagine representing this a little more simply like this.\nHere is just a more abstract representation of our neural network.\nWe have some input that might be like a vector\nof a whole bunch of different values as our input.\nThat gets passed into a network that performs some sort of calculation\nor computation, and that network produces some sort of output.\nThat output might be a single value.\nIt might be a whole bunch of different values.\nBut this is the general structure of the neural network that we've seen.\nThere is some sort of input that gets fed into the network.\nAnd using that input, the network calculates what the output should be.\nAnd this sort of model for a neural network\nis what we might call a feed-forward neural network.\nFeed-forward neural networks have connections only in one direction.\nThey move from one layer to the next layer to the layer after that,\nsuch that the inputs pass through various different hidden layers\nand then ultimately produce some sort of output.\nSo feed-forward neural networks were very helpful\nfor solving these types of classification problems that we saw before.\nWe have a whole bunch of input.\nWe want to learn what setting of weights will allow us\nto calculate the output effectively.\nBut there are some limitations on feed-forward neural networks\nthat we'll see in a moment.\nIn particular, the input needs to be of a fixed shape,\nlike a fixed number of neurons are in the input layer.\nAnd there's a fixed shape for the output,\nlike a fixed number of neurons in the output layer.\nAnd that has some limitations of its own.\nAnd a possible solution to this, and we'll\nsee examples of the types of problems we can solve for this in just a second,\nis instead of just a feed-forward neural network,\nwhere there are only connections in one direction from left to right\neffectively across the network, we could also imagine a recurrent neural\nnetwork, where a recurrent neural network generates\noutput that gets fed back into itself as input for future runs of that network.\nSo whereas in a traditional neural network,\nwe have inputs that get fed into the network, that get fed into the output.\nAnd the only thing that determines the output\nis based on the original input and based on the calculation\nwe do inside of the network itself.\nThis goes in contrast with a recurrent neural network,\nwhere in a recurrent neural network, you can imagine output from the network\nfeeding back to itself into the network again as input\nfor the next time you do the calculations inside of the network.\nWhat this allows is it allows the network to maintain some sort of state,\nto store some sort of information that can be used on future runs of the network.\nPreviously, the network just defined some weights,\nand we passed inputs through the network, and it generated outputs.\nBut the network wasn't saving any information based on those inputs\nto be able to remember for future iterations or for future runs.\nWhat a recurrent neural network will let us do\nis let the network store information that gets passed back in as input\nto the network again the next time we try and perform some sort of action.\nAnd this is particularly helpful when dealing with sequences of data.\nSo we'll see a real world example of this right now, actually.\nMicrosoft has developed an AI known as the caption bot.\nAnd what the caption bot does is it says,\nI can understand the content of any photograph,\nand I'll try to describe it as well as any human.\nI'll analyze your photo, but I won't store it or share it.\nAnd so what Microsoft's caption bot seems to be claiming to do\nis it can take an image and figure out what's in the image\nand just give us a caption to describe it.\nSo let's try it out.\nHere, for example, is an image of Harvard Square.\nIt's some people walking in front of one of the buildings at Harvard Square.\nI'll go ahead and take the URL for that image,\nand I'll paste it into caption bot and just press Go.\nSo caption bot is analyzing the image, and then it\nsays, I think it's a group of people walking\nin front of a building, which seems amazing.\nThe AI is able to look at this image and figure out what's in the image.\nAnd the important thing to recognize here\nis that this is no longer just a classification task.\nWe saw being able to classify images with a convolutional neural network\nwhere the job was take the image and then figure out,\nis it a 0 or a 1 or a 2, or is it this person's face or that person's face?\nWhat seems to be happening here is the input is an image,\nand we know how to get networks to take input of images,\nbut the output is text.\nIt's a sentence.\nIt's a phrase, like a group of people walking in front of a building.\nAnd this would seem to pose a challenge for our more traditional feed-forward\nneural networks, for the reason being that in traditional neural networks,\nwe just have a fixed-size input and a fixed-size output.\nThere are a certain number of neurons in the input to our neural network\nand a certain number of outputs for our neural network,\nand then some calculation that goes on in between.\nBut the size of the inputs and the number of values in the input\nand the number of values in the output, those\nare always going to be fixed based on the structure of the neural network.\nAnd that makes it difficult to imagine how a neural network could take an image\nlike this and say it's a group of people walking in front of the building\nbecause the output is text, like it's a sequence of words.\nNow, it might be possible for a neural network\nto output one word, one word you could represent as a vector of values,\nand you can imagine ways of doing that.\nNext time, we'll talk a little bit more about AI\nas it relates to language and language processing.\nBut a sequence of words is much more challenging\nbecause depending on the image, you might imagine the output\nis a different number of words.\nWe could have sequences of different lengths,\nand somehow we still want to be able to generate the appropriate output.\nAnd so the strategy here is to use a recurrent neural network,\na neural network that can feed its own output back into itself\nas input for the next time.\nAnd this allows us to do what we call a one-to-many relationship\nfor inputs to outputs, that in vanilla, more traditional neural networks,\nthese are what we might consider to be one-to-one neural networks.\nYou pass in one set of values as input.\nYou get one vector of values as the output.\nBut in this case, we want to pass in one value as input, the image,\nand we want to get a sequence, many values as output,\nwhere each value is like one of these words that\ngets produced by this particular algorithm.\nAnd so the way we might do this is we might imagine starting\nby providing input, the image, into our neural network.\nAnd the neural network is going to generate output,\nbut the output is not going to be the whole sequence of words,\nbecause we can't represent the whole sequence of words\nusing just a fixed set of neurons.\nInstead, the output is just going to be the first word.\nWe're going to train the network to output what the first word of the caption\nshould be.\nAnd you could imagine that Microsoft has trained this\nby running a whole bunch of training samples through the AI,\ngiving it a whole bunch of pictures and what the appropriate caption was,\nand having the AI begin to learn from that.\nBut now, because the network generates output\nthat can be fed back into itself, you could\nimagine the output of the network being fed back into the same network.\nThis here looks like a separate network, but it's really\nthe same network that's just getting different input,\nthat this network's output gets fed back into itself,\nbut it's going to generate another output.\nAnd that other output is going to be the second word in the caption.\nAnd this recurrent neural network then, this network\nis going to generate other output that can be fed back into itself\nto generate yet another word, fed back into itself\nto generate another word.\nAnd so recurrent neural networks allow us to represent this one-to-many\nstructure.\nYou provide one image as input, and the neural network\ncan pass data into the next run of the network, and then again and again,\nsuch that you could run the network multiple times,\neach time generating a different output still based on that original input.\nAnd this is where recurrent neural networks become particularly useful\nwhen dealing with sequences of inputs or outputs.\nAnd my output is a sequence of words, and since I can't very easily\nrepresent outputting an entire sequence of words,\nI'll instead output that sequence one word at a time\nby allowing my network to pass information about what still\nneeds to be said about the photo into the next stage of running the network.\nSo you could run the network multiple times, the same network\nwith the same weights, just getting different input each time.\nFirst, getting input from the image, and then getting input from the network\nitself as additional information about what additionally\nneeds to be given in a particular caption, for example.\nSo this then is a one-to-many relationship inside of a recurrent neural\nnetwork, but it turns out there are other models that we can use,\nother ways we can try and use recurrent neural networks\nto be able to represent data that might be stored in other forms as well.\nWe saw how we could use neural networks in order to analyze images\nin the context of convolutional neural networks that take an image,\nfigure out various different properties of the image,\nand are able to draw some sort of conclusion based on that.\nBut you might imagine that something like YouTube,\nthey need to be able to do a lot of learning based on video.\nThey need to look through videos to detect if they're like copyright\nviolations, or they need to be able to look through videos to maybe identify\nwhat particular items are inside of the video, for example.\nAnd video, you might imagine, is much more difficult to put in\nas input to a neural network, because whereas an image, you could just\ntreat each pixel as a different value, videos are sequences.\nThey're sequences of images, and each sequence might be of different length.\nAnd so it might be challenging to represent that entire video\nas a single vector of values that you could pass in to a neural network.\nAnd so here, too, recurrent neural networks\ncan be a valuable solution for trying to solve this type of problem.\nThen instead of just passing in a single input into our neural network,\nwe could pass in the input one frame at a time, you might imagine.\nFirst, taking the first frame of the video, passing it into the network,\nand then maybe not having the network output anything at all yet.\nLet it take in another input, and this time, pass it into the network.\nBut the network gets information from the last time\nwe provided an input into the network.\nThen we pass in a third input, and then a fourth input,\nwhere each time, what the network gets is it gets the most recent input,\nlike each frame of the video.\nBut it also gets information the network processed\nfrom all of the previous iterations.\nSo on frame number four, you end up getting the input for frame number four\nplus information the network has calculated from the first three frames.\nAnd using all of that data combined, this recurrent neural network\ncan begin to learn how to extract patterns from a sequence of data\nas well.\nAnd so you might imagine, if you want to classify a video\ninto a number of different genres, like an educational video,\nor a music video, or different types of videos,\nthat's a classification task, where you want\nto take as input each of the frames of the video,\nand you want to output something like what it is, what category\nthat it happens to belong to.\nAnd you can imagine doing this sort of thing,\nthis sort of many-to-one learning, any time your input is a sequence.\nAnd so input is a sequence in the context of video.\nIt could be in the context of, like, if someone has typed a message\nand you want to be able to categorize that message,\nlike if you're trying to take a movie review and trying to classify it\nas, is it a positive review or a negative review?\nThat input is a sequence of words, and the output\nis a classification, positive or negative.\nThere, too, a recurrent neural network might\nbe helpful for analyzing sequences of words.\nAnd they're quite popular when it comes to dealing with language.\nCould even be used for spoken language as well,\nthat spoken language is an audio waveform that\ncan be segmented into distinct chunks.\nAnd each of those could be passed in as an input\ninto a recurrent neural network to be able to classify someone's voice,\nfor instance.\nIf you want to do voice recognition to say, is this one person or is this\nanother, here are also cases where you might\nwant this many-to-one architecture for a recurrent neural network.\nAnd then as one final problem, just to take\na look at in terms of what we can do with these sorts of networks,\nimagine what Google Translate is doing.\nSo what Google Translate is doing is it's taking some text written\nin one language and converting it into text written in some other language,\nfor example, where now this input is a sequence of data.\nIt's a sequence of words.\nAnd the output is a sequence of words as well.\nIt's also a sequence.\nSo here we want effectively a many-to-many relationship.\nOur input is a sequence and our output is a sequence as well.\nAnd it's not quite going to work to just say,\ntake each word in the input and translate it into a word in the output.\nBecause ultimately, different languages put their words in different orders.\nAnd maybe one language uses two words for something,\nwhereas another language only uses one.\nSo we really want some way to take this information, this input,\nencode it somehow, and use that encoding to generate\nwhat the output ultimately should be.\nAnd this has been one of the big advancements in automated translation\ntechnology, is the ability to use the neural networks to do this instead\nof older, more traditional methods.\nAnd this has improved accuracy dramatically.\nAnd the way you might imagine doing this is, again,\nusing a recurrent neural network with multiple inputs and multiple outputs.\nWe start by passing in all the input.\nInput goes into the network.\nAnother input, like another word, goes into the network.\nAnd we do this multiple times, like once for each word in the input\nthat I'm trying to translate.\nAnd only after all of that is done does the network now\nstart to generate output, like the first word of the translated sentence,\nand the next word of the translated sentence, so on and so forth,\nwhere each time the network passes information to itself\nby allowing for this model of giving some sort of state\nfrom one run in the network to the next run,\nassembling information about all the inputs,\nand then passing in information about which part of the output\nin order to generate next.\nAnd there are a number of different types of these sorts of recurrent neural\nnetworks.\nOne of the most popular is known as the long short-term memory neural network,\notherwise known as LSTM.\nBut in general, these types of networks can be very, very powerful whenever\nwe're dealing with sequences, whether those are sequences of images\nor especially sequences of words when it comes\ntowards dealing with natural language.\nAnd so that then were just some of the different types\nof neural networks that can be used to do all sorts of different computations.\nAnd these are incredibly versatile tools that\ncan be applied to a number of different domains.\nWe only looked at a couple of the most popular types of neural networks\nfrom more traditional feed-forward neural networks, convolutional neural\nnetworks, and recurrent neural networks.\nBut there are other types as well.\nThere are adversarial networks where networks compete with each other\nto try and be able to generate new types of data,\nas well as other networks that can solve other tasks based\non what they happen to be structured and adapted for.\nAnd these are very powerful tools in machine learning\nfrom being able to very easily learn based on some set of input data\nand to be able to, therefore, figure out how to calculate some function\nfrom inputs to outputs, whether it's input to some sort of classification\nlike analyzing an image and getting a digit or machine translation\nwhere the input is in one language and the output is in another.\nThese tools have a lot of applications for machine learning more generally.\nNext time, we'll look at machine learning and AI in particular\nin the context of natural language.\nWe talked a little bit about this today, but looking at how it is that our AI\ncan begin to understand natural language and can\nbegin to be able to analyze and do useful tasks with regards\nto human language, which turns out to be a challenging and interesting task.\nSo we'll see you next time.\nAnd welcome back, everybody, to our final class\nin an introduction to artificial intelligence with Python.\nNow, so far in this class, we've been taking problems\nthat we want to solve intelligently and framing them\nin ways that computers are going to be able to make sense of.\nWe've been taking problems and framing them as search problems\nor constraint satisfaction problems or optimization problems, for example.\nIn essence, we have been trying to communicate\nabout problems in ways that our computer is going to be able to understand.\nToday, the goal is going to be to get computers\nto understand the way you and I communicate naturally\nvia our own natural languages, languages like English.\nBut natural language contains a lot of nuance and complexity\nthat's going to make it challenging for computers to be able to understand.\nSo we'll need to explore some new tools and some new techniques\nto allow computers to make sense of natural language.\nSo what is it exactly that we're trying to get computers to do?\nWell, they all fall under this general heading of natural language processing,\ngetting computers to work with natural language.\nAnd these tasks include tasks like automatic summarization.\nGiven a long text, can we train the computer\nto be able to come up with a shorter representation of it?\nInformation extraction, getting the computer\nto pull out relevant facts or details out of some text.\nMachine translation, like Google Translate,\ntranslating some text from one language into another language.\nQuestion answering, if you've ever asked a question to your phone\nor had a conversation with an AI chatbot where you provide some text\nto the computer, the computer is able to understand that text\nand then generate some text in response.\nText classification, where we provide some text to the computer\nand the computer assigns it a label, positive or negative,\ninbox or spam, for example.\nAnd there are several other kinds of tasks\nthat all fall under this heading of natural language processing.\nBut before we take a look at how the computer might\ntry to solve these kinds of tasks, it might be useful for us\nto think about language in general.\nWhat are the kinds of challenges that we might need to deal with\nas we start to think about language and getting a computer\nto be able to understand it?\nSo one part of language that we'll need to consider\nis the syntax of language.\nSyntax is all about the structure of language.\nLanguage is composed of individual words.\nAnd those words are composed together in some kind of structured whole.\nAnd if our computer is going to be able to understand language,\nit's going to need to understand something about that structure.\nSo let's take a couple of examples.\nHere, for instance, is a sentence.\nJust before 9 o'clock, Sherlock Holmes stepped briskly into the room.\nThat sentence is made up of words.\nAnd those words together form a structured whole.\nThis is syntactically valid as a sentence.\nBut we could take some of those same words,\nrearrange them, and come up with a sentence that is not syntactically valid.\nHere, for example, just before Sherlock Holmes 9 o'clock stepped briskly\nthe room is still composed of valid words.\nBut they're not in any kind of logical whole.\nThis is not a syntactically well-formed sentence.\nAnother interesting challenge is that some sentences will\nhave multiple possible valid structures.\nHere's a sentence, for example.\nI saw the man on the mountain with a telescope.\nAnd here, this is a valid sentence.\nBut it actually has two different possible structures\nthat lend themselves to two different interpretations\nand two different meanings.\nMaybe I, the one doing the seeing, am the one with the telescope.\nOr maybe the man on the mountain is the one with the telescope.\nAnd so natural language is ambiguous.\nSometimes the same sentence can be interpreted in multiple ways.\nAnd that's something that we'll need to think about as well.\nAnd this lends itself to another problem within language\nthat we'll need to think about, which is semantics.\nWhile syntax is all about the structure of language,\nsemantics is about the meaning of language.\nIt's not enough for a computer just to know\nthat a sentence is well-structured if it doesn't\nknow what that sentence means.\nAnd so semantics is going to concern itself\nwith the meaning of words and the meaning of sentences.\nSo if we go back to that same sentence as before,\njust before 9 o'clock, Sherlock Holmes stepped briskly into the room,\nI could come up with another sentence, say the sentence,\na few minutes before 9, Sherlock Holmes walked quickly into the room.\nAnd those are two different sentences with some of the words the same\nand some of the words different.\nBut the two sentences have essentially the same meaning.\nAnd so ideally, whatever model we build, we'll\nbe able to understand that these two sentences, while different,\nmean something very similar.\nSome syntactically well-formed sentences don't mean anything at all.\nA famous example from linguist Noam Chomsky\nis the sentence, colorless green ideas sleep furiously.\nThis is a syntactically, structurally well-formed sentence.\nWe've got adjectives modifying a noun, ideas.\nWe've got a verb and an adverb in the correct positions.\nBut when taken as a whole, the sentence doesn't really mean anything.\nAnd so if our computers are going to be able to work with natural language\nand perform tasks in natural language processing,\nthese are some concerns we'll need to think about.\nWe'll need to be thinking about syntax.\nAnd we'll need to be thinking about semantics.\nSo how could we go about trying to teach a computer how\nto understand the structure of natural language?\nWell, one approach we might take is by starting\nby thinking about the rules of natural language.\nOur natural languages have rules.\nIn English, for example, nouns tend to come before verbs.\nNouns can be modified by adjectives, for example.\nAnd so if only we could formalize those rules,\nthen we could give those rules to a computer,\nand the computer would be able to make sense of them and understand them.\nAnd so let's try to do exactly that.\nWe're going to try to define a formal grammar.\nWhere a formal grammar is some system of rules\nfor generating sentences in a language.\nThis is going to be a rule-based approach to natural language processing.\nWe're going to give the computer some rules that we know about language\nand have the computer use those rules to make\nsense of the structure of language.\nAnd there are a number of different types of formal grammars.\nEach one of them has slightly different use cases.\nBut today, we're going to focus specifically\non one kind of grammar known as a context-free grammar.\nSo how does the context-free grammar work?\nWell, here is a sentence that we might want a computer to generate.\nShe saw the city.\nAnd we're going to call each of these words a terminal symbol.\nA terminal symbol, because once our computer has generated the word,\nthere's nothing else for it to generate.\nOnce it's generated the sentence, the computer is done.\nWe're going to associate each of these terminal symbols\nwith a non-terminal symbol that generates it.\nSo here we've got n, which stands for noun, like she or city.\nWe've got v as a non-terminal symbol, which stands for a verb.\nAnd then we have d, which stands for determiner.\nA determiner is a word like the or a or an in English, for example.\nSo each of these non-terminal symbols can generate the terminal symbols\nthat we ultimately care about generating.\nBut how do we know, or how does the computer\nknow which non-terminal symbols are associated with which terminal symbols?\nWell, to do that, we need some kind of rule.\nHere are some what we call rewriting rules that\nhave a non-terminal symbol on the left-hand side of an arrow.\nAnd on the right side is what that non-terminal symbol can be replaced with.\nSo here we're saying the non-terminal symbol n, again,\nwhich stands for noun, could be replaced by any of these options separated\nby vertical bars.\nn could be replaced by she or city or car or hairy.\nd for determiner could be replaced by the a or an and so forth.\nEach of these non-terminal symbols could be replaced by any of these words.\nWe can also have non-terminal symbols that\nare replaced by other non-terminal symbols.\nHere is an interesting rule, np arrow n bar dn.\nSo what does that mean?\nWell, np stands for a noun phrase.\nSometimes when we have a noun phrase in a sentence,\nit's not just a single word, it could be multiple words.\nAnd so here we're saying a noun phrase could be just a noun,\nor it could be a determiner followed by a noun.\nSo we might have a noun phrase that's just a noun, like she,\nthat's a noun phrase.\nOr we could have a noun phrase that's multiple words, something\nlike the city also acts as a noun phrase.\nBut in this case, it's composed of two words, a determiner, the,\nand a noun city.\nWe could do the same for verb phrases.\nA verb phrase, or VP, might be just a verb,\nor it might be a verb followed by a noun phrase.\nSo we could have a verb phrase that's just a single word,\nlike the word walked, or we could have a verb phrase\nthat is an entire phrase, something like saw the city,\nas an entire verb phrase.\nA sentence, meanwhile, we might then define as a noun phrase\nfollowed by a verb phrase.\nAnd so this would allow us to generate a sentence like she saw the city,\nan entire sentence made up of a noun phrase, which is just the word she,\nand then a verb phrase, which is saw the city, saw which is a verb,\nand then the city, which itself is also a noun phrase.\nAnd so if we could give these rules to a computer explaining to it\nwhat non-terminal symbols could be replaced by what other symbols,\nthen a computer could take a sentence and begin\nto understand the structure of that sentence.\nAnd so let's take a look at an example of how we might do that.\nAnd to do that, we're going to use a Python library called NLTK,\nor the Natural Language Toolkit, which we'll see a couple of times today.\nIt contains a lot of helpful features and functions that we can use\nfor trying to deal with and process natural language.\nSo here we'll take a look at how we can use NLTK in order\nto parse a context-free grammar.\nSo let's go ahead and open up cfg0.py, cfg standing for context-free grammar.\nAnd what you'll see in this file is that I first import NLTK, the Natural\nLanguage Toolkit.\nAnd the first thing I do is define a context-free grammar,\nsaying that a sentence is a noun phrase followed by a verb phrase.\nI'm defining what a noun phrase is, defining what a verb phrase is,\nand then giving some examples of what I can\ndo with these non-terminal symbols, D for determiner, N for noun,\nand V for verb.\nWe're going to use NLTK to parse that grammar.\nThen we'll ask the user for some input in the form of a sentence\nand split it into words.\nAnd then we'll use this context-free grammar parser\nto try to parse that sentence and print out the resulting syntax tree.\nSo let's take a look at an example.\nWe'll go ahead and go into my cfg directory, and we'll run cfg0.py.\nAnd here I'm asked to type in a sentence.\nLet's say I type in she walked.\nAnd when I do that, I see that she walked is a valid sentence,\nwhere she is a noun phrase, and walked is the corresponding verb phrase.\nI could try to do this with a more complex sentence too.\nI could do something like she saw the city.\nAnd here we see that she is the noun phrase,\nand then saw the city is the entire verb phrase that makes up this sentence.\nSo that was a very simple grammar.\nLet's take a look at a slightly more complex grammar.\nHere is cfg1.py, where a sentence is still a noun phrase followed\nby a verb phrase, but I've added some other possible non-terminal symbols too.\nI have AP for adjective phrase and PP for prepositional phrase.\nAnd we specified that we could have an adjective phrase\nbefore a noun phrase or a prepositional phrase after a noun, for example.\nSo lots of additional ways that we might try to structure a sentence\nand interpret and parse one of those resulting sentences.\nSo let's see that one in action.\nWe'll go ahead and run cfg1.py with this new grammar.\nAnd we'll try a sentence like she saw the wide street.\nHere, Python's NLTK is able to parse that sentence\nand identify that she saw the wide street has this particular structure,\na sentence with a noun phrase and a verb phrase,\nwhere that verb phrase has a noun phrase that within it\ncontains an adjective.\nAnd so it's able to get some sense for what the structure of this language\nactually is.\nLet's try another example.\nLet's say she saw the dog with the binoculars.\nAnd we'll try that sentence.\nAnd here, we get one possible syntax tree,\nshe saw the dog with the binoculars.\nBut notice that this sentence is actually a little bit\nambiguous in our own natural language.\nWho has the binoculars?\nIs it she who has the binoculars or the dog who has the binoculars?\nAnd NLTK is able to identify both possible structures for the sentence.\nIn this case, the dog with the binoculars\nis an entire noun phrase.\nIt's all underneath this NP here.\nSo it's the dog that has the binoculars.\nBut we also got an alternative parse tree,\nwhere the dog is just the noun phrase.\nAnd with the binoculars is a prepositional phrase modifying saw.\nSo she saw the dog and she used the binoculars in order\nto see the dog as well.\nSo this allows us to get a sense for the structure of natural language.\nBut it relies on us writing all of these rules.\nAnd it would take a lot of effort to write all of the rules for any possible\nsentence that someone might write or say in the English language.\nLanguage is complicated.\nAnd as a result, there are going to be some very complex rules.\nSo what else might we try?\nWe might try to take a statistical lens towards approaching\nthis problem of natural language processing.\nIf we were able to give the computer a lot of existing data of sentences\nwritten in the English language, what could we try to learn from that data?\nWell, it might be difficult to try and interpret long pieces of text all\nat once.\nSo instead, what we might want to do is break up that longer text\ninto smaller pieces of information instead.\nIn particular, we might try to create n-grams out of a longer sequence of text.\nAn n-gram is just some contiguous sequence of n items from a sample of text.\nIt might be n characters in a row or n words in a row, for example.\nSo let's take a passage from Sherlock Holmes.\nAnd let's look for all of the trigrams.\nA trigram is an n-gram where n is equal to 3.\nSo in this case, we're looking for sequences of three words in a row.\nSo the trigrams here would be phrases like how often have.\nThat's three words in a row.\nOften have I is another trigram.\nHave I said, I said to, said to you, to you that.\nThese are all trigrams, sequences of three words that appear in sequence.\nAnd if we could give the computer a large corpus of text\nand have it pull out all of the trigrams in this case,\nit could get a sense for what sequences of three words\ntend to appear next to each other in our own natural language\nand, as a result, get some sense for what the structure of the language\nactually is.\nSo let's take a look at an example of that.\nHow can we use NLTK to try to get access to information about n-grams?\nSo here, we're going to open up ngrams.py.\nAnd this is a Python program that's going to load a corpus of data, just\nsome text files, into our computer's memory.\nAnd then we're going to use NLTK's ngrams function, which\nis going to go through the corpus of text, pulling out all of the ngrams\nfor a particular value of n.\nAnd then, by using Python's counter class,\nwe're going to figure out what are the most common ngrams inside\nof this entire corpus of text.\nAnd we're going to need a data set in order to do this.\nAnd I've prepared a data set of some of the stories of Sherlock Holmes.\nSo it's just a bunch of text files.\nA lot of words for it to analyze.\nAnd as a result, we'll get a sense for what sequences of two words or three\nwords that tend to be most common in natural language.\nSo let's give this a try.\nWe'll go into my ngrams directory.\nAnd we'll run ngrams.py.\nWe'll try an n value of 2.\nSo we're looking for sequences of two words in a row.\nAnd we'll use our corpus of stories from Sherlock Holmes.\nAnd when we run this program, we get a list of the most common ngrams\nwhere n is equal to 2, otherwise known as a bigram.\nSo the most common one is of the.\nThat's a sequence of two words that appears quite frequently\nin natural language.\nThen in the.\nAnd it was.\nThese are all common sequences of two words that appear in a row.\nLet's instead now try running ngrams with n equal to 3.\nLet's get all of the trigrams and see what we get.\nAnd now we see the most common trigrams are it was a.\nOne of the.\nI think that.\nThese are all sequences of three words that appear quite frequently.\nAnd we were able to do this essentially via a process known as tokenization.\nTokenization is the process of splitting a sequence of characters\ninto pieces.\nIn this case, we're splitting a long sequence of text into individual words\nand then looking at sequences of those words\nto get a sense for the structure of natural language.\nSo once we've done this, once we've done the tokenization,\nonce we've built up our corpus of ngrams, what\ncan we do with that information?\nSo the one thing that we might try is we could build a Markov chain,\nwhich you might recall from when we talked about probability.\nRecall that a Markov chain is some sequence of values\nwhere we can predict one value based on the values that came before it.\nAnd as a result, if we know all of the common ngrams in the English language,\nwhat words tend to be associated with what other words in sequence,\nwe can use that to predict what word might come next in a sequence of words.\nAnd so we could build a Markov chain for language\nin order to try to generate natural language that\nfollows the same statistical patterns as some input data.\nSo let's take a look at that and build a Markov chain for natural language.\nAnd as input, I'm going to use the works of William Shakespeare.\nSo here I have a file Shakespeare.txt, which\nis just a bunch of the works of William Shakespeare.\nIt's a long text file, so plenty of data to analyze.\nAnd here in generator.py, I'm using a third party Python library\nin order to do this analysis.\nWe're going to read in the sample of text,\nand then we're going to train a Markov model based on that text.\nAnd then we're going to have the Markov chain generate some sentences.\nWe're going to generate a sentence that doesn't appear in the original text,\nbut that follows the same statistical patterns that's generating it\nbased on the ngrams trying to predict what word is likely to come next\nthat we would expect based on those statistical patterns.\nSo we'll go ahead and go into our Markov directory,\nrun this generator with the works of William Shakespeare's input.\nAnd what we're going to get are five new sentences, where\nthese sentences are not necessarily sentences\nfrom the original input text itself, but just that\nfollow the same statistical patterns.\nIt's predicting what word is likely to come next based on the input data\nthat we've seen and the types of words that\ntend to appear in sequence there too.\nAnd so we're able to generate these sentences.\nOf course, so far, there's no guarantee that any of the sentences that\nare generated actually mean anything or make any sense.\nThey just happen to follow the statistical patterns\nthat our computer is already aware of.\nSo we'll return to this issue of how to generate text\nin perhaps a more accurate or more meaningful way a little bit later.\nSo let's now turn our attention to a slightly different problem,\nand that's the problem of text classification.\nText classification is the problem where we have some text\nand we want to put that text into some kind of category.\nWe want to apply some sort of label to that text.\nAnd this kind of problem shows up in a wide variety of places.\nA commonplace might be your email inbox, for example.\nYou get an email and you want your computer\nto be able to identify whether the email belongs in your inbox\nor whether it should be filtered out into spam.\nSo we need to classify the text.\nIs it a good email or is it spam?\nAnother common use case is sentiment analysis.\nWe might want to know whether the sentiment of some text\nis positive or negative.\nAnd so how might we do that?\nThis comes up in situations like product reviews,\nwhere we might have a bunch of reviews for a product on some website.\nMy grandson loved it so much fun.\nProduct broke after a few days.\nOne of the best games I've played in a long time and kind of cheap\nand flimsy, not worth it.\nHere's some example sentences that you might see on a product review website.\nAnd you and I could pretty easily look at this list of product reviews\nand decide which ones are positive and which ones are negative.\nWe might say the first one and the third one,\nthose seem like positive sentiment messages.\nBut the second one and the fourth one seem like negative sentiment messages.\nBut how did we know that?\nAnd how could we train a computer to be able to figure that out as well?\nWell, you might have clued your eye in on particular key words,\nwhere those particular words tend to mean something positive or negative.\nSo you might have identified words like loved and fun and best\ntend to be associated with positive messages.\nAnd words like broke and cheap and flimsy\ntend to be associated with negative messages.\nSo if only we could train a computer to be able to learn\nwhat words tend to be associated with positive versus negative messages,\nthen maybe we could train a computer to do this kind of sentiment analysis\nas well.\nSo we're going to try to do just that.\nWe're going to use a model known as the bag of words model, which\nis a model that represents text as just an unordered collection of words.\nFor the purpose of this model, we're not\ngoing to worry about the sequence and the ordering of the words,\nwhich word came first, second, or third.\nWe're just going to treat the text as a collection of words\nin no particular order.\nAnd we're losing information there, right?\nThe order of words is important.\nAnd we'll come back to that a little bit later.\nBut for now, to simplify our model, it'll\nhelp us tremendously just to think about text\nas some unordered collection of words.\nAnd in particular, we're going to use the bag of words model\nto build something known as a naive Bayes classifier.\nSo what is a naive Bayes classifier?\nWell, it's a tool that's going to allow us to classify text based on Bayes\nrule, again, which you might remember from when we talked about probability.\nBayes rule says that the probability of B given A\nis equal to the probability of A given B multiplied\nby the probability of B divided by the probability of A.\nSo how are we going to use this rule to be able to analyze text?\nWell, what are we interested in?\nWe're interested in the probability that a message has\na positive sentiment and the probability that a message has\na negative sentiment, which I'm here for simplicity\ngoing to represent just with these emoji, happy face and frown face,\nas positive and negative sentiment.\nAnd so if I had a review, something like my grandson loved it,\nthen what I'm interested in is not just the probability\nthat a message has positive sentiment, but the conditional probability\nthat a message has positive sentiment given\nthat this is the message my grandson loved it.\nBut how do I go about calculating this value, the probability\nthat the message is positive given that the review is this sequence of words?\nWell, here's where the bag of words model comes in.\nRather than treat this review as a string of a sequence of words in order,\nwe're just going to treat it as an unordered collection of words.\nWe're going to try to calculate the probability that the review is positive\ngiven that all of these words, my grandson loved it,\nare in the review in no particular order, just\nthis unordered collection of words.\nAnd this is a conditional probability, which we can then apply Bayes rule\nto try to make sense of.\nAnd so according to Bayes rule, this conditional probability is equal to what?\nIt's equal to the probability that all of these four words\nare in the review given that the review is positive multiplied\nby the probability that the review is positive divided by the probability\nthat all of these words happen to be in the review.\nSo this is the value now that we're going to try to calculate.\nNow, one thing you might notice is that the denominator here,\nthe probability that all of these words appear in the review,\ndoesn't actually depend on whether or not\nwe're looking at the positive sentiment or negative sentiment case.\nSo we can actually get rid of this denominator.\nWe don't need to calculate it.\nWe can just say that this probability is proportional to the numerator.\nAnd then at the end, we're going to need to normalize the probability\ndistribution to make sure that all of the values sum up to the value 1.\nSo now, how do we calculate this value?\nWell, this is the probability of all of these words given positive times\nprobability of positive.\nAnd that, by the definition of joint probability,\nis just one big joint probability, the probability\nthat all of these things are the case, that it's a positive review,\nand that all four of these words are in the review.\nBut still, it's not entirely obvious how we calculate that value.\nAnd here is where we need to make one more assumption.\nAnd this is where the naive part of naive Bayes comes in.\nWe're going to make the assumption that all of the words\nare independent of each other.\nAnd by that, I mean that if the word grandson is in the review,\nthat doesn't change the probability that the word loved is in the review\nor that the word it is in the review, for example.\nAnd in practice, this assumption might not be true.\nIt's almost certainly the case that the probability of words\ndo depend on each other.\nBut it's going to simplify our analysis and still give us reasonably good\nresults just to assume that the words are independent of each other\nand they only depend on whether it's positive or negative.\nYou might, for example, expect the word loved\nto appear more often in a positive review than in a negative review.\nSo what does that mean?\nWell, if we make this assumption, then we\ncan say that this value, the probability we're interested in,\nis not directly proportional to, but it's naively proportional to this value.\nThe probability that the review is positive times the probability\nthat my is in the review, given that it's positive,\ntimes the probability that grandson is in the review,\ngiven that it's positive, and so on for the other two words that\nhappen to be in this review.\nAnd now this value, which looks a little more complex,\nis actually a value that we can calculate pretty easily.\nSo how are we going to estimate the probability that the review is positive?\nWell, if we have some training data, some example data of example reviews\nwhere each one has already been labeled as positive or negative,\nthen we can estimate the probability that a review is positive\njust by counting the number of positive samples\nand dividing by the total number of samples that we have in our training\ndata.\nAnd for the conditional probabilities, the probability of loved,\ngiven that it's positive, well, that's going\nto be the number of positive samples with loved in it\ndivided by the total number of positive samples.\nSo let's take a look at an actual example to see how\nwe could try to calculate these values.\nHere I've put together some sample data.\nThe way to interpret the sample data is that based on the training data,\n49% of the reviews are positive, 51% are negative.\nAnd then over here in this table, we have some conditional probabilities.\nAnd then we have if the review is positive,\nthen there is a 30% chance that my appears in it.\nAnd if the review is negative, there is a 20% chance that my appears in it.\nAnd based on our training data among the positive reviews,\n1% of them contain the word grandson.\nAnd among the negative reviews, 2% contain the word grandson.\nSo using this data, let's try to calculate this value,\nthe value we're interested in.\nAnd to do that, we'll need to multiply all of these values together.\nThe probability of positive, and then all\nof these positive conditional probabilities.\nAnd when we do that, we get some value.\nAnd then we can do the same thing for the negative case.\nWe're going to do the same thing, take the probability that it's negative,\nmultiply it by all of these conditional probabilities,\nand we're going to get some other value.\nAnd now these values don't sum to one.\nThey're not a probability distribution yet.\nBut I can normalize them and get some values.\nAnd that tells me that we're going to predict that my grandson loved it.\nWe think there's a 68% chance, probability 0.68,\nthat that is a positive sentiment review, and 0.32 probability\nthat it's a negative review.\nSo what problems might we run into here?\nWhat could potentially go wrong when doing this kind of analysis\nin order to analyze whether text has a positive or negative sentiment?\nWell, a couple of problems might arise.\nOne problem might be, what if the word grandson never\nappears for any of the positive reviews?\nIf that were the case, then when we try to calculate the value,\nthe probability that we think the review is positive,\nwe're going to multiply all these values together,\nand we're just going to get 0 for the positive case,\nbecause we're all going to ultimately multiply by that 0 value.\nAnd so we're going to say that we think there is no chance\nthat the review is positive because it contains the word grandson.\nAnd in our training data, we've never seen the word grandson\nappear in a positive sentiment message before.\nAnd that's probably not the right analysis,\nbecause in cases of rare words, it might be the case\nthat in nowhere in our training data did we ever\nsee the word grandson appear in a message that has positive sentiment.\nSo what can we do to solve this problem?\nWell, one thing we'll often do is some kind of additive smoothing,\nwhere we add some value alpha to each value in our distribution\njust to smooth out the data a little bit.\nAnd a common form of this is Laplace smoothing,\nwhere we add 1 to each value in our distribution.\nIn essence, we pretend we've seen each value one more time\nthan we actually have.\nSo if we've never seen the word grandson for a positive review,\nwe pretend we've seen it once.\nIf we've seen it once, we pretend we've seen it twice,\njust to avoid the possibility that we might multiply by 0 and as a result,\nget some results we don't want in our analysis.\nSo let's see what this looks like in practice.\nLet's try to do some naive Bayes classification in order\nto classify text as either positive or negative.\nWe'll take a look at sentiment.py.\nAnd what this is going to do is load some sample data into memory,\nsome examples of positive reviews and negative reviews.\nAnd then we're going to train a naive Bayes classifier\non all of this training data, training data that\nincludes all of the words we see in positive reviews\nand all of the words we see in negative reviews.\nAnd then we're going to try to classify some input.\nAnd so we're going to do this based on a corpus of data.\nI have some example positive reviews.\nHere are some positive reviews.\nIt was great, so much fun, for example.\nAnd then some negative reviews, not worth it, kind of cheap.\nThese are some examples of negative reviews.\nSo now let's try to run this classifier and see\nhow it would classify particular text as either positive or negative.\nWe'll go ahead and run our sentiment analysis on this corpus.\nAnd we need to provide it with a review.\nSo I'll say something like, I enjoyed it.\nAnd we see that the classifier says there is about a 0.92 probability\nthat we think that this particular review is positive.\nLet's try something negative.\nWe'll try kind of overpriced.\nAnd we see that there is a 0.96 probability\nnow that we think that this particular review is negative.\nAnd so our naive Bayes classifier has learned what kinds of words\ntend to appear in positive reviews and what kinds of words\ntend to appear in negative reviews.\nAnd as a result of that, we've been able to design a classifier that\ncan predict whether a particular review is positive or negative.\nAnd so this definitely is a useful tool that we can use\nto try and make some predictions.\nBut we had to make some assumptions in order to get there.\nSo what if we want to now try to build some more sophisticated models,\nuse some tools from machine learning to try and take\nbetter advantage of language data to be able to draw\nmore accurate conclusions and solve new kinds of tasks\nand new kinds of problems?\nWell, we've seen a couple of times now that when we want to take some data\nand take some input, put it in a way that the computer is\ngoing to be able to make sense of, it can be helpful to take that data\nand turn it into numbers, ultimately.\nAnd so what we might want to try to do is come up\nwith some word representation, some way to take a word\nand translate its meaning into numbers.\nBecause, for example, if we wanted to use a neural network\nto be able to process language, give our language to a neural network\nand have it make some predictions or perform some analysis there,\na neural network takes its input and produces its output\na vector of values, a vector of numbers.\nAnd so what we might want to do is take our data\nand somehow take words and convert them into some kind\nof numeric representation.\nSo how might we do that?\nHow might we take words and turn them into numbers?\nLet's take a look at an example.\nHere's a sentence, he wrote a book.\nAnd let's say I wanted to take each of those words\nand turn it into a vector of values.\nHere's one way I might do that.\nWe'll say he is going to be a vector that has a 1 in the first position\nand the rest of the values are 0.\nWrote will have a 1 in the second position and the rest of the values\nare 0.\nA has a 1 in the third position with the rest of the value 0.\nAnd book has a 1 in the fourth position with the rest of the value 0.\nSo each of these words now has a distinct vector representation.\nAnd this is what we often call a one-hot representation,\na representation of the meaning of a word as a vector with a single 1\nand all of the rest of the values are 0.\nAnd so when doing this, we now have a numeric representation for every word\nand we could pass in those vector representations\ninto a neural network or other models that\nrequire some kind of numeric data as input.\nBut this one-hot representation actually has a couple of problems\nand it's not ideal for a few reasons.\nOne reason is, here we're just looking at four words.\nBut if you imagine a vocabulary of thousands of words or more,\nthese vectors are going to get quite long in order\nto have a distinct vector for every possible word in a vocabulary.\nAnd as a result of that, these longer vectors\nare going to be more difficult to deal with, more difficult to train,\nand so forth.\nAnd so that might be a problem.\nAnother problem is a little bit more subtle.\nIf we want to represent a word as a vector,\nand in particular the meaning of a word as a vector,\nthen ideally it should be the case that words that have similar meanings\nshould also have similar vector representations,\nso that they're close to each other together inside a vector space.\nBut that's not really going to be the case with these one-hot representations,\nbecause if we take some similar words, say the word\nwrote and the word authored, which means similar things,\nthey have entirely different vector representations.\nLikewise, book and novel, those two words mean somewhat similar things,\nbut they have entirely different vector representations\nbecause they each have a one in some different position.\nAnd so that's not ideal either.\nSo what we might be interested in instead\nis some kind of distributed representation.\nA distributed representation is the representation\nof the meaning of a word distributed across multiple values,\ninstead of just being one-hot with a one in one position.\nHere is what a distributed representation of words might be.\nEach word is associated with some vector of values,\nwith the meaning distributed across multiple values,\nideally in such a way that similar words have\na similar vector representation.\nBut how are we going to come up with those values?\nWhere do those values come from?\nHow can we define the meaning of a word in this distributed sequence of numbers?\nWell, to do that, we're going to draw inspiration\nfrom a quote from British linguist J.R. Firth, who said,\nyou shall know a word by the company it keeps.\nIn other words, we're going to define the meaning of a word\nbased on the words that appear around it, the context words around it.\nTake, for example, this context, for blank he ate.\nYou might wonder, what words could reasonably fill in that blank?\nWell, it might be words like breakfast or lunch or dinner.\nAll of those could reasonably fill in that blank.\nAnd so what we're going to say is because the words breakfast and lunch\nand dinner appear in a similar context, that they must have a similar meaning.\nAnd that's something our computer could understand and try to learn.\nA computer could look at a big corpus of text,\nlook at what words tend to appear in similar context to each other,\nand use that to identify which words have a similar meaning\nand should therefore appear close to each other inside a vector space.\nAnd so one common model for doing this is known as the word to vec model.\nIt's a model for generating word vectors, a vector representation for every word\nby looking at data and looking at the context in which a word appears.\nThe idea is going to be this.\nIf you start out with all of the words just in some random position in space\nand train it on some training data, what the word to vec model will do\nis start to learn what words appear in similar contexts.\nAnd it will move these vectors around in such a way\nthat hopefully words with similar meanings, breakfast, lunch, and dinner,\nbook, memoir, novel, will hopefully appear to be near to each other\nas vectors as well.\nSo let's now take a look at what word to vec\nmight look like in practice when implemented in code.\nWhat I have here inside of words.txt is a pre-trained model\nwhere each of these words has some vector representation\ntrained by word to vec.\nEach of these words has some sequence of values representing its meaning,\nhopefully in such a way that similar words are represented by similar vectors.\nI also have this file vectors.py, which is going to open up the words\nand form them into a dictionary.\nAnd we also define some useful functions like distance\nto get the distance between two word vectors and closest words\nto find which words are nearby in terms of having close vectors to each other.\nAnd so let's give this a try.\nWe'll go ahead and open a Python interpreter.\nAnd I'm going to import these vectors.\nAnd we might say, all right, what is the vector representation\nof the word book?\nAnd we get this big long vector that represents the word book\nas a sequence of values.\nAnd this sequence of values by itself is not all that meaningful.\nBut it is meaningful in the context of comparing it\nto other vectors for other words.\nSo we could use this distance function, which\nis going to get us the distance between two word vectors.\nAnd we might say, what is the distance between the vector\nrepresentation for the word book and the vector representation\nfor the word novel?\nAnd we see that it's 0.34.\nYou can kind of interpret 0 as being really close together and 1\nbeing very far apart.\nAnd so now, what is the distance between book and, let's say, breakfast?\nWell, book and breakfast are more different from each other\nthan book and novel are.\nSo I would hopefully expect the distance to be larger.\nAnd in fact, it is 0.64 approximately.\nThese two words are further away from each other.\nAnd what about now the distance between, let's say, lunch and breakfast?\nWell, that's about 0.2.\nThose are even closer together.\nThey have a meaning that is closer to each other.\nAnother interesting thing we might do is calculate the closest words.\nWe might say, what are the closest words, according to Word2Vec,\nto the word book?\nAnd let's say, let's get the 10 closest words.\nWhat are the 10 closest vectors to the vector representation\nfor the word book?\nAnd when we perform that analysis, we get this list of words.\nThe closest one is book itself, but we also have books plural,\nand then essay, memoir, essays, novella, anthology, and so on.\nAll of these words mean something similar to the word book,\naccording to Word2Vec, at least, because they\nhave a similar vector representation.\nSo it seems like we've done a pretty good job of trying\nto capture this kind of vector representation of word meaning.\nOne other interesting side effect of Word2Vec\nis that it's also able to capture something about the relationships\nbetween words as well.\nLet's take a look at an example.\nHere, for instance, are two words, man and king.\nAnd these are each represented by Word2Vec as vectors.\nSo what might happen if I subtracted one from the other,\ncalculated the value king minus man?\nWell, that will be the vector that will take us from man to king,\nsomehow represent this relationship between the vector representation\nof the word man and the vector representation of the word king.\nAnd that's what this value, king minus man, represents.\nSo what would happen if I took the vector representation of the word\nwoman and added that same value, king minus man, to it?\nWhat would we get as the closest word to that, for example?\nWell, we could try it.\nLet's go ahead and go back to our Python interpreter and give this a try.\nI could say, what is the closest word to the vector representation\nof the word king minus the representation of the word man\nplus the representation of the word woman?\nAnd we see that the closest word is the word queen.\nWe've somehow been able to capture the relationship between king and man.\nAnd then when we apply it to the word woman,\nwe get, as the result, the word queen.\nSo Word2Vec has been able to capture not just the words\nand how they're similar to each other, but also something\nabout the relationships between words and how those words are connected\nto each other.\nSo now that we have this vector representation of words,\nwhat can we now do with it?\nNow we can represent words as numbers.\nAnd so we might try to pass those words as input\nto, say, a neural network.\nNeural networks we've seen are very powerful tools\nfor identifying patterns and making predictions.\nRecall that a neural network you can think of as all of these units.\nBut really what the neural network is doing\nis taking some input, passing it into the network,\nand then producing some output.\nAnd by providing the neural network with training data,\nwe're able to update the weights inside of the network\nso that the neural network can do a more accurate job of translating\nthose inputs into those outputs.\nAnd now that we can represent words as numbers that\ncould be the input or output, you could imagine passing a word in\nas input to a neural network and getting a word as output.\nAnd so when might that be useful?\nOne common use for neural networks is in machine translation,\nwhen we want to translate text from one language into another,\nsay translate English into French by passing English into the neural\nnetwork and getting some French output.\nYou might imagine, for instance, that we could take the English word for lamp,\npass it into the neural network, get the French word for lamp as output.\nBut in practice, when we're translating text from one language to another,\nwe're usually not just interested in translating\na single word from one language to another, but a sequence,\nsay a sentence or a paragraph of words.\nHere, for example, is another paragraph, again taken\nfrom Sherlock Holmes, written in English.\nAnd what I might want to do is take that entire sentence,\npass it into the neural network, and get as output a French translation\nof the same sentence.\nBut recall that a neural network's input and output\nneeds to be of some fixed size.\nAnd a sentence is not a fixed size.\nIt's variable.\nYou might have shorter sentences, and you might have longer sentences.\nSo somehow, we need to solve the problem of translating\na sequence into another sequence by means of a neural network.\nAnd that's going to be true not only for machine translation,\nbut also for other problems, problems like question answering.\nIf I want to pass as input a question, something\nlike what is the capital of Massachusetts,\nfeed that as input into the neural network,\nI would hope that what I would get as output\nis a sentence like the capital is Boston, again,\ntranslating some sequence into some other sequence.\nAnd if you've ever had a conversation with an AI chatbot,\nor have ever asked your phone a question,\nit needs to do something like this.\nIt needs to understand the sequence of words that you, the human,\nprovided as input.\nAnd then the computer needs to generate some sequence of words as output.\nSo how can we do this?\nWell, one tool that we can use is the recurrent neural network, which\nwe took a look at last time, which is a way for us\nto provide a sequence of values to a neural network\nby running the neural network multiple times.\nAnd each time we run the neural network, what we're going to do\nis we're going to keep track of some hidden state.\nAnd that hidden state is going to be passed\nfrom one run of the neural network to the next run of the neural network,\nkeeping track of all of the relevant information.\nAnd so let's take a look at how we can apply that\nto something like this.\nAnd in particular, we're going to look at an architecture known\nas an encoder-decoder architecture, where\nwe're going to encode this question into some kind of hidden state,\nand then use a decoder to decode that hidden state into the output\nthat we're interested in.\nSo what's that going to look like?\nWe'll start with the first word, the word what.\nThat goes into our neural network, and it's\ngoing to produce some hidden state.\nThis is some information about the word what that our neural network is\ngoing to need to keep track of.\nThen when the second word comes along, we're\ngoing to feed it into that same encoder neural network,\nbut it's going to get as input that hidden state as well.\nSo we pass in the second word.\nWe also get the information about the hidden state,\nand that's going to continue for the other words in the input.\nThis is going to produce a new hidden state.\nAnd so then when we get to the third word, the, that goes into the encoder.\nIt also gets access to the hidden state, and then it\nproduces a new hidden state that gets passed into the next run\nwhen we use the word capital.\nAnd the same thing is going to repeat for the other words\nthat appear in the input.\nSo of Massachusetts, that produces one final piece of hidden state.\nNow somehow, we need to signal the fact that we're done.\nThere's nothing left in the input.\nAnd we typically do this by passing some kind of special token,\nsay an end token, into the neural network.\nAnd now the decoding process is going to start.\nWe're going to generate the word the.\nBut in addition to generating the word the,\nthis decoder network is also going to generate some kind of hidden state.\nAnd so what happens the next time?\nWell, to generate the next word, it might\nbe helpful to know what the first word was.\nSo we might pass the first word the back into the decoder network.\nIt's going to get as input this hidden state,\nand it's going to generate the next word capital.\nAnd that's also going to generate some hidden state.\nAnd we'll repeat that, passing capital into the network\nto generate the third word is, and then one more time\nin order to get the fourth word Boston.\nAnd at that point, we're done.\nBut how do we know we're done?\nUsually, we'll do this one more time, pass Boston into the decoder network,\nand get an output some end token to indicate that that is the end of our input.\nAnd so this then is how we could use a recurrent neural network\nto take some input, encode it into some hidden state,\nand then use that hidden state to decode it into the output we're interested in.\nTo visualize it in a slightly different way, we have some input sequence.\nThis is just some sequence of words.\nThat input sequence goes into the encoder, which in this case\nis a recurrent neural network generating these hidden states along the way\nuntil we generate some final hidden state, at which point\nwe start the decoding process.\nAgain, using a recurrent neural network, that's\ngoing to generate the output sequence as well.\nSo we've got the encoder, which is encoding the information\nabout the input sequence into this hidden state,\nand then the decoder, which takes that hidden state\nand uses it in order to generate the output sequence.\nBut there are some problems.\nAnd for many years, this was the state of the art.\nThe recurrent neural network and variance on this approach\nwere some of the best ways we knew in order\nto perform tasks in natural language processing.\nBut there are some problems that we might want to try to deal with\nand that have been dealt with over the years\nto try and improve upon this kind of model.\nAnd one problem you might notice happens in this encoder stage.\nWe've taken this input sequence, the sequence of words,\nand encoded it all into this final piece of hidden state.\nAnd that final piece of hidden state needs\nto contain all of the information from the input sequence\nthat we need in order to generate the output sequence.\nAnd while that's possible, it becomes increasingly difficult\nas the sequence gets larger and larger.\nFor larger and larger input sequences, it's\ngoing to become more and more difficult to store\nall of the information we need about the input\ninside this single hidden state piece of context.\nThat's a lot of information to pack into just a single value.\nIt might be useful for us, when generating output,\nto not just refer to this one value, but to all\nof the previous hidden values that have been generated by the encoder.\nAnd so that might be useful, but how could we do that?\nWe've got a lot of different values.\nWe need to combine them somehow.\nSo you could imagine adding them together,\ntaking the average of them, for example.\nBut doing that would assume that all of these pieces of hidden state\nare equally important.\nBut that's not necessarily true either.\nSome of these pieces of hidden state are going\nto be more important than others, depending\non what word they most closely correspond to.\nThis piece of hidden state very closely corresponds\nto the first word of the input sequence.\nThis one very closely corresponds to the second word of the input sequence,\nfor example.\nAnd some of those are going to be more important than others.\nTo make matters more complicated, depending\non which word of the output sequence we're generating,\ndifferent input words might be more or less important.\nAnd so what we really want is some way to decide for ourselves\nwhich of the input values are worth paying attention to,\nat what point in time.\nAnd this is the key idea behind a mechanism known as attention.\nAttention is all about letting us decide which values\nare important to pay attention to, when generating, in this case,\nthe next word in our sequence.\nSo let's take a look at an example of that.\nHere's a sentence.\nWhat is the capital of Massachusetts?\nSame sentence as before.\nAnd let's imagine that we were trying to answer that question\nby generating tokens of output.\nSo what would the output look like?\nWell, it's going to look like something like the capital is.\nAnd let's say we're now trying to generate this last word here.\nWhat is that last word?\nHow is the computer going to figure it out?\nWell, what it's going to need to do is decide\nwhich values it's going to pay attention to.\nAnd so the attention mechanism will allow\nus to calculate some attention scores for each word,\nsome value corresponding to each word, determining how relevant\nis it for us to pay attention to that word right now?\nAnd in this case, when generating the fourth word of the output sequence,\nthe most important words to pay attention to\nmight be capital and Massachusetts, for example.\nThat those words are going to be particularly relevant.\nAnd there are a number of different mechanisms\nthat have been used in order to calculate these attention scores.\nIt could be something as simple as a dot product\nto see how similar two vectors are, or we\ncould train an entire neural network to calculate these attention scores.\nBut the key idea is that during the training process for our neural network,\nwe're going to learn how to calculate these attention scores.\nOur model is going to learn what is important to pay attention\nto in order to decide what the next word should be.\nSo the result of all of this, calculating these attention scores,\nis that we can calculate some value, some value for each input word,\ndetermining how important is it for us to pay attention\nto that particular value.\nAnd recall that each of these input words\nis also associated with one of these hidden state context vectors,\ncapturing information about the sentence up to that point,\nbut primarily focused on that word in particular.\nAnd so what we can now do is if we have all of these vectors\nand we have values representing how important is it for us\nto pay attention to those particular vectors,\nis we can take a weighted average.\nWe can take all of these vectors, multiply them by their attention scores,\nand add them up to get some new vector value, which\nis going to represent the context from the input,\nbut specifically paying attention to the words\nthat we think are most important.\nAnd once we've done that, that context vector\ncan be fed into our decoder in order to say\nthat the word should be, in this case, Boston.\nSo attention is this very powerful tool that\nallows any word when we're trying to decode it\nto decide which words from the input should we pay attention to in order\nto determine what's important for generating the next word of the output.\nAnd one of the first places this was really used\nwas in the field of machine translation.\nHere's an example of a diagram from the paper\nthat introduced this idea, which was focused\non trying to translate English sentences into French sentences.\nSo we have an input English sentence up along the top,\nand then along the left side, the output French equivalent\nof that same sentence.\nAnd what you see in all of these squares are the attention scores\nvisualized, where a lighter square indicates a higher attention score.\nAnd what you'll notice is that there's a strong correspondence\nbetween the French word and the equivalent English word,\nthat the French word for agreement is really\npaying attention to the English word for agreement\nin order to decide what French word should be generated at that point\nin time.\nAnd sometimes you might pay attention to multiple words\nif you look at the French word for economic.\nThat's primarily paying attention to the English word for economic,\nbut also paying attention to the English word for European in this case too.\nAnd so attention scores are very easy to visualize\nto get a sense for what is our machine learning model really\npaying attention to, what information is it using in order\nto determine what's important and what's not in order\nto determine what the ultimate output token should be.\nAnd so when we combine the attention mechanism\nwith a recurrent neural network, we can get very powerful and useful results\nwhere we're able to generate an output sequence by paying attention\nto the input sequence too.\nBut there are other problems with this approach\nof using a recurrent neural network as well.\nIn particular, notice that every run of the neural network\ndepends on the output of the previous step.\nAnd that was important for getting a sense\nfor the sequence of words and the ordering of those particular words.\nBut we can't run this unit of the neural network\nuntil after we've calculated the hidden state from the run before it\nfrom the previous input token.\nAnd what that means is that it's very difficult to parallelize this process.\nThat as the input sequence get longer and longer,\nwe might want to use parallelism to try and speed up\nthis process of training the neural network\nand making sense of all of this language data.\nBut it's difficult to do that.\nAnd it's slow to do that with a recurrent neural network\nbecause all of it needs to be performed in sequence.\nAnd that's become an increasing challenge as we've\nstarted to get larger and larger language models.\nThe more language data that we have available to us\nto use to train our machine learning models,\nthe more accurate it can be, the better representation of language\nit can have, the better understanding it can have,\nand the better results that we can see.\nAnd so we've seen this growth of large language models\nthat are using larger and larger data sets.\nBut as a result, they take longer and longer to train.\nAnd so this problem that recurrent neural networks\nare not easy to parallelize has become an increasing problem.\nAnd as a result of that, that was one of the main motivations\nfor a different architecture, for thinking about how\nto deal with natural language.\nAnd that's known as the transformer architecture.\nAnd this has been a significant milestone in the world of natural language\nprocessing for really increasing how well we can perform\nthese kinds of natural language processing tasks,\nas well as how quickly we can train a machine learning model to be\nable to produce effective results.\nThere are a number of different types of transformers\nin terms of how they work.\nBut what we're going to take a look at here\nis the basic architecture for how one might work with a transformer\nto get a sense for what's involved and what we're doing.\nSo let's start with the model we were looking at before,\nspecifically at this encoder part of our encoder-decoder architecture,\nwhere we used a recurrent neural network to take this input\nsequence and capture all of this information about the hidden state\nand the information we need to know about that input sequence.\nRight now, it all needs to happen in this linear progression.\nBut what the transformer is going to allow us to do\nis process each of the words independently in a way that's\neasy to parallelize, rather than have each word wait for some other word.\nEach word is going to go through this same neural network\nand produce some kind of encoded representation\nof that particular input word.\nAnd all of this is going to happen in parallel.\nNow, it's happening for all of the words at once,\nbut we're really just going to focus on what's\nhappening for one word to make it clear.\nBut know that whatever you're seeing happen for this one word\nis going to happen for all of the other input words, too.\nSo what's going on here?\nWell, we start with some input word.\nThat input word goes into the neural network.\nAnd the output is hopefully some encoded representation of the input word,\nthe information we need to know about the input word that's\ngoing to be relevant to us as we're generating the output.\nAnd because we're doing this each word independently,\nit's easy to parallelize.\nWe don't have to wait for the previous word\nbefore we run this word through the neural network.\nBut what did we lose in this process by trying to parallelize this whole thing?\nWell, we've lost all notion of word ordering.\nThe order of words is important.\nThe sentence, Sherlock Holmes gave the book to Watson,\nhas a different meaning than Watson gave the book to Sherlock Holmes.\nAnd so we want to keep track of that information about word position.\nIn the recurrent neural network, that happened for us automatically\nbecause we could run each word one at a time through the neural network,\nget the hidden state, pass it on to the next run of the neural network.\nBut that's not the case here with the transformer,\nwhere each word is being processed independent of all of the other ones.\nSo what are we going to do to try to solve that problem?\nOne thing we can do is add some kind of positional encoding to the input word.\nThe positional encoding is some vector that\nrepresents the position of the word in the sentence.\nThis is the first word, the second word, the third word, and so forth.\nWe're going to add that to the input word.\nAnd the result of that is going to be a vector\nthat captures multiple pieces of information.\nIt captures the input word itself as well as where in the sentence it appears.\nThe result of that is we can pass the output of that addition,\nthe addition of the input word and the positional encoding\ninto the neural network.\nThat way, the neural network knows the word and where\nit appears in the sentence and can use both of those pieces of information\nto determine how best to represent the meaning of that word\nin the encoded representation at the end of it.\nIn addition to what we have here, in addition\nto the positional encoding and this feed forward neural network,\nwe're also going to add one additional component, which\nis going to be a self-attention step.\nThis is going to be attention where we're paying attention\nto the other input words.\nBecause the meaning or interpretation of an input word\nmight vary depending on the other words in the input as well.\nAnd so we're going to allow each word in the input\nto decide what other words in the input it should pay attention\nto in order to decide on its encoded representation.\nAnd that's going to allow us to get a better encoded representation\nfor each word because words are defined by their context,\nby the words around them and how they're used in that particular context.\nThis kind of self-attention is so valuable, in fact,\nthat oftentimes the transformer will use multiple different self-attention\nlayers at the same time to allow for this model\nto be able to pay attention to multiple facets of the input at the same time.\nAnd we call this multi-headed attention, where each attention head can pay\nattention to something different.\nAnd as a result, this network can learn to pay attention\nto many different parts of the input for this input word all at the same time.\nAnd in the spirit of deep learning, these two steps,\nthis multi-headed self-attention layer and this neural network layer,\nthat itself can be repeated multiple times, too,\nin order to get a deeper representation, in order\nto learn deeper patterns within the input text\nand ultimately get a better representation of language\nin order to get useful encoded representations of all of the input\nwords.\nAnd so this is the process that a transformer might\nuse in order to take an input word and get it its encoded representation.\nAnd the key idea is to really rely on this attention step\nin order to get information that's useful in order\nto determine how to encode that word.\nAnd that process is going to repeat for all of the input words that\nare in the input sequence.\nWe're going to take all of the input words,\nencode them with some kind of positional encoding,\nfeed those into these self-attention and feed-forward neural networks\nin order to ultimately get these encoded representations of the words.\nThat's the result of the encoder.\nWe get all of these encoded representations\nthat will be useful to us when it comes time\nthen to try to decode all of this information\ninto the output sequence we're interested in.\nAnd again, this might take place in the context of machine translation,\nwhere the output is going to be the same sentence in a different language,\nor it might be an answer to a question in the case of an AI chatbot,\nfor example.\nAnd so now let's take a look at how that decoder is going to work.\nUltimately, it's going to have a very similar structure.\nAny time we're trying to generate the next output word,\nwe need to know what the previous output word is,\nas well as its positional encoding.\nWhere in the output sequence are we?\nAnd we're going to have these same steps, self-attention,\nbecause we might want an output word to be\nable to pay attention to other words in that same output,\nas well as a neural network.\nAnd that might itself repeat multiple times.\nBut in this decoder, we're going to add one additional step.\nWe're going to add an additional attention step, where\ninstead of self-attention, where the output word is going\nto pay attention to other output words, in this step,\nwe're going to allow the output word to pay attention\nto the encoded representations.\nSo recall that the encoder is taking all of the input words\nand transforming them into these encoded representations\nof all of the input words.\nBut it's going to be important for us to be able to decide which\nof those encoded representations we want to pay attention\nto when generating any particular token in the output sequence.\nAnd that's what this additional attention step is going to allow us to do.\nIt's saying that every time we're generating a word of the output,\nwe can pay attention to the other words in the output,\nbecause we might want to know, what are the words we've generated previously?\nAnd we want to pay attention to some of them\nto decide what word is going to be next in the sequence.\nBut we also care about paying attention to the input words, too.\nAnd we want the ability to decide which of these encoded representations\nof the input words are going to be relevant in order\nfor us to generate the next step.\nAnd so these two pieces combine together.\nWe have this encoder that takes all of the input words\nand produces this encoded representation.\nAnd we have this decoder that is able to take the previous output word,\npay attention to that encoded input, and then generate the next output word.\nAnd this is one of the possible architectures\nwe could use for a transformer, with the key idea being\nthese attention steps that allow words to pay attention to each other.\nDuring the training process here, we can now much more easily parallelize this,\nbecause we don't have to wait for all of the words to happen in sequence.\nAnd we can learn how we should perform these attention steps.\nThe model is able to learn what is important to pay attention to,\nwhat things do I need to pay attention to,\nin order to be more accurate at predicting what the output word is.\nAnd this has proved to be a tremendously effective model\nfor conversational AI agents, for building machine translation systems.\nAnd there have been many variants proposed on this model, too.\nSome transformers only use an encoder.\nSome only use a decoder.\nSome use some other combination of these different particular features.\nBut the key ideas ultimately remain the same,\nthis real focus on trying to pay attention to what is most important.\nAnd the world of natural language processing\nis fast growing and fast evolving.\nYear after year, we keep coming up with new models\nthat allow us to do an even better job of performing\nthese natural language related tasks, all on the surface\nof solving the tricky problem, which is our own natural language.\nWe've seen how the syntax and semantics of our language is ambiguous,\nand it introduces all of these new challenges\nthat we need to think about, if we're going\nto be able to design AI agents that are able to work with language\neffectively.\nSo as we think about where we've been in this class,\nall of the different types of artificial intelligence we've considered,\nwe've looked at artificial intelligence in a wide variety\nof different forms now.\nWe started by taking a look at search problems,\nwhere we looked at how AI can search for solutions, play games,\nand find the optimal decision to make.\nWe talked about knowledge, how AI can represent information that it knows\nand use that information to generate new knowledge as well.\nThen we looked at what AI can do when it's less certain,\nwhen it doesn't know things for sure, and we\nhave to represent things in terms of probability.\nWe then took a look at optimization problems.\nWe saw how a lot of problems in AI can be boiled down\nto trying to maximize or minimize some function.\nAnd we looked at strategies that AI can use\nin order to do that kind of maximizing and minimizing.\nWe then looked at the world of machine learning,\nlearning from data in order to figure out some patterns\nand identify how to perform a task by looking at the training data\nthat we have available to it.\nAnd one of the most powerful tools there was the neural network,\nthe sequence of units whose weights can be trained in order\nto allow us to really effectively go from input to output\nand predict how to get there by learning these underlying patterns.\nAnd then today, we took a look at language itself,\ntrying to understand how can we train the computer to be\nable to understand our natural language, to be\nable to understand syntax and semantics, make sense of and generate\nnatural language, which introduces a number of interesting problems too.\nAnd we've really just scratched the surface of artificial intelligence.\nThere is so much interesting research and interesting new techniques\nand algorithms and ideas being introduced\nto try to solve these types of problems.\nSo I hope you enjoyed this exploration into the world\nof artificial intelligence.\nA huge thanks to all of the course's teaching staff and production team\nfor making the class possible.\nThis was an introduction to artificial intelligence with Python.\n",
  "words": [
    "course",
    "harvard",
    "university",
    "explores",
    "concepts",
    "algorithms",
    "foundation",
    "modern",
    "artificial",
    "intelligence",
    "diving",
    "ideas",
    "give",
    "rise",
    "technologies",
    "like",
    "engines",
    "handwriting",
    "recognition",
    "machine",
    "translation",
    "gain",
    "exposure",
    "theory",
    "behind",
    "graph",
    "search",
    "algorithms",
    "classification",
    "optimization",
    "reinforcement",
    "learning",
    "topics",
    "artificial",
    "intelligence",
    "machine",
    "learning",
    "brian",
    "yu",
    "teaches",
    "course",
    "hello",
    "world",
    "cs50",
    "introduction",
    "artificial",
    "intelligence",
    "python",
    "cs50",
    "brian",
    "yu",
    "course",
    "picks",
    "cs50",
    "leaves",
    "explores",
    "concepts",
    "algorithms",
    "foundation",
    "modern",
    "ai",
    "start",
    "look",
    "ai",
    "search",
    "solutions",
    "problems",
    "whether",
    "problems",
    "learning",
    "play",
    "game",
    "trying",
    "find",
    "driving",
    "directions",
    "destination",
    "look",
    "ai",
    "represent",
    "information",
    "knowledge",
    "ai",
    "certain",
    "also",
    "information",
    "events",
    "ai",
    "might",
    "uncertain",
    "learning",
    "represent",
    "information",
    "importantly",
    "use",
    "information",
    "draw",
    "inferences",
    "new",
    "conclusions",
    "well",
    "explore",
    "ai",
    "solve",
    "various",
    "types",
    "optimization",
    "problems",
    "trying",
    "maximize",
    "profits",
    "minimize",
    "costs",
    "satisfy",
    "constraints",
    "turning",
    "attention",
    "field",
    "machine",
    "learning",
    "wo",
    "tell",
    "ai",
    "exactly",
    "solve",
    "problem",
    "instead",
    "give",
    "ai",
    "access",
    "data",
    "experiences",
    "ai",
    "learn",
    "perform",
    "tasks",
    "particular",
    "look",
    "neural",
    "networks",
    "one",
    "popular",
    "tools",
    "modern",
    "machine",
    "learning",
    "inspired",
    "way",
    "human",
    "brains",
    "learn",
    "reason",
    "well",
    "finally",
    "taking",
    "look",
    "world",
    "natural",
    "language",
    "processing",
    "us",
    "humans",
    "learning",
    "learn",
    "artificial",
    "intelligence",
    "able",
    "speak",
    "also",
    "ai",
    "learning",
    "understand",
    "interpret",
    "human",
    "language",
    "well",
    "explore",
    "ideas",
    "algorithms",
    "along",
    "way",
    "give",
    "opportunity",
    "build",
    "ai",
    "programs",
    "implement",
    "cs50",
    "right",
    "welcome",
    "everyone",
    "introduction",
    "artificial",
    "intelligence",
    "python",
    "name",
    "brian",
    "yu",
    "class",
    "explore",
    "ideas",
    "techniques",
    "algorithms",
    "foundation",
    "artificial",
    "intelligence",
    "artificial",
    "intelligence",
    "covers",
    "wide",
    "variety",
    "types",
    "techniques",
    "anytime",
    "see",
    "computer",
    "something",
    "appears",
    "intelligent",
    "rational",
    "way",
    "like",
    "recognizing",
    "someone",
    "face",
    "photo",
    "able",
    "play",
    "game",
    "better",
    "people",
    "able",
    "understand",
    "human",
    "language",
    "talk",
    "phones",
    "understand",
    "mean",
    "able",
    "respond",
    "back",
    "us",
    "examples",
    "ai",
    "artificial",
    "intelligence",
    "class",
    "explore",
    "ideas",
    "make",
    "ai",
    "possible",
    "begin",
    "conversations",
    "search",
    "problem",
    "ai",
    "would",
    "like",
    "ai",
    "able",
    "search",
    "solutions",
    "kind",
    "problem",
    "matter",
    "problem",
    "might",
    "whether",
    "trying",
    "get",
    "driving",
    "directions",
    "point",
    "point",
    "b",
    "trying",
    "figure",
    "play",
    "game",
    "given",
    "game",
    "example",
    "figuring",
    "move",
    "ought",
    "make",
    "take",
    "look",
    "knowledge",
    "ideally",
    "want",
    "ai",
    "able",
    "know",
    "information",
    "able",
    "represent",
    "information",
    "importantly",
    "able",
    "draw",
    "inferences",
    "information",
    "able",
    "use",
    "information",
    "knows",
    "draw",
    "additional",
    "conclusions",
    "talk",
    "ai",
    "programmed",
    "order",
    "explore",
    "topic",
    "uncertainty",
    "talking",
    "ideas",
    "happens",
    "computer",
    "sure",
    "fact",
    "maybe",
    "sure",
    "certain",
    "probability",
    "talk",
    "ideas",
    "behind",
    "probability",
    "computers",
    "begin",
    "deal",
    "uncertain",
    "events",
    "order",
    "little",
    "bit",
    "intelligent",
    "sense",
    "well",
    "turn",
    "attention",
    "optimization",
    "problems",
    "computer",
    "trying",
    "optimize",
    "sort",
    "goal",
    "especially",
    "situation",
    "might",
    "multiple",
    "ways",
    "computer",
    "might",
    "solve",
    "problem",
    "looking",
    "better",
    "way",
    "potentially",
    "best",
    "way",
    "possible",
    "take",
    "look",
    "machine",
    "learning",
    "learning",
    "generally",
    "looking",
    "access",
    "data",
    "computers",
    "programmed",
    "quite",
    "intelligent",
    "learning",
    "data",
    "learning",
    "experience",
    "able",
    "perform",
    "task",
    "better",
    "better",
    "based",
    "greater",
    "access",
    "data",
    "email",
    "example",
    "email",
    "inbox",
    "somehow",
    "knows",
    "emails",
    "good",
    "emails",
    "emails",
    "spam",
    "examples",
    "computers",
    "able",
    "learn",
    "past",
    "experiences",
    "past",
    "data",
    "take",
    "look",
    "computers",
    "able",
    "draw",
    "inspiration",
    "human",
    "intelligence",
    "looking",
    "structure",
    "human",
    "brain",
    "neural",
    "networks",
    "computer",
    "analog",
    "sort",
    "idea",
    "taking",
    "advantage",
    "certain",
    "type",
    "structure",
    "computer",
    "program",
    "write",
    "neural",
    "networks",
    "able",
    "perform",
    "tasks",
    "effectively",
    "finally",
    "turn",
    "attention",
    "language",
    "programming",
    "languages",
    "human",
    "languages",
    "speak",
    "every",
    "day",
    "taking",
    "look",
    "challenges",
    "come",
    "computer",
    "tries",
    "understand",
    "natural",
    "language",
    "natural",
    "language",
    "processing",
    "occurs",
    "modern",
    "artificial",
    "intelligence",
    "actually",
    "work",
    "today",
    "begin",
    "conversation",
    "search",
    "problem",
    "trying",
    "figure",
    "sort",
    "situation",
    "computer",
    "sort",
    "environment",
    "agent",
    "speak",
    "would",
    "like",
    "agent",
    "able",
    "somehow",
    "look",
    "solution",
    "problem",
    "problems",
    "come",
    "number",
    "different",
    "types",
    "formats",
    "one",
    "example",
    "instance",
    "might",
    "something",
    "like",
    "classic",
    "15",
    "puzzle",
    "sliding",
    "tiles",
    "might",
    "seen",
    "trying",
    "slide",
    "tiles",
    "order",
    "make",
    "sure",
    "numbers",
    "line",
    "order",
    "example",
    "might",
    "call",
    "search",
    "problem",
    "15",
    "puzzle",
    "begins",
    "initially",
    "mixed",
    "state",
    "need",
    "way",
    "finding",
    "moves",
    "make",
    "order",
    "return",
    "puzzle",
    "solved",
    "state",
    "similar",
    "problems",
    "frame",
    "ways",
    "trying",
    "find",
    "way",
    "maze",
    "example",
    "another",
    "example",
    "search",
    "problem",
    "begin",
    "one",
    "place",
    "goal",
    "trying",
    "get",
    "need",
    "figure",
    "correct",
    "sequence",
    "actions",
    "take",
    "initial",
    "state",
    "goal",
    "little",
    "bit",
    "abstract",
    "time",
    "talk",
    "maze",
    "solving",
    "class",
    "translate",
    "something",
    "little",
    "real",
    "world",
    "something",
    "like",
    "driving",
    "directions",
    "ever",
    "wonder",
    "google",
    "maps",
    "able",
    "figure",
    "best",
    "way",
    "get",
    "point",
    "point",
    "b",
    "turns",
    "make",
    "time",
    "depending",
    "traffic",
    "example",
    "often",
    "sort",
    "search",
    "algorithm",
    "ai",
    "trying",
    "get",
    "initial",
    "position",
    "sort",
    "goal",
    "taking",
    "sequence",
    "actions",
    "start",
    "conversations",
    "today",
    "thinking",
    "types",
    "search",
    "problems",
    "goes",
    "solving",
    "search",
    "problem",
    "like",
    "order",
    "ai",
    "able",
    "find",
    "good",
    "solution",
    "order",
    "though",
    "going",
    "need",
    "introduce",
    "little",
    "bit",
    "terminology",
    "already",
    "used",
    "first",
    "term",
    "need",
    "think",
    "agent",
    "agent",
    "entity",
    "perceives",
    "environment",
    "somehow",
    "able",
    "perceive",
    "things",
    "around",
    "act",
    "environment",
    "way",
    "case",
    "driving",
    "directions",
    "agent",
    "might",
    "representation",
    "car",
    "trying",
    "figure",
    "actions",
    "take",
    "order",
    "arrive",
    "destination",
    "case",
    "15",
    "puzzle",
    "sliding",
    "tiles",
    "agent",
    "might",
    "ai",
    "person",
    "trying",
    "solve",
    "puzzle",
    "try",
    "figure",
    "tiles",
    "move",
    "order",
    "get",
    "solution",
    "next",
    "introduce",
    "idea",
    "state",
    "state",
    "configuration",
    "agent",
    "environment",
    "15",
    "puzzle",
    "example",
    "state",
    "might",
    "one",
    "three",
    "example",
    "state",
    "configuration",
    "tiles",
    "states",
    "different",
    "going",
    "require",
    "slightly",
    "different",
    "solution",
    "different",
    "sequence",
    "actions",
    "needed",
    "one",
    "order",
    "get",
    "initial",
    "state",
    "goal",
    "trying",
    "get",
    "initial",
    "state",
    "initial",
    "state",
    "state",
    "agent",
    "begins",
    "one",
    "state",
    "going",
    "start",
    "going",
    "starting",
    "point",
    "search",
    "algorithm",
    "speak",
    "going",
    "begin",
    "initial",
    "state",
    "start",
    "reason",
    "think",
    "actions",
    "might",
    "apply",
    "initial",
    "state",
    "order",
    "figure",
    "get",
    "beginning",
    "end",
    "initial",
    "position",
    "whatever",
    "goal",
    "happens",
    "make",
    "way",
    "initial",
    "position",
    "goal",
    "well",
    "ultimately",
    "via",
    "taking",
    "actions",
    "actions",
    "choices",
    "make",
    "given",
    "state",
    "ai",
    "always",
    "going",
    "try",
    "formalize",
    "ideas",
    "little",
    "bit",
    "precisely",
    "could",
    "program",
    "little",
    "bit",
    "mathematically",
    "speak",
    "recurring",
    "theme",
    "precisely",
    "define",
    "actions",
    "function",
    "going",
    "effectively",
    "define",
    "function",
    "called",
    "actions",
    "takes",
    "input",
    "going",
    "state",
    "exists",
    "inside",
    "environment",
    "actions",
    "going",
    "take",
    "state",
    "input",
    "return",
    "output",
    "set",
    "actions",
    "executed",
    "state",
    "possible",
    "actions",
    "valid",
    "certain",
    "states",
    "states",
    "see",
    "examples",
    "soon",
    "case",
    "15",
    "puzzle",
    "example",
    "generally",
    "going",
    "four",
    "possible",
    "actions",
    "time",
    "slide",
    "tile",
    "right",
    "slide",
    "tile",
    "left",
    "slide",
    "tile",
    "slide",
    "tile",
    "example",
    "going",
    "actions",
    "available",
    "us",
    "somehow",
    "ai",
    "program",
    "needs",
    "encoding",
    "state",
    "often",
    "going",
    "numerical",
    "format",
    "encoding",
    "actions",
    "also",
    "needs",
    "encoding",
    "relationship",
    "things",
    "states",
    "actions",
    "relate",
    "one",
    "another",
    "order",
    "introduce",
    "ai",
    "transition",
    "model",
    "description",
    "state",
    "get",
    "perform",
    "available",
    "action",
    "state",
    "little",
    "bit",
    "precise",
    "define",
    "transition",
    "model",
    "little",
    "bit",
    "formally",
    "function",
    "function",
    "going",
    "function",
    "called",
    "result",
    "time",
    "takes",
    "two",
    "inputs",
    "input",
    "number",
    "one",
    "state",
    "input",
    "number",
    "two",
    "action",
    "output",
    "function",
    "result",
    "going",
    "give",
    "us",
    "state",
    "get",
    "perform",
    "action",
    "state",
    "let",
    "take",
    "look",
    "example",
    "see",
    "precisely",
    "actually",
    "means",
    "example",
    "state",
    "15",
    "puzzle",
    "example",
    "example",
    "action",
    "sliding",
    "tile",
    "right",
    "happens",
    "pass",
    "inputs",
    "result",
    "function",
    "result",
    "function",
    "takes",
    "board",
    "state",
    "first",
    "input",
    "takes",
    "action",
    "second",
    "input",
    "course",
    "describing",
    "things",
    "visually",
    "see",
    "visually",
    "state",
    "action",
    "computer",
    "might",
    "represent",
    "one",
    "actions",
    "number",
    "represents",
    "action",
    "familiar",
    "enums",
    "allow",
    "enumerate",
    "multiple",
    "possibilities",
    "might",
    "something",
    "like",
    "state",
    "might",
    "represented",
    "array",
    "array",
    "numbers",
    "exist",
    "going",
    "show",
    "visually",
    "see",
    "take",
    "state",
    "action",
    "pass",
    "result",
    "function",
    "output",
    "new",
    "state",
    "state",
    "get",
    "take",
    "tile",
    "slide",
    "right",
    "state",
    "get",
    "result",
    "different",
    "action",
    "different",
    "state",
    "example",
    "pass",
    "result",
    "function",
    "get",
    "different",
    "answer",
    "altogether",
    "result",
    "function",
    "needs",
    "take",
    "care",
    "figuring",
    "take",
    "state",
    "take",
    "action",
    "get",
    "results",
    "going",
    "transition",
    "model",
    "describes",
    "states",
    "actions",
    "related",
    "take",
    "transition",
    "model",
    "think",
    "generally",
    "across",
    "entire",
    "problem",
    "form",
    "might",
    "call",
    "state",
    "space",
    "set",
    "states",
    "get",
    "initial",
    "state",
    "via",
    "sequence",
    "actions",
    "taking",
    "0",
    "1",
    "2",
    "actions",
    "addition",
    "could",
    "draw",
    "diagram",
    "looks",
    "something",
    "like",
    "every",
    "state",
    "represented",
    "game",
    "board",
    "arrows",
    "connect",
    "every",
    "state",
    "every",
    "state",
    "get",
    "state",
    "state",
    "space",
    "much",
    "larger",
    "see",
    "sample",
    "state",
    "space",
    "might",
    "actually",
    "look",
    "like",
    "general",
    "across",
    "many",
    "search",
    "problems",
    "whether",
    "particular",
    "15",
    "puzzle",
    "driving",
    "directions",
    "something",
    "else",
    "state",
    "space",
    "going",
    "look",
    "something",
    "like",
    "individual",
    "states",
    "arrows",
    "connecting",
    "oftentimes",
    "simplicity",
    "simplify",
    "representation",
    "entire",
    "thing",
    "graph",
    "sequence",
    "nodes",
    "edges",
    "connect",
    "nodes",
    "think",
    "abstract",
    "representation",
    "exact",
    "idea",
    "little",
    "circles",
    "nodes",
    "going",
    "represent",
    "one",
    "states",
    "inside",
    "problem",
    "arrows",
    "represent",
    "actions",
    "take",
    "particular",
    "state",
    "taking",
    "us",
    "one",
    "particular",
    "state",
    "another",
    "state",
    "example",
    "right",
    "idea",
    "nodes",
    "representing",
    "states",
    "actions",
    "take",
    "us",
    "one",
    "state",
    "another",
    "transition",
    "model",
    "defines",
    "happens",
    "take",
    "particular",
    "action",
    "next",
    "step",
    "need",
    "figure",
    "know",
    "ai",
    "done",
    "solving",
    "problem",
    "ai",
    "needs",
    "way",
    "know",
    "gets",
    "goal",
    "found",
    "goal",
    "next",
    "thing",
    "need",
    "encode",
    "artificial",
    "intelligence",
    "goal",
    "test",
    "way",
    "determine",
    "whether",
    "given",
    "state",
    "goal",
    "state",
    "case",
    "something",
    "like",
    "driving",
    "directions",
    "might",
    "pretty",
    "easy",
    "state",
    "corresponds",
    "whatever",
    "user",
    "typed",
    "intended",
    "destination",
    "well",
    "know",
    "goal",
    "state",
    "15",
    "puzzle",
    "might",
    "checking",
    "numbers",
    "make",
    "sure",
    "ascending",
    "order",
    "ai",
    "needs",
    "way",
    "encode",
    "whether",
    "state",
    "happen",
    "goal",
    "problems",
    "might",
    "one",
    "goal",
    "like",
    "maze",
    "one",
    "initial",
    "position",
    "one",
    "ending",
    "position",
    "goal",
    "complex",
    "problems",
    "might",
    "imagine",
    "multiple",
    "possible",
    "goals",
    "multiple",
    "ways",
    "solve",
    "problem",
    "might",
    "care",
    "one",
    "computer",
    "finds",
    "long",
    "find",
    "particular",
    "goal",
    "however",
    "sometimes",
    "computer",
    "care",
    "finding",
    "goal",
    "finding",
    "goal",
    "well",
    "one",
    "low",
    "cost",
    "reason",
    "last",
    "piece",
    "terminology",
    "use",
    "define",
    "search",
    "problems",
    "something",
    "called",
    "path",
    "cost",
    "might",
    "imagine",
    "case",
    "driving",
    "directions",
    "would",
    "pretty",
    "annoying",
    "said",
    "wanted",
    "directions",
    "point",
    "point",
    "b",
    "route",
    "google",
    "maps",
    "gave",
    "long",
    "route",
    "lots",
    "detours",
    "unnecessary",
    "took",
    "longer",
    "get",
    "destination",
    "reason",
    "formulating",
    "search",
    "problems",
    "often",
    "give",
    "every",
    "path",
    "sort",
    "numerical",
    "cost",
    "number",
    "telling",
    "us",
    "expensive",
    "take",
    "particular",
    "option",
    "tell",
    "ai",
    "instead",
    "finding",
    "solution",
    "way",
    "getting",
    "initial",
    "state",
    "goal",
    "really",
    "like",
    "find",
    "one",
    "minimizes",
    "path",
    "cost",
    "less",
    "expensive",
    "takes",
    "less",
    "time",
    "minimizes",
    "numerical",
    "value",
    "represent",
    "graphically",
    "take",
    "look",
    "graph",
    "imagine",
    "arrows",
    "actions",
    "take",
    "one",
    "state",
    "another",
    "state",
    "sort",
    "number",
    "associated",
    "number",
    "path",
    "cost",
    "particular",
    "action",
    "costs",
    "particular",
    "action",
    "might",
    "expensive",
    "cost",
    "action",
    "example",
    "although",
    "happen",
    "sorts",
    "problems",
    "problems",
    "simplify",
    "diagram",
    "assume",
    "cost",
    "particular",
    "action",
    "probably",
    "case",
    "something",
    "like",
    "15",
    "puzzle",
    "example",
    "really",
    "make",
    "difference",
    "whether",
    "moving",
    "right",
    "moving",
    "left",
    "thing",
    "matters",
    "total",
    "number",
    "steps",
    "take",
    "get",
    "point",
    "point",
    "steps",
    "equal",
    "cost",
    "assume",
    "constant",
    "cost",
    "like",
    "one",
    "forms",
    "basis",
    "might",
    "consider",
    "search",
    "problem",
    "search",
    "problem",
    "sort",
    "initial",
    "state",
    "place",
    "begin",
    "sort",
    "action",
    "take",
    "multiple",
    "actions",
    "take",
    "given",
    "state",
    "transition",
    "model",
    "way",
    "defining",
    "happens",
    "go",
    "one",
    "state",
    "take",
    "one",
    "action",
    "state",
    "end",
    "result",
    "addition",
    "need",
    "goal",
    "test",
    "know",
    "whether",
    "reached",
    "goal",
    "need",
    "path",
    "cost",
    "function",
    "tells",
    "us",
    "particular",
    "path",
    "following",
    "sequence",
    "actions",
    "expensive",
    "path",
    "cost",
    "terms",
    "money",
    "time",
    "resource",
    "trying",
    "minimize",
    "usage",
    "goal",
    "ultimately",
    "find",
    "solution",
    "solution",
    "case",
    "sequence",
    "actions",
    "take",
    "us",
    "initial",
    "state",
    "goal",
    "state",
    "ideally",
    "like",
    "find",
    "solution",
    "optimal",
    "solution",
    "solution",
    "lowest",
    "path",
    "cost",
    "among",
    "possible",
    "solutions",
    "cases",
    "might",
    "multiple",
    "optimal",
    "solutions",
    "optimal",
    "solution",
    "means",
    "way",
    "could",
    "done",
    "better",
    "terms",
    "finding",
    "solution",
    "defined",
    "problem",
    "need",
    "begin",
    "figure",
    "going",
    "solve",
    "kind",
    "search",
    "problem",
    "order",
    "probably",
    "imagine",
    "computer",
    "going",
    "need",
    "represent",
    "whole",
    "bunch",
    "data",
    "particular",
    "problem",
    "need",
    "represent",
    "data",
    "problem",
    "might",
    "need",
    "considering",
    "multiple",
    "different",
    "options",
    "oftentimes",
    "trying",
    "package",
    "whole",
    "bunch",
    "data",
    "related",
    "state",
    "together",
    "using",
    "data",
    "structure",
    "going",
    "call",
    "node",
    "node",
    "data",
    "structure",
    "going",
    "keep",
    "track",
    "variety",
    "different",
    "values",
    "specifically",
    "case",
    "search",
    "problem",
    "going",
    "keep",
    "track",
    "four",
    "values",
    "particular",
    "every",
    "node",
    "going",
    "keep",
    "track",
    "state",
    "state",
    "currently",
    "every",
    "node",
    "also",
    "going",
    "keep",
    "track",
    "parent",
    "parent",
    "state",
    "us",
    "node",
    "used",
    "order",
    "get",
    "current",
    "state",
    "going",
    "relevant",
    "eventually",
    "reach",
    "goal",
    "node",
    "get",
    "end",
    "want",
    "know",
    "sequence",
    "actions",
    "use",
    "order",
    "get",
    "goal",
    "way",
    "know",
    "looking",
    "parents",
    "keep",
    "track",
    "led",
    "us",
    "goal",
    "led",
    "us",
    "state",
    "led",
    "us",
    "state",
    "forth",
    "backtracking",
    "way",
    "beginning",
    "know",
    "entire",
    "sequence",
    "actions",
    "needed",
    "order",
    "get",
    "beginning",
    "end",
    "node",
    "also",
    "going",
    "keep",
    "track",
    "action",
    "took",
    "order",
    "get",
    "parent",
    "current",
    "state",
    "node",
    "also",
    "going",
    "keep",
    "track",
    "path",
    "cost",
    "words",
    "going",
    "keep",
    "track",
    "number",
    "represents",
    "long",
    "took",
    "get",
    "initial",
    "state",
    "state",
    "currently",
    "happen",
    "see",
    "relevant",
    "start",
    "talk",
    "optimizations",
    "make",
    "terms",
    "search",
    "problems",
    "generally",
    "data",
    "structure",
    "going",
    "use",
    "order",
    "solve",
    "problem",
    "let",
    "talk",
    "approach",
    "might",
    "actually",
    "begin",
    "solve",
    "problem",
    "well",
    "might",
    "imagine",
    "going",
    "going",
    "start",
    "one",
    "particular",
    "state",
    "going",
    "explore",
    "intuition",
    "given",
    "state",
    "multiple",
    "options",
    "could",
    "take",
    "going",
    "explore",
    "options",
    "explore",
    "options",
    "find",
    "options",
    "going",
    "make",
    "available",
    "going",
    "consider",
    "available",
    "options",
    "stored",
    "inside",
    "single",
    "data",
    "structure",
    "call",
    "frontier",
    "frontier",
    "going",
    "represent",
    "things",
    "could",
    "explore",
    "next",
    "yet",
    "explored",
    "visited",
    "approach",
    "going",
    "begin",
    "search",
    "algorithm",
    "starting",
    "frontier",
    "contains",
    "one",
    "state",
    "frontier",
    "going",
    "contain",
    "initial",
    "state",
    "beginning",
    "state",
    "know",
    "state",
    "exists",
    "search",
    "algorithm",
    "effectively",
    "going",
    "follow",
    "loop",
    "going",
    "repeat",
    "process",
    "first",
    "thing",
    "going",
    "frontier",
    "empty",
    "solution",
    "report",
    "way",
    "get",
    "goal",
    "certainly",
    "possible",
    "certain",
    "types",
    "problems",
    "ai",
    "might",
    "try",
    "explore",
    "realize",
    "way",
    "solve",
    "problem",
    "useful",
    "information",
    "humans",
    "know",
    "well",
    "ever",
    "frontier",
    "empty",
    "means",
    "nothing",
    "left",
    "explore",
    "yet",
    "found",
    "solution",
    "solution",
    "nothing",
    "left",
    "explore",
    "otherwise",
    "remove",
    "node",
    "frontier",
    "right",
    "beginning",
    "frontier",
    "contains",
    "one",
    "node",
    "representing",
    "initial",
    "state",
    "time",
    "frontier",
    "might",
    "grow",
    "might",
    "contain",
    "multiple",
    "states",
    "going",
    "remove",
    "single",
    "node",
    "frontier",
    "node",
    "happens",
    "goal",
    "found",
    "solution",
    "remove",
    "node",
    "frontier",
    "ask",
    "goal",
    "applying",
    "goal",
    "test",
    "talked",
    "earlier",
    "asking",
    "destination",
    "asking",
    "numbers",
    "15",
    "puzzle",
    "happen",
    "order",
    "node",
    "contains",
    "goal",
    "found",
    "solution",
    "great",
    "done",
    "otherwise",
    "need",
    "need",
    "expand",
    "node",
    "term",
    "art",
    "artificial",
    "intelligence",
    "expand",
    "node",
    "means",
    "look",
    "neighbors",
    "node",
    "words",
    "consider",
    "possible",
    "actions",
    "could",
    "take",
    "state",
    "node",
    "representing",
    "nodes",
    "could",
    "get",
    "going",
    "take",
    "nodes",
    "next",
    "nodes",
    "get",
    "current",
    "one",
    "looking",
    "add",
    "frontier",
    "repeat",
    "process",
    "high",
    "level",
    "idea",
    "start",
    "frontier",
    "contains",
    "initial",
    "state",
    "constantly",
    "removing",
    "node",
    "frontier",
    "looking",
    "get",
    "next",
    "adding",
    "nodes",
    "frontier",
    "repeating",
    "process",
    "either",
    "remove",
    "node",
    "frontier",
    "contains",
    "goal",
    "meaning",
    "solved",
    "problem",
    "run",
    "situation",
    "frontier",
    "empty",
    "point",
    "left",
    "solution",
    "let",
    "actually",
    "try",
    "take",
    "pseudocode",
    "put",
    "practice",
    "taking",
    "look",
    "example",
    "sample",
    "search",
    "problem",
    "right",
    "sample",
    "graph",
    "connected",
    "b",
    "via",
    "action",
    "b",
    "connected",
    "nodes",
    "c",
    "c",
    "connected",
    "connected",
    "like",
    "ai",
    "find",
    "path",
    "want",
    "get",
    "initial",
    "state",
    "goal",
    "state",
    "going",
    "well",
    "going",
    "start",
    "frontier",
    "contains",
    "initial",
    "state",
    "going",
    "represent",
    "frontier",
    "frontier",
    "initially",
    "contain",
    "initial",
    "state",
    "going",
    "begin",
    "repeat",
    "process",
    "frontier",
    "empty",
    "solution",
    "problem",
    "frontier",
    "empty",
    "remove",
    "node",
    "frontier",
    "one",
    "consider",
    "next",
    "one",
    "node",
    "frontier",
    "go",
    "ahead",
    "remove",
    "frontier",
    "initial",
    "node",
    "node",
    "currently",
    "considering",
    "follow",
    "next",
    "step",
    "ask",
    "node",
    "goal",
    "goal",
    "e",
    "goal",
    "return",
    "solution",
    "instead",
    "go",
    "last",
    "step",
    "expand",
    "node",
    "add",
    "resulting",
    "nodes",
    "frontier",
    "mean",
    "well",
    "means",
    "take",
    "state",
    "consider",
    "could",
    "get",
    "next",
    "could",
    "get",
    "next",
    "get",
    "expand",
    "find",
    "add",
    "b",
    "frontier",
    "b",
    "frontier",
    "repeat",
    "process",
    "say",
    "right",
    "frontier",
    "empty",
    "let",
    "remove",
    "b",
    "frontier",
    "b",
    "node",
    "considering",
    "ask",
    "b",
    "goal",
    "go",
    "ahead",
    "expand",
    "b",
    "add",
    "resulting",
    "nodes",
    "frontier",
    "happens",
    "expand",
    "b",
    "words",
    "nodes",
    "get",
    "b",
    "well",
    "get",
    "c",
    "go",
    "ahead",
    "add",
    "c",
    "frontier",
    "two",
    "nodes",
    "frontier",
    "c",
    "repeat",
    "process",
    "remove",
    "node",
    "frontier",
    "arbitrarily",
    "picking",
    "see",
    "later",
    "choosing",
    "node",
    "remove",
    "frontier",
    "actually",
    "quite",
    "important",
    "part",
    "algorithm",
    "arbitrarily",
    "remove",
    "c",
    "say",
    "goal",
    "add",
    "e",
    "next",
    "one",
    "frontier",
    "let",
    "say",
    "remove",
    "e",
    "frontier",
    "check",
    "currently",
    "looking",
    "state",
    "goal",
    "state",
    "trying",
    "find",
    "path",
    "would",
    "return",
    "goal",
    "would",
    "solution",
    "able",
    "return",
    "solution",
    "found",
    "path",
    "general",
    "idea",
    "general",
    "approach",
    "search",
    "algorithm",
    "follow",
    "steps",
    "constantly",
    "removing",
    "nodes",
    "frontier",
    "able",
    "find",
    "solution",
    "next",
    "question",
    "might",
    "reasonably",
    "ask",
    "could",
    "go",
    "wrong",
    "potential",
    "problems",
    "approach",
    "like",
    "one",
    "example",
    "problem",
    "could",
    "arise",
    "sort",
    "approach",
    "imagine",
    "graph",
    "one",
    "change",
    "change",
    "instead",
    "arrow",
    "b",
    "also",
    "arrow",
    "b",
    "meaning",
    "go",
    "directions",
    "true",
    "something",
    "like",
    "15",
    "puzzle",
    "slide",
    "tile",
    "right",
    "could",
    "slide",
    "tile",
    "left",
    "get",
    "back",
    "original",
    "position",
    "could",
    "go",
    "back",
    "forth",
    "double",
    "arrows",
    "symbolize",
    "idea",
    "one",
    "state",
    "get",
    "another",
    "get",
    "back",
    "true",
    "many",
    "search",
    "problems",
    "going",
    "happen",
    "try",
    "apply",
    "approach",
    "well",
    "begin",
    "remove",
    "frontier",
    "consider",
    "get",
    "place",
    "get",
    "b",
    "goes",
    "frontier",
    "say",
    "right",
    "let",
    "take",
    "look",
    "thing",
    "left",
    "frontier",
    "get",
    "b",
    "c",
    "reverse",
    "arrow",
    "get",
    "c",
    "three",
    "c",
    "go",
    "frontier",
    "places",
    "get",
    "remove",
    "one",
    "frontier",
    "maybe",
    "unlucky",
    "maybe",
    "pick",
    "looking",
    "consider",
    "get",
    "well",
    "get",
    "start",
    "see",
    "problem",
    "careful",
    "go",
    "b",
    "back",
    "b",
    "could",
    "going",
    "infinite",
    "loop",
    "never",
    "make",
    "progress",
    "constantly",
    "going",
    "back",
    "forth",
    "two",
    "states",
    "already",
    "seen",
    "solution",
    "need",
    "way",
    "deal",
    "problem",
    "way",
    "deal",
    "problem",
    "somehow",
    "keeping",
    "track",
    "already",
    "explored",
    "logic",
    "going",
    "well",
    "already",
    "explored",
    "state",
    "reason",
    "go",
    "back",
    "explored",
    "state",
    "go",
    "back",
    "bother",
    "adding",
    "frontier",
    "need",
    "going",
    "revised",
    "approach",
    "better",
    "way",
    "approach",
    "sort",
    "search",
    "problem",
    "going",
    "look",
    "similar",
    "couple",
    "modifications",
    "start",
    "frontier",
    "contains",
    "initial",
    "state",
    "start",
    "another",
    "data",
    "structure",
    "set",
    "nodes",
    "already",
    "explored",
    "states",
    "explored",
    "initially",
    "empty",
    "empty",
    "explored",
    "set",
    "repeat",
    "frontier",
    "empty",
    "solution",
    "remove",
    "node",
    "frontier",
    "check",
    "see",
    "goal",
    "state",
    "return",
    "solution",
    "none",
    "different",
    "far",
    "going",
    "going",
    "add",
    "node",
    "explored",
    "state",
    "happens",
    "case",
    "remove",
    "node",
    "frontier",
    "goal",
    "add",
    "explored",
    "set",
    "know",
    "already",
    "explored",
    "need",
    "go",
    "back",
    "happens",
    "come",
    "later",
    "final",
    "step",
    "expand",
    "node",
    "add",
    "resulting",
    "nodes",
    "frontier",
    "always",
    "added",
    "resulting",
    "nodes",
    "frontier",
    "going",
    "little",
    "clever",
    "time",
    "going",
    "add",
    "nodes",
    "frontier",
    "already",
    "frontier",
    "already",
    "explored",
    "set",
    "check",
    "frontier",
    "explored",
    "set",
    "make",
    "sure",
    "node",
    "already",
    "one",
    "two",
    "long",
    "go",
    "ahead",
    "add",
    "frontier",
    "otherwise",
    "revised",
    "approach",
    "ultimately",
    "going",
    "help",
    "make",
    "sure",
    "go",
    "back",
    "forth",
    "two",
    "nodes",
    "one",
    "point",
    "kind",
    "glossed",
    "far",
    "step",
    "removing",
    "node",
    "frontier",
    "chose",
    "arbitrarily",
    "like",
    "let",
    "remove",
    "node",
    "turns",
    "actually",
    "quite",
    "important",
    "decide",
    "structure",
    "frontier",
    "add",
    "remove",
    "nodes",
    "frontier",
    "data",
    "structure",
    "need",
    "make",
    "choice",
    "order",
    "going",
    "removing",
    "elements",
    "one",
    "simplest",
    "data",
    "structures",
    "adding",
    "removing",
    "elements",
    "something",
    "called",
    "stack",
    "stack",
    "data",
    "structure",
    "last",
    "first",
    "data",
    "type",
    "means",
    "last",
    "thing",
    "add",
    "frontier",
    "going",
    "first",
    "thing",
    "remove",
    "frontier",
    "recent",
    "thing",
    "go",
    "stack",
    "frontier",
    "case",
    "going",
    "node",
    "explore",
    "let",
    "see",
    "happens",
    "apply",
    "approach",
    "something",
    "like",
    "problem",
    "finding",
    "path",
    "going",
    "happen",
    "well",
    "start",
    "say",
    "right",
    "let",
    "go",
    "ahead",
    "look",
    "first",
    "notice",
    "time",
    "added",
    "explored",
    "set",
    "something",
    "explored",
    "data",
    "structure",
    "keeping",
    "track",
    "say",
    "get",
    "right",
    "b",
    "well",
    "b",
    "explore",
    "b",
    "get",
    "c",
    "added",
    "c",
    "explore",
    "node",
    "going",
    "treat",
    "frontier",
    "stack",
    "last",
    "first",
    "last",
    "one",
    "come",
    "go",
    "ahead",
    "explore",
    "next",
    "say",
    "right",
    "get",
    "well",
    "get",
    "right",
    "put",
    "f",
    "frontier",
    "frontier",
    "stack",
    "f",
    "recent",
    "thing",
    "gone",
    "stack",
    "f",
    "explore",
    "next",
    "explore",
    "f",
    "say",
    "right",
    "get",
    "f",
    "well",
    "ca",
    "get",
    "anywhere",
    "nothing",
    "gets",
    "added",
    "frontier",
    "new",
    "recent",
    "thing",
    "added",
    "frontier",
    "well",
    "c",
    "thing",
    "left",
    "frontier",
    "explore",
    "see",
    "right",
    "c",
    "get",
    "e",
    "goes",
    "frontier",
    "say",
    "right",
    "let",
    "look",
    "e",
    "solution",
    "solved",
    "problem",
    "treat",
    "frontier",
    "like",
    "stack",
    "last",
    "first",
    "data",
    "structure",
    "result",
    "get",
    "go",
    "b",
    "sort",
    "backed",
    "went",
    "c",
    "important",
    "get",
    "visual",
    "sense",
    "algorithm",
    "working",
    "went",
    "deep",
    "search",
    "tree",
    "speak",
    "way",
    "bottom",
    "hit",
    "dead",
    "end",
    "effectively",
    "backed",
    "explored",
    "route",
    "try",
    "going",
    "deep",
    "search",
    "tree",
    "idea",
    "way",
    "algorithm",
    "ends",
    "working",
    "use",
    "stack",
    "call",
    "version",
    "algorithm",
    "depth",
    "first",
    "search",
    "depth",
    "first",
    "search",
    "search",
    "algorithm",
    "always",
    "explore",
    "deepest",
    "node",
    "frontier",
    "keep",
    "going",
    "deeper",
    "deeper",
    "search",
    "tree",
    "hit",
    "dead",
    "end",
    "back",
    "try",
    "something",
    "else",
    "instead",
    "depth",
    "first",
    "search",
    "one",
    "possible",
    "search",
    "options",
    "could",
    "use",
    "turns",
    "another",
    "algorithm",
    "called",
    "breadth",
    "first",
    "search",
    "behaves",
    "similarly",
    "depth",
    "first",
    "search",
    "one",
    "difference",
    "instead",
    "always",
    "exploring",
    "deepest",
    "node",
    "search",
    "tree",
    "way",
    "depth",
    "first",
    "search",
    "breadth",
    "first",
    "search",
    "always",
    "going",
    "explore",
    "shallowest",
    "node",
    "frontier",
    "mean",
    "well",
    "means",
    "instead",
    "using",
    "stack",
    "depth",
    "first",
    "search",
    "dfs",
    "used",
    "recent",
    "item",
    "added",
    "frontier",
    "one",
    "explore",
    "next",
    "breadth",
    "first",
    "search",
    "bfs",
    "instead",
    "use",
    "queue",
    "queue",
    "first",
    "first",
    "data",
    "type",
    "first",
    "thing",
    "add",
    "frontier",
    "first",
    "one",
    "explore",
    "effectively",
    "form",
    "line",
    "queue",
    "earlier",
    "arrive",
    "frontier",
    "earlier",
    "get",
    "explored",
    "would",
    "mean",
    "exact",
    "problem",
    "finding",
    "path",
    "e",
    "well",
    "start",
    "go",
    "ahead",
    "explored",
    "say",
    "get",
    "well",
    "get",
    "b",
    "b",
    "get",
    "c",
    "c",
    "get",
    "added",
    "frontier",
    "time",
    "though",
    "added",
    "c",
    "frontier",
    "explore",
    "c",
    "first",
    "c",
    "gets",
    "explored",
    "c",
    "get",
    "well",
    "get",
    "e",
    "gets",
    "added",
    "frontier",
    "explored",
    "e",
    "look",
    "next",
    "explore",
    "say",
    "get",
    "get",
    "say",
    "right",
    "get",
    "breadth",
    "first",
    "search",
    "bfs",
    "started",
    "looked",
    "c",
    "looked",
    "effectively",
    "looking",
    "things",
    "one",
    "away",
    "initial",
    "state",
    "two",
    "away",
    "initial",
    "state",
    "things",
    "three",
    "away",
    "initial",
    "state",
    "unlike",
    "depth",
    "first",
    "search",
    "went",
    "deep",
    "possible",
    "search",
    "tree",
    "hit",
    "dead",
    "end",
    "ultimately",
    "back",
    "two",
    "different",
    "search",
    "algorithms",
    "could",
    "apply",
    "order",
    "try",
    "solve",
    "problem",
    "let",
    "take",
    "look",
    "would",
    "actually",
    "work",
    "practice",
    "something",
    "like",
    "maze",
    "solving",
    "example",
    "example",
    "maze",
    "empty",
    "cells",
    "represent",
    "places",
    "agent",
    "move",
    "darkened",
    "gray",
    "cells",
    "represent",
    "walls",
    "agent",
    "ca",
    "pass",
    "ultimately",
    "agent",
    "ai",
    "going",
    "try",
    "find",
    "way",
    "get",
    "position",
    "position",
    "b",
    "via",
    "sequence",
    "actions",
    "actions",
    "left",
    "right",
    "depth",
    "first",
    "search",
    "case",
    "well",
    "depth",
    "first",
    "search",
    "follow",
    "one",
    "path",
    "reaches",
    "fork",
    "road",
    "multiple",
    "different",
    "options",
    "depth",
    "first",
    "search",
    "case",
    "going",
    "choose",
    "one",
    "real",
    "preference",
    "going",
    "keep",
    "following",
    "one",
    "hits",
    "dead",
    "end",
    "hits",
    "dead",
    "end",
    "depth",
    "first",
    "search",
    "effectively",
    "goes",
    "back",
    "last",
    "decision",
    "point",
    "tries",
    "path",
    "fully",
    "exhausting",
    "entire",
    "path",
    "realizes",
    "ok",
    "goal",
    "turns",
    "attention",
    "path",
    "goes",
    "deep",
    "possible",
    "hits",
    "dead",
    "end",
    "backs",
    "tries",
    "path",
    "keeps",
    "going",
    "deep",
    "possible",
    "one",
    "particular",
    "path",
    "realizes",
    "dead",
    "end",
    "back",
    "ultimately",
    "find",
    "way",
    "goal",
    "maybe",
    "got",
    "lucky",
    "maybe",
    "made",
    "different",
    "choice",
    "earlier",
    "ultimately",
    "depth",
    "first",
    "search",
    "going",
    "work",
    "going",
    "keep",
    "following",
    "hits",
    "dead",
    "end",
    "hits",
    "dead",
    "end",
    "backs",
    "looks",
    "different",
    "solution",
    "one",
    "thing",
    "might",
    "reasonably",
    "ask",
    "algorithm",
    "always",
    "going",
    "work",
    "always",
    "actually",
    "find",
    "way",
    "get",
    "initial",
    "state",
    "goal",
    "turns",
    "long",
    "maze",
    "finite",
    "long",
    "finitely",
    "many",
    "spaces",
    "travel",
    "yes",
    "depth",
    "first",
    "search",
    "going",
    "find",
    "solution",
    "eventually",
    "explore",
    "everything",
    "maze",
    "happens",
    "infinite",
    "infinite",
    "state",
    "space",
    "exist",
    "certain",
    "types",
    "problems",
    "slightly",
    "different",
    "story",
    "long",
    "maze",
    "finitely",
    "many",
    "squares",
    "going",
    "find",
    "solution",
    "next",
    "question",
    "though",
    "want",
    "ask",
    "going",
    "good",
    "solution",
    "optimal",
    "solution",
    "find",
    "answer",
    "necessarily",
    "let",
    "take",
    "look",
    "example",
    "maze",
    "example",
    "trying",
    "find",
    "way",
    "notice",
    "multiple",
    "possible",
    "solutions",
    "could",
    "go",
    "way",
    "could",
    "go",
    "order",
    "make",
    "way",
    "lucky",
    "depth",
    "first",
    "search",
    "choose",
    "way",
    "get",
    "reason",
    "necessarily",
    "depth",
    "first",
    "search",
    "would",
    "choose",
    "going",
    "going",
    "right",
    "sort",
    "arbitrary",
    "decision",
    "point",
    "going",
    "added",
    "frontier",
    "ultimately",
    "get",
    "unlucky",
    "depth",
    "first",
    "search",
    "might",
    "choose",
    "explore",
    "path",
    "first",
    "random",
    "choice",
    "point",
    "explore",
    "explore",
    "explore",
    "eventually",
    "find",
    "goal",
    "particular",
    "path",
    "actuality",
    "better",
    "path",
    "optimal",
    "solution",
    "used",
    "fewer",
    "steps",
    "assuming",
    "measuring",
    "cost",
    "solution",
    "based",
    "number",
    "steps",
    "need",
    "take",
    "depth",
    "first",
    "search",
    "unlucky",
    "might",
    "end",
    "finding",
    "best",
    "solution",
    "better",
    "solution",
    "available",
    "dfs",
    "depth",
    "first",
    "search",
    "bfs",
    "breadth",
    "first",
    "search",
    "compare",
    "would",
    "work",
    "particular",
    "situation",
    "well",
    "algorithm",
    "going",
    "look",
    "different",
    "visually",
    "terms",
    "bfs",
    "explores",
    "bfs",
    "looks",
    "shallower",
    "nodes",
    "first",
    "idea",
    "going",
    "bfs",
    "first",
    "look",
    "nodes",
    "one",
    "away",
    "initial",
    "state",
    "look",
    "look",
    "example",
    "two",
    "nodes",
    "immediately",
    "next",
    "initial",
    "state",
    "explore",
    "nodes",
    "two",
    "away",
    "looking",
    "state",
    "state",
    "example",
    "explore",
    "nodes",
    "three",
    "away",
    "state",
    "state",
    "whereas",
    "depth",
    "first",
    "search",
    "picked",
    "one",
    "path",
    "kept",
    "following",
    "breadth",
    "first",
    "search",
    "hand",
    "taking",
    "option",
    "exploring",
    "possible",
    "paths",
    "kind",
    "time",
    "bouncing",
    "back",
    "looking",
    "deeper",
    "deeper",
    "one",
    "making",
    "sure",
    "explore",
    "shallower",
    "ones",
    "ones",
    "closer",
    "initial",
    "state",
    "earlier",
    "keep",
    "following",
    "pattern",
    "looking",
    "things",
    "four",
    "away",
    "looking",
    "things",
    "five",
    "away",
    "looking",
    "things",
    "six",
    "away",
    "eventually",
    "make",
    "way",
    "goal",
    "case",
    "true",
    "explore",
    "states",
    "ultimately",
    "lead",
    "us",
    "anywhere",
    "path",
    "found",
    "goal",
    "optimal",
    "path",
    "shortest",
    "way",
    "could",
    "get",
    "goal",
    "might",
    "happen",
    "larger",
    "maze",
    "well",
    "let",
    "take",
    "look",
    "something",
    "like",
    "breadth",
    "first",
    "search",
    "going",
    "behave",
    "well",
    "breadth",
    "first",
    "search",
    "keep",
    "following",
    "states",
    "receives",
    "decision",
    "point",
    "could",
    "go",
    "either",
    "left",
    "right",
    "dfs",
    "picked",
    "one",
    "kept",
    "following",
    "hit",
    "dead",
    "end",
    "bfs",
    "hand",
    "explore",
    "say",
    "look",
    "node",
    "node",
    "look",
    "node",
    "node",
    "forth",
    "hits",
    "decision",
    "point",
    "rather",
    "pick",
    "one",
    "left",
    "two",
    "right",
    "explore",
    "path",
    "explore",
    "alternating",
    "going",
    "deeper",
    "deeper",
    "explore",
    "maybe",
    "keep",
    "going",
    "explore",
    "slowly",
    "make",
    "way",
    "visually",
    "see",
    "get",
    "decision",
    "point",
    "explore",
    "ultimately",
    "make",
    "way",
    "goal",
    "notice",
    "yes",
    "breadth",
    "first",
    "search",
    "find",
    "way",
    "b",
    "following",
    "particular",
    "path",
    "needed",
    "explore",
    "lot",
    "states",
    "order",
    "see",
    "trade",
    "offs",
    "dfs",
    "bfs",
    "dfs",
    "may",
    "cases",
    "memory",
    "savings",
    "compared",
    "breadth",
    "first",
    "approach",
    "breadth",
    "first",
    "search",
    "case",
    "explore",
    "lot",
    "states",
    "maybe",
    "wo",
    "always",
    "case",
    "let",
    "actually",
    "turn",
    "attention",
    "code",
    "look",
    "code",
    "could",
    "actually",
    "write",
    "order",
    "implement",
    "something",
    "like",
    "depth",
    "first",
    "search",
    "breadth",
    "first",
    "search",
    "context",
    "solving",
    "maze",
    "example",
    "go",
    "ahead",
    "go",
    "terminal",
    "inside",
    "implementation",
    "idea",
    "maze",
    "solving",
    "defined",
    "class",
    "called",
    "node",
    "case",
    "keeping",
    "track",
    "state",
    "parent",
    "words",
    "state",
    "state",
    "action",
    "case",
    "keeping",
    "track",
    "path",
    "cost",
    "calculate",
    "cost",
    "path",
    "end",
    "found",
    "way",
    "initial",
    "state",
    "goal",
    "addition",
    "defined",
    "class",
    "called",
    "stack",
    "frontier",
    "unfamiliar",
    "class",
    "class",
    "way",
    "define",
    "way",
    "generate",
    "objects",
    "python",
    "refers",
    "idea",
    "object",
    "oriented",
    "programming",
    "idea",
    "would",
    "like",
    "create",
    "object",
    "able",
    "store",
    "frontier",
    "data",
    "would",
    "like",
    "functions",
    "otherwise",
    "known",
    "methods",
    "object",
    "use",
    "manipulate",
    "object",
    "going",
    "unfamiliar",
    "syntax",
    "function",
    "initially",
    "creates",
    "frontier",
    "going",
    "represent",
    "using",
    "list",
    "initially",
    "frontier",
    "represented",
    "empty",
    "list",
    "nothing",
    "frontier",
    "begin",
    "add",
    "function",
    "adds",
    "something",
    "frontier",
    "appending",
    "end",
    "list",
    "function",
    "checks",
    "frontier",
    "contains",
    "particular",
    "state",
    "empty",
    "function",
    "checks",
    "frontier",
    "empty",
    "frontier",
    "empty",
    "means",
    "length",
    "frontier",
    "function",
    "removing",
    "something",
    "frontier",
    "ca",
    "remove",
    "something",
    "frontier",
    "frontier",
    "empty",
    "check",
    "first",
    "otherwise",
    "frontier",
    "empty",
    "recall",
    "implementing",
    "frontier",
    "stack",
    "last",
    "first",
    "data",
    "structure",
    "means",
    "last",
    "thing",
    "add",
    "frontier",
    "words",
    "last",
    "thing",
    "list",
    "item",
    "remove",
    "frontier",
    "see",
    "removed",
    "last",
    "item",
    "list",
    "index",
    "python",
    "list",
    "negative",
    "1",
    "gets",
    "last",
    "item",
    "list",
    "since",
    "0",
    "first",
    "item",
    "negative",
    "1",
    "kind",
    "wraps",
    "around",
    "gets",
    "last",
    "item",
    "list",
    "give",
    "node",
    "call",
    "node",
    "update",
    "frontier",
    "line",
    "28",
    "say",
    "go",
    "ahead",
    "remove",
    "node",
    "removed",
    "frontier",
    "return",
    "node",
    "result",
    "class",
    "effectively",
    "implements",
    "idea",
    "frontier",
    "gives",
    "way",
    "add",
    "something",
    "frontier",
    "way",
    "remove",
    "something",
    "frontier",
    "stack",
    "also",
    "good",
    "measure",
    "implemented",
    "alternative",
    "version",
    "thing",
    "called",
    "queue",
    "frontier",
    "parentheses",
    "see",
    "inherits",
    "stack",
    "frontier",
    "meaning",
    "going",
    "things",
    "stack",
    "frontier",
    "except",
    "way",
    "remove",
    "node",
    "frontier",
    "going",
    "slightly",
    "different",
    "instead",
    "removing",
    "end",
    "list",
    "way",
    "would",
    "stack",
    "instead",
    "going",
    "remove",
    "beginning",
    "list",
    "0",
    "get",
    "first",
    "node",
    "frontier",
    "first",
    "one",
    "added",
    "going",
    "one",
    "return",
    "case",
    "queue",
    "definition",
    "class",
    "called",
    "maze",
    "going",
    "handle",
    "process",
    "taking",
    "sequence",
    "text",
    "file",
    "figuring",
    "solve",
    "take",
    "input",
    "text",
    "file",
    "looks",
    "something",
    "like",
    "example",
    "see",
    "hash",
    "marks",
    "representing",
    "walls",
    "character",
    "representing",
    "starting",
    "position",
    "character",
    "b",
    "representing",
    "ending",
    "position",
    "take",
    "look",
    "code",
    "parsing",
    "text",
    "file",
    "right",
    "less",
    "interesting",
    "part",
    "interesting",
    "part",
    "solve",
    "function",
    "solve",
    "function",
    "going",
    "figure",
    "actually",
    "get",
    "point",
    "point",
    "see",
    "implementation",
    "exact",
    "idea",
    "saw",
    "moment",
    "ago",
    "going",
    "keep",
    "track",
    "many",
    "states",
    "explored",
    "report",
    "data",
    "later",
    "start",
    "node",
    "represents",
    "start",
    "state",
    "start",
    "frontier",
    "case",
    "stack",
    "frontier",
    "given",
    "treating",
    "frontier",
    "stack",
    "might",
    "imagine",
    "algorithm",
    "using",
    "search",
    "search",
    "dfs",
    "uses",
    "stack",
    "data",
    "structure",
    "initially",
    "frontier",
    "going",
    "contain",
    "start",
    "state",
    "initialize",
    "explored",
    "set",
    "initially",
    "empty",
    "nothing",
    "explored",
    "far",
    "loop",
    "notion",
    "repeating",
    "something",
    "first",
    "check",
    "frontier",
    "empty",
    "calling",
    "empty",
    "function",
    "saw",
    "implementation",
    "moment",
    "ago",
    "frontier",
    "indeed",
    "empty",
    "go",
    "ahead",
    "raise",
    "exception",
    "python",
    "error",
    "say",
    "sorry",
    "solution",
    "problem",
    "otherwise",
    "go",
    "ahead",
    "remove",
    "node",
    "frontier",
    "calling",
    "update",
    "number",
    "states",
    "explored",
    "explored",
    "one",
    "additional",
    "state",
    "say",
    "plus",
    "equals",
    "1",
    "adding",
    "1",
    "number",
    "states",
    "explored",
    "remove",
    "node",
    "frontier",
    "recall",
    "next",
    "step",
    "see",
    "whether",
    "goal",
    "goal",
    "test",
    "case",
    "maze",
    "goal",
    "pretty",
    "easy",
    "check",
    "see",
    "whether",
    "state",
    "node",
    "equal",
    "goal",
    "initially",
    "set",
    "maze",
    "set",
    "value",
    "called",
    "goal",
    "property",
    "maze",
    "check",
    "see",
    "node",
    "actually",
    "goal",
    "goal",
    "want",
    "backtrack",
    "way",
    "towards",
    "figuring",
    "actions",
    "took",
    "order",
    "get",
    "goal",
    "recall",
    "every",
    "node",
    "stores",
    "parent",
    "node",
    "came",
    "used",
    "get",
    "node",
    "also",
    "action",
    "used",
    "order",
    "get",
    "create",
    "loop",
    "constantly",
    "looking",
    "parent",
    "every",
    "node",
    "keeping",
    "track",
    "parents",
    "action",
    "took",
    "get",
    "parent",
    "current",
    "node",
    "loop",
    "going",
    "keep",
    "repeating",
    "process",
    "looking",
    "parent",
    "nodes",
    "get",
    "back",
    "initial",
    "state",
    "parent",
    "going",
    "equal",
    "none",
    "going",
    "building",
    "list",
    "actions",
    "following",
    "list",
    "cells",
    "part",
    "solution",
    "reverse",
    "build",
    "going",
    "goal",
    "back",
    "initial",
    "state",
    "building",
    "sequence",
    "actions",
    "goal",
    "initial",
    "state",
    "want",
    "reverse",
    "order",
    "get",
    "sequence",
    "actions",
    "initial",
    "state",
    "goal",
    "ultimately",
    "going",
    "solution",
    "happens",
    "current",
    "state",
    "equal",
    "goal",
    "otherwise",
    "goal",
    "well",
    "go",
    "ahead",
    "add",
    "state",
    "explored",
    "set",
    "say",
    "explored",
    "state",
    "need",
    "go",
    "back",
    "come",
    "across",
    "future",
    "logic",
    "implements",
    "idea",
    "adding",
    "neighbors",
    "frontier",
    "saying",
    "look",
    "neighbors",
    "implemented",
    "function",
    "called",
    "neighbors",
    "take",
    "look",
    "neighbors",
    "going",
    "check",
    "state",
    "already",
    "frontier",
    "state",
    "already",
    "explored",
    "set",
    "either",
    "go",
    "ahead",
    "add",
    "new",
    "child",
    "node",
    "new",
    "node",
    "frontier",
    "fair",
    "amount",
    "syntax",
    "key",
    "understand",
    "nuances",
    "syntax",
    "feel",
    "free",
    "take",
    "closer",
    "look",
    "file",
    "get",
    "sense",
    "working",
    "key",
    "see",
    "implementation",
    "pseudocode",
    "idea",
    "describing",
    "moment",
    "ago",
    "screen",
    "looking",
    "steps",
    "might",
    "follow",
    "order",
    "solve",
    "kind",
    "search",
    "problem",
    "let",
    "actually",
    "see",
    "action",
    "go",
    "ahead",
    "run",
    "example",
    "see",
    "printout",
    "maze",
    "initially",
    "looked",
    "like",
    "solved",
    "explore",
    "11",
    "states",
    "order",
    "found",
    "path",
    "program",
    "happened",
    "generate",
    "graphical",
    "representation",
    "well",
    "open",
    "generated",
    "program",
    "shows",
    "darker",
    "color",
    "walls",
    "red",
    "initial",
    "state",
    "green",
    "goal",
    "yellow",
    "path",
    "followed",
    "found",
    "path",
    "initial",
    "state",
    "goal",
    "let",
    "take",
    "look",
    "sophisticated",
    "maze",
    "see",
    "might",
    "happen",
    "instead",
    "let",
    "look",
    "much",
    "larger",
    "maze",
    "trying",
    "find",
    "way",
    "point",
    "point",
    "imagine",
    "search",
    "might",
    "lucky",
    "might",
    "get",
    "goal",
    "first",
    "try",
    "might",
    "follow",
    "one",
    "path",
    "backtrack",
    "explore",
    "something",
    "else",
    "little",
    "bit",
    "later",
    "let",
    "try",
    "run",
    "python",
    "time",
    "trying",
    "maze",
    "search",
    "able",
    "find",
    "solution",
    "indicated",
    "stars",
    "way",
    "get",
    "represent",
    "visually",
    "opening",
    "maze",
    "maze",
    "looks",
    "like",
    "highlighted",
    "yellow",
    "path",
    "found",
    "initial",
    "state",
    "goal",
    "many",
    "states",
    "explore",
    "found",
    "path",
    "well",
    "recall",
    "program",
    "keeping",
    "track",
    "number",
    "states",
    "explored",
    "far",
    "go",
    "back",
    "terminal",
    "see",
    "right",
    "order",
    "solve",
    "problem",
    "explore",
    "399",
    "different",
    "states",
    "fact",
    "make",
    "one",
    "small",
    "modification",
    "program",
    "tell",
    "program",
    "end",
    "output",
    "image",
    "added",
    "argument",
    "called",
    "show",
    "explored",
    "set",
    "show",
    "explored",
    "equal",
    "true",
    "rerun",
    "program",
    "python",
    "running",
    "maze2",
    "open",
    "maze",
    "see",
    "highlighted",
    "red",
    "states",
    "explored",
    "get",
    "initial",
    "state",
    "goal",
    "search",
    "dfs",
    "find",
    "way",
    "goal",
    "right",
    "away",
    "made",
    "choice",
    "first",
    "explore",
    "direction",
    "explored",
    "direction",
    "follow",
    "every",
    "conceivable",
    "path",
    "way",
    "end",
    "even",
    "long",
    "winding",
    "one",
    "order",
    "realize",
    "know",
    "dead",
    "end",
    "instead",
    "program",
    "needed",
    "backtrack",
    "going",
    "direction",
    "must",
    "gone",
    "direction",
    "got",
    "lucky",
    "choosing",
    "path",
    "got",
    "unlucky",
    "exploring",
    "direction",
    "exploring",
    "bunch",
    "states",
    "need",
    "likewise",
    "exploring",
    "top",
    "part",
    "graph",
    "probably",
    "need",
    "either",
    "search",
    "really",
    "performing",
    "optimally",
    "probably",
    "exploring",
    "states",
    "needs",
    "finds",
    "optimal",
    "solution",
    "best",
    "path",
    "goal",
    "number",
    "states",
    "needed",
    "explore",
    "order",
    "number",
    "steps",
    "take",
    "much",
    "higher",
    "let",
    "compare",
    "would",
    "search",
    "bfs",
    "exact",
    "maze",
    "instead",
    "order",
    "easy",
    "change",
    "algorithm",
    "dfs",
    "bfs",
    "identical",
    "exception",
    "data",
    "structure",
    "use",
    "represent",
    "frontier",
    "dfs",
    "used",
    "stack",
    "frontier",
    "last",
    "first",
    "whereas",
    "bfs",
    "going",
    "use",
    "queue",
    "frontier",
    "first",
    "first",
    "first",
    "thing",
    "add",
    "frontier",
    "first",
    "thing",
    "remove",
    "go",
    "back",
    "terminal",
    "rerun",
    "program",
    "maze",
    "see",
    "number",
    "states",
    "explore",
    "77",
    "compared",
    "almost",
    "400",
    "used",
    "search",
    "see",
    "exactly",
    "see",
    "happened",
    "open",
    "take",
    "look",
    "yellow",
    "highlight",
    "solution",
    "search",
    "found",
    "incidentally",
    "solution",
    "search",
    "found",
    "finding",
    "best",
    "solution",
    "notice",
    "white",
    "unexplored",
    "cells",
    "much",
    "fewer",
    "states",
    "needed",
    "explored",
    "order",
    "make",
    "way",
    "goal",
    "search",
    "operates",
    "little",
    "shallowly",
    "exploring",
    "things",
    "close",
    "initial",
    "state",
    "without",
    "exploring",
    "things",
    "away",
    "goal",
    "far",
    "away",
    "search",
    "actually",
    "behave",
    "quite",
    "effectively",
    "maze",
    "looks",
    "little",
    "something",
    "like",
    "case",
    "bfs",
    "dfs",
    "ended",
    "finding",
    "solution",
    "wo",
    "always",
    "case",
    "fact",
    "let",
    "take",
    "look",
    "one",
    "example",
    "instance",
    "notice",
    "multiple",
    "ways",
    "could",
    "get",
    "relatively",
    "small",
    "maze",
    "let",
    "look",
    "happens",
    "use",
    "go",
    "ahead",
    "turn",
    "show",
    "explored",
    "see",
    "solution",
    "use",
    "bfs",
    "search",
    "solve",
    "well",
    "find",
    "solution",
    "open",
    "maze",
    "solution",
    "found",
    "optimal",
    "one",
    "four",
    "steps",
    "get",
    "initial",
    "state",
    "goal",
    "happens",
    "happens",
    "tried",
    "use",
    "search",
    "dfs",
    "instead",
    "well",
    "go",
    "back",
    "q",
    "frontier",
    "q",
    "frontier",
    "means",
    "using",
    "search",
    "change",
    "stack",
    "frontier",
    "means",
    "using",
    "search",
    "rerun",
    "see",
    "find",
    "solution",
    "optimal",
    "solution",
    "instead",
    "algorithm",
    "finds",
    "maybe",
    "search",
    "would",
    "found",
    "solution",
    "possible",
    "guaranteed",
    "happen",
    "unlucky",
    "choose",
    "state",
    "instead",
    "state",
    "search",
    "might",
    "find",
    "longer",
    "route",
    "get",
    "initial",
    "state",
    "goal",
    "see",
    "search",
    "might",
    "find",
    "optimal",
    "solution",
    "point",
    "seems",
    "like",
    "search",
    "pretty",
    "good",
    "best",
    "going",
    "find",
    "us",
    "optimal",
    "solution",
    "worry",
    "situations",
    "might",
    "end",
    "finding",
    "longer",
    "path",
    "solution",
    "actually",
    "exists",
    "goal",
    "far",
    "away",
    "initial",
    "state",
    "might",
    "take",
    "lots",
    "steps",
    "order",
    "get",
    "initial",
    "state",
    "goal",
    "ended",
    "happening",
    "algorithm",
    "bfs",
    "ended",
    "exploring",
    "basically",
    "entire",
    "graph",
    "go",
    "entire",
    "maze",
    "order",
    "find",
    "way",
    "initial",
    "state",
    "goal",
    "state",
    "ultimately",
    "like",
    "algorithm",
    "little",
    "bit",
    "intelligent",
    "would",
    "mean",
    "algorithm",
    "little",
    "bit",
    "intelligent",
    "case",
    "well",
    "let",
    "look",
    "back",
    "search",
    "might",
    "able",
    "make",
    "different",
    "decision",
    "consider",
    "human",
    "intuition",
    "process",
    "well",
    "might",
    "human",
    "solving",
    "maze",
    "different",
    "bfs",
    "ultimately",
    "chose",
    "well",
    "first",
    "decision",
    "point",
    "bfs",
    "made",
    "right",
    "made",
    "five",
    "steps",
    "ended",
    "position",
    "fork",
    "row",
    "could",
    "either",
    "go",
    "left",
    "could",
    "go",
    "right",
    "initial",
    "couple",
    "steps",
    "choice",
    "one",
    "action",
    "could",
    "taken",
    "states",
    "search",
    "algorithm",
    "thing",
    "search",
    "algorithm",
    "could",
    "keep",
    "following",
    "state",
    "next",
    "state",
    "decision",
    "point",
    "things",
    "get",
    "little",
    "bit",
    "interesting",
    "search",
    "first",
    "search",
    "algorithm",
    "looked",
    "chose",
    "say",
    "let",
    "pick",
    "one",
    "path",
    "exhaust",
    "path",
    "see",
    "anything",
    "way",
    "goal",
    "let",
    "try",
    "way",
    "search",
    "took",
    "alternative",
    "approach",
    "saying",
    "know",
    "let",
    "explore",
    "things",
    "shallow",
    "close",
    "us",
    "first",
    "look",
    "left",
    "right",
    "back",
    "left",
    "back",
    "right",
    "forth",
    "alternating",
    "options",
    "hopes",
    "finding",
    "something",
    "nearby",
    "ultimately",
    "might",
    "human",
    "confronted",
    "situation",
    "like",
    "go",
    "left",
    "go",
    "right",
    "well",
    "human",
    "might",
    "visually",
    "see",
    "right",
    "trying",
    "get",
    "state",
    "b",
    "way",
    "going",
    "right",
    "feels",
    "like",
    "closer",
    "goal",
    "feels",
    "like",
    "going",
    "right",
    "better",
    "going",
    "left",
    "making",
    "progress",
    "towards",
    "getting",
    "goal",
    "course",
    "couple",
    "assumptions",
    "making",
    "making",
    "assumption",
    "represent",
    "grid",
    "like",
    "grid",
    "know",
    "coordinates",
    "everything",
    "know",
    "coordinate",
    "0",
    "0",
    "b",
    "coordinate",
    "pair",
    "know",
    "coordinate",
    "calculate",
    "yeah",
    "going",
    "way",
    "closer",
    "goal",
    "might",
    "reasonable",
    "assumption",
    "types",
    "search",
    "problems",
    "maybe",
    "others",
    "go",
    "ahead",
    "assume",
    "know",
    "current",
    "coordinate",
    "pair",
    "know",
    "coordinate",
    "x",
    "goal",
    "trying",
    "get",
    "situation",
    "like",
    "algorithm",
    "little",
    "bit",
    "intelligent",
    "somehow",
    "knows",
    "making",
    "progress",
    "towards",
    "goal",
    "probably",
    "way",
    "maze",
    "moving",
    "coordinate",
    "direction",
    "goal",
    "usually",
    "though",
    "always",
    "good",
    "thing",
    "draw",
    "distinction",
    "two",
    "different",
    "types",
    "search",
    "algorithms",
    "uninformed",
    "search",
    "informed",
    "search",
    "uninformed",
    "search",
    "algorithms",
    "algorithms",
    "like",
    "dfs",
    "bfs",
    "two",
    "algorithms",
    "looked",
    "search",
    "strategies",
    "use",
    "knowledge",
    "able",
    "solve",
    "problem",
    "dfs",
    "bfs",
    "really",
    "care",
    "structure",
    "maze",
    "anything",
    "way",
    "maze",
    "order",
    "solve",
    "problem",
    "look",
    "actions",
    "available",
    "choose",
    "actions",
    "matter",
    "whether",
    "maze",
    "problem",
    "solution",
    "way",
    "tries",
    "solve",
    "problem",
    "really",
    "fundamentally",
    "going",
    "going",
    "take",
    "look",
    "improvement",
    "upon",
    "uninformed",
    "search",
    "going",
    "take",
    "look",
    "informed",
    "search",
    "informed",
    "search",
    "going",
    "search",
    "strategies",
    "use",
    "knowledge",
    "specific",
    "problem",
    "able",
    "better",
    "find",
    "solution",
    "case",
    "maze",
    "knowledge",
    "something",
    "like",
    "square",
    "geographically",
    "closer",
    "goal",
    "better",
    "square",
    "geographically",
    "away",
    "something",
    "know",
    "thinking",
    "problem",
    "reasoning",
    "knowledge",
    "might",
    "helpful",
    "ai",
    "agent",
    "know",
    "little",
    "something",
    "number",
    "different",
    "types",
    "informed",
    "search",
    "specifically",
    "first",
    "going",
    "look",
    "particular",
    "type",
    "search",
    "algorithm",
    "called",
    "greedy",
    "search",
    "greedy",
    "search",
    "often",
    "abbreviated",
    "search",
    "algorithm",
    "instead",
    "expanding",
    "deepest",
    "node",
    "like",
    "dfs",
    "shallowest",
    "node",
    "like",
    "bfs",
    "algorithm",
    "always",
    "going",
    "expand",
    "node",
    "thinks",
    "closest",
    "goal",
    "search",
    "algorithm",
    "going",
    "know",
    "sure",
    "whether",
    "closest",
    "thing",
    "goal",
    "knew",
    "closest",
    "goal",
    "time",
    "would",
    "already",
    "solution",
    "knowledge",
    "close",
    "goal",
    "could",
    "follow",
    "steps",
    "order",
    "get",
    "initial",
    "position",
    "solution",
    "know",
    "solution",
    "meaning",
    "know",
    "exactly",
    "closest",
    "goal",
    "instead",
    "use",
    "estimate",
    "closest",
    "goal",
    "otherwise",
    "known",
    "heuristic",
    "way",
    "estimating",
    "whether",
    "close",
    "goal",
    "using",
    "heuristic",
    "function",
    "conventionally",
    "called",
    "h",
    "n",
    "takes",
    "status",
    "input",
    "returns",
    "estimate",
    "close",
    "goal",
    "might",
    "heuristic",
    "function",
    "actually",
    "look",
    "like",
    "case",
    "maze",
    "solving",
    "algorithm",
    "trying",
    "solve",
    "maze",
    "heuristic",
    "look",
    "like",
    "well",
    "heuristic",
    "needs",
    "answer",
    "question",
    "two",
    "cells",
    "c",
    "one",
    "better",
    "one",
    "would",
    "rather",
    "trying",
    "find",
    "way",
    "goal",
    "well",
    "human",
    "could",
    "probably",
    "look",
    "tell",
    "know",
    "looks",
    "like",
    "better",
    "even",
    "maze",
    "convoluted",
    "thought",
    "walls",
    "probably",
    "better",
    "better",
    "well",
    "ignore",
    "wall",
    "let",
    "pretend",
    "walls",
    "exist",
    "moment",
    "relax",
    "problem",
    "speak",
    "terms",
    "coordinate",
    "pairs",
    "closer",
    "goal",
    "fewer",
    "steps",
    "would",
    "take",
    "get",
    "goal",
    "compared",
    "c",
    "even",
    "ignore",
    "walls",
    "know",
    "c",
    "goal",
    "likewise",
    "know",
    "calculate",
    "geographically",
    "ignoring",
    "walls",
    "looks",
    "like",
    "better",
    "heuristic",
    "function",
    "going",
    "use",
    "something",
    "called",
    "manhattan",
    "distance",
    "one",
    "specific",
    "type",
    "heuristic",
    "heuristic",
    "many",
    "squares",
    "vertically",
    "horizontally",
    "left",
    "right",
    "allowing",
    "go",
    "diagonally",
    "either",
    "right",
    "left",
    "many",
    "steps",
    "need",
    "take",
    "get",
    "cells",
    "goal",
    "well",
    "turns",
    "much",
    "closer",
    "fewer",
    "steps",
    "needs",
    "take",
    "six",
    "steps",
    "order",
    "get",
    "goal",
    "ignoring",
    "walls",
    "relaxed",
    "problem",
    "little",
    "bit",
    "concerned",
    "math",
    "subtract",
    "x",
    "values",
    "values",
    "estimate",
    "far",
    "away",
    "estimate",
    "closer",
    "goal",
    "c",
    "approach",
    "way",
    "picking",
    "node",
    "remove",
    "frontier",
    "stage",
    "algorithm",
    "going",
    "remove",
    "node",
    "frontier",
    "going",
    "explore",
    "node",
    "smallest",
    "value",
    "heuristic",
    "function",
    "smallest",
    "manhattan",
    "distance",
    "goal",
    "would",
    "actually",
    "look",
    "like",
    "well",
    "let",
    "first",
    "label",
    "graph",
    "label",
    "maze",
    "number",
    "representing",
    "value",
    "heuristic",
    "function",
    "value",
    "manhattan",
    "distance",
    "cells",
    "cell",
    "example",
    "one",
    "away",
    "goal",
    "cell",
    "two",
    "away",
    "goal",
    "three",
    "away",
    "four",
    "away",
    "five",
    "away",
    "go",
    "one",
    "right",
    "four",
    "somewhere",
    "like",
    "manhattan",
    "distance",
    "two",
    "two",
    "squares",
    "away",
    "goal",
    "geographically",
    "even",
    "though",
    "practice",
    "going",
    "take",
    "longer",
    "path",
    "know",
    "yet",
    "heuristic",
    "easy",
    "way",
    "estimate",
    "far",
    "away",
    "goal",
    "maybe",
    "heuristic",
    "overly",
    "optimistic",
    "thinks",
    "yeah",
    "two",
    "steps",
    "away",
    "practice",
    "consider",
    "walls",
    "might",
    "steps",
    "important",
    "thing",
    "heuristic",
    "guarantee",
    "many",
    "steps",
    "going",
    "take",
    "estimating",
    "attempt",
    "trying",
    "approximate",
    "seem",
    "generally",
    "case",
    "squares",
    "look",
    "closer",
    "goal",
    "smaller",
    "values",
    "heuristic",
    "function",
    "squares",
    "away",
    "using",
    "greedy",
    "search",
    "might",
    "algorithm",
    "actually",
    "well",
    "first",
    "five",
    "steps",
    "much",
    "choice",
    "start",
    "initial",
    "state",
    "say",
    "right",
    "explore",
    "five",
    "states",
    "decision",
    "point",
    "choice",
    "going",
    "left",
    "going",
    "right",
    "dfs",
    "bfs",
    "would",
    "pick",
    "arbitrarily",
    "depends",
    "order",
    "throw",
    "two",
    "nodes",
    "frontier",
    "specify",
    "order",
    "put",
    "frontier",
    "order",
    "take",
    "look",
    "13",
    "11",
    "say",
    "right",
    "square",
    "distance",
    "11",
    "away",
    "goal",
    "according",
    "heuristic",
    "according",
    "estimate",
    "one",
    "estimate",
    "13",
    "away",
    "goal",
    "two",
    "options",
    "two",
    "choices",
    "rather",
    "rather",
    "11",
    "steps",
    "away",
    "goal",
    "go",
    "right",
    "able",
    "make",
    "informed",
    "decision",
    "know",
    "little",
    "something",
    "problem",
    "keep",
    "following",
    "10",
    "9",
    "two",
    "7s",
    "really",
    "much",
    "way",
    "know",
    "make",
    "arbitrary",
    "choice",
    "know",
    "maybe",
    "choose",
    "wrong",
    "ok",
    "still",
    "say",
    "right",
    "let",
    "try",
    "say",
    "7",
    "6",
    "make",
    "choice",
    "even",
    "though",
    "increases",
    "value",
    "heuristic",
    "function",
    "another",
    "decision",
    "point",
    "6",
    "8",
    "two",
    "really",
    "also",
    "considering",
    "13",
    "much",
    "higher",
    "6",
    "8",
    "13",
    "well",
    "6",
    "smallest",
    "value",
    "rather",
    "take",
    "able",
    "make",
    "informed",
    "decision",
    "going",
    "way",
    "right",
    "probably",
    "better",
    "going",
    "turn",
    "way",
    "go",
    "find",
    "decision",
    "point",
    "actually",
    "make",
    "decision",
    "might",
    "want",
    "make",
    "unfortunately",
    "much",
    "way",
    "around",
    "see",
    "4",
    "6",
    "4",
    "looks",
    "closer",
    "goal",
    "right",
    "going",
    "goal",
    "end",
    "taking",
    "route",
    "ultimately",
    "leads",
    "us",
    "dead",
    "end",
    "ok",
    "still",
    "say",
    "right",
    "let",
    "try",
    "follow",
    "route",
    "ultimately",
    "lead",
    "us",
    "goal",
    "greedy",
    "might",
    "try",
    "approach",
    "problem",
    "saying",
    "whenever",
    "decision",
    "multiple",
    "nodes",
    "could",
    "explore",
    "let",
    "explore",
    "node",
    "smallest",
    "value",
    "h",
    "n",
    "heuristic",
    "function",
    "estimating",
    "far",
    "go",
    "happens",
    "case",
    "end",
    "better",
    "terms",
    "number",
    "states",
    "needed",
    "explore",
    "bfs",
    "needed",
    "bfs",
    "explored",
    "section",
    "section",
    "able",
    "eliminate",
    "taking",
    "advantage",
    "heuristic",
    "knowledge",
    "close",
    "goal",
    "estimate",
    "idea",
    "seems",
    "much",
    "better",
    "would",
    "always",
    "prefer",
    "algorithm",
    "like",
    "algorithm",
    "like",
    "search",
    "well",
    "maybe",
    "one",
    "thing",
    "take",
    "consideration",
    "need",
    "come",
    "good",
    "heuristic",
    "good",
    "heuristic",
    "going",
    "affect",
    "good",
    "algorithm",
    "coming",
    "good",
    "heuristic",
    "oftentimes",
    "challenging",
    "thing",
    "consider",
    "ask",
    "question",
    "prior",
    "two",
    "algorithms",
    "algorithm",
    "optimal",
    "always",
    "find",
    "shortest",
    "path",
    "initial",
    "state",
    "goal",
    "answer",
    "question",
    "let",
    "take",
    "look",
    "example",
    "moment",
    "take",
    "look",
    "example",
    "trying",
    "get",
    "labeled",
    "cells",
    "manhattan",
    "distance",
    "goal",
    "number",
    "squares",
    "right",
    "would",
    "need",
    "travel",
    "order",
    "get",
    "square",
    "goal",
    "let",
    "think",
    "would",
    "greedy",
    "search",
    "always",
    "picks",
    "smallest",
    "number",
    "end",
    "finding",
    "optimal",
    "solution",
    "shortest",
    "solution",
    "would",
    "algorithm",
    "find",
    "important",
    "thing",
    "realize",
    "right",
    "decision",
    "point",
    "estimated",
    "12",
    "away",
    "goal",
    "two",
    "choices",
    "go",
    "left",
    "estimate",
    "13",
    "away",
    "goal",
    "go",
    "estimate",
    "11",
    "away",
    "goal",
    "two",
    "greedy",
    "search",
    "going",
    "say",
    "11",
    "looks",
    "better",
    "greedy",
    "search",
    "end",
    "finding",
    "path",
    "goal",
    "turns",
    "path",
    "optimal",
    "way",
    "get",
    "goal",
    "using",
    "fewer",
    "steps",
    "actually",
    "way",
    "way",
    "ultimately",
    "involved",
    "fewer",
    "steps",
    "even",
    "though",
    "meant",
    "moment",
    "choosing",
    "worst",
    "option",
    "two",
    "estimated",
    "worst",
    "option",
    "based",
    "heuristics",
    "mean",
    "greedy",
    "algorithm",
    "making",
    "best",
    "decision",
    "locally",
    "decision",
    "point",
    "looks",
    "like",
    "better",
    "go",
    "go",
    "big",
    "picture",
    "necessarily",
    "optimal",
    "might",
    "find",
    "solution",
    "actuality",
    "better",
    "solution",
    "available",
    "would",
    "like",
    "way",
    "solve",
    "problem",
    "like",
    "idea",
    "heuristic",
    "able",
    "estimate",
    "path",
    "distance",
    "us",
    "goal",
    "helps",
    "us",
    "able",
    "make",
    "better",
    "decisions",
    "eliminate",
    "search",
    "entire",
    "parts",
    "state",
    "space",
    "would",
    "like",
    "modify",
    "algorithm",
    "achieve",
    "optimality",
    "optimal",
    "way",
    "intuition",
    "well",
    "let",
    "take",
    "look",
    "problem",
    "initial",
    "problem",
    "greedy",
    "best",
    "research",
    "found",
    "us",
    "solution",
    "long",
    "path",
    "reason",
    "great",
    "yes",
    "heuristic",
    "numbers",
    "went",
    "pretty",
    "low",
    "later",
    "started",
    "build",
    "back",
    "built",
    "back",
    "8",
    "9",
    "10",
    "11",
    "way",
    "12",
    "case",
    "might",
    "go",
    "trying",
    "improve",
    "algorithm",
    "well",
    "one",
    "thing",
    "might",
    "realize",
    "go",
    "way",
    "algorithm",
    "path",
    "end",
    "going",
    "12",
    "take",
    "many",
    "steps",
    "knows",
    "many",
    "steps",
    "get",
    "12",
    "could",
    "also",
    "alternative",
    "taken",
    "much",
    "fewer",
    "steps",
    "six",
    "steps",
    "ended",
    "13",
    "yes",
    "13",
    "12",
    "looks",
    "like",
    "good",
    "required",
    "far",
    "fewer",
    "steps",
    "took",
    "six",
    "steps",
    "get",
    "13",
    "versus",
    "many",
    "steps",
    "get",
    "greedy",
    "best",
    "research",
    "says",
    "oh",
    "well",
    "12",
    "better",
    "13",
    "pick",
    "12",
    "might",
    "intelligently",
    "say",
    "rather",
    "somewhere",
    "heuristically",
    "looks",
    "like",
    "takes",
    "slightly",
    "longer",
    "get",
    "much",
    "quickly",
    "going",
    "encode",
    "idea",
    "general",
    "idea",
    "formal",
    "algorithm",
    "known",
    "star",
    "search",
    "star",
    "search",
    "going",
    "solve",
    "problem",
    "instead",
    "considering",
    "heuristic",
    "also",
    "considering",
    "long",
    "took",
    "us",
    "get",
    "particular",
    "state",
    "distinction",
    "greedy",
    "best",
    "search",
    "state",
    "right",
    "thing",
    "care",
    "estimated",
    "distance",
    "heuristic",
    "value",
    "goal",
    "whereas",
    "star",
    "search",
    "take",
    "consideration",
    "two",
    "pieces",
    "information",
    "take",
    "consideration",
    "far",
    "estimate",
    "goal",
    "also",
    "far",
    "travel",
    "order",
    "get",
    "relevant",
    "search",
    "algorithms",
    "expanding",
    "node",
    "lowest",
    "value",
    "g",
    "n",
    "plus",
    "h",
    "h",
    "n",
    "heuristic",
    "talking",
    "moment",
    "ago",
    "going",
    "vary",
    "based",
    "problem",
    "g",
    "n",
    "going",
    "cost",
    "reach",
    "node",
    "many",
    "steps",
    "take",
    "case",
    "get",
    "current",
    "position",
    "search",
    "algorithm",
    "look",
    "like",
    "practice",
    "well",
    "let",
    "take",
    "look",
    "got",
    "maze",
    "labeled",
    "manhattan",
    "distance",
    "value",
    "h",
    "n",
    "value",
    "heuristic",
    "estimate",
    "far",
    "squares",
    "away",
    "goal",
    "begin",
    "explore",
    "states",
    "care",
    "heuristic",
    "value",
    "also",
    "g",
    "n",
    "number",
    "steps",
    "take",
    "order",
    "get",
    "care",
    "summing",
    "two",
    "numbers",
    "together",
    "look",
    "like",
    "first",
    "step",
    "taken",
    "one",
    "step",
    "estimated",
    "16",
    "steps",
    "away",
    "goal",
    "total",
    "value",
    "take",
    "one",
    "step",
    "taken",
    "two",
    "steps",
    "estimate",
    "15",
    "away",
    "goal",
    "total",
    "value",
    "taken",
    "three",
    "steps",
    "estimated",
    "14",
    "away",
    "goal",
    "forth",
    "four",
    "steps",
    "estimate",
    "five",
    "steps",
    "estimate",
    "decision",
    "point",
    "could",
    "either",
    "six",
    "steps",
    "away",
    "goal",
    "heuristic",
    "13",
    "total",
    "19",
    "could",
    "six",
    "steps",
    "away",
    "goal",
    "heuristic",
    "11",
    "estimate",
    "17",
    "total",
    "19",
    "17",
    "rather",
    "take",
    "17",
    "6",
    "plus",
    "far",
    "different",
    "saw",
    "still",
    "taking",
    "option",
    "appears",
    "better",
    "keep",
    "taking",
    "option",
    "appears",
    "better",
    "right",
    "things",
    "get",
    "little",
    "bit",
    "different",
    "could",
    "15",
    "steps",
    "away",
    "goal",
    "estimated",
    "distance",
    "15",
    "plus",
    "6",
    "total",
    "value",
    "alternatively",
    "could",
    "six",
    "steps",
    "away",
    "goal",
    "five",
    "steps",
    "away",
    "six",
    "steps",
    "away",
    "total",
    "value",
    "13",
    "estimate",
    "6",
    "plus",
    "13",
    "would",
    "evaluate",
    "g",
    "n",
    "plus",
    "h",
    "n",
    "19",
    "6",
    "plus",
    "whereas",
    "would",
    "15",
    "plus",
    "6",
    "intuition",
    "19",
    "less",
    "21",
    "pick",
    "idea",
    "ultimately",
    "rather",
    "taken",
    "fewer",
    "steps",
    "get",
    "13",
    "taken",
    "15",
    "steps",
    "6",
    "means",
    "take",
    "steps",
    "order",
    "get",
    "maybe",
    "better",
    "path",
    "way",
    "instead",
    "explore",
    "route",
    "go",
    "one",
    "seven",
    "steps",
    "plus",
    "14",
    "two",
    "sort",
    "might",
    "end",
    "exploring",
    "one",
    "anyways",
    "numbers",
    "start",
    "get",
    "bigger",
    "heuristic",
    "values",
    "heuristic",
    "values",
    "start",
    "get",
    "smaller",
    "find",
    "actually",
    "keep",
    "exploring",
    "path",
    "math",
    "see",
    "every",
    "decision",
    "point",
    "star",
    "search",
    "going",
    "make",
    "choice",
    "based",
    "sum",
    "many",
    "steps",
    "took",
    "get",
    "current",
    "position",
    "far",
    "estimate",
    "goal",
    "explore",
    "states",
    "ultimate",
    "solution",
    "found",
    "fact",
    "optimal",
    "solution",
    "find",
    "us",
    "quickest",
    "possible",
    "way",
    "get",
    "initial",
    "state",
    "goal",
    "turns",
    "star",
    "optimal",
    "search",
    "algorithm",
    "certain",
    "conditions",
    "conditions",
    "h",
    "n",
    "heuristic",
    "needs",
    "admissible",
    "mean",
    "heuristic",
    "admissible",
    "well",
    "heuristic",
    "admissible",
    "never",
    "overestimates",
    "true",
    "cost",
    "h",
    "n",
    "always",
    "needs",
    "either",
    "get",
    "exactly",
    "right",
    "terms",
    "far",
    "away",
    "needs",
    "underestimate",
    "saw",
    "example",
    "heuristic",
    "value",
    "much",
    "smaller",
    "actual",
    "cost",
    "would",
    "take",
    "totally",
    "fine",
    "heuristic",
    "value",
    "never",
    "overestimate",
    "never",
    "think",
    "away",
    "goal",
    "actually",
    "meanwhile",
    "make",
    "stronger",
    "statement",
    "h",
    "n",
    "also",
    "needs",
    "consistent",
    "mean",
    "consistent",
    "mathematically",
    "means",
    "every",
    "node",
    "call",
    "n",
    "successor",
    "node",
    "call",
    "n",
    "prime",
    "takes",
    "cost",
    "c",
    "make",
    "step",
    "heuristic",
    "value",
    "n",
    "needs",
    "less",
    "equal",
    "heuristic",
    "value",
    "n",
    "prime",
    "plus",
    "cost",
    "lot",
    "math",
    "words",
    "ultimately",
    "means",
    "state",
    "right",
    "heuristic",
    "value",
    "goal",
    "heuristic",
    "value",
    "successor",
    "next",
    "place",
    "could",
    "go",
    "plus",
    "however",
    "much",
    "would",
    "cost",
    "make",
    "step",
    "one",
    "step",
    "next",
    "step",
    "making",
    "sure",
    "heuristic",
    "consistent",
    "steps",
    "might",
    "take",
    "long",
    "true",
    "star",
    "search",
    "going",
    "find",
    "optimal",
    "solution",
    "much",
    "challenge",
    "solving",
    "search",
    "problems",
    "sometimes",
    "come",
    "star",
    "search",
    "algorithm",
    "known",
    "could",
    "write",
    "code",
    "fairly",
    "easily",
    "choosing",
    "heuristic",
    "interesting",
    "challenge",
    "better",
    "heuristic",
    "better",
    "able",
    "solve",
    "problem",
    "fewer",
    "states",
    "explore",
    "need",
    "make",
    "sure",
    "heuristic",
    "satisfies",
    "particular",
    "constraints",
    "examples",
    "search",
    "algorithms",
    "might",
    "work",
    "certainly",
    "many",
    "star",
    "example",
    "tendency",
    "use",
    "quite",
    "bit",
    "memory",
    "alternative",
    "approaches",
    "star",
    "ultimately",
    "use",
    "less",
    "memory",
    "version",
    "star",
    "happens",
    "use",
    "search",
    "algorithms",
    "optimized",
    "cases",
    "well",
    "far",
    "looking",
    "search",
    "algorithms",
    "one",
    "agent",
    "trying",
    "find",
    "solution",
    "problem",
    "trying",
    "navigate",
    "way",
    "maze",
    "trying",
    "solve",
    "15",
    "puzzle",
    "trying",
    "find",
    "driving",
    "directions",
    "point",
    "point",
    "sometimes",
    "search",
    "situations",
    "though",
    "enter",
    "adversarial",
    "situation",
    "agent",
    "trying",
    "make",
    "intelligent",
    "decisions",
    "someone",
    "else",
    "fighting",
    "speak",
    "opposite",
    "objectives",
    "someone",
    "trying",
    "succeed",
    "someone",
    "else",
    "wants",
    "fail",
    "popular",
    "something",
    "like",
    "game",
    "game",
    "like",
    "tic",
    "tac",
    "toe",
    "got",
    "3",
    "3",
    "grid",
    "x",
    "take",
    "turns",
    "either",
    "writing",
    "x",
    "one",
    "squares",
    "goal",
    "get",
    "three",
    "x",
    "row",
    "x",
    "player",
    "three",
    "row",
    "player",
    "computers",
    "gotten",
    "quite",
    "good",
    "playing",
    "games",
    "tic",
    "tac",
    "toe",
    "easily",
    "even",
    "complex",
    "games",
    "might",
    "imagine",
    "intelligent",
    "decision",
    "game",
    "look",
    "like",
    "maybe",
    "x",
    "makes",
    "initial",
    "move",
    "middle",
    "plays",
    "intelligent",
    "move",
    "x",
    "become",
    "move",
    "x",
    "turns",
    "couple",
    "possibilities",
    "ai",
    "playing",
    "game",
    "optimally",
    "ai",
    "might",
    "play",
    "somewhere",
    "like",
    "upper",
    "right",
    "situation",
    "opposite",
    "objective",
    "x",
    "trying",
    "win",
    "game",
    "get",
    "three",
    "row",
    "diagonally",
    "trying",
    "stop",
    "objective",
    "opposite",
    "objective",
    "going",
    "place",
    "try",
    "block",
    "x",
    "pretty",
    "clever",
    "move",
    "x",
    "make",
    "move",
    "like",
    "x",
    "two",
    "possible",
    "ways",
    "x",
    "win",
    "game",
    "x",
    "could",
    "win",
    "game",
    "getting",
    "three",
    "row",
    "across",
    "x",
    "could",
    "win",
    "game",
    "getting",
    "three",
    "row",
    "vertically",
    "way",
    "matter",
    "makes",
    "next",
    "move",
    "could",
    "play",
    "example",
    "blocking",
    "three",
    "row",
    "horizontally",
    "x",
    "going",
    "win",
    "game",
    "getting",
    "three",
    "row",
    "vertically",
    "fair",
    "amount",
    "reasoning",
    "going",
    "order",
    "computer",
    "able",
    "solve",
    "problem",
    "similar",
    "spirit",
    "problems",
    "looked",
    "far",
    "actions",
    "sort",
    "state",
    "board",
    "transition",
    "one",
    "action",
    "next",
    "different",
    "sense",
    "classical",
    "search",
    "problem",
    "adversarial",
    "search",
    "problem",
    "x",
    "player",
    "trying",
    "find",
    "best",
    "moves",
    "make",
    "know",
    "adversary",
    "trying",
    "stop",
    "need",
    "sort",
    "algorithm",
    "deal",
    "adversarial",
    "type",
    "search",
    "situations",
    "algorithm",
    "going",
    "take",
    "look",
    "algorithm",
    "called",
    "minimax",
    "works",
    "well",
    "deterministic",
    "games",
    "two",
    "players",
    "work",
    "types",
    "games",
    "well",
    "look",
    "right",
    "games",
    "make",
    "move",
    "opponent",
    "makes",
    "move",
    "trying",
    "win",
    "opponent",
    "trying",
    "win",
    "also",
    "words",
    "opponent",
    "trying",
    "get",
    "lose",
    "need",
    "order",
    "make",
    "algorithm",
    "work",
    "well",
    "time",
    "try",
    "translate",
    "human",
    "concept",
    "playing",
    "game",
    "winning",
    "losing",
    "computer",
    "want",
    "translate",
    "terms",
    "computer",
    "understand",
    "ultimately",
    "computer",
    "really",
    "understands",
    "numbers",
    "want",
    "way",
    "translating",
    "game",
    "x",
    "grid",
    "something",
    "numerical",
    "something",
    "computer",
    "understand",
    "computer",
    "normally",
    "understand",
    "notions",
    "win",
    "lose",
    "understand",
    "concept",
    "bigger",
    "smaller",
    "might",
    "might",
    "take",
    "possible",
    "ways",
    "game",
    "unfold",
    "assign",
    "value",
    "utility",
    "one",
    "possible",
    "ways",
    "game",
    "many",
    "types",
    "games",
    "three",
    "possible",
    "outcomes",
    "outcomes",
    "wins",
    "x",
    "wins",
    "nobody",
    "wins",
    "player",
    "one",
    "wins",
    "player",
    "two",
    "wins",
    "nobody",
    "wins",
    "let",
    "go",
    "ahead",
    "assign",
    "possible",
    "outcomes",
    "different",
    "value",
    "say",
    "winning",
    "value",
    "negative",
    "nobody",
    "winning",
    "value",
    "x",
    "winning",
    "value",
    "assigned",
    "numbers",
    "three",
    "possible",
    "outcomes",
    "two",
    "players",
    "x",
    "player",
    "player",
    "going",
    "go",
    "ahead",
    "call",
    "x",
    "player",
    "max",
    "player",
    "call",
    "player",
    "min",
    "player",
    "reason",
    "min",
    "max",
    "algorithm",
    "max",
    "player",
    "case",
    "x",
    "aiming",
    "maximize",
    "score",
    "possible",
    "options",
    "score",
    "negative",
    "1",
    "0",
    "x",
    "wants",
    "maximize",
    "score",
    "meaning",
    "possible",
    "x",
    "would",
    "like",
    "situation",
    "x",
    "wins",
    "game",
    "give",
    "score",
    "possible",
    "x",
    "needs",
    "choose",
    "two",
    "options",
    "negative",
    "1",
    "meaning",
    "winning",
    "0",
    "meaning",
    "nobody",
    "winning",
    "x",
    "would",
    "rather",
    "nobody",
    "wins",
    "score",
    "0",
    "score",
    "negative",
    "1",
    "winning",
    "notion",
    "winning",
    "losing",
    "tying",
    "reduced",
    "mathematically",
    "idea",
    "try",
    "maximize",
    "score",
    "x",
    "player",
    "always",
    "wants",
    "score",
    "bigger",
    "flip",
    "side",
    "min",
    "player",
    "case",
    "aiming",
    "minimize",
    "score",
    "player",
    "wants",
    "score",
    "small",
    "possible",
    "taken",
    "game",
    "x",
    "winning",
    "losing",
    "turned",
    "something",
    "mathematical",
    "something",
    "x",
    "trying",
    "maximize",
    "score",
    "trying",
    "minimize",
    "score",
    "let",
    "look",
    "parts",
    "game",
    "need",
    "order",
    "encode",
    "ai",
    "ai",
    "play",
    "game",
    "like",
    "game",
    "going",
    "need",
    "couple",
    "things",
    "need",
    "sort",
    "initial",
    "state",
    "case",
    "call",
    "s0",
    "game",
    "begins",
    "like",
    "empty",
    "board",
    "example",
    "also",
    "need",
    "function",
    "called",
    "player",
    "player",
    "function",
    "going",
    "take",
    "input",
    "state",
    "represented",
    "output",
    "player",
    "function",
    "going",
    "player",
    "turn",
    "need",
    "able",
    "give",
    "board",
    "computer",
    "run",
    "function",
    "function",
    "tells",
    "us",
    "whose",
    "turn",
    "need",
    "notion",
    "actions",
    "take",
    "see",
    "examples",
    "moment",
    "need",
    "notion",
    "transition",
    "model",
    "state",
    "take",
    "action",
    "need",
    "know",
    "results",
    "consequence",
    "need",
    "way",
    "knowing",
    "game",
    "equivalent",
    "kind",
    "like",
    "goal",
    "test",
    "need",
    "terminal",
    "test",
    "way",
    "check",
    "see",
    "state",
    "terminal",
    "state",
    "terminal",
    "state",
    "means",
    "game",
    "classic",
    "game",
    "terminal",
    "state",
    "means",
    "either",
    "someone",
    "gotten",
    "three",
    "row",
    "squares",
    "board",
    "filled",
    "either",
    "conditions",
    "make",
    "terminal",
    "state",
    "game",
    "chess",
    "might",
    "something",
    "like",
    "checkmate",
    "checkmate",
    "longer",
    "possible",
    "becomes",
    "terminal",
    "state",
    "finally",
    "need",
    "utility",
    "function",
    "function",
    "takes",
    "state",
    "gives",
    "us",
    "numerical",
    "value",
    "terminal",
    "state",
    "way",
    "saying",
    "x",
    "wins",
    "game",
    "value",
    "game",
    "value",
    "negative",
    "nobody",
    "game",
    "value",
    "let",
    "take",
    "look",
    "turn",
    "initial",
    "state",
    "represent",
    "empty",
    "game",
    "board",
    "begin",
    "place",
    "begin",
    "search",
    "representing",
    "things",
    "visually",
    "imagine",
    "really",
    "like",
    "array",
    "array",
    "possible",
    "squares",
    "need",
    "player",
    "function",
    "takes",
    "state",
    "tells",
    "us",
    "whose",
    "turn",
    "assuming",
    "x",
    "makes",
    "first",
    "move",
    "empty",
    "game",
    "board",
    "player",
    "function",
    "going",
    "return",
    "game",
    "board",
    "x",
    "made",
    "move",
    "player",
    "function",
    "going",
    "return",
    "player",
    "function",
    "takes",
    "game",
    "board",
    "tells",
    "us",
    "whose",
    "turn",
    "next",
    "consider",
    "actions",
    "function",
    "actions",
    "function",
    "much",
    "like",
    "classical",
    "search",
    "takes",
    "state",
    "gives",
    "us",
    "set",
    "possible",
    "actions",
    "take",
    "state",
    "let",
    "imagine",
    "turned",
    "move",
    "game",
    "board",
    "looks",
    "like",
    "happens",
    "pass",
    "actions",
    "function",
    "actions",
    "function",
    "takes",
    "state",
    "game",
    "input",
    "output",
    "set",
    "possible",
    "actions",
    "set",
    "could",
    "move",
    "upper",
    "left",
    "could",
    "move",
    "bottom",
    "middle",
    "two",
    "possible",
    "action",
    "choices",
    "begin",
    "particular",
    "state",
    "states",
    "actions",
    "need",
    "sort",
    "transition",
    "model",
    "tell",
    "us",
    "take",
    "action",
    "state",
    "new",
    "state",
    "get",
    "define",
    "using",
    "result",
    "function",
    "takes",
    "state",
    "input",
    "well",
    "action",
    "apply",
    "result",
    "function",
    "state",
    "saying",
    "let",
    "let",
    "move",
    "upper",
    "left",
    "corner",
    "new",
    "state",
    "get",
    "resulting",
    "state",
    "upper",
    "left",
    "corner",
    "seems",
    "obvious",
    "someone",
    "knows",
    "play",
    "course",
    "play",
    "upper",
    "left",
    "corner",
    "board",
    "get",
    "information",
    "needs",
    "encoded",
    "ai",
    "ai",
    "know",
    "play",
    "tell",
    "ai",
    "rules",
    "work",
    "function",
    "defining",
    "function",
    "allows",
    "us",
    "tell",
    "ai",
    "game",
    "actually",
    "works",
    "actions",
    "actually",
    "affect",
    "outcome",
    "game",
    "ai",
    "needs",
    "know",
    "game",
    "works",
    "ai",
    "also",
    "needs",
    "know",
    "game",
    "defining",
    "function",
    "called",
    "terminal",
    "takes",
    "input",
    "state",
    "take",
    "game",
    "yet",
    "pass",
    "terminal",
    "function",
    "output",
    "false",
    "game",
    "take",
    "game",
    "x",
    "gotten",
    "three",
    "row",
    "along",
    "diagonal",
    "pass",
    "terminal",
    "function",
    "output",
    "going",
    "true",
    "game",
    "fact",
    "finally",
    "told",
    "ai",
    "game",
    "works",
    "terms",
    "moves",
    "made",
    "happens",
    "make",
    "moves",
    "told",
    "ai",
    "game",
    "need",
    "tell",
    "ai",
    "value",
    "states",
    "defining",
    "utility",
    "function",
    "takes",
    "state",
    "tells",
    "us",
    "score",
    "utility",
    "state",
    "said",
    "x",
    "wins",
    "game",
    "utility",
    "value",
    "1",
    "whereas",
    "wins",
    "game",
    "utility",
    "negative",
    "ai",
    "needs",
    "know",
    "terminal",
    "states",
    "game",
    "utility",
    "state",
    "give",
    "game",
    "board",
    "like",
    "game",
    "fact",
    "ask",
    "ai",
    "tell",
    "value",
    "state",
    "could",
    "value",
    "state",
    "things",
    "get",
    "interesting",
    "though",
    "game",
    "yet",
    "let",
    "imagine",
    "game",
    "board",
    "like",
    "middle",
    "game",
    "turn",
    "make",
    "move",
    "know",
    "turn",
    "make",
    "move",
    "calculate",
    "using",
    "player",
    "function",
    "say",
    "player",
    "pass",
    "state",
    "answer",
    "know",
    "turn",
    "move",
    "value",
    "board",
    "action",
    "take",
    "well",
    "going",
    "depend",
    "calculation",
    "minimax",
    "algorithm",
    "really",
    "comes",
    "recall",
    "x",
    "trying",
    "maximize",
    "score",
    "means",
    "trying",
    "minimize",
    "score",
    "would",
    "like",
    "minimize",
    "total",
    "value",
    "get",
    "end",
    "game",
    "game",
    "yet",
    "really",
    "know",
    "yet",
    "value",
    "game",
    "board",
    "calculation",
    "order",
    "figure",
    "kind",
    "calculation",
    "well",
    "order",
    "going",
    "consider",
    "might",
    "classical",
    "search",
    "situation",
    "actions",
    "could",
    "happen",
    "next",
    "states",
    "take",
    "us",
    "turns",
    "position",
    "two",
    "open",
    "squares",
    "means",
    "two",
    "open",
    "places",
    "make",
    "move",
    "could",
    "either",
    "make",
    "move",
    "upper",
    "left",
    "make",
    "move",
    "bottom",
    "middle",
    "minimax",
    "know",
    "right",
    "box",
    "moves",
    "going",
    "better",
    "going",
    "consider",
    "sort",
    "run",
    "situation",
    "two",
    "game",
    "boards",
    "neither",
    "happens",
    "next",
    "sense",
    "minimax",
    "call",
    "recursive",
    "algorithm",
    "going",
    "repeat",
    "exact",
    "process",
    "although",
    "considering",
    "opposite",
    "perspective",
    "going",
    "put",
    "player",
    "going",
    "put",
    "opponent",
    "shoes",
    "opponent",
    "x",
    "player",
    "consider",
    "would",
    "opponent",
    "position",
    "would",
    "opponent",
    "x",
    "player",
    "position",
    "would",
    "happen",
    "well",
    "player",
    "opponent",
    "x",
    "player",
    "trying",
    "maximize",
    "score",
    "whereas",
    "trying",
    "minimize",
    "score",
    "player",
    "x",
    "trying",
    "find",
    "maximum",
    "possible",
    "value",
    "get",
    "going",
    "happen",
    "well",
    "board",
    "position",
    "x",
    "one",
    "choice",
    "x",
    "going",
    "play",
    "going",
    "get",
    "three",
    "row",
    "know",
    "board",
    "x",
    "winning",
    "value",
    "x",
    "wins",
    "game",
    "value",
    "game",
    "board",
    "position",
    "state",
    "ever",
    "lead",
    "state",
    "possible",
    "option",
    "state",
    "value",
    "1",
    "maximum",
    "possible",
    "value",
    "x",
    "player",
    "get",
    "game",
    "board",
    "also",
    "place",
    "get",
    "game",
    "value",
    "1",
    "game",
    "board",
    "also",
    "value",
    "consider",
    "one",
    "going",
    "happen",
    "well",
    "x",
    "needs",
    "make",
    "move",
    "move",
    "x",
    "make",
    "upper",
    "left",
    "x",
    "go",
    "game",
    "one",
    "wins",
    "game",
    "nobody",
    "three",
    "row",
    "value",
    "game",
    "board",
    "nobody",
    "logic",
    "board",
    "position",
    "place",
    "get",
    "board",
    "value",
    "0",
    "state",
    "must",
    "also",
    "value",
    "comes",
    "choice",
    "part",
    "idea",
    "trying",
    "minimize",
    "player",
    "know",
    "make",
    "choice",
    "moving",
    "upper",
    "left",
    "going",
    "result",
    "game",
    "value",
    "1",
    "assuming",
    "everyone",
    "plays",
    "optimally",
    "instead",
    "play",
    "lower",
    "middle",
    "choose",
    "fork",
    "road",
    "going",
    "result",
    "game",
    "board",
    "value",
    "two",
    "options",
    "1",
    "0",
    "choose",
    "need",
    "pick",
    "min",
    "player",
    "would",
    "rather",
    "choose",
    "option",
    "minimum",
    "value",
    "whenever",
    "player",
    "multiple",
    "choices",
    "min",
    "player",
    "choose",
    "option",
    "smallest",
    "value",
    "max",
    "player",
    "choose",
    "option",
    "largest",
    "value",
    "1",
    "0",
    "0",
    "smaller",
    "meaning",
    "rather",
    "tie",
    "game",
    "lose",
    "game",
    "game",
    "board",
    "say",
    "also",
    "value",
    "0",
    "playing",
    "optimally",
    "pick",
    "fork",
    "road",
    "place",
    "block",
    "x",
    "3",
    "row",
    "x",
    "move",
    "upper",
    "left",
    "game",
    "one",
    "game",
    "logic",
    "minimax",
    "consider",
    "possible",
    "options",
    "take",
    "actions",
    "take",
    "put",
    "opponent",
    "shoes",
    "decide",
    "move",
    "going",
    "make",
    "considering",
    "move",
    "opponent",
    "make",
    "next",
    "turn",
    "consider",
    "move",
    "would",
    "make",
    "turn",
    "forth",
    "get",
    "way",
    "end",
    "game",
    "one",
    "terminal",
    "states",
    "fact",
    "decision",
    "point",
    "trying",
    "decide",
    "player",
    "make",
    "decision",
    "might",
    "part",
    "logic",
    "x",
    "player",
    "opponent",
    "using",
    "move",
    "might",
    "part",
    "larger",
    "tree",
    "x",
    "trying",
    "make",
    "move",
    "situation",
    "needs",
    "pick",
    "three",
    "different",
    "options",
    "order",
    "make",
    "decision",
    "happen",
    "away",
    "end",
    "game",
    "deeper",
    "tree",
    "go",
    "every",
    "level",
    "tree",
    "going",
    "correspond",
    "one",
    "move",
    "one",
    "move",
    "action",
    "take",
    "one",
    "move",
    "action",
    "opponent",
    "takes",
    "order",
    "decide",
    "happens",
    "fact",
    "turns",
    "x",
    "player",
    "position",
    "recursively",
    "logic",
    "see",
    "choice",
    "three",
    "choices",
    "fact",
    "one",
    "leads",
    "value",
    "play",
    "everyone",
    "plays",
    "optimally",
    "game",
    "tie",
    "play",
    "going",
    "win",
    "lose",
    "playing",
    "optimally",
    "x",
    "player",
    "win",
    "well",
    "score",
    "0",
    "negative",
    "1",
    "1",
    "rather",
    "pick",
    "board",
    "value",
    "1",
    "maximum",
    "value",
    "get",
    "board",
    "would",
    "also",
    "maximum",
    "value",
    "tree",
    "get",
    "deep",
    "especially",
    "game",
    "starts",
    "moves",
    "logic",
    "works",
    "sorts",
    "games",
    "make",
    "move",
    "opponent",
    "makes",
    "move",
    "ultimately",
    "adversarial",
    "objectives",
    "simplify",
    "diagram",
    "diagram",
    "looks",
    "like",
    "abstract",
    "version",
    "minimax",
    "tree",
    "states",
    "longer",
    "representing",
    "exactly",
    "like",
    "boards",
    "representing",
    "generic",
    "game",
    "might",
    "might",
    "game",
    "altogether",
    "green",
    "arrows",
    "pointing",
    "represents",
    "maximizing",
    "state",
    "would",
    "like",
    "score",
    "big",
    "possible",
    "red",
    "arrows",
    "pointing",
    "minimizing",
    "states",
    "player",
    "min",
    "player",
    "trying",
    "make",
    "score",
    "small",
    "possible",
    "imagine",
    "situation",
    "maximizing",
    "player",
    "player",
    "three",
    "choices",
    "one",
    "choice",
    "gives",
    "score",
    "5",
    "one",
    "choice",
    "gives",
    "score",
    "3",
    "one",
    "choice",
    "gives",
    "score",
    "well",
    "three",
    "choices",
    "best",
    "option",
    "choose",
    "9",
    "score",
    "maximizes",
    "options",
    "three",
    "options",
    "give",
    "state",
    "value",
    "9",
    "among",
    "three",
    "options",
    "best",
    "choice",
    "available",
    "decision",
    "imagine",
    "like",
    "one",
    "move",
    "away",
    "end",
    "game",
    "could",
    "also",
    "ask",
    "reasonable",
    "question",
    "might",
    "opponent",
    "two",
    "moves",
    "away",
    "end",
    "game",
    "opponent",
    "minimizing",
    "player",
    "trying",
    "make",
    "score",
    "small",
    "possible",
    "imagine",
    "would",
    "happened",
    "pick",
    "choice",
    "make",
    "one",
    "choice",
    "leads",
    "us",
    "state",
    "maximizing",
    "player",
    "going",
    "opt",
    "9",
    "biggest",
    "score",
    "get",
    "1",
    "leads",
    "state",
    "maximizing",
    "player",
    "would",
    "choose",
    "8",
    "largest",
    "score",
    "get",
    "minimizing",
    "player",
    "forced",
    "choose",
    "9",
    "8",
    "going",
    "choose",
    "smallest",
    "possible",
    "score",
    "case",
    "process",
    "would",
    "unfold",
    "minimizing",
    "player",
    "case",
    "considers",
    "options",
    "options",
    "would",
    "happen",
    "result",
    "general",
    "picture",
    "minimax",
    "algorithm",
    "looks",
    "like",
    "let",
    "try",
    "formalize",
    "using",
    "little",
    "bit",
    "pseudocode",
    "exactly",
    "happening",
    "minimax",
    "algorithm",
    "well",
    "given",
    "state",
    "need",
    "decide",
    "happen",
    "max",
    "player",
    "max",
    "player",
    "turn",
    "max",
    "going",
    "pick",
    "action",
    "actions",
    "recall",
    "actions",
    "function",
    "takes",
    "state",
    "gives",
    "back",
    "possible",
    "actions",
    "take",
    "tells",
    "moves",
    "possible",
    "max",
    "player",
    "going",
    "specifically",
    "pick",
    "action",
    "set",
    "actions",
    "gives",
    "highest",
    "value",
    "min",
    "value",
    "result",
    "mean",
    "well",
    "means",
    "want",
    "make",
    "option",
    "gives",
    "highest",
    "score",
    "actions",
    "score",
    "going",
    "calculate",
    "need",
    "know",
    "opponent",
    "min",
    "player",
    "going",
    "try",
    "minimize",
    "value",
    "state",
    "results",
    "say",
    "state",
    "results",
    "take",
    "action",
    "happens",
    "min",
    "player",
    "tries",
    "minimize",
    "value",
    "state",
    "consider",
    "possible",
    "options",
    "considered",
    "possible",
    "options",
    "pick",
    "action",
    "highest",
    "value",
    "likewise",
    "min",
    "player",
    "going",
    "thing",
    "backwards",
    "also",
    "going",
    "consider",
    "possible",
    "actions",
    "take",
    "turn",
    "going",
    "pick",
    "action",
    "smallest",
    "possible",
    "value",
    "options",
    "way",
    "know",
    "smallest",
    "possible",
    "value",
    "options",
    "considering",
    "max",
    "player",
    "going",
    "saying",
    "result",
    "applying",
    "action",
    "current",
    "state",
    "would",
    "max",
    "player",
    "try",
    "value",
    "would",
    "max",
    "player",
    "calculate",
    "particular",
    "state",
    "everyone",
    "makes",
    "decision",
    "based",
    "trying",
    "estimate",
    "person",
    "would",
    "need",
    "turn",
    "attention",
    "two",
    "functions",
    "max",
    "value",
    "min",
    "value",
    "actually",
    "calculate",
    "value",
    "state",
    "trying",
    "maximize",
    "value",
    "calculate",
    "value",
    "state",
    "trying",
    "minimize",
    "value",
    "entire",
    "implementation",
    "min",
    "max",
    "algorithm",
    "let",
    "try",
    "let",
    "try",
    "implement",
    "max",
    "value",
    "function",
    "takes",
    "state",
    "returns",
    "output",
    "value",
    "state",
    "trying",
    "maximize",
    "value",
    "state",
    "well",
    "first",
    "thing",
    "check",
    "see",
    "game",
    "game",
    "words",
    "state",
    "terminal",
    "state",
    "easy",
    "already",
    "utility",
    "function",
    "tells",
    "value",
    "board",
    "game",
    "check",
    "x",
    "win",
    "win",
    "tie",
    "utility",
    "function",
    "knows",
    "value",
    "state",
    "trickier",
    "game",
    "need",
    "recursive",
    "reasoning",
    "thinking",
    "opponent",
    "going",
    "next",
    "move",
    "want",
    "calculate",
    "value",
    "state",
    "want",
    "value",
    "state",
    "high",
    "possible",
    "keep",
    "track",
    "value",
    "variable",
    "called",
    "want",
    "value",
    "high",
    "possible",
    "need",
    "give",
    "v",
    "initial",
    "value",
    "initially",
    "go",
    "ahead",
    "set",
    "low",
    "possible",
    "know",
    "options",
    "available",
    "yet",
    "initially",
    "set",
    "v",
    "equal",
    "negative",
    "infinity",
    "seems",
    "little",
    "bit",
    "strange",
    "idea",
    "want",
    "value",
    "initially",
    "low",
    "possible",
    "consider",
    "actions",
    "always",
    "going",
    "try",
    "better",
    "set",
    "v",
    "negative",
    "infinity",
    "know",
    "always",
    "better",
    "consider",
    "actions",
    "going",
    "kind",
    "loop",
    "every",
    "action",
    "actions",
    "state",
    "recall",
    "actions",
    "function",
    "takes",
    "state",
    "gives",
    "possible",
    "actions",
    "use",
    "state",
    "one",
    "actions",
    "want",
    "compare",
    "v",
    "say",
    "right",
    "v",
    "going",
    "equal",
    "maximum",
    "v",
    "expression",
    "expression",
    "well",
    "first",
    "get",
    "result",
    "taking",
    "action",
    "state",
    "get",
    "min",
    "value",
    "words",
    "let",
    "say",
    "want",
    "find",
    "state",
    "best",
    "min",
    "player",
    "going",
    "try",
    "minimize",
    "score",
    "whatever",
    "resulting",
    "score",
    "min",
    "value",
    "state",
    "compare",
    "current",
    "best",
    "value",
    "pick",
    "maximum",
    "two",
    "trying",
    "maximize",
    "value",
    "short",
    "three",
    "lines",
    "code",
    "going",
    "possible",
    "actions",
    "asking",
    "question",
    "maximize",
    "score",
    "given",
    "opponent",
    "going",
    "try",
    "entire",
    "loop",
    "return",
    "v",
    "value",
    "particular",
    "state",
    "min",
    "player",
    "exact",
    "opposite",
    "logic",
    "backwards",
    "calculate",
    "minimum",
    "value",
    "state",
    "first",
    "check",
    "terminal",
    "state",
    "return",
    "utility",
    "otherwise",
    "going",
    "try",
    "minimize",
    "value",
    "state",
    "given",
    "possible",
    "actions",
    "need",
    "initial",
    "value",
    "v",
    "value",
    "state",
    "initially",
    "set",
    "infinity",
    "know",
    "always",
    "get",
    "something",
    "less",
    "infinity",
    "starting",
    "v",
    "equals",
    "infinity",
    "make",
    "sure",
    "first",
    "action",
    "find",
    "less",
    "value",
    "thing",
    "loop",
    "possible",
    "actions",
    "results",
    "could",
    "get",
    "max",
    "player",
    "makes",
    "decision",
    "let",
    "take",
    "minimum",
    "current",
    "value",
    "said",
    "done",
    "get",
    "smallest",
    "possible",
    "value",
    "v",
    "return",
    "back",
    "user",
    "effect",
    "pseudocode",
    "minimax",
    "take",
    "gain",
    "figure",
    "best",
    "move",
    "make",
    "recursively",
    "using",
    "max",
    "value",
    "min",
    "value",
    "functions",
    "max",
    "value",
    "calls",
    "min",
    "value",
    "min",
    "value",
    "calls",
    "max",
    "value",
    "back",
    "forth",
    "way",
    "reach",
    "terminal",
    "state",
    "point",
    "algorithm",
    "simply",
    "return",
    "utility",
    "particular",
    "state",
    "might",
    "imagine",
    "going",
    "start",
    "long",
    "process",
    "especially",
    "games",
    "start",
    "get",
    "complex",
    "start",
    "add",
    "moves",
    "possible",
    "options",
    "games",
    "might",
    "last",
    "quite",
    "bit",
    "longer",
    "next",
    "question",
    "ask",
    "sort",
    "optimizations",
    "make",
    "better",
    "order",
    "use",
    "less",
    "space",
    "take",
    "less",
    "time",
    "able",
    "solve",
    "kind",
    "problem",
    "take",
    "look",
    "couple",
    "possible",
    "optimizations",
    "one",
    "take",
    "look",
    "example",
    "returning",
    "arrows",
    "arrows",
    "let",
    "imagine",
    "max",
    "player",
    "green",
    "arrow",
    "trying",
    "make",
    "score",
    "high",
    "possible",
    "easy",
    "game",
    "two",
    "moves",
    "make",
    "move",
    "one",
    "three",
    "options",
    "opponent",
    "makes",
    "move",
    "one",
    "three",
    "options",
    "based",
    "move",
    "make",
    "result",
    "get",
    "value",
    "let",
    "look",
    "order",
    "calculations",
    "figure",
    "optimizations",
    "might",
    "able",
    "make",
    "calculation",
    "process",
    "going",
    "look",
    "states",
    "one",
    "time",
    "let",
    "say",
    "start",
    "left",
    "say",
    "right",
    "going",
    "consider",
    "min",
    "player",
    "opponent",
    "try",
    "well",
    "min",
    "player",
    "going",
    "look",
    "three",
    "possible",
    "actions",
    "look",
    "value",
    "terminal",
    "states",
    "end",
    "game",
    "see",
    "right",
    "node",
    "value",
    "four",
    "value",
    "eight",
    "value",
    "five",
    "min",
    "player",
    "going",
    "say",
    "well",
    "right",
    "three",
    "options",
    "four",
    "eight",
    "five",
    "take",
    "smallest",
    "one",
    "take",
    "four",
    "state",
    "value",
    "four",
    "max",
    "player",
    "say",
    "right",
    "take",
    "action",
    "value",
    "four",
    "best",
    "min",
    "player",
    "going",
    "try",
    "minimize",
    "score",
    "take",
    "option",
    "explore",
    "next",
    "explore",
    "min",
    "player",
    "would",
    "choose",
    "action",
    "min",
    "player",
    "going",
    "say",
    "right",
    "three",
    "options",
    "min",
    "player",
    "options",
    "nine",
    "three",
    "seven",
    "three",
    "smallest",
    "among",
    "nine",
    "three",
    "seven",
    "go",
    "ahead",
    "say",
    "state",
    "value",
    "three",
    "max",
    "player",
    "explored",
    "two",
    "three",
    "options",
    "know",
    "one",
    "options",
    "guarantee",
    "score",
    "four",
    "least",
    "one",
    "options",
    "guarantee",
    "score",
    "three",
    "consider",
    "third",
    "option",
    "say",
    "right",
    "happens",
    "exact",
    "logic",
    "min",
    "player",
    "going",
    "look",
    "three",
    "states",
    "two",
    "four",
    "six",
    "say",
    "minimum",
    "possible",
    "option",
    "two",
    "min",
    "player",
    "wants",
    "two",
    "max",
    "player",
    "calculated",
    "information",
    "looking",
    "two",
    "layers",
    "deep",
    "looking",
    "nodes",
    "say",
    "four",
    "three",
    "two",
    "know",
    "rather",
    "take",
    "four",
    "choose",
    "option",
    "opponent",
    "plays",
    "optimally",
    "try",
    "get",
    "four",
    "best",
    "ca",
    "guarantee",
    "higher",
    "score",
    "pick",
    "either",
    "two",
    "options",
    "might",
    "get",
    "three",
    "might",
    "get",
    "two",
    "true",
    "nine",
    "highest",
    "score",
    "scores",
    "might",
    "tempted",
    "say",
    "know",
    "maybe",
    "take",
    "option",
    "might",
    "get",
    "nine",
    "min",
    "player",
    "playing",
    "intelligently",
    "making",
    "best",
    "moves",
    "possible",
    "option",
    "get",
    "make",
    "choice",
    "left",
    "three",
    "whereas",
    "could",
    "better",
    "playing",
    "optimally",
    "guaranteed",
    "would",
    "get",
    "four",
    "effect",
    "logic",
    "would",
    "use",
    "min",
    "max",
    "player",
    "trying",
    "maximize",
    "score",
    "node",
    "turns",
    "took",
    "quite",
    "bit",
    "computation",
    "figure",
    "reason",
    "nodes",
    "order",
    "draw",
    "conclusion",
    "pretty",
    "simple",
    "game",
    "three",
    "choices",
    "opponent",
    "three",
    "choices",
    "game",
    "like",
    "come",
    "way",
    "optimize",
    "maybe",
    "need",
    "calculation",
    "still",
    "reach",
    "conclusion",
    "know",
    "action",
    "left",
    "best",
    "could",
    "let",
    "go",
    "ahead",
    "try",
    "try",
    "little",
    "intelligent",
    "go",
    "first",
    "start",
    "exact",
    "way",
    "know",
    "initially",
    "consider",
    "one",
    "options",
    "consider",
    "min",
    "player",
    "might",
    "min",
    "three",
    "options",
    "four",
    "eight",
    "five",
    "three",
    "options",
    "min",
    "says",
    "four",
    "best",
    "want",
    "try",
    "minimize",
    "score",
    "max",
    "player",
    "consider",
    "second",
    "option",
    "making",
    "move",
    "considering",
    "opponent",
    "would",
    "response",
    "min",
    "player",
    "well",
    "min",
    "player",
    "going",
    "state",
    "look",
    "options",
    "would",
    "say",
    "right",
    "nine",
    "option",
    "three",
    "option",
    "math",
    "initial",
    "state",
    "calculation",
    "see",
    "three",
    "immediately",
    "red",
    "flag",
    "see",
    "three",
    "state",
    "know",
    "value",
    "state",
    "going",
    "three",
    "going",
    "three",
    "something",
    "less",
    "three",
    "even",
    "though",
    "yet",
    "looked",
    "last",
    "action",
    "even",
    "actions",
    "actions",
    "could",
    "taken",
    "know",
    "well",
    "know",
    "min",
    "player",
    "going",
    "try",
    "minimize",
    "score",
    "see",
    "three",
    "way",
    "could",
    "something",
    "three",
    "remaining",
    "thing",
    "yet",
    "looked",
    "less",
    "three",
    "means",
    "way",
    "value",
    "anything",
    "three",
    "min",
    "player",
    "already",
    "guarantee",
    "three",
    "trying",
    "minimize",
    "score",
    "tell",
    "well",
    "tells",
    "choose",
    "action",
    "score",
    "going",
    "three",
    "maybe",
    "even",
    "less",
    "three",
    "unlucky",
    "already",
    "know",
    "action",
    "guarantee",
    "four",
    "given",
    "know",
    "action",
    "guarantees",
    "score",
    "four",
    "action",
    "means",
    "ca",
    "better",
    "three",
    "trying",
    "maximize",
    "options",
    "need",
    "consider",
    "triangle",
    "value",
    "number",
    "could",
    "go",
    "would",
    "change",
    "mind",
    "two",
    "options",
    "always",
    "going",
    "opt",
    "path",
    "gets",
    "four",
    "opposed",
    "path",
    "best",
    "three",
    "opponent",
    "plays",
    "optimally",
    "going",
    "true",
    "future",
    "states",
    "look",
    "look",
    "min",
    "player",
    "might",
    "see",
    "state",
    "two",
    "know",
    "state",
    "two",
    "way",
    "value",
    "could",
    "something",
    "two",
    "one",
    "remaining",
    "states",
    "less",
    "two",
    "min",
    "player",
    "would",
    "opt",
    "instead",
    "even",
    "without",
    "looking",
    "remaining",
    "states",
    "maximizing",
    "player",
    "know",
    "choosing",
    "path",
    "left",
    "going",
    "better",
    "choosing",
    "either",
    "two",
    "paths",
    "right",
    "one",
    "ca",
    "better",
    "three",
    "one",
    "ca",
    "better",
    "two",
    "four",
    "case",
    "best",
    "order",
    "cut",
    "say",
    "state",
    "value",
    "four",
    "order",
    "type",
    "calculation",
    "little",
    "bit",
    "bookkeeping",
    "keeping",
    "track",
    "things",
    "keeping",
    "track",
    "time",
    "best",
    "worst",
    "states",
    "saying",
    "right",
    "well",
    "already",
    "know",
    "get",
    "four",
    "best",
    "state",
    "three",
    "reason",
    "consider",
    "effectively",
    "prune",
    "leaf",
    "anything",
    "tree",
    "reason",
    "approach",
    "optimization",
    "minimax",
    "called",
    "alpha",
    "beta",
    "pruning",
    "alpha",
    "beta",
    "stand",
    "two",
    "values",
    "keep",
    "track",
    "best",
    "far",
    "worst",
    "far",
    "pruning",
    "idea",
    "big",
    "long",
    "deep",
    "search",
    "tree",
    "might",
    "able",
    "search",
    "efficiently",
    "need",
    "search",
    "everything",
    "remove",
    "nodes",
    "try",
    "optimize",
    "way",
    "look",
    "entire",
    "search",
    "space",
    "alpha",
    "beta",
    "pruning",
    "definitely",
    "save",
    "us",
    "lot",
    "time",
    "go",
    "search",
    "process",
    "making",
    "searches",
    "efficient",
    "even",
    "still",
    "great",
    "games",
    "get",
    "complex",
    "fortunately",
    "relatively",
    "simple",
    "game",
    "might",
    "reasonably",
    "ask",
    "question",
    "like",
    "many",
    "total",
    "possible",
    "games",
    "think",
    "try",
    "estimate",
    "many",
    "moves",
    "given",
    "point",
    "many",
    "moves",
    "long",
    "game",
    "last",
    "turns",
    "possible",
    "games",
    "played",
    "compare",
    "complex",
    "game",
    "something",
    "like",
    "game",
    "chess",
    "example",
    "far",
    "pieces",
    "far",
    "moves",
    "games",
    "last",
    "much",
    "longer",
    "many",
    "total",
    "possible",
    "chess",
    "games",
    "could",
    "turns",
    "four",
    "moves",
    "four",
    "moves",
    "white",
    "player",
    "four",
    "moves",
    "black",
    "player",
    "288",
    "billion",
    "possible",
    "chess",
    "games",
    "result",
    "situation",
    "four",
    "moves",
    "going",
    "even",
    "look",
    "entire",
    "chess",
    "games",
    "many",
    "possible",
    "chess",
    "games",
    "could",
    "result",
    "10",
    "possible",
    "chess",
    "games",
    "far",
    "chess",
    "games",
    "could",
    "ever",
    "considered",
    "pretty",
    "big",
    "problem",
    "minimax",
    "algorithm",
    "minimax",
    "algorithm",
    "starts",
    "initial",
    "state",
    "considers",
    "possible",
    "actions",
    "possible",
    "actions",
    "way",
    "get",
    "end",
    "game",
    "going",
    "problem",
    "computer",
    "going",
    "need",
    "look",
    "many",
    "states",
    "far",
    "computer",
    "could",
    "ever",
    "reasonable",
    "amount",
    "time",
    "order",
    "solve",
    "problem",
    "instead",
    "looking",
    "states",
    "totally",
    "intractable",
    "computer",
    "need",
    "better",
    "approach",
    "turns",
    "better",
    "approach",
    "generally",
    "takes",
    "form",
    "something",
    "called",
    "minimax",
    "normally",
    "minimax",
    "keep",
    "going",
    "layer",
    "layer",
    "move",
    "move",
    "get",
    "end",
    "game",
    "minimax",
    "instead",
    "going",
    "say",
    "know",
    "certain",
    "number",
    "moves",
    "maybe",
    "look",
    "10",
    "moves",
    "ahead",
    "maybe",
    "look",
    "12",
    "moves",
    "ahead",
    "point",
    "going",
    "stop",
    "consider",
    "additional",
    "moves",
    "might",
    "come",
    "would",
    "computationally",
    "intractable",
    "consider",
    "possible",
    "options",
    "get",
    "10",
    "12",
    "moves",
    "deep",
    "arrive",
    "situation",
    "game",
    "minimax",
    "still",
    "needs",
    "way",
    "assign",
    "score",
    "game",
    "board",
    "game",
    "state",
    "figure",
    "current",
    "value",
    "easy",
    "game",
    "easy",
    "game",
    "yet",
    "order",
    "need",
    "add",
    "one",
    "additional",
    "feature",
    "minimax",
    "called",
    "evaluation",
    "function",
    "function",
    "going",
    "estimate",
    "expected",
    "utility",
    "game",
    "given",
    "state",
    "game",
    "like",
    "chess",
    "imagine",
    "game",
    "value",
    "1",
    "means",
    "white",
    "wins",
    "negative",
    "1",
    "means",
    "black",
    "wins",
    "0",
    "means",
    "draw",
    "might",
    "imagine",
    "score",
    "means",
    "white",
    "likely",
    "win",
    "though",
    "certainly",
    "guaranteed",
    "would",
    "evaluation",
    "function",
    "estimates",
    "good",
    "game",
    "state",
    "happens",
    "depending",
    "good",
    "evaluation",
    "function",
    "ultimately",
    "going",
    "constrain",
    "good",
    "ai",
    "better",
    "ai",
    "estimating",
    "good",
    "bad",
    "particular",
    "game",
    "state",
    "better",
    "ai",
    "going",
    "able",
    "play",
    "game",
    "evaluation",
    "function",
    "worse",
    "good",
    "estimating",
    "expected",
    "utility",
    "going",
    "whole",
    "lot",
    "harder",
    "imagine",
    "trying",
    "come",
    "evaluation",
    "functions",
    "chess",
    "example",
    "might",
    "write",
    "evaluation",
    "function",
    "based",
    "many",
    "pieces",
    "compared",
    "many",
    "pieces",
    "opponent",
    "one",
    "value",
    "evaluation",
    "function",
    "probably",
    "needs",
    "little",
    "bit",
    "complicated",
    "consider",
    "possible",
    "situations",
    "might",
    "arise",
    "well",
    "many",
    "variants",
    "minimax",
    "add",
    "additional",
    "features",
    "order",
    "help",
    "perform",
    "better",
    "larger",
    "computationally",
    "untractable",
    "situations",
    "could",
    "possibly",
    "explore",
    "possible",
    "moves",
    "need",
    "figure",
    "use",
    "evaluation",
    "functions",
    "techniques",
    "able",
    "play",
    "games",
    "ultimately",
    "better",
    "look",
    "kind",
    "adversarial",
    "search",
    "search",
    "problems",
    "situations",
    "trying",
    "play",
    "sort",
    "opponent",
    "search",
    "problems",
    "show",
    "place",
    "throughout",
    "artificial",
    "intelligence",
    "talking",
    "lot",
    "today",
    "classical",
    "search",
    "problems",
    "like",
    "trying",
    "find",
    "directions",
    "one",
    "location",
    "another",
    "time",
    "ai",
    "faced",
    "trying",
    "make",
    "decision",
    "like",
    "order",
    "something",
    "rational",
    "something",
    "intelligent",
    "trying",
    "play",
    "game",
    "like",
    "figuring",
    "move",
    "make",
    "sort",
    "algorithms",
    "really",
    "come",
    "handy",
    "turns",
    "solution",
    "pretty",
    "simple",
    "small",
    "game",
    "xkcd",
    "famously",
    "put",
    "together",
    "web",
    "comic",
    "tell",
    "exactly",
    "move",
    "make",
    "optimal",
    "move",
    "make",
    "matter",
    "opponent",
    "happens",
    "type",
    "thing",
    "quite",
    "possible",
    "much",
    "larger",
    "game",
    "like",
    "checkers",
    "chess",
    "example",
    "chess",
    "totally",
    "computationally",
    "untractable",
    "computers",
    "able",
    "explore",
    "possible",
    "states",
    "really",
    "need",
    "ai",
    "far",
    "intelligent",
    "go",
    "trying",
    "deal",
    "problems",
    "go",
    "taking",
    "environment",
    "find",
    "ultimately",
    "searching",
    "one",
    "solutions",
    "look",
    "search",
    "artificial",
    "intelligence",
    "next",
    "time",
    "take",
    "look",
    "knowledge",
    "thinking",
    "ais",
    "able",
    "know",
    "information",
    "reason",
    "information",
    "draw",
    "conclusions",
    "look",
    "ai",
    "principles",
    "behind",
    "see",
    "next",
    "time",
    "aims",
    "intro",
    "music",
    "right",
    "welcome",
    "back",
    "everyone",
    "introduction",
    "artificial",
    "intelligence",
    "python",
    "last",
    "time",
    "took",
    "look",
    "search",
    "problems",
    "particular",
    "ai",
    "agents",
    "trying",
    "solve",
    "sort",
    "problem",
    "taking",
    "actions",
    "sort",
    "environment",
    "whether",
    "environment",
    "trying",
    "take",
    "actions",
    "playing",
    "moves",
    "game",
    "whether",
    "actions",
    "something",
    "like",
    "trying",
    "figure",
    "make",
    "turns",
    "order",
    "get",
    "driving",
    "directions",
    "point",
    "point",
    "time",
    "going",
    "turn",
    "attention",
    "generally",
    "idea",
    "knowledge",
    "idea",
    "lot",
    "intelligence",
    "based",
    "knowledge",
    "especially",
    "think",
    "human",
    "intelligence",
    "people",
    "know",
    "information",
    "know",
    "facts",
    "world",
    "using",
    "information",
    "know",
    "able",
    "draw",
    "conclusions",
    "reason",
    "information",
    "know",
    "order",
    "figure",
    "something",
    "figure",
    "piece",
    "information",
    "conclude",
    "based",
    "information",
    "already",
    "available",
    "us",
    "like",
    "focus",
    "ability",
    "take",
    "idea",
    "knowledge",
    "able",
    "reason",
    "based",
    "knowledge",
    "apply",
    "ideas",
    "artificial",
    "intelligence",
    "particular",
    "going",
    "building",
    "known",
    "agents",
    "agents",
    "able",
    "reason",
    "act",
    "representing",
    "knowledge",
    "internally",
    "somehow",
    "inside",
    "ai",
    "understanding",
    "means",
    "know",
    "something",
    "ideally",
    "algorithms",
    "techniques",
    "use",
    "based",
    "knowledge",
    "know",
    "order",
    "figure",
    "solution",
    "problem",
    "figure",
    "additional",
    "piece",
    "information",
    "helpful",
    "sense",
    "mean",
    "reasoning",
    "based",
    "knowledge",
    "able",
    "draw",
    "conclusions",
    "well",
    "let",
    "look",
    "simple",
    "example",
    "drawn",
    "world",
    "harry",
    "potter",
    "take",
    "one",
    "sentence",
    "know",
    "true",
    "imagine",
    "rain",
    "harry",
    "visited",
    "hagrid",
    "today",
    "one",
    "fact",
    "might",
    "know",
    "world",
    "take",
    "another",
    "fact",
    "harry",
    "visited",
    "hagrid",
    "dumbledore",
    "today",
    "tells",
    "us",
    "something",
    "world",
    "harry",
    "either",
    "visited",
    "hagrid",
    "dumbledore",
    "harry",
    "visited",
    "dumbledore",
    "hagrid",
    "third",
    "piece",
    "information",
    "world",
    "harry",
    "visited",
    "dumbledore",
    "today",
    "three",
    "pieces",
    "information",
    "three",
    "facts",
    "inside",
    "knowledge",
    "base",
    "speak",
    "information",
    "know",
    "humans",
    "try",
    "reason",
    "figure",
    "based",
    "information",
    "additional",
    "information",
    "begin",
    "conclude",
    "well",
    "looking",
    "last",
    "two",
    "statements",
    "harry",
    "either",
    "visited",
    "hagrid",
    "dumbledore",
    "know",
    "harry",
    "visited",
    "dumbledore",
    "today",
    "well",
    "pretty",
    "reasonable",
    "could",
    "draw",
    "conclusion",
    "know",
    "harry",
    "must",
    "visited",
    "hagrid",
    "today",
    "based",
    "combination",
    "two",
    "statements",
    "draw",
    "inference",
    "speak",
    "conclusion",
    "harry",
    "visit",
    "hagrid",
    "today",
    "turns",
    "even",
    "little",
    "bit",
    "better",
    "get",
    "information",
    "taking",
    "look",
    "first",
    "statement",
    "reasoning",
    "first",
    "statement",
    "says",
    "rain",
    "harry",
    "visited",
    "hagrid",
    "today",
    "mean",
    "cases",
    "rain",
    "know",
    "harry",
    "visited",
    "hagrid",
    "also",
    "know",
    "harry",
    "visit",
    "hagrid",
    "tells",
    "us",
    "something",
    "initial",
    "premise",
    "thinking",
    "particular",
    "tells",
    "us",
    "rain",
    "today",
    "reason",
    "rain",
    "harry",
    "would",
    "visited",
    "hagrid",
    "know",
    "fact",
    "harry",
    "visit",
    "hagrid",
    "today",
    "kind",
    "reason",
    "sort",
    "logical",
    "reasoning",
    "use",
    "logic",
    "based",
    "information",
    "know",
    "order",
    "take",
    "information",
    "reach",
    "conclusions",
    "going",
    "focus",
    "going",
    "talking",
    "today",
    "make",
    "artificial",
    "intelligence",
    "logical",
    "perform",
    "kinds",
    "deduction",
    "kinds",
    "reasoning",
    "far",
    "course",
    "humans",
    "reason",
    "logic",
    "generally",
    "terms",
    "human",
    "language",
    "speaking",
    "english",
    "talking",
    "english",
    "sentences",
    "trying",
    "reason",
    "relate",
    "one",
    "another",
    "going",
    "need",
    "little",
    "bit",
    "formal",
    "turn",
    "attention",
    "computers",
    "able",
    "encode",
    "notion",
    "logic",
    "truthhood",
    "falsehood",
    "inside",
    "machine",
    "going",
    "need",
    "introduce",
    "terms",
    "symbols",
    "help",
    "us",
    "reason",
    "idea",
    "logic",
    "inside",
    "artificial",
    "intelligence",
    "begin",
    "idea",
    "sentence",
    "sentence",
    "natural",
    "language",
    "like",
    "english",
    "something",
    "saying",
    "like",
    "saying",
    "right",
    "context",
    "ai",
    "though",
    "sentence",
    "assertion",
    "world",
    "going",
    "call",
    "knowledge",
    "representation",
    "language",
    "way",
    "representing",
    "knowledge",
    "inside",
    "computers",
    "way",
    "going",
    "spend",
    "today",
    "reasoning",
    "knowledge",
    "type",
    "logic",
    "known",
    "propositional",
    "logic",
    "number",
    "different",
    "types",
    "logic",
    "touch",
    "propositional",
    "logic",
    "based",
    "logic",
    "propositions",
    "statements",
    "world",
    "begin",
    "propositional",
    "logic",
    "notion",
    "propositional",
    "symbols",
    "certain",
    "symbols",
    "oftentimes",
    "letters",
    "something",
    "like",
    "p",
    "q",
    "r",
    "symbols",
    "going",
    "represent",
    "fact",
    "sentence",
    "world",
    "p",
    "example",
    "might",
    "represent",
    "fact",
    "raining",
    "p",
    "going",
    "symbol",
    "represents",
    "idea",
    "q",
    "example",
    "might",
    "represent",
    "harry",
    "visited",
    "hagrid",
    "today",
    "propositional",
    "symbols",
    "represents",
    "sentence",
    "fact",
    "world",
    "addition",
    "individual",
    "facts",
    "world",
    "want",
    "way",
    "connect",
    "propositional",
    "symbols",
    "together",
    "order",
    "reason",
    "complexly",
    "facts",
    "might",
    "exist",
    "inside",
    "world",
    "reasoning",
    "order",
    "need",
    "introduce",
    "additional",
    "symbols",
    "known",
    "logical",
    "connectives",
    "number",
    "logical",
    "connectives",
    "five",
    "important",
    "ones",
    "going",
    "focus",
    "today",
    "five",
    "represented",
    "logical",
    "symbol",
    "represented",
    "symbol",
    "represented",
    "sort",
    "upside",
    "v",
    "represented",
    "v",
    "shape",
    "implication",
    "talk",
    "means",
    "moment",
    "represented",
    "arrow",
    "biconditional",
    "talk",
    "means",
    "moment",
    "represented",
    "double",
    "arrows",
    "five",
    "logical",
    "connectives",
    "main",
    "ones",
    "going",
    "focusing",
    "terms",
    "thinking",
    "computer",
    "reason",
    "facts",
    "draw",
    "conclusions",
    "based",
    "facts",
    "knows",
    "order",
    "get",
    "need",
    "take",
    "look",
    "logical",
    "connectives",
    "build",
    "understanding",
    "actually",
    "mean",
    "let",
    "go",
    "ahead",
    "begin",
    "symbol",
    "symbol",
    "going",
    "show",
    "logical",
    "connectives",
    "going",
    "call",
    "truth",
    "table",
    "table",
    "demonstrates",
    "word",
    "means",
    "attach",
    "propositional",
    "symbol",
    "sentence",
    "inside",
    "logical",
    "language",
    "truth",
    "table",
    "shown",
    "right",
    "p",
    "propositional",
    "symbol",
    "sentence",
    "even",
    "false",
    "p",
    "true",
    "p",
    "true",
    "p",
    "false",
    "imagine",
    "placing",
    "symbol",
    "front",
    "sentence",
    "propositional",
    "logic",
    "says",
    "opposite",
    "example",
    "p",
    "represented",
    "raining",
    "p",
    "would",
    "represent",
    "idea",
    "raining",
    "might",
    "expect",
    "p",
    "false",
    "meaning",
    "sentence",
    "raining",
    "false",
    "well",
    "sentence",
    "p",
    "must",
    "true",
    "sentence",
    "raining",
    "therefore",
    "true",
    "imagine",
    "takes",
    "whatever",
    "p",
    "inverts",
    "turns",
    "false",
    "true",
    "true",
    "false",
    "much",
    "analogously",
    "english",
    "word",
    "means",
    "taking",
    "whatever",
    "comes",
    "inverting",
    "mean",
    "opposite",
    "next",
    "also",
    "idea",
    "represented",
    "v",
    "shape",
    "point",
    "shape",
    "opposed",
    "taking",
    "single",
    "argument",
    "way",
    "p",
    "going",
    "combine",
    "two",
    "different",
    "sentences",
    "propositional",
    "logic",
    "together",
    "might",
    "one",
    "sentence",
    "p",
    "another",
    "sentence",
    "q",
    "want",
    "combine",
    "together",
    "say",
    "p",
    "general",
    "logic",
    "p",
    "q",
    "means",
    "means",
    "operands",
    "true",
    "p",
    "true",
    "also",
    "q",
    "true",
    "truth",
    "table",
    "looks",
    "like",
    "time",
    "two",
    "variables",
    "p",
    "two",
    "variables",
    "two",
    "possible",
    "states",
    "true",
    "false",
    "leads",
    "two",
    "squared",
    "four",
    "possible",
    "combinations",
    "truth",
    "falsehood",
    "p",
    "false",
    "q",
    "false",
    "p",
    "false",
    "q",
    "true",
    "p",
    "true",
    "q",
    "false",
    "p",
    "q",
    "true",
    "four",
    "possibilities",
    "p",
    "q",
    "could",
    "mean",
    "situations",
    "third",
    "column",
    "p",
    "q",
    "telling",
    "us",
    "little",
    "bit",
    "actually",
    "means",
    "p",
    "q",
    "true",
    "see",
    "case",
    "p",
    "q",
    "true",
    "fourth",
    "row",
    "p",
    "happens",
    "true",
    "q",
    "also",
    "happens",
    "true",
    "situations",
    "p",
    "q",
    "going",
    "evaluate",
    "false",
    "much",
    "line",
    "intuition",
    "might",
    "mean",
    "say",
    "p",
    "q",
    "probably",
    "mean",
    "expect",
    "p",
    "q",
    "true",
    "next",
    "also",
    "potentially",
    "consistent",
    "mean",
    "word",
    "represented",
    "v",
    "shape",
    "sort",
    "upside",
    "symbol",
    "name",
    "might",
    "suggest",
    "true",
    "either",
    "arguments",
    "true",
    "long",
    "p",
    "true",
    "q",
    "true",
    "p",
    "q",
    "going",
    "true",
    "means",
    "time",
    "p",
    "q",
    "false",
    "operands",
    "false",
    "p",
    "false",
    "q",
    "false",
    "p",
    "q",
    "going",
    "false",
    "cases",
    "least",
    "one",
    "operands",
    "true",
    "maybe",
    "true",
    "case",
    "p",
    "q",
    "going",
    "evaluate",
    "true",
    "mostly",
    "consistent",
    "way",
    "people",
    "might",
    "use",
    "word",
    "sense",
    "speaking",
    "word",
    "normal",
    "english",
    "though",
    "sometimes",
    "might",
    "say",
    "mean",
    "p",
    "q",
    "mean",
    "sort",
    "one",
    "important",
    "note",
    "symbol",
    "means",
    "p",
    "q",
    "totally",
    "long",
    "either",
    "true",
    "going",
    "evaluate",
    "true",
    "well",
    "case",
    "operands",
    "false",
    "p",
    "q",
    "ultimately",
    "evaluates",
    "false",
    "well",
    "logic",
    "another",
    "symbol",
    "known",
    "exclusive",
    "encodes",
    "idea",
    "exclusivity",
    "one",
    "going",
    "focusing",
    "today",
    "whenever",
    "talk",
    "always",
    "talking",
    "either",
    "case",
    "represented",
    "truth",
    "table",
    "next",
    "might",
    "call",
    "implication",
    "denoted",
    "arrow",
    "symbol",
    "p",
    "sentence",
    "generally",
    "read",
    "p",
    "implies",
    "p",
    "implies",
    "q",
    "means",
    "p",
    "true",
    "q",
    "also",
    "true",
    "might",
    "say",
    "something",
    "like",
    "raining",
    "indoors",
    "meaning",
    "raining",
    "implies",
    "indoors",
    "logical",
    "sentence",
    "saying",
    "truth",
    "table",
    "sometimes",
    "little",
    "bit",
    "tricky",
    "obviously",
    "p",
    "true",
    "q",
    "true",
    "p",
    "implies",
    "true",
    "definitely",
    "makes",
    "sense",
    "also",
    "stand",
    "reason",
    "p",
    "true",
    "q",
    "false",
    "p",
    "implies",
    "q",
    "false",
    "said",
    "raining",
    "indoors",
    "raining",
    "indoors",
    "well",
    "would",
    "seem",
    "original",
    "statement",
    "true",
    "p",
    "implies",
    "q",
    "means",
    "p",
    "true",
    "q",
    "also",
    "needs",
    "true",
    "well",
    "statement",
    "false",
    "also",
    "worth",
    "noting",
    "though",
    "happens",
    "p",
    "false",
    "p",
    "false",
    "implication",
    "makes",
    "claim",
    "say",
    "something",
    "like",
    "raining",
    "indoors",
    "turns",
    "raining",
    "case",
    "making",
    "statement",
    "whether",
    "indoors",
    "p",
    "implies",
    "q",
    "means",
    "p",
    "true",
    "q",
    "must",
    "true",
    "p",
    "true",
    "make",
    "claim",
    "whether",
    "q",
    "true",
    "either",
    "case",
    "p",
    "false",
    "matter",
    "q",
    "whether",
    "false",
    "true",
    "making",
    "claim",
    "q",
    "whatsoever",
    "still",
    "evaluate",
    "implication",
    "true",
    "way",
    "implication",
    "ever",
    "false",
    "premise",
    "p",
    "true",
    "conclusion",
    "drawing",
    "q",
    "happens",
    "false",
    "case",
    "would",
    "say",
    "p",
    "imply",
    "q",
    "case",
    "finally",
    "last",
    "connective",
    "discuss",
    "think",
    "condition",
    "goes",
    "directions",
    "originally",
    "said",
    "something",
    "like",
    "raining",
    "indoors",
    "say",
    "would",
    "happen",
    "raining",
    "maybe",
    "indoors",
    "maybe",
    "outdoors",
    "read",
    "say",
    "indoors",
    "raining",
    "meaning",
    "raining",
    "indoors",
    "indoors",
    "reasonable",
    "conclude",
    "also",
    "raining",
    "true",
    "p",
    "q",
    "p",
    "true",
    "q",
    "true",
    "also",
    "true",
    "p",
    "implies",
    "q",
    "also",
    "reverse",
    "true",
    "q",
    "also",
    "implies",
    "p",
    "q",
    "happen",
    "false",
    "would",
    "still",
    "say",
    "true",
    "two",
    "situations",
    "p",
    "q",
    "going",
    "ultimately",
    "evaluate",
    "false",
    "lot",
    "trues",
    "falses",
    "going",
    "five",
    "basic",
    "logical",
    "connectives",
    "going",
    "form",
    "core",
    "language",
    "propositional",
    "logic",
    "language",
    "going",
    "use",
    "order",
    "describe",
    "ideas",
    "language",
    "going",
    "use",
    "order",
    "reason",
    "ideas",
    "order",
    "draw",
    "conclusions",
    "let",
    "take",
    "look",
    "additional",
    "terms",
    "need",
    "know",
    "order",
    "go",
    "trying",
    "form",
    "language",
    "propositional",
    "logic",
    "writing",
    "ai",
    "actually",
    "able",
    "understand",
    "sort",
    "logic",
    "next",
    "thing",
    "going",
    "need",
    "notion",
    "actually",
    "true",
    "world",
    "whole",
    "bunch",
    "propositional",
    "symbols",
    "p",
    "q",
    "r",
    "maybe",
    "others",
    "need",
    "way",
    "knowing",
    "actually",
    "true",
    "world",
    "p",
    "true",
    "false",
    "q",
    "true",
    "false",
    "forth",
    "introduce",
    "notion",
    "model",
    "model",
    "assigns",
    "truth",
    "value",
    "truth",
    "value",
    "either",
    "true",
    "false",
    "every",
    "propositional",
    "symbol",
    "words",
    "creating",
    "might",
    "call",
    "possible",
    "world",
    "let",
    "give",
    "example",
    "example",
    "two",
    "propositional",
    "symbols",
    "p",
    "raining",
    "q",
    "tuesday",
    "model",
    "takes",
    "two",
    "symbols",
    "assigns",
    "truth",
    "value",
    "either",
    "true",
    "false",
    "sample",
    "model",
    "model",
    "words",
    "possible",
    "world",
    "possible",
    "p",
    "true",
    "meaning",
    "raining",
    "q",
    "false",
    "meaning",
    "tuesday",
    "possible",
    "worlds",
    "models",
    "well",
    "model",
    "variables",
    "true",
    "model",
    "variables",
    "false",
    "fact",
    "n",
    "variables",
    "propositional",
    "symbols",
    "like",
    "either",
    "true",
    "false",
    "number",
    "possible",
    "models",
    "2",
    "n",
    "possible",
    "models",
    "possible",
    "variables",
    "within",
    "model",
    "could",
    "set",
    "either",
    "true",
    "false",
    "know",
    "information",
    "symbols",
    "connectives",
    "going",
    "need",
    "order",
    "construct",
    "parts",
    "knowledge",
    "need",
    "way",
    "represent",
    "knowledge",
    "going",
    "allow",
    "ai",
    "access",
    "call",
    "knowledge",
    "base",
    "knowledge",
    "base",
    "really",
    "set",
    "sentences",
    "ai",
    "knows",
    "true",
    "set",
    "sentences",
    "propositional",
    "logic",
    "things",
    "ai",
    "knows",
    "world",
    "might",
    "tell",
    "ai",
    "information",
    "information",
    "situation",
    "finds",
    "situation",
    "problem",
    "happens",
    "trying",
    "solve",
    "would",
    "give",
    "information",
    "ai",
    "ai",
    "would",
    "store",
    "inside",
    "knowledge",
    "base",
    "happens",
    "next",
    "ai",
    "would",
    "like",
    "use",
    "information",
    "knowledge",
    "base",
    "able",
    "draw",
    "conclusions",
    "rest",
    "world",
    "conclusions",
    "look",
    "like",
    "well",
    "understand",
    "conclusions",
    "need",
    "introduce",
    "one",
    "idea",
    "one",
    "symbol",
    "notion",
    "entailment",
    "sentence",
    "double",
    "turnstile",
    "greek",
    "letters",
    "greek",
    "letter",
    "alpha",
    "greek",
    "letter",
    "beta",
    "read",
    "alpha",
    "entails",
    "beta",
    "alpha",
    "beta",
    "sentences",
    "propositional",
    "logic",
    "means",
    "alpha",
    "entails",
    "beta",
    "means",
    "every",
    "model",
    "words",
    "every",
    "possible",
    "world",
    "sentence",
    "alpha",
    "true",
    "sentence",
    "beta",
    "also",
    "true",
    "something",
    "entails",
    "something",
    "else",
    "alpha",
    "entails",
    "beta",
    "means",
    "know",
    "alpha",
    "true",
    "beta",
    "must",
    "therefore",
    "also",
    "true",
    "alpha",
    "something",
    "like",
    "know",
    "tuesday",
    "january",
    "reasonable",
    "beta",
    "might",
    "something",
    "like",
    "know",
    "january",
    "worlds",
    "tuesday",
    "january",
    "know",
    "sure",
    "must",
    "january",
    "definition",
    "first",
    "statement",
    "sentence",
    "world",
    "entails",
    "second",
    "statement",
    "reasonably",
    "use",
    "deduction",
    "based",
    "first",
    "sentence",
    "figure",
    "second",
    "sentence",
    "fact",
    "true",
    "well",
    "ultimately",
    "idea",
    "entailment",
    "going",
    "try",
    "encode",
    "computer",
    "want",
    "ai",
    "agent",
    "able",
    "figure",
    "possible",
    "entailments",
    "want",
    "ai",
    "able",
    "take",
    "three",
    "sentences",
    "sentences",
    "like",
    "rain",
    "harry",
    "visited",
    "hagrid",
    "harry",
    "visited",
    "hagrid",
    "dumbledore",
    "harry",
    "visited",
    "dumbledore",
    "using",
    "information",
    "like",
    "ai",
    "able",
    "infer",
    "figure",
    "using",
    "three",
    "sentences",
    "inside",
    "knowledge",
    "base",
    "draw",
    "conclusions",
    "particular",
    "draw",
    "conclusions",
    "one",
    "harry",
    "visit",
    "hagrid",
    "today",
    "draw",
    "entailment",
    "fact",
    "rain",
    "today",
    "process",
    "known",
    "inference",
    "going",
    "focusing",
    "today",
    "process",
    "deriving",
    "new",
    "sentences",
    "old",
    "ones",
    "give",
    "three",
    "sentences",
    "put",
    "knowledge",
    "base",
    "say",
    "ai",
    "ai",
    "able",
    "use",
    "sort",
    "inference",
    "algorithm",
    "figure",
    "two",
    "sentences",
    "must",
    "also",
    "true",
    "define",
    "inference",
    "let",
    "take",
    "look",
    "inference",
    "example",
    "see",
    "might",
    "actually",
    "go",
    "inferring",
    "things",
    "human",
    "sense",
    "take",
    "algorithmic",
    "approach",
    "see",
    "could",
    "encode",
    "idea",
    "inference",
    "ai",
    "see",
    "number",
    "ways",
    "actually",
    "achieve",
    "deal",
    "couple",
    "propositional",
    "symbols",
    "deal",
    "p",
    "q",
    "p",
    "tuesday",
    "q",
    "raining",
    "r",
    "harry",
    "go",
    "run",
    "three",
    "propositional",
    "symbols",
    "defining",
    "mean",
    "saying",
    "anything",
    "yet",
    "whether",
    "true",
    "false",
    "defining",
    "give",
    "ai",
    "access",
    "knowledge",
    "base",
    "abbreviated",
    "kb",
    "knowledge",
    "know",
    "world",
    "know",
    "statement",
    "right",
    "let",
    "try",
    "parse",
    "parentheses",
    "used",
    "precedent",
    "see",
    "associates",
    "would",
    "read",
    "p",
    "q",
    "implies",
    "right",
    "mean",
    "let",
    "put",
    "piece",
    "piece",
    "p",
    "tuesday",
    "q",
    "raining",
    "q",
    "raining",
    "implies",
    "r",
    "harry",
    "go",
    "run",
    "way",
    "read",
    "entire",
    "sentence",
    "human",
    "natural",
    "language",
    "least",
    "tuesday",
    "raining",
    "harry",
    "go",
    "run",
    "tuesday",
    "raining",
    "harry",
    "go",
    "run",
    "inside",
    "knowledge",
    "base",
    "let",
    "imagine",
    "knowledge",
    "base",
    "two",
    "pieces",
    "information",
    "well",
    "information",
    "p",
    "true",
    "tuesday",
    "also",
    "information",
    "q",
    "raining",
    "sentence",
    "q",
    "raining",
    "happens",
    "false",
    "three",
    "sentences",
    "access",
    "p",
    "q",
    "implies",
    "r",
    "p",
    "using",
    "information",
    "able",
    "draw",
    "inferences",
    "p",
    "q",
    "true",
    "p",
    "q",
    "true",
    "right",
    "know",
    "p",
    "true",
    "know",
    "q",
    "true",
    "know",
    "whole",
    "expression",
    "true",
    "definition",
    "implication",
    "whole",
    "thing",
    "left",
    "true",
    "thing",
    "right",
    "must",
    "also",
    "true",
    "know",
    "p",
    "q",
    "true",
    "r",
    "must",
    "true",
    "well",
    "inference",
    "able",
    "draw",
    "r",
    "true",
    "know",
    "harry",
    "go",
    "run",
    "taking",
    "knowledge",
    "inside",
    "knowledge",
    "base",
    "able",
    "reason",
    "based",
    "idea",
    "ultimately",
    "beginning",
    "might",
    "consider",
    "sort",
    "inference",
    "algorithm",
    "process",
    "use",
    "try",
    "figure",
    "whether",
    "draw",
    "conclusion",
    "ultimately",
    "inference",
    "algorithms",
    "going",
    "answer",
    "central",
    "question",
    "entailment",
    "given",
    "query",
    "world",
    "something",
    "wondering",
    "world",
    "call",
    "query",
    "alpha",
    "question",
    "want",
    "ask",
    "using",
    "inference",
    "algorithms",
    "kb",
    "knowledge",
    "base",
    "entail",
    "alpha",
    "words",
    "using",
    "information",
    "know",
    "inside",
    "knowledge",
    "base",
    "knowledge",
    "access",
    "conclude",
    "sentence",
    "alpha",
    "true",
    "ultimately",
    "would",
    "like",
    "go",
    "writing",
    "algorithm",
    "look",
    "knowledge",
    "base",
    "figure",
    "whether",
    "query",
    "alpha",
    "actually",
    "true",
    "well",
    "turns",
    "couple",
    "different",
    "algorithms",
    "one",
    "simplest",
    "perhaps",
    "known",
    "model",
    "checking",
    "remember",
    "model",
    "assignment",
    "propositional",
    "symbols",
    "inside",
    "language",
    "truth",
    "value",
    "true",
    "false",
    "think",
    "model",
    "possible",
    "world",
    "many",
    "possible",
    "worlds",
    "different",
    "things",
    "might",
    "true",
    "false",
    "enumerate",
    "model",
    "checking",
    "algorithm",
    "exactly",
    "model",
    "checking",
    "algorithm",
    "well",
    "wanted",
    "determine",
    "knowledge",
    "base",
    "entails",
    "query",
    "alpha",
    "going",
    "enumerate",
    "possible",
    "models",
    "words",
    "consider",
    "possible",
    "values",
    "true",
    "false",
    "variables",
    "possible",
    "states",
    "world",
    "every",
    "model",
    "knowledge",
    "base",
    "true",
    "alpha",
    "also",
    "true",
    "know",
    "knowledge",
    "base",
    "entails",
    "alpha",
    "let",
    "take",
    "closer",
    "look",
    "sentence",
    "try",
    "figure",
    "actually",
    "means",
    "know",
    "every",
    "model",
    "words",
    "every",
    "possible",
    "world",
    "matter",
    "assignment",
    "true",
    "false",
    "variables",
    "give",
    "know",
    "whenever",
    "knowledge",
    "true",
    "know",
    "true",
    "true",
    "query",
    "alpha",
    "also",
    "true",
    "well",
    "stands",
    "reason",
    "long",
    "knowledge",
    "base",
    "true",
    "alpha",
    "must",
    "also",
    "true",
    "going",
    "form",
    "foundation",
    "model",
    "checking",
    "algorithm",
    "going",
    "enumerate",
    "possible",
    "worlds",
    "ask",
    "whenever",
    "knowledge",
    "base",
    "true",
    "alpha",
    "true",
    "case",
    "know",
    "alpha",
    "true",
    "otherwise",
    "entailment",
    "knowledge",
    "base",
    "entail",
    "alpha",
    "right",
    "little",
    "bit",
    "abstract",
    "let",
    "take",
    "look",
    "example",
    "try",
    "put",
    "real",
    "propositional",
    "symbols",
    "idea",
    "work",
    "example",
    "p",
    "tuesday",
    "q",
    "raining",
    "r",
    "harry",
    "go",
    "run",
    "knowledge",
    "base",
    "contains",
    "pieces",
    "information",
    "p",
    "q",
    "implies",
    "also",
    "know",
    "tuesday",
    "raining",
    "query",
    "alpha",
    "case",
    "thing",
    "want",
    "ask",
    "want",
    "know",
    "guaranteed",
    "entailed",
    "harry",
    "go",
    "run",
    "first",
    "step",
    "enumerate",
    "possible",
    "models",
    "three",
    "propositional",
    "symbols",
    "p",
    "q",
    "r",
    "means",
    "2",
    "third",
    "power",
    "eight",
    "possible",
    "models",
    "false",
    "false",
    "false",
    "true",
    "false",
    "true",
    "false",
    "false",
    "true",
    "true",
    "et",
    "cetera",
    "eight",
    "possible",
    "ways",
    "could",
    "assign",
    "true",
    "false",
    "models",
    "might",
    "ask",
    "one",
    "knowledge",
    "base",
    "true",
    "set",
    "things",
    "know",
    "worlds",
    "could",
    "knowledge",
    "base",
    "possibly",
    "apply",
    "world",
    "knowledge",
    "base",
    "true",
    "well",
    "knowledge",
    "base",
    "example",
    "know",
    "know",
    "tuesday",
    "means",
    "know",
    "four",
    "first",
    "four",
    "rows",
    "p",
    "false",
    "none",
    "going",
    "true",
    "going",
    "work",
    "particular",
    "knowledge",
    "base",
    "knowledge",
    "base",
    "true",
    "worlds",
    "likewise",
    "also",
    "know",
    "know",
    "raining",
    "models",
    "q",
    "true",
    "like",
    "two",
    "two",
    "going",
    "work",
    "either",
    "know",
    "q",
    "true",
    "finally",
    "also",
    "know",
    "p",
    "q",
    "implies",
    "r",
    "means",
    "p",
    "true",
    "p",
    "true",
    "q",
    "false",
    "q",
    "false",
    "two",
    "r",
    "must",
    "true",
    "ever",
    "p",
    "true",
    "q",
    "false",
    "r",
    "also",
    "false",
    "well",
    "satisfy",
    "implication",
    "implication",
    "hold",
    "true",
    "situations",
    "could",
    "say",
    "knowledge",
    "base",
    "conclude",
    "possible",
    "worlds",
    "knowledge",
    "base",
    "true",
    "possible",
    "worlds",
    "knowledge",
    "base",
    "false",
    "turns",
    "one",
    "possible",
    "world",
    "knowledge",
    "base",
    "actually",
    "true",
    "cases",
    "might",
    "multiple",
    "possible",
    "worlds",
    "knowledge",
    "base",
    "true",
    "case",
    "happens",
    "one",
    "one",
    "possible",
    "world",
    "definitively",
    "say",
    "something",
    "knowledge",
    "base",
    "case",
    "would",
    "look",
    "query",
    "query",
    "r",
    "r",
    "true",
    "r",
    "true",
    "result",
    "draw",
    "conclusion",
    "idea",
    "model",
    "enumerate",
    "possible",
    "models",
    "look",
    "possible",
    "models",
    "see",
    "whether",
    "knowledge",
    "base",
    "true",
    "query",
    "question",
    "true",
    "well",
    "let",
    "take",
    "look",
    "might",
    "actually",
    "go",
    "writing",
    "programming",
    "language",
    "like",
    "python",
    "take",
    "look",
    "actual",
    "code",
    "would",
    "encode",
    "notion",
    "propositional",
    "symbols",
    "logic",
    "connectives",
    "like",
    "implication",
    "forth",
    "see",
    "code",
    "might",
    "actually",
    "look",
    "like",
    "written",
    "advance",
    "logic",
    "library",
    "detailed",
    "need",
    "worry",
    "entirely",
    "today",
    "important",
    "thing",
    "one",
    "class",
    "every",
    "type",
    "logical",
    "symbol",
    "connective",
    "might",
    "one",
    "class",
    "logical",
    "symbols",
    "example",
    "every",
    "symbol",
    "going",
    "represent",
    "store",
    "name",
    "particular",
    "symbol",
    "also",
    "class",
    "takes",
    "operand",
    "might",
    "say",
    "one",
    "symbol",
    "say",
    "something",
    "true",
    "sentence",
    "true",
    "one",
    "one",
    "forth",
    "demonstrate",
    "works",
    "take",
    "look",
    "actual",
    "later",
    "go",
    "ahead",
    "call",
    "file",
    "going",
    "store",
    "information",
    "world",
    "harry",
    "potter",
    "example",
    "go",
    "ahead",
    "import",
    "logic",
    "module",
    "import",
    "everything",
    "library",
    "order",
    "create",
    "symbol",
    "use",
    "capital",
    "symbol",
    "create",
    "symbol",
    "rain",
    "mean",
    "raining",
    "example",
    "create",
    "symbol",
    "hagrid",
    "mean",
    "harry",
    "visited",
    "hagrid",
    "symbol",
    "going",
    "mean",
    "symbol",
    "means",
    "raining",
    "symbol",
    "means",
    "harry",
    "visited",
    "hagrid",
    "add",
    "another",
    "symbol",
    "called",
    "dumbledore",
    "harry",
    "visited",
    "dumbledore",
    "like",
    "save",
    "symbols",
    "use",
    "later",
    "logical",
    "analysis",
    "go",
    "ahead",
    "save",
    "one",
    "inside",
    "variable",
    "like",
    "rain",
    "hagrid",
    "dumbledore",
    "could",
    "call",
    "variables",
    "anything",
    "logical",
    "symbols",
    "use",
    "logical",
    "connectives",
    "combine",
    "together",
    "example",
    "sentence",
    "like",
    "rain",
    "hagrid",
    "example",
    "necessarily",
    "true",
    "demonstration",
    "try",
    "print",
    "function",
    "wrote",
    "takes",
    "sentence",
    "propositional",
    "logic",
    "prints",
    "programmers",
    "see",
    "order",
    "get",
    "understanding",
    "actually",
    "works",
    "run",
    "python",
    "see",
    "sentence",
    "propositional",
    "logic",
    "rain",
    "hagrid",
    "logical",
    "representation",
    "python",
    "program",
    "saying",
    "whose",
    "arguments",
    "rain",
    "hagrid",
    "saying",
    "rain",
    "hagrid",
    "encoding",
    "idea",
    "quite",
    "common",
    "python",
    "programming",
    "number",
    "different",
    "classes",
    "pass",
    "arguments",
    "order",
    "create",
    "new",
    "object",
    "example",
    "order",
    "represent",
    "idea",
    "like",
    "somehow",
    "encode",
    "knowledge",
    "world",
    "order",
    "solve",
    "problem",
    "beginning",
    "class",
    "talked",
    "trying",
    "figure",
    "harry",
    "visited",
    "trying",
    "figure",
    "raining",
    "raining",
    "knowledge",
    "go",
    "ahead",
    "create",
    "new",
    "variable",
    "called",
    "knowledge",
    "know",
    "well",
    "know",
    "first",
    "sentence",
    "talked",
    "idea",
    "raining",
    "harry",
    "visit",
    "hagrid",
    "right",
    "encode",
    "idea",
    "raining",
    "well",
    "use",
    "rain",
    "symbol",
    "saying",
    "raining",
    "implication",
    "raining",
    "harry",
    "visited",
    "hagrid",
    "wrap",
    "inside",
    "implication",
    "say",
    "raining",
    "first",
    "argument",
    "implication",
    "harry",
    "visited",
    "hagrid",
    "saying",
    "implication",
    "premise",
    "raining",
    "raining",
    "harry",
    "visited",
    "hagrid",
    "print",
    "see",
    "logical",
    "formula",
    "equivalent",
    "idea",
    "run",
    "python",
    "logical",
    "formula",
    "see",
    "result",
    "version",
    "looking",
    "raining",
    "implies",
    "harry",
    "visited",
    "hagrid",
    "additional",
    "information",
    "access",
    "well",
    "case",
    "access",
    "fact",
    "harry",
    "visited",
    "either",
    "hagrid",
    "dumbledore",
    "encode",
    "well",
    "means",
    "knowledge",
    "really",
    "got",
    "multiple",
    "pieces",
    "knowledge",
    "going",
    "know",
    "one",
    "thing",
    "another",
    "thing",
    "another",
    "thing",
    "go",
    "ahead",
    "wrap",
    "knowledge",
    "inside",
    "move",
    "things",
    "new",
    "lines",
    "good",
    "measure",
    "know",
    "multiple",
    "things",
    "saying",
    "knowledge",
    "multiple",
    "different",
    "sentences",
    "know",
    "multiple",
    "different",
    "sentences",
    "true",
    "one",
    "sentence",
    "know",
    "true",
    "implication",
    "raining",
    "harry",
    "visited",
    "hagrid",
    "another",
    "sentence",
    "know",
    "true",
    "hagrid",
    "dumbledore",
    "words",
    "hagrid",
    "dumbledore",
    "true",
    "know",
    "harry",
    "visited",
    "hagrid",
    "dumbledore",
    "know",
    "actually",
    "initial",
    "sentence",
    "said",
    "harry",
    "visited",
    "hagrid",
    "dumbledore",
    "want",
    "sentence",
    "encode",
    "idea",
    "harry",
    "visit",
    "hagrid",
    "dumbledore",
    "well",
    "notion",
    "harry",
    "visiting",
    "hagrid",
    "dumbledore",
    "would",
    "represented",
    "like",
    "hagrid",
    "dumbledore",
    "true",
    "want",
    "say",
    "wrap",
    "whole",
    "thing",
    "inside",
    "three",
    "lines",
    "line",
    "8",
    "says",
    "raining",
    "harry",
    "visited",
    "hagrid",
    "line",
    "9",
    "says",
    "harry",
    "visited",
    "hagrid",
    "dumbledore",
    "line",
    "10",
    "says",
    "harry",
    "visit",
    "hagrid",
    "dumbledore",
    "true",
    "hagrid",
    "symbol",
    "dumbledore",
    "symbol",
    "true",
    "one",
    "true",
    "finally",
    "last",
    "piece",
    "information",
    "knew",
    "fact",
    "harry",
    "visited",
    "dumbledore",
    "pieces",
    "knowledge",
    "know",
    "one",
    "sentence",
    "another",
    "sentence",
    "another",
    "another",
    "print",
    "know",
    "see",
    "little",
    "bit",
    "visually",
    "logical",
    "representation",
    "information",
    "computer",
    "internally",
    "representing",
    "using",
    "various",
    "different",
    "python",
    "objects",
    "take",
    "look",
    "want",
    "take",
    "look",
    "exactly",
    "implementing",
    "need",
    "worry",
    "much",
    "details",
    "saying",
    "raining",
    "harry",
    "visited",
    "hagrid",
    "saying",
    "hagrid",
    "dumbledore",
    "true",
    "saying",
    "case",
    "hagrid",
    "dumbledore",
    "true",
    "true",
    "also",
    "know",
    "dumbledore",
    "true",
    "long",
    "logical",
    "sentence",
    "represents",
    "knowledge",
    "base",
    "thing",
    "know",
    "like",
    "like",
    "use",
    "model",
    "checking",
    "ask",
    "query",
    "ask",
    "question",
    "like",
    "based",
    "information",
    "know",
    "whether",
    "raining",
    "humans",
    "able",
    "logic",
    "way",
    "figure",
    "right",
    "based",
    "sentences",
    "conclude",
    "figure",
    "yes",
    "must",
    "raining",
    "like",
    "computer",
    "well",
    "let",
    "take",
    "look",
    "model",
    "checking",
    "algorithm",
    "going",
    "follow",
    "pattern",
    "drew",
    "pseudocode",
    "moment",
    "ago",
    "defined",
    "function",
    "take",
    "look",
    "called",
    "model",
    "check",
    "model",
    "check",
    "takes",
    "two",
    "arguments",
    "knowledge",
    "already",
    "know",
    "query",
    "idea",
    "order",
    "model",
    "checking",
    "need",
    "enumerate",
    "possible",
    "models",
    "possible",
    "models",
    "need",
    "ask",
    "knowledge",
    "base",
    "true",
    "query",
    "true",
    "first",
    "thing",
    "need",
    "somehow",
    "enumerate",
    "possible",
    "models",
    "meaning",
    "possible",
    "symbols",
    "exist",
    "need",
    "assign",
    "true",
    "false",
    "one",
    "see",
    "whether",
    "still",
    "true",
    "way",
    "going",
    "going",
    "start",
    "defined",
    "another",
    "helper",
    "function",
    "internally",
    "get",
    "moment",
    "function",
    "starts",
    "getting",
    "symbols",
    "knowledge",
    "query",
    "figuring",
    "symbols",
    "dealing",
    "case",
    "symbols",
    "dealing",
    "rain",
    "hagrid",
    "dumbledore",
    "might",
    "symbols",
    "depending",
    "problem",
    "take",
    "look",
    "soon",
    "examples",
    "situations",
    "ultimately",
    "going",
    "need",
    "additional",
    "symbols",
    "order",
    "represent",
    "problem",
    "going",
    "run",
    "check",
    "function",
    "helper",
    "function",
    "basically",
    "going",
    "recursively",
    "call",
    "checking",
    "every",
    "possible",
    "configuration",
    "propositional",
    "symbols",
    "start",
    "looking",
    "check",
    "function",
    "symbols",
    "means",
    "finish",
    "assigning",
    "symbols",
    "assigned",
    "every",
    "symbol",
    "value",
    "far",
    "done",
    "ever",
    "check",
    "model",
    "knowledge",
    "true",
    "line",
    "saying",
    "evaluate",
    "knowledge",
    "propositional",
    "logic",
    "formula",
    "using",
    "model",
    "assignment",
    "truth",
    "values",
    "knowledge",
    "true",
    "knowledge",
    "true",
    "return",
    "true",
    "query",
    "true",
    "knowledge",
    "true",
    "want",
    "query",
    "true",
    "well",
    "order",
    "entailment",
    "otherwise",
    "know",
    "otherwise",
    "wo",
    "entailment",
    "ever",
    "situation",
    "know",
    "knowledge",
    "true",
    "query",
    "thing",
    "asking",
    "happens",
    "false",
    "line",
    "checking",
    "idea",
    "worlds",
    "knowledge",
    "true",
    "query",
    "must",
    "also",
    "true",
    "otherwise",
    "return",
    "true",
    "knowledge",
    "true",
    "care",
    "equivalent",
    "enumerating",
    "table",
    "moment",
    "ago",
    "situations",
    "knowledge",
    "base",
    "true",
    "seven",
    "rows",
    "care",
    "whether",
    "query",
    "true",
    "care",
    "check",
    "whether",
    "query",
    "true",
    "knowledge",
    "base",
    "actually",
    "true",
    "green",
    "highlighted",
    "row",
    "right",
    "logic",
    "encoded",
    "using",
    "statement",
    "otherwise",
    "assigned",
    "symbols",
    "yet",
    "seen",
    "anything",
    "yet",
    "first",
    "thing",
    "pop",
    "one",
    "symbols",
    "make",
    "copy",
    "symbols",
    "first",
    "save",
    "existing",
    "copy",
    "pop",
    "one",
    "symbol",
    "remaining",
    "symbols",
    "pick",
    "one",
    "symbol",
    "random",
    "create",
    "one",
    "copy",
    "model",
    "symbol",
    "true",
    "create",
    "second",
    "copy",
    "model",
    "symbol",
    "false",
    "two",
    "copies",
    "model",
    "one",
    "symbol",
    "true",
    "one",
    "symbol",
    "false",
    "need",
    "make",
    "sure",
    "entailment",
    "holds",
    "models",
    "recursively",
    "check",
    "model",
    "statement",
    "true",
    "check",
    "model",
    "statement",
    "false",
    "take",
    "look",
    "function",
    "try",
    "get",
    "sense",
    "exactly",
    "logic",
    "working",
    "effect",
    "recursively",
    "calling",
    "check",
    "function",
    "every",
    "level",
    "recursion",
    "saying",
    "let",
    "pick",
    "new",
    "symbol",
    "yet",
    "assigned",
    "assign",
    "true",
    "assign",
    "false",
    "check",
    "make",
    "sure",
    "entailment",
    "holds",
    "cases",
    "ultimately",
    "need",
    "check",
    "every",
    "possible",
    "world",
    "need",
    "take",
    "every",
    "combination",
    "symbols",
    "try",
    "every",
    "combination",
    "true",
    "false",
    "order",
    "figure",
    "whether",
    "entailment",
    "relation",
    "actually",
    "holds",
    "function",
    "written",
    "order",
    "use",
    "function",
    "inside",
    "write",
    "something",
    "like",
    "would",
    "like",
    "model",
    "check",
    "based",
    "knowledge",
    "provide",
    "second",
    "argument",
    "query",
    "thing",
    "want",
    "ask",
    "want",
    "ask",
    "case",
    "raining",
    "model",
    "check",
    "takes",
    "two",
    "arguments",
    "first",
    "argument",
    "information",
    "know",
    "knowledge",
    "case",
    "information",
    "given",
    "beginning",
    "second",
    "argument",
    "rain",
    "encoding",
    "idea",
    "query",
    "asking",
    "would",
    "like",
    "ask",
    "based",
    "knowledge",
    "know",
    "sure",
    "raining",
    "try",
    "print",
    "result",
    "run",
    "program",
    "see",
    "answer",
    "true",
    "based",
    "information",
    "conclusively",
    "say",
    "raining",
    "using",
    "model",
    "checking",
    "algorithm",
    "able",
    "check",
    "every",
    "world",
    "knowledge",
    "true",
    "raining",
    "words",
    "world",
    "knowledge",
    "true",
    "raining",
    "conclude",
    "fact",
    "raining",
    "sort",
    "logic",
    "applied",
    "number",
    "different",
    "types",
    "problems",
    "confronted",
    "problem",
    "sort",
    "logical",
    "deduction",
    "used",
    "order",
    "try",
    "solve",
    "might",
    "try",
    "thinking",
    "propositional",
    "symbols",
    "might",
    "need",
    "order",
    "represent",
    "information",
    "statements",
    "propositional",
    "logic",
    "might",
    "use",
    "order",
    "encode",
    "information",
    "know",
    "process",
    "trying",
    "take",
    "problem",
    "figure",
    "propositional",
    "symbols",
    "use",
    "order",
    "encode",
    "idea",
    "represent",
    "logically",
    "known",
    "knowledge",
    "engineering",
    "software",
    "engineers",
    "ai",
    "engineers",
    "take",
    "problem",
    "try",
    "figure",
    "distill",
    "knowledge",
    "representable",
    "computer",
    "take",
    "general",
    "purpose",
    "problem",
    "problem",
    "find",
    "human",
    "world",
    "turn",
    "problem",
    "computers",
    "know",
    "solve",
    "using",
    "number",
    "different",
    "variables",
    "well",
    "take",
    "computer",
    "able",
    "something",
    "like",
    "model",
    "checking",
    "inference",
    "algorithm",
    "actually",
    "figure",
    "solve",
    "problem",
    "take",
    "look",
    "two",
    "three",
    "examples",
    "knowledge",
    "engineering",
    "practice",
    "taking",
    "problem",
    "figuring",
    "apply",
    "logical",
    "symbols",
    "use",
    "logical",
    "formulas",
    "able",
    "encode",
    "idea",
    "start",
    "popular",
    "board",
    "game",
    "us",
    "uk",
    "known",
    "clue",
    "game",
    "clue",
    "number",
    "different",
    "factors",
    "going",
    "basic",
    "premise",
    "game",
    "never",
    "played",
    "number",
    "different",
    "people",
    "use",
    "three",
    "colonel",
    "mustard",
    "professor",
    "plumb",
    "miss",
    "scarlet",
    "number",
    "different",
    "rooms",
    "like",
    "ballroom",
    "kitchen",
    "library",
    "number",
    "different",
    "weapons",
    "knife",
    "revolver",
    "wrench",
    "three",
    "one",
    "person",
    "one",
    "room",
    "one",
    "weapon",
    "solution",
    "mystery",
    "murderer",
    "room",
    "weapon",
    "happened",
    "use",
    "happens",
    "beginning",
    "game",
    "cards",
    "randomly",
    "shuffled",
    "together",
    "three",
    "one",
    "person",
    "one",
    "room",
    "one",
    "weapon",
    "placed",
    "sealed",
    "envelope",
    "know",
    "would",
    "like",
    "figure",
    "using",
    "sort",
    "logical",
    "process",
    "inside",
    "envelope",
    "person",
    "room",
    "weapon",
    "looking",
    "cards",
    "looking",
    "cards",
    "try",
    "figure",
    "might",
    "going",
    "popular",
    "game",
    "let",
    "try",
    "formalize",
    "see",
    "could",
    "train",
    "computer",
    "able",
    "play",
    "game",
    "reasoning",
    "logically",
    "order",
    "begin",
    "thinking",
    "propositional",
    "symbols",
    "ultimately",
    "going",
    "need",
    "remember",
    "propositional",
    "symbols",
    "symbol",
    "variable",
    "either",
    "true",
    "false",
    "world",
    "case",
    "propositional",
    "symbols",
    "really",
    "going",
    "correspond",
    "possible",
    "things",
    "could",
    "inside",
    "envelope",
    "mustard",
    "propositional",
    "symbol",
    "case",
    "true",
    "colonel",
    "mustard",
    "inside",
    "envelope",
    "murderer",
    "false",
    "otherwise",
    "likewise",
    "plum",
    "professor",
    "plum",
    "scarlet",
    "miss",
    "scarlet",
    "likewise",
    "rooms",
    "weapons",
    "one",
    "propositional",
    "symbol",
    "ideas",
    "using",
    "propositional",
    "symbols",
    "begin",
    "create",
    "logical",
    "sentences",
    "create",
    "knowledge",
    "know",
    "world",
    "example",
    "know",
    "someone",
    "murderer",
    "one",
    "three",
    "people",
    "fact",
    "murderer",
    "would",
    "encode",
    "well",
    "know",
    "sure",
    "murderer",
    "know",
    "one",
    "person",
    "second",
    "person",
    "third",
    "person",
    "could",
    "say",
    "something",
    "like",
    "mustard",
    "plum",
    "scarlet",
    "piece",
    "knowledge",
    "encodes",
    "one",
    "three",
    "people",
    "murderer",
    "know",
    "one",
    "three",
    "things",
    "must",
    "true",
    "information",
    "know",
    "well",
    "know",
    "example",
    "one",
    "rooms",
    "must",
    "room",
    "envelope",
    "crime",
    "committed",
    "either",
    "ballroom",
    "kitchen",
    "library",
    "right",
    "know",
    "knowledge",
    "know",
    "outset",
    "knowledge",
    "one",
    "three",
    "must",
    "inside",
    "envelope",
    "likewise",
    "say",
    "thing",
    "weapon",
    "either",
    "knife",
    "revolver",
    "wrench",
    "one",
    "weapons",
    "must",
    "weapon",
    "choice",
    "therefore",
    "weapon",
    "envelope",
    "game",
    "progresses",
    "gameplay",
    "works",
    "people",
    "get",
    "various",
    "different",
    "cards",
    "using",
    "cards",
    "deduce",
    "information",
    "someone",
    "gives",
    "card",
    "example",
    "professor",
    "plum",
    "card",
    "hand",
    "know",
    "professor",
    "plum",
    "card",
    "ca",
    "inside",
    "envelope",
    "know",
    "professor",
    "plum",
    "criminal",
    "know",
    "piece",
    "information",
    "like",
    "plum",
    "example",
    "know",
    "professor",
    "plum",
    "false",
    "propositional",
    "symbol",
    "true",
    "sometimes",
    "might",
    "know",
    "sure",
    "particular",
    "card",
    "middle",
    "sometimes",
    "someone",
    "make",
    "guess",
    "know",
    "one",
    "three",
    "possibilities",
    "true",
    "someone",
    "guess",
    "colonel",
    "mustard",
    "library",
    "revolver",
    "something",
    "effect",
    "case",
    "card",
    "might",
    "revealed",
    "see",
    "card",
    "either",
    "colonel",
    "mustard",
    "revolver",
    "library",
    "know",
    "least",
    "one",
    "ca",
    "middle",
    "know",
    "something",
    "like",
    "either",
    "mustard",
    "library",
    "revolver",
    "maybe",
    "multiple",
    "true",
    "know",
    "least",
    "one",
    "mustard",
    "library",
    "revolver",
    "must",
    "fact",
    "false",
    "propositional",
    "logic",
    "representation",
    "game",
    "clue",
    "way",
    "encoding",
    "knowledge",
    "know",
    "inside",
    "game",
    "using",
    "propositional",
    "logic",
    "computer",
    "algorithm",
    "something",
    "like",
    "model",
    "checking",
    "saw",
    "moment",
    "ago",
    "actually",
    "look",
    "understand",
    "let",
    "take",
    "look",
    "code",
    "see",
    "algorithm",
    "might",
    "actually",
    "work",
    "practice",
    "right",
    "going",
    "open",
    "file",
    "called",
    "started",
    "already",
    "see",
    "defined",
    "couple",
    "things",
    "find",
    "symbols",
    "initially",
    "notice",
    "symbol",
    "colonel",
    "mustard",
    "symbol",
    "professor",
    "plum",
    "symbol",
    "miss",
    "scarlett",
    "put",
    "inside",
    "list",
    "characters",
    "symbol",
    "ballroom",
    "kitchen",
    "library",
    "inside",
    "list",
    "rooms",
    "symbols",
    "knife",
    "revolver",
    "wrench",
    "weapons",
    "characters",
    "rooms",
    "weapons",
    "altogether",
    "symbols",
    "also",
    "check",
    "knowledge",
    "function",
    "check",
    "knowledge",
    "function",
    "takes",
    "knowledge",
    "going",
    "try",
    "draw",
    "conclusions",
    "know",
    "example",
    "loop",
    "possible",
    "symbols",
    "check",
    "know",
    "symbol",
    "true",
    "symbol",
    "going",
    "something",
    "like",
    "professor",
    "plum",
    "knife",
    "library",
    "know",
    "true",
    "words",
    "know",
    "must",
    "card",
    "envelope",
    "going",
    "print",
    "using",
    "function",
    "called",
    "cprint",
    "prints",
    "things",
    "color",
    "going",
    "print",
    "word",
    "yes",
    "going",
    "print",
    "green",
    "make",
    "clear",
    "us",
    "sure",
    "symbol",
    "true",
    "maybe",
    "check",
    "see",
    "sure",
    "symbol",
    "true",
    "like",
    "know",
    "sure",
    "professor",
    "plum",
    "example",
    "running",
    "model",
    "check",
    "time",
    "checking",
    "knowledge",
    "symbol",
    "know",
    "sure",
    "symbol",
    "true",
    "know",
    "sure",
    "symbol",
    "true",
    "say",
    "model",
    "check",
    "meaning",
    "sure",
    "symbol",
    "false",
    "well",
    "go",
    "ahead",
    "print",
    "maybe",
    "next",
    "symbol",
    "maybe",
    "symbol",
    "true",
    "maybe",
    "actually",
    "know",
    "knowledge",
    "actually",
    "well",
    "let",
    "try",
    "represent",
    "knowledge",
    "knowledge",
    "know",
    "couple",
    "things",
    "put",
    "know",
    "one",
    "three",
    "people",
    "must",
    "criminal",
    "know",
    "mustard",
    "plum",
    "scarlet",
    "way",
    "encoding",
    "either",
    "colonel",
    "mustard",
    "professor",
    "plum",
    "miss",
    "scarlet",
    "know",
    "must",
    "happened",
    "one",
    "rooms",
    "know",
    "ballroom",
    "kitchen",
    "library",
    "example",
    "know",
    "one",
    "weapons",
    "must",
    "used",
    "well",
    "know",
    "knife",
    "revolver",
    "wrench",
    "might",
    "initial",
    "knowledge",
    "know",
    "must",
    "one",
    "people",
    "know",
    "must",
    "one",
    "rooms",
    "know",
    "must",
    "one",
    "weapons",
    "see",
    "knowledge",
    "looks",
    "like",
    "formula",
    "printing",
    "run",
    "python",
    "information",
    "know",
    "logical",
    "format",
    "know",
    "colonel",
    "mustard",
    "professor",
    "plum",
    "miss",
    "scarlet",
    "know",
    "ballroom",
    "kitchen",
    "library",
    "know",
    "knife",
    "revolver",
    "wrench",
    "know",
    "much",
    "ca",
    "really",
    "draw",
    "firm",
    "conclusions",
    "fact",
    "see",
    "try",
    "let",
    "go",
    "ahead",
    "run",
    "knowledge",
    "check",
    "function",
    "knowledge",
    "knowledge",
    "check",
    "function",
    "check",
    "knowledge",
    "rather",
    "function",
    "wrote",
    "looks",
    "symbols",
    "tries",
    "see",
    "conclusions",
    "actually",
    "draw",
    "symbols",
    "go",
    "ahead",
    "run",
    "see",
    "know",
    "seems",
    "really",
    "know",
    "anything",
    "sure",
    "three",
    "people",
    "maybes",
    "three",
    "rooms",
    "maybes",
    "three",
    "weapons",
    "maybes",
    "really",
    "know",
    "anything",
    "certain",
    "yet",
    "let",
    "try",
    "add",
    "additional",
    "information",
    "see",
    "additional",
    "information",
    "additional",
    "knowledge",
    "help",
    "us",
    "logically",
    "reason",
    "way",
    "process",
    "going",
    "provide",
    "information",
    "ai",
    "going",
    "take",
    "care",
    "inference",
    "figuring",
    "conclusions",
    "able",
    "draw",
    "start",
    "cards",
    "cards",
    "tell",
    "something",
    "kernel",
    "mustard",
    "card",
    "example",
    "know",
    "mustard",
    "symbol",
    "must",
    "false",
    "words",
    "mustard",
    "one",
    "envelope",
    "criminal",
    "say",
    "knowledge",
    "supports",
    "something",
    "called",
    "every",
    "library",
    "supports",
    "dot",
    "add",
    "way",
    "adding",
    "knowledge",
    "adding",
    "additional",
    "logical",
    "sentence",
    "clause",
    "say",
    "knowledge",
    "dot",
    "add",
    "mustard",
    "happen",
    "know",
    "mustard",
    "card",
    "kernel",
    "mustard",
    "suspect",
    "maybe",
    "couple",
    "cards",
    "maybe",
    "also",
    "card",
    "kitchen",
    "know",
    "kitchen",
    "maybe",
    "another",
    "card",
    "says",
    "revolver",
    "three",
    "cards",
    "kernel",
    "mustard",
    "kitchen",
    "revolver",
    "encode",
    "ai",
    "way",
    "saying",
    "kernel",
    "mustard",
    "kitchen",
    "revolver",
    "know",
    "true",
    "rerun",
    "see",
    "able",
    "eliminate",
    "possibilities",
    "sure",
    "knife",
    "revolver",
    "wrench",
    "knife",
    "maybe",
    "revolver",
    "maybe",
    "wrench",
    "maybe",
    "knife",
    "wrench",
    "two",
    "know",
    "one",
    "maybes",
    "able",
    "eliminate",
    "revolver",
    "one",
    "know",
    "false",
    "revolver",
    "card",
    "additional",
    "information",
    "might",
    "acquired",
    "course",
    "game",
    "would",
    "represent",
    "adding",
    "knowledge",
    "knowledge",
    "set",
    "knowledge",
    "base",
    "building",
    "example",
    "additionally",
    "got",
    "information",
    "someone",
    "made",
    "guess",
    "someone",
    "guessed",
    "like",
    "miss",
    "scarlet",
    "library",
    "wrench",
    "know",
    "card",
    "revealed",
    "means",
    "one",
    "three",
    "cards",
    "either",
    "miss",
    "scarlet",
    "library",
    "wrench",
    "one",
    "minimum",
    "must",
    "inside",
    "envelope",
    "could",
    "add",
    "knowledge",
    "say",
    "going",
    "add",
    "clause",
    "know",
    "sure",
    "one",
    "know",
    "one",
    "envelope",
    "either",
    "scarlet",
    "library",
    "supports",
    "multiple",
    "arguments",
    "say",
    "also",
    "wrench",
    "least",
    "one",
    "needs",
    "scarlet",
    "library",
    "wrench",
    "least",
    "one",
    "needs",
    "false",
    "know",
    "though",
    "maybe",
    "multiple",
    "maybe",
    "one",
    "least",
    "one",
    "know",
    "needs",
    "hold",
    "rerun",
    "actually",
    "additional",
    "information",
    "yet",
    "nothing",
    "say",
    "conclusively",
    "still",
    "know",
    "maybe",
    "professor",
    "plum",
    "maybe",
    "miss",
    "scarlet",
    "eliminated",
    "options",
    "let",
    "imagine",
    "get",
    "information",
    "someone",
    "shows",
    "professor",
    "plum",
    "card",
    "example",
    "say",
    "right",
    "let",
    "go",
    "back",
    "plum",
    "professor",
    "plum",
    "card",
    "know",
    "professor",
    "plum",
    "middle",
    "rerun",
    "right",
    "able",
    "draw",
    "conclusions",
    "able",
    "eliminate",
    "professor",
    "plum",
    "person",
    "could",
    "left",
    "remaining",
    "miss",
    "scarlet",
    "know",
    "yes",
    "miss",
    "scarlet",
    "variable",
    "must",
    "true",
    "able",
    "infer",
    "based",
    "information",
    "already",
    "ballroom",
    "library",
    "knife",
    "wrench",
    "two",
    "still",
    "sure",
    "let",
    "add",
    "one",
    "piece",
    "information",
    "let",
    "say",
    "know",
    "ballroom",
    "someone",
    "shown",
    "ballroom",
    "card",
    "know",
    "ballroom",
    "means",
    "point",
    "able",
    "conclude",
    "library",
    "let",
    "see",
    "say",
    "ballroom",
    "go",
    "ahead",
    "run",
    "turns",
    "conclude",
    "know",
    "library",
    "also",
    "know",
    "weapon",
    "knife",
    "might",
    "inference",
    "little",
    "bit",
    "trickier",
    "something",
    "would",
    "realized",
    "immediately",
    "ai",
    "via",
    "model",
    "checking",
    "algorithm",
    "able",
    "draw",
    "conclusion",
    "know",
    "sure",
    "must",
    "miss",
    "scarlet",
    "library",
    "knife",
    "know",
    "well",
    "know",
    "clause",
    "know",
    "either",
    "scarlet",
    "library",
    "wrench",
    "given",
    "know",
    "miss",
    "scarlet",
    "know",
    "library",
    "remaining",
    "option",
    "weapon",
    "wrench",
    "means",
    "must",
    "knife",
    "humans",
    "go",
    "back",
    "reason",
    "even",
    "though",
    "might",
    "immediately",
    "clear",
    "one",
    "advantages",
    "using",
    "ai",
    "sort",
    "algorithm",
    "order",
    "computer",
    "exhaust",
    "possibilities",
    "try",
    "figure",
    "solution",
    "actually",
    "reason",
    "often",
    "helpful",
    "able",
    "represent",
    "knowledge",
    "way",
    "knowledge",
    "engineering",
    "situation",
    "use",
    "computer",
    "able",
    "represent",
    "knowledge",
    "draw",
    "conclusions",
    "based",
    "knowledge",
    "time",
    "translate",
    "something",
    "propositional",
    "logic",
    "symbols",
    "like",
    "type",
    "approach",
    "useful",
    "might",
    "familiar",
    "logic",
    "puzzles",
    "puzzle",
    "way",
    "trying",
    "figure",
    "something",
    "classic",
    "logic",
    "puzzle",
    "might",
    "look",
    "like",
    "something",
    "like",
    "gilderoy",
    "minerva",
    "pomona",
    "horace",
    "belong",
    "different",
    "one",
    "four",
    "houses",
    "gryffindor",
    "hufflepuff",
    "ravenclaw",
    "slytherin",
    "information",
    "gilderoy",
    "belongs",
    "gryffindor",
    "ravenclaw",
    "pomona",
    "belong",
    "slytherin",
    "minerva",
    "belong",
    "gryffindor",
    "couple",
    "pieces",
    "information",
    "using",
    "information",
    "need",
    "able",
    "draw",
    "conclusions",
    "person",
    "assigned",
    "house",
    "use",
    "exact",
    "idea",
    "try",
    "implement",
    "notion",
    "need",
    "propositional",
    "symbols",
    "case",
    "propositional",
    "symbols",
    "going",
    "get",
    "little",
    "complex",
    "although",
    "see",
    "ways",
    "make",
    "little",
    "bit",
    "cleaner",
    "later",
    "need",
    "16",
    "propositional",
    "symbols",
    "one",
    "person",
    "house",
    "need",
    "say",
    "remember",
    "every",
    "propositional",
    "symbol",
    "either",
    "true",
    "false",
    "gilderoy",
    "gryffindor",
    "either",
    "true",
    "false",
    "either",
    "gryffindor",
    "likewise",
    "gilderoy",
    "hufflepuff",
    "also",
    "true",
    "false",
    "either",
    "true",
    "false",
    "true",
    "every",
    "combination",
    "person",
    "house",
    "could",
    "come",
    "sort",
    "propositional",
    "symbol",
    "one",
    "using",
    "type",
    "knowledge",
    "begin",
    "think",
    "types",
    "logical",
    "sentences",
    "say",
    "puzzle",
    "know",
    "even",
    "think",
    "information",
    "given",
    "think",
    "premise",
    "problem",
    "every",
    "person",
    "assigned",
    "different",
    "house",
    "tell",
    "us",
    "well",
    "tells",
    "us",
    "sentences",
    "like",
    "tells",
    "us",
    "like",
    "pomona",
    "slytherin",
    "implies",
    "pomona",
    "hufflepuff",
    "something",
    "like",
    "pomona",
    "slytherin",
    "know",
    "pomona",
    "hufflepuff",
    "know",
    "four",
    "people",
    "combinations",
    "houses",
    "matter",
    "person",
    "pick",
    "one",
    "house",
    "house",
    "probably",
    "whole",
    "bunch",
    "knowledge",
    "statements",
    "form",
    "know",
    "pomona",
    "slytherin",
    "know",
    "pomona",
    "hufflepuff",
    "also",
    "given",
    "information",
    "person",
    "different",
    "house",
    "also",
    "pieces",
    "knowledge",
    "look",
    "something",
    "like",
    "minerva",
    "ravenclaw",
    "implies",
    "gilderoy",
    "ravenclaw",
    "different",
    "houses",
    "minerva",
    "ravenclaw",
    "know",
    "gilderoy",
    "ravenclaw",
    "well",
    "whole",
    "bunch",
    "similar",
    "sentences",
    "like",
    "expressing",
    "idea",
    "people",
    "houses",
    "well",
    "addition",
    "sentences",
    "form",
    "also",
    "knowledge",
    "given",
    "information",
    "like",
    "gilderoy",
    "gryffindor",
    "ravenclaw",
    "would",
    "represented",
    "like",
    "gilderoy",
    "gryffindor",
    "gilderoy",
    "ravenclaw",
    "using",
    "sorts",
    "sentences",
    "begin",
    "draw",
    "conclusions",
    "world",
    "let",
    "see",
    "example",
    "go",
    "ahead",
    "actually",
    "try",
    "implement",
    "logic",
    "puzzle",
    "see",
    "figure",
    "answer",
    "go",
    "ahead",
    "open",
    "already",
    "started",
    "implement",
    "sort",
    "idea",
    "defined",
    "list",
    "people",
    "list",
    "houses",
    "far",
    "created",
    "one",
    "symbol",
    "every",
    "person",
    "every",
    "house",
    "double",
    "four",
    "loop",
    "looping",
    "people",
    "looping",
    "houses",
    "creating",
    "new",
    "symbol",
    "added",
    "information",
    "know",
    "every",
    "person",
    "belongs",
    "house",
    "added",
    "information",
    "every",
    "person",
    "person",
    "gryffindor",
    "person",
    "hufflepuff",
    "person",
    "ravenclaw",
    "person",
    "slytherin",
    "one",
    "four",
    "things",
    "must",
    "true",
    "every",
    "person",
    "belongs",
    "house",
    "information",
    "know",
    "also",
    "know",
    "one",
    "house",
    "per",
    "person",
    "person",
    "belongs",
    "multiple",
    "houses",
    "work",
    "well",
    "going",
    "true",
    "people",
    "loop",
    "every",
    "person",
    "need",
    "loop",
    "different",
    "pairs",
    "houses",
    "idea",
    "want",
    "encode",
    "idea",
    "minerva",
    "gryffindor",
    "minerva",
    "ca",
    "ravenclaw",
    "loop",
    "houses",
    "one",
    "loop",
    "houses",
    "h2",
    "long",
    "different",
    "h1",
    "equal",
    "h2",
    "add",
    "knowledge",
    "base",
    "piece",
    "information",
    "implication",
    "words",
    "person",
    "h1",
    "know",
    "house",
    "h2",
    "lines",
    "encoding",
    "notion",
    "every",
    "person",
    "belong",
    "house",
    "one",
    "house",
    "two",
    "piece",
    "logic",
    "need",
    "encode",
    "idea",
    "every",
    "house",
    "one",
    "person",
    "words",
    "pomona",
    "hufflepuff",
    "nobody",
    "else",
    "allowed",
    "hufflepuff",
    "either",
    "logic",
    "sort",
    "backwards",
    "loop",
    "houses",
    "loop",
    "different",
    "pairs",
    "people",
    "loop",
    "people",
    "loop",
    "people",
    "people",
    "different",
    "p1",
    "equal",
    "p2",
    "add",
    "knowledge",
    "given",
    "implication",
    "person",
    "one",
    "belongs",
    "house",
    "case",
    "person",
    "two",
    "belongs",
    "house",
    "encoding",
    "knowledge",
    "represents",
    "problem",
    "constraints",
    "know",
    "everyone",
    "different",
    "house",
    "know",
    "person",
    "belong",
    "one",
    "house",
    "take",
    "knowledge",
    "try",
    "print",
    "information",
    "happen",
    "know",
    "go",
    "ahead",
    "print",
    "see",
    "action",
    "go",
    "ahead",
    "skip",
    "come",
    "back",
    "second",
    "let",
    "print",
    "knowledge",
    "know",
    "running",
    "python",
    "lot",
    "information",
    "lot",
    "scroll",
    "16",
    "different",
    "variables",
    "going",
    "basic",
    "idea",
    "scroll",
    "top",
    "see",
    "initial",
    "information",
    "gilderoy",
    "either",
    "gryffindor",
    "gilderoy",
    "hufflepuff",
    "gilderoy",
    "ravenclaw",
    "gilderoy",
    "slytherin",
    "way",
    "information",
    "well",
    "quite",
    "messy",
    "really",
    "want",
    "looking",
    "soon",
    "see",
    "ways",
    "representing",
    "little",
    "bit",
    "nicely",
    "using",
    "logic",
    "say",
    "variables",
    "dealing",
    "like",
    "add",
    "information",
    "information",
    "going",
    "add",
    "gilderoy",
    "gryffindor",
    "ravenclaw",
    "knowledge",
    "given",
    "us",
    "go",
    "ahead",
    "say",
    "know",
    "either",
    "gilderoy",
    "gryffindor",
    "gilderoy",
    "ravenclaw",
    "one",
    "two",
    "things",
    "must",
    "true",
    "also",
    "know",
    "pomona",
    "slytherin",
    "say",
    "symbol",
    "symbol",
    "add",
    "knowledge",
    "minerva",
    "gryffindor",
    "adding",
    "symbol",
    "minerva",
    "gryffindor",
    "pieces",
    "knowledge",
    "know",
    "loop",
    "bottom",
    "loops",
    "symbols",
    "checks",
    "see",
    "knowledge",
    "entails",
    "symbol",
    "calling",
    "model",
    "check",
    "function",
    "know",
    "symbol",
    "true",
    "print",
    "symbol",
    "run",
    "python",
    "python",
    "going",
    "solve",
    "puzzle",
    "able",
    "conclude",
    "gilderoy",
    "belongs",
    "ravenclaw",
    "pomona",
    "belongs",
    "hufflepuff",
    "minerva",
    "gryffindor",
    "horace",
    "slytherin",
    "encoding",
    "knowledge",
    "inside",
    "computer",
    "although",
    "quite",
    "tedious",
    "case",
    "result",
    "able",
    "get",
    "conclusion",
    "well",
    "imagine",
    "applied",
    "many",
    "sorts",
    "different",
    "deductive",
    "situations",
    "situations",
    "trying",
    "deal",
    "harry",
    "potter",
    "characters",
    "puzzle",
    "ever",
    "played",
    "games",
    "like",
    "mastermind",
    "trying",
    "figure",
    "order",
    "different",
    "colors",
    "go",
    "trying",
    "make",
    "predictions",
    "could",
    "tell",
    "example",
    "let",
    "play",
    "simplified",
    "version",
    "mastermind",
    "four",
    "colors",
    "red",
    "blue",
    "green",
    "yellow",
    "order",
    "telling",
    "order",
    "make",
    "guess",
    "tell",
    "red",
    "blue",
    "green",
    "yellow",
    "many",
    "four",
    "got",
    "right",
    "position",
    "simplified",
    "version",
    "game",
    "might",
    "make",
    "guess",
    "like",
    "red",
    "blue",
    "green",
    "yellow",
    "would",
    "tell",
    "something",
    "like",
    "two",
    "four",
    "correct",
    "position",
    "two",
    "could",
    "reasonably",
    "make",
    "guess",
    "say",
    "right",
    "look",
    "blue",
    "red",
    "green",
    "yellow",
    "try",
    "switching",
    "two",
    "around",
    "time",
    "maybe",
    "tell",
    "know",
    "none",
    "correct",
    "position",
    "question",
    "right",
    "correct",
    "order",
    "four",
    "colors",
    "humans",
    "could",
    "begin",
    "reason",
    "right",
    "well",
    "none",
    "correct",
    "two",
    "correct",
    "well",
    "must",
    "switched",
    "red",
    "blue",
    "means",
    "red",
    "blue",
    "must",
    "correct",
    "means",
    "green",
    "yellow",
    "probably",
    "correct",
    "begin",
    "sort",
    "deductive",
    "reasoning",
    "also",
    "equivalently",
    "try",
    "take",
    "encode",
    "inside",
    "computer",
    "well",
    "going",
    "similar",
    "logic",
    "puzzle",
    "moment",
    "ago",
    "wo",
    "spend",
    "much",
    "time",
    "code",
    "fairly",
    "similar",
    "whole",
    "bunch",
    "colors",
    "four",
    "different",
    "positions",
    "colors",
    "additional",
    "knowledge",
    "encode",
    "knowledge",
    "take",
    "look",
    "code",
    "time",
    "want",
    "demonstrate",
    "run",
    "code",
    "run",
    "python",
    "run",
    "see",
    "get",
    "ultimately",
    "able",
    "compute",
    "red",
    "0",
    "0",
    "position",
    "blue",
    "1",
    "position",
    "yellow",
    "2",
    "position",
    "green",
    "3",
    "position",
    "ordering",
    "symbols",
    "ultimately",
    "might",
    "noticed",
    "process",
    "taking",
    "quite",
    "long",
    "time",
    "fact",
    "model",
    "checking",
    "particularly",
    "efficient",
    "algorithm",
    "right",
    "need",
    "order",
    "model",
    "check",
    "take",
    "possible",
    "different",
    "variables",
    "enumerate",
    "possibilities",
    "could",
    "n",
    "variables",
    "2",
    "n",
    "possible",
    "worlds",
    "need",
    "looking",
    "order",
    "perform",
    "model",
    "checking",
    "algorithm",
    "probably",
    "tractable",
    "especially",
    "start",
    "get",
    "much",
    "larger",
    "larger",
    "sets",
    "data",
    "many",
    "many",
    "variables",
    "play",
    "right",
    "relatively",
    "small",
    "number",
    "variables",
    "sort",
    "approach",
    "actually",
    "work",
    "number",
    "variables",
    "increases",
    "model",
    "checking",
    "becomes",
    "less",
    "less",
    "good",
    "way",
    "trying",
    "solve",
    "sorts",
    "problems",
    "might",
    "ok",
    "something",
    "like",
    "mastermind",
    "conclude",
    "indeed",
    "correct",
    "sequence",
    "four",
    "correct",
    "position",
    "like",
    "come",
    "better",
    "ways",
    "able",
    "make",
    "inferences",
    "rather",
    "enumerate",
    "possibilities",
    "transition",
    "next",
    "idea",
    "inference",
    "rules",
    "sort",
    "rules",
    "apply",
    "take",
    "knowledge",
    "already",
    "exists",
    "translate",
    "new",
    "forms",
    "knowledge",
    "general",
    "way",
    "structure",
    "inference",
    "rule",
    "horizontal",
    "line",
    "anything",
    "line",
    "going",
    "represent",
    "premise",
    "something",
    "know",
    "true",
    "anything",
    "line",
    "conclusion",
    "arrive",
    "apply",
    "logic",
    "inference",
    "rule",
    "going",
    "demonstrate",
    "inference",
    "rules",
    "demonstrating",
    "english",
    "first",
    "translating",
    "world",
    "propositional",
    "logic",
    "see",
    "inference",
    "rules",
    "actually",
    "look",
    "like",
    "example",
    "let",
    "imagine",
    "access",
    "two",
    "pieces",
    "information",
    "know",
    "example",
    "raining",
    "harry",
    "inside",
    "example",
    "let",
    "say",
    "also",
    "know",
    "raining",
    "us",
    "could",
    "reasonably",
    "look",
    "information",
    "conclude",
    "right",
    "harry",
    "must",
    "inside",
    "inference",
    "rule",
    "known",
    "modus",
    "ponens",
    "phrased",
    "formally",
    "logic",
    "know",
    "alpha",
    "implies",
    "beta",
    "words",
    "alpha",
    "beta",
    "also",
    "know",
    "alpha",
    "true",
    "able",
    "conclude",
    "beta",
    "also",
    "true",
    "apply",
    "inference",
    "rule",
    "take",
    "two",
    "pieces",
    "information",
    "generate",
    "new",
    "piece",
    "information",
    "notice",
    "totally",
    "different",
    "approach",
    "model",
    "checking",
    "approach",
    "approach",
    "look",
    "possible",
    "worlds",
    "see",
    "true",
    "worlds",
    "dealing",
    "specific",
    "world",
    "dealing",
    "knowledge",
    "know",
    "conclusions",
    "arrive",
    "based",
    "knowledge",
    "know",
    "implies",
    "b",
    "know",
    "conclusion",
    "seem",
    "like",
    "relatively",
    "obvious",
    "rule",
    "course",
    "alpha",
    "beta",
    "know",
    "alpha",
    "able",
    "conclude",
    "beta",
    "also",
    "true",
    "going",
    "true",
    "many",
    "maybe",
    "even",
    "inference",
    "rules",
    "take",
    "look",
    "able",
    "look",
    "say",
    "yeah",
    "course",
    "going",
    "true",
    "putting",
    "together",
    "figuring",
    "right",
    "combination",
    "inference",
    "rules",
    "applied",
    "ultimately",
    "going",
    "allow",
    "us",
    "generate",
    "interesting",
    "knowledge",
    "inside",
    "ai",
    "modus",
    "ponensis",
    "application",
    "implication",
    "know",
    "alpha",
    "know",
    "alpha",
    "implies",
    "beta",
    "conclude",
    "beta",
    "let",
    "take",
    "look",
    "another",
    "example",
    "fairly",
    "straightforward",
    "something",
    "like",
    "harry",
    "friends",
    "ron",
    "hermione",
    "based",
    "information",
    "reasonably",
    "conclude",
    "harry",
    "friends",
    "hermione",
    "must",
    "also",
    "true",
    "inference",
    "rule",
    "known",
    "elimination",
    "elimination",
    "says",
    "situation",
    "alpha",
    "beta",
    "true",
    "information",
    "alpha",
    "beta",
    "well",
    "alpha",
    "true",
    "likewise",
    "beta",
    "true",
    "know",
    "parts",
    "true",
    "one",
    "parts",
    "must",
    "also",
    "true",
    "something",
    "obvious",
    "point",
    "view",
    "human",
    "intuition",
    "computer",
    "needs",
    "told",
    "kind",
    "information",
    "able",
    "apply",
    "inference",
    "rule",
    "need",
    "tell",
    "computer",
    "inference",
    "rule",
    "apply",
    "computer",
    "access",
    "able",
    "use",
    "order",
    "translate",
    "information",
    "one",
    "form",
    "another",
    "addition",
    "let",
    "take",
    "look",
    "another",
    "example",
    "inference",
    "rule",
    "something",
    "like",
    "true",
    "harry",
    "pass",
    "test",
    "bit",
    "tricky",
    "sentence",
    "parse",
    "read",
    "true",
    "false",
    "harry",
    "pass",
    "test",
    "well",
    "false",
    "harry",
    "pass",
    "test",
    "reasonable",
    "conclusion",
    "harry",
    "pass",
    "test",
    "instead",
    "elimination",
    "call",
    "double",
    "negation",
    "elimination",
    "two",
    "negatives",
    "inside",
    "premise",
    "remove",
    "altogether",
    "cancel",
    "one",
    "turns",
    "true",
    "false",
    "one",
    "turns",
    "false",
    "back",
    "true",
    "phrased",
    "little",
    "bit",
    "formally",
    "say",
    "premise",
    "alpha",
    "conclusion",
    "draw",
    "alpha",
    "say",
    "alpha",
    "true",
    "take",
    "look",
    "couple",
    "raining",
    "harry",
    "inside",
    "reframe",
    "well",
    "one",
    "little",
    "bit",
    "trickier",
    "know",
    "raining",
    "harry",
    "inside",
    "conclude",
    "one",
    "two",
    "things",
    "must",
    "true",
    "either",
    "raining",
    "harry",
    "inside",
    "one",
    "trickier",
    "let",
    "think",
    "little",
    "bit",
    "first",
    "premise",
    "raining",
    "harry",
    "inside",
    "saying",
    "know",
    "raining",
    "harry",
    "must",
    "inside",
    "possible",
    "case",
    "well",
    "harry",
    "inside",
    "know",
    "must",
    "raining",
    "one",
    "two",
    "situations",
    "must",
    "true",
    "either",
    "raining",
    "raining",
    "case",
    "harry",
    "inside",
    "conclusion",
    "draw",
    "either",
    "raining",
    "raining",
    "therefore",
    "harry",
    "inside",
    "way",
    "translate",
    "statements",
    "statements",
    "known",
    "implication",
    "elimination",
    "similar",
    "actually",
    "beginning",
    "first",
    "looking",
    "first",
    "sentences",
    "harry",
    "hagrid",
    "dumbledore",
    "phrased",
    "little",
    "bit",
    "formally",
    "says",
    "implication",
    "alpha",
    "implies",
    "beta",
    "draw",
    "conclusion",
    "either",
    "alpha",
    "beta",
    "two",
    "possibilities",
    "either",
    "alpha",
    "true",
    "alpha",
    "true",
    "one",
    "possibilities",
    "alpha",
    "true",
    "alpha",
    "true",
    "well",
    "draw",
    "conclusion",
    "beta",
    "must",
    "true",
    "either",
    "alpha",
    "true",
    "alpha",
    "true",
    "case",
    "beta",
    "also",
    "true",
    "one",
    "way",
    "turn",
    "implication",
    "statement",
    "addition",
    "eliminating",
    "implications",
    "also",
    "eliminate",
    "biconditionals",
    "well",
    "let",
    "take",
    "english",
    "example",
    "something",
    "like",
    "raining",
    "harry",
    "inside",
    "really",
    "sounds",
    "like",
    "biconditional",
    "double",
    "arrow",
    "sign",
    "saw",
    "propositional",
    "logic",
    "long",
    "ago",
    "actually",
    "mean",
    "translate",
    "well",
    "means",
    "raining",
    "harry",
    "inside",
    "harry",
    "inside",
    "raining",
    "implication",
    "goes",
    "ways",
    "would",
    "call",
    "biconditional",
    "elimination",
    "take",
    "biconditional",
    "b",
    "translate",
    "something",
    "like",
    "implies",
    "b",
    "b",
    "implies",
    "many",
    "inference",
    "rules",
    "taking",
    "logic",
    "uses",
    "certain",
    "symbols",
    "turning",
    "different",
    "symbols",
    "taking",
    "implication",
    "turning",
    "taking",
    "biconditional",
    "turning",
    "implication",
    "another",
    "example",
    "would",
    "something",
    "like",
    "true",
    "harry",
    "ron",
    "passed",
    "test",
    "well",
    "right",
    "translate",
    "mean",
    "well",
    "true",
    "passed",
    "test",
    "well",
    "reasonable",
    "conclusion",
    "might",
    "draw",
    "least",
    "one",
    "pass",
    "test",
    "conclusion",
    "either",
    "harry",
    "pass",
    "test",
    "ron",
    "pass",
    "test",
    "exclusive",
    "true",
    "true",
    "harry",
    "ron",
    "passed",
    "test",
    "well",
    "either",
    "harry",
    "pass",
    "test",
    "ron",
    "pass",
    "test",
    "type",
    "law",
    "one",
    "de",
    "morgan",
    "laws",
    "quite",
    "famous",
    "logic",
    "idea",
    "turn",
    "say",
    "take",
    "harry",
    "ron",
    "passed",
    "test",
    "turn",
    "moving",
    "nots",
    "around",
    "true",
    "harry",
    "ron",
    "passed",
    "test",
    "well",
    "either",
    "harry",
    "pass",
    "test",
    "ron",
    "pass",
    "test",
    "either",
    "way",
    "frame",
    "formally",
    "using",
    "logic",
    "say",
    "true",
    "alpha",
    "beta",
    "well",
    "either",
    "alpha",
    "beta",
    "way",
    "like",
    "think",
    "negation",
    "front",
    "expression",
    "move",
    "negation",
    "inwards",
    "speak",
    "moving",
    "negation",
    "individual",
    "sentences",
    "flip",
    "negation",
    "moves",
    "inwards",
    "flips",
    "go",
    "b",
    "actually",
    "reverse",
    "de",
    "morgan",
    "law",
    "goes",
    "direction",
    "something",
    "like",
    "say",
    "true",
    "harry",
    "ron",
    "passed",
    "test",
    "meaning",
    "neither",
    "passed",
    "test",
    "well",
    "conclusion",
    "draw",
    "harry",
    "pass",
    "test",
    "ron",
    "pass",
    "test",
    "case",
    "instead",
    "turning",
    "turning",
    "idea",
    "another",
    "example",
    "de",
    "morgan",
    "laws",
    "way",
    "works",
    "b",
    "time",
    "logic",
    "going",
    "apply",
    "going",
    "move",
    "negation",
    "inwards",
    "going",
    "flip",
    "time",
    "flip",
    "b",
    "meaning",
    "true",
    "b",
    "alpha",
    "beta",
    "say",
    "alpha",
    "beta",
    "moving",
    "negation",
    "inwards",
    "order",
    "make",
    "conclusion",
    "de",
    "morgan",
    "laws",
    "couple",
    "inference",
    "rules",
    "worth",
    "taking",
    "look",
    "one",
    "distributive",
    "law",
    "works",
    "way",
    "alpha",
    "beta",
    "gamma",
    "well",
    "much",
    "way",
    "use",
    "math",
    "use",
    "distributive",
    "laws",
    "distribute",
    "operands",
    "like",
    "addition",
    "multiplication",
    "similar",
    "thing",
    "say",
    "alpha",
    "beta",
    "gamma",
    "say",
    "something",
    "like",
    "alpha",
    "beta",
    "alpha",
    "gamma",
    "able",
    "distribute",
    "sign",
    "throughout",
    "expression",
    "example",
    "distributive",
    "property",
    "distributive",
    "law",
    "applied",
    "logic",
    "much",
    "way",
    "would",
    "distribute",
    "multiplication",
    "addition",
    "something",
    "example",
    "works",
    "way",
    "example",
    "alpha",
    "beta",
    "gamma",
    "distribute",
    "throughout",
    "expression",
    "say",
    "alpha",
    "beta",
    "alpha",
    "gamma",
    "distributive",
    "law",
    "works",
    "way",
    "helpful",
    "want",
    "take",
    "move",
    "expression",
    "see",
    "example",
    "soon",
    "might",
    "actually",
    "care",
    "something",
    "like",
    "right",
    "seen",
    "lot",
    "different",
    "inference",
    "rules",
    "question",
    "use",
    "inference",
    "rules",
    "actually",
    "try",
    "draw",
    "conclusions",
    "actually",
    "try",
    "prove",
    "something",
    "entailment",
    "proving",
    "given",
    "initial",
    "knowledge",
    "base",
    "would",
    "like",
    "find",
    "way",
    "prove",
    "query",
    "true",
    "well",
    "one",
    "way",
    "think",
    "actually",
    "think",
    "back",
    "talked",
    "last",
    "time",
    "talked",
    "search",
    "problems",
    "recall",
    "search",
    "problems",
    "sort",
    "initial",
    "state",
    "actions",
    "take",
    "one",
    "state",
    "another",
    "defined",
    "transition",
    "model",
    "tells",
    "get",
    "one",
    "state",
    "another",
    "talked",
    "testing",
    "see",
    "goal",
    "path",
    "cost",
    "function",
    "see",
    "many",
    "steps",
    "take",
    "costly",
    "solution",
    "found",
    "inference",
    "rules",
    "take",
    "set",
    "sentences",
    "propositional",
    "logic",
    "get",
    "us",
    "new",
    "set",
    "sentences",
    "propositional",
    "logic",
    "actually",
    "treat",
    "sentences",
    "sets",
    "sentences",
    "states",
    "inside",
    "search",
    "problem",
    "want",
    "prove",
    "query",
    "true",
    "prove",
    "logical",
    "theorem",
    "true",
    "treat",
    "theorem",
    "proving",
    "form",
    "search",
    "problem",
    "say",
    "begin",
    "initial",
    "state",
    "initial",
    "state",
    "knowledge",
    "base",
    "begin",
    "set",
    "sentences",
    "know",
    "true",
    "actions",
    "available",
    "well",
    "actions",
    "inference",
    "rules",
    "apply",
    "given",
    "time",
    "transition",
    "model",
    "tells",
    "apply",
    "inference",
    "rule",
    "new",
    "set",
    "knowledge",
    "old",
    "set",
    "knowledge",
    "plus",
    "additional",
    "inference",
    "able",
    "draw",
    "much",
    "way",
    "saw",
    "got",
    "applied",
    "inference",
    "rules",
    "got",
    "sort",
    "conclusion",
    "conclusion",
    "gets",
    "added",
    "knowledge",
    "base",
    "transition",
    "model",
    "encode",
    "goal",
    "test",
    "well",
    "goal",
    "test",
    "checking",
    "see",
    "proved",
    "statement",
    "trying",
    "prove",
    "thing",
    "trying",
    "prove",
    "inside",
    "knowledge",
    "base",
    "path",
    "cost",
    "function",
    "thing",
    "trying",
    "minimize",
    "maybe",
    "number",
    "inference",
    "rules",
    "needed",
    "use",
    "number",
    "steps",
    "speak",
    "inside",
    "proof",
    "able",
    "apply",
    "types",
    "ideas",
    "saw",
    "last",
    "time",
    "search",
    "problems",
    "something",
    "like",
    "trying",
    "prove",
    "something",
    "knowledge",
    "taking",
    "knowledge",
    "framing",
    "terms",
    "understand",
    "search",
    "problem",
    "initial",
    "state",
    "actions",
    "transition",
    "model",
    "shows",
    "couple",
    "things",
    "one",
    "versatile",
    "search",
    "problems",
    "types",
    "algorithms",
    "use",
    "solve",
    "maze",
    "figure",
    "get",
    "point",
    "point",
    "b",
    "inside",
    "driving",
    "directions",
    "example",
    "also",
    "used",
    "theorem",
    "proving",
    "method",
    "taking",
    "sort",
    "starting",
    "knowledge",
    "base",
    "trying",
    "prove",
    "something",
    "knowledge",
    "yet",
    "second",
    "way",
    "addition",
    "model",
    "checking",
    "try",
    "prove",
    "certain",
    "statements",
    "true",
    "turns",
    "yet",
    "another",
    "way",
    "try",
    "apply",
    "inference",
    "talk",
    "way",
    "certainly",
    "one",
    "common",
    "known",
    "resolution",
    "resolution",
    "based",
    "another",
    "inference",
    "rule",
    "take",
    "look",
    "quite",
    "powerful",
    "inference",
    "rule",
    "let",
    "us",
    "prove",
    "anything",
    "proven",
    "knowledge",
    "base",
    "based",
    "basic",
    "idea",
    "let",
    "say",
    "know",
    "either",
    "ron",
    "great",
    "hall",
    "hermione",
    "library",
    "let",
    "say",
    "also",
    "know",
    "ron",
    "great",
    "hall",
    "based",
    "two",
    "pieces",
    "information",
    "conclude",
    "well",
    "could",
    "pretty",
    "reasonably",
    "conclude",
    "hermione",
    "must",
    "library",
    "know",
    "well",
    "two",
    "statements",
    "two",
    "call",
    "complementary",
    "literals",
    "literals",
    "complement",
    "opposites",
    "seem",
    "conflict",
    "sentence",
    "tells",
    "us",
    "either",
    "ron",
    "great",
    "hall",
    "hermione",
    "library",
    "know",
    "ron",
    "great",
    "hall",
    "conflicts",
    "one",
    "means",
    "hermione",
    "must",
    "library",
    "frame",
    "general",
    "rule",
    "known",
    "unit",
    "resolution",
    "rule",
    "rule",
    "says",
    "p",
    "q",
    "also",
    "know",
    "p",
    "well",
    "reasonably",
    "conclude",
    "p",
    "q",
    "true",
    "know",
    "p",
    "true",
    "possibility",
    "q",
    "true",
    "turns",
    "quite",
    "powerful",
    "inference",
    "rule",
    "terms",
    "part",
    "quickly",
    "start",
    "generalize",
    "rule",
    "q",
    "right",
    "need",
    "single",
    "propositional",
    "symbol",
    "could",
    "multiple",
    "chained",
    "together",
    "single",
    "clause",
    "call",
    "something",
    "like",
    "p",
    "q1",
    "q2",
    "q3",
    "forth",
    "qn",
    "n",
    "different",
    "variables",
    "p",
    "well",
    "happens",
    "two",
    "complement",
    "two",
    "clauses",
    "resolve",
    "speak",
    "produce",
    "new",
    "clause",
    "q1",
    "q2",
    "way",
    "qn",
    "order",
    "arguments",
    "actually",
    "matter",
    "p",
    "need",
    "first",
    "thing",
    "could",
    "middle",
    "idea",
    "p",
    "one",
    "clause",
    "p",
    "clause",
    "well",
    "know",
    "one",
    "remaining",
    "things",
    "must",
    "true",
    "resolved",
    "order",
    "produce",
    "new",
    "clause",
    "turns",
    "generalize",
    "idea",
    "even",
    "fact",
    "display",
    "even",
    "power",
    "resolution",
    "rule",
    "let",
    "take",
    "another",
    "example",
    "let",
    "say",
    "instance",
    "know",
    "piece",
    "information",
    "either",
    "ron",
    "great",
    "hall",
    "hermione",
    "library",
    "second",
    "piece",
    "information",
    "know",
    "ron",
    "great",
    "hall",
    "harry",
    "sleeping",
    "single",
    "piece",
    "information",
    "two",
    "different",
    "clauses",
    "define",
    "clauses",
    "precisely",
    "moment",
    "know",
    "well",
    "propositional",
    "symbol",
    "like",
    "ron",
    "great",
    "hall",
    "two",
    "possibilities",
    "either",
    "ron",
    "great",
    "hall",
    "case",
    "based",
    "resolution",
    "know",
    "harry",
    "must",
    "sleeping",
    "ron",
    "great",
    "hall",
    "case",
    "know",
    "based",
    "rule",
    "hermione",
    "must",
    "library",
    "based",
    "two",
    "things",
    "combination",
    "say",
    "based",
    "two",
    "premises",
    "conclude",
    "either",
    "hermione",
    "library",
    "harry",
    "sleeping",
    "two",
    "conflict",
    "know",
    "one",
    "two",
    "must",
    "true",
    "take",
    "closer",
    "look",
    "try",
    "reason",
    "logic",
    "make",
    "sure",
    "convince",
    "believe",
    "conclusion",
    "stated",
    "generally",
    "name",
    "resolution",
    "rule",
    "saying",
    "know",
    "p",
    "q",
    "true",
    "also",
    "know",
    "p",
    "r",
    "true",
    "resolve",
    "two",
    "clauses",
    "together",
    "get",
    "new",
    "clause",
    "q",
    "r",
    "either",
    "q",
    "r",
    "must",
    "true",
    "much",
    "last",
    "case",
    "q",
    "r",
    "need",
    "single",
    "propositional",
    "symbols",
    "could",
    "multiple",
    "symbols",
    "rule",
    "p",
    "q1",
    "q2",
    "q3",
    "forth",
    "qn",
    "n",
    "number",
    "likewise",
    "p",
    "r1",
    "r2",
    "forth",
    "rm",
    "number",
    "resolve",
    "two",
    "clauses",
    "together",
    "get",
    "one",
    "must",
    "true",
    "q1",
    "q2",
    "qn",
    "r1",
    "r2",
    "rm",
    "generalization",
    "rule",
    "saw",
    "things",
    "going",
    "call",
    "clause",
    "clause",
    "formally",
    "defined",
    "disjunction",
    "literals",
    "disjunction",
    "means",
    "bunch",
    "things",
    "connected",
    "disjunction",
    "means",
    "things",
    "connected",
    "conjunction",
    "meanwhile",
    "things",
    "connected",
    "literal",
    "either",
    "propositional",
    "symbol",
    "opposite",
    "propositional",
    "symbol",
    "something",
    "like",
    "p",
    "q",
    "p",
    "propositional",
    "symbols",
    "propositional",
    "symbols",
    "call",
    "literals",
    "clause",
    "something",
    "like",
    "p",
    "q",
    "r",
    "example",
    "meanwhile",
    "gives",
    "us",
    "ability",
    "gives",
    "us",
    "ability",
    "turn",
    "logic",
    "logical",
    "sentence",
    "something",
    "called",
    "conjunctive",
    "normal",
    "form",
    "conjunctive",
    "normal",
    "form",
    "sentence",
    "logical",
    "sentence",
    "conjunction",
    "clauses",
    "recall",
    "conjunction",
    "means",
    "things",
    "connected",
    "one",
    "another",
    "using",
    "conjunction",
    "clauses",
    "means",
    "individual",
    "clauses",
    "ors",
    "something",
    "like",
    "b",
    "c",
    "e",
    "f",
    "everything",
    "parentheses",
    "one",
    "clause",
    "clauses",
    "connected",
    "using",
    "everything",
    "clause",
    "separated",
    "using",
    "standard",
    "form",
    "translate",
    "logical",
    "sentence",
    "makes",
    "easy",
    "work",
    "easy",
    "manipulate",
    "turns",
    "take",
    "sentence",
    "logic",
    "turn",
    "conjunctive",
    "normal",
    "form",
    "applying",
    "inference",
    "rules",
    "transformations",
    "take",
    "look",
    "actually",
    "process",
    "taking",
    "logical",
    "formula",
    "converting",
    "conjunctive",
    "normal",
    "form",
    "otherwise",
    "known",
    "c",
    "f",
    "well",
    "process",
    "looks",
    "little",
    "something",
    "like",
    "need",
    "take",
    "symbols",
    "part",
    "conjunctive",
    "normal",
    "form",
    "implications",
    "forth",
    "turn",
    "something",
    "closely",
    "like",
    "conjunctive",
    "normal",
    "form",
    "first",
    "step",
    "eliminate",
    "double",
    "arrows",
    "know",
    "eliminate",
    "saw",
    "inference",
    "rule",
    "time",
    "expression",
    "like",
    "alpha",
    "beta",
    "turn",
    "alpha",
    "implies",
    "beta",
    "beta",
    "implies",
    "alpha",
    "based",
    "inference",
    "rule",
    "saw",
    "likewise",
    "addition",
    "eliminating",
    "eliminate",
    "implications",
    "well",
    "arrows",
    "using",
    "inference",
    "rule",
    "saw",
    "taking",
    "alpha",
    "implies",
    "beta",
    "turning",
    "alpha",
    "beta",
    "logically",
    "equivalent",
    "first",
    "thing",
    "move",
    "knots",
    "inwards",
    "want",
    "knots",
    "outsides",
    "expressions",
    "conjunctive",
    "normal",
    "form",
    "requires",
    "claws",
    "claws",
    "claws",
    "claws",
    "knots",
    "need",
    "immediately",
    "next",
    "propositional",
    "symbols",
    "move",
    "knots",
    "around",
    "using",
    "de",
    "morgan",
    "laws",
    "taking",
    "something",
    "like",
    "b",
    "turn",
    "b",
    "example",
    "using",
    "de",
    "morgan",
    "laws",
    "manipulate",
    "left",
    "ands",
    "ors",
    "easy",
    "deal",
    "use",
    "distributive",
    "law",
    "distribute",
    "ors",
    "ors",
    "end",
    "inside",
    "expression",
    "speak",
    "ands",
    "end",
    "outside",
    "general",
    "pattern",
    "take",
    "formula",
    "convert",
    "conjunctive",
    "normal",
    "form",
    "let",
    "take",
    "look",
    "example",
    "would",
    "explore",
    "would",
    "want",
    "something",
    "like",
    "let",
    "take",
    "formula",
    "example",
    "p",
    "q",
    "implies",
    "like",
    "convert",
    "conjunctive",
    "normal",
    "form",
    "ands",
    "clauses",
    "every",
    "clause",
    "disjunctive",
    "clause",
    "ors",
    "together",
    "first",
    "thing",
    "need",
    "well",
    "implication",
    "let",
    "go",
    "ahead",
    "remove",
    "implication",
    "using",
    "implication",
    "inference",
    "rule",
    "turn",
    "p",
    "q",
    "p",
    "q",
    "implies",
    "r",
    "p",
    "q",
    "first",
    "step",
    "gotten",
    "rid",
    "implication",
    "next",
    "get",
    "rid",
    "outside",
    "expression",
    "move",
    "nots",
    "inwards",
    "closer",
    "literals",
    "using",
    "de",
    "morgan",
    "laws",
    "de",
    "morgan",
    "law",
    "says",
    "p",
    "q",
    "equivalent",
    "p",
    "applying",
    "inference",
    "rules",
    "already",
    "seen",
    "order",
    "translate",
    "statements",
    "two",
    "things",
    "separated",
    "thing",
    "inside",
    "really",
    "like",
    "move",
    "ors",
    "ors",
    "inside",
    "conjunctive",
    "normal",
    "form",
    "means",
    "need",
    "clause",
    "clause",
    "clause",
    "clause",
    "use",
    "distributive",
    "law",
    "p",
    "q",
    "r",
    "distribute",
    "r",
    "get",
    "p",
    "r",
    "q",
    "r",
    "using",
    "distributive",
    "law",
    "bottom",
    "conjunctive",
    "normal",
    "form",
    "conjunction",
    "disjunctions",
    "clauses",
    "separated",
    "ors",
    "process",
    "used",
    "formula",
    "take",
    "logical",
    "sentence",
    "turn",
    "conjunctive",
    "normal",
    "form",
    "clause",
    "clause",
    "clause",
    "clause",
    "clause",
    "helpful",
    "even",
    "care",
    "taking",
    "sentences",
    "converting",
    "form",
    "form",
    "clauses",
    "clauses",
    "inputs",
    "resolution",
    "inference",
    "rule",
    "saw",
    "moment",
    "ago",
    "two",
    "clauses",
    "something",
    "conflicts",
    "something",
    "complementary",
    "two",
    "clauses",
    "resolve",
    "get",
    "new",
    "clause",
    "draw",
    "new",
    "conclusion",
    "call",
    "process",
    "inference",
    "resolution",
    "using",
    "resolution",
    "rule",
    "draw",
    "sort",
    "inference",
    "based",
    "idea",
    "p",
    "q",
    "clause",
    "p",
    "r",
    "resolve",
    "two",
    "clauses",
    "together",
    "get",
    "q",
    "r",
    "resulting",
    "clause",
    "new",
    "piece",
    "information",
    "couple",
    "key",
    "points",
    "worth",
    "noting",
    "talk",
    "actual",
    "algorithm",
    "one",
    "thing",
    "let",
    "imagine",
    "p",
    "q",
    "also",
    "p",
    "r",
    "resolution",
    "rule",
    "says",
    "p",
    "conflicts",
    "p",
    "would",
    "resolve",
    "put",
    "everything",
    "else",
    "together",
    "get",
    "q",
    "r",
    "turns",
    "double",
    "redundant",
    "change",
    "meaning",
    "sentence",
    "resolution",
    "resolution",
    "process",
    "usually",
    "also",
    "process",
    "known",
    "factoring",
    "take",
    "duplicate",
    "variables",
    "show",
    "eliminate",
    "q",
    "r",
    "becomes",
    "q",
    "r",
    "needs",
    "appear",
    "need",
    "include",
    "multiple",
    "times",
    "one",
    "final",
    "question",
    "worth",
    "considering",
    "happens",
    "try",
    "resolve",
    "p",
    "p",
    "together",
    "know",
    "p",
    "true",
    "know",
    "p",
    "true",
    "well",
    "resolution",
    "says",
    "merge",
    "clauses",
    "together",
    "look",
    "everything",
    "else",
    "well",
    "case",
    "nothing",
    "else",
    "left",
    "might",
    "call",
    "empty",
    "clause",
    "left",
    "nothing",
    "empty",
    "clause",
    "always",
    "false",
    "empty",
    "clause",
    "equivalent",
    "false",
    "pretty",
    "reasonable",
    "impossible",
    "p",
    "p",
    "hold",
    "time",
    "p",
    "either",
    "true",
    "true",
    "means",
    "p",
    "true",
    "must",
    "false",
    "true",
    "must",
    "false",
    "way",
    "hold",
    "time",
    "ever",
    "try",
    "resolve",
    "two",
    "contradiction",
    "end",
    "getting",
    "empty",
    "clause",
    "empty",
    "clause",
    "call",
    "equivalent",
    "false",
    "idea",
    "resolve",
    "two",
    "contradictory",
    "terms",
    "get",
    "empty",
    "clause",
    "basis",
    "inference",
    "resolution",
    "algorithm",
    "going",
    "perform",
    "inference",
    "resolution",
    "high",
    "level",
    "want",
    "prove",
    "knowledge",
    "base",
    "entails",
    "query",
    "alpha",
    "based",
    "knowledge",
    "prove",
    "conclusively",
    "alpha",
    "going",
    "true",
    "going",
    "well",
    "order",
    "going",
    "try",
    "prove",
    "know",
    "knowledge",
    "alpha",
    "would",
    "contradiction",
    "common",
    "technique",
    "computer",
    "science",
    "generally",
    "idea",
    "proving",
    "something",
    "contradiction",
    "want",
    "prove",
    "something",
    "true",
    "first",
    "assuming",
    "false",
    "showing",
    "would",
    "contradictory",
    "showing",
    "leads",
    "contradiction",
    "thing",
    "trying",
    "prove",
    "assume",
    "false",
    "leads",
    "contradiction",
    "must",
    "true",
    "logical",
    "approach",
    "idea",
    "behind",
    "proof",
    "contradiction",
    "going",
    "want",
    "prove",
    "query",
    "alpha",
    "true",
    "going",
    "assume",
    "true",
    "going",
    "assume",
    "alpha",
    "going",
    "try",
    "prove",
    "contradiction",
    "get",
    "contradiction",
    "well",
    "know",
    "knowledge",
    "entails",
    "query",
    "alpha",
    "get",
    "contradiction",
    "entailment",
    "idea",
    "proof",
    "contradiction",
    "assuming",
    "opposite",
    "trying",
    "prove",
    "demonstrate",
    "contradiction",
    "proving",
    "must",
    "true",
    "formally",
    "actually",
    "check",
    "knowledge",
    "base",
    "alpha",
    "going",
    "lead",
    "contradiction",
    "well",
    "resolution",
    "comes",
    "play",
    "determine",
    "knowledge",
    "base",
    "entails",
    "query",
    "alpha",
    "going",
    "convert",
    "knowledge",
    "base",
    "alpha",
    "conjunctive",
    "normal",
    "form",
    "form",
    "whole",
    "bunch",
    "clauses",
    "anded",
    "together",
    "individual",
    "clauses",
    "keep",
    "checking",
    "see",
    "use",
    "resolution",
    "produce",
    "new",
    "clause",
    "take",
    "pair",
    "clauses",
    "check",
    "literal",
    "opposite",
    "complementary",
    "example",
    "p",
    "one",
    "clause",
    "p",
    "another",
    "clause",
    "r",
    "one",
    "clause",
    "r",
    "another",
    "clause",
    "ever",
    "situation",
    "convert",
    "conjunctive",
    "normal",
    "form",
    "whole",
    "bunch",
    "clauses",
    "see",
    "two",
    "clauses",
    "resolve",
    "produce",
    "new",
    "clause",
    "process",
    "occurs",
    "loop",
    "going",
    "keep",
    "checking",
    "see",
    "use",
    "resolution",
    "produce",
    "new",
    "clause",
    "keep",
    "using",
    "new",
    "clauses",
    "try",
    "generate",
    "new",
    "clauses",
    "may",
    "happen",
    "eventually",
    "may",
    "produce",
    "empty",
    "clause",
    "clause",
    "talking",
    "resolve",
    "p",
    "p",
    "together",
    "produces",
    "empty",
    "clause",
    "empty",
    "clause",
    "know",
    "false",
    "know",
    "way",
    "p",
    "p",
    "simultaneously",
    "true",
    "ever",
    "produce",
    "empty",
    "clause",
    "contradiction",
    "contradiction",
    "exactly",
    "trying",
    "fruit",
    "contradiction",
    "contradiction",
    "know",
    "knowledge",
    "base",
    "must",
    "entail",
    "query",
    "alpha",
    "know",
    "alpha",
    "must",
    "true",
    "turns",
    "wo",
    "go",
    "proof",
    "show",
    "otherwise",
    "produce",
    "empty",
    "clause",
    "entailment",
    "run",
    "situation",
    "new",
    "clauses",
    "add",
    "done",
    "resolution",
    "yet",
    "still",
    "produced",
    "empty",
    "clause",
    "entailment",
    "case",
    "resolution",
    "algorithm",
    "abstract",
    "looking",
    "especially",
    "idea",
    "like",
    "even",
    "mean",
    "empty",
    "clause",
    "let",
    "take",
    "look",
    "example",
    "actually",
    "try",
    "prove",
    "entailment",
    "using",
    "inference",
    "resolution",
    "process",
    "question",
    "knowledge",
    "base",
    "knowledge",
    "know",
    "b",
    "b",
    "c",
    "want",
    "know",
    "entails",
    "knowledge",
    "base",
    "whole",
    "log",
    "thing",
    "query",
    "alpha",
    "propositional",
    "symbol",
    "well",
    "first",
    "want",
    "prove",
    "contradiction",
    "want",
    "first",
    "assume",
    "false",
    "see",
    "leads",
    "sort",
    "contradiction",
    "going",
    "start",
    "b",
    "b",
    "c",
    "knowledge",
    "base",
    "going",
    "assume",
    "going",
    "assume",
    "thing",
    "trying",
    "prove",
    "fact",
    "false",
    "conjunctive",
    "normal",
    "form",
    "four",
    "different",
    "clauses",
    "b",
    "c",
    "begin",
    "pick",
    "two",
    "clauses",
    "resolve",
    "apply",
    "resolution",
    "rule",
    "looking",
    "four",
    "clauses",
    "see",
    "right",
    "two",
    "clauses",
    "ones",
    "resolve",
    "resolve",
    "complementary",
    "literals",
    "show",
    "c",
    "c",
    "looking",
    "two",
    "clauses",
    "know",
    "b",
    "c",
    "true",
    "know",
    "c",
    "true",
    "well",
    "resolve",
    "two",
    "clauses",
    "say",
    "right",
    "b",
    "must",
    "true",
    "generate",
    "new",
    "clause",
    "new",
    "piece",
    "information",
    "know",
    "true",
    "right",
    "repeat",
    "process",
    "process",
    "use",
    "resolution",
    "get",
    "new",
    "conclusion",
    "well",
    "turns",
    "use",
    "new",
    "clause",
    "generated",
    "along",
    "one",
    "complementary",
    "literals",
    "b",
    "complementary",
    "conflicts",
    "b",
    "know",
    "b",
    "true",
    "know",
    "b",
    "true",
    "well",
    "remaining",
    "possibility",
    "must",
    "true",
    "new",
    "clause",
    "able",
    "generate",
    "one",
    "time",
    "looking",
    "two",
    "clauses",
    "resolved",
    "might",
    "programmatically",
    "looping",
    "possible",
    "pairs",
    "clauses",
    "checking",
    "complementary",
    "literals",
    "say",
    "right",
    "found",
    "two",
    "clauses",
    "conflict",
    "resolve",
    "two",
    "together",
    "well",
    "resolving",
    "p",
    "p",
    "resolve",
    "two",
    "clauses",
    "together",
    "get",
    "rid",
    "left",
    "empty",
    "clause",
    "empty",
    "clause",
    "know",
    "false",
    "means",
    "contradiction",
    "means",
    "safely",
    "say",
    "whole",
    "knowledge",
    "base",
    "entail",
    "sentence",
    "true",
    "know",
    "sure",
    "also",
    "true",
    "using",
    "inference",
    "resolution",
    "entirely",
    "different",
    "way",
    "take",
    "statement",
    "try",
    "prove",
    "fact",
    "true",
    "instead",
    "enumerating",
    "possible",
    "worlds",
    "might",
    "order",
    "try",
    "figure",
    "cases",
    "knowledge",
    "base",
    "true",
    "cases",
    "query",
    "true",
    "instead",
    "use",
    "resolution",
    "algorithm",
    "say",
    "let",
    "keep",
    "trying",
    "figure",
    "conclusions",
    "draw",
    "see",
    "reach",
    "contradiction",
    "reach",
    "contradiction",
    "tells",
    "us",
    "something",
    "whether",
    "knowledge",
    "actually",
    "entails",
    "query",
    "turns",
    "many",
    "different",
    "algorithms",
    "used",
    "inference",
    "looked",
    "couple",
    "fact",
    "based",
    "one",
    "particular",
    "type",
    "logic",
    "based",
    "propositional",
    "logic",
    "individual",
    "symbols",
    "connect",
    "using",
    "implies",
    "conditionals",
    "propositional",
    "logic",
    "kind",
    "logic",
    "exists",
    "fact",
    "see",
    "limitations",
    "exist",
    "propositional",
    "logic",
    "especially",
    "saw",
    "examples",
    "like",
    "mastermind",
    "example",
    "example",
    "logic",
    "puzzle",
    "different",
    "hogwarts",
    "house",
    "people",
    "belong",
    "different",
    "houses",
    "trying",
    "figure",
    "belonged",
    "houses",
    "lot",
    "different",
    "propositional",
    "symbols",
    "needed",
    "order",
    "represent",
    "fairly",
    "basic",
    "ideas",
    "final",
    "topic",
    "take",
    "look",
    "end",
    "class",
    "today",
    "one",
    "final",
    "type",
    "logic",
    "different",
    "propositional",
    "logic",
    "known",
    "first",
    "order",
    "logic",
    "little",
    "bit",
    "powerful",
    "propositional",
    "logic",
    "going",
    "make",
    "easier",
    "us",
    "express",
    "certain",
    "types",
    "ideas",
    "propositional",
    "logic",
    "think",
    "back",
    "puzzle",
    "people",
    "hogwarts",
    "houses",
    "whole",
    "bunch",
    "symbols",
    "every",
    "symbol",
    "could",
    "true",
    "false",
    "symbol",
    "minerva",
    "gryffindor",
    "either",
    "true",
    "minerva",
    "within",
    "gryffindor",
    "false",
    "otherwise",
    "likewise",
    "minerva",
    "hufflepuff",
    "minerva",
    "ravenclaw",
    "minerva",
    "slytherin",
    "forth",
    "starting",
    "get",
    "quite",
    "redundant",
    "wanted",
    "way",
    "able",
    "express",
    "relationship",
    "propositional",
    "symbols",
    "minerva",
    "shows",
    "also",
    "would",
    "liked",
    "many",
    "different",
    "symbols",
    "represent",
    "really",
    "fairly",
    "straightforward",
    "problem",
    "first",
    "order",
    "logic",
    "give",
    "us",
    "different",
    "way",
    "trying",
    "deal",
    "idea",
    "giving",
    "us",
    "two",
    "different",
    "types",
    "symbols",
    "going",
    "constant",
    "symbols",
    "going",
    "represent",
    "objects",
    "like",
    "people",
    "houses",
    "predicate",
    "symbols",
    "think",
    "relations",
    "functions",
    "take",
    "input",
    "evaluate",
    "true",
    "false",
    "example",
    "tell",
    "us",
    "whether",
    "property",
    "constant",
    "pair",
    "constants",
    "multiple",
    "constants",
    "actually",
    "holds",
    "see",
    "example",
    "moment",
    "problem",
    "constant",
    "symbols",
    "might",
    "objects",
    "things",
    "like",
    "people",
    "houses",
    "minerva",
    "pomona",
    "horace",
    "gilderoy",
    "constant",
    "symbols",
    "four",
    "houses",
    "gryffindor",
    "hufflepuff",
    "ravenclaw",
    "slytherin",
    "predicates",
    "meanwhile",
    "predicate",
    "symbols",
    "going",
    "properties",
    "might",
    "hold",
    "true",
    "false",
    "individual",
    "constants",
    "person",
    "might",
    "hold",
    "true",
    "minerva",
    "would",
    "false",
    "gryffindor",
    "gryffindor",
    "person",
    "house",
    "going",
    "hold",
    "true",
    "ravenclaw",
    "going",
    "hold",
    "true",
    "horace",
    "example",
    "horace",
    "person",
    "belongs",
    "meanwhile",
    "going",
    "relation",
    "going",
    "relate",
    "people",
    "houses",
    "going",
    "tell",
    "someone",
    "belongs",
    "house",
    "let",
    "take",
    "look",
    "examples",
    "sentence",
    "first",
    "order",
    "logic",
    "might",
    "actually",
    "look",
    "like",
    "sentence",
    "might",
    "look",
    "like",
    "something",
    "like",
    "person",
    "minerva",
    "minerva",
    "parentheses",
    "person",
    "predicate",
    "symbol",
    "minerva",
    "constant",
    "symbol",
    "sentence",
    "first",
    "order",
    "logic",
    "effectively",
    "means",
    "minerva",
    "person",
    "person",
    "property",
    "applies",
    "minerva",
    "object",
    "want",
    "say",
    "something",
    "like",
    "minerva",
    "person",
    "express",
    "idea",
    "using",
    "first",
    "order",
    "logic",
    "meanwhile",
    "say",
    "something",
    "like",
    "house",
    "gryffindor",
    "likewise",
    "express",
    "idea",
    "gryffindor",
    "house",
    "way",
    "logical",
    "connectives",
    "saw",
    "propositional",
    "logic",
    "going",
    "work",
    "implication",
    "conditional",
    "fact",
    "use",
    "say",
    "something",
    "like",
    "house",
    "minerva",
    "sentence",
    "first",
    "order",
    "logic",
    "means",
    "something",
    "like",
    "minerva",
    "house",
    "true",
    "house",
    "property",
    "applies",
    "minerva",
    "meanwhile",
    "addition",
    "predicate",
    "symbols",
    "take",
    "single",
    "argument",
    "predicate",
    "symbols",
    "going",
    "express",
    "binary",
    "relations",
    "relations",
    "two",
    "arguments",
    "could",
    "say",
    "something",
    "like",
    "belongs",
    "two",
    "inputs",
    "minerva",
    "gryffindor",
    "express",
    "idea",
    "minerva",
    "belongs",
    "gryffindor",
    "key",
    "difference",
    "one",
    "key",
    "differences",
    "propositional",
    "logic",
    "propositional",
    "logic",
    "needed",
    "one",
    "symbol",
    "minerva",
    "gryffindor",
    "one",
    "symbol",
    "minerva",
    "hufflepuff",
    "one",
    "symbol",
    "people",
    "gryffindor",
    "hufflepuff",
    "variables",
    "case",
    "need",
    "one",
    "symbol",
    "people",
    "one",
    "symbol",
    "houses",
    "express",
    "predicate",
    "something",
    "like",
    "belongs",
    "say",
    "belongs",
    "minerva",
    "gryffindor",
    "express",
    "idea",
    "minerva",
    "belongs",
    "gryffindor",
    "house",
    "already",
    "see",
    "first",
    "order",
    "logic",
    "quite",
    "expressive",
    "able",
    "express",
    "sorts",
    "sentences",
    "using",
    "existing",
    "constant",
    "symbols",
    "predicates",
    "already",
    "exist",
    "minimizing",
    "number",
    "new",
    "symbols",
    "need",
    "create",
    "use",
    "eight",
    "symbols",
    "people",
    "houses",
    "instead",
    "16",
    "symbols",
    "every",
    "possible",
    "combination",
    "first",
    "order",
    "logic",
    "gives",
    "us",
    "couple",
    "additional",
    "features",
    "use",
    "express",
    "even",
    "complex",
    "ideas",
    "additional",
    "features",
    "generally",
    "known",
    "quantifiers",
    "two",
    "main",
    "quantifiers",
    "first",
    "order",
    "logic",
    "first",
    "universal",
    "quantification",
    "universal",
    "quantification",
    "lets",
    "express",
    "idea",
    "like",
    "something",
    "going",
    "true",
    "values",
    "variable",
    "like",
    "values",
    "x",
    "statement",
    "going",
    "hold",
    "true",
    "might",
    "sentence",
    "universal",
    "quantification",
    "look",
    "like",
    "well",
    "going",
    "use",
    "upside",
    "mean",
    "upside",
    "ax",
    "means",
    "values",
    "x",
    "x",
    "object",
    "going",
    "hold",
    "true",
    "belongs",
    "x",
    "gryffindor",
    "implies",
    "belongs",
    "x",
    "hufflepuff",
    "let",
    "try",
    "parse",
    "means",
    "values",
    "x",
    "holds",
    "true",
    "x",
    "belongs",
    "gryffindor",
    "hold",
    "true",
    "x",
    "belong",
    "hufflepuff",
    "translated",
    "english",
    "sentence",
    "saying",
    "something",
    "like",
    "objects",
    "x",
    "x",
    "belongs",
    "gryffindor",
    "x",
    "belong",
    "hufflepuff",
    "example",
    "phrase",
    "even",
    "simply",
    "anyone",
    "gryffindor",
    "hufflepuff",
    "simplified",
    "way",
    "saying",
    "thing",
    "universal",
    "quantification",
    "lets",
    "us",
    "express",
    "idea",
    "like",
    "something",
    "going",
    "hold",
    "true",
    "values",
    "particular",
    "variable",
    "addition",
    "universal",
    "quantification",
    "though",
    "also",
    "existential",
    "quantification",
    "whereas",
    "universal",
    "quantification",
    "said",
    "something",
    "going",
    "true",
    "values",
    "variable",
    "existential",
    "quantification",
    "says",
    "expression",
    "going",
    "true",
    "value",
    "variable",
    "least",
    "one",
    "value",
    "variable",
    "let",
    "take",
    "look",
    "sample",
    "sentence",
    "using",
    "existential",
    "quantification",
    "one",
    "sentence",
    "looks",
    "like",
    "exists",
    "backwards",
    "e",
    "stands",
    "exists",
    "saying",
    "exists",
    "x",
    "house",
    "x",
    "belongs",
    "minerva",
    "words",
    "exists",
    "object",
    "x",
    "x",
    "house",
    "minerva",
    "belongs",
    "phrased",
    "little",
    "succinctly",
    "english",
    "saying",
    "minerva",
    "belongs",
    "house",
    "object",
    "house",
    "minerva",
    "belongs",
    "house",
    "combining",
    "universal",
    "existential",
    "quantification",
    "create",
    "far",
    "sophisticated",
    "logical",
    "statements",
    "able",
    "using",
    "propositional",
    "logic",
    "could",
    "combine",
    "say",
    "something",
    "like",
    "x",
    "person",
    "x",
    "implies",
    "exists",
    "house",
    "belongs",
    "xy",
    "right",
    "lot",
    "stuff",
    "going",
    "lot",
    "symbols",
    "let",
    "try",
    "parse",
    "understand",
    "saying",
    "saying",
    "values",
    "x",
    "x",
    "person",
    "true",
    "words",
    "saying",
    "people",
    "call",
    "person",
    "x",
    "statement",
    "going",
    "true",
    "statement",
    "true",
    "people",
    "well",
    "exists",
    "house",
    "exists",
    "house",
    "x",
    "belongs",
    "words",
    "saying",
    "people",
    "exists",
    "house",
    "x",
    "person",
    "belongs",
    "house",
    "phrased",
    "succinctly",
    "saying",
    "every",
    "person",
    "belongs",
    "house",
    "x",
    "x",
    "person",
    "exists",
    "house",
    "x",
    "belongs",
    "express",
    "lot",
    "powerful",
    "ideas",
    "using",
    "idea",
    "first",
    "order",
    "logic",
    "turns",
    "many",
    "kinds",
    "logic",
    "second",
    "order",
    "logic",
    "higher",
    "order",
    "logic",
    "allows",
    "us",
    "express",
    "complex",
    "ideas",
    "case",
    "really",
    "pursuit",
    "goal",
    "representation",
    "knowledge",
    "want",
    "ai",
    "agents",
    "able",
    "know",
    "information",
    "represent",
    "information",
    "whether",
    "using",
    "propositional",
    "logic",
    "first",
    "order",
    "logic",
    "logic",
    "able",
    "reason",
    "based",
    "able",
    "draw",
    "conclusions",
    "make",
    "inferences",
    "figure",
    "whether",
    "sort",
    "entailment",
    "relationship",
    "using",
    "sort",
    "inference",
    "algorithm",
    "something",
    "like",
    "inference",
    "resolution",
    "model",
    "checking",
    "number",
    "algorithms",
    "use",
    "order",
    "take",
    "information",
    "know",
    "translate",
    "additional",
    "conclusions",
    "helped",
    "us",
    "create",
    "ai",
    "able",
    "represent",
    "information",
    "knows",
    "know",
    "next",
    "time",
    "though",
    "take",
    "look",
    "make",
    "ai",
    "even",
    "powerful",
    "encoding",
    "information",
    "know",
    "sure",
    "true",
    "true",
    "also",
    "take",
    "look",
    "uncertainty",
    "look",
    "happens",
    "ai",
    "thinks",
    "something",
    "might",
    "probable",
    "maybe",
    "probable",
    "somewhere",
    "two",
    "extremes",
    "pursuit",
    "trying",
    "build",
    "intelligent",
    "systems",
    "even",
    "intelligent",
    "see",
    "next",
    "time",
    "thank",
    "right",
    "welcome",
    "back",
    "everyone",
    "introduction",
    "artificial",
    "intelligence",
    "python",
    "last",
    "time",
    "took",
    "look",
    "ai",
    "inside",
    "computers",
    "represent",
    "knowledge",
    "represented",
    "knowledge",
    "form",
    "logical",
    "sentences",
    "variety",
    "different",
    "logical",
    "languages",
    "idea",
    "wanted",
    "ai",
    "able",
    "represent",
    "knowledge",
    "information",
    "somehow",
    "use",
    "pieces",
    "information",
    "able",
    "derive",
    "new",
    "pieces",
    "information",
    "inference",
    "able",
    "take",
    "information",
    "deduce",
    "additional",
    "conclusions",
    "based",
    "information",
    "already",
    "knew",
    "sure",
    "reality",
    "think",
    "computers",
    "think",
    "ai",
    "rarely",
    "machines",
    "going",
    "able",
    "know",
    "things",
    "sure",
    "oftentimes",
    "going",
    "amount",
    "uncertainty",
    "information",
    "ais",
    "computers",
    "dealing",
    "might",
    "believe",
    "something",
    "probability",
    "soon",
    "discuss",
    "probability",
    "means",
    "entirely",
    "certain",
    "want",
    "use",
    "information",
    "knowledge",
    "even",
    "perfect",
    "knowledge",
    "still",
    "able",
    "make",
    "inferences",
    "still",
    "able",
    "draw",
    "conclusions",
    "might",
    "imagine",
    "example",
    "context",
    "robot",
    "sensors",
    "exploring",
    "environment",
    "might",
    "know",
    "exactly",
    "exactly",
    "around",
    "access",
    "data",
    "allow",
    "draw",
    "inferences",
    "probability",
    "likelihood",
    "one",
    "thing",
    "true",
    "another",
    "imagine",
    "context",
    "little",
    "bit",
    "randomness",
    "uncertainty",
    "something",
    "like",
    "predicting",
    "weather",
    "might",
    "able",
    "know",
    "sure",
    "tomorrow",
    "weather",
    "100",
    "certainty",
    "probably",
    "infer",
    "probability",
    "tomorrow",
    "weather",
    "going",
    "based",
    "maybe",
    "today",
    "weather",
    "yesterday",
    "weather",
    "data",
    "might",
    "access",
    "well",
    "oftentimes",
    "distill",
    "terms",
    "possible",
    "events",
    "might",
    "happen",
    "likelihood",
    "events",
    "comes",
    "lot",
    "games",
    "example",
    "element",
    "chance",
    "inside",
    "games",
    "imagine",
    "rolling",
    "dice",
    "sure",
    "exactly",
    "die",
    "roll",
    "going",
    "know",
    "going",
    "one",
    "possibilities",
    "1",
    "6",
    "example",
    "introduce",
    "idea",
    "probability",
    "theory",
    "take",
    "look",
    "today",
    "beginning",
    "looking",
    "mathematical",
    "foundations",
    "probability",
    "theory",
    "getting",
    "understanding",
    "key",
    "concepts",
    "within",
    "probability",
    "diving",
    "use",
    "probability",
    "ideas",
    "look",
    "mathematically",
    "represent",
    "ideas",
    "terms",
    "models",
    "put",
    "computers",
    "order",
    "program",
    "ai",
    "able",
    "use",
    "information",
    "probability",
    "draw",
    "inferences",
    "make",
    "judgments",
    "world",
    "probability",
    "likelihood",
    "true",
    "probability",
    "ultimately",
    "boils",
    "idea",
    "possible",
    "worlds",
    "representing",
    "using",
    "little",
    "greek",
    "letter",
    "omega",
    "idea",
    "possible",
    "world",
    "roll",
    "die",
    "six",
    "possible",
    "worlds",
    "could",
    "result",
    "could",
    "roll",
    "1",
    "2",
    "3",
    "4",
    "5",
    "possible",
    "world",
    "possible",
    "worlds",
    "probability",
    "true",
    "probability",
    "roll",
    "1",
    "2",
    "3",
    "something",
    "else",
    "represent",
    "probability",
    "like",
    "using",
    "capital",
    "letter",
    "parentheses",
    "want",
    "probability",
    "right",
    "would",
    "probability",
    "possible",
    "world",
    "represented",
    "little",
    "letter",
    "omega",
    "couple",
    "basic",
    "axioms",
    "probability",
    "become",
    "relevant",
    "consider",
    "deal",
    "probability",
    "think",
    "first",
    "foremost",
    "every",
    "probability",
    "value",
    "must",
    "range",
    "0",
    "1",
    "inclusive",
    "smallest",
    "value",
    "probability",
    "number",
    "0",
    "impossible",
    "event",
    "something",
    "like",
    "roll",
    "die",
    "die",
    "7",
    "roll",
    "get",
    "die",
    "numbers",
    "1",
    "6",
    "event",
    "roll",
    "7",
    "impossible",
    "would",
    "probability",
    "end",
    "spectrum",
    "probability",
    "range",
    "way",
    "positive",
    "number",
    "1",
    "meaning",
    "event",
    "certain",
    "happen",
    "roll",
    "die",
    "number",
    "less",
    "10",
    "example",
    "event",
    "guaranteed",
    "happen",
    "sides",
    "die",
    "1",
    "6",
    "instance",
    "range",
    "real",
    "number",
    "two",
    "values",
    "generally",
    "speaking",
    "higher",
    "value",
    "probability",
    "means",
    "event",
    "likely",
    "take",
    "place",
    "lower",
    "value",
    "probability",
    "means",
    "event",
    "less",
    "likely",
    "take",
    "place",
    "key",
    "rule",
    "probability",
    "looks",
    "little",
    "bit",
    "like",
    "sigma",
    "notation",
    "seen",
    "refers",
    "summation",
    "idea",
    "going",
    "adding",
    "whole",
    "sequence",
    "values",
    "sigma",
    "notation",
    "going",
    "come",
    "couple",
    "times",
    "today",
    "deal",
    "probability",
    "oftentimes",
    "adding",
    "whole",
    "bunch",
    "individual",
    "values",
    "individual",
    "probabilities",
    "get",
    "value",
    "see",
    "come",
    "couple",
    "times",
    "notation",
    "means",
    "sum",
    "possible",
    "worlds",
    "omega",
    "big",
    "omega",
    "represents",
    "set",
    "possible",
    "worlds",
    "meaning",
    "take",
    "worlds",
    "set",
    "possible",
    "worlds",
    "add",
    "probabilities",
    "ultimately",
    "get",
    "number",
    "take",
    "possible",
    "worlds",
    "add",
    "probabilities",
    "get",
    "number",
    "1",
    "end",
    "meaning",
    "probabilities",
    "need",
    "sum",
    "example",
    "take",
    "dice",
    "example",
    "imagine",
    "fair",
    "die",
    "numbers",
    "1",
    "6",
    "roll",
    "die",
    "one",
    "rolls",
    "equal",
    "probability",
    "taking",
    "place",
    "probability",
    "1",
    "6",
    "example",
    "probabilities",
    "0",
    "1",
    "0",
    "meaning",
    "impossible",
    "1",
    "meaning",
    "certain",
    "add",
    "probabilities",
    "possible",
    "worlds",
    "get",
    "number",
    "represent",
    "one",
    "probabilities",
    "like",
    "probability",
    "roll",
    "number",
    "2",
    "example",
    "1",
    "every",
    "six",
    "times",
    "roll",
    "die",
    "expect",
    "one",
    "time",
    "instance",
    "die",
    "might",
    "come",
    "probability",
    "certain",
    "little",
    "nothing",
    "instance",
    "fairly",
    "straightforward",
    "single",
    "die",
    "things",
    "get",
    "interesting",
    "models",
    "world",
    "get",
    "little",
    "bit",
    "complex",
    "let",
    "imagine",
    "dealing",
    "single",
    "die",
    "two",
    "dice",
    "example",
    "red",
    "die",
    "blue",
    "die",
    "care",
    "individual",
    "roll",
    "care",
    "sum",
    "two",
    "rolls",
    "case",
    "sum",
    "two",
    "rolls",
    "number",
    "begin",
    "reason",
    "probability",
    "look",
    "like",
    "instead",
    "one",
    "die",
    "two",
    "dice",
    "well",
    "might",
    "imagine",
    "could",
    "first",
    "consider",
    "possible",
    "worlds",
    "case",
    "possible",
    "worlds",
    "every",
    "combination",
    "red",
    "blue",
    "die",
    "could",
    "come",
    "red",
    "die",
    "could",
    "1",
    "2",
    "3",
    "4",
    "5",
    "possibilities",
    "blue",
    "die",
    "likewise",
    "could",
    "also",
    "either",
    "1",
    "2",
    "3",
    "4",
    "5",
    "happens",
    "particular",
    "case",
    "possible",
    "combinations",
    "equally",
    "likely",
    "equally",
    "likely",
    "various",
    "different",
    "possible",
    "worlds",
    "always",
    "going",
    "case",
    "imagine",
    "complex",
    "models",
    "could",
    "try",
    "build",
    "things",
    "could",
    "try",
    "represent",
    "real",
    "world",
    "probably",
    "going",
    "case",
    "every",
    "single",
    "possible",
    "world",
    "always",
    "equally",
    "likely",
    "case",
    "fair",
    "dice",
    "given",
    "die",
    "roll",
    "one",
    "number",
    "good",
    "chance",
    "coming",
    "number",
    "consider",
    "possible",
    "worlds",
    "equally",
    "likely",
    "even",
    "though",
    "possible",
    "worlds",
    "equally",
    "likely",
    "necessarily",
    "mean",
    "sums",
    "equally",
    "likely",
    "consider",
    "sum",
    "two",
    "1",
    "plus",
    "1",
    "2",
    "2",
    "plus",
    "1",
    "consider",
    "possible",
    "pairs",
    "numbers",
    "sum",
    "ultimately",
    "notice",
    "patterns",
    "entirely",
    "case",
    "every",
    "number",
    "comes",
    "equally",
    "likely",
    "consider",
    "7",
    "example",
    "probability",
    "roll",
    "two",
    "dice",
    "sum",
    "7",
    "several",
    "ways",
    "happen",
    "six",
    "possible",
    "worlds",
    "sum",
    "could",
    "1",
    "6",
    "2",
    "5",
    "3",
    "4",
    "4",
    "3",
    "forth",
    "instead",
    "consider",
    "probability",
    "roll",
    "two",
    "dice",
    "sum",
    "two",
    "die",
    "rolls",
    "12",
    "example",
    "looking",
    "diagram",
    "one",
    "possible",
    "world",
    "happen",
    "possible",
    "world",
    "red",
    "die",
    "blue",
    "die",
    "come",
    "sixes",
    "give",
    "us",
    "sum",
    "total",
    "based",
    "taking",
    "look",
    "diagram",
    "see",
    "probabilities",
    "likely",
    "different",
    "probability",
    "sum",
    "7",
    "must",
    "greater",
    "probability",
    "sum",
    "represent",
    "even",
    "formally",
    "saying",
    "ok",
    "probability",
    "sum",
    "12",
    "1",
    "36",
    "equally",
    "likely",
    "possible",
    "worlds",
    "6",
    "squared",
    "six",
    "options",
    "red",
    "die",
    "six",
    "options",
    "blue",
    "die",
    "36",
    "options",
    "one",
    "sums",
    "whereas",
    "hand",
    "probability",
    "take",
    "two",
    "dice",
    "rolls",
    "sum",
    "number",
    "7",
    "well",
    "36",
    "possible",
    "worlds",
    "six",
    "worlds",
    "sum",
    "get",
    "6",
    "36",
    "simplify",
    "fraction",
    "1",
    "able",
    "represent",
    "different",
    "ideas",
    "probability",
    "representing",
    "events",
    "might",
    "likely",
    "events",
    "less",
    "likely",
    "well",
    "sorts",
    "judgments",
    "figuring",
    "abstract",
    "probability",
    "thing",
    "takes",
    "place",
    "generally",
    "known",
    "unconditional",
    "probabilities",
    "degree",
    "belief",
    "proposition",
    "fact",
    "world",
    "absence",
    "evidence",
    "without",
    "knowing",
    "additional",
    "information",
    "roll",
    "die",
    "chance",
    "comes",
    "2",
    "roll",
    "two",
    "dice",
    "chance",
    "sum",
    "two",
    "die",
    "rolls",
    "7",
    "usually",
    "thinking",
    "probability",
    "especially",
    "thinking",
    "training",
    "ai",
    "intelligently",
    "able",
    "know",
    "something",
    "world",
    "make",
    "predictions",
    "based",
    "information",
    "unconditional",
    "probability",
    "ai",
    "dealing",
    "rather",
    "conditional",
    "probability",
    "probability",
    "rather",
    "original",
    "knowledge",
    "initial",
    "knowledge",
    "world",
    "world",
    "actually",
    "works",
    "conditional",
    "probability",
    "degree",
    "belief",
    "proposition",
    "given",
    "evidence",
    "already",
    "revealed",
    "us",
    "look",
    "like",
    "well",
    "looks",
    "like",
    "terms",
    "notation",
    "going",
    "represent",
    "conditional",
    "probability",
    "probability",
    "vertical",
    "bar",
    "way",
    "read",
    "thing",
    "side",
    "vertical",
    "bar",
    "want",
    "probability",
    "want",
    "probability",
    "true",
    "real",
    "world",
    "event",
    "actually",
    "take",
    "place",
    "right",
    "side",
    "vertical",
    "bar",
    "evidence",
    "information",
    "already",
    "know",
    "certain",
    "world",
    "example",
    "b",
    "true",
    "way",
    "read",
    "entire",
    "expression",
    "probability",
    "given",
    "b",
    "probability",
    "true",
    "given",
    "already",
    "know",
    "b",
    "true",
    "type",
    "judgment",
    "conditional",
    "probability",
    "probability",
    "one",
    "thing",
    "given",
    "fact",
    "comes",
    "quite",
    "lot",
    "think",
    "types",
    "calculations",
    "might",
    "want",
    "ai",
    "able",
    "example",
    "might",
    "care",
    "probability",
    "rain",
    "today",
    "given",
    "know",
    "rained",
    "yesterday",
    "could",
    "think",
    "probability",
    "rain",
    "today",
    "abstract",
    "chance",
    "today",
    "rains",
    "usually",
    "additional",
    "evidence",
    "know",
    "certain",
    "rained",
    "yesterday",
    "would",
    "like",
    "calculate",
    "probability",
    "rains",
    "today",
    "given",
    "know",
    "rained",
    "yesterday",
    "might",
    "imagine",
    "want",
    "know",
    "probability",
    "optimal",
    "route",
    "destination",
    "changes",
    "given",
    "current",
    "traffic",
    "condition",
    "whether",
    "traffic",
    "conditions",
    "change",
    "might",
    "change",
    "probability",
    "route",
    "actually",
    "optimal",
    "route",
    "might",
    "imagine",
    "medical",
    "context",
    "want",
    "know",
    "probability",
    "patient",
    "particular",
    "disease",
    "given",
    "results",
    "tests",
    "performed",
    "patient",
    "evidence",
    "results",
    "test",
    "would",
    "like",
    "know",
    "probability",
    "patient",
    "particular",
    "disease",
    "notion",
    "conditional",
    "probability",
    "comes",
    "everywhere",
    "begin",
    "think",
    "would",
    "like",
    "reason",
    "able",
    "reason",
    "little",
    "intelligently",
    "taking",
    "account",
    "evidence",
    "already",
    "able",
    "get",
    "accurate",
    "result",
    "likelihood",
    "someone",
    "disease",
    "know",
    "evidence",
    "results",
    "test",
    "opposed",
    "calculating",
    "unconditional",
    "probability",
    "saying",
    "probability",
    "disease",
    "without",
    "evidence",
    "try",
    "back",
    "result",
    "one",
    "way",
    "got",
    "idea",
    "conditional",
    "probability",
    "next",
    "question",
    "ask",
    "right",
    "calculate",
    "conditional",
    "probability",
    "figure",
    "mathematically",
    "expression",
    "like",
    "get",
    "number",
    "conditional",
    "probability",
    "actually",
    "mean",
    "well",
    "formula",
    "conditional",
    "probability",
    "looks",
    "little",
    "something",
    "like",
    "probability",
    "given",
    "b",
    "probability",
    "true",
    "given",
    "know",
    "b",
    "true",
    "equal",
    "fraction",
    "probability",
    "b",
    "true",
    "divided",
    "probability",
    "b",
    "true",
    "way",
    "intuitively",
    "try",
    "think",
    "want",
    "know",
    "probability",
    "true",
    "given",
    "b",
    "true",
    "well",
    "want",
    "consider",
    "ways",
    "could",
    "true",
    "worlds",
    "care",
    "worlds",
    "b",
    "already",
    "true",
    "sort",
    "ignore",
    "cases",
    "b",
    "true",
    "relevant",
    "ultimate",
    "computation",
    "relevant",
    "want",
    "get",
    "information",
    "let",
    "take",
    "look",
    "example",
    "let",
    "go",
    "back",
    "example",
    "rolling",
    "two",
    "dice",
    "idea",
    "two",
    "dice",
    "might",
    "sum",
    "number",
    "discussed",
    "earlier",
    "unconditional",
    "probability",
    "roll",
    "two",
    "dice",
    "sum",
    "12",
    "1",
    "36",
    "36",
    "possible",
    "worlds",
    "might",
    "care",
    "one",
    "sum",
    "two",
    "dice",
    "red",
    "6",
    "blue",
    "also",
    "let",
    "say",
    "additional",
    "information",
    "want",
    "know",
    "probability",
    "two",
    "dice",
    "sum",
    "12",
    "given",
    "know",
    "red",
    "die",
    "already",
    "evidence",
    "already",
    "know",
    "red",
    "die",
    "know",
    "blue",
    "die",
    "information",
    "given",
    "expression",
    "given",
    "fact",
    "know",
    "red",
    "die",
    "rolled",
    "6",
    "probability",
    "sum",
    "12",
    "begin",
    "math",
    "using",
    "expression",
    "possibilities",
    "possible",
    "combinations",
    "red",
    "die",
    "1",
    "6",
    "blue",
    "die",
    "1",
    "might",
    "consider",
    "first",
    "right",
    "probability",
    "evidence",
    "b",
    "variable",
    "want",
    "know",
    "probability",
    "red",
    "die",
    "6",
    "well",
    "probability",
    "red",
    "die",
    "6",
    "1",
    "1",
    "6",
    "options",
    "really",
    "worlds",
    "care",
    "rest",
    "irrelevant",
    "calculation",
    "already",
    "evidence",
    "red",
    "die",
    "6",
    "need",
    "care",
    "possibilities",
    "could",
    "result",
    "addition",
    "fact",
    "red",
    "die",
    "rolled",
    "6",
    "probability",
    "piece",
    "information",
    "need",
    "know",
    "order",
    "calculate",
    "conditional",
    "probability",
    "probability",
    "variables",
    "b",
    "true",
    "probability",
    "red",
    "die",
    "6",
    "sum",
    "probability",
    "things",
    "happen",
    "well",
    "happens",
    "one",
    "possible",
    "case",
    "1",
    "36",
    "cases",
    "case",
    "red",
    "blue",
    "die",
    "equal",
    "piece",
    "information",
    "already",
    "knew",
    "probability",
    "equal",
    "1",
    "get",
    "conditional",
    "probability",
    "sum",
    "12",
    "given",
    "know",
    "red",
    "dice",
    "equal",
    "6",
    "well",
    "divide",
    "two",
    "values",
    "together",
    "1",
    "36",
    "divided",
    "1",
    "6",
    "gives",
    "us",
    "probability",
    "1",
    "given",
    "know",
    "red",
    "die",
    "rolled",
    "value",
    "6",
    "probability",
    "sum",
    "two",
    "dice",
    "12",
    "also",
    "1",
    "probably",
    "makes",
    "intuitive",
    "sense",
    "red",
    "die",
    "6",
    "way",
    "get",
    "12",
    "blue",
    "die",
    "also",
    "rolls",
    "6",
    "know",
    "probability",
    "blue",
    "die",
    "rolling",
    "6",
    "1",
    "case",
    "conditional",
    "probability",
    "seems",
    "fairly",
    "straightforward",
    "idea",
    "calculating",
    "conditional",
    "probability",
    "looking",
    "probability",
    "events",
    "take",
    "place",
    "idea",
    "going",
    "come",
    "definition",
    "conditional",
    "probability",
    "going",
    "use",
    "definition",
    "think",
    "probability",
    "generally",
    "able",
    "draw",
    "conclusions",
    "world",
    "formula",
    "probability",
    "given",
    "b",
    "equal",
    "probability",
    "b",
    "take",
    "place",
    "divided",
    "probability",
    "see",
    "formula",
    "sometimes",
    "written",
    "couple",
    "different",
    "ways",
    "could",
    "imagine",
    "algebraically",
    "multiplying",
    "sides",
    "equation",
    "probability",
    "b",
    "get",
    "rid",
    "fraction",
    "get",
    "expression",
    "like",
    "probability",
    "b",
    "expression",
    "probability",
    "b",
    "times",
    "probability",
    "given",
    "could",
    "represent",
    "equivalently",
    "since",
    "b",
    "expression",
    "interchangeable",
    "b",
    "thing",
    "b",
    "could",
    "imagine",
    "also",
    "representing",
    "probability",
    "b",
    "probability",
    "times",
    "probability",
    "b",
    "given",
    "switching",
    "b",
    "three",
    "equivalent",
    "ways",
    "trying",
    "represent",
    "joint",
    "probability",
    "means",
    "sometimes",
    "see",
    "equations",
    "might",
    "useful",
    "begin",
    "reason",
    "probability",
    "think",
    "values",
    "might",
    "taking",
    "place",
    "real",
    "world",
    "sometimes",
    "deal",
    "probability",
    "care",
    "boolean",
    "event",
    "like",
    "happen",
    "happen",
    "sometimes",
    "might",
    "want",
    "ability",
    "represent",
    "variable",
    "values",
    "probability",
    "space",
    "variable",
    "might",
    "take",
    "multiple",
    "different",
    "possible",
    "values",
    "probability",
    "call",
    "variable",
    "probability",
    "theory",
    "random",
    "variable",
    "random",
    "variable",
    "probability",
    "variable",
    "probability",
    "theory",
    "domain",
    "values",
    "take",
    "mean",
    "well",
    "mean",
    "might",
    "random",
    "variable",
    "called",
    "roll",
    "example",
    "six",
    "possible",
    "values",
    "roll",
    "variable",
    "possible",
    "values",
    "domain",
    "values",
    "take",
    "1",
    "2",
    "3",
    "4",
    "5",
    "might",
    "like",
    "know",
    "probability",
    "case",
    "happen",
    "random",
    "variables",
    "might",
    "case",
    "example",
    "might",
    "random",
    "variable",
    "represent",
    "weather",
    "example",
    "domain",
    "values",
    "could",
    "take",
    "things",
    "like",
    "sun",
    "cloudy",
    "rainy",
    "windy",
    "snowy",
    "might",
    "different",
    "probability",
    "care",
    "knowing",
    "probability",
    "weather",
    "equals",
    "sun",
    "weather",
    "equals",
    "clouds",
    "instance",
    "might",
    "like",
    "mathematical",
    "calculations",
    "based",
    "information",
    "random",
    "variables",
    "might",
    "something",
    "like",
    "traffic",
    "odds",
    "traffic",
    "light",
    "traffic",
    "heavy",
    "traffic",
    "traffic",
    "case",
    "random",
    "variable",
    "values",
    "random",
    "variable",
    "take",
    "either",
    "none",
    "light",
    "heavy",
    "person",
    "calculations",
    "person",
    "encoding",
    "random",
    "variables",
    "computer",
    "need",
    "make",
    "decision",
    "possible",
    "values",
    "actually",
    "might",
    "imagine",
    "example",
    "flight",
    "care",
    "whether",
    "make",
    "flight",
    "time",
    "flight",
    "couple",
    "possible",
    "values",
    "could",
    "take",
    "flight",
    "could",
    "time",
    "flight",
    "could",
    "delayed",
    "flight",
    "could",
    "canceled",
    "flight",
    "case",
    "random",
    "variable",
    "values",
    "take",
    "often",
    "want",
    "know",
    "something",
    "probability",
    "random",
    "variable",
    "takes",
    "possible",
    "values",
    "call",
    "probability",
    "distribution",
    "probability",
    "distribution",
    "takes",
    "random",
    "variable",
    "gives",
    "probability",
    "possible",
    "values",
    "domain",
    "case",
    "flight",
    "example",
    "probability",
    "distribution",
    "might",
    "look",
    "something",
    "like",
    "probability",
    "distribution",
    "says",
    "probability",
    "random",
    "variable",
    "flight",
    "equal",
    "value",
    "time",
    "otherwise",
    "put",
    "english",
    "terms",
    "likelihood",
    "flight",
    "time",
    "60",
    "example",
    "case",
    "probability",
    "flight",
    "delayed",
    "30",
    "probability",
    "flight",
    "canceled",
    "10",
    "sum",
    "possible",
    "values",
    "sum",
    "going",
    "1",
    "right",
    "take",
    "possible",
    "worlds",
    "three",
    "possible",
    "worlds",
    "value",
    "random",
    "variable",
    "flight",
    "add",
    "together",
    "result",
    "needs",
    "number",
    "1",
    "per",
    "axiom",
    "probability",
    "theory",
    "discussed",
    "one",
    "way",
    "representing",
    "probability",
    "distribution",
    "random",
    "variable",
    "flight",
    "sometimes",
    "see",
    "represented",
    "little",
    "bit",
    "concisely",
    "pretty",
    "verbose",
    "really",
    "trying",
    "express",
    "three",
    "possible",
    "values",
    "often",
    "instead",
    "see",
    "notation",
    "representing",
    "using",
    "vector",
    "vector",
    "sequence",
    "values",
    "opposed",
    "single",
    "value",
    "might",
    "multiple",
    "values",
    "could",
    "extend",
    "instead",
    "represent",
    "idea",
    "way",
    "bold",
    "p",
    "larger",
    "p",
    "generally",
    "meaning",
    "probability",
    "distribution",
    "variable",
    "flight",
    "equal",
    "vector",
    "represented",
    "angle",
    "brackets",
    "probability",
    "distribution",
    "would",
    "know",
    "probability",
    "distribution",
    "order",
    "time",
    "delayed",
    "canceled",
    "know",
    "interpret",
    "vector",
    "mean",
    "first",
    "value",
    "vector",
    "probability",
    "flight",
    "time",
    "second",
    "value",
    "vector",
    "probability",
    "flight",
    "delayed",
    "third",
    "value",
    "vector",
    "probability",
    "flight",
    "canceled",
    "alternate",
    "way",
    "representing",
    "idea",
    "little",
    "verbosely",
    "oftentimes",
    "see",
    "us",
    "talk",
    "probability",
    "distribution",
    "random",
    "variable",
    "whenever",
    "talk",
    "really",
    "trying",
    "figure",
    "probabilities",
    "possible",
    "values",
    "random",
    "variable",
    "take",
    "notation",
    "little",
    "bit",
    "succinct",
    "even",
    "though",
    "sometimes",
    "little",
    "confusing",
    "depending",
    "context",
    "see",
    "start",
    "look",
    "examples",
    "use",
    "sort",
    "notation",
    "describe",
    "probability",
    "describe",
    "events",
    "might",
    "take",
    "place",
    "couple",
    "important",
    "ideas",
    "know",
    "regards",
    "probability",
    "theory",
    "one",
    "idea",
    "independence",
    "independence",
    "refers",
    "idea",
    "knowledge",
    "one",
    "event",
    "influence",
    "probability",
    "another",
    "event",
    "example",
    "context",
    "two",
    "dice",
    "rolls",
    "red",
    "die",
    "blue",
    "die",
    "probability",
    "roll",
    "red",
    "die",
    "blue",
    "die",
    "two",
    "events",
    "red",
    "die",
    "blue",
    "die",
    "independent",
    "knowing",
    "result",
    "red",
    "die",
    "change",
    "probabilities",
    "blue",
    "die",
    "give",
    "additional",
    "information",
    "value",
    "blue",
    "die",
    "ultimately",
    "going",
    "always",
    "going",
    "case",
    "might",
    "imagine",
    "case",
    "weather",
    "something",
    "like",
    "clouds",
    "rain",
    "probably",
    "independent",
    "cloudy",
    "might",
    "increase",
    "probability",
    "later",
    "day",
    "going",
    "rain",
    "information",
    "informs",
    "event",
    "random",
    "variable",
    "independence",
    "refers",
    "idea",
    "one",
    "event",
    "influence",
    "independent",
    "might",
    "relationship",
    "mathematically",
    "formally",
    "independence",
    "actually",
    "mean",
    "well",
    "recall",
    "formula",
    "probability",
    "b",
    "probability",
    "times",
    "probability",
    "b",
    "given",
    "intuitive",
    "way",
    "think",
    "know",
    "likely",
    "b",
    "happen",
    "well",
    "let",
    "first",
    "figure",
    "likelihood",
    "happens",
    "given",
    "know",
    "happens",
    "let",
    "figure",
    "likelihood",
    "b",
    "happens",
    "multiply",
    "two",
    "things",
    "together",
    "b",
    "independent",
    "meaning",
    "knowing",
    "change",
    "anything",
    "likelihood",
    "b",
    "true",
    "well",
    "probability",
    "b",
    "given",
    "meaning",
    "probability",
    "b",
    "true",
    "given",
    "know",
    "true",
    "well",
    "know",
    "true",
    "really",
    "make",
    "difference",
    "two",
    "things",
    "independent",
    "influence",
    "b",
    "probability",
    "b",
    "given",
    "really",
    "probability",
    "true",
    "b",
    "independent",
    "right",
    "one",
    "example",
    "definition",
    "means",
    "b",
    "independent",
    "probability",
    "b",
    "probability",
    "times",
    "probability",
    "anytime",
    "find",
    "two",
    "events",
    "b",
    "relationship",
    "holds",
    "say",
    "b",
    "independent",
    "example",
    "might",
    "dice",
    "taking",
    "look",
    "wanted",
    "probability",
    "red",
    "6",
    "blue",
    "6",
    "well",
    "probability",
    "red",
    "6",
    "multiplied",
    "probability",
    "blue",
    "equal",
    "1",
    "say",
    "two",
    "events",
    "independent",
    "would",
    "independent",
    "example",
    "would",
    "example",
    "example",
    "probability",
    "1",
    "36",
    "talked",
    "would",
    "independent",
    "would",
    "case",
    "like",
    "probability",
    "red",
    "die",
    "rolls",
    "6",
    "red",
    "die",
    "rolls",
    "naively",
    "took",
    "ok",
    "red",
    "die",
    "6",
    "red",
    "die",
    "4",
    "well",
    "rolling",
    "die",
    "might",
    "imagine",
    "naive",
    "approach",
    "say",
    "well",
    "probability",
    "1",
    "multiply",
    "together",
    "probability",
    "1",
    "course",
    "rolling",
    "red",
    "die",
    "way",
    "could",
    "get",
    "two",
    "different",
    "values",
    "red",
    "die",
    "could",
    "6",
    "probability",
    "multiply",
    "probability",
    "red",
    "6",
    "times",
    "probability",
    "red",
    "4",
    "well",
    "would",
    "equal",
    "1",
    "course",
    "true",
    "know",
    "way",
    "probability",
    "0",
    "roll",
    "red",
    "die",
    "get",
    "6",
    "4",
    "one",
    "possibilities",
    "actually",
    "result",
    "say",
    "event",
    "red",
    "roll",
    "6",
    "event",
    "red",
    "roll",
    "4",
    "two",
    "events",
    "independent",
    "know",
    "red",
    "roll",
    "6",
    "know",
    "red",
    "roll",
    "possibly",
    "4",
    "things",
    "independent",
    "instead",
    "wanted",
    "calculate",
    "probability",
    "would",
    "need",
    "use",
    "conditional",
    "probability",
    "regular",
    "definition",
    "probability",
    "two",
    "events",
    "taking",
    "place",
    "probability",
    "well",
    "probability",
    "red",
    "roll",
    "6",
    "1",
    "probability",
    "roll",
    "4",
    "given",
    "roll",
    "6",
    "well",
    "0",
    "way",
    "red",
    "roll",
    "4",
    "given",
    "already",
    "know",
    "red",
    "roll",
    "value",
    "add",
    "multiplication",
    "get",
    "number",
    "idea",
    "conditional",
    "probability",
    "going",
    "come",
    "especially",
    "begin",
    "reason",
    "multiple",
    "different",
    "random",
    "variables",
    "might",
    "interacting",
    "way",
    "gets",
    "us",
    "one",
    "important",
    "rules",
    "probability",
    "theory",
    "known",
    "bayes",
    "rule",
    "turns",
    "using",
    "information",
    "already",
    "learned",
    "probability",
    "applying",
    "little",
    "bit",
    "algebra",
    "actually",
    "derive",
    "bayes",
    "rule",
    "important",
    "rule",
    "comes",
    "inference",
    "thinking",
    "probability",
    "context",
    "computer",
    "mathematician",
    "could",
    "access",
    "information",
    "probability",
    "let",
    "go",
    "back",
    "equations",
    "able",
    "derive",
    "bayes",
    "rule",
    "know",
    "probability",
    "b",
    "likelihood",
    "b",
    "take",
    "place",
    "likelihood",
    "b",
    "likelihood",
    "given",
    "know",
    "b",
    "already",
    "true",
    "likewise",
    "probability",
    "given",
    "b",
    "probability",
    "times",
    "probability",
    "b",
    "given",
    "know",
    "already",
    "true",
    "sort",
    "symmetric",
    "relationship",
    "matter",
    "order",
    "b",
    "b",
    "mean",
    "thing",
    "equations",
    "swap",
    "b",
    "able",
    "represent",
    "exact",
    "idea",
    "know",
    "two",
    "equations",
    "already",
    "true",
    "seen",
    "already",
    "let",
    "little",
    "bit",
    "algebraic",
    "manipulation",
    "stuff",
    "expressions",
    "side",
    "equal",
    "probability",
    "take",
    "two",
    "expressions",
    "side",
    "set",
    "equal",
    "equal",
    "probability",
    "b",
    "must",
    "equal",
    "probability",
    "times",
    "probability",
    "b",
    "given",
    "equal",
    "probability",
    "b",
    "times",
    "probability",
    "given",
    "going",
    "little",
    "bit",
    "division",
    "going",
    "divide",
    "sides",
    "p",
    "get",
    "bayes",
    "rule",
    "probability",
    "b",
    "given",
    "equal",
    "probability",
    "b",
    "times",
    "probability",
    "given",
    "b",
    "divided",
    "probability",
    "sometimes",
    "bayes",
    "rule",
    "see",
    "order",
    "two",
    "arguments",
    "switched",
    "instead",
    "b",
    "times",
    "given",
    "b",
    "given",
    "b",
    "times",
    "ultimately",
    "matter",
    "multiplication",
    "switch",
    "order",
    "two",
    "things",
    "multiplying",
    "change",
    "result",
    "right",
    "common",
    "formulation",
    "bayes",
    "rule",
    "probability",
    "b",
    "given",
    "equal",
    "probability",
    "given",
    "b",
    "times",
    "probability",
    "b",
    "divided",
    "probability",
    "rule",
    "turns",
    "really",
    "important",
    "comes",
    "trying",
    "infer",
    "things",
    "world",
    "means",
    "express",
    "one",
    "conditional",
    "probability",
    "conditional",
    "probability",
    "b",
    "given",
    "using",
    "knowledge",
    "probability",
    "given",
    "b",
    "using",
    "reverse",
    "conditional",
    "probability",
    "let",
    "first",
    "little",
    "bit",
    "example",
    "see",
    "might",
    "use",
    "explore",
    "means",
    "little",
    "bit",
    "generally",
    "going",
    "construct",
    "situation",
    "information",
    "two",
    "events",
    "care",
    "idea",
    "cloudy",
    "morning",
    "idea",
    "rainy",
    "afternoon",
    "two",
    "different",
    "possible",
    "events",
    "could",
    "take",
    "place",
    "cloudy",
    "morning",
    "rainy",
    "pm",
    "care",
    "given",
    "clouds",
    "morning",
    "probability",
    "rain",
    "afternoon",
    "reasonable",
    "question",
    "might",
    "ask",
    "morning",
    "look",
    "outside",
    "ai",
    "camera",
    "looks",
    "outside",
    "sees",
    "clouds",
    "morning",
    "want",
    "conclude",
    "want",
    "figure",
    "probability",
    "afternoon",
    "going",
    "rain",
    "course",
    "abstract",
    "access",
    "kind",
    "information",
    "use",
    "data",
    "begin",
    "try",
    "figure",
    "let",
    "imagine",
    "access",
    "pieces",
    "information",
    "access",
    "idea",
    "80",
    "rainy",
    "afternoons",
    "start",
    "cloudy",
    "morning",
    "might",
    "imagine",
    "could",
    "gathered",
    "data",
    "looking",
    "data",
    "sequence",
    "time",
    "know",
    "80",
    "time",
    "raining",
    "afternoon",
    "cloudy",
    "morning",
    "also",
    "know",
    "40",
    "days",
    "cloudy",
    "mornings",
    "also",
    "know",
    "10",
    "days",
    "rainy",
    "afternoons",
    "using",
    "information",
    "would",
    "like",
    "figure",
    "given",
    "clouds",
    "morning",
    "probability",
    "rains",
    "afternoon",
    "want",
    "know",
    "probability",
    "afternoon",
    "rain",
    "given",
    "morning",
    "clouds",
    "particular",
    "using",
    "fact",
    "probability",
    "know",
    "80",
    "rainy",
    "afternoons",
    "start",
    "cloudy",
    "mornings",
    "know",
    "probability",
    "cloudy",
    "mornings",
    "given",
    "rainy",
    "afternoons",
    "using",
    "sort",
    "reverse",
    "conditional",
    "probability",
    "figure",
    "expressed",
    "terms",
    "bayes",
    "rule",
    "would",
    "look",
    "like",
    "probability",
    "rain",
    "given",
    "clouds",
    "probability",
    "clouds",
    "given",
    "rain",
    "times",
    "probability",
    "rain",
    "divided",
    "probability",
    "clouds",
    "substituting",
    "values",
    "b",
    "equation",
    "bayes",
    "rule",
    "math",
    "information",
    "know",
    "80",
    "time",
    "raining",
    "clouds",
    "morning",
    "probability",
    "rain",
    "10",
    "days",
    "rainy",
    "40",
    "days",
    "cloudy",
    "math",
    "figure",
    "answer",
    "probability",
    "rains",
    "afternoon",
    "given",
    "cloudy",
    "morning",
    "case",
    "application",
    "bayes",
    "rule",
    "idea",
    "using",
    "one",
    "conditional",
    "probability",
    "get",
    "reverse",
    "conditional",
    "probability",
    "often",
    "useful",
    "one",
    "conditional",
    "probabilities",
    "might",
    "easier",
    "us",
    "know",
    "easier",
    "us",
    "data",
    "using",
    "information",
    "calculate",
    "conditional",
    "probability",
    "look",
    "like",
    "well",
    "means",
    "knowing",
    "probability",
    "cloudy",
    "mornings",
    "given",
    "rainy",
    "afternoons",
    "calculate",
    "probability",
    "rainy",
    "afternoons",
    "given",
    "cloudy",
    "mornings",
    "example",
    "generally",
    "know",
    "probability",
    "visible",
    "effect",
    "effect",
    "see",
    "observe",
    "given",
    "unknown",
    "cause",
    "sure",
    "well",
    "calculate",
    "probability",
    "unknown",
    "cause",
    "given",
    "visible",
    "effect",
    "might",
    "look",
    "like",
    "well",
    "context",
    "medicine",
    "example",
    "might",
    "know",
    "probability",
    "medical",
    "test",
    "result",
    "given",
    "disease",
    "like",
    "know",
    "someone",
    "disease",
    "x",
    "time",
    "medical",
    "test",
    "result",
    "show",
    "instance",
    "using",
    "information",
    "calculate",
    "right",
    "probability",
    "given",
    "know",
    "medical",
    "test",
    "result",
    "likelihood",
    "someone",
    "disease",
    "piece",
    "information",
    "usually",
    "easier",
    "know",
    "easier",
    "immediately",
    "access",
    "data",
    "information",
    "actually",
    "want",
    "calculate",
    "might",
    "want",
    "know",
    "example",
    "know",
    "probability",
    "counterfeit",
    "bills",
    "blurry",
    "text",
    "around",
    "edges",
    "counterfeit",
    "printers",
    "nearly",
    "good",
    "printing",
    "text",
    "precisely",
    "information",
    "given",
    "something",
    "counterfeit",
    "bill",
    "like",
    "x",
    "counterfeit",
    "bills",
    "blurry",
    "text",
    "example",
    "using",
    "information",
    "calculate",
    "piece",
    "information",
    "might",
    "want",
    "know",
    "like",
    "given",
    "know",
    "blurry",
    "text",
    "bill",
    "probability",
    "bill",
    "counterfeit",
    "given",
    "one",
    "conditional",
    "probability",
    "calculate",
    "conditional",
    "probability",
    "well",
    "taken",
    "look",
    "couple",
    "different",
    "types",
    "probability",
    "looked",
    "unconditional",
    "probability",
    "look",
    "probability",
    "event",
    "occurring",
    "given",
    "additional",
    "evidence",
    "might",
    "access",
    "also",
    "looked",
    "conditional",
    "probability",
    "sort",
    "evidence",
    "would",
    "like",
    "using",
    "evidence",
    "able",
    "calculate",
    "probability",
    "well",
    "kind",
    "probability",
    "important",
    "us",
    "think",
    "joint",
    "probability",
    "considering",
    "likelihood",
    "multiple",
    "different",
    "events",
    "simultaneously",
    "mean",
    "example",
    "might",
    "probability",
    "distributions",
    "look",
    "little",
    "something",
    "like",
    "like",
    "oh",
    "want",
    "know",
    "probability",
    "distribution",
    "clouds",
    "morning",
    "distribution",
    "looks",
    "like",
    "40",
    "time",
    "c",
    "random",
    "variable",
    "equal",
    "cloudy",
    "60",
    "time",
    "cloudy",
    "simple",
    "probability",
    "distribution",
    "effectively",
    "telling",
    "40",
    "time",
    "cloudy",
    "might",
    "also",
    "probability",
    "distribution",
    "rain",
    "afternoon",
    "10",
    "time",
    "probability",
    "raining",
    "afternoon",
    "probability",
    "raining",
    "afternoon",
    "using",
    "two",
    "pieces",
    "information",
    "actually",
    "whole",
    "lot",
    "information",
    "two",
    "variables",
    "relate",
    "could",
    "access",
    "joint",
    "probability",
    "meaning",
    "every",
    "combination",
    "two",
    "things",
    "meaning",
    "morning",
    "cloudy",
    "afternoon",
    "rain",
    "morning",
    "cloudy",
    "afternoon",
    "rain",
    "morning",
    "cloudy",
    "afternoon",
    "rain",
    "morning",
    "cloudy",
    "afternoon",
    "raining",
    "access",
    "values",
    "four",
    "information",
    "information",
    "organized",
    "table",
    "like",
    "rather",
    "probability",
    "distribution",
    "joint",
    "probability",
    "distribution",
    "tells",
    "probability",
    "distribution",
    "possible",
    "combinations",
    "values",
    "random",
    "variables",
    "take",
    "want",
    "know",
    "probability",
    "given",
    "day",
    "cloudy",
    "rainy",
    "well",
    "would",
    "say",
    "right",
    "looking",
    "cases",
    "cloudy",
    "cases",
    "raining",
    "intersection",
    "two",
    "row",
    "column",
    "probability",
    "cloudy",
    "rainy",
    "using",
    "information",
    "using",
    "conditional",
    "probability",
    "table",
    "using",
    "joint",
    "probability",
    "table",
    "begin",
    "draw",
    "pieces",
    "information",
    "things",
    "like",
    "conditional",
    "probability",
    "might",
    "ask",
    "question",
    "like",
    "probability",
    "distribution",
    "clouds",
    "given",
    "know",
    "raining",
    "meaning",
    "know",
    "sure",
    "raining",
    "tell",
    "probability",
    "distribution",
    "whether",
    "cloudy",
    "given",
    "know",
    "already",
    "fact",
    "raining",
    "using",
    "c",
    "stand",
    "random",
    "variable",
    "looking",
    "distribution",
    "meaning",
    "answer",
    "going",
    "single",
    "value",
    "going",
    "two",
    "values",
    "vector",
    "two",
    "values",
    "first",
    "value",
    "probability",
    "clouds",
    "second",
    "value",
    "probability",
    "cloudy",
    "sum",
    "two",
    "values",
    "going",
    "add",
    "probabilities",
    "possible",
    "worlds",
    "result",
    "get",
    "must",
    "number",
    "well",
    "know",
    "calculate",
    "conditional",
    "probability",
    "well",
    "know",
    "probability",
    "given",
    "b",
    "probability",
    "b",
    "divided",
    "probability",
    "mean",
    "well",
    "means",
    "calculate",
    "probability",
    "clouds",
    "given",
    "raining",
    "probability",
    "clouds",
    "raining",
    "divided",
    "probability",
    "rain",
    "comma",
    "probability",
    "distribution",
    "clouds",
    "rain",
    "comma",
    "sort",
    "stands",
    "word",
    "sort",
    "see",
    "logical",
    "operator",
    "comma",
    "used",
    "interchangeably",
    "means",
    "probability",
    "distribution",
    "clouds",
    "knowing",
    "fact",
    "raining",
    "divided",
    "probability",
    "rain",
    "interesting",
    "thing",
    "note",
    "often",
    "order",
    "simplify",
    "mathematics",
    "dividing",
    "probability",
    "rain",
    "probability",
    "rain",
    "numerical",
    "constant",
    "number",
    "dividing",
    "probability",
    "rain",
    "dividing",
    "constant",
    "words",
    "multiplying",
    "inverse",
    "constant",
    "turns",
    "oftentimes",
    "worry",
    "exact",
    "value",
    "know",
    "fact",
    "constant",
    "value",
    "see",
    "moment",
    "instead",
    "expressing",
    "joint",
    "probability",
    "divided",
    "probability",
    "rain",
    "sometimes",
    "represent",
    "alpha",
    "times",
    "numerator",
    "probability",
    "distribution",
    "c",
    "variable",
    "know",
    "raining",
    "instance",
    "done",
    "said",
    "value",
    "1",
    "probability",
    "rain",
    "really",
    "constant",
    "going",
    "divide",
    "equivalently",
    "multiply",
    "inverse",
    "end",
    "call",
    "alpha",
    "deal",
    "little",
    "bit",
    "later",
    "key",
    "idea",
    "idea",
    "going",
    "come",
    "conditional",
    "distribution",
    "c",
    "given",
    "rain",
    "proportional",
    "meaning",
    "factor",
    "multiplied",
    "joint",
    "probability",
    "c",
    "rain",
    "true",
    "figure",
    "well",
    "going",
    "probability",
    "cloudy",
    "given",
    "raining",
    "probability",
    "cloudy",
    "given",
    "raining",
    "get",
    "alpha",
    "times",
    "probability",
    "distribution",
    "clouds",
    "rain",
    "cloudy",
    "rain",
    "course",
    "sum",
    "number",
    "know",
    "probability",
    "distribution",
    "consider",
    "possible",
    "values",
    "must",
    "sum",
    "probability",
    "know",
    "need",
    "figure",
    "constant",
    "normalize",
    "speak",
    "values",
    "something",
    "multiply",
    "divide",
    "get",
    "probabilities",
    "sum",
    "1",
    "turns",
    "multiply",
    "numbers",
    "10",
    "get",
    "result",
    "proportions",
    "still",
    "equivalent",
    "plus",
    "sum",
    "number",
    "take",
    "look",
    "see",
    "understand",
    "step",
    "step",
    "getting",
    "one",
    "point",
    "another",
    "key",
    "idea",
    "using",
    "joint",
    "probabilities",
    "probabilities",
    "cloudy",
    "rainy",
    "cloudy",
    "rainy",
    "take",
    "information",
    "figure",
    "conditional",
    "probability",
    "given",
    "raining",
    "chance",
    "cloudy",
    "versus",
    "cloudy",
    "multiplying",
    "normalization",
    "constant",
    "speak",
    "computer",
    "begin",
    "use",
    "able",
    "interact",
    "various",
    "different",
    "types",
    "probabilities",
    "turns",
    "number",
    "probability",
    "rules",
    "going",
    "useful",
    "us",
    "begin",
    "explore",
    "actually",
    "use",
    "information",
    "encode",
    "computers",
    "complex",
    "analysis",
    "might",
    "want",
    "probability",
    "distributions",
    "random",
    "variables",
    "might",
    "interacting",
    "couple",
    "important",
    "probability",
    "rules",
    "one",
    "simplest",
    "rules",
    "negation",
    "rule",
    "probability",
    "event",
    "event",
    "probability",
    "would",
    "like",
    "know",
    "probability",
    "occur",
    "turns",
    "1",
    "minus",
    "p",
    "makes",
    "sense",
    "two",
    "possible",
    "cases",
    "either",
    "happens",
    "happen",
    "add",
    "two",
    "cases",
    "must",
    "get",
    "1",
    "means",
    "p",
    "must",
    "1",
    "minus",
    "p",
    "p",
    "p",
    "must",
    "sum",
    "number",
    "must",
    "include",
    "possible",
    "cases",
    "seen",
    "expression",
    "calculating",
    "probability",
    "might",
    "also",
    "reasonably",
    "want",
    "calculate",
    "probability",
    "probability",
    "one",
    "thing",
    "happens",
    "another",
    "thing",
    "happens",
    "example",
    "might",
    "want",
    "calculate",
    "probability",
    "roll",
    "two",
    "dice",
    "red",
    "die",
    "blue",
    "die",
    "likelihood",
    "6",
    "b",
    "6",
    "like",
    "one",
    "might",
    "imagine",
    "could",
    "wrong",
    "way",
    "approach",
    "would",
    "say",
    "right",
    "well",
    "comes",
    "6",
    "red",
    "die",
    "comes",
    "6",
    "probability",
    "1",
    "blue",
    "die",
    "also",
    "1",
    "add",
    "together",
    "get",
    "2",
    "6",
    "otherwise",
    "known",
    "1",
    "third",
    "suffers",
    "problem",
    "counting",
    "double",
    "counted",
    "case",
    "b",
    "red",
    "die",
    "blue",
    "die",
    "come",
    "counted",
    "instance",
    "twice",
    "resolve",
    "actual",
    "expression",
    "calculating",
    "probability",
    "b",
    "uses",
    "call",
    "formula",
    "take",
    "probability",
    "add",
    "probability",
    "need",
    "exclude",
    "cases",
    "double",
    "counted",
    "subtract",
    "probability",
    "gets",
    "result",
    "consider",
    "cases",
    "true",
    "cases",
    "b",
    "true",
    "imagine",
    "like",
    "venn",
    "diagram",
    "cases",
    "true",
    "cases",
    "b",
    "true",
    "need",
    "subtract",
    "middle",
    "get",
    "rid",
    "cases",
    "overcounted",
    "double",
    "counting",
    "inside",
    "individual",
    "expressions",
    "one",
    "rule",
    "going",
    "quite",
    "helpful",
    "rule",
    "called",
    "marginalization",
    "marginalization",
    "answering",
    "question",
    "figure",
    "probability",
    "using",
    "variable",
    "might",
    "access",
    "like",
    "b",
    "even",
    "know",
    "additional",
    "information",
    "know",
    "b",
    "event",
    "two",
    "possible",
    "states",
    "either",
    "b",
    "happens",
    "b",
    "happen",
    "assuming",
    "boolean",
    "true",
    "false",
    "well",
    "means",
    "able",
    "calculate",
    "probability",
    "two",
    "cases",
    "either",
    "happens",
    "b",
    "happens",
    "happens",
    "b",
    "happen",
    "two",
    "disjoint",
    "meaning",
    "ca",
    "happen",
    "together",
    "either",
    "b",
    "happens",
    "b",
    "happen",
    "disjoint",
    "separate",
    "cases",
    "figure",
    "probability",
    "adding",
    "two",
    "cases",
    "probability",
    "true",
    "probability",
    "b",
    "true",
    "plus",
    "probability",
    "true",
    "b",
    "true",
    "marginalizing",
    "looked",
    "two",
    "possible",
    "cases",
    "might",
    "take",
    "place",
    "either",
    "b",
    "happens",
    "b",
    "happen",
    "either",
    "cases",
    "look",
    "probability",
    "happens",
    "add",
    "together",
    "well",
    "get",
    "probability",
    "happens",
    "whole",
    "take",
    "look",
    "rule",
    "matter",
    "b",
    "related",
    "long",
    "know",
    "joint",
    "distributions",
    "figure",
    "overall",
    "probability",
    "useful",
    "way",
    "joint",
    "distribution",
    "like",
    "joint",
    "distribution",
    "b",
    "figure",
    "unconditional",
    "probability",
    "like",
    "probability",
    "see",
    "examples",
    "soon",
    "well",
    "sometimes",
    "might",
    "random",
    "might",
    "variables",
    "events",
    "like",
    "happened",
    "happen",
    "like",
    "b",
    "might",
    "broader",
    "probability",
    "distribution",
    "multiple",
    "possible",
    "values",
    "order",
    "use",
    "marginalization",
    "rule",
    "need",
    "sum",
    "b",
    "b",
    "possible",
    "values",
    "random",
    "variable",
    "could",
    "take",
    "see",
    "version",
    "rule",
    "random",
    "variables",
    "going",
    "include",
    "summation",
    "notation",
    "indicate",
    "summing",
    "adding",
    "whole",
    "bunch",
    "individual",
    "values",
    "rule",
    "looks",
    "lot",
    "complicated",
    "actually",
    "equivalent",
    "exactly",
    "rule",
    "saying",
    "two",
    "random",
    "variables",
    "one",
    "called",
    "x",
    "one",
    "called",
    "well",
    "probability",
    "x",
    "equal",
    "value",
    "x",
    "sub",
    "value",
    "variable",
    "takes",
    "figure",
    "well",
    "going",
    "sum",
    "j",
    "j",
    "going",
    "range",
    "possible",
    "values",
    "take",
    "well",
    "let",
    "look",
    "probability",
    "x",
    "equals",
    "xi",
    "equals",
    "yj",
    "exact",
    "rule",
    "difference",
    "summing",
    "possible",
    "values",
    "take",
    "saying",
    "let",
    "add",
    "possible",
    "cases",
    "look",
    "joint",
    "distribution",
    "joint",
    "probability",
    "x",
    "takes",
    "value",
    "care",
    "given",
    "possible",
    "values",
    "add",
    "get",
    "unconditional",
    "probability",
    "x",
    "equal",
    "whether",
    "x",
    "equal",
    "value",
    "x",
    "sub",
    "let",
    "take",
    "look",
    "rule",
    "look",
    "little",
    "bit",
    "complicated",
    "let",
    "try",
    "put",
    "concrete",
    "example",
    "joint",
    "distribution",
    "cloud",
    "cloudy",
    "rainy",
    "rainy",
    "maybe",
    "want",
    "access",
    "variable",
    "want",
    "know",
    "probability",
    "cloudy",
    "well",
    "marginalization",
    "says",
    "joint",
    "distribution",
    "want",
    "know",
    "probability",
    "cloudy",
    "well",
    "need",
    "consider",
    "variable",
    "variable",
    "idea",
    "rainy",
    "consider",
    "two",
    "cases",
    "either",
    "raining",
    "raining",
    "sum",
    "values",
    "possibilities",
    "words",
    "probability",
    "cloudy",
    "equal",
    "sum",
    "probability",
    "cloudy",
    "rainy",
    "probability",
    "cloudy",
    "raining",
    "values",
    "access",
    "values",
    "inside",
    "joint",
    "probability",
    "table",
    "probability",
    "cloudy",
    "rainy",
    "well",
    "intersection",
    "two",
    "probability",
    "cloudy",
    "raining",
    "right",
    "cloudy",
    "raining",
    "plus",
    "gives",
    "us",
    "equal",
    "unconditional",
    "probability",
    "fact",
    "cloudy",
    "marginalization",
    "gives",
    "us",
    "way",
    "go",
    "joint",
    "distributions",
    "individual",
    "probability",
    "might",
    "care",
    "see",
    "little",
    "bit",
    "later",
    "care",
    "actually",
    "useful",
    "us",
    "begin",
    "calculations",
    "last",
    "rule",
    "take",
    "look",
    "transitioning",
    "something",
    "little",
    "bit",
    "different",
    "rule",
    "conditioning",
    "similar",
    "marginalization",
    "rule",
    "says",
    "two",
    "events",
    "b",
    "instead",
    "access",
    "joint",
    "probabilities",
    "access",
    "conditional",
    "probabilities",
    "relate",
    "well",
    "want",
    "know",
    "probability",
    "happens",
    "know",
    "variable",
    "b",
    "either",
    "b",
    "happens",
    "b",
    "happen",
    "say",
    "probability",
    "probability",
    "given",
    "b",
    "times",
    "probability",
    "b",
    "meaning",
    "b",
    "happened",
    "given",
    "know",
    "b",
    "happened",
    "likelihood",
    "happened",
    "consider",
    "case",
    "b",
    "happen",
    "probability",
    "b",
    "happen",
    "probability",
    "happens",
    "given",
    "know",
    "b",
    "happen",
    "really",
    "equivalent",
    "rule",
    "using",
    "conditional",
    "probability",
    "instead",
    "joint",
    "probability",
    "saying",
    "let",
    "look",
    "two",
    "cases",
    "condition",
    "look",
    "case",
    "b",
    "happens",
    "look",
    "case",
    "b",
    "happen",
    "look",
    "probabilities",
    "get",
    "result",
    "case",
    "marginalization",
    "equivalent",
    "rule",
    "random",
    "variables",
    "could",
    "take",
    "multiple",
    "possible",
    "values",
    "domain",
    "possible",
    "values",
    "conditioning",
    "equivalent",
    "rule",
    "summation",
    "mean",
    "summing",
    "possible",
    "values",
    "random",
    "variable",
    "could",
    "take",
    "want",
    "know",
    "probability",
    "x",
    "takes",
    "value",
    "going",
    "sum",
    "values",
    "j",
    "could",
    "take",
    "say",
    "right",
    "chance",
    "takes",
    "value",
    "yj",
    "multiply",
    "conditional",
    "probability",
    "x",
    "takes",
    "value",
    "given",
    "took",
    "value",
    "yj",
    "equivalent",
    "rule",
    "using",
    "conditional",
    "probabilities",
    "instead",
    "joint",
    "probabilities",
    "using",
    "equation",
    "know",
    "joint",
    "probabilities",
    "translate",
    "two",
    "right",
    "seen",
    "whole",
    "lot",
    "mathematics",
    "laid",
    "foundation",
    "mathematics",
    "need",
    "worry",
    "seen",
    "probability",
    "much",
    "detail",
    "point",
    "foundations",
    "ideas",
    "going",
    "come",
    "begin",
    "explore",
    "take",
    "ideas",
    "probability",
    "begin",
    "apply",
    "represent",
    "something",
    "inside",
    "computer",
    "something",
    "inside",
    "ai",
    "agent",
    "trying",
    "design",
    "able",
    "represent",
    "information",
    "probabilities",
    "likelihoods",
    "various",
    "different",
    "events",
    "number",
    "different",
    "probabilistic",
    "models",
    "generate",
    "first",
    "models",
    "going",
    "talk",
    "known",
    "bayesian",
    "networks",
    "bayesian",
    "network",
    "going",
    "network",
    "random",
    "variables",
    "connected",
    "random",
    "variables",
    "going",
    "represent",
    "dependence",
    "random",
    "variables",
    "odds",
    "random",
    "variables",
    "world",
    "independent",
    "relationship",
    "things",
    "happening",
    "care",
    "rainy",
    "today",
    "might",
    "increase",
    "likelihood",
    "flight",
    "train",
    "gets",
    "delayed",
    "example",
    "dependence",
    "random",
    "variables",
    "bayesian",
    "network",
    "going",
    "able",
    "capture",
    "dependencies",
    "bayesian",
    "network",
    "actual",
    "structure",
    "work",
    "well",
    "bayesian",
    "network",
    "going",
    "directed",
    "graph",
    "seen",
    "directed",
    "graphs",
    "individual",
    "nodes",
    "arrows",
    "edges",
    "connect",
    "one",
    "node",
    "another",
    "node",
    "pointing",
    "particular",
    "direction",
    "directed",
    "graph",
    "going",
    "nodes",
    "well",
    "node",
    "directed",
    "graph",
    "going",
    "represent",
    "random",
    "variable",
    "something",
    "like",
    "weather",
    "something",
    "like",
    "whether",
    "train",
    "time",
    "delayed",
    "going",
    "arrow",
    "node",
    "x",
    "node",
    "mean",
    "x",
    "parent",
    "notation",
    "arrow",
    "x",
    "x",
    "going",
    "considered",
    "parent",
    "reason",
    "important",
    "nodes",
    "going",
    "probability",
    "distribution",
    "going",
    "store",
    "along",
    "distribution",
    "x",
    "given",
    "evidence",
    "given",
    "parents",
    "way",
    "intuitively",
    "think",
    "parents",
    "seem",
    "thought",
    "sort",
    "causes",
    "effect",
    "going",
    "observe",
    "let",
    "take",
    "look",
    "actual",
    "example",
    "bayesian",
    "network",
    "think",
    "types",
    "logic",
    "might",
    "involved",
    "reasoning",
    "network",
    "let",
    "imagine",
    "moment",
    "appointment",
    "town",
    "need",
    "take",
    "train",
    "order",
    "get",
    "appointment",
    "things",
    "might",
    "care",
    "well",
    "care",
    "getting",
    "appointment",
    "time",
    "whether",
    "make",
    "appointment",
    "able",
    "attend",
    "miss",
    "appointment",
    "might",
    "imagine",
    "influenced",
    "train",
    "train",
    "either",
    "time",
    "delayed",
    "example",
    "train",
    "also",
    "influenced",
    "whether",
    "train",
    "time",
    "depends",
    "maybe",
    "rain",
    "rain",
    "light",
    "rain",
    "heavy",
    "rain",
    "might",
    "also",
    "influenced",
    "variables",
    "might",
    "influenced",
    "well",
    "whether",
    "maintenance",
    "train",
    "track",
    "example",
    "maintenance",
    "train",
    "track",
    "probably",
    "increases",
    "likelihood",
    "train",
    "delayed",
    "represent",
    "ideas",
    "using",
    "bayesian",
    "network",
    "looks",
    "little",
    "something",
    "like",
    "four",
    "nodes",
    "representing",
    "four",
    "random",
    "variables",
    "would",
    "like",
    "keep",
    "track",
    "one",
    "random",
    "variable",
    "called",
    "rain",
    "take",
    "three",
    "possible",
    "values",
    "domain",
    "either",
    "none",
    "light",
    "heavy",
    "rain",
    "light",
    "rain",
    "heavy",
    "rain",
    "variable",
    "called",
    "maintenance",
    "whether",
    "maintenance",
    "train",
    "track",
    "two",
    "possible",
    "values",
    "either",
    "yes",
    "either",
    "maintenance",
    "maintenance",
    "happening",
    "track",
    "random",
    "variable",
    "train",
    "indicating",
    "whether",
    "train",
    "time",
    "random",
    "variable",
    "two",
    "possible",
    "values",
    "domain",
    "train",
    "either",
    "time",
    "train",
    "delayed",
    "finally",
    "random",
    "variable",
    "whether",
    "make",
    "appointment",
    "appointment",
    "random",
    "variable",
    "called",
    "appointment",
    "two",
    "possible",
    "values",
    "attend",
    "miss",
    "possible",
    "values",
    "four",
    "nodes",
    "represents",
    "random",
    "variable",
    "domain",
    "possible",
    "values",
    "take",
    "arrows",
    "edges",
    "pointing",
    "one",
    "node",
    "another",
    "encode",
    "notion",
    "dependence",
    "inside",
    "graph",
    "whether",
    "make",
    "appointment",
    "dependent",
    "upon",
    "whether",
    "train",
    "time",
    "delayed",
    "whether",
    "train",
    "time",
    "delayed",
    "dependent",
    "two",
    "things",
    "given",
    "two",
    "arrows",
    "pointing",
    "node",
    "dependent",
    "whether",
    "maintenance",
    "train",
    "track",
    "also",
    "dependent",
    "upon",
    "whether",
    "raining",
    "whether",
    "raining",
    "make",
    "things",
    "little",
    "complicated",
    "let",
    "say",
    "well",
    "whether",
    "maintenance",
    "track",
    "might",
    "influenced",
    "rain",
    "heavier",
    "rain",
    "well",
    "maybe",
    "less",
    "likely",
    "going",
    "maintenance",
    "train",
    "track",
    "day",
    "likely",
    "want",
    "maintenance",
    "track",
    "days",
    "raining",
    "example",
    "nodes",
    "might",
    "different",
    "relationships",
    "idea",
    "come",
    "probability",
    "distribution",
    "nodes",
    "based",
    "upon",
    "parents",
    "let",
    "look",
    "node",
    "node",
    "probability",
    "distribution",
    "might",
    "actually",
    "look",
    "like",
    "go",
    "ahead",
    "begin",
    "root",
    "node",
    "rain",
    "node",
    "top",
    "arrows",
    "pointing",
    "means",
    "probability",
    "distribution",
    "going",
    "conditional",
    "distribution",
    "based",
    "anything",
    "probability",
    "distribution",
    "possible",
    "values",
    "rain",
    "random",
    "variable",
    "distribution",
    "might",
    "look",
    "little",
    "something",
    "like",
    "none",
    "light",
    "heavy",
    "possible",
    "value",
    "saying",
    "likelihood",
    "rain",
    "light",
    "rain",
    "heavy",
    "rain",
    "example",
    "probability",
    "distribution",
    "root",
    "node",
    "bayesian",
    "network",
    "let",
    "consider",
    "next",
    "node",
    "network",
    "maintenance",
    "track",
    "maintenance",
    "yes",
    "general",
    "idea",
    "distribution",
    "going",
    "encode",
    "least",
    "story",
    "idea",
    "heavier",
    "rain",
    "less",
    "likely",
    "going",
    "maintenance",
    "track",
    "people",
    "maintenance",
    "track",
    "probably",
    "want",
    "wait",
    "day",
    "rainy",
    "order",
    "track",
    "maintenance",
    "example",
    "might",
    "probability",
    "distribution",
    "look",
    "like",
    "well",
    "going",
    "conditional",
    "probability",
    "distribution",
    "three",
    "possible",
    "values",
    "rain",
    "random",
    "variable",
    "going",
    "abbreviate",
    "r",
    "either",
    "rain",
    "light",
    "rain",
    "heavy",
    "rain",
    "possible",
    "values",
    "either",
    "yes",
    "track",
    "maintenance",
    "track",
    "maintenance",
    "probabilities",
    "associated",
    "see",
    "raining",
    "probability",
    "track",
    "maintenance",
    "probability",
    "heavy",
    "rain",
    "chance",
    "track",
    "maintenance",
    "chance",
    "track",
    "maintenance",
    "rows",
    "going",
    "sum",
    "represent",
    "different",
    "values",
    "whether",
    "raining",
    "three",
    "possible",
    "values",
    "random",
    "variable",
    "take",
    "associated",
    "probability",
    "distribution",
    "ultimately",
    "going",
    "add",
    "number",
    "distribution",
    "random",
    "variable",
    "called",
    "maintenance",
    "whether",
    "maintenance",
    "train",
    "track",
    "let",
    "consider",
    "next",
    "variable",
    "node",
    "inside",
    "bayesian",
    "network",
    "called",
    "train",
    "two",
    "possible",
    "values",
    "time",
    "delayed",
    "node",
    "going",
    "dependent",
    "upon",
    "two",
    "nodes",
    "pointing",
    "towards",
    "whether",
    "train",
    "time",
    "delayed",
    "depends",
    "whether",
    "track",
    "maintenance",
    "depends",
    "whether",
    "rain",
    "heavier",
    "rain",
    "probably",
    "means",
    "likely",
    "train",
    "delayed",
    "track",
    "maintenance",
    "also",
    "probably",
    "means",
    "likely",
    "train",
    "delayed",
    "well",
    "could",
    "construct",
    "larger",
    "probability",
    "distribution",
    "conditional",
    "probability",
    "distribution",
    "instead",
    "conditioning",
    "one",
    "variable",
    "case",
    "conditioning",
    "two",
    "variables",
    "conditioning",
    "rain",
    "represented",
    "r",
    "maintenance",
    "represented",
    "yes",
    "rows",
    "two",
    "values",
    "sum",
    "number",
    "1",
    "one",
    "whether",
    "train",
    "time",
    "one",
    "whether",
    "train",
    "delayed",
    "say",
    "something",
    "like",
    "right",
    "know",
    "light",
    "rain",
    "track",
    "maintenance",
    "well",
    "ok",
    "would",
    "r",
    "light",
    "yes",
    "well",
    "probability",
    "train",
    "time",
    "probability",
    "train",
    "delayed",
    "imagine",
    "gathering",
    "data",
    "looking",
    "real",
    "world",
    "data",
    "looking",
    "data",
    "right",
    "knew",
    "light",
    "rain",
    "track",
    "maintenance",
    "often",
    "train",
    "delayed",
    "delayed",
    "could",
    "begin",
    "construct",
    "thing",
    "interesting",
    "thing",
    "intelligently",
    "able",
    "try",
    "figure",
    "might",
    "go",
    "ordering",
    "things",
    "things",
    "might",
    "influence",
    "nodes",
    "inside",
    "bayesian",
    "network",
    "last",
    "thing",
    "care",
    "whether",
    "make",
    "appointment",
    "attend",
    "miss",
    "appointment",
    "ultimately",
    "whether",
    "attend",
    "miss",
    "appointment",
    "influenced",
    "track",
    "maintenance",
    "indirectly",
    "idea",
    "right",
    "track",
    "maintenance",
    "well",
    "train",
    "might",
    "likely",
    "delayed",
    "train",
    "likely",
    "delayed",
    "likely",
    "miss",
    "appointment",
    "encode",
    "bayesian",
    "network",
    "might",
    "consider",
    "direct",
    "relationships",
    "train",
    "direct",
    "influence",
    "appointment",
    "given",
    "know",
    "whether",
    "train",
    "time",
    "delayed",
    "knowing",
    "whether",
    "track",
    "maintenance",
    "going",
    "give",
    "additional",
    "information",
    "already",
    "know",
    "train",
    "nodes",
    "really",
    "going",
    "influence",
    "result",
    "might",
    "represent",
    "using",
    "another",
    "conditional",
    "probability",
    "distribution",
    "looks",
    "little",
    "something",
    "like",
    "train",
    "take",
    "two",
    "possible",
    "values",
    "either",
    "train",
    "time",
    "train",
    "delayed",
    "two",
    "possible",
    "values",
    "distribution",
    "odds",
    "able",
    "attend",
    "meeting",
    "odds",
    "missed",
    "meeting",
    "obviously",
    "train",
    "time",
    "much",
    "likely",
    "able",
    "attend",
    "meeting",
    "train",
    "delayed",
    "case",
    "likely",
    "miss",
    "meeting",
    "nodes",
    "put",
    "together",
    "represent",
    "bayesian",
    "network",
    "network",
    "random",
    "variables",
    "whose",
    "values",
    "ultimately",
    "care",
    "sort",
    "relationship",
    "sort",
    "dependence",
    "arrows",
    "one",
    "node",
    "another",
    "indicate",
    "dependence",
    "calculate",
    "probability",
    "node",
    "given",
    "parents",
    "happen",
    "exist",
    "able",
    "describe",
    "structure",
    "bayesian",
    "network",
    "relationships",
    "nodes",
    "associating",
    "nodes",
    "network",
    "probability",
    "distribution",
    "whether",
    "unconditional",
    "probability",
    "distribution",
    "case",
    "root",
    "node",
    "like",
    "rain",
    "conditional",
    "probability",
    "distribution",
    "case",
    "nodes",
    "whose",
    "probabilities",
    "dependent",
    "upon",
    "values",
    "parents",
    "begin",
    "computation",
    "calculation",
    "using",
    "information",
    "inside",
    "table",
    "let",
    "imagine",
    "example",
    "wanted",
    "compute",
    "something",
    "simple",
    "like",
    "probability",
    "light",
    "rain",
    "would",
    "get",
    "probability",
    "light",
    "rain",
    "well",
    "light",
    "rain",
    "rain",
    "root",
    "node",
    "wanted",
    "calculate",
    "probability",
    "could",
    "look",
    "probability",
    "distribution",
    "rain",
    "extract",
    "probability",
    "light",
    "rains",
    "single",
    "value",
    "already",
    "access",
    "could",
    "also",
    "imagine",
    "wanting",
    "compute",
    "complex",
    "joint",
    "probabilities",
    "like",
    "probability",
    "light",
    "rain",
    "also",
    "track",
    "maintenance",
    "joint",
    "probability",
    "two",
    "values",
    "light",
    "rain",
    "track",
    "maintenance",
    "way",
    "might",
    "first",
    "starting",
    "saying",
    "right",
    "well",
    "let",
    "get",
    "probability",
    "light",
    "rain",
    "also",
    "want",
    "probability",
    "track",
    "maintenance",
    "course",
    "node",
    "dependent",
    "upon",
    "value",
    "rain",
    "really",
    "want",
    "probability",
    "track",
    "maintenance",
    "given",
    "know",
    "light",
    "rain",
    "expression",
    "calculating",
    "idea",
    "probability",
    "light",
    "rain",
    "track",
    "maintenance",
    "really",
    "probability",
    "light",
    "rain",
    "probability",
    "track",
    "maintenance",
    "given",
    "know",
    "already",
    "light",
    "rain",
    "take",
    "unconditional",
    "probability",
    "light",
    "rain",
    "multiply",
    "conditional",
    "probability",
    "track",
    "maintenance",
    "given",
    "know",
    "light",
    "rain",
    "continue",
    "every",
    "variable",
    "want",
    "add",
    "joint",
    "probability",
    "might",
    "want",
    "calculate",
    "wanted",
    "know",
    "probability",
    "light",
    "rain",
    "track",
    "maintenance",
    "delayed",
    "train",
    "well",
    "going",
    "probability",
    "light",
    "rain",
    "multiplied",
    "probability",
    "track",
    "maintenance",
    "given",
    "light",
    "rain",
    "multiplied",
    "probability",
    "delayed",
    "train",
    "given",
    "light",
    "rain",
    "track",
    "maintenance",
    "whether",
    "train",
    "time",
    "delayed",
    "dependent",
    "upon",
    "two",
    "variables",
    "two",
    "pieces",
    "evidence",
    "go",
    "calculation",
    "conditional",
    "probability",
    "three",
    "values",
    "value",
    "look",
    "looking",
    "one",
    "individual",
    "probability",
    "distributions",
    "encoded",
    "bayesian",
    "network",
    "wanted",
    "joint",
    "probability",
    "four",
    "variables",
    "something",
    "like",
    "probability",
    "light",
    "rain",
    "track",
    "maintenance",
    "delayed",
    "train",
    "miss",
    "appointment",
    "well",
    "going",
    "multiplying",
    "four",
    "different",
    "values",
    "one",
    "individual",
    "nodes",
    "going",
    "probability",
    "light",
    "rain",
    "track",
    "maintenance",
    "given",
    "light",
    "rain",
    "delayed",
    "train",
    "given",
    "light",
    "rain",
    "track",
    "maintenance",
    "finally",
    "node",
    "whether",
    "make",
    "appointment",
    "dependent",
    "upon",
    "two",
    "variables",
    "given",
    "know",
    "whether",
    "train",
    "time",
    "need",
    "care",
    "conditional",
    "probability",
    "miss",
    "train",
    "miss",
    "appointment",
    "given",
    "train",
    "happens",
    "delayed",
    "represented",
    "four",
    "probabilities",
    "located",
    "inside",
    "one",
    "probability",
    "distributions",
    "nodes",
    "multiplied",
    "together",
    "take",
    "variable",
    "like",
    "figure",
    "joint",
    "probability",
    "multiplying",
    "whole",
    "bunch",
    "individual",
    "probabilities",
    "bayesian",
    "network",
    "course",
    "last",
    "time",
    "really",
    "wanted",
    "able",
    "get",
    "new",
    "pieces",
    "information",
    "going",
    "want",
    "bayesian",
    "network",
    "context",
    "knowledge",
    "talked",
    "problem",
    "inference",
    "given",
    "things",
    "know",
    "true",
    "draw",
    "conclusions",
    "make",
    "deductions",
    "facts",
    "world",
    "also",
    "know",
    "true",
    "going",
    "apply",
    "sort",
    "idea",
    "probability",
    "using",
    "information",
    "knowledge",
    "whether",
    "evidence",
    "probabilities",
    "figure",
    "variables",
    "certain",
    "figure",
    "probabilities",
    "variables",
    "taking",
    "particular",
    "values",
    "introduce",
    "problem",
    "inference",
    "probabilistic",
    "setting",
    "case",
    "variables",
    "might",
    "necessarily",
    "true",
    "sure",
    "might",
    "random",
    "variables",
    "take",
    "different",
    "values",
    "probability",
    "formally",
    "define",
    "exactly",
    "inference",
    "problem",
    "actually",
    "well",
    "inference",
    "problem",
    "couple",
    "parts",
    "query",
    "variable",
    "x",
    "want",
    "compute",
    "distribution",
    "maybe",
    "want",
    "probability",
    "miss",
    "train",
    "want",
    "probability",
    "track",
    "maintenance",
    "something",
    "want",
    "information",
    "evidence",
    "variables",
    "maybe",
    "one",
    "piece",
    "evidence",
    "maybe",
    "multiple",
    "pieces",
    "evidence",
    "observed",
    "certain",
    "variables",
    "sort",
    "event",
    "example",
    "might",
    "observed",
    "raining",
    "evidence",
    "know",
    "light",
    "rain",
    "know",
    "heavy",
    "rain",
    "evidence",
    "using",
    "evidence",
    "want",
    "know",
    "probability",
    "train",
    "delayed",
    "example",
    "query",
    "might",
    "want",
    "ask",
    "based",
    "evidence",
    "query",
    "variable",
    "evidence",
    "variables",
    "observed",
    "inside",
    "bayesian",
    "network",
    "course",
    "leave",
    "hidden",
    "variables",
    "variables",
    "evidence",
    "variables",
    "query",
    "variables",
    "might",
    "imagine",
    "case",
    "know",
    "whether",
    "raining",
    "want",
    "know",
    "whether",
    "train",
    "going",
    "delayed",
    "hidden",
    "variable",
    "thing",
    "access",
    "something",
    "like",
    "maintenance",
    "track",
    "going",
    "make",
    "make",
    "appointment",
    "example",
    "variables",
    "access",
    "hidden",
    "things",
    "observed",
    "also",
    "query",
    "thing",
    "asking",
    "ultimately",
    "want",
    "calculate",
    "want",
    "know",
    "probability",
    "distribution",
    "x",
    "given",
    "e",
    "event",
    "observed",
    "given",
    "observed",
    "event",
    "observed",
    "raining",
    "would",
    "like",
    "know",
    "distribution",
    "possible",
    "values",
    "train",
    "random",
    "variable",
    "time",
    "delayed",
    "likelihood",
    "going",
    "turns",
    "calculation",
    "using",
    "lot",
    "probability",
    "rules",
    "already",
    "seen",
    "action",
    "ultimately",
    "going",
    "take",
    "look",
    "math",
    "little",
    "bit",
    "high",
    "level",
    "abstract",
    "level",
    "ultimately",
    "allow",
    "computers",
    "programming",
    "libraries",
    "already",
    "exist",
    "begin",
    "math",
    "us",
    "good",
    "get",
    "general",
    "sense",
    "actually",
    "happening",
    "inference",
    "process",
    "takes",
    "place",
    "let",
    "imagine",
    "example",
    "want",
    "compute",
    "probability",
    "distribution",
    "appointment",
    "random",
    "variable",
    "given",
    "evidence",
    "given",
    "know",
    "light",
    "rain",
    "track",
    "maintenance",
    "evidence",
    "two",
    "variables",
    "observe",
    "values",
    "observe",
    "value",
    "rain",
    "know",
    "light",
    "rain",
    "know",
    "track",
    "maintenance",
    "going",
    "today",
    "care",
    "knowing",
    "query",
    "random",
    "variable",
    "appointment",
    "want",
    "know",
    "distribution",
    "random",
    "variable",
    "appointment",
    "like",
    "chance",
    "able",
    "attend",
    "appointment",
    "chance",
    "miss",
    "appointment",
    "given",
    "evidence",
    "hidden",
    "variable",
    "information",
    "access",
    "variable",
    "train",
    "information",
    "part",
    "evidence",
    "see",
    "something",
    "observe",
    "also",
    "query",
    "asking",
    "might",
    "inference",
    "procedure",
    "look",
    "like",
    "well",
    "recall",
    "back",
    "defining",
    "conditional",
    "probability",
    "math",
    "conditional",
    "probabilities",
    "know",
    "conditional",
    "probability",
    "proportional",
    "joint",
    "probability",
    "remembered",
    "recalling",
    "probability",
    "given",
    "b",
    "constant",
    "factor",
    "alpha",
    "multiplied",
    "probability",
    "constant",
    "factor",
    "alpha",
    "turns",
    "like",
    "dividing",
    "probability",
    "important",
    "thing",
    "constant",
    "multiplied",
    "joint",
    "distribution",
    "probability",
    "individual",
    "things",
    "happen",
    "case",
    "take",
    "probability",
    "appointment",
    "random",
    "variable",
    "given",
    "light",
    "rain",
    "track",
    "maintenance",
    "say",
    "going",
    "proportional",
    "constant",
    "alpha",
    "multiplied",
    "joint",
    "probability",
    "probability",
    "particular",
    "value",
    "appointment",
    "random",
    "variable",
    "light",
    "rain",
    "track",
    "maintenance",
    "well",
    "right",
    "calculate",
    "probability",
    "appointment",
    "light",
    "rain",
    "track",
    "maintenance",
    "really",
    "care",
    "knowing",
    "need",
    "four",
    "values",
    "able",
    "calculate",
    "joint",
    "distribution",
    "across",
    "everything",
    "particular",
    "appointment",
    "depends",
    "upon",
    "value",
    "train",
    "well",
    "order",
    "begin",
    "use",
    "marginalization",
    "trick",
    "two",
    "ways",
    "get",
    "configuration",
    "appointment",
    "light",
    "rain",
    "track",
    "maintenance",
    "either",
    "particular",
    "setting",
    "variables",
    "happens",
    "train",
    "time",
    "particular",
    "setting",
    "variables",
    "happens",
    "train",
    "delayed",
    "two",
    "possible",
    "cases",
    "would",
    "want",
    "consider",
    "add",
    "two",
    "cases",
    "well",
    "get",
    "result",
    "adding",
    "possibilities",
    "hidden",
    "variable",
    "variables",
    "multiple",
    "since",
    "one",
    "hidden",
    "variable",
    "train",
    "need",
    "iterate",
    "possible",
    "values",
    "hidden",
    "variable",
    "train",
    "add",
    "probabilities",
    "probability",
    "expression",
    "becomes",
    "probability",
    "distribution",
    "appointment",
    "light",
    "rain",
    "train",
    "time",
    "probability",
    "distribution",
    "appointment",
    "light",
    "rain",
    "track",
    "maintenance",
    "train",
    "delayed",
    "example",
    "take",
    "possible",
    "values",
    "train",
    "go",
    "ahead",
    "add",
    "joint",
    "probabilities",
    "saw",
    "earlier",
    "calculate",
    "going",
    "parent",
    "parent",
    "parent",
    "parent",
    "calculating",
    "probabilities",
    "multiplying",
    "together",
    "need",
    "normalize",
    "end",
    "speaking",
    "high",
    "level",
    "make",
    "sure",
    "everything",
    "adds",
    "number",
    "formula",
    "process",
    "known",
    "inference",
    "enumeration",
    "looks",
    "little",
    "bit",
    "complicated",
    "ultimately",
    "looks",
    "like",
    "let",
    "try",
    "distill",
    "symbols",
    "actually",
    "mean",
    "let",
    "start",
    "care",
    "knowing",
    "probability",
    "x",
    "query",
    "variable",
    "given",
    "sort",
    "evidence",
    "know",
    "conditional",
    "probabilities",
    "well",
    "conditional",
    "probability",
    "proportional",
    "joint",
    "probability",
    "alpha",
    "normalizing",
    "constant",
    "multiplied",
    "joint",
    "probability",
    "x",
    "evidence",
    "calculate",
    "well",
    "going",
    "marginalize",
    "hidden",
    "variables",
    "variables",
    "directly",
    "observe",
    "values",
    "basically",
    "going",
    "iterate",
    "possibilities",
    "could",
    "happen",
    "sum",
    "translate",
    "sum",
    "ranges",
    "possible",
    "hidden",
    "variables",
    "values",
    "could",
    "take",
    "adds",
    "possible",
    "individual",
    "probabilities",
    "going",
    "allow",
    "process",
    "inference",
    "enumeration",
    "ultimately",
    "pretty",
    "annoying",
    "humans",
    "math",
    "turns",
    "computers",
    "ai",
    "particularly",
    "helpful",
    "program",
    "computer",
    "understand",
    "bayesian",
    "network",
    "able",
    "understand",
    "inference",
    "procedures",
    "able",
    "calculations",
    "using",
    "information",
    "seen",
    "could",
    "implement",
    "bayesian",
    "network",
    "scratch",
    "turns",
    "lot",
    "libraries",
    "especially",
    "written",
    "python",
    "allow",
    "us",
    "make",
    "easier",
    "sort",
    "probabilistic",
    "inference",
    "able",
    "take",
    "bayesian",
    "network",
    "sorts",
    "calculations",
    "need",
    "know",
    "understand",
    "underlying",
    "math",
    "though",
    "helpful",
    "general",
    "sense",
    "works",
    "need",
    "able",
    "describe",
    "structure",
    "network",
    "make",
    "queries",
    "order",
    "able",
    "produce",
    "result",
    "let",
    "take",
    "look",
    "example",
    "right",
    "turns",
    "lot",
    "possible",
    "libraries",
    "exist",
    "python",
    "sort",
    "inference",
    "matter",
    "much",
    "specific",
    "library",
    "use",
    "behave",
    "fairly",
    "similar",
    "ways",
    "library",
    "going",
    "use",
    "one",
    "known",
    "pomegranate",
    "inside",
    "defined",
    "bayesian",
    "network",
    "using",
    "structure",
    "syntax",
    "pomegranate",
    "library",
    "expects",
    "effectively",
    "python",
    "creating",
    "nodes",
    "represent",
    "nodes",
    "bayesian",
    "network",
    "saw",
    "describe",
    "moment",
    "ago",
    "line",
    "four",
    "imported",
    "pomegranate",
    "defining",
    "variable",
    "called",
    "rain",
    "going",
    "represent",
    "node",
    "inside",
    "bayesian",
    "network",
    "going",
    "node",
    "follows",
    "distribution",
    "three",
    "possible",
    "values",
    "none",
    "rain",
    "light",
    "light",
    "rain",
    "heavy",
    "heavy",
    "rain",
    "probabilities",
    "taking",
    "place",
    "likelihood",
    "rain",
    "light",
    "rain",
    "heavy",
    "rain",
    "go",
    "next",
    "variable",
    "variable",
    "track",
    "maintenance",
    "example",
    "dependent",
    "upon",
    "rain",
    "variable",
    "instead",
    "unconditional",
    "distribution",
    "conditional",
    "distribution",
    "indicated",
    "conditional",
    "probability",
    "table",
    "idea",
    "following",
    "conditional",
    "distribution",
    "rain",
    "rain",
    "chance",
    "yes",
    "track",
    "maintenance",
    "rain",
    "chance",
    "track",
    "maintenance",
    "likewise",
    "light",
    "rain",
    "distribution",
    "heavy",
    "rain",
    "distribution",
    "well",
    "effectively",
    "encoding",
    "information",
    "saw",
    "represented",
    "graphically",
    "moment",
    "ago",
    "telling",
    "python",
    "program",
    "maintenance",
    "node",
    "obeys",
    "particular",
    "conditional",
    "probability",
    "distribution",
    "thing",
    "random",
    "variables",
    "well",
    "train",
    "node",
    "inside",
    "distribution",
    "conditional",
    "probability",
    "table",
    "two",
    "parents",
    "dependent",
    "rain",
    "also",
    "track",
    "maintenance",
    "saying",
    "something",
    "like",
    "given",
    "rain",
    "yes",
    "track",
    "maintenance",
    "probability",
    "train",
    "time",
    "probability",
    "delayed",
    "likewise",
    "thing",
    "possible",
    "values",
    "parents",
    "train",
    "node",
    "inside",
    "bayesian",
    "network",
    "saying",
    "possible",
    "values",
    "distribution",
    "train",
    "node",
    "follow",
    "thing",
    "appointment",
    "based",
    "distribution",
    "variable",
    "train",
    "end",
    "actually",
    "construct",
    "network",
    "describing",
    "states",
    "network",
    "adding",
    "edges",
    "dependent",
    "nodes",
    "create",
    "new",
    "bayesian",
    "network",
    "add",
    "states",
    "one",
    "rain",
    "one",
    "maintenance",
    "one",
    "train",
    "one",
    "appointment",
    "add",
    "edges",
    "connecting",
    "related",
    "pieces",
    "rain",
    "arrow",
    "maintenance",
    "rain",
    "influences",
    "track",
    "maintenance",
    "rain",
    "also",
    "influences",
    "train",
    "maintenance",
    "also",
    "influences",
    "train",
    "train",
    "influences",
    "whether",
    "make",
    "appointment",
    "bake",
    "finalizes",
    "model",
    "additional",
    "computation",
    "specific",
    "syntax",
    "really",
    "important",
    "part",
    "pomegranate",
    "happens",
    "one",
    "several",
    "different",
    "libraries",
    "used",
    "similar",
    "purposes",
    "could",
    "describe",
    "define",
    "library",
    "implemented",
    "similar",
    "things",
    "key",
    "idea",
    "someone",
    "design",
    "library",
    "general",
    "bayesian",
    "network",
    "nodes",
    "based",
    "upon",
    "parents",
    "programmer",
    "needs",
    "using",
    "one",
    "libraries",
    "define",
    "nodes",
    "probability",
    "distributions",
    "begin",
    "interesting",
    "logic",
    "based",
    "let",
    "try",
    "conditional",
    "joint",
    "probability",
    "calculation",
    "saw",
    "us",
    "hand",
    "going",
    "importing",
    "model",
    "defined",
    "moment",
    "ago",
    "like",
    "calculate",
    "calculates",
    "probability",
    "given",
    "observation",
    "like",
    "calculate",
    "probability",
    "rain",
    "track",
    "maintenance",
    "train",
    "time",
    "able",
    "attend",
    "meeting",
    "sort",
    "optimal",
    "scenario",
    "rain",
    "maintenance",
    "track",
    "train",
    "time",
    "able",
    "attend",
    "meeting",
    "probability",
    "actually",
    "happens",
    "calculate",
    "using",
    "library",
    "print",
    "probability",
    "go",
    "ahead",
    "run",
    "python",
    "see",
    "ok",
    "probability",
    "third",
    "time",
    "everything",
    "goes",
    "right",
    "case",
    "rain",
    "track",
    "maintenance",
    "train",
    "time",
    "able",
    "attend",
    "meeting",
    "could",
    "experiment",
    "try",
    "calculate",
    "probabilities",
    "well",
    "probability",
    "everything",
    "goes",
    "right",
    "train",
    "still",
    "miss",
    "meeting",
    "rain",
    "track",
    "maintenance",
    "train",
    "time",
    "miss",
    "appointment",
    "let",
    "calculate",
    "probability",
    "right",
    "probability",
    "4",
    "time",
    "train",
    "time",
    "wo",
    "rain",
    "track",
    "maintenance",
    "yet",
    "still",
    "miss",
    "meeting",
    "really",
    "implementation",
    "calculation",
    "joint",
    "probabilities",
    "library",
    "likely",
    "first",
    "figuring",
    "probability",
    "rain",
    "figuring",
    "probability",
    "track",
    "maintenance",
    "given",
    "rain",
    "probability",
    "train",
    "time",
    "given",
    "values",
    "probability",
    "miss",
    "appointment",
    "given",
    "know",
    "train",
    "time",
    "calculation",
    "joint",
    "probability",
    "turns",
    "also",
    "begin",
    "computer",
    "solve",
    "inference",
    "problems",
    "well",
    "begin",
    "infer",
    "based",
    "information",
    "evidence",
    "see",
    "likelihood",
    "variables",
    "also",
    "true",
    "let",
    "go",
    "example",
    "importing",
    "exact",
    "model",
    "importing",
    "nodes",
    "edges",
    "probability",
    "distribution",
    "encoded",
    "well",
    "function",
    "sort",
    "prediction",
    "model",
    "pass",
    "evidence",
    "observe",
    "encoded",
    "python",
    "program",
    "evidence",
    "observed",
    "observed",
    "fact",
    "train",
    "delayed",
    "value",
    "one",
    "four",
    "random",
    "variables",
    "inside",
    "bayesian",
    "network",
    "using",
    "information",
    "would",
    "like",
    "able",
    "draw",
    "inspiration",
    "figure",
    "inferences",
    "values",
    "random",
    "variables",
    "inside",
    "bayesian",
    "network",
    "would",
    "like",
    "make",
    "predictions",
    "everything",
    "else",
    "actual",
    "computational",
    "logic",
    "happening",
    "three",
    "lines",
    "making",
    "call",
    "prediction",
    "iterating",
    "states",
    "predictions",
    "printing",
    "visually",
    "see",
    "results",
    "let",
    "find",
    "given",
    "train",
    "delayed",
    "predict",
    "values",
    "random",
    "variables",
    "let",
    "go",
    "ahead",
    "run",
    "python",
    "run",
    "right",
    "result",
    "get",
    "given",
    "fact",
    "know",
    "train",
    "delayed",
    "evidence",
    "observed",
    "well",
    "given",
    "45",
    "chance",
    "46",
    "chance",
    "rain",
    "31",
    "chance",
    "light",
    "rain",
    "23",
    "chance",
    "heavy",
    "rain",
    "see",
    "probability",
    "distribution",
    "track",
    "maintenance",
    "probability",
    "distribution",
    "whether",
    "able",
    "attend",
    "miss",
    "appointment",
    "know",
    "whether",
    "attend",
    "miss",
    "appointment",
    "dependent",
    "upon",
    "train",
    "delayed",
    "delayed",
    "depend",
    "anything",
    "else",
    "let",
    "imagine",
    "example",
    "knew",
    "heavy",
    "rain",
    "affect",
    "distribution",
    "making",
    "appointment",
    "indeed",
    "go",
    "add",
    "evidence",
    "say",
    "know",
    "value",
    "rain",
    "heavy",
    "evidence",
    "access",
    "two",
    "pieces",
    "evidence",
    "know",
    "rain",
    "heavy",
    "know",
    "train",
    "delayed",
    "calculate",
    "probability",
    "running",
    "inference",
    "procedure",
    "seeing",
    "result",
    "know",
    "rain",
    "heavy",
    "know",
    "train",
    "delayed",
    "probability",
    "distribution",
    "track",
    "maintenance",
    "changed",
    "given",
    "know",
    "heavy",
    "rain",
    "likely",
    "track",
    "maintenance",
    "88",
    "opposed",
    "64",
    "probability",
    "make",
    "appointment",
    "well",
    "still",
    "going",
    "attend",
    "appointment",
    "probability",
    "missed",
    "appointment",
    "probability",
    "dependent",
    "upon",
    "whether",
    "train",
    "time",
    "delayed",
    "implementing",
    "idea",
    "inference",
    "algorithm",
    "able",
    "figure",
    "based",
    "evidence",
    "infer",
    "values",
    "variables",
    "exist",
    "well",
    "inference",
    "enumeration",
    "one",
    "way",
    "inference",
    "procedure",
    "looping",
    "values",
    "hidden",
    "variables",
    "could",
    "take",
    "figuring",
    "probability",
    "turns",
    "particularly",
    "efficient",
    "definitely",
    "optimizations",
    "make",
    "avoiding",
    "repeated",
    "work",
    "calculating",
    "sort",
    "probability",
    "multiple",
    "times",
    "ways",
    "optimizing",
    "program",
    "avoid",
    "recalculate",
    "probabilities",
    "even",
    "number",
    "variables",
    "get",
    "large",
    "number",
    "possible",
    "values",
    "variables",
    "could",
    "take",
    "get",
    "large",
    "going",
    "start",
    "lot",
    "computation",
    "lot",
    "calculation",
    "able",
    "inference",
    "point",
    "might",
    "start",
    "get",
    "unreasonable",
    "terms",
    "amount",
    "time",
    "would",
    "take",
    "able",
    "sort",
    "exact",
    "inference",
    "reason",
    "oftentimes",
    "comes",
    "towards",
    "probability",
    "things",
    "entirely",
    "sure",
    "always",
    "care",
    "exact",
    "inference",
    "knowing",
    "exactly",
    "probability",
    "approximate",
    "inference",
    "procedure",
    "sort",
    "approximate",
    "inference",
    "pretty",
    "good",
    "well",
    "know",
    "exact",
    "probability",
    "general",
    "sense",
    "probability",
    "get",
    "increasingly",
    "accurate",
    "time",
    "probably",
    "pretty",
    "good",
    "especially",
    "get",
    "happen",
    "even",
    "faster",
    "could",
    "approximate",
    "inference",
    "inside",
    "bayesian",
    "network",
    "well",
    "one",
    "method",
    "procedure",
    "known",
    "sampling",
    "process",
    "sampling",
    "going",
    "take",
    "sample",
    "variables",
    "inside",
    "bayesian",
    "network",
    "going",
    "sample",
    "well",
    "going",
    "sample",
    "one",
    "values",
    "nodes",
    "according",
    "probability",
    "distribution",
    "might",
    "take",
    "sample",
    "nodes",
    "well",
    "start",
    "root",
    "start",
    "rain",
    "distribution",
    "rain",
    "go",
    "ahead",
    "using",
    "random",
    "number",
    "generator",
    "something",
    "like",
    "randomly",
    "pick",
    "one",
    "three",
    "values",
    "pick",
    "none",
    "probability",
    "light",
    "probability",
    "heavy",
    "probability",
    "randomly",
    "pick",
    "one",
    "according",
    "distribution",
    "maybe",
    "case",
    "pick",
    "none",
    "example",
    "thing",
    "variable",
    "maintenance",
    "also",
    "probability",
    "distribution",
    "going",
    "sample",
    "three",
    "probability",
    "distributions",
    "going",
    "sample",
    "first",
    "row",
    "observed",
    "already",
    "sample",
    "value",
    "rain",
    "none",
    "given",
    "rain",
    "none",
    "going",
    "sample",
    "distribution",
    "say",
    "right",
    "value",
    "maintenance",
    "case",
    "maintenance",
    "going",
    "let",
    "say",
    "yes",
    "happens",
    "40",
    "time",
    "event",
    "rain",
    "example",
    "sample",
    "rest",
    "nodes",
    "way",
    "well",
    "want",
    "sample",
    "train",
    "distribution",
    "sample",
    "first",
    "row",
    "rain",
    "track",
    "maintenance",
    "sample",
    "80",
    "time",
    "say",
    "train",
    "time",
    "20",
    "time",
    "say",
    "train",
    "delayed",
    "finally",
    "thing",
    "whether",
    "make",
    "appointment",
    "attend",
    "miss",
    "appointment",
    "sample",
    "based",
    "distribution",
    "maybe",
    "say",
    "case",
    "attend",
    "appointment",
    "happens",
    "90",
    "time",
    "train",
    "actually",
    "time",
    "going",
    "nodes",
    "quickly",
    "sampling",
    "get",
    "sample",
    "possible",
    "values",
    "could",
    "come",
    "going",
    "entire",
    "bayesian",
    "network",
    "according",
    "probability",
    "distributions",
    "becomes",
    "powerful",
    "thousands",
    "tens",
    "thousands",
    "times",
    "generate",
    "whole",
    "bunch",
    "samples",
    "using",
    "distribution",
    "get",
    "different",
    "samples",
    "maybe",
    "get",
    "value",
    "possible",
    "variables",
    "could",
    "come",
    "ever",
    "faced",
    "question",
    "question",
    "like",
    "probability",
    "train",
    "time",
    "could",
    "exact",
    "inference",
    "procedure",
    "different",
    "inference",
    "problem",
    "could",
    "marginalize",
    "look",
    "possible",
    "values",
    "variables",
    "computation",
    "inference",
    "enumeration",
    "find",
    "probability",
    "exactly",
    "could",
    "also",
    "care",
    "exact",
    "probability",
    "sample",
    "approximate",
    "get",
    "close",
    "powerful",
    "tool",
    "ai",
    "need",
    "right",
    "100",
    "time",
    "need",
    "exactly",
    "right",
    "need",
    "right",
    "probability",
    "often",
    "effectively",
    "efficiently",
    "possible",
    "samples",
    "highlight",
    "ones",
    "train",
    "time",
    "ignoring",
    "ones",
    "train",
    "delayed",
    "case",
    "like",
    "six",
    "eight",
    "samples",
    "train",
    "arriving",
    "time",
    "maybe",
    "case",
    "say",
    "six",
    "eight",
    "cases",
    "likelihood",
    "train",
    "time",
    "eight",
    "samples",
    "might",
    "great",
    "prediction",
    "thousands",
    "upon",
    "thousands",
    "samples",
    "could",
    "much",
    "better",
    "inference",
    "procedure",
    "able",
    "sorts",
    "calculations",
    "direct",
    "sampling",
    "method",
    "bunch",
    "samples",
    "figure",
    "probability",
    "event",
    "unconditional",
    "probability",
    "probability",
    "train",
    "time",
    "looking",
    "samples",
    "figuring",
    "right",
    "ones",
    "train",
    "time",
    "sometimes",
    "want",
    "calculate",
    "unconditional",
    "probability",
    "rather",
    "conditional",
    "probability",
    "something",
    "like",
    "probability",
    "light",
    "rain",
    "given",
    "train",
    "time",
    "something",
    "effect",
    "kind",
    "calculation",
    "well",
    "might",
    "samples",
    "want",
    "calculate",
    "probability",
    "distribution",
    "given",
    "know",
    "train",
    "time",
    "able",
    "kind",
    "look",
    "two",
    "cases",
    "train",
    "delayed",
    "ignore",
    "reject",
    "sort",
    "exclude",
    "possible",
    "samples",
    "considering",
    "want",
    "look",
    "remaining",
    "cases",
    "train",
    "time",
    "cases",
    "light",
    "rain",
    "say",
    "ok",
    "two",
    "six",
    "possible",
    "cases",
    "give",
    "approximation",
    "probability",
    "light",
    "rain",
    "given",
    "fact",
    "know",
    "train",
    "time",
    "almost",
    "exactly",
    "way",
    "adding",
    "additional",
    "step",
    "saying",
    "right",
    "take",
    "sample",
    "let",
    "reject",
    "samples",
    "match",
    "evidence",
    "consider",
    "samples",
    "match",
    "evidence",
    "want",
    "make",
    "sort",
    "calculation",
    "turns",
    "using",
    "libraries",
    "bayesian",
    "networks",
    "begin",
    "implement",
    "sort",
    "idea",
    "like",
    "implement",
    "rejection",
    "sampling",
    "method",
    "called",
    "able",
    "figure",
    "probability",
    "via",
    "direct",
    "inference",
    "instead",
    "sampling",
    "program",
    "called",
    "imports",
    "exact",
    "model",
    "define",
    "first",
    "program",
    "generate",
    "sample",
    "way",
    "generate",
    "sample",
    "looping",
    "states",
    "states",
    "need",
    "sort",
    "order",
    "make",
    "sure",
    "looping",
    "correct",
    "order",
    "effectively",
    "conditional",
    "distribution",
    "going",
    "sample",
    "based",
    "parents",
    "otherwise",
    "going",
    "directly",
    "sample",
    "variable",
    "like",
    "rain",
    "parents",
    "unconditional",
    "distribution",
    "keep",
    "track",
    "parent",
    "samples",
    "return",
    "final",
    "sample",
    "exact",
    "syntax",
    "particularly",
    "important",
    "happens",
    "part",
    "implementation",
    "details",
    "particular",
    "library",
    "interesting",
    "logic",
    "ability",
    "generate",
    "sample",
    "want",
    "know",
    "distribution",
    "appointment",
    "random",
    "variable",
    "given",
    "train",
    "delayed",
    "well",
    "begin",
    "calculations",
    "like",
    "let",
    "take",
    "samples",
    "assemble",
    "results",
    "list",
    "called",
    "data",
    "go",
    "ahead",
    "loop",
    "n",
    "times",
    "case",
    "times",
    "generate",
    "sample",
    "want",
    "know",
    "distribution",
    "appointment",
    "given",
    "train",
    "delayed",
    "according",
    "rejection",
    "sampling",
    "going",
    "consider",
    "samples",
    "train",
    "delayed",
    "train",
    "delayed",
    "going",
    "consider",
    "values",
    "going",
    "say",
    "right",
    "take",
    "sample",
    "look",
    "value",
    "train",
    "random",
    "variable",
    "train",
    "delayed",
    "well",
    "let",
    "go",
    "ahead",
    "add",
    "data",
    "collecting",
    "value",
    "appointment",
    "random",
    "variable",
    "took",
    "particular",
    "sample",
    "considering",
    "samples",
    "train",
    "delayed",
    "samples",
    "considering",
    "value",
    "appointment",
    "end",
    "using",
    "python",
    "class",
    "called",
    "counter",
    "quickly",
    "counts",
    "values",
    "inside",
    "data",
    "set",
    "take",
    "list",
    "data",
    "figure",
    "many",
    "times",
    "appointment",
    "made",
    "many",
    "times",
    "appointment",
    "missed",
    "couple",
    "lines",
    "code",
    "implementation",
    "rejection",
    "sampling",
    "run",
    "going",
    "ahead",
    "running",
    "python",
    "result",
    "get",
    "result",
    "counter",
    "times",
    "able",
    "attend",
    "meeting",
    "856",
    "times",
    "able",
    "miss",
    "meeting",
    "imagine",
    "samples",
    "able",
    "get",
    "better",
    "better",
    "accurate",
    "result",
    "randomized",
    "process",
    "going",
    "approximation",
    "probability",
    "run",
    "different",
    "time",
    "notice",
    "numbers",
    "similar",
    "12",
    "72",
    "identical",
    "randomization",
    "likelihood",
    "things",
    "might",
    "higher",
    "lower",
    "generally",
    "want",
    "try",
    "use",
    "samples",
    "greater",
    "amount",
    "confidence",
    "result",
    "sure",
    "result",
    "getting",
    "whether",
    "accurately",
    "reflects",
    "represents",
    "actual",
    "underlying",
    "probabilities",
    "inherent",
    "inside",
    "distribution",
    "instance",
    "rejection",
    "sampling",
    "turns",
    "number",
    "sampling",
    "methods",
    "could",
    "use",
    "begin",
    "try",
    "sample",
    "one",
    "problem",
    "rejection",
    "sampling",
    "evidence",
    "looking",
    "fairly",
    "unlikely",
    "event",
    "well",
    "going",
    "rejecting",
    "lot",
    "samples",
    "like",
    "looking",
    "probability",
    "x",
    "given",
    "evidence",
    "e",
    "e",
    "unlikely",
    "occur",
    "like",
    "occurs",
    "maybe",
    "one",
    "every",
    "times",
    "going",
    "considering",
    "1",
    "every",
    "samples",
    "pretty",
    "inefficient",
    "method",
    "trying",
    "sort",
    "calculation",
    "throwing",
    "away",
    "lot",
    "samples",
    "takes",
    "computational",
    "effort",
    "able",
    "generate",
    "samples",
    "like",
    "something",
    "like",
    "sampling",
    "methods",
    "try",
    "address",
    "one",
    "sampling",
    "method",
    "called",
    "likelihood",
    "weighting",
    "likelihood",
    "weighting",
    "follow",
    "slightly",
    "different",
    "procedure",
    "goal",
    "avoid",
    "needing",
    "throw",
    "samples",
    "match",
    "evidence",
    "start",
    "fixing",
    "values",
    "evidence",
    "variables",
    "rather",
    "sample",
    "everything",
    "going",
    "fix",
    "values",
    "evidence",
    "variables",
    "sample",
    "going",
    "sample",
    "variables",
    "way",
    "using",
    "bayesian",
    "network",
    "looking",
    "probability",
    "distributions",
    "sampling",
    "variables",
    "need",
    "weight",
    "sample",
    "likelihood",
    "evidence",
    "really",
    "unlikely",
    "want",
    "make",
    "sure",
    "taken",
    "account",
    "likely",
    "evidence",
    "actually",
    "show",
    "sample",
    "sample",
    "evidence",
    "much",
    "likely",
    "show",
    "another",
    "sample",
    "want",
    "weight",
    "likely",
    "one",
    "higher",
    "going",
    "weight",
    "sample",
    "likelihood",
    "likelihood",
    "defined",
    "probability",
    "evidence",
    "given",
    "evidence",
    "probability",
    "would",
    "happen",
    "particular",
    "sample",
    "samples",
    "weighted",
    "equally",
    "weight",
    "1",
    "calculating",
    "overall",
    "average",
    "case",
    "going",
    "weight",
    "sample",
    "multiply",
    "sample",
    "likelihood",
    "order",
    "get",
    "accurate",
    "distribution",
    "would",
    "look",
    "like",
    "well",
    "ask",
    "question",
    "probability",
    "light",
    "rain",
    "given",
    "train",
    "time",
    "sampling",
    "procedure",
    "start",
    "trying",
    "sample",
    "going",
    "start",
    "fixing",
    "evidence",
    "variable",
    "already",
    "going",
    "sample",
    "train",
    "time",
    "way",
    "throw",
    "anything",
    "sampling",
    "things",
    "know",
    "value",
    "variables",
    "evidence",
    "expect",
    "go",
    "ahead",
    "sample",
    "rain",
    "maybe",
    "time",
    "sample",
    "light",
    "rain",
    "instead",
    "rain",
    "sample",
    "track",
    "maintenance",
    "say",
    "maybe",
    "yes",
    "track",
    "maintenance",
    "train",
    "well",
    "already",
    "fixed",
    "place",
    "train",
    "evidence",
    "variable",
    "going",
    "bother",
    "sampling",
    "go",
    "ahead",
    "move",
    "move",
    "appointment",
    "go",
    "ahead",
    "sample",
    "appointment",
    "well",
    "generated",
    "sample",
    "generated",
    "sample",
    "fixing",
    "evidence",
    "variable",
    "sampling",
    "three",
    "last",
    "step",
    "weighting",
    "sample",
    "much",
    "weight",
    "weight",
    "based",
    "probable",
    "train",
    "actually",
    "time",
    "evidence",
    "actually",
    "happened",
    "given",
    "values",
    "variables",
    "light",
    "rain",
    "fact",
    "yes",
    "track",
    "maintenance",
    "well",
    "go",
    "back",
    "train",
    "variable",
    "say",
    "right",
    "light",
    "rain",
    "track",
    "maintenance",
    "likelihood",
    "evidence",
    "likelihood",
    "train",
    "time",
    "particular",
    "sample",
    "would",
    "weight",
    "could",
    "repeat",
    "sampling",
    "procedure",
    "time",
    "every",
    "sample",
    "would",
    "given",
    "weight",
    "according",
    "probability",
    "evidence",
    "see",
    "associated",
    "sampling",
    "methods",
    "exist",
    "well",
    "designed",
    "try",
    "get",
    "idea",
    "approximate",
    "inference",
    "procedure",
    "figuring",
    "value",
    "variable",
    "dealt",
    "probability",
    "pertains",
    "particular",
    "variables",
    "discrete",
    "values",
    "really",
    "considered",
    "values",
    "might",
    "change",
    "time",
    "considered",
    "something",
    "like",
    "variable",
    "rain",
    "rain",
    "take",
    "values",
    "none",
    "light",
    "rain",
    "heavy",
    "rain",
    "practice",
    "usually",
    "consider",
    "values",
    "variables",
    "like",
    "rain",
    "like",
    "consider",
    "time",
    "values",
    "variables",
    "change",
    "dealing",
    "uncertainty",
    "period",
    "time",
    "come",
    "context",
    "weather",
    "example",
    "sunny",
    "days",
    "rainy",
    "days",
    "like",
    "know",
    "probability",
    "raining",
    "probability",
    "rains",
    "tomorrow",
    "day",
    "day",
    "going",
    "introduce",
    "slightly",
    "different",
    "kind",
    "model",
    "going",
    "random",
    "variable",
    "one",
    "weather",
    "every",
    "possible",
    "time",
    "step",
    "define",
    "time",
    "step",
    "however",
    "like",
    "simple",
    "way",
    "use",
    "days",
    "time",
    "step",
    "define",
    "variable",
    "called",
    "x",
    "sub",
    "going",
    "weather",
    "time",
    "x",
    "sub",
    "0",
    "might",
    "weather",
    "day",
    "x",
    "sub",
    "1",
    "might",
    "weather",
    "day",
    "1",
    "forth",
    "x",
    "sub",
    "2",
    "weather",
    "day",
    "imagine",
    "start",
    "longer",
    "longer",
    "periods",
    "time",
    "incredible",
    "amount",
    "data",
    "might",
    "go",
    "keeping",
    "track",
    "data",
    "weather",
    "year",
    "suddenly",
    "might",
    "trying",
    "predict",
    "weather",
    "tomorrow",
    "given",
    "365",
    "days",
    "previous",
    "pieces",
    "evidence",
    "lot",
    "evidence",
    "deal",
    "manipulate",
    "calculate",
    "probably",
    "nobody",
    "knows",
    "exact",
    "conditional",
    "probability",
    "distribution",
    "combinations",
    "variables",
    "trying",
    "inference",
    "inside",
    "computer",
    "trying",
    "reasonably",
    "sort",
    "analysis",
    "helpful",
    "make",
    "simplifying",
    "assumptions",
    "assumptions",
    "problem",
    "assume",
    "true",
    "make",
    "lives",
    "little",
    "bit",
    "easier",
    "even",
    "totally",
    "accurate",
    "assumptions",
    "close",
    "accurate",
    "approximate",
    "usually",
    "pretty",
    "good",
    "assumption",
    "going",
    "make",
    "called",
    "markov",
    "assumption",
    "assumption",
    "current",
    "state",
    "depends",
    "finite",
    "fixed",
    "number",
    "previous",
    "states",
    "current",
    "day",
    "weather",
    "depends",
    "previous",
    "day",
    "weather",
    "rest",
    "history",
    "current",
    "day",
    "weather",
    "predict",
    "based",
    "yesterday",
    "weather",
    "based",
    "last",
    "two",
    "days",
    "weather",
    "last",
    "three",
    "days",
    "weather",
    "oftentimes",
    "going",
    "deal",
    "one",
    "previous",
    "state",
    "helps",
    "predict",
    "current",
    "state",
    "putting",
    "whole",
    "bunch",
    "random",
    "variables",
    "together",
    "using",
    "markov",
    "assumption",
    "create",
    "called",
    "markov",
    "chain",
    "markov",
    "chain",
    "sequence",
    "random",
    "variables",
    "variables",
    "distribution",
    "follows",
    "markov",
    "assumption",
    "example",
    "markov",
    "assumption",
    "predict",
    "weather",
    "sunny",
    "rainy",
    "consider",
    "two",
    "possibilities",
    "even",
    "though",
    "types",
    "weather",
    "predict",
    "day",
    "weather",
    "prior",
    "day",
    "weather",
    "using",
    "today",
    "weather",
    "come",
    "probability",
    "distribution",
    "tomorrow",
    "weather",
    "weather",
    "might",
    "look",
    "like",
    "formatted",
    "terms",
    "matrix",
    "might",
    "describe",
    "rows",
    "columns",
    "values",
    "side",
    "today",
    "weather",
    "represented",
    "variable",
    "x",
    "sub",
    "columns",
    "tomorrow",
    "weather",
    "represented",
    "variable",
    "x",
    "sub",
    "plus",
    "1",
    "plus",
    "1",
    "day",
    "weather",
    "instead",
    "matrix",
    "saying",
    "today",
    "sunny",
    "well",
    "likely",
    "tomorrow",
    "also",
    "sunny",
    "oftentimes",
    "weather",
    "stays",
    "consistent",
    "multiple",
    "days",
    "row",
    "example",
    "let",
    "say",
    "today",
    "sunny",
    "model",
    "says",
    "tomorrow",
    "probability",
    "also",
    "sunny",
    "probability",
    "raining",
    "likewise",
    "today",
    "raining",
    "likely",
    "tomorrow",
    "also",
    "raining",
    "probability",
    "raining",
    "probability",
    "sunny",
    "matrix",
    "description",
    "transition",
    "one",
    "state",
    "next",
    "state",
    "going",
    "call",
    "transition",
    "model",
    "using",
    "transition",
    "model",
    "begin",
    "construct",
    "markov",
    "chain",
    "predicting",
    "given",
    "today",
    "weather",
    "likelihood",
    "tomorrow",
    "weather",
    "happening",
    "imagine",
    "similar",
    "sampling",
    "procedure",
    "take",
    "information",
    "sample",
    "tomorrow",
    "weather",
    "going",
    "using",
    "sample",
    "next",
    "day",
    "weather",
    "result",
    "form",
    "markov",
    "chain",
    "like",
    "x0",
    "time",
    "time",
    "day",
    "zero",
    "sunny",
    "next",
    "day",
    "sunny",
    "maybe",
    "next",
    "day",
    "changes",
    "raining",
    "raining",
    "raining",
    "pattern",
    "markov",
    "chain",
    "follows",
    "given",
    "distribution",
    "access",
    "transition",
    "model",
    "sunny",
    "tends",
    "stay",
    "sunny",
    "little",
    "next",
    "couple",
    "days",
    "tend",
    "sunny",
    "raining",
    "tends",
    "raining",
    "well",
    "get",
    "markov",
    "chain",
    "looks",
    "like",
    "analysis",
    "say",
    "given",
    "today",
    "raining",
    "probability",
    "tomorrow",
    "raining",
    "begin",
    "ask",
    "probability",
    "questions",
    "like",
    "probability",
    "sequence",
    "five",
    "values",
    "sun",
    "sun",
    "rain",
    "rain",
    "rain",
    "answer",
    "sorts",
    "questions",
    "turns",
    "many",
    "python",
    "libraries",
    "interacting",
    "models",
    "like",
    "probabilities",
    "distributions",
    "random",
    "variables",
    "based",
    "previous",
    "variables",
    "according",
    "markov",
    "assumption",
    "pomegranate2",
    "ways",
    "dealing",
    "sorts",
    "variables",
    "go",
    "ahead",
    "go",
    "chain",
    "directory",
    "information",
    "markov",
    "chains",
    "defined",
    "file",
    "called",
    "defined",
    "similar",
    "syntax",
    "exact",
    "syntax",
    "matter",
    "much",
    "idea",
    "encoding",
    "information",
    "python",
    "program",
    "program",
    "access",
    "distributions",
    "defined",
    "starting",
    "distribution",
    "every",
    "markov",
    "model",
    "begins",
    "point",
    "time",
    "need",
    "give",
    "starting",
    "distribution",
    "say",
    "know",
    "start",
    "pick",
    "sunny",
    "rainy",
    "say",
    "sunny",
    "50",
    "time",
    "rainy",
    "50",
    "time",
    "defined",
    "transition",
    "model",
    "transition",
    "one",
    "day",
    "next",
    "encoded",
    "exact",
    "matrix",
    "sunny",
    "today",
    "probability",
    "sunny",
    "tomorrow",
    "rainy",
    "tomorrow",
    "probability",
    "likewise",
    "another",
    "distribution",
    "raining",
    "today",
    "instead",
    "alone",
    "defines",
    "markov",
    "model",
    "begin",
    "answer",
    "questions",
    "using",
    "model",
    "one",
    "thing",
    "sample",
    "markov",
    "chain",
    "turns",
    "method",
    "built",
    "markov",
    "chain",
    "library",
    "allows",
    "sample",
    "50",
    "states",
    "chain",
    "basically",
    "simulating",
    "like",
    "50",
    "instances",
    "weather",
    "let",
    "go",
    "ahead",
    "run",
    "python",
    "run",
    "get",
    "going",
    "sample",
    "markov",
    "chain",
    "50",
    "states",
    "50",
    "days",
    "worth",
    "weather",
    "going",
    "randomly",
    "sample",
    "imagine",
    "sampling",
    "many",
    "times",
    "able",
    "get",
    "data",
    "able",
    "analysis",
    "example",
    "sunny",
    "two",
    "days",
    "row",
    "rainy",
    "whole",
    "bunch",
    "days",
    "row",
    "changes",
    "back",
    "sun",
    "get",
    "model",
    "follows",
    "distribution",
    "originally",
    "described",
    "follows",
    "distribution",
    "sunny",
    "days",
    "tend",
    "lead",
    "sunny",
    "days",
    "rainy",
    "days",
    "tend",
    "lead",
    "rainy",
    "days",
    "markov",
    "model",
    "markov",
    "models",
    "rely",
    "us",
    "knowing",
    "values",
    "individual",
    "states",
    "know",
    "today",
    "sunny",
    "today",
    "raining",
    "using",
    "information",
    "draw",
    "sort",
    "inference",
    "tomorrow",
    "going",
    "like",
    "practice",
    "often",
    "case",
    "often",
    "case",
    "know",
    "certain",
    "exact",
    "state",
    "world",
    "oftentimes",
    "state",
    "world",
    "exactly",
    "unknown",
    "able",
    "somehow",
    "sense",
    "information",
    "state",
    "robot",
    "ai",
    "exact",
    "knowledge",
    "world",
    "around",
    "sort",
    "sensor",
    "whether",
    "sensor",
    "camera",
    "sensors",
    "detect",
    "distance",
    "microphone",
    "sensing",
    "audio",
    "example",
    "sensing",
    "data",
    "using",
    "data",
    "data",
    "somehow",
    "related",
    "state",
    "world",
    "even",
    "actually",
    "know",
    "ai",
    "know",
    "underlying",
    "true",
    "state",
    "world",
    "actually",
    "need",
    "get",
    "world",
    "sensor",
    "models",
    "way",
    "describing",
    "translate",
    "hidden",
    "state",
    "underlying",
    "true",
    "state",
    "world",
    "observation",
    "ai",
    "knows",
    "ai",
    "access",
    "actually",
    "example",
    "hidden",
    "state",
    "might",
    "robot",
    "position",
    "robot",
    "exploring",
    "new",
    "uncharted",
    "territory",
    "robot",
    "likely",
    "know",
    "exactly",
    "observation",
    "robot",
    "sensor",
    "data",
    "sense",
    "far",
    "away",
    "possible",
    "obstacles",
    "around",
    "using",
    "information",
    "using",
    "observed",
    "information",
    "infer",
    "something",
    "hidden",
    "state",
    "true",
    "hidden",
    "state",
    "influences",
    "observations",
    "whatever",
    "robot",
    "true",
    "position",
    "affects",
    "effect",
    "upon",
    "sensor",
    "data",
    "robot",
    "able",
    "collect",
    "even",
    "robot",
    "actually",
    "know",
    "certain",
    "true",
    "position",
    "likewise",
    "think",
    "voice",
    "recognition",
    "speech",
    "recognition",
    "program",
    "listens",
    "able",
    "respond",
    "something",
    "like",
    "alexa",
    "apple",
    "google",
    "voice",
    "recognition",
    "well",
    "might",
    "imagine",
    "hidden",
    "state",
    "underlying",
    "state",
    "words",
    "actually",
    "spoken",
    "true",
    "nature",
    "world",
    "contains",
    "saying",
    "particular",
    "sequence",
    "words",
    "phone",
    "smart",
    "home",
    "device",
    "know",
    "sure",
    "exactly",
    "words",
    "said",
    "observation",
    "ai",
    "access",
    "audio",
    "waveforms",
    "audio",
    "waveforms",
    "course",
    "dependent",
    "upon",
    "hidden",
    "state",
    "infer",
    "based",
    "audio",
    "waveforms",
    "words",
    "spoken",
    "likely",
    "might",
    "know",
    "100",
    "certainty",
    "hidden",
    "state",
    "actually",
    "might",
    "task",
    "try",
    "predict",
    "given",
    "observation",
    "given",
    "audio",
    "waveforms",
    "figure",
    "actual",
    "words",
    "spoken",
    "likewise",
    "might",
    "imagine",
    "website",
    "true",
    "user",
    "engagement",
    "might",
    "information",
    "directly",
    "access",
    "observe",
    "data",
    "like",
    "website",
    "app",
    "analytics",
    "often",
    "button",
    "clicked",
    "often",
    "people",
    "interacting",
    "page",
    "particular",
    "way",
    "use",
    "infer",
    "things",
    "users",
    "well",
    "type",
    "problem",
    "comes",
    "time",
    "dealing",
    "ai",
    "trying",
    "infer",
    "things",
    "world",
    "often",
    "ai",
    "really",
    "know",
    "hidden",
    "true",
    "state",
    "world",
    "ai",
    "access",
    "observation",
    "related",
    "hidden",
    "true",
    "state",
    "direct",
    "might",
    "noise",
    "audio",
    "waveform",
    "might",
    "additional",
    "noise",
    "might",
    "difficult",
    "parse",
    "sensor",
    "data",
    "might",
    "exactly",
    "correct",
    "noise",
    "might",
    "allow",
    "conclude",
    "certainty",
    "hidden",
    "state",
    "allow",
    "infer",
    "might",
    "simple",
    "example",
    "take",
    "look",
    "imagining",
    "hidden",
    "state",
    "weather",
    "whether",
    "sunny",
    "rainy",
    "imagine",
    "programming",
    "ai",
    "inside",
    "building",
    "maybe",
    "access",
    "camera",
    "inside",
    "building",
    "access",
    "observation",
    "whether",
    "employees",
    "bringing",
    "umbrella",
    "building",
    "detect",
    "whether",
    "umbrella",
    "might",
    "observation",
    "whether",
    "umbrella",
    "brought",
    "building",
    "using",
    "information",
    "want",
    "predict",
    "whether",
    "sunny",
    "rainy",
    "even",
    "know",
    "underlying",
    "weather",
    "underlying",
    "weather",
    "might",
    "sunny",
    "rainy",
    "raining",
    "obviously",
    "people",
    "likely",
    "bring",
    "umbrella",
    "whether",
    "people",
    "bring",
    "umbrella",
    "observation",
    "tells",
    "something",
    "hidden",
    "state",
    "course",
    "bit",
    "contrived",
    "example",
    "idea",
    "think",
    "broadly",
    "terms",
    "generally",
    "time",
    "observe",
    "something",
    "underlying",
    "hidden",
    "state",
    "try",
    "model",
    "type",
    "idea",
    "hidden",
    "states",
    "observations",
    "rather",
    "use",
    "markov",
    "model",
    "state",
    "state",
    "state",
    "state",
    "connected",
    "transition",
    "matrix",
    "described",
    "going",
    "use",
    "call",
    "hidden",
    "markov",
    "model",
    "similar",
    "markov",
    "model",
    "going",
    "allow",
    "us",
    "model",
    "system",
    "hidden",
    "states",
    "directly",
    "observe",
    "along",
    "observed",
    "event",
    "actually",
    "see",
    "addition",
    "transition",
    "model",
    "still",
    "need",
    "saying",
    "given",
    "underlying",
    "state",
    "world",
    "sunny",
    "rainy",
    "probability",
    "tomorrow",
    "weather",
    "also",
    "need",
    "another",
    "model",
    "given",
    "state",
    "going",
    "give",
    "us",
    "observation",
    "green",
    "yes",
    "someone",
    "brings",
    "umbrella",
    "office",
    "red",
    "nobody",
    "brings",
    "umbrellas",
    "office",
    "observation",
    "might",
    "sunny",
    "odds",
    "nobody",
    "going",
    "bring",
    "umbrella",
    "office",
    "maybe",
    "people",
    "cautious",
    "bring",
    "umbrella",
    "office",
    "anyways",
    "raining",
    "much",
    "higher",
    "probability",
    "people",
    "going",
    "bring",
    "umbrellas",
    "office",
    "maybe",
    "rain",
    "unexpected",
    "people",
    "bring",
    "umbrella",
    "might",
    "probability",
    "well",
    "using",
    "observations",
    "begin",
    "predict",
    "reasonable",
    "likelihood",
    "underlying",
    "state",
    "even",
    "actually",
    "get",
    "observe",
    "underlying",
    "state",
    "get",
    "see",
    "hidden",
    "state",
    "actually",
    "equal",
    "often",
    "call",
    "sensor",
    "model",
    "also",
    "often",
    "called",
    "emission",
    "probabilities",
    "state",
    "underlying",
    "state",
    "emits",
    "sort",
    "emission",
    "observe",
    "another",
    "way",
    "describing",
    "idea",
    "sensor",
    "markov",
    "assumption",
    "going",
    "use",
    "assumption",
    "evidence",
    "variable",
    "thing",
    "observe",
    "emission",
    "gets",
    "produced",
    "depends",
    "corresponding",
    "state",
    "meaning",
    "predict",
    "whether",
    "people",
    "bring",
    "umbrellas",
    "entirely",
    "dependent",
    "whether",
    "sunny",
    "rainy",
    "today",
    "course",
    "assumption",
    "might",
    "hold",
    "practice",
    "practice",
    "might",
    "depend",
    "whether",
    "people",
    "bring",
    "umbrellas",
    "might",
    "depend",
    "today",
    "weather",
    "also",
    "yesterday",
    "weather",
    "day",
    "simplification",
    "purposes",
    "helpful",
    "apply",
    "sort",
    "assumption",
    "allow",
    "us",
    "able",
    "reason",
    "probabilities",
    "little",
    "easily",
    "able",
    "approximate",
    "still",
    "often",
    "get",
    "good",
    "answer",
    "hidden",
    "markov",
    "models",
    "end",
    "looking",
    "like",
    "little",
    "something",
    "like",
    "rather",
    "one",
    "chain",
    "states",
    "like",
    "sun",
    "sun",
    "rain",
    "rain",
    "rain",
    "instead",
    "upper",
    "level",
    "underlying",
    "state",
    "world",
    "sunny",
    "rainy",
    "connected",
    "transition",
    "matrix",
    "described",
    "states",
    "produces",
    "emission",
    "produces",
    "observation",
    "see",
    "day",
    "sunny",
    "people",
    "bring",
    "umbrellas",
    "day",
    "sunny",
    "people",
    "bring",
    "umbrellas",
    "day",
    "raining",
    "people",
    "bring",
    "umbrellas",
    "forth",
    "underlying",
    "states",
    "represented",
    "x",
    "sub",
    "x",
    "sub",
    "1",
    "0",
    "1",
    "2",
    "forth",
    "produces",
    "sort",
    "observation",
    "emission",
    "e",
    "stands",
    "e",
    "sub",
    "0",
    "e",
    "sub",
    "1",
    "e",
    "sub",
    "2",
    "forth",
    "way",
    "trying",
    "represent",
    "idea",
    "want",
    "think",
    "underlying",
    "states",
    "true",
    "nature",
    "world",
    "robot",
    "position",
    "moves",
    "time",
    "produces",
    "sort",
    "sensor",
    "data",
    "might",
    "observed",
    "people",
    "actually",
    "saying",
    "using",
    "emission",
    "data",
    "audio",
    "waveforms",
    "detect",
    "order",
    "process",
    "data",
    "try",
    "figure",
    "number",
    "possible",
    "tasks",
    "might",
    "want",
    "given",
    "kind",
    "information",
    "one",
    "simplest",
    "trying",
    "infer",
    "something",
    "future",
    "past",
    "sort",
    "hidden",
    "states",
    "might",
    "exist",
    "tasks",
    "often",
    "see",
    "going",
    "go",
    "mathematics",
    "tasks",
    "based",
    "idea",
    "conditional",
    "probabilities",
    "using",
    "probability",
    "distributions",
    "draw",
    "sorts",
    "conclusions",
    "one",
    "task",
    "called",
    "filtering",
    "given",
    "observations",
    "start",
    "calculate",
    "distribution",
    "current",
    "state",
    "meaning",
    "given",
    "information",
    "beginning",
    "time",
    "days",
    "people",
    "bring",
    "umbrella",
    "bring",
    "umbrella",
    "calculate",
    "probability",
    "current",
    "state",
    "today",
    "sunny",
    "raining",
    "another",
    "task",
    "might",
    "possible",
    "prediction",
    "looking",
    "towards",
    "future",
    "given",
    "observations",
    "people",
    "bringing",
    "umbrellas",
    "beginning",
    "started",
    "counting",
    "time",
    "figure",
    "distribution",
    "tomorrow",
    "sunny",
    "raining",
    "also",
    "go",
    "backwards",
    "well",
    "smoothing",
    "say",
    "given",
    "observations",
    "start",
    "calculate",
    "distributions",
    "past",
    "state",
    "like",
    "know",
    "today",
    "people",
    "brought",
    "umbrellas",
    "tomorrow",
    "people",
    "brought",
    "umbrellas",
    "given",
    "two",
    "days",
    "worth",
    "data",
    "people",
    "bringing",
    "umbrellas",
    "probability",
    "yesterday",
    "raining",
    "know",
    "people",
    "brought",
    "umbrellas",
    "today",
    "might",
    "inform",
    "decision",
    "well",
    "might",
    "influence",
    "probabilities",
    "also",
    "likely",
    "explanation",
    "task",
    "addition",
    "tasks",
    "might",
    "exist",
    "well",
    "combining",
    "given",
    "observations",
    "start",
    "figuring",
    "likely",
    "sequence",
    "states",
    "going",
    "take",
    "look",
    "idea",
    "observations",
    "umbrella",
    "umbrella",
    "umbrella",
    "umbrella",
    "calculate",
    "likely",
    "states",
    "sun",
    "rain",
    "sun",
    "rain",
    "whatnot",
    "actually",
    "represented",
    "true",
    "weather",
    "would",
    "produce",
    "observations",
    "quite",
    "common",
    "trying",
    "something",
    "like",
    "voice",
    "recognition",
    "example",
    "emissions",
    "audio",
    "waveforms",
    "would",
    "like",
    "calculate",
    "based",
    "observations",
    "likely",
    "sequence",
    "actual",
    "words",
    "syllables",
    "sounds",
    "user",
    "actually",
    "made",
    "speaking",
    "particular",
    "device",
    "tasks",
    "might",
    "come",
    "context",
    "well",
    "try",
    "going",
    "ahead",
    "going",
    "hmm",
    "directory",
    "hmm",
    "hidden",
    "markov",
    "model",
    "done",
    "defined",
    "model",
    "model",
    "first",
    "defines",
    "possible",
    "state",
    "sun",
    "rain",
    "along",
    "emission",
    "probabilities",
    "observation",
    "model",
    "emission",
    "model",
    "given",
    "know",
    "sunny",
    "probability",
    "see",
    "people",
    "bring",
    "umbrella",
    "probability",
    "umbrella",
    "likewise",
    "raining",
    "people",
    "likely",
    "bring",
    "umbrella",
    "umbrella",
    "probability",
    "umbrella",
    "probability",
    "actual",
    "underlying",
    "hidden",
    "states",
    "states",
    "sun",
    "rain",
    "things",
    "observe",
    "observations",
    "see",
    "either",
    "umbrella",
    "umbrella",
    "things",
    "observe",
    "result",
    "also",
    "need",
    "add",
    "transition",
    "matrix",
    "saying",
    "today",
    "sunny",
    "tomorrow",
    "likely",
    "sunny",
    "today",
    "rainy",
    "tomorrow",
    "likely",
    "raining",
    "give",
    "starting",
    "probabilities",
    "saying",
    "first",
    "chance",
    "whether",
    "sunny",
    "rainy",
    "create",
    "model",
    "based",
    "information",
    "exact",
    "syntax",
    "important",
    "much",
    "data",
    "encoding",
    "program",
    "begin",
    "inference",
    "give",
    "program",
    "example",
    "list",
    "observations",
    "umbrella",
    "umbrella",
    "umbrella",
    "umbrella",
    "umbrella",
    "forth",
    "umbrella",
    "umbrella",
    "would",
    "like",
    "calculate",
    "would",
    "like",
    "figure",
    "likely",
    "explanation",
    "observations",
    "likely",
    "whether",
    "rain",
    "rain",
    "rain",
    "likely",
    "actually",
    "sunny",
    "switched",
    "back",
    "rainy",
    "interesting",
    "question",
    "might",
    "sure",
    "might",
    "happened",
    "rainy",
    "day",
    "people",
    "decided",
    "bring",
    "umbrella",
    "could",
    "switched",
    "rainy",
    "sunny",
    "back",
    "rainy",
    "seem",
    "likely",
    "certainly",
    "could",
    "happen",
    "using",
    "data",
    "give",
    "hidden",
    "markov",
    "model",
    "model",
    "begin",
    "predict",
    "answers",
    "begin",
    "figure",
    "going",
    "go",
    "ahead",
    "predict",
    "observations",
    "predictions",
    "go",
    "ahead",
    "print",
    "prediction",
    "library",
    "happens",
    "function",
    "called",
    "predict",
    "prediction",
    "process",
    "run",
    "python",
    "result",
    "get",
    "prediction",
    "based",
    "observations",
    "states",
    "likely",
    "likely",
    "rain",
    "rain",
    "case",
    "thinks",
    "likely",
    "happened",
    "sunny",
    "day",
    "went",
    "back",
    "rainy",
    "different",
    "situations",
    "rainy",
    "longer",
    "maybe",
    "probabilities",
    "slightly",
    "different",
    "might",
    "imagine",
    "likely",
    "rainy",
    "way",
    "happened",
    "one",
    "rainy",
    "day",
    "people",
    "decided",
    "bring",
    "umbrellas",
    "python",
    "libraries",
    "begin",
    "allow",
    "sort",
    "inference",
    "procedure",
    "taking",
    "know",
    "putting",
    "terms",
    "tasks",
    "already",
    "exist",
    "general",
    "tasks",
    "work",
    "hidden",
    "markov",
    "models",
    "time",
    "take",
    "idea",
    "formulate",
    "hidden",
    "markov",
    "model",
    "formulate",
    "something",
    "hidden",
    "states",
    "observed",
    "emissions",
    "result",
    "states",
    "take",
    "advantage",
    "algorithms",
    "known",
    "exist",
    "trying",
    "sort",
    "inference",
    "seen",
    "couple",
    "ways",
    "ai",
    "begin",
    "deal",
    "uncertainty",
    "taken",
    "look",
    "probability",
    "use",
    "probability",
    "describe",
    "numerically",
    "things",
    "likely",
    "likely",
    "less",
    "likely",
    "happen",
    "events",
    "variables",
    "using",
    "information",
    "begin",
    "construct",
    "standard",
    "types",
    "models",
    "things",
    "like",
    "bayesian",
    "networks",
    "markov",
    "chains",
    "hidden",
    "markov",
    "models",
    "allow",
    "us",
    "able",
    "describe",
    "particular",
    "events",
    "relate",
    "events",
    "values",
    "particular",
    "variables",
    "relate",
    "variables",
    "certain",
    "sort",
    "probability",
    "distribution",
    "formulating",
    "things",
    "terms",
    "models",
    "already",
    "exist",
    "take",
    "advantage",
    "python",
    "libraries",
    "implement",
    "sort",
    "models",
    "already",
    "allow",
    "us",
    "able",
    "use",
    "produce",
    "sort",
    "resulting",
    "effect",
    "allows",
    "ai",
    "begin",
    "deal",
    "sort",
    "uncertain",
    "problems",
    "ai",
    "need",
    "know",
    "things",
    "certain",
    "infer",
    "based",
    "information",
    "know",
    "next",
    "time",
    "take",
    "look",
    "additional",
    "types",
    "problems",
    "solve",
    "taking",
    "advantage",
    "algorithms",
    "even",
    "beyond",
    "world",
    "types",
    "problems",
    "already",
    "explored",
    "see",
    "next",
    "time",
    "welcome",
    "back",
    "everyone",
    "introduction",
    "artificial",
    "intelligence",
    "python",
    "far",
    "taken",
    "look",
    "couple",
    "different",
    "types",
    "problems",
    "seen",
    "classical",
    "search",
    "problems",
    "trying",
    "get",
    "initial",
    "state",
    "goal",
    "figuring",
    "optimal",
    "path",
    "taken",
    "look",
    "adversarial",
    "search",
    "agent",
    "trying",
    "make",
    "best",
    "move",
    "seen",
    "problems",
    "trying",
    "use",
    "logic",
    "inference",
    "able",
    "figure",
    "draw",
    "additional",
    "conclusions",
    "seen",
    "probabilistic",
    "models",
    "well",
    "might",
    "certain",
    "information",
    "world",
    "want",
    "use",
    "knowledge",
    "probabilities",
    "able",
    "draw",
    "conclusions",
    "today",
    "going",
    "turn",
    "attention",
    "another",
    "category",
    "problems",
    "generally",
    "known",
    "optimization",
    "problems",
    "optimization",
    "really",
    "choosing",
    "best",
    "option",
    "set",
    "possible",
    "options",
    "already",
    "seen",
    "optimization",
    "contexts",
    "like",
    "trying",
    "create",
    "ai",
    "chooses",
    "best",
    "move",
    "set",
    "possible",
    "moves",
    "take",
    "look",
    "today",
    "category",
    "types",
    "problems",
    "algorithms",
    "solve",
    "used",
    "order",
    "deal",
    "broader",
    "range",
    "potential",
    "optimization",
    "problems",
    "first",
    "algorithms",
    "take",
    "look",
    "known",
    "local",
    "search",
    "local",
    "search",
    "differs",
    "search",
    "algorithms",
    "seen",
    "sense",
    "search",
    "algorithms",
    "looked",
    "far",
    "things",
    "like",
    "search",
    "search",
    "example",
    "generally",
    "maintain",
    "whole",
    "bunch",
    "different",
    "paths",
    "simultaneously",
    "exploring",
    "looking",
    "bunch",
    "different",
    "paths",
    "trying",
    "find",
    "way",
    "solution",
    "hand",
    "local",
    "search",
    "going",
    "search",
    "algorithm",
    "really",
    "going",
    "maintain",
    "single",
    "node",
    "looking",
    "single",
    "state",
    "generally",
    "run",
    "algorithm",
    "maintaining",
    "single",
    "node",
    "moving",
    "one",
    "neighboring",
    "nodes",
    "throughout",
    "search",
    "process",
    "generally",
    "useful",
    "context",
    "like",
    "problems",
    "seen",
    "like",
    "situation",
    "trying",
    "find",
    "way",
    "initial",
    "state",
    "goal",
    "following",
    "path",
    "local",
    "search",
    "applicable",
    "really",
    "care",
    "path",
    "care",
    "solution",
    "case",
    "solving",
    "maze",
    "solution",
    "always",
    "obvious",
    "could",
    "point",
    "solution",
    "know",
    "exactly",
    "goal",
    "real",
    "question",
    "path",
    "get",
    "local",
    "search",
    "going",
    "come",
    "cases",
    "figuring",
    "exactly",
    "solution",
    "exactly",
    "goal",
    "looks",
    "like",
    "actually",
    "heart",
    "challenge",
    "give",
    "example",
    "one",
    "kinds",
    "problems",
    "consider",
    "scenario",
    "two",
    "types",
    "buildings",
    "example",
    "houses",
    "hospitals",
    "goal",
    "might",
    "world",
    "formatted",
    "grid",
    "whole",
    "bunch",
    "houses",
    "house",
    "house",
    "two",
    "houses",
    "maybe",
    "want",
    "try",
    "find",
    "way",
    "place",
    "two",
    "hospitals",
    "map",
    "maybe",
    "hospital",
    "hospital",
    "problem",
    "want",
    "place",
    "two",
    "hospitals",
    "map",
    "want",
    "sort",
    "objective",
    "objective",
    "case",
    "try",
    "minimize",
    "distance",
    "houses",
    "hospital",
    "might",
    "imagine",
    "right",
    "distance",
    "houses",
    "nearest",
    "hospital",
    "number",
    "ways",
    "could",
    "calculate",
    "distance",
    "one",
    "way",
    "using",
    "heuristic",
    "looked",
    "manhattan",
    "distance",
    "idea",
    "many",
    "rows",
    "columns",
    "would",
    "move",
    "inside",
    "grid",
    "layout",
    "order",
    "get",
    "hospital",
    "example",
    "turns",
    "take",
    "four",
    "houses",
    "figure",
    "right",
    "close",
    "nearest",
    "hospital",
    "get",
    "something",
    "like",
    "house",
    "three",
    "away",
    "hospital",
    "house",
    "six",
    "away",
    "two",
    "houses",
    "four",
    "away",
    "add",
    "numbers",
    "together",
    "get",
    "total",
    "cost",
    "17",
    "example",
    "particular",
    "configuration",
    "hospitals",
    "hospital",
    "hospital",
    "state",
    "might",
    "say",
    "cost",
    "goal",
    "problem",
    "would",
    "like",
    "apply",
    "search",
    "algorithm",
    "figure",
    "solve",
    "problem",
    "find",
    "way",
    "minimize",
    "cost",
    "minimize",
    "total",
    "amount",
    "sum",
    "distances",
    "houses",
    "nearest",
    "hospital",
    "minimize",
    "final",
    "value",
    "think",
    "problem",
    "little",
    "bit",
    "abstractly",
    "abstracting",
    "away",
    "specific",
    "problem",
    "thinking",
    "generally",
    "problems",
    "like",
    "often",
    "formulate",
    "problems",
    "thinking",
    "landscape",
    "soon",
    "call",
    "diagram",
    "landscape",
    "vertical",
    "bars",
    "represents",
    "particular",
    "state",
    "world",
    "could",
    "example",
    "vertical",
    "bars",
    "represents",
    "particular",
    "configuration",
    "two",
    "hospitals",
    "height",
    "vertical",
    "bar",
    "generally",
    "going",
    "represent",
    "function",
    "state",
    "value",
    "state",
    "maybe",
    "case",
    "height",
    "vertical",
    "bar",
    "represents",
    "cost",
    "particular",
    "configuration",
    "hospitals",
    "terms",
    "sum",
    "total",
    "distances",
    "houses",
    "nearest",
    "hospital",
    "generally",
    "speaking",
    "landscape",
    "want",
    "one",
    "two",
    "things",
    "might",
    "trying",
    "maximize",
    "value",
    "function",
    "trying",
    "find",
    "global",
    "maximum",
    "speak",
    "landscape",
    "single",
    "state",
    "whose",
    "value",
    "higher",
    "states",
    "could",
    "possibly",
    "choose",
    "generally",
    "case",
    "trying",
    "find",
    "global",
    "maximum",
    "call",
    "function",
    "trying",
    "optimize",
    "objective",
    "function",
    "function",
    "measures",
    "given",
    "state",
    "good",
    "state",
    "take",
    "state",
    "pass",
    "objective",
    "function",
    "get",
    "value",
    "good",
    "state",
    "ultimately",
    "goal",
    "find",
    "one",
    "states",
    "highest",
    "possible",
    "value",
    "objective",
    "function",
    "equivalent",
    "reversed",
    "problem",
    "problem",
    "finding",
    "global",
    "minimum",
    "state",
    "value",
    "pass",
    "function",
    "lower",
    "possible",
    "values",
    "might",
    "choose",
    "generally",
    "speaking",
    "trying",
    "find",
    "global",
    "minimum",
    "call",
    "function",
    "calculating",
    "cost",
    "function",
    "generally",
    "state",
    "sort",
    "cost",
    "whether",
    "cost",
    "monetary",
    "cost",
    "time",
    "cost",
    "case",
    "houses",
    "hospitals",
    "looking",
    "distance",
    "cost",
    "terms",
    "far",
    "away",
    "houses",
    "hospital",
    "trying",
    "minimize",
    "cost",
    "find",
    "state",
    "lowest",
    "possible",
    "value",
    "cost",
    "general",
    "types",
    "ideas",
    "might",
    "trying",
    "go",
    "within",
    "state",
    "space",
    "landscape",
    "trying",
    "find",
    "global",
    "maximum",
    "trying",
    "find",
    "global",
    "minimum",
    "exactly",
    "recall",
    "local",
    "search",
    "generally",
    "operate",
    "algorithm",
    "maintaining",
    "single",
    "state",
    "current",
    "state",
    "represented",
    "inside",
    "node",
    "maybe",
    "inside",
    "data",
    "structure",
    "keeping",
    "track",
    "currently",
    "ultimately",
    "going",
    "state",
    "move",
    "one",
    "neighbor",
    "states",
    "case",
    "represented",
    "space",
    "state",
    "immediately",
    "left",
    "right",
    "different",
    "problem",
    "might",
    "define",
    "means",
    "neighbor",
    "particular",
    "state",
    "case",
    "hospital",
    "example",
    "looking",
    "neighbor",
    "might",
    "moving",
    "one",
    "hospital",
    "one",
    "space",
    "left",
    "right",
    "state",
    "close",
    "current",
    "state",
    "slightly",
    "different",
    "result",
    "might",
    "slightly",
    "different",
    "value",
    "terms",
    "objective",
    "function",
    "terms",
    "cost",
    "function",
    "going",
    "general",
    "strategy",
    "local",
    "search",
    "able",
    "take",
    "state",
    "maintaining",
    "current",
    "node",
    "move",
    "looking",
    "state",
    "space",
    "landscape",
    "order",
    "try",
    "find",
    "global",
    "maximum",
    "global",
    "minimum",
    "somehow",
    "perhaps",
    "simplest",
    "algorithms",
    "could",
    "use",
    "implement",
    "idea",
    "local",
    "search",
    "algorithm",
    "known",
    "hill",
    "climbing",
    "basic",
    "idea",
    "hill",
    "climbing",
    "let",
    "say",
    "trying",
    "maximize",
    "value",
    "state",
    "trying",
    "figure",
    "global",
    "maximum",
    "going",
    "start",
    "state",
    "generally",
    "hill",
    "climbing",
    "going",
    "going",
    "consider",
    "neighbors",
    "state",
    "state",
    "right",
    "could",
    "go",
    "left",
    "could",
    "go",
    "right",
    "neighbor",
    "happens",
    "higher",
    "neighbor",
    "happens",
    "lower",
    "hill",
    "climbing",
    "trying",
    "maximize",
    "value",
    "generally",
    "pick",
    "highest",
    "one",
    "state",
    "left",
    "right",
    "one",
    "higher",
    "go",
    "ahead",
    "move",
    "consider",
    "state",
    "instead",
    "repeat",
    "process",
    "continually",
    "looking",
    "neighbors",
    "picking",
    "highest",
    "neighbor",
    "thing",
    "looking",
    "neighbors",
    "picking",
    "highest",
    "neighbors",
    "get",
    "point",
    "like",
    "right",
    "consider",
    "neighbors",
    "neighbors",
    "lower",
    "value",
    "current",
    "state",
    "value",
    "higher",
    "neighbors",
    "point",
    "algorithm",
    "terminates",
    "say",
    "right",
    "found",
    "solution",
    "thing",
    "works",
    "exactly",
    "opposite",
    "way",
    "trying",
    "find",
    "global",
    "minimum",
    "algorithm",
    "fundamentally",
    "trying",
    "find",
    "global",
    "minimum",
    "say",
    "current",
    "state",
    "starts",
    "continually",
    "look",
    "neighbors",
    "pick",
    "lowest",
    "value",
    "possibly",
    "eventually",
    "hopefully",
    "find",
    "global",
    "minimum",
    "point",
    "look",
    "neighbors",
    "higher",
    "value",
    "trying",
    "minimize",
    "total",
    "score",
    "cost",
    "value",
    "get",
    "result",
    "calculating",
    "sort",
    "cost",
    "function",
    "formulate",
    "graphical",
    "idea",
    "terms",
    "pseudocode",
    "pseudocode",
    "hill",
    "climbing",
    "might",
    "look",
    "like",
    "define",
    "function",
    "called",
    "hill",
    "climb",
    "takes",
    "input",
    "problem",
    "trying",
    "solve",
    "generally",
    "going",
    "start",
    "sort",
    "initial",
    "state",
    "start",
    "variable",
    "called",
    "current",
    "keeping",
    "track",
    "initial",
    "state",
    "like",
    "initial",
    "configuration",
    "hospitals",
    "maybe",
    "problems",
    "lend",
    "initial",
    "state",
    "place",
    "begin",
    "cases",
    "maybe",
    "case",
    "might",
    "randomly",
    "generate",
    "initial",
    "state",
    "choosing",
    "two",
    "locations",
    "hospitals",
    "random",
    "example",
    "figuring",
    "might",
    "able",
    "improve",
    "initial",
    "state",
    "going",
    "store",
    "inside",
    "current",
    "comes",
    "loop",
    "repetitive",
    "process",
    "going",
    "algorithm",
    "terminates",
    "going",
    "first",
    "say",
    "let",
    "figure",
    "neighbors",
    "current",
    "state",
    "state",
    "neighboring",
    "states",
    "definition",
    "means",
    "neighbor",
    "go",
    "ahead",
    "choose",
    "highest",
    "value",
    "neighbors",
    "save",
    "inside",
    "variable",
    "called",
    "neighbor",
    "keep",
    "track",
    "neighbor",
    "case",
    "trying",
    "maximize",
    "value",
    "case",
    "trying",
    "minimize",
    "value",
    "might",
    "imagine",
    "pick",
    "neighbor",
    "lowest",
    "possible",
    "value",
    "ideas",
    "really",
    "fundamentally",
    "interchangeable",
    "possible",
    "cases",
    "might",
    "multiple",
    "neighbors",
    "equally",
    "high",
    "value",
    "equally",
    "low",
    "value",
    "minimizing",
    "case",
    "case",
    "choose",
    "randomly",
    "among",
    "choose",
    "one",
    "save",
    "inside",
    "variable",
    "neighbor",
    "key",
    "question",
    "ask",
    "neighbor",
    "better",
    "current",
    "state",
    "neighbor",
    "best",
    "neighbor",
    "able",
    "find",
    "better",
    "current",
    "state",
    "well",
    "algorithm",
    "go",
    "ahead",
    "return",
    "current",
    "state",
    "none",
    "neighbors",
    "better",
    "may",
    "well",
    "stay",
    "general",
    "logic",
    "hill",
    "climbing",
    "algorithm",
    "otherwise",
    "neighbor",
    "better",
    "may",
    "well",
    "move",
    "neighbor",
    "might",
    "imagine",
    "setting",
    "current",
    "equal",
    "neighbor",
    "general",
    "idea",
    "current",
    "state",
    "see",
    "neighbor",
    "better",
    "go",
    "ahead",
    "move",
    "repeat",
    "process",
    "continually",
    "moving",
    "better",
    "neighbor",
    "reach",
    "point",
    "none",
    "neighbors",
    "better",
    "point",
    "say",
    "algorithm",
    "terminate",
    "let",
    "take",
    "look",
    "real",
    "example",
    "houses",
    "hospitals",
    "seen",
    "put",
    "hospitals",
    "two",
    "locations",
    "total",
    "cost",
    "need",
    "define",
    "going",
    "implement",
    "hill",
    "climbing",
    "algorithm",
    "means",
    "take",
    "particular",
    "configuration",
    "hospitals",
    "particular",
    "state",
    "get",
    "neighbor",
    "state",
    "simple",
    "definition",
    "neighbor",
    "might",
    "let",
    "pick",
    "one",
    "hospitals",
    "move",
    "one",
    "square",
    "left",
    "right",
    "example",
    "would",
    "mean",
    "six",
    "possible",
    "neighbors",
    "particular",
    "configuration",
    "could",
    "take",
    "hospital",
    "move",
    "three",
    "possible",
    "squares",
    "take",
    "hospital",
    "move",
    "three",
    "possible",
    "squares",
    "would",
    "generate",
    "neighbor",
    "might",
    "say",
    "right",
    "locations",
    "distances",
    "houses",
    "nearest",
    "hospital",
    "let",
    "consider",
    "neighbors",
    "see",
    "better",
    "cost",
    "turns",
    "couple",
    "ways",
    "could",
    "matter",
    "randomly",
    "choose",
    "among",
    "ways",
    "best",
    "one",
    "possible",
    "way",
    "taking",
    "look",
    "hospital",
    "considering",
    "directions",
    "might",
    "move",
    "hold",
    "hospital",
    "constant",
    "take",
    "hospital",
    "move",
    "one",
    "square",
    "example",
    "really",
    "help",
    "us",
    "gets",
    "closer",
    "house",
    "gets",
    "away",
    "house",
    "really",
    "change",
    "anything",
    "two",
    "houses",
    "along",
    "side",
    "take",
    "hospital",
    "right",
    "move",
    "one",
    "square",
    "opposite",
    "problem",
    "gets",
    "away",
    "house",
    "gets",
    "closer",
    "house",
    "real",
    "idea",
    "goal",
    "able",
    "take",
    "hospital",
    "move",
    "one",
    "square",
    "left",
    "moving",
    "one",
    "square",
    "left",
    "move",
    "closer",
    "houses",
    "right",
    "without",
    "changing",
    "anything",
    "houses",
    "left",
    "hospital",
    "still",
    "closer",
    "one",
    "affected",
    "able",
    "improve",
    "situation",
    "picking",
    "neighbor",
    "results",
    "decrease",
    "total",
    "cost",
    "might",
    "move",
    "current",
    "state",
    "neighbor",
    "taking",
    "hospital",
    "moving",
    "point",
    "whole",
    "lot",
    "done",
    "hospital",
    "still",
    "optimizations",
    "make",
    "neighbors",
    "move",
    "going",
    "better",
    "value",
    "consider",
    "hospital",
    "example",
    "might",
    "imagine",
    "right",
    "bit",
    "far",
    "houses",
    "little",
    "bit",
    "lower",
    "might",
    "able",
    "better",
    "taking",
    "hospital",
    "moving",
    "one",
    "square",
    "moving",
    "instead",
    "cost",
    "15",
    "cost",
    "13",
    "particular",
    "configuration",
    "even",
    "better",
    "taking",
    "hospital",
    "moving",
    "one",
    "square",
    "left",
    "instead",
    "cost",
    "13",
    "cost",
    "11",
    "house",
    "one",
    "away",
    "hospital",
    "one",
    "four",
    "away",
    "one",
    "three",
    "away",
    "one",
    "also",
    "three",
    "away",
    "able",
    "much",
    "better",
    "initial",
    "cost",
    "using",
    "initial",
    "configuration",
    "taking",
    "every",
    "state",
    "asking",
    "question",
    "better",
    "making",
    "small",
    "incremental",
    "changes",
    "moving",
    "neighbor",
    "moving",
    "neighbor",
    "moving",
    "neighbor",
    "point",
    "potentially",
    "see",
    "point",
    "algorithm",
    "going",
    "terminate",
    "actually",
    "neighbor",
    "move",
    "going",
    "improve",
    "situation",
    "get",
    "us",
    "cost",
    "less",
    "take",
    "hospital",
    "move",
    "upper",
    "right",
    "well",
    "going",
    "make",
    "away",
    "take",
    "move",
    "really",
    "change",
    "situation",
    "gets",
    "away",
    "house",
    "closer",
    "house",
    "likewise",
    "story",
    "true",
    "hospital",
    "neighbor",
    "move",
    "left",
    "right",
    "either",
    "going",
    "make",
    "away",
    "houses",
    "increase",
    "cost",
    "going",
    "effect",
    "cost",
    "whatsoever",
    "question",
    "might",
    "ask",
    "best",
    "could",
    "best",
    "placement",
    "hospitals",
    "could",
    "possibly",
    "turns",
    "answer",
    "better",
    "way",
    "could",
    "place",
    "hospitals",
    "particular",
    "number",
    "ways",
    "could",
    "one",
    "ways",
    "taking",
    "hospital",
    "moving",
    "square",
    "example",
    "moving",
    "diagonally",
    "one",
    "square",
    "part",
    "definition",
    "neighbor",
    "could",
    "move",
    "left",
    "right",
    "fact",
    "better",
    "total",
    "cost",
    "closer",
    "houses",
    "result",
    "total",
    "cost",
    "less",
    "able",
    "find",
    "order",
    "get",
    "go",
    "state",
    "actually",
    "better",
    "current",
    "state",
    "previously",
    "appears",
    "limitation",
    "concern",
    "might",
    "go",
    "trying",
    "implement",
    "hill",
    "climbing",
    "algorithm",
    "might",
    "always",
    "give",
    "optimal",
    "solution",
    "trying",
    "maximize",
    "value",
    "particular",
    "state",
    "trying",
    "find",
    "global",
    "maximum",
    "concern",
    "might",
    "could",
    "get",
    "stuck",
    "one",
    "local",
    "maxima",
    "highlighted",
    "blue",
    "local",
    "maxima",
    "state",
    "whose",
    "value",
    "higher",
    "neighbors",
    "ever",
    "find",
    "one",
    "two",
    "states",
    "trying",
    "maximize",
    "value",
    "state",
    "going",
    "make",
    "changes",
    "going",
    "move",
    "left",
    "right",
    "going",
    "move",
    "left",
    "states",
    "worse",
    "yet",
    "found",
    "global",
    "optimum",
    "done",
    "best",
    "could",
    "likewise",
    "case",
    "hospitals",
    "ultimately",
    "trying",
    "find",
    "global",
    "minimum",
    "find",
    "value",
    "lower",
    "others",
    "potential",
    "get",
    "stuck",
    "one",
    "local",
    "minima",
    "states",
    "whose",
    "value",
    "lower",
    "neighbors",
    "still",
    "low",
    "local",
    "minima",
    "takeaway",
    "always",
    "going",
    "case",
    "run",
    "naive",
    "hill",
    "climbing",
    "algorithm",
    "always",
    "going",
    "find",
    "optimal",
    "solution",
    "things",
    "could",
    "go",
    "wrong",
    "started",
    "example",
    "tried",
    "maximize",
    "value",
    "much",
    "possible",
    "might",
    "move",
    "highest",
    "possible",
    "neighbor",
    "move",
    "highest",
    "possible",
    "neighbor",
    "move",
    "highest",
    "possible",
    "neighbor",
    "stop",
    "never",
    "realize",
    "actually",
    "better",
    "state",
    "way",
    "could",
    "gone",
    "instead",
    "problems",
    "might",
    "imagine",
    "taking",
    "look",
    "state",
    "space",
    "landscape",
    "various",
    "different",
    "types",
    "plateaus",
    "something",
    "like",
    "flat",
    "local",
    "maximum",
    "six",
    "states",
    "exact",
    "value",
    "case",
    "algorithm",
    "showed",
    "none",
    "neighbors",
    "better",
    "might",
    "get",
    "stuck",
    "flat",
    "local",
    "maximum",
    "even",
    "allowed",
    "move",
    "one",
    "neighbors",
    "would",
    "clear",
    "neighbor",
    "would",
    "ultimately",
    "move",
    "could",
    "get",
    "stuck",
    "well",
    "another",
    "one",
    "one",
    "called",
    "shoulder",
    "really",
    "local",
    "maximum",
    "still",
    "places",
    "go",
    "higher",
    "local",
    "minimum",
    "go",
    "lower",
    "still",
    "make",
    "progress",
    "still",
    "flat",
    "area",
    "local",
    "search",
    "algorithm",
    "potential",
    "get",
    "lost",
    "unable",
    "make",
    "upward",
    "downward",
    "progress",
    "depending",
    "whether",
    "trying",
    "maximize",
    "minimize",
    "therefore",
    "another",
    "potential",
    "us",
    "able",
    "find",
    "solution",
    "might",
    "actually",
    "optimal",
    "solution",
    "potential",
    "potential",
    "hill",
    "climbing",
    "always",
    "find",
    "us",
    "optimal",
    "result",
    "turns",
    "number",
    "different",
    "varieties",
    "variations",
    "hill",
    "climbing",
    "algorithm",
    "help",
    "solve",
    "problem",
    "better",
    "depending",
    "context",
    "depending",
    "specific",
    "type",
    "problem",
    "variants",
    "might",
    "applicable",
    "others",
    "taken",
    "look",
    "far",
    "version",
    "hill",
    "climbing",
    "generally",
    "called",
    "steepest",
    "ascent",
    "hill",
    "climbing",
    "idea",
    "steepest",
    "ascent",
    "hill",
    "climbing",
    "going",
    "choose",
    "highest",
    "valued",
    "neighbor",
    "case",
    "trying",
    "maximize",
    "lowest",
    "valued",
    "neighbor",
    "cases",
    "trying",
    "minimize",
    "generally",
    "speaking",
    "five",
    "neighbors",
    "better",
    "current",
    "state",
    "pick",
    "best",
    "one",
    "five",
    "sometimes",
    "might",
    "work",
    "pretty",
    "well",
    "sort",
    "greedy",
    "approach",
    "trying",
    "take",
    "best",
    "operation",
    "particular",
    "time",
    "step",
    "might",
    "always",
    "work",
    "might",
    "cases",
    "actually",
    "want",
    "choose",
    "option",
    "slightly",
    "better",
    "maybe",
    "best",
    "one",
    "later",
    "might",
    "lead",
    "better",
    "outcome",
    "ultimately",
    "variants",
    "might",
    "consider",
    "basic",
    "hill",
    "climbing",
    "algorithm",
    "one",
    "known",
    "stochastic",
    "hill",
    "climbing",
    "case",
    "choose",
    "randomly",
    "higher",
    "value",
    "neighbors",
    "current",
    "state",
    "five",
    "neighbors",
    "better",
    "rather",
    "choosing",
    "best",
    "one",
    "steep",
    "set",
    "would",
    "stochastic",
    "choose",
    "randomly",
    "one",
    "thinking",
    "better",
    "better",
    "maybe",
    "potential",
    "make",
    "forward",
    "progress",
    "even",
    "locally",
    "best",
    "option",
    "could",
    "possibly",
    "choose",
    "first",
    "choice",
    "hill",
    "climbing",
    "ends",
    "choosing",
    "first",
    "highest",
    "valued",
    "neighbor",
    "follows",
    "behaving",
    "similar",
    "idea",
    "rather",
    "consider",
    "neighbors",
    "soon",
    "find",
    "neighbor",
    "better",
    "current",
    "state",
    "go",
    "ahead",
    "move",
    "may",
    "efficiency",
    "improvements",
    "maybe",
    "potential",
    "find",
    "solution",
    "strategies",
    "able",
    "find",
    "variants",
    "still",
    "suffer",
    "potential",
    "risk",
    "risk",
    "might",
    "end",
    "local",
    "minimum",
    "local",
    "maximum",
    "reduce",
    "risk",
    "repeating",
    "process",
    "multiple",
    "times",
    "one",
    "variant",
    "hill",
    "climbing",
    "random",
    "restart",
    "hill",
    "climbing",
    "general",
    "idea",
    "conduct",
    "hill",
    "climbing",
    "multiple",
    "times",
    "apply",
    "steepest",
    "descent",
    "hill",
    "climbing",
    "example",
    "start",
    "random",
    "state",
    "try",
    "figure",
    "solve",
    "problem",
    "figure",
    "local",
    "maximum",
    "local",
    "minimum",
    "get",
    "randomly",
    "restart",
    "try",
    "choose",
    "new",
    "starting",
    "configuration",
    "try",
    "figure",
    "local",
    "maximum",
    "minimum",
    "number",
    "times",
    "done",
    "number",
    "times",
    "pick",
    "best",
    "one",
    "ones",
    "taken",
    "look",
    "another",
    "option",
    "access",
    "well",
    "although",
    "said",
    "generally",
    "local",
    "search",
    "usually",
    "keep",
    "track",
    "single",
    "node",
    "move",
    "one",
    "neighbors",
    "variants",
    "hill",
    "climbing",
    "known",
    "local",
    "beam",
    "searches",
    "rather",
    "keep",
    "track",
    "one",
    "current",
    "best",
    "state",
    "keeping",
    "track",
    "k",
    "highest",
    "valued",
    "neighbors",
    "rather",
    "starting",
    "one",
    "random",
    "initial",
    "configuration",
    "might",
    "start",
    "3",
    "4",
    "5",
    "randomly",
    "generate",
    "neighbors",
    "pick",
    "3",
    "4",
    "5",
    "best",
    "neighbors",
    "find",
    "continually",
    "repeat",
    "process",
    "idea",
    "options",
    "considering",
    "ways",
    "could",
    "potentially",
    "navigate",
    "optimal",
    "solution",
    "might",
    "exist",
    "particular",
    "problem",
    "let",
    "take",
    "look",
    "actual",
    "code",
    "implement",
    "kinds",
    "ideas",
    "something",
    "like",
    "steepest",
    "ascent",
    "hill",
    "climbing",
    "example",
    "trying",
    "solve",
    "hospital",
    "problem",
    "going",
    "go",
    "ahead",
    "go",
    "hospitals",
    "directory",
    "actually",
    "set",
    "basic",
    "framework",
    "solving",
    "type",
    "problem",
    "go",
    "ahead",
    "go",
    "take",
    "look",
    "code",
    "created",
    "defined",
    "class",
    "going",
    "represent",
    "state",
    "space",
    "space",
    "height",
    "width",
    "also",
    "number",
    "hospitals",
    "configure",
    "big",
    "map",
    "many",
    "hospitals",
    "go",
    "function",
    "adding",
    "new",
    "house",
    "state",
    "space",
    "functions",
    "going",
    "get",
    "available",
    "spaces",
    "want",
    "randomly",
    "place",
    "hospitals",
    "particular",
    "locations",
    "hill",
    "climbing",
    "algorithm",
    "going",
    "hill",
    "climbing",
    "algorithm",
    "well",
    "going",
    "start",
    "randomly",
    "initializing",
    "hospitals",
    "going",
    "go",
    "know",
    "hospitals",
    "actually",
    "let",
    "randomly",
    "place",
    "running",
    "loop",
    "hospitals",
    "going",
    "go",
    "ahead",
    "add",
    "new",
    "hospital",
    "random",
    "location",
    "basically",
    "get",
    "available",
    "spaces",
    "randomly",
    "choose",
    "one",
    "would",
    "like",
    "add",
    "particular",
    "hospital",
    "logging",
    "output",
    "generating",
    "images",
    "take",
    "look",
    "little",
    "bit",
    "later",
    "key",
    "idea",
    "going",
    "keep",
    "repeating",
    "algorithm",
    "could",
    "specify",
    "maximum",
    "many",
    "times",
    "want",
    "run",
    "could",
    "run",
    "hits",
    "local",
    "maximum",
    "local",
    "minimum",
    "basically",
    "consider",
    "hospitals",
    "could",
    "potentially",
    "move",
    "consider",
    "two",
    "hospitals",
    "hospitals",
    "consider",
    "places",
    "hospital",
    "could",
    "move",
    "neighbor",
    "hospital",
    "move",
    "neighbor",
    "see",
    "going",
    "better",
    "currently",
    "going",
    "better",
    "go",
    "ahead",
    "update",
    "best",
    "neighbor",
    "keep",
    "track",
    "new",
    "best",
    "neighbor",
    "found",
    "afterwards",
    "ask",
    "question",
    "best",
    "neighbor",
    "cost",
    "greater",
    "equal",
    "cost",
    "current",
    "set",
    "hospitals",
    "meaning",
    "cost",
    "best",
    "neighbor",
    "greater",
    "current",
    "cost",
    "meaning",
    "best",
    "neighbor",
    "worse",
    "current",
    "state",
    "well",
    "make",
    "changes",
    "go",
    "ahead",
    "return",
    "current",
    "set",
    "hospitals",
    "otherwise",
    "update",
    "hospitals",
    "order",
    "change",
    "one",
    "best",
    "neighbors",
    "multiple",
    "equivalent",
    "using",
    "say",
    "go",
    "ahead",
    "choose",
    "one",
    "randomly",
    "really",
    "python",
    "implementation",
    "idea",
    "talking",
    "idea",
    "taking",
    "current",
    "state",
    "current",
    "set",
    "hospitals",
    "generating",
    "neighbors",
    "looking",
    "ways",
    "could",
    "take",
    "one",
    "hospital",
    "move",
    "one",
    "square",
    "left",
    "right",
    "figuring",
    "based",
    "information",
    "best",
    "neighbor",
    "set",
    "best",
    "neighbors",
    "choosing",
    "one",
    "time",
    "go",
    "ahead",
    "generate",
    "image",
    "order",
    "look",
    "bottom",
    "going",
    "randomly",
    "generate",
    "space",
    "height",
    "10",
    "width",
    "say",
    "go",
    "ahead",
    "put",
    "three",
    "hospitals",
    "somewhere",
    "space",
    "randomly",
    "generate",
    "15",
    "houses",
    "go",
    "ahead",
    "add",
    "random",
    "locations",
    "going",
    "run",
    "hill",
    "climbing",
    "algorithm",
    "order",
    "try",
    "figure",
    "place",
    "hospitals",
    "go",
    "ahead",
    "run",
    "program",
    "running",
    "python",
    "hospitals",
    "see",
    "started",
    "initial",
    "state",
    "cost",
    "72",
    "able",
    "continually",
    "find",
    "neighbors",
    "able",
    "decrease",
    "cost",
    "decrease",
    "69",
    "66",
    "63",
    "forth",
    "way",
    "53",
    "best",
    "neighbor",
    "able",
    "ultimately",
    "find",
    "take",
    "look",
    "looked",
    "like",
    "opening",
    "files",
    "example",
    "initial",
    "configuration",
    "randomly",
    "selected",
    "location",
    "15",
    "different",
    "houses",
    "randomly",
    "selected",
    "locations",
    "one",
    "two",
    "three",
    "hospitals",
    "located",
    "somewhere",
    "inside",
    "state",
    "space",
    "add",
    "distances",
    "houses",
    "nearest",
    "hospital",
    "get",
    "total",
    "cost",
    "question",
    "neighbors",
    "move",
    "improve",
    "situation",
    "looks",
    "like",
    "first",
    "one",
    "algorithm",
    "found",
    "taking",
    "house",
    "right",
    "moving",
    "left",
    "probably",
    "makes",
    "sense",
    "look",
    "houses",
    "general",
    "area",
    "really",
    "five",
    "houses",
    "look",
    "like",
    "probably",
    "ones",
    "going",
    "closest",
    "hospital",
    "moving",
    "left",
    "decreases",
    "total",
    "distance",
    "least",
    "houses",
    "though",
    "increase",
    "distance",
    "one",
    "able",
    "make",
    "improvements",
    "situation",
    "continually",
    "finding",
    "ways",
    "move",
    "hospitals",
    "around",
    "eventually",
    "settle",
    "particular",
    "state",
    "cost",
    "53",
    "figured",
    "position",
    "hospitals",
    "none",
    "neighbors",
    "could",
    "move",
    "actually",
    "going",
    "improve",
    "situation",
    "take",
    "hospital",
    "hospital",
    "hospital",
    "look",
    "neighbors",
    "none",
    "going",
    "better",
    "particular",
    "configuration",
    "say",
    "best",
    "could",
    "might",
    "configuration",
    "hospitals",
    "global",
    "minimum",
    "might",
    "local",
    "minimum",
    "best",
    "neighbors",
    "maybe",
    "best",
    "entire",
    "possible",
    "state",
    "space",
    "could",
    "search",
    "entire",
    "state",
    "space",
    "considering",
    "possible",
    "configurations",
    "hospitals",
    "ultimately",
    "going",
    "time",
    "intensive",
    "especially",
    "state",
    "space",
    "gets",
    "bigger",
    "might",
    "possible",
    "states",
    "going",
    "take",
    "quite",
    "long",
    "time",
    "look",
    "able",
    "use",
    "sort",
    "local",
    "search",
    "algorithms",
    "often",
    "quite",
    "good",
    "trying",
    "find",
    "best",
    "solution",
    "especially",
    "care",
    "best",
    "possible",
    "care",
    "pretty",
    "good",
    "finding",
    "pretty",
    "good",
    "placement",
    "hospitals",
    "methods",
    "particularly",
    "powerful",
    "course",
    "try",
    "mitigate",
    "concern",
    "instead",
    "using",
    "hill",
    "climbing",
    "use",
    "random",
    "restart",
    "idea",
    "rather",
    "hill",
    "climb",
    "one",
    "time",
    "hill",
    "climb",
    "multiple",
    "times",
    "say",
    "try",
    "hill",
    "climbing",
    "whole",
    "bunch",
    "times",
    "exact",
    "map",
    "figure",
    "best",
    "one",
    "able",
    "find",
    "implemented",
    "function",
    "random",
    "restart",
    "restarts",
    "maximum",
    "number",
    "times",
    "going",
    "repeat",
    "number",
    "times",
    "process",
    "go",
    "ahead",
    "run",
    "hill",
    "climbing",
    "algorithm",
    "figure",
    "cost",
    "getting",
    "houses",
    "hospitals",
    "figure",
    "better",
    "done",
    "far",
    "try",
    "exact",
    "idea",
    "instead",
    "running",
    "hill",
    "climbing",
    "go",
    "ahead",
    "run",
    "random",
    "restart",
    "randomly",
    "restart",
    "maybe",
    "20",
    "times",
    "example",
    "go",
    "ahead",
    "remove",
    "images",
    "rerun",
    "program",
    "started",
    "finding",
    "original",
    "state",
    "initially",
    "ran",
    "hill",
    "climbing",
    "best",
    "cost",
    "able",
    "find",
    "iterations",
    "different",
    "iteration",
    "hill",
    "climbing",
    "algorithm",
    "running",
    "hill",
    "climbing",
    "one",
    "time",
    "20",
    "times",
    "time",
    "going",
    "find",
    "local",
    "minimum",
    "case",
    "look",
    "see",
    "time",
    "better",
    "best",
    "time",
    "done",
    "far",
    "went",
    "56",
    "one",
    "greater",
    "ignored",
    "one",
    "41",
    "less",
    "went",
    "ahead",
    "kept",
    "one",
    "remaining",
    "16",
    "times",
    "tried",
    "implement",
    "hill",
    "climbing",
    "tried",
    "run",
    "hill",
    "climbing",
    "algorithm",
    "could",
    "better",
    "maybe",
    "way",
    "better",
    "find",
    "looks",
    "like",
    "way",
    "ended",
    "pretty",
    "good",
    "solution",
    "problem",
    "attempt",
    "number",
    "three",
    "starting",
    "counting",
    "zero",
    "take",
    "look",
    "open",
    "number",
    "three",
    "state",
    "happened",
    "cost",
    "41",
    "running",
    "hill",
    "climbing",
    "algorithm",
    "particular",
    "random",
    "initial",
    "configuration",
    "hospitals",
    "found",
    "local",
    "minimum",
    "terms",
    "trying",
    "minimize",
    "cost",
    "looks",
    "like",
    "pretty",
    "well",
    "hospital",
    "pretty",
    "close",
    "region",
    "one",
    "pretty",
    "close",
    "houses",
    "hospital",
    "looks",
    "good",
    "trying",
    "capture",
    "houses",
    "side",
    "sorts",
    "algorithms",
    "quite",
    "useful",
    "trying",
    "solve",
    "problems",
    "real",
    "problem",
    "many",
    "different",
    "types",
    "hill",
    "climbing",
    "steepest",
    "sense",
    "stochastic",
    "first",
    "choice",
    "forth",
    "never",
    "make",
    "move",
    "makes",
    "situation",
    "worse",
    "always",
    "going",
    "take",
    "current",
    "state",
    "look",
    "neighbors",
    "consider",
    "better",
    "current",
    "state",
    "move",
    "one",
    "neighbors",
    "neighbors",
    "choose",
    "might",
    "vary",
    "among",
    "various",
    "different",
    "types",
    "algorithms",
    "never",
    "go",
    "current",
    "position",
    "position",
    "worse",
    "current",
    "position",
    "ultimately",
    "going",
    "need",
    "want",
    "able",
    "find",
    "global",
    "maximum",
    "global",
    "minimum",
    "sometimes",
    "get",
    "stuck",
    "want",
    "find",
    "way",
    "dislodging",
    "local",
    "maximum",
    "local",
    "minimum",
    "order",
    "find",
    "global",
    "maximum",
    "global",
    "minimum",
    "increase",
    "probability",
    "find",
    "popular",
    "technique",
    "trying",
    "approach",
    "problem",
    "angle",
    "technique",
    "known",
    "simulated",
    "annealing",
    "simulated",
    "modeling",
    "real",
    "physical",
    "process",
    "annealing",
    "think",
    "terms",
    "physics",
    "physical",
    "situation",
    "system",
    "particles",
    "might",
    "imagine",
    "heat",
    "particular",
    "physical",
    "system",
    "lot",
    "energy",
    "things",
    "moving",
    "around",
    "quite",
    "randomly",
    "time",
    "system",
    "cools",
    "eventually",
    "settles",
    "final",
    "position",
    "going",
    "general",
    "idea",
    "simulated",
    "annealing",
    "going",
    "simulate",
    "process",
    "high",
    "temperature",
    "system",
    "things",
    "moving",
    "around",
    "randomly",
    "quite",
    "frequently",
    "time",
    "decreasing",
    "temperature",
    "eventually",
    "settle",
    "ultimate",
    "solution",
    "idea",
    "going",
    "state",
    "space",
    "landscape",
    "looks",
    "like",
    "begin",
    "initial",
    "state",
    "looking",
    "global",
    "maximum",
    "trying",
    "maximize",
    "value",
    "state",
    "traditional",
    "hill",
    "climbing",
    "algorithms",
    "would",
    "take",
    "state",
    "look",
    "two",
    "neighbor",
    "ones",
    "always",
    "pick",
    "one",
    "going",
    "increase",
    "value",
    "state",
    "want",
    "chance",
    "able",
    "find",
    "global",
    "maximum",
    "ca",
    "always",
    "make",
    "good",
    "moves",
    "sometimes",
    "make",
    "bad",
    "moves",
    "allow",
    "make",
    "move",
    "direction",
    "actually",
    "seems",
    "make",
    "situation",
    "worse",
    "later",
    "find",
    "way",
    "global",
    "maximum",
    "terms",
    "trying",
    "solve",
    "problem",
    "course",
    "get",
    "global",
    "maximum",
    "done",
    "whole",
    "lot",
    "searching",
    "probably",
    "want",
    "moving",
    "states",
    "worse",
    "current",
    "state",
    "metaphor",
    "annealing",
    "starts",
    "come",
    "want",
    "start",
    "making",
    "random",
    "moves",
    "time",
    "start",
    "make",
    "fewer",
    "random",
    "moves",
    "based",
    "particular",
    "temperature",
    "schedule",
    "basic",
    "outline",
    "looks",
    "something",
    "like",
    "early",
    "simulated",
    "annealing",
    "higher",
    "temperature",
    "state",
    "mean",
    "higher",
    "temperature",
    "state",
    "likely",
    "accept",
    "neighbors",
    "worse",
    "current",
    "state",
    "might",
    "look",
    "neighbors",
    "one",
    "neighbors",
    "worse",
    "current",
    "state",
    "especially",
    "much",
    "worse",
    "pretty",
    "close",
    "slightly",
    "worse",
    "might",
    "likely",
    "accept",
    "go",
    "ahead",
    "move",
    "neighbor",
    "anyways",
    "later",
    "run",
    "simulated",
    "annealing",
    "going",
    "decrease",
    "temperature",
    "lower",
    "temperature",
    "going",
    "less",
    "likely",
    "accept",
    "neighbors",
    "worse",
    "current",
    "state",
    "formalize",
    "put",
    "little",
    "bit",
    "pseudocode",
    "algorithm",
    "might",
    "look",
    "like",
    "function",
    "called",
    "simulated",
    "annealing",
    "takes",
    "input",
    "problem",
    "trying",
    "solve",
    "also",
    "potentially",
    "maximum",
    "number",
    "times",
    "might",
    "want",
    "run",
    "simulated",
    "annealing",
    "process",
    "many",
    "different",
    "neighbors",
    "going",
    "try",
    "look",
    "value",
    "going",
    "vary",
    "based",
    "problem",
    "trying",
    "solve",
    "start",
    "current",
    "state",
    "equal",
    "initial",
    "state",
    "problem",
    "need",
    "repeat",
    "process",
    "max",
    "number",
    "times",
    "repeat",
    "process",
    "number",
    "times",
    "first",
    "going",
    "calculate",
    "temperature",
    "temperature",
    "function",
    "takes",
    "current",
    "time",
    "starting",
    "1",
    "going",
    "way",
    "max",
    "gives",
    "us",
    "temperature",
    "use",
    "computation",
    "idea",
    "temperature",
    "going",
    "higher",
    "early",
    "going",
    "lower",
    "later",
    "number",
    "ways",
    "temperature",
    "function",
    "could",
    "often",
    "work",
    "one",
    "simplest",
    "ways",
    "say",
    "like",
    "proportion",
    "time",
    "still",
    "remaining",
    "max",
    "units",
    "time",
    "much",
    "time",
    "remaining",
    "start",
    "lot",
    "time",
    "remaining",
    "time",
    "goes",
    "temperature",
    "going",
    "decrease",
    "less",
    "less",
    "remaining",
    "time",
    "still",
    "available",
    "calculate",
    "temperature",
    "current",
    "time",
    "pick",
    "random",
    "neighbor",
    "current",
    "state",
    "longer",
    "going",
    "picking",
    "best",
    "neighbor",
    "possibly",
    "one",
    "better",
    "neighbors",
    "going",
    "pick",
    "random",
    "neighbor",
    "might",
    "better",
    "might",
    "worse",
    "going",
    "calculate",
    "going",
    "calculate",
    "delta",
    "e",
    "e",
    "energy",
    "case",
    "much",
    "better",
    "neighbor",
    "current",
    "state",
    "delta",
    "e",
    "positive",
    "means",
    "neighbor",
    "better",
    "current",
    "state",
    "delta",
    "e",
    "negative",
    "means",
    "neighbor",
    "worse",
    "current",
    "state",
    "condition",
    "looks",
    "like",
    "delta",
    "e",
    "greater",
    "0",
    "means",
    "neighbor",
    "state",
    "better",
    "current",
    "state",
    "ever",
    "situation",
    "arises",
    "go",
    "ahead",
    "update",
    "current",
    "neighbor",
    "move",
    "currently",
    "neighbor",
    "neighbor",
    "better",
    "current",
    "state",
    "go",
    "ahead",
    "accept",
    "difference",
    "whereas",
    "never",
    "ever",
    "wanted",
    "take",
    "move",
    "made",
    "situation",
    "worse",
    "sometimes",
    "want",
    "make",
    "move",
    "actually",
    "going",
    "make",
    "situation",
    "worse",
    "sometimes",
    "going",
    "need",
    "dislodge",
    "local",
    "minimum",
    "local",
    "maximum",
    "increase",
    "probability",
    "able",
    "find",
    "global",
    "minimum",
    "global",
    "maximum",
    "little",
    "bit",
    "later",
    "decide",
    "sometimes",
    "accept",
    "state",
    "might",
    "actually",
    "worse",
    "well",
    "going",
    "accept",
    "worse",
    "state",
    "probability",
    "probability",
    "needs",
    "based",
    "couple",
    "factors",
    "needs",
    "based",
    "part",
    "temperature",
    "temperature",
    "higher",
    "likely",
    "move",
    "worse",
    "neighbor",
    "temperature",
    "lower",
    "less",
    "likely",
    "move",
    "worse",
    "neighbor",
    "also",
    "degree",
    "based",
    "delta",
    "neighbor",
    "much",
    "worse",
    "current",
    "state",
    "probably",
    "want",
    "less",
    "likely",
    "choose",
    "neighbor",
    "little",
    "bit",
    "worse",
    "current",
    "state",
    "couple",
    "ways",
    "could",
    "calculate",
    "turns",
    "one",
    "popular",
    "calculate",
    "e",
    "power",
    "delta",
    "e",
    "e",
    "constant",
    "delta",
    "e",
    "based",
    "delta",
    "e",
    "calculate",
    "value",
    "value",
    "0",
    "probability",
    "say",
    "right",
    "let",
    "go",
    "ahead",
    "move",
    "neighbor",
    "turns",
    "math",
    "value",
    "delta",
    "e",
    "neighbor",
    "much",
    "worse",
    "current",
    "state",
    "going",
    "likely",
    "going",
    "go",
    "ahead",
    "move",
    "state",
    "likewise",
    "temperature",
    "lower",
    "going",
    "less",
    "likely",
    "move",
    "neighboring",
    "state",
    "well",
    "big",
    "picture",
    "simulated",
    "annealing",
    "process",
    "taking",
    "problem",
    "going",
    "ahead",
    "generating",
    "random",
    "neighbors",
    "always",
    "move",
    "neighbor",
    "better",
    "current",
    "state",
    "even",
    "neighbor",
    "worse",
    "current",
    "state",
    "sometimes",
    "move",
    "depending",
    "much",
    "worse",
    "also",
    "based",
    "temperature",
    "result",
    "hope",
    "goal",
    "whole",
    "process",
    "begin",
    "try",
    "find",
    "way",
    "global",
    "maximum",
    "global",
    "minimum",
    "dislodge",
    "ever",
    "get",
    "stuck",
    "local",
    "maximum",
    "local",
    "minimum",
    "order",
    "eventually",
    "make",
    "way",
    "exploring",
    "part",
    "state",
    "space",
    "going",
    "best",
    "temperature",
    "decreases",
    "eventually",
    "settle",
    "without",
    "moving",
    "around",
    "much",
    "found",
    "globally",
    "best",
    "thing",
    "thus",
    "far",
    "end",
    "return",
    "whatever",
    "current",
    "state",
    "happens",
    "conclusion",
    "algorithm",
    "able",
    "figure",
    "solution",
    "types",
    "algorithms",
    "lot",
    "different",
    "applications",
    "time",
    "take",
    "problem",
    "formulate",
    "something",
    "explore",
    "particular",
    "configuration",
    "ask",
    "neighbors",
    "better",
    "current",
    "configuration",
    "way",
    "measuring",
    "applicable",
    "case",
    "hill",
    "climbing",
    "simulated",
    "annealing",
    "types",
    "algorithms",
    "sometimes",
    "facility",
    "location",
    "type",
    "problems",
    "like",
    "trying",
    "plan",
    "city",
    "figure",
    "hospitals",
    "definitely",
    "applications",
    "well",
    "one",
    "famous",
    "problems",
    "computer",
    "science",
    "traveling",
    "salesman",
    "problem",
    "traveling",
    "salesman",
    "problem",
    "generally",
    "formulated",
    "like",
    "whole",
    "bunch",
    "cities",
    "indicated",
    "dots",
    "like",
    "find",
    "route",
    "takes",
    "cities",
    "ends",
    "back",
    "started",
    "route",
    "starts",
    "goes",
    "cities",
    "ends",
    "back",
    "originally",
    "started",
    "might",
    "like",
    "minimize",
    "total",
    "distance",
    "travel",
    "total",
    "cost",
    "taking",
    "entire",
    "path",
    "imagine",
    "problem",
    "applicable",
    "situations",
    "like",
    "delivery",
    "companies",
    "trying",
    "deliver",
    "things",
    "whole",
    "bunch",
    "different",
    "houses",
    "want",
    "figure",
    "get",
    "warehouse",
    "various",
    "different",
    "houses",
    "get",
    "back",
    "using",
    "minimal",
    "time",
    "distance",
    "energy",
    "possible",
    "might",
    "want",
    "try",
    "solve",
    "sorts",
    "problems",
    "turns",
    "solving",
    "particular",
    "kind",
    "problem",
    "computationally",
    "difficult",
    "computationally",
    "expensive",
    "task",
    "able",
    "figure",
    "falls",
    "category",
    "known",
    "problems",
    "problems",
    "known",
    "efficient",
    "way",
    "try",
    "solve",
    "sorts",
    "problems",
    "ultimately",
    "come",
    "approximation",
    "ways",
    "trying",
    "find",
    "good",
    "solution",
    "even",
    "going",
    "find",
    "globally",
    "best",
    "solution",
    "possibly",
    "least",
    "feasible",
    "tractable",
    "amount",
    "time",
    "could",
    "take",
    "traveling",
    "salesman",
    "problem",
    "try",
    "formulate",
    "using",
    "local",
    "search",
    "ask",
    "question",
    "like",
    "right",
    "pick",
    "state",
    "configuration",
    "route",
    "nodes",
    "measure",
    "cost",
    "state",
    "figure",
    "distance",
    "might",
    "want",
    "try",
    "minimize",
    "cost",
    "much",
    "possible",
    "question",
    "mean",
    "neighbor",
    "state",
    "mean",
    "take",
    "particular",
    "route",
    "neighboring",
    "route",
    "close",
    "slightly",
    "different",
    "might",
    "different",
    "total",
    "distance",
    "number",
    "different",
    "definitions",
    "neighbor",
    "traveling",
    "salesman",
    "configuration",
    "might",
    "look",
    "like",
    "one",
    "way",
    "say",
    "neighbor",
    "happens",
    "pick",
    "two",
    "edges",
    "nodes",
    "switch",
    "effectively",
    "example",
    "might",
    "pick",
    "two",
    "edges",
    "two",
    "happened",
    "across",
    "node",
    "goes",
    "node",
    "goes",
    "go",
    "ahead",
    "switch",
    "process",
    "generally",
    "look",
    "like",
    "removing",
    "edges",
    "graph",
    "taking",
    "node",
    "connecting",
    "node",
    "connected",
    "connecting",
    "instead",
    "need",
    "take",
    "arrows",
    "originally",
    "going",
    "way",
    "reverse",
    "move",
    "going",
    "way",
    "fill",
    "last",
    "remaining",
    "blank",
    "add",
    "arrow",
    "goes",
    "direction",
    "instead",
    "taking",
    "two",
    "edges",
    "switching",
    "able",
    "consider",
    "one",
    "possible",
    "neighbor",
    "particular",
    "configuration",
    "looks",
    "like",
    "neighbor",
    "actually",
    "better",
    "looks",
    "like",
    "probably",
    "travels",
    "shorter",
    "distance",
    "order",
    "get",
    "cities",
    "route",
    "current",
    "state",
    "could",
    "imagine",
    "implementing",
    "idea",
    "inside",
    "hill",
    "climbing",
    "simulated",
    "annealing",
    "algorithm",
    "repeat",
    "process",
    "try",
    "take",
    "state",
    "traveling",
    "salesman",
    "problem",
    "look",
    "neighbors",
    "move",
    "neighbors",
    "better",
    "maybe",
    "even",
    "move",
    "neighbors",
    "worse",
    "eventually",
    "settle",
    "upon",
    "best",
    "solution",
    "able",
    "find",
    "turns",
    "types",
    "approximation",
    "algorithms",
    "even",
    "always",
    "find",
    "best",
    "solution",
    "often",
    "pretty",
    "well",
    "trying",
    "find",
    "solutions",
    "helpful",
    "look",
    "local",
    "search",
    "particular",
    "category",
    "algorithms",
    "used",
    "solving",
    "particular",
    "type",
    "problem",
    "really",
    "care",
    "path",
    "solution",
    "care",
    "steps",
    "took",
    "decide",
    "hospitals",
    "go",
    "cared",
    "solution",
    "care",
    "hospitals",
    "route",
    "traveling",
    "salesman",
    "journey",
    "really",
    "ought",
    "another",
    "type",
    "algorithm",
    "might",
    "come",
    "known",
    "categories",
    "linear",
    "programming",
    "types",
    "problems",
    "linear",
    "programming",
    "often",
    "comes",
    "context",
    "trying",
    "optimize",
    "mathematical",
    "function",
    "oftentimes",
    "linear",
    "programming",
    "come",
    "might",
    "real",
    "numbered",
    "values",
    "discrete",
    "fixed",
    "values",
    "might",
    "decimal",
    "values",
    "might",
    "want",
    "able",
    "calculate",
    "linear",
    "programming",
    "family",
    "types",
    "problems",
    "might",
    "situation",
    "looks",
    "like",
    "goal",
    "linear",
    "programming",
    "minimize",
    "cost",
    "function",
    "invert",
    "numbers",
    "say",
    "try",
    "maximize",
    "often",
    "frame",
    "trying",
    "minimize",
    "cost",
    "function",
    "number",
    "variables",
    "x1",
    "x2",
    "x3",
    "way",
    "xn",
    "number",
    "variables",
    "involved",
    "things",
    "want",
    "know",
    "values",
    "cost",
    "function",
    "might",
    "coefficients",
    "front",
    "variables",
    "would",
    "call",
    "linear",
    "equation",
    "variables",
    "might",
    "multiplied",
    "coefficient",
    "add",
    "together",
    "going",
    "square",
    "anything",
    "cube",
    "anything",
    "give",
    "us",
    "different",
    "types",
    "equations",
    "linear",
    "programming",
    "dealing",
    "linear",
    "equations",
    "addition",
    "linear",
    "constraints",
    "constraint",
    "going",
    "look",
    "something",
    "like",
    "sum",
    "particular",
    "equation",
    "linear",
    "combination",
    "variables",
    "less",
    "equal",
    "bound",
    "might",
    "whole",
    "number",
    "various",
    "different",
    "constraints",
    "might",
    "place",
    "onto",
    "linear",
    "programming",
    "exercise",
    "likewise",
    "constraints",
    "saying",
    "linear",
    "equation",
    "less",
    "equal",
    "bound",
    "b",
    "might",
    "also",
    "equal",
    "something",
    "want",
    "sum",
    "combination",
    "variables",
    "equal",
    "value",
    "specify",
    "also",
    "maybe",
    "specify",
    "variable",
    "lower",
    "upper",
    "bounds",
    "needs",
    "positive",
    "number",
    "example",
    "needs",
    "number",
    "less",
    "50",
    "example",
    "number",
    "choices",
    "make",
    "defining",
    "bounds",
    "variable",
    "turns",
    "take",
    "problem",
    "formulate",
    "terms",
    "formulate",
    "problem",
    "goal",
    "minimize",
    "cost",
    "function",
    "minimizing",
    "cost",
    "function",
    "subject",
    "particular",
    "constraints",
    "subjects",
    "equations",
    "form",
    "like",
    "sequence",
    "variables",
    "less",
    "bound",
    "equal",
    "particular",
    "value",
    "number",
    "algorithms",
    "already",
    "exist",
    "solving",
    "sorts",
    "problems",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "example",
    "example",
    "problem",
    "might",
    "come",
    "world",
    "linear",
    "programming",
    "often",
    "going",
    "come",
    "trying",
    "optimize",
    "something",
    "want",
    "able",
    "calculations",
    "constraints",
    "trying",
    "optimize",
    "might",
    "something",
    "like",
    "context",
    "factory",
    "two",
    "machines",
    "x1",
    "x2",
    "x1",
    "costs",
    "50",
    "hour",
    "run",
    "x2",
    "costs",
    "80",
    "hour",
    "run",
    "goal",
    "trying",
    "objective",
    "minimize",
    "total",
    "cost",
    "like",
    "need",
    "subject",
    "certain",
    "constraints",
    "might",
    "labor",
    "constraint",
    "x1",
    "requires",
    "five",
    "units",
    "labor",
    "per",
    "hour",
    "x2",
    "requires",
    "two",
    "units",
    "labor",
    "per",
    "hour",
    "total",
    "20",
    "units",
    "labor",
    "spend",
    "constraint",
    "20",
    "units",
    "labor",
    "spend",
    "spend",
    "across",
    "x1",
    "x2",
    "requires",
    "different",
    "amount",
    "labor",
    "might",
    "also",
    "constraint",
    "like",
    "tells",
    "us",
    "x1",
    "going",
    "produce",
    "10",
    "units",
    "output",
    "per",
    "hour",
    "x2",
    "going",
    "produce",
    "12",
    "units",
    "output",
    "per",
    "hour",
    "company",
    "needs",
    "90",
    "units",
    "output",
    "goal",
    "something",
    "need",
    "achieve",
    "need",
    "achieve",
    "90",
    "units",
    "output",
    "constraints",
    "x1",
    "produce",
    "10",
    "units",
    "output",
    "per",
    "hour",
    "x2",
    "produces",
    "12",
    "units",
    "output",
    "per",
    "hour",
    "types",
    "problems",
    "come",
    "quite",
    "frequently",
    "start",
    "notice",
    "patterns",
    "types",
    "problems",
    "problems",
    "trying",
    "optimize",
    "goal",
    "minimizing",
    "cost",
    "maximizing",
    "output",
    "maximizing",
    "profits",
    "something",
    "like",
    "constraints",
    "placed",
    "process",
    "need",
    "formulate",
    "problem",
    "terms",
    "linear",
    "equations",
    "let",
    "start",
    "first",
    "point",
    "two",
    "machines",
    "x1",
    "x2",
    "x",
    "costs",
    "50",
    "hour",
    "x2",
    "costs",
    "80",
    "hour",
    "come",
    "objective",
    "function",
    "might",
    "look",
    "like",
    "cost",
    "function",
    "rather",
    "50",
    "times",
    "x1",
    "plus",
    "80",
    "times",
    "x2",
    "x1",
    "going",
    "variable",
    "representing",
    "many",
    "hours",
    "run",
    "machine",
    "x1",
    "x2",
    "going",
    "variable",
    "representing",
    "many",
    "hours",
    "running",
    "machine",
    "x2",
    "trying",
    "minimize",
    "cost",
    "function",
    "much",
    "costs",
    "run",
    "machines",
    "per",
    "hour",
    "summed",
    "example",
    "linear",
    "equation",
    "combination",
    "variables",
    "plus",
    "coefficients",
    "placed",
    "front",
    "would",
    "like",
    "minimize",
    "total",
    "value",
    "need",
    "subject",
    "constraints",
    "x1",
    "requires",
    "50",
    "units",
    "labor",
    "per",
    "hour",
    "x2",
    "requires",
    "2",
    "total",
    "20",
    "units",
    "labor",
    "spend",
    "gives",
    "us",
    "constraint",
    "form",
    "5",
    "times",
    "x1",
    "plus",
    "2",
    "times",
    "x2",
    "less",
    "equal",
    "20",
    "20",
    "total",
    "number",
    "units",
    "labor",
    "spend",
    "spent",
    "across",
    "x1",
    "x2",
    "requires",
    "different",
    "number",
    "units",
    "labor",
    "per",
    "hour",
    "example",
    "finally",
    "constraint",
    "x1",
    "produces",
    "10",
    "units",
    "output",
    "per",
    "hour",
    "x2",
    "produces",
    "12",
    "need",
    "90",
    "units",
    "output",
    "might",
    "look",
    "something",
    "like",
    "10x1",
    "plus",
    "12x2",
    "amount",
    "output",
    "per",
    "hour",
    "needs",
    "least",
    "better",
    "great",
    "needs",
    "least",
    "recall",
    "formulation",
    "said",
    "generally",
    "speaking",
    "linear",
    "programming",
    "deal",
    "equals",
    "constraints",
    "less",
    "equal",
    "constraints",
    "greater",
    "equal",
    "sign",
    "problem",
    "whenever",
    "greater",
    "equal",
    "sign",
    "multiply",
    "equation",
    "negative",
    "1",
    "flip",
    "around",
    "less",
    "equals",
    "negative",
    "90",
    "example",
    "instead",
    "greater",
    "equal",
    "going",
    "equivalent",
    "expression",
    "use",
    "represent",
    "problem",
    "cost",
    "function",
    "constraints",
    "subject",
    "turns",
    "number",
    "algorithms",
    "used",
    "order",
    "solve",
    "types",
    "problems",
    "problems",
    "go",
    "little",
    "bit",
    "geometry",
    "linear",
    "algebra",
    "really",
    "going",
    "get",
    "popular",
    "types",
    "algorithms",
    "simplex",
    "one",
    "first",
    "algorithms",
    "discovered",
    "trying",
    "solve",
    "linear",
    "programs",
    "later",
    "class",
    "interior",
    "point",
    "algorithms",
    "used",
    "solve",
    "type",
    "problem",
    "well",
    "key",
    "understand",
    "exactly",
    "algorithms",
    "work",
    "realize",
    "algorithms",
    "exist",
    "efficiently",
    "finding",
    "solutions",
    "time",
    "problem",
    "particular",
    "form",
    "take",
    "look",
    "example",
    "production",
    "directory",
    "file",
    "called",
    "using",
    "scipy",
    "library",
    "lot",
    "functions",
    "within",
    "python",
    "go",
    "ahead",
    "run",
    "optimization",
    "function",
    "order",
    "run",
    "linear",
    "program",
    "going",
    "try",
    "solve",
    "linear",
    "program",
    "provide",
    "expression",
    "function",
    "call",
    "data",
    "linear",
    "program",
    "needs",
    "particular",
    "format",
    "might",
    "little",
    "confusing",
    "first",
    "first",
    "argument",
    "cost",
    "function",
    "case",
    "array",
    "list",
    "50",
    "80",
    "original",
    "cost",
    "function",
    "50",
    "times",
    "x1",
    "plus",
    "80",
    "times",
    "x2",
    "tell",
    "python",
    "50",
    "80",
    "coefficients",
    "trying",
    "optimize",
    "provide",
    "constraints",
    "constraints",
    "wrote",
    "comments",
    "constraint",
    "1",
    "5x1",
    "plus",
    "2x2",
    "less",
    "equal",
    "constraint",
    "2",
    "negative",
    "10x1",
    "plus",
    "negative",
    "12x2",
    "less",
    "equal",
    "negative",
    "scipy",
    "expects",
    "constraints",
    "particular",
    "format",
    "first",
    "expects",
    "provide",
    "coefficients",
    "upper",
    "bound",
    "equations",
    "ub",
    "upper",
    "bound",
    "coefficients",
    "first",
    "equation",
    "5",
    "2",
    "5x1",
    "2x2",
    "coefficients",
    "second",
    "equation",
    "negative",
    "10",
    "negative",
    "12",
    "negative",
    "10x1",
    "plus",
    "negative",
    "12x2",
    "provide",
    "separate",
    "argument",
    "keep",
    "things",
    "separate",
    "actual",
    "bound",
    "upper",
    "bound",
    "constraints",
    "well",
    "first",
    "constraint",
    "upper",
    "bound",
    "constraint",
    "number",
    "constraint",
    "number",
    "2",
    "upper",
    "bound",
    "bit",
    "cryptic",
    "way",
    "representing",
    "quite",
    "simple",
    "writing",
    "mathematical",
    "equations",
    "really",
    "expected",
    "coefficients",
    "numbers",
    "equations",
    "first",
    "providing",
    "coefficients",
    "cost",
    "function",
    "providing",
    "coefficients",
    "inequality",
    "constraints",
    "providing",
    "upper",
    "bounds",
    "inequality",
    "constraints",
    "information",
    "run",
    "interior",
    "point",
    "algorithms",
    "simplex",
    "algorithm",
    "even",
    "understand",
    "works",
    "run",
    "function",
    "figure",
    "result",
    "said",
    "result",
    "success",
    "able",
    "solve",
    "problem",
    "go",
    "ahead",
    "print",
    "value",
    "x1",
    "x2",
    "otherwise",
    "go",
    "ahead",
    "print",
    "solution",
    "run",
    "program",
    "running",
    "python",
    "takes",
    "second",
    "calculate",
    "see",
    "optimal",
    "solution",
    "x1",
    "run",
    "hours",
    "x2",
    "run",
    "hours",
    "able",
    "formulating",
    "problem",
    "linear",
    "equation",
    "trying",
    "optimize",
    "cost",
    "trying",
    "minimize",
    "constraints",
    "placed",
    "many",
    "many",
    "problems",
    "fall",
    "category",
    "problems",
    "solve",
    "figure",
    "use",
    "equations",
    "use",
    "constraints",
    "represent",
    "general",
    "idea",
    "theme",
    "going",
    "come",
    "couple",
    "times",
    "today",
    "want",
    "able",
    "take",
    "problem",
    "reduce",
    "problem",
    "know",
    "solve",
    "order",
    "begin",
    "find",
    "solution",
    "use",
    "existing",
    "methods",
    "use",
    "order",
    "find",
    "solution",
    "effectively",
    "efficiently",
    "turns",
    "types",
    "problems",
    "constraints",
    "show",
    "ways",
    "entire",
    "class",
    "problems",
    "generally",
    "known",
    "constraint",
    "satisfaction",
    "problems",
    "going",
    "take",
    "look",
    "might",
    "formulate",
    "constraint",
    "satisfaction",
    "problem",
    "might",
    "go",
    "solving",
    "constraint",
    "satisfaction",
    "problem",
    "basic",
    "idea",
    "constraint",
    "satisfaction",
    "problem",
    "number",
    "variables",
    "need",
    "take",
    "values",
    "need",
    "figure",
    "values",
    "variables",
    "take",
    "variables",
    "subject",
    "particular",
    "constraints",
    "going",
    "limit",
    "values",
    "variables",
    "actually",
    "take",
    "let",
    "take",
    "look",
    "real",
    "world",
    "example",
    "example",
    "let",
    "look",
    "exam",
    "scheduling",
    "four",
    "students",
    "students",
    "1",
    "2",
    "3",
    "taking",
    "number",
    "different",
    "classes",
    "classes",
    "going",
    "represented",
    "letters",
    "student",
    "1",
    "enrolled",
    "courses",
    "b",
    "student",
    "2",
    "enrolled",
    "courses",
    "b",
    "e",
    "forth",
    "say",
    "university",
    "example",
    "trying",
    "schedule",
    "exams",
    "courses",
    "three",
    "exam",
    "slots",
    "monday",
    "tuesday",
    "wednesday",
    "schedule",
    "exam",
    "courses",
    "constraint",
    "constraint",
    "deal",
    "scheduling",
    "want",
    "anyone",
    "take",
    "two",
    "exams",
    "day",
    "would",
    "like",
    "try",
    "minimize",
    "eliminate",
    "possible",
    "begin",
    "represent",
    "idea",
    "structure",
    "way",
    "computer",
    "ai",
    "algorithm",
    "begin",
    "try",
    "solve",
    "problem",
    "well",
    "let",
    "particular",
    "look",
    "classes",
    "might",
    "take",
    "represent",
    "courses",
    "node",
    "inside",
    "graph",
    "create",
    "edge",
    "two",
    "nodes",
    "graph",
    "constraint",
    "two",
    "nodes",
    "mean",
    "well",
    "start",
    "student",
    "1",
    "enrolled",
    "courses",
    "b",
    "means",
    "b",
    "ca",
    "exam",
    "time",
    "c",
    "ca",
    "exam",
    "time",
    "b",
    "c",
    "also",
    "ca",
    "exam",
    "time",
    "represent",
    "graph",
    "drawing",
    "edges",
    "one",
    "edge",
    "b",
    "one",
    "b",
    "c",
    "one",
    "c",
    "encodes",
    "idea",
    "nodes",
    "constraint",
    "particular",
    "constraint",
    "happens",
    "two",
    "ca",
    "equal",
    "though",
    "types",
    "constraints",
    "possible",
    "depending",
    "type",
    "problem",
    "trying",
    "solve",
    "thing",
    "students",
    "student",
    "2",
    "enrolled",
    "courses",
    "b",
    "e",
    "well",
    "means",
    "b",
    "e",
    "need",
    "edges",
    "connect",
    "well",
    "student",
    "3",
    "enrolled",
    "courses",
    "c",
    "e",
    "go",
    "ahead",
    "take",
    "c",
    "e",
    "f",
    "connect",
    "drawing",
    "edges",
    "finally",
    "student",
    "4",
    "enrolled",
    "courses",
    "e",
    "f",
    "represent",
    "drawing",
    "edges",
    "e",
    "f",
    "g",
    "although",
    "e",
    "f",
    "already",
    "edge",
    "need",
    "another",
    "one",
    "constraint",
    "encoding",
    "idea",
    "course",
    "e",
    "course",
    "f",
    "exam",
    "day",
    "might",
    "call",
    "constraint",
    "graph",
    "graphical",
    "representation",
    "variables",
    "speak",
    "constraints",
    "possible",
    "variables",
    "particular",
    "case",
    "constraints",
    "represents",
    "inequality",
    "constraint",
    "edge",
    "b",
    "means",
    "whatever",
    "value",
    "variable",
    "b",
    "takes",
    "value",
    "variable",
    "takes",
    "well",
    "actually",
    "constraint",
    "satisfaction",
    "problem",
    "well",
    "constraint",
    "satisfaction",
    "problem",
    "set",
    "variables",
    "x1",
    "way",
    "xn",
    "set",
    "domains",
    "variables",
    "every",
    "variable",
    "needs",
    "take",
    "values",
    "maybe",
    "every",
    "variable",
    "domain",
    "maybe",
    "variable",
    "slightly",
    "different",
    "domain",
    "set",
    "constraints",
    "call",
    "set",
    "c",
    "constraints",
    "placed",
    "upon",
    "variables",
    "like",
    "x1",
    "equal",
    "x2",
    "could",
    "forms",
    "like",
    "maybe",
    "x1",
    "equals",
    "x2",
    "plus",
    "1",
    "variables",
    "taking",
    "numerical",
    "values",
    "domain",
    "example",
    "types",
    "constraints",
    "going",
    "vary",
    "based",
    "types",
    "problems",
    "constraint",
    "satisfaction",
    "shows",
    "place",
    "well",
    "situation",
    "variables",
    "subject",
    "particular",
    "constraints",
    "one",
    "popular",
    "game",
    "sudoku",
    "example",
    "9",
    "9",
    "grid",
    "need",
    "fill",
    "numbers",
    "cells",
    "want",
    "make",
    "sure",
    "never",
    "duplicate",
    "number",
    "row",
    "column",
    "grid",
    "3",
    "3",
    "cells",
    "example",
    "might",
    "look",
    "like",
    "constraint",
    "satisfaction",
    "problem",
    "well",
    "variables",
    "empty",
    "squares",
    "puzzle",
    "represented",
    "like",
    "x",
    "comma",
    "coordinate",
    "example",
    "squares",
    "need",
    "plug",
    "value",
    "know",
    "value",
    "take",
    "domain",
    "going",
    "numbers",
    "1",
    "9",
    "value",
    "could",
    "fill",
    "one",
    "cells",
    "going",
    "domain",
    "variables",
    "constraints",
    "going",
    "form",
    "like",
    "cell",
    "ca",
    "equal",
    "cell",
    "ca",
    "equal",
    "cell",
    "ca",
    "need",
    "different",
    "example",
    "rows",
    "columns",
    "3",
    "3",
    "squares",
    "well",
    "constraints",
    "going",
    "enforce",
    "values",
    "actually",
    "allowed",
    "formulate",
    "idea",
    "case",
    "exam",
    "scheduling",
    "problem",
    "variables",
    "different",
    "courses",
    "domain",
    "variables",
    "going",
    "monday",
    "tuesday",
    "wednesday",
    "possible",
    "values",
    "variables",
    "take",
    "case",
    "represent",
    "exam",
    "class",
    "constraints",
    "form",
    "equal",
    "b",
    "equal",
    "c",
    "meaning",
    "b",
    "ca",
    "exam",
    "day",
    "c",
    "ca",
    "exam",
    "day",
    "formally",
    "two",
    "variables",
    "take",
    "value",
    "within",
    "domain",
    "formulation",
    "constraint",
    "satisfaction",
    "problem",
    "begin",
    "use",
    "try",
    "solve",
    "problem",
    "constraints",
    "come",
    "number",
    "different",
    "forms",
    "hard",
    "constraints",
    "constraints",
    "must",
    "satisfied",
    "correct",
    "solution",
    "something",
    "like",
    "sudoku",
    "puzzle",
    "cell",
    "cell",
    "row",
    "take",
    "value",
    "hard",
    "constraint",
    "problems",
    "also",
    "soft",
    "constraints",
    "constraints",
    "express",
    "notion",
    "preference",
    "maybe",
    "b",
    "ca",
    "exam",
    "day",
    "maybe",
    "someone",
    "preference",
    "exam",
    "earlier",
    "b",
    "exam",
    "need",
    "case",
    "expression",
    "solution",
    "better",
    "another",
    "solution",
    "case",
    "might",
    "formulate",
    "problem",
    "trying",
    "optimize",
    "maximizing",
    "people",
    "preferences",
    "want",
    "people",
    "preferences",
    "satisfied",
    "much",
    "possible",
    "case",
    "though",
    "mostly",
    "deal",
    "hard",
    "constraints",
    "constraints",
    "must",
    "met",
    "order",
    "correct",
    "solution",
    "problem",
    "want",
    "figure",
    "assignment",
    "variables",
    "particular",
    "values",
    "ultimately",
    "going",
    "give",
    "us",
    "solution",
    "problem",
    "allowing",
    "us",
    "assign",
    "day",
    "classes",
    "conflicts",
    "classes",
    "turns",
    "classify",
    "constraints",
    "constraint",
    "satisfaction",
    "problem",
    "number",
    "different",
    "categories",
    "first",
    "categories",
    "perhaps",
    "simplest",
    "types",
    "constraints",
    "known",
    "unary",
    "constraints",
    "unary",
    "constraint",
    "constraint",
    "involves",
    "single",
    "variable",
    "example",
    "unary",
    "constraint",
    "might",
    "something",
    "like",
    "equal",
    "monday",
    "meaning",
    "course",
    "exam",
    "monday",
    "reason",
    "instructor",
    "course",
    "available",
    "monday",
    "might",
    "constraint",
    "problem",
    "looks",
    "like",
    "something",
    "single",
    "variable",
    "maybe",
    "says",
    "equal",
    "monday",
    "equal",
    "something",
    "case",
    "numbers",
    "greater",
    "less",
    "something",
    "constraint",
    "one",
    "variable",
    "consider",
    "unary",
    "constraint",
    "contrast",
    "something",
    "like",
    "binary",
    "constraint",
    "constraint",
    "involves",
    "two",
    "variables",
    "example",
    "would",
    "constraint",
    "like",
    "ones",
    "looking",
    "something",
    "like",
    "equal",
    "b",
    "example",
    "binary",
    "constraint",
    "constraint",
    "two",
    "variables",
    "involved",
    "represented",
    "using",
    "arc",
    "edge",
    "connects",
    "variable",
    "variable",
    "using",
    "knowledge",
    "ok",
    "unary",
    "constraint",
    "binary",
    "constraint",
    "different",
    "types",
    "things",
    "say",
    "particular",
    "constraint",
    "satisfaction",
    "problem",
    "one",
    "thing",
    "say",
    "try",
    "make",
    "problem",
    "node",
    "consistent",
    "node",
    "consistency",
    "mean",
    "node",
    "consistency",
    "means",
    "values",
    "variable",
    "domain",
    "satisfying",
    "variable",
    "unary",
    "constraints",
    "variables",
    "inside",
    "constraint",
    "satisfaction",
    "problem",
    "values",
    "satisfy",
    "unary",
    "constraints",
    "particular",
    "variable",
    "say",
    "entire",
    "problem",
    "node",
    "consistent",
    "even",
    "say",
    "particular",
    "variable",
    "node",
    "consistent",
    "want",
    "make",
    "one",
    "node",
    "consistent",
    "within",
    "actually",
    "look",
    "like",
    "let",
    "look",
    "simplified",
    "example",
    "instead",
    "whole",
    "bunch",
    "different",
    "classes",
    "two",
    "classes",
    "b",
    "exam",
    "either",
    "monday",
    "tuesday",
    "wednesday",
    "domain",
    "variable",
    "domain",
    "variable",
    "let",
    "imagine",
    "constraints",
    "equal",
    "monday",
    "b",
    "equal",
    "tuesday",
    "b",
    "equal",
    "monday",
    "equal",
    "constraints",
    "particular",
    "problem",
    "try",
    "enforce",
    "node",
    "consistency",
    "node",
    "consistency",
    "means",
    "make",
    "sure",
    "values",
    "variable",
    "domain",
    "satisfy",
    "unary",
    "constraints",
    "could",
    "start",
    "trying",
    "make",
    "node",
    "node",
    "consistent",
    "consistent",
    "every",
    "value",
    "inside",
    "domain",
    "satisfy",
    "unary",
    "constraints",
    "well",
    "initially",
    "see",
    "monday",
    "satisfy",
    "unary",
    "constraints",
    "constraint",
    "unary",
    "constraint",
    "equal",
    "monday",
    "monday",
    "still",
    "domain",
    "something",
    "node",
    "consistent",
    "monday",
    "domain",
    "valid",
    "value",
    "particular",
    "node",
    "make",
    "node",
    "consistent",
    "well",
    "make",
    "node",
    "consistent",
    "go",
    "ahead",
    "remove",
    "monday",
    "domain",
    "tuesday",
    "wednesday",
    "constraint",
    "said",
    "equal",
    "monday",
    "point",
    "node",
    "consistent",
    "values",
    "take",
    "tuesday",
    "wednesday",
    "constraint",
    "unary",
    "constraint",
    "conflicts",
    "idea",
    "constraint",
    "says",
    "ca",
    "tuesday",
    "unary",
    "constraint",
    "says",
    "wednesday",
    "turn",
    "attention",
    "b",
    "also",
    "domain",
    "monday",
    "tuesday",
    "wednesday",
    "begin",
    "see",
    "whether",
    "variables",
    "satisfy",
    "unary",
    "constraints",
    "well",
    "well",
    "unary",
    "constraint",
    "b",
    "equal",
    "tuesday",
    "appear",
    "satisfied",
    "domain",
    "monday",
    "tuesday",
    "wednesday",
    "tuesday",
    "possible",
    "value",
    "variable",
    "b",
    "could",
    "take",
    "consistent",
    "unary",
    "constraint",
    "b",
    "equal",
    "tuesday",
    "solve",
    "problem",
    "go",
    "ahead",
    "remove",
    "tuesday",
    "b",
    "domain",
    "b",
    "domain",
    "contains",
    "monday",
    "wednesday",
    "turns",
    "yet",
    "another",
    "unary",
    "constraint",
    "placed",
    "variable",
    "b",
    "b",
    "equal",
    "monday",
    "means",
    "value",
    "monday",
    "inside",
    "b",
    "domain",
    "consistent",
    "b",
    "unary",
    "constraints",
    "constraint",
    "says",
    "b",
    "monday",
    "remove",
    "monday",
    "b",
    "domain",
    "made",
    "unary",
    "constraints",
    "yet",
    "considered",
    "constraint",
    "binary",
    "constraint",
    "considered",
    "unary",
    "constraints",
    "constraints",
    "involve",
    "single",
    "variable",
    "made",
    "sure",
    "every",
    "node",
    "consistent",
    "unary",
    "constraints",
    "say",
    "enforced",
    "node",
    "consistency",
    "possible",
    "nodes",
    "pick",
    "values",
    "domain",
    "wo",
    "unary",
    "constraint",
    "violated",
    "result",
    "node",
    "consistency",
    "fairly",
    "easy",
    "enforce",
    "take",
    "node",
    "make",
    "sure",
    "values",
    "domain",
    "satisfy",
    "unary",
    "constraints",
    "things",
    "get",
    "little",
    "bit",
    "interesting",
    "consider",
    "different",
    "types",
    "consistency",
    "something",
    "like",
    "arc",
    "consistency",
    "example",
    "arc",
    "consistency",
    "refers",
    "values",
    "variable",
    "domain",
    "satisfy",
    "variable",
    "binary",
    "constraints",
    "looking",
    "trying",
    "make",
    "arc",
    "consistent",
    "longer",
    "considering",
    "unary",
    "constraints",
    "involve",
    "trying",
    "consider",
    "binary",
    "constraints",
    "involve",
    "well",
    "edge",
    "connects",
    "another",
    "variable",
    "inside",
    "constraint",
    "graph",
    "taking",
    "look",
    "put",
    "little",
    "bit",
    "formally",
    "arc",
    "consistency",
    "arc",
    "really",
    "another",
    "word",
    "edge",
    "connects",
    "two",
    "nodes",
    "inside",
    "constraint",
    "graph",
    "define",
    "arc",
    "consistency",
    "little",
    "precisely",
    "like",
    "order",
    "make",
    "variable",
    "x",
    "arc",
    "consistent",
    "respect",
    "variable",
    "need",
    "remove",
    "element",
    "x",
    "domain",
    "make",
    "sure",
    "every",
    "choice",
    "x",
    "every",
    "choice",
    "x",
    "domain",
    "possible",
    "choice",
    "put",
    "another",
    "way",
    "variable",
    "x",
    "want",
    "make",
    "x",
    "arc",
    "consistent",
    "going",
    "look",
    "possible",
    "values",
    "x",
    "take",
    "make",
    "sure",
    "possible",
    "values",
    "still",
    "choice",
    "make",
    "arc",
    "x",
    "make",
    "sure",
    "possible",
    "option",
    "choose",
    "well",
    "let",
    "look",
    "example",
    "going",
    "back",
    "example",
    "enforced",
    "node",
    "consistency",
    "already",
    "saying",
    "tuesday",
    "wednesday",
    "knew",
    "could",
    "monday",
    "also",
    "said",
    "b",
    "domain",
    "consists",
    "wednesday",
    "know",
    "b",
    "equal",
    "tuesday",
    "also",
    "b",
    "equal",
    "monday",
    "let",
    "begin",
    "consider",
    "arc",
    "consistency",
    "let",
    "try",
    "make",
    "arc",
    "consistent",
    "means",
    "make",
    "arc",
    "consistent",
    "respect",
    "b",
    "means",
    "choice",
    "make",
    "domain",
    "choice",
    "make",
    "b",
    "domain",
    "going",
    "consistent",
    "try",
    "choose",
    "tuesday",
    "possible",
    "value",
    "choose",
    "tuesday",
    "value",
    "b",
    "satisfies",
    "binary",
    "constraint",
    "well",
    "yes",
    "b",
    "wednesday",
    "would",
    "satisfy",
    "constraint",
    "equal",
    "b",
    "tuesday",
    "equal",
    "wednesday",
    "however",
    "chose",
    "wednesday",
    "well",
    "choice",
    "b",
    "domain",
    "satisfies",
    "binary",
    "constraint",
    "way",
    "choose",
    "something",
    "b",
    "satisfies",
    "equal",
    "b",
    "know",
    "b",
    "must",
    "wednesday",
    "ever",
    "run",
    "situation",
    "like",
    "see",
    "possible",
    "value",
    "choice",
    "value",
    "b",
    "satisfies",
    "binary",
    "constraint",
    "well",
    "arc",
    "consistent",
    "make",
    "arc",
    "consistent",
    "would",
    "need",
    "take",
    "wednesday",
    "remove",
    "domain",
    "wednesday",
    "going",
    "possible",
    "choice",
    "make",
    "consistent",
    "binary",
    "constraint",
    "way",
    "could",
    "choose",
    "wednesday",
    "still",
    "available",
    "solution",
    "choosing",
    "something",
    "b",
    "well",
    "able",
    "enforce",
    "arc",
    "consistency",
    "actually",
    "solved",
    "entire",
    "problem",
    "given",
    "constraints",
    "b",
    "exams",
    "either",
    "monday",
    "tuesday",
    "wednesday",
    "solution",
    "would",
    "appear",
    "exam",
    "must",
    "tuesday",
    "b",
    "exam",
    "must",
    "wednesday",
    "option",
    "available",
    "want",
    "apply",
    "consistency",
    "larger",
    "graph",
    "looking",
    "one",
    "particular",
    "pair",
    "consistency",
    "ways",
    "begin",
    "formalize",
    "pseudocode",
    "would",
    "look",
    "like",
    "trying",
    "write",
    "algorithm",
    "enforces",
    "arc",
    "consistency",
    "start",
    "defining",
    "function",
    "called",
    "revise",
    "revise",
    "going",
    "take",
    "input",
    "csp",
    "otherwise",
    "known",
    "constraint",
    "satisfaction",
    "problem",
    "also",
    "two",
    "variables",
    "x",
    "revise",
    "going",
    "going",
    "make",
    "x",
    "arc",
    "consistent",
    "respect",
    "meaning",
    "remove",
    "anything",
    "x",
    "domain",
    "allow",
    "possible",
    "option",
    "work",
    "well",
    "go",
    "ahead",
    "first",
    "keep",
    "track",
    "whether",
    "made",
    "revision",
    "revise",
    "ultimately",
    "going",
    "return",
    "true",
    "false",
    "return",
    "true",
    "event",
    "make",
    "revision",
    "x",
    "domain",
    "return",
    "false",
    "make",
    "change",
    "x",
    "domain",
    "see",
    "moment",
    "going",
    "helpful",
    "start",
    "saying",
    "revised",
    "equals",
    "false",
    "made",
    "changes",
    "say",
    "right",
    "let",
    "go",
    "ahead",
    "loop",
    "possible",
    "values",
    "x",
    "domain",
    "loop",
    "x",
    "domain",
    "little",
    "x",
    "x",
    "domain",
    "want",
    "make",
    "sure",
    "choices",
    "available",
    "choice",
    "satisfies",
    "binary",
    "constraints",
    "defined",
    "inside",
    "csp",
    "inside",
    "constraint",
    "satisfaction",
    "problem",
    "ever",
    "case",
    "value",
    "domain",
    "satisfies",
    "constraint",
    "x",
    "well",
    "case",
    "means",
    "value",
    "x",
    "x",
    "domain",
    "go",
    "ahead",
    "delete",
    "x",
    "x",
    "domain",
    "set",
    "revised",
    "equal",
    "true",
    "change",
    "x",
    "domain",
    "changed",
    "x",
    "domain",
    "removing",
    "little",
    "removed",
    "little",
    "x",
    "art",
    "consistent",
    "way",
    "could",
    "choose",
    "value",
    "would",
    "satisfy",
    "xy",
    "constraint",
    "case",
    "go",
    "ahead",
    "set",
    "revised",
    "equal",
    "true",
    "every",
    "value",
    "x",
    "domain",
    "sometimes",
    "might",
    "fine",
    "cases",
    "might",
    "allow",
    "possible",
    "choice",
    "case",
    "need",
    "remove",
    "value",
    "x",
    "domain",
    "end",
    "return",
    "revised",
    "indicate",
    "whether",
    "actually",
    "made",
    "change",
    "function",
    "revised",
    "function",
    "effectively",
    "implementation",
    "saw",
    "graphically",
    "moment",
    "ago",
    "makes",
    "one",
    "variable",
    "x",
    "arc",
    "consistent",
    "another",
    "variable",
    "case",
    "generally",
    "speaking",
    "want",
    "enforce",
    "consistency",
    "often",
    "want",
    "enforce",
    "consistency",
    "single",
    "arc",
    "entire",
    "constraint",
    "satisfaction",
    "problem",
    "turns",
    "algorithm",
    "well",
    "algorithm",
    "known",
    "ac3",
    "ac3",
    "takes",
    "constraint",
    "satisfaction",
    "problem",
    "enforces",
    "consistency",
    "across",
    "entire",
    "problem",
    "well",
    "going",
    "basically",
    "maintain",
    "queue",
    "basically",
    "line",
    "arcs",
    "needs",
    "make",
    "consistent",
    "time",
    "might",
    "remove",
    "things",
    "queue",
    "begin",
    "dealing",
    "consistency",
    "might",
    "need",
    "add",
    "things",
    "queue",
    "well",
    "things",
    "need",
    "make",
    "arc",
    "consistent",
    "go",
    "ahead",
    "start",
    "queue",
    "contains",
    "arcs",
    "constraint",
    "satisfaction",
    "problem",
    "edges",
    "connect",
    "two",
    "nodes",
    "sort",
    "binary",
    "constraint",
    "long",
    "queue",
    "work",
    "done",
    "queue",
    "things",
    "need",
    "make",
    "arc",
    "consistent",
    "long",
    "queue",
    "still",
    "things",
    "well",
    "start",
    "queue",
    "remove",
    "something",
    "queue",
    "strictly",
    "speaking",
    "need",
    "queue",
    "queue",
    "traditional",
    "way",
    "queue",
    "give",
    "us",
    "arc",
    "x",
    "two",
    "variables",
    "would",
    "like",
    "make",
    "x",
    "arc",
    "consistent",
    "make",
    "x",
    "arc",
    "consistent",
    "well",
    "go",
    "ahead",
    "use",
    "revise",
    "function",
    "talked",
    "moment",
    "ago",
    "called",
    "revise",
    "function",
    "passing",
    "input",
    "constraint",
    "satisfaction",
    "problem",
    "also",
    "variables",
    "x",
    "want",
    "make",
    "x",
    "arc",
    "consistent",
    "words",
    "remove",
    "values",
    "x",
    "domain",
    "leave",
    "available",
    "option",
    "recall",
    "revised",
    "return",
    "well",
    "returns",
    "true",
    "actually",
    "made",
    "change",
    "removed",
    "something",
    "x",
    "domain",
    "available",
    "option",
    "example",
    "returns",
    "false",
    "make",
    "change",
    "x",
    "domain",
    "turns",
    "revised",
    "returns",
    "false",
    "make",
    "changes",
    "well",
    "whole",
    "lot",
    "work",
    "done",
    "arc",
    "move",
    "ahead",
    "next",
    "arc",
    "queue",
    "make",
    "change",
    "reduce",
    "x",
    "domain",
    "removing",
    "values",
    "x",
    "domain",
    "well",
    "might",
    "realize",
    "creates",
    "potential",
    "problems",
    "later",
    "might",
    "mean",
    "arc",
    "arc",
    "consistent",
    "x",
    "node",
    "might",
    "longer",
    "arc",
    "consistent",
    "x",
    "used",
    "option",
    "could",
    "choose",
    "x",
    "might",
    "might",
    "removed",
    "something",
    "x",
    "necessary",
    "arc",
    "arc",
    "consistent",
    "ever",
    "revise",
    "x",
    "domain",
    "going",
    "need",
    "add",
    "things",
    "queue",
    "additional",
    "arcs",
    "might",
    "want",
    "check",
    "well",
    "first",
    "thing",
    "want",
    "check",
    "make",
    "sure",
    "x",
    "domain",
    "x",
    "domain",
    "0",
    "means",
    "available",
    "options",
    "x",
    "means",
    "way",
    "solve",
    "constraint",
    "satisfaction",
    "problem",
    "removed",
    "everything",
    "x",
    "domain",
    "go",
    "ahead",
    "return",
    "false",
    "indicate",
    "way",
    "solve",
    "problem",
    "nothing",
    "left",
    "x",
    "domain",
    "otherwise",
    "things",
    "left",
    "x",
    "domain",
    "fewer",
    "things",
    "well",
    "loop",
    "variable",
    "z",
    "x",
    "neighbors",
    "except",
    "already",
    "handled",
    "consider",
    "x",
    "neighbors",
    "ask",
    "right",
    "arc",
    "z",
    "x",
    "arc",
    "might",
    "longer",
    "arc",
    "consistent",
    "z",
    "might",
    "possible",
    "option",
    "could",
    "choose",
    "x",
    "correspond",
    "z",
    "possible",
    "values",
    "might",
    "removed",
    "elements",
    "x",
    "domain",
    "go",
    "ahead",
    "enqueue",
    "adding",
    "something",
    "queue",
    "arc",
    "zx",
    "neighbors",
    "need",
    "add",
    "back",
    "arcs",
    "queue",
    "order",
    "continue",
    "enforce",
    "arc",
    "consistency",
    "end",
    "make",
    "process",
    "return",
    "true",
    "ac3",
    "algorithm",
    "enforcing",
    "arc",
    "consistency",
    "constraint",
    "satisfaction",
    "problem",
    "big",
    "idea",
    "really",
    "keep",
    "track",
    "arcs",
    "might",
    "need",
    "make",
    "arc",
    "consistent",
    "make",
    "arc",
    "consistent",
    "calling",
    "revise",
    "function",
    "revise",
    "new",
    "arcs",
    "might",
    "need",
    "added",
    "queue",
    "order",
    "make",
    "sure",
    "everything",
    "still",
    "arc",
    "consistent",
    "even",
    "removed",
    "elements",
    "particular",
    "variable",
    "domain",
    "would",
    "happen",
    "tried",
    "enforce",
    "arc",
    "consistency",
    "graph",
    "like",
    "graph",
    "variables",
    "domain",
    "monday",
    "tuesday",
    "wednesday",
    "well",
    "turns",
    "enforcing",
    "arc",
    "consistency",
    "graph",
    "well",
    "solve",
    "types",
    "problems",
    "nothing",
    "actually",
    "changes",
    "particular",
    "arc",
    "considering",
    "two",
    "variables",
    "always",
    "way",
    "choices",
    "make",
    "one",
    "make",
    "choice",
    "one",
    "three",
    "options",
    "need",
    "two",
    "different",
    "actually",
    "quite",
    "easy",
    "take",
    "arc",
    "declare",
    "arc",
    "consistent",
    "pick",
    "monday",
    "pick",
    "something",
    "monday",
    "arc",
    "consistency",
    "consider",
    "consistency",
    "binary",
    "constraint",
    "two",
    "nodes",
    "really",
    "considering",
    "rest",
    "nodes",
    "yet",
    "using",
    "ac3",
    "enforcement",
    "arc",
    "consistency",
    "sometimes",
    "effect",
    "reducing",
    "domains",
    "make",
    "easier",
    "find",
    "solutions",
    "always",
    "actually",
    "solve",
    "problem",
    "might",
    "still",
    "need",
    "somehow",
    "search",
    "try",
    "find",
    "solution",
    "use",
    "classical",
    "traditional",
    "search",
    "algorithms",
    "try",
    "recall",
    "search",
    "problem",
    "generally",
    "consists",
    "parts",
    "initial",
    "state",
    "actions",
    "transition",
    "model",
    "takes",
    "one",
    "state",
    "another",
    "state",
    "goal",
    "test",
    "tell",
    "satisfied",
    "objective",
    "correctly",
    "path",
    "cost",
    "function",
    "case",
    "like",
    "maze",
    "solving",
    "trying",
    "get",
    "goal",
    "quickly",
    "possible",
    "could",
    "formulate",
    "csp",
    "constraint",
    "satisfaction",
    "problem",
    "one",
    "types",
    "search",
    "problems",
    "initial",
    "state",
    "empty",
    "assignment",
    "assignment",
    "way",
    "assign",
    "particular",
    "variable",
    "particular",
    "value",
    "empty",
    "assignment",
    "variables",
    "assigned",
    "values",
    "yet",
    "action",
    "take",
    "adding",
    "new",
    "variable",
    "equals",
    "value",
    "pair",
    "assignment",
    "saying",
    "assignment",
    "let",
    "add",
    "new",
    "value",
    "variable",
    "transition",
    "model",
    "defines",
    "happens",
    "take",
    "action",
    "get",
    "new",
    "assignment",
    "variable",
    "equal",
    "value",
    "inside",
    "goal",
    "test",
    "checking",
    "make",
    "sure",
    "variables",
    "assigned",
    "making",
    "sure",
    "constraints",
    "satisfied",
    "path",
    "cost",
    "function",
    "sort",
    "irrelevant",
    "really",
    "care",
    "path",
    "really",
    "care",
    "finding",
    "assignment",
    "actually",
    "satisfies",
    "constraints",
    "really",
    "paths",
    "cost",
    "really",
    "care",
    "path",
    "goal",
    "care",
    "solution",
    "much",
    "talked",
    "problem",
    "though",
    "implement",
    "naive",
    "search",
    "algorithm",
    "implementing",
    "like",
    "search",
    "search",
    "going",
    "inefficient",
    "ways",
    "take",
    "advantage",
    "efficiencies",
    "structure",
    "constraint",
    "satisfaction",
    "problem",
    "one",
    "key",
    "ideas",
    "really",
    "order",
    "variables",
    "matter",
    "order",
    "assign",
    "variables",
    "assignment",
    "equals",
    "2",
    "b",
    "equals",
    "8",
    "identical",
    "assignment",
    "b",
    "equals",
    "8",
    "equals",
    "switching",
    "order",
    "really",
    "change",
    "anything",
    "fundamental",
    "nature",
    "assignment",
    "ways",
    "try",
    "revise",
    "idea",
    "search",
    "algorithm",
    "apply",
    "specifically",
    "problem",
    "like",
    "constraint",
    "satisfaction",
    "problem",
    "turns",
    "search",
    "algorithm",
    "generally",
    "use",
    "talking",
    "constraint",
    "satisfaction",
    "problems",
    "something",
    "known",
    "backtracking",
    "search",
    "big",
    "idea",
    "backtracking",
    "search",
    "go",
    "ahead",
    "make",
    "assignments",
    "variables",
    "values",
    "ever",
    "get",
    "stuck",
    "arrive",
    "place",
    "way",
    "make",
    "forward",
    "progress",
    "still",
    "preserving",
    "constraints",
    "need",
    "enforce",
    "go",
    "ahead",
    "backtrack",
    "try",
    "something",
    "else",
    "instead",
    "basic",
    "sketch",
    "backtracking",
    "search",
    "looks",
    "like",
    "looks",
    "like",
    "function",
    "called",
    "backtrack",
    "takes",
    "input",
    "assignment",
    "constraint",
    "satisfaction",
    "problem",
    "initially",
    "assigned",
    "variables",
    "begin",
    "backtracking",
    "search",
    "assignment",
    "going",
    "empty",
    "assignment",
    "variables",
    "inside",
    "see",
    "later",
    "going",
    "recursive",
    "function",
    "backtrack",
    "takes",
    "input",
    "assignment",
    "problem",
    "assignment",
    "complete",
    "meaning",
    "variables",
    "assigned",
    "return",
    "assignment",
    "course",
    "wo",
    "true",
    "initially",
    "start",
    "empty",
    "assignment",
    "time",
    "might",
    "add",
    "things",
    "assignment",
    "ever",
    "assignment",
    "actually",
    "complete",
    "done",
    "go",
    "ahead",
    "return",
    "assignment",
    "otherwise",
    "work",
    "done",
    "need",
    "select",
    "unassigned",
    "variable",
    "particular",
    "problem",
    "need",
    "take",
    "problem",
    "look",
    "variables",
    "already",
    "assigned",
    "pick",
    "variable",
    "yet",
    "assigned",
    "go",
    "ahead",
    "take",
    "variable",
    "need",
    "consider",
    "values",
    "variable",
    "domain",
    "go",
    "ahead",
    "call",
    "domain",
    "values",
    "function",
    "talk",
    "little",
    "later",
    "takes",
    "variable",
    "gives",
    "back",
    "ordered",
    "list",
    "values",
    "domain",
    "taken",
    "random",
    "unselected",
    "variable",
    "going",
    "loop",
    "possible",
    "values",
    "idea",
    "let",
    "try",
    "values",
    "possible",
    "values",
    "variable",
    "value",
    "consistent",
    "assignment",
    "far",
    "violate",
    "constraints",
    "well",
    "let",
    "go",
    "ahead",
    "add",
    "variable",
    "equals",
    "value",
    "assignment",
    "far",
    "consistent",
    "let",
    "recursively",
    "call",
    "backtrack",
    "try",
    "make",
    "rest",
    "assignments",
    "also",
    "consistent",
    "go",
    "ahead",
    "call",
    "backtrack",
    "new",
    "assignment",
    "added",
    "variable",
    "equals",
    "value",
    "recursively",
    "call",
    "backtrack",
    "see",
    "result",
    "result",
    "failure",
    "well",
    "let",
    "return",
    "result",
    "otherwise",
    "else",
    "could",
    "happen",
    "well",
    "turns",
    "result",
    "failure",
    "well",
    "means",
    "value",
    "probably",
    "bad",
    "choice",
    "particular",
    "variable",
    "assigned",
    "variable",
    "equal",
    "value",
    "eventually",
    "road",
    "ran",
    "situation",
    "violated",
    "constraints",
    "nothing",
    "could",
    "remove",
    "variable",
    "equals",
    "value",
    "assignment",
    "effectively",
    "backtracking",
    "say",
    "right",
    "value",
    "work",
    "let",
    "try",
    "another",
    "value",
    "instead",
    "end",
    "never",
    "able",
    "return",
    "complete",
    "assignment",
    "go",
    "ahead",
    "return",
    "failure",
    "means",
    "none",
    "values",
    "worked",
    "particular",
    "variable",
    "idea",
    "backtracking",
    "search",
    "take",
    "variables",
    "try",
    "values",
    "recursively",
    "try",
    "backtracking",
    "search",
    "see",
    "make",
    "progress",
    "ever",
    "run",
    "dead",
    "end",
    "run",
    "situation",
    "possible",
    "value",
    "choose",
    "satisfies",
    "constraints",
    "return",
    "failure",
    "propagates",
    "eventually",
    "make",
    "different",
    "choice",
    "going",
    "back",
    "trying",
    "something",
    "else",
    "instead",
    "let",
    "put",
    "algorithm",
    "practice",
    "let",
    "actually",
    "try",
    "use",
    "backtracking",
    "search",
    "solve",
    "problem",
    "need",
    "figure",
    "assign",
    "courses",
    "exam",
    "slot",
    "monday",
    "tuesday",
    "wednesday",
    "way",
    "satisfies",
    "constraints",
    "edges",
    "mean",
    "two",
    "classes",
    "exam",
    "day",
    "start",
    "starting",
    "node",
    "really",
    "matter",
    "start",
    "case",
    "start",
    "ask",
    "question",
    "right",
    "let",
    "loop",
    "values",
    "domain",
    "maybe",
    "case",
    "start",
    "monday",
    "say",
    "right",
    "let",
    "go",
    "ahead",
    "assign",
    "monday",
    "go",
    "order",
    "monday",
    "tuesday",
    "wednesday",
    "let",
    "consider",
    "node",
    "made",
    "assignment",
    "recursively",
    "call",
    "backtrack",
    "new",
    "part",
    "assignment",
    "looking",
    "pick",
    "another",
    "unassigned",
    "variable",
    "like",
    "say",
    "right",
    "maybe",
    "start",
    "monday",
    "first",
    "value",
    "b",
    "domain",
    "ask",
    "right",
    "monday",
    "violate",
    "constraints",
    "turns",
    "yes",
    "violates",
    "constraint",
    "b",
    "b",
    "monday",
    "work",
    "b",
    "ca",
    "day",
    "work",
    "might",
    "instead",
    "try",
    "tuesday",
    "try",
    "next",
    "value",
    "b",
    "domain",
    "consistent",
    "assignment",
    "far",
    "well",
    "yeah",
    "b",
    "tuesday",
    "monday",
    "consistent",
    "far",
    "day",
    "good",
    "recursively",
    "call",
    "backtrack",
    "try",
    "pick",
    "another",
    "unassigned",
    "variable",
    "something",
    "like",
    "say",
    "right",
    "let",
    "go",
    "possible",
    "values",
    "monday",
    "consistent",
    "assignment",
    "well",
    "yes",
    "b",
    "different",
    "days",
    "monday",
    "versus",
    "tuesday",
    "b",
    "also",
    "different",
    "days",
    "monday",
    "versus",
    "tuesday",
    "fine",
    "far",
    "go",
    "ahead",
    "try",
    "maybe",
    "go",
    "variable",
    "say",
    "make",
    "consistent",
    "let",
    "go",
    "possible",
    "values",
    "recursively",
    "called",
    "backtrack",
    "might",
    "start",
    "monday",
    "say",
    "right",
    "consistent",
    "e",
    "exams",
    "day",
    "might",
    "try",
    "tuesday",
    "instead",
    "going",
    "next",
    "one",
    "ask",
    "consistent",
    "well",
    "b",
    "e",
    "exams",
    "day",
    "try",
    "right",
    "wednesday",
    "consistent",
    "turn",
    "like",
    "right",
    "yes",
    "wednesday",
    "consistent",
    "e",
    "exams",
    "different",
    "days",
    "b",
    "e",
    "exams",
    "different",
    "days",
    "seems",
    "well",
    "far",
    "recursively",
    "call",
    "backtrack",
    "select",
    "another",
    "unassigned",
    "variable",
    "say",
    "maybe",
    "choose",
    "c",
    "time",
    "say",
    "right",
    "let",
    "try",
    "values",
    "c",
    "could",
    "take",
    "let",
    "start",
    "monday",
    "turns",
    "consistent",
    "c",
    "exams",
    "day",
    "try",
    "tuesday",
    "say",
    "consistent",
    "either",
    "b",
    "c",
    "exams",
    "day",
    "say",
    "right",
    "let",
    "go",
    "ahead",
    "try",
    "wednesday",
    "consistent",
    "either",
    "c",
    "e",
    "exams",
    "day",
    "gone",
    "possible",
    "values",
    "c",
    "monday",
    "tuesday",
    "wednesday",
    "none",
    "consistent",
    "way",
    "consistent",
    "assignment",
    "backtrack",
    "case",
    "return",
    "failure",
    "say",
    "right",
    "backtrack",
    "back",
    "well",
    "e",
    "tried",
    "monday",
    "tuesday",
    "wednesday",
    "none",
    "work",
    "wednesday",
    "seemed",
    "work",
    "turned",
    "failure",
    "means",
    "possible",
    "way",
    "assign",
    "failure",
    "go",
    "back",
    "means",
    "monday",
    "assignment",
    "must",
    "wrong",
    "must",
    "try",
    "something",
    "else",
    "try",
    "right",
    "instead",
    "monday",
    "try",
    "tuesday",
    "tuesday",
    "turns",
    "consistent",
    "b",
    "exam",
    "day",
    "wednesday",
    "turns",
    "works",
    "begin",
    "mix",
    "forward",
    "progress",
    "go",
    "back",
    "e",
    "say",
    "right",
    "values",
    "works",
    "monday",
    "turns",
    "work",
    "violating",
    "constraints",
    "go",
    "c",
    "monday",
    "work",
    "violates",
    "constraint",
    "violates",
    "two",
    "actually",
    "tuesday",
    "work",
    "violates",
    "constraint",
    "well",
    "wednesday",
    "work",
    "go",
    "next",
    "variable",
    "f",
    "say",
    "right",
    "monday",
    "work",
    "know",
    "violates",
    "constraint",
    "tuesday",
    "work",
    "finally",
    "look",
    "last",
    "variable",
    "g",
    "recursively",
    "calling",
    "backtrack",
    "one",
    "time",
    "monday",
    "inconsistent",
    "violates",
    "constraint",
    "tuesday",
    "also",
    "violates",
    "constraint",
    "wednesday",
    "violate",
    "constraint",
    "point",
    "recursively",
    "call",
    "backtrack",
    "one",
    "last",
    "time",
    "satisfactory",
    "assignment",
    "variables",
    "point",
    "say",
    "done",
    "able",
    "successfully",
    "assign",
    "variable",
    "value",
    "one",
    "variables",
    "way",
    "violating",
    "constraints",
    "going",
    "go",
    "ahead",
    "classes",
    "e",
    "exams",
    "monday",
    "classes",
    "b",
    "f",
    "exams",
    "tuesday",
    "classes",
    "c",
    "g",
    "exams",
    "wednesday",
    "violated",
    "constraints",
    "might",
    "come",
    "graphical",
    "look",
    "might",
    "work",
    "let",
    "take",
    "look",
    "code",
    "could",
    "use",
    "actually",
    "try",
    "solve",
    "problem",
    "well",
    "go",
    "ahead",
    "go",
    "scheduling",
    "directory",
    "start",
    "looking",
    "define",
    "list",
    "variables",
    "b",
    "c",
    "e",
    "f",
    "different",
    "classes",
    "underneath",
    "define",
    "list",
    "constraints",
    "constraint",
    "constraint",
    "ca",
    "day",
    "likewise",
    "c",
    "b",
    "c",
    "forth",
    "enforcing",
    "exact",
    "constraints",
    "backtracking",
    "function",
    "might",
    "look",
    "like",
    "first",
    "assignment",
    "complete",
    "made",
    "assignment",
    "every",
    "variable",
    "value",
    "go",
    "ahead",
    "return",
    "assignment",
    "select",
    "unassigned",
    "variable",
    "assignment",
    "possible",
    "values",
    "domain",
    "monday",
    "tuesday",
    "wednesday",
    "let",
    "go",
    "ahead",
    "create",
    "new",
    "assignment",
    "assigns",
    "variable",
    "value",
    "call",
    "consistent",
    "function",
    "show",
    "moment",
    "checks",
    "make",
    "sure",
    "new",
    "assignment",
    "consistent",
    "consistent",
    "go",
    "ahead",
    "call",
    "backtrack",
    "go",
    "ahead",
    "continue",
    "trying",
    "run",
    "backtracking",
    "search",
    "long",
    "result",
    "none",
    "meaning",
    "failure",
    "go",
    "ahead",
    "return",
    "result",
    "make",
    "values",
    "nothing",
    "works",
    "failure",
    "solution",
    "go",
    "ahead",
    "return",
    "none",
    "functions",
    "select",
    "unassigned",
    "variable",
    "going",
    "choose",
    "variable",
    "yet",
    "assigned",
    "going",
    "loop",
    "variables",
    "already",
    "assigned",
    "go",
    "ahead",
    "return",
    "variable",
    "consistent",
    "function",
    "well",
    "consistent",
    "function",
    "goes",
    "constraints",
    "situation",
    "assigned",
    "values",
    "variables",
    "well",
    "violation",
    "constraint",
    "case",
    "return",
    "false",
    "nothing",
    "inconsistent",
    "assignment",
    "consistent",
    "return",
    "true",
    "program",
    "calls",
    "backtrack",
    "empty",
    "assignment",
    "empty",
    "dictionary",
    "variable",
    "assigned",
    "values",
    "yet",
    "save",
    "inside",
    "solution",
    "print",
    "solution",
    "running",
    "run",
    "python",
    "get",
    "result",
    "assignment",
    "variables",
    "values",
    "turns",
    "assign",
    "monday",
    "would",
    "expect",
    "b",
    "tuesday",
    "c",
    "wednesday",
    "exactly",
    "type",
    "thing",
    "talking",
    "assignment",
    "variables",
    "values",
    "violate",
    "constraints",
    "fair",
    "amount",
    "work",
    "order",
    "implement",
    "idea",
    "write",
    "backtrack",
    "function",
    "went",
    "ahead",
    "went",
    "process",
    "recursively",
    "trying",
    "backtracking",
    "search",
    "turns",
    "constraint",
    "satisfaction",
    "problems",
    "popular",
    "exist",
    "many",
    "libraries",
    "already",
    "implement",
    "type",
    "idea",
    "specific",
    "library",
    "important",
    "fact",
    "libraries",
    "exist",
    "one",
    "example",
    "python",
    "constraint",
    "library",
    "rather",
    "work",
    "scratch",
    "inside",
    "taking",
    "advantage",
    "library",
    "implements",
    "lot",
    "ideas",
    "already",
    "create",
    "new",
    "problem",
    "add",
    "variables",
    "particular",
    "domains",
    "add",
    "whole",
    "bunch",
    "individual",
    "constraints",
    "call",
    "addconstraint",
    "pass",
    "function",
    "describing",
    "constraint",
    "constraint",
    "basically",
    "says",
    "function",
    "takes",
    "two",
    "variables",
    "x",
    "makes",
    "sure",
    "x",
    "equal",
    "enforcing",
    "idea",
    "two",
    "classes",
    "exams",
    "day",
    "constraint",
    "satisfaction",
    "problem",
    "call",
    "getsolutions",
    "get",
    "solutions",
    "problem",
    "solutions",
    "print",
    "solution",
    "happens",
    "run",
    "python",
    "see",
    "actually",
    "number",
    "different",
    "solutions",
    "used",
    "solve",
    "problem",
    "fact",
    "six",
    "different",
    "solutions",
    "assignments",
    "variables",
    "values",
    "give",
    "satisfactory",
    "answer",
    "constraint",
    "satisfaction",
    "problem",
    "implementation",
    "basic",
    "backtracking",
    "search",
    "method",
    "really",
    "went",
    "variables",
    "picked",
    "one",
    "assigned",
    "tried",
    "possible",
    "values",
    "variable",
    "could",
    "take",
    "worked",
    "violate",
    "constraints",
    "kept",
    "trying",
    "variables",
    "ever",
    "hit",
    "dead",
    "end",
    "backtrack",
    "ultimately",
    "might",
    "able",
    "little",
    "bit",
    "intelligent",
    "order",
    "improve",
    "efficiency",
    "solve",
    "sorts",
    "problems",
    "one",
    "thing",
    "might",
    "imagine",
    "trying",
    "going",
    "back",
    "idea",
    "inference",
    "using",
    "knowledge",
    "know",
    "able",
    "draw",
    "conclusions",
    "order",
    "make",
    "rest",
    "problem",
    "solving",
    "process",
    "little",
    "bit",
    "easier",
    "let",
    "go",
    "back",
    "got",
    "stuck",
    "problem",
    "first",
    "time",
    "solving",
    "constraint",
    "satisfaction",
    "problem",
    "dealt",
    "went",
    "went",
    "ahead",
    "assigned",
    "monday",
    "seemed",
    "work",
    "assignment",
    "far",
    "violate",
    "constraints",
    "turned",
    "later",
    "choice",
    "turned",
    "bad",
    "one",
    "choice",
    "consistent",
    "rest",
    "values",
    "could",
    "take",
    "question",
    "anything",
    "could",
    "avoid",
    "getting",
    "situation",
    "like",
    "avoid",
    "trying",
    "go",
    "path",
    "ultimately",
    "going",
    "lead",
    "anywhere",
    "taking",
    "advantage",
    "knowledge",
    "initially",
    "turns",
    "kind",
    "knowledge",
    "look",
    "structure",
    "graph",
    "far",
    "say",
    "right",
    "c",
    "domain",
    "example",
    "contains",
    "values",
    "monday",
    "tuesday",
    "wednesday",
    "based",
    "values",
    "say",
    "graph",
    "arc",
    "consistent",
    "recall",
    "arc",
    "consistency",
    "making",
    "sure",
    "every",
    "possible",
    "value",
    "particular",
    "node",
    "value",
    "able",
    "choose",
    "see",
    "monday",
    "tuesday",
    "going",
    "possible",
    "values",
    "choose",
    "going",
    "consistent",
    "node",
    "like",
    "b",
    "example",
    "b",
    "equal",
    "tuesday",
    "means",
    "c",
    "tuesday",
    "equal",
    "monday",
    "c",
    "also",
    "monday",
    "using",
    "information",
    "making",
    "c",
    "arc",
    "consistent",
    "b",
    "could",
    "remove",
    "monday",
    "tuesday",
    "c",
    "domain",
    "leave",
    "c",
    "wednesday",
    "example",
    "continued",
    "try",
    "enforce",
    "arc",
    "consistency",
    "see",
    "conclusions",
    "draw",
    "well",
    "see",
    "b",
    "option",
    "tuesday",
    "c",
    "option",
    "wednesday",
    "want",
    "make",
    "e",
    "arc",
    "consistent",
    "well",
    "e",
    "ca",
    "tuesday",
    "would",
    "arc",
    "consistent",
    "e",
    "ca",
    "wednesday",
    "would",
    "arc",
    "consistent",
    "go",
    "ahead",
    "say",
    "e",
    "set",
    "equal",
    "monday",
    "example",
    "begin",
    "process",
    "order",
    "make",
    "arc",
    "consistent",
    "b",
    "e",
    "would",
    "wednesday",
    "possible",
    "option",
    "likewise",
    "make",
    "judgments",
    "f",
    "g",
    "well",
    "turns",
    "without",
    "additional",
    "search",
    "enforcing",
    "arc",
    "consistency",
    "able",
    "actually",
    "figure",
    "assignment",
    "variables",
    "without",
    "needing",
    "backtrack",
    "way",
    "interleaving",
    "search",
    "process",
    "inference",
    "step",
    "step",
    "trying",
    "enforce",
    "arc",
    "consistency",
    "algorithm",
    "often",
    "called",
    "maintaining",
    "arc",
    "consistency",
    "algorithm",
    "enforces",
    "arc",
    "consistency",
    "every",
    "time",
    "make",
    "new",
    "assignment",
    "value",
    "existing",
    "variable",
    "sometimes",
    "enforce",
    "consistency",
    "using",
    "ac3",
    "algorithm",
    "beginning",
    "problem",
    "even",
    "begin",
    "searching",
    "order",
    "limit",
    "domain",
    "variables",
    "order",
    "make",
    "easier",
    "search",
    "also",
    "take",
    "advantage",
    "interleaving",
    "enforcing",
    "consistency",
    "search",
    "every",
    "time",
    "search",
    "process",
    "make",
    "new",
    "assignment",
    "go",
    "ahead",
    "enforce",
    "arc",
    "consistency",
    "well",
    "make",
    "sure",
    "eliminating",
    "possible",
    "values",
    "domains",
    "whenever",
    "possible",
    "well",
    "really",
    "equivalent",
    "every",
    "time",
    "make",
    "new",
    "assignment",
    "variable",
    "go",
    "ahead",
    "call",
    "ac3",
    "algorithm",
    "algorithm",
    "enforces",
    "arc",
    "consistency",
    "constraint",
    "satisfaction",
    "problem",
    "go",
    "ahead",
    "call",
    "starting",
    "q",
    "arcs",
    "originally",
    "arcs",
    "want",
    "make",
    "arc",
    "consistent",
    "x",
    "thing",
    "made",
    "assignment",
    "arcs",
    "yx",
    "neighbor",
    "x",
    "something",
    "shares",
    "constraint",
    "x",
    "example",
    "maintaining",
    "arc",
    "consistency",
    "backtracking",
    "search",
    "process",
    "ultimately",
    "make",
    "search",
    "process",
    "little",
    "bit",
    "efficient",
    "revised",
    "version",
    "backtrack",
    "function",
    "changes",
    "highlighted",
    "yellow",
    "every",
    "time",
    "add",
    "new",
    "variable",
    "equals",
    "value",
    "assignment",
    "go",
    "ahead",
    "run",
    "inference",
    "procedure",
    "might",
    "number",
    "different",
    "things",
    "one",
    "thing",
    "could",
    "call",
    "maintaining",
    "arc",
    "consistency",
    "algorithm",
    "make",
    "sure",
    "able",
    "enforce",
    "arc",
    "consistency",
    "problem",
    "might",
    "able",
    "draw",
    "new",
    "inferences",
    "result",
    "process",
    "get",
    "new",
    "guarantees",
    "variable",
    "needs",
    "equal",
    "value",
    "example",
    "might",
    "happen",
    "one",
    "time",
    "might",
    "happen",
    "many",
    "times",
    "long",
    "inferences",
    "failure",
    "long",
    "lead",
    "situation",
    "possible",
    "way",
    "make",
    "forward",
    "progress",
    "well",
    "go",
    "ahead",
    "add",
    "inferences",
    "new",
    "knowledge",
    "new",
    "pieces",
    "knowledge",
    "know",
    "variables",
    "assigned",
    "values",
    "add",
    "assignment",
    "order",
    "quickly",
    "make",
    "forward",
    "progress",
    "taking",
    "advantage",
    "information",
    "deduce",
    "information",
    "know",
    "based",
    "rest",
    "structure",
    "constraint",
    "satisfaction",
    "problem",
    "change",
    "need",
    "make",
    "turns",
    "value",
    "work",
    "well",
    "go",
    "ahead",
    "need",
    "remove",
    "variable",
    "equals",
    "value",
    "also",
    "inferences",
    "made",
    "remove",
    "assignment",
    "well",
    "often",
    "able",
    "solve",
    "problem",
    "backtracking",
    "less",
    "might",
    "originally",
    "needed",
    "taking",
    "advantage",
    "fact",
    "every",
    "time",
    "make",
    "new",
    "assignment",
    "one",
    "variable",
    "one",
    "value",
    "might",
    "reduce",
    "domains",
    "variables",
    "well",
    "use",
    "information",
    "begin",
    "quickly",
    "draw",
    "conclusions",
    "order",
    "try",
    "solve",
    "problem",
    "efficiently",
    "well",
    "turns",
    "heuristics",
    "use",
    "try",
    "improve",
    "efficiency",
    "search",
    "process",
    "well",
    "really",
    "boils",
    "couple",
    "functions",
    "talked",
    "really",
    "talked",
    "working",
    "one",
    "function",
    "select",
    "unassigned",
    "variable",
    "selecting",
    "variable",
    "constraint",
    "satisfaction",
    "problem",
    "yet",
    "assigned",
    "far",
    "sort",
    "selecting",
    "variables",
    "randomly",
    "like",
    "picking",
    "one",
    "variable",
    "one",
    "unassigned",
    "variable",
    "order",
    "decide",
    "right",
    "variable",
    "going",
    "assign",
    "next",
    "going",
    "turns",
    "little",
    "bit",
    "intelligent",
    "following",
    "certain",
    "heuristics",
    "might",
    "able",
    "make",
    "search",
    "process",
    "much",
    "efficient",
    "choosing",
    "carefully",
    "variable",
    "explore",
    "next",
    "heuristics",
    "include",
    "minimum",
    "remaining",
    "values",
    "mrv",
    "heuristic",
    "generally",
    "says",
    "choice",
    "variable",
    "select",
    "select",
    "variable",
    "smallest",
    "domain",
    "variable",
    "fewest",
    "number",
    "remaining",
    "values",
    "left",
    "idea",
    "two",
    "remaining",
    "values",
    "left",
    "well",
    "may",
    "well",
    "prune",
    "one",
    "quickly",
    "order",
    "get",
    "one",
    "two",
    "got",
    "solution",
    "solution",
    "exist",
    "sometimes",
    "minimum",
    "remaining",
    "values",
    "might",
    "give",
    "conclusive",
    "result",
    "nodes",
    "number",
    "remaining",
    "values",
    "example",
    "case",
    "another",
    "heuristic",
    "helpful",
    "look",
    "degree",
    "heuristic",
    "degree",
    "node",
    "number",
    "nodes",
    "attached",
    "node",
    "number",
    "nodes",
    "constrained",
    "particular",
    "node",
    "imagine",
    "variable",
    "choose",
    "choose",
    "variable",
    "high",
    "degree",
    "connected",
    "lot",
    "different",
    "things",
    "variable",
    "low",
    "degree",
    "connected",
    "lot",
    "different",
    "things",
    "well",
    "often",
    "make",
    "sense",
    "choose",
    "variable",
    "highest",
    "degree",
    "connected",
    "nodes",
    "thing",
    "would",
    "search",
    "first",
    "case",
    "well",
    "choosing",
    "variable",
    "high",
    "degree",
    "immediately",
    "going",
    "constrain",
    "rest",
    "variables",
    "likely",
    "able",
    "eliminate",
    "large",
    "sections",
    "state",
    "space",
    "need",
    "search",
    "could",
    "actually",
    "look",
    "like",
    "let",
    "go",
    "back",
    "search",
    "problem",
    "particular",
    "case",
    "made",
    "assignment",
    "made",
    "assignment",
    "question",
    "look",
    "next",
    "according",
    "minimum",
    "remaining",
    "values",
    "heuristic",
    "choose",
    "variable",
    "fewest",
    "remaining",
    "possible",
    "values",
    "case",
    "node",
    "node",
    "c",
    "one",
    "variable",
    "left",
    "domain",
    "case",
    "wednesday",
    "reasonable",
    "choice",
    "next",
    "assignment",
    "make",
    "know",
    "option",
    "example",
    "know",
    "possible",
    "option",
    "c",
    "wednesday",
    "may",
    "well",
    "make",
    "assignment",
    "potentially",
    "explore",
    "rest",
    "space",
    "meanwhile",
    "start",
    "problem",
    "knowledge",
    "nodes",
    "values",
    "yet",
    "still",
    "pick",
    "node",
    "first",
    "one",
    "try",
    "assign",
    "value",
    "arbitrarily",
    "chose",
    "one",
    "top",
    "node",
    "originally",
    "intelligent",
    "look",
    "particular",
    "graph",
    "domains",
    "size",
    "domain",
    "size",
    "minimum",
    "remaining",
    "values",
    "really",
    "help",
    "us",
    "might",
    "notice",
    "node",
    "e",
    "highest",
    "degree",
    "connected",
    "things",
    "perhaps",
    "makes",
    "sense",
    "begin",
    "search",
    "rather",
    "starting",
    "node",
    "top",
    "start",
    "node",
    "highest",
    "degree",
    "start",
    "searching",
    "node",
    "e",
    "going",
    "much",
    "easily",
    "allow",
    "us",
    "enforce",
    "constraints",
    "nearby",
    "eliminating",
    "large",
    "portions",
    "search",
    "space",
    "might",
    "need",
    "search",
    "fact",
    "starting",
    "e",
    "immediately",
    "assign",
    "variables",
    "following",
    "actually",
    "assign",
    "rest",
    "variables",
    "without",
    "needing",
    "backtracking",
    "even",
    "using",
    "inference",
    "procedure",
    "starting",
    "node",
    "high",
    "degree",
    "going",
    "quickly",
    "restrict",
    "possible",
    "values",
    "nodes",
    "take",
    "go",
    "selecting",
    "unassigned",
    "variable",
    "particular",
    "order",
    "rather",
    "randomly",
    "picking",
    "variable",
    "little",
    "bit",
    "intelligent",
    "choose",
    "make",
    "search",
    "process",
    "much",
    "much",
    "efficient",
    "making",
    "sure",
    "search",
    "portions",
    "search",
    "space",
    "ultimately",
    "going",
    "matter",
    "variable",
    "really",
    "talked",
    "function",
    "domain",
    "values",
    "function",
    "domain",
    "values",
    "function",
    "takes",
    "variable",
    "gives",
    "back",
    "sequence",
    "values",
    "inside",
    "variable",
    "domain",
    "naive",
    "way",
    "approach",
    "go",
    "order",
    "go",
    "monday",
    "tuesday",
    "wednesday",
    "problem",
    "going",
    "order",
    "might",
    "efficient",
    "order",
    "search",
    "sometimes",
    "might",
    "efficient",
    "choose",
    "values",
    "likely",
    "solutions",
    "first",
    "go",
    "values",
    "assess",
    "whether",
    "value",
    "likelier",
    "lead",
    "solution",
    "less",
    "likely",
    "lead",
    "solution",
    "well",
    "one",
    "thing",
    "take",
    "look",
    "many",
    "constraints",
    "get",
    "added",
    "many",
    "things",
    "get",
    "removed",
    "domains",
    "make",
    "new",
    "assignment",
    "variable",
    "particular",
    "value",
    "heuristic",
    "use",
    "least",
    "constraining",
    "value",
    "heuristic",
    "idea",
    "return",
    "variables",
    "order",
    "based",
    "number",
    "choices",
    "ruled",
    "neighboring",
    "values",
    "want",
    "start",
    "least",
    "constraining",
    "value",
    "value",
    "rules",
    "fewest",
    "possible",
    "options",
    "idea",
    "care",
    "finding",
    "solution",
    "start",
    "value",
    "rules",
    "lot",
    "choices",
    "ruling",
    "lot",
    "possibilities",
    "maybe",
    "going",
    "make",
    "less",
    "likely",
    "particular",
    "choice",
    "leads",
    "solution",
    "whereas",
    "hand",
    "variable",
    "start",
    "choosing",
    "value",
    "rule",
    "much",
    "well",
    "still",
    "lot",
    "space",
    "might",
    "solution",
    "could",
    "ultimately",
    "find",
    "might",
    "seem",
    "little",
    "bit",
    "counterintuitive",
    "little",
    "bit",
    "odds",
    "talking",
    "said",
    "picking",
    "variable",
    "pick",
    "variable",
    "going",
    "fewest",
    "possible",
    "values",
    "remaining",
    "want",
    "pick",
    "value",
    "variable",
    "least",
    "constraining",
    "general",
    "idea",
    "picking",
    "variable",
    "would",
    "like",
    "prune",
    "large",
    "portions",
    "search",
    "space",
    "choosing",
    "variable",
    "going",
    "allow",
    "quickly",
    "eliminate",
    "possible",
    "options",
    "whereas",
    "within",
    "particular",
    "variable",
    "considering",
    "values",
    "variable",
    "could",
    "take",
    "would",
    "like",
    "find",
    "solution",
    "want",
    "ultimately",
    "choose",
    "value",
    "still",
    "leaves",
    "open",
    "possibility",
    "finding",
    "solution",
    "likely",
    "possible",
    "ruling",
    "many",
    "options",
    "leave",
    "open",
    "possibility",
    "still",
    "find",
    "solution",
    "without",
    "needing",
    "go",
    "back",
    "later",
    "backtrack",
    "example",
    "might",
    "particular",
    "situation",
    "trying",
    "choose",
    "variable",
    "value",
    "node",
    "c",
    "c",
    "equal",
    "either",
    "tuesday",
    "wednesday",
    "know",
    "ca",
    "monday",
    "conflicts",
    "domain",
    "already",
    "know",
    "monday",
    "c",
    "must",
    "tuesday",
    "wednesday",
    "question",
    "try",
    "tuesday",
    "first",
    "try",
    "wednesday",
    "first",
    "try",
    "tuesday",
    "gets",
    "ruled",
    "well",
    "one",
    "option",
    "gets",
    "ruled",
    "second",
    "option",
    "gets",
    "ruled",
    "third",
    "option",
    "gets",
    "ruled",
    "choosing",
    "tuesday",
    "would",
    "rule",
    "three",
    "possible",
    "options",
    "choosing",
    "wednesday",
    "well",
    "choosing",
    "wednesday",
    "would",
    "rule",
    "one",
    "option",
    "would",
    "rule",
    "one",
    "option",
    "two",
    "choices",
    "choose",
    "tuesday",
    "rules",
    "three",
    "options",
    "wednesday",
    "rules",
    "two",
    "options",
    "according",
    "least",
    "constraining",
    "value",
    "heuristic",
    "probably",
    "go",
    "ahead",
    "choose",
    "wednesday",
    "one",
    "rules",
    "fewest",
    "number",
    "possible",
    "options",
    "leaving",
    "open",
    "many",
    "chances",
    "possible",
    "eventually",
    "find",
    "solution",
    "inside",
    "state",
    "space",
    "ultimately",
    "continue",
    "process",
    "find",
    "solution",
    "assignment",
    "variables",
    "two",
    "values",
    "allows",
    "us",
    "give",
    "exams",
    "classes",
    "exam",
    "date",
    "conflict",
    "anyone",
    "happens",
    "enrolled",
    "two",
    "classes",
    "time",
    "big",
    "takeaway",
    "number",
    "different",
    "ways",
    "formulate",
    "problem",
    "ways",
    "looked",
    "today",
    "formulate",
    "problem",
    "local",
    "search",
    "problem",
    "problem",
    "looking",
    "current",
    "node",
    "moving",
    "neighbor",
    "based",
    "whether",
    "neighbor",
    "better",
    "worse",
    "current",
    "node",
    "looking",
    "looked",
    "formulating",
    "problems",
    "linear",
    "programs",
    "putting",
    "things",
    "terms",
    "equations",
    "constraints",
    "able",
    "solve",
    "problems",
    "little",
    "bit",
    "efficiently",
    "saw",
    "formulating",
    "problem",
    "constraint",
    "satisfaction",
    "problem",
    "creating",
    "graph",
    "constraints",
    "connect",
    "two",
    "variables",
    "constraint",
    "using",
    "information",
    "able",
    "figure",
    "solution",
    "takeaway",
    "problem",
    "artificial",
    "intelligence",
    "would",
    "like",
    "use",
    "ai",
    "able",
    "solve",
    "whether",
    "trying",
    "figure",
    "hospitals",
    "trying",
    "solve",
    "traveling",
    "salesman",
    "problem",
    "trying",
    "optimize",
    "productions",
    "costs",
    "whatnot",
    "trying",
    "figure",
    "satisfy",
    "certain",
    "constraints",
    "whether",
    "sudoku",
    "puzzle",
    "whether",
    "trying",
    "figure",
    "schedule",
    "exams",
    "university",
    "number",
    "wide",
    "variety",
    "types",
    "problems",
    "formulate",
    "problem",
    "one",
    "sorts",
    "problems",
    "use",
    "known",
    "algorithms",
    "algorithms",
    "enforcing",
    "art",
    "consistency",
    "backtracking",
    "search",
    "hill",
    "climbing",
    "simulated",
    "annealing",
    "algorithms",
    "simplex",
    "algorithms",
    "interior",
    "point",
    "algorithms",
    "used",
    "solve",
    "linear",
    "programs",
    "use",
    "techniques",
    "begin",
    "solve",
    "whole",
    "wide",
    "variety",
    "problems",
    "world",
    "optimization",
    "inside",
    "artificial",
    "intelligence",
    "introduction",
    "artificial",
    "intelligence",
    "python",
    "today",
    "see",
    "next",
    "time",
    "right",
    "welcome",
    "back",
    "everyone",
    "introduction",
    "artificial",
    "intelligence",
    "python",
    "far",
    "class",
    "used",
    "ai",
    "solve",
    "number",
    "different",
    "problems",
    "giving",
    "ai",
    "instructions",
    "search",
    "solution",
    "satisfy",
    "certain",
    "constraints",
    "order",
    "find",
    "way",
    "input",
    "point",
    "output",
    "point",
    "order",
    "solve",
    "sort",
    "problem",
    "today",
    "going",
    "turn",
    "world",
    "learning",
    "particular",
    "idea",
    "machine",
    "learning",
    "generally",
    "refers",
    "idea",
    "going",
    "give",
    "computer",
    "explicit",
    "instructions",
    "perform",
    "task",
    "rather",
    "going",
    "give",
    "computer",
    "access",
    "information",
    "form",
    "data",
    "patterns",
    "learn",
    "let",
    "computer",
    "try",
    "figure",
    "patterns",
    "try",
    "understand",
    "data",
    "able",
    "perform",
    "task",
    "machine",
    "learning",
    "comes",
    "number",
    "different",
    "forms",
    "wide",
    "field",
    "today",
    "explore",
    "foundational",
    "algorithms",
    "ideas",
    "behind",
    "lot",
    "different",
    "areas",
    "within",
    "machine",
    "learning",
    "one",
    "popular",
    "idea",
    "supervised",
    "machine",
    "learning",
    "supervised",
    "learning",
    "supervised",
    "learning",
    "particular",
    "type",
    "task",
    "refers",
    "task",
    "give",
    "computer",
    "access",
    "data",
    "set",
    "data",
    "set",
    "consists",
    "pairs",
    "would",
    "like",
    "computer",
    "would",
    "like",
    "ai",
    "able",
    "figure",
    "function",
    "maps",
    "inputs",
    "outputs",
    "whole",
    "bunch",
    "data",
    "generally",
    "consists",
    "kind",
    "input",
    "evidence",
    "information",
    "computer",
    "access",
    "would",
    "like",
    "computer",
    "based",
    "input",
    "information",
    "predict",
    "output",
    "going",
    "give",
    "data",
    "computer",
    "train",
    "model",
    "begin",
    "understand",
    "information",
    "works",
    "inputs",
    "outputs",
    "relate",
    "ultimately",
    "hope",
    "computer",
    "able",
    "figure",
    "function",
    "given",
    "inputs",
    "able",
    "get",
    "outputs",
    "couple",
    "different",
    "tasks",
    "within",
    "supervised",
    "learning",
    "one",
    "focus",
    "start",
    "known",
    "classification",
    "classification",
    "problem",
    "give",
    "whole",
    "bunch",
    "inputs",
    "need",
    "figure",
    "way",
    "map",
    "inputs",
    "discrete",
    "categories",
    "decide",
    "categories",
    "job",
    "computer",
    "predict",
    "categories",
    "going",
    "might",
    "example",
    "give",
    "information",
    "bank",
    "note",
    "like",
    "us",
    "dollar",
    "asking",
    "predict",
    "belong",
    "category",
    "authentic",
    "bank",
    "notes",
    "belong",
    "category",
    "counterfeit",
    "bank",
    "notes",
    "need",
    "categorize",
    "input",
    "want",
    "train",
    "computer",
    "figure",
    "function",
    "able",
    "calculation",
    "another",
    "example",
    "might",
    "case",
    "weather",
    "someone",
    "talked",
    "little",
    "bit",
    "far",
    "class",
    "would",
    "like",
    "predict",
    "given",
    "day",
    "going",
    "rain",
    "day",
    "going",
    "cloudy",
    "day",
    "seen",
    "could",
    "really",
    "give",
    "computer",
    "exact",
    "probabilities",
    "conditions",
    "probability",
    "rain",
    "oftentimes",
    "access",
    "information",
    "though",
    "access",
    "whole",
    "bunch",
    "data",
    "wanted",
    "able",
    "predict",
    "something",
    "like",
    "going",
    "rain",
    "going",
    "rain",
    "would",
    "give",
    "computer",
    "historical",
    "information",
    "days",
    "raining",
    "days",
    "raining",
    "ask",
    "computer",
    "look",
    "patterns",
    "data",
    "might",
    "data",
    "look",
    "like",
    "well",
    "could",
    "structure",
    "data",
    "table",
    "like",
    "might",
    "table",
    "looks",
    "like",
    "particular",
    "day",
    "going",
    "back",
    "information",
    "day",
    "humidity",
    "day",
    "air",
    "pressure",
    "importantly",
    "label",
    "something",
    "human",
    "said",
    "particular",
    "day",
    "raining",
    "raining",
    "could",
    "fill",
    "table",
    "whole",
    "bunch",
    "data",
    "makes",
    "would",
    "call",
    "supervised",
    "learning",
    "exercise",
    "human",
    "gone",
    "labeled",
    "data",
    "points",
    "said",
    "day",
    "values",
    "humidity",
    "pressure",
    "day",
    "rainy",
    "day",
    "day",
    "rainy",
    "day",
    "would",
    "like",
    "computer",
    "able",
    "able",
    "figure",
    "given",
    "inputs",
    "given",
    "humidity",
    "pressure",
    "computer",
    "predict",
    "label",
    "associated",
    "day",
    "day",
    "look",
    "like",
    "going",
    "day",
    "rains",
    "look",
    "like",
    "day",
    "going",
    "rain",
    "put",
    "little",
    "bit",
    "mathematically",
    "think",
    "function",
    "takes",
    "two",
    "inputs",
    "inputs",
    "data",
    "points",
    "computer",
    "access",
    "things",
    "like",
    "humidity",
    "pressure",
    "could",
    "write",
    "function",
    "f",
    "takes",
    "input",
    "humidity",
    "pressure",
    "output",
    "going",
    "category",
    "would",
    "ascribe",
    "particular",
    "input",
    "points",
    "label",
    "would",
    "associate",
    "input",
    "seen",
    "couple",
    "example",
    "data",
    "points",
    "given",
    "value",
    "humidity",
    "value",
    "pressure",
    "predict",
    "going",
    "rain",
    "going",
    "rain",
    "information",
    "gathered",
    "world",
    "measured",
    "various",
    "different",
    "days",
    "humidity",
    "pressure",
    "observed",
    "whether",
    "saw",
    "rain",
    "rain",
    "particular",
    "day",
    "function",
    "f",
    "would",
    "like",
    "approximate",
    "computer",
    "humans",
    "really",
    "know",
    "exactly",
    "function",
    "f",
    "works",
    "probably",
    "quite",
    "complex",
    "function",
    "going",
    "instead",
    "attempt",
    "estimate",
    "would",
    "like",
    "come",
    "hypothesis",
    "function",
    "h",
    "going",
    "try",
    "approximate",
    "f",
    "want",
    "come",
    "function",
    "h",
    "also",
    "take",
    "inputs",
    "also",
    "produce",
    "output",
    "rain",
    "rain",
    "ideally",
    "like",
    "two",
    "functions",
    "agree",
    "much",
    "possible",
    "goal",
    "supervised",
    "learning",
    "classification",
    "tasks",
    "going",
    "figure",
    "function",
    "h",
    "look",
    "like",
    "begin",
    "estimate",
    "given",
    "information",
    "data",
    "category",
    "label",
    "assigned",
    "particular",
    "data",
    "point",
    "could",
    "begin",
    "well",
    "reasonable",
    "thing",
    "especially",
    "situation",
    "two",
    "numerical",
    "values",
    "could",
    "try",
    "plot",
    "graph",
    "two",
    "axes",
    "case",
    "going",
    "using",
    "two",
    "numerical",
    "values",
    "input",
    "types",
    "ideas",
    "scale",
    "add",
    "inputs",
    "well",
    "plotting",
    "things",
    "two",
    "dimensions",
    "soon",
    "see",
    "could",
    "add",
    "inputs",
    "imagine",
    "things",
    "multiple",
    "dimensions",
    "humans",
    "trouble",
    "conceptualizing",
    "anything",
    "really",
    "beyond",
    "three",
    "dimensions",
    "least",
    "visually",
    "computer",
    "problem",
    "trying",
    "imagine",
    "things",
    "many",
    "many",
    "dimensions",
    "computer",
    "dimension",
    "separate",
    "number",
    "keeping",
    "track",
    "would",
    "unreasonable",
    "computer",
    "think",
    "10",
    "dimensions",
    "100",
    "dimensions",
    "able",
    "try",
    "solve",
    "problem",
    "got",
    "two",
    "inputs",
    "graph",
    "things",
    "along",
    "two",
    "axes",
    "represent",
    "humidity",
    "represents",
    "pressure",
    "might",
    "say",
    "let",
    "take",
    "days",
    "raining",
    "try",
    "plot",
    "graph",
    "see",
    "fall",
    "graph",
    "might",
    "rainy",
    "days",
    "rainy",
    "day",
    "one",
    "blue",
    "dots",
    "corresponds",
    "particular",
    "value",
    "humidity",
    "particular",
    "value",
    "pressure",
    "might",
    "thing",
    "days",
    "rainy",
    "take",
    "rainy",
    "days",
    "figure",
    "values",
    "two",
    "inputs",
    "go",
    "ahead",
    "plot",
    "graph",
    "well",
    "plotted",
    "red",
    "blue",
    "stands",
    "rainy",
    "day",
    "red",
    "stands",
    "rainy",
    "day",
    "input",
    "computer",
    "access",
    "input",
    "would",
    "like",
    "computer",
    "able",
    "train",
    "model",
    "ever",
    "presented",
    "new",
    "input",
    "label",
    "associated",
    "something",
    "like",
    "white",
    "dot",
    "would",
    "like",
    "predict",
    "given",
    "values",
    "two",
    "inputs",
    "classify",
    "blue",
    "dot",
    "rainy",
    "day",
    "classify",
    "red",
    "dot",
    "rainy",
    "day",
    "looking",
    "picture",
    "graphically",
    "trying",
    "say",
    "right",
    "white",
    "dot",
    "look",
    "like",
    "belongs",
    "blue",
    "category",
    "look",
    "like",
    "belongs",
    "red",
    "category",
    "think",
    "people",
    "would",
    "agree",
    "probably",
    "belongs",
    "blue",
    "category",
    "well",
    "looks",
    "like",
    "close",
    "blue",
    "dots",
    "formal",
    "notion",
    "notion",
    "formalize",
    "moment",
    "seems",
    "close",
    "blue",
    "dot",
    "nothing",
    "else",
    "closer",
    "might",
    "say",
    "categorized",
    "blue",
    "fall",
    "category",
    "think",
    "day",
    "going",
    "rainy",
    "day",
    "based",
    "input",
    "might",
    "totally",
    "accurate",
    "pretty",
    "good",
    "guess",
    "type",
    "algorithm",
    "actually",
    "popular",
    "common",
    "machine",
    "learning",
    "algorithm",
    "known",
    "nearest",
    "neighbor",
    "classification",
    "algorithm",
    "solving",
    "problems",
    "nearest",
    "neighbor",
    "classification",
    "going",
    "perform",
    "algorithm",
    "given",
    "input",
    "choose",
    "class",
    "nearest",
    "data",
    "point",
    "input",
    "class",
    "mean",
    "category",
    "like",
    "rain",
    "rain",
    "counterfeit",
    "counterfeit",
    "choose",
    "category",
    "class",
    "based",
    "nearest",
    "data",
    "point",
    "given",
    "data",
    "looked",
    "nearest",
    "data",
    "point",
    "blue",
    "point",
    "red",
    "point",
    "depending",
    "answer",
    "question",
    "able",
    "make",
    "sort",
    "judgment",
    "able",
    "say",
    "something",
    "like",
    "think",
    "going",
    "blue",
    "think",
    "going",
    "red",
    "likewise",
    "could",
    "apply",
    "data",
    "points",
    "encounter",
    "well",
    "suddenly",
    "data",
    "point",
    "comes",
    "well",
    "nearest",
    "data",
    "red",
    "would",
    "go",
    "ahead",
    "classify",
    "red",
    "point",
    "raining",
    "things",
    "get",
    "little",
    "bit",
    "trickier",
    "though",
    "look",
    "point",
    "like",
    "white",
    "point",
    "ask",
    "sort",
    "question",
    "belong",
    "category",
    "blue",
    "points",
    "rainy",
    "days",
    "belong",
    "category",
    "red",
    "points",
    "rainy",
    "days",
    "nearest",
    "neighbor",
    "classification",
    "would",
    "say",
    "way",
    "solve",
    "problem",
    "look",
    "point",
    "nearest",
    "point",
    "look",
    "nearest",
    "point",
    "say",
    "red",
    "rainy",
    "day",
    "therefore",
    "according",
    "nearest",
    "neighbor",
    "classification",
    "would",
    "say",
    "unlabeled",
    "point",
    "well",
    "also",
    "red",
    "also",
    "classified",
    "rainy",
    "day",
    "intuition",
    "might",
    "think",
    "reasonable",
    "judgment",
    "make",
    "closest",
    "thing",
    "rainy",
    "day",
    "may",
    "well",
    "guess",
    "rainy",
    "day",
    "probably",
    "also",
    "reasonable",
    "look",
    "bigger",
    "picture",
    "things",
    "say",
    "yes",
    "true",
    "nearest",
    "point",
    "red",
    "point",
    "surrounded",
    "whole",
    "bunch",
    "blue",
    "points",
    "looking",
    "bigger",
    "picture",
    "potentially",
    "argument",
    "made",
    "point",
    "actually",
    "blue",
    "data",
    "actually",
    "know",
    "sure",
    "given",
    "input",
    "something",
    "trying",
    "predict",
    "necessarily",
    "know",
    "output",
    "going",
    "case",
    "one",
    "correct",
    "difficult",
    "say",
    "oftentimes",
    "considering",
    "single",
    "neighbor",
    "considering",
    "multiple",
    "neighbors",
    "sometimes",
    "give",
    "us",
    "better",
    "result",
    "variant",
    "nearest",
    "neighbor",
    "classification",
    "algorithm",
    "known",
    "k",
    "nearest",
    "neighbor",
    "classification",
    "algorithm",
    "k",
    "parameter",
    "number",
    "choose",
    "many",
    "neighbors",
    "going",
    "look",
    "one",
    "nearest",
    "neighbor",
    "classification",
    "saw",
    "pick",
    "one",
    "nearest",
    "neighbor",
    "use",
    "category",
    "k",
    "nearest",
    "neighbor",
    "classification",
    "k",
    "might",
    "3",
    "5",
    "7",
    "say",
    "look",
    "3",
    "5",
    "7",
    "closest",
    "neighbors",
    "closest",
    "data",
    "points",
    "point",
    "works",
    "little",
    "bit",
    "differently",
    "algorithm",
    "give",
    "input",
    "choose",
    "common",
    "class",
    "k",
    "nearest",
    "data",
    "points",
    "input",
    "look",
    "five",
    "nearest",
    "points",
    "three",
    "say",
    "raining",
    "two",
    "say",
    "raining",
    "go",
    "three",
    "instead",
    "two",
    "one",
    "effectively",
    "gets",
    "one",
    "vote",
    "towards",
    "believe",
    "category",
    "ought",
    "ultimately",
    "choose",
    "category",
    "votes",
    "consequence",
    "k",
    "nearest",
    "neighbor",
    "classification",
    "fairly",
    "straightforward",
    "one",
    "understand",
    "intuitively",
    "look",
    "neighbors",
    "figure",
    "answer",
    "might",
    "turns",
    "work",
    "well",
    "solving",
    "whole",
    "variety",
    "different",
    "types",
    "classification",
    "problems",
    "every",
    "model",
    "going",
    "work",
    "every",
    "situation",
    "one",
    "things",
    "take",
    "look",
    "today",
    "especially",
    "context",
    "supervised",
    "machine",
    "learning",
    "number",
    "different",
    "approaches",
    "machine",
    "learning",
    "number",
    "different",
    "algorithms",
    "apply",
    "solving",
    "type",
    "problem",
    "solving",
    "kind",
    "classification",
    "problem",
    "want",
    "take",
    "inputs",
    "organize",
    "different",
    "categories",
    "one",
    "algorithm",
    "necessarily",
    "always",
    "going",
    "better",
    "algorithm",
    "maybe",
    "depending",
    "data",
    "one",
    "type",
    "algorithm",
    "going",
    "better",
    "suited",
    "trying",
    "model",
    "information",
    "algorithm",
    "lot",
    "machine",
    "learning",
    "research",
    "ends",
    "trying",
    "apply",
    "machine",
    "learning",
    "techniques",
    "often",
    "looking",
    "one",
    "particular",
    "algorithm",
    "trying",
    "multiple",
    "different",
    "algorithms",
    "trying",
    "see",
    "going",
    "give",
    "best",
    "results",
    "trying",
    "predict",
    "function",
    "maps",
    "inputs",
    "outputs",
    "drawbacks",
    "k",
    "nearest",
    "neighbor",
    "classification",
    "well",
    "couple",
    "one",
    "might",
    "naive",
    "approach",
    "least",
    "could",
    "fairly",
    "slow",
    "go",
    "measure",
    "distance",
    "point",
    "every",
    "single",
    "one",
    "points",
    "exist",
    "ways",
    "trying",
    "get",
    "around",
    "data",
    "structures",
    "help",
    "make",
    "quickly",
    "able",
    "find",
    "neighbors",
    "also",
    "techniques",
    "use",
    "try",
    "prune",
    "data",
    "remove",
    "data",
    "points",
    "left",
    "relevant",
    "data",
    "points",
    "make",
    "little",
    "bit",
    "easier",
    "ultimately",
    "might",
    "like",
    "come",
    "another",
    "way",
    "trying",
    "classification",
    "one",
    "way",
    "trying",
    "classification",
    "looking",
    "neighboring",
    "points",
    "another",
    "way",
    "might",
    "try",
    "look",
    "data",
    "see",
    "come",
    "decision",
    "boundary",
    "boundary",
    "separate",
    "rainy",
    "days",
    "rainy",
    "days",
    "case",
    "two",
    "dimensions",
    "drawing",
    "line",
    "example",
    "might",
    "want",
    "try",
    "find",
    "line",
    "find",
    "separator",
    "divides",
    "rainy",
    "days",
    "blue",
    "points",
    "rainy",
    "days",
    "red",
    "points",
    "trying",
    "different",
    "approach",
    "contrast",
    "nearest",
    "neighbor",
    "approach",
    "looked",
    "local",
    "data",
    "around",
    "input",
    "data",
    "point",
    "cared",
    "trying",
    "use",
    "technique",
    "known",
    "linear",
    "regression",
    "find",
    "sort",
    "line",
    "separate",
    "two",
    "halves",
    "sometimes",
    "actually",
    "possible",
    "come",
    "line",
    "perfectly",
    "separates",
    "rainy",
    "days",
    "rainy",
    "days",
    "realistically",
    "though",
    "probably",
    "cleaner",
    "many",
    "data",
    "sets",
    "actually",
    "oftentimes",
    "data",
    "messier",
    "outliers",
    "random",
    "noise",
    "happens",
    "inside",
    "particular",
    "system",
    "like",
    "still",
    "able",
    "figure",
    "line",
    "might",
    "look",
    "like",
    "practice",
    "data",
    "always",
    "linearly",
    "separable",
    "linearly",
    "separable",
    "refers",
    "data",
    "set",
    "could",
    "draw",
    "line",
    "separate",
    "two",
    "halves",
    "perfectly",
    "instead",
    "might",
    "situation",
    "like",
    "rainy",
    "points",
    "side",
    "line",
    "rainy",
    "points",
    "side",
    "line",
    "may",
    "line",
    "perfectly",
    "separates",
    "path",
    "inputs",
    "half",
    "perfectly",
    "separates",
    "rainy",
    "days",
    "rainy",
    "days",
    "still",
    "say",
    "line",
    "pretty",
    "good",
    "job",
    "try",
    "formalize",
    "little",
    "bit",
    "later",
    "mean",
    "say",
    "something",
    "like",
    "line",
    "pretty",
    "good",
    "job",
    "trying",
    "make",
    "prediction",
    "let",
    "say",
    "looking",
    "line",
    "good",
    "job",
    "trying",
    "separate",
    "one",
    "category",
    "things",
    "another",
    "category",
    "things",
    "let",
    "try",
    "formalize",
    "little",
    "bit",
    "mathematically",
    "want",
    "come",
    "sort",
    "function",
    "way",
    "define",
    "line",
    "inputs",
    "things",
    "like",
    "humidity",
    "pressure",
    "case",
    "inputs",
    "might",
    "call",
    "x1",
    "going",
    "represent",
    "humidity",
    "x2",
    "going",
    "represent",
    "pressure",
    "inputs",
    "going",
    "provide",
    "machine",
    "learning",
    "algorithm",
    "given",
    "inputs",
    "would",
    "like",
    "model",
    "able",
    "predict",
    "sort",
    "output",
    "going",
    "predict",
    "using",
    "hypothesis",
    "function",
    "called",
    "hypothesis",
    "function",
    "going",
    "take",
    "input",
    "x1",
    "x2",
    "humidity",
    "pressure",
    "case",
    "imagine",
    "two",
    "inputs",
    "three",
    "four",
    "five",
    "inputs",
    "could",
    "hypothesis",
    "function",
    "take",
    "input",
    "see",
    "examples",
    "little",
    "bit",
    "later",
    "well",
    "question",
    "hypothesis",
    "function",
    "well",
    "really",
    "needs",
    "measure",
    "data",
    "point",
    "one",
    "side",
    "boundary",
    "side",
    "boundary",
    "formalize",
    "boundary",
    "well",
    "boundary",
    "generally",
    "going",
    "linear",
    "combination",
    "input",
    "variables",
    "least",
    "particular",
    "case",
    "trying",
    "say",
    "linear",
    "combination",
    "take",
    "inputs",
    "multiply",
    "number",
    "going",
    "figure",
    "generally",
    "call",
    "number",
    "weight",
    "important",
    "variables",
    "trying",
    "determine",
    "answer",
    "weight",
    "variables",
    "weight",
    "might",
    "add",
    "constant",
    "try",
    "make",
    "function",
    "little",
    "bit",
    "different",
    "result",
    "need",
    "compare",
    "greater",
    "0",
    "less",
    "0",
    "say",
    "belong",
    "one",
    "side",
    "line",
    "side",
    "line",
    "mathematical",
    "expression",
    "might",
    "look",
    "like",
    "would",
    "take",
    "variables",
    "x1",
    "x2",
    "multiply",
    "weight",
    "yet",
    "know",
    "weight",
    "going",
    "number",
    "weight",
    "1",
    "weight",
    "maybe",
    "want",
    "add",
    "weight",
    "0",
    "function",
    "might",
    "require",
    "us",
    "shift",
    "entire",
    "value",
    "certain",
    "amount",
    "compare",
    "math",
    "greater",
    "equal",
    "0",
    "might",
    "categorize",
    "data",
    "point",
    "rainy",
    "day",
    "otherwise",
    "might",
    "say",
    "rain",
    "key",
    "expression",
    "going",
    "calculate",
    "whether",
    "rainy",
    "day",
    "going",
    "bunch",
    "math",
    "take",
    "variables",
    "multiply",
    "weight",
    "maybe",
    "add",
    "extra",
    "weight",
    "see",
    "result",
    "greater",
    "equal",
    "using",
    "result",
    "expression",
    "able",
    "determine",
    "whether",
    "raining",
    "raining",
    "expression",
    "case",
    "going",
    "refer",
    "line",
    "plot",
    "graphically",
    "would",
    "line",
    "line",
    "actually",
    "looks",
    "like",
    "depends",
    "upon",
    "weights",
    "x1",
    "x2",
    "inputs",
    "weights",
    "really",
    "determine",
    "shape",
    "line",
    "slope",
    "line",
    "line",
    "actually",
    "looks",
    "like",
    "would",
    "like",
    "figure",
    "weights",
    "choose",
    "whatever",
    "weights",
    "want",
    "want",
    "choose",
    "weights",
    "way",
    "pass",
    "rainy",
    "day",
    "humidity",
    "pressure",
    "end",
    "result",
    "greater",
    "equal",
    "would",
    "like",
    "passed",
    "hypothesis",
    "function",
    "rainy",
    "day",
    "inputs",
    "output",
    "get",
    "raining",
    "get",
    "let",
    "try",
    "formalize",
    "little",
    "bit",
    "mathematically",
    "get",
    "sense",
    "often",
    "see",
    "ever",
    "go",
    "supervised",
    "machine",
    "learning",
    "explore",
    "idea",
    "one",
    "thing",
    "generally",
    "categories",
    "sometimes",
    "use",
    "names",
    "categories",
    "like",
    "rain",
    "rain",
    "often",
    "mathematically",
    "trying",
    "comparisons",
    "things",
    "easier",
    "deal",
    "world",
    "numbers",
    "could",
    "say",
    "1",
    "0",
    "1",
    "raining",
    "0",
    "raining",
    "math",
    "result",
    "greater",
    "equal",
    "0",
    "go",
    "ahead",
    "say",
    "hypothesis",
    "function",
    "outputs",
    "1",
    "meaning",
    "raining",
    "otherwise",
    "outputs",
    "0",
    "meaning",
    "raining",
    "oftentimes",
    "type",
    "expression",
    "instead",
    "express",
    "using",
    "vector",
    "mathematics",
    "vector",
    "familiar",
    "term",
    "refers",
    "sequence",
    "numerical",
    "values",
    "could",
    "represent",
    "python",
    "using",
    "list",
    "numerical",
    "values",
    "tuple",
    "numerical",
    "values",
    "couple",
    "sequences",
    "numerical",
    "values",
    "one",
    "vectors",
    "one",
    "sequences",
    "numerical",
    "values",
    "individual",
    "weights",
    "w0",
    "w1",
    "w2",
    "could",
    "construct",
    "call",
    "weight",
    "vector",
    "see",
    "useful",
    "moment",
    "called",
    "w",
    "generally",
    "represented",
    "using",
    "boldface",
    "w",
    "sequence",
    "three",
    "weights",
    "weight",
    "0",
    "weight",
    "1",
    "weight",
    "able",
    "calculate",
    "based",
    "weights",
    "whether",
    "think",
    "day",
    "raining",
    "raining",
    "going",
    "multiply",
    "weights",
    "one",
    "input",
    "variables",
    "w2",
    "weight",
    "going",
    "multiplied",
    "input",
    "variable",
    "x2",
    "w1",
    "going",
    "multiplied",
    "input",
    "variable",
    "x1",
    "w0",
    "well",
    "multiplied",
    "anything",
    "make",
    "sure",
    "vectors",
    "length",
    "see",
    "useful",
    "second",
    "go",
    "ahead",
    "say",
    "w0",
    "multiplied",
    "multiply",
    "something",
    "1",
    "end",
    "getting",
    "exact",
    "number",
    "addition",
    "weight",
    "vector",
    "w",
    "also",
    "input",
    "vector",
    "call",
    "x",
    "three",
    "values",
    "1",
    "multiplying",
    "w0",
    "1",
    "eventually",
    "x1",
    "x2",
    "represented",
    "two",
    "distinct",
    "vectors",
    "vector",
    "weights",
    "need",
    "somehow",
    "learn",
    "goal",
    "machine",
    "learning",
    "algorithm",
    "learn",
    "weight",
    "vector",
    "supposed",
    "could",
    "choose",
    "arbitrary",
    "set",
    "numbers",
    "would",
    "produce",
    "function",
    "tries",
    "predict",
    "rain",
    "rain",
    "probably",
    "would",
    "good",
    "want",
    "come",
    "good",
    "choice",
    "weights",
    "able",
    "accurate",
    "predictions",
    "input",
    "vector",
    "represents",
    "particular",
    "input",
    "function",
    "data",
    "point",
    "would",
    "like",
    "estimate",
    "day",
    "rainy",
    "day",
    "day",
    "rainy",
    "day",
    "going",
    "vary",
    "depending",
    "input",
    "provided",
    "function",
    "trying",
    "estimate",
    "calculation",
    "want",
    "calculate",
    "expression",
    "turns",
    "expression",
    "would",
    "call",
    "dot",
    "product",
    "two",
    "vectors",
    "dot",
    "product",
    "two",
    "vectors",
    "means",
    "taking",
    "terms",
    "vectors",
    "multiplying",
    "together",
    "w0",
    "multiply",
    "1",
    "w1",
    "multiply",
    "x1",
    "w2",
    "multiply",
    "x2",
    "vectors",
    "need",
    "length",
    "add",
    "results",
    "together",
    "dot",
    "product",
    "w",
    "x",
    "weight",
    "vector",
    "input",
    "vector",
    "going",
    "w0",
    "times",
    "1",
    "w0",
    "plus",
    "w1",
    "times",
    "x1",
    "multiplying",
    "two",
    "terms",
    "together",
    "plus",
    "w2",
    "times",
    "x2",
    "multiplying",
    "terms",
    "together",
    "weight",
    "vector",
    "need",
    "figure",
    "need",
    "machine",
    "learning",
    "algorithm",
    "figure",
    "weights",
    "input",
    "vector",
    "representing",
    "data",
    "point",
    "trying",
    "predict",
    "category",
    "predict",
    "label",
    "able",
    "calculation",
    "taking",
    "dot",
    "product",
    "often",
    "see",
    "represented",
    "vector",
    "form",
    "seen",
    "vectors",
    "think",
    "identical",
    "mathematical",
    "expression",
    "multiplication",
    "adding",
    "results",
    "together",
    "seeing",
    "whether",
    "result",
    "greater",
    "equal",
    "0",
    "expression",
    "identical",
    "expression",
    "calculating",
    "see",
    "whether",
    "answer",
    "greater",
    "equal",
    "0",
    "case",
    "reason",
    "often",
    "see",
    "hypothesis",
    "function",
    "written",
    "something",
    "like",
    "simpler",
    "representation",
    "hypothesis",
    "takes",
    "input",
    "input",
    "vector",
    "x",
    "humidity",
    "pressure",
    "day",
    "want",
    "predict",
    "output",
    "like",
    "rain",
    "rain",
    "1",
    "0",
    "choose",
    "represent",
    "things",
    "numerically",
    "way",
    "taking",
    "dot",
    "product",
    "weights",
    "input",
    "greater",
    "equal",
    "0",
    "go",
    "ahead",
    "say",
    "output",
    "otherwise",
    "output",
    "going",
    "hypothesis",
    "say",
    "parameterized",
    "weights",
    "depending",
    "weights",
    "choose",
    "end",
    "getting",
    "different",
    "hypothesis",
    "choose",
    "weights",
    "randomly",
    "probably",
    "going",
    "get",
    "good",
    "hypothesis",
    "function",
    "get",
    "1",
    "probably",
    "accurately",
    "going",
    "reflect",
    "whether",
    "think",
    "day",
    "going",
    "rainy",
    "rainy",
    "choose",
    "weights",
    "right",
    "often",
    "pretty",
    "good",
    "job",
    "trying",
    "estimate",
    "whether",
    "think",
    "output",
    "function",
    "1",
    "question",
    "figure",
    "weights",
    "able",
    "tune",
    "parameters",
    "number",
    "ways",
    "one",
    "common",
    "known",
    "perceptron",
    "learning",
    "rule",
    "see",
    "later",
    "idea",
    "perceptron",
    "learning",
    "rule",
    "going",
    "get",
    "deep",
    "mathematics",
    "mostly",
    "introduce",
    "conceptually",
    "say",
    "given",
    "data",
    "point",
    "would",
    "like",
    "learn",
    "data",
    "point",
    "input",
    "x",
    "output",
    "like",
    "1",
    "rain",
    "0",
    "rain",
    "going",
    "update",
    "weights",
    "look",
    "formula",
    "moment",
    "big",
    "picture",
    "idea",
    "start",
    "random",
    "weights",
    "learn",
    "data",
    "take",
    "data",
    "points",
    "one",
    "time",
    "one",
    "data",
    "points",
    "figure",
    "right",
    "parameters",
    "need",
    "change",
    "inside",
    "weights",
    "order",
    "better",
    "match",
    "input",
    "point",
    "value",
    "access",
    "lot",
    "data",
    "supervised",
    "machine",
    "learning",
    "algorithm",
    "take",
    "data",
    "points",
    "maybe",
    "look",
    "multiple",
    "times",
    "constantly",
    "try",
    "figure",
    "whether",
    "need",
    "shift",
    "weights",
    "order",
    "better",
    "create",
    "weight",
    "vector",
    "able",
    "correctly",
    "accurately",
    "try",
    "estimate",
    "output",
    "whether",
    "think",
    "going",
    "raining",
    "whether",
    "think",
    "going",
    "raining",
    "weight",
    "update",
    "look",
    "like",
    "without",
    "going",
    "much",
    "mathematics",
    "going",
    "update",
    "weights",
    "result",
    "original",
    "weight",
    "plus",
    "additional",
    "expression",
    "understand",
    "expression",
    "well",
    "actual",
    "output",
    "hypothesis",
    "x",
    "input",
    "going",
    "thought",
    "input",
    "replace",
    "saying",
    "actual",
    "value",
    "minus",
    "estimate",
    "based",
    "difference",
    "actual",
    "value",
    "estimate",
    "might",
    "want",
    "change",
    "hypothesis",
    "change",
    "way",
    "estimation",
    "actual",
    "value",
    "estimate",
    "thing",
    "meaning",
    "correctly",
    "able",
    "predict",
    "category",
    "data",
    "point",
    "belonged",
    "well",
    "actual",
    "value",
    "minus",
    "estimate",
    "going",
    "0",
    "means",
    "whole",
    "term",
    "side",
    "goes",
    "0",
    "weight",
    "change",
    "weight",
    "like",
    "weight",
    "1",
    "weight",
    "2",
    "weight",
    "0",
    "weight",
    "stays",
    "weight",
    "none",
    "weights",
    "change",
    "able",
    "correctly",
    "predict",
    "category",
    "input",
    "belonged",
    "hypothesis",
    "correctly",
    "predict",
    "category",
    "input",
    "belonged",
    "well",
    "maybe",
    "need",
    "make",
    "changes",
    "adjust",
    "weights",
    "better",
    "able",
    "predict",
    "kind",
    "data",
    "point",
    "future",
    "way",
    "might",
    "well",
    "actual",
    "value",
    "bigger",
    "estimate",
    "go",
    "ahead",
    "assume",
    "x",
    "positive",
    "values",
    "actual",
    "value",
    "bigger",
    "estimate",
    "well",
    "means",
    "need",
    "increase",
    "weight",
    "order",
    "make",
    "output",
    "bigger",
    "therefore",
    "likely",
    "get",
    "right",
    "actual",
    "value",
    "actual",
    "value",
    "bigger",
    "estimate",
    "actual",
    "value",
    "minus",
    "estimate",
    "positive",
    "number",
    "imagine",
    "adding",
    "positive",
    "number",
    "weight",
    "increase",
    "ever",
    "slightly",
    "likewise",
    "inverse",
    "case",
    "true",
    "actual",
    "value",
    "less",
    "estimate",
    "actual",
    "value",
    "0",
    "estimated",
    "1",
    "meaning",
    "actually",
    "raining",
    "predicted",
    "going",
    "raining",
    "well",
    "want",
    "decrease",
    "value",
    "weight",
    "case",
    "want",
    "try",
    "lower",
    "total",
    "value",
    "computing",
    "dot",
    "product",
    "order",
    "make",
    "less",
    "likely",
    "would",
    "predict",
    "would",
    "actually",
    "raining",
    "need",
    "get",
    "deep",
    "mathematics",
    "general",
    "idea",
    "every",
    "time",
    "encounter",
    "data",
    "point",
    "adjust",
    "weights",
    "accordingly",
    "try",
    "make",
    "weights",
    "better",
    "line",
    "actual",
    "data",
    "access",
    "repeat",
    "process",
    "data",
    "point",
    "data",
    "point",
    "eventually",
    "hopefully",
    "algorithm",
    "converges",
    "set",
    "weights",
    "pretty",
    "good",
    "job",
    "trying",
    "figure",
    "whether",
    "day",
    "going",
    "rainy",
    "raining",
    "final",
    "point",
    "particular",
    "equation",
    "value",
    "alpha",
    "generally",
    "call",
    "learning",
    "rate",
    "parameter",
    "number",
    "choose",
    "quickly",
    "actually",
    "going",
    "updating",
    "weight",
    "values",
    "alpha",
    "bigger",
    "going",
    "update",
    "weight",
    "values",
    "lot",
    "alpha",
    "smaller",
    "update",
    "weight",
    "values",
    "less",
    "choose",
    "value",
    "alpha",
    "depending",
    "problem",
    "different",
    "values",
    "might",
    "suit",
    "situation",
    "better",
    "worse",
    "others",
    "done",
    "training",
    "process",
    "take",
    "data",
    "using",
    "learning",
    "rule",
    "look",
    "pieces",
    "data",
    "use",
    "piece",
    "data",
    "indication",
    "us",
    "weights",
    "stay",
    "increase",
    "weights",
    "decrease",
    "weights",
    "much",
    "end",
    "effectively",
    "threshold",
    "function",
    "look",
    "threshold",
    "function",
    "looks",
    "like",
    "like",
    "output",
    "function",
    "taking",
    "weights",
    "taking",
    "dot",
    "product",
    "input",
    "output",
    "going",
    "0",
    "case",
    "represented",
    "raining",
    "1",
    "case",
    "represented",
    "raining",
    "way",
    "hypothesis",
    "function",
    "works",
    "calculates",
    "value",
    "greater",
    "0",
    "greater",
    "threshold",
    "value",
    "declare",
    "rainy",
    "day",
    "otherwise",
    "declare",
    "rainy",
    "day",
    "graphically",
    "function",
    "looks",
    "like",
    "initially",
    "value",
    "dot",
    "product",
    "small",
    "raining",
    "raining",
    "raining",
    "soon",
    "crosses",
    "threshold",
    "suddenly",
    "say",
    "ok",
    "raining",
    "raining",
    "raining",
    "way",
    "interpret",
    "kind",
    "representation",
    "anything",
    "side",
    "line",
    "would",
    "category",
    "data",
    "points",
    "say",
    "yes",
    "raining",
    "anything",
    "falls",
    "side",
    "line",
    "data",
    "points",
    "would",
    "say",
    "raining",
    "want",
    "choose",
    "value",
    "weights",
    "results",
    "function",
    "pretty",
    "good",
    "job",
    "trying",
    "estimation",
    "one",
    "tricky",
    "thing",
    "type",
    "hard",
    "threshold",
    "leaves",
    "two",
    "possible",
    "outcomes",
    "plug",
    "data",
    "input",
    "output",
    "get",
    "raining",
    "raining",
    "room",
    "anywhere",
    "maybe",
    "want",
    "maybe",
    "want",
    "given",
    "data",
    "point",
    "would",
    "like",
    "able",
    "classify",
    "one",
    "two",
    "various",
    "different",
    "categories",
    "might",
    "also",
    "case",
    "care",
    "knowing",
    "strong",
    "prediction",
    "example",
    "go",
    "back",
    "instance",
    "rainy",
    "days",
    "side",
    "line",
    "rainy",
    "days",
    "side",
    "line",
    "might",
    "imagine",
    "let",
    "look",
    "two",
    "white",
    "data",
    "points",
    "data",
    "point",
    "would",
    "like",
    "predict",
    "label",
    "category",
    "data",
    "point",
    "would",
    "also",
    "like",
    "predict",
    "label",
    "category",
    "seems",
    "likely",
    "could",
    "pretty",
    "confidently",
    "say",
    "data",
    "point",
    "rainy",
    "day",
    "seems",
    "close",
    "rainy",
    "days",
    "going",
    "nearest",
    "neighbor",
    "strategy",
    "side",
    "line",
    "going",
    "strategy",
    "saying",
    "side",
    "line",
    "fall",
    "figuring",
    "weights",
    "using",
    "line",
    "strategy",
    "side",
    "line",
    "fall",
    "side",
    "decision",
    "boundary",
    "well",
    "also",
    "say",
    "point",
    "also",
    "rainy",
    "day",
    "falls",
    "side",
    "line",
    "corresponds",
    "rainy",
    "days",
    "likely",
    "even",
    "case",
    "would",
    "know",
    "feel",
    "nearly",
    "confident",
    "data",
    "point",
    "left",
    "compared",
    "data",
    "point",
    "right",
    "one",
    "right",
    "feel",
    "confident",
    "yes",
    "rainy",
    "day",
    "one",
    "pretty",
    "close",
    "line",
    "judging",
    "distance",
    "might",
    "less",
    "sure",
    "threshold",
    "function",
    "allow",
    "notion",
    "less",
    "sure",
    "sure",
    "something",
    "would",
    "call",
    "hard",
    "threshold",
    "crossed",
    "line",
    "immediately",
    "say",
    "yes",
    "going",
    "rainy",
    "day",
    "anywhere",
    "going",
    "say",
    "rainy",
    "day",
    "may",
    "helpful",
    "number",
    "cases",
    "one",
    "particularly",
    "easy",
    "function",
    "deal",
    "get",
    "deeper",
    "world",
    "machine",
    "learning",
    "trying",
    "things",
    "like",
    "taking",
    "derivatives",
    "curves",
    "type",
    "function",
    "makes",
    "things",
    "challenging",
    "challenge",
    "really",
    "notion",
    "gradation",
    "things",
    "notion",
    "yes",
    "strong",
    "belief",
    "going",
    "raining",
    "opposed",
    "probably",
    "likely",
    "going",
    "raining",
    "maybe",
    "totally",
    "sure",
    "either",
    "taking",
    "advantage",
    "technique",
    "known",
    "logistic",
    "regression",
    "instead",
    "using",
    "hard",
    "threshold",
    "type",
    "function",
    "use",
    "instead",
    "logistic",
    "function",
    "something",
    "might",
    "call",
    "soft",
    "threshold",
    "going",
    "transform",
    "looking",
    "something",
    "little",
    "like",
    "something",
    "nicely",
    "curves",
    "result",
    "possible",
    "output",
    "values",
    "longer",
    "0",
    "1",
    "0",
    "raining",
    "1",
    "raining",
    "actually",
    "get",
    "real",
    "numbered",
    "value",
    "0",
    "way",
    "side",
    "get",
    "value",
    "ok",
    "going",
    "raining",
    "pretty",
    "sure",
    "side",
    "get",
    "value",
    "yes",
    "sure",
    "going",
    "raining",
    "could",
    "get",
    "real",
    "numbered",
    "value",
    "value",
    "like",
    "might",
    "mean",
    "think",
    "going",
    "rain",
    "probable",
    "going",
    "rain",
    "based",
    "data",
    "confident",
    "data",
    "points",
    "might",
    "one",
    "advantages",
    "soft",
    "threshold",
    "allows",
    "us",
    "output",
    "could",
    "real",
    "number",
    "potentially",
    "reflects",
    "sort",
    "probability",
    "likelihood",
    "think",
    "particular",
    "data",
    "point",
    "belongs",
    "particular",
    "category",
    "nice",
    "mathematical",
    "properties",
    "well",
    "two",
    "different",
    "approaches",
    "trying",
    "solve",
    "type",
    "classification",
    "problem",
    "one",
    "nearest",
    "neighbor",
    "type",
    "approach",
    "take",
    "data",
    "point",
    "look",
    "data",
    "points",
    "nearby",
    "try",
    "estimate",
    "category",
    "think",
    "belongs",
    "approach",
    "approach",
    "saying",
    "right",
    "let",
    "try",
    "use",
    "linear",
    "regression",
    "figure",
    "weights",
    "adjust",
    "weights",
    "order",
    "figure",
    "line",
    "decision",
    "boundary",
    "going",
    "best",
    "separate",
    "two",
    "categories",
    "turns",
    "another",
    "popular",
    "approach",
    "popular",
    "approach",
    "data",
    "set",
    "want",
    "start",
    "trying",
    "learning",
    "call",
    "support",
    "vector",
    "machine",
    "going",
    "go",
    "much",
    "mathematics",
    "support",
    "vector",
    "machine",
    "least",
    "explore",
    "graphically",
    "see",
    "looks",
    "like",
    "idea",
    "motivation",
    "behind",
    "support",
    "vector",
    "machine",
    "idea",
    "actually",
    "lot",
    "different",
    "lines",
    "could",
    "draw",
    "lot",
    "different",
    "decision",
    "boundaries",
    "could",
    "draw",
    "separate",
    "two",
    "groups",
    "example",
    "red",
    "data",
    "points",
    "blue",
    "data",
    "points",
    "one",
    "possible",
    "line",
    "could",
    "draw",
    "line",
    "like",
    "line",
    "would",
    "separate",
    "red",
    "points",
    "blue",
    "points",
    "perfectly",
    "red",
    "points",
    "one",
    "side",
    "line",
    "blue",
    "points",
    "side",
    "line",
    "probably",
    "make",
    "little",
    "bit",
    "nervous",
    "come",
    "model",
    "model",
    "comes",
    "line",
    "looks",
    "like",
    "reason",
    "worry",
    "well",
    "going",
    "generalize",
    "data",
    "points",
    "necessarily",
    "data",
    "set",
    "access",
    "example",
    "point",
    "fell",
    "like",
    "right",
    "example",
    "right",
    "side",
    "line",
    "well",
    "based",
    "might",
    "want",
    "guess",
    "fact",
    "red",
    "point",
    "falls",
    "side",
    "line",
    "instead",
    "would",
    "estimate",
    "blue",
    "point",
    "instead",
    "based",
    "line",
    "probably",
    "great",
    "choice",
    "close",
    "various",
    "data",
    "points",
    "might",
    "instead",
    "prefer",
    "like",
    "diagonal",
    "line",
    "goes",
    "diagonally",
    "data",
    "set",
    "like",
    "seen",
    "lot",
    "diagonal",
    "lines",
    "could",
    "draw",
    "well",
    "example",
    "could",
    "draw",
    "diagonal",
    "line",
    "also",
    "successfully",
    "separates",
    "red",
    "points",
    "blue",
    "points",
    "perspective",
    "something",
    "like",
    "trying",
    "figure",
    "setting",
    "weights",
    "allows",
    "us",
    "predict",
    "correct",
    "output",
    "line",
    "predict",
    "correct",
    "output",
    "particular",
    "set",
    "data",
    "every",
    "single",
    "time",
    "red",
    "points",
    "one",
    "side",
    "blue",
    "points",
    "yet",
    "probably",
    "little",
    "nervous",
    "line",
    "close",
    "red",
    "points",
    "even",
    "though",
    "able",
    "correctly",
    "predict",
    "input",
    "data",
    "point",
    "fell",
    "somewhere",
    "general",
    "area",
    "algorithm",
    "model",
    "would",
    "say",
    "yeah",
    "think",
    "blue",
    "point",
    "actuality",
    "might",
    "belong",
    "red",
    "category",
    "instead",
    "looks",
    "like",
    "close",
    "red",
    "points",
    "really",
    "want",
    "able",
    "say",
    "given",
    "data",
    "generalize",
    "best",
    "possible",
    "come",
    "line",
    "like",
    "seems",
    "like",
    "intuitive",
    "line",
    "draw",
    "reason",
    "intuitive",
    "seems",
    "far",
    "apart",
    "possible",
    "red",
    "data",
    "blue",
    "data",
    "generalize",
    "little",
    "bit",
    "assume",
    "maybe",
    "points",
    "different",
    "input",
    "still",
    "slightly",
    "away",
    "still",
    "say",
    "something",
    "side",
    "probably",
    "red",
    "something",
    "side",
    "probably",
    "blue",
    "make",
    "judgments",
    "way",
    "support",
    "vector",
    "machines",
    "designed",
    "designed",
    "try",
    "find",
    "call",
    "maximum",
    "margin",
    "separator",
    "maximum",
    "margin",
    "separator",
    "boundary",
    "maximizes",
    "distance",
    "groups",
    "points",
    "rather",
    "come",
    "boundary",
    "close",
    "one",
    "set",
    "case",
    "would",
    "cared",
    "long",
    "categorizing",
    "input",
    "well",
    "seems",
    "need",
    "support",
    "vector",
    "machine",
    "try",
    "find",
    "maximum",
    "margin",
    "separator",
    "way",
    "trying",
    "maximize",
    "particular",
    "distance",
    "finding",
    "call",
    "support",
    "vectors",
    "vectors",
    "closest",
    "line",
    "trying",
    "maximize",
    "distance",
    "line",
    "particular",
    "points",
    "works",
    "way",
    "two",
    "dimensions",
    "also",
    "works",
    "higher",
    "dimensions",
    "looking",
    "line",
    "separates",
    "two",
    "data",
    "points",
    "instead",
    "looking",
    "generally",
    "call",
    "hyperplane",
    "decision",
    "boundary",
    "effectively",
    "separates",
    "one",
    "set",
    "data",
    "set",
    "data",
    "ability",
    "support",
    "vector",
    "machines",
    "work",
    "higher",
    "dimensions",
    "actually",
    "number",
    "applications",
    "well",
    "one",
    "helpfully",
    "deals",
    "cases",
    "data",
    "may",
    "linearly",
    "separable",
    "talked",
    "linear",
    "separability",
    "idea",
    "take",
    "data",
    "draw",
    "line",
    "linear",
    "combination",
    "inputs",
    "allows",
    "us",
    "perfectly",
    "separate",
    "two",
    "sets",
    "data",
    "sets",
    "linearly",
    "separable",
    "even",
    "two",
    "would",
    "able",
    "find",
    "good",
    "line",
    "would",
    "try",
    "kind",
    "separation",
    "something",
    "like",
    "example",
    "imagine",
    "red",
    "points",
    "blue",
    "points",
    "around",
    "try",
    "find",
    "line",
    "divides",
    "red",
    "points",
    "blue",
    "points",
    "actually",
    "going",
    "difficult",
    "impossible",
    "line",
    "choose",
    "well",
    "draw",
    "line",
    "ignore",
    "blue",
    "points",
    "actually",
    "blue",
    "red",
    "anywhere",
    "else",
    "draw",
    "line",
    "going",
    "lot",
    "error",
    "lot",
    "mistakes",
    "lot",
    "soon",
    "call",
    "loss",
    "line",
    "draw",
    "lot",
    "points",
    "going",
    "categorize",
    "incorrectly",
    "really",
    "want",
    "able",
    "find",
    "better",
    "decision",
    "boundary",
    "may",
    "straight",
    "line",
    "two",
    "dimensional",
    "space",
    "support",
    "vector",
    "machines",
    "begin",
    "operate",
    "higher",
    "dimensions",
    "able",
    "find",
    "decision",
    "boundary",
    "like",
    "circle",
    "case",
    "actually",
    "able",
    "separate",
    "one",
    "sets",
    "data",
    "set",
    "data",
    "lot",
    "better",
    "oftentimes",
    "data",
    "sets",
    "data",
    "linearly",
    "separable",
    "support",
    "vector",
    "machines",
    "working",
    "higher",
    "dimensions",
    "actually",
    "figure",
    "way",
    "solve",
    "kind",
    "problem",
    "effectively",
    "three",
    "different",
    "approaches",
    "trying",
    "solve",
    "sorts",
    "problems",
    "seen",
    "support",
    "vector",
    "machines",
    "seen",
    "trying",
    "use",
    "linear",
    "regression",
    "perceptron",
    "learning",
    "rule",
    "able",
    "figure",
    "categorize",
    "inputs",
    "outputs",
    "seen",
    "nearest",
    "neighbor",
    "approach",
    "one",
    "necessarily",
    "better",
    "going",
    "depend",
    "data",
    "set",
    "information",
    "access",
    "going",
    "depend",
    "function",
    "looks",
    "like",
    "ultimately",
    "trying",
    "predict",
    "lot",
    "research",
    "experimentation",
    "involved",
    "trying",
    "figure",
    "best",
    "perform",
    "kind",
    "estimation",
    "classification",
    "one",
    "tasks",
    "might",
    "encounter",
    "supervised",
    "machine",
    "learning",
    "classification",
    "trying",
    "predict",
    "discrete",
    "category",
    "trying",
    "predict",
    "red",
    "blue",
    "rain",
    "rain",
    "authentic",
    "counterfeit",
    "sometimes",
    "want",
    "predict",
    "real",
    "numbered",
    "value",
    "related",
    "problem",
    "classification",
    "instead",
    "known",
    "regression",
    "regression",
    "supervised",
    "learning",
    "problem",
    "try",
    "learn",
    "function",
    "mapping",
    "inputs",
    "outputs",
    "instead",
    "outputs",
    "discrete",
    "categories",
    "things",
    "like",
    "rain",
    "rain",
    "regression",
    "problem",
    "output",
    "values",
    "generally",
    "continuous",
    "values",
    "real",
    "number",
    "would",
    "like",
    "predict",
    "happens",
    "time",
    "well",
    "might",
    "imagine",
    "company",
    "might",
    "take",
    "approach",
    "trying",
    "figure",
    "instance",
    "effect",
    "advertising",
    "advertising",
    "dollars",
    "spent",
    "translate",
    "sales",
    "company",
    "product",
    "example",
    "might",
    "like",
    "try",
    "predict",
    "function",
    "takes",
    "input",
    "amount",
    "money",
    "spent",
    "advertising",
    "going",
    "use",
    "one",
    "input",
    "could",
    "scale",
    "many",
    "inputs",
    "well",
    "lot",
    "different",
    "kinds",
    "data",
    "access",
    "goal",
    "learn",
    "function",
    "given",
    "amount",
    "spending",
    "advertising",
    "going",
    "get",
    "amount",
    "sales",
    "might",
    "judge",
    "based",
    "access",
    "whole",
    "bunch",
    "data",
    "like",
    "every",
    "past",
    "month",
    "much",
    "spent",
    "advertising",
    "sales",
    "would",
    "like",
    "predict",
    "sort",
    "hypothesis",
    "function",
    "given",
    "amount",
    "spent",
    "advertising",
    "predict",
    "case",
    "real",
    "number",
    "number",
    "estimate",
    "much",
    "sales",
    "expect",
    "company",
    "month",
    "quarter",
    "whatever",
    "unit",
    "time",
    "choosing",
    "measure",
    "things",
    "approach",
    "solving",
    "type",
    "problem",
    "could",
    "try",
    "using",
    "linear",
    "regression",
    "type",
    "approach",
    "take",
    "data",
    "plot",
    "advertising",
    "dollars",
    "spent",
    "sales",
    "might",
    "want",
    "try",
    "draw",
    "line",
    "pretty",
    "good",
    "job",
    "trying",
    "estimate",
    "relationship",
    "advertising",
    "sales",
    "case",
    "unlike",
    "trying",
    "separate",
    "data",
    "points",
    "discrete",
    "categories",
    "instead",
    "case",
    "trying",
    "find",
    "line",
    "approximates",
    "relationship",
    "advertising",
    "sales",
    "want",
    "figure",
    "estimated",
    "sales",
    "particular",
    "advertising",
    "budget",
    "look",
    "line",
    "figure",
    "amount",
    "advertising",
    "would",
    "amount",
    "sales",
    "try",
    "make",
    "estimate",
    "way",
    "try",
    "come",
    "line",
    "figuring",
    "modify",
    "weights",
    "using",
    "various",
    "different",
    "techniques",
    "try",
    "make",
    "line",
    "fits",
    "well",
    "possible",
    "approaches",
    "trying",
    "solve",
    "machine",
    "learning",
    "style",
    "problems",
    "question",
    "becomes",
    "evaluate",
    "approaches",
    "evaluate",
    "various",
    "different",
    "hypotheses",
    "could",
    "come",
    "algorithms",
    "give",
    "us",
    "sort",
    "hypothesis",
    "function",
    "maps",
    "inputs",
    "outputs",
    "want",
    "know",
    "well",
    "function",
    "work",
    "think",
    "evaluating",
    "hypotheses",
    "trying",
    "get",
    "better",
    "hypothesis",
    "kind",
    "like",
    "optimization",
    "problem",
    "optimization",
    "problem",
    "recall",
    "either",
    "trying",
    "maximize",
    "objective",
    "function",
    "trying",
    "find",
    "global",
    "maximum",
    "trying",
    "minimize",
    "cost",
    "function",
    "trying",
    "find",
    "global",
    "minimum",
    "case",
    "evaluating",
    "hypotheses",
    "one",
    "thing",
    "might",
    "say",
    "cost",
    "function",
    "thing",
    "trying",
    "minimize",
    "might",
    "trying",
    "minimize",
    "would",
    "call",
    "loss",
    "function",
    "loss",
    "function",
    "function",
    "going",
    "estimate",
    "us",
    "poorly",
    "function",
    "performs",
    "formally",
    "like",
    "loss",
    "utility",
    "whenever",
    "predict",
    "something",
    "wrong",
    "loss",
    "utility",
    "going",
    "add",
    "output",
    "loss",
    "function",
    "could",
    "come",
    "loss",
    "function",
    "want",
    "mathematical",
    "way",
    "estimating",
    "given",
    "data",
    "points",
    "given",
    "actual",
    "output",
    "given",
    "projected",
    "output",
    "estimate",
    "could",
    "calculate",
    "sort",
    "numerical",
    "loss",
    "couple",
    "popular",
    "loss",
    "functions",
    "worth",
    "discussing",
    "seen",
    "comes",
    "discrete",
    "categories",
    "things",
    "like",
    "rain",
    "rain",
    "counterfeit",
    "counterfeit",
    "one",
    "approaches",
    "0",
    "1",
    "loss",
    "function",
    "way",
    "works",
    "data",
    "points",
    "loss",
    "function",
    "takes",
    "input",
    "actual",
    "output",
    "like",
    "whether",
    "actually",
    "raining",
    "raining",
    "takes",
    "prediction",
    "account",
    "predict",
    "given",
    "data",
    "point",
    "raining",
    "raining",
    "actual",
    "value",
    "equals",
    "prediction",
    "well",
    "0",
    "1",
    "loss",
    "function",
    "say",
    "loss",
    "loss",
    "utility",
    "able",
    "predict",
    "correctly",
    "otherwise",
    "actual",
    "value",
    "thing",
    "predicted",
    "well",
    "case",
    "loss",
    "lost",
    "something",
    "lost",
    "utility",
    "predicted",
    "output",
    "function",
    "actually",
    "goal",
    "situation",
    "like",
    "would",
    "come",
    "hypothesis",
    "minimizes",
    "total",
    "empirical",
    "loss",
    "total",
    "amount",
    "lost",
    "add",
    "data",
    "points",
    "actual",
    "output",
    "hypothesis",
    "would",
    "predicted",
    "case",
    "example",
    "go",
    "back",
    "classifying",
    "days",
    "raining",
    "raining",
    "came",
    "decision",
    "boundary",
    "would",
    "evaluate",
    "decision",
    "boundary",
    "much",
    "better",
    "drawing",
    "line",
    "drawing",
    "line",
    "well",
    "could",
    "take",
    "input",
    "data",
    "points",
    "input",
    "data",
    "point",
    "label",
    "whether",
    "raining",
    "whether",
    "raining",
    "could",
    "compare",
    "prediction",
    "whether",
    "predicted",
    "would",
    "raining",
    "raining",
    "assign",
    "numerical",
    "value",
    "result",
    "example",
    "points",
    "rainy",
    "days",
    "predicted",
    "would",
    "raining",
    "fall",
    "bottom",
    "side",
    "line",
    "loss",
    "0",
    "nothing",
    "lost",
    "situations",
    "likewise",
    "true",
    "points",
    "raining",
    "predicted",
    "would",
    "raining",
    "either",
    "loss",
    "points",
    "like",
    "point",
    "point",
    "predicted",
    "would",
    "raining",
    "actuality",
    "blue",
    "point",
    "raining",
    "likewise",
    "predicted",
    "would",
    "raining",
    "actuality",
    "red",
    "point",
    "raining",
    "result",
    "miscategorized",
    "data",
    "points",
    "trying",
    "train",
    "result",
    "loss",
    "one",
    "loss",
    "total",
    "loss",
    "4",
    "example",
    "case",
    "might",
    "would",
    "estimate",
    "would",
    "say",
    "line",
    "better",
    "line",
    "goes",
    "somewhere",
    "else",
    "line",
    "line",
    "might",
    "minimize",
    "loss",
    "way",
    "better",
    "four",
    "points",
    "loss",
    "drawing",
    "straight",
    "line",
    "space",
    "0",
    "1",
    "loss",
    "function",
    "checks",
    "get",
    "right",
    "get",
    "wrong",
    "got",
    "right",
    "loss",
    "0",
    "nothing",
    "lost",
    "got",
    "wrong",
    "loss",
    "function",
    "data",
    "point",
    "says",
    "add",
    "losses",
    "across",
    "data",
    "points",
    "get",
    "sort",
    "empirical",
    "loss",
    "much",
    "lost",
    "across",
    "original",
    "data",
    "points",
    "algorithm",
    "access",
    "forms",
    "loss",
    "well",
    "work",
    "especially",
    "well",
    "deal",
    "real",
    "valued",
    "cases",
    "cases",
    "like",
    "mapping",
    "advertising",
    "budget",
    "amount",
    "sales",
    "example",
    "case",
    "care",
    "get",
    "number",
    "exactly",
    "right",
    "care",
    "close",
    "actual",
    "value",
    "actual",
    "value",
    "like",
    "sales",
    "predicted",
    "would",
    "sales",
    "maybe",
    "pretty",
    "good",
    "much",
    "better",
    "predicted",
    "sales",
    "example",
    "would",
    "like",
    "loss",
    "function",
    "able",
    "take",
    "account",
    "well",
    "take",
    "account",
    "whether",
    "actual",
    "value",
    "expected",
    "value",
    "exactly",
    "also",
    "take",
    "account",
    "far",
    "apart",
    "one",
    "approach",
    "call",
    "l1",
    "loss",
    "l1",
    "loss",
    "look",
    "whether",
    "actual",
    "predicted",
    "equal",
    "take",
    "absolute",
    "value",
    "actual",
    "value",
    "minus",
    "predicted",
    "value",
    "words",
    "ask",
    "far",
    "apart",
    "actual",
    "predicted",
    "values",
    "sum",
    "across",
    "data",
    "points",
    "able",
    "get",
    "answer",
    "ultimately",
    "might",
    "actually",
    "look",
    "like",
    "data",
    "set",
    "well",
    "go",
    "back",
    "representation",
    "advertising",
    "along",
    "sales",
    "along",
    "line",
    "prediction",
    "estimate",
    "given",
    "amount",
    "advertising",
    "predicted",
    "sales",
    "going",
    "l1",
    "loss",
    "far",
    "apart",
    "vertically",
    "along",
    "sales",
    "axis",
    "prediction",
    "data",
    "points",
    "could",
    "figure",
    "exactly",
    "far",
    "apart",
    "prediction",
    "data",
    "points",
    "figure",
    "result",
    "loss",
    "overall",
    "particular",
    "hypothesis",
    "adding",
    "various",
    "different",
    "individual",
    "losses",
    "data",
    "points",
    "goal",
    "try",
    "minimize",
    "loss",
    "try",
    "come",
    "line",
    "minimizes",
    "utility",
    "loss",
    "judging",
    "far",
    "away",
    "estimate",
    "amount",
    "sales",
    "actual",
    "amount",
    "sales",
    "turns",
    "loss",
    "functions",
    "well",
    "one",
    "quite",
    "popular",
    "l2",
    "loss",
    "l2",
    "loss",
    "instead",
    "using",
    "absolute",
    "value",
    "like",
    "far",
    "away",
    "actual",
    "value",
    "predicted",
    "value",
    "uses",
    "square",
    "actual",
    "minus",
    "predicted",
    "far",
    "apart",
    "actual",
    "predicted",
    "value",
    "squares",
    "value",
    "effectively",
    "penalizing",
    "much",
    "harshly",
    "anything",
    "worse",
    "prediction",
    "imagine",
    "two",
    "data",
    "points",
    "predict",
    "one",
    "value",
    "away",
    "actual",
    "value",
    "opposed",
    "one",
    "data",
    "point",
    "predict",
    "two",
    "away",
    "actual",
    "value",
    "l2",
    "loss",
    "function",
    "harshly",
    "penalize",
    "one",
    "two",
    "away",
    "going",
    "square",
    "however",
    "much",
    "differences",
    "actual",
    "value",
    "predicted",
    "value",
    "depending",
    "situation",
    "might",
    "want",
    "choose",
    "loss",
    "function",
    "depending",
    "care",
    "minimizing",
    "really",
    "care",
    "minimizing",
    "error",
    "outlier",
    "cases",
    "might",
    "want",
    "consider",
    "something",
    "like",
    "got",
    "lot",
    "outliers",
    "necessarily",
    "care",
    "modeling",
    "maybe",
    "l1",
    "loss",
    "function",
    "preferable",
    "need",
    "decide",
    "based",
    "particular",
    "set",
    "data",
    "run",
    "risk",
    "loss",
    "functions",
    "anything",
    "trying",
    "problem",
    "known",
    "overfitting",
    "overfitting",
    "big",
    "problem",
    "encounter",
    "machine",
    "learning",
    "happens",
    "anytime",
    "model",
    "fits",
    "closely",
    "data",
    "set",
    "result",
    "fails",
    "generalize",
    "would",
    "like",
    "model",
    "able",
    "accurately",
    "predict",
    "data",
    "inputs",
    "output",
    "pairs",
    "data",
    "access",
    "reason",
    "wanted",
    "want",
    "model",
    "generalize",
    "well",
    "data",
    "seen",
    "would",
    "like",
    "take",
    "data",
    "past",
    "year",
    "whether",
    "raining",
    "raining",
    "use",
    "data",
    "generalize",
    "towards",
    "future",
    "say",
    "future",
    "going",
    "raining",
    "raining",
    "whole",
    "bunch",
    "data",
    "counterfeit",
    "counterfeit",
    "us",
    "dollar",
    "bills",
    "look",
    "like",
    "past",
    "people",
    "encountered",
    "like",
    "train",
    "computer",
    "able",
    "future",
    "generalize",
    "dollar",
    "bills",
    "might",
    "see",
    "well",
    "problem",
    "overfitting",
    "try",
    "tie",
    "closely",
    "data",
    "set",
    "training",
    "model",
    "end",
    "generalizing",
    "well",
    "look",
    "like",
    "well",
    "might",
    "imagine",
    "rainy",
    "day",
    "rainy",
    "day",
    "example",
    "blue",
    "points",
    "indicate",
    "rainy",
    "days",
    "red",
    "points",
    "indicate",
    "rainy",
    "days",
    "decided",
    "felt",
    "pretty",
    "comfortable",
    "drawing",
    "line",
    "like",
    "decision",
    "boundary",
    "rainy",
    "days",
    "rainy",
    "days",
    "pretty",
    "comfortably",
    "say",
    "points",
    "side",
    "likely",
    "rainy",
    "days",
    "points",
    "side",
    "likely",
    "rainy",
    "days",
    "loss",
    "empirical",
    "loss",
    "zero",
    "particular",
    "case",
    "categorize",
    "everything",
    "perfectly",
    "one",
    "outlier",
    "one",
    "day",
    "raining",
    "yet",
    "model",
    "still",
    "predicts",
    "raining",
    "necessarily",
    "mean",
    "model",
    "bad",
    "means",
    "model",
    "100",
    "accurate",
    "really",
    "wanted",
    "try",
    "find",
    "hypothesis",
    "resulted",
    "minimizing",
    "loss",
    "could",
    "come",
    "different",
    "decision",
    "boundary",
    "would",
    "line",
    "would",
    "look",
    "something",
    "like",
    "decision",
    "boundary",
    "separate",
    "red",
    "points",
    "blue",
    "points",
    "red",
    "points",
    "fall",
    "side",
    "decision",
    "boundary",
    "blue",
    "points",
    "fall",
    "side",
    "decision",
    "boundary",
    "would",
    "probably",
    "argue",
    "good",
    "prediction",
    "even",
    "though",
    "seems",
    "accurate",
    "based",
    "available",
    "training",
    "data",
    "training",
    "machine",
    "learning",
    "model",
    "might",
    "say",
    "probably",
    "going",
    "generalize",
    "well",
    "data",
    "points",
    "like",
    "might",
    "still",
    "want",
    "consider",
    "rainy",
    "days",
    "think",
    "probably",
    "outlier",
    "thing",
    "care",
    "minimizing",
    "loss",
    "data",
    "available",
    "run",
    "risk",
    "overfitting",
    "happen",
    "classification",
    "case",
    "also",
    "happen",
    "regression",
    "case",
    "predicted",
    "thought",
    "pretty",
    "good",
    "line",
    "relating",
    "advertising",
    "sales",
    "trying",
    "predict",
    "sales",
    "going",
    "given",
    "amount",
    "advertising",
    "could",
    "come",
    "line",
    "better",
    "job",
    "predicting",
    "training",
    "data",
    "would",
    "something",
    "looks",
    "like",
    "connecting",
    "various",
    "different",
    "data",
    "points",
    "loss",
    "perfectly",
    "predicted",
    "given",
    "advertising",
    "sales",
    "data",
    "available",
    "going",
    "accurate",
    "probably",
    "going",
    "generalize",
    "well",
    "overfit",
    "model",
    "training",
    "data",
    "available",
    "general",
    "want",
    "avoid",
    "overfitting",
    "like",
    "strategies",
    "make",
    "sure",
    "overfit",
    "model",
    "particular",
    "data",
    "set",
    "number",
    "ways",
    "could",
    "try",
    "one",
    "way",
    "examining",
    "optimizing",
    "optimization",
    "problem",
    "say",
    "cost",
    "want",
    "minimize",
    "cost",
    "far",
    "defined",
    "cost",
    "function",
    "cost",
    "hypothesis",
    "equal",
    "empirical",
    "loss",
    "hypothesis",
    "like",
    "far",
    "away",
    "actual",
    "data",
    "points",
    "outputs",
    "away",
    "predicted",
    "based",
    "particular",
    "hypothesis",
    "trying",
    "minimize",
    "cost",
    "meaning",
    "minimizing",
    "loss",
    "case",
    "result",
    "going",
    "might",
    "overfit",
    "minimize",
    "cost",
    "going",
    "try",
    "find",
    "way",
    "perfectly",
    "match",
    "input",
    "data",
    "might",
    "happen",
    "result",
    "overfitting",
    "particular",
    "input",
    "data",
    "order",
    "address",
    "could",
    "add",
    "something",
    "cost",
    "function",
    "counts",
    "cost",
    "loss",
    "also",
    "measure",
    "complexity",
    "hypothesis",
    "word",
    "complexity",
    "hypothesis",
    "something",
    "would",
    "need",
    "define",
    "complicated",
    "line",
    "look",
    "sort",
    "occam",
    "approach",
    "want",
    "give",
    "preference",
    "simpler",
    "decision",
    "boundary",
    "like",
    "straight",
    "line",
    "example",
    "simpler",
    "curve",
    "opposed",
    "something",
    "far",
    "complex",
    "might",
    "represent",
    "training",
    "data",
    "better",
    "might",
    "generalize",
    "well",
    "generally",
    "say",
    "simpler",
    "solution",
    "probably",
    "better",
    "solution",
    "probably",
    "one",
    "likely",
    "generalize",
    "well",
    "inputs",
    "measure",
    "loss",
    "also",
    "measure",
    "complexity",
    "gets",
    "taken",
    "account",
    "consider",
    "overall",
    "cost",
    "yes",
    "something",
    "might",
    "less",
    "loss",
    "better",
    "predicts",
    "training",
    "data",
    "much",
    "complex",
    "still",
    "might",
    "best",
    "option",
    "need",
    "come",
    "balance",
    "loss",
    "complexity",
    "reason",
    "often",
    "see",
    "represented",
    "multiplying",
    "complexity",
    "parameter",
    "choose",
    "parameter",
    "lambda",
    "case",
    "saying",
    "lambda",
    "greater",
    "value",
    "really",
    "want",
    "penalize",
    "complex",
    "hypotheses",
    "whereas",
    "lambda",
    "smaller",
    "going",
    "penalize",
    "complex",
    "hypotheses",
    "little",
    "bit",
    "machine",
    "learning",
    "programmer",
    "decide",
    "want",
    "set",
    "value",
    "lambda",
    "much",
    "want",
    "penalize",
    "complex",
    "hypothesis",
    "might",
    "fit",
    "data",
    "little",
    "better",
    "one",
    "right",
    "answer",
    "lot",
    "things",
    "depending",
    "data",
    "set",
    "depending",
    "data",
    "available",
    "problem",
    "trying",
    "solve",
    "choice",
    "parameters",
    "may",
    "vary",
    "may",
    "need",
    "experiment",
    "little",
    "bit",
    "figure",
    "right",
    "choice",
    "ultimately",
    "going",
    "process",
    "considering",
    "loss",
    "also",
    "measure",
    "complexity",
    "known",
    "regularization",
    "regularization",
    "process",
    "penalizing",
    "hypothesis",
    "complex",
    "order",
    "favor",
    "simpler",
    "hypothesis",
    "likely",
    "generalize",
    "well",
    "likely",
    "able",
    "apply",
    "situations",
    "dealing",
    "input",
    "points",
    "unlike",
    "ones",
    "necessarily",
    "seen",
    "oftentimes",
    "see",
    "us",
    "add",
    "regularizing",
    "term",
    "trying",
    "minimize",
    "order",
    "avoid",
    "problem",
    "overfitting",
    "another",
    "way",
    "making",
    "sure",
    "overfit",
    "run",
    "experiments",
    "see",
    "whether",
    "able",
    "generalize",
    "model",
    "created",
    "data",
    "sets",
    "well",
    "reason",
    "oftentimes",
    "machine",
    "learning",
    "experiment",
    "got",
    "data",
    "want",
    "try",
    "come",
    "function",
    "predicts",
    "given",
    "input",
    "output",
    "going",
    "necessarily",
    "want",
    "training",
    "data",
    "available",
    "could",
    "employ",
    "method",
    "known",
    "holdout",
    "holdout",
    "split",
    "data",
    "split",
    "data",
    "training",
    "set",
    "testing",
    "set",
    "training",
    "set",
    "set",
    "data",
    "going",
    "use",
    "train",
    "machine",
    "learning",
    "model",
    "testing",
    "set",
    "set",
    "data",
    "going",
    "use",
    "order",
    "test",
    "see",
    "well",
    "machine",
    "learning",
    "model",
    "actually",
    "performed",
    "learning",
    "happens",
    "training",
    "set",
    "figure",
    "parameters",
    "figure",
    "right",
    "model",
    "see",
    "right",
    "trained",
    "model",
    "see",
    "well",
    "predicting",
    "things",
    "inside",
    "testing",
    "set",
    "set",
    "data",
    "seen",
    "hope",
    "going",
    "able",
    "predict",
    "testing",
    "set",
    "pretty",
    "well",
    "able",
    "generalize",
    "based",
    "training",
    "data",
    "available",
    "us",
    "overfit",
    "training",
    "data",
    "though",
    "able",
    "generalize",
    "well",
    "look",
    "testing",
    "set",
    "likely",
    "going",
    "case",
    "going",
    "predict",
    "things",
    "testing",
    "set",
    "nearly",
    "effectively",
    "one",
    "method",
    "validating",
    "make",
    "sure",
    "work",
    "done",
    "actually",
    "going",
    "generalize",
    "data",
    "sets",
    "well",
    "statistical",
    "techniques",
    "use",
    "well",
    "one",
    "downsides",
    "hold",
    "say",
    "split",
    "train",
    "using",
    "50",
    "data",
    "test",
    "using",
    "50",
    "could",
    "choose",
    "percentages",
    "well",
    "fair",
    "amount",
    "data",
    "using",
    "train",
    "might",
    "able",
    "get",
    "better",
    "model",
    "result",
    "example",
    "one",
    "approach",
    "known",
    "rather",
    "divide",
    "things",
    "two",
    "sets",
    "run",
    "one",
    "experiment",
    "divide",
    "things",
    "k",
    "different",
    "sets",
    "maybe",
    "divide",
    "things",
    "10",
    "different",
    "sets",
    "run",
    "10",
    "different",
    "experiments",
    "split",
    "data",
    "10",
    "different",
    "sets",
    "data",
    "time",
    "10",
    "experiments",
    "hold",
    "one",
    "sets",
    "data",
    "say",
    "let",
    "train",
    "model",
    "nine",
    "sets",
    "test",
    "see",
    "well",
    "predicts",
    "set",
    "number",
    "pick",
    "another",
    "set",
    "nine",
    "sets",
    "train",
    "test",
    "one",
    "held",
    "time",
    "train",
    "model",
    "everything",
    "minus",
    "one",
    "set",
    "holding",
    "test",
    "see",
    "well",
    "model",
    "performs",
    "test",
    "hold",
    "end",
    "getting",
    "10",
    "different",
    "results",
    "10",
    "different",
    "answers",
    "accurately",
    "model",
    "worked",
    "oftentimes",
    "could",
    "take",
    "average",
    "10",
    "get",
    "approximation",
    "well",
    "think",
    "model",
    "performs",
    "overall",
    "key",
    "idea",
    "separating",
    "training",
    "data",
    "testing",
    "data",
    "want",
    "test",
    "model",
    "data",
    "different",
    "trained",
    "model",
    "training",
    "want",
    "avoid",
    "overfitting",
    "want",
    "able",
    "generalize",
    "way",
    "test",
    "whether",
    "able",
    "generalize",
    "looking",
    "data",
    "seen",
    "seeing",
    "well",
    "actually",
    "able",
    "perform",
    "want",
    "actually",
    "implement",
    "techniques",
    "inside",
    "programming",
    "language",
    "like",
    "python",
    "number",
    "ways",
    "could",
    "could",
    "write",
    "scratch",
    "libraries",
    "allow",
    "us",
    "take",
    "advantage",
    "existing",
    "implementations",
    "algorithms",
    "use",
    "types",
    "algorithms",
    "lot",
    "different",
    "situations",
    "library",
    "popular",
    "one",
    "known",
    "allows",
    "us",
    "python",
    "able",
    "quickly",
    "get",
    "set",
    "lot",
    "different",
    "machine",
    "learning",
    "models",
    "library",
    "already",
    "written",
    "algorithm",
    "nearest",
    "neighbor",
    "classification",
    "perceptron",
    "learning",
    "bunch",
    "types",
    "inference",
    "supervised",
    "learning",
    "yet",
    "talked",
    "using",
    "begin",
    "try",
    "actually",
    "testing",
    "methods",
    "work",
    "accurately",
    "perform",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "one",
    "approach",
    "trying",
    "solve",
    "type",
    "problem",
    "right",
    "first",
    "going",
    "pull",
    "whole",
    "bunch",
    "data",
    "provided",
    "uc",
    "irvine",
    "information",
    "various",
    "different",
    "banknotes",
    "people",
    "took",
    "pictures",
    "various",
    "different",
    "banknotes",
    "measured",
    "various",
    "different",
    "properties",
    "banknotes",
    "particular",
    "human",
    "categorized",
    "banknotes",
    "either",
    "counterfeit",
    "banknote",
    "counterfeit",
    "looking",
    "row",
    "represents",
    "one",
    "banknote",
    "formatted",
    "csv",
    "spreadsheet",
    "comma",
    "separated",
    "values",
    "separating",
    "various",
    "different",
    "fields",
    "four",
    "different",
    "input",
    "values",
    "data",
    "points",
    "information",
    "measurement",
    "made",
    "banknote",
    "measurements",
    "exactly",
    "important",
    "fact",
    "access",
    "data",
    "importantly",
    "access",
    "data",
    "points",
    "label",
    "0",
    "indicates",
    "something",
    "like",
    "counterfeit",
    "bill",
    "meaning",
    "authentic",
    "bill",
    "data",
    "point",
    "labeled",
    "1",
    "means",
    "counterfeit",
    "bill",
    "least",
    "according",
    "human",
    "researcher",
    "labeled",
    "particular",
    "data",
    "whole",
    "bunch",
    "data",
    "representing",
    "whole",
    "bunch",
    "different",
    "data",
    "points",
    "various",
    "different",
    "measurements",
    "made",
    "particular",
    "bill",
    "output",
    "value",
    "0",
    "1",
    "0",
    "meaning",
    "genuine",
    "bill",
    "1",
    "meaning",
    "counterfeit",
    "bill",
    "would",
    "like",
    "use",
    "supervised",
    "learning",
    "begin",
    "predict",
    "model",
    "sort",
    "function",
    "take",
    "four",
    "values",
    "input",
    "predict",
    "output",
    "would",
    "want",
    "learning",
    "algorithm",
    "find",
    "sort",
    "pattern",
    "able",
    "predict",
    "based",
    "measurements",
    "something",
    "could",
    "measure",
    "taking",
    "photo",
    "bill",
    "predict",
    "whether",
    "bill",
    "authentic",
    "whether",
    "bill",
    "counterfeit",
    "well",
    "first",
    "going",
    "open",
    "see",
    "first",
    "importing",
    "lot",
    "things",
    "importantly",
    "going",
    "set",
    "model",
    "equal",
    "perceptron",
    "model",
    "one",
    "models",
    "talked",
    "going",
    "try",
    "figure",
    "setting",
    "weights",
    "able",
    "divide",
    "data",
    "two",
    "different",
    "groups",
    "going",
    "go",
    "ahead",
    "read",
    "data",
    "file",
    "basically",
    "every",
    "row",
    "going",
    "separate",
    "row",
    "first",
    "four",
    "values",
    "row",
    "evidence",
    "row",
    "label",
    "final",
    "column",
    "row",
    "0",
    "label",
    "authentic",
    "otherwise",
    "going",
    "counterfeit",
    "effectively",
    "reading",
    "data",
    "csv",
    "file",
    "dividing",
    "whole",
    "bunch",
    "rows",
    "row",
    "evidence",
    "four",
    "input",
    "values",
    "going",
    "inputs",
    "hypothesis",
    "function",
    "label",
    "output",
    "whether",
    "authentic",
    "counterfeit",
    "thing",
    "trying",
    "predict",
    "next",
    "step",
    "would",
    "like",
    "split",
    "data",
    "set",
    "training",
    "set",
    "testing",
    "set",
    "set",
    "data",
    "would",
    "like",
    "train",
    "machine",
    "learning",
    "model",
    "set",
    "data",
    "would",
    "like",
    "use",
    "test",
    "model",
    "see",
    "well",
    "performed",
    "go",
    "ahead",
    "figure",
    "length",
    "data",
    "many",
    "data",
    "points",
    "go",
    "ahead",
    "take",
    "half",
    "save",
    "number",
    "number",
    "called",
    "holdout",
    "many",
    "items",
    "going",
    "hold",
    "data",
    "set",
    "save",
    "testing",
    "phase",
    "randomly",
    "shuffle",
    "data",
    "random",
    "order",
    "say",
    "testing",
    "set",
    "data",
    "holdout",
    "take",
    "holdout",
    "many",
    "data",
    "items",
    "testing",
    "set",
    "training",
    "data",
    "everything",
    "else",
    "information",
    "going",
    "train",
    "model",
    "say",
    "need",
    "divide",
    "training",
    "data",
    "two",
    "different",
    "sets",
    "need",
    "divide",
    "x",
    "values",
    "x",
    "represents",
    "inputs",
    "x",
    "values",
    "x",
    "values",
    "going",
    "train",
    "basically",
    "every",
    "row",
    "training",
    "set",
    "going",
    "get",
    "evidence",
    "row",
    "four",
    "values",
    "basically",
    "vector",
    "four",
    "numbers",
    "going",
    "input",
    "need",
    "values",
    "outputs",
    "want",
    "learn",
    "labels",
    "belong",
    "various",
    "different",
    "input",
    "points",
    "well",
    "going",
    "thing",
    "row",
    "training",
    "data",
    "time",
    "take",
    "row",
    "get",
    "label",
    "whether",
    "authentic",
    "counterfeit",
    "end",
    "one",
    "list",
    "vectors",
    "input",
    "data",
    "one",
    "list",
    "follows",
    "order",
    "labels",
    "correspond",
    "vectors",
    "train",
    "model",
    "case",
    "perceptron",
    "model",
    "call",
    "pass",
    "training",
    "data",
    "labels",
    "training",
    "data",
    "take",
    "care",
    "fitting",
    "model",
    "entire",
    "algorithm",
    "done",
    "test",
    "see",
    "well",
    "model",
    "performed",
    "say",
    "let",
    "get",
    "input",
    "vectors",
    "want",
    "test",
    "row",
    "testing",
    "data",
    "set",
    "go",
    "ahead",
    "get",
    "evidence",
    "values",
    "actual",
    "values",
    "rows",
    "testing",
    "data",
    "set",
    "actual",
    "label",
    "going",
    "generate",
    "predictions",
    "going",
    "use",
    "model",
    "try",
    "predict",
    "based",
    "testing",
    "vectors",
    "want",
    "predict",
    "output",
    "goal",
    "compare",
    "testing",
    "predictions",
    "want",
    "see",
    "well",
    "predictions",
    "based",
    "model",
    "actually",
    "reflect",
    "values",
    "output",
    "actually",
    "labeled",
    "label",
    "data",
    "assess",
    "well",
    "algorithm",
    "worked",
    "compute",
    "well",
    "going",
    "zip",
    "function",
    "basically",
    "lets",
    "look",
    "two",
    "different",
    "lists",
    "one",
    "one",
    "time",
    "actual",
    "value",
    "predicted",
    "value",
    "actual",
    "thing",
    "predicted",
    "go",
    "ahead",
    "increment",
    "counter",
    "one",
    "otherwise",
    "increment",
    "incorrect",
    "counter",
    "one",
    "end",
    "print",
    "results",
    "many",
    "got",
    "right",
    "many",
    "got",
    "wrong",
    "overall",
    "accuracy",
    "example",
    "go",
    "ahead",
    "run",
    "run",
    "python",
    "going",
    "train",
    "half",
    "data",
    "set",
    "test",
    "half",
    "data",
    "set",
    "results",
    "perceptron",
    "model",
    "case",
    "correctly",
    "able",
    "classify",
    "679",
    "bills",
    "correctly",
    "either",
    "authentic",
    "counterfeit",
    "incorrectly",
    "classified",
    "seven",
    "overall",
    "accuracy",
    "close",
    "99",
    "accurate",
    "particular",
    "data",
    "set",
    "using",
    "perceptron",
    "model",
    "able",
    "predict",
    "well",
    "output",
    "going",
    "try",
    "different",
    "models",
    "makes",
    "easy",
    "swap",
    "one",
    "model",
    "another",
    "model",
    "instead",
    "perceptron",
    "model",
    "use",
    "support",
    "vector",
    "machine",
    "using",
    "svc",
    "otherwise",
    "known",
    "support",
    "vector",
    "classifier",
    "using",
    "support",
    "vector",
    "machine",
    "classify",
    "things",
    "two",
    "different",
    "groups",
    "see",
    "right",
    "well",
    "perform",
    "right",
    "time",
    "able",
    "correctly",
    "predict",
    "682",
    "incorrectly",
    "predicted",
    "four",
    "accuracy",
    "could",
    "even",
    "try",
    "classifier",
    "model",
    "instead",
    "takes",
    "parameter",
    "n",
    "neighbors",
    "many",
    "neighbors",
    "want",
    "look",
    "let",
    "look",
    "one",
    "neighbor",
    "one",
    "nearest",
    "neighbor",
    "use",
    "predict",
    "go",
    "ahead",
    "run",
    "well",
    "looks",
    "like",
    "based",
    "classifier",
    "looking",
    "one",
    "neighbor",
    "able",
    "correctly",
    "classify",
    "685",
    "data",
    "points",
    "incorrectly",
    "classified",
    "one",
    "maybe",
    "let",
    "try",
    "three",
    "neighbors",
    "instead",
    "instead",
    "using",
    "one",
    "neighbor",
    "neighbors",
    "approach",
    "look",
    "three",
    "nearest",
    "neighbors",
    "see",
    "performs",
    "one",
    "case",
    "seems",
    "gotten",
    "100",
    "predictions",
    "correctly",
    "described",
    "either",
    "authentic",
    "banknotes",
    "counterfeit",
    "banknotes",
    "could",
    "run",
    "experiments",
    "multiple",
    "times",
    "randomly",
    "reorganizing",
    "data",
    "every",
    "time",
    "technically",
    "training",
    "slightly",
    "different",
    "data",
    "sets",
    "might",
    "want",
    "run",
    "multiple",
    "experiments",
    "really",
    "see",
    "well",
    "actually",
    "going",
    "perform",
    "short",
    "perform",
    "well",
    "perform",
    "slightly",
    "better",
    "others",
    "might",
    "always",
    "case",
    "every",
    "data",
    "set",
    "begin",
    "test",
    "quickly",
    "putting",
    "together",
    "machine",
    "learning",
    "models",
    "using",
    "able",
    "train",
    "training",
    "set",
    "test",
    "testing",
    "set",
    "well",
    "splitting",
    "training",
    "groups",
    "testing",
    "groups",
    "testing",
    "happens",
    "often",
    "functions",
    "built",
    "trying",
    "hand",
    "take",
    "look",
    "banknotes",
    "one",
    "take",
    "advantage",
    "features",
    "exist",
    "really",
    "simplify",
    "lot",
    "logic",
    "function",
    "built",
    "called",
    "train",
    "test",
    "split",
    "automatically",
    "split",
    "data",
    "training",
    "group",
    "testing",
    "group",
    "say",
    "proportion",
    "testing",
    "group",
    "something",
    "like",
    "half",
    "data",
    "inside",
    "testing",
    "group",
    "fit",
    "model",
    "training",
    "data",
    "make",
    "predictions",
    "testing",
    "data",
    "count",
    "nice",
    "methods",
    "counting",
    "many",
    "times",
    "testing",
    "data",
    "match",
    "predictions",
    "many",
    "times",
    "testing",
    "data",
    "match",
    "predictions",
    "quickly",
    "write",
    "programs",
    "many",
    "lines",
    "code",
    "maybe",
    "like",
    "40",
    "lines",
    "code",
    "get",
    "predictions",
    "result",
    "see",
    "well",
    "able",
    "types",
    "libraries",
    "allow",
    "us",
    "without",
    "really",
    "knowing",
    "implementation",
    "details",
    "algorithms",
    "able",
    "use",
    "algorithms",
    "practical",
    "way",
    "able",
    "solve",
    "types",
    "problems",
    "supervised",
    "learning",
    "task",
    "given",
    "whole",
    "set",
    "data",
    "input",
    "output",
    "pairs",
    "would",
    "like",
    "learn",
    "function",
    "maps",
    "inputs",
    "outputs",
    "turns",
    "forms",
    "learning",
    "well",
    "another",
    "popular",
    "type",
    "machine",
    "learning",
    "especially",
    "nowadays",
    "known",
    "reinforcement",
    "learning",
    "idea",
    "reinforcement",
    "learning",
    "rather",
    "given",
    "whole",
    "data",
    "set",
    "beginning",
    "input",
    "output",
    "pairs",
    "reinforcement",
    "learning",
    "learning",
    "experience",
    "reinforcement",
    "learning",
    "agent",
    "whether",
    "like",
    "physical",
    "robot",
    "trying",
    "make",
    "actions",
    "world",
    "virtual",
    "agent",
    "program",
    "running",
    "somewhere",
    "agent",
    "going",
    "given",
    "set",
    "rewards",
    "punishments",
    "form",
    "numerical",
    "values",
    "think",
    "reward",
    "punishment",
    "based",
    "learns",
    "actions",
    "take",
    "future",
    "agent",
    "ai",
    "put",
    "sort",
    "environment",
    "make",
    "actions",
    "based",
    "actions",
    "makes",
    "learns",
    "something",
    "either",
    "gets",
    "reward",
    "something",
    "well",
    "gets",
    "punishment",
    "something",
    "poorly",
    "learns",
    "future",
    "based",
    "individual",
    "experiences",
    "often",
    "look",
    "like",
    "often",
    "start",
    "agent",
    "ai",
    "might",
    "physical",
    "robot",
    "imagining",
    "physical",
    "robot",
    "moving",
    "around",
    "also",
    "program",
    "agent",
    "situated",
    "environment",
    "environment",
    "going",
    "make",
    "actions",
    "going",
    "give",
    "rewards",
    "punishments",
    "various",
    "actions",
    "example",
    "environment",
    "going",
    "start",
    "putting",
    "agent",
    "inside",
    "state",
    "agent",
    "state",
    "game",
    "might",
    "state",
    "game",
    "agent",
    "playing",
    "world",
    "agent",
    "exploring",
    "might",
    "position",
    "inside",
    "grid",
    "representing",
    "world",
    "exploring",
    "agent",
    "sort",
    "state",
    "state",
    "agent",
    "needs",
    "choose",
    "take",
    "action",
    "agent",
    "likely",
    "multiple",
    "actions",
    "choose",
    "pick",
    "action",
    "take",
    "action",
    "particular",
    "state",
    "result",
    "agent",
    "generally",
    "get",
    "two",
    "things",
    "response",
    "model",
    "agent",
    "gets",
    "new",
    "state",
    "find",
    "state",
    "taking",
    "one",
    "action",
    "end",
    "state",
    "also",
    "given",
    "sort",
    "numerical",
    "reward",
    "positive",
    "meaning",
    "reward",
    "meaning",
    "good",
    "thing",
    "negative",
    "generally",
    "meaning",
    "something",
    "bad",
    "received",
    "sort",
    "punishment",
    "information",
    "agent",
    "told",
    "state",
    "makes",
    "sort",
    "action",
    "based",
    "ends",
    "another",
    "state",
    "ends",
    "getting",
    "particular",
    "reward",
    "needs",
    "learn",
    "based",
    "information",
    "actions",
    "begin",
    "take",
    "future",
    "could",
    "imagine",
    "generalizing",
    "lot",
    "different",
    "situations",
    "oftentimes",
    "train",
    "ever",
    "seen",
    "robots",
    "able",
    "walk",
    "around",
    "way",
    "humans",
    "would",
    "quite",
    "difficult",
    "program",
    "robot",
    "exactly",
    "right",
    "way",
    "get",
    "walk",
    "way",
    "humans",
    "could",
    "instead",
    "train",
    "reinforcement",
    "learning",
    "give",
    "sort",
    "numerical",
    "reward",
    "every",
    "time",
    "something",
    "good",
    "like",
    "take",
    "steps",
    "forward",
    "punish",
    "every",
    "time",
    "something",
    "bad",
    "like",
    "fall",
    "let",
    "ai",
    "learn",
    "based",
    "sequence",
    "rewards",
    "based",
    "trying",
    "take",
    "various",
    "different",
    "actions",
    "begin",
    "agent",
    "learn",
    "future",
    "order",
    "begin",
    "formalize",
    "first",
    "thing",
    "need",
    "formalize",
    "notion",
    "mean",
    "states",
    "actions",
    "rewards",
    "like",
    "world",
    "look",
    "like",
    "oftentimes",
    "formulate",
    "world",
    "known",
    "markov",
    "decision",
    "process",
    "similar",
    "spirit",
    "markov",
    "chains",
    "might",
    "recall",
    "markov",
    "decision",
    "process",
    "model",
    "use",
    "decision",
    "making",
    "agent",
    "trying",
    "make",
    "decisions",
    "environment",
    "model",
    "allows",
    "us",
    "represent",
    "various",
    "different",
    "states",
    "agent",
    "various",
    "different",
    "actions",
    "take",
    "also",
    "reward",
    "taking",
    "one",
    "action",
    "opposed",
    "another",
    "action",
    "actually",
    "look",
    "like",
    "well",
    "recall",
    "markov",
    "chain",
    "markov",
    "chain",
    "looked",
    "little",
    "something",
    "like",
    "whole",
    "bunch",
    "individual",
    "states",
    "state",
    "immediately",
    "transitioned",
    "another",
    "state",
    "based",
    "probability",
    "distribution",
    "saw",
    "context",
    "weather",
    "sunny",
    "said",
    "probability",
    "sunny",
    "next",
    "day",
    "probability",
    "rainy",
    "example",
    "could",
    "also",
    "imagine",
    "generalizing",
    "sun",
    "rain",
    "anymore",
    "states",
    "one",
    "state",
    "leads",
    "another",
    "state",
    "according",
    "probability",
    "distribution",
    "original",
    "model",
    "agent",
    "control",
    "process",
    "entirely",
    "probability",
    "based",
    "probability",
    "moved",
    "next",
    "state",
    "maybe",
    "going",
    "state",
    "probability",
    "ability",
    "agent",
    "state",
    "choose",
    "set",
    "actions",
    "maybe",
    "instead",
    "one",
    "path",
    "forward",
    "three",
    "different",
    "choices",
    "actions",
    "lead",
    "different",
    "paths",
    "even",
    "bit",
    "oversimplification",
    "states",
    "might",
    "imagine",
    "branching",
    "points",
    "decisions",
    "taken",
    "well",
    "extended",
    "markov",
    "chain",
    "say",
    "state",
    "available",
    "action",
    "choices",
    "actions",
    "might",
    "associated",
    "probability",
    "distribution",
    "going",
    "various",
    "different",
    "states",
    "addition",
    "add",
    "another",
    "extension",
    "time",
    "move",
    "state",
    "taking",
    "action",
    "going",
    "state",
    "associate",
    "reward",
    "outcome",
    "saying",
    "either",
    "r",
    "positive",
    "meaning",
    "positive",
    "reward",
    "r",
    "negative",
    "meaning",
    "sort",
    "punishment",
    "consider",
    "markov",
    "decision",
    "process",
    "markov",
    "decision",
    "process",
    "initial",
    "set",
    "states",
    "states",
    "world",
    "set",
    "actions",
    "given",
    "state",
    "say",
    "actions",
    "available",
    "state",
    "action",
    "choose",
    "transition",
    "model",
    "transition",
    "model",
    "said",
    "given",
    "current",
    "state",
    "probability",
    "end",
    "next",
    "state",
    "state",
    "transition",
    "model",
    "effectively",
    "two",
    "things",
    "conditioning",
    "saying",
    "given",
    "state",
    "take",
    "action",
    "probability",
    "end",
    "next",
    "state",
    "maybe",
    "live",
    "deterministic",
    "world",
    "markov",
    "decision",
    "process",
    "given",
    "state",
    "given",
    "action",
    "know",
    "sure",
    "next",
    "state",
    "end",
    "maybe",
    "randomness",
    "world",
    "take",
    "state",
    "take",
    "action",
    "might",
    "always",
    "end",
    "exact",
    "state",
    "might",
    "probabilities",
    "involved",
    "well",
    "markov",
    "decision",
    "process",
    "handle",
    "possible",
    "cases",
    "finally",
    "reward",
    "function",
    "generally",
    "called",
    "r",
    "case",
    "says",
    "reward",
    "state",
    "taking",
    "action",
    "getting",
    "prime",
    "next",
    "state",
    "original",
    "state",
    "take",
    "action",
    "get",
    "next",
    "state",
    "reward",
    "process",
    "add",
    "rewards",
    "every",
    "time",
    "take",
    "action",
    "get",
    "total",
    "amount",
    "rewards",
    "agent",
    "might",
    "get",
    "interacting",
    "particular",
    "environment",
    "modeled",
    "using",
    "markov",
    "decision",
    "process",
    "might",
    "actually",
    "look",
    "like",
    "practice",
    "well",
    "let",
    "create",
    "little",
    "simulated",
    "world",
    "agent",
    "trying",
    "navigate",
    "way",
    "agent",
    "yellow",
    "dot",
    "like",
    "robot",
    "world",
    "trying",
    "navigate",
    "way",
    "grid",
    "ultimately",
    "trying",
    "find",
    "way",
    "goal",
    "gets",
    "green",
    "goal",
    "going",
    "get",
    "sort",
    "reward",
    "might",
    "also",
    "red",
    "squares",
    "places",
    "get",
    "sort",
    "punishment",
    "bad",
    "place",
    "want",
    "agent",
    "go",
    "ends",
    "red",
    "square",
    "agent",
    "going",
    "get",
    "sort",
    "punishment",
    "result",
    "agent",
    "originally",
    "know",
    "details",
    "know",
    "states",
    "associated",
    "punishments",
    "maybe",
    "know",
    "state",
    "associated",
    "reward",
    "maybe",
    "needs",
    "sort",
    "interact",
    "environment",
    "try",
    "figure",
    "first",
    "thing",
    "agent",
    "might",
    "given",
    "additional",
    "information",
    "know",
    "punishments",
    "know",
    "rewards",
    "might",
    "try",
    "take",
    "action",
    "takes",
    "action",
    "ends",
    "realizing",
    "got",
    "sort",
    "punishment",
    "learn",
    "experience",
    "well",
    "might",
    "learn",
    "state",
    "future",
    "take",
    "action",
    "move",
    "right",
    "bad",
    "action",
    "take",
    "future",
    "ever",
    "find",
    "back",
    "state",
    "take",
    "action",
    "going",
    "right",
    "particular",
    "state",
    "leads",
    "punishment",
    "might",
    "intuition",
    "least",
    "could",
    "try",
    "actions",
    "move",
    "right",
    "lead",
    "immediate",
    "rewards",
    "maybe",
    "try",
    "something",
    "else",
    "maybe",
    "try",
    "something",
    "else",
    "right",
    "found",
    "got",
    "another",
    "punishment",
    "learn",
    "something",
    "experience",
    "next",
    "time",
    "whole",
    "process",
    "know",
    "ever",
    "end",
    "square",
    "take",
    "action",
    "state",
    "taking",
    "action",
    "ultimately",
    "leads",
    "sort",
    "punishment",
    "negative",
    "reward",
    "words",
    "process",
    "repeats",
    "might",
    "imagine",
    "letting",
    "agent",
    "explore",
    "world",
    "learning",
    "time",
    "states",
    "tend",
    "correspond",
    "poor",
    "actions",
    "learning",
    "time",
    "states",
    "correspond",
    "poor",
    "actions",
    "eventually",
    "tries",
    "enough",
    "things",
    "randomly",
    "might",
    "find",
    "eventually",
    "get",
    "state",
    "take",
    "action",
    "state",
    "might",
    "find",
    "actually",
    "get",
    "reward",
    "learn",
    "state",
    "take",
    "action",
    "leads",
    "reward",
    "time",
    "also",
    "learn",
    "state",
    "take",
    "left",
    "action",
    "leads",
    "state",
    "also",
    "lets",
    "eventually",
    "get",
    "reward",
    "begin",
    "learn",
    "time",
    "actions",
    "good",
    "particular",
    "states",
    "also",
    "actions",
    "bad",
    "know",
    "sequence",
    "good",
    "actions",
    "leads",
    "sort",
    "reward",
    "agent",
    "follow",
    "instructions",
    "follow",
    "experience",
    "learned",
    "tell",
    "agent",
    "goal",
    "tell",
    "agent",
    "punishments",
    "agent",
    "begin",
    "learn",
    "experience",
    "learn",
    "begin",
    "perform",
    "sorts",
    "tasks",
    "better",
    "future",
    "let",
    "try",
    "formalize",
    "idea",
    "formalize",
    "idea",
    "would",
    "like",
    "able",
    "learn",
    "state",
    "taking",
    "action",
    "good",
    "thing",
    "bad",
    "thing",
    "lots",
    "different",
    "models",
    "reinforcement",
    "learning",
    "going",
    "look",
    "one",
    "today",
    "one",
    "going",
    "look",
    "method",
    "known",
    "learning",
    "function",
    "function",
    "q",
    "takes",
    "inputs",
    "state",
    "action",
    "take",
    "state",
    "q",
    "function",
    "going",
    "going",
    "estimate",
    "value",
    "much",
    "reward",
    "get",
    "taking",
    "action",
    "state",
    "originally",
    "know",
    "q",
    "function",
    "time",
    "based",
    "experience",
    "based",
    "trying",
    "things",
    "seeing",
    "result",
    "would",
    "like",
    "try",
    "learn",
    "q",
    "sa",
    "particular",
    "state",
    "particular",
    "action",
    "might",
    "take",
    "state",
    "approach",
    "well",
    "approach",
    "originally",
    "start",
    "q",
    "sa",
    "equal",
    "0",
    "states",
    "actions",
    "initially",
    "ever",
    "started",
    "anything",
    "experiences",
    "know",
    "value",
    "taking",
    "action",
    "given",
    "state",
    "going",
    "assume",
    "value",
    "0",
    "across",
    "board",
    "interact",
    "world",
    "experience",
    "rewards",
    "punishments",
    "maybe",
    "go",
    "cell",
    "get",
    "either",
    "reward",
    "punishment",
    "want",
    "somehow",
    "update",
    "estimate",
    "q",
    "sa",
    "want",
    "continually",
    "update",
    "estimate",
    "q",
    "sa",
    "based",
    "experiences",
    "rewards",
    "punishments",
    "received",
    "future",
    "knowledge",
    "actions",
    "good",
    "states",
    "better",
    "take",
    "action",
    "receive",
    "sort",
    "reward",
    "want",
    "estimate",
    "new",
    "value",
    "q",
    "sa",
    "estimate",
    "based",
    "couple",
    "different",
    "things",
    "estimate",
    "based",
    "reward",
    "getting",
    "taking",
    "action",
    "getting",
    "next",
    "state",
    "assuming",
    "situation",
    "assuming",
    "still",
    "future",
    "actions",
    "might",
    "take",
    "well",
    "also",
    "need",
    "take",
    "account",
    "expected",
    "future",
    "rewards",
    "imagine",
    "agent",
    "interacting",
    "environment",
    "sometimes",
    "take",
    "action",
    "get",
    "reward",
    "keep",
    "taking",
    "actions",
    "get",
    "rewards",
    "relevant",
    "current",
    "reward",
    "getting",
    "current",
    "step",
    "also",
    "future",
    "reward",
    "might",
    "case",
    "want",
    "take",
    "step",
    "immediately",
    "lead",
    "reward",
    "later",
    "line",
    "know",
    "lead",
    "rewards",
    "well",
    "balancing",
    "act",
    "current",
    "rewards",
    "agent",
    "experiences",
    "future",
    "rewards",
    "agent",
    "experiences",
    "well",
    "need",
    "update",
    "qsa",
    "estimate",
    "value",
    "qsa",
    "based",
    "current",
    "reward",
    "expected",
    "future",
    "rewards",
    "need",
    "update",
    "q",
    "function",
    "take",
    "account",
    "new",
    "estimate",
    "already",
    "go",
    "process",
    "already",
    "estimate",
    "think",
    "value",
    "new",
    "estimate",
    "somehow",
    "need",
    "combine",
    "two",
    "estimates",
    "together",
    "look",
    "formal",
    "ways",
    "actually",
    "begin",
    "actually",
    "show",
    "formula",
    "looks",
    "like",
    "approach",
    "take",
    "q",
    "learning",
    "going",
    "start",
    "q",
    "equal",
    "0",
    "states",
    "every",
    "time",
    "take",
    "action",
    "state",
    "observer",
    "reward",
    "r",
    "going",
    "update",
    "value",
    "estimate",
    "q",
    "sa",
    "idea",
    "going",
    "figure",
    "new",
    "value",
    "estimate",
    "minus",
    "existing",
    "value",
    "estimate",
    "preconceived",
    "notion",
    "value",
    "taking",
    "action",
    "state",
    "maybe",
    "expectation",
    "currently",
    "think",
    "value",
    "going",
    "estimate",
    "think",
    "going",
    "maybe",
    "new",
    "value",
    "estimate",
    "something",
    "like",
    "delta",
    "10",
    "new",
    "value",
    "estimate",
    "10",
    "points",
    "higher",
    "current",
    "value",
    "estimate",
    "happens",
    "couple",
    "options",
    "need",
    "decide",
    "much",
    "want",
    "adjust",
    "current",
    "expectation",
    "value",
    "taking",
    "action",
    "particular",
    "state",
    "difference",
    "much",
    "add",
    "subtract",
    "existing",
    "notion",
    "much",
    "expect",
    "value",
    "dependent",
    "parameter",
    "alpha",
    "also",
    "called",
    "learning",
    "rate",
    "alpha",
    "represents",
    "effect",
    "much",
    "value",
    "new",
    "information",
    "compared",
    "much",
    "value",
    "old",
    "information",
    "alpha",
    "value",
    "1",
    "means",
    "really",
    "value",
    "new",
    "information",
    "new",
    "estimate",
    "matter",
    "old",
    "estimate",
    "going",
    "consider",
    "new",
    "estimate",
    "always",
    "want",
    "take",
    "consideration",
    "new",
    "information",
    "way",
    "works",
    "imagine",
    "alpha",
    "1",
    "well",
    "taking",
    "old",
    "value",
    "qsa",
    "adding",
    "1",
    "times",
    "new",
    "value",
    "minus",
    "old",
    "value",
    "leaves",
    "us",
    "new",
    "value",
    "alpha",
    "1",
    "take",
    "consideration",
    "new",
    "estimate",
    "happens",
    "time",
    "go",
    "lot",
    "experiences",
    "already",
    "existing",
    "information",
    "might",
    "tried",
    "taking",
    "action",
    "nine",
    "times",
    "already",
    "tried",
    "10th",
    "time",
    "want",
    "consider",
    "10th",
    "experience",
    "also",
    "want",
    "consider",
    "fact",
    "prior",
    "nine",
    "experiences",
    "meaningful",
    "data",
    "necessarily",
    "want",
    "lose",
    "alpha",
    "controls",
    "decision",
    "controls",
    "important",
    "new",
    "information",
    "0",
    "would",
    "mean",
    "ignore",
    "new",
    "information",
    "keep",
    "q",
    "value",
    "1",
    "means",
    "replace",
    "old",
    "information",
    "entirely",
    "new",
    "information",
    "somewhere",
    "keep",
    "sort",
    "balance",
    "two",
    "values",
    "put",
    "equation",
    "little",
    "bit",
    "formally",
    "well",
    "old",
    "value",
    "estimate",
    "old",
    "estimate",
    "value",
    "taking",
    "action",
    "particular",
    "state",
    "q",
    "sna",
    "going",
    "add",
    "something",
    "going",
    "add",
    "alpha",
    "times",
    "new",
    "value",
    "estimate",
    "minus",
    "old",
    "value",
    "estimate",
    "old",
    "value",
    "estimate",
    "look",
    "calling",
    "q",
    "function",
    "new",
    "value",
    "estimate",
    "based",
    "experience",
    "taken",
    "new",
    "estimate",
    "value",
    "taking",
    "action",
    "particular",
    "state",
    "well",
    "going",
    "composed",
    "two",
    "parts",
    "going",
    "composed",
    "reward",
    "get",
    "taking",
    "action",
    "state",
    "going",
    "expect",
    "future",
    "rewards",
    "point",
    "forward",
    "going",
    "r",
    "reward",
    "getting",
    "right",
    "plus",
    "whatever",
    "estimate",
    "going",
    "get",
    "future",
    "estimate",
    "going",
    "get",
    "future",
    "well",
    "bit",
    "another",
    "call",
    "q",
    "function",
    "going",
    "take",
    "maximum",
    "across",
    "possible",
    "actions",
    "could",
    "take",
    "next",
    "say",
    "right",
    "possible",
    "actions",
    "could",
    "take",
    "one",
    "going",
    "highest",
    "reward",
    "looks",
    "little",
    "bit",
    "complicated",
    "going",
    "notion",
    "going",
    "perform",
    "kind",
    "update",
    "estimate",
    "old",
    "estimate",
    "value",
    "taking",
    "action",
    "state",
    "going",
    "update",
    "based",
    "new",
    "information",
    "experience",
    "reward",
    "predict",
    "future",
    "reward",
    "going",
    "using",
    "update",
    "estimate",
    "reward",
    "taking",
    "action",
    "particular",
    "state",
    "additions",
    "might",
    "make",
    "algorithm",
    "well",
    "sometimes",
    "might",
    "case",
    "future",
    "rewards",
    "want",
    "wait",
    "equally",
    "current",
    "rewards",
    "maybe",
    "want",
    "agent",
    "values",
    "reward",
    "reward",
    "later",
    "sometimes",
    "even",
    "add",
    "another",
    "term",
    "parameter",
    "discount",
    "future",
    "rewards",
    "say",
    "future",
    "rewards",
    "valuable",
    "rewards",
    "immediately",
    "getting",
    "reward",
    "current",
    "time",
    "step",
    "better",
    "waiting",
    "year",
    "getting",
    "rewards",
    "later",
    "something",
    "programmer",
    "decide",
    "parameter",
    "ought",
    "big",
    "picture",
    "idea",
    "entire",
    "formula",
    "say",
    "every",
    "time",
    "experience",
    "new",
    "reward",
    "take",
    "account",
    "update",
    "estimate",
    "good",
    "action",
    "future",
    "make",
    "decisions",
    "based",
    "algorithm",
    "good",
    "estimate",
    "every",
    "state",
    "every",
    "action",
    "value",
    "taking",
    "action",
    "something",
    "like",
    "implement",
    "greedy",
    "decision",
    "making",
    "policy",
    "state",
    "want",
    "know",
    "action",
    "take",
    "state",
    "well",
    "consider",
    "possible",
    "actions",
    "value",
    "qsa",
    "estimated",
    "value",
    "taking",
    "action",
    "state",
    "pick",
    "action",
    "highest",
    "value",
    "evaluate",
    "expression",
    "pick",
    "action",
    "highest",
    "value",
    "based",
    "tells",
    "action",
    "take",
    "given",
    "state",
    "greedily",
    "say",
    "across",
    "actions",
    "action",
    "gives",
    "highest",
    "expected",
    "value",
    "go",
    "ahead",
    "choose",
    "action",
    "action",
    "take",
    "well",
    "downside",
    "kind",
    "approach",
    "downside",
    "comes",
    "situation",
    "like",
    "know",
    "solution",
    "gets",
    "reward",
    "agent",
    "able",
    "figure",
    "might",
    "necessarily",
    "best",
    "way",
    "fastest",
    "way",
    "agent",
    "allowed",
    "explore",
    "little",
    "bit",
    "might",
    "find",
    "get",
    "reward",
    "faster",
    "taking",
    "route",
    "instead",
    "going",
    "particular",
    "path",
    "faster",
    "way",
    "get",
    "ultimate",
    "goal",
    "maybe",
    "would",
    "like",
    "agent",
    "able",
    "figure",
    "well",
    "agent",
    "always",
    "takes",
    "actions",
    "knows",
    "best",
    "well",
    "gets",
    "particular",
    "square",
    "know",
    "good",
    "action",
    "never",
    "really",
    "tried",
    "knows",
    "going",
    "eventually",
    "leads",
    "way",
    "reward",
    "might",
    "learn",
    "future",
    "always",
    "take",
    "route",
    "never",
    "going",
    "explore",
    "go",
    "along",
    "route",
    "instead",
    "reinforcement",
    "learning",
    "tension",
    "exploration",
    "exploitation",
    "exploitation",
    "generally",
    "refers",
    "using",
    "knowledge",
    "ai",
    "already",
    "ai",
    "already",
    "knows",
    "move",
    "leads",
    "reward",
    "go",
    "ahead",
    "use",
    "move",
    "exploration",
    "exploring",
    "actions",
    "may",
    "explored",
    "thoroughly",
    "maybe",
    "one",
    "actions",
    "even",
    "know",
    "anything",
    "might",
    "lead",
    "better",
    "rewards",
    "faster",
    "rewards",
    "future",
    "agent",
    "ever",
    "exploits",
    "information",
    "never",
    "explores",
    "might",
    "able",
    "get",
    "reward",
    "might",
    "maximize",
    "rewards",
    "know",
    "possibilities",
    "possibilities",
    "know",
    "taking",
    "advantage",
    "exploration",
    "try",
    "address",
    "well",
    "one",
    "possible",
    "solution",
    "known",
    "epsilon",
    "greedy",
    "algorithm",
    "set",
    "epsilon",
    "equal",
    "often",
    "want",
    "make",
    "random",
    "move",
    "occasionally",
    "make",
    "random",
    "move",
    "order",
    "say",
    "let",
    "try",
    "explore",
    "see",
    "happens",
    "logic",
    "algorithm",
    "probability",
    "1",
    "minus",
    "epsilon",
    "choose",
    "estimated",
    "best",
    "move",
    "greedy",
    "case",
    "always",
    "choose",
    "best",
    "move",
    "epsilon",
    "greedy",
    "time",
    "going",
    "choose",
    "best",
    "move",
    "sometimes",
    "going",
    "choose",
    "best",
    "move",
    "sometimes",
    "probability",
    "epsilon",
    "going",
    "choose",
    "random",
    "move",
    "instead",
    "every",
    "time",
    "faced",
    "ability",
    "take",
    "action",
    "sometimes",
    "going",
    "choose",
    "best",
    "move",
    "sometimes",
    "going",
    "choose",
    "random",
    "move",
    "type",
    "algorithm",
    "quite",
    "powerful",
    "reinforcement",
    "learning",
    "context",
    "always",
    "choosing",
    "best",
    "possible",
    "move",
    "right",
    "sometimes",
    "especially",
    "early",
    "allowing",
    "make",
    "random",
    "moves",
    "allow",
    "explore",
    "various",
    "different",
    "possible",
    "states",
    "actions",
    "maybe",
    "time",
    "might",
    "decrease",
    "value",
    "epsilon",
    "often",
    "choosing",
    "best",
    "move",
    "confident",
    "explored",
    "possibilities",
    "actually",
    "put",
    "practice",
    "one",
    "common",
    "application",
    "reinforcement",
    "learning",
    "game",
    "playing",
    "want",
    "teach",
    "agent",
    "play",
    "game",
    "let",
    "agent",
    "play",
    "game",
    "whole",
    "bunch",
    "reward",
    "signal",
    "happens",
    "end",
    "game",
    "game",
    "ai",
    "game",
    "gets",
    "reward",
    "like",
    "1",
    "example",
    "lost",
    "game",
    "gets",
    "reward",
    "negative",
    "begins",
    "learn",
    "actions",
    "good",
    "actions",
    "bad",
    "tell",
    "ai",
    "good",
    "bad",
    "ai",
    "figures",
    "based",
    "reward",
    "winning",
    "game",
    "signal",
    "losing",
    "game",
    "signal",
    "based",
    "begins",
    "figure",
    "decisions",
    "actually",
    "make",
    "one",
    "simple",
    "game",
    "may",
    "played",
    "game",
    "called",
    "nim",
    "game",
    "nim",
    "got",
    "whole",
    "bunch",
    "objects",
    "whole",
    "bunch",
    "different",
    "piles",
    "represented",
    "pile",
    "individual",
    "row",
    "got",
    "one",
    "object",
    "first",
    "pile",
    "three",
    "second",
    "pile",
    "five",
    "third",
    "pile",
    "seven",
    "fourth",
    "pile",
    "game",
    "nim",
    "two",
    "player",
    "game",
    "players",
    "take",
    "turns",
    "removing",
    "objects",
    "piles",
    "rule",
    "given",
    "turn",
    "allowed",
    "remove",
    "many",
    "objects",
    "want",
    "one",
    "piles",
    "one",
    "rows",
    "remove",
    "least",
    "one",
    "object",
    "remove",
    "many",
    "want",
    "exactly",
    "one",
    "piles",
    "whoever",
    "takes",
    "last",
    "object",
    "loses",
    "player",
    "one",
    "might",
    "remove",
    "four",
    "pile",
    "player",
    "two",
    "might",
    "remove",
    "four",
    "pile",
    "got",
    "four",
    "piles",
    "left",
    "one",
    "three",
    "one",
    "three",
    "player",
    "one",
    "might",
    "remove",
    "entirety",
    "second",
    "pile",
    "player",
    "two",
    "strategic",
    "might",
    "remove",
    "two",
    "third",
    "pile",
    "got",
    "three",
    "piles",
    "left",
    "one",
    "object",
    "left",
    "player",
    "one",
    "might",
    "remove",
    "one",
    "one",
    "pile",
    "player",
    "two",
    "removes",
    "one",
    "pile",
    "player",
    "one",
    "left",
    "choosing",
    "one",
    "object",
    "last",
    "pile",
    "point",
    "player",
    "one",
    "loses",
    "game",
    "fairly",
    "simple",
    "game",
    "piles",
    "objects",
    "turn",
    "choose",
    "many",
    "objects",
    "remove",
    "pile",
    "whoever",
    "removes",
    "last",
    "object",
    "loses",
    "type",
    "game",
    "could",
    "encode",
    "ai",
    "fairly",
    "easily",
    "states",
    "really",
    "four",
    "numbers",
    "every",
    "state",
    "many",
    "objects",
    "four",
    "piles",
    "actions",
    "things",
    "like",
    "many",
    "going",
    "remove",
    "one",
    "individual",
    "piles",
    "reward",
    "happens",
    "end",
    "player",
    "remove",
    "last",
    "object",
    "get",
    "sort",
    "punishment",
    "player",
    "remove",
    "last",
    "object",
    "well",
    "get",
    "sort",
    "reward",
    "could",
    "actually",
    "try",
    "show",
    "demonstration",
    "implemented",
    "ai",
    "play",
    "game",
    "nim",
    "right",
    "going",
    "create",
    "ai",
    "result",
    "training",
    "ai",
    "number",
    "games",
    "ai",
    "going",
    "play",
    "idea",
    "ai",
    "play",
    "games",
    "learn",
    "experiences",
    "learn",
    "future",
    "human",
    "play",
    "ai",
    "initially",
    "say",
    "train",
    "zero",
    "times",
    "meaning",
    "going",
    "let",
    "ai",
    "play",
    "practice",
    "games",
    "order",
    "learn",
    "experiences",
    "going",
    "see",
    "well",
    "plays",
    "looks",
    "like",
    "four",
    "piles",
    "choose",
    "many",
    "remove",
    "one",
    "piles",
    "maybe",
    "pile",
    "three",
    "remove",
    "five",
    "objects",
    "example",
    "ai",
    "chose",
    "take",
    "one",
    "item",
    "pile",
    "zero",
    "left",
    "piles",
    "example",
    "could",
    "choose",
    "maybe",
    "say",
    "would",
    "like",
    "remove",
    "pile",
    "two",
    "remove",
    "five",
    "example",
    "ai",
    "chose",
    "take",
    "two",
    "away",
    "pile",
    "one",
    "left",
    "one",
    "pile",
    "one",
    "object",
    "one",
    "pile",
    "two",
    "objects",
    "pile",
    "three",
    "remove",
    "two",
    "objects",
    "left",
    "ai",
    "choice",
    "take",
    "last",
    "one",
    "game",
    "able",
    "win",
    "ai",
    "really",
    "playing",
    "randomly",
    "prior",
    "experience",
    "using",
    "order",
    "make",
    "sorts",
    "judgments",
    "let",
    "let",
    "ai",
    "train",
    "games",
    "going",
    "let",
    "ai",
    "play",
    "games",
    "nim",
    "every",
    "time",
    "wins",
    "loses",
    "going",
    "learn",
    "experience",
    "learn",
    "future",
    "go",
    "ahead",
    "run",
    "see",
    "ai",
    "running",
    "whole",
    "bunch",
    "training",
    "games",
    "training",
    "games",
    "going",
    "let",
    "make",
    "sorts",
    "decisions",
    "going",
    "play",
    "ai",
    "maybe",
    "remove",
    "one",
    "pile",
    "three",
    "ai",
    "took",
    "everything",
    "pile",
    "three",
    "left",
    "three",
    "piles",
    "go",
    "ahead",
    "pile",
    "two",
    "maybe",
    "remove",
    "three",
    "items",
    "ai",
    "removes",
    "one",
    "item",
    "pile",
    "zero",
    "left",
    "two",
    "piles",
    "two",
    "items",
    "remove",
    "one",
    "pile",
    "one",
    "guess",
    "ai",
    "took",
    "two",
    "pile",
    "two",
    "leaving",
    "choice",
    "take",
    "one",
    "away",
    "pile",
    "one",
    "seems",
    "like",
    "playing",
    "games",
    "nim",
    "ai",
    "learned",
    "something",
    "states",
    "actions",
    "tend",
    "good",
    "begun",
    "learn",
    "sort",
    "pattern",
    "predict",
    "actions",
    "going",
    "good",
    "actions",
    "going",
    "bad",
    "given",
    "state",
    "reinforcement",
    "learning",
    "powerful",
    "technique",
    "achieving",
    "sorts",
    "agents",
    "agents",
    "able",
    "play",
    "game",
    "well",
    "learning",
    "experience",
    "whether",
    "playing",
    "people",
    "playing",
    "learning",
    "experiences",
    "well",
    "nim",
    "bit",
    "easy",
    "game",
    "use",
    "reinforcement",
    "learning",
    "states",
    "states",
    "many",
    "many",
    "different",
    "objects",
    "various",
    "different",
    "piles",
    "might",
    "imagine",
    "going",
    "harder",
    "think",
    "game",
    "like",
    "chess",
    "games",
    "many",
    "many",
    "states",
    "many",
    "many",
    "actions",
    "imagine",
    "taking",
    "going",
    "easy",
    "learn",
    "every",
    "state",
    "every",
    "action",
    "value",
    "going",
    "oftentimes",
    "case",
    "ca",
    "necessarily",
    "learn",
    "exactly",
    "value",
    "every",
    "state",
    "every",
    "action",
    "approximate",
    "much",
    "saw",
    "minimax",
    "could",
    "use",
    "approach",
    "stop",
    "calculating",
    "certain",
    "point",
    "time",
    "similar",
    "type",
    "approximation",
    "known",
    "function",
    "approximation",
    "reinforcement",
    "learning",
    "context",
    "instead",
    "learning",
    "value",
    "q",
    "every",
    "state",
    "every",
    "action",
    "function",
    "estimates",
    "value",
    "taking",
    "action",
    "particular",
    "state",
    "might",
    "based",
    "various",
    "different",
    "features",
    "state",
    "agent",
    "happens",
    "might",
    "choose",
    "features",
    "actually",
    "begin",
    "learn",
    "patterns",
    "generalize",
    "beyond",
    "one",
    "specific",
    "state",
    "one",
    "specific",
    "action",
    "begin",
    "learn",
    "certain",
    "features",
    "tend",
    "good",
    "things",
    "bad",
    "things",
    "reinforcement",
    "learning",
    "allow",
    "using",
    "similar",
    "mechanism",
    "generalize",
    "beyond",
    "one",
    "particular",
    "state",
    "say",
    "state",
    "looks",
    "kind",
    "like",
    "state",
    "maybe",
    "similar",
    "types",
    "actions",
    "worked",
    "one",
    "state",
    "also",
    "work",
    "another",
    "state",
    "well",
    "type",
    "approach",
    "quite",
    "helpful",
    "begin",
    "deal",
    "reinforcement",
    "learning",
    "exist",
    "larger",
    "larger",
    "state",
    "spaces",
    "feasible",
    "explore",
    "possible",
    "states",
    "could",
    "actually",
    "exist",
    "two",
    "main",
    "categories",
    "reinforcement",
    "learning",
    "supervised",
    "learning",
    "labeled",
    "input",
    "output",
    "pairs",
    "reinforcement",
    "learning",
    "agent",
    "learns",
    "rewards",
    "punishments",
    "receives",
    "third",
    "major",
    "category",
    "machine",
    "learning",
    "touch",
    "briefly",
    "known",
    "unsupervised",
    "learning",
    "unsupervised",
    "learning",
    "happens",
    "data",
    "without",
    "additional",
    "feedback",
    "without",
    "labels",
    "supervised",
    "learning",
    "case",
    "data",
    "labels",
    "labeled",
    "data",
    "point",
    "whether",
    "rainy",
    "day",
    "rainy",
    "day",
    "using",
    "labels",
    "able",
    "infer",
    "pattern",
    "labeled",
    "data",
    "counterfeit",
    "banknote",
    "counterfeit",
    "using",
    "labels",
    "able",
    "draw",
    "inferences",
    "patterns",
    "figure",
    "banknote",
    "look",
    "like",
    "versus",
    "unsupervised",
    "learning",
    "access",
    "labels",
    "still",
    "would",
    "like",
    "learn",
    "patterns",
    "one",
    "tasks",
    "might",
    "want",
    "perform",
    "unsupervised",
    "learning",
    "something",
    "like",
    "clustering",
    "clustering",
    "task",
    "given",
    "set",
    "objects",
    "organize",
    "distinct",
    "clusters",
    "groups",
    "objects",
    "similar",
    "one",
    "another",
    "lots",
    "applications",
    "clustering",
    "comes",
    "genetic",
    "research",
    "might",
    "whole",
    "bunch",
    "different",
    "genes",
    "want",
    "cluster",
    "similar",
    "genes",
    "trying",
    "analyze",
    "across",
    "population",
    "across",
    "species",
    "comes",
    "image",
    "want",
    "take",
    "pixels",
    "image",
    "cluster",
    "different",
    "parts",
    "image",
    "comes",
    "lot",
    "market",
    "research",
    "want",
    "divide",
    "consumers",
    "different",
    "groups",
    "know",
    "groups",
    "target",
    "certain",
    "types",
    "product",
    "advertisements",
    "example",
    "number",
    "contexts",
    "well",
    "clustering",
    "applicable",
    "one",
    "technique",
    "clustering",
    "algorithm",
    "known",
    "clustering",
    "clustering",
    "going",
    "going",
    "divide",
    "data",
    "points",
    "k",
    "different",
    "clusters",
    "going",
    "repeating",
    "process",
    "assigning",
    "points",
    "clusters",
    "moving",
    "around",
    "clusters",
    "centers",
    "going",
    "define",
    "cluster",
    "center",
    "middle",
    "cluster",
    "assign",
    "points",
    "cluster",
    "based",
    "center",
    "closest",
    "point",
    "show",
    "example",
    "example",
    "whole",
    "bunch",
    "unlabeled",
    "data",
    "various",
    "data",
    "points",
    "sort",
    "graphical",
    "space",
    "would",
    "like",
    "group",
    "various",
    "different",
    "clusters",
    "know",
    "originally",
    "let",
    "say",
    "want",
    "assign",
    "like",
    "three",
    "clusters",
    "group",
    "choose",
    "many",
    "clusters",
    "want",
    "clustering",
    "could",
    "try",
    "multiple",
    "see",
    "well",
    "values",
    "perform",
    "start",
    "randomly",
    "picking",
    "places",
    "put",
    "centers",
    "clusters",
    "maybe",
    "blue",
    "cluster",
    "red",
    "cluster",
    "green",
    "cluster",
    "going",
    "start",
    "centers",
    "clusters",
    "three",
    "locations",
    "clustering",
    "tells",
    "us",
    "centers",
    "clusters",
    "assign",
    "every",
    "point",
    "cluster",
    "based",
    "cluster",
    "center",
    "closest",
    "end",
    "something",
    "like",
    "points",
    "closer",
    "blue",
    "cluster",
    "center",
    "cluster",
    "center",
    "points",
    "closer",
    "green",
    "cluster",
    "center",
    "cluster",
    "center",
    "two",
    "points",
    "plus",
    "points",
    "closest",
    "red",
    "cluster",
    "center",
    "instead",
    "one",
    "possible",
    "assignment",
    "points",
    "three",
    "different",
    "clusters",
    "great",
    "seems",
    "like",
    "red",
    "cluster",
    "points",
    "kind",
    "far",
    "apart",
    "green",
    "cluster",
    "points",
    "kind",
    "far",
    "apart",
    "might",
    "ideal",
    "choice",
    "would",
    "cluster",
    "various",
    "different",
    "data",
    "points",
    "clustering",
    "iterative",
    "process",
    "next",
    "step",
    "assigned",
    "points",
    "cluster",
    "center",
    "nearest",
    "going",
    "clusters",
    "meaning",
    "take",
    "cluster",
    "centers",
    "diamond",
    "shapes",
    "move",
    "middle",
    "average",
    "effectively",
    "points",
    "cluster",
    "take",
    "blue",
    "point",
    "blue",
    "center",
    "go",
    "ahead",
    "move",
    "middle",
    "center",
    "points",
    "assigned",
    "blue",
    "cluster",
    "moving",
    "slightly",
    "right",
    "case",
    "thing",
    "red",
    "move",
    "cluster",
    "center",
    "middle",
    "points",
    "weighted",
    "many",
    "points",
    "points",
    "red",
    "center",
    "ends",
    "moving",
    "little",
    "bit",
    "way",
    "likewise",
    "green",
    "center",
    "many",
    "points",
    "side",
    "green",
    "center",
    "green",
    "center",
    "ends",
    "pulled",
    "little",
    "bit",
    "direction",
    "clusters",
    "repeat",
    "process",
    "go",
    "ahead",
    "reassign",
    "points",
    "cluster",
    "center",
    "closest",
    "moved",
    "around",
    "cluster",
    "centers",
    "cluster",
    "assignments",
    "might",
    "change",
    "point",
    "originally",
    "closer",
    "red",
    "cluster",
    "center",
    "actually",
    "closer",
    "blue",
    "cluster",
    "center",
    "goes",
    "point",
    "well",
    "three",
    "points",
    "originally",
    "closer",
    "green",
    "cluster",
    "center",
    "closer",
    "red",
    "cluster",
    "center",
    "instead",
    "reassign",
    "colors",
    "clusters",
    "data",
    "points",
    "belongs",
    "repeat",
    "process",
    "moving",
    "cluster",
    "means",
    "middles",
    "clusterism",
    "mean",
    "average",
    "points",
    "happen",
    "repeat",
    "process",
    "go",
    "ahead",
    "assign",
    "points",
    "cluster",
    "closest",
    "reach",
    "point",
    "assigned",
    "points",
    "clusters",
    "cluster",
    "nearest",
    "nothing",
    "changed",
    "reached",
    "sort",
    "equilibrium",
    "situation",
    "points",
    "changing",
    "allegiance",
    "result",
    "declare",
    "algorithm",
    "assignment",
    "points",
    "three",
    "different",
    "clusters",
    "looks",
    "like",
    "pretty",
    "good",
    "job",
    "trying",
    "identify",
    "points",
    "similar",
    "one",
    "another",
    "points",
    "groups",
    "green",
    "cluster",
    "blue",
    "cluster",
    "red",
    "cluster",
    "well",
    "without",
    "access",
    "labels",
    "tell",
    "us",
    "various",
    "different",
    "clusters",
    "used",
    "algorithm",
    "unsupervised",
    "sense",
    "without",
    "labels",
    "figure",
    "points",
    "belonged",
    "categories",
    "lots",
    "applications",
    "type",
    "clustering",
    "technique",
    "many",
    "algorithms",
    "various",
    "different",
    "fields",
    "within",
    "machine",
    "learning",
    "supervised",
    "reinforcement",
    "unsupervised",
    "many",
    "big",
    "picture",
    "foundational",
    "ideas",
    "underlie",
    "lot",
    "techniques",
    "problems",
    "trying",
    "solve",
    "try",
    "solve",
    "problems",
    "using",
    "number",
    "different",
    "methods",
    "trying",
    "take",
    "data",
    "learn",
    "patterns",
    "data",
    "whether",
    "trying",
    "find",
    "neighboring",
    "data",
    "points",
    "similar",
    "trying",
    "minimize",
    "sort",
    "loss",
    "function",
    "number",
    "techniques",
    "allow",
    "us",
    "begin",
    "try",
    "solve",
    "sorts",
    "problems",
    "look",
    "principles",
    "foundation",
    "modern",
    "machine",
    "learning",
    "ability",
    "take",
    "data",
    "learn",
    "data",
    "computer",
    "perform",
    "task",
    "even",
    "explicitly",
    "given",
    "instructions",
    "order",
    "next",
    "time",
    "continue",
    "conversation",
    "machine",
    "learning",
    "looking",
    "techniques",
    "use",
    "solving",
    "sorts",
    "problems",
    "see",
    "right",
    "welcome",
    "back",
    "everyone",
    "introduction",
    "artificial",
    "intelligence",
    "python",
    "last",
    "time",
    "took",
    "look",
    "machine",
    "learning",
    "set",
    "techniques",
    "computers",
    "use",
    "order",
    "take",
    "set",
    "data",
    "learn",
    "patterns",
    "inside",
    "data",
    "learn",
    "perform",
    "task",
    "even",
    "programmers",
    "give",
    "computer",
    "explicit",
    "instructions",
    "perform",
    "task",
    "today",
    "transition",
    "one",
    "popular",
    "techniques",
    "tools",
    "within",
    "machine",
    "learning",
    "neural",
    "networks",
    "neural",
    "networks",
    "inspired",
    "early",
    "1940s",
    "researchers",
    "thinking",
    "humans",
    "learn",
    "studying",
    "neuroscience",
    "human",
    "brain",
    "trying",
    "see",
    "whether",
    "could",
    "apply",
    "ideas",
    "computers",
    "well",
    "model",
    "computer",
    "learning",
    "human",
    "learning",
    "brain",
    "structured",
    "well",
    "simply",
    "put",
    "brain",
    "consists",
    "whole",
    "bunch",
    "neurons",
    "neurons",
    "connected",
    "one",
    "another",
    "communicate",
    "one",
    "another",
    "way",
    "particular",
    "think",
    "structure",
    "biological",
    "neural",
    "network",
    "something",
    "like",
    "couple",
    "key",
    "properties",
    "scientists",
    "observed",
    "one",
    "neurons",
    "connected",
    "receive",
    "electrical",
    "signals",
    "one",
    "another",
    "one",
    "neuron",
    "propagate",
    "electrical",
    "signals",
    "another",
    "neuron",
    "another",
    "point",
    "neurons",
    "process",
    "input",
    "signals",
    "activated",
    "neuron",
    "becomes",
    "activated",
    "certain",
    "point",
    "propagate",
    "signals",
    "onto",
    "neurons",
    "future",
    "question",
    "became",
    "could",
    "take",
    "biological",
    "idea",
    "humans",
    "learn",
    "brains",
    "neurons",
    "apply",
    "machine",
    "well",
    "effect",
    "designing",
    "artificial",
    "neural",
    "network",
    "ann",
    "mathematical",
    "model",
    "learning",
    "inspired",
    "biological",
    "neural",
    "networks",
    "artificial",
    "neural",
    "networks",
    "allow",
    "us",
    "first",
    "able",
    "model",
    "sort",
    "mathematical",
    "function",
    "every",
    "time",
    "look",
    "neural",
    "network",
    "see",
    "later",
    "today",
    "one",
    "really",
    "mathematical",
    "function",
    "mapping",
    "certain",
    "inputs",
    "particular",
    "outputs",
    "based",
    "structure",
    "network",
    "depending",
    "place",
    "particular",
    "units",
    "inside",
    "neural",
    "network",
    "going",
    "determine",
    "network",
    "going",
    "function",
    "particular",
    "artificial",
    "neural",
    "networks",
    "going",
    "lend",
    "way",
    "learn",
    "network",
    "parameters",
    "see",
    "moment",
    "effect",
    "want",
    "model",
    "easy",
    "us",
    "able",
    "write",
    "code",
    "allows",
    "network",
    "able",
    "figure",
    "model",
    "right",
    "mathematical",
    "function",
    "given",
    "particular",
    "set",
    "input",
    "data",
    "order",
    "create",
    "artificial",
    "neural",
    "network",
    "instead",
    "using",
    "biological",
    "neurons",
    "going",
    "use",
    "going",
    "call",
    "units",
    "units",
    "inside",
    "neural",
    "network",
    "represent",
    "kind",
    "like",
    "node",
    "graph",
    "represented",
    "blue",
    "circle",
    "like",
    "artificial",
    "units",
    "artificial",
    "neurons",
    "connected",
    "one",
    "another",
    "instance",
    "two",
    "units",
    "connected",
    "edge",
    "inside",
    "graph",
    "effectively",
    "going",
    "think",
    "idea",
    "sort",
    "mapping",
    "inputs",
    "outputs",
    "one",
    "unit",
    "connected",
    "another",
    "unit",
    "might",
    "think",
    "side",
    "input",
    "side",
    "output",
    "trying",
    "figure",
    "solve",
    "problem",
    "model",
    "sort",
    "mathematical",
    "function",
    "might",
    "take",
    "form",
    "something",
    "saw",
    "last",
    "time",
    "something",
    "like",
    "certain",
    "inputs",
    "like",
    "variables",
    "x1",
    "x2",
    "given",
    "inputs",
    "want",
    "perform",
    "sort",
    "task",
    "task",
    "like",
    "predicting",
    "whether",
    "going",
    "rain",
    "ideally",
    "like",
    "way",
    "given",
    "inputs",
    "x1",
    "x2",
    "stand",
    "sort",
    "variables",
    "weather",
    "would",
    "like",
    "able",
    "predict",
    "case",
    "boolean",
    "classification",
    "going",
    "rain",
    "going",
    "rain",
    "last",
    "time",
    "way",
    "mathematical",
    "function",
    "defined",
    "function",
    "h",
    "hypothesis",
    "function",
    "took",
    "input",
    "x1",
    "x2",
    "two",
    "inputs",
    "cared",
    "processing",
    "order",
    "determine",
    "whether",
    "thought",
    "going",
    "rain",
    "whether",
    "thought",
    "going",
    "rain",
    "question",
    "becomes",
    "hypothesis",
    "function",
    "order",
    "make",
    "determination",
    "decided",
    "last",
    "time",
    "use",
    "linear",
    "combination",
    "input",
    "variables",
    "determine",
    "output",
    "hypothesis",
    "function",
    "equal",
    "something",
    "like",
    "weight",
    "0",
    "plus",
    "weight",
    "1",
    "times",
    "x1",
    "plus",
    "weight",
    "2",
    "times",
    "x2",
    "going",
    "x1",
    "x2",
    "input",
    "variables",
    "inputs",
    "hypothesis",
    "function",
    "input",
    "variables",
    "multiplied",
    "weight",
    "number",
    "x1",
    "multiplied",
    "weight",
    "1",
    "x2",
    "multiplied",
    "weight",
    "additional",
    "weight",
    "weight",
    "0",
    "get",
    "multiplied",
    "input",
    "variable",
    "serves",
    "either",
    "move",
    "function",
    "move",
    "function",
    "value",
    "think",
    "either",
    "weight",
    "multiplied",
    "dummy",
    "value",
    "like",
    "number",
    "multiplied",
    "1",
    "multiplied",
    "anything",
    "sometimes",
    "see",
    "literature",
    "people",
    "call",
    "variable",
    "weight",
    "0",
    "bias",
    "think",
    "variables",
    "slightly",
    "different",
    "weights",
    "multiplied",
    "input",
    "separately",
    "add",
    "bias",
    "result",
    "well",
    "hear",
    "terminologies",
    "used",
    "people",
    "talk",
    "neural",
    "networks",
    "machine",
    "learning",
    "effect",
    "done",
    "order",
    "define",
    "hypothesis",
    "function",
    "need",
    "decide",
    "figure",
    "weights",
    "determine",
    "values",
    "multiply",
    "inputs",
    "get",
    "sort",
    "result",
    "course",
    "end",
    "need",
    "make",
    "sort",
    "classification",
    "like",
    "rainy",
    "rainy",
    "use",
    "sort",
    "function",
    "defines",
    "sort",
    "threshold",
    "saw",
    "instance",
    "step",
    "function",
    "defined",
    "1",
    "result",
    "multiplying",
    "weights",
    "inputs",
    "least",
    "0",
    "otherwise",
    "think",
    "line",
    "middle",
    "kind",
    "like",
    "dotted",
    "line",
    "effectively",
    "stays",
    "0",
    "way",
    "one",
    "point",
    "function",
    "steps",
    "jumps",
    "0",
    "reaches",
    "threshold",
    "1",
    "reaches",
    "particular",
    "threshold",
    "one",
    "way",
    "could",
    "define",
    "come",
    "call",
    "activation",
    "function",
    "function",
    "determines",
    "output",
    "becomes",
    "active",
    "changes",
    "1",
    "instead",
    "also",
    "saw",
    "want",
    "purely",
    "binary",
    "classification",
    "want",
    "purely",
    "1",
    "0",
    "wanted",
    "allow",
    "real",
    "numbered",
    "values",
    "could",
    "use",
    "different",
    "function",
    "number",
    "choices",
    "one",
    "looked",
    "logistic",
    "sigmoid",
    "function",
    "sort",
    "curve",
    "could",
    "represent",
    "probability",
    "may",
    "somewhere",
    "probability",
    "rain",
    "something",
    "like",
    "maybe",
    "little",
    "bit",
    "later",
    "probability",
    "rain",
    "rather",
    "binary",
    "classification",
    "0",
    "1",
    "could",
    "allow",
    "numbers",
    "well",
    "turns",
    "many",
    "different",
    "types",
    "activation",
    "functions",
    "activation",
    "function",
    "takes",
    "output",
    "multiplying",
    "weights",
    "together",
    "adding",
    "bias",
    "figuring",
    "actual",
    "output",
    "another",
    "popular",
    "one",
    "rectified",
    "linear",
    "unit",
    "otherwise",
    "known",
    "relu",
    "way",
    "works",
    "takes",
    "input",
    "takes",
    "maximum",
    "input",
    "positive",
    "remains",
    "unchanged",
    "0",
    "negative",
    "goes",
    "ahead",
    "levels",
    "activation",
    "functions",
    "could",
    "choose",
    "well",
    "short",
    "activation",
    "functions",
    "think",
    "function",
    "gets",
    "applied",
    "result",
    "computation",
    "take",
    "function",
    "g",
    "apply",
    "result",
    "calculation",
    "saw",
    "last",
    "time",
    "way",
    "defining",
    "hypothesis",
    "function",
    "takes",
    "inputs",
    "calculate",
    "linear",
    "combination",
    "inputs",
    "passes",
    "sort",
    "activation",
    "function",
    "get",
    "output",
    "actually",
    "turns",
    "model",
    "simplest",
    "neural",
    "networks",
    "going",
    "instead",
    "represent",
    "mathematical",
    "idea",
    "graphically",
    "using",
    "structure",
    "like",
    "neural",
    "network",
    "two",
    "inputs",
    "think",
    "x1",
    "x2",
    "one",
    "output",
    "think",
    "classifying",
    "whether",
    "think",
    "going",
    "rain",
    "rain",
    "example",
    "particular",
    "instance",
    "exactly",
    "model",
    "work",
    "well",
    "two",
    "inputs",
    "represents",
    "one",
    "input",
    "variables",
    "x1",
    "x2",
    "notice",
    "inputs",
    "connected",
    "output",
    "via",
    "edges",
    "going",
    "defined",
    "weights",
    "edges",
    "weight",
    "associated",
    "weight",
    "1",
    "weight",
    "output",
    "unit",
    "going",
    "going",
    "calculate",
    "output",
    "based",
    "inputs",
    "based",
    "weights",
    "output",
    "unit",
    "going",
    "multiply",
    "inputs",
    "weights",
    "add",
    "bias",
    "term",
    "think",
    "extra",
    "w0",
    "term",
    "gets",
    "added",
    "pass",
    "activation",
    "function",
    "graphical",
    "way",
    "representing",
    "idea",
    "saw",
    "last",
    "time",
    "mathematically",
    "going",
    "call",
    "simple",
    "neural",
    "network",
    "like",
    "neural",
    "network",
    "able",
    "learn",
    "calculate",
    "function",
    "want",
    "function",
    "neural",
    "network",
    "learn",
    "neural",
    "network",
    "going",
    "learn",
    "values",
    "w0",
    "w1",
    "w2",
    "activation",
    "function",
    "order",
    "get",
    "result",
    "would",
    "expect",
    "actually",
    "take",
    "look",
    "example",
    "simple",
    "function",
    "might",
    "calculate",
    "well",
    "recall",
    "back",
    "looking",
    "propositional",
    "logic",
    "one",
    "simplest",
    "functions",
    "looked",
    "something",
    "like",
    "function",
    "takes",
    "two",
    "inputs",
    "x",
    "outputs",
    "1",
    "otherwise",
    "known",
    "true",
    "either",
    "one",
    "inputs",
    "1",
    "outputs",
    "0",
    "inputs",
    "0",
    "false",
    "function",
    "truth",
    "table",
    "function",
    "long",
    "either",
    "inputs",
    "1",
    "output",
    "function",
    "1",
    "case",
    "output",
    "0",
    "inputs",
    "question",
    "could",
    "take",
    "train",
    "neural",
    "network",
    "able",
    "learn",
    "particular",
    "function",
    "would",
    "weights",
    "look",
    "like",
    "well",
    "could",
    "something",
    "like",
    "neural",
    "network",
    "propose",
    "order",
    "calculate",
    "function",
    "going",
    "use",
    "value",
    "1",
    "weights",
    "use",
    "bias",
    "negative",
    "use",
    "step",
    "function",
    "activation",
    "function",
    "work",
    "well",
    "wanted",
    "calculate",
    "something",
    "like",
    "0",
    "0",
    "know",
    "0",
    "false",
    "false",
    "false",
    "going",
    "well",
    "output",
    "unit",
    "going",
    "calculate",
    "input",
    "multiplied",
    "weight",
    "0",
    "times",
    "1",
    "thing",
    "0",
    "times",
    "1",
    "add",
    "bias",
    "minus",
    "give",
    "us",
    "result",
    "negative",
    "plot",
    "activation",
    "function",
    "negative",
    "1",
    "threshold",
    "means",
    "either",
    "0",
    "1",
    "threshold",
    "since",
    "negative",
    "1",
    "threshold",
    "output",
    "unit",
    "provides",
    "going",
    "would",
    "expect",
    "0",
    "0",
    "instead",
    "1",
    "0",
    "number",
    "1",
    "well",
    "case",
    "order",
    "calculate",
    "output",
    "going",
    "weighted",
    "sum",
    "1",
    "times",
    "1",
    "1",
    "0",
    "times",
    "1",
    "sum",
    "far",
    "add",
    "negative",
    "1",
    "well",
    "output",
    "plot",
    "0",
    "step",
    "function",
    "0",
    "ends",
    "threshold",
    "output",
    "going",
    "1",
    "output",
    "1",
    "0",
    "would",
    "expect",
    "well",
    "one",
    "example",
    "1",
    "1",
    "would",
    "result",
    "well",
    "1",
    "times",
    "1",
    "1",
    "1",
    "times",
    "1",
    "sum",
    "add",
    "bias",
    "term",
    "get",
    "number",
    "1",
    "1",
    "plotted",
    "graph",
    "way",
    "well",
    "beyond",
    "threshold",
    "output",
    "going",
    "1",
    "well",
    "output",
    "always",
    "0",
    "1",
    "depending",
    "whether",
    "past",
    "threshold",
    "neural",
    "network",
    "models",
    "function",
    "simple",
    "function",
    "definitely",
    "still",
    "able",
    "model",
    "correctly",
    "give",
    "inputs",
    "tell",
    "x1",
    "x2",
    "happens",
    "could",
    "imagine",
    "trying",
    "functions",
    "well",
    "function",
    "like",
    "function",
    "instance",
    "takes",
    "two",
    "inputs",
    "calculates",
    "whether",
    "x",
    "true",
    "x",
    "1",
    "1",
    "output",
    "x",
    "cases",
    "output",
    "could",
    "model",
    "inside",
    "neural",
    "network",
    "well",
    "well",
    "turns",
    "could",
    "way",
    "except",
    "instead",
    "negative",
    "1",
    "bias",
    "use",
    "negative",
    "2",
    "bias",
    "instead",
    "end",
    "looking",
    "like",
    "well",
    "1",
    "1",
    "1",
    "1",
    "true",
    "true",
    "equal",
    "true",
    "well",
    "take",
    "1",
    "times",
    "1",
    "1",
    "1",
    "times",
    "1",
    "get",
    "total",
    "sum",
    "2",
    "far",
    "add",
    "bias",
    "negative",
    "2",
    "get",
    "value",
    "0",
    "plot",
    "activation",
    "function",
    "past",
    "threshold",
    "output",
    "going",
    "input",
    "example",
    "like",
    "1",
    "0",
    "well",
    "weighted",
    "sum",
    "1",
    "plus",
    "0",
    "going",
    "minus",
    "2",
    "going",
    "give",
    "us",
    "negative",
    "1",
    "negative",
    "1",
    "past",
    "threshold",
    "output",
    "going",
    "simple",
    "functions",
    "model",
    "using",
    "neural",
    "network",
    "two",
    "inputs",
    "one",
    "output",
    "goal",
    "able",
    "figure",
    "weights",
    "order",
    "determine",
    "output",
    "could",
    "imagine",
    "generalizing",
    "calculate",
    "complex",
    "functions",
    "well",
    "maybe",
    "given",
    "humidity",
    "pressure",
    "want",
    "calculate",
    "probability",
    "going",
    "rain",
    "example",
    "might",
    "want",
    "problem",
    "given",
    "amount",
    "advertising",
    "given",
    "month",
    "maybe",
    "want",
    "predict",
    "expected",
    "sales",
    "going",
    "particular",
    "month",
    "could",
    "imagine",
    "inputs",
    "outputs",
    "different",
    "well",
    "turns",
    "problems",
    "going",
    "two",
    "inputs",
    "nice",
    "thing",
    "neural",
    "networks",
    "compose",
    "multiple",
    "units",
    "together",
    "make",
    "networks",
    "complex",
    "adding",
    "units",
    "particular",
    "neural",
    "network",
    "network",
    "looking",
    "two",
    "inputs",
    "one",
    "output",
    "could",
    "easily",
    "say",
    "let",
    "go",
    "ahead",
    "three",
    "inputs",
    "even",
    "inputs",
    "could",
    "arbitrarily",
    "decide",
    "however",
    "many",
    "inputs",
    "problem",
    "going",
    "calculating",
    "sort",
    "output",
    "care",
    "figuring",
    "value",
    "math",
    "work",
    "figuring",
    "output",
    "well",
    "going",
    "work",
    "similar",
    "way",
    "case",
    "two",
    "inputs",
    "two",
    "weights",
    "indicated",
    "edges",
    "multiplied",
    "weights",
    "numbers",
    "adding",
    "bias",
    "term",
    "thing",
    "cases",
    "well",
    "three",
    "inputs",
    "imagine",
    "multiplying",
    "three",
    "inputs",
    "weights",
    "five",
    "inputs",
    "instead",
    "going",
    "thing",
    "saying",
    "sum",
    "1",
    "5",
    "xi",
    "multiplied",
    "weight",
    "take",
    "five",
    "input",
    "variables",
    "multiply",
    "corresponding",
    "weight",
    "add",
    "bias",
    "would",
    "case",
    "five",
    "inputs",
    "neural",
    "network",
    "example",
    "could",
    "arbitrarily",
    "many",
    "nodes",
    "want",
    "inside",
    "neural",
    "network",
    "time",
    "going",
    "sum",
    "input",
    "variables",
    "multiplied",
    "weight",
    "add",
    "bias",
    "term",
    "end",
    "allows",
    "us",
    "able",
    "represent",
    "problems",
    "even",
    "inputs",
    "growing",
    "size",
    "neural",
    "network",
    "next",
    "question",
    "might",
    "ask",
    "question",
    "train",
    "neural",
    "networks",
    "case",
    "function",
    "function",
    "simple",
    "enough",
    "functions",
    "could",
    "tell",
    "like",
    "weights",
    "could",
    "probably",
    "reason",
    "weights",
    "order",
    "calculate",
    "output",
    "want",
    "general",
    "functions",
    "like",
    "predicting",
    "sales",
    "predicting",
    "whether",
    "going",
    "rain",
    "much",
    "trickier",
    "functions",
    "able",
    "figure",
    "would",
    "like",
    "computer",
    "mechanism",
    "calculating",
    "weights",
    "set",
    "weights",
    "neural",
    "network",
    "able",
    "accurately",
    "model",
    "function",
    "care",
    "trying",
    "estimate",
    "turns",
    "strategy",
    "inspired",
    "domain",
    "calculus",
    "technique",
    "called",
    "gradient",
    "descent",
    "gradient",
    "descent",
    "algorithm",
    "minimizing",
    "loss",
    "training",
    "neural",
    "network",
    "recall",
    "loss",
    "refers",
    "bad",
    "hypothesis",
    "function",
    "happens",
    "define",
    "certain",
    "loss",
    "functions",
    "saw",
    "examples",
    "loss",
    "functions",
    "last",
    "time",
    "give",
    "us",
    "number",
    "particular",
    "hypothesis",
    "saying",
    "poorly",
    "model",
    "data",
    "many",
    "examples",
    "get",
    "wrong",
    "worse",
    "less",
    "bad",
    "compared",
    "hypothesis",
    "functions",
    "might",
    "define",
    "loss",
    "function",
    "mathematical",
    "function",
    "mathematical",
    "function",
    "calculus",
    "could",
    "calculate",
    "something",
    "known",
    "gradient",
    "think",
    "like",
    "slope",
    "direction",
    "loss",
    "function",
    "moving",
    "particular",
    "point",
    "going",
    "tell",
    "us",
    "direction",
    "moving",
    "weights",
    "order",
    "minimize",
    "amount",
    "loss",
    "generally",
    "speaking",
    "wo",
    "get",
    "calculus",
    "high",
    "level",
    "idea",
    "gradient",
    "descent",
    "going",
    "look",
    "something",
    "like",
    "want",
    "train",
    "neural",
    "network",
    "go",
    "ahead",
    "start",
    "choosing",
    "weights",
    "randomly",
    "pick",
    "random",
    "weights",
    "weights",
    "neural",
    "network",
    "use",
    "input",
    "data",
    "access",
    "order",
    "train",
    "network",
    "order",
    "figure",
    "weights",
    "actually",
    "repeat",
    "process",
    "first",
    "step",
    "going",
    "calculate",
    "gradient",
    "based",
    "data",
    "points",
    "look",
    "data",
    "figure",
    "gradient",
    "place",
    "currently",
    "current",
    "setting",
    "weights",
    "means",
    "direction",
    "move",
    "weights",
    "order",
    "minimize",
    "total",
    "amount",
    "loss",
    "order",
    "make",
    "solution",
    "better",
    "calculated",
    "gradient",
    "direction",
    "move",
    "loss",
    "function",
    "well",
    "update",
    "weights",
    "according",
    "gradient",
    "take",
    "small",
    "step",
    "direction",
    "weights",
    "order",
    "try",
    "make",
    "solution",
    "little",
    "bit",
    "better",
    "size",
    "step",
    "take",
    "going",
    "vary",
    "choose",
    "training",
    "particular",
    "neural",
    "network",
    "short",
    "idea",
    "going",
    "take",
    "data",
    "points",
    "figure",
    "based",
    "data",
    "points",
    "direction",
    "weights",
    "move",
    "move",
    "weights",
    "one",
    "small",
    "step",
    "direction",
    "repeat",
    "process",
    "adjusting",
    "weights",
    "little",
    "bit",
    "time",
    "based",
    "data",
    "points",
    "eventually",
    "end",
    "pretty",
    "good",
    "solution",
    "trying",
    "solve",
    "sort",
    "problem",
    "least",
    "would",
    "hope",
    "happen",
    "look",
    "algorithm",
    "good",
    "question",
    "ask",
    "anytime",
    "analyzing",
    "algorithm",
    "going",
    "expensive",
    "part",
    "calculation",
    "going",
    "take",
    "lot",
    "work",
    "try",
    "figure",
    "going",
    "expensive",
    "calculate",
    "particular",
    "case",
    "gradient",
    "descent",
    "really",
    "expensive",
    "part",
    "data",
    "points",
    "part",
    "right",
    "take",
    "data",
    "points",
    "using",
    "data",
    "points",
    "figure",
    "gradient",
    "particular",
    "setting",
    "weights",
    "odds",
    "big",
    "machine",
    "learning",
    "problem",
    "trying",
    "solve",
    "big",
    "problem",
    "lot",
    "data",
    "lot",
    "data",
    "points",
    "order",
    "calculate",
    "figuring",
    "gradient",
    "based",
    "data",
    "points",
    "going",
    "expensive",
    "many",
    "times",
    "likely",
    "repeat",
    "process",
    "going",
    "data",
    "points",
    "taking",
    "one",
    "small",
    "step",
    "try",
    "figure",
    "optimal",
    "setting",
    "weights",
    "happens",
    "turns",
    "would",
    "ideally",
    "like",
    "able",
    "train",
    "neural",
    "networks",
    "faster",
    "able",
    "quickly",
    "converge",
    "sort",
    "solution",
    "going",
    "good",
    "solution",
    "problem",
    "case",
    "alternatives",
    "standard",
    "gradient",
    "descent",
    "looks",
    "data",
    "points",
    "employ",
    "method",
    "like",
    "stochastic",
    "gradient",
    "descent",
    "randomly",
    "choose",
    "one",
    "data",
    "point",
    "time",
    "calculate",
    "gradient",
    "based",
    "instead",
    "calculating",
    "based",
    "data",
    "points",
    "idea",
    "setting",
    "weights",
    "pick",
    "data",
    "point",
    "based",
    "one",
    "data",
    "point",
    "figure",
    "direction",
    "move",
    "weights",
    "move",
    "weights",
    "small",
    "direction",
    "take",
    "another",
    "data",
    "point",
    "repeat",
    "process",
    "maybe",
    "looking",
    "data",
    "points",
    "multiple",
    "times",
    "time",
    "using",
    "one",
    "data",
    "point",
    "calculate",
    "gradient",
    "calculate",
    "direction",
    "move",
    "using",
    "one",
    "data",
    "point",
    "instead",
    "data",
    "points",
    "probably",
    "gives",
    "us",
    "less",
    "accurate",
    "estimate",
    "gradient",
    "actually",
    "plus",
    "side",
    "going",
    "much",
    "faster",
    "able",
    "calculate",
    "much",
    "quickly",
    "calculate",
    "gradient",
    "based",
    "one",
    "data",
    "point",
    "instead",
    "calculating",
    "based",
    "data",
    "points",
    "computational",
    "work",
    "looking",
    "data",
    "points",
    "looking",
    "one",
    "data",
    "point",
    "turns",
    "middle",
    "ground",
    "also",
    "quite",
    "popular",
    "technique",
    "called",
    "gradient",
    "descent",
    "idea",
    "instead",
    "looking",
    "data",
    "versus",
    "single",
    "point",
    "instead",
    "divide",
    "data",
    "set",
    "small",
    "batches",
    "groups",
    "data",
    "points",
    "decide",
    "big",
    "particular",
    "batch",
    "short",
    "going",
    "look",
    "small",
    "number",
    "points",
    "given",
    "time",
    "hopefully",
    "getting",
    "accurate",
    "estimate",
    "gradient",
    "also",
    "requiring",
    "computational",
    "effort",
    "needed",
    "look",
    "every",
    "single",
    "one",
    "data",
    "points",
    "gradient",
    "descent",
    "technique",
    "use",
    "order",
    "train",
    "neural",
    "networks",
    "order",
    "figure",
    "setting",
    "weights",
    "want",
    "way",
    "try",
    "get",
    "accurate",
    "notion",
    "function",
    "work",
    "way",
    "modeling",
    "transform",
    "inputs",
    "particular",
    "outputs",
    "far",
    "networks",
    "taken",
    "look",
    "structured",
    "similar",
    "number",
    "inputs",
    "maybe",
    "two",
    "three",
    "five",
    "one",
    "output",
    "predicting",
    "like",
    "rain",
    "rain",
    "predicting",
    "one",
    "particular",
    "value",
    "often",
    "machine",
    "learning",
    "problems",
    "care",
    "one",
    "output",
    "might",
    "care",
    "output",
    "multiple",
    "different",
    "values",
    "associated",
    "way",
    "could",
    "take",
    "neural",
    "network",
    "add",
    "units",
    "input",
    "layer",
    "likewise",
    "add",
    "inputs",
    "add",
    "outputs",
    "output",
    "layer",
    "well",
    "instead",
    "one",
    "output",
    "could",
    "imagine",
    "two",
    "outputs",
    "could",
    "four",
    "outputs",
    "example",
    "case",
    "add",
    "inputs",
    "add",
    "outputs",
    "want",
    "keep",
    "network",
    "fully",
    "connected",
    "two",
    "layers",
    "need",
    "add",
    "weights",
    "input",
    "nodes",
    "four",
    "weights",
    "associated",
    "four",
    "outputs",
    "true",
    "various",
    "different",
    "input",
    "nodes",
    "add",
    "nodes",
    "add",
    "weights",
    "order",
    "make",
    "sure",
    "inputs",
    "somehow",
    "connected",
    "outputs",
    "output",
    "value",
    "calculated",
    "based",
    "value",
    "input",
    "happens",
    "might",
    "case",
    "want",
    "multiple",
    "different",
    "output",
    "values",
    "well",
    "might",
    "consider",
    "case",
    "weather",
    "predicting",
    "example",
    "might",
    "care",
    "whether",
    "raining",
    "raining",
    "might",
    "multiple",
    "different",
    "categories",
    "weather",
    "would",
    "like",
    "categorize",
    "weather",
    "single",
    "output",
    "variable",
    "binary",
    "classification",
    "like",
    "rain",
    "rain",
    "instance",
    "1",
    "allow",
    "us",
    "much",
    "multiple",
    "output",
    "variables",
    "might",
    "able",
    "use",
    "one",
    "predict",
    "something",
    "little",
    "different",
    "maybe",
    "want",
    "categorize",
    "weather",
    "one",
    "four",
    "different",
    "categories",
    "something",
    "like",
    "going",
    "raining",
    "sunny",
    "cloudy",
    "snowy",
    "four",
    "output",
    "variables",
    "used",
    "represent",
    "maybe",
    "probability",
    "rainy",
    "opposed",
    "sunny",
    "opposed",
    "cloudy",
    "opposed",
    "snowy",
    "would",
    "neural",
    "network",
    "work",
    "well",
    "input",
    "variables",
    "represent",
    "data",
    "collected",
    "weather",
    "inputs",
    "gets",
    "multiplied",
    "various",
    "different",
    "weights",
    "multiplications",
    "fairly",
    "quick",
    "mathematical",
    "operations",
    "perform",
    "get",
    "passing",
    "sort",
    "activation",
    "function",
    "outputs",
    "end",
    "getting",
    "sort",
    "number",
    "number",
    "might",
    "imagine",
    "could",
    "interpret",
    "probability",
    "like",
    "probability",
    "one",
    "category",
    "opposed",
    "another",
    "category",
    "saying",
    "based",
    "inputs",
    "think",
    "10",
    "chance",
    "raining",
    "60",
    "chance",
    "sunny",
    "20",
    "chance",
    "cloudy",
    "10",
    "chance",
    "snowy",
    "given",
    "output",
    "represent",
    "probability",
    "distribution",
    "well",
    "could",
    "pick",
    "whichever",
    "one",
    "highest",
    "value",
    "case",
    "sunny",
    "say",
    "well",
    "likely",
    "think",
    "categorization",
    "inputs",
    "means",
    "output",
    "snowy",
    "sunny",
    "would",
    "expect",
    "weather",
    "particular",
    "instance",
    "allows",
    "us",
    "sort",
    "classifications",
    "instead",
    "binary",
    "classification",
    "1",
    "0",
    "many",
    "different",
    "categories",
    "want",
    "neural",
    "network",
    "output",
    "probabilities",
    "categories",
    "likely",
    "categories",
    "using",
    "data",
    "able",
    "draw",
    "sort",
    "inference",
    "sort",
    "idea",
    "supervised",
    "machine",
    "learning",
    "give",
    "neural",
    "network",
    "whole",
    "bunch",
    "data",
    "whole",
    "bunch",
    "input",
    "data",
    "corresponding",
    "label",
    "output",
    "data",
    "like",
    "know",
    "raining",
    "day",
    "know",
    "sunny",
    "day",
    "using",
    "data",
    "algorithm",
    "use",
    "gradient",
    "descent",
    "figure",
    "weights",
    "order",
    "create",
    "sort",
    "model",
    "hopefully",
    "allows",
    "us",
    "way",
    "predict",
    "think",
    "weather",
    "going",
    "neural",
    "networks",
    "lot",
    "applications",
    "well",
    "could",
    "imagine",
    "applying",
    "sort",
    "idea",
    "reinforcement",
    "learning",
    "sort",
    "example",
    "well",
    "remember",
    "reinforcement",
    "learning",
    "wanted",
    "train",
    "sort",
    "agent",
    "learn",
    "action",
    "take",
    "depending",
    "state",
    "currently",
    "happen",
    "depending",
    "current",
    "state",
    "world",
    "wanted",
    "agent",
    "pick",
    "one",
    "available",
    "actions",
    "available",
    "might",
    "model",
    "input",
    "variables",
    "represent",
    "information",
    "state",
    "data",
    "state",
    "agent",
    "currently",
    "output",
    "example",
    "could",
    "various",
    "different",
    "actions",
    "agent",
    "could",
    "take",
    "action",
    "1",
    "2",
    "3",
    "might",
    "imagine",
    "network",
    "would",
    "work",
    "way",
    "based",
    "particular",
    "inputs",
    "go",
    "ahead",
    "calculate",
    "values",
    "outputs",
    "outputs",
    "could",
    "model",
    "action",
    "better",
    "actions",
    "could",
    "choose",
    "based",
    "looking",
    "outputs",
    "action",
    "take",
    "neural",
    "networks",
    "broadly",
    "applicable",
    "really",
    "modeling",
    "mathematical",
    "function",
    "anything",
    "frame",
    "mathematical",
    "function",
    "something",
    "like",
    "classifying",
    "inputs",
    "various",
    "different",
    "categories",
    "figuring",
    "based",
    "input",
    "state",
    "action",
    "take",
    "mathematical",
    "functions",
    "could",
    "attempt",
    "model",
    "taking",
    "advantage",
    "neural",
    "network",
    "structure",
    "particular",
    "taking",
    "advantage",
    "technique",
    "gradient",
    "descent",
    "use",
    "order",
    "figure",
    "weights",
    "order",
    "sort",
    "calculation",
    "would",
    "go",
    "training",
    "neural",
    "network",
    "multiple",
    "outputs",
    "instead",
    "one",
    "well",
    "single",
    "output",
    "could",
    "see",
    "output",
    "value",
    "update",
    "weights",
    "corresponded",
    "multiple",
    "outputs",
    "least",
    "particular",
    "case",
    "really",
    "think",
    "four",
    "separate",
    "neural",
    "networks",
    "really",
    "one",
    "network",
    "three",
    "inputs",
    "corresponding",
    "three",
    "weights",
    "corresponding",
    "one",
    "output",
    "value",
    "thing",
    "true",
    "output",
    "value",
    "output",
    "value",
    "effectively",
    "defines",
    "yet",
    "another",
    "neural",
    "network",
    "three",
    "inputs",
    "different",
    "set",
    "weights",
    "correspond",
    "output",
    "likewise",
    "output",
    "set",
    "weights",
    "well",
    "thing",
    "fourth",
    "output",
    "wanted",
    "train",
    "neural",
    "network",
    "four",
    "outputs",
    "instead",
    "one",
    "case",
    "inputs",
    "directly",
    "connected",
    "outputs",
    "could",
    "really",
    "think",
    "training",
    "four",
    "independent",
    "neural",
    "networks",
    "know",
    "outputs",
    "four",
    "based",
    "input",
    "data",
    "using",
    "data",
    "begin",
    "figure",
    "individual",
    "weights",
    "maybe",
    "additional",
    "step",
    "end",
    "make",
    "sure",
    "turn",
    "values",
    "probability",
    "distribution",
    "interpret",
    "one",
    "better",
    "another",
    "likely",
    "another",
    "category",
    "something",
    "like",
    "seems",
    "like",
    "pretty",
    "good",
    "job",
    "taking",
    "inputs",
    "trying",
    "predict",
    "outputs",
    "see",
    "real",
    "examples",
    "moment",
    "well",
    "important",
    "think",
    "limitations",
    "sort",
    "approach",
    "taking",
    "linear",
    "combination",
    "inputs",
    "passing",
    "sort",
    "activation",
    "function",
    "turns",
    "case",
    "binary",
    "classification",
    "trying",
    "predict",
    "belong",
    "one",
    "category",
    "another",
    "predict",
    "things",
    "linearly",
    "separable",
    "taking",
    "linear",
    "combination",
    "inputs",
    "using",
    "define",
    "decision",
    "boundary",
    "threshold",
    "get",
    "situation",
    "set",
    "data",
    "predict",
    "line",
    "separates",
    "linearly",
    "red",
    "points",
    "blue",
    "points",
    "single",
    "unit",
    "making",
    "binary",
    "classification",
    "otherwise",
    "known",
    "perceptron",
    "ca",
    "deal",
    "situation",
    "like",
    "seen",
    "type",
    "situation",
    "straight",
    "line",
    "goes",
    "straight",
    "data",
    "divide",
    "red",
    "points",
    "away",
    "blue",
    "points",
    "complex",
    "decision",
    "boundary",
    "decision",
    "boundary",
    "somehow",
    "needs",
    "capture",
    "things",
    "inside",
    "circle",
    "really",
    "line",
    "allow",
    "us",
    "deal",
    "limitation",
    "perceptron",
    "units",
    "make",
    "binary",
    "decisions",
    "based",
    "inputs",
    "single",
    "perceptron",
    "capable",
    "learning",
    "linearly",
    "separable",
    "decision",
    "boundary",
    "define",
    "line",
    "sure",
    "give",
    "us",
    "probabilities",
    "based",
    "close",
    "decision",
    "boundary",
    "really",
    "decide",
    "based",
    "linear",
    "decision",
    "boundary",
    "seem",
    "like",
    "going",
    "generalize",
    "well",
    "situations",
    "real",
    "world",
    "data",
    "involved",
    "real",
    "world",
    "data",
    "often",
    "linearly",
    "separable",
    "often",
    "case",
    "draw",
    "line",
    "data",
    "able",
    "divide",
    "multiple",
    "groups",
    "solution",
    "well",
    "proposed",
    "idea",
    "multilayer",
    "neural",
    "network",
    "far",
    "neural",
    "networks",
    "seen",
    "set",
    "inputs",
    "set",
    "outputs",
    "inputs",
    "connected",
    "outputs",
    "multilayer",
    "neural",
    "network",
    "going",
    "artificial",
    "neural",
    "network",
    "input",
    "layer",
    "still",
    "output",
    "layer",
    "also",
    "one",
    "hidden",
    "layers",
    "layers",
    "artificial",
    "neurons",
    "units",
    "going",
    "calculate",
    "values",
    "well",
    "instead",
    "neural",
    "network",
    "looks",
    "like",
    "three",
    "inputs",
    "one",
    "output",
    "might",
    "imagine",
    "middle",
    "injecting",
    "hidden",
    "layer",
    "something",
    "like",
    "hidden",
    "layer",
    "four",
    "nodes",
    "could",
    "choose",
    "many",
    "nodes",
    "units",
    "end",
    "going",
    "hidden",
    "layer",
    "multiple",
    "hidden",
    "layers",
    "well",
    "inputs",
    "directly",
    "connected",
    "output",
    "inputs",
    "connected",
    "hidden",
    "layer",
    "nodes",
    "hidden",
    "layer",
    "connected",
    "one",
    "output",
    "another",
    "step",
    "take",
    "towards",
    "calculating",
    "complex",
    "functions",
    "hidden",
    "units",
    "calculate",
    "output",
    "value",
    "otherwise",
    "known",
    "activation",
    "based",
    "linear",
    "combination",
    "inputs",
    "values",
    "nodes",
    "opposed",
    "output",
    "thing",
    "calculate",
    "output",
    "node",
    "based",
    "multiplying",
    "values",
    "units",
    "weights",
    "well",
    "effect",
    "way",
    "works",
    "start",
    "inputs",
    "get",
    "multiplied",
    "weights",
    "order",
    "calculate",
    "values",
    "hidden",
    "nodes",
    "get",
    "multiplied",
    "weights",
    "order",
    "figure",
    "ultimate",
    "output",
    "going",
    "advantage",
    "layering",
    "things",
    "like",
    "gives",
    "us",
    "ability",
    "model",
    "complex",
    "functions",
    "instead",
    "single",
    "decision",
    "boundary",
    "single",
    "line",
    "dividing",
    "red",
    "points",
    "blue",
    "points",
    "hidden",
    "nodes",
    "learn",
    "different",
    "decision",
    "boundary",
    "combine",
    "decision",
    "boundaries",
    "figure",
    "ultimate",
    "output",
    "going",
    "begin",
    "imagine",
    "complex",
    "situations",
    "could",
    "imagine",
    "nodes",
    "learning",
    "useful",
    "property",
    "learning",
    "useful",
    "feature",
    "inputs",
    "us",
    "somehow",
    "learning",
    "combine",
    "features",
    "together",
    "order",
    "get",
    "output",
    "actually",
    "want",
    "natural",
    "question",
    "begin",
    "look",
    "ask",
    "question",
    "train",
    "neural",
    "network",
    "hidden",
    "layers",
    "inside",
    "turns",
    "initially",
    "bit",
    "tricky",
    "question",
    "input",
    "data",
    "given",
    "given",
    "values",
    "inputs",
    "given",
    "value",
    "output",
    "category",
    "example",
    "input",
    "data",
    "tell",
    "us",
    "values",
    "nodes",
    "know",
    "far",
    "nodes",
    "actually",
    "given",
    "data",
    "inputs",
    "outputs",
    "reason",
    "called",
    "hidden",
    "layer",
    "data",
    "made",
    "available",
    "us",
    "tell",
    "us",
    "values",
    "intermediate",
    "nodes",
    "actually",
    "strategy",
    "people",
    "came",
    "say",
    "know",
    "error",
    "losses",
    "output",
    "node",
    "well",
    "based",
    "weights",
    "one",
    "weights",
    "higher",
    "another",
    "calculate",
    "estimate",
    "much",
    "error",
    "node",
    "due",
    "part",
    "hidden",
    "node",
    "part",
    "hidden",
    "layer",
    "part",
    "hidden",
    "layer",
    "based",
    "values",
    "weights",
    "effect",
    "saying",
    "based",
    "error",
    "output",
    "back",
    "propagate",
    "error",
    "figure",
    "estimate",
    "error",
    "nodes",
    "hidden",
    "layer",
    "well",
    "calculus",
    "wo",
    "get",
    "details",
    "idea",
    "algorithm",
    "known",
    "back",
    "propagation",
    "algorithm",
    "training",
    "neural",
    "network",
    "multiple",
    "different",
    "hidden",
    "layers",
    "idea",
    "pseudocode",
    "want",
    "run",
    "gradient",
    "descent",
    "back",
    "propagation",
    "start",
    "random",
    "choice",
    "weights",
    "go",
    "ahead",
    "repeat",
    "training",
    "process",
    "going",
    "time",
    "going",
    "calculate",
    "error",
    "output",
    "layer",
    "first",
    "know",
    "output",
    "know",
    "calculated",
    "figure",
    "error",
    "going",
    "repeat",
    "every",
    "layer",
    "starting",
    "output",
    "layer",
    "moving",
    "back",
    "hidden",
    "layer",
    "hidden",
    "layer",
    "multiple",
    "hidden",
    "layers",
    "going",
    "back",
    "way",
    "first",
    "hidden",
    "layer",
    "assuming",
    "multiple",
    "going",
    "propagate",
    "error",
    "back",
    "one",
    "layer",
    "whatever",
    "error",
    "output",
    "figure",
    "error",
    "layer",
    "based",
    "values",
    "weights",
    "update",
    "weights",
    "graphically",
    "way",
    "might",
    "think",
    "first",
    "start",
    "output",
    "know",
    "output",
    "know",
    "output",
    "calculated",
    "based",
    "figure",
    "right",
    "need",
    "update",
    "weights",
    "backpropagating",
    "error",
    "nodes",
    "using",
    "figure",
    "update",
    "weights",
    "might",
    "imagine",
    "multiple",
    "layers",
    "could",
    "repeat",
    "process",
    "begin",
    "figure",
    "weights",
    "updated",
    "backpropagation",
    "algorithm",
    "really",
    "key",
    "algorithm",
    "makes",
    "neural",
    "networks",
    "possible",
    "makes",
    "possible",
    "take",
    "structures",
    "able",
    "train",
    "structures",
    "depending",
    "values",
    "weights",
    "order",
    "figure",
    "go",
    "updating",
    "weights",
    "order",
    "create",
    "function",
    "able",
    "minimize",
    "total",
    "amount",
    "loss",
    "figure",
    "good",
    "setting",
    "weights",
    "take",
    "inputs",
    "translate",
    "output",
    "expect",
    "works",
    "said",
    "single",
    "hidden",
    "layer",
    "imagine",
    "multiple",
    "hidden",
    "layers",
    "hidden",
    "layer",
    "define",
    "however",
    "many",
    "nodes",
    "want",
    "nodes",
    "one",
    "layer",
    "connect",
    "nodes",
    "next",
    "layer",
    "defining",
    "complex",
    "networks",
    "able",
    "model",
    "complex",
    "types",
    "functions",
    "type",
    "network",
    "might",
    "call",
    "deep",
    "neural",
    "network",
    "part",
    "larger",
    "family",
    "deep",
    "learning",
    "algorithms",
    "ever",
    "heard",
    "term",
    "deep",
    "learning",
    "using",
    "multiple",
    "layers",
    "able",
    "predict",
    "able",
    "model",
    "higher",
    "level",
    "features",
    "inside",
    "input",
    "able",
    "figure",
    "output",
    "deep",
    "neural",
    "network",
    "neural",
    "network",
    "multiple",
    "hidden",
    "layers",
    "start",
    "input",
    "calculate",
    "values",
    "layer",
    "layer",
    "layer",
    "ultimately",
    "get",
    "output",
    "allows",
    "us",
    "able",
    "model",
    "sophisticated",
    "types",
    "functions",
    "layers",
    "calculate",
    "something",
    "little",
    "bit",
    "different",
    "combine",
    "information",
    "figure",
    "output",
    "course",
    "situation",
    "machine",
    "learning",
    "begin",
    "make",
    "models",
    "complex",
    "model",
    "complex",
    "functions",
    "risk",
    "run",
    "something",
    "like",
    "overfitting",
    "talked",
    "overfitting",
    "last",
    "time",
    "context",
    "overfitting",
    "based",
    "training",
    "models",
    "able",
    "learn",
    "sort",
    "decision",
    "boundary",
    "overfitting",
    "happens",
    "fit",
    "closely",
    "training",
    "data",
    "result",
    "generalize",
    "well",
    "situations",
    "well",
    "one",
    "risks",
    "run",
    "far",
    "complex",
    "neural",
    "network",
    "many",
    "many",
    "different",
    "nodes",
    "might",
    "overfit",
    "based",
    "input",
    "data",
    "might",
    "grow",
    "reliant",
    "certain",
    "nodes",
    "calculate",
    "things",
    "purely",
    "based",
    "input",
    "data",
    "allow",
    "us",
    "generalize",
    "well",
    "output",
    "number",
    "strategies",
    "dealing",
    "overfitting",
    "one",
    "popular",
    "context",
    "neural",
    "networks",
    "technique",
    "known",
    "dropout",
    "dropout",
    "training",
    "neural",
    "network",
    "dropout",
    "temporarily",
    "remove",
    "units",
    "temporarily",
    "remove",
    "artificial",
    "neurons",
    "network",
    "chosen",
    "random",
    "goal",
    "prevent",
    "certain",
    "units",
    "generally",
    "happens",
    "overfitting",
    "begin",
    "certain",
    "units",
    "inside",
    "neural",
    "network",
    "able",
    "tell",
    "us",
    "interpret",
    "input",
    "data",
    "dropout",
    "randomly",
    "remove",
    "units",
    "order",
    "reduce",
    "chance",
    "certain",
    "units",
    "make",
    "neural",
    "network",
    "robust",
    "able",
    "handle",
    "situations",
    "even",
    "drop",
    "particular",
    "neurons",
    "entirely",
    "way",
    "might",
    "work",
    "network",
    "like",
    "training",
    "go",
    "trying",
    "update",
    "weights",
    "first",
    "time",
    "randomly",
    "pick",
    "percentage",
    "nodes",
    "drop",
    "network",
    "nodes",
    "weights",
    "associated",
    "nodes",
    "train",
    "way",
    "next",
    "time",
    "update",
    "weights",
    "pick",
    "different",
    "set",
    "go",
    "ahead",
    "train",
    "way",
    "randomly",
    "choose",
    "train",
    "nodes",
    "dropped",
    "well",
    "goal",
    "training",
    "process",
    "train",
    "dropping",
    "random",
    "nodes",
    "inside",
    "neural",
    "network",
    "hopefully",
    "end",
    "network",
    "little",
    "bit",
    "robust",
    "rely",
    "heavily",
    "one",
    "particular",
    "node",
    "generally",
    "learns",
    "approximate",
    "function",
    "general",
    "look",
    "techniques",
    "use",
    "order",
    "implement",
    "neural",
    "network",
    "get",
    "idea",
    "taking",
    "input",
    "passing",
    "various",
    "different",
    "layers",
    "order",
    "produce",
    "sort",
    "output",
    "like",
    "take",
    "ideas",
    "put",
    "code",
    "number",
    "different",
    "machine",
    "learning",
    "libraries",
    "neural",
    "network",
    "libraries",
    "use",
    "allow",
    "us",
    "get",
    "access",
    "someone",
    "implementation",
    "back",
    "propagation",
    "hidden",
    "layers",
    "one",
    "popular",
    "developed",
    "google",
    "known",
    "tensorflow",
    "library",
    "use",
    "quickly",
    "creating",
    "neural",
    "networks",
    "modeling",
    "running",
    "sample",
    "data",
    "see",
    "output",
    "going",
    "actually",
    "start",
    "writing",
    "code",
    "go",
    "ahead",
    "take",
    "look",
    "tensorflow",
    "playground",
    "opportunity",
    "us",
    "play",
    "around",
    "idea",
    "neural",
    "networks",
    "different",
    "layers",
    "get",
    "sense",
    "taking",
    "advantage",
    "neural",
    "networks",
    "let",
    "go",
    "ahead",
    "go",
    "tensorflow",
    "playground",
    "go",
    "visiting",
    "url",
    "going",
    "going",
    "try",
    "learn",
    "decision",
    "boundary",
    "particular",
    "output",
    "want",
    "learn",
    "separate",
    "orange",
    "points",
    "blue",
    "points",
    "like",
    "learn",
    "sort",
    "setting",
    "weights",
    "inside",
    "neural",
    "network",
    "able",
    "separate",
    "features",
    "access",
    "input",
    "data",
    "x",
    "value",
    "value",
    "two",
    "values",
    "along",
    "two",
    "axes",
    "set",
    "particular",
    "parameters",
    "like",
    "activation",
    "function",
    "would",
    "like",
    "use",
    "go",
    "ahead",
    "press",
    "play",
    "see",
    "happens",
    "happens",
    "see",
    "using",
    "two",
    "input",
    "features",
    "x",
    "value",
    "value",
    "hidden",
    "layers",
    "take",
    "input",
    "x",
    "values",
    "figure",
    "decision",
    "boundary",
    "neural",
    "network",
    "learns",
    "pretty",
    "quickly",
    "order",
    "divide",
    "two",
    "points",
    "use",
    "line",
    "line",
    "acts",
    "decision",
    "boundary",
    "separates",
    "group",
    "points",
    "group",
    "points",
    "well",
    "see",
    "loss",
    "training",
    "loss",
    "0",
    "meaning",
    "able",
    "perfectly",
    "model",
    "separating",
    "two",
    "points",
    "inside",
    "training",
    "data",
    "fairly",
    "simple",
    "case",
    "trying",
    "apply",
    "neural",
    "network",
    "data",
    "clean",
    "nicely",
    "linearly",
    "separable",
    "could",
    "draw",
    "line",
    "separates",
    "points",
    "let",
    "consider",
    "complex",
    "case",
    "go",
    "ahead",
    "pause",
    "simulation",
    "go",
    "ahead",
    "look",
    "data",
    "set",
    "data",
    "set",
    "little",
    "bit",
    "complex",
    "data",
    "set",
    "still",
    "blue",
    "orange",
    "points",
    "like",
    "separate",
    "single",
    "line",
    "draw",
    "going",
    "able",
    "figure",
    "separate",
    "blue",
    "orange",
    "blue",
    "located",
    "two",
    "quadrants",
    "orange",
    "located",
    "complex",
    "function",
    "able",
    "learn",
    "let",
    "see",
    "happens",
    "try",
    "predict",
    "based",
    "inputs",
    "x",
    "coordinates",
    "output",
    "press",
    "play",
    "notice",
    "really",
    "able",
    "draw",
    "much",
    "conclusion",
    "able",
    "cleanly",
    "see",
    "divide",
    "orange",
    "points",
    "blue",
    "points",
    "see",
    "clean",
    "separation",
    "seems",
    "like",
    "enough",
    "sophistication",
    "inside",
    "network",
    "able",
    "model",
    "something",
    "complex",
    "need",
    "better",
    "model",
    "neural",
    "network",
    "adding",
    "hidden",
    "layer",
    "hidden",
    "layer",
    "two",
    "neurons",
    "inside",
    "two",
    "inputs",
    "go",
    "two",
    "neurons",
    "inside",
    "hidden",
    "layer",
    "go",
    "output",
    "press",
    "play",
    "notice",
    "able",
    "slightly",
    "better",
    "able",
    "say",
    "right",
    "points",
    "definitely",
    "blue",
    "points",
    "definitely",
    "orange",
    "still",
    "struggling",
    "little",
    "bit",
    "points",
    "though",
    "see",
    "hidden",
    "neurons",
    "exactly",
    "hidden",
    "neurons",
    "hidden",
    "neuron",
    "learning",
    "decision",
    "boundary",
    "see",
    "boundary",
    "first",
    "neuron",
    "learning",
    "right",
    "line",
    "seems",
    "separate",
    "blue",
    "points",
    "rest",
    "points",
    "hidden",
    "neuron",
    "learning",
    "another",
    "line",
    "seems",
    "separating",
    "orange",
    "points",
    "lower",
    "right",
    "rest",
    "points",
    "able",
    "figure",
    "two",
    "areas",
    "bottom",
    "region",
    "still",
    "able",
    "perfectly",
    "classify",
    "points",
    "let",
    "go",
    "ahead",
    "add",
    "another",
    "neuron",
    "got",
    "three",
    "neurons",
    "inside",
    "hidden",
    "layer",
    "see",
    "able",
    "learn",
    "right",
    "well",
    "seem",
    "better",
    "job",
    "learning",
    "three",
    "different",
    "decision",
    "boundaries",
    "three",
    "neurons",
    "inside",
    "hidden",
    "layer",
    "able",
    "much",
    "better",
    "figure",
    "separate",
    "blue",
    "points",
    "orange",
    "points",
    "see",
    "hidden",
    "neurons",
    "learning",
    "one",
    "learning",
    "slightly",
    "different",
    "decision",
    "boundary",
    "combining",
    "decision",
    "boundaries",
    "together",
    "figure",
    "overall",
    "output",
    "try",
    "one",
    "time",
    "adding",
    "fourth",
    "neuron",
    "try",
    "learning",
    "seems",
    "like",
    "even",
    "better",
    "trying",
    "separate",
    "blue",
    "points",
    "orange",
    "points",
    "able",
    "adding",
    "hidden",
    "layer",
    "adding",
    "layer",
    "learning",
    "boundaries",
    "combining",
    "boundaries",
    "determine",
    "output",
    "strength",
    "size",
    "thickness",
    "lines",
    "indicate",
    "high",
    "weights",
    "important",
    "inputs",
    "making",
    "sort",
    "calculation",
    "maybe",
    "one",
    "simulation",
    "let",
    "go",
    "ahead",
    "try",
    "data",
    "set",
    "looks",
    "like",
    "go",
    "ahead",
    "get",
    "rid",
    "hidden",
    "layer",
    "trying",
    "separate",
    "blue",
    "points",
    "orange",
    "points",
    "blue",
    "points",
    "located",
    "inside",
    "circle",
    "effectively",
    "going",
    "able",
    "learn",
    "line",
    "notice",
    "press",
    "play",
    "really",
    "able",
    "draw",
    "sort",
    "classification",
    "line",
    "cleanly",
    "separates",
    "blue",
    "points",
    "orange",
    "points",
    "let",
    "try",
    "solve",
    "introducing",
    "hidden",
    "layer",
    "go",
    "ahead",
    "press",
    "play",
    "right",
    "two",
    "neurons",
    "hidden",
    "layer",
    "able",
    "little",
    "better",
    "effectively",
    "learned",
    "two",
    "different",
    "decision",
    "boundaries",
    "learned",
    "line",
    "learned",
    "line",
    "side",
    "right",
    "saying",
    "right",
    "well",
    "call",
    "blue",
    "outside",
    "call",
    "orange",
    "great",
    "certainly",
    "better",
    "learning",
    "one",
    "decision",
    "boundary",
    "another",
    "based",
    "figure",
    "output",
    "let",
    "go",
    "ahead",
    "add",
    "third",
    "neuron",
    "see",
    "happens",
    "go",
    "ahead",
    "train",
    "using",
    "three",
    "different",
    "decision",
    "boundaries",
    "learned",
    "hidden",
    "neurons",
    "able",
    "much",
    "accurately",
    "model",
    "distinction",
    "blue",
    "points",
    "orange",
    "points",
    "able",
    "figure",
    "maybe",
    "three",
    "decision",
    "boundaries",
    "combining",
    "together",
    "imagine",
    "figuring",
    "output",
    "make",
    "sort",
    "classification",
    "goal",
    "get",
    "sense",
    "neurons",
    "hidden",
    "layers",
    "allows",
    "us",
    "learn",
    "structure",
    "data",
    "allows",
    "us",
    "figure",
    "relevant",
    "important",
    "decision",
    "boundaries",
    "using",
    "backpropagation",
    "algorithm",
    "able",
    "figure",
    "values",
    "weights",
    "order",
    "train",
    "network",
    "able",
    "classify",
    "one",
    "category",
    "points",
    "away",
    "another",
    "category",
    "points",
    "instead",
    "ultimately",
    "going",
    "trying",
    "whenever",
    "training",
    "neural",
    "network",
    "let",
    "go",
    "ahead",
    "actually",
    "see",
    "example",
    "recall",
    "last",
    "time",
    "banknotes",
    "file",
    "included",
    "information",
    "counterfeit",
    "banknotes",
    "opposed",
    "authentic",
    "banknotes",
    "four",
    "different",
    "values",
    "banknote",
    "categorization",
    "whether",
    "banknote",
    "considered",
    "authentic",
    "counterfeit",
    "note",
    "wanted",
    "based",
    "input",
    "information",
    "figure",
    "function",
    "could",
    "calculate",
    "based",
    "input",
    "information",
    "category",
    "belonged",
    "written",
    "neural",
    "network",
    "learn",
    "network",
    "learns",
    "based",
    "input",
    "whether",
    "categorize",
    "banknote",
    "authentic",
    "counterfeit",
    "first",
    "step",
    "saw",
    "last",
    "time",
    "really",
    "reading",
    "data",
    "getting",
    "appropriate",
    "format",
    "writing",
    "python",
    "code",
    "comes",
    "terms",
    "manipulating",
    "data",
    "massaging",
    "data",
    "format",
    "understood",
    "machine",
    "learning",
    "library",
    "like",
    "like",
    "tensorflow",
    "separate",
    "training",
    "testing",
    "set",
    "creating",
    "neural",
    "network",
    "using",
    "tf",
    "stands",
    "tensorflow",
    "said",
    "import",
    "tensorflow",
    "tf",
    "tf",
    "abbreviation",
    "often",
    "use",
    "need",
    "write",
    "tensorflow",
    "every",
    "time",
    "want",
    "use",
    "anything",
    "inside",
    "library",
    "using",
    "keras",
    "api",
    "set",
    "functions",
    "use",
    "order",
    "manipulate",
    "neural",
    "networks",
    "inside",
    "tensorflow",
    "turns",
    "machine",
    "learning",
    "libraries",
    "also",
    "use",
    "keras",
    "api",
    "saying",
    "right",
    "go",
    "ahead",
    "give",
    "model",
    "sequential",
    "model",
    "sequential",
    "neural",
    "network",
    "meaning",
    "one",
    "layer",
    "another",
    "going",
    "add",
    "model",
    "layers",
    "want",
    "inside",
    "neural",
    "network",
    "saying",
    "go",
    "ahead",
    "add",
    "dense",
    "layer",
    "say",
    "dense",
    "layer",
    "mean",
    "layer",
    "nodes",
    "inside",
    "layer",
    "going",
    "connected",
    "nodes",
    "previous",
    "layer",
    "densely",
    "connected",
    "layer",
    "layer",
    "going",
    "eight",
    "units",
    "inside",
    "going",
    "hidden",
    "layer",
    "inside",
    "neural",
    "network",
    "eight",
    "different",
    "units",
    "eight",
    "artificial",
    "neurons",
    "might",
    "learn",
    "something",
    "different",
    "sort",
    "chose",
    "eight",
    "arbitrarily",
    "could",
    "choose",
    "different",
    "number",
    "hidden",
    "nodes",
    "inside",
    "layer",
    "saw",
    "depending",
    "number",
    "units",
    "inside",
    "hidden",
    "layer",
    "units",
    "means",
    "learn",
    "complex",
    "functions",
    "maybe",
    "accurately",
    "model",
    "training",
    "data",
    "comes",
    "cost",
    "units",
    "means",
    "weights",
    "need",
    "figure",
    "update",
    "might",
    "expensive",
    "calculation",
    "also",
    "run",
    "risk",
    "overfitting",
    "data",
    "many",
    "units",
    "learn",
    "overfit",
    "training",
    "data",
    "good",
    "either",
    "balance",
    "often",
    "testing",
    "process",
    "train",
    "data",
    "maybe",
    "validate",
    "well",
    "separate",
    "set",
    "data",
    "often",
    "called",
    "validation",
    "set",
    "see",
    "right",
    "setting",
    "parameters",
    "many",
    "layers",
    "many",
    "units",
    "layer",
    "one",
    "performs",
    "best",
    "validation",
    "set",
    "testing",
    "figure",
    "hyper",
    "parameters",
    "called",
    "equal",
    "next",
    "specify",
    "input",
    "shape",
    "meaning",
    "right",
    "input",
    "look",
    "like",
    "input",
    "four",
    "values",
    "input",
    "shape",
    "four",
    "four",
    "inputs",
    "specify",
    "activation",
    "function",
    "activation",
    "function",
    "choose",
    "number",
    "different",
    "activation",
    "functions",
    "using",
    "relu",
    "might",
    "recall",
    "earlier",
    "add",
    "output",
    "layer",
    "hidden",
    "layer",
    "adding",
    "one",
    "layer",
    "one",
    "unit",
    "want",
    "predict",
    "something",
    "like",
    "counterfeit",
    "build",
    "authentic",
    "build",
    "need",
    "single",
    "unit",
    "activation",
    "function",
    "going",
    "use",
    "sigmoid",
    "activation",
    "function",
    "curve",
    "gave",
    "us",
    "probability",
    "probability",
    "counterfeit",
    "build",
    "opposed",
    "authentic",
    "build",
    "structure",
    "neural",
    "network",
    "sequential",
    "neural",
    "network",
    "one",
    "hidden",
    "layer",
    "eight",
    "units",
    "inside",
    "one",
    "output",
    "layer",
    "single",
    "unit",
    "inside",
    "choose",
    "many",
    "units",
    "choose",
    "activation",
    "function",
    "going",
    "compile",
    "model",
    "tensorflow",
    "gives",
    "choice",
    "would",
    "like",
    "optimize",
    "weights",
    "various",
    "different",
    "algorithms",
    "type",
    "loss",
    "function",
    "want",
    "use",
    "many",
    "different",
    "options",
    "want",
    "evaluate",
    "model",
    "well",
    "care",
    "accuracy",
    "care",
    "many",
    "points",
    "able",
    "classify",
    "correctly",
    "versus",
    "correctly",
    "counterfeit",
    "counterfeit",
    "would",
    "like",
    "report",
    "accurate",
    "model",
    "performing",
    "defined",
    "model",
    "call",
    "say",
    "go",
    "ahead",
    "train",
    "model",
    "train",
    "training",
    "data",
    "plus",
    "training",
    "labels",
    "labels",
    "pieces",
    "training",
    "data",
    "saying",
    "run",
    "20",
    "epics",
    "meaning",
    "go",
    "ahead",
    "go",
    "training",
    "points",
    "20",
    "times",
    "effectively",
    "go",
    "data",
    "20",
    "times",
    "keep",
    "trying",
    "update",
    "weights",
    "could",
    "train",
    "even",
    "longer",
    "maybe",
    "get",
    "accurate",
    "result",
    "fit",
    "data",
    "go",
    "ahead",
    "test",
    "evaluate",
    "model",
    "using",
    "built",
    "tensorflow",
    "going",
    "tell",
    "well",
    "perform",
    "testing",
    "data",
    "ultimately",
    "going",
    "give",
    "numbers",
    "tell",
    "well",
    "particular",
    "case",
    "going",
    "go",
    "banknotes",
    "go",
    "ahead",
    "run",
    "going",
    "happen",
    "going",
    "read",
    "training",
    "data",
    "going",
    "generate",
    "neural",
    "network",
    "inputs",
    "eight",
    "hidden",
    "units",
    "inside",
    "layer",
    "output",
    "unit",
    "training",
    "training",
    "20",
    "times",
    "time",
    "see",
    "accuracy",
    "increasing",
    "training",
    "data",
    "starts",
    "first",
    "time",
    "accurate",
    "though",
    "better",
    "random",
    "something",
    "like",
    "79",
    "time",
    "able",
    "accurately",
    "classify",
    "one",
    "bill",
    "another",
    "keep",
    "training",
    "notice",
    "accuracy",
    "value",
    "improves",
    "improves",
    "improves",
    "trained",
    "data",
    "points",
    "20",
    "times",
    "looks",
    "like",
    "accuracy",
    "99",
    "training",
    "data",
    "tested",
    "whole",
    "bunch",
    "testing",
    "data",
    "looks",
    "like",
    "case",
    "also",
    "like",
    "accurate",
    "using",
    "able",
    "generate",
    "neural",
    "network",
    "detect",
    "counterfeit",
    "bills",
    "authentic",
    "bills",
    "based",
    "input",
    "data",
    "time",
    "least",
    "based",
    "particular",
    "testing",
    "data",
    "might",
    "want",
    "test",
    "data",
    "well",
    "confident",
    "really",
    "value",
    "using",
    "machine",
    "learning",
    "library",
    "like",
    "tensorflow",
    "others",
    "available",
    "python",
    "languages",
    "well",
    "define",
    "structure",
    "network",
    "define",
    "data",
    "going",
    "pass",
    "network",
    "tensorflow",
    "runs",
    "backpropagation",
    "algorithm",
    "learning",
    "weights",
    "figuring",
    "train",
    "neural",
    "network",
    "able",
    "accurately",
    "accurately",
    "possible",
    "figure",
    "output",
    "values",
    "well",
    "look",
    "neural",
    "networks",
    "using",
    "sequences",
    "layer",
    "layer",
    "layer",
    "begin",
    "imagine",
    "applying",
    "much",
    "general",
    "problems",
    "one",
    "big",
    "problem",
    "computing",
    "artificial",
    "intelligence",
    "generally",
    "problem",
    "computer",
    "vision",
    "computer",
    "vision",
    "computational",
    "methods",
    "analyzing",
    "understanding",
    "images",
    "might",
    "pictures",
    "want",
    "computer",
    "figure",
    "deal",
    "process",
    "images",
    "figure",
    "produce",
    "sort",
    "useful",
    "result",
    "seen",
    "context",
    "social",
    "media",
    "websites",
    "able",
    "look",
    "photo",
    "contains",
    "whole",
    "bunch",
    "faces",
    "able",
    "figure",
    "picture",
    "label",
    "tag",
    "appropriate",
    "people",
    "becoming",
    "increasingly",
    "relevant",
    "begin",
    "discuss",
    "cars",
    "cars",
    "cameras",
    "would",
    "like",
    "computer",
    "sort",
    "algorithm",
    "looks",
    "image",
    "figures",
    "color",
    "light",
    "cars",
    "around",
    "us",
    "direction",
    "example",
    "computer",
    "vision",
    "taking",
    "image",
    "figuring",
    "sort",
    "computation",
    "sort",
    "calculation",
    "image",
    "also",
    "relevant",
    "context",
    "something",
    "like",
    "handwriting",
    "recognition",
    "looking",
    "example",
    "mnist",
    "data",
    "set",
    "big",
    "data",
    "set",
    "handwritten",
    "digits",
    "could",
    "use",
    "ideally",
    "try",
    "figure",
    "predict",
    "given",
    "someone",
    "handwriting",
    "given",
    "photo",
    "digit",
    "drawn",
    "predict",
    "whether",
    "0",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "example",
    "sort",
    "handwriting",
    "recognition",
    "yet",
    "another",
    "task",
    "might",
    "want",
    "use",
    "computer",
    "vision",
    "tasks",
    "tools",
    "able",
    "apply",
    "towards",
    "might",
    "task",
    "might",
    "care",
    "use",
    "neural",
    "networks",
    "able",
    "solve",
    "problem",
    "like",
    "well",
    "neural",
    "networks",
    "rely",
    "upon",
    "sort",
    "input",
    "input",
    "numerical",
    "data",
    "whole",
    "bunch",
    "units",
    "one",
    "represents",
    "sort",
    "number",
    "context",
    "something",
    "like",
    "handwriting",
    "recognition",
    "context",
    "image",
    "might",
    "imagine",
    "image",
    "really",
    "grid",
    "pixels",
    "grid",
    "dots",
    "dot",
    "sort",
    "color",
    "context",
    "something",
    "like",
    "handwriting",
    "recognition",
    "might",
    "imagine",
    "fill",
    "dots",
    "particular",
    "way",
    "generate",
    "2",
    "8",
    "example",
    "based",
    "dots",
    "happen",
    "shaded",
    "dots",
    "represent",
    "pixel",
    "values",
    "using",
    "numbers",
    "particular",
    "pixel",
    "example",
    "0",
    "might",
    "represent",
    "entirely",
    "black",
    "depending",
    "representing",
    "color",
    "often",
    "common",
    "represent",
    "color",
    "values",
    "0",
    "255",
    "range",
    "represent",
    "color",
    "using",
    "8",
    "bits",
    "particular",
    "value",
    "like",
    "much",
    "white",
    "image",
    "0",
    "might",
    "represent",
    "black",
    "255",
    "might",
    "represent",
    "entirely",
    "white",
    "pixel",
    "somewhere",
    "might",
    "represent",
    "shade",
    "gray",
    "example",
    "might",
    "imagine",
    "single",
    "slider",
    "determines",
    "much",
    "white",
    "image",
    "color",
    "image",
    "might",
    "imagine",
    "three",
    "different",
    "numerical",
    "values",
    "red",
    "green",
    "blue",
    "value",
    "red",
    "value",
    "controls",
    "much",
    "red",
    "image",
    "one",
    "value",
    "controlling",
    "much",
    "green",
    "pixel",
    "one",
    "value",
    "much",
    "blue",
    "pixel",
    "well",
    "depending",
    "set",
    "values",
    "red",
    "green",
    "blue",
    "get",
    "different",
    "color",
    "pixel",
    "really",
    "represented",
    "case",
    "three",
    "numerical",
    "values",
    "red",
    "value",
    "green",
    "value",
    "blue",
    "value",
    "take",
    "whole",
    "bunch",
    "pixels",
    "assemble",
    "together",
    "inside",
    "grid",
    "pixels",
    "really",
    "whole",
    "bunch",
    "numerical",
    "values",
    "use",
    "order",
    "perform",
    "sort",
    "prediction",
    "task",
    "might",
    "imagine",
    "using",
    "techniques",
    "talked",
    "design",
    "neural",
    "network",
    "lot",
    "inputs",
    "pixels",
    "might",
    "one",
    "three",
    "different",
    "inputs",
    "case",
    "color",
    "image",
    "different",
    "input",
    "connected",
    "deep",
    "neural",
    "network",
    "example",
    "deep",
    "neural",
    "network",
    "might",
    "take",
    "pixels",
    "inside",
    "image",
    "digit",
    "person",
    "drew",
    "output",
    "might",
    "like",
    "10",
    "neurons",
    "classify",
    "0",
    "1",
    "2",
    "3",
    "tells",
    "us",
    "way",
    "digit",
    "happens",
    "couple",
    "drawbacks",
    "approach",
    "first",
    "drawback",
    "approach",
    "size",
    "input",
    "array",
    "whole",
    "bunch",
    "inputs",
    "big",
    "image",
    "lot",
    "different",
    "channels",
    "looking",
    "lot",
    "inputs",
    "therefore",
    "lot",
    "weights",
    "calculate",
    "second",
    "problem",
    "fact",
    "flattening",
    "everything",
    "structure",
    "pixels",
    "lost",
    "access",
    "lot",
    "information",
    "structure",
    "image",
    "relevant",
    "really",
    "person",
    "looks",
    "image",
    "looking",
    "particular",
    "features",
    "image",
    "looking",
    "curves",
    "looking",
    "shapes",
    "looking",
    "things",
    "identify",
    "different",
    "regions",
    "image",
    "maybe",
    "put",
    "things",
    "together",
    "order",
    "get",
    "better",
    "picture",
    "overall",
    "image",
    "turning",
    "pixel",
    "values",
    "pixels",
    "sure",
    "might",
    "able",
    "learn",
    "structure",
    "might",
    "challenging",
    "order",
    "might",
    "helpful",
    "take",
    "advantage",
    "fact",
    "use",
    "properties",
    "image",
    "fact",
    "structured",
    "particular",
    "way",
    "able",
    "improve",
    "way",
    "learn",
    "based",
    "image",
    "order",
    "figure",
    "train",
    "neural",
    "networks",
    "better",
    "able",
    "deal",
    "images",
    "introduce",
    "couple",
    "ideas",
    "couple",
    "algorithms",
    "apply",
    "allow",
    "us",
    "take",
    "image",
    "extract",
    "useful",
    "information",
    "image",
    "first",
    "idea",
    "introduce",
    "notion",
    "image",
    "convolution",
    "image",
    "convolution",
    "filtering",
    "image",
    "sort",
    "extracting",
    "useful",
    "relevant",
    "features",
    "image",
    "way",
    "applying",
    "particular",
    "filter",
    "basically",
    "adds",
    "value",
    "every",
    "pixel",
    "values",
    "neighboring",
    "pixels",
    "according",
    "sort",
    "kernel",
    "matrix",
    "see",
    "moment",
    "going",
    "allow",
    "us",
    "weight",
    "pixels",
    "various",
    "different",
    "ways",
    "goal",
    "image",
    "convolution",
    "extract",
    "sort",
    "interesting",
    "useful",
    "features",
    "image",
    "able",
    "take",
    "pixel",
    "based",
    "neighboring",
    "pixels",
    "maybe",
    "predict",
    "sort",
    "valuable",
    "information",
    "something",
    "like",
    "taking",
    "pixel",
    "looking",
    "neighboring",
    "pixels",
    "might",
    "able",
    "predict",
    "whether",
    "sort",
    "curve",
    "inside",
    "image",
    "whether",
    "forming",
    "outline",
    "particular",
    "line",
    "shape",
    "example",
    "might",
    "useful",
    "trying",
    "use",
    "various",
    "different",
    "features",
    "combine",
    "say",
    "something",
    "meaningful",
    "image",
    "whole",
    "image",
    "convolution",
    "work",
    "well",
    "start",
    "kernel",
    "matrix",
    "kernel",
    "matrix",
    "looks",
    "something",
    "like",
    "idea",
    "given",
    "pixel",
    "middle",
    "pixel",
    "going",
    "multiply",
    "neighboring",
    "pixels",
    "values",
    "order",
    "get",
    "sort",
    "result",
    "summing",
    "numbers",
    "together",
    "take",
    "kernel",
    "think",
    "filter",
    "going",
    "apply",
    "image",
    "let",
    "say",
    "take",
    "image",
    "4",
    "4",
    "image",
    "think",
    "black",
    "white",
    "image",
    "one",
    "single",
    "pixel",
    "value",
    "somewhere",
    "0",
    "255",
    "example",
    "whole",
    "bunch",
    "individual",
    "pixel",
    "values",
    "like",
    "like",
    "apply",
    "kernel",
    "filter",
    "speak",
    "image",
    "way",
    "right",
    "kernel",
    "3",
    "imagine",
    "5",
    "5",
    "kernel",
    "larger",
    "kernel",
    "take",
    "first",
    "apply",
    "first",
    "3",
    "3",
    "section",
    "image",
    "take",
    "pixel",
    "values",
    "multiply",
    "corresponding",
    "value",
    "filter",
    "matrix",
    "add",
    "results",
    "together",
    "example",
    "say",
    "10",
    "times",
    "0",
    "plus",
    "20",
    "times",
    "negative",
    "1",
    "plus",
    "30",
    "times",
    "0",
    "forth",
    "calculation",
    "end",
    "take",
    "values",
    "multiply",
    "corresponding",
    "value",
    "kernel",
    "add",
    "results",
    "together",
    "particular",
    "set",
    "9",
    "pixels",
    "get",
    "value",
    "10",
    "example",
    "slide",
    "3",
    "3",
    "grid",
    "effectively",
    "slide",
    "kernel",
    "1",
    "look",
    "next",
    "3",
    "3",
    "section",
    "sliding",
    "1",
    "pixel",
    "might",
    "imagine",
    "different",
    "stride",
    "length",
    "maybe",
    "jump",
    "multiple",
    "pixels",
    "time",
    "really",
    "wanted",
    "different",
    "options",
    "sliding",
    "looking",
    "next",
    "3",
    "3",
    "section",
    "math",
    "20",
    "times",
    "0",
    "plus",
    "30",
    "times",
    "negative",
    "1",
    "plus",
    "40",
    "times",
    "0",
    "plus",
    "20",
    "times",
    "negative",
    "1",
    "forth",
    "plus",
    "30",
    "times",
    "end",
    "getting",
    "number",
    "imagine",
    "shifting",
    "one",
    "thing",
    "calculating",
    "number",
    "40",
    "example",
    "thing",
    "calculating",
    "value",
    "well",
    "call",
    "feature",
    "map",
    "taken",
    "kernel",
    "applied",
    "various",
    "different",
    "regions",
    "get",
    "representation",
    "filtered",
    "version",
    "image",
    "give",
    "concrete",
    "example",
    "kind",
    "thing",
    "could",
    "useful",
    "let",
    "take",
    "kernel",
    "matrix",
    "example",
    "quite",
    "famous",
    "one",
    "8",
    "middle",
    "neighboring",
    "pixels",
    "get",
    "negative",
    "let",
    "imagine",
    "wanted",
    "apply",
    "3",
    "3",
    "part",
    "image",
    "looks",
    "like",
    "values",
    "20",
    "instance",
    "well",
    "case",
    "20",
    "times",
    "8",
    "subtract",
    "20",
    "subtract",
    "20",
    "subtract",
    "20",
    "eight",
    "neighbors",
    "well",
    "result",
    "get",
    "expression",
    "comes",
    "multiplied",
    "20",
    "8",
    "subtracted",
    "20",
    "eight",
    "times",
    "according",
    "particular",
    "kernel",
    "result",
    "takeaway",
    "lot",
    "pixels",
    "value",
    "end",
    "getting",
    "value",
    "close",
    "though",
    "something",
    "like",
    "20",
    "along",
    "first",
    "row",
    "50",
    "second",
    "row",
    "50",
    "third",
    "row",
    "well",
    "kind",
    "math",
    "20",
    "times",
    "negative",
    "1",
    "20",
    "times",
    "negative",
    "1",
    "forth",
    "get",
    "higher",
    "value",
    "value",
    "like",
    "90",
    "particular",
    "case",
    "general",
    "idea",
    "applying",
    "kernel",
    "negative",
    "1s",
    "8",
    "middle",
    "negative",
    "1s",
    "get",
    "middle",
    "value",
    "different",
    "neighboring",
    "values",
    "like",
    "50",
    "greater",
    "20s",
    "end",
    "value",
    "higher",
    "number",
    "higher",
    "neighbors",
    "end",
    "getting",
    "bigger",
    "output",
    "value",
    "neighbors",
    "get",
    "lower",
    "output",
    "something",
    "like",
    "turns",
    "sort",
    "filter",
    "therefore",
    "used",
    "something",
    "like",
    "detecting",
    "edges",
    "image",
    "want",
    "detect",
    "boundaries",
    "various",
    "different",
    "objects",
    "inside",
    "image",
    "might",
    "use",
    "filter",
    "like",
    "able",
    "tell",
    "whether",
    "value",
    "pixel",
    "different",
    "values",
    "neighboring",
    "pixel",
    "greater",
    "values",
    "pixels",
    "happen",
    "surround",
    "use",
    "terms",
    "image",
    "filtering",
    "show",
    "example",
    "file",
    "uses",
    "python",
    "image",
    "library",
    "pil",
    "image",
    "filtering",
    "go",
    "ahead",
    "open",
    "image",
    "going",
    "apply",
    "kernel",
    "image",
    "going",
    "3",
    "3",
    "kernel",
    "kind",
    "kernel",
    "saw",
    "kernel",
    "list",
    "representation",
    "matrix",
    "showed",
    "moment",
    "ago",
    "negative",
    "1",
    "negative",
    "1",
    "negative",
    "second",
    "row",
    "negative",
    "1",
    "8",
    "negative",
    "third",
    "row",
    "negative",
    "1s",
    "end",
    "going",
    "go",
    "ahead",
    "show",
    "filtered",
    "image",
    "example",
    "go",
    "convolution",
    "directory",
    "open",
    "image",
    "like",
    "input",
    "image",
    "might",
    "look",
    "like",
    "image",
    "bridge",
    "river",
    "going",
    "go",
    "ahead",
    "run",
    "filter",
    "program",
    "bridge",
    "get",
    "image",
    "taking",
    "original",
    "image",
    "applying",
    "filter",
    "3",
    "3",
    "grid",
    "extracted",
    "boundaries",
    "edges",
    "inside",
    "image",
    "separate",
    "one",
    "part",
    "image",
    "another",
    "got",
    "representation",
    "boundaries",
    "particular",
    "parts",
    "image",
    "might",
    "imagine",
    "machine",
    "learning",
    "algorithm",
    "trying",
    "learn",
    "image",
    "filter",
    "like",
    "could",
    "pretty",
    "useful",
    "maybe",
    "machine",
    "learning",
    "algorithm",
    "care",
    "details",
    "image",
    "cares",
    "certain",
    "useful",
    "features",
    "cares",
    "particular",
    "shapes",
    "able",
    "help",
    "determine",
    "based",
    "image",
    "going",
    "bridge",
    "example",
    "type",
    "idea",
    "image",
    "convolution",
    "allow",
    "us",
    "apply",
    "filters",
    "images",
    "allow",
    "us",
    "extract",
    "useful",
    "results",
    "images",
    "taking",
    "image",
    "extracting",
    "edges",
    "example",
    "might",
    "imagine",
    "many",
    "filters",
    "could",
    "applied",
    "image",
    "able",
    "extract",
    "particular",
    "values",
    "well",
    "filter",
    "might",
    "separate",
    "kernels",
    "red",
    "values",
    "green",
    "values",
    "blue",
    "values",
    "summed",
    "together",
    "end",
    "could",
    "particular",
    "filters",
    "looking",
    "red",
    "part",
    "image",
    "green",
    "parts",
    "image",
    "begin",
    "assemble",
    "relevant",
    "useful",
    "filters",
    "able",
    "calculations",
    "well",
    "idea",
    "image",
    "convolution",
    "applying",
    "sort",
    "filter",
    "image",
    "able",
    "extract",
    "useful",
    "features",
    "image",
    "images",
    "still",
    "pretty",
    "big",
    "lot",
    "pixels",
    "involved",
    "image",
    "realistically",
    "speaking",
    "got",
    "really",
    "big",
    "image",
    "poses",
    "couple",
    "problems",
    "one",
    "means",
    "lot",
    "input",
    "going",
    "neural",
    "network",
    "two",
    "also",
    "means",
    "really",
    "care",
    "particular",
    "pixel",
    "whereas",
    "realistically",
    "often",
    "looking",
    "image",
    "care",
    "whether",
    "something",
    "one",
    "particular",
    "pixel",
    "versus",
    "pixel",
    "immediately",
    "right",
    "pretty",
    "close",
    "together",
    "really",
    "care",
    "whether",
    "particular",
    "feature",
    "region",
    "image",
    "maybe",
    "care",
    "exactly",
    "pixel",
    "happens",
    "technique",
    "use",
    "known",
    "pooling",
    "pooling",
    "means",
    "reducing",
    "size",
    "input",
    "sampling",
    "regions",
    "inside",
    "input",
    "going",
    "take",
    "big",
    "image",
    "turn",
    "smaller",
    "image",
    "using",
    "pooling",
    "particular",
    "one",
    "popular",
    "types",
    "pooling",
    "called",
    "max",
    "pooling",
    "max",
    "pooling",
    "pools",
    "choosing",
    "maximum",
    "value",
    "particular",
    "region",
    "example",
    "let",
    "imagine",
    "4",
    "4",
    "image",
    "wanted",
    "reduce",
    "dimensions",
    "wanted",
    "make",
    "smaller",
    "image",
    "fewer",
    "inputs",
    "work",
    "well",
    "could",
    "could",
    "apply",
    "2",
    "2",
    "max",
    "pool",
    "idea",
    "would",
    "going",
    "first",
    "look",
    "2",
    "2",
    "region",
    "say",
    "maximum",
    "value",
    "region",
    "well",
    "number",
    "go",
    "ahead",
    "use",
    "number",
    "look",
    "2",
    "2",
    "region",
    "maximum",
    "value",
    "110",
    "going",
    "value",
    "likewise",
    "maximum",
    "value",
    "looks",
    "like",
    "go",
    "ahead",
    "put",
    "last",
    "region",
    "maximum",
    "value",
    "go",
    "ahead",
    "use",
    "smaller",
    "representation",
    "original",
    "image",
    "obtained",
    "picking",
    "maximum",
    "value",
    "regions",
    "advantages",
    "deal",
    "2",
    "2",
    "input",
    "instead",
    "4",
    "imagine",
    "shrinking",
    "size",
    "image",
    "even",
    "addition",
    "able",
    "make",
    "analysis",
    "independent",
    "whether",
    "particular",
    "value",
    "pixel",
    "pixel",
    "care",
    "50",
    "long",
    "generally",
    "region",
    "still",
    "get",
    "access",
    "value",
    "makes",
    "algorithms",
    "little",
    "bit",
    "robust",
    "well",
    "pooling",
    "taking",
    "size",
    "image",
    "reducing",
    "little",
    "bit",
    "sampling",
    "particular",
    "regions",
    "inside",
    "image",
    "put",
    "ideas",
    "together",
    "pooling",
    "image",
    "convolution",
    "neural",
    "networks",
    "together",
    "another",
    "type",
    "neural",
    "network",
    "called",
    "convolutional",
    "neural",
    "network",
    "cnn",
    "neural",
    "network",
    "uses",
    "convolution",
    "step",
    "usually",
    "context",
    "analyzing",
    "image",
    "example",
    "way",
    "convolutional",
    "neural",
    "network",
    "works",
    "start",
    "sort",
    "input",
    "image",
    "grid",
    "pixels",
    "rather",
    "immediately",
    "put",
    "neural",
    "network",
    "layers",
    "seen",
    "start",
    "applying",
    "convolution",
    "step",
    "convolution",
    "step",
    "involves",
    "applying",
    "number",
    "different",
    "image",
    "filters",
    "original",
    "image",
    "order",
    "get",
    "call",
    "feature",
    "map",
    "result",
    "applying",
    "filter",
    "image",
    "could",
    "general",
    "multiple",
    "times",
    "getting",
    "whole",
    "bunch",
    "different",
    "feature",
    "maps",
    "might",
    "extract",
    "different",
    "relevant",
    "feature",
    "image",
    "different",
    "important",
    "characteristic",
    "image",
    "might",
    "care",
    "using",
    "order",
    "calculate",
    "result",
    "way",
    "train",
    "neural",
    "networks",
    "train",
    "neural",
    "networks",
    "learn",
    "weights",
    "particular",
    "units",
    "inside",
    "neural",
    "networks",
    "also",
    "train",
    "neural",
    "networks",
    "learn",
    "filters",
    "values",
    "filters",
    "order",
    "get",
    "useful",
    "relevant",
    "information",
    "original",
    "image",
    "figuring",
    "setting",
    "filter",
    "values",
    "values",
    "inside",
    "kernel",
    "results",
    "minimizing",
    "loss",
    "function",
    "minimizing",
    "poorly",
    "hypothesis",
    "actually",
    "performs",
    "figuring",
    "classification",
    "particular",
    "image",
    "example",
    "first",
    "apply",
    "convolution",
    "step",
    "get",
    "whole",
    "bunch",
    "various",
    "different",
    "feature",
    "maps",
    "feature",
    "maps",
    "quite",
    "large",
    "lot",
    "pixel",
    "values",
    "happen",
    "logical",
    "next",
    "step",
    "take",
    "pooling",
    "step",
    "reduce",
    "size",
    "images",
    "using",
    "max",
    "pooling",
    "example",
    "extracting",
    "maximum",
    "value",
    "particular",
    "region",
    "pooling",
    "methods",
    "exist",
    "well",
    "depending",
    "situation",
    "could",
    "use",
    "something",
    "like",
    "average",
    "pooling",
    "instead",
    "taking",
    "maximum",
    "value",
    "region",
    "take",
    "average",
    "value",
    "region",
    "uses",
    "well",
    "effect",
    "pooling",
    "take",
    "feature",
    "maps",
    "reduce",
    "dimensions",
    "end",
    "smaller",
    "grids",
    "fewer",
    "pixels",
    "going",
    "easier",
    "us",
    "deal",
    "going",
    "mean",
    "fewer",
    "inputs",
    "worry",
    "also",
    "going",
    "mean",
    "resilient",
    "robust",
    "potential",
    "movements",
    "particular",
    "values",
    "one",
    "pixel",
    "ultimately",
    "really",
    "care",
    "differences",
    "might",
    "arise",
    "original",
    "image",
    "done",
    "pooling",
    "step",
    "whole",
    "bunch",
    "values",
    "flatten",
    "put",
    "traditional",
    "neural",
    "network",
    "go",
    "ahead",
    "flatten",
    "end",
    "traditional",
    "neural",
    "network",
    "one",
    "input",
    "values",
    "resulting",
    "feature",
    "maps",
    "convolution",
    "pooling",
    "step",
    "general",
    "structure",
    "convolutional",
    "network",
    "begin",
    "image",
    "apply",
    "convolution",
    "apply",
    "pooling",
    "flatten",
    "results",
    "put",
    "traditional",
    "neural",
    "network",
    "might",
    "hidden",
    "layers",
    "deep",
    "convolutional",
    "networks",
    "hidden",
    "layers",
    "flattened",
    "layer",
    "eventual",
    "output",
    "able",
    "calculate",
    "various",
    "different",
    "features",
    "values",
    "help",
    "us",
    "able",
    "use",
    "convolution",
    "pooling",
    "use",
    "knowledge",
    "structure",
    "image",
    "able",
    "get",
    "better",
    "results",
    "able",
    "train",
    "networks",
    "faster",
    "order",
    "better",
    "capture",
    "particular",
    "parts",
    "image",
    "reason",
    "necessarily",
    "use",
    "steps",
    "fact",
    "practice",
    "often",
    "use",
    "convolution",
    "pooling",
    "multiple",
    "times",
    "multiple",
    "different",
    "steps",
    "see",
    "might",
    "imagine",
    "starting",
    "image",
    "first",
    "applying",
    "convolution",
    "get",
    "whole",
    "bunch",
    "maps",
    "applying",
    "pooling",
    "applying",
    "convolution",
    "maps",
    "still",
    "pretty",
    "big",
    "apply",
    "convolution",
    "try",
    "extract",
    "relevant",
    "features",
    "result",
    "take",
    "results",
    "apply",
    "pooling",
    "order",
    "reduce",
    "dimensions",
    "take",
    "feed",
    "neural",
    "network",
    "maybe",
    "fewer",
    "inputs",
    "two",
    "different",
    "convolution",
    "pooling",
    "steps",
    "convolution",
    "pooling",
    "convolution",
    "pooling",
    "second",
    "time",
    "time",
    "extracting",
    "useful",
    "features",
    "layer",
    "time",
    "using",
    "pooling",
    "reduce",
    "dimensions",
    "ultimately",
    "looking",
    "goal",
    "sort",
    "model",
    "steps",
    "begin",
    "learn",
    "different",
    "types",
    "features",
    "original",
    "image",
    "maybe",
    "first",
    "step",
    "learn",
    "low",
    "level",
    "features",
    "learn",
    "look",
    "features",
    "like",
    "edges",
    "curves",
    "shapes",
    "based",
    "pixels",
    "neighboring",
    "values",
    "figure",
    "right",
    "edges",
    "curves",
    "various",
    "different",
    "shapes",
    "might",
    "present",
    "mapping",
    "represents",
    "edges",
    "curves",
    "shapes",
    "happen",
    "imagine",
    "applying",
    "sort",
    "process",
    "begin",
    "look",
    "higher",
    "level",
    "features",
    "look",
    "objects",
    "maybe",
    "look",
    "people",
    "eyes",
    "facial",
    "recognition",
    "example",
    "maybe",
    "look",
    "complex",
    "shapes",
    "like",
    "curves",
    "particular",
    "number",
    "trying",
    "recognize",
    "digit",
    "handwriting",
    "recognition",
    "sort",
    "scenario",
    "results",
    "represent",
    "higher",
    "level",
    "features",
    "pass",
    "neural",
    "network",
    "really",
    "deep",
    "neural",
    "network",
    "looks",
    "like",
    "might",
    "imagine",
    "making",
    "binary",
    "classification",
    "classifying",
    "multiple",
    "categories",
    "performing",
    "various",
    "different",
    "tasks",
    "sort",
    "model",
    "convolutional",
    "neural",
    "networks",
    "quite",
    "powerful",
    "quite",
    "popular",
    "comes",
    "towards",
    "trying",
    "analyze",
    "images",
    "strictly",
    "need",
    "could",
    "used",
    "vanilla",
    "neural",
    "network",
    "operates",
    "layer",
    "layer",
    "seen",
    "convolutional",
    "neural",
    "networks",
    "quite",
    "helpful",
    "particular",
    "way",
    "model",
    "way",
    "human",
    "might",
    "look",
    "image",
    "instead",
    "human",
    "looking",
    "every",
    "single",
    "pixel",
    "simultaneously",
    "trying",
    "convolve",
    "multiplying",
    "together",
    "might",
    "imagine",
    "convolution",
    "really",
    "looking",
    "various",
    "different",
    "regions",
    "image",
    "extracting",
    "relevant",
    "information",
    "features",
    "parts",
    "image",
    "way",
    "human",
    "might",
    "visual",
    "receptors",
    "looking",
    "particular",
    "parts",
    "see",
    "using",
    "combining",
    "figure",
    "meaning",
    "draw",
    "various",
    "different",
    "inputs",
    "might",
    "imagine",
    "applying",
    "situation",
    "like",
    "handwriting",
    "recognition",
    "go",
    "ahead",
    "see",
    "example",
    "go",
    "ahead",
    "open",
    "first",
    "import",
    "tensorflow",
    "tensorflow",
    "turns",
    "data",
    "sets",
    "built",
    "library",
    "immediately",
    "access",
    "one",
    "famous",
    "data",
    "sets",
    "machine",
    "learning",
    "mnist",
    "data",
    "set",
    "data",
    "set",
    "whole",
    "bunch",
    "samples",
    "people",
    "handwritten",
    "digits",
    "showed",
    "slide",
    "little",
    "ago",
    "immediately",
    "access",
    "data",
    "set",
    "built",
    "library",
    "want",
    "something",
    "like",
    "train",
    "whole",
    "bunch",
    "handwritten",
    "digits",
    "use",
    "data",
    "set",
    "provided",
    "course",
    "data",
    "set",
    "handwritten",
    "images",
    "apply",
    "idea",
    "first",
    "need",
    "take",
    "images",
    "turn",
    "array",
    "pixels",
    "way",
    "going",
    "formatted",
    "going",
    "formatted",
    "effectively",
    "array",
    "individual",
    "pixels",
    "bit",
    "reshaping",
    "need",
    "turning",
    "data",
    "format",
    "put",
    "convolutional",
    "neural",
    "network",
    "things",
    "like",
    "taking",
    "values",
    "dividing",
    "remember",
    "color",
    "values",
    "tend",
    "range",
    "0",
    "divide",
    "255",
    "put",
    "0",
    "1",
    "range",
    "might",
    "little",
    "bit",
    "easier",
    "train",
    "various",
    "modifications",
    "data",
    "get",
    "nice",
    "usable",
    "format",
    "interesting",
    "important",
    "part",
    "create",
    "convolutional",
    "neural",
    "network",
    "cnn",
    "saying",
    "go",
    "ahead",
    "use",
    "sequential",
    "model",
    "could",
    "use",
    "say",
    "add",
    "layer",
    "add",
    "layer",
    "add",
    "layer",
    "another",
    "way",
    "could",
    "define",
    "passing",
    "input",
    "sequential",
    "neural",
    "network",
    "list",
    "layers",
    "want",
    "first",
    "layer",
    "model",
    "convolution",
    "layer",
    "first",
    "going",
    "apply",
    "convolution",
    "image",
    "going",
    "use",
    "13",
    "different",
    "filters",
    "model",
    "going",
    "learn",
    "32",
    "rather",
    "32",
    "different",
    "filters",
    "would",
    "like",
    "learn",
    "input",
    "image",
    "filter",
    "going",
    "3",
    "3",
    "kernel",
    "saw",
    "3",
    "3",
    "kernels",
    "could",
    "multiply",
    "value",
    "3",
    "3",
    "grid",
    "value",
    "multiply",
    "add",
    "results",
    "together",
    "going",
    "learn",
    "32",
    "different",
    "3",
    "3",
    "filters",
    "specify",
    "activation",
    "function",
    "specify",
    "input",
    "shape",
    "input",
    "shape",
    "banknotes",
    "case",
    "4",
    "inputs",
    "input",
    "shape",
    "going",
    "28",
    "28",
    "1",
    "handwritten",
    "digits",
    "turns",
    "mnist",
    "data",
    "set",
    "organizes",
    "data",
    "image",
    "28",
    "28",
    "pixel",
    "grid",
    "going",
    "28",
    "28",
    "pixel",
    "grid",
    "one",
    "images",
    "one",
    "channel",
    "value",
    "handwritten",
    "digits",
    "black",
    "white",
    "single",
    "color",
    "value",
    "representing",
    "much",
    "black",
    "much",
    "white",
    "might",
    "imagine",
    "color",
    "image",
    "sort",
    "thing",
    "might",
    "three",
    "different",
    "channels",
    "red",
    "green",
    "blue",
    "channel",
    "example",
    "case",
    "handwriting",
    "recognition",
    "recognizing",
    "digit",
    "going",
    "use",
    "single",
    "value",
    "like",
    "shaded",
    "shaded",
    "might",
    "range",
    "single",
    "color",
    "value",
    "first",
    "layer",
    "neural",
    "network",
    "convolutional",
    "layer",
    "take",
    "input",
    "learn",
    "whole",
    "bunch",
    "different",
    "filters",
    "apply",
    "input",
    "extract",
    "meaningful",
    "features",
    "next",
    "step",
    "going",
    "max",
    "pooling",
    "layer",
    "also",
    "built",
    "right",
    "tensorflow",
    "going",
    "layer",
    "going",
    "use",
    "pool",
    "size",
    "2",
    "2",
    "meaning",
    "going",
    "look",
    "2",
    "2",
    "regions",
    "inside",
    "image",
    "extract",
    "maximum",
    "value",
    "seen",
    "helpful",
    "help",
    "reduce",
    "size",
    "input",
    "done",
    "go",
    "ahead",
    "flatten",
    "units",
    "single",
    "layer",
    "pass",
    "rest",
    "neural",
    "network",
    "rest",
    "neural",
    "network",
    "saying",
    "let",
    "add",
    "hidden",
    "layer",
    "neural",
    "network",
    "128",
    "units",
    "whole",
    "bunch",
    "hidden",
    "units",
    "inside",
    "hidden",
    "layer",
    "prevent",
    "overfitting",
    "add",
    "dropout",
    "say",
    "know",
    "training",
    "randomly",
    "dropout",
    "half",
    "nodes",
    "hidden",
    "layer",
    "make",
    "sure",
    "become",
    "particular",
    "node",
    "begin",
    "really",
    "generalize",
    "stop",
    "overfitting",
    "tensorflow",
    "allows",
    "us",
    "adding",
    "single",
    "line",
    "add",
    "dropout",
    "model",
    "well",
    "training",
    "perform",
    "dropout",
    "step",
    "order",
    "help",
    "make",
    "sure",
    "overfit",
    "particular",
    "data",
    "finally",
    "add",
    "output",
    "layer",
    "output",
    "layer",
    "going",
    "10",
    "units",
    "one",
    "category",
    "would",
    "like",
    "classify",
    "digits",
    "0",
    "9",
    "10",
    "different",
    "categories",
    "activation",
    "function",
    "going",
    "use",
    "called",
    "softmax",
    "activation",
    "function",
    "short",
    "softmax",
    "activation",
    "function",
    "going",
    "going",
    "take",
    "output",
    "turn",
    "probability",
    "distribution",
    "ultimately",
    "going",
    "tell",
    "estimate",
    "probability",
    "2",
    "versus",
    "3",
    "versus",
    "turn",
    "probability",
    "distribution",
    "next",
    "go",
    "ahead",
    "compile",
    "model",
    "fit",
    "training",
    "data",
    "evaluate",
    "well",
    "neural",
    "network",
    "performs",
    "added",
    "python",
    "program",
    "provided",
    "command",
    "line",
    "argument",
    "like",
    "name",
    "file",
    "going",
    "go",
    "ahead",
    "save",
    "model",
    "file",
    "quite",
    "useful",
    "done",
    "training",
    "step",
    "could",
    "take",
    "time",
    "terms",
    "taking",
    "time",
    "going",
    "data",
    "running",
    "back",
    "propagation",
    "gradient",
    "descent",
    "able",
    "say",
    "right",
    "adjust",
    "weight",
    "particular",
    "model",
    "end",
    "calculating",
    "values",
    "weights",
    "calculating",
    "values",
    "filters",
    "like",
    "remember",
    "information",
    "use",
    "later",
    "tensorflow",
    "allows",
    "us",
    "save",
    "model",
    "file",
    "later",
    "want",
    "use",
    "model",
    "learned",
    "use",
    "weights",
    "learned",
    "make",
    "sort",
    "new",
    "prediction",
    "use",
    "model",
    "already",
    "exists",
    "done",
    "calculation",
    "go",
    "ahead",
    "save",
    "model",
    "file",
    "use",
    "little",
    "bit",
    "later",
    "example",
    "go",
    "digits",
    "going",
    "run",
    "wo",
    "save",
    "time",
    "run",
    "go",
    "ahead",
    "see",
    "happens",
    "happen",
    "need",
    "go",
    "model",
    "order",
    "train",
    "samples",
    "handwritten",
    "digits",
    "mnist",
    "data",
    "set",
    "gives",
    "us",
    "thousands",
    "thousands",
    "sample",
    "handwritten",
    "digits",
    "format",
    "use",
    "order",
    "train",
    "seeing",
    "training",
    "process",
    "unlike",
    "banknotes",
    "case",
    "much",
    "fewer",
    "data",
    "points",
    "data",
    "simple",
    "data",
    "complex",
    "training",
    "process",
    "takes",
    "time",
    "another",
    "one",
    "cases",
    "training",
    "neural",
    "networks",
    "computational",
    "power",
    "important",
    "oftentimes",
    "see",
    "people",
    "wanting",
    "use",
    "sophisticated",
    "gpus",
    "order",
    "efficiently",
    "able",
    "sort",
    "neural",
    "network",
    "training",
    "also",
    "speaks",
    "reason",
    "data",
    "helpful",
    "sample",
    "data",
    "points",
    "better",
    "begin",
    "training",
    "going",
    "different",
    "samples",
    "handwritten",
    "digits",
    "said",
    "going",
    "go",
    "10",
    "times",
    "going",
    "go",
    "data",
    "set",
    "10",
    "times",
    "training",
    "time",
    "hopefully",
    "improving",
    "upon",
    "weights",
    "every",
    "time",
    "run",
    "data",
    "set",
    "see",
    "right",
    "accuracy",
    "time",
    "go",
    "ahead",
    "run",
    "model",
    "first",
    "time",
    "looks",
    "like",
    "got",
    "accuracy",
    "92",
    "digits",
    "correct",
    "based",
    "training",
    "set",
    "increased",
    "96",
    "97",
    "every",
    "time",
    "run",
    "going",
    "see",
    "hopefully",
    "accuracy",
    "improve",
    "continue",
    "try",
    "use",
    "gradient",
    "descent",
    "process",
    "trying",
    "run",
    "algorithm",
    "minimize",
    "loss",
    "get",
    "order",
    "accurately",
    "predict",
    "output",
    "process",
    "learning",
    "weights",
    "learning",
    "features",
    "use",
    "kernel",
    "matrix",
    "use",
    "performing",
    "convolution",
    "step",
    "convolutional",
    "neural",
    "network",
    "first",
    "performing",
    "convolutions",
    "traditional",
    "neural",
    "network",
    "structure",
    "going",
    "learn",
    "individual",
    "steps",
    "well",
    "see",
    "tensorflow",
    "provides",
    "nice",
    "output",
    "telling",
    "many",
    "seconds",
    "left",
    "training",
    "runs",
    "allows",
    "see",
    "well",
    "go",
    "ahead",
    "see",
    "network",
    "performs",
    "looks",
    "like",
    "gone",
    "data",
    "set",
    "seven",
    "times",
    "going",
    "eighth",
    "time",
    "point",
    "accuracy",
    "pretty",
    "high",
    "saw",
    "went",
    "92",
    "97",
    "looks",
    "like",
    "98",
    "point",
    "seems",
    "like",
    "things",
    "starting",
    "level",
    "probably",
    "limit",
    "accurate",
    "ultimately",
    "without",
    "running",
    "risk",
    "overfitting",
    "course",
    "enough",
    "nodes",
    "would",
    "memorize",
    "input",
    "overfit",
    "upon",
    "like",
    "avoid",
    "dropout",
    "help",
    "us",
    "see",
    "almost",
    "done",
    "finishing",
    "training",
    "step",
    "right",
    "finished",
    "training",
    "going",
    "go",
    "ahead",
    "test",
    "us",
    "samples",
    "looks",
    "like",
    "testing",
    "set",
    "accurate",
    "ended",
    "pretty",
    "well",
    "seems",
    "testing",
    "set",
    "see",
    "accurately",
    "predict",
    "handwritten",
    "digits",
    "could",
    "actually",
    "test",
    "written",
    "program",
    "called",
    "using",
    "pygame",
    "pass",
    "model",
    "trained",
    "example",
    "model",
    "using",
    "input",
    "data",
    "see",
    "whether",
    "able",
    "train",
    "convolutional",
    "neural",
    "network",
    "able",
    "predict",
    "handwriting",
    "example",
    "try",
    "like",
    "drawing",
    "handwritten",
    "digit",
    "go",
    "ahead",
    "draw",
    "number",
    "2",
    "example",
    "number",
    "messy",
    "tried",
    "imagine",
    "would",
    "write",
    "program",
    "ifs",
    "thens",
    "able",
    "sort",
    "calculation",
    "would",
    "tricky",
    "press",
    "classify",
    "right",
    "seems",
    "able",
    "correctly",
    "classify",
    "drew",
    "number",
    "go",
    "ahead",
    "reset",
    "try",
    "draw",
    "8",
    "example",
    "press",
    "classify",
    "right",
    "predicts",
    "digit",
    "drew",
    "key",
    "really",
    "begins",
    "show",
    "power",
    "neural",
    "network",
    "somehow",
    "looking",
    "various",
    "different",
    "features",
    "different",
    "pixels",
    "figuring",
    "relevant",
    "features",
    "figuring",
    "combine",
    "get",
    "classification",
    "would",
    "difficult",
    "task",
    "provide",
    "explicit",
    "instructions",
    "computer",
    "use",
    "whole",
    "bunch",
    "ifs",
    "ands",
    "process",
    "pixel",
    "values",
    "figure",
    "handwritten",
    "digit",
    "everyone",
    "going",
    "draw",
    "8s",
    "little",
    "bit",
    "differently",
    "drew",
    "8",
    "would",
    "look",
    "little",
    "bit",
    "different",
    "yet",
    "ideally",
    "want",
    "train",
    "network",
    "robust",
    "enough",
    "begins",
    "learn",
    "patterns",
    "said",
    "structure",
    "network",
    "data",
    "train",
    "network",
    "network",
    "learning",
    "algorithm",
    "tries",
    "figure",
    "optimal",
    "set",
    "weights",
    "optimal",
    "set",
    "filters",
    "use",
    "order",
    "able",
    "accurately",
    "classify",
    "digit",
    "one",
    "category",
    "another",
    "going",
    "show",
    "power",
    "sorts",
    "convolutional",
    "neural",
    "networks",
    "look",
    "use",
    "convolutional",
    "neural",
    "networks",
    "begin",
    "solve",
    "problems",
    "regards",
    "computer",
    "vision",
    "ability",
    "take",
    "image",
    "begin",
    "analyze",
    "type",
    "analysis",
    "might",
    "imagine",
    "happening",
    "cars",
    "able",
    "figure",
    "filters",
    "apply",
    "image",
    "understand",
    "computer",
    "looking",
    "type",
    "idea",
    "might",
    "applied",
    "facial",
    "recognition",
    "social",
    "media",
    "able",
    "determine",
    "recognize",
    "faces",
    "image",
    "well",
    "imagine",
    "neural",
    "network",
    "instead",
    "classifying",
    "one",
    "10",
    "different",
    "digits",
    "could",
    "instead",
    "classify",
    "like",
    "person",
    "person",
    "b",
    "trying",
    "tell",
    "people",
    "apart",
    "based",
    "convolution",
    "take",
    "look",
    "yet",
    "another",
    "type",
    "neural",
    "network",
    "quite",
    "popular",
    "certain",
    "types",
    "tasks",
    "try",
    "generalize",
    "think",
    "neural",
    "network",
    "little",
    "bit",
    "abstractly",
    "sample",
    "deep",
    "neural",
    "network",
    "input",
    "layer",
    "whole",
    "bunch",
    "different",
    "hidden",
    "layers",
    "performing",
    "certain",
    "types",
    "calculations",
    "output",
    "layer",
    "generates",
    "sort",
    "output",
    "care",
    "calculating",
    "could",
    "imagine",
    "representing",
    "little",
    "simply",
    "like",
    "abstract",
    "representation",
    "neural",
    "network",
    "input",
    "might",
    "like",
    "vector",
    "whole",
    "bunch",
    "different",
    "values",
    "input",
    "gets",
    "passed",
    "network",
    "performs",
    "sort",
    "calculation",
    "computation",
    "network",
    "produces",
    "sort",
    "output",
    "output",
    "might",
    "single",
    "value",
    "might",
    "whole",
    "bunch",
    "different",
    "values",
    "general",
    "structure",
    "neural",
    "network",
    "seen",
    "sort",
    "input",
    "gets",
    "fed",
    "network",
    "using",
    "input",
    "network",
    "calculates",
    "output",
    "sort",
    "model",
    "neural",
    "network",
    "might",
    "call",
    "neural",
    "network",
    "neural",
    "networks",
    "connections",
    "one",
    "direction",
    "move",
    "one",
    "layer",
    "next",
    "layer",
    "layer",
    "inputs",
    "pass",
    "various",
    "different",
    "hidden",
    "layers",
    "ultimately",
    "produce",
    "sort",
    "output",
    "neural",
    "networks",
    "helpful",
    "solving",
    "types",
    "classification",
    "problems",
    "saw",
    "whole",
    "bunch",
    "input",
    "want",
    "learn",
    "setting",
    "weights",
    "allow",
    "us",
    "calculate",
    "output",
    "effectively",
    "limitations",
    "neural",
    "networks",
    "see",
    "moment",
    "particular",
    "input",
    "needs",
    "fixed",
    "shape",
    "like",
    "fixed",
    "number",
    "neurons",
    "input",
    "layer",
    "fixed",
    "shape",
    "output",
    "like",
    "fixed",
    "number",
    "neurons",
    "output",
    "layer",
    "limitations",
    "possible",
    "solution",
    "see",
    "examples",
    "types",
    "problems",
    "solve",
    "second",
    "instead",
    "neural",
    "network",
    "connections",
    "one",
    "direction",
    "left",
    "right",
    "effectively",
    "across",
    "network",
    "could",
    "also",
    "imagine",
    "recurrent",
    "neural",
    "network",
    "recurrent",
    "neural",
    "network",
    "generates",
    "output",
    "gets",
    "fed",
    "back",
    "input",
    "future",
    "runs",
    "network",
    "whereas",
    "traditional",
    "neural",
    "network",
    "inputs",
    "get",
    "fed",
    "network",
    "get",
    "fed",
    "output",
    "thing",
    "determines",
    "output",
    "based",
    "original",
    "input",
    "based",
    "calculation",
    "inside",
    "network",
    "goes",
    "contrast",
    "recurrent",
    "neural",
    "network",
    "recurrent",
    "neural",
    "network",
    "imagine",
    "output",
    "network",
    "feeding",
    "back",
    "network",
    "input",
    "next",
    "time",
    "calculations",
    "inside",
    "network",
    "allows",
    "allows",
    "network",
    "maintain",
    "sort",
    "state",
    "store",
    "sort",
    "information",
    "used",
    "future",
    "runs",
    "network",
    "previously",
    "network",
    "defined",
    "weights",
    "passed",
    "inputs",
    "network",
    "generated",
    "outputs",
    "network",
    "saving",
    "information",
    "based",
    "inputs",
    "able",
    "remember",
    "future",
    "iterations",
    "future",
    "runs",
    "recurrent",
    "neural",
    "network",
    "let",
    "us",
    "let",
    "network",
    "store",
    "information",
    "gets",
    "passed",
    "back",
    "input",
    "network",
    "next",
    "time",
    "try",
    "perform",
    "sort",
    "action",
    "particularly",
    "helpful",
    "dealing",
    "sequences",
    "data",
    "see",
    "real",
    "world",
    "example",
    "right",
    "actually",
    "microsoft",
    "developed",
    "ai",
    "known",
    "caption",
    "bot",
    "caption",
    "bot",
    "says",
    "understand",
    "content",
    "photograph",
    "try",
    "describe",
    "well",
    "human",
    "analyze",
    "photo",
    "wo",
    "store",
    "share",
    "microsoft",
    "caption",
    "bot",
    "seems",
    "claiming",
    "take",
    "image",
    "figure",
    "image",
    "give",
    "us",
    "caption",
    "describe",
    "let",
    "try",
    "example",
    "image",
    "harvard",
    "square",
    "people",
    "walking",
    "front",
    "one",
    "buildings",
    "harvard",
    "square",
    "go",
    "ahead",
    "take",
    "url",
    "image",
    "paste",
    "caption",
    "bot",
    "press",
    "go",
    "caption",
    "bot",
    "analyzing",
    "image",
    "says",
    "think",
    "group",
    "people",
    "walking",
    "front",
    "building",
    "seems",
    "amazing",
    "ai",
    "able",
    "look",
    "image",
    "figure",
    "image",
    "important",
    "thing",
    "recognize",
    "longer",
    "classification",
    "task",
    "saw",
    "able",
    "classify",
    "images",
    "convolutional",
    "neural",
    "network",
    "job",
    "take",
    "image",
    "figure",
    "0",
    "1",
    "2",
    "person",
    "face",
    "person",
    "face",
    "seems",
    "happening",
    "input",
    "image",
    "know",
    "get",
    "networks",
    "take",
    "input",
    "images",
    "output",
    "text",
    "sentence",
    "phrase",
    "like",
    "group",
    "people",
    "walking",
    "front",
    "building",
    "would",
    "seem",
    "pose",
    "challenge",
    "traditional",
    "neural",
    "networks",
    "reason",
    "traditional",
    "neural",
    "networks",
    "input",
    "output",
    "certain",
    "number",
    "neurons",
    "input",
    "neural",
    "network",
    "certain",
    "number",
    "outputs",
    "neural",
    "network",
    "calculation",
    "goes",
    "size",
    "inputs",
    "number",
    "values",
    "input",
    "number",
    "values",
    "output",
    "always",
    "going",
    "fixed",
    "based",
    "structure",
    "neural",
    "network",
    "makes",
    "difficult",
    "imagine",
    "neural",
    "network",
    "could",
    "take",
    "image",
    "like",
    "say",
    "group",
    "people",
    "walking",
    "front",
    "building",
    "output",
    "text",
    "like",
    "sequence",
    "words",
    "might",
    "possible",
    "neural",
    "network",
    "output",
    "one",
    "word",
    "one",
    "word",
    "could",
    "represent",
    "vector",
    "values",
    "imagine",
    "ways",
    "next",
    "time",
    "talk",
    "little",
    "bit",
    "ai",
    "relates",
    "language",
    "language",
    "processing",
    "sequence",
    "words",
    "much",
    "challenging",
    "depending",
    "image",
    "might",
    "imagine",
    "output",
    "different",
    "number",
    "words",
    "could",
    "sequences",
    "different",
    "lengths",
    "somehow",
    "still",
    "want",
    "able",
    "generate",
    "appropriate",
    "output",
    "strategy",
    "use",
    "recurrent",
    "neural",
    "network",
    "neural",
    "network",
    "feed",
    "output",
    "back",
    "input",
    "next",
    "time",
    "allows",
    "us",
    "call",
    "relationship",
    "inputs",
    "outputs",
    "vanilla",
    "traditional",
    "neural",
    "networks",
    "might",
    "consider",
    "neural",
    "networks",
    "pass",
    "one",
    "set",
    "values",
    "input",
    "get",
    "one",
    "vector",
    "values",
    "output",
    "case",
    "want",
    "pass",
    "one",
    "value",
    "input",
    "image",
    "want",
    "get",
    "sequence",
    "many",
    "values",
    "output",
    "value",
    "like",
    "one",
    "words",
    "gets",
    "produced",
    "particular",
    "algorithm",
    "way",
    "might",
    "might",
    "imagine",
    "starting",
    "providing",
    "input",
    "image",
    "neural",
    "network",
    "neural",
    "network",
    "going",
    "generate",
    "output",
    "output",
    "going",
    "whole",
    "sequence",
    "words",
    "ca",
    "represent",
    "whole",
    "sequence",
    "words",
    "using",
    "fixed",
    "set",
    "neurons",
    "instead",
    "output",
    "going",
    "first",
    "word",
    "going",
    "train",
    "network",
    "output",
    "first",
    "word",
    "caption",
    "could",
    "imagine",
    "microsoft",
    "trained",
    "running",
    "whole",
    "bunch",
    "training",
    "samples",
    "ai",
    "giving",
    "whole",
    "bunch",
    "pictures",
    "appropriate",
    "caption",
    "ai",
    "begin",
    "learn",
    "network",
    "generates",
    "output",
    "fed",
    "back",
    "could",
    "imagine",
    "output",
    "network",
    "fed",
    "back",
    "network",
    "looks",
    "like",
    "separate",
    "network",
    "really",
    "network",
    "getting",
    "different",
    "input",
    "network",
    "output",
    "gets",
    "fed",
    "back",
    "going",
    "generate",
    "another",
    "output",
    "output",
    "going",
    "second",
    "word",
    "caption",
    "recurrent",
    "neural",
    "network",
    "network",
    "going",
    "generate",
    "output",
    "fed",
    "back",
    "generate",
    "yet",
    "another",
    "word",
    "fed",
    "back",
    "generate",
    "another",
    "word",
    "recurrent",
    "neural",
    "networks",
    "allow",
    "us",
    "represent",
    "structure",
    "provide",
    "one",
    "image",
    "input",
    "neural",
    "network",
    "pass",
    "data",
    "next",
    "run",
    "network",
    "could",
    "run",
    "network",
    "multiple",
    "times",
    "time",
    "generating",
    "different",
    "output",
    "still",
    "based",
    "original",
    "input",
    "recurrent",
    "neural",
    "networks",
    "become",
    "particularly",
    "useful",
    "dealing",
    "sequences",
    "inputs",
    "outputs",
    "output",
    "sequence",
    "words",
    "since",
    "ca",
    "easily",
    "represent",
    "outputting",
    "entire",
    "sequence",
    "words",
    "instead",
    "output",
    "sequence",
    "one",
    "word",
    "time",
    "allowing",
    "network",
    "pass",
    "information",
    "still",
    "needs",
    "said",
    "photo",
    "next",
    "stage",
    "running",
    "network",
    "could",
    "run",
    "network",
    "multiple",
    "times",
    "network",
    "weights",
    "getting",
    "different",
    "input",
    "time",
    "first",
    "getting",
    "input",
    "image",
    "getting",
    "input",
    "network",
    "additional",
    "information",
    "additionally",
    "needs",
    "given",
    "particular",
    "caption",
    "example",
    "relationship",
    "inside",
    "recurrent",
    "neural",
    "network",
    "turns",
    "models",
    "use",
    "ways",
    "try",
    "use",
    "recurrent",
    "neural",
    "networks",
    "able",
    "represent",
    "data",
    "might",
    "stored",
    "forms",
    "well",
    "saw",
    "could",
    "use",
    "neural",
    "networks",
    "order",
    "analyze",
    "images",
    "context",
    "convolutional",
    "neural",
    "networks",
    "take",
    "image",
    "figure",
    "various",
    "different",
    "properties",
    "image",
    "able",
    "draw",
    "sort",
    "conclusion",
    "based",
    "might",
    "imagine",
    "something",
    "like",
    "youtube",
    "need",
    "able",
    "lot",
    "learning",
    "based",
    "video",
    "need",
    "look",
    "videos",
    "detect",
    "like",
    "copyright",
    "violations",
    "need",
    "able",
    "look",
    "videos",
    "maybe",
    "identify",
    "particular",
    "items",
    "inside",
    "video",
    "example",
    "video",
    "might",
    "imagine",
    "much",
    "difficult",
    "put",
    "input",
    "neural",
    "network",
    "whereas",
    "image",
    "could",
    "treat",
    "pixel",
    "different",
    "value",
    "videos",
    "sequences",
    "sequences",
    "images",
    "sequence",
    "might",
    "different",
    "length",
    "might",
    "challenging",
    "represent",
    "entire",
    "video",
    "single",
    "vector",
    "values",
    "could",
    "pass",
    "neural",
    "network",
    "recurrent",
    "neural",
    "networks",
    "valuable",
    "solution",
    "trying",
    "solve",
    "type",
    "problem",
    "instead",
    "passing",
    "single",
    "input",
    "neural",
    "network",
    "could",
    "pass",
    "input",
    "one",
    "frame",
    "time",
    "might",
    "imagine",
    "first",
    "taking",
    "first",
    "frame",
    "video",
    "passing",
    "network",
    "maybe",
    "network",
    "output",
    "anything",
    "yet",
    "let",
    "take",
    "another",
    "input",
    "time",
    "pass",
    "network",
    "network",
    "gets",
    "information",
    "last",
    "time",
    "provided",
    "input",
    "network",
    "pass",
    "third",
    "input",
    "fourth",
    "input",
    "time",
    "network",
    "gets",
    "gets",
    "recent",
    "input",
    "like",
    "frame",
    "video",
    "also",
    "gets",
    "information",
    "network",
    "processed",
    "previous",
    "iterations",
    "frame",
    "number",
    "four",
    "end",
    "getting",
    "input",
    "frame",
    "number",
    "four",
    "plus",
    "information",
    "network",
    "calculated",
    "first",
    "three",
    "frames",
    "using",
    "data",
    "combined",
    "recurrent",
    "neural",
    "network",
    "begin",
    "learn",
    "extract",
    "patterns",
    "sequence",
    "data",
    "well",
    "might",
    "imagine",
    "want",
    "classify",
    "video",
    "number",
    "different",
    "genres",
    "like",
    "educational",
    "video",
    "music",
    "video",
    "different",
    "types",
    "videos",
    "classification",
    "task",
    "want",
    "take",
    "input",
    "frames",
    "video",
    "want",
    "output",
    "something",
    "like",
    "category",
    "happens",
    "belong",
    "imagine",
    "sort",
    "thing",
    "sort",
    "learning",
    "time",
    "input",
    "sequence",
    "input",
    "sequence",
    "context",
    "video",
    "could",
    "context",
    "like",
    "someone",
    "typed",
    "message",
    "want",
    "able",
    "categorize",
    "message",
    "like",
    "trying",
    "take",
    "movie",
    "review",
    "trying",
    "classify",
    "positive",
    "review",
    "negative",
    "review",
    "input",
    "sequence",
    "words",
    "output",
    "classification",
    "positive",
    "negative",
    "recurrent",
    "neural",
    "network",
    "might",
    "helpful",
    "analyzing",
    "sequences",
    "words",
    "quite",
    "popular",
    "comes",
    "dealing",
    "language",
    "could",
    "even",
    "used",
    "spoken",
    "language",
    "well",
    "spoken",
    "language",
    "audio",
    "waveform",
    "segmented",
    "distinct",
    "chunks",
    "could",
    "passed",
    "input",
    "recurrent",
    "neural",
    "network",
    "able",
    "classify",
    "someone",
    "voice",
    "instance",
    "want",
    "voice",
    "recognition",
    "say",
    "one",
    "person",
    "another",
    "also",
    "cases",
    "might",
    "want",
    "architecture",
    "recurrent",
    "neural",
    "network",
    "one",
    "final",
    "problem",
    "take",
    "look",
    "terms",
    "sorts",
    "networks",
    "imagine",
    "google",
    "translate",
    "google",
    "translate",
    "taking",
    "text",
    "written",
    "one",
    "language",
    "converting",
    "text",
    "written",
    "language",
    "example",
    "input",
    "sequence",
    "data",
    "sequence",
    "words",
    "output",
    "sequence",
    "words",
    "well",
    "also",
    "sequence",
    "want",
    "effectively",
    "relationship",
    "input",
    "sequence",
    "output",
    "sequence",
    "well",
    "quite",
    "going",
    "work",
    "say",
    "take",
    "word",
    "input",
    "translate",
    "word",
    "output",
    "ultimately",
    "different",
    "languages",
    "put",
    "words",
    "different",
    "orders",
    "maybe",
    "one",
    "language",
    "uses",
    "two",
    "words",
    "something",
    "whereas",
    "another",
    "language",
    "uses",
    "one",
    "really",
    "want",
    "way",
    "take",
    "information",
    "input",
    "encode",
    "somehow",
    "use",
    "encoding",
    "generate",
    "output",
    "ultimately",
    "one",
    "big",
    "advancements",
    "automated",
    "translation",
    "technology",
    "ability",
    "use",
    "neural",
    "networks",
    "instead",
    "older",
    "traditional",
    "methods",
    "improved",
    "accuracy",
    "dramatically",
    "way",
    "might",
    "imagine",
    "using",
    "recurrent",
    "neural",
    "network",
    "multiple",
    "inputs",
    "multiple",
    "outputs",
    "start",
    "passing",
    "input",
    "input",
    "goes",
    "network",
    "another",
    "input",
    "like",
    "another",
    "word",
    "goes",
    "network",
    "multiple",
    "times",
    "like",
    "word",
    "input",
    "trying",
    "translate",
    "done",
    "network",
    "start",
    "generate",
    "output",
    "like",
    "first",
    "word",
    "translated",
    "sentence",
    "next",
    "word",
    "translated",
    "sentence",
    "forth",
    "time",
    "network",
    "passes",
    "information",
    "allowing",
    "model",
    "giving",
    "sort",
    "state",
    "one",
    "run",
    "network",
    "next",
    "run",
    "assembling",
    "information",
    "inputs",
    "passing",
    "information",
    "part",
    "output",
    "order",
    "generate",
    "next",
    "number",
    "different",
    "types",
    "sorts",
    "recurrent",
    "neural",
    "networks",
    "one",
    "popular",
    "known",
    "long",
    "memory",
    "neural",
    "network",
    "otherwise",
    "known",
    "lstm",
    "general",
    "types",
    "networks",
    "powerful",
    "whenever",
    "dealing",
    "sequences",
    "whether",
    "sequences",
    "images",
    "especially",
    "sequences",
    "words",
    "comes",
    "towards",
    "dealing",
    "natural",
    "language",
    "different",
    "types",
    "neural",
    "networks",
    "used",
    "sorts",
    "different",
    "computations",
    "incredibly",
    "versatile",
    "tools",
    "applied",
    "number",
    "different",
    "domains",
    "looked",
    "couple",
    "popular",
    "types",
    "neural",
    "networks",
    "traditional",
    "neural",
    "networks",
    "convolutional",
    "neural",
    "networks",
    "recurrent",
    "neural",
    "networks",
    "types",
    "well",
    "adversarial",
    "networks",
    "networks",
    "compete",
    "try",
    "able",
    "generate",
    "new",
    "types",
    "data",
    "well",
    "networks",
    "solve",
    "tasks",
    "based",
    "happen",
    "structured",
    "adapted",
    "powerful",
    "tools",
    "machine",
    "learning",
    "able",
    "easily",
    "learn",
    "based",
    "set",
    "input",
    "data",
    "able",
    "therefore",
    "figure",
    "calculate",
    "function",
    "inputs",
    "outputs",
    "whether",
    "input",
    "sort",
    "classification",
    "like",
    "analyzing",
    "image",
    "getting",
    "digit",
    "machine",
    "translation",
    "input",
    "one",
    "language",
    "output",
    "another",
    "tools",
    "lot",
    "applications",
    "machine",
    "learning",
    "generally",
    "next",
    "time",
    "look",
    "machine",
    "learning",
    "ai",
    "particular",
    "context",
    "natural",
    "language",
    "talked",
    "little",
    "bit",
    "today",
    "looking",
    "ai",
    "begin",
    "understand",
    "natural",
    "language",
    "begin",
    "able",
    "analyze",
    "useful",
    "tasks",
    "regards",
    "human",
    "language",
    "turns",
    "challenging",
    "interesting",
    "task",
    "see",
    "next",
    "time",
    "welcome",
    "back",
    "everybody",
    "final",
    "class",
    "introduction",
    "artificial",
    "intelligence",
    "python",
    "far",
    "class",
    "taking",
    "problems",
    "want",
    "solve",
    "intelligently",
    "framing",
    "ways",
    "computers",
    "going",
    "able",
    "make",
    "sense",
    "taking",
    "problems",
    "framing",
    "search",
    "problems",
    "constraint",
    "satisfaction",
    "problems",
    "optimization",
    "problems",
    "example",
    "essence",
    "trying",
    "communicate",
    "problems",
    "ways",
    "computer",
    "going",
    "able",
    "understand",
    "today",
    "goal",
    "going",
    "get",
    "computers",
    "understand",
    "way",
    "communicate",
    "naturally",
    "via",
    "natural",
    "languages",
    "languages",
    "like",
    "english",
    "natural",
    "language",
    "contains",
    "lot",
    "nuance",
    "complexity",
    "going",
    "make",
    "challenging",
    "computers",
    "able",
    "understand",
    "need",
    "explore",
    "new",
    "tools",
    "new",
    "techniques",
    "allow",
    "computers",
    "make",
    "sense",
    "natural",
    "language",
    "exactly",
    "trying",
    "get",
    "computers",
    "well",
    "fall",
    "general",
    "heading",
    "natural",
    "language",
    "processing",
    "getting",
    "computers",
    "work",
    "natural",
    "language",
    "tasks",
    "include",
    "tasks",
    "like",
    "automatic",
    "summarization",
    "given",
    "long",
    "text",
    "train",
    "computer",
    "able",
    "come",
    "shorter",
    "representation",
    "information",
    "extraction",
    "getting",
    "computer",
    "pull",
    "relevant",
    "facts",
    "details",
    "text",
    "machine",
    "translation",
    "like",
    "google",
    "translate",
    "translating",
    "text",
    "one",
    "language",
    "another",
    "language",
    "question",
    "answering",
    "ever",
    "asked",
    "question",
    "phone",
    "conversation",
    "ai",
    "chatbot",
    "provide",
    "text",
    "computer",
    "computer",
    "able",
    "understand",
    "text",
    "generate",
    "text",
    "response",
    "text",
    "classification",
    "provide",
    "text",
    "computer",
    "computer",
    "assigns",
    "label",
    "positive",
    "negative",
    "inbox",
    "spam",
    "example",
    "several",
    "kinds",
    "tasks",
    "fall",
    "heading",
    "natural",
    "language",
    "processing",
    "take",
    "look",
    "computer",
    "might",
    "try",
    "solve",
    "kinds",
    "tasks",
    "might",
    "useful",
    "us",
    "think",
    "language",
    "general",
    "kinds",
    "challenges",
    "might",
    "need",
    "deal",
    "start",
    "think",
    "language",
    "getting",
    "computer",
    "able",
    "understand",
    "one",
    "part",
    "language",
    "need",
    "consider",
    "syntax",
    "language",
    "syntax",
    "structure",
    "language",
    "language",
    "composed",
    "individual",
    "words",
    "words",
    "composed",
    "together",
    "kind",
    "structured",
    "whole",
    "computer",
    "going",
    "able",
    "understand",
    "language",
    "going",
    "need",
    "understand",
    "something",
    "structure",
    "let",
    "take",
    "couple",
    "examples",
    "instance",
    "sentence",
    "9",
    "sherlock",
    "holmes",
    "stepped",
    "briskly",
    "room",
    "sentence",
    "made",
    "words",
    "words",
    "together",
    "form",
    "structured",
    "whole",
    "syntactically",
    "valid",
    "sentence",
    "could",
    "take",
    "words",
    "rearrange",
    "come",
    "sentence",
    "syntactically",
    "valid",
    "example",
    "sherlock",
    "holmes",
    "9",
    "stepped",
    "briskly",
    "room",
    "still",
    "composed",
    "valid",
    "words",
    "kind",
    "logical",
    "whole",
    "syntactically",
    "sentence",
    "another",
    "interesting",
    "challenge",
    "sentences",
    "multiple",
    "possible",
    "valid",
    "structures",
    "sentence",
    "example",
    "saw",
    "man",
    "mountain",
    "telescope",
    "valid",
    "sentence",
    "actually",
    "two",
    "different",
    "possible",
    "structures",
    "lend",
    "two",
    "different",
    "interpretations",
    "two",
    "different",
    "meanings",
    "maybe",
    "one",
    "seeing",
    "one",
    "telescope",
    "maybe",
    "man",
    "mountain",
    "one",
    "telescope",
    "natural",
    "language",
    "ambiguous",
    "sometimes",
    "sentence",
    "interpreted",
    "multiple",
    "ways",
    "something",
    "need",
    "think",
    "well",
    "lends",
    "another",
    "problem",
    "within",
    "language",
    "need",
    "think",
    "semantics",
    "syntax",
    "structure",
    "language",
    "semantics",
    "meaning",
    "language",
    "enough",
    "computer",
    "know",
    "sentence",
    "know",
    "sentence",
    "means",
    "semantics",
    "going",
    "concern",
    "meaning",
    "words",
    "meaning",
    "sentences",
    "go",
    "back",
    "sentence",
    "9",
    "sherlock",
    "holmes",
    "stepped",
    "briskly",
    "room",
    "could",
    "come",
    "another",
    "sentence",
    "say",
    "sentence",
    "minutes",
    "9",
    "sherlock",
    "holmes",
    "walked",
    "quickly",
    "room",
    "two",
    "different",
    "sentences",
    "words",
    "words",
    "different",
    "two",
    "sentences",
    "essentially",
    "meaning",
    "ideally",
    "whatever",
    "model",
    "build",
    "able",
    "understand",
    "two",
    "sentences",
    "different",
    "mean",
    "something",
    "similar",
    "syntactically",
    "sentences",
    "mean",
    "anything",
    "famous",
    "example",
    "linguist",
    "noam",
    "chomsky",
    "sentence",
    "colorless",
    "green",
    "ideas",
    "sleep",
    "furiously",
    "syntactically",
    "structurally",
    "sentence",
    "got",
    "adjectives",
    "modifying",
    "noun",
    "ideas",
    "got",
    "verb",
    "adverb",
    "correct",
    "positions",
    "taken",
    "whole",
    "sentence",
    "really",
    "mean",
    "anything",
    "computers",
    "going",
    "able",
    "work",
    "natural",
    "language",
    "perform",
    "tasks",
    "natural",
    "language",
    "processing",
    "concerns",
    "need",
    "think",
    "need",
    "thinking",
    "syntax",
    "need",
    "thinking",
    "semantics",
    "could",
    "go",
    "trying",
    "teach",
    "computer",
    "understand",
    "structure",
    "natural",
    "language",
    "well",
    "one",
    "approach",
    "might",
    "take",
    "starting",
    "thinking",
    "rules",
    "natural",
    "language",
    "natural",
    "languages",
    "rules",
    "english",
    "example",
    "nouns",
    "tend",
    "come",
    "verbs",
    "nouns",
    "modified",
    "adjectives",
    "example",
    "could",
    "formalize",
    "rules",
    "could",
    "give",
    "rules",
    "computer",
    "computer",
    "would",
    "able",
    "make",
    "sense",
    "understand",
    "let",
    "try",
    "exactly",
    "going",
    "try",
    "define",
    "formal",
    "grammar",
    "formal",
    "grammar",
    "system",
    "rules",
    "generating",
    "sentences",
    "language",
    "going",
    "approach",
    "natural",
    "language",
    "processing",
    "going",
    "give",
    "computer",
    "rules",
    "know",
    "language",
    "computer",
    "use",
    "rules",
    "make",
    "sense",
    "structure",
    "language",
    "number",
    "different",
    "types",
    "formal",
    "grammars",
    "one",
    "slightly",
    "different",
    "use",
    "cases",
    "today",
    "going",
    "focus",
    "specifically",
    "one",
    "kind",
    "grammar",
    "known",
    "grammar",
    "grammar",
    "work",
    "well",
    "sentence",
    "might",
    "want",
    "computer",
    "generate",
    "saw",
    "city",
    "going",
    "call",
    "words",
    "terminal",
    "symbol",
    "terminal",
    "symbol",
    "computer",
    "generated",
    "word",
    "nothing",
    "else",
    "generate",
    "generated",
    "sentence",
    "computer",
    "done",
    "going",
    "associate",
    "terminal",
    "symbols",
    "symbol",
    "generates",
    "got",
    "n",
    "stands",
    "noun",
    "like",
    "city",
    "got",
    "v",
    "symbol",
    "stands",
    "verb",
    "stands",
    "determiner",
    "determiner",
    "word",
    "like",
    "english",
    "example",
    "symbols",
    "generate",
    "terminal",
    "symbols",
    "ultimately",
    "care",
    "generating",
    "know",
    "computer",
    "know",
    "symbols",
    "associated",
    "terminal",
    "symbols",
    "well",
    "need",
    "kind",
    "rule",
    "call",
    "rewriting",
    "rules",
    "symbol",
    "side",
    "arrow",
    "right",
    "side",
    "symbol",
    "replaced",
    "saying",
    "symbol",
    "n",
    "stands",
    "noun",
    "could",
    "replaced",
    "options",
    "separated",
    "vertical",
    "bars",
    "n",
    "could",
    "replaced",
    "city",
    "car",
    "hairy",
    "determiner",
    "could",
    "replaced",
    "forth",
    "symbols",
    "could",
    "replaced",
    "words",
    "also",
    "symbols",
    "replaced",
    "symbols",
    "interesting",
    "rule",
    "np",
    "arrow",
    "n",
    "bar",
    "dn",
    "mean",
    "well",
    "np",
    "stands",
    "noun",
    "phrase",
    "sometimes",
    "noun",
    "phrase",
    "sentence",
    "single",
    "word",
    "could",
    "multiple",
    "words",
    "saying",
    "noun",
    "phrase",
    "could",
    "noun",
    "could",
    "determiner",
    "followed",
    "noun",
    "might",
    "noun",
    "phrase",
    "noun",
    "like",
    "noun",
    "phrase",
    "could",
    "noun",
    "phrase",
    "multiple",
    "words",
    "something",
    "like",
    "city",
    "also",
    "acts",
    "noun",
    "phrase",
    "case",
    "composed",
    "two",
    "words",
    "determiner",
    "noun",
    "city",
    "could",
    "verb",
    "phrases",
    "verb",
    "phrase",
    "vp",
    "might",
    "verb",
    "might",
    "verb",
    "followed",
    "noun",
    "phrase",
    "could",
    "verb",
    "phrase",
    "single",
    "word",
    "like",
    "word",
    "walked",
    "could",
    "verb",
    "phrase",
    "entire",
    "phrase",
    "something",
    "like",
    "saw",
    "city",
    "entire",
    "verb",
    "phrase",
    "sentence",
    "meanwhile",
    "might",
    "define",
    "noun",
    "phrase",
    "followed",
    "verb",
    "phrase",
    "would",
    "allow",
    "us",
    "generate",
    "sentence",
    "like",
    "saw",
    "city",
    "entire",
    "sentence",
    "made",
    "noun",
    "phrase",
    "word",
    "verb",
    "phrase",
    "saw",
    "city",
    "saw",
    "verb",
    "city",
    "also",
    "noun",
    "phrase",
    "could",
    "give",
    "rules",
    "computer",
    "explaining",
    "symbols",
    "could",
    "replaced",
    "symbols",
    "computer",
    "could",
    "take",
    "sentence",
    "begin",
    "understand",
    "structure",
    "sentence",
    "let",
    "take",
    "look",
    "example",
    "might",
    "going",
    "use",
    "python",
    "library",
    "called",
    "nltk",
    "natural",
    "language",
    "toolkit",
    "see",
    "couple",
    "times",
    "today",
    "contains",
    "lot",
    "helpful",
    "features",
    "functions",
    "use",
    "trying",
    "deal",
    "process",
    "natural",
    "language",
    "take",
    "look",
    "use",
    "nltk",
    "order",
    "parse",
    "grammar",
    "let",
    "go",
    "ahead",
    "open",
    "cfg",
    "standing",
    "grammar",
    "see",
    "file",
    "first",
    "import",
    "nltk",
    "natural",
    "language",
    "toolkit",
    "first",
    "thing",
    "define",
    "grammar",
    "saying",
    "sentence",
    "noun",
    "phrase",
    "followed",
    "verb",
    "phrase",
    "defining",
    "noun",
    "phrase",
    "defining",
    "verb",
    "phrase",
    "giving",
    "examples",
    "symbols",
    "determiner",
    "n",
    "noun",
    "v",
    "verb",
    "going",
    "use",
    "nltk",
    "parse",
    "grammar",
    "ask",
    "user",
    "input",
    "form",
    "sentence",
    "split",
    "words",
    "use",
    "grammar",
    "parser",
    "try",
    "parse",
    "sentence",
    "print",
    "resulting",
    "syntax",
    "tree",
    "let",
    "take",
    "look",
    "example",
    "go",
    "ahead",
    "go",
    "cfg",
    "directory",
    "run",
    "asked",
    "type",
    "sentence",
    "let",
    "say",
    "type",
    "walked",
    "see",
    "walked",
    "valid",
    "sentence",
    "noun",
    "phrase",
    "walked",
    "corresponding",
    "verb",
    "phrase",
    "could",
    "try",
    "complex",
    "sentence",
    "could",
    "something",
    "like",
    "saw",
    "city",
    "see",
    "noun",
    "phrase",
    "saw",
    "city",
    "entire",
    "verb",
    "phrase",
    "makes",
    "sentence",
    "simple",
    "grammar",
    "let",
    "take",
    "look",
    "slightly",
    "complex",
    "grammar",
    "sentence",
    "still",
    "noun",
    "phrase",
    "followed",
    "verb",
    "phrase",
    "added",
    "possible",
    "symbols",
    "ap",
    "adjective",
    "phrase",
    "pp",
    "prepositional",
    "phrase",
    "specified",
    "could",
    "adjective",
    "phrase",
    "noun",
    "phrase",
    "prepositional",
    "phrase",
    "noun",
    "example",
    "lots",
    "additional",
    "ways",
    "might",
    "try",
    "structure",
    "sentence",
    "interpret",
    "parse",
    "one",
    "resulting",
    "sentences",
    "let",
    "see",
    "one",
    "action",
    "go",
    "ahead",
    "run",
    "new",
    "grammar",
    "try",
    "sentence",
    "like",
    "saw",
    "wide",
    "street",
    "python",
    "nltk",
    "able",
    "parse",
    "sentence",
    "identify",
    "saw",
    "wide",
    "street",
    "particular",
    "structure",
    "sentence",
    "noun",
    "phrase",
    "verb",
    "phrase",
    "verb",
    "phrase",
    "noun",
    "phrase",
    "within",
    "contains",
    "adjective",
    "able",
    "get",
    "sense",
    "structure",
    "language",
    "actually",
    "let",
    "try",
    "another",
    "example",
    "let",
    "say",
    "saw",
    "dog",
    "binoculars",
    "try",
    "sentence",
    "get",
    "one",
    "possible",
    "syntax",
    "tree",
    "saw",
    "dog",
    "binoculars",
    "notice",
    "sentence",
    "actually",
    "little",
    "bit",
    "ambiguous",
    "natural",
    "language",
    "binoculars",
    "binoculars",
    "dog",
    "binoculars",
    "nltk",
    "able",
    "identify",
    "possible",
    "structures",
    "sentence",
    "case",
    "dog",
    "binoculars",
    "entire",
    "noun",
    "phrase",
    "underneath",
    "np",
    "dog",
    "binoculars",
    "also",
    "got",
    "alternative",
    "parse",
    "tree",
    "dog",
    "noun",
    "phrase",
    "binoculars",
    "prepositional",
    "phrase",
    "modifying",
    "saw",
    "saw",
    "dog",
    "used",
    "binoculars",
    "order",
    "see",
    "dog",
    "well",
    "allows",
    "us",
    "get",
    "sense",
    "structure",
    "natural",
    "language",
    "relies",
    "us",
    "writing",
    "rules",
    "would",
    "take",
    "lot",
    "effort",
    "write",
    "rules",
    "possible",
    "sentence",
    "someone",
    "might",
    "write",
    "say",
    "english",
    "language",
    "language",
    "complicated",
    "result",
    "going",
    "complex",
    "rules",
    "else",
    "might",
    "try",
    "might",
    "try",
    "take",
    "statistical",
    "lens",
    "towards",
    "approaching",
    "problem",
    "natural",
    "language",
    "processing",
    "able",
    "give",
    "computer",
    "lot",
    "existing",
    "data",
    "sentences",
    "written",
    "english",
    "language",
    "could",
    "try",
    "learn",
    "data",
    "well",
    "might",
    "difficult",
    "try",
    "interpret",
    "long",
    "pieces",
    "text",
    "instead",
    "might",
    "want",
    "break",
    "longer",
    "text",
    "smaller",
    "pieces",
    "information",
    "instead",
    "particular",
    "might",
    "try",
    "create",
    "longer",
    "sequence",
    "text",
    "contiguous",
    "sequence",
    "n",
    "items",
    "sample",
    "text",
    "might",
    "n",
    "characters",
    "row",
    "n",
    "words",
    "row",
    "example",
    "let",
    "take",
    "passage",
    "sherlock",
    "holmes",
    "let",
    "look",
    "trigrams",
    "trigram",
    "n",
    "equal",
    "case",
    "looking",
    "sequences",
    "three",
    "words",
    "row",
    "trigrams",
    "would",
    "phrases",
    "like",
    "often",
    "three",
    "words",
    "row",
    "often",
    "another",
    "trigram",
    "said",
    "said",
    "said",
    "trigrams",
    "sequences",
    "three",
    "words",
    "appear",
    "sequence",
    "could",
    "give",
    "computer",
    "large",
    "corpus",
    "text",
    "pull",
    "trigrams",
    "case",
    "could",
    "get",
    "sense",
    "sequences",
    "three",
    "words",
    "tend",
    "appear",
    "next",
    "natural",
    "language",
    "result",
    "get",
    "sense",
    "structure",
    "language",
    "actually",
    "let",
    "take",
    "look",
    "example",
    "use",
    "nltk",
    "try",
    "get",
    "access",
    "information",
    "going",
    "open",
    "python",
    "program",
    "going",
    "load",
    "corpus",
    "data",
    "text",
    "files",
    "computer",
    "memory",
    "going",
    "use",
    "nltk",
    "ngrams",
    "function",
    "going",
    "go",
    "corpus",
    "text",
    "pulling",
    "ngrams",
    "particular",
    "value",
    "using",
    "python",
    "counter",
    "class",
    "going",
    "figure",
    "common",
    "ngrams",
    "inside",
    "entire",
    "corpus",
    "text",
    "going",
    "need",
    "data",
    "set",
    "order",
    "prepared",
    "data",
    "set",
    "stories",
    "sherlock",
    "holmes",
    "bunch",
    "text",
    "files",
    "lot",
    "words",
    "analyze",
    "result",
    "get",
    "sense",
    "sequences",
    "two",
    "words",
    "three",
    "words",
    "tend",
    "common",
    "natural",
    "language",
    "let",
    "give",
    "try",
    "go",
    "ngrams",
    "directory",
    "run",
    "try",
    "n",
    "value",
    "looking",
    "sequences",
    "two",
    "words",
    "row",
    "use",
    "corpus",
    "stories",
    "sherlock",
    "holmes",
    "run",
    "program",
    "get",
    "list",
    "common",
    "ngrams",
    "n",
    "equal",
    "2",
    "otherwise",
    "known",
    "bigram",
    "common",
    "one",
    "sequence",
    "two",
    "words",
    "appears",
    "quite",
    "frequently",
    "natural",
    "language",
    "common",
    "sequences",
    "two",
    "words",
    "appear",
    "row",
    "let",
    "instead",
    "try",
    "running",
    "ngrams",
    "n",
    "equal",
    "let",
    "get",
    "trigrams",
    "see",
    "get",
    "see",
    "common",
    "trigrams",
    "one",
    "think",
    "sequences",
    "three",
    "words",
    "appear",
    "quite",
    "frequently",
    "able",
    "essentially",
    "via",
    "process",
    "known",
    "tokenization",
    "tokenization",
    "process",
    "splitting",
    "sequence",
    "characters",
    "pieces",
    "case",
    "splitting",
    "long",
    "sequence",
    "text",
    "individual",
    "words",
    "looking",
    "sequences",
    "words",
    "get",
    "sense",
    "structure",
    "natural",
    "language",
    "done",
    "done",
    "tokenization",
    "built",
    "corpus",
    "ngrams",
    "information",
    "one",
    "thing",
    "might",
    "try",
    "could",
    "build",
    "markov",
    "chain",
    "might",
    "recall",
    "talked",
    "probability",
    "recall",
    "markov",
    "chain",
    "sequence",
    "values",
    "predict",
    "one",
    "value",
    "based",
    "values",
    "came",
    "result",
    "know",
    "common",
    "ngrams",
    "english",
    "language",
    "words",
    "tend",
    "associated",
    "words",
    "sequence",
    "use",
    "predict",
    "word",
    "might",
    "come",
    "next",
    "sequence",
    "words",
    "could",
    "build",
    "markov",
    "chain",
    "language",
    "order",
    "try",
    "generate",
    "natural",
    "language",
    "follows",
    "statistical",
    "patterns",
    "input",
    "data",
    "let",
    "take",
    "look",
    "build",
    "markov",
    "chain",
    "natural",
    "language",
    "input",
    "going",
    "use",
    "works",
    "william",
    "shakespeare",
    "file",
    "bunch",
    "works",
    "william",
    "shakespeare",
    "long",
    "text",
    "file",
    "plenty",
    "data",
    "analyze",
    "using",
    "third",
    "party",
    "python",
    "library",
    "order",
    "analysis",
    "going",
    "read",
    "sample",
    "text",
    "going",
    "train",
    "markov",
    "model",
    "based",
    "text",
    "going",
    "markov",
    "chain",
    "generate",
    "sentences",
    "going",
    "generate",
    "sentence",
    "appear",
    "original",
    "text",
    "follows",
    "statistical",
    "patterns",
    "generating",
    "based",
    "ngrams",
    "trying",
    "predict",
    "word",
    "likely",
    "come",
    "next",
    "would",
    "expect",
    "based",
    "statistical",
    "patterns",
    "go",
    "ahead",
    "go",
    "markov",
    "directory",
    "run",
    "generator",
    "works",
    "william",
    "shakespeare",
    "input",
    "going",
    "get",
    "five",
    "new",
    "sentences",
    "sentences",
    "necessarily",
    "sentences",
    "original",
    "input",
    "text",
    "follow",
    "statistical",
    "patterns",
    "predicting",
    "word",
    "likely",
    "come",
    "next",
    "based",
    "input",
    "data",
    "seen",
    "types",
    "words",
    "tend",
    "appear",
    "sequence",
    "able",
    "generate",
    "sentences",
    "course",
    "far",
    "guarantee",
    "sentences",
    "generated",
    "actually",
    "mean",
    "anything",
    "make",
    "sense",
    "happen",
    "follow",
    "statistical",
    "patterns",
    "computer",
    "already",
    "aware",
    "return",
    "issue",
    "generate",
    "text",
    "perhaps",
    "accurate",
    "meaningful",
    "way",
    "little",
    "bit",
    "later",
    "let",
    "turn",
    "attention",
    "slightly",
    "different",
    "problem",
    "problem",
    "text",
    "classification",
    "text",
    "classification",
    "problem",
    "text",
    "want",
    "put",
    "text",
    "kind",
    "category",
    "want",
    "apply",
    "sort",
    "label",
    "text",
    "kind",
    "problem",
    "shows",
    "wide",
    "variety",
    "places",
    "commonplace",
    "might",
    "email",
    "inbox",
    "example",
    "get",
    "email",
    "want",
    "computer",
    "able",
    "identify",
    "whether",
    "email",
    "belongs",
    "inbox",
    "whether",
    "filtered",
    "spam",
    "need",
    "classify",
    "text",
    "good",
    "email",
    "spam",
    "another",
    "common",
    "use",
    "case",
    "sentiment",
    "analysis",
    "might",
    "want",
    "know",
    "whether",
    "sentiment",
    "text",
    "positive",
    "negative",
    "might",
    "comes",
    "situations",
    "like",
    "product",
    "reviews",
    "might",
    "bunch",
    "reviews",
    "product",
    "website",
    "grandson",
    "loved",
    "much",
    "fun",
    "product",
    "broke",
    "days",
    "one",
    "best",
    "games",
    "played",
    "long",
    "time",
    "kind",
    "cheap",
    "flimsy",
    "worth",
    "example",
    "sentences",
    "might",
    "see",
    "product",
    "review",
    "website",
    "could",
    "pretty",
    "easily",
    "look",
    "list",
    "product",
    "reviews",
    "decide",
    "ones",
    "positive",
    "ones",
    "negative",
    "might",
    "say",
    "first",
    "one",
    "third",
    "one",
    "seem",
    "like",
    "positive",
    "sentiment",
    "messages",
    "second",
    "one",
    "fourth",
    "one",
    "seem",
    "like",
    "negative",
    "sentiment",
    "messages",
    "know",
    "could",
    "train",
    "computer",
    "able",
    "figure",
    "well",
    "well",
    "might",
    "clued",
    "eye",
    "particular",
    "key",
    "words",
    "particular",
    "words",
    "tend",
    "mean",
    "something",
    "positive",
    "negative",
    "might",
    "identified",
    "words",
    "like",
    "loved",
    "fun",
    "best",
    "tend",
    "associated",
    "positive",
    "messages",
    "words",
    "like",
    "broke",
    "cheap",
    "flimsy",
    "tend",
    "associated",
    "negative",
    "messages",
    "could",
    "train",
    "computer",
    "able",
    "learn",
    "words",
    "tend",
    "associated",
    "positive",
    "versus",
    "negative",
    "messages",
    "maybe",
    "could",
    "train",
    "computer",
    "kind",
    "sentiment",
    "analysis",
    "well",
    "going",
    "try",
    "going",
    "use",
    "model",
    "known",
    "bag",
    "words",
    "model",
    "model",
    "represents",
    "text",
    "unordered",
    "collection",
    "words",
    "purpose",
    "model",
    "going",
    "worry",
    "sequence",
    "ordering",
    "words",
    "word",
    "came",
    "first",
    "second",
    "third",
    "going",
    "treat",
    "text",
    "collection",
    "words",
    "particular",
    "order",
    "losing",
    "information",
    "right",
    "order",
    "words",
    "important",
    "come",
    "back",
    "little",
    "bit",
    "later",
    "simplify",
    "model",
    "help",
    "us",
    "tremendously",
    "think",
    "text",
    "unordered",
    "collection",
    "words",
    "particular",
    "going",
    "use",
    "bag",
    "words",
    "model",
    "build",
    "something",
    "known",
    "naive",
    "bayes",
    "classifier",
    "naive",
    "bayes",
    "classifier",
    "well",
    "tool",
    "going",
    "allow",
    "us",
    "classify",
    "text",
    "based",
    "bayes",
    "rule",
    "might",
    "remember",
    "talked",
    "probability",
    "bayes",
    "rule",
    "says",
    "probability",
    "b",
    "given",
    "equal",
    "probability",
    "given",
    "b",
    "multiplied",
    "probability",
    "b",
    "divided",
    "probability",
    "going",
    "use",
    "rule",
    "able",
    "analyze",
    "text",
    "well",
    "interested",
    "interested",
    "probability",
    "message",
    "positive",
    "sentiment",
    "probability",
    "message",
    "negative",
    "sentiment",
    "simplicity",
    "going",
    "represent",
    "emoji",
    "happy",
    "face",
    "frown",
    "face",
    "positive",
    "negative",
    "sentiment",
    "review",
    "something",
    "like",
    "grandson",
    "loved",
    "interested",
    "probability",
    "message",
    "positive",
    "sentiment",
    "conditional",
    "probability",
    "message",
    "positive",
    "sentiment",
    "given",
    "message",
    "grandson",
    "loved",
    "go",
    "calculating",
    "value",
    "probability",
    "message",
    "positive",
    "given",
    "review",
    "sequence",
    "words",
    "well",
    "bag",
    "words",
    "model",
    "comes",
    "rather",
    "treat",
    "review",
    "string",
    "sequence",
    "words",
    "order",
    "going",
    "treat",
    "unordered",
    "collection",
    "words",
    "going",
    "try",
    "calculate",
    "probability",
    "review",
    "positive",
    "given",
    "words",
    "grandson",
    "loved",
    "review",
    "particular",
    "order",
    "unordered",
    "collection",
    "words",
    "conditional",
    "probability",
    "apply",
    "bayes",
    "rule",
    "try",
    "make",
    "sense",
    "according",
    "bayes",
    "rule",
    "conditional",
    "probability",
    "equal",
    "equal",
    "probability",
    "four",
    "words",
    "review",
    "given",
    "review",
    "positive",
    "multiplied",
    "probability",
    "review",
    "positive",
    "divided",
    "probability",
    "words",
    "happen",
    "review",
    "value",
    "going",
    "try",
    "calculate",
    "one",
    "thing",
    "might",
    "notice",
    "denominator",
    "probability",
    "words",
    "appear",
    "review",
    "actually",
    "depend",
    "whether",
    "looking",
    "positive",
    "sentiment",
    "negative",
    "sentiment",
    "case",
    "actually",
    "get",
    "rid",
    "denominator",
    "need",
    "calculate",
    "say",
    "probability",
    "proportional",
    "numerator",
    "end",
    "going",
    "need",
    "normalize",
    "probability",
    "distribution",
    "make",
    "sure",
    "values",
    "sum",
    "value",
    "calculate",
    "value",
    "well",
    "probability",
    "words",
    "given",
    "positive",
    "times",
    "probability",
    "positive",
    "definition",
    "joint",
    "probability",
    "one",
    "big",
    "joint",
    "probability",
    "probability",
    "things",
    "case",
    "positive",
    "review",
    "four",
    "words",
    "review",
    "still",
    "entirely",
    "obvious",
    "calculate",
    "value",
    "need",
    "make",
    "one",
    "assumption",
    "naive",
    "part",
    "naive",
    "bayes",
    "comes",
    "going",
    "make",
    "assumption",
    "words",
    "independent",
    "mean",
    "word",
    "grandson",
    "review",
    "change",
    "probability",
    "word",
    "loved",
    "review",
    "word",
    "review",
    "example",
    "practice",
    "assumption",
    "might",
    "true",
    "almost",
    "certainly",
    "case",
    "probability",
    "words",
    "depend",
    "going",
    "simplify",
    "analysis",
    "still",
    "give",
    "us",
    "reasonably",
    "good",
    "results",
    "assume",
    "words",
    "independent",
    "depend",
    "whether",
    "positive",
    "negative",
    "might",
    "example",
    "expect",
    "word",
    "loved",
    "appear",
    "often",
    "positive",
    "review",
    "negative",
    "review",
    "mean",
    "well",
    "make",
    "assumption",
    "say",
    "value",
    "probability",
    "interested",
    "directly",
    "proportional",
    "naively",
    "proportional",
    "value",
    "probability",
    "review",
    "positive",
    "times",
    "probability",
    "review",
    "given",
    "positive",
    "times",
    "probability",
    "grandson",
    "review",
    "given",
    "positive",
    "two",
    "words",
    "happen",
    "review",
    "value",
    "looks",
    "little",
    "complex",
    "actually",
    "value",
    "calculate",
    "pretty",
    "easily",
    "going",
    "estimate",
    "probability",
    "review",
    "positive",
    "well",
    "training",
    "data",
    "example",
    "data",
    "example",
    "reviews",
    "one",
    "already",
    "labeled",
    "positive",
    "negative",
    "estimate",
    "probability",
    "review",
    "positive",
    "counting",
    "number",
    "positive",
    "samples",
    "dividing",
    "total",
    "number",
    "samples",
    "training",
    "data",
    "conditional",
    "probabilities",
    "probability",
    "loved",
    "given",
    "positive",
    "well",
    "going",
    "number",
    "positive",
    "samples",
    "loved",
    "divided",
    "total",
    "number",
    "positive",
    "samples",
    "let",
    "take",
    "look",
    "actual",
    "example",
    "see",
    "could",
    "try",
    "calculate",
    "values",
    "put",
    "together",
    "sample",
    "data",
    "way",
    "interpret",
    "sample",
    "data",
    "based",
    "training",
    "data",
    "49",
    "reviews",
    "positive",
    "51",
    "negative",
    "table",
    "conditional",
    "probabilities",
    "review",
    "positive",
    "30",
    "chance",
    "appears",
    "review",
    "negative",
    "20",
    "chance",
    "appears",
    "based",
    "training",
    "data",
    "among",
    "positive",
    "reviews",
    "1",
    "contain",
    "word",
    "grandson",
    "among",
    "negative",
    "reviews",
    "2",
    "contain",
    "word",
    "grandson",
    "using",
    "data",
    "let",
    "try",
    "calculate",
    "value",
    "value",
    "interested",
    "need",
    "multiply",
    "values",
    "together",
    "probability",
    "positive",
    "positive",
    "conditional",
    "probabilities",
    "get",
    "value",
    "thing",
    "negative",
    "case",
    "going",
    "thing",
    "take",
    "probability",
    "negative",
    "multiply",
    "conditional",
    "probabilities",
    "going",
    "get",
    "value",
    "values",
    "sum",
    "one",
    "probability",
    "distribution",
    "yet",
    "normalize",
    "get",
    "values",
    "tells",
    "going",
    "predict",
    "grandson",
    "loved",
    "think",
    "68",
    "chance",
    "probability",
    "positive",
    "sentiment",
    "review",
    "probability",
    "negative",
    "review",
    "problems",
    "might",
    "run",
    "could",
    "potentially",
    "go",
    "wrong",
    "kind",
    "analysis",
    "order",
    "analyze",
    "whether",
    "text",
    "positive",
    "negative",
    "sentiment",
    "well",
    "couple",
    "problems",
    "might",
    "arise",
    "one",
    "problem",
    "might",
    "word",
    "grandson",
    "never",
    "appears",
    "positive",
    "reviews",
    "case",
    "try",
    "calculate",
    "value",
    "probability",
    "think",
    "review",
    "positive",
    "going",
    "multiply",
    "values",
    "together",
    "going",
    "get",
    "0",
    "positive",
    "case",
    "going",
    "ultimately",
    "multiply",
    "0",
    "value",
    "going",
    "say",
    "think",
    "chance",
    "review",
    "positive",
    "contains",
    "word",
    "grandson",
    "training",
    "data",
    "never",
    "seen",
    "word",
    "grandson",
    "appear",
    "positive",
    "sentiment",
    "message",
    "probably",
    "right",
    "analysis",
    "cases",
    "rare",
    "words",
    "might",
    "case",
    "nowhere",
    "training",
    "data",
    "ever",
    "see",
    "word",
    "grandson",
    "appear",
    "message",
    "positive",
    "sentiment",
    "solve",
    "problem",
    "well",
    "one",
    "thing",
    "often",
    "kind",
    "additive",
    "smoothing",
    "add",
    "value",
    "alpha",
    "value",
    "distribution",
    "smooth",
    "data",
    "little",
    "bit",
    "common",
    "form",
    "laplace",
    "smoothing",
    "add",
    "1",
    "value",
    "distribution",
    "essence",
    "pretend",
    "seen",
    "value",
    "one",
    "time",
    "actually",
    "never",
    "seen",
    "word",
    "grandson",
    "positive",
    "review",
    "pretend",
    "seen",
    "seen",
    "pretend",
    "seen",
    "twice",
    "avoid",
    "possibility",
    "might",
    "multiply",
    "0",
    "result",
    "get",
    "results",
    "want",
    "analysis",
    "let",
    "see",
    "looks",
    "like",
    "practice",
    "let",
    "try",
    "naive",
    "bayes",
    "classification",
    "order",
    "classify",
    "text",
    "either",
    "positive",
    "negative",
    "take",
    "look",
    "going",
    "load",
    "sample",
    "data",
    "memory",
    "examples",
    "positive",
    "reviews",
    "negative",
    "reviews",
    "going",
    "train",
    "naive",
    "bayes",
    "classifier",
    "training",
    "data",
    "training",
    "data",
    "includes",
    "words",
    "see",
    "positive",
    "reviews",
    "words",
    "see",
    "negative",
    "reviews",
    "going",
    "try",
    "classify",
    "input",
    "going",
    "based",
    "corpus",
    "data",
    "example",
    "positive",
    "reviews",
    "positive",
    "reviews",
    "great",
    "much",
    "fun",
    "example",
    "negative",
    "reviews",
    "worth",
    "kind",
    "cheap",
    "examples",
    "negative",
    "reviews",
    "let",
    "try",
    "run",
    "classifier",
    "see",
    "would",
    "classify",
    "particular",
    "text",
    "either",
    "positive",
    "negative",
    "go",
    "ahead",
    "run",
    "sentiment",
    "analysis",
    "corpus",
    "need",
    "provide",
    "review",
    "say",
    "something",
    "like",
    "enjoyed",
    "see",
    "classifier",
    "says",
    "probability",
    "think",
    "particular",
    "review",
    "positive",
    "let",
    "try",
    "something",
    "negative",
    "try",
    "kind",
    "overpriced",
    "see",
    "probability",
    "think",
    "particular",
    "review",
    "negative",
    "naive",
    "bayes",
    "classifier",
    "learned",
    "kinds",
    "words",
    "tend",
    "appear",
    "positive",
    "reviews",
    "kinds",
    "words",
    "tend",
    "appear",
    "negative",
    "reviews",
    "result",
    "able",
    "design",
    "classifier",
    "predict",
    "whether",
    "particular",
    "review",
    "positive",
    "negative",
    "definitely",
    "useful",
    "tool",
    "use",
    "try",
    "make",
    "predictions",
    "make",
    "assumptions",
    "order",
    "get",
    "want",
    "try",
    "build",
    "sophisticated",
    "models",
    "use",
    "tools",
    "machine",
    "learning",
    "try",
    "take",
    "better",
    "advantage",
    "language",
    "data",
    "able",
    "draw",
    "accurate",
    "conclusions",
    "solve",
    "new",
    "kinds",
    "tasks",
    "new",
    "kinds",
    "problems",
    "well",
    "seen",
    "couple",
    "times",
    "want",
    "take",
    "data",
    "take",
    "input",
    "put",
    "way",
    "computer",
    "going",
    "able",
    "make",
    "sense",
    "helpful",
    "take",
    "data",
    "turn",
    "numbers",
    "ultimately",
    "might",
    "want",
    "try",
    "come",
    "word",
    "representation",
    "way",
    "take",
    "word",
    "translate",
    "meaning",
    "numbers",
    "example",
    "wanted",
    "use",
    "neural",
    "network",
    "able",
    "process",
    "language",
    "give",
    "language",
    "neural",
    "network",
    "make",
    "predictions",
    "perform",
    "analysis",
    "neural",
    "network",
    "takes",
    "input",
    "produces",
    "output",
    "vector",
    "values",
    "vector",
    "numbers",
    "might",
    "want",
    "take",
    "data",
    "somehow",
    "take",
    "words",
    "convert",
    "kind",
    "numeric",
    "representation",
    "might",
    "might",
    "take",
    "words",
    "turn",
    "numbers",
    "let",
    "take",
    "look",
    "example",
    "sentence",
    "wrote",
    "book",
    "let",
    "say",
    "wanted",
    "take",
    "words",
    "turn",
    "vector",
    "values",
    "one",
    "way",
    "might",
    "say",
    "going",
    "vector",
    "1",
    "first",
    "position",
    "rest",
    "values",
    "wrote",
    "1",
    "second",
    "position",
    "rest",
    "values",
    "1",
    "third",
    "position",
    "rest",
    "value",
    "book",
    "1",
    "fourth",
    "position",
    "rest",
    "value",
    "words",
    "distinct",
    "vector",
    "representation",
    "often",
    "call",
    "representation",
    "representation",
    "meaning",
    "word",
    "vector",
    "single",
    "1",
    "rest",
    "values",
    "numeric",
    "representation",
    "every",
    "word",
    "could",
    "pass",
    "vector",
    "representations",
    "neural",
    "network",
    "models",
    "require",
    "kind",
    "numeric",
    "data",
    "input",
    "representation",
    "actually",
    "couple",
    "problems",
    "ideal",
    "reasons",
    "one",
    "reason",
    "looking",
    "four",
    "words",
    "imagine",
    "vocabulary",
    "thousands",
    "words",
    "vectors",
    "going",
    "get",
    "quite",
    "long",
    "order",
    "distinct",
    "vector",
    "every",
    "possible",
    "word",
    "vocabulary",
    "result",
    "longer",
    "vectors",
    "going",
    "difficult",
    "deal",
    "difficult",
    "train",
    "forth",
    "might",
    "problem",
    "another",
    "problem",
    "little",
    "bit",
    "subtle",
    "want",
    "represent",
    "word",
    "vector",
    "particular",
    "meaning",
    "word",
    "vector",
    "ideally",
    "case",
    "words",
    "similar",
    "meanings",
    "also",
    "similar",
    "vector",
    "representations",
    "close",
    "together",
    "inside",
    "vector",
    "space",
    "really",
    "going",
    "case",
    "representations",
    "take",
    "similar",
    "words",
    "say",
    "word",
    "wrote",
    "word",
    "authored",
    "means",
    "similar",
    "things",
    "entirely",
    "different",
    "vector",
    "representations",
    "likewise",
    "book",
    "novel",
    "two",
    "words",
    "mean",
    "somewhat",
    "similar",
    "things",
    "entirely",
    "different",
    "vector",
    "representations",
    "one",
    "different",
    "position",
    "ideal",
    "either",
    "might",
    "interested",
    "instead",
    "kind",
    "distributed",
    "representation",
    "distributed",
    "representation",
    "representation",
    "meaning",
    "word",
    "distributed",
    "across",
    "multiple",
    "values",
    "instead",
    "one",
    "one",
    "position",
    "distributed",
    "representation",
    "words",
    "might",
    "word",
    "associated",
    "vector",
    "values",
    "meaning",
    "distributed",
    "across",
    "multiple",
    "values",
    "ideally",
    "way",
    "similar",
    "words",
    "similar",
    "vector",
    "representation",
    "going",
    "come",
    "values",
    "values",
    "come",
    "define",
    "meaning",
    "word",
    "distributed",
    "sequence",
    "numbers",
    "well",
    "going",
    "draw",
    "inspiration",
    "quote",
    "british",
    "linguist",
    "firth",
    "said",
    "shall",
    "know",
    "word",
    "company",
    "keeps",
    "words",
    "going",
    "define",
    "meaning",
    "word",
    "based",
    "words",
    "appear",
    "around",
    "context",
    "words",
    "around",
    "take",
    "example",
    "context",
    "blank",
    "ate",
    "might",
    "wonder",
    "words",
    "could",
    "reasonably",
    "fill",
    "blank",
    "well",
    "might",
    "words",
    "like",
    "breakfast",
    "lunch",
    "dinner",
    "could",
    "reasonably",
    "fill",
    "blank",
    "going",
    "say",
    "words",
    "breakfast",
    "lunch",
    "dinner",
    "appear",
    "similar",
    "context",
    "must",
    "similar",
    "meaning",
    "something",
    "computer",
    "could",
    "understand",
    "try",
    "learn",
    "computer",
    "could",
    "look",
    "big",
    "corpus",
    "text",
    "look",
    "words",
    "tend",
    "appear",
    "similar",
    "context",
    "use",
    "identify",
    "words",
    "similar",
    "meaning",
    "therefore",
    "appear",
    "close",
    "inside",
    "vector",
    "space",
    "one",
    "common",
    "model",
    "known",
    "word",
    "vec",
    "model",
    "model",
    "generating",
    "word",
    "vectors",
    "vector",
    "representation",
    "every",
    "word",
    "looking",
    "data",
    "looking",
    "context",
    "word",
    "appears",
    "idea",
    "going",
    "start",
    "words",
    "random",
    "position",
    "space",
    "train",
    "training",
    "data",
    "word",
    "vec",
    "model",
    "start",
    "learn",
    "words",
    "appear",
    "similar",
    "contexts",
    "move",
    "vectors",
    "around",
    "way",
    "hopefully",
    "words",
    "similar",
    "meanings",
    "breakfast",
    "lunch",
    "dinner",
    "book",
    "memoir",
    "novel",
    "hopefully",
    "appear",
    "near",
    "vectors",
    "well",
    "let",
    "take",
    "look",
    "word",
    "vec",
    "might",
    "look",
    "like",
    "practice",
    "implemented",
    "code",
    "inside",
    "model",
    "words",
    "vector",
    "representation",
    "trained",
    "word",
    "vec",
    "words",
    "sequence",
    "values",
    "representing",
    "meaning",
    "hopefully",
    "way",
    "similar",
    "words",
    "represented",
    "similar",
    "vectors",
    "also",
    "file",
    "going",
    "open",
    "words",
    "form",
    "dictionary",
    "also",
    "define",
    "useful",
    "functions",
    "like",
    "distance",
    "get",
    "distance",
    "two",
    "word",
    "vectors",
    "closest",
    "words",
    "find",
    "words",
    "nearby",
    "terms",
    "close",
    "vectors",
    "let",
    "give",
    "try",
    "go",
    "ahead",
    "open",
    "python",
    "interpreter",
    "going",
    "import",
    "vectors",
    "might",
    "say",
    "right",
    "vector",
    "representation",
    "word",
    "book",
    "get",
    "big",
    "long",
    "vector",
    "represents",
    "word",
    "book",
    "sequence",
    "values",
    "sequence",
    "values",
    "meaningful",
    "meaningful",
    "context",
    "comparing",
    "vectors",
    "words",
    "could",
    "use",
    "distance",
    "function",
    "going",
    "get",
    "us",
    "distance",
    "two",
    "word",
    "vectors",
    "might",
    "say",
    "distance",
    "vector",
    "representation",
    "word",
    "book",
    "vector",
    "representation",
    "word",
    "novel",
    "see",
    "kind",
    "interpret",
    "0",
    "really",
    "close",
    "together",
    "1",
    "far",
    "apart",
    "distance",
    "book",
    "let",
    "say",
    "breakfast",
    "well",
    "book",
    "breakfast",
    "different",
    "book",
    "novel",
    "would",
    "hopefully",
    "expect",
    "distance",
    "larger",
    "fact",
    "approximately",
    "two",
    "words",
    "away",
    "distance",
    "let",
    "say",
    "lunch",
    "breakfast",
    "well",
    "even",
    "closer",
    "together",
    "meaning",
    "closer",
    "another",
    "interesting",
    "thing",
    "might",
    "calculate",
    "closest",
    "words",
    "might",
    "say",
    "closest",
    "words",
    "according",
    "word2vec",
    "word",
    "book",
    "let",
    "say",
    "let",
    "get",
    "10",
    "closest",
    "words",
    "10",
    "closest",
    "vectors",
    "vector",
    "representation",
    "word",
    "book",
    "perform",
    "analysis",
    "get",
    "list",
    "words",
    "closest",
    "one",
    "book",
    "also",
    "books",
    "plural",
    "essay",
    "memoir",
    "essays",
    "novella",
    "anthology",
    "words",
    "mean",
    "something",
    "similar",
    "word",
    "book",
    "according",
    "word2vec",
    "least",
    "similar",
    "vector",
    "representation",
    "seems",
    "like",
    "done",
    "pretty",
    "good",
    "job",
    "trying",
    "capture",
    "kind",
    "vector",
    "representation",
    "word",
    "meaning",
    "one",
    "interesting",
    "side",
    "effect",
    "word2vec",
    "also",
    "able",
    "capture",
    "something",
    "relationships",
    "words",
    "well",
    "let",
    "take",
    "look",
    "example",
    "instance",
    "two",
    "words",
    "man",
    "king",
    "represented",
    "word2vec",
    "vectors",
    "might",
    "happen",
    "subtracted",
    "one",
    "calculated",
    "value",
    "king",
    "minus",
    "man",
    "well",
    "vector",
    "take",
    "us",
    "man",
    "king",
    "somehow",
    "represent",
    "relationship",
    "vector",
    "representation",
    "word",
    "man",
    "vector",
    "representation",
    "word",
    "king",
    "value",
    "king",
    "minus",
    "man",
    "represents",
    "would",
    "happen",
    "took",
    "vector",
    "representation",
    "word",
    "woman",
    "added",
    "value",
    "king",
    "minus",
    "man",
    "would",
    "get",
    "closest",
    "word",
    "example",
    "well",
    "could",
    "try",
    "let",
    "go",
    "ahead",
    "go",
    "back",
    "python",
    "interpreter",
    "give",
    "try",
    "could",
    "say",
    "closest",
    "word",
    "vector",
    "representation",
    "word",
    "king",
    "minus",
    "representation",
    "word",
    "man",
    "plus",
    "representation",
    "word",
    "woman",
    "see",
    "closest",
    "word",
    "word",
    "queen",
    "somehow",
    "able",
    "capture",
    "relationship",
    "king",
    "man",
    "apply",
    "word",
    "woman",
    "get",
    "result",
    "word",
    "queen",
    "word2vec",
    "able",
    "capture",
    "words",
    "similar",
    "also",
    "something",
    "relationships",
    "words",
    "words",
    "connected",
    "vector",
    "representation",
    "words",
    "represent",
    "words",
    "numbers",
    "might",
    "try",
    "pass",
    "words",
    "input",
    "say",
    "neural",
    "network",
    "neural",
    "networks",
    "seen",
    "powerful",
    "tools",
    "identifying",
    "patterns",
    "making",
    "predictions",
    "recall",
    "neural",
    "network",
    "think",
    "units",
    "really",
    "neural",
    "network",
    "taking",
    "input",
    "passing",
    "network",
    "producing",
    "output",
    "providing",
    "neural",
    "network",
    "training",
    "data",
    "able",
    "update",
    "weights",
    "inside",
    "network",
    "neural",
    "network",
    "accurate",
    "job",
    "translating",
    "inputs",
    "outputs",
    "represent",
    "words",
    "numbers",
    "could",
    "input",
    "output",
    "could",
    "imagine",
    "passing",
    "word",
    "input",
    "neural",
    "network",
    "getting",
    "word",
    "output",
    "might",
    "useful",
    "one",
    "common",
    "use",
    "neural",
    "networks",
    "machine",
    "translation",
    "want",
    "translate",
    "text",
    "one",
    "language",
    "another",
    "say",
    "translate",
    "english",
    "french",
    "passing",
    "english",
    "neural",
    "network",
    "getting",
    "french",
    "output",
    "might",
    "imagine",
    "instance",
    "could",
    "take",
    "english",
    "word",
    "lamp",
    "pass",
    "neural",
    "network",
    "get",
    "french",
    "word",
    "lamp",
    "output",
    "practice",
    "translating",
    "text",
    "one",
    "language",
    "another",
    "usually",
    "interested",
    "translating",
    "single",
    "word",
    "one",
    "language",
    "another",
    "sequence",
    "say",
    "sentence",
    "paragraph",
    "words",
    "example",
    "another",
    "paragraph",
    "taken",
    "sherlock",
    "holmes",
    "written",
    "english",
    "might",
    "want",
    "take",
    "entire",
    "sentence",
    "pass",
    "neural",
    "network",
    "get",
    "output",
    "french",
    "translation",
    "sentence",
    "recall",
    "neural",
    "network",
    "input",
    "output",
    "needs",
    "fixed",
    "size",
    "sentence",
    "fixed",
    "size",
    "variable",
    "might",
    "shorter",
    "sentences",
    "might",
    "longer",
    "sentences",
    "somehow",
    "need",
    "solve",
    "problem",
    "translating",
    "sequence",
    "another",
    "sequence",
    "means",
    "neural",
    "network",
    "going",
    "true",
    "machine",
    "translation",
    "also",
    "problems",
    "problems",
    "like",
    "question",
    "answering",
    "want",
    "pass",
    "input",
    "question",
    "something",
    "like",
    "capital",
    "massachusetts",
    "feed",
    "input",
    "neural",
    "network",
    "would",
    "hope",
    "would",
    "get",
    "output",
    "sentence",
    "like",
    "capital",
    "boston",
    "translating",
    "sequence",
    "sequence",
    "ever",
    "conversation",
    "ai",
    "chatbot",
    "ever",
    "asked",
    "phone",
    "question",
    "needs",
    "something",
    "like",
    "needs",
    "understand",
    "sequence",
    "words",
    "human",
    "provided",
    "input",
    "computer",
    "needs",
    "generate",
    "sequence",
    "words",
    "output",
    "well",
    "one",
    "tool",
    "use",
    "recurrent",
    "neural",
    "network",
    "took",
    "look",
    "last",
    "time",
    "way",
    "us",
    "provide",
    "sequence",
    "values",
    "neural",
    "network",
    "running",
    "neural",
    "network",
    "multiple",
    "times",
    "time",
    "run",
    "neural",
    "network",
    "going",
    "going",
    "keep",
    "track",
    "hidden",
    "state",
    "hidden",
    "state",
    "going",
    "passed",
    "one",
    "run",
    "neural",
    "network",
    "next",
    "run",
    "neural",
    "network",
    "keeping",
    "track",
    "relevant",
    "information",
    "let",
    "take",
    "look",
    "apply",
    "something",
    "like",
    "particular",
    "going",
    "look",
    "architecture",
    "known",
    "architecture",
    "going",
    "encode",
    "question",
    "kind",
    "hidden",
    "state",
    "use",
    "decoder",
    "decode",
    "hidden",
    "state",
    "output",
    "interested",
    "going",
    "look",
    "like",
    "start",
    "first",
    "word",
    "word",
    "goes",
    "neural",
    "network",
    "going",
    "produce",
    "hidden",
    "state",
    "information",
    "word",
    "neural",
    "network",
    "going",
    "need",
    "keep",
    "track",
    "second",
    "word",
    "comes",
    "along",
    "going",
    "feed",
    "encoder",
    "neural",
    "network",
    "going",
    "get",
    "input",
    "hidden",
    "state",
    "well",
    "pass",
    "second",
    "word",
    "also",
    "get",
    "information",
    "hidden",
    "state",
    "going",
    "continue",
    "words",
    "input",
    "going",
    "produce",
    "new",
    "hidden",
    "state",
    "get",
    "third",
    "word",
    "goes",
    "encoder",
    "also",
    "gets",
    "access",
    "hidden",
    "state",
    "produces",
    "new",
    "hidden",
    "state",
    "gets",
    "passed",
    "next",
    "run",
    "use",
    "word",
    "capital",
    "thing",
    "going",
    "repeat",
    "words",
    "appear",
    "input",
    "massachusetts",
    "produces",
    "one",
    "final",
    "piece",
    "hidden",
    "state",
    "somehow",
    "need",
    "signal",
    "fact",
    "done",
    "nothing",
    "left",
    "input",
    "typically",
    "passing",
    "kind",
    "special",
    "token",
    "say",
    "end",
    "token",
    "neural",
    "network",
    "decoding",
    "process",
    "going",
    "start",
    "going",
    "generate",
    "word",
    "addition",
    "generating",
    "word",
    "decoder",
    "network",
    "also",
    "going",
    "generate",
    "kind",
    "hidden",
    "state",
    "happens",
    "next",
    "time",
    "well",
    "generate",
    "next",
    "word",
    "might",
    "helpful",
    "know",
    "first",
    "word",
    "might",
    "pass",
    "first",
    "word",
    "back",
    "decoder",
    "network",
    "going",
    "get",
    "input",
    "hidden",
    "state",
    "going",
    "generate",
    "next",
    "word",
    "capital",
    "also",
    "going",
    "generate",
    "hidden",
    "state",
    "repeat",
    "passing",
    "capital",
    "network",
    "generate",
    "third",
    "word",
    "one",
    "time",
    "order",
    "get",
    "fourth",
    "word",
    "boston",
    "point",
    "done",
    "know",
    "done",
    "usually",
    "one",
    "time",
    "pass",
    "boston",
    "decoder",
    "network",
    "get",
    "output",
    "end",
    "token",
    "indicate",
    "end",
    "input",
    "could",
    "use",
    "recurrent",
    "neural",
    "network",
    "take",
    "input",
    "encode",
    "hidden",
    "state",
    "use",
    "hidden",
    "state",
    "decode",
    "output",
    "interested",
    "visualize",
    "slightly",
    "different",
    "way",
    "input",
    "sequence",
    "sequence",
    "words",
    "input",
    "sequence",
    "goes",
    "encoder",
    "case",
    "recurrent",
    "neural",
    "network",
    "generating",
    "hidden",
    "states",
    "along",
    "way",
    "generate",
    "final",
    "hidden",
    "state",
    "point",
    "start",
    "decoding",
    "process",
    "using",
    "recurrent",
    "neural",
    "network",
    "going",
    "generate",
    "output",
    "sequence",
    "well",
    "got",
    "encoder",
    "encoding",
    "information",
    "input",
    "sequence",
    "hidden",
    "state",
    "decoder",
    "takes",
    "hidden",
    "state",
    "uses",
    "order",
    "generate",
    "output",
    "sequence",
    "problems",
    "many",
    "years",
    "state",
    "art",
    "recurrent",
    "neural",
    "network",
    "variance",
    "approach",
    "best",
    "ways",
    "knew",
    "order",
    "perform",
    "tasks",
    "natural",
    "language",
    "processing",
    "problems",
    "might",
    "want",
    "try",
    "deal",
    "dealt",
    "years",
    "try",
    "improve",
    "upon",
    "kind",
    "model",
    "one",
    "problem",
    "might",
    "notice",
    "happens",
    "encoder",
    "stage",
    "taken",
    "input",
    "sequence",
    "sequence",
    "words",
    "encoded",
    "final",
    "piece",
    "hidden",
    "state",
    "final",
    "piece",
    "hidden",
    "state",
    "needs",
    "contain",
    "information",
    "input",
    "sequence",
    "need",
    "order",
    "generate",
    "output",
    "sequence",
    "possible",
    "becomes",
    "increasingly",
    "difficult",
    "sequence",
    "gets",
    "larger",
    "larger",
    "larger",
    "larger",
    "input",
    "sequences",
    "going",
    "become",
    "difficult",
    "store",
    "information",
    "need",
    "input",
    "inside",
    "single",
    "hidden",
    "state",
    "piece",
    "context",
    "lot",
    "information",
    "pack",
    "single",
    "value",
    "might",
    "useful",
    "us",
    "generating",
    "output",
    "refer",
    "one",
    "value",
    "previous",
    "hidden",
    "values",
    "generated",
    "encoder",
    "might",
    "useful",
    "could",
    "got",
    "lot",
    "different",
    "values",
    "need",
    "combine",
    "somehow",
    "could",
    "imagine",
    "adding",
    "together",
    "taking",
    "average",
    "example",
    "would",
    "assume",
    "pieces",
    "hidden",
    "state",
    "equally",
    "important",
    "necessarily",
    "true",
    "either",
    "pieces",
    "hidden",
    "state",
    "going",
    "important",
    "others",
    "depending",
    "word",
    "closely",
    "correspond",
    "piece",
    "hidden",
    "state",
    "closely",
    "corresponds",
    "first",
    "word",
    "input",
    "sequence",
    "one",
    "closely",
    "corresponds",
    "second",
    "word",
    "input",
    "sequence",
    "example",
    "going",
    "important",
    "others",
    "make",
    "matters",
    "complicated",
    "depending",
    "word",
    "output",
    "sequence",
    "generating",
    "different",
    "input",
    "words",
    "might",
    "less",
    "important",
    "really",
    "want",
    "way",
    "decide",
    "input",
    "values",
    "worth",
    "paying",
    "attention",
    "point",
    "time",
    "key",
    "idea",
    "behind",
    "mechanism",
    "known",
    "attention",
    "attention",
    "letting",
    "us",
    "decide",
    "values",
    "important",
    "pay",
    "attention",
    "generating",
    "case",
    "next",
    "word",
    "sequence",
    "let",
    "take",
    "look",
    "example",
    "sentence",
    "capital",
    "massachusetts",
    "sentence",
    "let",
    "imagine",
    "trying",
    "answer",
    "question",
    "generating",
    "tokens",
    "output",
    "would",
    "output",
    "look",
    "like",
    "well",
    "going",
    "look",
    "like",
    "something",
    "like",
    "capital",
    "let",
    "say",
    "trying",
    "generate",
    "last",
    "word",
    "last",
    "word",
    "computer",
    "going",
    "figure",
    "well",
    "going",
    "need",
    "decide",
    "values",
    "going",
    "pay",
    "attention",
    "attention",
    "mechanism",
    "allow",
    "us",
    "calculate",
    "attention",
    "scores",
    "word",
    "value",
    "corresponding",
    "word",
    "determining",
    "relevant",
    "us",
    "pay",
    "attention",
    "word",
    "right",
    "case",
    "generating",
    "fourth",
    "word",
    "output",
    "sequence",
    "important",
    "words",
    "pay",
    "attention",
    "might",
    "capital",
    "massachusetts",
    "example",
    "words",
    "going",
    "particularly",
    "relevant",
    "number",
    "different",
    "mechanisms",
    "used",
    "order",
    "calculate",
    "attention",
    "scores",
    "could",
    "something",
    "simple",
    "dot",
    "product",
    "see",
    "similar",
    "two",
    "vectors",
    "could",
    "train",
    "entire",
    "neural",
    "network",
    "calculate",
    "attention",
    "scores",
    "key",
    "idea",
    "training",
    "process",
    "neural",
    "network",
    "going",
    "learn",
    "calculate",
    "attention",
    "scores",
    "model",
    "going",
    "learn",
    "important",
    "pay",
    "attention",
    "order",
    "decide",
    "next",
    "word",
    "result",
    "calculating",
    "attention",
    "scores",
    "calculate",
    "value",
    "value",
    "input",
    "word",
    "determining",
    "important",
    "us",
    "pay",
    "attention",
    "particular",
    "value",
    "recall",
    "input",
    "words",
    "also",
    "associated",
    "one",
    "hidden",
    "state",
    "context",
    "vectors",
    "capturing",
    "information",
    "sentence",
    "point",
    "primarily",
    "focused",
    "word",
    "particular",
    "vectors",
    "values",
    "representing",
    "important",
    "us",
    "pay",
    "attention",
    "particular",
    "vectors",
    "take",
    "weighted",
    "average",
    "take",
    "vectors",
    "multiply",
    "attention",
    "scores",
    "add",
    "get",
    "new",
    "vector",
    "value",
    "going",
    "represent",
    "context",
    "input",
    "specifically",
    "paying",
    "attention",
    "words",
    "think",
    "important",
    "done",
    "context",
    "vector",
    "fed",
    "decoder",
    "order",
    "say",
    "word",
    "case",
    "boston",
    "attention",
    "powerful",
    "tool",
    "allows",
    "word",
    "trying",
    "decode",
    "decide",
    "words",
    "input",
    "pay",
    "attention",
    "order",
    "determine",
    "important",
    "generating",
    "next",
    "word",
    "output",
    "one",
    "first",
    "places",
    "really",
    "used",
    "field",
    "machine",
    "translation",
    "example",
    "diagram",
    "paper",
    "introduced",
    "idea",
    "focused",
    "trying",
    "translate",
    "english",
    "sentences",
    "french",
    "sentences",
    "input",
    "english",
    "sentence",
    "along",
    "top",
    "along",
    "left",
    "side",
    "output",
    "french",
    "equivalent",
    "sentence",
    "see",
    "squares",
    "attention",
    "scores",
    "visualized",
    "lighter",
    "square",
    "indicates",
    "higher",
    "attention",
    "score",
    "notice",
    "strong",
    "correspondence",
    "french",
    "word",
    "equivalent",
    "english",
    "word",
    "french",
    "word",
    "agreement",
    "really",
    "paying",
    "attention",
    "english",
    "word",
    "agreement",
    "order",
    "decide",
    "french",
    "word",
    "generated",
    "point",
    "time",
    "sometimes",
    "might",
    "pay",
    "attention",
    "multiple",
    "words",
    "look",
    "french",
    "word",
    "economic",
    "primarily",
    "paying",
    "attention",
    "english",
    "word",
    "economic",
    "also",
    "paying",
    "attention",
    "english",
    "word",
    "european",
    "case",
    "attention",
    "scores",
    "easy",
    "visualize",
    "get",
    "sense",
    "machine",
    "learning",
    "model",
    "really",
    "paying",
    "attention",
    "information",
    "using",
    "order",
    "determine",
    "important",
    "order",
    "determine",
    "ultimate",
    "output",
    "token",
    "combine",
    "attention",
    "mechanism",
    "recurrent",
    "neural",
    "network",
    "get",
    "powerful",
    "useful",
    "results",
    "able",
    "generate",
    "output",
    "sequence",
    "paying",
    "attention",
    "input",
    "sequence",
    "problems",
    "approach",
    "using",
    "recurrent",
    "neural",
    "network",
    "well",
    "particular",
    "notice",
    "every",
    "run",
    "neural",
    "network",
    "depends",
    "output",
    "previous",
    "step",
    "important",
    "getting",
    "sense",
    "sequence",
    "words",
    "ordering",
    "particular",
    "words",
    "ca",
    "run",
    "unit",
    "neural",
    "network",
    "calculated",
    "hidden",
    "state",
    "run",
    "previous",
    "input",
    "token",
    "means",
    "difficult",
    "parallelize",
    "process",
    "input",
    "sequence",
    "get",
    "longer",
    "longer",
    "might",
    "want",
    "use",
    "parallelism",
    "try",
    "speed",
    "process",
    "training",
    "neural",
    "network",
    "making",
    "sense",
    "language",
    "data",
    "difficult",
    "slow",
    "recurrent",
    "neural",
    "network",
    "needs",
    "performed",
    "sequence",
    "become",
    "increasing",
    "challenge",
    "started",
    "get",
    "larger",
    "larger",
    "language",
    "models",
    "language",
    "data",
    "available",
    "us",
    "use",
    "train",
    "machine",
    "learning",
    "models",
    "accurate",
    "better",
    "representation",
    "language",
    "better",
    "understanding",
    "better",
    "results",
    "see",
    "seen",
    "growth",
    "large",
    "language",
    "models",
    "using",
    "larger",
    "larger",
    "data",
    "sets",
    "result",
    "take",
    "longer",
    "longer",
    "train",
    "problem",
    "recurrent",
    "neural",
    "networks",
    "easy",
    "parallelize",
    "become",
    "increasing",
    "problem",
    "result",
    "one",
    "main",
    "motivations",
    "different",
    "architecture",
    "thinking",
    "deal",
    "natural",
    "language",
    "known",
    "transformer",
    "architecture",
    "significant",
    "milestone",
    "world",
    "natural",
    "language",
    "processing",
    "really",
    "increasing",
    "well",
    "perform",
    "kinds",
    "natural",
    "language",
    "processing",
    "tasks",
    "well",
    "quickly",
    "train",
    "machine",
    "learning",
    "model",
    "able",
    "produce",
    "effective",
    "results",
    "number",
    "different",
    "types",
    "transformers",
    "terms",
    "work",
    "going",
    "take",
    "look",
    "basic",
    "architecture",
    "one",
    "might",
    "work",
    "transformer",
    "get",
    "sense",
    "involved",
    "let",
    "start",
    "model",
    "looking",
    "specifically",
    "encoder",
    "part",
    "architecture",
    "used",
    "recurrent",
    "neural",
    "network",
    "take",
    "input",
    "sequence",
    "capture",
    "information",
    "hidden",
    "state",
    "information",
    "need",
    "know",
    "input",
    "sequence",
    "right",
    "needs",
    "happen",
    "linear",
    "progression",
    "transformer",
    "going",
    "allow",
    "us",
    "process",
    "words",
    "independently",
    "way",
    "easy",
    "parallelize",
    "rather",
    "word",
    "wait",
    "word",
    "word",
    "going",
    "go",
    "neural",
    "network",
    "produce",
    "kind",
    "encoded",
    "representation",
    "particular",
    "input",
    "word",
    "going",
    "happen",
    "parallel",
    "happening",
    "words",
    "really",
    "going",
    "focus",
    "happening",
    "one",
    "word",
    "make",
    "clear",
    "know",
    "whatever",
    "seeing",
    "happen",
    "one",
    "word",
    "going",
    "happen",
    "input",
    "words",
    "going",
    "well",
    "start",
    "input",
    "word",
    "input",
    "word",
    "goes",
    "neural",
    "network",
    "output",
    "hopefully",
    "encoded",
    "representation",
    "input",
    "word",
    "information",
    "need",
    "know",
    "input",
    "word",
    "going",
    "relevant",
    "us",
    "generating",
    "output",
    "word",
    "independently",
    "easy",
    "parallelize",
    "wait",
    "previous",
    "word",
    "run",
    "word",
    "neural",
    "network",
    "lose",
    "process",
    "trying",
    "parallelize",
    "whole",
    "thing",
    "well",
    "lost",
    "notion",
    "word",
    "ordering",
    "order",
    "words",
    "important",
    "sentence",
    "sherlock",
    "holmes",
    "gave",
    "book",
    "watson",
    "different",
    "meaning",
    "watson",
    "gave",
    "book",
    "sherlock",
    "holmes",
    "want",
    "keep",
    "track",
    "information",
    "word",
    "position",
    "recurrent",
    "neural",
    "network",
    "happened",
    "us",
    "automatically",
    "could",
    "run",
    "word",
    "one",
    "time",
    "neural",
    "network",
    "get",
    "hidden",
    "state",
    "pass",
    "next",
    "run",
    "neural",
    "network",
    "case",
    "transformer",
    "word",
    "processed",
    "independent",
    "ones",
    "going",
    "try",
    "solve",
    "problem",
    "one",
    "thing",
    "add",
    "kind",
    "positional",
    "encoding",
    "input",
    "word",
    "positional",
    "encoding",
    "vector",
    "represents",
    "position",
    "word",
    "sentence",
    "first",
    "word",
    "second",
    "word",
    "third",
    "word",
    "forth",
    "going",
    "add",
    "input",
    "word",
    "result",
    "going",
    "vector",
    "captures",
    "multiple",
    "pieces",
    "information",
    "captures",
    "input",
    "word",
    "well",
    "sentence",
    "appears",
    "result",
    "pass",
    "output",
    "addition",
    "addition",
    "input",
    "word",
    "positional",
    "encoding",
    "neural",
    "network",
    "way",
    "neural",
    "network",
    "knows",
    "word",
    "appears",
    "sentence",
    "use",
    "pieces",
    "information",
    "determine",
    "best",
    "represent",
    "meaning",
    "word",
    "encoded",
    "representation",
    "end",
    "addition",
    "addition",
    "positional",
    "encoding",
    "feed",
    "forward",
    "neural",
    "network",
    "also",
    "going",
    "add",
    "one",
    "additional",
    "component",
    "going",
    "step",
    "going",
    "attention",
    "paying",
    "attention",
    "input",
    "words",
    "meaning",
    "interpretation",
    "input",
    "word",
    "might",
    "vary",
    "depending",
    "words",
    "input",
    "well",
    "going",
    "allow",
    "word",
    "input",
    "decide",
    "words",
    "input",
    "pay",
    "attention",
    "order",
    "decide",
    "encoded",
    "representation",
    "going",
    "allow",
    "us",
    "get",
    "better",
    "encoded",
    "representation",
    "word",
    "words",
    "defined",
    "context",
    "words",
    "around",
    "used",
    "particular",
    "context",
    "kind",
    "valuable",
    "fact",
    "oftentimes",
    "transformer",
    "use",
    "multiple",
    "different",
    "layers",
    "time",
    "allow",
    "model",
    "able",
    "pay",
    "attention",
    "multiple",
    "facets",
    "input",
    "time",
    "call",
    "attention",
    "attention",
    "head",
    "pay",
    "attention",
    "something",
    "different",
    "result",
    "network",
    "learn",
    "pay",
    "attention",
    "many",
    "different",
    "parts",
    "input",
    "input",
    "word",
    "time",
    "spirit",
    "deep",
    "learning",
    "two",
    "steps",
    "layer",
    "neural",
    "network",
    "layer",
    "repeated",
    "multiple",
    "times",
    "order",
    "get",
    "deeper",
    "representation",
    "order",
    "learn",
    "deeper",
    "patterns",
    "within",
    "input",
    "text",
    "ultimately",
    "get",
    "better",
    "representation",
    "language",
    "order",
    "get",
    "useful",
    "encoded",
    "representations",
    "input",
    "words",
    "process",
    "transformer",
    "might",
    "use",
    "order",
    "take",
    "input",
    "word",
    "get",
    "encoded",
    "representation",
    "key",
    "idea",
    "really",
    "rely",
    "attention",
    "step",
    "order",
    "get",
    "information",
    "useful",
    "order",
    "determine",
    "encode",
    "word",
    "process",
    "going",
    "repeat",
    "input",
    "words",
    "input",
    "sequence",
    "going",
    "take",
    "input",
    "words",
    "encode",
    "kind",
    "positional",
    "encoding",
    "feed",
    "neural",
    "networks",
    "order",
    "ultimately",
    "get",
    "encoded",
    "representations",
    "words",
    "result",
    "encoder",
    "get",
    "encoded",
    "representations",
    "useful",
    "us",
    "comes",
    "time",
    "try",
    "decode",
    "information",
    "output",
    "sequence",
    "interested",
    "might",
    "take",
    "place",
    "context",
    "machine",
    "translation",
    "output",
    "going",
    "sentence",
    "different",
    "language",
    "might",
    "answer",
    "question",
    "case",
    "ai",
    "chatbot",
    "example",
    "let",
    "take",
    "look",
    "decoder",
    "going",
    "work",
    "ultimately",
    "going",
    "similar",
    "structure",
    "time",
    "trying",
    "generate",
    "next",
    "output",
    "word",
    "need",
    "know",
    "previous",
    "output",
    "word",
    "well",
    "positional",
    "encoding",
    "output",
    "sequence",
    "going",
    "steps",
    "might",
    "want",
    "output",
    "word",
    "able",
    "pay",
    "attention",
    "words",
    "output",
    "well",
    "neural",
    "network",
    "might",
    "repeat",
    "multiple",
    "times",
    "decoder",
    "going",
    "add",
    "one",
    "additional",
    "step",
    "going",
    "add",
    "additional",
    "attention",
    "step",
    "instead",
    "output",
    "word",
    "going",
    "pay",
    "attention",
    "output",
    "words",
    "step",
    "going",
    "allow",
    "output",
    "word",
    "pay",
    "attention",
    "encoded",
    "representations",
    "recall",
    "encoder",
    "taking",
    "input",
    "words",
    "transforming",
    "encoded",
    "representations",
    "input",
    "words",
    "going",
    "important",
    "us",
    "able",
    "decide",
    "encoded",
    "representations",
    "want",
    "pay",
    "attention",
    "generating",
    "particular",
    "token",
    "output",
    "sequence",
    "additional",
    "attention",
    "step",
    "going",
    "allow",
    "us",
    "saying",
    "every",
    "time",
    "generating",
    "word",
    "output",
    "pay",
    "attention",
    "words",
    "output",
    "might",
    "want",
    "know",
    "words",
    "generated",
    "previously",
    "want",
    "pay",
    "attention",
    "decide",
    "word",
    "going",
    "next",
    "sequence",
    "also",
    "care",
    "paying",
    "attention",
    "input",
    "words",
    "want",
    "ability",
    "decide",
    "encoded",
    "representations",
    "input",
    "words",
    "going",
    "relevant",
    "order",
    "us",
    "generate",
    "next",
    "step",
    "two",
    "pieces",
    "combine",
    "together",
    "encoder",
    "takes",
    "input",
    "words",
    "produces",
    "encoded",
    "representation",
    "decoder",
    "able",
    "take",
    "previous",
    "output",
    "word",
    "pay",
    "attention",
    "encoded",
    "input",
    "generate",
    "next",
    "output",
    "word",
    "one",
    "possible",
    "architectures",
    "could",
    "use",
    "transformer",
    "key",
    "idea",
    "attention",
    "steps",
    "allow",
    "words",
    "pay",
    "attention",
    "training",
    "process",
    "much",
    "easily",
    "parallelize",
    "wait",
    "words",
    "happen",
    "sequence",
    "learn",
    "perform",
    "attention",
    "steps",
    "model",
    "able",
    "learn",
    "important",
    "pay",
    "attention",
    "things",
    "need",
    "pay",
    "attention",
    "order",
    "accurate",
    "predicting",
    "output",
    "word",
    "proved",
    "tremendously",
    "effective",
    "model",
    "conversational",
    "ai",
    "agents",
    "building",
    "machine",
    "translation",
    "systems",
    "many",
    "variants",
    "proposed",
    "model",
    "transformers",
    "use",
    "encoder",
    "use",
    "decoder",
    "use",
    "combination",
    "different",
    "particular",
    "features",
    "key",
    "ideas",
    "ultimately",
    "remain",
    "real",
    "focus",
    "trying",
    "pay",
    "attention",
    "important",
    "world",
    "natural",
    "language",
    "processing",
    "fast",
    "growing",
    "fast",
    "evolving",
    "year",
    "year",
    "keep",
    "coming",
    "new",
    "models",
    "allow",
    "us",
    "even",
    "better",
    "job",
    "performing",
    "natural",
    "language",
    "related",
    "tasks",
    "surface",
    "solving",
    "tricky",
    "problem",
    "natural",
    "language",
    "seen",
    "syntax",
    "semantics",
    "language",
    "ambiguous",
    "introduces",
    "new",
    "challenges",
    "need",
    "think",
    "going",
    "able",
    "design",
    "ai",
    "agents",
    "able",
    "work",
    "language",
    "effectively",
    "think",
    "class",
    "different",
    "types",
    "artificial",
    "intelligence",
    "considered",
    "looked",
    "artificial",
    "intelligence",
    "wide",
    "variety",
    "different",
    "forms",
    "started",
    "taking",
    "look",
    "search",
    "problems",
    "looked",
    "ai",
    "search",
    "solutions",
    "play",
    "games",
    "find",
    "optimal",
    "decision",
    "make",
    "talked",
    "knowledge",
    "ai",
    "represent",
    "information",
    "knows",
    "use",
    "information",
    "generate",
    "new",
    "knowledge",
    "well",
    "looked",
    "ai",
    "less",
    "certain",
    "know",
    "things",
    "sure",
    "represent",
    "things",
    "terms",
    "probability",
    "took",
    "look",
    "optimization",
    "problems",
    "saw",
    "lot",
    "problems",
    "ai",
    "boiled",
    "trying",
    "maximize",
    "minimize",
    "function",
    "looked",
    "strategies",
    "ai",
    "use",
    "order",
    "kind",
    "maximizing",
    "minimizing",
    "looked",
    "world",
    "machine",
    "learning",
    "learning",
    "data",
    "order",
    "figure",
    "patterns",
    "identify",
    "perform",
    "task",
    "looking",
    "training",
    "data",
    "available",
    "one",
    "powerful",
    "tools",
    "neural",
    "network",
    "sequence",
    "units",
    "whose",
    "weights",
    "trained",
    "order",
    "allow",
    "us",
    "really",
    "effectively",
    "go",
    "input",
    "output",
    "predict",
    "get",
    "learning",
    "underlying",
    "patterns",
    "today",
    "took",
    "look",
    "language",
    "trying",
    "understand",
    "train",
    "computer",
    "able",
    "understand",
    "natural",
    "language",
    "able",
    "understand",
    "syntax",
    "semantics",
    "make",
    "sense",
    "generate",
    "natural",
    "language",
    "introduces",
    "number",
    "interesting",
    "problems",
    "really",
    "scratched",
    "surface",
    "artificial",
    "intelligence",
    "much",
    "interesting",
    "research",
    "interesting",
    "new",
    "techniques",
    "algorithms",
    "ideas",
    "introduced",
    "try",
    "solve",
    "types",
    "problems",
    "hope",
    "enjoyed",
    "exploration",
    "world",
    "artificial",
    "intelligence",
    "huge",
    "thanks",
    "course",
    "teaching",
    "staff",
    "production",
    "team",
    "making",
    "class",
    "possible",
    "introduction",
    "artificial",
    "intelligence",
    "python"
  ],
  "keywords": [
    "course",
    "algorithms",
    "artificial",
    "intelligence",
    "ideas",
    "give",
    "like",
    "machine",
    "graph",
    "search",
    "classification",
    "reinforcement",
    "learning",
    "world",
    "python",
    "ai",
    "start",
    "look",
    "problems",
    "whether",
    "play",
    "game",
    "trying",
    "find",
    "represent",
    "information",
    "knowledge",
    "certain",
    "also",
    "events",
    "might",
    "use",
    "draw",
    "new",
    "conclusions",
    "well",
    "explore",
    "solve",
    "various",
    "types",
    "maximize",
    "minimize",
    "constraints",
    "attention",
    "tell",
    "exactly",
    "problem",
    "instead",
    "access",
    "data",
    "learn",
    "perform",
    "tasks",
    "particular",
    "neural",
    "networks",
    "one",
    "popular",
    "way",
    "human",
    "reason",
    "taking",
    "natural",
    "language",
    "us",
    "able",
    "understand",
    "implement",
    "right",
    "class",
    "see",
    "computer",
    "something",
    "someone",
    "better",
    "people",
    "mean",
    "back",
    "examples",
    "make",
    "possible",
    "begin",
    "would",
    "kind",
    "get",
    "point",
    "b",
    "figure",
    "given",
    "example",
    "figuring",
    "move",
    "take",
    "want",
    "know",
    "additional",
    "order",
    "happens",
    "sure",
    "fact",
    "maybe",
    "probability",
    "computers",
    "deal",
    "little",
    "bit",
    "sense",
    "turn",
    "sort",
    "goal",
    "especially",
    "situation",
    "multiple",
    "ways",
    "looking",
    "best",
    "generally",
    "quite",
    "task",
    "based",
    "greater",
    "somehow",
    "good",
    "structure",
    "idea",
    "advantage",
    "type",
    "program",
    "effectively",
    "every",
    "day",
    "come",
    "actually",
    "work",
    "today",
    "agent",
    "solution",
    "number",
    "different",
    "instance",
    "15",
    "puzzle",
    "seen",
    "numbers",
    "line",
    "call",
    "initially",
    "state",
    "need",
    "finding",
    "moves",
    "return",
    "similar",
    "maze",
    "another",
    "place",
    "sequence",
    "actions",
    "initial",
    "time",
    "solving",
    "translate",
    "real",
    "ever",
    "turns",
    "depending",
    "often",
    "algorithm",
    "position",
    "goes",
    "though",
    "going",
    "already",
    "used",
    "first",
    "think",
    "things",
    "around",
    "case",
    "representation",
    "person",
    "try",
    "next",
    "configuration",
    "three",
    "states",
    "slightly",
    "starting",
    "apply",
    "end",
    "ultimately",
    "always",
    "could",
    "define",
    "function",
    "called",
    "takes",
    "input",
    "inside",
    "output",
    "set",
    "four",
    "left",
    "available",
    "needs",
    "encoding",
    "numerical",
    "transition",
    "model",
    "action",
    "result",
    "two",
    "inputs",
    "let",
    "means",
    "pass",
    "board",
    "second",
    "represents",
    "allow",
    "possibilities",
    "represented",
    "exist",
    "show",
    "answer",
    "care",
    "results",
    "across",
    "entire",
    "form",
    "space",
    "0",
    "1",
    "2",
    "addition",
    "looks",
    "much",
    "larger",
    "sample",
    "general",
    "many",
    "else",
    "individual",
    "oftentimes",
    "thing",
    "nodes",
    "edges",
    "exact",
    "representing",
    "step",
    "done",
    "gets",
    "found",
    "encode",
    "test",
    "pretty",
    "easy",
    "checking",
    "happen",
    "complex",
    "imagine",
    "long",
    "sometimes",
    "cost",
    "last",
    "piece",
    "path",
    "said",
    "wanted",
    "route",
    "took",
    "longer",
    "option",
    "getting",
    "really",
    "less",
    "value",
    "associated",
    "sorts",
    "probably",
    "moving",
    "total",
    "steps",
    "equal",
    "constant",
    "consider",
    "go",
    "tells",
    "terms",
    "optimal",
    "cases",
    "defined",
    "whole",
    "bunch",
    "considering",
    "options",
    "together",
    "using",
    "node",
    "keep",
    "track",
    "values",
    "current",
    "relevant",
    "eventually",
    "forth",
    "words",
    "approach",
    "single",
    "frontier",
    "yet",
    "explored",
    "visited",
    "loop",
    "repeat",
    "process",
    "empty",
    "useful",
    "nothing",
    "otherwise",
    "remove",
    "ask",
    "talked",
    "neighbors",
    "add",
    "adding",
    "either",
    "meaning",
    "run",
    "put",
    "practice",
    "connected",
    "c",
    "ahead",
    "e",
    "say",
    "later",
    "choosing",
    "important",
    "part",
    "check",
    "question",
    "change",
    "true",
    "pick",
    "logic",
    "couple",
    "none",
    "far",
    "added",
    "decide",
    "choice",
    "stack",
    "notice",
    "f",
    "ca",
    "deep",
    "depth",
    "bfs",
    "queue",
    "looked",
    "away",
    "choose",
    "decision",
    "got",
    "made",
    "yes",
    "everything",
    "necessarily",
    "random",
    "making",
    "closer",
    "five",
    "six",
    "rather",
    "lot",
    "code",
    "context",
    "terminal",
    "calculate",
    "generate",
    "create",
    "functions",
    "known",
    "list",
    "recall",
    "negative",
    "update",
    "gives",
    "text",
    "file",
    "interesting",
    "saw",
    "moment",
    "notion",
    "plus",
    "equals",
    "backtrack",
    "future",
    "saying",
    "amount",
    "key",
    "open",
    "red",
    "green",
    "image",
    "running",
    "direction",
    "even",
    "must",
    "likewise",
    "higher",
    "close",
    "q",
    "seems",
    "situations",
    "row",
    "taken",
    "anything",
    "x",
    "upon",
    "square",
    "helpful",
    "closest",
    "estimate",
    "heuristic",
    "n",
    "distance",
    "math",
    "label",
    "10",
    "still",
    "6",
    "4",
    "12",
    "big",
    "says",
    "quickly",
    "pieces",
    "sum",
    "actual",
    "consistent",
    "3",
    "player",
    "games",
    "makes",
    "middle",
    "works",
    "opponent",
    "assign",
    "assigned",
    "max",
    "min",
    "score",
    "side",
    "mathematical",
    "equivalent",
    "encoded",
    "rules",
    "allows",
    "false",
    "calculation",
    "comes",
    "maximum",
    "minimum",
    "highest",
    "variable",
    "expression",
    "effect",
    "least",
    "third",
    "layers",
    "conclusion",
    "simple",
    "remaining",
    "alpha",
    "beta",
    "layer",
    "likely",
    "worse",
    "features",
    "conclude",
    "harry",
    "sentence",
    "rain",
    "hagrid",
    "dumbledore",
    "base",
    "combination",
    "inference",
    "logical",
    "english",
    "sentences",
    "symbols",
    "propositional",
    "p",
    "r",
    "raining",
    "symbol",
    "implication",
    "word",
    "variables",
    "implies",
    "tuesday",
    "worlds",
    "models",
    "rest",
    "query",
    "assignment",
    "library",
    "print",
    "common",
    "dealing",
    "mustard",
    "miss",
    "randomly",
    "train",
    "plum",
    "kernel",
    "clause",
    "minerva",
    "houses",
    "gryffindor",
    "belongs",
    "house",
    "blue",
    "sets",
    "rule",
    "prove",
    "testing",
    "resolution",
    "generalize",
    "clauses",
    "produce",
    "points",
    "appear",
    "times",
    "contradiction",
    "conditional",
    "binary",
    "phrase",
    "likelihood",
    "weather",
    "tomorrow",
    "chance",
    "die",
    "roll",
    "event",
    "positive",
    "probabilities",
    "patterns",
    "evidence",
    "training",
    "accurate",
    "calculating",
    "joint",
    "domain",
    "cloudy",
    "rainy",
    "light",
    "heavy",
    "delayed",
    "distribution",
    "vector",
    "multiply",
    "multiplied",
    "days",
    "counterfeit",
    "separate",
    "bayesian",
    "network",
    "appointment",
    "maintenance",
    "hidden",
    "predict",
    "sampling",
    "20",
    "samples",
    "weight",
    "sunny",
    "markov",
    "chain",
    "50",
    "umbrella",
    "category",
    "local",
    "hospitals",
    "hospital",
    "nearest",
    "global",
    "neighbor",
    "hill",
    "climbing",
    "temperature",
    "units",
    "categories",
    "linear",
    "x1",
    "x2",
    "constraint",
    "satisfaction",
    "exam",
    "monday",
    "wednesday",
    "classify",
    "unary",
    "arc",
    "consistency",
    "outputs",
    "hypothesis",
    "boundary",
    "weights",
    "sequences",
    "vectors",
    "predicted",
    "threshold",
    "loss",
    "sales",
    "rewards",
    "reward",
    "pile",
    "cluster",
    "pixels",
    "center",
    "neurons",
    "activation",
    "gradient",
    "pixel",
    "convolution",
    "pooling",
    "recurrent",
    "review",
    "noun",
    "verb",
    "pay"
  ]
}