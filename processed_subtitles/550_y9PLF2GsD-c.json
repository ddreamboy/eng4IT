{
  "text": "so in this video let's learn about\nrecurrent neural networks how they work\nhow they learn and also some of the\nimprovements that were made on them over\ntime so let's get started a nice thing\nto do before start talking about rnns is\nto remember how neural networks look\nlike so if you remember in a normal\nneural network or a deep neural network\nyou can call it\nwe have an input layer a hidden layer\nand an output layer and information runs\nsequentially so first the information\ncomes to the input layer then it is\npassed to the hidden layer and then\nfinally the output is produced so all of\nthe data points that you have you give\nthem at the same time all of the\nfeatures of your specific data point\nthat you're training on the difference\nin neural networks of recurrent neural\nnetworks is that they are recurrent so\nthat means that the same thing happens\nover and over again in a way\num so this is what a recurrent neural\nnetwork looks like when it's unrolled\nand i will also show you the rolled firm\nbut basically what you need to know is\nthat each of these little sections are\nactually\none of the same they're the same network\nor they are the same neurons so what\nhappens is these are all the different\ninput features and you give them one\nafter another so you first give the\nfirst feature of your input\nand output is calculated and also pass\nto the second time step we call them\ntime steps\nand then the second feature of your\ninput data input or a data point is\ngiven\nincluding the output of the first one\nanother output is calculated and so on\nand so forth\nso when you put it when you show it as\nit is actually and how it is is that it\nis just one\nunit\nso what happens is you give it an input\noutput is calculated but the same time\noutput is passed again the second\nfeature of the input is given so when\nyou compare it to neural networks what\nhappens is in neural networks you give\nall of the features at the same time\nwhereas in the current neural networks\nyou give all of the features one step at\na time so it is basically things happen\nin a time step manner so we would call\nthis the first time step the second time\nstep third fourth and fifth time step so\nbasically in each of these time steps\nexcept the first one you have two inputs\none of them is the output from the\nprevious time step and one of them is\nthe input of this time step so how do we\ncalculate the output of recurrent neural\nnetworks well we do it with this formula\nit's actually quite simple even though\nit looks like a complicated math formula\nthing\nyou have weights for your input so for\neach time step you have the weights\nand for all of the outputs from the\nprevious step you have another set of\nweights\nand you also have the biases all of this\nis of course passed through an\nactivation function and at the end you\nhave the output of your recurrent neural\nnetwork one thing to understand here is\nthat even though we have it looks like\nwe have many steps as i said it's\nactually just one unit so\nw x and w y which are the weights of the\nprevious the output of the previous\nlayer and the input of this layer or\ntime step is the same because they're\nactually the same here we're just\nshowing them in an unfold way because\nit's easier to understand so there's\nonly one set of\nweights for the input and one set of\nways for the output so there is a very\ncommon way of showing cells of\nrnns because these cells here how the\noutput is calculated can be different\nand we will talk about different types\nof cells too but let's talk about how to\ndepict a simple rnn cell\nso what we normally have of course as we\ntalked about the input at the output\nfrom the previous step time step and the\ninput of this time step so these t's\nbasically depict this time step t this\ntime step t minus 1 is the previous time\nstep\nso to show this formula on a diagram we\ncan say okay there is a multiplication\nhappening here this is multiplied with w\ny and here another multiplication in\nthis is multiply with wx\nand then we also add the bias here so\nthat's why you know the crosses and the\npluses here and this all goes through an\nactivation function of course and very\ncommonly what's used is the hyperbolic\ntangent function\num but of course you can use other\nthings too but generally hyperbolic\ntangent function is the one that is\nbeing used with simple rns and at the\nend we pass the output of this time step\nto the next time step and also we output\nit\nbut\nsometimes this is not the case so for\nsimplest of rnn cells this is the case\nwhere you pass the output to the outside\nworld and also to your next time step\nbut sometimes you\ndo an extra step\non before you output something so you\nmaybe pass it through a soft mag\nfunction if you want it to be between\nzero and one instead of minus one and\none because that's what the hyperbolic\ntangent function where it will produce\nuh then basically what you pass to the\nnext state will not go through the\nsoftmax function that extra step and\nthen it will still pass it to the next\nfunction next time step so\nit still is the same thing but\nhas seen less processing so then what\nhappens is we call this the hidden state\nof this time step so the hidden state of\nthis time step is passed to the next\ntime step so this is just a\ninteresting difference to keep in your\nmind that you do not always just pass\nthe output that you get to the next time\nstep but sometimes you call it the\nhidden hidden state there are of course\na bunch of ways how you can use this\narchitecture because you do not always\nhave to pass all of your or output all\nof the outputs that you calculate in\nyour cells so let's look at a couple of\ndifferent options the first type is\nsequence to sequence rnn model so\nbasically for every input that you give\nin each time step you get an output for\nthat time step and for these kind of\nthings you can use for things that where\nyou're forecasting things so for example\nprice price forecasting or stock\nexchange forecasting sort of things\nuh the second one that you can use is\ncalled sequence to vector sometimes it's\ncalled sequence to single because you\nonly get one output at the end of the\nwhole rnn uh network that you have um\nand these things that you use for let's\nsay you have a sentence or you have an\nemail and at the end of the\nor as an output of the model what you\nwant to know is is it scam or not or for\nexample sentiment analysis like what\nkind of sentiment does it have is it\nnegative or positive that kind of things\nso for that in each time step you give\nyour network a input and then you just\nignore the outputs even if it's\nproducing the outputs you do not look at\nthem the only output that you look at is\nthe one at the end because\nbefore seeing the whole sentence your\nmodel cannot come to a conclusion of\ncourse another one that you have is\nvector to sequence or as i said single\nto sequence uh by the way the reason\nthat they are not called single but\nvector here is that because the output\nthat you have is in the form of a vector\nmost of the time and it is not just a\nsingle number that's why we just call it\nvector instead of single but\nboth ways are fine so in vector the\nsequence sort of architectures what\nyou're doing is you're giving the\nnetwork one input and you are letting it\noutput a sequence of things so this\ncould be for example you give your\nnetwork an image and then you're having\nit output a explanation of that image\nword by word so let's say you give a\nphoto of a dog running on a beach then\nfor each of these time steps the network\nwill output\na dog running on the\nbeach for example so those are the kind\nof things that you would use a vector to\nsequence uh architecture for\nand lastly we have encoded decoder sort\nof architecture so in these kind of\narchitectures rn architectures at first\nyou are only giving your network inputs\nso input input input for a couple of\ntime steps or how many uh ever that you\nneed and then you get outputs\nso and then in the second part you only\nget outputs you do not give any inputs\nand these kind of architectures are good\nfor translation because to translate a\nsentence your network needs to see the\nwhole sentence words because\nmeaning of some words might change the\ntranslation of some words might change\nif you\nsee the whole sentence based on the\ncontext so that's why you first give it\nthe whole sentence and then you give the\ntrend get the translation word by word\non the decoder part all right but how\ndoes rnns learn so how does the training\nwork it's actually\nquite simple it's very similar to\nnormal neural networks that do not have\nany interesting architecture\nbut we just call it back propagation\nthrough time so what happens is the\noutput of the network is calculated of\ncourse and we do it in you can think of\nit as like it's unrolled for a form so\nat first you give it input as\nzero and then uh or the hidden states\nfrom the previous time step because it\ndoesn't exist as zero you give the first\ninput you get a first output you give\nthe second input you get a second output\nand then you just calculate which\nwhatever you want to calculate and then\nwe calculate the cost of this uh network\nbut as we said in the previous slide\nsometimes the network how you use the\nnet network might change so you might\nwant to ignore the first couple of\noutputs or you maybe you're just\ninterested in the last two outputs or\nmaybe even you're just interested in the\nlast one so based on that the cost is\ncalculated and then based on that as we\ndid with normal neural networks we\ncalculate the gradient and then the\ngradient is passed back uh through the\nnetwork and the weights are updated but\nas we said all the weights of all of\nthese time steps are actually the same\nbecause they're actually one time step\nand then the gradients are calculated as\nwe do with normal neural networks and\nthen these gradients are passed back in\nthe network to update the weights but as\ni said you might ignore some of the\noutputs while you're calculating cost so\nthe gradients are passed back only\nthrough the ones that you\nused in the cost calculation so rnns are\nactually really good for analyzing\nsequential data so this could be time\nseries data text or audio files for\nexample but of course they have some\nshortcomings so the first one is that\nthey have unstable gradients\nand you can imagine that right because\nit's a very long sequential sort of\narchitecture the further back you go in\nthis architecture\nthe smaller your gradients are going to\nget so you might not be able to update\nthe weights on the previous timestamps\nin a way or to previous time steps in a\nway that will be helpful for the whole\nnetwork\nwhat you can do for this problem in iron\nends is use other techniques that we use\nfor normal neural networks too or just\ndeep neural networks too to deal with\nunstable gradients or you can use layer\nnormalization instead of batch\nnormalization because batch\nnormalization is not as effective about\nyour current neural network so they're\nkind of tricky to apply to recurrent\nneural networks so instead you can use\nlayer normalization and another problem\nwith simpler noun cells is that they\nforget if you give it a very long\nsentence\nit tends to forget what was being said\nat the beginning of the sentence so the\napplications that are created with it\nare not really\neffective or they don't work as well so\ninstead we have the lstm or dru cells\nthat we can use to make sure that we\nstill remember the beginning of a\nsentence at the end of it so let's see\nwhat a lstm cell looks like if you\nremember how i showed you in that\ndiagram the rnn architecture the simple\nrnan cell looks like it will be easier\nto understand this one here\nso what we have is a couple of sigmoid\nactivation functions\nand then we have another hyperbolic\nactivation function and you know we here\nsee that there are two hidden\nstates that are passed from the previous\ntime step step to us and then we again\npass those two different hidden states\nto the next time step\nand then we again have an input and then\nwe have an output so let's look closely\nwhat all of these things mean so the\nfirst things that we need to understand\nhere is that as i said we have two\nhidden states coming and going to the\nprevious and next time steps the first\none is the previous one as we talked\nabout is just a hidden state from the\nprevious step but the other one c\nis a long-term memory hidden state so\nbasically as you say there are less\nthings happening to this hidden state\nand we either forget or add some things\nto this long-term hidden state and it\npasses to the next time step without\nmuch\ncoming out of it or going into it\num why do i know or how do i know things\nare being added or extracted from these\ntime steps well because\nwe have the forget gate here we have the\ninput gate here and we have the output\ngate here so basically these are the\ngates that\ncreate the information workflow or the\nhow this information is used in this\nspecific time step so let's talk a\nlittle bit more in detail about that so\nwhat we have here are called gate\ncontrollers so anything other than the\nhyperbolic tangent function we call the\ngate controls and what the gate\ncontrollers do is they either input zero\nor one and in this way they determine if\nsomething is going to be forgotten if\nsomething is going to be input or\nsomething is going to be output from the\nlong term state and how this works is\nbasically in the forget gate we decide\nwhich part of the long-term memory\nshould be removed from the long-term\nmemory so you know we're saying okay\nthis can be forgotten now we don't need\nto unders we don't need to remember this\ninformation anymore in the input gate we\ndecide which part of the hidden state or\nthe information that we just added to\nthe hidden state needs to also be passed\non to the long-term memory so we're\ndeciding okay actually this piece of\ninformation is important to remember\nlater and in the output gate we decide\nwhich part of the long-term state we\nneed to extract from the long-term state\nright now and use as an output either as\nin the hidden state to pass on the next\ntimestamp or as an output in this or as\npart of the output that we generate in\nthis uh time step so it sounds kind of\ncomplicated even though it's sort of\nintuitive that there is one long-term\nmemory one short term shorter term\nmemory and then either forget things\nfrom the long term one either use it or\nadd new things and it's kind of\nconfusing to understand okay but like\nhow does this thing hold work why\nwhy do we forget input or how does the\nforgetting and inputing happen\num but you don't have to understand\neverything behind it you don't have to\nunderstand how it's all working\nbasically what you need to know is that\nit is working and this is the intuition\nand i think that will get you\nwhere you need to go so you don't have\nto obsess over how this hole works\nso another different kind of cell that\nwe have is called a gru cell dru cell is\nbasically like a simplified version of\nthe lstm cell you know you have less\nthings happening here\nyou do not have a separate output as you\ncan see the hidden state that is passed\nto the next time step and the output is\nexactly the same and here the gate\ncontroller r decides which part of the\nprevious state will be shown in the main\nlayer so main layer being\nthe one where we pass through the\nhyperbolic tangent and basically the\nmain state that is being passed to the\nnext time step the r decides which part\nof the previous state that we got from\nthe previous timestamp is added to the\ntimestamp and the output of this or the\nstate and the\noutput of this time step and that's it\nthat's kind of like a first step at\nrnn's kind of beginner level information\nthat you need to know if you want to\nlearn more about rnns and deep learning\nin general go check out my course deep\nlearning 101. i will leave the link in\nthe description below before you leave\ndon't forget to give me a like and maybe\neven subscribe to show your support i\nwould also love to hear your opinions\nabout this video or any questions that\nyou have in the comment section below\nbut for now thanks for watching and i\nwill see you in the next video\n",
  "words": [
    "video",
    "let",
    "learn",
    "recurrent",
    "neural",
    "networks",
    "work",
    "learn",
    "also",
    "improvements",
    "made",
    "time",
    "let",
    "get",
    "started",
    "nice",
    "thing",
    "start",
    "talking",
    "rnns",
    "remember",
    "neural",
    "networks",
    "look",
    "like",
    "remember",
    "normal",
    "neural",
    "network",
    "deep",
    "neural",
    "network",
    "call",
    "input",
    "layer",
    "hidden",
    "layer",
    "output",
    "layer",
    "information",
    "runs",
    "sequentially",
    "first",
    "information",
    "comes",
    "input",
    "layer",
    "passed",
    "hidden",
    "layer",
    "finally",
    "output",
    "produced",
    "data",
    "points",
    "give",
    "time",
    "features",
    "specific",
    "data",
    "point",
    "training",
    "difference",
    "neural",
    "networks",
    "recurrent",
    "neural",
    "networks",
    "recurrent",
    "means",
    "thing",
    "happens",
    "way",
    "um",
    "recurrent",
    "neural",
    "network",
    "looks",
    "like",
    "unrolled",
    "also",
    "show",
    "rolled",
    "firm",
    "basically",
    "need",
    "know",
    "little",
    "sections",
    "actually",
    "one",
    "network",
    "neurons",
    "happens",
    "different",
    "input",
    "features",
    "give",
    "one",
    "another",
    "first",
    "give",
    "first",
    "feature",
    "input",
    "output",
    "calculated",
    "also",
    "pass",
    "second",
    "time",
    "step",
    "call",
    "time",
    "steps",
    "second",
    "feature",
    "input",
    "data",
    "input",
    "data",
    "point",
    "given",
    "including",
    "output",
    "first",
    "one",
    "another",
    "output",
    "calculated",
    "forth",
    "put",
    "show",
    "actually",
    "one",
    "unit",
    "happens",
    "give",
    "input",
    "output",
    "calculated",
    "time",
    "output",
    "passed",
    "second",
    "feature",
    "input",
    "given",
    "compare",
    "neural",
    "networks",
    "happens",
    "neural",
    "networks",
    "give",
    "features",
    "time",
    "whereas",
    "current",
    "neural",
    "networks",
    "give",
    "features",
    "one",
    "step",
    "time",
    "basically",
    "things",
    "happen",
    "time",
    "step",
    "manner",
    "would",
    "call",
    "first",
    "time",
    "step",
    "second",
    "time",
    "step",
    "third",
    "fourth",
    "fifth",
    "time",
    "step",
    "basically",
    "time",
    "steps",
    "except",
    "first",
    "one",
    "two",
    "inputs",
    "one",
    "output",
    "previous",
    "time",
    "step",
    "one",
    "input",
    "time",
    "step",
    "calculate",
    "output",
    "recurrent",
    "neural",
    "networks",
    "well",
    "formula",
    "actually",
    "quite",
    "simple",
    "even",
    "though",
    "looks",
    "like",
    "complicated",
    "math",
    "formula",
    "thing",
    "weights",
    "input",
    "time",
    "step",
    "weights",
    "outputs",
    "previous",
    "step",
    "another",
    "set",
    "weights",
    "also",
    "biases",
    "course",
    "passed",
    "activation",
    "function",
    "end",
    "output",
    "recurrent",
    "neural",
    "network",
    "one",
    "thing",
    "understand",
    "even",
    "though",
    "looks",
    "like",
    "many",
    "steps",
    "said",
    "actually",
    "one",
    "unit",
    "w",
    "x",
    "w",
    "weights",
    "previous",
    "output",
    "previous",
    "layer",
    "input",
    "layer",
    "time",
    "step",
    "actually",
    "showing",
    "unfold",
    "way",
    "easier",
    "understand",
    "one",
    "set",
    "weights",
    "input",
    "one",
    "set",
    "ways",
    "output",
    "common",
    "way",
    "showing",
    "cells",
    "rnns",
    "cells",
    "output",
    "calculated",
    "different",
    "talk",
    "different",
    "types",
    "cells",
    "let",
    "talk",
    "depict",
    "simple",
    "rnn",
    "cell",
    "normally",
    "course",
    "talked",
    "input",
    "output",
    "previous",
    "step",
    "time",
    "step",
    "input",
    "time",
    "step",
    "basically",
    "depict",
    "time",
    "step",
    "time",
    "step",
    "minus",
    "1",
    "previous",
    "time",
    "step",
    "show",
    "formula",
    "diagram",
    "say",
    "okay",
    "multiplication",
    "happening",
    "multiplied",
    "w",
    "another",
    "multiplication",
    "multiply",
    "wx",
    "also",
    "add",
    "bias",
    "know",
    "crosses",
    "pluses",
    "goes",
    "activation",
    "function",
    "course",
    "commonly",
    "used",
    "hyperbolic",
    "tangent",
    "function",
    "um",
    "course",
    "use",
    "things",
    "generally",
    "hyperbolic",
    "tangent",
    "function",
    "one",
    "used",
    "simple",
    "rns",
    "end",
    "pass",
    "output",
    "time",
    "step",
    "next",
    "time",
    "step",
    "also",
    "output",
    "sometimes",
    "case",
    "simplest",
    "rnn",
    "cells",
    "case",
    "pass",
    "output",
    "outside",
    "world",
    "also",
    "next",
    "time",
    "step",
    "sometimes",
    "extra",
    "step",
    "output",
    "something",
    "maybe",
    "pass",
    "soft",
    "mag",
    "function",
    "want",
    "zero",
    "one",
    "instead",
    "minus",
    "one",
    "one",
    "hyperbolic",
    "tangent",
    "function",
    "produce",
    "uh",
    "basically",
    "pass",
    "next",
    "state",
    "go",
    "softmax",
    "function",
    "extra",
    "step",
    "still",
    "pass",
    "next",
    "function",
    "next",
    "time",
    "step",
    "still",
    "thing",
    "seen",
    "less",
    "processing",
    "happens",
    "call",
    "hidden",
    "state",
    "time",
    "step",
    "hidden",
    "state",
    "time",
    "step",
    "passed",
    "next",
    "time",
    "step",
    "interesting",
    "difference",
    "keep",
    "mind",
    "always",
    "pass",
    "output",
    "get",
    "next",
    "time",
    "step",
    "sometimes",
    "call",
    "hidden",
    "hidden",
    "state",
    "course",
    "bunch",
    "ways",
    "use",
    "architecture",
    "always",
    "pass",
    "output",
    "outputs",
    "calculate",
    "cells",
    "let",
    "look",
    "couple",
    "different",
    "options",
    "first",
    "type",
    "sequence",
    "sequence",
    "rnn",
    "model",
    "basically",
    "every",
    "input",
    "give",
    "time",
    "step",
    "get",
    "output",
    "time",
    "step",
    "kind",
    "things",
    "use",
    "things",
    "forecasting",
    "things",
    "example",
    "price",
    "price",
    "forecasting",
    "stock",
    "exchange",
    "forecasting",
    "sort",
    "things",
    "uh",
    "second",
    "one",
    "use",
    "called",
    "sequence",
    "vector",
    "sometimes",
    "called",
    "sequence",
    "single",
    "get",
    "one",
    "output",
    "end",
    "whole",
    "rnn",
    "uh",
    "network",
    "um",
    "things",
    "use",
    "let",
    "say",
    "sentence",
    "email",
    "end",
    "output",
    "model",
    "want",
    "know",
    "scam",
    "example",
    "sentiment",
    "analysis",
    "like",
    "kind",
    "sentiment",
    "negative",
    "positive",
    "kind",
    "things",
    "time",
    "step",
    "give",
    "network",
    "input",
    "ignore",
    "outputs",
    "even",
    "producing",
    "outputs",
    "look",
    "output",
    "look",
    "one",
    "end",
    "seeing",
    "whole",
    "sentence",
    "model",
    "come",
    "conclusion",
    "course",
    "another",
    "one",
    "vector",
    "sequence",
    "said",
    "single",
    "sequence",
    "uh",
    "way",
    "reason",
    "called",
    "single",
    "vector",
    "output",
    "form",
    "vector",
    "time",
    "single",
    "number",
    "call",
    "vector",
    "instead",
    "single",
    "ways",
    "fine",
    "vector",
    "sequence",
    "sort",
    "architectures",
    "giving",
    "network",
    "one",
    "input",
    "letting",
    "output",
    "sequence",
    "things",
    "could",
    "example",
    "give",
    "network",
    "image",
    "output",
    "explanation",
    "image",
    "word",
    "word",
    "let",
    "say",
    "give",
    "photo",
    "dog",
    "running",
    "beach",
    "time",
    "steps",
    "network",
    "output",
    "dog",
    "running",
    "beach",
    "example",
    "kind",
    "things",
    "would",
    "use",
    "vector",
    "sequence",
    "uh",
    "architecture",
    "lastly",
    "encoded",
    "decoder",
    "sort",
    "architecture",
    "kind",
    "architectures",
    "rn",
    "architectures",
    "first",
    "giving",
    "network",
    "inputs",
    "input",
    "input",
    "input",
    "couple",
    "time",
    "steps",
    "many",
    "uh",
    "ever",
    "need",
    "get",
    "outputs",
    "second",
    "part",
    "get",
    "outputs",
    "give",
    "inputs",
    "kind",
    "architectures",
    "good",
    "translation",
    "translate",
    "sentence",
    "network",
    "needs",
    "see",
    "whole",
    "sentence",
    "words",
    "meaning",
    "words",
    "might",
    "change",
    "translation",
    "words",
    "might",
    "change",
    "see",
    "whole",
    "sentence",
    "based",
    "context",
    "first",
    "give",
    "whole",
    "sentence",
    "give",
    "trend",
    "get",
    "translation",
    "word",
    "word",
    "decoder",
    "part",
    "right",
    "rnns",
    "learn",
    "training",
    "work",
    "actually",
    "quite",
    "simple",
    "similar",
    "normal",
    "neural",
    "networks",
    "interesting",
    "architecture",
    "call",
    "back",
    "propagation",
    "time",
    "happens",
    "output",
    "network",
    "calculated",
    "course",
    "think",
    "like",
    "unrolled",
    "form",
    "first",
    "give",
    "input",
    "zero",
    "uh",
    "hidden",
    "states",
    "previous",
    "time",
    "step",
    "exist",
    "zero",
    "give",
    "first",
    "input",
    "get",
    "first",
    "output",
    "give",
    "second",
    "input",
    "get",
    "second",
    "output",
    "calculate",
    "whatever",
    "want",
    "calculate",
    "calculate",
    "cost",
    "uh",
    "network",
    "said",
    "previous",
    "slide",
    "sometimes",
    "network",
    "use",
    "net",
    "network",
    "might",
    "change",
    "might",
    "want",
    "ignore",
    "first",
    "couple",
    "outputs",
    "maybe",
    "interested",
    "last",
    "two",
    "outputs",
    "maybe",
    "even",
    "interested",
    "last",
    "one",
    "based",
    "cost",
    "calculated",
    "based",
    "normal",
    "neural",
    "networks",
    "calculate",
    "gradient",
    "gradient",
    "passed",
    "back",
    "uh",
    "network",
    "weights",
    "updated",
    "said",
    "weights",
    "time",
    "steps",
    "actually",
    "actually",
    "one",
    "time",
    "step",
    "gradients",
    "calculated",
    "normal",
    "neural",
    "networks",
    "gradients",
    "passed",
    "back",
    "network",
    "update",
    "weights",
    "said",
    "might",
    "ignore",
    "outputs",
    "calculating",
    "cost",
    "gradients",
    "passed",
    "back",
    "ones",
    "used",
    "cost",
    "calculation",
    "rnns",
    "actually",
    "really",
    "good",
    "analyzing",
    "sequential",
    "data",
    "could",
    "time",
    "series",
    "data",
    "text",
    "audio",
    "files",
    "example",
    "course",
    "shortcomings",
    "first",
    "one",
    "unstable",
    "gradients",
    "imagine",
    "right",
    "long",
    "sequential",
    "sort",
    "architecture",
    "back",
    "go",
    "architecture",
    "smaller",
    "gradients",
    "going",
    "get",
    "might",
    "able",
    "update",
    "weights",
    "previous",
    "timestamps",
    "way",
    "previous",
    "time",
    "steps",
    "way",
    "helpful",
    "whole",
    "network",
    "problem",
    "iron",
    "ends",
    "use",
    "techniques",
    "use",
    "normal",
    "neural",
    "networks",
    "deep",
    "neural",
    "networks",
    "deal",
    "unstable",
    "gradients",
    "use",
    "layer",
    "normalization",
    "instead",
    "batch",
    "normalization",
    "batch",
    "normalization",
    "effective",
    "current",
    "neural",
    "network",
    "kind",
    "tricky",
    "apply",
    "recurrent",
    "neural",
    "networks",
    "instead",
    "use",
    "layer",
    "normalization",
    "another",
    "problem",
    "simpler",
    "noun",
    "cells",
    "forget",
    "give",
    "long",
    "sentence",
    "tends",
    "forget",
    "said",
    "beginning",
    "sentence",
    "applications",
    "created",
    "really",
    "effective",
    "work",
    "well",
    "instead",
    "lstm",
    "dru",
    "cells",
    "use",
    "make",
    "sure",
    "still",
    "remember",
    "beginning",
    "sentence",
    "end",
    "let",
    "see",
    "lstm",
    "cell",
    "looks",
    "like",
    "remember",
    "showed",
    "diagram",
    "rnn",
    "architecture",
    "simple",
    "rnan",
    "cell",
    "looks",
    "like",
    "easier",
    "understand",
    "one",
    "couple",
    "sigmoid",
    "activation",
    "functions",
    "another",
    "hyperbolic",
    "activation",
    "function",
    "know",
    "see",
    "two",
    "hidden",
    "states",
    "passed",
    "previous",
    "time",
    "step",
    "step",
    "us",
    "pass",
    "two",
    "different",
    "hidden",
    "states",
    "next",
    "time",
    "step",
    "input",
    "output",
    "let",
    "look",
    "closely",
    "things",
    "mean",
    "first",
    "things",
    "need",
    "understand",
    "said",
    "two",
    "hidden",
    "states",
    "coming",
    "going",
    "previous",
    "next",
    "time",
    "steps",
    "first",
    "one",
    "previous",
    "one",
    "talked",
    "hidden",
    "state",
    "previous",
    "step",
    "one",
    "c",
    "memory",
    "hidden",
    "state",
    "basically",
    "say",
    "less",
    "things",
    "happening",
    "hidden",
    "state",
    "either",
    "forget",
    "add",
    "things",
    "hidden",
    "state",
    "passes",
    "next",
    "time",
    "step",
    "without",
    "much",
    "coming",
    "going",
    "um",
    "know",
    "know",
    "things",
    "added",
    "extracted",
    "time",
    "steps",
    "well",
    "forget",
    "gate",
    "input",
    "gate",
    "output",
    "gate",
    "basically",
    "gates",
    "create",
    "information",
    "workflow",
    "information",
    "used",
    "specific",
    "time",
    "step",
    "let",
    "talk",
    "little",
    "bit",
    "detail",
    "called",
    "gate",
    "controllers",
    "anything",
    "hyperbolic",
    "tangent",
    "function",
    "call",
    "gate",
    "controls",
    "gate",
    "controllers",
    "either",
    "input",
    "zero",
    "one",
    "way",
    "determine",
    "something",
    "going",
    "forgotten",
    "something",
    "going",
    "input",
    "something",
    "going",
    "output",
    "long",
    "term",
    "state",
    "works",
    "basically",
    "forget",
    "gate",
    "decide",
    "part",
    "memory",
    "removed",
    "memory",
    "know",
    "saying",
    "okay",
    "forgotten",
    "need",
    "unders",
    "need",
    "remember",
    "information",
    "anymore",
    "input",
    "gate",
    "decide",
    "part",
    "hidden",
    "state",
    "information",
    "added",
    "hidden",
    "state",
    "needs",
    "also",
    "passed",
    "memory",
    "deciding",
    "okay",
    "actually",
    "piece",
    "information",
    "important",
    "remember",
    "later",
    "output",
    "gate",
    "decide",
    "part",
    "state",
    "need",
    "extract",
    "state",
    "right",
    "use",
    "output",
    "either",
    "hidden",
    "state",
    "pass",
    "next",
    "timestamp",
    "output",
    "part",
    "output",
    "generate",
    "uh",
    "time",
    "step",
    "sounds",
    "kind",
    "complicated",
    "even",
    "though",
    "sort",
    "intuitive",
    "one",
    "memory",
    "one",
    "short",
    "term",
    "shorter",
    "term",
    "memory",
    "either",
    "forget",
    "things",
    "long",
    "term",
    "one",
    "either",
    "use",
    "add",
    "new",
    "things",
    "kind",
    "confusing",
    "understand",
    "okay",
    "like",
    "thing",
    "hold",
    "work",
    "forget",
    "input",
    "forgetting",
    "inputing",
    "happen",
    "um",
    "understand",
    "everything",
    "behind",
    "understand",
    "working",
    "basically",
    "need",
    "know",
    "working",
    "intuition",
    "think",
    "get",
    "need",
    "go",
    "obsess",
    "hole",
    "works",
    "another",
    "different",
    "kind",
    "cell",
    "called",
    "gru",
    "cell",
    "dru",
    "cell",
    "basically",
    "like",
    "simplified",
    "version",
    "lstm",
    "cell",
    "know",
    "less",
    "things",
    "happening",
    "separate",
    "output",
    "see",
    "hidden",
    "state",
    "passed",
    "next",
    "time",
    "step",
    "output",
    "exactly",
    "gate",
    "controller",
    "r",
    "decides",
    "part",
    "previous",
    "state",
    "shown",
    "main",
    "layer",
    "main",
    "layer",
    "one",
    "pass",
    "hyperbolic",
    "tangent",
    "basically",
    "main",
    "state",
    "passed",
    "next",
    "time",
    "step",
    "r",
    "decides",
    "part",
    "previous",
    "state",
    "got",
    "previous",
    "timestamp",
    "added",
    "timestamp",
    "output",
    "state",
    "output",
    "time",
    "step",
    "kind",
    "like",
    "first",
    "step",
    "rnn",
    "kind",
    "beginner",
    "level",
    "information",
    "need",
    "know",
    "want",
    "learn",
    "rnns",
    "deep",
    "learning",
    "general",
    "go",
    "check",
    "course",
    "deep",
    "learning",
    "leave",
    "link",
    "description",
    "leave",
    "forget",
    "give",
    "like",
    "maybe",
    "even",
    "subscribe",
    "show",
    "support",
    "would",
    "also",
    "love",
    "hear",
    "opinions",
    "video",
    "questions",
    "comment",
    "section",
    "thanks",
    "watching",
    "see",
    "next",
    "video"
  ],
  "keywords": [
    "let",
    "learn",
    "recurrent",
    "neural",
    "networks",
    "work",
    "also",
    "time",
    "get",
    "thing",
    "rnns",
    "remember",
    "look",
    "like",
    "normal",
    "network",
    "deep",
    "call",
    "input",
    "layer",
    "hidden",
    "output",
    "information",
    "first",
    "passed",
    "data",
    "give",
    "features",
    "happens",
    "way",
    "um",
    "looks",
    "show",
    "basically",
    "need",
    "know",
    "actually",
    "one",
    "different",
    "another",
    "calculated",
    "pass",
    "second",
    "step",
    "steps",
    "things",
    "two",
    "previous",
    "calculate",
    "simple",
    "even",
    "weights",
    "outputs",
    "course",
    "activation",
    "function",
    "end",
    "understand",
    "said",
    "cells",
    "rnn",
    "cell",
    "say",
    "okay",
    "used",
    "hyperbolic",
    "tangent",
    "use",
    "next",
    "sometimes",
    "something",
    "maybe",
    "want",
    "zero",
    "instead",
    "uh",
    "state",
    "go",
    "architecture",
    "couple",
    "sequence",
    "kind",
    "example",
    "sort",
    "called",
    "vector",
    "single",
    "whole",
    "sentence",
    "architectures",
    "word",
    "part",
    "see",
    "might",
    "back",
    "states",
    "cost",
    "gradients",
    "long",
    "going",
    "normalization",
    "forget",
    "memory",
    "either",
    "gate",
    "term"
  ]
}