{
  "text": "Once I wanted to buy nest thermostat and\nI wasn't sure if I should be buying that\nor not.\nI then called four of my friends who\nalready have that device and then asked\nfor their opinion. Three of them told me\nI should buy it. One guy told me no I\nshouldn't buy it, and I just took a\nmajority vote and I went ahead and\ninstalled nest thermostat in my home.\nWe use ensemble learning in our real life,\nwhere to make a decision we take opinion\nfrom different people. Similarly in\nmachine learning\nsometimes what happens is if you have\njust one model and if you train\nthat model\nusing all your data set, that model might\nget over fit or it might suffer from a\nhigh variance problem. If you don't know\nabout bias and variance I have another\nvideo, which you must check before\ncontinuing on this video but to tackle\nthis high variance problem\nwe can use ensemble learning. In the case\nof my nest thermostat why didn't I call\njust one of my friend, because\nthat if I call only one person that\nperson might be biased. So I wanted to\nmake a good decision. Hence I talked to\nmultiple people and took a majority\nvote. In ensemble learning we train\nmultiple models on a same data set and\nthen we\nwhen we do prediction\nwe\ndo prediction on multiple models and\nthen whatever output, we get we combine\nthat result somehow to get our final\noutput.\nBagging and Boosting are the two main\ntechniques used in ensemble.. ensemble\nlearning and in this video we are going\nto talk about Bagging. We'll also write\nsome python code. Let's get started\nLet's say I have this data set of 100\nsamples and when I train a machine\nlearning model one of the problem I\nmight encounter is overfitting and\noverfitting happens due to the nature of\nthe data set, your machine learning,\nmethodology, etc.,\nand usually overfitting model has a\nproblem of high variance.\nTo tackle this problem one of the things\nyou can do is out of 100 samples, create\na small data set of 70 samples. Let's say\nI'm just giving an example\nand the way you create\nthis subset is by using resampling with\nreplacement. Now what is that exactly?\nLet's first understand that. Let's say I\nhave this 10 samples,\nout of which I want to create smaller\ndata set with four samples.\nIn resampling with replacement, we\nrandomly pick any data point; let's say\nfour\nand then\nwhen we go and pick second data point we\nagain randomly pick any data point from\n1 to 10 with equal probability. We don't\nlook at what we already have in our\nsubset.\nSo second time I will let's say get 8.\nThird time also we randomly pick any\ndata point from 1 to 10 and this time I\nmight get same sample again. So this is a\nresampling with replacement where\nin my subset I can get same data sample\nmultiple times.\nSo here,\nfrom my original data set I created a\nsubset of 70 sample. I might create n\nnumber of such\nsmaller data sets from my original data\nset, using resampling with replacement\nand then on individual data set I can\ntrain my machine learning model. Let's\nsay I'm trying to classify if a person\nshould buy insurance or not and I'm\nusing logistic regression. So I will use\nlogistic regression model. So here M1 M2\nM3 they all are logistic regression but\nthey are trained on a different\ndata set.\nAnd when they're trained and now I have\nto perform the prediction or inference\nI will\nperform that prediction on all three\nmodels in parallel individually\nand whatever result I get I just take a\nmajority vote. So here M1 and M3 is\nsaying person should buy insurance M2 is\nsaying\nthey should not.\nMajority vote is clearly one and that is\nmy final outcome. The benefit you get\nhere is these individual models are weak\nlearners. Weak learners means\nthey are trained on a subset of data set\nand hence\nthey\nthey will not overfit. You know it is\nlikely that they will generalize better,\nthey will not over fit and these\nindividual weak learners when you\ncombine the result you get\noverall a good quality result. This was a\ncase of classification, same thing\napplies for regression. Let's say you're\ndoing housing price prediction.\nHere,\nyou take an average of\nwhatever is the prediction by individual\nmodel.\nNow\nthis technique is also called Bootstrap\nAggregation because\nwhen you are creating this small\nsubset of data set using\nresampling with replacement\nthat\nprocedure is called bootstrap, and when\nyou combine their results using either\nan average or majority vote that is\ncalled aggregation. So hence bagging is\nalso called bootstrap aggregation.\nMany many times you hear all these terms\nand jargons and you get worried what it\nis, but really these concepts are very\nvery easy. You know you already\nunderstood what bootstrap aggregation\nmeans.\nNow Random Forest is one of the bagging\ntechnique with only one distinction,\nwhich is\nhere we not only\nsample\nyour your data rows but you also samples\nyour features. So basically you sample\nyour rows as well as your columns.\nLet's look at our classical housing\nprice prediction example where town, area,\nbedroom, etc are features and pricing\nwhich is a green column is your target\nvariable. Here you will\nsample\nrows and columns both.\nSo here you can see\nI don't have a bedroom column or a plot\ncolumn. I randomly resample out of see one\ntwo three four five six out of seven\ncolumns, I got only four columns. In the\nsecond time\nagain I randomly sample\nthis column. So I did not get\nuh for example here bedrooms in this\nparticular\ndata set. And in the third one for\nexample here I did not get\nschool rating. Okay.\nSo you are resampling\nyou're randomly picking rows as well as\ncolumns then on individual data set you\ntrain a decision tree model,\nand then you aggregate the results. Here\nI have decision tree regression\nyou can use it for classification\nproblem as well. But the point is very\nsimple random tree is basically a\nbagging technique, but\nhere\nwe do one additional thing which is we\nrandomly pick\nfeatures as well.\nAnd the difference between these terms\nbagging and bagged trees is that, in\nbagging individual model\ncan be svm knn logistic regression\npretty much any model. Whereas in bagged\ntrees, so random forest is a bag tree\nexample.\nHere the every model that you're\ntraining is\na tree. Alright so I hope\nthe theory is clear. Let's move on to\npython coding using SKlearn.\nI will be using this data set for the\ncoding today. The data set is about Pima\nIndian Diabetes where\nbased on certain features you have to\npredict whether the person has a\ndiabetes or not. By clicking on this link\nI have downloaded this csv file, which\nlooks something like this. Here the\nfeatures are pregnancies, glucose, blood\npressure; these are all contributing\nfactors for diabetes.\nBased on these you are deciding whether\nthe person has diabetes or not.\nI have loaded this csv file in pandas\ndata frame as you can see here\nand as soon as I load data in my data\nframe, I like to do some basic\nexploration.\nSo let's first start and find out if\nthere is any column which has null\nvalues.\nSo the way you find it out is you will\ndo df dot is null dot sum.\nThis will tell you in this column, let's\nsay if the number is 5 which means it\nhas 5 null values. But we are lucky here\nthere are no null values, so no need to\nworry about it.\nSecond thing that I do is df.describe.\nThis tells me the basic statistics for\neach of the columns.\nFor pregnancies look at min and max. You\nknow max, I understand 17 is little high\nbut it's not unrealistic.\nWhen I examine min max values in all\nthese columns they look normal and I\ndon't feel like we need to do any\noutlier\ndetection or\noutlier removal, etc. So I will\njust go ahead\nand assume\nthat there are no outliers.\nThe second thing I check is\nwhether there is any imbalance in the\ndata set. Because see\nthere's this outcome right. So here\nif you do value counts what you will\nfind is\nthere are 500 samples\nwhich which says no diabetes, 268 samples\nwhich says yes person has a diabetes,\nand if you look at the ratio\nit's\n0.53. So it looks like some imbalance but\nit is not a major imbalance. Major\nimbalance would be like you know 10 to 1\nor 100 to 1 ratio.\nThis is more like\n2 to 1 ratio, you know.\nSo I would\nsay that this\nthere is slight imbalance, but it's it's\nnot something that you should worry\nabout. So I would go ahead and\nmove on to the next stage, which is\ncreating x and y. So my x will be df dot\ndrop, because outcome is my target column.\nI need to drop that\nand the way you do that is\nby using drop function in pandas\nand in the\naxis you will say columns.\nAnd y would be df dot outcome.\nOkay so my x and y is ready.\nNow I will do scaling, because\nthe values are\non a different scale, here. You can see\nthis particular number you know 0 to 17\nversus 0 to 199.\nI mean it's not a huge difference in the\nscale but still just to be on a safe\nside\nI will use standard scaling. You can use\nmin max scalar as well.\nLet's create our scaler object, scaler\nand if you have followed my previous\nvideos you know how to use scalar object\nyou can just say fit transform x\nand what you get in return is your x\nscale. This will be NumPy array. Hence I\nwill just print like first three rows\nout of it.\nYou can see the values are scaled.\nIf you use min max scalar, you'll get\ndifferent set of values but both works\nokay. Now\nthis should be in your muscle memory.\nWhen you have x scale and y ready you\nwill do train test split\nand for that I will import this method\nin train test split what do you supply x\nand y, right x and y. What is our x? x\nscale\nright we want to use the scale value and\nthen\nin the output,\nyou know the standard output that I get\nis x train x test y train and y test.\nNow here,\nI will use stratify argument because\nthere is slight imbalance. So I want to\nmake sure the test and train data set e\nhas equal number of samples like\nequal proportion basically and I will\nsay y I mean it won't be equal but at\nleast the\nthis ratio should be maintained\nand\nrandom state I will set to\n10 or maybe 20 let's go 10.\nThis is just a random number by the way\nand\nit allows you the\nreproducibility. Every time you run this\nmethod you will get same x train,\ny train.\nSo if you do the shape okay 576 samples\nin your train data set\nand this data set has\n192\nand if you look at this.\nOh no actually you know what I should be\nlooking at\ny\ntrain dot value counts\nand if you look at this\naround same kind of ratio that you saw\nhere, okay.\nNow,\nwe will\nuse decision tree classifier\nto train a standalone model. You can use\nsvm kanye west neighbor any\nclassification model.\nI am using decision tree to demonstrate\nthat decision tree is relatively\nimbalanced classifier.\nIt can overfit and it will\nyou know it might generate an\num high variance model.\nSo\nlet's train decision tree\nfirst and I will use cross validation\nscore here,\nto just try it out on a different type\nof data set rather than just x train\nand you know x test.\nSo cross validation, if you don't know\nabout cross-validation score I have done\nseparate video on k-fold\ncross-validation. You should watch that\nif you are not aware. Otherwise\nyou know this thing will bounce off your\nhead.\nSo this isn't cross validation score\nexpects a classifier which is my\ndecision tree classifier then x and y\nand then cv is equal to\n5 10;\nI'll give 5.\nSo what this will do is\nuh\nif I have\nif I have um\n768 samples.\nIt will divide them into five folds\nand it will\ntry different\nset of\nx test and\nyou know y test to\nx train and x test to try the model.\nYou should watch my video on k4 cross\nvalidation. You will get a good\nunderstanding of it. And this will return\nthis will do like five time training and\nall those training results would be\ninside the scores.\nSo see the score that you got\nby running 5 iteration of training is\nthis and if you take a this is a NumPy\narray, so just take a mean of it.\nYou'll find your model is giving you 71\npercent accuracy, which is okay.\nBut now let me use bagging classifier\nand\nfirst thing you can do is\nask your friend sklearn bagging\nclassifier. Your friend is Google and\nGoogle will tell you\nwhich api you need to use.\nSo here see,\nI will use the most important tool for\nany programmer which is copy paste\nand I\ncreate a bagging classifier.\nAnd backing classifier you can read all\nthe arguments but\nI'm just going to use couple of\narguments. First of all okay which\nestimator you are using. Well I am using\ndecision tree.\nHow many estimators like how many sub\ngroups of data set? 100,\ntrial and error, okay. You try 10 20\nfigure out, which one is giving you best\nperformance and this 100 is nothing but\nthis.\nSee in my presentation I said\n3 model,\nI am doing\n100 models and 100 subset of data sets\nand I will be training them in parallel\nand how many samples. See here we used\n70 out of 100.\nSo for sampling use 80%,\nuse 80% of my samples.\nThere is another thing called\noob score. So oob score is equal to.. now\nwhat is oob score? Well\noob means out of bag.\nWhen you are sampling,\nbecause you are doing random\nsampling by the law of probability\nyou are not going to cover all 100\nsamples in this subset. Let's say in this\nsubset,\nall this subset\nnumber\n29 did not appear at all.\nOkay, so number 29, let's say number 29 is\nhere right 1 to 100 number,\nthat 29 number sample\ndid not appear in any of this subset.\nSo now\nall these models which are trained they\nhave not seen that data data point 29.\nSo you can use that 29\nto test\nthe accuracy of these models. So you are\nkind of treating that 29 sample as a\ntest data set.\nIdeally what you do is you take your\ndata set, you split it into train\nand test. So this\ndiagram that I have shown here this\nblock is actually your x train. So your x\ntest is separate already, which you will\nbe\nusing to test the performance of your\nfinal model before before deploying that\ninto a wild.\nBut even within x train because of our\nsampling strategy you might miss some\nsamples. Let's say you might you have\n20 data samples, which which\nwhich has not appeared in any of this\nsubset and those 20 samples after these\nmodels are trained. You can use those 20\nsamples for prediction take the majority\nvote\nand figure out what was the accuracy, and\nthat accuracy is your oob score.\nSo you realize that okay okay let me\nfirst do random state here.\nAgain random state is for predictability\nand I will call this a bag model and\nwhen I have a bag model\nI will do\noob score.\nOob\nscore.\noob \n[Music]\nBagging\nclassifier.\nActually you know what\nI have to fit\ndot fit.\nSo I am doing x you know x and y train\nfit\nand then I get this.\nYou realize I did not try even x test\nand y test,\non training data set when I did 80\nsamples. When I train 100 classifier, it\nmight have missed some of the samples\nfrom my training data set and\non them I ran my\nmodel prediction\nand the accuracy I got is stored in this\noob score.\nnow I can do\nregular scoring x test y test and you\nsee improvement right\n77 percent versus standalone model\ngiving you 71 percent. Now I agree you\nwill tell me here you use cross\nvalidation. Here, I did not use cross\nvalidation. So let me use cross\nvalidation then. So I'm going to do some\ncopy paste magic\nand create the same bagging model and\nthen\nuse\ncross validation scroll, okay. In cross\nvalidation score what do you supply? You\nsupply first your model\nthen x\nthen y and how many\nfolds well five folds, okay.\nYou get scores back\nand those scores\nyou can just take a mean.\nYou will find that\nthe base model gives you seventy one\npercent accuracy\nbagged model gives you seventy five\npercent accuracy. So for\nunstable classifier like decision tree,\nbagging helps.\nIf you have a classifier\nyou know sometimes, you have unstable\nclassifier like decision tree and\nsometimes your data set is such that\nthere are so many null values, you know\nyour columns are such that\nyour resulting model has high variance.\nAnd whenever you have this high variance\nit makes sense to use bagging classifier. \nNow,\nwe talked about random forest. So let's\nlet me try random forest as well on this\nparticular data set. So\nI will try random forest here and I will\nthey say okay random forest classifier x\ny cv equal to 5, I get scores\nand\npretty\nstraightforward x\nmean.\nrandom forest classifier gives me one\nlittle better performance.\nInside like underneath it will use\nbagging.\nIt will not only sample your data rows\nbut it will sample your\nfeature columns as well as we saw in the\npresentation.\nNow comes the most important part of\nthis video, which is an exercise.\nLearning coding or data science is like\nlearning swimming. By watching the\nswimming video you are not going to\nlearn swimming obviously. Similarly\nyou need to work on this exercise,\notherwise\nit will be hard to grasp the concepts\nwhich I\njust taught you.\nHere I'm giving\na csv file, which I took from kaggle by\nthe way and it is about heart disease\nprediction.\nYou have to load the data set apply some\noutlier removal. I have given all the\ninformation here. So\nwork on this exercise and once you have\nput your sensor effort then only click\non solution link, because I have an AI\ntechnology built into this video where\nif you click on this link without trying\nout on your own your laptop or computer\nwill get a fever\nand it will not recover for next 10 days!\nOkay, so you will miss all the fun. So\nbetter you try first on your own and\nthen click on the solution link. I hope\nthis you like this video. If you did give\nit a thumbs up!! at least\nand share it with your friends. I wish\nyou all the best. If you have any\nquestion,\nthere is a comment section below. Thank\nyou!\n",
  "words": [
    "wanted",
    "buy",
    "nest",
    "thermostat",
    "sure",
    "buying",
    "called",
    "four",
    "friends",
    "already",
    "device",
    "asked",
    "opinion",
    "three",
    "told",
    "buy",
    "one",
    "guy",
    "told",
    "buy",
    "took",
    "majority",
    "vote",
    "went",
    "ahead",
    "installed",
    "nest",
    "thermostat",
    "home",
    "use",
    "ensemble",
    "learning",
    "real",
    "life",
    "make",
    "decision",
    "take",
    "opinion",
    "different",
    "people",
    "similarly",
    "machine",
    "learning",
    "sometimes",
    "happens",
    "one",
    "model",
    "train",
    "model",
    "using",
    "data",
    "set",
    "model",
    "might",
    "get",
    "fit",
    "might",
    "suffer",
    "high",
    "variance",
    "problem",
    "know",
    "bias",
    "variance",
    "another",
    "video",
    "must",
    "check",
    "continuing",
    "video",
    "tackle",
    "high",
    "variance",
    "problem",
    "use",
    "ensemble",
    "learning",
    "case",
    "nest",
    "thermostat",
    "call",
    "one",
    "friend",
    "call",
    "one",
    "person",
    "person",
    "might",
    "biased",
    "wanted",
    "make",
    "good",
    "decision",
    "hence",
    "talked",
    "multiple",
    "people",
    "took",
    "majority",
    "vote",
    "ensemble",
    "learning",
    "train",
    "multiple",
    "models",
    "data",
    "set",
    "prediction",
    "prediction",
    "multiple",
    "models",
    "whatever",
    "output",
    "get",
    "combine",
    "result",
    "somehow",
    "get",
    "final",
    "output",
    "bagging",
    "boosting",
    "two",
    "main",
    "techniques",
    "used",
    "ensemble",
    "ensemble",
    "learning",
    "video",
    "going",
    "talk",
    "bagging",
    "also",
    "write",
    "python",
    "code",
    "let",
    "get",
    "started",
    "let",
    "say",
    "data",
    "set",
    "100",
    "samples",
    "train",
    "machine",
    "learning",
    "model",
    "one",
    "problem",
    "might",
    "encounter",
    "overfitting",
    "overfitting",
    "happens",
    "due",
    "nature",
    "data",
    "set",
    "machine",
    "learning",
    "methodology",
    "usually",
    "overfitting",
    "model",
    "problem",
    "high",
    "variance",
    "tackle",
    "problem",
    "one",
    "things",
    "100",
    "samples",
    "create",
    "small",
    "data",
    "set",
    "70",
    "samples",
    "let",
    "say",
    "giving",
    "example",
    "way",
    "create",
    "subset",
    "using",
    "resampling",
    "replacement",
    "exactly",
    "let",
    "first",
    "understand",
    "let",
    "say",
    "10",
    "samples",
    "want",
    "create",
    "smaller",
    "data",
    "set",
    "four",
    "samples",
    "resampling",
    "replacement",
    "randomly",
    "pick",
    "data",
    "point",
    "let",
    "say",
    "four",
    "go",
    "pick",
    "second",
    "data",
    "point",
    "randomly",
    "pick",
    "data",
    "point",
    "1",
    "10",
    "equal",
    "probability",
    "look",
    "already",
    "subset",
    "second",
    "time",
    "let",
    "say",
    "get",
    "third",
    "time",
    "also",
    "randomly",
    "pick",
    "data",
    "point",
    "1",
    "10",
    "time",
    "might",
    "get",
    "sample",
    "resampling",
    "replacement",
    "subset",
    "get",
    "data",
    "sample",
    "multiple",
    "times",
    "original",
    "data",
    "set",
    "created",
    "subset",
    "70",
    "sample",
    "might",
    "create",
    "n",
    "number",
    "smaller",
    "data",
    "sets",
    "original",
    "data",
    "set",
    "using",
    "resampling",
    "replacement",
    "individual",
    "data",
    "set",
    "train",
    "machine",
    "learning",
    "model",
    "let",
    "say",
    "trying",
    "classify",
    "person",
    "buy",
    "insurance",
    "using",
    "logistic",
    "regression",
    "use",
    "logistic",
    "regression",
    "model",
    "m1",
    "m2",
    "m3",
    "logistic",
    "regression",
    "trained",
    "different",
    "data",
    "set",
    "trained",
    "perform",
    "prediction",
    "inference",
    "perform",
    "prediction",
    "three",
    "models",
    "parallel",
    "individually",
    "whatever",
    "result",
    "get",
    "take",
    "majority",
    "vote",
    "m1",
    "m3",
    "saying",
    "person",
    "buy",
    "insurance",
    "m2",
    "saying",
    "majority",
    "vote",
    "clearly",
    "one",
    "final",
    "outcome",
    "benefit",
    "get",
    "individual",
    "models",
    "weak",
    "learners",
    "weak",
    "learners",
    "means",
    "trained",
    "subset",
    "data",
    "set",
    "hence",
    "overfit",
    "know",
    "likely",
    "generalize",
    "better",
    "fit",
    "individual",
    "weak",
    "learners",
    "combine",
    "result",
    "get",
    "overall",
    "good",
    "quality",
    "result",
    "case",
    "classification",
    "thing",
    "applies",
    "regression",
    "let",
    "say",
    "housing",
    "price",
    "prediction",
    "take",
    "average",
    "whatever",
    "prediction",
    "individual",
    "model",
    "technique",
    "also",
    "called",
    "bootstrap",
    "aggregation",
    "creating",
    "small",
    "subset",
    "data",
    "set",
    "using",
    "resampling",
    "replacement",
    "procedure",
    "called",
    "bootstrap",
    "combine",
    "results",
    "using",
    "either",
    "average",
    "majority",
    "vote",
    "called",
    "aggregation",
    "hence",
    "bagging",
    "also",
    "called",
    "bootstrap",
    "aggregation",
    "many",
    "many",
    "times",
    "hear",
    "terms",
    "jargons",
    "get",
    "worried",
    "really",
    "concepts",
    "easy",
    "know",
    "already",
    "understood",
    "bootstrap",
    "aggregation",
    "means",
    "random",
    "forest",
    "one",
    "bagging",
    "technique",
    "one",
    "distinction",
    "sample",
    "data",
    "rows",
    "also",
    "samples",
    "features",
    "basically",
    "sample",
    "rows",
    "well",
    "columns",
    "let",
    "look",
    "classical",
    "housing",
    "price",
    "prediction",
    "example",
    "town",
    "area",
    "bedroom",
    "etc",
    "features",
    "pricing",
    "green",
    "column",
    "target",
    "variable",
    "sample",
    "rows",
    "columns",
    "see",
    "bedroom",
    "column",
    "plot",
    "column",
    "randomly",
    "resample",
    "see",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "columns",
    "got",
    "four",
    "columns",
    "second",
    "time",
    "randomly",
    "sample",
    "column",
    "get",
    "uh",
    "example",
    "bedrooms",
    "particular",
    "data",
    "set",
    "third",
    "one",
    "example",
    "get",
    "school",
    "rating",
    "okay",
    "resampling",
    "randomly",
    "picking",
    "rows",
    "well",
    "columns",
    "individual",
    "data",
    "set",
    "train",
    "decision",
    "tree",
    "model",
    "aggregate",
    "results",
    "decision",
    "tree",
    "regression",
    "use",
    "classification",
    "problem",
    "well",
    "point",
    "simple",
    "random",
    "tree",
    "basically",
    "bagging",
    "technique",
    "one",
    "additional",
    "thing",
    "randomly",
    "pick",
    "features",
    "well",
    "difference",
    "terms",
    "bagging",
    "bagged",
    "trees",
    "bagging",
    "individual",
    "model",
    "svm",
    "knn",
    "logistic",
    "regression",
    "pretty",
    "much",
    "model",
    "whereas",
    "bagged",
    "trees",
    "random",
    "forest",
    "bag",
    "tree",
    "example",
    "every",
    "model",
    "training",
    "tree",
    "alright",
    "hope",
    "theory",
    "clear",
    "let",
    "move",
    "python",
    "coding",
    "using",
    "sklearn",
    "using",
    "data",
    "set",
    "coding",
    "today",
    "data",
    "set",
    "pima",
    "indian",
    "diabetes",
    "based",
    "certain",
    "features",
    "predict",
    "whether",
    "person",
    "diabetes",
    "clicking",
    "link",
    "downloaded",
    "csv",
    "file",
    "looks",
    "something",
    "like",
    "features",
    "pregnancies",
    "glucose",
    "blood",
    "pressure",
    "contributing",
    "factors",
    "diabetes",
    "based",
    "deciding",
    "whether",
    "person",
    "diabetes",
    "loaded",
    "csv",
    "file",
    "pandas",
    "data",
    "frame",
    "see",
    "soon",
    "load",
    "data",
    "data",
    "frame",
    "like",
    "basic",
    "exploration",
    "let",
    "first",
    "start",
    "find",
    "column",
    "null",
    "values",
    "way",
    "find",
    "df",
    "dot",
    "null",
    "dot",
    "sum",
    "tell",
    "column",
    "let",
    "say",
    "number",
    "5",
    "means",
    "5",
    "null",
    "values",
    "lucky",
    "null",
    "values",
    "need",
    "worry",
    "second",
    "thing",
    "tells",
    "basic",
    "statistics",
    "columns",
    "pregnancies",
    "look",
    "min",
    "max",
    "know",
    "max",
    "understand",
    "17",
    "little",
    "high",
    "unrealistic",
    "examine",
    "min",
    "max",
    "values",
    "columns",
    "look",
    "normal",
    "feel",
    "like",
    "need",
    "outlier",
    "detection",
    "outlier",
    "removal",
    "etc",
    "go",
    "ahead",
    "assume",
    "outliers",
    "second",
    "thing",
    "check",
    "whether",
    "imbalance",
    "data",
    "set",
    "see",
    "outcome",
    "right",
    "value",
    "counts",
    "find",
    "500",
    "samples",
    "says",
    "diabetes",
    "268",
    "samples",
    "says",
    "yes",
    "person",
    "diabetes",
    "look",
    "ratio",
    "looks",
    "like",
    "imbalance",
    "major",
    "imbalance",
    "major",
    "imbalance",
    "would",
    "like",
    "know",
    "10",
    "1",
    "100",
    "1",
    "ratio",
    "like",
    "2",
    "1",
    "ratio",
    "know",
    "would",
    "say",
    "slight",
    "imbalance",
    "something",
    "worry",
    "would",
    "go",
    "ahead",
    "move",
    "next",
    "stage",
    "creating",
    "x",
    "x",
    "df",
    "dot",
    "drop",
    "outcome",
    "target",
    "column",
    "need",
    "drop",
    "way",
    "using",
    "drop",
    "function",
    "pandas",
    "axis",
    "say",
    "columns",
    "would",
    "df",
    "dot",
    "outcome",
    "okay",
    "x",
    "ready",
    "scaling",
    "values",
    "different",
    "scale",
    "see",
    "particular",
    "number",
    "know",
    "0",
    "17",
    "versus",
    "0",
    "mean",
    "huge",
    "difference",
    "scale",
    "still",
    "safe",
    "side",
    "use",
    "standard",
    "scaling",
    "use",
    "min",
    "max",
    "scalar",
    "well",
    "let",
    "create",
    "scaler",
    "object",
    "scaler",
    "followed",
    "previous",
    "videos",
    "know",
    "use",
    "scalar",
    "object",
    "say",
    "fit",
    "transform",
    "x",
    "get",
    "return",
    "x",
    "scale",
    "numpy",
    "array",
    "hence",
    "print",
    "like",
    "first",
    "three",
    "rows",
    "see",
    "values",
    "scaled",
    "use",
    "min",
    "max",
    "scalar",
    "get",
    "different",
    "set",
    "values",
    "works",
    "okay",
    "muscle",
    "memory",
    "x",
    "scale",
    "ready",
    "train",
    "test",
    "split",
    "import",
    "method",
    "train",
    "test",
    "split",
    "supply",
    "x",
    "right",
    "x",
    "x",
    "x",
    "scale",
    "right",
    "want",
    "use",
    "scale",
    "value",
    "output",
    "know",
    "standard",
    "output",
    "get",
    "x",
    "train",
    "x",
    "test",
    "train",
    "test",
    "use",
    "stratify",
    "argument",
    "slight",
    "imbalance",
    "want",
    "make",
    "sure",
    "test",
    "train",
    "data",
    "set",
    "e",
    "equal",
    "number",
    "samples",
    "like",
    "equal",
    "proportion",
    "basically",
    "say",
    "mean",
    "wo",
    "equal",
    "least",
    "ratio",
    "maintained",
    "random",
    "state",
    "set",
    "10",
    "maybe",
    "20",
    "let",
    "go",
    "random",
    "number",
    "way",
    "allows",
    "reproducibility",
    "every",
    "time",
    "run",
    "method",
    "get",
    "x",
    "train",
    "train",
    "shape",
    "okay",
    "576",
    "samples",
    "train",
    "data",
    "set",
    "data",
    "set",
    "192",
    "look",
    "oh",
    "actually",
    "know",
    "looking",
    "train",
    "dot",
    "value",
    "counts",
    "look",
    "around",
    "kind",
    "ratio",
    "saw",
    "okay",
    "use",
    "decision",
    "tree",
    "classifier",
    "train",
    "standalone",
    "model",
    "use",
    "svm",
    "kanye",
    "west",
    "neighbor",
    "classification",
    "model",
    "using",
    "decision",
    "tree",
    "demonstrate",
    "decision",
    "tree",
    "relatively",
    "imbalanced",
    "classifier",
    "overfit",
    "know",
    "might",
    "generate",
    "um",
    "high",
    "variance",
    "model",
    "let",
    "train",
    "decision",
    "tree",
    "first",
    "use",
    "cross",
    "validation",
    "score",
    "try",
    "different",
    "type",
    "data",
    "set",
    "rather",
    "x",
    "train",
    "know",
    "x",
    "test",
    "cross",
    "validation",
    "know",
    "score",
    "done",
    "separate",
    "video",
    "watch",
    "aware",
    "otherwise",
    "know",
    "thing",
    "bounce",
    "head",
    "cross",
    "validation",
    "score",
    "expects",
    "classifier",
    "decision",
    "tree",
    "classifier",
    "x",
    "cv",
    "equal",
    "5",
    "10",
    "give",
    "uh",
    "um",
    "768",
    "samples",
    "divide",
    "five",
    "folds",
    "try",
    "different",
    "set",
    "x",
    "test",
    "know",
    "test",
    "x",
    "train",
    "x",
    "test",
    "try",
    "model",
    "watch",
    "video",
    "k4",
    "cross",
    "validation",
    "get",
    "good",
    "understanding",
    "return",
    "like",
    "five",
    "time",
    "training",
    "training",
    "results",
    "would",
    "inside",
    "scores",
    "see",
    "score",
    "got",
    "running",
    "5",
    "iteration",
    "training",
    "take",
    "numpy",
    "array",
    "take",
    "mean",
    "find",
    "model",
    "giving",
    "71",
    "percent",
    "accuracy",
    "okay",
    "let",
    "use",
    "bagging",
    "classifier",
    "first",
    "thing",
    "ask",
    "friend",
    "sklearn",
    "bagging",
    "classifier",
    "friend",
    "google",
    "google",
    "tell",
    "api",
    "need",
    "use",
    "see",
    "use",
    "important",
    "tool",
    "programmer",
    "copy",
    "paste",
    "create",
    "bagging",
    "classifier",
    "backing",
    "classifier",
    "read",
    "arguments",
    "going",
    "use",
    "couple",
    "arguments",
    "first",
    "okay",
    "estimator",
    "using",
    "well",
    "using",
    "decision",
    "tree",
    "many",
    "estimators",
    "like",
    "many",
    "sub",
    "groups",
    "data",
    "set",
    "100",
    "trial",
    "error",
    "okay",
    "try",
    "10",
    "20",
    "figure",
    "one",
    "giving",
    "best",
    "performance",
    "100",
    "nothing",
    "see",
    "presentation",
    "said",
    "3",
    "model",
    "100",
    "models",
    "100",
    "subset",
    "data",
    "sets",
    "training",
    "parallel",
    "many",
    "samples",
    "see",
    "used",
    "70",
    "sampling",
    "use",
    "80",
    "use",
    "80",
    "samples",
    "another",
    "thing",
    "called",
    "oob",
    "score",
    "oob",
    "score",
    "equal",
    "oob",
    "score",
    "well",
    "oob",
    "means",
    "bag",
    "sampling",
    "random",
    "sampling",
    "law",
    "probability",
    "going",
    "cover",
    "100",
    "samples",
    "subset",
    "let",
    "say",
    "subset",
    "subset",
    "number",
    "29",
    "appear",
    "okay",
    "number",
    "29",
    "let",
    "say",
    "number",
    "29",
    "right",
    "1",
    "100",
    "number",
    "29",
    "number",
    "sample",
    "appear",
    "subset",
    "models",
    "trained",
    "seen",
    "data",
    "data",
    "point",
    "use",
    "29",
    "test",
    "accuracy",
    "models",
    "kind",
    "treating",
    "29",
    "sample",
    "test",
    "data",
    "set",
    "ideally",
    "take",
    "data",
    "set",
    "split",
    "train",
    "test",
    "diagram",
    "shown",
    "block",
    "actually",
    "x",
    "train",
    "x",
    "test",
    "separate",
    "already",
    "using",
    "test",
    "performance",
    "final",
    "model",
    "deploying",
    "wild",
    "even",
    "within",
    "x",
    "train",
    "sampling",
    "strategy",
    "might",
    "miss",
    "samples",
    "let",
    "say",
    "might",
    "20",
    "data",
    "samples",
    "appeared",
    "subset",
    "20",
    "samples",
    "models",
    "trained",
    "use",
    "20",
    "samples",
    "prediction",
    "take",
    "majority",
    "vote",
    "figure",
    "accuracy",
    "accuracy",
    "oob",
    "score",
    "realize",
    "okay",
    "okay",
    "let",
    "first",
    "random",
    "state",
    "random",
    "state",
    "predictability",
    "call",
    "bag",
    "model",
    "bag",
    "model",
    "oob",
    "score",
    "oob",
    "score",
    "oob",
    "music",
    "bagging",
    "classifier",
    "actually",
    "know",
    "fit",
    "dot",
    "fit",
    "x",
    "know",
    "x",
    "train",
    "fit",
    "get",
    "realize",
    "try",
    "even",
    "x",
    "test",
    "test",
    "training",
    "data",
    "set",
    "80",
    "samples",
    "train",
    "100",
    "classifier",
    "might",
    "missed",
    "samples",
    "training",
    "data",
    "set",
    "ran",
    "model",
    "prediction",
    "accuracy",
    "got",
    "stored",
    "oob",
    "score",
    "regular",
    "scoring",
    "x",
    "test",
    "test",
    "see",
    "improvement",
    "right",
    "77",
    "percent",
    "versus",
    "standalone",
    "model",
    "giving",
    "71",
    "percent",
    "agree",
    "tell",
    "use",
    "cross",
    "validation",
    "use",
    "cross",
    "validation",
    "let",
    "use",
    "cross",
    "validation",
    "going",
    "copy",
    "paste",
    "magic",
    "create",
    "bagging",
    "model",
    "use",
    "cross",
    "validation",
    "scroll",
    "okay",
    "cross",
    "validation",
    "score",
    "supply",
    "supply",
    "first",
    "model",
    "x",
    "many",
    "folds",
    "well",
    "five",
    "folds",
    "okay",
    "get",
    "scores",
    "back",
    "scores",
    "take",
    "mean",
    "find",
    "base",
    "model",
    "gives",
    "seventy",
    "one",
    "percent",
    "accuracy",
    "bagged",
    "model",
    "gives",
    "seventy",
    "five",
    "percent",
    "accuracy",
    "unstable",
    "classifier",
    "like",
    "decision",
    "tree",
    "bagging",
    "helps",
    "classifier",
    "know",
    "sometimes",
    "unstable",
    "classifier",
    "like",
    "decision",
    "tree",
    "sometimes",
    "data",
    "set",
    "many",
    "null",
    "values",
    "know",
    "columns",
    "resulting",
    "model",
    "high",
    "variance",
    "whenever",
    "high",
    "variance",
    "makes",
    "sense",
    "use",
    "bagging",
    "classifier",
    "talked",
    "random",
    "forest",
    "let",
    "let",
    "try",
    "random",
    "forest",
    "well",
    "particular",
    "data",
    "set",
    "try",
    "random",
    "forest",
    "say",
    "okay",
    "random",
    "forest",
    "classifier",
    "x",
    "cv",
    "equal",
    "5",
    "get",
    "scores",
    "pretty",
    "straightforward",
    "x",
    "mean",
    "random",
    "forest",
    "classifier",
    "gives",
    "one",
    "little",
    "better",
    "performance",
    "inside",
    "like",
    "underneath",
    "use",
    "bagging",
    "sample",
    "data",
    "rows",
    "sample",
    "feature",
    "columns",
    "well",
    "saw",
    "presentation",
    "comes",
    "important",
    "part",
    "video",
    "exercise",
    "learning",
    "coding",
    "data",
    "science",
    "like",
    "learning",
    "swimming",
    "watching",
    "swimming",
    "video",
    "going",
    "learn",
    "swimming",
    "obviously",
    "similarly",
    "need",
    "work",
    "exercise",
    "otherwise",
    "hard",
    "grasp",
    "concepts",
    "taught",
    "giving",
    "csv",
    "file",
    "took",
    "kaggle",
    "way",
    "heart",
    "disease",
    "prediction",
    "load",
    "data",
    "set",
    "apply",
    "outlier",
    "removal",
    "given",
    "information",
    "work",
    "exercise",
    "put",
    "sensor",
    "effort",
    "click",
    "solution",
    "link",
    "ai",
    "technology",
    "built",
    "video",
    "click",
    "link",
    "without",
    "trying",
    "laptop",
    "computer",
    "get",
    "fever",
    "recover",
    "next",
    "10",
    "days",
    "okay",
    "miss",
    "fun",
    "better",
    "try",
    "first",
    "click",
    "solution",
    "link",
    "hope",
    "like",
    "video",
    "give",
    "thumbs",
    "least",
    "share",
    "friends",
    "wish",
    "best",
    "question",
    "comment",
    "section",
    "thank"
  ],
  "keywords": [
    "buy",
    "called",
    "four",
    "already",
    "three",
    "one",
    "majority",
    "vote",
    "use",
    "ensemble",
    "learning",
    "decision",
    "take",
    "different",
    "machine",
    "model",
    "train",
    "using",
    "data",
    "set",
    "might",
    "get",
    "fit",
    "high",
    "variance",
    "problem",
    "know",
    "video",
    "person",
    "hence",
    "multiple",
    "models",
    "prediction",
    "output",
    "result",
    "bagging",
    "going",
    "also",
    "let",
    "say",
    "100",
    "samples",
    "create",
    "giving",
    "example",
    "way",
    "subset",
    "resampling",
    "replacement",
    "first",
    "10",
    "randomly",
    "pick",
    "point",
    "go",
    "second",
    "1",
    "equal",
    "look",
    "time",
    "sample",
    "number",
    "individual",
    "logistic",
    "regression",
    "trained",
    "outcome",
    "means",
    "thing",
    "bootstrap",
    "aggregation",
    "many",
    "random",
    "forest",
    "rows",
    "features",
    "well",
    "columns",
    "column",
    "see",
    "five",
    "okay",
    "tree",
    "bag",
    "training",
    "diabetes",
    "link",
    "like",
    "find",
    "null",
    "values",
    "dot",
    "5",
    "need",
    "min",
    "max",
    "imbalance",
    "right",
    "ratio",
    "would",
    "x",
    "scale",
    "mean",
    "test",
    "20",
    "classifier",
    "cross",
    "validation",
    "score",
    "try",
    "scores",
    "percent",
    "accuracy",
    "sampling",
    "oob",
    "29"
  ]
}