{
  "text": "hello everyone so i'm michael spence and\ni'm the lead researcher on cder's\nlighthouse project in self-supervised\nand semi-supervised learning and in this\nvideo i'm going to talk about some basic\nsemi-supervised learning methods and\nsome of their applications as well\nso i'll start with just a brief recap of\nsemi-supervised learning so\nsemi-supervised learning is a paradigm\nof machine learning which sits in\nbetween supervised learning and\nunsupervised learning so where\nsupervised learning uses only labeled\ndata and this is human annotated data\nsuch as captioned images or things like\nthat\nto train models and unsupervised\nlearning uses only unlabeled data\nsemi-supervised learning makes use of\nboth\nwhen people talk about semi-supervised\nlearning they generally divide it into\ntwo different categories so there's\ninductive learning and transductive\nlearning\nso inductive learning as you can see in\nthe diagram here it works quite\nsimilarly to supervised learning you\ntake labeled and unlabeled data and\nusing a training process you end up with\na predictive model which can give you\npredictions on future unseen data\nthis works for both classification and\nregression problems\nso transductive learning\nworks a bit differently you feed in\nlabel data and unlabeled data\nand through transductive learning\nprocesses you get a bunch of predictions\nreturned rather than a predictive model\num so this has some limitations it means\nyou can't\neasily genera generalize the information\nyou've learned to future unseen data\nalthough there are ways of doing this\nit's it's a bit more complicated than in\nthe inductive scenario\num so i'm going to talk next about some\nconcrete examples of these two\num semi-supervised paradigms the\nsimplest example of inductive learning\nand possibly the simplest example of\nsemi-supervised learning in general is\nwhat's known as self-training\nso this is a procedure where you can\ntake any supervised method\nfor classification or regression\nand modify it so that it works in a\nsemi-supervised manner taking advantage\nof labeled and unlabeled data\nso the procedure is quite simple you\nbegin with a small amount of label data\nin this case it could be images\nand you use this to train a base model\nusing normal supervised methods\nand then what you do is you\nuse this base model to label your\nunlabeled data\nwhich is a process known as\npseudo-labeling\nand you would then take your most\nconfident predictions\nmade with this base model\nand you would add them into your label\ndata set\nand then use that to train the next\niteration of your model\nso i have some diagrams here which\nshould hopefully make the process a bit\nclearer\nso as i said you have your first\nclassifier which has just been trained\num on the original labeled images\nyou take these and you label your\nunlabeled images\nin a process called pseudo labeling\nand then you take the most confident\npredictions made with your model\nyou might say you want a confidence of\nover 75 for example\nand if any of the pseudo labels exceed\nthis confidence level\nyou would then add it into the into the\nlabel data set so you would combine your\nyour most confident pseudo labels with\nthe original labels into a larger label\ndata set\nand you would then train a new\nclassifier on that\num which\nin the best case scenario would have\nimproved performance over the original\none\nand you can generally iterate this\nprocess every time adding more and more\npseudo labels you might do 10 iterations\nfor example is is a fairly standard\namount um\nand hopefully if if your data sets well\nsuited for the process the performance\nof the model should continue increasing\nat each iteration\num so as an example of a successful use\nof self training methods um\nlast year facebook made use of self\ntraining to improve their speech\nrecognition models\nso starting off with the base model\nwhich was trained with 100 hours of uh\nlabeled data so\ndata you know which has been actually\nhumanly annotated\num they managed to increase their\nperformance of this by incorporating 500\nhours of unlabeled speech data um and\nusing self-training and they find that\nthey had a decreased word error rate of\n33.9 percent\nwhich is obviously uh a really\nsignificant improvement especially when\nno extra effort was actually needed uh\nfor this unlabeled data\nso this is a successful example of self\ntraining being used\nbut it should be stressed that the\nperformance will vary like very much\nfrom data set to data set and there's\nplenty of cases where self turning might\nactually\nend up decreasing the the performance\num compared to just the supervised model\nso self training was probably the\nsimplest example of an inductive method\nof semi supervised learning\num i'll talk now about probably the\nsimplest method of transductive semi\nsupervised learning\nso this is known as label propagation um\nit's a graph based method of semi\nsupervised learning and what it allows\nyou to do is it allows you to spread uh\nhuman annotated labels um within a\nnetwork made up of label data and\nunlabeled data\nso um\nthe mathematics can get a bit technical\nbut i'll just tell you now about the\ngeneral\num the general kind of intuition uh\nbehind this method\nso if we look at the toy example here at\nthe bottom of the slide we have a\nnetwork of data points um most of which\nare uncolored but four of which are\ncolored there's two red ones and two\ngreen ones and we'd like to find a way\nso that we can spread these colors\nthroughout the network\nso one way of doing this\nis we could pick a point in this case\nfour for example\nand we could count up all the different\npaths which travel to the network from\nfour to a colored node\num and if we do this we find here on the\nright\nthese are the paths leading to red\npoints so four to seven to eight for\nexample\num there's five pass overall leading to\nred points and if we count up the number\nof paths leading to green points we find\nthere's only four\nso in this sense we consider the point\nfour as being closer to red and green we\nwould then color it in as red\nand we would repeat this process for\nevery point in the network\num until every point was labeled and\nuncovered in another diagram which might\nbe useful um is displayed here so this\nmight help you understand label\npropagation\num this is taken off the scikit-learn\nuser guide\num from one of their implementations of\nlabel propagation\num so on the left here is the original\ndata set you can see it's mostly\nunlabeled it's mostly orange\num but you have two label points one\nlight blue one dark blue\nand if we apply the label propagation\nalgorithm you can see the end result is\nthat all points end up labeled which i\nshould hopefully provide a a good\ndemonstration of how it works and what\nhis end goal is so i hope you find this\nquick run-through of uh two basic\nmethods of semi-supervised learning\nuseful\nif you'd like to be more involved in\nthis project or would like to see any of\nthe future content i will be sharing\num feel free to get in touch with me um\nthere'll also be a link to the project\npage in the description of the video\num so thanks very much for listening\n",
  "words": [
    "hello",
    "everyone",
    "michael",
    "spence",
    "lead",
    "researcher",
    "cder",
    "lighthouse",
    "project",
    "learning",
    "video",
    "going",
    "talk",
    "basic",
    "learning",
    "methods",
    "applications",
    "well",
    "start",
    "brief",
    "recap",
    "learning",
    "learning",
    "paradigm",
    "machine",
    "learning",
    "sits",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "supervised",
    "learning",
    "uses",
    "labeled",
    "data",
    "human",
    "annotated",
    "data",
    "captioned",
    "images",
    "things",
    "like",
    "train",
    "models",
    "unsupervised",
    "learning",
    "uses",
    "unlabeled",
    "data",
    "learning",
    "makes",
    "use",
    "people",
    "talk",
    "learning",
    "generally",
    "divide",
    "two",
    "different",
    "categories",
    "inductive",
    "learning",
    "transductive",
    "learning",
    "inductive",
    "learning",
    "see",
    "diagram",
    "works",
    "quite",
    "similarly",
    "supervised",
    "learning",
    "take",
    "labeled",
    "unlabeled",
    "data",
    "using",
    "training",
    "process",
    "end",
    "predictive",
    "model",
    "give",
    "predictions",
    "future",
    "unseen",
    "data",
    "works",
    "classification",
    "regression",
    "problems",
    "transductive",
    "learning",
    "works",
    "bit",
    "differently",
    "feed",
    "label",
    "data",
    "unlabeled",
    "data",
    "transductive",
    "learning",
    "processes",
    "get",
    "bunch",
    "predictions",
    "returned",
    "rather",
    "predictive",
    "model",
    "um",
    "limitations",
    "means",
    "ca",
    "easily",
    "genera",
    "generalize",
    "information",
    "learned",
    "future",
    "unseen",
    "data",
    "although",
    "ways",
    "bit",
    "complicated",
    "inductive",
    "scenario",
    "um",
    "going",
    "talk",
    "next",
    "concrete",
    "examples",
    "two",
    "um",
    "paradigms",
    "simplest",
    "example",
    "inductive",
    "learning",
    "possibly",
    "simplest",
    "example",
    "learning",
    "general",
    "known",
    "procedure",
    "take",
    "supervised",
    "method",
    "classification",
    "regression",
    "modify",
    "works",
    "manner",
    "taking",
    "advantage",
    "labeled",
    "unlabeled",
    "data",
    "procedure",
    "quite",
    "simple",
    "begin",
    "small",
    "amount",
    "label",
    "data",
    "case",
    "could",
    "images",
    "use",
    "train",
    "base",
    "model",
    "using",
    "normal",
    "supervised",
    "methods",
    "use",
    "base",
    "model",
    "label",
    "unlabeled",
    "data",
    "process",
    "known",
    "would",
    "take",
    "confident",
    "predictions",
    "made",
    "base",
    "model",
    "would",
    "add",
    "label",
    "data",
    "set",
    "use",
    "train",
    "next",
    "iteration",
    "model",
    "diagrams",
    "hopefully",
    "make",
    "process",
    "bit",
    "clearer",
    "said",
    "first",
    "classifier",
    "trained",
    "um",
    "original",
    "labeled",
    "images",
    "take",
    "label",
    "unlabeled",
    "images",
    "process",
    "called",
    "pseudo",
    "labeling",
    "take",
    "confident",
    "predictions",
    "made",
    "model",
    "might",
    "say",
    "want",
    "confidence",
    "75",
    "example",
    "pseudo",
    "labels",
    "exceed",
    "confidence",
    "level",
    "would",
    "add",
    "label",
    "data",
    "set",
    "would",
    "combine",
    "confident",
    "pseudo",
    "labels",
    "original",
    "labels",
    "larger",
    "label",
    "data",
    "set",
    "would",
    "train",
    "new",
    "classifier",
    "um",
    "best",
    "case",
    "scenario",
    "would",
    "improved",
    "performance",
    "original",
    "one",
    "generally",
    "iterate",
    "process",
    "every",
    "time",
    "adding",
    "pseudo",
    "labels",
    "might",
    "10",
    "iterations",
    "example",
    "fairly",
    "standard",
    "amount",
    "um",
    "hopefully",
    "data",
    "sets",
    "well",
    "suited",
    "process",
    "performance",
    "model",
    "continue",
    "increasing",
    "iteration",
    "um",
    "example",
    "successful",
    "use",
    "self",
    "training",
    "methods",
    "um",
    "last",
    "year",
    "facebook",
    "made",
    "use",
    "self",
    "training",
    "improve",
    "speech",
    "recognition",
    "models",
    "starting",
    "base",
    "model",
    "trained",
    "100",
    "hours",
    "uh",
    "labeled",
    "data",
    "data",
    "know",
    "actually",
    "humanly",
    "annotated",
    "um",
    "managed",
    "increase",
    "performance",
    "incorporating",
    "500",
    "hours",
    "unlabeled",
    "speech",
    "data",
    "um",
    "using",
    "find",
    "decreased",
    "word",
    "error",
    "rate",
    "percent",
    "obviously",
    "uh",
    "really",
    "significant",
    "improvement",
    "especially",
    "extra",
    "effort",
    "actually",
    "needed",
    "uh",
    "unlabeled",
    "data",
    "successful",
    "example",
    "self",
    "training",
    "used",
    "stressed",
    "performance",
    "vary",
    "like",
    "much",
    "data",
    "set",
    "data",
    "set",
    "plenty",
    "cases",
    "self",
    "turning",
    "might",
    "actually",
    "end",
    "decreasing",
    "performance",
    "um",
    "compared",
    "supervised",
    "model",
    "self",
    "training",
    "probably",
    "simplest",
    "example",
    "inductive",
    "method",
    "semi",
    "supervised",
    "learning",
    "um",
    "talk",
    "probably",
    "simplest",
    "method",
    "transductive",
    "semi",
    "supervised",
    "learning",
    "known",
    "label",
    "propagation",
    "um",
    "graph",
    "based",
    "method",
    "semi",
    "supervised",
    "learning",
    "allows",
    "allows",
    "spread",
    "uh",
    "human",
    "annotated",
    "labels",
    "um",
    "within",
    "network",
    "made",
    "label",
    "data",
    "unlabeled",
    "data",
    "um",
    "mathematics",
    "get",
    "bit",
    "technical",
    "tell",
    "general",
    "um",
    "general",
    "kind",
    "intuition",
    "uh",
    "behind",
    "method",
    "look",
    "toy",
    "example",
    "bottom",
    "slide",
    "network",
    "data",
    "points",
    "um",
    "uncolored",
    "four",
    "colored",
    "two",
    "red",
    "ones",
    "two",
    "green",
    "ones",
    "like",
    "find",
    "way",
    "spread",
    "colors",
    "throughout",
    "network",
    "one",
    "way",
    "could",
    "pick",
    "point",
    "case",
    "four",
    "example",
    "could",
    "count",
    "different",
    "paths",
    "travel",
    "network",
    "four",
    "colored",
    "node",
    "um",
    "find",
    "right",
    "paths",
    "leading",
    "red",
    "points",
    "four",
    "seven",
    "eight",
    "example",
    "um",
    "five",
    "pass",
    "overall",
    "leading",
    "red",
    "points",
    "count",
    "number",
    "paths",
    "leading",
    "green",
    "points",
    "find",
    "four",
    "sense",
    "consider",
    "point",
    "four",
    "closer",
    "red",
    "green",
    "would",
    "color",
    "red",
    "would",
    "repeat",
    "process",
    "every",
    "point",
    "network",
    "um",
    "every",
    "point",
    "labeled",
    "uncovered",
    "another",
    "diagram",
    "might",
    "useful",
    "um",
    "displayed",
    "might",
    "help",
    "understand",
    "label",
    "propagation",
    "um",
    "taken",
    "user",
    "guide",
    "um",
    "one",
    "implementations",
    "label",
    "propagation",
    "um",
    "left",
    "original",
    "data",
    "set",
    "see",
    "mostly",
    "unlabeled",
    "mostly",
    "orange",
    "um",
    "two",
    "label",
    "points",
    "one",
    "light",
    "blue",
    "one",
    "dark",
    "blue",
    "apply",
    "label",
    "propagation",
    "algorithm",
    "see",
    "end",
    "result",
    "points",
    "end",
    "labeled",
    "hopefully",
    "provide",
    "good",
    "demonstration",
    "works",
    "end",
    "goal",
    "hope",
    "find",
    "quick",
    "uh",
    "two",
    "basic",
    "methods",
    "learning",
    "useful",
    "like",
    "involved",
    "project",
    "would",
    "like",
    "see",
    "future",
    "content",
    "sharing",
    "um",
    "feel",
    "free",
    "get",
    "touch",
    "um",
    "also",
    "link",
    "project",
    "page",
    "description",
    "video",
    "um",
    "thanks",
    "much",
    "listening"
  ],
  "keywords": [
    "project",
    "learning",
    "talk",
    "methods",
    "supervised",
    "labeled",
    "data",
    "annotated",
    "images",
    "like",
    "train",
    "unlabeled",
    "use",
    "two",
    "inductive",
    "transductive",
    "see",
    "works",
    "take",
    "using",
    "training",
    "process",
    "end",
    "model",
    "predictions",
    "future",
    "bit",
    "label",
    "get",
    "um",
    "simplest",
    "example",
    "general",
    "known",
    "method",
    "case",
    "could",
    "base",
    "would",
    "confident",
    "made",
    "set",
    "hopefully",
    "original",
    "pseudo",
    "might",
    "labels",
    "performance",
    "one",
    "every",
    "self",
    "uh",
    "actually",
    "find",
    "semi",
    "propagation",
    "network",
    "points",
    "four",
    "red",
    "green",
    "point",
    "paths",
    "leading"
  ]
}