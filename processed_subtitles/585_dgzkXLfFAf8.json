{
  "text": "good morning everyone my name is Kanu a\nbroad essentially I help with any gatk\nrelated questions so on the forum so we\nhave a gatk forum so any questions that\nyou may have ask us now while we're\npresenting our scales during the coffee\nbreak or after the day ends or after the\nworkshop if you have any questions feel\nfree to post questions on the gatk forum\nand I'll help you out with them alright\nwhile I'm explaining data pre-processing\nplease feel free to stop me and ask me\nany questions that you might have all\nright so this morning I'm gonna walk you\nthrough our best practices for data\npre-processing Andre explained how so\nmany things can go wrong with your\nsequencing there so now we're going to\ntalk about how do you correct all of\nthose so there are three main steps to\ndata pre-processing first you want to\nmap your data to a reference genome you\nwant to remove duplicates that Andre\nspoke about and then recalibrate the\nbase quality scores we'll go into\ndetails about each of these steps so\nwhat is data pre-processing even before\nthat why do we pre process our data\nessentially the ruler is garbage in\ngarbage out so you give your variant\ncallers unverified reads your calls your\nway and call the confidence that you\nhave on your variant calls will be that\nmuch lesser so you want to provide your\nvagin calls with as good reads as\npossible and so from the sequencer you\nessentially just get a huge pile of\npaired-end reads and these reads are\naffected with technical biases and\nartifacts and duplicates so you\nessentially want to clean up your reads\nbefore you do variant calling and so on\nthe slide you can see here this is the\nworkflow diagram for our best practices\nfor data pre-processing it's essentially\nfirst you map your reads mock duplicates\nand then recalibrate the quality scores\nso the first step is to map the reads to\na reference genome what you're looking\nat here are essentially reads\ncolor-coded by which region in the\ngenome they come from so once you put it\nthrough an aligner it fits each of these\nreads against the region in the genome\nthat it comes from for DNA sequencing\ndata we use we recommend and use bwa 2\nfor RNA seek we recommend star aligner 2\nso when you're mapping your reads to a\nreference sometimes it's as simple as\nyou you can tell which region it's\ncoming from for example in region 1 and\nregion 2 there might be some local\nvariants but it's still no the alignment\nknows which region it's coming from in\nsome cases in your samples you have this\nduplicated regions and that's when it\ngets complicated and say yeah this is in\nsimple cases it just directly Maps these\nand gives it a high mapping quality in\ncases where it's a little confusing\nabout which reads come from a particular\nregion that's when the mapping quality\ndrops so an easy case would be\nessentially you have these pile of reads\nand you know which regions they come\nfrom and in most cases your genome is\npretty different from the rest of it so\nin most cases it's easy to find these\nreads and map it through specific\nregions but sometimes a\ngets complicated for example if your\nreference itself has duplicated ratings\nand so then your reads the aligners\nconfused where do these reads fit should\ngo to a or to B and so to get around\nthis we have paired and sequencing so\nwhen you have paired and sequencing and\nthey are repeat region duplicated\nregions in your reference then using the\nusing the read one every two pair you\ncan fit it on the reference more easily\nand so the output of your sequencing is\na SAM file and the binary version of a\nSAM file is a BAM file and essentially\nthis file format has two sections\nthere's the header which is common to\nall the reads in that sample and then\nthere are records which are unique to\neach read so in the header you have\ninformation such as the version the\nsorting order of the leads in this case\nit's coordinates sorted you have\nreference sequence dictionary entries to\nsay which of those reference sequence\ndictionaries was used in the sample and\nalso read group information and then in\nyour records that are unique to each\nlead you have the read name which gives\nyou like essentially the sample name the\nflow cell information the lane\ninformation then you have Flags this is\nessentially binary information about the\norientation of your read orientation of\nthe read and its pair whether it's\naligned to the forward or the reverse\nstrand all that information is given in\nthe flag and then you have the read\nposition information you have the\nmapping quality you have this cigar\nstring go into more details about that\nin a second\nyou have mate information which is\nessentially telling you at what position\nwas here\nthe mate of that read mapped and also\nthe insert size and then you have this\nread sequence and quality score that\ncome from the sequencer and you have\nsome metadata for example read group\ninformation and some other tags this\nlink is hidden here but we have a link\nto the Sam format to get more details of\nspecific tags and other information\nabout this format so yeah let's talk\nmore about any questions so far\nyes so oh right and this is just for the\nrecording so the in a nutshell no you\ndon't need to do any trimming in our\nworkflow there's early on in the\nprocessing you can mark the adapters so\nthat they will be ignored in the rest of\nthe processing and anything like if\ntraditionally you were doing quality\ntrimming that's not necessary at all\nbecause the variant callers will take\nthe the quality of the basis into\naccounts it's actually used as part of\nthe the variant calling model so it's\nit's really not necessary to do any\ncollege trimming and adapter trimming\neither the adapters will be marked and\nrecognized and ignored accordingly all\nright now to talk about the cigar string\nthe second string is essentially\nsummarizing the alignment structure of\neach read for example in in this example\nit's the cigar string is essentially\ntelling you that the first base was soft\nclear\nthere are three matches one deletion two\nmatches and one-one insertion and\nanother match\nwhen I say matches I don't mean exact\nbase match for example in this case here\nat saying two matches but the bases\ndon't match yes sorry can you say that\nagain oh that's just for the neck\nso this read has these positions that's\njust the next base of the after the\nreading okay so going back to these\nthree matches like I said these matches\ndon't mean that is a base match it's\nmore about that that the aligner is\nsaying that these three bases match\nagainst these three positions of the\nreference that's it it's not talking\nanything about base magic alright so I'm\nin usually and like usually you have a\nfast Q file you map it to a reference\nusing BW mm and you get this mapped BAM\nthe problem with this is where is my\nread group information or metadata for\nmy sample and that's not included in the\nfast Q file so if you want to add\nregroup information to your analysis you\nneed to do it which in after this\nprocess in a very unnatural way and so\nto get around that I'd broad what we do\nis use a file format called unmapped BAM\nwhich essentially keeps your read\ninformation and your metadata together\nin one file and so what happens is it\nuses this unmapped BAM file process it\nit processes it\nand then creates this mapped Bam and\nthen it combines so this has all the\nalignment information and then it\ncombines this unmapped band with mavim\nto include the metadata information\nusing this tool called merge BAM\nalignment Andre spoke about sometimes\nyour reads may have some bacterial\ncontamination\nso apart from merging on my bandwidth\nmap BAM merge bam alignment also does\nsome cleanup of the reads all right so\nwe spoke about how alignment is done for\nDNA data for RNA seek though it's it's a\nspecial case because in you have these\nintronic regions that are spliced from\nyour familiar from the reads and so if\nyou map these using DNA alignment tool\nyou have these huge deletions in your\nreads and so you use a special kind of\nalignment tool and we recommend the star\ntube bus alignment tool it's not on this\nslide but star alignment tool is what we\nrecommend for RNA seek Elementary\nalright so\nthat was step one alignment now let's\ntalk about step two which is marking any\nduplicates in your reads why do we need\nto do that for example you have this one\noops you have this one variant in your\ndata that is basically being amplified\nby your duplicates and so radiant color\nwill look at that and it will take it to\nmean that there is all this confidence\nfor that book evidence and confidence\nfor that one particular variant and this\nis essentially inflated so you want to\nremove all of these duplicates for that\nreason also in case of RNA seek\nduplicates give this artificial sense of\nincreased expression so again even for\nRNA seek data you need to remove these\nduplicates and so how did these yes\n[Music]\nimplementation detail so what we do is\nthat we provide we do the the mark\nduplicate step across all of the lanes\nall of the read groups because there is\nthere are really two types of duplicates\nsome of them are at the library level\nand you want to be able to identify\nthose and so by providing all the data\ntogether we're able to do that kind of\ncross library across regroup duplicate\nmarketing\n[Music]\nwe do it the same way regardless yeah so\nwe so it's true when when we're doing\nthe marketing duplicates we're really\njust identifying which ones we're\nconsidering as duplicates we don't\nactually remove them from the file they\nstay in the data it's true that\ntechnically you could just remove them\ncompletely and there is actually an\noption the tool has an option to remove\nthe tools to remove the reads from the\nfrom the file we prefer to keep it just\nbecause we really hate throwing away\ndata and it's it's a little scary\nbecause sometimes we want to reprocess\nthings you could decide to throw the\nmore inputs but that does mean that\nyou've lost some information and you've\nlost the ability to go back and change\nyour mind them how you're going to do it\nbut yeah if if you're very concerned\nabout storage for example you can throw\nthem out\nwe don't have slides on um eyes in this\nworkshop we have some information some\ndocumentation that we could point you to\nif you actually if you post on the forum\nthen we can get that to you and I\nsuggest using the forum because then\nthey'll be attracting its and we'll get\nback to you with the that's information\nit'll probably be Bonnie who gets back\nto you but yeah that's that's the\neasiest way to make sure we get that to\nyou yeah go ahead sorry what's instead\nof bwa I'm sorry I'm not hearing you\nwell\nthe variation craft toolkit Oh graph\nsorry sorry\nyeah we're not we're not currently using\ngraph based reference if that's what you\nmean Oh graph based alignments oh okay\nwe're not using that yet there might be\nsome evaluation going on but I will say\nabout the choice of a liner so the\nchoice of mapper aligner is the single\nmost impactful change that you can make\nto a pipeline that causes batch effects\nand so we're extremely conservative in\nterms of not changing a liner unless we\nreally have to unless it really gives us\na great advantage because anytime you\nchange your liner the the effects on the\nresults is such that if we were to\nanalyze different data sets aligned\ndifferently we get batch effects and\nthere was a recent paper I think about\nbatch effects and thousand genomes that\nkind of highlights that the we're kind\nof paranoid about that partly because we\nprocess very large datasets for example\nthe Nomad call sets at this point is\nover a hundred thousand whole genomes it\ncosts a ton of money if we have to\nreprocess all of them and for for for a\ncohort like that you have to have them\nall aligned in exactly the same way so\nif we if we change your liner we have to\nreprocess all of the data and we often\nreprocess a lot of the other datasets\nsequence at the roads for internal\ngroups as well as some of our customers\nand so it's yeah changing that is a\nhonestly traumatic process for us so as\nmuch as I think it's very exciting that\nthere's some improvements made like a\ngraph based research for alignments and\nvariant calling I think\nit'll probably be a few years before we\ncan consider moving to adopting that in\nkind of a production setting does that\nanswer your question I think that was a\nquestion there first reduce bam oh so\nthere was a tool called reduce reads a\nfew years ago that we were experimenting\nwith for compressing a read data and we\nhave abandoned that one and it's it's\nit's bad don't use it and that was like\nfive years ago I think or more\ndefinitely not using that anymore\nhowever for for compression purposes\nwe're very happy with crime and so by\ndefault in our production pipeline we we\nproduce cram files for storage so we\nstill often have the data as a BAM file\nfor running variant calling etc but for\nfor storage we store everything is crime\nfiles and that that has made some pretty\nimportant improvements in reducing the\nfile size it depends it depends on on\nthe the settings that you use because\nyou have several different settings and\nthere's there's your basic lossless\nsetting where you're not losing any\ninformation and then there's\nprogressively more lossy options where\nyou can do things like compressing the\nbase quality to having just bins base\nqualities and the more you compress the\nmore you can gain but I hesitate to give\nyou a number because I don't have one\noff the top of my head but I can look it\nup but you can have some pretty dramatic\nimprovements yeah sure I think you had a\nquestion\ndealing with batch effects we reprocess\neverything like if we know that we're\ngoing to have batch effects from like if\nwe know that there's something in the\npipeline that is going to cause batch\neffects we change the pipeline so that\ndoesn't happen and we will reprocess\neverything with the same pipeline\nbecause it the the kinds of batch\neffects that you can have will lead to\nthings like thinking that's there's a\nvariant that's unique to a particular\npopulation when it's not and it's just\nbecause of different alignments choices\nit's it's often different artifacts or\nwith the ability to detect some variants\nin difficult regions that's that's so\ninfluenced and so we prefer to have even\nif it means that we're going to have an\nartifact in the data we prefer to have\nit everywhere as opposed to having it\nonly in some some subsets so yeah we\nwe've done some work of so the broad a\ncollaborated with a few other major\nsequencing centers in North America to\ndefine what we call the functional\nequivalence specification and so it's a\nspecification of how you do the\nalignments and the pre-processing in a\nway that removes batch effects and if\nyou have a pipeline that's compliant\nwith that specification you can be\npretty sure that's any datasets aligned\nwith those pipelines compliant pipelines\nwill be compatible and there's a there's\na paper about it it's if you google\nfunctional equivalence pipeline I think\nit should come up and there's also a\nblog post on the GTA blog about its we\nreally encourage people to you to adopt\nthat and that the the biggest factor was\nactually the the aligner but there's a\ncouple of additional things that you can\ndo towards that\nany more questions on that all right all\nright so where did these duplicates come\nfrom they're our library duplicates and\noptical duplicates Andre went over those\ndetails so we'll move fastest so now\ngoing into how does the mod duplicates\nto mark duplicates essentially what it\ndoes is it looks at the orientation of\nthe Reed and the unclipped v prime\nposition of the first strip\nit doesn't do for base matching it just\nlooks at the antelope v prime position\nand the orientation of the Reed and then\ndecides whether it's a duplicate or not\nand for parent Reed it'll do the same\nthing for both the pair's orientation\nand the unclip phi prime position so\nhere are 2 igv screenshots one without\nany of the duplicate smart and then here\nis how multiplicate works in essentially\nmarking all of these duplicates and then\nyou have this option in i GD to show you\nreeds after hiding all the duplicate\nreeds just to note that mark duplicates\ndoes not remove any reads it only marks\nthem as a duplicate there's a tag that\nit adds to the alignment to show that\nit's a duplicate read back in the day we\nhad a tool to do locally realignment\naround complex regions of the genome for\nexample repetative regions as shown in\nthis example here it would call all of\nthese variants but then if you do\nrealignment it sees that there's\nactually one insertion here which then\nclears out and cleans out all of these\nreads and removes these extra snips i\nwas originally calling\nthis is a deprecated tool we don't do\nthis anymore because our variant colors\nhaplotype collar and mutex you that\nwe'll talk in more detail about does\nthis intrinsically so this is basically\na deprecated tor and the final step of\nthis process is to fine tune the bass\nquality scores why do we do that\nyour variant calling pipeline heavily\ndepends on the quality of the basses\ncalled by the sequencer so any errors\nthat the sequencer might have added to\nyour quality scores it will get used by\nthe vain caller irrespective so you want\nto clean up those quality scores and for\nthat we use this tool called BQ SR which\nessentially detects the systematic\nerrors created by the sequencer and\nthese could be due to the physics or the\nchemistry of the sequencing technique\nitself or some manufacturing defaults in\nthe sequencer which causes these errors\nacross the sample and so BQ SR uses\nmachine learning to create empirical\nerror model and then uses that model on\nall the bases in the sample and fine\nTunes the Quality Score some of the\ncovariance that it uses is say for\nexample you have two A's in a row and\nany read after these two is has a one\npercent increased error rate then it\nfine Tunes the quality score to say that\nany base after the first two is is has a\n1% lower base quality and that's\nessentially shown in this diagram here\nyou have the dinucleotide contacts on\nthe x-axis on the y-axis you have the\naccuracy which is the empirical quality\nminus the reported quality so the\nnegative values show show that the bases\nhave been over as to the quality scores\nhave been\nestimated a positive value shows\nunderestimated quality scores and in\nmost cases we have seen that sequences\ntend to overestimate quality scores than\nunderestimate and so when you map the\nrip these pink dots are before P Q SR\nand so and the purple are after BQ SR\nand then when you compare the reported\nversus empirical quality you see that\nthis neat little line the forms after\nyou apply the base quality recalibration\nsomething to note here is that BQ SR\ndoes not change the basis it does not\nsay that oh this this particular base\nwas called wrong I'm going to change it\nthat's how it does all it does is fine\nTunes the quality score and the\nconfidence of calling a base so so this\nis independent of coverage I mean this\nis this really affects like individual\nbase calls and and what quality we have\nthat now the link that you can have is\nthat if you there are some filters so\ngatk does some filtering on the quality\nof base calls so if that they're below a\ncertain thresholds some bases will just\nbe ignored and that goes back to the\nquestion about trimming do you need to\ntrim for quality no in part because we\ntake take the quality of bases into\naccount so if you have and I forget what\nthe exact threshold is but let's say\nit's a Q 10 below Q 10 you just ignore\nall of them your if you're looking if\nyou're counting how much coverage you\nhave and you're doing it\na way that is aware of the usable\nquality you might end up with less\ncoverage than you originally had if\nyou're looking at usable coverage does\nthat answer your question yes yeah so\nbonnie was going to explain how we how\nwe mask out real variation and what I\nsuggest is we go through that\nexplanation and then after that we can\naddress the question of how that applies\nto cancer samples all right\nso essentially what Loney was trying to\nsay was that's how this error model is\ncreated is that first all the known\nvariants are masked out for example we\nuse DB snip and see what the known\nvariants are and mask those out and then\ncreate the error model on the rest of\nthe variance rest of the mismatches so\nanything that's not unknown variant is\nassumed to be an error and then the\nmodel is created on top of that and so\nwe're where the the somatic aspect comes\nin is that you can imagine that when\nwe're masking out known variation with\nDB snip for example certainly we can't\nmask out variants that have not been\nreported previously so even in germline\nsamples my sequence will have some\nvariants that are not known to DB snip\nand will therefore not be masked now the\nnice thing is because the number of\nbases is much much larger\nthan the number of variants where that\nmight be the case you have you have\nbillions of bases there's not there's\nnot many reads that will be affected by\nhaving a real variance from my\npreviously undisclosed bearings so so\nthat will completely get washed out the\nquestion here is well if you have a\ncancer sample with a much higher amount\nof of\nmutations is it possible that that\nstarts introducing some noise what we've\nseen and we work with a with the Cancer\nGenome analysis team on that at the time\nis that for the most parts we're still\nnot seeing in effects of having more\nmore mutations in the cancer context\nthere's still way more unaffected bases\nthen you have affected basis by somatic\npatterns of additional variation however\nsome of the analysts in in the group we\nwork with sometimes use a database like\ncosmic to mask out the most common kind\nof recurring mutations they see in\ncancer because there are some things\nthat do come up kind of in the same\nplaces so it is possible to mask that\nout but in general what we see is that\nit's not really worth doing at worse you\nget a little bit of extra noise in the\nmodel but you can still detect\nsystematic errors because ultimately\nwhat we're detecting here is systematic\nerrors where the Machine systematically\nmade errors in the same places where as\ncancer mutations for the most part will\nshow up as random mutations they're not\nassociated with a particular covariates\nthat's another model or not as much and\nso that's why it's not a problem not as\nmuch of a problem did that answer your\nquestion\ngreat all right and so finally how is\nthis show up how does this\ninformation new based quality scores\nshow up in your alignment file you have\noops you have these recalibrated based\nqualities and you also have you can have\noriginal based qualities we usually\nremove these or your base qualities\nbecause to save space but yeah if you\nwanted it you could have the original\nbase qualities and that wraps up this\ntalk any questions we can take them in\nthe coffee break next and talk more\nabout it them alright thank you everyone\n",
  "words": [
    "good",
    "morning",
    "everyone",
    "name",
    "kanu",
    "broad",
    "essentially",
    "help",
    "gatk",
    "related",
    "questions",
    "forum",
    "gatk",
    "forum",
    "questions",
    "may",
    "ask",
    "us",
    "presenting",
    "scales",
    "coffee",
    "break",
    "day",
    "ends",
    "workshop",
    "questions",
    "feel",
    "free",
    "post",
    "questions",
    "gatk",
    "forum",
    "help",
    "alright",
    "explaining",
    "data",
    "please",
    "feel",
    "free",
    "stop",
    "ask",
    "questions",
    "might",
    "right",
    "morning",
    "gon",
    "na",
    "walk",
    "best",
    "practices",
    "data",
    "andre",
    "explained",
    "many",
    "things",
    "go",
    "wrong",
    "sequencing",
    "going",
    "talk",
    "correct",
    "three",
    "main",
    "steps",
    "data",
    "first",
    "want",
    "map",
    "data",
    "reference",
    "genome",
    "want",
    "remove",
    "duplicates",
    "andre",
    "spoke",
    "recalibrate",
    "base",
    "quality",
    "scores",
    "go",
    "details",
    "steps",
    "data",
    "even",
    "pre",
    "process",
    "data",
    "essentially",
    "ruler",
    "garbage",
    "garbage",
    "give",
    "variant",
    "callers",
    "unverified",
    "reads",
    "calls",
    "way",
    "call",
    "confidence",
    "variant",
    "calls",
    "much",
    "lesser",
    "want",
    "provide",
    "vagin",
    "calls",
    "good",
    "reads",
    "possible",
    "sequencer",
    "essentially",
    "get",
    "huge",
    "pile",
    "reads",
    "reads",
    "affected",
    "technical",
    "biases",
    "artifacts",
    "duplicates",
    "essentially",
    "want",
    "clean",
    "reads",
    "variant",
    "calling",
    "slide",
    "see",
    "workflow",
    "diagram",
    "best",
    "practices",
    "data",
    "essentially",
    "first",
    "map",
    "reads",
    "mock",
    "duplicates",
    "recalibrate",
    "quality",
    "scores",
    "first",
    "step",
    "map",
    "reads",
    "reference",
    "genome",
    "looking",
    "essentially",
    "reads",
    "region",
    "genome",
    "come",
    "put",
    "aligner",
    "fits",
    "reads",
    "region",
    "genome",
    "comes",
    "dna",
    "sequencing",
    "data",
    "use",
    "recommend",
    "use",
    "bwa",
    "2",
    "rna",
    "seek",
    "recommend",
    "star",
    "aligner",
    "2",
    "mapping",
    "reads",
    "reference",
    "sometimes",
    "simple",
    "tell",
    "region",
    "coming",
    "example",
    "region",
    "1",
    "region",
    "2",
    "might",
    "local",
    "variants",
    "still",
    "alignment",
    "knows",
    "region",
    "coming",
    "cases",
    "samples",
    "duplicated",
    "regions",
    "gets",
    "complicated",
    "say",
    "yeah",
    "simple",
    "cases",
    "directly",
    "maps",
    "gives",
    "high",
    "mapping",
    "quality",
    "cases",
    "little",
    "confusing",
    "reads",
    "come",
    "particular",
    "region",
    "mapping",
    "quality",
    "drops",
    "easy",
    "case",
    "would",
    "essentially",
    "pile",
    "reads",
    "know",
    "regions",
    "come",
    "cases",
    "genome",
    "pretty",
    "different",
    "rest",
    "cases",
    "easy",
    "find",
    "reads",
    "map",
    "specific",
    "regions",
    "sometimes",
    "gets",
    "complicated",
    "example",
    "reference",
    "duplicated",
    "ratings",
    "reads",
    "aligners",
    "confused",
    "reads",
    "fit",
    "go",
    "b",
    "get",
    "around",
    "paired",
    "sequencing",
    "paired",
    "sequencing",
    "repeat",
    "region",
    "duplicated",
    "regions",
    "reference",
    "using",
    "using",
    "read",
    "one",
    "every",
    "two",
    "pair",
    "fit",
    "reference",
    "easily",
    "output",
    "sequencing",
    "sam",
    "file",
    "binary",
    "version",
    "sam",
    "file",
    "bam",
    "file",
    "essentially",
    "file",
    "format",
    "two",
    "sections",
    "header",
    "common",
    "reads",
    "sample",
    "records",
    "unique",
    "read",
    "header",
    "information",
    "version",
    "sorting",
    "order",
    "leads",
    "case",
    "coordinates",
    "sorted",
    "reference",
    "sequence",
    "dictionary",
    "entries",
    "say",
    "reference",
    "sequence",
    "dictionaries",
    "used",
    "sample",
    "also",
    "read",
    "group",
    "information",
    "records",
    "unique",
    "lead",
    "read",
    "name",
    "gives",
    "like",
    "essentially",
    "sample",
    "name",
    "flow",
    "cell",
    "information",
    "lane",
    "information",
    "flags",
    "essentially",
    "binary",
    "information",
    "orientation",
    "read",
    "orientation",
    "read",
    "pair",
    "whether",
    "aligned",
    "forward",
    "reverse",
    "strand",
    "information",
    "given",
    "flag",
    "read",
    "position",
    "information",
    "mapping",
    "quality",
    "cigar",
    "string",
    "go",
    "details",
    "second",
    "mate",
    "information",
    "essentially",
    "telling",
    "position",
    "mate",
    "read",
    "mapped",
    "also",
    "insert",
    "size",
    "read",
    "sequence",
    "quality",
    "score",
    "come",
    "sequencer",
    "metadata",
    "example",
    "read",
    "group",
    "information",
    "tags",
    "link",
    "hidden",
    "link",
    "sam",
    "format",
    "get",
    "details",
    "specific",
    "tags",
    "information",
    "format",
    "yeah",
    "let",
    "talk",
    "questions",
    "far",
    "yes",
    "oh",
    "right",
    "recording",
    "nutshell",
    "need",
    "trimming",
    "workflow",
    "early",
    "processing",
    "mark",
    "adapters",
    "ignored",
    "rest",
    "processing",
    "anything",
    "like",
    "traditionally",
    "quality",
    "trimming",
    "necessary",
    "variant",
    "callers",
    "take",
    "quality",
    "basis",
    "accounts",
    "actually",
    "used",
    "part",
    "variant",
    "calling",
    "model",
    "really",
    "necessary",
    "college",
    "trimming",
    "adapter",
    "trimming",
    "either",
    "adapters",
    "marked",
    "recognized",
    "ignored",
    "accordingly",
    "right",
    "talk",
    "cigar",
    "string",
    "second",
    "string",
    "essentially",
    "summarizing",
    "alignment",
    "structure",
    "read",
    "example",
    "example",
    "cigar",
    "string",
    "essentially",
    "telling",
    "first",
    "base",
    "soft",
    "clear",
    "three",
    "matches",
    "one",
    "deletion",
    "two",
    "matches",
    "insertion",
    "another",
    "match",
    "say",
    "matches",
    "mean",
    "exact",
    "base",
    "match",
    "example",
    "case",
    "saying",
    "two",
    "matches",
    "bases",
    "match",
    "yes",
    "sorry",
    "say",
    "oh",
    "neck",
    "read",
    "positions",
    "next",
    "base",
    "reading",
    "okay",
    "going",
    "back",
    "three",
    "matches",
    "like",
    "said",
    "matches",
    "mean",
    "base",
    "match",
    "aligner",
    "saying",
    "three",
    "bases",
    "match",
    "three",
    "positions",
    "reference",
    "talking",
    "anything",
    "base",
    "magic",
    "alright",
    "usually",
    "like",
    "usually",
    "fast",
    "q",
    "file",
    "map",
    "reference",
    "using",
    "bw",
    "mm",
    "get",
    "mapped",
    "bam",
    "problem",
    "read",
    "group",
    "information",
    "metadata",
    "sample",
    "included",
    "fast",
    "q",
    "file",
    "want",
    "add",
    "regroup",
    "information",
    "analysis",
    "need",
    "process",
    "unnatural",
    "way",
    "get",
    "around",
    "broad",
    "use",
    "file",
    "format",
    "called",
    "unmapped",
    "bam",
    "essentially",
    "keeps",
    "read",
    "information",
    "metadata",
    "together",
    "one",
    "file",
    "happens",
    "uses",
    "unmapped",
    "bam",
    "file",
    "process",
    "processes",
    "creates",
    "mapped",
    "bam",
    "combines",
    "alignment",
    "information",
    "combines",
    "unmapped",
    "band",
    "mavim",
    "include",
    "metadata",
    "information",
    "using",
    "tool",
    "called",
    "merge",
    "bam",
    "alignment",
    "andre",
    "spoke",
    "sometimes",
    "reads",
    "may",
    "bacterial",
    "contamination",
    "apart",
    "merging",
    "bandwidth",
    "map",
    "bam",
    "merge",
    "bam",
    "alignment",
    "also",
    "cleanup",
    "reads",
    "right",
    "spoke",
    "alignment",
    "done",
    "dna",
    "data",
    "rna",
    "seek",
    "though",
    "special",
    "case",
    "intronic",
    "regions",
    "spliced",
    "familiar",
    "reads",
    "map",
    "using",
    "dna",
    "alignment",
    "tool",
    "huge",
    "deletions",
    "reads",
    "use",
    "special",
    "kind",
    "alignment",
    "tool",
    "recommend",
    "star",
    "tube",
    "bus",
    "alignment",
    "tool",
    "slide",
    "star",
    "alignment",
    "tool",
    "recommend",
    "rna",
    "seek",
    "elementary",
    "alright",
    "step",
    "one",
    "alignment",
    "let",
    "talk",
    "step",
    "two",
    "marking",
    "duplicates",
    "reads",
    "need",
    "example",
    "one",
    "oops",
    "one",
    "variant",
    "data",
    "basically",
    "amplified",
    "duplicates",
    "radiant",
    "color",
    "look",
    "take",
    "mean",
    "confidence",
    "book",
    "evidence",
    "confidence",
    "one",
    "particular",
    "variant",
    "essentially",
    "inflated",
    "want",
    "remove",
    "duplicates",
    "reason",
    "also",
    "case",
    "rna",
    "seek",
    "duplicates",
    "give",
    "artificial",
    "sense",
    "increased",
    "expression",
    "even",
    "rna",
    "seek",
    "data",
    "need",
    "remove",
    "duplicates",
    "yes",
    "music",
    "implementation",
    "detail",
    "provide",
    "mark",
    "duplicate",
    "step",
    "across",
    "lanes",
    "read",
    "groups",
    "really",
    "two",
    "types",
    "duplicates",
    "library",
    "level",
    "want",
    "able",
    "identify",
    "providing",
    "data",
    "together",
    "able",
    "kind",
    "cross",
    "library",
    "across",
    "regroup",
    "duplicate",
    "marketing",
    "music",
    "way",
    "regardless",
    "yeah",
    "true",
    "marketing",
    "duplicates",
    "really",
    "identifying",
    "ones",
    "considering",
    "duplicates",
    "actually",
    "remove",
    "file",
    "stay",
    "data",
    "true",
    "technically",
    "could",
    "remove",
    "completely",
    "actually",
    "option",
    "tool",
    "option",
    "remove",
    "tools",
    "remove",
    "reads",
    "file",
    "prefer",
    "keep",
    "really",
    "hate",
    "throwing",
    "away",
    "data",
    "little",
    "scary",
    "sometimes",
    "want",
    "reprocess",
    "things",
    "could",
    "decide",
    "throw",
    "inputs",
    "mean",
    "lost",
    "information",
    "lost",
    "ability",
    "go",
    "back",
    "change",
    "mind",
    "going",
    "yeah",
    "concerned",
    "storage",
    "example",
    "throw",
    "slides",
    "um",
    "eyes",
    "workshop",
    "information",
    "documentation",
    "could",
    "point",
    "actually",
    "post",
    "forum",
    "get",
    "suggest",
    "using",
    "forum",
    "attracting",
    "get",
    "back",
    "information",
    "probably",
    "bonnie",
    "gets",
    "back",
    "yeah",
    "easiest",
    "way",
    "make",
    "sure",
    "get",
    "yeah",
    "go",
    "ahead",
    "sorry",
    "instead",
    "bwa",
    "sorry",
    "hearing",
    "well",
    "variation",
    "craft",
    "toolkit",
    "oh",
    "graph",
    "sorry",
    "sorry",
    "yeah",
    "currently",
    "using",
    "graph",
    "based",
    "reference",
    "mean",
    "oh",
    "graph",
    "based",
    "alignments",
    "oh",
    "okay",
    "using",
    "yet",
    "might",
    "evaluation",
    "going",
    "say",
    "choice",
    "liner",
    "choice",
    "mapper",
    "aligner",
    "single",
    "impactful",
    "change",
    "make",
    "pipeline",
    "causes",
    "batch",
    "effects",
    "extremely",
    "conservative",
    "terms",
    "changing",
    "liner",
    "unless",
    "really",
    "unless",
    "really",
    "gives",
    "us",
    "great",
    "advantage",
    "anytime",
    "change",
    "liner",
    "effects",
    "results",
    "analyze",
    "different",
    "data",
    "sets",
    "aligned",
    "differently",
    "get",
    "batch",
    "effects",
    "recent",
    "paper",
    "think",
    "batch",
    "effects",
    "thousand",
    "genomes",
    "kind",
    "highlights",
    "kind",
    "paranoid",
    "partly",
    "process",
    "large",
    "datasets",
    "example",
    "nomad",
    "call",
    "sets",
    "point",
    "hundred",
    "thousand",
    "whole",
    "genomes",
    "costs",
    "ton",
    "money",
    "reprocess",
    "cohort",
    "like",
    "aligned",
    "exactly",
    "way",
    "change",
    "liner",
    "reprocess",
    "data",
    "often",
    "reprocess",
    "lot",
    "datasets",
    "sequence",
    "roads",
    "internal",
    "groups",
    "well",
    "customers",
    "yeah",
    "changing",
    "honestly",
    "traumatic",
    "process",
    "us",
    "much",
    "think",
    "exciting",
    "improvements",
    "made",
    "like",
    "graph",
    "based",
    "research",
    "alignments",
    "variant",
    "calling",
    "think",
    "probably",
    "years",
    "consider",
    "moving",
    "adopting",
    "kind",
    "production",
    "setting",
    "answer",
    "question",
    "think",
    "question",
    "first",
    "reduce",
    "bam",
    "oh",
    "tool",
    "called",
    "reduce",
    "reads",
    "years",
    "ago",
    "experimenting",
    "compressing",
    "read",
    "data",
    "abandoned",
    "one",
    "bad",
    "use",
    "like",
    "five",
    "years",
    "ago",
    "think",
    "definitely",
    "using",
    "anymore",
    "however",
    "compression",
    "purposes",
    "happy",
    "crime",
    "default",
    "production",
    "pipeline",
    "produce",
    "cram",
    "files",
    "storage",
    "still",
    "often",
    "data",
    "bam",
    "file",
    "running",
    "variant",
    "calling",
    "etc",
    "storage",
    "store",
    "everything",
    "crime",
    "files",
    "made",
    "pretty",
    "important",
    "improvements",
    "reducing",
    "file",
    "size",
    "depends",
    "depends",
    "settings",
    "use",
    "several",
    "different",
    "settings",
    "basic",
    "lossless",
    "setting",
    "losing",
    "information",
    "progressively",
    "lossy",
    "options",
    "things",
    "like",
    "compressing",
    "base",
    "quality",
    "bins",
    "base",
    "qualities",
    "compress",
    "gain",
    "hesitate",
    "give",
    "number",
    "one",
    "top",
    "head",
    "look",
    "pretty",
    "dramatic",
    "improvements",
    "yeah",
    "sure",
    "think",
    "question",
    "dealing",
    "batch",
    "effects",
    "reprocess",
    "everything",
    "like",
    "know",
    "going",
    "batch",
    "effects",
    "like",
    "know",
    "something",
    "pipeline",
    "going",
    "cause",
    "batch",
    "effects",
    "change",
    "pipeline",
    "happen",
    "reprocess",
    "everything",
    "pipeline",
    "kinds",
    "batch",
    "effects",
    "lead",
    "things",
    "like",
    "thinking",
    "variant",
    "unique",
    "particular",
    "population",
    "different",
    "alignments",
    "choices",
    "often",
    "different",
    "artifacts",
    "ability",
    "detect",
    "variants",
    "difficult",
    "regions",
    "influenced",
    "prefer",
    "even",
    "means",
    "going",
    "artifact",
    "data",
    "prefer",
    "everywhere",
    "opposed",
    "subsets",
    "yeah",
    "done",
    "work",
    "broad",
    "collaborated",
    "major",
    "sequencing",
    "centers",
    "north",
    "america",
    "define",
    "call",
    "functional",
    "equivalence",
    "specification",
    "specification",
    "alignments",
    "way",
    "removes",
    "batch",
    "effects",
    "pipeline",
    "compliant",
    "specification",
    "pretty",
    "sure",
    "datasets",
    "aligned",
    "pipelines",
    "compliant",
    "pipelines",
    "compatible",
    "paper",
    "google",
    "functional",
    "equivalence",
    "pipeline",
    "think",
    "come",
    "also",
    "blog",
    "post",
    "gta",
    "blog",
    "really",
    "encourage",
    "people",
    "adopt",
    "biggest",
    "factor",
    "actually",
    "aligner",
    "couple",
    "additional",
    "things",
    "towards",
    "questions",
    "right",
    "right",
    "duplicates",
    "come",
    "library",
    "duplicates",
    "optical",
    "duplicates",
    "andre",
    "went",
    "details",
    "move",
    "fastest",
    "going",
    "mod",
    "duplicates",
    "mark",
    "duplicates",
    "essentially",
    "looks",
    "orientation",
    "reed",
    "unclipped",
    "v",
    "prime",
    "position",
    "first",
    "strip",
    "base",
    "matching",
    "looks",
    "antelope",
    "v",
    "prime",
    "position",
    "orientation",
    "reed",
    "decides",
    "whether",
    "duplicate",
    "parent",
    "reed",
    "thing",
    "pair",
    "orientation",
    "unclip",
    "phi",
    "prime",
    "position",
    "2",
    "igv",
    "screenshots",
    "one",
    "without",
    "duplicate",
    "smart",
    "multiplicate",
    "works",
    "essentially",
    "marking",
    "duplicates",
    "option",
    "gd",
    "show",
    "reeds",
    "hiding",
    "duplicate",
    "reeds",
    "note",
    "mark",
    "duplicates",
    "remove",
    "reads",
    "marks",
    "duplicate",
    "tag",
    "adds",
    "alignment",
    "show",
    "duplicate",
    "read",
    "back",
    "day",
    "tool",
    "locally",
    "realignment",
    "around",
    "complex",
    "regions",
    "genome",
    "example",
    "repetative",
    "regions",
    "shown",
    "example",
    "would",
    "call",
    "variants",
    "realignment",
    "sees",
    "actually",
    "one",
    "insertion",
    "clears",
    "cleans",
    "reads",
    "removes",
    "extra",
    "snips",
    "originally",
    "calling",
    "deprecated",
    "tool",
    "anymore",
    "variant",
    "colors",
    "haplotype",
    "collar",
    "mutex",
    "talk",
    "detail",
    "intrinsically",
    "basically",
    "deprecated",
    "tor",
    "final",
    "step",
    "process",
    "fine",
    "tune",
    "bass",
    "quality",
    "scores",
    "variant",
    "calling",
    "pipeline",
    "heavily",
    "depends",
    "quality",
    "basses",
    "called",
    "sequencer",
    "errors",
    "sequencer",
    "might",
    "added",
    "quality",
    "scores",
    "get",
    "used",
    "vain",
    "caller",
    "irrespective",
    "want",
    "clean",
    "quality",
    "scores",
    "use",
    "tool",
    "called",
    "bq",
    "sr",
    "essentially",
    "detects",
    "systematic",
    "errors",
    "created",
    "sequencer",
    "could",
    "due",
    "physics",
    "chemistry",
    "sequencing",
    "technique",
    "manufacturing",
    "defaults",
    "sequencer",
    "causes",
    "errors",
    "across",
    "sample",
    "bq",
    "sr",
    "uses",
    "machine",
    "learning",
    "create",
    "empirical",
    "error",
    "model",
    "uses",
    "model",
    "bases",
    "sample",
    "fine",
    "tunes",
    "quality",
    "score",
    "covariance",
    "uses",
    "say",
    "example",
    "two",
    "row",
    "read",
    "two",
    "one",
    "percent",
    "increased",
    "error",
    "rate",
    "fine",
    "tunes",
    "quality",
    "score",
    "say",
    "base",
    "first",
    "two",
    "1",
    "lower",
    "base",
    "quality",
    "essentially",
    "shown",
    "diagram",
    "dinucleotide",
    "contacts",
    "accuracy",
    "empirical",
    "quality",
    "minus",
    "reported",
    "quality",
    "negative",
    "values",
    "show",
    "show",
    "bases",
    "quality",
    "scores",
    "estimated",
    "positive",
    "value",
    "shows",
    "underestimated",
    "quality",
    "scores",
    "cases",
    "seen",
    "sequences",
    "tend",
    "overestimate",
    "quality",
    "scores",
    "underestimate",
    "map",
    "rip",
    "pink",
    "dots",
    "p",
    "q",
    "sr",
    "purple",
    "bq",
    "sr",
    "compare",
    "reported",
    "versus",
    "empirical",
    "quality",
    "see",
    "neat",
    "little",
    "line",
    "forms",
    "apply",
    "base",
    "quality",
    "recalibration",
    "something",
    "note",
    "bq",
    "sr",
    "change",
    "basis",
    "say",
    "oh",
    "particular",
    "base",
    "called",
    "wrong",
    "going",
    "change",
    "fine",
    "tunes",
    "quality",
    "score",
    "confidence",
    "calling",
    "base",
    "independent",
    "coverage",
    "mean",
    "really",
    "affects",
    "like",
    "individual",
    "base",
    "calls",
    "quality",
    "link",
    "filters",
    "gatk",
    "filtering",
    "quality",
    "base",
    "calls",
    "certain",
    "thresholds",
    "bases",
    "ignored",
    "goes",
    "back",
    "question",
    "trimming",
    "need",
    "trim",
    "quality",
    "part",
    "take",
    "take",
    "quality",
    "bases",
    "account",
    "forget",
    "exact",
    "threshold",
    "let",
    "say",
    "q",
    "10",
    "q",
    "10",
    "ignore",
    "looking",
    "counting",
    "much",
    "coverage",
    "way",
    "aware",
    "usable",
    "quality",
    "might",
    "end",
    "less",
    "coverage",
    "originally",
    "looking",
    "usable",
    "coverage",
    "answer",
    "question",
    "yes",
    "yeah",
    "bonnie",
    "going",
    "explain",
    "mask",
    "real",
    "variation",
    "suggest",
    "go",
    "explanation",
    "address",
    "question",
    "applies",
    "cancer",
    "samples",
    "right",
    "essentially",
    "loney",
    "trying",
    "say",
    "error",
    "model",
    "created",
    "first",
    "known",
    "variants",
    "masked",
    "example",
    "use",
    "db",
    "snip",
    "see",
    "known",
    "variants",
    "mask",
    "create",
    "error",
    "model",
    "rest",
    "variance",
    "rest",
    "mismatches",
    "anything",
    "unknown",
    "variant",
    "assumed",
    "error",
    "model",
    "created",
    "top",
    "somatic",
    "aspect",
    "comes",
    "imagine",
    "masking",
    "known",
    "variation",
    "db",
    "snip",
    "example",
    "certainly",
    "ca",
    "mask",
    "variants",
    "reported",
    "previously",
    "even",
    "germline",
    "samples",
    "sequence",
    "variants",
    "known",
    "db",
    "snip",
    "therefore",
    "masked",
    "nice",
    "thing",
    "number",
    "bases",
    "much",
    "much",
    "larger",
    "number",
    "variants",
    "might",
    "case",
    "billions",
    "bases",
    "many",
    "reads",
    "affected",
    "real",
    "variance",
    "previously",
    "undisclosed",
    "bearings",
    "completely",
    "get",
    "washed",
    "question",
    "well",
    "cancer",
    "sample",
    "much",
    "higher",
    "amount",
    "mutations",
    "possible",
    "starts",
    "introducing",
    "noise",
    "seen",
    "work",
    "cancer",
    "genome",
    "analysis",
    "team",
    "time",
    "parts",
    "still",
    "seeing",
    "effects",
    "mutations",
    "cancer",
    "context",
    "still",
    "way",
    "unaffected",
    "bases",
    "affected",
    "basis",
    "somatic",
    "patterns",
    "additional",
    "variation",
    "however",
    "analysts",
    "group",
    "work",
    "sometimes",
    "use",
    "database",
    "like",
    "cosmic",
    "mask",
    "common",
    "kind",
    "recurring",
    "mutations",
    "see",
    "cancer",
    "things",
    "come",
    "kind",
    "places",
    "possible",
    "mask",
    "general",
    "see",
    "really",
    "worth",
    "worse",
    "get",
    "little",
    "bit",
    "extra",
    "noise",
    "model",
    "still",
    "detect",
    "systematic",
    "errors",
    "ultimately",
    "detecting",
    "systematic",
    "errors",
    "machine",
    "systematically",
    "made",
    "errors",
    "places",
    "cancer",
    "mutations",
    "part",
    "show",
    "random",
    "mutations",
    "associated",
    "particular",
    "covariates",
    "another",
    "model",
    "much",
    "problem",
    "much",
    "problem",
    "answer",
    "question",
    "great",
    "right",
    "finally",
    "show",
    "information",
    "new",
    "based",
    "quality",
    "scores",
    "show",
    "alignment",
    "file",
    "oops",
    "recalibrated",
    "based",
    "qualities",
    "also",
    "original",
    "based",
    "qualities",
    "usually",
    "remove",
    "base",
    "qualities",
    "save",
    "space",
    "yeah",
    "wanted",
    "could",
    "original",
    "base",
    "qualities",
    "wraps",
    "talk",
    "questions",
    "take",
    "coffee",
    "break",
    "next",
    "talk",
    "alright",
    "thank",
    "everyone"
  ],
  "keywords": [
    "name",
    "broad",
    "essentially",
    "gatk",
    "questions",
    "forum",
    "us",
    "post",
    "alright",
    "data",
    "might",
    "right",
    "andre",
    "things",
    "go",
    "sequencing",
    "going",
    "talk",
    "three",
    "first",
    "want",
    "map",
    "reference",
    "genome",
    "remove",
    "duplicates",
    "spoke",
    "base",
    "quality",
    "scores",
    "details",
    "even",
    "process",
    "give",
    "variant",
    "reads",
    "calls",
    "way",
    "call",
    "confidence",
    "much",
    "possible",
    "sequencer",
    "get",
    "affected",
    "calling",
    "see",
    "step",
    "looking",
    "region",
    "come",
    "aligner",
    "dna",
    "use",
    "recommend",
    "2",
    "rna",
    "seek",
    "star",
    "mapping",
    "sometimes",
    "example",
    "variants",
    "still",
    "alignment",
    "cases",
    "samples",
    "duplicated",
    "regions",
    "gets",
    "say",
    "yeah",
    "gives",
    "little",
    "particular",
    "case",
    "know",
    "pretty",
    "different",
    "rest",
    "around",
    "using",
    "read",
    "one",
    "two",
    "pair",
    "sam",
    "file",
    "bam",
    "format",
    "sample",
    "unique",
    "information",
    "sequence",
    "used",
    "also",
    "group",
    "like",
    "orientation",
    "aligned",
    "position",
    "cigar",
    "string",
    "mapped",
    "score",
    "metadata",
    "link",
    "let",
    "yes",
    "oh",
    "need",
    "trimming",
    "mark",
    "ignored",
    "anything",
    "take",
    "basis",
    "actually",
    "part",
    "model",
    "really",
    "matches",
    "match",
    "mean",
    "bases",
    "sorry",
    "back",
    "usually",
    "q",
    "problem",
    "called",
    "unmapped",
    "uses",
    "tool",
    "kind",
    "duplicate",
    "across",
    "library",
    "could",
    "option",
    "prefer",
    "reprocess",
    "change",
    "storage",
    "sure",
    "well",
    "variation",
    "graph",
    "based",
    "alignments",
    "liner",
    "pipeline",
    "batch",
    "effects",
    "think",
    "datasets",
    "often",
    "improvements",
    "made",
    "years",
    "answer",
    "question",
    "everything",
    "depends",
    "qualities",
    "number",
    "work",
    "specification",
    "reed",
    "prime",
    "show",
    "fine",
    "errors",
    "bq",
    "sr",
    "systematic",
    "created",
    "empirical",
    "error",
    "tunes",
    "reported",
    "coverage",
    "mask",
    "cancer",
    "known",
    "db",
    "snip",
    "mutations"
  ]
}