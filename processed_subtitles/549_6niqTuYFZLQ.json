{
  "text": "- Okay.\nCan everyone hear me?\nOkay.\nSorry for the delay.\nI had a bit of technical difficulty.\nToday was the first time\nI was trying to use my\nnew touch bar Mac book pro for presenting,\nand none of the adapters are working.\nSo, I had to switch\nlaptops at the last minute.\nSo, thanks.\nSorry about that.\nSo, today is lecture 10.\nWe're talking about\nrecurrent neural networks.\nSo, as of, as usual, a\ncouple administrative notes.\nSo, We're working hard\non assignment one grading.\nThose grades will probably be out\nsometime later today.\nHopefully, they can get out\nbefore the A2 deadline.\nThat's what I'm hoping for.\nOn a related note, Assignment\ntwo is due today at 11:59 p.m.\nso, who's done with that already?\nAbout half you guys.\nSo, you remember, I did warn you\nwhen the assignment went out\nthat it was quite long, to start early.\nSo, you were warned about that.\nBut, hopefully, you guys\nhave some late days left.\nAlso, as another reminder,\nthe midterm will be in class on Tuesday.\nIf you kind of look\naround the lecture hall,\nthere are not enough seats in this room\nto seat all the enrolled\nstudents in the class.\nSo, we'll actually be having the midterm\nin several other lecture\nhalls across campus.\nAnd we'll be sending out some more details\non exactly where to go in\nthe next couple of days.\nSo a bit of a, another\nbit of announcement.\nWe've been working on this sort of\nfun bit of extra credit\nthing for you to play with\nthat we're calling the training game.\nThis is this cool\nbrowser based experience,\nwhere you can go in\nand interactively train neural networks\nand tweak the hyper\nparameters during training.\nAnd this should be a\nreally cool interactive way\nfor you to practice\nsome of these hyper\nparameter tuning skills\nthat we've been talking about\nthe last couple of lectures.\nSo this is not required,\nbut this, I think, will be\na really useful experience\nto gain a little bit more intuition\ninto how some of these\nhyper parameters work\nfor different types of\ndata sets in practice.\nSo we're still working on getting\nall the bugs worked out of this setup,\nand we'll probably send out\nsome more instructions\non exactly how this will work\nin the next couple of days.\nBut again, not required.\nBut please do check it out.\nI think it'll be really fun\nand a really cool thing\nfor you to play with.\nAnd will give you a bit of extra credit\nif you do some,\nif you end up working with this\nand doing a couple of runs with it.\nSo, we'll again send out\nsome more details about this\nsoon once we get all the bugs worked out.\nAs a reminder,\nlast time we were talking\nabout CNN Architectures.\nWe kind of walked through the time line\nof some of the various winners\nof the image net classification challenge,\nkind of the breakthrough result.\nAs we saw was the AlexNet\narchitecture in 2012,\nwhich was a nine layer\nconvolutional network.\nIt did amazingly well,\nand it sort of kick started\nthis whole deep learning\nrevolution in computer vision,\nand kind of brought a lot of these models\ninto the mainstream.\nThen we skipped ahead a couple years,\nand saw that in 2014 image net challenge,\nwe had these two really\ninteresting models,\nVGG and GoogLeNet,\nwhich were much deeper.\nSo VGG was,\nthey had a 16 and a 19 layer model,\nand GoogLeNet was, I believe,\na 22 layer model.\nAlthough one thing that\nis kind of interesting\nabout these models\nis that the 2014 image net challenge\nwas right before batch\nnormalization was invented.\nSo at this time,\nbefore the invention\nof batch normalization,\ntraining these relatively deep models\nof roughly twenty layers\nwas very challenging.\nSo, in fact, both of these two models\nhad to resort to a little bit of hackery\nin order to get their\ndeep models to converge.\nSo for VGG, they had the\n16 and 19 layer models,\nbut actually they first\ntrained an 11 layer model,\nbecause that was what they\ncould get to converge.\nAnd then added some extra\nrandom layers in the middle\nand then continued training,\nactually training the\n16 and 19 layer models.\nSo, managing this training process\nwas very challenging in 2014\nbefore the invention\nof batch normalization.\nSimilarly, for GoogLeNet,\nwe saw that GoogLeNet has\nthese auxiliary classifiers\nthat were stuck into lower\nlayers of the network.\nAnd these were not really\nneeded for the class to,\nto get good classification performance.\nThis was just sort of a way to cause\nextra gradient to be injected\ndirectly into the lower\nlayers of the network.\nAnd this sort of,\nthis again was before the\ninvention of batch normalization\nand now once you have these networks\nwith batch normalization,\nthen you no longer need\nthese slightly ugly hacks\nin order to get these\ndeeper models to converge.\nThen we also saw in the\n2015 image net challenge\nwas this really cool model called ResNet,\nthese residual networks\nthat now have these shortcut connections\nthat actually have these\nlittle residual blocks\nwhere we're going to take our input,\npass it through the residual blocks,\nand then add that output of the,\nthen add our input to the block,\nto the output from these\nconvolutional layers.\nThis is kind of a funny architecture,\nbut it actually has two\nreally nice properties.\nOne is that if we just set all the weights\nin this residual block to zero,\nthen this block is competing the identity.\nSo in some way,\nit's relatively easy for this model\nto learn not to use the\nlayers that it doesn't need.\nIn addition, it kind of adds this\ninterpretation to L2 regularization\nin the context of these neural networks,\ncause once you put L2 regularization,\nremember, on your,\non the weights of your network,\nthat's going to drive all\nthe parameters towards zero.\nAnd maybe your standard\nconvolutional architecture\nis driving towards zero.\nMaybe it doesn't make sense.\nBut in the context of a residual network,\nif you drive all the\nparameters towards zero,\nthat's kind of encouraging the model\nto not use layers that it doesn't need,\nbecause it will just drive those,\nthe residual blocks towards the identity,\nwhether or not needed for classification.\nThe other really useful property\nof these residual networks\nhas to do with the gradient\nflow in the backward paths.\nIf you remember what happens\nat these addition gates\nin the backward pass,\nwhen upstream gradient is coming in\nthrough an addition gate,\nthen it will split\nand fork along these two different paths.\nSo then, when upstream gradient comes in,\nit'll take one path through\nthese convolutional blocks,\nbut it will also have a direct\nconnection of the gradient\nthrough this residual connection.\nSo then when you look at,\nwhen you imagine stacking many of these\nresidual blocks on top of each other,\nand our network ends up with hundreds of,\npotentially hundreds of layers.\nThen, these residual connections\ngive a sort of gradient super highway\nfor gradients to flow backward\nthrough the entire network.\nAnd this allows it to train much easier\nand much faster.\nAnd actually allows\nthese things to converge\nreasonably well,\neven when the model is potentially\nhundreds of layers deep.\nAnd this idea of managing\ngradient flow in your models\nis actually super important\neverywhere in machine learning.\nAnd super prevalent in\nrecurrent networks as well.\nSo we'll definitely revisit\nthis idea of gradient flow\nlater in today's lecture.\nSo then, we kind of also saw\na couple other more exotic,\nmore recent CNN architectures last time,\nincluding DenseNet and FractalNet,\nand once you think about\nthese architectures\nin terms of gradient flow,\nthey make a little bit more sense.\nThese things like DenseNet and FractalNet\nare adding these additional shortcut\nor identity connections inside the model.\nAnd if you think about what happens\nin the backwards pass for these models,\nthese additional funny topologies\nare basically providing direct paths\nfor gradients to flow\nfrom the loss at the end of the network\nmore easily into all the\ndifferent layers of the network.\nSo I think that,\nagain, this idea of managing\ngradient flow properly\nin your CNN Architectures\nis something that we've really seen\na lot more in the last couple of years.\nAnd will probably see more moving forward\nas more exotic architectures are invented.\nWe also saw this kind of nice plot,\nplotting performance of\nthe number of flops versus\nthe number of parameters\nversus the run time of\nthese various models.\nAnd there's some\ninteresting characteristics\nthat you can dive in\nand see from this plot.\nOne idea is that VGG and AlexNet\nhave a huge number of parameters,\nand these parameters actually come\nalmost entirely from the\nfully connected layers\nof the models.\nSo AlexNet has something like\nroughly 62 million parameters,\nand if you look at that\nlast fully connected layer,\nthe final fully connected layer in AlexNet\nis going from an activation\nvolume of six by six by 256\ninto this fully connected vector of 496.\nSo if you imagine what the weight matrix\nneeds to look like at that layer,\nthe weight matrix is gigantic.\nIt's number of entries is six by six,\nsix times six times 256 times 496.\nAnd if you multiply that out,\nyou see that that single layer\nhas 38 million parameters.\nSo more than half of the parameters\nof the entire AlexNet model\nare just sitting in that\nlast fully connected layer.\nAnd if you add up all the parameters\nin just the fully connected\nlayers of AlexNet,\nincluding these other\nfully connected layers,\nyou see something like\n59 of the 62 million\nparameters in AlexNet\nare sitting in these\nfully connected layers.\nSo then when we move other architectures,\nlike GoogLeNet and ResNet,\nthey do away with a lot of these large\nfully connected layers\nin favor of global average pooling\nat the end of the network.\nAnd this allows these\nnetworks to really cut,\nthese nicer architectures,\nto really cut down the parameter count\nin these architectures.\nSo that was kind of our brief recap\nof the CNN architectures\nthat we saw last lecture,\nand then today,\nwe're going to move to\none of my favorite topics to talk about,\nwhich is recurrent neural networks.\nSo, so far in this class,\nwe've seen, what I like to think of\nas kind of a vanilla feed forward network,\nall of our network\narchitectures have this flavor,\nwhere we receive some input\nand that input is a fixed size object,\nlike an image or vector.\nThat input is fed through\nsome set of hidden layers\nand produces a single output,\nlike a classifications,\nlike a set of classifications scores\nover a set of categories.\nBut in some context in machine learning,\nwe want to have more flexibility\nin the types of data that\nour models can process.\nSo once we move to this idea\nof recurrent neural networks,\nwe have a lot more opportunities\nto play around with the types\nof input and output data\nthat our networks can handle.\nSo once we have recurrent neural networks,\nwe can do what we call\nthese one to many models.\nOr where maybe our input is\nsome object of fixed size,\nlike an image,\nbut now our output is a\nsequence of variable length,\nsuch as a caption.\nWhere different captions\nmight have different numbers of words,\nso our output needs to\nbe variable in length.\nWe also might have many to one models,\nwhere our input could be variably sized.\nThis might be something\nlike a piece of text,\nand we want to say what is\nthe sentiment of that text,\nwhether it's positive or\nnegative in sentiment.\nOr in a computer vision context,\nyou might imagine taking as input a video,\nand that video might have a\nvariable number of frames.\nAnd now we want to read this entire video\nof potentially variable length.\nAnd then at the end,\nmake a classification decision\nabout maybe what kind\nof activity or action\nis going on in that video.\nWe also have a, we\nmight also have problems\nwhere we want both the inputs\nand the output to be variable in length.\nWe might see something like this\nin machine translation,\nwhere our input is some,\nmaybe, sentence in English,\nwhich could have a variable length,\nand our output is maybe\nsome sentence in French,\nwhich also could have a variable length.\nAnd crucially, the length\nof the English sentence\nmight be different from the\nlength of the French sentence.\nSo we need some models\nthat have the capacity\nto accept both variable length sequences\non the input and on the output.\nFinally, we might also\nconsider problems where\nour input is variably length,\nlike something like a video sequence\nwith a variable number of frames.\nAnd now we want to make a decision\nfor each element of that input sequence.\nSo in the context of videos,\nthat might be making some\nclassification decision\nalong every frame of the video.\nAnd recurrent neural networks\nare this kind of general paradigm\nfor handling variable sized sequence data\nthat allow us to pretty naturally capture\nall of these different types\nof setups in our models.\nSo recurring neural networks\nare actually important,\neven for some problems that\nhave a fixed size input\nand a fixed size output.\nRecurrent neural networks\ncan still be pretty useful.\nSo in this example,\nwe might want to do, for example,\nsequential processing of our input.\nSo here, we're receiving\na fixed size input\nlike an image,\nand we want to make a\nclassification decision\nabout, like, what number is\nbeing shown in this image?\nBut now, rather than just doing\na single feed forward pass\nand making the decision all at once,\nthis network is actually\nlooking around the image\nand taking various glimpses of\ndifferent parts of the image.\nAnd then after making\nsome series of glimpses,\nthen it makes its final decision\nas to what kind of number is present.\nSo here, we had one,\nSo here, even though\nour input and outputs,\nour input was an image,\nand our output was a\nclassification decision,\neven this context,\nthis idea of being able to handle\nvariably length processing\nwith recurrent neural networks\ncan lead to some really\ninteresting types of models.\nThere's a really cool paper that I like\nthat applied this same type of idea\nto generating new images.\nWhere now, we want the model\nto synthesize brand new images\nthat look kind of like the\nimages it saw in training,\nand we can use a recurrent\nneural network architecture\nto actually paint these output images\nsort of one piece at a time in the output.\nYou can see that,\neven though our output\nis this fixed size image,\nwe can have these models\nthat are working over time\nto compute parts of the output\none at a time sequentially.\nAnd we can use recurrent neural networds\nfor that type of setup as well.\nSo after this sort of cool pitch\nabout all these cool\nthings that RNNs can do,\nyou might wonder, like what\nexactly are these things?\nSo in general, a recurrent neural network\nis this little, has this\nlittle recurrent core cell\nand it will take some input x,\nfeed that input into the RNN,\nand that RNN has some\ninternal hidden state,\nand that internal hidden\nstate will be updated\nevery time that the RNN reads a new input.\nAnd that internal hidden state\nwill be then fed back to the model\nthe next time it reads an input.\nAnd frequently, we will want our RNN\"s\nto also produce some\noutput at every time step,\nso we'll have this pattern\nwhere it will read an input,\nupdate its hidden state,\nand then produce an output.\nSo then the question is\nwhat is the functional form\nof this recurrence relation\nthat we're computing?\nSo inside this little green RNN block,\nwe're computing some recurrence relation,\nwith a function f.\nSo this function f will\ndepend on some weights, w.\nIt will accept the previous\nhidden state, h t - 1,\nas well as the input at\nthe current state, x t,\nand this will output\nthe next hidden state, or\nthe updated hidden state,\nthat we call h t.\nAnd now,\nthen as we read the next input,\nthis hidden state,\nthis new hidden state, h t,\nwill then just be passed\ninto the same function\nas we read the next input, x t plus one.\nAnd now, if we wanted\nto produce some output\nat every time step of this network,\nwe might attach some additional\nfully connected layers\nthat read in this h t at every time step.\nAnd make that decision based\non the hidden state at every time step.\nAnd one thing to note is that\nwe use the same function, f w,\nand the same weights, w,\nat every time step of the computation.\nSo then kind of the simplest function form\nthat you can imagine\nis what we call this vanilla\nrecurrent neural network.\nSo here, we have this same functional form\nfrom the previous slide,\nwhere we're taking in\nour previous hidden state\nand our current input\nand we need to produce\nthe next hidden state.\nAnd the kind of simplest\nthing you might imagine\nis that we have some weight matrix, w x h,\nthat we multiply against the input, x t,\nas well as another weight matrix, w h h,\nthat we multiply against\nthe previous hidden state.\nSo we make these two multiplications\nagainst our two states,\nadd them together,\nand squash them through a tanh,\nso we get some kind of non\nlinearity in the system.\nYou might be wondering\nwhy we use a tanh here\nand not some other type of non-linearity?\nAfter all that we've said negative\nabout tanh's in previous lectures,\nand I think we'll return\na little bit to that\nlater on when we talk about\nmore advanced architectures, like lstm.\nSo then, this,\nSo then, in addition in this architecture,\nif we wanted to produce\nsome y t at every time step,\nyou might have another weight matrix, w,\nyou might have another weight matrix\nthat accepts this hidden state\nand then transforms it to some y\nto produce maybe some\nclass score predictions\nat every time step.\nAnd when I think about\nrecurrent neural networks,\nI kind of think about, you can also,\nyou can kind of think of\nrecurrent neural networks\nin two ways.\nOne is this concept of\nhaving a hidden state\nthat feeds back at itself, recurrently.\nBut I find that picture\na little bit confusing.\nAnd sometimes, I find it clearer\nto think about unrolling\nthis computational graph\nfor multiple time steps.\nAnd this makes the data\nflow of the hidden states\nand the inputs and the\noutputs and the weights\nmaybe a little bit more clear.\nSo then at the first time step,\nwe'll have some initial\nhidden state h zero.\nThis is usually initialized\nto zeros for most context,\nin most contexts, an then\nwe'll have some input, x t.\nThis initial hidden state, h zero,\nand our current input, x t,\nwill go into our f w function.\nThis will produce our\nnext hidden state, h one.\nAnd then, we'll repeat this process\nwhen we receive the next input.\nSo now our current h one and our x one,\nwill go into that same f w,\nto produce our next output, h two.\nAnd this process will\nrepeat over and over again,\nas we consume all of the input, x ts,\nin our sequence of inputs.\nAnd now, one thing to note, is that\nwe can actually make\nthis even more explicit\nand write the w matrix in\nour computational graph.\nAnd here you can see that\nwe're re-using the same w matrix\nat every time step of the computation.\nSo now every time that we\nhave this little f w block,\nit's receiving a unique h and a unique x,\nbut all of these blocks\nare taking the same w.\nAnd if you remember,\nwe talked about how gradient\nflows in back propagation,\nwhen you re-use the same,\nwhen you re-use the\nsame node multiple times\nin a computational graph,\nthen remember during the backward pass,\nyou end up summing the gradients\ninto the w matrix\nwhen you're computing a d los d w.\nSo, if you kind of think about\nthe back propagation for this model,\nthen you'll have a separate gradient for w\nflowing from each of those time steps,\nand then the final gradient for w\nwill be the sum of all of those\nindividual per time step gradiants.\nWe can also write to this y t explicitly\nin this computational graph.\nSo then, this output,\nh t, at every time step\nmight feed into some other\nlittle neural network\nthat can produce a y t,\nwhich might be some class\nscores, or something like that,\nat every time step.\nWe can also make the loss more explicit.\nSo in many cases, you\nmight imagine producing,\nyou might imagine that you\nhave some ground truth label\nat every time step of your sequence,\nand then you'll compute some loss,\nsome individual loss,\nat every time step of\nthese outputs, y t's.\nAnd this loss might,\nit will frequently be\nsomething like soft max loss,\nin the case where you have, maybe,\na ground truth label at every\ntime step of the sequence.\nAnd now the final loss for the entire,\nfor this entire training stop,\nwill be the sum of\nthese individual losses.\nSo now, we had a scaler\nloss at every time step?\nAnd we just summed them\nup to get our final\nscaler loss at the top of the network.\nAnd now, if you think about,\nagain, back propagation\nthrough this thing,\nwe need, in order to train the model,\nwe need to compute the gradient\nof the loss with respect to w.\nSo, we'll have loss flowing\nfrom that final loss\ninto each of these time steps.\nAnd then each of those time steps\nwill compute a local\ngradient on the weights, w,\nwhich will all then be\nsummed to give us our final\ngradient for the weights, w.\nNow if we have a, sort of,\nthis many to one situation,\nwhere maybe we want to do\nsomething like sentiment analysis,\nthen we would typically make that decision\nbased on the final hidden\nstate of this network.\nBecause this final hidden state\nkind of summarizes all of the context\nfrom the entire sequence.\nAlso, if we have a kind of\na one to many situation,\nwhere we want to receive a fix sized input\nand then produce a variably sized output.\nThen you'll commonly use\nthat fixed size input\nto initialize, somehow,\nthe initial hidden state of the model,\nand now the recurrent network will tick\nfor each cell in the output.\nAnd now, as you produce\nyour variably sized output,\nyou'll unroll the graph for\neach element in the output.\nSo this, when we talk about\nthe sequence to sequence models\nwhere you might do something\nlike machine translation,\nwhere you take a variably sized input\nand a variably sized output.\nYou can think of this as a combination\nof the many to one,\nplus a one to many.\nSo, we'll kind of proceed in two stages,\nwhat we call an encoder and a decoder.\nSo if you're the encoder,\nwe'll receive the variably sized input,\nwhich might be your sentence in English,\nand then summarize that entire sentence\nusing the final hidden state\nof the encoder network.\nAnd now we're in this\nmany to one situation\nwhere we've summarized this\nentire variably sized input\nin this single vector,\nand now, we have a second decoder network,\nwhich is a one to many situation,\nwhich will input that single vector\nsummarizing the input sentence\nand now produce this\nvariably sized output,\nwhich might be your sentence\nin another language.\nAnd now in this variably sized output,\nwe might make some predictions\nat every time step,\nmaybe about what word to use.\nAnd you can imagine kind of\ntraining this entire thing\nby unrolling this computational graph\nsumming the losses at the output sequence\nand just performing back\npropagation, as usual.\nSo as a bit of a concrete example,\none thing that we frequently use\nrecurrent neural networks for,\nis this problem called language modeling.\nSo in the language modeling problem,\nwe want to read some sequence of,\nwe want to have our\nnetwork, sort of, understand\nhow to produce natural language.\nSo in the, so this, this might\nhappen at the character level\nwhere our model will produce\ncharacters one at a time.\nThis might also happen at the word level\nwhere our model will\nproduce words one at a time.\nBut in a very simple example,\nyou can imagine this\ncharacter level language model\nwhere we want,\nwhere the network will read\nsome sequence of characters\nand then it needs to predict,\nwhat will the next character\nbe in this stream of text?\nSo in this example,\nwe have this very small\nvocabulary of four letters,\nh, e, l, and o, and we have\nthis example training sequence\nof the word hello, h, e, l, l, o.\nSo during training,\nwhen we're training this language model,\nwe will feed the characters\nof this training sequence\nas inputs, as x ts, to out input of our,\nwe'll feed the characters\nof our training sequence,\nthese will be the x ts that\nwe feed in as the inputs\nto our recurrent neural network.\nAnd then, each of these inputs,\nit's a letter,\nand we need to figure out a way\nto represent letters in our network.\nSo what we'll typically do is figure out\nwhat is our total vocabulary.\nIn this case, our vocabulary\nhas four elements.\nAnd each letter will be\nrepresented by a vector\nthat has zeros in every slot but one,\nand a one for the slot in the vocabulary\ncorresponding to that letter.\nIn this little example,\nsince our vocab has the\nfour letters, h, e, l, o,\nthen our input sequence,\nthe h is represented by\na four element vector\nwith a one in the first slot\nand zero's in the other three slots.\nAnd we use the same sort of pattern\nto represent all the different letters\nin the input sequence.\nNow, during this forward pass\nof what this network is doing,\nat the first time step,\nit will receive the input letter h.\nThat will go into the first RNN,\nto the RNN cell,\nand then we'll produce this output, y t,\nwhich is the network making predictions\nabout for each letter in the vocabulary,\nwhich letter does it think is most likely\ngoing to come next.\nIn this example,\nthe correct output letter was e\nbecause our training sequence was hello,\nbut the model is actually predicting,\nI think it's actually\npredicting o as the most likely letter.\nSo in this case, this prediction was wrong\nand we would use softmaxt loss\nto quantify our unhappiness\nwith these predictions.\nThe next time step,\nwe would feed in the second letter\nin the training sequence, e,\nand this process will repeat.\nWe'll now represent e as a vector.\nUse that input vector together\nwith the previous hidden state\nto produce a new hidden state\nand now use the second hidden state\nto, again, make predictions\nover every letter in the vocabulary.\nIn this case, because our\ntraining sequence was hello,\nafter the letter e,\nwe want our model to predict l.\nIn this case,\nour model may have very low predictions\nfor the letter l, so we\nwould incur high loss.\nAnd you kind of repeat\nthis process over and over,\nand if you train this model\nwith many different sequences,\nthen eventually it should learn\nhow to predict the next\ncharacter in a sequence\nbased on the context of\nall the previous characters\nthat it's seen before.\nAnd now, if you think about\nwhat happens at test time,\nafter we train this model,\none thing that we might want to do with it\nis a sample from the model,\nand actually use this\ntrained neural network model\nto synthesize new text\nthat kind of looks similar in spirit\nto the text that it was trained on.\nThe way that this will work\nis we'll typically see the model\nwith some input prefix of text.\nIn this case, the prefix is\njust the single letter h,\nand now we'll feed that letter h\nthrough the first time step of\nour recurrent neural network.\nIt will product this\ndistribution of scores\nover all the characters in the vocabulary.\nNow, at training time,\nwe'll use these scores\nto actually sample from it.\nSo we'll use a softmaxt function\nto convert those scores into\na probability distribution\nand then we will sample from\nthat probability distribution\nto actually synthesize the\nsecond letter in the sequence.\nAnd in this case, even though\nthe scores were pretty bad,\nmaybe we got lucky and\nsampled the letter e\nfrom this probability distribution.\nAnd now, we'll take this letter e\nthat was sampled from this distribution\nand feed it back as input into the network\nat the next time step.\nNow, we'll take this e,\npull it down from the top,\nfeed it back into the network\nas one of these, sort of, one\nhot vectorial representations,\nand then repeat the process\nin order to synthesize the\nsecond letter in the output.\nAnd we can repeat this\nprocess over and over again\nto synthesize a new sequence\nusing this trained model,\nwhere we're synthesizing the sequence\none character at a time\nusing these predicted\nprobability distributions\nat each time step.\nQuestion?\nYeah, that's a great question.\nSo the question is why might we sample\ninstead of just taking the character\nwith the largest score?\nIn this case,\nbecause of the probability\ndistribution that we had,\nit was impossible to\nget the right character,\nso we had the sample so\nthe example could work out,\nand it would make sense.\nBut in practice,\nsometimes you'll see both.\nSo sometimes you'll just\ntake the argmax probability,\nand that will sometimes be\na little bit more stable,\nbut one advantage of sampling, in general,\nis that it lets you get\ndiversity from your models.\nSometimes you might have the same input,\nmaybe the same prefix,\nor in the case of image captioning,\nmaybe the same image.\nBut then if you sample rather\nthan taking the argmax,\nthen you'll see that\nsometimes these trained models\nare actually able to produce\nmultiple different types\nof reasonable output sequences,\ndepending on the kind,\ndepending on which samples they take\nat the first time steps.\nIt's actually kind of a benefit\ncause we can get now more\ndiversity in our outputs.\nAnother question?\nCould we feed in the softmax vector\ninstead of the one element vector?\nYou mean at test time?\nYeah yeah, so the\nquestion is, at test time,\ncould we feed in this whole softmax vector\nrather than a one hot vector?\nThere's kind of two problems with that.\nOne is that that's very different\nfrom the data that it\nsaw at training time.\nIn general, if you ask your model\nto do something at test time,\nwhich is different from training time,\nthen it'll usually blow up.\nIt'll usually give you garbage\nand you'll usually be sad.\nThe other problem is that in practice,\nour vocabularies might be very large.\nSo maybe, in this simple example,\nour vocabulary is only four elements,\nso it's not a big problem.\nBut if you're thinking about\ngenerating words one at a time,\nnow your vocabulary is every\nword in the English language,\nwhich could be something like\ntens of thousands of elements.\nSo in practice, this first element,\nthis first operation that's\ntaking in this one hot vector,\nis often performed using\nsparse vector operations\nrather than dense factors.\nIt would be, sort of,\ncomputationally really bad\nif you wanted to have this load of\n10,000 elements softmax vector.\nSo that's usually why we\nuse a one hot instead,\neven at test time.\nThis idea that we have a sequence\nand we produce an output at\nevery time step of the sequence\nand then finally compute some loss,\nthis is sometimes called\nbackpropagation through time\nbecause you're imagining\nthat in the forward pass,\nyou're kind of stepping\nforward through time\nand then during the backward pass,\nyou're sort of going\nbackwards through time\nto compute all your gradients.\nThis can actually be kind of problematic\nif you want to train the sequences\nthat are very, very long.\nSo if you imagine that we\nwere kind of trying to train\na neural network language model\non maybe the entire text of Wikipedia,\nwhich is, by the way,\nsomething that people\ndo pretty frequently,\nthis would be super slow,\nand every time we made a gradient step,\nwe would have to make a forward pass\nthrough the entire text\nof all of wikipedia,\nand then make a backward pass\nthrough all of wikipedia,\nand then make a single gradient update.\nAnd that would be super slow.\nYour model would never converge.\nIt would also take a\nridiculous amount of memory\nso this would be just really bad.\nIn practice, what people\ndo is this, sort of,\napproximation called truncated\nbackpropagation through time.\nHere, the idea is that,\neven though our input\nsequence is very, very long,\nand even potentially infinite,\nwhat we'll do is that during,\nwhen we're training the model,\nwe'll step forward for\nsome number of steps,\nmaybe like a hundred is\nkind of a ballpark number\nthat people frequently use,\nand we'll step forward\nfor maybe a hundred steps,\ncompute a loss only over this\nsub sequence of the data,\nand then back propagate\nthrough this sub sequence,\nand now make a gradient step.\nAnd now, when we repeat, well,\nwe still have these hidden states\nthat we computed from the first batch,\nand now, when we compute\nthis next batch of data,\nwe will carry those hidden\nstates forward in time,\nso the forward pass will\nbe exactly the same.\nBut now when we compute a gradient step\nfor this next batch of data,\nwe will only backpropagate\nagain through this second batch.\nNow, we'll make a gradient step\nbased on this truncated\nbackpropagation through time.\nThis process will continue,\nwhere now when we make the next batch,\nwe'll again copy these\nhidden states forward,\nbut then step forward\nand then step backward,\nbut only for some small\nnumber of time steps.\nSo this is,\nyou can kind of think of this\nas being an alegist who's\nthe cast at gradient descent\nin the case of sequences.\nRemember, when we talked\nabout training our models\non large data sets,\nthen these data sets,\nit would be super expensive\nto compute the gradients\nover every element in the data set.\nSo instead, we kind of take small samples,\nsmall mini batches instead,\nand use mini batches of data\nto compute gradient stops\nin any kind of image classification case.\nQuestion?\nIs this kind of, the question is,\nis this kind of making\nthe Mark Hobb assumption?\nNo, not really.\nBecause we're carrying\nthis hidden state forward\nin time forever.\nIt's making a Marcovian assumption\nin the sense that, conditioned\non the hidden state,\nbut the hidden state is all that we need\nto predict the entire future\nof the sequence.\nBut that assumption is kind of built\ninto the recurrent neural network formula\nfrom the start.\nAnd that's not really particular\nto back propagation through time.\nBack propagation through time,\nor sorry, truncated back prop though time\nis just the way to\napproximate these gradients\nwithout going making a backwards pass\nthrough your potentially\nvery large sequence of data.\nThis all sounds very\ncomplicated and confusing\nand it sounds like a lot of code to write,\nbut in fact, this can\nacutally be pretty concise.\nAndrea has this example of\nwhat he calls min-char-rnn,\nthat does all of this stuff\nin just like a 112 lines of Python.\nIt handles building the vocabulary.\nIt trains the model\nwith truncated back\npropagation through time.\nAnd then, it can actually\nsample from that model\nin actually not too much code.\nSo even though this sounds like\nkind of a big, scary process,\nit's actually not too difficult.\nI'd encourage you, if you're confused,\nto maybe go check this out\nand step through the\ncode on your own time,\nand see, kind of, all\nof these concrete steps\nhappening in code.\nSo this is all in just a single file,\nall using numpy with no dependencies.\nThis was relatively easy to read.\nSo then, once we have this idea\nof training a recurrent\nneural network language model,\nwe can actually have a\nlot of fun with this.\nAnd we can take in, sort\nof, any text that we want.\nTake in, like, whatever\nrandom text you can think of\nfrom the internet,\ntrain our recurrent neural\nnetwork language model\non this text,\nand then generate new text.\nSo in this example, we\ntook this entire text\nof all of Shakespeare's works,\nand then used that to train\na recurrent neural network language model\non all of Shakespeare.\nAnd you can see that the\nbeginning of training,\nit's kind of producing maybe\nrandom gibberish garbage,\nbut throughout the course of training,\nit ends up producing things\nthat seem relatively reasonable.\nAnd after you've,\nafter this model has\nbeen trained pretty well,\nthen it produces text that seems,\nkind of, Shakespeare-esque to me.\n\"Why do what that day,\" replied,\nwhatever, right, you can read this.\nLike, it kind of looks\nkind of like Shakespeare.\nAnd if you actually train\nthis model even more,\nand let it converge even further,\nand then sample these\neven longer sequences,\nyou can see that it learns\nall kinds of crazy cool stuff\nthat really looks like a Shakespeare play.\nIt knows that it uses,\nmaybe, these headings\nto say who's speaking.\nThen it produces these bits of text\nthat have crazy dialogue\nthat sounds kind of Shakespeare-esque.\nIt knows to put line breaks\nin between these different things.\nAnd this is all, like, really cool,\nall just sort of learned from\nthe structure of the data.\nWe can actually get\neven crazier than this.\nThis was one of my favorite examples.\nI found online, there's this.\nIs anyone a mathematician in this room?\nHas anyone taken an algebraic\ntopology course by any chance?\nWow, a couple, that's impressive.\nSo you probably know more\nalgebraic topology than me,\nbut I found this open source\nalgebraic topology textbook online.\nIt's just a whole bunch of tech files\nthat are like this\nsuper dense mathematics.\nAnd LaTac, cause LaTac is sort of this,\nlet's you write equations and diagrams\nand everything just using plain text.\nWe can actually train our\nrecurrent neural network language model\non the raw Latac source code\nof this algebraic topology textbook.\nAnd if we do that, then after\nwe sample from the model,\nthen we get something that seems like,\nkind of like algebraic topology.\nSo it knows to like put equations.\nIt puts all kinds of crazy stuff.\nIt's like, to prove study,\nwe see that F sub U is\na covering of x prime,\nblah, blah, blah, blah, blah.\nIt knows where to put unions.\nIt knows to put squares\nat the end of proofs.\nIt makes lemmas.\nIt makes references to previous lemmas.\nRight, like we hear, like.\nIt's namely a bi-lemma question.\nWe see that R is geometrically something.\nSo it's actually pretty crazy.\nIt also sometimes tries to make diagrams.\nFor those of you that have\ntaken algebraic topology,\nyou know that these commutative diagrams\nare kind of a thing\nthat you work with a lot\nSo it kind of got the general gist\nof how to make those diagrams,\nbut they actually don't make any sense.\nAnd actually,\none of my favorite examples here\nis that it sometimes omits proofs.\nSo it'll sometimes say,\nit'll sometimes say something like\ntheorem, blah, blah, blah,\nblah, blah, proof omitted.\nThis thing kind of has gotten the gist\nof how some of these\nmath textbooks look like.\nWe can have a lot of fun with this.\nSo we also tried training\none of these models\non the entire source\ncode of the Linux kernel.\n'Cause again, this character level stuff\nthat we can train on,\nAnd then, when we sample this,\nit acutally again looks\nlike C source code.\nIt knows how to write if statements.\nIt has, like, pretty good\ncode formatting skills.\nIt knows to indent after\nthese if statements.\nIt knows to put curly braces.\nIt actually even makes\ncomments about some things\nthat are usually nonsense.\nOne problem with this model is that\nit knows how to declare variables.\nBut it doesn't always use the\nvariables that it declares.\nAnd sometimes it tries\nto use variables that\nhaven't been declared.\nThis wouldn't compile.\nI would not recommend sending this\nas a pull request to Linux.\nThis thing also figures\nout how to recite the GNU,\nthis GNU license character by character.\nIt kind of knows that you\nneed to recite the GNU license\nand after the license comes some includes,\nthen some other includes,\nthen source code.\nThis thing has actually\nlearned quite a lot\nabout the general structure of the data.\nWhere, again, during training,\nall we asked this model to do\nwas try to predict the next\ncharacter in the sequence.\nWe didn't tell it any of this structure,\nbut somehow, just through the course\nof this training process,\nit learned a lot about\nthe latent structure\nin the sequential data.\nYeah, so it knows how to write code.\nIt does a lot of cool stuff.\nI had this paper with\nAndre a couple years ago\nwhere we trained a bunch of these models\nand then we wanted to try\nto poke into the brains\nof these models\nand figure out like what are they doing\nand why are they working.\nSo we saw, in our,\nthese recurring neural networks\nhas this hidden vector which is,\nmaybe, some vector that's\nupdated over every time step.\nAnd then what we wanted\nto try to figure out is\ncould we find some elements of this vector\nthat have some Symantec\ninterpretable meaning.\nSo what we did is\nwe trained a neural\nnetwork language model,\none of these character level models\non one of these data sets,\nand then we picked one of the\nelements in that hidden vector\nand now we look at what is the\nvalue of that hidden vector\nover the course of a sequence\nto try to get some sense of maybe\nwhat these different hidden\nstates are looking for.\nWhen you do this, a lot\nof them end up looking\nkind of like random gibberish garbage.\nSo here again, what we've done,\nis we've picked one\nelement of that vector,\nand now we run the sequence forward\nthrough the trained model,\nand now the color of each character\ncorresponds to the\nmagnitude of that single\nscaler element of the hidden\nvector at every time step\nwhen it's reading the sequence.\nSo you can see that a lot\nof the vectors in these hidden states\nare kind of not very interpretable.\nIt seems like they're\nkind of doing some of this\nlow level language modeling\nto figure out what\ncharacter should come next.\nBut some of them end up quite nice.\nSo here we found this vector\nthat is looking for quotes.\nYou can see that there's\nthis one hidden element,\nthis one element in the vector,\nthat is off, off, off, off, off blue\nand then once it hits a quote,\nit turns on and remains\non for the duration\nof this quote.\nAnd now when we hit the\nsecond quotation mark,\nthen that cell turns off.\nSo somehow, even though\nthis model was only trained\nto predict the next\ncharacter in a sequence,\nit somehow learned that a useful thing,\nin order to do this,\nmight be to have some cell\nthat's trying to detect quotes.\nWe also found this other cell\nthat is, looks like it's\ncounting the number of characters\nsince a line break.\nSo you can see that at the\nbeginning of each line,\nthis element starts off at zero.\nThroughout the course of the line,\nit's gradually more red,\nso that value increases.\nAnd then after the new line character,\nit resets to zero.\nSo you can imagine that maybe this cell\nis letting the network keep track\nof when it needs to write\nto produce these new line characters.\nWe also found some that,\nwhen we trained on the linux source code,\nwe found some examples that are turning on\ninside the conditions of if statements.\nSo this maybe allows the network\nto differentiate whether\nit's outside an if statement\nor inside that condition,\nwhich might help it model\nthese sequences better.\nWe also found some that\nturn on in comments,\nor some that seem like they're counting\nthe number of indentation levels.\nThis is all just really cool stuff\nbecause it's saying that\neven though we are only\ntrying to train this model\nto predict next characters,\nit somehow ends up learning a lot\nof useful structure about the input data.\nOne kind of thing that we often use,\nso this is not really been\ncomputer vision so far,\nand we need to pull this\nback to computer vision\nsince this is a vision class.\nWe've alluded many times to this\nimage captioning model\nwhere we want to build\nmodels that can input\nan image and then output a\ncaption in natural language.\nThere were a bunch of\npapers a couple years ago\nthat all had relatively\nsimilar approaches.\nBut I'm showing the figure\nfrom the paper from our lab\nin a totally un-biased way.\nBut, the idea here is that the caption\nis this variably length\nsequence that we might,\nthe sequence might have different numbers\nof words for different captions.\nSo this is a totally natural fit\nfor a recurrent neural\nnetwork language model.\nSo then what this model looks like\nis we have some convolutional network\nwhich will input the,\nwhich will take as input the image,\nand we've seen a lot about how\nconvolution networks work at this point,\nand that convolutional\nnetwork will produce\na summary vector of the image\nwhich will then feed\ninto the first time step\nof one of these recurrent\nneural network language models\nwhich will then produce words\nof the caption one at a time.\nSo the way that this kind\nof works at test time\nafter the model is trained\nlooks almost exactly the same\nas these character level language models\nthat we saw a little bit ago.\nWe'll take our input image,\nfeed it through our convolutional network.\nBut now instead of\ntaking the softmax scores\nfrom an image net model,\nwe'll instead take this\n4,096 dimensional vector\nfrom the end of the model,\nand we'll take that vector and use it\nto summarize the whole\ncontent of the image.\nNow, remember when we talked\nabout RNN language models,\nwe said that we need to\nsee the language model\nwith that first initial input\nto tell it to start generating text.\nSo in this case, we'll give\nit some special start token,\nwhich is just saying, hey, this\nis the start of a sentence.\nPlease start generating some text\nconditioned on this image information.\nSo now previously, we saw that\nin this RNN language model,\nwe had these matrices that\nwere taking the previous,\nthe input at the current time step\nand the hidden state of\nthe previous time step\nand combining those to\nget the next hidden state.\nWell now, we also need to add\nin this image information.\nSo one way, people play\naround with exactly\ndifferent ways to incorporate\nthis image information,\nbut one simple way is just to add\na third weight matrix that is\nadding in this image\ninformation at every time step\nto compute the next hidden state.\nSo now, we'll compute this distribution\nover all scores in our vocabulary\nand here, our vocabulary is something\nlike all English words,\nso it could be pretty large.\nWe'll sample from that distribution\nand now pass that word back as\ninput at the next time step.\nAnd that will then feed that word in,\nagain get a distribution\nover all words in the vocab,\nand again sample to produce the next word.\nSo then, after that thing is all done,\nwe'll maybe generate,\nwe'll generate this complete sentence.\nWe stop generation once we\nsample the special ends token,\nwhich kind of corresponds to the period\nat the end of the sentence.\nThen once the network\nsamples this ends token,\nwe stop generation and we're done\nand we've gotten our\ncaption for this image.\nAnd now, during training,\nwe trained this thing to generate,\nlike we put an end token at the end\nof every caption during training\nso that the network kind\nof learned during training\nthat end tokens come at\nthe end of sequences.\nSo then, during test time,\nit tends to sample these end tokens\nonce it's done generating.\nSo we trained this model\nin kind of a completely supervised way.\nYou can find data sets\nthat have images together with\nnatural language captions.\nMicrosoft COCO is probably the biggest\nand most widely used for this task.\nBut you can just train this model\nin a purely supervised way.\nAnd then backpropagate\nthrough to jointly train\nboth this recurrent neural\nnetwork language model\nand then also pass gradients\nback into this final layer of this the CNN\nand additionally update the weights\nof the CNN to jointly tune\nall parts of the model\nto perform this task.\nOnce you train these models,\nthey actually do some\npretty reasonable things.\nThese are some real results from a model,\nfrom one of these trained models,\nand it says things like a cat sitting\non a suitcase on the floor,\nwhich is pretty impressive.\nIt knows about cats\nsitting on a tree branch,\nwhich is also pretty cool.\nIt knows about two people walking\non the beach with surfboards.\nSo these models are\nactually pretty powerful\nand can produce relatively\ncomplex captions\nto describe the image.\nBut that being said,\nthese models are really not perfect.\nThey're not magical.\nJust like any machine learning model,\nif you try to run them on data\nthat was very different\nfrom the training data,\nthey don't work very well.\nSo for example, this example,\nit says a woman is\nholding a cat in her hand.\nThere's clearly no cat in the image.\nBut she is wearing a fur coat,\nand maybe the texture of that coat\nkind of looked like a cat to the model.\nOver here, we see a\nwoman standing on a beach\nholding a surfboard.\nWell, she's definitely\nnot holding a surfboard\nand she's doing a handstand,\nwhich is maybe the interesting\npart of that image,\nand the model totally missed that.\nAlso, over here, we see this example\nwhere there's this picture of a spider web\nin the tree branch,\nand it totally,\nand it says something like\na bird sitting on a tree branch.\nSo it totally missed the spider,\nbut during training,\nit never really saw examples of spiders.\nIt just knows that birds sit\non tree branches during training.\nSo it kind of makes these\nreasonable mistakes.\nOr here at the bottom,\nit can't really tell the difference\nbetween this guy throwing\nand catching the ball,\nbut it does know that\nit's a baseball player\nand there's balls and things involved.\nSo again, just want to\nsay that these models\nare not perfect.\nThey work pretty well when\nyou ask them to caption images\nthat were similar to the training data,\nbut they definitely have a hard time\ngeneralizing far beyond that.\nSo another thing you'll sometimes see\nis this slightly more advanced\nmodel called Attention,\nwhere now when we're generating\nthe words of this caption,\nwe can allow the model\nto steer it's attention\nto different parts of the image.\nAnd I don't want to spend\ntoo much time on this.\nBut the general way\nthat this works is that\nnow our convolutional network,\nrather than producing a single vector\nsummarizing the entire image,\nnow it produces some grid of vectors\nthat summarize the,\nthat give maybe one vector\nfor each spatial location\nin the image.\nAnd now, when we,\nwhen this model runs forward,\nin addition to sampling the\nvocabulary at every time step,\nit also produces a distribution\nover the locations in the image\nwhere it wants to look.\nAnd now this distribution\nover image locations\ncan be seen as a kind of a tension\nof where the model should\nlook during training.\nSo now that first hidden state\ncomputes this distribution\nover image locations,\nwhich then goes back to the set of vectors\nto give a single summary vector\nthat maybe focuses the attention\non one part of that image.\nAnd now that summary vector gets fed,\nas an additional input,\nat the next time step\nof the neural network.\nAnd now again, it will\nproduce two outputs.\nOne is our distribution\nover vocabulary words.\nAnd the other is a distribution\nover image locations.\nThis whole process will continue,\nand it will sort of do\nthese two different things\nat every time step.\nAnd after you train the model,\nthen you can see that it kind of\nwill shift it's attention around the image\nfor every word that it\ngenerates in the caption.\nHere you can see that it\nproduced the caption,\na bird is flying over,\nI can't see that far.\nBut you can see that its attention\nis shifting around\ndifferent parts of the image\nfor each word in the\ncaption that it generates.\nThere's this notion of hard attention\nversus soft attention,\nwhich I don't really want\nto get into too much,\nbut with this idea of soft attention,\nwe're kind of taking\na weighted combination\nof all features from all image locations,\nwhereas in the hard attention case,\nwe're forcing the model to\nselect exactly one location\nto look at in the image at each time step.\nSo the hard attention case\nwhere we're selecting\nexactly one image location\nis a little bit tricky\nbecause that is not really\na differentiable function,\nso you need to do\nsomething slightly fancier\nthan vanilla backpropagation\nin order to just train the\nmodel in that scenario.\nAnd I think we'll talk about\nthat a little bit later\nin the lecture on reinforcement learning.\nNow, when you look at after you train\none of these attention models\nand then run it on to generate captions,\nyou can see that it tends\nto focus it's attention\non maybe the salient or\nsemanticly meaningful part\nof the image when generating captions.\nYou can see that the caption was a woman\nis throwing a frisbee in a park\nand you can see that this attention mask,\nwhen it generated the word,\nwhen the model generated the word frisbee,\nat the same time,\nit was focusing it's\nattention on this image region\nthat actually contains the frisbee.\nThis is actually really cool.\nWe did not tell the model\nwhere it should be looking\nat every time step.\nIt sort of figured all that out for itself\nduring the training process.\nBecause somehow, it\nfigured out that looking\nat that image region was\nthe right thing to do\nfor this image.\nAnd because everything in\nthis model is differentiable,\nbecause we can backpropagate\nthrough all these soft attention steps,\nall of this soft attention stuff\njust comes out through\nthe training process.\nSo that's really, really cool.\nBy the way, this idea of\nrecurrent neural networks\nand attention actually\ngets used in other tasks\nbeyond image captioning.\nOne recent example is this idea\nof visual question answering.\nSo here, our model is going\nto take two things as input.\nIt's going to take an image\nand it will also take a\nnatural language question\nthat's asking some\nquestion about the image.\nHere, we might see this image on the left\nand we might ask the question,\nwhat endangered animal\nis featured on the truck?\nAnd now the model needs to select from one\nof these four natural language answers\nabout which of these answers\ncorrectly answers that question\nin the context of the image.\nSo you can imagine kind of\nstitching this model together\nusing CNNs and RNNs in\nkind of a natural way.\nNow, we're in this\nmany to one scenario,\nwhere now our model needs to take as input\nthis natural language sequence,\nso we can imagine running\na recurrent neural network\nover each element of that input question,\nto now summarize the input\nquestion in a single vector.\nAnd then we can have a CNN\nto again summarize the image,\nand now combine both\nthe vector from the CNN\nand the vector from the\nquestion and coding RNN\nto then predict a\ndistribution over answers.\nWe also sometimes,\nyou'll also sometimes see this idea\nof soft spacial attention\nbeing incorporated\ninto things like visual\nquestion answering.\nSo you can see that here,\nthis model is also having\nthe spatial attention\nover the image when it's trying\nto determine answers to the questions.\nJust to, yeah, question?\nSo the question is\nHow are the different inputs combined?\nDo you mean like the\nencoded question vector\nand the encoded image vector?\nYeah, so the question is\nhow are the encoded image\nand the encoded question vector combined?\nKind of the simplest thing to do\nis just to concatenate them\nand stick them into\nfully connected layers.\nThat's probably the most common\nand that's probably\nthe first thing to try.\nSometimes people do\nslightly fancier things\nwhere they might try to have\nmultiplicative interactions\nbetween those two vectors\nto allow a more powerful function.\nBut generally, concatenation\nis kind of a good\nfirst thing to try.\nOkay, so now we've talked\nabout a bunch of scenarios\nwhere RNNs are used for\ndifferent kinds of problems.\nAnd I think it's super cool\nbecause it allows you to\nstart tackling really complicated problems\ncombining images and computer vision\nwith natural language processing.\nAnd you can see that we\ncan kind of stith together\nthese models like Lego blocks\nand attack really complicated things,\nLike image captioning or\nvisual question answering\njust by stitching together\nthese relatively simple\ntypes of neural network modules.\nBut I'd also like to mention that\nso far, we've talked about this idea\nof a single recurrent network layer,\nwhere we have sort of one hidden state,\nand another thing that\nyou'll see pretty commonly\nis this idea of a multilayer\nrecurrent neural network.\nHere, this is a three layer\nrecurrent neural network,\nso now our input goes in,\ngoes into, goes in and produces\na sequence of hidden states\nfrom the first recurrent\nneural network layer.\nAnd now, after we run kind of\none recurrent neural network layer,\nthen we have this whole\nsequence of hidden states.\nAnd now, we can use the\nsequence of hidden states\nas an input sequence to another\nrecurrent neural network layer.\nAnd then you can just imagine,\nwhich will then produce another\nsequence of hidden states\nfrom the second RNN layer.\nAnd then you can just imagine\nstacking these things\non top of each other,\ncause we know that we've\nseen in other contexts\nthat deeper models tend to perform better\nfor various problems.\nAnd the same kind of\nholds in RNNs as well.\nFor many problems, you'll see maybe a two\nor three layer recurrent\nneural network model\nis pretty commonly used.\nYou typically don't see\nsuper deep models in RNNs.\nSo generally, like two,\nthree, four layer RNNs\nis maybe as deep as you'll typically go.\nThen, I think it's also really\ninteresting and important\nto think about,\nnow we've seen kind of\nwhat kinds of problems\nthese RNNs can be used for,\nbut then you need to think\na little bit more carefully\nabout exactly what happens to these models\nwhen we try to train them.\nSo here, I've drawn this\nlittle vanilla RNN cell\nthat we've talked about so far.\nSo here, we're taking\nour current input, x t,\nand our previous hidden\nstate, h t minus one,\nand then we stack, those are two vectors.\nSo we can just stack them together.\nAnd then perform this\nmatrix multiplication\nwith our weight matrix,\nto give our,\nand then squash that\noutput through a tanh,\nand that will give us\nour next hidden state.\nAnd that's kind of the\nbasic functional form\nof this vanilla recurrent neural network.\nBut then, we need to think about\nwhat happens in this architecture\nduring the backward pass when\nwe try to compute gradients?\nSo then if we think\nabout trying to compute,\nso then during the backwards pass,\nwe'll receive the derivative of our h t,\nwe'll receive derivative of loss\nwith respect to h t.\nAnd during the backward\npass through the cell,\nwe'll need to compute derivative of loss\nto the respect of h t minus one.\nThen, when we compute this backward pass,\nwe see that the gradient flows backward\nthrough this red path.\nSo first, that gradient\nwill flow backwards\nthrough this tanh gate,\nand then it will flow backwards\nthrough this matrix multiplication gate.\nAnd then, as we've seen in the homework\nand when implementing these\nmatrix multiplication layers,\nwhen you backpropagate through\nthis matrix multiplication gate,\nyou end up mulitplying by the transpose\nof that weight matrix.\nSo that means that every\ntime we backpropagate\nthrough one of these vanilla RNN cells,\nwe end up multiplying by some\npart of the weight matrix.\nSo now if you imagine\nthat we are sticking many\nof these recurrent neural\nnetwork cells in sequence,\nbecause again this is an RNN.\nWe want a model sequences.\nNow if you imagine what\nhappens to the gradient flow\nthrough a sequence of these layers,\nthen something kind of\nfishy starts to happen.\nBecause now, when we want to compute\nthe gradient of the loss\nwith respect to h zero,\nwe need to backpropagate through every one\nof these RNN cells.\nAnd every time you\nbackpropagate through one cell,\nyou'll pick up one of\nthese w transpose factors.\nSo that means that the final expression\nfor the gradient on h zero\nwill involve many, many factors\nof this weight matrix,\nwhich could be kind of bad.\nMaybe don't think about the weight,\nthe matrix case,\nbut imagine a scaler case.\nIf we end up, if we have some scaler\nand we multiply by that\nsame number over and over\nand over again,\nmaybe not for four examples,\nbut for something like a hundred\nor several hundred time steps,\nthen multiplying by the same number\nover and over again is really bad.\nIn the scaler case,\nit's either going to explode\nin the case that that\nnumber is greater than one\nor it's going to vanish towards zero\nin the case that number is less than one\nin absolute value.\nAnd the only way in which\nthis will not happen\nis if that number is exactly one,\nwhich is actually very\nrare to happen in practice.\nThat leaves us to,\nthat same intuition\nextends to the matrix case,\nbut now, rather than the absolute\nvalue of a scaler number,\nyou instead need to look at the largest,\nthe largest singular value\nof this weight matrix.\nNow if that largest singular\nvalue is greater than one,\nthen during this backward pass,\nwhen we multiply by the\nweight matrix over and over,\nthat gradient on h w, on h zero, sorry,\nwill become very, very large,\nwhen that matrix is too large.\nAnd that's something we call\nthe exploding gradient problem.\nWhere now this gradient will\nexplode exponentially in depth\nwith the number of time steps\nthat we backpropagate through.\nAnd if the largest singular\nvalue is less than one,\nthen we get the opposite problem,\nwhere now our gradients will shrink\nand shrink and shrink exponentially,\nas we backpropagate and pick\nup more and more factors\nof this weight matrix.\nThat's called the\nvanishing gradient problem.\nTHere's a bit of a hack\nthat people sometimes do\nto fix the exploding gradient problem\ncalled gradient clipping,\nwhich is just this simple heuristic\nsaying that after we compute our gradient,\nif that gradient,\nif it's L2 norm is above some threshold,\nthen just clamp it down and divide,\njust clamp it down so it\nhas this maximum threshold.\nThis is kind of a nasty hack,\nbut it actually gets used\nin practice quite a lot\nwhen training recurrent neural networks.\nAnd it's a relatively useful tool\nfor attacking this\nexploding gradient problem.\nBut now for the vanishing\ngradient problem,\nwhat we typically do\nis we might need to move to\na more complicated RNN architecture.\nSo that motivates this idea of an LSTM.\nAn LSTM, which stands for\nLong Short Term Memory,\nis this slightly fancier\nrecurrence relation\nfor these recurrent neural networks.\nIt's really designed to help alleviate\nthis problem of vanishing\nand exploding gradients.\nSo that rather than kind\nof hacking on top of it,\nwe just kind of design the architecture\nto have better gradient flow properties.\nKind of an analogy to those\nfancier CNN architectures\nthat we saw at the top of the lecture.\nAnother thing to point out\nis that the LSTM cell\nactually comes from 1997.\nSo this idea of an LSTM\nhas been around for quite a while,\nand these folks were\nworking on these ideas\nway back in the 90s,\nwere definitely ahead of the curve.\nBecause these models are\nkind of used everywhere now\n20 years later.\nAnd LSTMs kind of have\nthis funny functional form.\nSo remember when we had this vanilla\nrecurrent neural network,\nit had this hidden state.\nAnd we used this recurrence relation\nto update the hidden\nstate at every time step.\nWell, now in an LSTM,\nwe actually have two,\nwe maintain two hidden\nstates at every time step.\nOne is this h t,\nwhich is called the hidden state,\nwhich is kind of an\nanalogy to the hidden state\nthat we had in the vanilla RNN.\nBut an LSTM also maintains\nthe second vector, c t,\ncalled the cell state.\nAnd the cell state is this\nvector which is kind of internal,\nkept inside the LSTM,\nand it does not really get\nexposed to the outside world.\nAnd we'll see,\nand you can kind of see that\nthrough this update equation,\nwhere you can see that when we,\nfirst when we compute these,\nwe take our two inputs,\nwe use them to compute these four gates\ncalled i, f, o, n, g.\nWe use those gates to\nupdate our cell states, c t,\nand then we expose part of our cell state\nas the hidden state at the next time step.\nThis is kind of a funny functional form,\nand I want to walk through\nfor a couple slides\nexactly why do we use this architecture\nand why does it make sense,\nespecially in the context\nof vanishing or exploding gradients.\nThis first thing that we do in an LSTM\nis that we're given this\nprevious hidden state, h t,\nand we're given our\ncurrent input vector, x t,\nand just like the vanilla RNN.\nIn the vanilla RNN, remember,\nwe took those two input vectors.\nWe concatenated them.\nThen we did a matrix multiply\nto directly compute the next\nhidden state in the RNN.\nNow, the LSTM does something\na little bit different.\nWe're going to take our\nprevious hidden state\nand our current input,\nstack them,\nand now multiply by a\nvery big weight matrix, w,\nto compute four different gates,\nWhich all have the same\nsize as the hidden state.\nSometimes, you'll see this\nwritten in different ways.\nSome authors will write\na different weight matrix\nfor each gate.\nSome authors will combine them all\ninto one big weight matrix.\nBut it's all really the same thing.\nThe ideas is that we\ntake our hidden state,\nour current input,\nand then we use those to\ncompute these four gates.\nThese four gates are the,\nyou often see this written\nas i, f, o, g, ifog,\nwhich makes it pretty easy\nto remember what they are.\nI is the input gate.\nIt says how much do we want\nto input into our cell.\nF is the forget gate.\nHow much do we want to\nforget the cell memory\nat the previous, from\nthe previous time step.\nO is the output gate,\nwhich is how much do we\nwant to reveal ourself\nto the outside world.\nAnd G really doesn't have a nice name,\nso I usually call it the gate gate.\nG, it tells us how much\ndo we want to write\ninto our input cell.\nAnd then you notice that\neach of these four gates\nare using a different non linearity.\nThe input, forget and output gate\nare all using sigmoids,\nwhich means that their values\nwill be between zero and one.\nWhereas the gate gate uses a tanh,\nwhich means it's output will\nbe between minus one and one.\nSo, these are kind of weird,\nbut it makes a little bit more sense\nif you imagine them all as binary values.\nRight, like what happens at the extremes\nof these two values?\nIt's kind of what happens,\nif you look after we compute these gates\nif you look at this next equation,\nyou can see that our cell state\nis being multiplied element\nwise by the forget gate.\nSorry, our cell state from\nthe previous time step\nis being multiplied element\nwise by this forget gate.\nAnd now if this forget gate,\nyou can think of it as being\na vector of zeros and ones,\nthat's telling us for each\nelement in the cell state,\ndo we want to forget\nthat element of the cell\nin the case if the forget gate was zero?\nOr do we want to remember\nthat element of the cell\nin the case if the forget gate was one.\nNow, once we've used the forget gate\nto gate off the part of the cell state,\nthen we have the second term,\nwhich is the element\nwise product of i and g.\nSo now, i is this vector\nof zeros and ones,\ncause it's coming through a sigmoid,\ntelling us for each\nelement of the cell state,\ndo we want to write to that\nelement of the cell state\nin the case that i is one,\nor do we not want to write to\nthat element of the cell state\nat this time step\nin the case that i is zero.\nAnd now the gate gate,\nbecause it's coming through a tanh,\nwill be either one or minus one.\nSo that is the value that we want,\nthe candidate value that\nwe might consider writing\nto each element of the cell\nstate at this time step.\nThen if you look at the\ncell state equation,\nyou can see that at every time step,\nthe cell state has these kind of\nthese different,\nindependent scaler values,\nand they're all being incremented\nor decremented by one.\nSo there's kind of like,\ninside the cell state,\nwe can either remember\nor forget our previous state,\nand then we can either\nincrement or decrement\neach element of that cell state\nby up to one at each time step.\nSo you can kind of think of\nthese elements of the cell state\nas being little scaler integer counters\nthat can be incremented and decremented\nat each time step.\nAnd now, after we've\ncomputed our cell state,\nthen we use our now updated cell state\nto compute a hidden state,\nwhich we will reveal to the outside world.\nSo because this cell state\nhas this interpretation\nof being counters,\nand sort of counting up by one\nor minus one at each time step,\nwe want to squash that counter value\ninto a nice zero to\none range using a tanh.\nAnd now, we multiply element wise,\nby this output gate.\nAnd the output gate is again\ncoming through a sigmoid,\nso you can think of it as\nbeing mostly zeros and ones,\nand the output gate tells us\nfor each element of our cell state,\ndo we want to reveal or not reveal\nthat element of our cell state\nwhen we're computing the\nexternal hidden state\nfor this time step.\nAnd then, I think there's\nkind of a tradition\nin people trying to explain LSTMs,\nthat everyone needs to come up\nwith their own potentially\nconfusing LSTM diagram.\nSo here's my attempt.\nHere, we can see what's going\non inside this LSTM cell,\nis that we take our,\nwe're taking as input on the\nleft our previous cell state\nand the previous hidden state,\nas well as our current input, x t.\nNow we're going to take our current,\nour previous hidden state,\nas well as our current input,\nstack them,\nand then multiply with\nthis weight matrix, w,\nto produce our four gates.\nAnd here, I've left\nout the non linearities\nbecause we saw those on a previous slide.\nAnd now the forget gate\nmultiplies element wise\nwith the cell state.\nThe input and gate gate\nare multiplied element wise\nand added to the cell state.\nAnd that gives us our next cell.\nThe next cell gets\nsquashed through a tanh,\nand multiplied element\nwise with this output gate\nto produce our next hidden state.\nQuestion?\nNo, So they're coming through this,\nthey're coming from different\nparts of this weight matrix.\nSo if our hidden,\nif our x and our h all\nhave this dimension h,\nthen after we stack them,\nthey'll be a vector size two h,\nand now our weight matrix\nwill be this matrix\nof size four h times two h.\nSo you can think of that as sort of having\nfour chunks of this weight matrix.\nAnd each of these four\nchunks of the weight matrix\nis going to compute a\ndifferent one of these gates.\nYou'll often see this written for clarity,\nkind of combining all\nfour of those different\nweight matrices into a\nsingle large matrix, w,\njust for notational convenience.\nBut they're all computed\nusing different parts\nof the weight matrix.\nBut you're correct in\nthat they're all computed\nusing the same functional form\nof just stacking the two things\nand taking the matrix multiplication.\nNow that we have this picture,\nwe can think about what\nhappens to an LSTM cell\nduring the backwards pass?\nWe saw, in the context of vanilla\nrecurrent neural network,\nthat some bad things happened\nduring the backwards pass,\nwhere we were continually multiplying\nby that weight matrix, w.\nBut now, the situation looks much,\nquite a bit different in the LSTM.\nIf you imagine this path backwards\nof computing the gradients\nof the cell state,\nwe get quite a nice picture.\nNow, when we have our upstream gradient\nfrom the cell coming in,\nthen once we backpropagate backwards\nthrough this addition operation,\nremember that this addition just copies\nthat upstream gradient\ninto the two branches,\nso our upstream gradient\ngets copied directly\nand passed directly to backpropagating\nthrough this element wise multiply.\nSo then our upstream\ngradient ends up getting\nmultiplied element wise\nby the forget gate.\nAs we backpropagate backwards\nthrough this cell state,\nthe only thing that happens\nto our upstream cell state gradient\nis that it ends up getting\nmultiplied element wise\nby the forget gate.\nThis is really a lot nicer\nthan the vanilla RNN for two reasons.\nOne is that this forget gate\nis now an element wise multiplication\nrather than a full matrix multiplication.\nSo element wise multiplication\nis going to be a little bit nicer\nthan full matrix multiplication.\nSecond is that element wise multiplication\nwill potentially be\nmultiplying by a different\nforget gate at every time step.\nSo remember, in the vanilla RNN,\nwe were continually multiplying\nby that same weight matrix\nover and over again,\nwhich led very explicitly\nto these exploding or vanishing gradients.\nBut now in the LSTM case,\nthis forget gate can\nvary from each time step.\nNow, it's much easier for the model\nto avoid these problems\nof exploding and vanishing gradients.\nFinally, because this forget gate\nis coming out from a sigmoid,\nthis element wise multiply\nis guaranteed to be between zero and one,\nwhich again, leads to sort\nof nicer numerical properties\nif you imagine multiplying by these things\nover and over again.\nAnother thing to notice\nis that in the context\nof the vanilla recurrent neural network,\nwe saw that during the backward pass,\nour gradients were flowing\nthrough also a tanh\nat every time step.\nBut now, in an LSTM,\nour outputs are,\nin an LSTM, our hidden state is used\nto compute those outputs, y t,\nso now, each hidden state,\nif you imagine backpropagating\nfrom the final hidden state\nback to the first cell state,\nthen through that backward path,\nwe only backpropagate through\na single tanh non linearity\nrather than through a separate\ntanh at every time step.\nSo kind of when you put\nall these things together,\nyou can see this backwards pass\nbackpropagating through the cell state\nis kind of a gradient super highway\nthat lets gradients pass\nrelatively unimpeded\nfrom the loss at the very end of the model\nall the way back to the initial cell state\nat the beginning of the model.\nWas there a question?\nYeah, what about the\ngradient in respect to w?\n'Cause that's ultimately the\nthing that we care about.\nSo, the gradient with respect to w\nwill come through,\nat every time step,\nwill take our current cell state\nas well as our current hidden state\nand that will give us an element,\nthat will give us our local gradient on w\nfor that time step.\nSo because our cell state,\nand just in the vanilla RNN case,\nwe'll end up adding those\nfirst time step w gradients\nto compute our final gradient on w.\nBut now, if you imagine the situation\nwhere we have a very long sequence,\nand we're only getting\ngradients to the very end\nof the sequence.\nNow, as you backpropagate through,\nwe'll get a local gradient on w\nfor each time step,\nand that local gradient on w\nwill be coming through\nthese gradients on c and h.\nSo because we're maintaining\nthe gradients on c\nmuch more nicely in the LSTM case,\nthose local gradients\non w at each time step\nwill also be carried forward and backward\nthrough time much more cleanly.\nAnother question?\nYeah, so the question is\ndue to the non linearities,\ncould this still be susceptible\nto vanishing gradients?\nAnd that could be the case.\nActually, so one problem you might imagine\nis that maybe if these forget gates\nare always less than zero,\nor always less than one,\nyou might get vanishing gradients\nas you continually go\nthrough these forget gates.\nWell, one sort of trick\nthat people do in practice\nis that they will, sometimes,\ninitialize the biases of the forget gate\nto be somewhat positive.\nSo that at the beginning of training,\nthose forget gates are\nalways very close to one.\nSo that at least at the\nbeginning of training,\nthen we have not so,\nrelatively clean gradient flow\nthrough these forget gates,\nsince they're all\ninitialized to be near one.\nAnd then throughout\nthe course of training,\nthen the model can learn those biases\nand kind of learn to\nforget where it needs to.\nYou're right that there\nstill could be some potential\nfor vanishing gradients here.\nBut it's much less extreme\nthan the vanilla RNN case,\nboth because those fs can\nvary at each time step,\nand also because we're doing\nthis element wise multiplication\nrather than a full matrix multiplication.\nSo you can see that this LSTM\nactually looks quite similar to ResNet.\nIn this residual network,\nwe had this path of identity connections\ngoing backward through the network\nand that gave, sort of\na gradient super highway\nfor gradients to flow backward in ResNet.\nAnd now it's kind of the\nsame intuition in LSTM\nwhere these additive and element wise\nmultiplicative interactions\nof the cell state\ncan give a similar gradient super highway\nfor gradients to flow backwards\nthrough the cell state\nin an LSTM.\nAnd by the way, there's this\nother kind of nice paper\ncalled highway networks,\nwhich is kind of in between this idea\nof this LSTM cell\nand these residual networks.\nSo these highway networks\nactually came before residual networks,\nand they had this idea where\nat every layer of the highway network,\nwe're going to compute\nsort of a candidate activation,\nas well as a gating function\nthat tells us that interprelates\nbetween our previous input at that layer,\nand that candidate activation\nthat came through our\nconvolutions or what not.\nSo there's actually a lot of\narchitectural similarities\nbetween these things,\nand people take a lot of inspiration\nfrom training very deep CNNs\nand very deep RNNs\nand there's a lot of crossover here.\nVery briefly, you'll see a\nlot of other types of variance\nof recurrent neural network\narchitectures out there\nin the wild.\nProbably the most common,\napart from the LSTM,\nis this GRU, called the\ngated recurrent unit.\nAnd you can see those\nupdate equations here,\nand it kind of has this\nsimilar flavor of the LSTM,\nwhere it uses these\nmultiplicative element wise gates\ntogether with these additive interactions\nto avoid this vanishing gradient problem.\nThere's also this cool paper\ncalled LSTM: a search based oddysey,\nvery inventive title,\nwhere they tried to play\naround with the LSTM equations\nand swap out the non\nlinearities at one point,\nlike do we really need that tanh\nfor exposing the output gate,\nand they tried to answer a lot\nof these different questions\nabout each of those non linearities,\neach of those pieces of\nthe LSTM update equations.\nWhat happens if we change the model\nand tweak those LSTM\nequations a little bit.\nAnd kind of the conclusion is\nthat they all work about the same\nSome of them work a little\nbit better than others\nfor one problem or another.\nBut generally, none of the things,\nnone of the tweaks of LSTM that they tried\nwere significantly better\nthat the original LSTM\nfor all problems.\nSo that gives you a little bit more faith\nthat the LSTM update\nequations seem kind of magical\nbut they're useful anyway.\nYou should probably consider\nthem for your problem.\nThere's also this cool paper\nfrom Google a couple years ago\nwhere they tried to use,\nwhere they did kind of\nan evolutionary search\nand did a search over many,\nover a very large number of\nrandom RNN architectures,\nthey kind of randomly premute\nthese update equations\nand try putting the additions\nand the multiplications\nand the gates and the non linearities\nin different kinds of combinations.\nThey blasted this out over\ntheir huge Google cluster\nand just tried a whole bunch\nof these different weigh\nupdates in various flavors.\nAnd again, it was the same story\nthat they didn't really find anything\nthat was significantly better\nthan these existing GRU or LSTM styles.\nAlthough there were some\nvariations that worked\nmaybe slightly better or\nworse for certain problems.\nBut kind of the take away is that\nprobably and using an LSTM or GRU\nis not so much magic in those equations,\nbut this idea of managing\ngradient flow properly\nthrough these additive connections\nand these multiplicative gates\nis super useful.\nSo yeah, the summary is\nthat RNNs are super cool.\nThey can allow you to attack\ntons of new types of problems.\nThey sometimes are\nsusceptible to vanishing\nor exploding gradients.\nBut we can address that\nwith weight clipping\nand with fancier architectures.\nAnd there's a lot of cool overlap\nbetween CNN architectures\nand RNN architectures.\nSo next time, you'll\nbe taking the midterm.\nBut after that, we'll\nhave a, sorry, a question?\nMidterm is after this lecture\nso anything up to this point is fair game.\nAnd so you guys, good luck\non the midterm on Tuesday.\n",
  "words": [
    "okay",
    "everyone",
    "hear",
    "okay",
    "sorry",
    "delay",
    "bit",
    "technical",
    "difficulty",
    "today",
    "first",
    "time",
    "trying",
    "use",
    "new",
    "touch",
    "bar",
    "mac",
    "book",
    "pro",
    "presenting",
    "none",
    "adapters",
    "working",
    "switch",
    "laptops",
    "last",
    "minute",
    "thanks",
    "sorry",
    "today",
    "lecture",
    "talking",
    "recurrent",
    "neural",
    "networks",
    "usual",
    "couple",
    "administrative",
    "notes",
    "working",
    "hard",
    "assignment",
    "one",
    "grading",
    "grades",
    "probably",
    "sometime",
    "later",
    "today",
    "hopefully",
    "get",
    "a2",
    "deadline",
    "hoping",
    "related",
    "note",
    "assignment",
    "two",
    "due",
    "today",
    "done",
    "already",
    "half",
    "guys",
    "remember",
    "warn",
    "assignment",
    "went",
    "quite",
    "long",
    "start",
    "early",
    "warned",
    "hopefully",
    "guys",
    "late",
    "days",
    "left",
    "also",
    "another",
    "reminder",
    "midterm",
    "class",
    "tuesday",
    "kind",
    "look",
    "around",
    "lecture",
    "hall",
    "enough",
    "seats",
    "room",
    "seat",
    "enrolled",
    "students",
    "class",
    "actually",
    "midterm",
    "several",
    "lecture",
    "halls",
    "across",
    "campus",
    "sending",
    "details",
    "exactly",
    "go",
    "next",
    "couple",
    "days",
    "bit",
    "another",
    "bit",
    "announcement",
    "working",
    "sort",
    "fun",
    "bit",
    "extra",
    "credit",
    "thing",
    "play",
    "calling",
    "training",
    "game",
    "cool",
    "browser",
    "based",
    "experience",
    "go",
    "interactively",
    "train",
    "neural",
    "networks",
    "tweak",
    "hyper",
    "parameters",
    "training",
    "really",
    "cool",
    "interactive",
    "way",
    "practice",
    "hyper",
    "parameter",
    "tuning",
    "skills",
    "talking",
    "last",
    "couple",
    "lectures",
    "required",
    "think",
    "really",
    "useful",
    "experience",
    "gain",
    "little",
    "bit",
    "intuition",
    "hyper",
    "parameters",
    "work",
    "different",
    "types",
    "data",
    "sets",
    "practice",
    "still",
    "working",
    "getting",
    "bugs",
    "worked",
    "setup",
    "probably",
    "send",
    "instructions",
    "exactly",
    "work",
    "next",
    "couple",
    "days",
    "required",
    "please",
    "check",
    "think",
    "really",
    "fun",
    "really",
    "cool",
    "thing",
    "play",
    "give",
    "bit",
    "extra",
    "credit",
    "end",
    "working",
    "couple",
    "runs",
    "send",
    "details",
    "soon",
    "get",
    "bugs",
    "worked",
    "reminder",
    "last",
    "time",
    "talking",
    "cnn",
    "architectures",
    "kind",
    "walked",
    "time",
    "line",
    "various",
    "winners",
    "image",
    "net",
    "classification",
    "challenge",
    "kind",
    "breakthrough",
    "result",
    "saw",
    "alexnet",
    "architecture",
    "2012",
    "nine",
    "layer",
    "convolutional",
    "network",
    "amazingly",
    "well",
    "sort",
    "kick",
    "started",
    "whole",
    "deep",
    "learning",
    "revolution",
    "computer",
    "vision",
    "kind",
    "brought",
    "lot",
    "models",
    "mainstream",
    "skipped",
    "ahead",
    "couple",
    "years",
    "saw",
    "2014",
    "image",
    "net",
    "challenge",
    "two",
    "really",
    "interesting",
    "models",
    "vgg",
    "googlenet",
    "much",
    "deeper",
    "vgg",
    "16",
    "19",
    "layer",
    "model",
    "googlenet",
    "believe",
    "22",
    "layer",
    "model",
    "although",
    "one",
    "thing",
    "kind",
    "interesting",
    "models",
    "2014",
    "image",
    "net",
    "challenge",
    "right",
    "batch",
    "normalization",
    "invented",
    "time",
    "invention",
    "batch",
    "normalization",
    "training",
    "relatively",
    "deep",
    "models",
    "roughly",
    "twenty",
    "layers",
    "challenging",
    "fact",
    "two",
    "models",
    "resort",
    "little",
    "bit",
    "hackery",
    "order",
    "get",
    "deep",
    "models",
    "converge",
    "vgg",
    "16",
    "19",
    "layer",
    "models",
    "actually",
    "first",
    "trained",
    "11",
    "layer",
    "model",
    "could",
    "get",
    "converge",
    "added",
    "extra",
    "random",
    "layers",
    "middle",
    "continued",
    "training",
    "actually",
    "training",
    "16",
    "19",
    "layer",
    "models",
    "managing",
    "training",
    "process",
    "challenging",
    "2014",
    "invention",
    "batch",
    "normalization",
    "similarly",
    "googlenet",
    "saw",
    "googlenet",
    "auxiliary",
    "classifiers",
    "stuck",
    "lower",
    "layers",
    "network",
    "really",
    "needed",
    "class",
    "get",
    "good",
    "classification",
    "performance",
    "sort",
    "way",
    "cause",
    "extra",
    "gradient",
    "injected",
    "directly",
    "lower",
    "layers",
    "network",
    "sort",
    "invention",
    "batch",
    "normalization",
    "networks",
    "batch",
    "normalization",
    "longer",
    "need",
    "slightly",
    "ugly",
    "hacks",
    "order",
    "get",
    "deeper",
    "models",
    "converge",
    "also",
    "saw",
    "2015",
    "image",
    "net",
    "challenge",
    "really",
    "cool",
    "model",
    "called",
    "resnet",
    "residual",
    "networks",
    "shortcut",
    "connections",
    "actually",
    "little",
    "residual",
    "blocks",
    "going",
    "take",
    "input",
    "pass",
    "residual",
    "blocks",
    "add",
    "output",
    "add",
    "input",
    "block",
    "output",
    "convolutional",
    "layers",
    "kind",
    "funny",
    "architecture",
    "actually",
    "two",
    "really",
    "nice",
    "properties",
    "one",
    "set",
    "weights",
    "residual",
    "block",
    "zero",
    "block",
    "competing",
    "identity",
    "way",
    "relatively",
    "easy",
    "model",
    "learn",
    "use",
    "layers",
    "need",
    "addition",
    "kind",
    "adds",
    "interpretation",
    "l2",
    "regularization",
    "context",
    "neural",
    "networks",
    "cause",
    "put",
    "l2",
    "regularization",
    "remember",
    "weights",
    "network",
    "going",
    "drive",
    "parameters",
    "towards",
    "zero",
    "maybe",
    "standard",
    "convolutional",
    "architecture",
    "driving",
    "towards",
    "zero",
    "maybe",
    "make",
    "sense",
    "context",
    "residual",
    "network",
    "drive",
    "parameters",
    "towards",
    "zero",
    "kind",
    "encouraging",
    "model",
    "use",
    "layers",
    "need",
    "drive",
    "residual",
    "blocks",
    "towards",
    "identity",
    "whether",
    "needed",
    "classification",
    "really",
    "useful",
    "property",
    "residual",
    "networks",
    "gradient",
    "flow",
    "backward",
    "paths",
    "remember",
    "happens",
    "addition",
    "gates",
    "backward",
    "pass",
    "upstream",
    "gradient",
    "coming",
    "addition",
    "gate",
    "split",
    "fork",
    "along",
    "two",
    "different",
    "paths",
    "upstream",
    "gradient",
    "comes",
    "take",
    "one",
    "path",
    "convolutional",
    "blocks",
    "also",
    "direct",
    "connection",
    "gradient",
    "residual",
    "connection",
    "look",
    "imagine",
    "stacking",
    "many",
    "residual",
    "blocks",
    "top",
    "network",
    "ends",
    "hundreds",
    "potentially",
    "hundreds",
    "layers",
    "residual",
    "connections",
    "give",
    "sort",
    "gradient",
    "super",
    "highway",
    "gradients",
    "flow",
    "backward",
    "entire",
    "network",
    "allows",
    "train",
    "much",
    "easier",
    "much",
    "faster",
    "actually",
    "allows",
    "things",
    "converge",
    "reasonably",
    "well",
    "even",
    "model",
    "potentially",
    "hundreds",
    "layers",
    "deep",
    "idea",
    "managing",
    "gradient",
    "flow",
    "models",
    "actually",
    "super",
    "important",
    "everywhere",
    "machine",
    "learning",
    "super",
    "prevalent",
    "recurrent",
    "networks",
    "well",
    "definitely",
    "revisit",
    "idea",
    "gradient",
    "flow",
    "later",
    "today",
    "lecture",
    "kind",
    "also",
    "saw",
    "couple",
    "exotic",
    "recent",
    "cnn",
    "architectures",
    "last",
    "time",
    "including",
    "densenet",
    "fractalnet",
    "think",
    "architectures",
    "terms",
    "gradient",
    "flow",
    "make",
    "little",
    "bit",
    "sense",
    "things",
    "like",
    "densenet",
    "fractalnet",
    "adding",
    "additional",
    "shortcut",
    "identity",
    "connections",
    "inside",
    "model",
    "think",
    "happens",
    "backwards",
    "pass",
    "models",
    "additional",
    "funny",
    "topologies",
    "basically",
    "providing",
    "direct",
    "paths",
    "gradients",
    "flow",
    "loss",
    "end",
    "network",
    "easily",
    "different",
    "layers",
    "network",
    "think",
    "idea",
    "managing",
    "gradient",
    "flow",
    "properly",
    "cnn",
    "architectures",
    "something",
    "really",
    "seen",
    "lot",
    "last",
    "couple",
    "years",
    "probably",
    "see",
    "moving",
    "forward",
    "exotic",
    "architectures",
    "invented",
    "also",
    "saw",
    "kind",
    "nice",
    "plot",
    "plotting",
    "performance",
    "number",
    "flops",
    "versus",
    "number",
    "parameters",
    "versus",
    "run",
    "time",
    "various",
    "models",
    "interesting",
    "characteristics",
    "dive",
    "see",
    "plot",
    "one",
    "idea",
    "vgg",
    "alexnet",
    "huge",
    "number",
    "parameters",
    "parameters",
    "actually",
    "come",
    "almost",
    "entirely",
    "fully",
    "connected",
    "layers",
    "models",
    "alexnet",
    "something",
    "like",
    "roughly",
    "62",
    "million",
    "parameters",
    "look",
    "last",
    "fully",
    "connected",
    "layer",
    "final",
    "fully",
    "connected",
    "layer",
    "alexnet",
    "going",
    "activation",
    "volume",
    "six",
    "six",
    "256",
    "fully",
    "connected",
    "vector",
    "imagine",
    "weight",
    "matrix",
    "needs",
    "look",
    "like",
    "layer",
    "weight",
    "matrix",
    "gigantic",
    "number",
    "entries",
    "six",
    "six",
    "six",
    "times",
    "six",
    "times",
    "256",
    "times",
    "multiply",
    "see",
    "single",
    "layer",
    "38",
    "million",
    "parameters",
    "half",
    "parameters",
    "entire",
    "alexnet",
    "model",
    "sitting",
    "last",
    "fully",
    "connected",
    "layer",
    "add",
    "parameters",
    "fully",
    "connected",
    "layers",
    "alexnet",
    "including",
    "fully",
    "connected",
    "layers",
    "see",
    "something",
    "like",
    "59",
    "62",
    "million",
    "parameters",
    "alexnet",
    "sitting",
    "fully",
    "connected",
    "layers",
    "move",
    "architectures",
    "like",
    "googlenet",
    "resnet",
    "away",
    "lot",
    "large",
    "fully",
    "connected",
    "layers",
    "favor",
    "global",
    "average",
    "pooling",
    "end",
    "network",
    "allows",
    "networks",
    "really",
    "cut",
    "nicer",
    "architectures",
    "really",
    "cut",
    "parameter",
    "count",
    "architectures",
    "kind",
    "brief",
    "recap",
    "cnn",
    "architectures",
    "saw",
    "last",
    "lecture",
    "today",
    "going",
    "move",
    "one",
    "favorite",
    "topics",
    "talk",
    "recurrent",
    "neural",
    "networks",
    "far",
    "class",
    "seen",
    "like",
    "think",
    "kind",
    "vanilla",
    "feed",
    "forward",
    "network",
    "network",
    "architectures",
    "flavor",
    "receive",
    "input",
    "input",
    "fixed",
    "size",
    "object",
    "like",
    "image",
    "vector",
    "input",
    "fed",
    "set",
    "hidden",
    "layers",
    "produces",
    "single",
    "output",
    "like",
    "classifications",
    "like",
    "set",
    "classifications",
    "scores",
    "set",
    "categories",
    "context",
    "machine",
    "learning",
    "want",
    "flexibility",
    "types",
    "data",
    "models",
    "process",
    "move",
    "idea",
    "recurrent",
    "neural",
    "networks",
    "lot",
    "opportunities",
    "play",
    "around",
    "types",
    "input",
    "output",
    "data",
    "networks",
    "handle",
    "recurrent",
    "neural",
    "networks",
    "call",
    "one",
    "many",
    "models",
    "maybe",
    "input",
    "object",
    "fixed",
    "size",
    "like",
    "image",
    "output",
    "sequence",
    "variable",
    "length",
    "caption",
    "different",
    "captions",
    "might",
    "different",
    "numbers",
    "words",
    "output",
    "needs",
    "variable",
    "length",
    "also",
    "might",
    "many",
    "one",
    "models",
    "input",
    "could",
    "variably",
    "sized",
    "might",
    "something",
    "like",
    "piece",
    "text",
    "want",
    "say",
    "sentiment",
    "text",
    "whether",
    "positive",
    "negative",
    "sentiment",
    "computer",
    "vision",
    "context",
    "might",
    "imagine",
    "taking",
    "input",
    "video",
    "video",
    "might",
    "variable",
    "number",
    "frames",
    "want",
    "read",
    "entire",
    "video",
    "potentially",
    "variable",
    "length",
    "end",
    "make",
    "classification",
    "decision",
    "maybe",
    "kind",
    "activity",
    "action",
    "going",
    "video",
    "also",
    "might",
    "also",
    "problems",
    "want",
    "inputs",
    "output",
    "variable",
    "length",
    "might",
    "see",
    "something",
    "like",
    "machine",
    "translation",
    "input",
    "maybe",
    "sentence",
    "english",
    "could",
    "variable",
    "length",
    "output",
    "maybe",
    "sentence",
    "french",
    "also",
    "could",
    "variable",
    "length",
    "crucially",
    "length",
    "english",
    "sentence",
    "might",
    "different",
    "length",
    "french",
    "sentence",
    "need",
    "models",
    "capacity",
    "accept",
    "variable",
    "length",
    "sequences",
    "input",
    "output",
    "finally",
    "might",
    "also",
    "consider",
    "problems",
    "input",
    "variably",
    "length",
    "like",
    "something",
    "like",
    "video",
    "sequence",
    "variable",
    "number",
    "frames",
    "want",
    "make",
    "decision",
    "element",
    "input",
    "sequence",
    "context",
    "videos",
    "might",
    "making",
    "classification",
    "decision",
    "along",
    "every",
    "frame",
    "video",
    "recurrent",
    "neural",
    "networks",
    "kind",
    "general",
    "paradigm",
    "handling",
    "variable",
    "sized",
    "sequence",
    "data",
    "allow",
    "us",
    "pretty",
    "naturally",
    "capture",
    "different",
    "types",
    "setups",
    "models",
    "recurring",
    "neural",
    "networks",
    "actually",
    "important",
    "even",
    "problems",
    "fixed",
    "size",
    "input",
    "fixed",
    "size",
    "output",
    "recurrent",
    "neural",
    "networks",
    "still",
    "pretty",
    "useful",
    "example",
    "might",
    "want",
    "example",
    "sequential",
    "processing",
    "input",
    "receiving",
    "fixed",
    "size",
    "input",
    "like",
    "image",
    "want",
    "make",
    "classification",
    "decision",
    "like",
    "number",
    "shown",
    "image",
    "rather",
    "single",
    "feed",
    "forward",
    "pass",
    "making",
    "decision",
    "network",
    "actually",
    "looking",
    "around",
    "image",
    "taking",
    "various",
    "glimpses",
    "different",
    "parts",
    "image",
    "making",
    "series",
    "glimpses",
    "makes",
    "final",
    "decision",
    "kind",
    "number",
    "present",
    "one",
    "even",
    "though",
    "input",
    "outputs",
    "input",
    "image",
    "output",
    "classification",
    "decision",
    "even",
    "context",
    "idea",
    "able",
    "handle",
    "variably",
    "length",
    "processing",
    "recurrent",
    "neural",
    "networks",
    "lead",
    "really",
    "interesting",
    "types",
    "models",
    "really",
    "cool",
    "paper",
    "like",
    "applied",
    "type",
    "idea",
    "generating",
    "new",
    "images",
    "want",
    "model",
    "synthesize",
    "brand",
    "new",
    "images",
    "look",
    "kind",
    "like",
    "images",
    "saw",
    "training",
    "use",
    "recurrent",
    "neural",
    "network",
    "architecture",
    "actually",
    "paint",
    "output",
    "images",
    "sort",
    "one",
    "piece",
    "time",
    "output",
    "see",
    "even",
    "though",
    "output",
    "fixed",
    "size",
    "image",
    "models",
    "working",
    "time",
    "compute",
    "parts",
    "output",
    "one",
    "time",
    "sequentially",
    "use",
    "recurrent",
    "neural",
    "networds",
    "type",
    "setup",
    "well",
    "sort",
    "cool",
    "pitch",
    "cool",
    "things",
    "rnns",
    "might",
    "wonder",
    "like",
    "exactly",
    "things",
    "general",
    "recurrent",
    "neural",
    "network",
    "little",
    "little",
    "recurrent",
    "core",
    "cell",
    "take",
    "input",
    "x",
    "feed",
    "input",
    "rnn",
    "rnn",
    "internal",
    "hidden",
    "state",
    "internal",
    "hidden",
    "state",
    "updated",
    "every",
    "time",
    "rnn",
    "reads",
    "new",
    "input",
    "internal",
    "hidden",
    "state",
    "fed",
    "back",
    "model",
    "next",
    "time",
    "reads",
    "input",
    "frequently",
    "want",
    "rnn",
    "also",
    "produce",
    "output",
    "every",
    "time",
    "step",
    "pattern",
    "read",
    "input",
    "update",
    "hidden",
    "state",
    "produce",
    "output",
    "question",
    "functional",
    "form",
    "recurrence",
    "relation",
    "computing",
    "inside",
    "little",
    "green",
    "rnn",
    "block",
    "computing",
    "recurrence",
    "relation",
    "function",
    "function",
    "f",
    "depend",
    "weights",
    "accept",
    "previous",
    "hidden",
    "state",
    "h",
    "1",
    "well",
    "input",
    "current",
    "state",
    "x",
    "output",
    "next",
    "hidden",
    "state",
    "updated",
    "hidden",
    "state",
    "call",
    "h",
    "read",
    "next",
    "input",
    "hidden",
    "state",
    "new",
    "hidden",
    "state",
    "h",
    "passed",
    "function",
    "read",
    "next",
    "input",
    "x",
    "plus",
    "one",
    "wanted",
    "produce",
    "output",
    "every",
    "time",
    "step",
    "network",
    "might",
    "attach",
    "additional",
    "fully",
    "connected",
    "layers",
    "read",
    "h",
    "every",
    "time",
    "step",
    "make",
    "decision",
    "based",
    "hidden",
    "state",
    "every",
    "time",
    "step",
    "one",
    "thing",
    "note",
    "use",
    "function",
    "f",
    "w",
    "weights",
    "w",
    "every",
    "time",
    "step",
    "computation",
    "kind",
    "simplest",
    "function",
    "form",
    "imagine",
    "call",
    "vanilla",
    "recurrent",
    "neural",
    "network",
    "functional",
    "form",
    "previous",
    "slide",
    "taking",
    "previous",
    "hidden",
    "state",
    "current",
    "input",
    "need",
    "produce",
    "next",
    "hidden",
    "state",
    "kind",
    "simplest",
    "thing",
    "might",
    "imagine",
    "weight",
    "matrix",
    "w",
    "x",
    "h",
    "multiply",
    "input",
    "x",
    "well",
    "another",
    "weight",
    "matrix",
    "w",
    "h",
    "h",
    "multiply",
    "previous",
    "hidden",
    "state",
    "make",
    "two",
    "multiplications",
    "two",
    "states",
    "add",
    "together",
    "squash",
    "tanh",
    "get",
    "kind",
    "non",
    "linearity",
    "system",
    "might",
    "wondering",
    "use",
    "tanh",
    "type",
    "said",
    "negative",
    "tanh",
    "previous",
    "lectures",
    "think",
    "return",
    "little",
    "bit",
    "later",
    "talk",
    "advanced",
    "architectures",
    "like",
    "lstm",
    "addition",
    "architecture",
    "wanted",
    "produce",
    "every",
    "time",
    "step",
    "might",
    "another",
    "weight",
    "matrix",
    "w",
    "might",
    "another",
    "weight",
    "matrix",
    "accepts",
    "hidden",
    "state",
    "transforms",
    "produce",
    "maybe",
    "class",
    "score",
    "predictions",
    "every",
    "time",
    "step",
    "think",
    "recurrent",
    "neural",
    "networks",
    "kind",
    "think",
    "also",
    "kind",
    "think",
    "recurrent",
    "neural",
    "networks",
    "two",
    "ways",
    "one",
    "concept",
    "hidden",
    "state",
    "feeds",
    "back",
    "recurrently",
    "find",
    "picture",
    "little",
    "bit",
    "confusing",
    "sometimes",
    "find",
    "clearer",
    "think",
    "unrolling",
    "computational",
    "graph",
    "multiple",
    "time",
    "steps",
    "makes",
    "data",
    "flow",
    "hidden",
    "states",
    "inputs",
    "outputs",
    "weights",
    "maybe",
    "little",
    "bit",
    "clear",
    "first",
    "time",
    "step",
    "initial",
    "hidden",
    "state",
    "h",
    "zero",
    "usually",
    "initialized",
    "zeros",
    "context",
    "contexts",
    "input",
    "x",
    "initial",
    "hidden",
    "state",
    "h",
    "zero",
    "current",
    "input",
    "x",
    "go",
    "f",
    "w",
    "function",
    "produce",
    "next",
    "hidden",
    "state",
    "h",
    "one",
    "repeat",
    "process",
    "receive",
    "next",
    "input",
    "current",
    "h",
    "one",
    "x",
    "one",
    "go",
    "f",
    "w",
    "produce",
    "next",
    "output",
    "h",
    "two",
    "process",
    "repeat",
    "consume",
    "input",
    "x",
    "ts",
    "sequence",
    "inputs",
    "one",
    "thing",
    "note",
    "actually",
    "make",
    "even",
    "explicit",
    "write",
    "w",
    "matrix",
    "computational",
    "graph",
    "see",
    "w",
    "matrix",
    "every",
    "time",
    "step",
    "computation",
    "every",
    "time",
    "little",
    "f",
    "w",
    "block",
    "receiving",
    "unique",
    "h",
    "unique",
    "x",
    "blocks",
    "taking",
    "remember",
    "talked",
    "gradient",
    "flows",
    "back",
    "propagation",
    "node",
    "multiple",
    "times",
    "computational",
    "graph",
    "remember",
    "backward",
    "pass",
    "end",
    "summing",
    "gradients",
    "w",
    "matrix",
    "computing",
    "los",
    "kind",
    "think",
    "back",
    "propagation",
    "model",
    "separate",
    "gradient",
    "w",
    "flowing",
    "time",
    "steps",
    "final",
    "gradient",
    "w",
    "sum",
    "individual",
    "per",
    "time",
    "step",
    "gradiants",
    "also",
    "write",
    "explicitly",
    "computational",
    "graph",
    "output",
    "h",
    "every",
    "time",
    "step",
    "might",
    "feed",
    "little",
    "neural",
    "network",
    "produce",
    "might",
    "class",
    "scores",
    "something",
    "like",
    "every",
    "time",
    "step",
    "also",
    "make",
    "loss",
    "explicit",
    "many",
    "cases",
    "might",
    "imagine",
    "producing",
    "might",
    "imagine",
    "ground",
    "truth",
    "label",
    "every",
    "time",
    "step",
    "sequence",
    "compute",
    "loss",
    "individual",
    "loss",
    "every",
    "time",
    "step",
    "outputs",
    "loss",
    "might",
    "frequently",
    "something",
    "like",
    "soft",
    "max",
    "loss",
    "case",
    "maybe",
    "ground",
    "truth",
    "label",
    "every",
    "time",
    "step",
    "sequence",
    "final",
    "loss",
    "entire",
    "entire",
    "training",
    "stop",
    "sum",
    "individual",
    "losses",
    "scaler",
    "loss",
    "every",
    "time",
    "step",
    "summed",
    "get",
    "final",
    "scaler",
    "loss",
    "top",
    "network",
    "think",
    "back",
    "propagation",
    "thing",
    "need",
    "order",
    "train",
    "model",
    "need",
    "compute",
    "gradient",
    "loss",
    "respect",
    "loss",
    "flowing",
    "final",
    "loss",
    "time",
    "steps",
    "time",
    "steps",
    "compute",
    "local",
    "gradient",
    "weights",
    "w",
    "summed",
    "give",
    "us",
    "final",
    "gradient",
    "weights",
    "sort",
    "many",
    "one",
    "situation",
    "maybe",
    "want",
    "something",
    "like",
    "sentiment",
    "analysis",
    "would",
    "typically",
    "make",
    "decision",
    "based",
    "final",
    "hidden",
    "state",
    "network",
    "final",
    "hidden",
    "state",
    "kind",
    "summarizes",
    "context",
    "entire",
    "sequence",
    "also",
    "kind",
    "one",
    "many",
    "situation",
    "want",
    "receive",
    "fix",
    "sized",
    "input",
    "produce",
    "variably",
    "sized",
    "output",
    "commonly",
    "use",
    "fixed",
    "size",
    "input",
    "initialize",
    "somehow",
    "initial",
    "hidden",
    "state",
    "model",
    "recurrent",
    "network",
    "tick",
    "cell",
    "output",
    "produce",
    "variably",
    "sized",
    "output",
    "unroll",
    "graph",
    "element",
    "output",
    "talk",
    "sequence",
    "sequence",
    "models",
    "might",
    "something",
    "like",
    "machine",
    "translation",
    "take",
    "variably",
    "sized",
    "input",
    "variably",
    "sized",
    "output",
    "think",
    "combination",
    "many",
    "one",
    "plus",
    "one",
    "many",
    "kind",
    "proceed",
    "two",
    "stages",
    "call",
    "encoder",
    "decoder",
    "encoder",
    "receive",
    "variably",
    "sized",
    "input",
    "might",
    "sentence",
    "english",
    "summarize",
    "entire",
    "sentence",
    "using",
    "final",
    "hidden",
    "state",
    "encoder",
    "network",
    "many",
    "one",
    "situation",
    "summarized",
    "entire",
    "variably",
    "sized",
    "input",
    "single",
    "vector",
    "second",
    "decoder",
    "network",
    "one",
    "many",
    "situation",
    "input",
    "single",
    "vector",
    "summarizing",
    "input",
    "sentence",
    "produce",
    "variably",
    "sized",
    "output",
    "might",
    "sentence",
    "another",
    "language",
    "variably",
    "sized",
    "output",
    "might",
    "make",
    "predictions",
    "every",
    "time",
    "step",
    "maybe",
    "word",
    "use",
    "imagine",
    "kind",
    "training",
    "entire",
    "thing",
    "unrolling",
    "computational",
    "graph",
    "summing",
    "losses",
    "output",
    "sequence",
    "performing",
    "back",
    "propagation",
    "usual",
    "bit",
    "concrete",
    "example",
    "one",
    "thing",
    "frequently",
    "use",
    "recurrent",
    "neural",
    "networks",
    "problem",
    "called",
    "language",
    "modeling",
    "language",
    "modeling",
    "problem",
    "want",
    "read",
    "sequence",
    "want",
    "network",
    "sort",
    "understand",
    "produce",
    "natural",
    "language",
    "might",
    "happen",
    "character",
    "level",
    "model",
    "produce",
    "characters",
    "one",
    "time",
    "might",
    "also",
    "happen",
    "word",
    "level",
    "model",
    "produce",
    "words",
    "one",
    "time",
    "simple",
    "example",
    "imagine",
    "character",
    "level",
    "language",
    "model",
    "want",
    "network",
    "read",
    "sequence",
    "characters",
    "needs",
    "predict",
    "next",
    "character",
    "stream",
    "text",
    "example",
    "small",
    "vocabulary",
    "four",
    "letters",
    "h",
    "e",
    "l",
    "example",
    "training",
    "sequence",
    "word",
    "hello",
    "h",
    "e",
    "l",
    "l",
    "training",
    "training",
    "language",
    "model",
    "feed",
    "characters",
    "training",
    "sequence",
    "inputs",
    "x",
    "ts",
    "input",
    "feed",
    "characters",
    "training",
    "sequence",
    "x",
    "ts",
    "feed",
    "inputs",
    "recurrent",
    "neural",
    "network",
    "inputs",
    "letter",
    "need",
    "figure",
    "way",
    "represent",
    "letters",
    "network",
    "typically",
    "figure",
    "total",
    "vocabulary",
    "case",
    "vocabulary",
    "four",
    "elements",
    "letter",
    "represented",
    "vector",
    "zeros",
    "every",
    "slot",
    "one",
    "one",
    "slot",
    "vocabulary",
    "corresponding",
    "letter",
    "little",
    "example",
    "since",
    "vocab",
    "four",
    "letters",
    "h",
    "e",
    "l",
    "input",
    "sequence",
    "h",
    "represented",
    "four",
    "element",
    "vector",
    "one",
    "first",
    "slot",
    "zero",
    "three",
    "slots",
    "use",
    "sort",
    "pattern",
    "represent",
    "different",
    "letters",
    "input",
    "sequence",
    "forward",
    "pass",
    "network",
    "first",
    "time",
    "step",
    "receive",
    "input",
    "letter",
    "go",
    "first",
    "rnn",
    "rnn",
    "cell",
    "produce",
    "output",
    "network",
    "making",
    "predictions",
    "letter",
    "vocabulary",
    "letter",
    "think",
    "likely",
    "going",
    "come",
    "next",
    "example",
    "correct",
    "output",
    "letter",
    "e",
    "training",
    "sequence",
    "hello",
    "model",
    "actually",
    "predicting",
    "think",
    "actually",
    "predicting",
    "likely",
    "letter",
    "case",
    "prediction",
    "wrong",
    "would",
    "use",
    "softmaxt",
    "loss",
    "quantify",
    "unhappiness",
    "predictions",
    "next",
    "time",
    "step",
    "would",
    "feed",
    "second",
    "letter",
    "training",
    "sequence",
    "e",
    "process",
    "repeat",
    "represent",
    "e",
    "vector",
    "use",
    "input",
    "vector",
    "together",
    "previous",
    "hidden",
    "state",
    "produce",
    "new",
    "hidden",
    "state",
    "use",
    "second",
    "hidden",
    "state",
    "make",
    "predictions",
    "every",
    "letter",
    "vocabulary",
    "case",
    "training",
    "sequence",
    "hello",
    "letter",
    "e",
    "want",
    "model",
    "predict",
    "case",
    "model",
    "may",
    "low",
    "predictions",
    "letter",
    "l",
    "would",
    "incur",
    "high",
    "loss",
    "kind",
    "repeat",
    "process",
    "train",
    "model",
    "many",
    "different",
    "sequences",
    "eventually",
    "learn",
    "predict",
    "next",
    "character",
    "sequence",
    "based",
    "context",
    "previous",
    "characters",
    "seen",
    "think",
    "happens",
    "test",
    "time",
    "train",
    "model",
    "one",
    "thing",
    "might",
    "want",
    "sample",
    "model",
    "actually",
    "use",
    "trained",
    "neural",
    "network",
    "model",
    "synthesize",
    "new",
    "text",
    "kind",
    "looks",
    "similar",
    "spirit",
    "text",
    "trained",
    "way",
    "work",
    "typically",
    "see",
    "model",
    "input",
    "prefix",
    "text",
    "case",
    "prefix",
    "single",
    "letter",
    "h",
    "feed",
    "letter",
    "h",
    "first",
    "time",
    "step",
    "recurrent",
    "neural",
    "network",
    "product",
    "distribution",
    "scores",
    "characters",
    "vocabulary",
    "training",
    "time",
    "use",
    "scores",
    "actually",
    "sample",
    "use",
    "softmaxt",
    "function",
    "convert",
    "scores",
    "probability",
    "distribution",
    "sample",
    "probability",
    "distribution",
    "actually",
    "synthesize",
    "second",
    "letter",
    "sequence",
    "case",
    "even",
    "though",
    "scores",
    "pretty",
    "bad",
    "maybe",
    "got",
    "lucky",
    "sampled",
    "letter",
    "e",
    "probability",
    "distribution",
    "take",
    "letter",
    "e",
    "sampled",
    "distribution",
    "feed",
    "back",
    "input",
    "network",
    "next",
    "time",
    "step",
    "take",
    "e",
    "pull",
    "top",
    "feed",
    "back",
    "network",
    "one",
    "sort",
    "one",
    "hot",
    "vectorial",
    "representations",
    "repeat",
    "process",
    "order",
    "synthesize",
    "second",
    "letter",
    "output",
    "repeat",
    "process",
    "synthesize",
    "new",
    "sequence",
    "using",
    "trained",
    "model",
    "synthesizing",
    "sequence",
    "one",
    "character",
    "time",
    "using",
    "predicted",
    "probability",
    "distributions",
    "time",
    "step",
    "question",
    "yeah",
    "great",
    "question",
    "question",
    "might",
    "sample",
    "instead",
    "taking",
    "character",
    "largest",
    "score",
    "case",
    "probability",
    "distribution",
    "impossible",
    "get",
    "right",
    "character",
    "sample",
    "example",
    "could",
    "work",
    "would",
    "make",
    "sense",
    "practice",
    "sometimes",
    "see",
    "sometimes",
    "take",
    "argmax",
    "probability",
    "sometimes",
    "little",
    "bit",
    "stable",
    "one",
    "advantage",
    "sampling",
    "general",
    "lets",
    "get",
    "diversity",
    "models",
    "sometimes",
    "might",
    "input",
    "maybe",
    "prefix",
    "case",
    "image",
    "captioning",
    "maybe",
    "image",
    "sample",
    "rather",
    "taking",
    "argmax",
    "see",
    "sometimes",
    "trained",
    "models",
    "actually",
    "able",
    "produce",
    "multiple",
    "different",
    "types",
    "reasonable",
    "output",
    "sequences",
    "depending",
    "kind",
    "depending",
    "samples",
    "take",
    "first",
    "time",
    "steps",
    "actually",
    "kind",
    "benefit",
    "cause",
    "get",
    "diversity",
    "outputs",
    "another",
    "question",
    "could",
    "feed",
    "softmax",
    "vector",
    "instead",
    "one",
    "element",
    "vector",
    "mean",
    "test",
    "time",
    "yeah",
    "yeah",
    "question",
    "test",
    "time",
    "could",
    "feed",
    "whole",
    "softmax",
    "vector",
    "rather",
    "one",
    "hot",
    "vector",
    "kind",
    "two",
    "problems",
    "one",
    "different",
    "data",
    "saw",
    "training",
    "time",
    "general",
    "ask",
    "model",
    "something",
    "test",
    "time",
    "different",
    "training",
    "time",
    "usually",
    "blow",
    "usually",
    "give",
    "garbage",
    "usually",
    "sad",
    "problem",
    "practice",
    "vocabularies",
    "might",
    "large",
    "maybe",
    "simple",
    "example",
    "vocabulary",
    "four",
    "elements",
    "big",
    "problem",
    "thinking",
    "generating",
    "words",
    "one",
    "time",
    "vocabulary",
    "every",
    "word",
    "english",
    "language",
    "could",
    "something",
    "like",
    "tens",
    "thousands",
    "elements",
    "practice",
    "first",
    "element",
    "first",
    "operation",
    "taking",
    "one",
    "hot",
    "vector",
    "often",
    "performed",
    "using",
    "sparse",
    "vector",
    "operations",
    "rather",
    "dense",
    "factors",
    "would",
    "sort",
    "computationally",
    "really",
    "bad",
    "wanted",
    "load",
    "elements",
    "softmax",
    "vector",
    "usually",
    "use",
    "one",
    "hot",
    "instead",
    "even",
    "test",
    "time",
    "idea",
    "sequence",
    "produce",
    "output",
    "every",
    "time",
    "step",
    "sequence",
    "finally",
    "compute",
    "loss",
    "sometimes",
    "called",
    "backpropagation",
    "time",
    "imagining",
    "forward",
    "pass",
    "kind",
    "stepping",
    "forward",
    "time",
    "backward",
    "pass",
    "sort",
    "going",
    "backwards",
    "time",
    "compute",
    "gradients",
    "actually",
    "kind",
    "problematic",
    "want",
    "train",
    "sequences",
    "long",
    "imagine",
    "kind",
    "trying",
    "train",
    "neural",
    "network",
    "language",
    "model",
    "maybe",
    "entire",
    "text",
    "wikipedia",
    "way",
    "something",
    "people",
    "pretty",
    "frequently",
    "would",
    "super",
    "slow",
    "every",
    "time",
    "made",
    "gradient",
    "step",
    "would",
    "make",
    "forward",
    "pass",
    "entire",
    "text",
    "wikipedia",
    "make",
    "backward",
    "pass",
    "wikipedia",
    "make",
    "single",
    "gradient",
    "update",
    "would",
    "super",
    "slow",
    "model",
    "would",
    "never",
    "converge",
    "would",
    "also",
    "take",
    "ridiculous",
    "amount",
    "memory",
    "would",
    "really",
    "bad",
    "practice",
    "people",
    "sort",
    "approximation",
    "called",
    "truncated",
    "backpropagation",
    "time",
    "idea",
    "even",
    "though",
    "input",
    "sequence",
    "long",
    "even",
    "potentially",
    "infinite",
    "training",
    "model",
    "step",
    "forward",
    "number",
    "steps",
    "maybe",
    "like",
    "hundred",
    "kind",
    "ballpark",
    "number",
    "people",
    "frequently",
    "use",
    "step",
    "forward",
    "maybe",
    "hundred",
    "steps",
    "compute",
    "loss",
    "sub",
    "sequence",
    "data",
    "back",
    "propagate",
    "sub",
    "sequence",
    "make",
    "gradient",
    "step",
    "repeat",
    "well",
    "still",
    "hidden",
    "states",
    "computed",
    "first",
    "batch",
    "compute",
    "next",
    "batch",
    "data",
    "carry",
    "hidden",
    "states",
    "forward",
    "time",
    "forward",
    "pass",
    "exactly",
    "compute",
    "gradient",
    "step",
    "next",
    "batch",
    "data",
    "backpropagate",
    "second",
    "batch",
    "make",
    "gradient",
    "step",
    "based",
    "truncated",
    "backpropagation",
    "time",
    "process",
    "continue",
    "make",
    "next",
    "batch",
    "copy",
    "hidden",
    "states",
    "forward",
    "step",
    "forward",
    "step",
    "backward",
    "small",
    "number",
    "time",
    "steps",
    "kind",
    "think",
    "alegist",
    "cast",
    "gradient",
    "descent",
    "case",
    "sequences",
    "remember",
    "talked",
    "training",
    "models",
    "large",
    "data",
    "sets",
    "data",
    "sets",
    "would",
    "super",
    "expensive",
    "compute",
    "gradients",
    "every",
    "element",
    "data",
    "set",
    "instead",
    "kind",
    "take",
    "small",
    "samples",
    "small",
    "mini",
    "batches",
    "instead",
    "use",
    "mini",
    "batches",
    "data",
    "compute",
    "gradient",
    "stops",
    "kind",
    "image",
    "classification",
    "case",
    "question",
    "kind",
    "question",
    "kind",
    "making",
    "mark",
    "hobb",
    "assumption",
    "really",
    "carrying",
    "hidden",
    "state",
    "forward",
    "time",
    "forever",
    "making",
    "marcovian",
    "assumption",
    "sense",
    "conditioned",
    "hidden",
    "state",
    "hidden",
    "state",
    "need",
    "predict",
    "entire",
    "future",
    "sequence",
    "assumption",
    "kind",
    "built",
    "recurrent",
    "neural",
    "network",
    "formula",
    "start",
    "really",
    "particular",
    "back",
    "propagation",
    "time",
    "back",
    "propagation",
    "time",
    "sorry",
    "truncated",
    "back",
    "prop",
    "though",
    "time",
    "way",
    "approximate",
    "gradients",
    "without",
    "going",
    "making",
    "backwards",
    "pass",
    "potentially",
    "large",
    "sequence",
    "data",
    "sounds",
    "complicated",
    "confusing",
    "sounds",
    "like",
    "lot",
    "code",
    "write",
    "fact",
    "acutally",
    "pretty",
    "concise",
    "andrea",
    "example",
    "calls",
    "stuff",
    "like",
    "112",
    "lines",
    "python",
    "handles",
    "building",
    "vocabulary",
    "trains",
    "model",
    "truncated",
    "back",
    "propagation",
    "time",
    "actually",
    "sample",
    "model",
    "actually",
    "much",
    "code",
    "even",
    "though",
    "sounds",
    "like",
    "kind",
    "big",
    "scary",
    "process",
    "actually",
    "difficult",
    "encourage",
    "confused",
    "maybe",
    "go",
    "check",
    "step",
    "code",
    "time",
    "see",
    "kind",
    "concrete",
    "steps",
    "happening",
    "code",
    "single",
    "file",
    "using",
    "numpy",
    "dependencies",
    "relatively",
    "easy",
    "read",
    "idea",
    "training",
    "recurrent",
    "neural",
    "network",
    "language",
    "model",
    "actually",
    "lot",
    "fun",
    "take",
    "sort",
    "text",
    "want",
    "take",
    "like",
    "whatever",
    "random",
    "text",
    "think",
    "internet",
    "train",
    "recurrent",
    "neural",
    "network",
    "language",
    "model",
    "text",
    "generate",
    "new",
    "text",
    "example",
    "took",
    "entire",
    "text",
    "shakespeare",
    "works",
    "used",
    "train",
    "recurrent",
    "neural",
    "network",
    "language",
    "model",
    "shakespeare",
    "see",
    "beginning",
    "training",
    "kind",
    "producing",
    "maybe",
    "random",
    "gibberish",
    "garbage",
    "throughout",
    "course",
    "training",
    "ends",
    "producing",
    "things",
    "seem",
    "relatively",
    "reasonable",
    "model",
    "trained",
    "pretty",
    "well",
    "produces",
    "text",
    "seems",
    "kind",
    "day",
    "replied",
    "whatever",
    "right",
    "read",
    "like",
    "kind",
    "looks",
    "kind",
    "like",
    "shakespeare",
    "actually",
    "train",
    "model",
    "even",
    "let",
    "converge",
    "even",
    "sample",
    "even",
    "longer",
    "sequences",
    "see",
    "learns",
    "kinds",
    "crazy",
    "cool",
    "stuff",
    "really",
    "looks",
    "like",
    "shakespeare",
    "play",
    "knows",
    "uses",
    "maybe",
    "headings",
    "say",
    "speaking",
    "produces",
    "bits",
    "text",
    "crazy",
    "dialogue",
    "sounds",
    "kind",
    "knows",
    "put",
    "line",
    "breaks",
    "different",
    "things",
    "like",
    "really",
    "cool",
    "sort",
    "learned",
    "structure",
    "data",
    "actually",
    "get",
    "even",
    "crazier",
    "one",
    "favorite",
    "examples",
    "found",
    "online",
    "anyone",
    "mathematician",
    "room",
    "anyone",
    "taken",
    "algebraic",
    "topology",
    "course",
    "chance",
    "wow",
    "couple",
    "impressive",
    "probably",
    "know",
    "algebraic",
    "topology",
    "found",
    "open",
    "source",
    "algebraic",
    "topology",
    "textbook",
    "online",
    "whole",
    "bunch",
    "tech",
    "files",
    "like",
    "super",
    "dense",
    "mathematics",
    "latac",
    "cause",
    "latac",
    "sort",
    "let",
    "write",
    "equations",
    "diagrams",
    "everything",
    "using",
    "plain",
    "text",
    "actually",
    "train",
    "recurrent",
    "neural",
    "network",
    "language",
    "model",
    "raw",
    "latac",
    "source",
    "code",
    "algebraic",
    "topology",
    "textbook",
    "sample",
    "model",
    "get",
    "something",
    "seems",
    "like",
    "kind",
    "like",
    "algebraic",
    "topology",
    "knows",
    "like",
    "put",
    "equations",
    "puts",
    "kinds",
    "crazy",
    "stuff",
    "like",
    "prove",
    "study",
    "see",
    "f",
    "sub",
    "u",
    "covering",
    "x",
    "prime",
    "blah",
    "blah",
    "blah",
    "blah",
    "blah",
    "knows",
    "put",
    "unions",
    "knows",
    "put",
    "squares",
    "end",
    "proofs",
    "makes",
    "lemmas",
    "makes",
    "references",
    "previous",
    "lemmas",
    "right",
    "like",
    "hear",
    "like",
    "namely",
    "question",
    "see",
    "r",
    "geometrically",
    "something",
    "actually",
    "pretty",
    "crazy",
    "also",
    "sometimes",
    "tries",
    "make",
    "diagrams",
    "taken",
    "algebraic",
    "topology",
    "know",
    "commutative",
    "diagrams",
    "kind",
    "thing",
    "work",
    "lot",
    "kind",
    "got",
    "general",
    "gist",
    "make",
    "diagrams",
    "actually",
    "make",
    "sense",
    "actually",
    "one",
    "favorite",
    "examples",
    "sometimes",
    "omits",
    "proofs",
    "sometimes",
    "say",
    "sometimes",
    "say",
    "something",
    "like",
    "theorem",
    "blah",
    "blah",
    "blah",
    "blah",
    "blah",
    "proof",
    "omitted",
    "thing",
    "kind",
    "gotten",
    "gist",
    "math",
    "textbooks",
    "look",
    "like",
    "lot",
    "fun",
    "also",
    "tried",
    "training",
    "one",
    "models",
    "entire",
    "source",
    "code",
    "linux",
    "kernel",
    "character",
    "level",
    "stuff",
    "train",
    "sample",
    "acutally",
    "looks",
    "like",
    "c",
    "source",
    "code",
    "knows",
    "write",
    "statements",
    "like",
    "pretty",
    "good",
    "code",
    "formatting",
    "skills",
    "knows",
    "indent",
    "statements",
    "knows",
    "put",
    "curly",
    "braces",
    "actually",
    "even",
    "makes",
    "comments",
    "things",
    "usually",
    "nonsense",
    "one",
    "problem",
    "model",
    "knows",
    "declare",
    "variables",
    "always",
    "use",
    "variables",
    "declares",
    "sometimes",
    "tries",
    "use",
    "variables",
    "declared",
    "would",
    "compile",
    "would",
    "recommend",
    "sending",
    "pull",
    "request",
    "linux",
    "thing",
    "also",
    "figures",
    "recite",
    "gnu",
    "gnu",
    "license",
    "character",
    "character",
    "kind",
    "knows",
    "need",
    "recite",
    "gnu",
    "license",
    "license",
    "comes",
    "includes",
    "includes",
    "source",
    "code",
    "thing",
    "actually",
    "learned",
    "quite",
    "lot",
    "general",
    "structure",
    "data",
    "training",
    "asked",
    "model",
    "try",
    "predict",
    "next",
    "character",
    "sequence",
    "tell",
    "structure",
    "somehow",
    "course",
    "training",
    "process",
    "learned",
    "lot",
    "latent",
    "structure",
    "sequential",
    "data",
    "yeah",
    "knows",
    "write",
    "code",
    "lot",
    "cool",
    "stuff",
    "paper",
    "andre",
    "couple",
    "years",
    "ago",
    "trained",
    "bunch",
    "models",
    "wanted",
    "try",
    "poke",
    "brains",
    "models",
    "figure",
    "like",
    "working",
    "saw",
    "recurring",
    "neural",
    "networks",
    "hidden",
    "vector",
    "maybe",
    "vector",
    "updated",
    "every",
    "time",
    "step",
    "wanted",
    "try",
    "figure",
    "could",
    "find",
    "elements",
    "vector",
    "symantec",
    "interpretable",
    "meaning",
    "trained",
    "neural",
    "network",
    "language",
    "model",
    "one",
    "character",
    "level",
    "models",
    "one",
    "data",
    "sets",
    "picked",
    "one",
    "elements",
    "hidden",
    "vector",
    "look",
    "value",
    "hidden",
    "vector",
    "course",
    "sequence",
    "try",
    "get",
    "sense",
    "maybe",
    "different",
    "hidden",
    "states",
    "looking",
    "lot",
    "end",
    "looking",
    "kind",
    "like",
    "random",
    "gibberish",
    "garbage",
    "done",
    "picked",
    "one",
    "element",
    "vector",
    "run",
    "sequence",
    "forward",
    "trained",
    "model",
    "color",
    "character",
    "corresponds",
    "magnitude",
    "single",
    "scaler",
    "element",
    "hidden",
    "vector",
    "every",
    "time",
    "step",
    "reading",
    "sequence",
    "see",
    "lot",
    "vectors",
    "hidden",
    "states",
    "kind",
    "interpretable",
    "seems",
    "like",
    "kind",
    "low",
    "level",
    "language",
    "modeling",
    "figure",
    "character",
    "come",
    "next",
    "end",
    "quite",
    "nice",
    "found",
    "vector",
    "looking",
    "quotes",
    "see",
    "one",
    "hidden",
    "element",
    "one",
    "element",
    "vector",
    "blue",
    "hits",
    "quote",
    "turns",
    "remains",
    "duration",
    "quote",
    "hit",
    "second",
    "quotation",
    "mark",
    "cell",
    "turns",
    "somehow",
    "even",
    "though",
    "model",
    "trained",
    "predict",
    "next",
    "character",
    "sequence",
    "somehow",
    "learned",
    "useful",
    "thing",
    "order",
    "might",
    "cell",
    "trying",
    "detect",
    "quotes",
    "also",
    "found",
    "cell",
    "looks",
    "like",
    "counting",
    "number",
    "characters",
    "since",
    "line",
    "break",
    "see",
    "beginning",
    "line",
    "element",
    "starts",
    "zero",
    "throughout",
    "course",
    "line",
    "gradually",
    "red",
    "value",
    "increases",
    "new",
    "line",
    "character",
    "resets",
    "zero",
    "imagine",
    "maybe",
    "cell",
    "letting",
    "network",
    "keep",
    "track",
    "needs",
    "write",
    "produce",
    "new",
    "line",
    "characters",
    "also",
    "found",
    "trained",
    "linux",
    "source",
    "code",
    "found",
    "examples",
    "turning",
    "inside",
    "conditions",
    "statements",
    "maybe",
    "allows",
    "network",
    "differentiate",
    "whether",
    "outside",
    "statement",
    "inside",
    "condition",
    "might",
    "help",
    "model",
    "sequences",
    "better",
    "also",
    "found",
    "turn",
    "comments",
    "seem",
    "like",
    "counting",
    "number",
    "indentation",
    "levels",
    "really",
    "cool",
    "stuff",
    "saying",
    "even",
    "though",
    "trying",
    "train",
    "model",
    "predict",
    "next",
    "characters",
    "somehow",
    "ends",
    "learning",
    "lot",
    "useful",
    "structure",
    "input",
    "data",
    "one",
    "kind",
    "thing",
    "often",
    "use",
    "really",
    "computer",
    "vision",
    "far",
    "need",
    "pull",
    "back",
    "computer",
    "vision",
    "since",
    "vision",
    "class",
    "alluded",
    "many",
    "times",
    "image",
    "captioning",
    "model",
    "want",
    "build",
    "models",
    "input",
    "image",
    "output",
    "caption",
    "natural",
    "language",
    "bunch",
    "papers",
    "couple",
    "years",
    "ago",
    "relatively",
    "similar",
    "approaches",
    "showing",
    "figure",
    "paper",
    "lab",
    "totally",
    "way",
    "idea",
    "caption",
    "variably",
    "length",
    "sequence",
    "might",
    "sequence",
    "might",
    "different",
    "numbers",
    "words",
    "different",
    "captions",
    "totally",
    "natural",
    "fit",
    "recurrent",
    "neural",
    "network",
    "language",
    "model",
    "model",
    "looks",
    "like",
    "convolutional",
    "network",
    "input",
    "take",
    "input",
    "image",
    "seen",
    "lot",
    "convolution",
    "networks",
    "work",
    "point",
    "convolutional",
    "network",
    "produce",
    "summary",
    "vector",
    "image",
    "feed",
    "first",
    "time",
    "step",
    "one",
    "recurrent",
    "neural",
    "network",
    "language",
    "models",
    "produce",
    "words",
    "caption",
    "one",
    "time",
    "way",
    "kind",
    "works",
    "test",
    "time",
    "model",
    "trained",
    "looks",
    "almost",
    "exactly",
    "character",
    "level",
    "language",
    "models",
    "saw",
    "little",
    "bit",
    "ago",
    "take",
    "input",
    "image",
    "feed",
    "convolutional",
    "network",
    "instead",
    "taking",
    "softmax",
    "scores",
    "image",
    "net",
    "model",
    "instead",
    "take",
    "dimensional",
    "vector",
    "end",
    "model",
    "take",
    "vector",
    "use",
    "summarize",
    "whole",
    "content",
    "image",
    "remember",
    "talked",
    "rnn",
    "language",
    "models",
    "said",
    "need",
    "see",
    "language",
    "model",
    "first",
    "initial",
    "input",
    "tell",
    "start",
    "generating",
    "text",
    "case",
    "give",
    "special",
    "start",
    "token",
    "saying",
    "hey",
    "start",
    "sentence",
    "please",
    "start",
    "generating",
    "text",
    "conditioned",
    "image",
    "information",
    "previously",
    "saw",
    "rnn",
    "language",
    "model",
    "matrices",
    "taking",
    "previous",
    "input",
    "current",
    "time",
    "step",
    "hidden",
    "state",
    "previous",
    "time",
    "step",
    "combining",
    "get",
    "next",
    "hidden",
    "state",
    "well",
    "also",
    "need",
    "add",
    "image",
    "information",
    "one",
    "way",
    "people",
    "play",
    "around",
    "exactly",
    "different",
    "ways",
    "incorporate",
    "image",
    "information",
    "one",
    "simple",
    "way",
    "add",
    "third",
    "weight",
    "matrix",
    "adding",
    "image",
    "information",
    "every",
    "time",
    "step",
    "compute",
    "next",
    "hidden",
    "state",
    "compute",
    "distribution",
    "scores",
    "vocabulary",
    "vocabulary",
    "something",
    "like",
    "english",
    "words",
    "could",
    "pretty",
    "large",
    "sample",
    "distribution",
    "pass",
    "word",
    "back",
    "input",
    "next",
    "time",
    "step",
    "feed",
    "word",
    "get",
    "distribution",
    "words",
    "vocab",
    "sample",
    "produce",
    "next",
    "word",
    "thing",
    "done",
    "maybe",
    "generate",
    "generate",
    "complete",
    "sentence",
    "stop",
    "generation",
    "sample",
    "special",
    "ends",
    "token",
    "kind",
    "corresponds",
    "period",
    "end",
    "sentence",
    "network",
    "samples",
    "ends",
    "token",
    "stop",
    "generation",
    "done",
    "gotten",
    "caption",
    "image",
    "training",
    "trained",
    "thing",
    "generate",
    "like",
    "put",
    "end",
    "token",
    "end",
    "every",
    "caption",
    "training",
    "network",
    "kind",
    "learned",
    "training",
    "end",
    "tokens",
    "come",
    "end",
    "sequences",
    "test",
    "time",
    "tends",
    "sample",
    "end",
    "tokens",
    "done",
    "generating",
    "trained",
    "model",
    "kind",
    "completely",
    "supervised",
    "way",
    "find",
    "data",
    "sets",
    "images",
    "together",
    "natural",
    "language",
    "captions",
    "microsoft",
    "coco",
    "probably",
    "biggest",
    "widely",
    "used",
    "task",
    "train",
    "model",
    "purely",
    "supervised",
    "way",
    "backpropagate",
    "jointly",
    "train",
    "recurrent",
    "neural",
    "network",
    "language",
    "model",
    "also",
    "pass",
    "gradients",
    "back",
    "final",
    "layer",
    "cnn",
    "additionally",
    "update",
    "weights",
    "cnn",
    "jointly",
    "tune",
    "parts",
    "model",
    "perform",
    "task",
    "train",
    "models",
    "actually",
    "pretty",
    "reasonable",
    "things",
    "real",
    "results",
    "model",
    "one",
    "trained",
    "models",
    "says",
    "things",
    "like",
    "cat",
    "sitting",
    "suitcase",
    "floor",
    "pretty",
    "impressive",
    "knows",
    "cats",
    "sitting",
    "tree",
    "branch",
    "also",
    "pretty",
    "cool",
    "knows",
    "two",
    "people",
    "walking",
    "beach",
    "surfboards",
    "models",
    "actually",
    "pretty",
    "powerful",
    "produce",
    "relatively",
    "complex",
    "captions",
    "describe",
    "image",
    "said",
    "models",
    "really",
    "perfect",
    "magical",
    "like",
    "machine",
    "learning",
    "model",
    "try",
    "run",
    "data",
    "different",
    "training",
    "data",
    "work",
    "well",
    "example",
    "example",
    "says",
    "woman",
    "holding",
    "cat",
    "hand",
    "clearly",
    "cat",
    "image",
    "wearing",
    "fur",
    "coat",
    "maybe",
    "texture",
    "coat",
    "kind",
    "looked",
    "like",
    "cat",
    "model",
    "see",
    "woman",
    "standing",
    "beach",
    "holding",
    "surfboard",
    "well",
    "definitely",
    "holding",
    "surfboard",
    "handstand",
    "maybe",
    "interesting",
    "part",
    "image",
    "model",
    "totally",
    "missed",
    "also",
    "see",
    "example",
    "picture",
    "spider",
    "web",
    "tree",
    "branch",
    "totally",
    "says",
    "something",
    "like",
    "bird",
    "sitting",
    "tree",
    "branch",
    "totally",
    "missed",
    "spider",
    "training",
    "never",
    "really",
    "saw",
    "examples",
    "spiders",
    "knows",
    "birds",
    "sit",
    "tree",
    "branches",
    "training",
    "kind",
    "makes",
    "reasonable",
    "mistakes",
    "bottom",
    "ca",
    "really",
    "tell",
    "difference",
    "guy",
    "throwing",
    "catching",
    "ball",
    "know",
    "baseball",
    "player",
    "balls",
    "things",
    "involved",
    "want",
    "say",
    "models",
    "perfect",
    "work",
    "pretty",
    "well",
    "ask",
    "caption",
    "images",
    "similar",
    "training",
    "data",
    "definitely",
    "hard",
    "time",
    "generalizing",
    "far",
    "beyond",
    "another",
    "thing",
    "sometimes",
    "see",
    "slightly",
    "advanced",
    "model",
    "called",
    "attention",
    "generating",
    "words",
    "caption",
    "allow",
    "model",
    "steer",
    "attention",
    "different",
    "parts",
    "image",
    "want",
    "spend",
    "much",
    "time",
    "general",
    "way",
    "works",
    "convolutional",
    "network",
    "rather",
    "producing",
    "single",
    "vector",
    "summarizing",
    "entire",
    "image",
    "produces",
    "grid",
    "vectors",
    "summarize",
    "give",
    "maybe",
    "one",
    "vector",
    "spatial",
    "location",
    "image",
    "model",
    "runs",
    "forward",
    "addition",
    "sampling",
    "vocabulary",
    "every",
    "time",
    "step",
    "also",
    "produces",
    "distribution",
    "locations",
    "image",
    "wants",
    "look",
    "distribution",
    "image",
    "locations",
    "seen",
    "kind",
    "tension",
    "model",
    "look",
    "training",
    "first",
    "hidden",
    "state",
    "computes",
    "distribution",
    "image",
    "locations",
    "goes",
    "back",
    "set",
    "vectors",
    "give",
    "single",
    "summary",
    "vector",
    "maybe",
    "focuses",
    "attention",
    "one",
    "part",
    "image",
    "summary",
    "vector",
    "gets",
    "fed",
    "additional",
    "input",
    "next",
    "time",
    "step",
    "neural",
    "network",
    "produce",
    "two",
    "outputs",
    "one",
    "distribution",
    "vocabulary",
    "words",
    "distribution",
    "image",
    "locations",
    "whole",
    "process",
    "continue",
    "sort",
    "two",
    "different",
    "things",
    "every",
    "time",
    "step",
    "train",
    "model",
    "see",
    "kind",
    "shift",
    "attention",
    "around",
    "image",
    "every",
    "word",
    "generates",
    "caption",
    "see",
    "produced",
    "caption",
    "bird",
    "flying",
    "ca",
    "see",
    "far",
    "see",
    "attention",
    "shifting",
    "around",
    "different",
    "parts",
    "image",
    "word",
    "caption",
    "generates",
    "notion",
    "hard",
    "attention",
    "versus",
    "soft",
    "attention",
    "really",
    "want",
    "get",
    "much",
    "idea",
    "soft",
    "attention",
    "kind",
    "taking",
    "weighted",
    "combination",
    "features",
    "image",
    "locations",
    "whereas",
    "hard",
    "attention",
    "case",
    "forcing",
    "model",
    "select",
    "exactly",
    "one",
    "location",
    "look",
    "image",
    "time",
    "step",
    "hard",
    "attention",
    "case",
    "selecting",
    "exactly",
    "one",
    "image",
    "location",
    "little",
    "bit",
    "tricky",
    "really",
    "differentiable",
    "function",
    "need",
    "something",
    "slightly",
    "fancier",
    "vanilla",
    "backpropagation",
    "order",
    "train",
    "model",
    "scenario",
    "think",
    "talk",
    "little",
    "bit",
    "later",
    "lecture",
    "reinforcement",
    "learning",
    "look",
    "train",
    "one",
    "attention",
    "models",
    "run",
    "generate",
    "captions",
    "see",
    "tends",
    "focus",
    "attention",
    "maybe",
    "salient",
    "semanticly",
    "meaningful",
    "part",
    "image",
    "generating",
    "captions",
    "see",
    "caption",
    "woman",
    "throwing",
    "frisbee",
    "park",
    "see",
    "attention",
    "mask",
    "generated",
    "word",
    "model",
    "generated",
    "word",
    "frisbee",
    "time",
    "focusing",
    "attention",
    "image",
    "region",
    "actually",
    "contains",
    "frisbee",
    "actually",
    "really",
    "cool",
    "tell",
    "model",
    "looking",
    "every",
    "time",
    "step",
    "sort",
    "figured",
    "training",
    "process",
    "somehow",
    "figured",
    "looking",
    "image",
    "region",
    "right",
    "thing",
    "image",
    "everything",
    "model",
    "differentiable",
    "backpropagate",
    "soft",
    "attention",
    "steps",
    "soft",
    "attention",
    "stuff",
    "comes",
    "training",
    "process",
    "really",
    "really",
    "cool",
    "way",
    "idea",
    "recurrent",
    "neural",
    "networks",
    "attention",
    "actually",
    "gets",
    "used",
    "tasks",
    "beyond",
    "image",
    "captioning",
    "one",
    "recent",
    "example",
    "idea",
    "visual",
    "question",
    "answering",
    "model",
    "going",
    "take",
    "two",
    "things",
    "input",
    "going",
    "take",
    "image",
    "also",
    "take",
    "natural",
    "language",
    "question",
    "asking",
    "question",
    "image",
    "might",
    "see",
    "image",
    "left",
    "might",
    "ask",
    "question",
    "endangered",
    "animal",
    "featured",
    "truck",
    "model",
    "needs",
    "select",
    "one",
    "four",
    "natural",
    "language",
    "answers",
    "answers",
    "correctly",
    "answers",
    "question",
    "context",
    "image",
    "imagine",
    "kind",
    "stitching",
    "model",
    "together",
    "using",
    "cnns",
    "rnns",
    "kind",
    "natural",
    "way",
    "many",
    "one",
    "scenario",
    "model",
    "needs",
    "take",
    "input",
    "natural",
    "language",
    "sequence",
    "imagine",
    "running",
    "recurrent",
    "neural",
    "network",
    "element",
    "input",
    "question",
    "summarize",
    "input",
    "question",
    "single",
    "vector",
    "cnn",
    "summarize",
    "image",
    "combine",
    "vector",
    "cnn",
    "vector",
    "question",
    "coding",
    "rnn",
    "predict",
    "distribution",
    "answers",
    "also",
    "sometimes",
    "also",
    "sometimes",
    "see",
    "idea",
    "soft",
    "spacial",
    "attention",
    "incorporated",
    "things",
    "like",
    "visual",
    "question",
    "answering",
    "see",
    "model",
    "also",
    "spatial",
    "attention",
    "image",
    "trying",
    "determine",
    "answers",
    "questions",
    "yeah",
    "question",
    "question",
    "different",
    "inputs",
    "combined",
    "mean",
    "like",
    "encoded",
    "question",
    "vector",
    "encoded",
    "image",
    "vector",
    "yeah",
    "question",
    "encoded",
    "image",
    "encoded",
    "question",
    "vector",
    "combined",
    "kind",
    "simplest",
    "thing",
    "concatenate",
    "stick",
    "fully",
    "connected",
    "layers",
    "probably",
    "common",
    "probably",
    "first",
    "thing",
    "try",
    "sometimes",
    "people",
    "slightly",
    "fancier",
    "things",
    "might",
    "try",
    "multiplicative",
    "interactions",
    "two",
    "vectors",
    "allow",
    "powerful",
    "function",
    "generally",
    "concatenation",
    "kind",
    "good",
    "first",
    "thing",
    "try",
    "okay",
    "talked",
    "bunch",
    "scenarios",
    "rnns",
    "used",
    "different",
    "kinds",
    "problems",
    "think",
    "super",
    "cool",
    "allows",
    "start",
    "tackling",
    "really",
    "complicated",
    "problems",
    "combining",
    "images",
    "computer",
    "vision",
    "natural",
    "language",
    "processing",
    "see",
    "kind",
    "stith",
    "together",
    "models",
    "like",
    "lego",
    "blocks",
    "attack",
    "really",
    "complicated",
    "things",
    "like",
    "image",
    "captioning",
    "visual",
    "question",
    "answering",
    "stitching",
    "together",
    "relatively",
    "simple",
    "types",
    "neural",
    "network",
    "modules",
    "also",
    "like",
    "mention",
    "far",
    "talked",
    "idea",
    "single",
    "recurrent",
    "network",
    "layer",
    "sort",
    "one",
    "hidden",
    "state",
    "another",
    "thing",
    "see",
    "pretty",
    "commonly",
    "idea",
    "multilayer",
    "recurrent",
    "neural",
    "network",
    "three",
    "layer",
    "recurrent",
    "neural",
    "network",
    "input",
    "goes",
    "goes",
    "goes",
    "produces",
    "sequence",
    "hidden",
    "states",
    "first",
    "recurrent",
    "neural",
    "network",
    "layer",
    "run",
    "kind",
    "one",
    "recurrent",
    "neural",
    "network",
    "layer",
    "whole",
    "sequence",
    "hidden",
    "states",
    "use",
    "sequence",
    "hidden",
    "states",
    "input",
    "sequence",
    "another",
    "recurrent",
    "neural",
    "network",
    "layer",
    "imagine",
    "produce",
    "another",
    "sequence",
    "hidden",
    "states",
    "second",
    "rnn",
    "layer",
    "imagine",
    "stacking",
    "things",
    "top",
    "cause",
    "know",
    "seen",
    "contexts",
    "deeper",
    "models",
    "tend",
    "perform",
    "better",
    "various",
    "problems",
    "kind",
    "holds",
    "rnns",
    "well",
    "many",
    "problems",
    "see",
    "maybe",
    "two",
    "three",
    "layer",
    "recurrent",
    "neural",
    "network",
    "model",
    "pretty",
    "commonly",
    "used",
    "typically",
    "see",
    "super",
    "deep",
    "models",
    "rnns",
    "generally",
    "like",
    "two",
    "three",
    "four",
    "layer",
    "rnns",
    "maybe",
    "deep",
    "typically",
    "go",
    "think",
    "also",
    "really",
    "interesting",
    "important",
    "think",
    "seen",
    "kind",
    "kinds",
    "problems",
    "rnns",
    "used",
    "need",
    "think",
    "little",
    "bit",
    "carefully",
    "exactly",
    "happens",
    "models",
    "try",
    "train",
    "drawn",
    "little",
    "vanilla",
    "rnn",
    "cell",
    "talked",
    "far",
    "taking",
    "current",
    "input",
    "x",
    "previous",
    "hidden",
    "state",
    "h",
    "minus",
    "one",
    "stack",
    "two",
    "vectors",
    "stack",
    "together",
    "perform",
    "matrix",
    "multiplication",
    "weight",
    "matrix",
    "give",
    "squash",
    "output",
    "tanh",
    "give",
    "us",
    "next",
    "hidden",
    "state",
    "kind",
    "basic",
    "functional",
    "form",
    "vanilla",
    "recurrent",
    "neural",
    "network",
    "need",
    "think",
    "happens",
    "architecture",
    "backward",
    "pass",
    "try",
    "compute",
    "gradients",
    "think",
    "trying",
    "compute",
    "backwards",
    "pass",
    "receive",
    "derivative",
    "h",
    "receive",
    "derivative",
    "loss",
    "respect",
    "h",
    "backward",
    "pass",
    "cell",
    "need",
    "compute",
    "derivative",
    "loss",
    "respect",
    "h",
    "minus",
    "one",
    "compute",
    "backward",
    "pass",
    "see",
    "gradient",
    "flows",
    "backward",
    "red",
    "path",
    "first",
    "gradient",
    "flow",
    "backwards",
    "tanh",
    "gate",
    "flow",
    "backwards",
    "matrix",
    "multiplication",
    "gate",
    "seen",
    "homework",
    "implementing",
    "matrix",
    "multiplication",
    "layers",
    "backpropagate",
    "matrix",
    "multiplication",
    "gate",
    "end",
    "mulitplying",
    "transpose",
    "weight",
    "matrix",
    "means",
    "every",
    "time",
    "backpropagate",
    "one",
    "vanilla",
    "rnn",
    "cells",
    "end",
    "multiplying",
    "part",
    "weight",
    "matrix",
    "imagine",
    "sticking",
    "many",
    "recurrent",
    "neural",
    "network",
    "cells",
    "sequence",
    "rnn",
    "want",
    "model",
    "sequences",
    "imagine",
    "happens",
    "gradient",
    "flow",
    "sequence",
    "layers",
    "something",
    "kind",
    "fishy",
    "starts",
    "happen",
    "want",
    "compute",
    "gradient",
    "loss",
    "respect",
    "h",
    "zero",
    "need",
    "backpropagate",
    "every",
    "one",
    "rnn",
    "cells",
    "every",
    "time",
    "backpropagate",
    "one",
    "cell",
    "pick",
    "one",
    "w",
    "transpose",
    "factors",
    "means",
    "final",
    "expression",
    "gradient",
    "h",
    "zero",
    "involve",
    "many",
    "many",
    "factors",
    "weight",
    "matrix",
    "could",
    "kind",
    "bad",
    "maybe",
    "think",
    "weight",
    "matrix",
    "case",
    "imagine",
    "scaler",
    "case",
    "end",
    "scaler",
    "multiply",
    "number",
    "maybe",
    "four",
    "examples",
    "something",
    "like",
    "hundred",
    "several",
    "hundred",
    "time",
    "steps",
    "multiplying",
    "number",
    "really",
    "bad",
    "scaler",
    "case",
    "either",
    "going",
    "explode",
    "case",
    "number",
    "greater",
    "one",
    "going",
    "vanish",
    "towards",
    "zero",
    "case",
    "number",
    "less",
    "one",
    "absolute",
    "value",
    "way",
    "happen",
    "number",
    "exactly",
    "one",
    "actually",
    "rare",
    "happen",
    "practice",
    "leaves",
    "us",
    "intuition",
    "extends",
    "matrix",
    "case",
    "rather",
    "absolute",
    "value",
    "scaler",
    "number",
    "instead",
    "need",
    "look",
    "largest",
    "largest",
    "singular",
    "value",
    "weight",
    "matrix",
    "largest",
    "singular",
    "value",
    "greater",
    "one",
    "backward",
    "pass",
    "multiply",
    "weight",
    "matrix",
    "gradient",
    "h",
    "w",
    "h",
    "zero",
    "sorry",
    "become",
    "large",
    "matrix",
    "large",
    "something",
    "call",
    "exploding",
    "gradient",
    "problem",
    "gradient",
    "explode",
    "exponentially",
    "depth",
    "number",
    "time",
    "steps",
    "backpropagate",
    "largest",
    "singular",
    "value",
    "less",
    "one",
    "get",
    "opposite",
    "problem",
    "gradients",
    "shrink",
    "shrink",
    "shrink",
    "exponentially",
    "backpropagate",
    "pick",
    "factors",
    "weight",
    "matrix",
    "called",
    "vanishing",
    "gradient",
    "problem",
    "bit",
    "hack",
    "people",
    "sometimes",
    "fix",
    "exploding",
    "gradient",
    "problem",
    "called",
    "gradient",
    "clipping",
    "simple",
    "heuristic",
    "saying",
    "compute",
    "gradient",
    "gradient",
    "l2",
    "norm",
    "threshold",
    "clamp",
    "divide",
    "clamp",
    "maximum",
    "threshold",
    "kind",
    "nasty",
    "hack",
    "actually",
    "gets",
    "used",
    "practice",
    "quite",
    "lot",
    "training",
    "recurrent",
    "neural",
    "networks",
    "relatively",
    "useful",
    "tool",
    "attacking",
    "exploding",
    "gradient",
    "problem",
    "vanishing",
    "gradient",
    "problem",
    "typically",
    "might",
    "need",
    "move",
    "complicated",
    "rnn",
    "architecture",
    "motivates",
    "idea",
    "lstm",
    "lstm",
    "stands",
    "long",
    "short",
    "term",
    "memory",
    "slightly",
    "fancier",
    "recurrence",
    "relation",
    "recurrent",
    "neural",
    "networks",
    "really",
    "designed",
    "help",
    "alleviate",
    "problem",
    "vanishing",
    "exploding",
    "gradients",
    "rather",
    "kind",
    "hacking",
    "top",
    "kind",
    "design",
    "architecture",
    "better",
    "gradient",
    "flow",
    "properties",
    "kind",
    "analogy",
    "fancier",
    "cnn",
    "architectures",
    "saw",
    "top",
    "lecture",
    "another",
    "thing",
    "point",
    "lstm",
    "cell",
    "actually",
    "comes",
    "idea",
    "lstm",
    "around",
    "quite",
    "folks",
    "working",
    "ideas",
    "way",
    "back",
    "90s",
    "definitely",
    "ahead",
    "curve",
    "models",
    "kind",
    "used",
    "everywhere",
    "20",
    "years",
    "later",
    "lstms",
    "kind",
    "funny",
    "functional",
    "form",
    "remember",
    "vanilla",
    "recurrent",
    "neural",
    "network",
    "hidden",
    "state",
    "used",
    "recurrence",
    "relation",
    "update",
    "hidden",
    "state",
    "every",
    "time",
    "step",
    "well",
    "lstm",
    "actually",
    "two",
    "maintain",
    "two",
    "hidden",
    "states",
    "every",
    "time",
    "step",
    "one",
    "h",
    "called",
    "hidden",
    "state",
    "kind",
    "analogy",
    "hidden",
    "state",
    "vanilla",
    "rnn",
    "lstm",
    "also",
    "maintains",
    "second",
    "vector",
    "c",
    "called",
    "cell",
    "state",
    "cell",
    "state",
    "vector",
    "kind",
    "internal",
    "kept",
    "inside",
    "lstm",
    "really",
    "get",
    "exposed",
    "outside",
    "world",
    "see",
    "kind",
    "see",
    "update",
    "equation",
    "see",
    "first",
    "compute",
    "take",
    "two",
    "inputs",
    "use",
    "compute",
    "four",
    "gates",
    "called",
    "f",
    "n",
    "use",
    "gates",
    "update",
    "cell",
    "states",
    "c",
    "expose",
    "part",
    "cell",
    "state",
    "hidden",
    "state",
    "next",
    "time",
    "step",
    "kind",
    "funny",
    "functional",
    "form",
    "want",
    "walk",
    "couple",
    "slides",
    "exactly",
    "use",
    "architecture",
    "make",
    "sense",
    "especially",
    "context",
    "vanishing",
    "exploding",
    "gradients",
    "first",
    "thing",
    "lstm",
    "given",
    "previous",
    "hidden",
    "state",
    "h",
    "given",
    "current",
    "input",
    "vector",
    "x",
    "like",
    "vanilla",
    "rnn",
    "vanilla",
    "rnn",
    "remember",
    "took",
    "two",
    "input",
    "vectors",
    "concatenated",
    "matrix",
    "multiply",
    "directly",
    "compute",
    "next",
    "hidden",
    "state",
    "rnn",
    "lstm",
    "something",
    "little",
    "bit",
    "different",
    "going",
    "take",
    "previous",
    "hidden",
    "state",
    "current",
    "input",
    "stack",
    "multiply",
    "big",
    "weight",
    "matrix",
    "w",
    "compute",
    "four",
    "different",
    "gates",
    "size",
    "hidden",
    "state",
    "sometimes",
    "see",
    "written",
    "different",
    "ways",
    "authors",
    "write",
    "different",
    "weight",
    "matrix",
    "gate",
    "authors",
    "combine",
    "one",
    "big",
    "weight",
    "matrix",
    "really",
    "thing",
    "ideas",
    "take",
    "hidden",
    "state",
    "current",
    "input",
    "use",
    "compute",
    "four",
    "gates",
    "four",
    "gates",
    "often",
    "see",
    "written",
    "f",
    "g",
    "ifog",
    "makes",
    "pretty",
    "easy",
    "remember",
    "input",
    "gate",
    "says",
    "much",
    "want",
    "input",
    "cell",
    "f",
    "forget",
    "gate",
    "much",
    "want",
    "forget",
    "cell",
    "memory",
    "previous",
    "previous",
    "time",
    "step",
    "output",
    "gate",
    "much",
    "want",
    "reveal",
    "ourself",
    "outside",
    "world",
    "g",
    "really",
    "nice",
    "name",
    "usually",
    "call",
    "gate",
    "gate",
    "g",
    "tells",
    "us",
    "much",
    "want",
    "write",
    "input",
    "cell",
    "notice",
    "four",
    "gates",
    "using",
    "different",
    "non",
    "linearity",
    "input",
    "forget",
    "output",
    "gate",
    "using",
    "sigmoids",
    "means",
    "values",
    "zero",
    "one",
    "whereas",
    "gate",
    "gate",
    "uses",
    "tanh",
    "means",
    "output",
    "minus",
    "one",
    "one",
    "kind",
    "weird",
    "makes",
    "little",
    "bit",
    "sense",
    "imagine",
    "binary",
    "values",
    "right",
    "like",
    "happens",
    "extremes",
    "two",
    "values",
    "kind",
    "happens",
    "look",
    "compute",
    "gates",
    "look",
    "next",
    "equation",
    "see",
    "cell",
    "state",
    "multiplied",
    "element",
    "wise",
    "forget",
    "gate",
    "sorry",
    "cell",
    "state",
    "previous",
    "time",
    "step",
    "multiplied",
    "element",
    "wise",
    "forget",
    "gate",
    "forget",
    "gate",
    "think",
    "vector",
    "zeros",
    "ones",
    "telling",
    "us",
    "element",
    "cell",
    "state",
    "want",
    "forget",
    "element",
    "cell",
    "case",
    "forget",
    "gate",
    "zero",
    "want",
    "remember",
    "element",
    "cell",
    "case",
    "forget",
    "gate",
    "one",
    "used",
    "forget",
    "gate",
    "gate",
    "part",
    "cell",
    "state",
    "second",
    "term",
    "element",
    "wise",
    "product",
    "vector",
    "zeros",
    "ones",
    "cause",
    "coming",
    "sigmoid",
    "telling",
    "us",
    "element",
    "cell",
    "state",
    "want",
    "write",
    "element",
    "cell",
    "state",
    "case",
    "one",
    "want",
    "write",
    "element",
    "cell",
    "state",
    "time",
    "step",
    "case",
    "zero",
    "gate",
    "gate",
    "coming",
    "tanh",
    "either",
    "one",
    "minus",
    "one",
    "value",
    "want",
    "candidate",
    "value",
    "might",
    "consider",
    "writing",
    "element",
    "cell",
    "state",
    "time",
    "step",
    "look",
    "cell",
    "state",
    "equation",
    "see",
    "every",
    "time",
    "step",
    "cell",
    "state",
    "kind",
    "different",
    "independent",
    "scaler",
    "values",
    "incremented",
    "decremented",
    "one",
    "kind",
    "like",
    "inside",
    "cell",
    "state",
    "either",
    "remember",
    "forget",
    "previous",
    "state",
    "either",
    "increment",
    "decrement",
    "element",
    "cell",
    "state",
    "one",
    "time",
    "step",
    "kind",
    "think",
    "elements",
    "cell",
    "state",
    "little",
    "scaler",
    "integer",
    "counters",
    "incremented",
    "decremented",
    "time",
    "step",
    "computed",
    "cell",
    "state",
    "use",
    "updated",
    "cell",
    "state",
    "compute",
    "hidden",
    "state",
    "reveal",
    "outside",
    "world",
    "cell",
    "state",
    "interpretation",
    "counters",
    "sort",
    "counting",
    "one",
    "minus",
    "one",
    "time",
    "step",
    "want",
    "squash",
    "counter",
    "value",
    "nice",
    "zero",
    "one",
    "range",
    "using",
    "tanh",
    "multiply",
    "element",
    "wise",
    "output",
    "gate",
    "output",
    "gate",
    "coming",
    "sigmoid",
    "think",
    "mostly",
    "zeros",
    "ones",
    "output",
    "gate",
    "tells",
    "us",
    "element",
    "cell",
    "state",
    "want",
    "reveal",
    "reveal",
    "element",
    "cell",
    "state",
    "computing",
    "external",
    "hidden",
    "state",
    "time",
    "step",
    "think",
    "kind",
    "tradition",
    "people",
    "trying",
    "explain",
    "lstms",
    "everyone",
    "needs",
    "come",
    "potentially",
    "confusing",
    "lstm",
    "diagram",
    "attempt",
    "see",
    "going",
    "inside",
    "lstm",
    "cell",
    "take",
    "taking",
    "input",
    "left",
    "previous",
    "cell",
    "state",
    "previous",
    "hidden",
    "state",
    "well",
    "current",
    "input",
    "x",
    "going",
    "take",
    "current",
    "previous",
    "hidden",
    "state",
    "well",
    "current",
    "input",
    "stack",
    "multiply",
    "weight",
    "matrix",
    "w",
    "produce",
    "four",
    "gates",
    "left",
    "non",
    "linearities",
    "saw",
    "previous",
    "slide",
    "forget",
    "gate",
    "multiplies",
    "element",
    "wise",
    "cell",
    "state",
    "input",
    "gate",
    "gate",
    "multiplied",
    "element",
    "wise",
    "added",
    "cell",
    "state",
    "gives",
    "us",
    "next",
    "cell",
    "next",
    "cell",
    "gets",
    "squashed",
    "tanh",
    "multiplied",
    "element",
    "wise",
    "output",
    "gate",
    "produce",
    "next",
    "hidden",
    "state",
    "question",
    "coming",
    "coming",
    "different",
    "parts",
    "weight",
    "matrix",
    "hidden",
    "x",
    "h",
    "dimension",
    "h",
    "stack",
    "vector",
    "size",
    "two",
    "h",
    "weight",
    "matrix",
    "matrix",
    "size",
    "four",
    "h",
    "times",
    "two",
    "think",
    "sort",
    "four",
    "chunks",
    "weight",
    "matrix",
    "four",
    "chunks",
    "weight",
    "matrix",
    "going",
    "compute",
    "different",
    "one",
    "gates",
    "often",
    "see",
    "written",
    "clarity",
    "kind",
    "combining",
    "four",
    "different",
    "weight",
    "matrices",
    "single",
    "large",
    "matrix",
    "w",
    "notational",
    "convenience",
    "computed",
    "using",
    "different",
    "parts",
    "weight",
    "matrix",
    "correct",
    "computed",
    "using",
    "functional",
    "form",
    "stacking",
    "two",
    "things",
    "taking",
    "matrix",
    "multiplication",
    "picture",
    "think",
    "happens",
    "lstm",
    "cell",
    "backwards",
    "pass",
    "saw",
    "context",
    "vanilla",
    "recurrent",
    "neural",
    "network",
    "bad",
    "things",
    "happened",
    "backwards",
    "pass",
    "continually",
    "multiplying",
    "weight",
    "matrix",
    "situation",
    "looks",
    "much",
    "quite",
    "bit",
    "different",
    "lstm",
    "imagine",
    "path",
    "backwards",
    "computing",
    "gradients",
    "cell",
    "state",
    "get",
    "quite",
    "nice",
    "picture",
    "upstream",
    "gradient",
    "cell",
    "coming",
    "backpropagate",
    "backwards",
    "addition",
    "operation",
    "remember",
    "addition",
    "copies",
    "upstream",
    "gradient",
    "two",
    "branches",
    "upstream",
    "gradient",
    "gets",
    "copied",
    "directly",
    "passed",
    "directly",
    "backpropagating",
    "element",
    "wise",
    "multiply",
    "upstream",
    "gradient",
    "ends",
    "getting",
    "multiplied",
    "element",
    "wise",
    "forget",
    "gate",
    "backpropagate",
    "backwards",
    "cell",
    "state",
    "thing",
    "happens",
    "upstream",
    "cell",
    "state",
    "gradient",
    "ends",
    "getting",
    "multiplied",
    "element",
    "wise",
    "forget",
    "gate",
    "really",
    "lot",
    "nicer",
    "vanilla",
    "rnn",
    "two",
    "reasons",
    "one",
    "forget",
    "gate",
    "element",
    "wise",
    "multiplication",
    "rather",
    "full",
    "matrix",
    "multiplication",
    "element",
    "wise",
    "multiplication",
    "going",
    "little",
    "bit",
    "nicer",
    "full",
    "matrix",
    "multiplication",
    "second",
    "element",
    "wise",
    "multiplication",
    "potentially",
    "multiplying",
    "different",
    "forget",
    "gate",
    "every",
    "time",
    "step",
    "remember",
    "vanilla",
    "rnn",
    "continually",
    "multiplying",
    "weight",
    "matrix",
    "led",
    "explicitly",
    "exploding",
    "vanishing",
    "gradients",
    "lstm",
    "case",
    "forget",
    "gate",
    "vary",
    "time",
    "step",
    "much",
    "easier",
    "model",
    "avoid",
    "problems",
    "exploding",
    "vanishing",
    "gradients",
    "finally",
    "forget",
    "gate",
    "coming",
    "sigmoid",
    "element",
    "wise",
    "multiply",
    "guaranteed",
    "zero",
    "one",
    "leads",
    "sort",
    "nicer",
    "numerical",
    "properties",
    "imagine",
    "multiplying",
    "things",
    "another",
    "thing",
    "notice",
    "context",
    "vanilla",
    "recurrent",
    "neural",
    "network",
    "saw",
    "backward",
    "pass",
    "gradients",
    "flowing",
    "also",
    "tanh",
    "every",
    "time",
    "step",
    "lstm",
    "outputs",
    "lstm",
    "hidden",
    "state",
    "used",
    "compute",
    "outputs",
    "hidden",
    "state",
    "imagine",
    "backpropagating",
    "final",
    "hidden",
    "state",
    "back",
    "first",
    "cell",
    "state",
    "backward",
    "path",
    "backpropagate",
    "single",
    "tanh",
    "non",
    "linearity",
    "rather",
    "separate",
    "tanh",
    "every",
    "time",
    "step",
    "kind",
    "put",
    "things",
    "together",
    "see",
    "backwards",
    "pass",
    "backpropagating",
    "cell",
    "state",
    "kind",
    "gradient",
    "super",
    "highway",
    "lets",
    "gradients",
    "pass",
    "relatively",
    "unimpeded",
    "loss",
    "end",
    "model",
    "way",
    "back",
    "initial",
    "cell",
    "state",
    "beginning",
    "model",
    "question",
    "yeah",
    "gradient",
    "respect",
    "w",
    "ultimately",
    "thing",
    "care",
    "gradient",
    "respect",
    "w",
    "come",
    "every",
    "time",
    "step",
    "take",
    "current",
    "cell",
    "state",
    "well",
    "current",
    "hidden",
    "state",
    "give",
    "us",
    "element",
    "give",
    "us",
    "local",
    "gradient",
    "w",
    "time",
    "step",
    "cell",
    "state",
    "vanilla",
    "rnn",
    "case",
    "end",
    "adding",
    "first",
    "time",
    "step",
    "w",
    "gradients",
    "compute",
    "final",
    "gradient",
    "imagine",
    "situation",
    "long",
    "sequence",
    "getting",
    "gradients",
    "end",
    "sequence",
    "backpropagate",
    "get",
    "local",
    "gradient",
    "w",
    "time",
    "step",
    "local",
    "gradient",
    "w",
    "coming",
    "gradients",
    "c",
    "maintaining",
    "gradients",
    "c",
    "much",
    "nicely",
    "lstm",
    "case",
    "local",
    "gradients",
    "w",
    "time",
    "step",
    "also",
    "carried",
    "forward",
    "backward",
    "time",
    "much",
    "cleanly",
    "another",
    "question",
    "yeah",
    "question",
    "due",
    "non",
    "linearities",
    "could",
    "still",
    "susceptible",
    "vanishing",
    "gradients",
    "could",
    "case",
    "actually",
    "one",
    "problem",
    "might",
    "imagine",
    "maybe",
    "forget",
    "gates",
    "always",
    "less",
    "zero",
    "always",
    "less",
    "one",
    "might",
    "get",
    "vanishing",
    "gradients",
    "continually",
    "go",
    "forget",
    "gates",
    "well",
    "one",
    "sort",
    "trick",
    "people",
    "practice",
    "sometimes",
    "initialize",
    "biases",
    "forget",
    "gate",
    "somewhat",
    "positive",
    "beginning",
    "training",
    "forget",
    "gates",
    "always",
    "close",
    "one",
    "least",
    "beginning",
    "training",
    "relatively",
    "clean",
    "gradient",
    "flow",
    "forget",
    "gates",
    "since",
    "initialized",
    "near",
    "one",
    "throughout",
    "course",
    "training",
    "model",
    "learn",
    "biases",
    "kind",
    "learn",
    "forget",
    "needs",
    "right",
    "still",
    "could",
    "potential",
    "vanishing",
    "gradients",
    "much",
    "less",
    "extreme",
    "vanilla",
    "rnn",
    "case",
    "fs",
    "vary",
    "time",
    "step",
    "also",
    "element",
    "wise",
    "multiplication",
    "rather",
    "full",
    "matrix",
    "multiplication",
    "see",
    "lstm",
    "actually",
    "looks",
    "quite",
    "similar",
    "resnet",
    "residual",
    "network",
    "path",
    "identity",
    "connections",
    "going",
    "backward",
    "network",
    "gave",
    "sort",
    "gradient",
    "super",
    "highway",
    "gradients",
    "flow",
    "backward",
    "resnet",
    "kind",
    "intuition",
    "lstm",
    "additive",
    "element",
    "wise",
    "multiplicative",
    "interactions",
    "cell",
    "state",
    "give",
    "similar",
    "gradient",
    "super",
    "highway",
    "gradients",
    "flow",
    "backwards",
    "cell",
    "state",
    "lstm",
    "way",
    "kind",
    "nice",
    "paper",
    "called",
    "highway",
    "networks",
    "kind",
    "idea",
    "lstm",
    "cell",
    "residual",
    "networks",
    "highway",
    "networks",
    "actually",
    "came",
    "residual",
    "networks",
    "idea",
    "every",
    "layer",
    "highway",
    "network",
    "going",
    "compute",
    "sort",
    "candidate",
    "activation",
    "well",
    "gating",
    "function",
    "tells",
    "us",
    "interprelates",
    "previous",
    "input",
    "layer",
    "candidate",
    "activation",
    "came",
    "convolutions",
    "actually",
    "lot",
    "architectural",
    "similarities",
    "things",
    "people",
    "take",
    "lot",
    "inspiration",
    "training",
    "deep",
    "cnns",
    "deep",
    "rnns",
    "lot",
    "crossover",
    "briefly",
    "see",
    "lot",
    "types",
    "variance",
    "recurrent",
    "neural",
    "network",
    "architectures",
    "wild",
    "probably",
    "common",
    "apart",
    "lstm",
    "gru",
    "called",
    "gated",
    "recurrent",
    "unit",
    "see",
    "update",
    "equations",
    "kind",
    "similar",
    "flavor",
    "lstm",
    "uses",
    "multiplicative",
    "element",
    "wise",
    "gates",
    "together",
    "additive",
    "interactions",
    "avoid",
    "vanishing",
    "gradient",
    "problem",
    "also",
    "cool",
    "paper",
    "called",
    "lstm",
    "search",
    "based",
    "oddysey",
    "inventive",
    "title",
    "tried",
    "play",
    "around",
    "lstm",
    "equations",
    "swap",
    "non",
    "linearities",
    "one",
    "point",
    "like",
    "really",
    "need",
    "tanh",
    "exposing",
    "output",
    "gate",
    "tried",
    "answer",
    "lot",
    "different",
    "questions",
    "non",
    "linearities",
    "pieces",
    "lstm",
    "update",
    "equations",
    "happens",
    "change",
    "model",
    "tweak",
    "lstm",
    "equations",
    "little",
    "bit",
    "kind",
    "conclusion",
    "work",
    "work",
    "little",
    "bit",
    "better",
    "others",
    "one",
    "problem",
    "another",
    "generally",
    "none",
    "things",
    "none",
    "tweaks",
    "lstm",
    "tried",
    "significantly",
    "better",
    "original",
    "lstm",
    "problems",
    "gives",
    "little",
    "bit",
    "faith",
    "lstm",
    "update",
    "equations",
    "seem",
    "kind",
    "magical",
    "useful",
    "anyway",
    "probably",
    "consider",
    "problem",
    "also",
    "cool",
    "paper",
    "google",
    "couple",
    "years",
    "ago",
    "tried",
    "use",
    "kind",
    "evolutionary",
    "search",
    "search",
    "many",
    "large",
    "number",
    "random",
    "rnn",
    "architectures",
    "kind",
    "randomly",
    "premute",
    "update",
    "equations",
    "try",
    "putting",
    "additions",
    "multiplications",
    "gates",
    "non",
    "linearities",
    "different",
    "kinds",
    "combinations",
    "blasted",
    "huge",
    "google",
    "cluster",
    "tried",
    "whole",
    "bunch",
    "different",
    "weigh",
    "updates",
    "various",
    "flavors",
    "story",
    "really",
    "find",
    "anything",
    "significantly",
    "better",
    "existing",
    "gru",
    "lstm",
    "styles",
    "although",
    "variations",
    "worked",
    "maybe",
    "slightly",
    "better",
    "worse",
    "certain",
    "problems",
    "kind",
    "take",
    "away",
    "probably",
    "using",
    "lstm",
    "gru",
    "much",
    "magic",
    "equations",
    "idea",
    "managing",
    "gradient",
    "flow",
    "properly",
    "additive",
    "connections",
    "multiplicative",
    "gates",
    "super",
    "useful",
    "yeah",
    "summary",
    "rnns",
    "super",
    "cool",
    "allow",
    "attack",
    "tons",
    "new",
    "types",
    "problems",
    "sometimes",
    "susceptible",
    "vanishing",
    "exploding",
    "gradients",
    "address",
    "weight",
    "clipping",
    "fancier",
    "architectures",
    "lot",
    "cool",
    "overlap",
    "cnn",
    "architectures",
    "rnn",
    "architectures",
    "next",
    "time",
    "taking",
    "midterm",
    "sorry",
    "question",
    "midterm",
    "lecture",
    "anything",
    "point",
    "fair",
    "game",
    "guys",
    "good",
    "luck",
    "midterm",
    "tuesday"
  ],
  "keywords": [
    "sorry",
    "bit",
    "today",
    "first",
    "time",
    "trying",
    "use",
    "new",
    "working",
    "last",
    "lecture",
    "recurrent",
    "neural",
    "networks",
    "couple",
    "one",
    "probably",
    "get",
    "two",
    "remember",
    "quite",
    "start",
    "also",
    "another",
    "class",
    "kind",
    "look",
    "around",
    "actually",
    "exactly",
    "go",
    "next",
    "sort",
    "thing",
    "play",
    "training",
    "cool",
    "based",
    "train",
    "parameters",
    "really",
    "way",
    "practice",
    "think",
    "useful",
    "little",
    "work",
    "different",
    "types",
    "data",
    "give",
    "end",
    "cnn",
    "architectures",
    "line",
    "image",
    "classification",
    "saw",
    "alexnet",
    "architecture",
    "layer",
    "convolutional",
    "network",
    "well",
    "whole",
    "deep",
    "learning",
    "vision",
    "lot",
    "models",
    "years",
    "interesting",
    "much",
    "model",
    "right",
    "batch",
    "relatively",
    "layers",
    "order",
    "converge",
    "trained",
    "could",
    "process",
    "cause",
    "gradient",
    "need",
    "slightly",
    "called",
    "residual",
    "blocks",
    "going",
    "take",
    "input",
    "pass",
    "add",
    "output",
    "nice",
    "set",
    "weights",
    "zero",
    "addition",
    "context",
    "put",
    "maybe",
    "make",
    "sense",
    "flow",
    "backward",
    "happens",
    "gates",
    "upstream",
    "coming",
    "gate",
    "imagine",
    "many",
    "top",
    "ends",
    "potentially",
    "super",
    "highway",
    "gradients",
    "entire",
    "things",
    "even",
    "idea",
    "like",
    "inside",
    "backwards",
    "loss",
    "something",
    "seen",
    "see",
    "forward",
    "number",
    "come",
    "fully",
    "connected",
    "final",
    "six",
    "vector",
    "weight",
    "matrix",
    "needs",
    "times",
    "multiply",
    "single",
    "large",
    "far",
    "vanilla",
    "feed",
    "receive",
    "fixed",
    "size",
    "hidden",
    "produces",
    "scores",
    "want",
    "call",
    "sequence",
    "variable",
    "length",
    "caption",
    "captions",
    "might",
    "words",
    "variably",
    "sized",
    "text",
    "taking",
    "video",
    "read",
    "decision",
    "problems",
    "inputs",
    "sentence",
    "sequences",
    "element",
    "making",
    "every",
    "general",
    "us",
    "pretty",
    "example",
    "rather",
    "looking",
    "parts",
    "makes",
    "though",
    "outputs",
    "paper",
    "generating",
    "images",
    "compute",
    "rnns",
    "cell",
    "x",
    "rnn",
    "state",
    "back",
    "produce",
    "step",
    "update",
    "question",
    "functional",
    "form",
    "function",
    "f",
    "previous",
    "h",
    "current",
    "w",
    "states",
    "together",
    "tanh",
    "non",
    "lstm",
    "predictions",
    "sometimes",
    "graph",
    "steps",
    "usually",
    "repeat",
    "write",
    "talked",
    "propagation",
    "soft",
    "case",
    "scaler",
    "respect",
    "situation",
    "would",
    "typically",
    "somehow",
    "using",
    "second",
    "language",
    "word",
    "problem",
    "natural",
    "character",
    "level",
    "characters",
    "predict",
    "vocabulary",
    "four",
    "e",
    "letter",
    "figure",
    "elements",
    "test",
    "sample",
    "looks",
    "similar",
    "distribution",
    "probability",
    "bad",
    "yeah",
    "instead",
    "people",
    "backpropagate",
    "code",
    "stuff",
    "used",
    "course",
    "knows",
    "found",
    "algebraic",
    "topology",
    "source",
    "equations",
    "blah",
    "tried",
    "try",
    "value",
    "vectors",
    "better",
    "part",
    "attention",
    "multiplication",
    "multiplying",
    "exploding",
    "vanishing",
    "forget",
    "multiplied",
    "wise"
  ]
}