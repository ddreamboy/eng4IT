{
  "text": "welcome to wide world programming where\nwe simplify programming for you with\neasy to understand by code videos and\ntoday I'll be giving you a brief\nexplanation of all machine learning\nmodels so let's get started\nbroadly speaking all machine learning\nmodels can be categorized as supervised\nor unsupervised we'll uncovered each one\nof them and what all types they have\n[Music]\nnumber one supervised learning it\ninvolves a series of function that map's\nan input to an output based on a series\nof example input-output pairs for\nexample if we have a data set of two\nvariables one being age which is the\ninput and other being the shoe size as\noutput we could implement a supervised\nlearning models to predict the shoe size\nof a person based on their age further\nwith supervised learning there are two\nsub categories one is regression and\nother is classification\nin relation model we find a target value\nbased on independent predictors that\nmeans you can use this to find\nrelationship between a dependent\nvariable and an independent variable in\nregression models the output is\ncontinuous some of the most common types\nof resistant model include number one\nlinear regression which is simply\nfinding a line that fits the data its\nextensions include multiple linear\nregression that is finding a plane of\nbest fit and polynomial regression that\nis finding a curve for best fit next one\ndecision tree it looks something like\nthis where each square above is called a\nnode and the more nodes you have the\nmore accurate your decision tree will be\nin general next and the third type\nrandom forest\nthese are assemble learning techniques\nthat builds off over decision trees and\ninvolve creating multiple decision trees\nusing bootstrap data sets of original\ndata and randomly selecting a subset of\nvariables at each step of the decision\ntree the model then selects the mode of\nall the predictions of each decision\ntrees and by relying on the majority\nwinds model it reduces the risk of error\nfrom individual tree next neural network\nit is quite popular and is a multi\nlayered model inspired by human minds\nlike the neurons in our brain the circle\nrepresents a node the blue circle\nrepresents an input layer the black\ncircle represents a hidden layer and the\ngreen circle represents the output layer\neach node in the hidden layer represents\na function that input goes through\nultimately leading to the output in the\ngreen circles\nnext classification so with regression\ntypes being over now let's jump to\nclassification so in classification the\noutput is discrete some of the most\ncommon types of classification models\ninclude first logistic regression which\nis similar to linear regression but is\nused to model the probability of a\nfinite number of outcomes typically two\nnext support vector machine it is a\nsupervised classification technique that\ncarries an objective to find a hyper\nlane in n-dimensional space that can\ndistinctly classify the data points next\nnavies it's a classifier which acts as a\nprobabilistic machine learning model\nused for classification tasks the crux\nof the classifier is based on the Bayes\ntheorem coming up next\ndecision trees random forests and neural\nnetworks these models follow the same\nlogic as previously explained the only\ndifference here is that the output is\ndiscrete rather than continuous now next\nlet's jump over to unsupervised learning\nunlike supervised learning unsupervised\nlearning is used to draw inferences and\nfind patterns from input data without\nreferences to the labeled outcome two\nmain methods used in supervised learning\ninclude clustering and dimensionality\nreduction clustering involves grouping\nof data points it's frequently used for\ncustomer segmentation fraud detection\nand document classification common\nclustering techniques include k-means\nclustering hierarchical clustering means\nshape clustering and density based\nclustering while each technique has\ndifferent methods in finding clusters\nthey all aim to achieve the same thing\ncoming up next dimensionality reduction\nit is a process of reducing dimensions\nof your feature set Auto States simply\nreducing the number of features most\ndimensionality reduction techniques can\nbe categorized as either feature\nelimination or feature extraction a\npopular method of dimensionality\nreduction is called principal component\nanalysis or PCA obviously there's a ton\nof complexity if we dive into any\nparticular model to help you with each I\nwill be publishing new videos so be sure\nto smash that subscribe button to be\nnotified on every upload next if this\nvideo helped you be sure to like it and\nshare it with someone who might need it\n[Music]\n",
  "words": [
    "welcome",
    "wide",
    "world",
    "programming",
    "simplify",
    "programming",
    "easy",
    "understand",
    "code",
    "videos",
    "today",
    "giving",
    "brief",
    "explanation",
    "machine",
    "learning",
    "models",
    "let",
    "get",
    "started",
    "broadly",
    "speaking",
    "machine",
    "learning",
    "models",
    "categorized",
    "supervised",
    "unsupervised",
    "uncovered",
    "one",
    "types",
    "music",
    "number",
    "one",
    "supervised",
    "learning",
    "involves",
    "series",
    "function",
    "map",
    "input",
    "output",
    "based",
    "series",
    "example",
    "pairs",
    "example",
    "data",
    "set",
    "two",
    "variables",
    "one",
    "age",
    "input",
    "shoe",
    "size",
    "output",
    "could",
    "implement",
    "supervised",
    "learning",
    "models",
    "predict",
    "shoe",
    "size",
    "person",
    "based",
    "age",
    "supervised",
    "learning",
    "two",
    "sub",
    "categories",
    "one",
    "regression",
    "classification",
    "relation",
    "model",
    "find",
    "target",
    "value",
    "based",
    "independent",
    "predictors",
    "means",
    "use",
    "find",
    "relationship",
    "dependent",
    "variable",
    "independent",
    "variable",
    "regression",
    "models",
    "output",
    "continuous",
    "common",
    "types",
    "resistant",
    "model",
    "include",
    "number",
    "one",
    "linear",
    "regression",
    "simply",
    "finding",
    "line",
    "fits",
    "data",
    "extensions",
    "include",
    "multiple",
    "linear",
    "regression",
    "finding",
    "plane",
    "best",
    "fit",
    "polynomial",
    "regression",
    "finding",
    "curve",
    "best",
    "fit",
    "next",
    "one",
    "decision",
    "tree",
    "looks",
    "something",
    "like",
    "square",
    "called",
    "node",
    "nodes",
    "accurate",
    "decision",
    "tree",
    "general",
    "next",
    "third",
    "type",
    "random",
    "forest",
    "assemble",
    "learning",
    "techniques",
    "builds",
    "decision",
    "trees",
    "involve",
    "creating",
    "multiple",
    "decision",
    "trees",
    "using",
    "bootstrap",
    "data",
    "sets",
    "original",
    "data",
    "randomly",
    "selecting",
    "subset",
    "variables",
    "step",
    "decision",
    "tree",
    "model",
    "selects",
    "mode",
    "predictions",
    "decision",
    "trees",
    "relying",
    "majority",
    "winds",
    "model",
    "reduces",
    "risk",
    "error",
    "individual",
    "tree",
    "next",
    "neural",
    "network",
    "quite",
    "popular",
    "multi",
    "layered",
    "model",
    "inspired",
    "human",
    "minds",
    "like",
    "neurons",
    "brain",
    "circle",
    "represents",
    "node",
    "blue",
    "circle",
    "represents",
    "input",
    "layer",
    "black",
    "circle",
    "represents",
    "hidden",
    "layer",
    "green",
    "circle",
    "represents",
    "output",
    "layer",
    "node",
    "hidden",
    "layer",
    "represents",
    "function",
    "input",
    "goes",
    "ultimately",
    "leading",
    "output",
    "green",
    "circles",
    "next",
    "classification",
    "regression",
    "types",
    "let",
    "jump",
    "classification",
    "classification",
    "output",
    "discrete",
    "common",
    "types",
    "classification",
    "models",
    "include",
    "first",
    "logistic",
    "regression",
    "similar",
    "linear",
    "regression",
    "used",
    "model",
    "probability",
    "finite",
    "number",
    "outcomes",
    "typically",
    "two",
    "next",
    "support",
    "vector",
    "machine",
    "supervised",
    "classification",
    "technique",
    "carries",
    "objective",
    "find",
    "hyper",
    "lane",
    "space",
    "distinctly",
    "classify",
    "data",
    "points",
    "next",
    "navies",
    "classifier",
    "acts",
    "probabilistic",
    "machine",
    "learning",
    "model",
    "used",
    "classification",
    "tasks",
    "crux",
    "classifier",
    "based",
    "bayes",
    "theorem",
    "coming",
    "next",
    "decision",
    "trees",
    "random",
    "forests",
    "neural",
    "networks",
    "models",
    "follow",
    "logic",
    "previously",
    "explained",
    "difference",
    "output",
    "discrete",
    "rather",
    "continuous",
    "next",
    "let",
    "jump",
    "unsupervised",
    "learning",
    "unlike",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "used",
    "draw",
    "inferences",
    "find",
    "patterns",
    "input",
    "data",
    "without",
    "references",
    "labeled",
    "outcome",
    "two",
    "main",
    "methods",
    "used",
    "supervised",
    "learning",
    "include",
    "clustering",
    "dimensionality",
    "reduction",
    "clustering",
    "involves",
    "grouping",
    "data",
    "points",
    "frequently",
    "used",
    "customer",
    "segmentation",
    "fraud",
    "detection",
    "document",
    "classification",
    "common",
    "clustering",
    "techniques",
    "include",
    "clustering",
    "hierarchical",
    "clustering",
    "means",
    "shape",
    "clustering",
    "density",
    "based",
    "clustering",
    "technique",
    "different",
    "methods",
    "finding",
    "clusters",
    "aim",
    "achieve",
    "thing",
    "coming",
    "next",
    "dimensionality",
    "reduction",
    "process",
    "reducing",
    "dimensions",
    "feature",
    "set",
    "auto",
    "states",
    "simply",
    "reducing",
    "number",
    "features",
    "dimensionality",
    "reduction",
    "techniques",
    "categorized",
    "either",
    "feature",
    "elimination",
    "feature",
    "extraction",
    "popular",
    "method",
    "dimensionality",
    "reduction",
    "called",
    "principal",
    "component",
    "analysis",
    "pca",
    "obviously",
    "ton",
    "complexity",
    "dive",
    "particular",
    "model",
    "help",
    "publishing",
    "new",
    "videos",
    "sure",
    "smash",
    "subscribe",
    "button",
    "notified",
    "every",
    "upload",
    "next",
    "video",
    "helped",
    "sure",
    "like",
    "share",
    "someone",
    "might",
    "need",
    "music"
  ],
  "keywords": [
    "programming",
    "videos",
    "machine",
    "learning",
    "models",
    "let",
    "categorized",
    "supervised",
    "unsupervised",
    "one",
    "types",
    "music",
    "number",
    "involves",
    "series",
    "function",
    "input",
    "output",
    "based",
    "example",
    "data",
    "set",
    "two",
    "variables",
    "age",
    "shoe",
    "size",
    "regression",
    "classification",
    "model",
    "find",
    "independent",
    "means",
    "variable",
    "continuous",
    "common",
    "include",
    "linear",
    "simply",
    "finding",
    "multiple",
    "best",
    "fit",
    "next",
    "decision",
    "tree",
    "like",
    "called",
    "node",
    "random",
    "techniques",
    "trees",
    "neural",
    "popular",
    "circle",
    "represents",
    "layer",
    "hidden",
    "green",
    "jump",
    "discrete",
    "used",
    "technique",
    "points",
    "classifier",
    "coming",
    "methods",
    "clustering",
    "dimensionality",
    "reduction",
    "reducing",
    "feature",
    "sure"
  ]
}