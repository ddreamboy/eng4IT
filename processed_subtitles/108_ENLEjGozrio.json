{
  "text": "hello guys so I'm quite excited to start\nthe NLP series that is natural language\nprocessing for machine learning in this\nvideo we are going to see the entire\nroad map like how you should go ahead\nand prepare for natural language\nprocessing and NLP is an amazing domain\namazing Tech part with respect to\nmachine learning or deep learning a lot\nof research is basically happening in\nnatural language processing itself so\nlet me go ahead and let me just share my\nscreen and uh let me go ahead and let me\nexplain about the road map like how you\nshould basically go ahead with the\npreparation so uh to begin with uh till\nnow probably if you know machine\nlearning okay so in machine learning you\nhave actually seen right we solve two\ndifferent kind of problems one is\nsupervised that is supervised and the\nother one that we basically solve is\nsomething called as unsupervised machine\nlearning now in most of the supervis\nmachine learning use cases where we\nspecifically solve two different kind of\nproblem statements like classification\nand regression right in all this problem\nstatement right what we have seen is\nthat let's say if we have some specific\nset of features like F1 FS2 F3 F4 like\nthis it can be any number of features\nthese features are usually called as\nindependent features right so if I\nprobably talk about this these are my\ninput features or I can also say this as\nmy independent features\nindependent features right and similarly\nI also have my output feature which is\nmy dependent feature right which is my\ndependent\nfeature so what is our aim in a supervis\nmachine learning model is that with\nrespect to this features we obviously\nwe'll be having lot of data points in\nthe output it can be a classification\nproblem or a regression problem and over\nhere I may have continuous values or I\nmay have uh classified points like ones\nand zeros it can be binary it can be\nmulticlass okay so let's say if I have\nthis kind of data points over here what\nis our main aim we usually create a\nmodel okay we usually create a model and\nwe train the model with this data right\nso we basically create a model and we\ntrain this particular model with this\nspecific data set right with this\nspecific data set then our model will be\nable to make or will be capable to make\nsome predictions whenever I give this\nkind of input data now in machine\nlearning specifically if I talk about\nthis features right now F1 can be a\ncontinuous features FS2 can basically be\na categorical features and it can also\nbe different types of features over here\nand during this particular scenario\nlet's say that some of our features are\ncompletely made up of text let's say one\nbasic example that I really want to give\nis about spam class classification let's\nsay this is my example over here spam\nclassification now in spam\nclassification what all features we may\nhave we may have let's say uh my main\naim is to basically detect whether a\nemail that comes to me is a\nspam or not spam okay so let's say that\nthis is a classification problem that I\nreally want to solve now in this\nscenario some of the features that I may\nhave one feature is that I may have\nsomething called as email subject\nokay I may have the next feature as\nemail\nbody and my output feature is basically\nwhether this mail is Spam or ham ham\nbasically means not spam so let's say\nover here I'll give you one example\nlet's say the email subject is like\nbillion so here you can see that it is a\ncompletely a text right email body it\ncan be like you wanton a lottery\nof okay I'm just giving you an example\nyou won a lottery of billion dollar and\nI think you get this kind of emails\nright now obviously when you get this\nemail u in real world scenario we will\nbe classifying these particular points\nand this will basically be a Spam so\nI'll put a category as one or I can also\nput it as spam\nitself now here one thing that you can\nnotice over here is that whenever in our\ninput features we have we we have a\ncontinuous variable and we have a\ncategorical variables and obviously we\nhave different different techniques to\nconvert this categorical variables into\ncontinuous values right uh there are\ntechniques like one hot encoding there\nare techniques like Target encoding\nordinal encoding all those techniques\nare there which we basically do it in\nfeature engineering but let's say that\nif my entire data is a text or a\nsentence like this all right in this\nparticular scenario I will definitely it\nwill not be that easy for a model to\nunderstand right obviously because the\nmodel cannot understand human language\nright right now I have written in\nEnglish tomorrow it can be in Chinese\nthere after tomorrow it can be in some\nother languages so model is not directly\ncapable of understanding this particular\ntext so what should we do in this\nparticular scenario so we have\ntechniques we have techniques where we\ncan convert this all text into some\nmeaningful\nvectors meaningful vectors now what are\nvectors vectors are just like numbers\nonly but understand those vectors\nrepresent some meaningful information\nwith respect to this particular text\nokay and whenever your input data is in\nform of text or sentences we basically\nuse something called as natural language\nprocessing so that we'll be able to\nprocess this particular data and we'll\nbe able to make the model understand to\nsolve use cases like spam classification\nright so this is the entire context\nbehind NLP and why it is so much popular\nbecause nowadays you see lot of examples\nAlexa right I think many people use\nAlexa many people use Google home\nright many people use some automated\ndevice like let's say the AC is running\nright and you say hey switch off the AC\nswitch on the AC how that particular\nmachine is able to understand that it is\nall because of natural language\nprocessing Google is extensively doing\nsome amazing research with respect to\nNLP and they're coming up with some\namazing things yes we will be learning\nuh both with respect to machine learning\nand as we go we' also try to learn with\nrespect to deep learning but just try to\nunderstand in this video we are going to\nunderstand the road map like how we\nshould go ahead and prepare with respect\nto the NLP so again I'm going to write\nthe road map of\nNLP okay and again we are also going to\nuse different different\nlibraries I will just going to draw a\npyramid kind of structure and we will go\nin the bottom to top approach okay so\nlet's go ahead and let's try to\nunderstand what should be the road map\nof NLP initially to begin with initially\nto begin with I'm just going to create a\nsmall\nblock okay to begin with this first\nblock is\nbasically initially you need to know one\nprogramming language so let's say that I\nam going to probably go ahead with\npython programming language super\nimportant right with the help of Python\nprogramming language we'll obviously be\nable to solve lot of use cases of NLP\nbut when I go to step one so this is\nbasically the step one the step one is\nnothing but it is basically called as\ntext\npreprocessing and this text\npre-processing initially will start with\nsome basic things like whenever we have\nthis kind of text Data what are the text\npre-processing things we need to do how\nwe can Bic basically clean that\nparticular text Data all those things\nwill basically come in text\npre-processing okay the techniques that\nwe are probably going to apply in this\nis like\ntokenization tokenization is a concept\nwherein you convert a paragraph into a\nsentence a sentence into a words\ndifferent different things we also going\nto learn techniques like\nclimatization we're going to learn\ntechniques like stemming we also going\nto introduce to words like something\ncalled as stop words all these things\nwill basically be come covering in the\ntext pre-processing part one so this is\nbasically the step one okay so I'm just\ngoing to write this as step one super\nimportant and initially we'll be\nstarting with this okay and again uh the\nentire detailed syllabus obviously I'm\ngoing to make video by video to make you\nunderstand each and everything but on\nthe thousand feet overview I'm just\ngiving you this there are a lot of\ntopics inside this which we really need\nto be familiar with now coming to the\nsecond one okay so the second one we\nbasically say it as text pre-processing\nstep two okay so again this is also uh\ntext pre-processing technique but we\nlittle bit we we try to increase the\ncomplexity of it and it tries to solve\nmore problems so here in the text\npre-processing to here we focus on\nconverting the text Data into vectors so\nhere I'm just going to write it down\nthis is my step two here I'll write or I\ncan basically text pre-processing part\ntwo here we are going to learn topics\nlike bag of\nwords okay TF\nIDF we also going to learn things like\nunigrams byrams there a lot of Concepts\nlike this which we are going to cover it\nagain this is again a text\npre-processing technique but again\nunderstand what is the main aim the main\naim is basically to\nconvert or let me just write like this\nstep one and step two instead of writing\nlike like this I will just write it in a\nsimpler way okay so here I'm we are\nbasically focusing\non cleaning the text right cleaning the\ninput cleaning the input in this\nparticular step we are trying to focus\non converting our input\ntext to vectors and this is a super\nimportant step because this Vector\nshould be able to make sure that the\ncontext of the statement should be able\nto get captured right so at the end of\nthe day in NLP with whatever techniques\nright now it is there like a\nTransformers birds are there which is\nquite Advanced Techniques if you are\nable to convert this input text into\nsome meaningful vectors you'll be able\nto solve those particular use cases in a\nbetter manner right so this is basically\nthe Second Step where we focus on uh\nconverting the input text into vectors\nstill there are more Advanced Techniques\nof text pre-processing which I will go\nwith the third step and here we focus\non here I'm just going to write it as\ntext\npreprocessing with respect to part three\nokay and here what all things we\nbasically focus on we use more Advanced\nTechniques which is like word\nto word to W average word to\nW and this is also a technique to\nconvert the input text into\nvectors now you may be thinking Krish in\nthe Second Step also you have written\nthe same thing in the third step also\nyou have written same thing yes guys\nunderstand as we go from the second step\nto the third step this conversion of the\ninput text to vectors it is better than\nthe approach that we basically use in B\nthat is bag of words tfidf unigram\nbyrams right but as a learner learner we\nreally need to know all these particular\nsteps right now over here we focus more\non techniques like word to we and\naverage word to here the uh word to and\naverage word to is again a kind of a\ndeep learning technique but uh we will\ntry to learn this and we'll try to\nunderstand how it basically happens okay\nnow coming to the next step uh that we\nfocus on right if we continue\nunderstanding the entire road map here\nwe also focus on understanding RNN lstm\nRNN\nGru okay Gru RNN now again guys this is\na deep learning technique which is\nbasically used for handling or solving\ntext related use cases like spam\nclassification text summarization and\nmany more things so again over here\nthese are some neural networks you\nshould be familiar with in the road map\nbefore uh basically when we enter into\nthe deep learning part this is super\nimportant to understand and again uh as\nI said this is a part of deep learning\ntechnique okay but since I'm writing\nabout the road map I really need to\nmention about all those things now\ncoming to the next one uh there's also a\ntechnique which is called as word\nembedding okay so this is an also an\namazing way to convert input text into\nvectors internally if I talk about so\nthis particular text preprocessing it\nalso uses techniques like word\nembedding okay so this is also a\ntechnique but this technique is\nbasically called as word embeddings and\nword embeddings internally uses some\namount of word to W but we can train our\nown word embedding techniques okay so\nagain uh this is a technique of\nconverting input text into vectors now\nsimilarly we have techniques like\nTransformers and birds also so coming to\nthe next one this is basically\nTransformer again this is an advanced\ntechnique I just really wanted to\nmention all these things to you so that\nyou will be able to understand but so if\nyou really want to become a pro you\nreally need to go with this particular\npattern and try to complete till B and\ntry to see the application of that but\nas we go up from bottom to top the\naccuracy of the model keeps on\nincreasing because and and remember one\nmore thing the size of the model also\nincreases right which whichever machine\nlearning models or deep learning models\nthat you are probably trying to create\nto solve the NLP use cases it will keep\non increasing as you go to words\nTransformer and BD now initially as I\nsaid we are going to learn with respect\nto NLP with respect to machine learning\nNLP for machine learning so what all\nthings we are going to focus on we are\ngoing to focus on these three things\nfirst and when we'll start deep learning\nwe will probably focus on this three\nokay so here I'm just going to write\nthis important thing is that all the\nthis three steps will be the part of\nmachine learning and now when I say\nmachine learning we will be using\nlibraries like\nnltk and there also libraries like\nSpacey which will actually help us to do\nperform all the task which is available\nover here in the bottom three right but\nwe'll focus on one Library called that\nis called as nltk so suppose if you know\nnltk I think learning space is also very\nmuch easy in the case of deep learning\nwe will usually go with libraries like\ntensor flow or pytorch so both libraries\nare quite amazing with respect to this\nas you know tensorflow is now an open\nsource py toch is an open source\ntensorflow has been created by Google py\ntoch has been created by Facebook right\nbut at the end of the day what is the\nmain aim your input data is in the form\nof Text data and you basically have to\nperform some amazing kind of text pre\nprocessing where you convert the input\ntext Data into vectors and you are able\nto solve amazing use cases of NLP using\nboth machine learning and deep learning\nand this is a brief idea about the\nentire road map app uh like how we are\ngoing to prepare hello guys so we are\ngoing to continue the discussion with\nrespect to NLP and in this video we are\ngoing to discuss about some of the\namazing use cases and these use cases we\nuse it every day in our day-to-day\nactivities right so let me go ahead and\nlet me share my screen and let me talk\nabout every use cases one by one we'll\nbe discussing about it and this will\ngive you a clear idea about what is\nnatural language processing now here\nI've opened my Gmail tab now in this\nspecific Gmail you'll be able to see\nthat uh let's say that I really want to\nwrite some email right so I'll say hello\nCrush okay and let's say I made a\nspelling mistake okay I\nwanted\nto clarify so I have actually written a\nwrong spelling so here you can see that\nautomatically the spelling has been\ncorrected right and this is all because\nof NLP and in this also we can like\nsuppose if we receive a email from\nsomeone right we can also write an\nautomated email from them so that\nbasically means it will basically tell\nus that what kind of text it will\nautogenerate and it will give it for us\nand we can actually send it so I I would\nlike to I wanted to clarify about the\ndata science and let's say I'll write\ncourse okay now let's see what it\nbasically gives it can also give you a\nsuggestion like this and now you can\nbasically select the course right so\nthis is also one amazing use cases and\nwe use it every day now let's say I go\nto my LinkedIn so here uh rudra Pratap\nSingh is one of my student and\nautomatically we can see that two tags\nautomated replies there in the LinkedIn\nitself and this is also an amazing\napplication from NLP so that if you\nreally want to just provide this any of\nthis particular two tag you can just\nclick it and you can reply back so in\nthis in short is saving you a lot of\ntime now similarly if I go with respect\nto one more application that is Google\ntranslate and I hope everybody uses this\nlet's say that I just go ahead and write\none language I'll just say how are you\nright and I probably want to convert\nthis into Arabic okay so it looks\nsomething like this with this specific\nlanguage suppose if I really want to\nconvert this into Hindi to uh then you\ncan basically say that automatically the\ntext is coming like right in Hindi we\nbasically say for how are you so this is\nalso happening because of NLP amazing\napplication and we use this and in most\nof the websites also right let's say in\nLinkedIn if somebody's posting in some\ndifferent language below only you'll get\nan option of something called a c\ntranslation so once you click that\nparticular thing and automatically gets\nconverted to English not only that guys\nlet's say that I'm just going to go\nahead and search for krishn so krishn is\nmy name I'm a YouTuber I'm also a\nco-founder of ion. but here if I\nprobably go and selct on select on\nimages now here you can see that it is\nalready been able to detect my my image\nand it is in turn converting that text\ninto images it is understanding the text\nand it is basically thinking that okay\nit it knows that it is talking about\nmyself and based on the web right it is\nbeing able to capture all the images\nthat is available in the web so this is\nalso an amazing thing and probably if I\nprobably go ahead and select videos here\nalso you'll be able to see all the\nvideos of my in my YouTube channel or\nprobably in all the other public\nplatform that I have which is being\naccessible of my web so this is an also\namazing example text to image text to\nvideo and a lot of research is also\ncurrently going on with respect to\ndifferent different companies there's an\namazing company which is called as\nhugging face and hugging face have\nactually created an amazing models and\nsolutions for solving question answering\nsession uh question answering it has\nbasically question answering models sum\nsummarization text classification\ntranslation right all these particular\nuse cases are there and you can see that\nhow many different kind of models it has\nlike for question answering right it has\n23 380 models summarization it has 588\nmodels uh for test classification it has\nthis much models right and here you can\nsee that how many companies are\nbasically using this Google AI Intel\nSpeen Bridge Microsoft grammarly right\nall these things are basically using\nthis uh and again this is an amazing\napplication of NLP itself because all\nthese applications are in short taking\nthe text Data only and Performing\ndifferent kind of task on top of it so\none more amazing thing that I really\nwant to show you as an application with\nrespect to Alexa and Google Assistant uh\nI have some of the Alexa home configured\nover here I can control my AC's and\nlights but with respect to Google\nAssistant a simple application I can go\nand open it so here I have my Google\nAssistant now here you can see that hey\nGoogle do I have any doctor appointments\ntomorrow sorry I can't find anything on\nyour calendar that matches that so now\nhere you can see that uh already it is\nbeing able to retrieve from my calendar\nit is able to see whether I have a\ndoctor appointment or not tomorrow so\nthis way\nit is really really easying all the task\nwith respect to most of the critical\nthings right that we do in our\nday-to-day activities so we use\nextensively NLP a lot in our day-to-day\nactivities and uh this is what we are\ngoing to learn in the specific course\nhow we can build this kind of models\nwhat kind of text pre-processing\nactually happens we'll get a clex idea\nbehind it how these applications also\nworks hello guys so we are going to\ncontinue the discussion with respect to\nnatural language processing in this\nvideo we are going to cover some of the\nbasic terminologies that is required in\nNLP you really need to understand these\nterminologies because I'm going to\nrepeat this terminologies again and\nagain when we are discussing the other\ntopics so the topics that is going to\nget covered in this video is about\nCorpus documents vocabulary words you\nreally need to know all these topics\nwhat exactly it is with some basic\nexamples now usually whenever we get a\nparagraph a paragraph is usually called\nas a corpus okay with respect to\ndocuments whenever you have any kind of\nsentences you really need to understand\nthat the sentences are also usually\ncalled as\ndocuments what about vocabulary\nvocabulary is nothing but all the unique\nwords that are present in this paragraph\nthat is basically called as vocabulary\nusually we have a dictionary right we\nusually say that what is the vocabulary\nin that particular dictionary all the\nunique words are the count of all the\nunique words or all the unique words\nthat is present in the dictionary it is\ncalled as vocabulary and with respect to\nthe words all the words that are present\nin a corpus that we will basically\nDefine all those separately as a\nspecific words itself so these are the\nbasic terminologies that you really need\nto understand as said in this video\nwe're going to discuss about something\ncalled as\ntokenization and tokenization is a very\nimportant step whenever we try to solve\nany kind of use cases with respect to\nNLP now what exactly is\ntokenization right so let's say that I\nhave a paragraph I I write over here\nthat my name is\nCrush my name is\nCrush\nokay and I have\na I have an interest in\nteaching I have\ninterest in\nteaching machine learning NLP and deep\nlearning and\nDL now let's say that if I have this\nspecific\ntext this test I can consider basically\nas paragraphs so this will be entirely\nCorpus okay so this is my entire Corpus\nthat is available which is nothing but a\nparagraph of uh words right so if I\nprobably combine all these words it\nbecomes a paragraph now tokenization is\na process wherein we take either a\nparagraph or a sentences and we convert\nthis into tokens right now suppose let's\nsay I want to perform a tokenization on\nthis particular\nparagraph and over here from this\nparagraph the tokens that are usually\ngenerated it will basically be called as\nsentences or documents so let's say that\nI will be applying a tokenization on\nthis and with respect to this let's say\nthat there I'll try to convert this\nentire paragraph into sentence so I may\nalso add one more line over here let's\nsay full stop I'm just writing one more\nfull stop over\nhere okay and I will also write that I\nam also a\nYouTuber okay so these are the the two\nsentences that is present in this\nparagraph So with respect to this\nparticular\ntokenization if I perform a tokenization\non this paragraph it will basically\ncreate sentences my first sentence in\nthis particular case will\nbe my name is\ncrish okay and I have interest\nin interest in\nteaching\nml\nNLP NLP and DL okay so this what this is\nbasically my document one or sentence\none my next sentence that I'm going to\nprobably write over here because the\nfull stop is over here right so when we\nconvert from a paragraph when we do talk\ntokenization from paragraph into\nsentence it will be looking for this\nkind of characters like full stop or\nexclamation I'll show you practically\nhow this can be actually done with the\nhelp of Python programming language so\nthe second sentence that I will probably\nbe have having is like I am also a\nYouTuber right I am also a YouTuber so\nagain if you really want to understand\nwhat exactly is tokenization\ntokenization is a simple process wherein\nwe are converting a sentences into sorry\nwhere we are converting a paragraph into\nsentences now there may also be a scenar\nArio that let's say that I have some\nsentences okay and on top of this I can\nalso perform tokenization again so let's\nsay on top of this I'm performing a\ntokenization now this tokenization\ntechnique that I'm probably applying\nwill convert the sentences into\nwords right so let's say I say over here\nit is basically getting converted into\nwords so each and every word will be a\nseparate word so my will be a separate\nword name will be a separate word is\nwill be a separate word Kish will be a\nseparate word and will be a separate\nword I will be separate word have\ninterest in\nteaching all this will be a separate\nwords itself right so this process is\nalso called as tokenization so in short\nwords can also be a token sentences can\nalso be a token right this is very\nimportant to understand and why it is\nrequired because this is a part of text\npre-processing because each and every\nword in NLP needs to be converted into a\nvectors so we really need to take up\neach word and try to do this kind of\npre-processing and there are lot of\nsteps like cleaning and all which I will\nalso be showing you but in this video we\nare going trying to understand about\ntokenization so I hope you have got an\nidea about Corpus you have got an idea\nabout sentences now let's go ahead and\nunderstand about vocabulary which is\nalso called as unique words okay now\nlet's say I have two\nsentences I like to\neat apple juice sorry how can we eat\napple juice I like to\ndrink apple\njuice\nokay here I will again continue and I'll\nwrite my friend\nlikes mango juice\nokay now let's say that this is my\nentire\nparagraph okay now in this paragraph you\nknow how many sentences are there there\nare two sentences because there is a\nfull stop over here right so I will just\ndivide this into tokens so let's say I'm\ngoing to perform something called as\ntokenization over here okay and this\nwill get converted into tokens and right\nnow the tokens that is present over here\nwill be\nsentences right so my first sentence\nwill be I I like to\ndrink\napple juice so this is my first sentence\nand second sentence is nothing but\nbecause there is a full\nstop my\nfriend\nlikes mango\njuice mango juice now see when we have\nthe sentences obviously you can you can\ngo and count each and every words right\nlet's say how many total number of words\nare over here so if I probably go and\ncount 1 2 3 4 5 5 6 7 8 9 10 right so if\nI again count it 1 2 3 4 5 6 7 8 9 10 11\nso total I have 11\nwords but if I try to count the unique\nwords how many unique words are there if\nI make the count again so I will be one\nunique word like another unique word 1 2\n3\n4\n5 6 7 8 9 see like and likes are two\ndifferent word so I'll say 9 10 but this\njuice is getting repeated so the total\nnumber of unique words will basically be\n10 words right let's say instead of this\nlikes there was something called as like\nat that point of time the number of\nunique word the number of unique word\nwill be 1 2 3 4 5 6 7 8 9 right I will\nnot count like and juice right but\nalready likes is there so it'll be\ncounted as a separate word so whenever I\nget this unique word as 10 words that\nbasically means in my dictionary in my\nthis complete paragraph This is my\nvocabulary so this is all the possible\nwords that I have right that is the 10\nwords right now since I've have\nconverted this into like so I'm just\ngoing to make this as not n\nwords I hope you're able to understand\nthe basic differences between Corpus\ndocuments vocabulary and words right so\nthis entire thing is super important\nwhen we are learning about tokenization\nagain if somebody ask you what is the\ndefinition of tokenization you can just\nsay that tokenization is a process to\nconvert either a paragraph or a\nsentences into tokens if I convert a\nparagraph into tokens that basically\nmeans I'm converting a parag into\nsentences I can also convert a paragraph\ninto words hello guys so we are going to\ncontinue the discussion with respect to\nNLP in our previous video we have\nalready seen about tokenization we have\nunderstood about basic terminologies\nlike Corpus paragraph We have understood\nvocabulary we have understood about\nwords now let's go ahead and do some\npractical things you know how much we'll\nbe learning with respect to Theory so\nfirst of all I'll just go ahead and open\nmy Python 3 notebook file so here uh\nfirst of all uh and and for this right\nwe are going going to use libraries like\nnltk now nltk is one amazing Library let\nme just go through this and let me just\nshow you nltk is a leading platform for\nbuilding python programs to work with\nhuman language data so if you really\nwant to work with NLP things like\ntokenization creating converting a\nsentence into vectors can be easily done\nwith the help of this nltk libraries\nthere's also one more Library which we\nbasically say it as Spacey so if I\nprobably search for Spacey NLP so here\nyou'll be able to see that again these\nare completely open- Source libraries\nyou can also use spacy and I will also\nbe showing you with the help of spy how\nyou can perform different different\nthings right so Spacey is also there\nnltk is also there one important\nassignment that I really want to give it\nto you is that try to find out the\ndifferences between nltk and Spacey and\nuh just let me know probably in the\ncomment section of this particular video\nor in the upcoming videos Okay so this\none task I want to really give it to you\nso that you try and try to understand\nwhat is the difference between this two\nopen Source libraries now to begin with\nuh since we are going to initially start\nwith nltk what I'm actually going to do\nI'm first of all going to install nltk\nnow here you can directly install it\nfrom here or you can also open the\ncommand prompt and directly install nltk\nright so what you have to do is that\njust write pip install nltk once you do\nthis automatically the installation will\nbe done so what I'm actually going to do\nI'm going to basically install from here\nand I'm going to show you a tokenization\nexample so let me write it down\ntokenization\nuh example\nokay perfect so this is the tokenization\nexample so here you can see that uh\ncollecting nltk requirement already this\nis done the installation has been done a\nnew Rel release of pip is available I\ndon't want to update it right now\nbecause this pip will almost cover each\nand everything right now right now uh\nthis is perfect now what I'm actually\ngoing to do I'm going to show you how we\ncan perform tokenization like let's say\nif I have a paragraph how I can convert\ninto sentences and then how I can\nconvert into words all those things I'll\nbe discussing and multiple ways of\ntokenization also I'll be showing you\nokay so let me go ahead and let me make\nfew cells so that I can directly go\nahead and execute it now one thing over\nhere yeah let's go ahead and let's start\nnow over here first of all I will try to\nDefine my own Corpus okay Corpus\nbasically means paragraph so let's say\nuh if I really want to create a\nmulti-line comments I have to basically\nuse this triple Cotes I will write hello\nwelcome comma okay I'm just giving some\nsentences over here so that you'll be\nable to understand okay so hello welcome\nto fresh\nNX tutorials or let's say I just write I\nneurons\nright or I will write something like\nthis\nkushak NLP Kush n's nlps to tutorials\nokay and this is my sentence okay and I\ncan continue with my second line so I'll\nwrite please do watch the entire\ncourse to I'm just writing exclamation\nI'm using different different characters\nover here to become\nexpert in NLP okay so this is what I\nhave actually done i' I've basically\ndefined a simple Corpus uh which is just\nlike a paragraph which has around two\nsentences and I'm just going to use this\nparticular Corpus so let's go ahead and\nsee this corpus now once I see this\nCorpus here you can see that even though\nI just I'm just printing this right I\ncan also print this see then the sln\nwill go slash and indicates the new line\nokay so if I'm probably printing Corpus\nhere you'll be able to see the text how\nit is basically visible now with respect\nto tokenization the first step that I'm\nactually going to do is that I'm going\nto convert a\nsentence into\nparagraphs okay so I'm going to\nbasically convert a sentence into\nparagraphs so for this how do I convert\nit and with the help of nltk it is\ndefinitely possible so I will write from\nnltk dot\ntokenize okay and I'm going to just\nimport okay so in nltk there is uh there\nis a library which is called as toonize\nand if I import this sentore tokenize so\nwhat sentore tokenize does is that it\ntries to convert a paragraph into\nsentences so this is the function that\nwe are basically going to use or\nfunctionality that is we are going to\nuse which is present inside nltk do\ntokenize okay so you can just say that\nthis is a kind of package inside this\nand I will initialize this package in\norder to convert or sentence into a uh\nsorry a paragraph into a sentences so\nhere I'm just going to write sent\ntokenize and let me just go ahead and\ngive my Corpus so once I give my Corpus\nhere you can see that it is giving us a\nlist a list of sentence so here's you\ncan see over here this is my sentence\nover here hello welcome to Krish nik's\nNLP tutorial so here you can see this is\nmy first sentence and this is my second\nsentence and from exclamation also it\nhas divided it because I'm getting three\nright so here you can see hello welcome\nto Krishna NLP tutorials full stop as\nsoon as it finds full stop it is just\ngoing to to make a next sentence so here\nwherever slash in was present it is\nusing that and wherever exclamation is\nalso present it is basically making sure\nthat a new sentence is getting created\nwith respect to that and that is what\nsend tokenize actually does if you\nreally want to find out the definition\nof this here you can see that uh you can\nalso provide different different\nlanguages what are languages it supports\nyou can just go ahead and have a look\nonto that and uh if you don't find much\ndocumentation out of it so what you can\ndo over here you can just go and search\nfor\nnltk sent tokenize right sent tokenize\nso you will be able to find out the doc\ndocumentation page directly and you can\nrefer it from here right there are\ndifferent different tokenization see\nsend tokenize is there word tokenize is\nthere which we are basically going to\ndiscuss right now we going to focus on\nthis okay so in short it returns a sent\ntokenized copy of text using nalt case\nrecommended s sentence tokenizer and\ncurrent class it uses this called as\nPunk send tokenizer so along with the\nfull stop it is making sure that whever\nexclamation is basically coming it is\ngoing to make as a uh as another\nsentence so this is perfect uh we are\nable to get this right now I will go\nwith the next tokenization and before\nthat what I also want is that if I\nprobably save this in a list of\nsentences so let's say this sentence is\nalso called as documents which I already\ndiscussed in my previous class uh\nprevious session right so if I probably\ngo and see these documents this is\nbasically my list you can also check out\nwith the help help of type so type\ndocuments right and if I probably want\nto iterate through this let's say from\nsent in documents from from for\nsentences for sentence and documents I\ncan also print this sentence parall\nright so here I can just Define it as\nsentence so this is my first sentence\nsecond sentence and third sentence this\nis perfect we were able to do with\nrespect to the sentences now let's go\nahead and do it with respect to the word\ntokenize so next tokenization technique\nwhat I am actually going to do is\nthat next tokenization is that I can\nconvert a\nparagraph I can convert a paragraph into\nwords I can also convert a sentence into\nwords sentence into words okay perfect\nnow for this what I'm actually going to\ndo first of all let's go ahead and see\nwith respect to paragraph uh so already\nuh you know that uh uh with respect to\nconverting a paragraph into words I'll\nbe using again another Library so I can\nwrite from nltk dot okay\ntokenize from nltk do tokenize okay\nspelling mistake is there so tokenize\nI'm going to\nimport\nword let me just write it down word\ntokenize okay so here we can basically\nuse this word tokenize over here and let\nme just execute it over here and let me\ngo ahead and write word tokenize\nand here if I directly give up my Corpus\nhere you can see that each and every\nword has been divided hello world and\nhello welcome here you can see all the\ncharacters like comma full stop has been\ntreated as a separate character all\ntogether or separate words all together\nright so here you can definitely clearly\nsee that each and every word has been\nsplitted with respect to this only one\nword that is not having this right so if\nI probably go and see over here hello\nwelcome to krishn so this kind of word\nhas not been splitted this all is being\nconsidered as a single word but if I\nconsider with respect to full stop with\nrespect to exclamation with respect to\ncomma it has been considered as a\nseparate word so here also you can find\nit out with respect to this right so\nthis is very very simple with respect to\nuh converting a paragraph into words and\nwhy do we do this because each and every\nword will have a different importance\nand we really need to perform some\npre-processing on top of it so right now\ninitially when you get it you know which\nare the important words you have to take\nit you have to pre-process you have to\nclean it you know and uh that is the\nreason why we specifically focus on each\nand every word okay so this was about\nthe word tokenize and uh now with\nrespect to sentences also you know that\nhow you can basically do just go over\nhere and just write paste it over here\nfor sentences in this now here you're\nprinting the sentences right now what\nyou can basically do in that is that\nafter you probably get this sentence\nafter you get the sentence all together\nyou can just just directly apply word\ntokenize right so here you can basically\nwrite word tokenize on sentences so you\nwill be able to print everything over\nhere right so hello welcome to this this\nthis this uh please do watch the entire\ncourse this this is there right perfect\nso here we have seen that how we can\nbasically convert a sentence into words\nright now you can also do one thing uh\nover here is that you can use another\none more Library let me just talk about\none more Library probably you have seen\nthat so so I'm going to just write word\npunct toonize and in this word punct\ntokenize what we are basically going to\ndo is that if I try to apply this if I\ntry to initialize this word Punk\ntokenize and if I try to provide my\nCorpus let's say so here you just try to\nfind out the difference and one\ndifference that is clearly seen is that\nthis apostrophe s has also got splitted\nbefore it was not getting splitted right\nsee apostrophe s was a single word but\nnow you can see that it has been\nsplitted so that is the reason why we\nusing this punctuations um this\nPunctuation is it is basically uh making\nsure that the punctuation will also be\ntreated as a separate word perfect so\nthis is good there's also one more\ntechnique which is basically called as\nTree Bank word tokenizer and also again\nI'll try to tell you the difference what\nexactly it is with respect to Tree Bank\nword tokenizer so uh I'll just try to\nexecute it and you try to find out the\ndifferences with respect to that let's\nsee how much you'll be able to do so I\nwill write from nltk dot to\ntoize\nimport Tre bank tokenizer so I will\ninitialize this tree Bank tokenizer okay\nlet's say that I'm initializing this\ninto tokenizer something like this\nsomething to one variable and then I can\nbasically use a function which is called\nas tokenizer do tokenize and once I give\nmy\nCorpus here you'll be able to see that\nI'm actually able to get it okay now\njust see over this what is the\ndifference with respect to this you will\ndefinitely be able to find out some\ndifference when comparing with this\nspecific thing okay so just let me know\nwhat is the difference that you are able\nto see I know there's a very minute\ndifference so with the let me tell you\nthe answer you can just pause for some\ntime and but you can check it out but\nlet me tell you an answer with the help\nof tree Bank word tokenizer here you can\nsee that full stop will not be treated\nas a separate word it will be included\nin the previous word itself now here you\ncan see that full stop was a separate\nword here also you can see that full\nstop is a separate word right but with\nrespect to the last word right full stop\nwill be separate you know because here\nyou can see that right if I probably see\nthe sentence right after this we have a\nnew line right and after this our\nsentence is getting closed for the last\nFull Stop only it will be considering as\na separate word but with respect to this\nparticular full stop it will be\nconsidering as a part of this it will\nnot be considered as a separate word\nonly this is the difference with respect\nto this and again it can be handed in\nsome of the use cases but not in all but\nat in a generic way we basically most of\nthe time use word tokenize or send\ntokenize right so yes uh this was it\nwith respect to the tokenization example\nI hope you like this particular video\nhello guys so we are going to continue\nthe discussion with respect to natural\nlanguage processing and now we are going\nto move towards some more techniques\nwith respect to text pre-processing\nalready in our previous video we have\nseen tokenization we have seen that how\nwe can convert a paragraph into\nsentences and then probably a paragraph\ninto words or converting a sentences\ninto words right so in short we have\nseen that how we can actually do\ntokenization with the help of nltk so in\nthis video we are going to focus on\nsomething called as stemming which is a\nvery important process altogether and uh\nwhat exactly is stemming I'll also\nprovide you the definition we'll also\nsee a lot of examples with respect to\nthat and we'll also try to see the\ndifferent types of steming\nso I have opened a file over here so\nhere you can see regarding the stemming\nand I also given the definition now\nlet's understand what exactly this is\nwith some good examples now first of all\nwe'll see the definition over here it\nshows that stemming and this definition\nis taken from the Wikipedia so stemming\nis the process of reducing a word to its\nword stem okay this is super important\nguys to its word stem that affixes or\nsuffixes or prefixes or to the roots of\nthe word known as LMA okay stemming is\nimportant in natural language\nunderstanding and natural language\nprocessing now what exactly this is I'll\ntell you some examples let's say I want\nto just create some cell below okay over\nhere if I have lot of text right let's\nsay that I'm trying to solve a sentiment\nor I'm just trying to solve a\nclassification problem classification\nproblem and the classification problem\nis very simple we basically need to find\nout whether the comments on the\nproduct is\na positive\nreview or negative review right so when\nwe are solving this kind of problem\nstatement so in this what we'll be\nhaving in our data set we'll be having\nthe comments or reviews I can say I'll\nbe having reviews and based on this\nparticular reviews and this reviews will\nobviously be some kind of Text data and\nwe need to basically create a model\nwhere then we can basically classify\nwhether it is a positive review or\nnegative review that is very simple now\nusually in this reviews let's say that I\nhave some of the words like eating okay\nor it can be eat right or it can be like\neaten right it can be different kind of\nwords but at the end of the day it\nactually represents the same thing\nregarding eating right so this is\nbasically eat eat is the root word or I\ncan also say it is the stem word word\nstem of this all the words right because\nitat is very much common and having this\nvariety of words for a problem statement\nwill not impact much with respect to\nfinding the output like positive or\nnegative review try to understand what\nI'm actually trying to say over here I\nmay have different kind of words like\neating eat eaten or there may be also\nother words like I can also make a\ncombination of like going\ngone right gone goes right\nat the end of the day it is basically\ntalking about go right so go is a word\nstem of all these words that are present\nright so it is not necessary that we\nneed to have similar kind of words again\nand again because this increases the\nnumber of input features in short\nbecause each and every word represents a\nvector as we'll see as we'll go ahead\nyou know after text pre-processing we'll\ntry to convert this text into vectors so\nhaving this similar kind of words\ninstead of having this similar kind of\nwords I can just have one word that is\njust like go right and it will try to\nand we'll try to solve the problem with\nrespect to that so stemming is actually\nhelping us to do the same thing so\nfinding this word stem can be actually\ndone with the help of stemming and there\nis also a concept which is called as\nlimitation we'll try to understand the\ndifference as we go ahead but first of\nall let's go ahead and see that how with\nthe help of nltk we can perform stemming\nokay so what I'm actually going to do is\nthat I'm just going to take some example\nexamples let's say that I have all these\nwords okay so I'm just going to remove\nthis and let's say that I have all these\nwords now right now you have words like\neating eats eaten writing writes\nprogramming programs history finally and\nfinalized okay so I'm just going to\nexecute it and let me make some more\ncells okay so I'm just going to delete\nthis cell because I don't require it\nokay I'll just create a cell below okay\nand let me press\nexcape and let us go ahead right so here\nyou can see all these particular words\nare there now let's see how we can find\nout the word stem of all these\nparticular words with the help of\nstemming the first stemming technique\nthat we are probably going to use is\nsomething called as Porter stemmer okay\nPorter stemmer and there are again\ndifferent different types of stemming\ntechniques which I will probably be\nshowing you and then we'll be able to\nunderstand that what all things it will\nbe able to give us okay now in order to\napply this pter stemmer it is very much\nsimple in nltk already those\nfunctionalities basically present so I\nwill just write from\nnltk dot\nstem\nimport pter\nstemmer okay so once I initialize this\nhere you can see this I'm going to use\nthis pter stemmer which is just a kind\nof class and for this we have to\ninitialize it we have to initialize an\nobject for that so let me just create an\nobject and this will basically be my\nstemming and once I do this now in The\nNext Step what I'm actually going to do\nfor each and every word I'm just going\nto apply this stemming process okay so\nit's very simple how do I do it I will\njust iterate it so I'll say for\nword for word in words right and here\nI'm just basically going to write\nprint and uh let me write it down as\nword\nplus I'll just give some type of marking\nthis is the word and the stem part will\nbe nothing but I'll be using the same\nobject\nstemming dot stem there's a\nfunctionality there's a function called\nas stem which will actually whenever we\npush any words inside this it will do\nthe stemming thing that basically means\nfor eating probably it may give you eat\nfor eats it may give you eat right so\nsomething like that so I'm just going to\ngive my word over here so once I execute\nnow here you can see that perfect eating\nis coming as eat eats comes as eat eaten\nis coming as eaten only writing is\ncoming as right which is good rights\ncomes as right programming comes with\nprogram programs is nothing but program\nhistory here you can see history is\nbecoming his h i t o r i so here it is a\nmajor issue right and I'll also talk\nabout the disadvantages finally becomes\nfinal finalized becomes final this is\nthis is there right now let's say that\nuh over here it looks good right\nprobably for eaten you can see that\nnothing has happened it is given the\nsame word but if you see some words like\nhistory now here you are actually\ngetting history Ri so the entire meaning\nof this particular word has actually\nchanged and this is the major\ndisadvantage of stemming when stemming\nis basically applied you know for some\nof the words you know you may not get a\ncorrect exact meaning the form of that\nspecific word may change so this is the\nmajor major disadvantage with respect to\nstemming let me show you some more\nexamples now suppose if I say stemming\ndo stem I'm just going to apply this\nparticular stem word on a word called as\ncongratulations okay and if I execute it\nhere you can see the word the meaning of\nthe word is basically changing it should\nhave told like something like\ncongratulate right but here you can see\ncongratul it is being it is basically\nchanging the form of the word now the\nword does not have any kind of meaning\nright so this is again one major major\ndisadvantage of stemming okay similarly\nif I try to show you something like\nstemming do stem and if I probably write\nsomething like sitting so here you'll be\nable to see let me just write it down\nsit now see for stay sitting it is\ngiving a very good word so that\nbasically me stemming works for uh very\ngood number of words but for some words\nit does not give you us a good answer\nright so this is the major disadvantage\nof stemming and this all will get fixed\nwith the help of lemmatization but\nwhenever you have any kind of problem\nstatement like uh classification problem\nreview classification or we really want\nto see whether it is whether an email is\na spam or a ham right whether it's a\nSpam or not a Spam we should definitely\ngo ahead with using stemming you know\nand uh again some of the words will not\ncome in the right form but yes instead\nof using pter stemmer we have other\ndifferent kind of stem stemmer\ntechniques which we can definitely use\nto improve it okay now let me go with\nrespect to the second one now with\nrespect to the second one I'm just going\nto this cell because they don't require\nit the second cell is that which is\ncalled as Regus stemmer class now this\nregular expression stemmer class it is\nnothing but this is a class with the\nhelp of which we can easily Implement\nregular expression stemmer algorithm so\nwe can just provide a regular expression\nand it will be able to apply the\nstemming purpose in that okay so it\nbasically takes a single regular\nexpression and remove any prefix or\nsuffix that matches the expression\nperfect now what I'll do I'll just\ncreate a cell below and and again I will\nmake some more cells and I'll try to\nshow you some example now first of all\nwe need need to initialize this so I\nwill write from nltk do stem import what\nis the name Regus right so I'm just\ngoing to write reg stem perfect then we\nbasically need to initialize it so I'll\nsay regor stemma is equal to Reg right\nso I initialized it this is perfect uh\nit is giving us an uh error saying that\nregular expression is required now this\nis super important now let's press go\nahead and press shift and tab now here\nyou can see the first parameter that\ngoes is something called as regular\nexpression okay and this is some minimum\nvalue we'll try to understand what is\nexactly it is now here you can see a\nstemmer that uses regular expression to\nidentify morphology affixes right\nunderstand this morphological affixes\nI'll try to show you an example if you\ndon't know the meaning of this any\nsubstring that match the regular\nexpression will match will be removed\nso this morphological affixes is\nbasically in short a regular expression\nwhich will match whatever words we are\nbasically giving and if it matches it\nwill get removed okay now let's take\nsome example okay over here uh and uh\nhere you can see that this param is the\nminimum length of the string to stem\nokay so if the minimum length is\nsomewhere around four then only you'll\nbe able to apply this so I'm just going\nto apply this same\nthing and I'm going to paste it over\nover here okay so here is my reg stemmer\nnow what I'm actually going to do I'm\njust going to write reg. stemmer now you\nshould understand over here what I have\nactually given I have given ing dollar\nokay here I given s dollar e dollar able\ndollar okay so I've given all this\nregular expression and this will make\ncompletely sense when I will be\nimplementing it now you see just pause\nthe video and just let me know what is\nthe output that you feel will be getting\nfor this\neating now eating you should see over\nhere ing is there a regular expression\nis there so and but dollar is also there\nokay this is super important dollar is\nalso there now if I try to execute this\nI'm able to get something called as\neight okay now in short what this is\nhappening is that this is basically\nsaying that wherever on the last word it\nis ing or it is s or it is e or it is\nable just try to remove that now the\nnext example that I really want to give\nis that let's say I want to I have this\nparticular word ining okay so here you\ncan see ing eating right what do you\nthink the output will be will it whether\nit will be eat or whether it'll be\nsomething else okay so here if I try to\nexecute it here you'll be seeing in8 why\nbecause the regular expression says that\nonly in the last last with dollar if I\nprobably remove dollar and if I just\nexecute it now see everything will get\nremoved okay or probably I just want in\nthe starting so I'll just use this so\nhere you can see ing eating so this this\nwill not work okay so we can basically\nhave something like this uh and we can\nbasically check how we can actually\nremove it so this is perfect so here you\ncan see wherever ing is there it is\ngetting removed perfect so this is good\nwe have done this okay I eat now let's\nsay that uh I also want to try something\nelse uh you can definitely try with\ndifferent different word you can use\nthis able e s whatever regular\nexpression you can basically write you\ncan go ahead and write it and you can\ncheck it okay so this is with respect to\nregular expression stemmer class now uh\nthe next one that we are basically going\nto discuss uh and uh let's see where\nthis\nis portter stemmer this this this this\nagain it has written the same thing I\ndon't want to write the same\nthing I'll go ahead with something\ncalled as snowball\nstemmer and this is also an amazing\ntechnique and this is a better technique\nwhen compared\nto okay so snowball stemmer I think I've\ngiven the definition somewhere here no\nno no no okay snowball stemmer is again\na stemming technique but in this\nsnowball stemmer it is it performs\nbetter than this Porter stemmer okay\nthat is the reason why snowball stemmer\nhad actually come initially we came up\nwith pter stemmer we saw that lot of\nthings lot of words were getting messed\nup you know so that is the reason why we\nuse snowball stemmer because it gives a\nbetter accuracy when compared to the uh\nP STL when I say accuracy better form of\na word okay now for using snowball STL\nwhat I'll do I'll write from Escalon do\nstem\nimport snowball stemmer so here you can\nsee I'll just write\nsnowball for my scalon oh not for my\nscalon sorry from my ltk but because it\nis basically present in the nltk R so\nfrom nltk do stem import snowball stem\nand I'm just going to import it then we\nare going to initialize with respect to\nsnowball stemmer and let's see what are\nthe parameters so here first of all\nsnowball stemmer is also provided with\ndifferent different languages like uh\nArabic okay\nDanish uh English Finnish French German\nHungarian Italian so you can use\nbasically all these words okay so for\nright now I'm just going to use English\nso what I'm going to do is that I'm in\nquotes I will just say English okay and\nuh I'll just use this and finally I will\nbasically use a snowball stemmer over\nhere I'll just create a\nvariable okay and just execute it\nperfect now the next thing is that I\nwill just use another condition saying\nthat for word in words\nokay and uh I'll just write for words in\nWords uh so I'll write snowball stemmer\ndo\nstem on Word word okay but I'll print\nthis in a better way so that you'll be\nable to understand it so\nprint\nword\nplus and this will be like this\nsomething like this just for formatting\npurpose I'm writing this so that it look\ngood better for you all and here I'm\njust going to write the word right so\nsame thing what I did in the things so\nhere you can see that I'm getting eating\nas eat eats as eat eaten has eat right\nright this is all fine right for history\nalso it is not being able to give the\ncorrect form right then you may be\nthinking chrish then what is the\ndifference right but let's see with\nrespect to some Porter stemmer where\nthis pter stemmer will give some bad\nresults or bad form of the word and\nwhere the snowball will give us a better\nform of a word so for this I'm just\ngoing to uh execute this line of code so\nhere you can see when I'm applying the\nstemming the stemming was for the bter\nstemmer right when I apply stemming do\nstem on fairly and sportingly so what is\nthe output that I'm getting I'm getting\nfairly and sportingly right Li Li but if\nI try to use the same word with respect\nto Snowball right here you'll be able to\nsee just let me write the snowball\nstemmer here we'll be able to see I am\ngetting some amazing answers snow balls\nstemmer\nokay\nokay let me just remove this I think\nthere is a problem with respect to\nthis now it'll work\nokay snowball stemmer\ncomma I will just copy this and I'll\napply to the same word something called\nas sportingly\nokay so now if you probably go and see\nI'm getting a good output which is like\nfair and Sport so altogether you will be\nseeing that snowball stemmer when\napplied to various other other words\nperforms better than stemming right so\nhere see pter stemmer is obviously a\ntechnique where you'll be able to find\nout the word stem but for many of the\nwords it is not being able to give a\ngood answers some of the examples are\nlike fairly sportingly and all now in\norder to overcome this disadvantage\nsnowball stammer is basically used and\nagain guys understand these are some\ntechniques which will actually help you\nto find out the word step and where it\nis specifically getting used in text\npre-processing you really need to clean\nthe data you need to make sure that the\ndata is ready so that we will be able to\nconvert it into vectors in an efficient\nway right so in this part we have seen\nabout stemming and again one major\ndisadvantage of stemming is that\nobviously see even though snowball\nstemmer is performing exceptionally well\nbut some of the words like history\nthere'll be also something like going\nright so suppose if I probably write\nthis right snowball do stem right and if\nI use going I think going whether it\nwill give us good or not so going is\nperforming well if I write goes so here\nyou can see for goes also it gives us a\nbad word right even stemming do stem if\nI probably try to see this right so in\nshort how much we try right for some of\nthe words obviously the form of the word\nis changing so this is one of the\ndisadvantage with respect to stemming\nand understand that for use cases like\nchat Bots and all these techniques\ncannot be used so for that we have to go\nahead with something called as\nlemmatization because lemmatization\nsolves all this particular problem\nbecause it has the dictionary of all the\nwords all the root words that is\nbasically there so whatever word you\nbasically give it will be giving you a\ngood grammatical form of the word like\nif I'm giving goes it'll be go if I'm\ngiving fairly it'll be fair you know if\nI'm giving eating it'll be eat so that\nkind of disadvantage is getting removed\ncompletely with the help of\nlemmatization and that particular part\nwe'll be seeing in the next video so I\nhope you have understood till here\nplease make sure that you try practice\nwith different different examples hello\nguys so we are going to continue the\ndiscussion with respect to natural\nlanguage processing in this video we are\ngoing to discuss about limitation in our\nprevious video we have already seen\nsomething called as stemming we\nunderstood that stemming is the process\nof reducing a word to its word stem\nright and we understood what is the\ndisadvantage of stemming because for\nsome of the words right when we perform\nstemming we do not get the correct form\nof the word then and the entire meaning\nof the word actually gets changed and\nplease remember this word which is\ncalled as word stem right so in short\nthis is just a kind of algorithm which\ntries to find out the word stem but for\nsome of the words it is working\nabsolutely fine we have seen different\ntypes of stemming techniques like PTO\nstemmer Regus stemmer snowball stemmer\nand we also found out that snowball\nstemmer was better than poto so all\nthese things were covered today in this\nvideo right now in this video we're\ngoing to discuss about lemmatization and\nthe lemmatization technique that we are\ngoing to use is something called as word\nnet lemmatizer I've already told you\nwhat is the disadvantage of steming\nbecause the words that we are getting is\nnot in the correct form the meaning of\nthe word is basically changing so in\norder to prevent that we use something\ncalled as lemmatizer and in this we have\nsomething called as word net lemmatizer\nnow what is lemmatizer okay\nlemmatization technique is like stemming\nthe output will we will get after\nlemmatization is called LMA which is a\nroot word understand this which is a\nroot word like suppose if I have eating\nthen it will become eat is the root word\nright if I probably talk about history\nhistory is the root word right if I talk\nabout go goes go is the root word right\nover here we are getting the stem word\nright in stemming right it it'll\nprobably apply an algorithm and with\nrespect to that it'll try to find out\nthe stem word right word stem we\nbasically say it this as but here we get\nthe root exact word so again the main\naim of lemmatizer is to give you the\nexact form of the word which is\nMeaningful and it does not change the\nmeaning as it was happening in stemming\nitself so here you have rather than the\nroot stem the output of the steming now\nagain I'll repeat the definition\nlemmatization technique is like a\nstemming the output we will get after\nlemmatization is called LMA which is a\nroot word rather than the root stem the\noutput of stemming after lemmatization\nwe will get we'll be getting a valid\nword that means the same thing so\nsuppose if there are words like eating\neats eaten it will become eat only right\nso it'll be giving you a meaningful word\nwhich will represent many words over\nthere okay so nltk provides word let\nnemati class and I'll try to show you\nwith respect to this and understand guys\num you know this lization occurs or it\ngets performed with the help of this\nword net Corpus reader so there will be\na dictionary kind of Words which will be\ncomparing from there and you'll be able\nto do the worder very much properly so\nfirst of all let's go ahead and see that\nhow we can basically implemented so\nfirst of all I will try to import from\nan ltk stem uh import word net\nlemmatizer okay so I'm just going to\nimport this and this word net lemmatizer\nuh again helps us to perform\nlemmatization uh so first of all I'll\ncreate an object so I will write\nlitier and here I'm just going to right\nword net nemati so here if I see this it\nis nothing but word net nemzer so let me\njust go ahead and execute it so this has\ngot executed perfectly now one thing\nthat you really need to see that okay so\nI will just be writing something called\nas let's say let's try some easy word\nokay so lemmatizer is equal to oh I'm\njust going to say dot limiti so there is\na function which is called as limiti and\nhere I have to give my words so let's\nsay if I'm giving going and if I try to\nsee the answer I'm actually getting\ngoing so that basically means that it is\na uh it is trying to find out the root\nword with respect to this but let's go\nahead and see the functionality of\nlitti in lemiti we give two important\nparameters one is word and this is\nsomething called as post tag okay I will\ntalk about this post tag right now it is\nwritten as n n basically means this word\nthat I'm actually passing it is being\ntreated as a noun so you may be thinking\nCH how many uh post tags will be there\nso let me just write down a comment over\nhere okay so here you have different\ndifferent post tag for noun we give it\nas small n for verb we give it as V for\nadjective we give it as a for adverb we\ngive it as R now what I'm actually going\nto do by default right now the post tag\nis n right so for with respect to n I'm\ngetting this output okay let's say I\nchange it to V because I just want to\nshow you whether this will be changed or\nnot now when I give post tag as n then\nI'm just saying that consider going as a\nnoun keyword but going is not a noun\nkeyword right but if I consider with\nrespect to Verve now you see with\nrespect to verb I'm able to get a good\nlemmatization that is like go okay\nsimilarly if I probably try to see with\nrespect to a adjective so here you can\nsee I'm getting going right and\nobviously this going is a kind of verb\nright it is not an adjective and let's\nsay if I try to go with respect to ad\nhere also I'm getting going right now\nwith respect to the going word right I\nfeel verb is the correct one which we\nreally need to select it now let me do\none thing the all the words that I had\ncopied over here right let's apply this\nentire litiz on this entire words okay\nso I'm just going to execute this and\nexecute it and again I'm just going to\nwrite this for Loop okay and I'm just\ngoing to copy this instead of writing\nstemming do stem I'm just going to write\nlitier\ndot limiti okay and I'm just going to\napply for this words now see this eating\nis becoming eating eats becomes eat\neaten eaten writing writing WR WR\nprogramming programs programs has become\nprogram because again by default in this\nlitti remember that I've given my post\ntag as n right we are considering that\nall these words all these words are\nbasically noun okay so programming\nprograms history finally and so here\nyou'll be seeing that lemmatization has\nnot occurred that much we have not done\nany kind of we are not able to find out\nbecause it is being considered as a noun\nand in noun whenever we give noun\nsuppose let's say that all my words has\nnames and in names obviously those LZ\ndoes not occur right so if I have like\nKrishna if I have Sudan shukumar if I\nhave this kind of words which is name\nright in short this is nouns right we\nhave some famous place name like Taj\nMahal India uh places name and all right\nso this all will be considered as nouns\nright so that basic difference you\nreally need to know in order to\nunderstand this let's say that I want to\nagain convert this into adjective so\nhere you can see eating has become eats\neats writing everything is same right\nbut now let me try with respect to verb\nbecause most of these words are\nbasically verbs itself right so here you\ncan see eating has become eat eats has\nbecome eat writing WR WR programming\nprogram this history now you can see\nthis right uh history is becoming as\nhistory only in stemming we used to get\nsomething like history right so this is\nquite amazing right finally finally and\nthis all and we could also see that in\nstemming right we also have some of the\nwords which is not actually being able\nto perform well right like fairly and\nsportingly and all right and even goes\nright so suppose if I probably go ahead\nand write something like litier do\nlemiti and let's say I'm going to write\nsomething like goes right so I'm going\nto get go which is good right and here I\ncan also play with post tag right so\nlet's say if I'm using a v post tag I'm\ngoing to get go let me go and try with\nthis so here I'm just going to write\nlitier okay and I'm just going to say\ntry to to do with this fairly let's see\nwhat kind of output we are getting now\nthis lemmatizer is super amazing right\nbecause it is giving you the good word\nform and the meaning of the word is also\nmaintained right so here you can see\nfairly sportingly now let's me write a\npost tag obviously because it was noun\nso it may not give you something so here\nalso with respect to verb also I'm\ngetting the same answer which is good\nright so this is just to tell you that\nhow good litiz is but one question that\nI really wanted to ask you\nwhich will take more time wordland nezer\nor stemming the answer is simple\nwordland ntier why because you can see\nnltk provides word ntier crass within a\nthin wrapper around a word net Corpus so\nit is going to compare from there right\nit uses morphe function to the word net\nCorpus reader class to find a LMA so\nthis will basically take time right now\nI just have some number of words so it\nis happening very fast but if you have a\nparagraph if you have a bigger sentence\nlemmatization is going to take time\nright so this this is the basic\ndifference between stemming and lization\nfor which use cases you can use this uh\none simple use Cas is uh if I really\nwant to write uh Q&A chat Bots right\nchat Bots all this is an amazing\nexamples for all this right you can\nbasically use this right so let me write\nit down Q&A chat Bots right uh text\nsummarization right uh Q&A that these\nare some of the examples which you can\nbasically consider uh text summarization\nis also one example and in many many\ncompanies it is being used so right so\nyou can use basically all these things\nand you can basically uh Implement word\nlemmatizer over there because you get\nthe exact good form of the word that is\nthe root form of the word which is\nMeaningful right hello guys so we'll be\ncontinuing the discussion with respect\nto natural language processing we are\nstill in text pre-processing techniques\nwe have seen tokenization we have seen\nstemming we have also seen its different\ntypes along with with that we have seen\nlemmatization now we are going to\nconsider a topic which is called as stop\nwords so in this video I'm going to\ndiscuss about stop wordss the importance\nof stop wordss and again I'll show you\nwith the help of nltk Now text\nprocessing is a very important step in\nnatural language processing because you\nreally need to clean the data you need\nto make the data in the right format\nlater on we'll try to convert all this\ntext Data into vectors and then only\nwe'll be able to train the model because\nmodel you know whenever we say any\nmachine learning model internally we\nreally need to train with some\nmathematical equations so whenever we\ntrain something with mathematical\nequations there we really need to give\nthe input data in the form of numerical\nor floating values so let us go ahead\nand let's just understand what exactly\nstop wordss is so I've opened a new\nnotebook file over here so here I have\none amazing speech from Dr apj Abdul\nKalam he was the former president of\nIndia and it was an amazing speech you\ncan probably read out completely over\nhere and obviously it is given in the\nmaterials now what I'm actually going to\ndo is that I'm going to probably talk\nabout stop words and why it is important\nthat we should try to remove the stop\nwords okay and uh just for a definition\nwhat exactly stop words now here in this\nparticular speech you can see that uh\nthere are a lot of sentences like I have\nthree visions for India in 3,000 years\nof our history people from all over the\nworld have come and invaded us so this\nis the entire speech it is an amazing\nspeech if you're probably learning it I\nwould like to just uh tell you that\nplease read this uh you'll be getting a\nlot of information out of it very\nmotivational speech alog together now\nfrom this particular speech you can see\nthat and I can definitely say this as\nparagraph or Corpus right now here there\nare some words like I The Have and you\nknow uh let's say off uh the you know to\nthere why right all these kind of words\nright it will not play a very big role\nwhen we are doing task like spam\nclassification or let's say if you're\ntrying to do some kind of task with\nrespect to uh you know like spam or ham\nclassification I have already told about\nthat and along with that to just see\nthat whether this is a positive review\nor negative review but some of the words\nlike not can actually play a very\nimportant role not and all so what we do\nis that with the help of stop words you\nknow we try to remove this particular\nwords because uh with this kind of use\ncases where you're specifically focusing\non some of the important words to\ndetermine the outc these all words like\nI the he she of there is not at all\nrequired so what we can do is that we\ncan basically pass this entire paragraph\nto that particular stop words and see\nthat what all words can be basically\nremoved okay and that is the importance\nof stop wordss in short so let us go\nahead right now I'll just go ahead and\nexecute it let me make some cells so\nthat it'll be very much easy for you all\nto understand and how we can apply stop\nwords along with stop wordss we can also\napply stemming so I'll show you both the\ncombination uh which which will be super\nimportant for everyone okay so let's go\nahead and let's uh try to do that okay\nnow first of all I really need to import\nuh for stemming you obviously know what\nwe need to import so from for stemming\nI'll write for nltk do stem import pter\nstemmer so I can basically write pter\nstem over here and I'll execute it along\nwith this uh uh I obviously need to also\nimport Stop words because stop words for\nEnglish it will be different because\nyou'll be having that entire list of\nwords like the he she and all right so\nwhat I'm going to do is that I'm going\nto say that from\nnltk do Corpus import Stop wordss So\nfrom this I will be able to use the stop\nwordss itself so now I have imported uh\nfrom nltk Doc Corpus import Stop wordss\nNow this stop wordss you know I have to\nalso download it so for that uh let me\ndo one thing I'll also import nltk and I\nwill execute it and along with that I'll\nwrite nltk doownload\nand here in this parameter I am going to\ngive about the Stop wordss and there are\ndifferent different language stop wordss\nalso and we'll also try to see that so\nif I probably write this here you'll be\nable to see that downloading package\nstop wordss to this particular location\nso the package stop wordss is already up\nto date and I'm getting true so in short\nI've downloaded all the different\ndifferent languages stop Words which is\nalready present in the nltk library\nperfect now we have done that now let's\nsee that what are the stop words that\nare available in English so in order to\ndo that I I have already imported from\nnltk Corpus import Stop wordss I'll take\nthis stop wordss I'll copy and paste it\nover here and I will say dot download\nokay or instead of download I will just\nwrite wordss and here I just need to\ngive my language like what language I\nreally want to uh give it for like\nEnglish or something else like German\nand all so I'm just going to write this\nlet me just write English and if I\nexecute this here now you can see the\nlist of of all the stop wordss that you\nobviously have and all the stop words\ncan actually be removed right now you\nmay be thinking K this may depend on\ndata to data right now here you can see\nthat guys this is a list you can also\ncreate your own stop wordss in English\nlike let's say over here some of the\nimportant words are like aren't couldn't\nright these all words can actually play\na very important role uh to find out\nwhether a statement is positive or\nnegative like not is also there if you\nprobably search for it not you'll also\nto be able to find it not okay so it is\nalways a good way that you create your\nown stop words and try to remove all\nthose kind of words from the paragraph\nSo I hope everybody's able to understand\nnow with respect to English you have\nthis now let's see whether we have with\nrespect to different different language\nand obviously you can go ahead and check\nthe documentation but I will just try to\nshow you with respect to German so in\nGerman also you have the specific stop\nwords along with this you can also use\nFrench you have this particular stop\nwords so with respect to different\ndifferent text or different different\nlanguage of text you can definitely\napply different different stop words\nwith respect to that now you may be\nthinking is there Hindi or Arabic or\nsome other I think for Arabic also I\nthink it is there let's see whether it\nis there or not yes for Arabic also it\nis there but I do not find it for Hindi\nI guess so again from the documentation\nyou can check it out but till Arabic I\nwas able to see it again all the\ninformation will be given in the NLT\ndocumentation now what I'm actually\ngoing to do is that my sentence is\nalready English right now I'm going to\nperform two important task one is I will\napply stemming and before applying\nstemming you know what I'm actually\ngoing to do wherever I find out the stop\nwords I'm just going to remove the stop\nwords from this particular paragraph so\nthat this entire paragraph will be\nshortened up right so for that what I'm\nactually going to do now see whatever\nthings we have learned from starting\neverything I'm actually going to cover\nup okay so first thing first I'm just\ngoing to say from\nnltk do stem I'm going to import pter\nstemmer port stemma and I'll go just\nexecute I'm just going to execute this\nokay and then what I'm actually going to\ndo is is that I'm just going to write\nstemmer is equal to pter stemmer this we\nreally need to initialize it now when we\ndo this task right now the next step\nwhat I'm actually going to do is that\nI'm going to perform the tokenization on\nthe entire paragraph So for that I can\nuse nltk do sent tokenize and here I'm\njust going to give my paragraph now see\nthis guys here I'm going to get my\nentire par entire sentences like see I\nhave three visions for India then in\n3,000 years these all things I'm I'm\nable to get and this is my second\nsentence third sentence four sentence\nlike this all the sentences in the form\nof list I'm able to get just by using\nsentore tokenize right this is a\ntokenization process wherein we take a\nparagraph divide that into sentences\nokay now let me do one thing is that I'm\njust going to save this in a variable\nwhich is called as sentences which will\nlet later become a list right so this is\nmy sentences and if you probably see the\ntype of sentences I'm just going to\nbasically see this it is a list now\nperfect till here we have done it\namazingly well right we have have done\nport a stemmer on that uh sorry we have\ninitialized stemmer over here and we\nhave tokenized it now understand what we\nare going to do is that I'm going to\nTraverse through all the sentences first\nof all apply a stop Words which all\nwords are not present in the stop words\nwe are only going to take that and apply\nstemming this is what we really want to\ndo so here I'm saying that first of\nall apply stop\nwordss and\nfilter and then apply tokenization right\nsorry then apply stemming so this is the\nstep that I'm actually going to do now\nsee this very simple very important so\nI'll write a for Loop saying that for I\nin\nrange for I in range and here I'm going\nto basically give the length of the\nsentences I can also go with respect to\nsentences but there I'll not be getting\nthe indexes here I'll be getting the\nindexes okay so range basically says\nthat whatever length I'm actually giving\nthat becomes an index right 0 to that\nspecific length now what I'm actually\ngoing to take I'm going to take this\nspecific n um I and I'm going to write n\nnltk dot word tokenize because I'm\ngetting in the form of sentences I need\nto get each and every word right so I'm\ngetting the word over here and inside\nword unders token I I'll give I sorry\nsentence of I because this will be an\nindex sentences of I so this will be an\nindex and here I'll be able to get the\nword so here what I'll do I'll make a\nlist of words so in short I'll be\ngetting the list of words inside the\nsentences perfect now till here we have\ndone it now after this we are going to\napply one very important thing that is\nfirst of all I need to apply stop words\nfor each and every word and see whether\nthat it falls in the stop word or not if\nit does not fall in the stop word then\nonly I have to do the stemming so\nunderstand the task step by step this is\nsuper important with respect to all the\nsteps that I'm actually taking up okay\nso here I will write a list\ncomprehension I will say stemmer do stem\nokay and here I'm going to write word\nokay of the word because uh from this\nparticular words this words is a list of\nwords and I have to take each and every\nword so here I will write a for Loop\nokay and this is called as list\ncomprehension so I'll write for word in\nwords if word not in see if the word is\nnot present in the stop words then only\nyou apply stemming that is what I'm\nactually trying to do okay so here you\ncan basically see if the word is not in\nI'll use a set\nalong with that I will download all the\nall the stop words with respect to\nEnglish right so why I'm using step set\nbecause some of the words may get\nrepeated so I don't want that so I'm\ngoing to basically write it over here\nright now through this what I'm actually\ngoing to do I'll get that specific word\nthat is not present in the stop words\nand only stemming will be getting\napplied to that specific Word Perfect\nnow here what I'm actually going to do\nis that I'm going to save it in a\nvariable called as words perfect I hope\nit is very much clear I'm getting back\neverything after doing the stemming back\nin the words itself and then finally\nwhat I'm actually going to do is that\nI'm going to take the sentences and I'm\ngoing to replace it on the same index\nwith respect to this words but once we\nget this words right I need to join all\nthese words together so how do I join\nthere will obviously be a space dot I'll\njust use do join so that it'll join\ntogether and it'll convert that into a\nsentences so this exactly is\nconverting all the\nwords into sentences right this is very\nmuch simple and we have actually done\nthis perfect so here what all things we\nhave done again let me repeat I'm I'm\niterating through each and every\nsentence and then I'm doing a word\ntokenize that basically from every\nsenten I'm getting the list of words and\nfrom that list of words I'm iterating\nI'm seeing that whether it is present in\nthe stop words if it is not present I'm\ndoing the stemming after stemming I'm\nagain storing back in that same list and\nthen I'm converting all these words into\nsentences I can say converting all the\nlist of words into sentences perfect now\nonce I execute it and now if I go and\nsee my\nsentences here you'll be able to see now\nI3 Vision India right in 3,000 year\nhistory right so here h h i s t o r y\nbecame r i okay people people it became\npeople world came invaded us invade it\nbecame capture land conquer mind from\nyou can see all the special words like\nlike I I have everything is gone see see\nthis I is there have is gone right the\nyou'll not be able to find out the\nanywhere right so whatever stop words\nwere present over here that all got\nremoved and then Only We performed the\nstemming right now you may be saying\nKish uh the stemming does not look very\ngood right so for that what you need to\ndo I've already taught you with respect\nto snowball stemmer so I will just\nimport this this okay and it is very\nsimple I think you can do the same task\nagain so snowball stemmer and then I\nwill try to import this with respect to\nEnglish and obviously after this you'll\nbe able to get a good sentence right so\nlet me just remove this one so snowball\nstemmer I've already done it and I'm\ngoing to copy the same thing okay and\nhere I'm just going to say apply\nsnowball stemmer stemming right and then\ninstead of stemmer I will just write\nthis word that is snowball stemmer\nthat's it yeah and now once I execute it\nuh let me go back again back to the\nsentences because that sentences have\ngot changed now so where is the\nsentences let's see okay uh this is the\nsentences I've got executed now now I'm\njust going to execute this now if I\nprobably go and see my sentences here\nyou can see that now it is good right\nnow snow uh one more important thing\nthat snowball has done see over here I\nstill have capital letters right like\nthere may be some of the sentences which\nmay be in small letter it also so that\nbecomes a repeated word but since this\nis a capital letter it will be\nconsidered as a separate word right for\nthe model to understand right so what it\ndoes is that snowball one more Advantage\nis that it is making sure that all the\nletter is becoming small right so all\nthe letter is becoming small and for\nsome of the words it is not even giving\na good result like poverty has become\npoverty but if you try to do this with\nthe help of lemmatization you can also\nget a good word now let's try it with\nthe help of lemmatization so I'm just\ngoing to uh do the same thing okay I\nhope everybody is understood with\nrespect to snowball stemmer now what I'm\ngoing to do is that again going to go\nback to my limitation code so I'm just\ngoing to import this nltk do stem and it\nis very simple guys I think uh we are\njust repeating things so that you also\npractice in a better way okay so here\nI've got the word net climatizer I'm\ngoing to initialize it perfect this is\ndone now I'm going to go go ahead and\ncopy the same code right and I will just\nwrite it over here and instead of\nwriting snowball I'm just going to copy\nthis I'm going to paste it over here\nperfect so I've done it but let me just\ngo ahead and execute the sentence part\nagain because I need to get the updated\nsentence so I think it is somewhere here\nparagraph okay and uh here is a sentence\nokay perfect now let me just go ahead\nand execute the same thing and now if\nyou okay I'm getting has no stem okay\nsorry it should be\nlemiti lemiti okay so liter. lemiti now\nyou see some time it took right because\nit is basically checking from the entire\ncorpus now if I probably go and see the\nsentences now you have all amazing\nthings with respect to words that are\ncoming up correctly and all the things\nare here right so with respect to this\nyou are able to get some good thing now\nnow one more thing you can do is that\nafter this word you can also put the\npost tag now if you put the post tag to\nV right now you see what will be the\noutput you'll get a better output I\nguess because most of the words it'll be\nbasically considering as um you know as\nan adverb oh sorry as a verb so but\nanyhow we will try to understand about\npost tag again more more things right so\nI3 Visions India in 3,000 years history\npeople come would world come invade US\ncapture land so all the stop words has\ngot deleted now we are getting a very\ngood one you know uh at least better\nthan stemming okay and the other one\nthat is snowball stemming right so this\nwas the entire process with respect to\ntext pre-processing and here we have\ndiscussed with stop words and how you\nshould also go ahead and do the text\npre-processing I hope everybody got that\nidea now in lemmatization also you can\nsee that it is not lowering it so what\nyou can actually do is that you can\nbasically lower all the sentences right\nso let's say if you write sentences of I\nis equal to sentences dot of\nI dot to lower right so you can\nbasically give two lower let's see\nwhether it will work or not uh and uh\nlet's see whether it will completely\nwork or not I'm I'm not sure whether dot\nto lower will work with respect to list\nbut I think it should work I'm just\ngoing to execute this again go down okay\nand apply this litier\nokay uh Str Str object has uh no\nattribute to lower okay it's okay not a\nproblem not a problem what I'm actually\ngoing to do I'm just going to comment\nthis and before writing here I'll just\nsay word to\nlower okay you have to definitely try\ndifferent different things okay and it's\nall about Google you know just Google it\nyou'll be able to get it so again I'm\ngoing to execute this and now I think\nit'll execute I know it is I'm doing a\nlot of up and downs but just try to\nfollow the lecture uh string object has\nno attribute to lower why two underscore\nlower is\nthere m two lower um okay let me just\nsee uh Str Str to lower case right\npython let me just see this it's okay if\nI don't have any other way to see it but\nI think do lower will definitely work\nlet's see dot\nlower okay perfect it has worked now now\nif I go and see the sentences it is done\nokay so I don't have any any regrets to\nsearch in the Google you should also do\nthe search in the Google so now here you\ncan see all the small letters are there\nalong with the limitation so we are\ngoing to continue the discussion with\nrespect to NLP and in this video we are\ngoing to discuss about something called\nas parts of speech tagging now I hope\nyou have understood in limitation this\nparts of speech tagging it plays a very\nimportant role because if we are giving\nit as a verb or a noun right based on\nthat you know we are able to get the\nroot form of the specific word right and\nwe had seen a lot of examples with\nrespect to this in this video we'll try\nto understand that how many different\ntypes of parts of speech tagging is\nthere and with respect to that we'll\nalso see a practical example let's say\nthat if I am actually getting a sentence\nlike this Taj Mahal is a beautiful\nMonument How with the help of nltk we\nare able to understand that and you'll\nbe seeing that the output of this let's\nsay I'm just taking this particular\nexample the output for this will be that\ntajil will be considered as a noun\nbeautiful may be considered as a\nadjective Monument can be considered as\na verb right I'm just giving as an\nexample but we'll definitely see this\nparticular example and we'll also see\nsome extensive example altogether now\nwith respect to parts of speech how many\ndifferent types of things are there\nright so here you have something called\nas CC coordinating conjunction CD\nCardinal digit DT determiner ex uh\nexistential there FW foreign word I in\npreposition JJ adjective jjr adjective\nso that basically mean suppose if I'm\ngiving a sentence automatically it will\nbe able to categorize in all this\nspecific different different parts of\nspeech automatically with the help of\nnltk and similarly there are a lot of\nthings over here you can see a personal\nNow personal pronoun like uh for the\nletters like hi he she will be put up as\nPRP right it'll be getting a tag called\nas PR P if you see over here some more\nexample like RB adverb right words like\nvery silently will be put up over here\nrbr it it is also adverb but it will be\nlike a comparative one right so example\nlike better so you have RBS adverb like\nthis you have lot right so what I'm\nactually going to do is that I'm going\nto take a very good example and one\nassignment you just have to do is that\ntake this particular simple example and\ntry to find out and try to write down\nthe comment in the comment section okay\nnow over here you'll be able to see that\nI have this particular speech of Dr uh\napj Abdul Kalam and obviously the same\nexample I've shown you in the stop\nwordss example itself right so I'm going\nto take this very simple okay and then\nlet's consider that uh in this parts of\nspeech tagging I really wanted to\nperform uh stemming let's say but uh I\ndon't want to perform any stemming\nbecause I want to see for each and every\nword right what kind of post speech\ntagging will be there so no need to\nimport all these things okay so I'm just\ngoing to uh remove all these things\ninstead I'll just be focusing on\nimporting nltk so I'll write import nltk\nand then here you can see that I'm just\nwriting nltk do sentore tokenize so that\nbasically means I'm actually converting\nthe paragraph into sentences so let me\njust execute this and once I get it here\nyou'll be able to see all the sentences\nso in this example I'm trying to show\nyou that how for each and every word\nwe'll be able to find out the post tag\nokay so the next thing is that now what\nI'm actually going to do is that apply\nthe stop wordss okay and we need to\napply the stop words also because we\ndon't want to remove anything so I'm\njust going to uncomment all these things\nand I'm going to say that we will find\nout the PA tag okay find out the PA tag\nright so this is perfect uh till here we\nare going good right now what I'm\nactually doing over here uh it is very\nmuch Clear very much easy to understand\nthat uh over here we are able to uh do\nthat like I'm I'm doing I'm just simply\nputting a for Loop over here with\nrespect to all the sentences and this\nsentences here what I'm doing is that\nI'm just trying to convert this into\nwords I'll be getting a list of words\nand now no need to do the stemming so\nI'll just remove the stemming okay and\nuh over here I'll just add this W word\nitself I'll be getting the word and I'm\njust saying that if the word is not in\nset stop words do wordss of English so\nuh let's see that okay I told you that\nI'll not apply stop wordss but let's let\nus just remove the stop wordss because\nsome of the stop wordss will not be\nplaying an important role so in order to\ndo that uh I'll just uh import all the\nlibraries with respect to stop word and\nobviously over here I had already\nwritten that but let me do contrl Zed so\nhere it is stop words with respect to\nthis and I will also remove this okay so\ntwo things and I will just import the\nstop wordss so I'm writing that from\nnltk Corpus import Stop wordss and now\nlet's apply for this same thing whatever\nwe have done in the previous time uh the\nsame part we are actually repeating it\nso here you can see that for word for\nword in words if word not in this\nparticular stop word with respect to\nEnglish I'm going to take all the words\nand then I'm just going to comment down\nthis code because I'm not going to join\nthem back into the sentences because I\nreally need to understand what pause tag\neach word will be getting so I will\nbasically write print okay so here I'm\njust going to write a simple print\nstatement and in this particular print\nstatement uh before printing this also\nwhat I can do I can write basically this\nin the next line here I'll be using\nsomething called as nltk do post tag\nokay so here you'll be able to see post\ntag I can also apply it for words and I\ncan also apply it for sentences right\nnow I'm just going to apply this for the\nwords itself so this is my entire thing\nand uh here I'll just create a uh P _\ntag variable so this will basically\nindicate that everything is getting\nstored over here and with respect to\nthat I'm just going to print this now\nlet me revise it again what all things\nwe did I'm iterating through each and\nevery sentence and then I'm converting\nthis particular sentence into words and\nfor each words I'm applying stop words\ninitially I thought that I'll not apply\nbut let's apply the stop words because\nsee at the end of the day they are so\nsmall small words like isn't the he she\nI don't want this particular words right\nso I'll I'll remove the stop words and\nI'll will take all the list of words\nover here I do not apply stemming\nbecause I really need to find out\nwhatever words are present over here it\nneeds to be uh like this NK will be able\nto find out all the different different\ntypes of parts of speech tagging with\nrespect to that and then I just apply\nthis particular simple code that is nltk\ndo postore tag ofws and then finally I'm\nprinting it so here you can see I'll\njust execute it I'm getting some error\nI'm not going to uh make sure that\nnothing I'm not going to like if there\nis an error I'm not going to edit the\nvideo so here it says nltk do download\naverage percepton trigger tagger so I\ndefinitely require this particular\ntagger to apply post tag so what I will\ndo is that I'll just go and uh copy and\npaste this you also have to do this\nbecause you will also face this\nparticular ER now here you can see that\nit has been downloaded now if I probably\ngo ahead and run it now here you'll be\nseeing for the first sentence I is\nbasically a PRP PRP basically means what\nso if you probably go over here and see\npersonal pronoun and I had given the\nexamples like I he she right similarly\nyou'll be able to see that other words\nlike three three CD Vision nns uh India\nnnp let's go ahead and see what is nnp\nbecause India has been categorized with\nnnp so nnp is nothing but proper noun\nsingular it's like a name place it can\nbe monuments it can be different\ndifferent things like nnps is pro proper\nnoun plural Americans Indians like that\nright so all those things are there and\nhow easily it is being able to Simply\nyou know just show the post tags right\nwith the help of nltk this is an amazing\nthing and here with respect to all the\nsentences you'll be able to see this\nokay and that is how powerful nltk is uh\nlet me just show you whatever assignment\nthat had actually given you right so\nthis was a sentence over here I'm just\ngoing to copy this and I'm just going to\ntake this entirely okay and let's see\nwhether we'll be able to do it so I'm\njust going to use\nnltk do PA tag sense now let's see with\nrespect to sense also if you're able to\ndo this or not so I'm just going to put\nthis Taj mahel is a beautiful monument\nand I'm going to execute this now here\nyou can see T okay now in this\nparticular case what has happened each\nand every word has been been considered\nover here right this should not\nbasically happen so what I'm actually\ngoing to do is that I'm going to\nbasically use post tag let's see what is\nthe output with the respect to this so\nhere also the same thing is coming so\nthe simple way in this particular\nscenario is that again I'll put a for\nLoop for I\nin okay I'll say sentence or I in in\nthis entire quotes I'll just put this\nI'll go through each and every word over\nhere\nokay\nand I can write dot split with respect\nto this because if I'm just using this\ndot split this is my string okay and if\nI probably use do split you'll be able\nto see that what I'm getting Taj mahel\nis a beautiful Monument right so for I\nin this uh and here you can see that I'm\niterating through each and every things\nright each and every word so I'm just\ngoing to\nprint nltk dop tag and I am actually\ngoing to to use this so you here you can\nuh just go ahead and write this specific\nI or instead of writing this I I'll just\nwrite\nwords okay for word in this nl. post tag\nI'm just going to use this specific word\nnow here uh still uh there is an issue\nbecause I am iterating through each and\neverything so if I probably get this\nentire thing uh with respect to the list\nyou'll be able to see okay uh let's see\nlet's see let's see there's something\nagain I'm not going to delete it\nI'm just going to see what is the word\nover here so I'm getting this okay and\nif I take this and ltk do Post\ntag with respect to this specific word\nuh okay this is the problem let's see\nwhat did I do over here uh I need to\nprovide a list of words okay and then\nI'll be applying this okay this is the\nproblem okay now uh all you have to do\nis that I don't have to to give word by\nword if it is giving word by word here\nyou can be able to see for every single\ncharacter it is giving this right I\nreally need to provide the list of words\nso if I probably just copy this part and\nput it over here and remove all these\nthings now what I have actually done\nafter this error see I'm not editing\nthis particular video with respect to\nthis error because you really need to\nsee all these errors in short when I\ngive like this I'll be getting a list of\nwords so if I execute this here now you\ncan see that Taj it is showing nnp Mahal\nnnp is is vbz a DT beautiful JJ Monument\nNN now see what did I do over here post\ntag basically uh whatever parameter we\ngive we should be giving in the form of\nlist of words and if you are able to do\nthat we will be getting the same answer\nright so guys I hope you are able to\nunderstand parts of speech tagging I\ndefinitely got some errors but I really\nwanted to show you all the errors that\nwe are getting hello guys so we are\ngoing to continue the discussion with\nrespect to natural language processing\nand in this video we are going to\ndiscuss about about name entity\nrecognition this is an amazing topic so\nlet me open a file over here so here you\ncan see that let's say that we have a\nlot of sentences right like one example\nof this particular sentence is over here\nthe FL Tower was built from 1887 to 1889\nby French engineer Gustav eel uh whose\ncompany specialized in building metal\nFrameworks and structures right now this\nis a sentence now from this particular\nsentence we know that what is parts of\nspeech tagging right which is noun and\nall but along with that with the help of\nnltk we will also be able to get\nsomething called as name entity tags\nright now what are some of the examples\nof name entity tags so here you can see\none of the tag is something called as\nperson the second tag can be location or\nplace third tag can be date time right\nand here also I have given some examples\nso if I probably say FL Tower it may be\ncoming as a place it can come as a\nlocation right over here you can see\nthat okay GF F this will basically be\ngetting tagged as a name itself right\nhere suppose if this numbers are\nactually there it can be something else\nright suppose if you're giving some\nmoney value like $1 million so let's say\nthis $1 million is present somewhere\nhere right and uh you know that will be\nbasically tagged as something called as\nmoney right so it'll be given some kind\nof name entity tags right so let us see\nsome examples and let us see that how\nthis name entity recognition will be\ngiven with the help of nltk library to\nbegin with what I'm actually going to do\nI'm going to take this particular\nsentence I'm going to just execute it\nover here okay and again we'll be going\nlike uh how we did in the past videos\nitself so let me do one thing over here\nlet me create some more cells okay now\nthe first word first thing I will just\ngo ahead and import nltk and as you know\nthat uh with respect to nltk also I can\nuse something called as uh word\ntokenize word word\ntokenize word unor tokenize and here I'm\njust going to give all my the complete\nsentence itself so here uh let's go\nahead and give the sentence over here\nand once I execute it here you'll be\nable to see that the FL Tower was\neverything is coming over here so this\nis my entire words the list of words\nright so this is my entire list of words\nnow perfect now see this usually uh for\ndoing the parts of speech tagging what\nwe what do we do we basically write nltk\ndo Post underscore tag and I basically\ngive this entire words to this and based\non this every each and every word will\nbe assigned to some tags okay so let me\njust write this is uh as my tagged uh\nelements or tagged something I'm just\ngiving a variable name which is like\ntagged elements and this will get stored\nover here okay now see this if I really\nwant to provide some uh named entity\nrecognition all I have to do is that use\nthis\nnltk dot named entity so there is\nsomething called as n NE e okay so let\nme just show you\nNE uh NE chunk okay so NE chunk is a\nfunction over here and if I probably see\nthe definition of this particular thing\nuh function you'll be seeing use nltk\ncurrently recommended name entity\nchunker to chunk the given list to or\ngiven list of tag tokens okay and inside\nthis I will just past my all my Tagg\nelements okay so this will be my tag\nelements I'm just going to give this\nprobably we may get an error the reason\nis there because we need to download\nthis one right so here you can see that\nnltk doownload maxnet any chunker\nbecause here we are specifically using\nsome chunker techniques to basically get\nthe named entity and for that we really\nneed to download it and that is what is\nthe first requirement so here you can\nsee that I'm downloading it and once\nthis downloaded gets uh once this entire\nthis uh Max Max and any chunker gets\ndownloaded we are good to run the\nspecific code so this may take some\namount of time because this may also be\na huge uh it can be a huge library\ninside it which will needs to be\ndownloaded so now I will just go ahead\nand execute it still I'm getting some\nerror It also says that okay you need to\ndownload nltk do download wordss perfect\nso what I'm actually going to do go and\nmake one more cell and please don't\nworry whenever you get some errors I've\nseen some people who who gets worried\nfirst of all just go ahead and see the\nerror what exactly it is it is very much\nsimple and you just need to execute it\nso guys now once this NLT or download of\nwords gets downloaded now all I have to\ndo is that over here you'll be seeing\nthat nltk neore chunk I have to give the\ntag elements and then I just need to\nwrite dot draw once I execute this here\nyou'll be able to see this what an\namazing graph I'm able to get now see\nthis everybody observe this I don't know\nwhether you able to see this properly or\nnot but just clearly if you will be able\nto see it it has given most of the\ninformation very much clearly so here it\nis very much clear that you uh this\nentire sentence right the organization\nis recognized for FL and Tower right\nwhich is a noun right over here you can\nsee this word was is a vbd built it is\nover here 1887 is something like CD and\nhere you can see GP right French JJ\nright it is being able to determine as\nJJ here you can see person information\nis being able to get captured which is\nlike Gustav NP so here you can clearly\nsee all this information very nicely\nright so which which ever entity is been\nable to get recognized here you'll be\nable to find that so person is there GP\nis there and organization is there what\nthat is what we will be able to find out\nthis s is the entire sentence over here\nbut with this graph you'll be able to\nunderstand that you're able to get the\nentire information so like this if you\nprobably go and see that what all uh\nother entities you can find person place\nlocation date time money and all so I\nhope you were able to understand about\nnamed entity recognition and how you\nwill be able to see this and how you'll\nbe able to see the diagrams so yes this\nwas it with respect to this particular\nvideo but understand that nltk provides\nthis amazing feature you should\ndefinitely use it when it where it is\nrequired hello guys so we are going to\ncontinue the discussion with respect to\nNLP already we have already seen that\nnow what is our next step you know after\ntext pre-processing where we have\nspecifically performed stemming\nlemmatization stop wordss and we have\ncleaned the data right now the main\nthing is that we really need to convert\nthe text into vectors and they are m\nmultiple ways the first way that we are\ngoing to discuss about is something\ncalled as one hot encoding okay now\nwe'll try to understand how how with the\nhelp of one hot encoding we are\nconverting the words into a vectors\nitself so let's take this specific\nexample so here I have a text and a\nspecific output right our main aim is\nthat first of all we need to take this\nparticular text and we need to convert\nthis into vectors okay now with respect\nto this obviously uh I'm also not going\nto again Lower all the sentences of\nwords but let me focus on more on like\nhow you can basically implement this one\nhot encoding and what is the theoretical\nintuition behind it and how the vectors\nare actually created so uh to begin with\nover here let's say that this is my text\nthe document one is the food is good the\nfood is bad P Piza is amazing okay so\nthis is basically Piza is amazing okay\nnow how do you find out how many unique\nvocabulary is there because see the\nunique vocabulary will play a very\nimportant role okay while creating your\nvectors okay so to begin with what I'm\nactually going to do is that we just\nneed to find out how many unique words\nare there so let me go ahead and write\nit down and obviously you know that if I\ncombine all these three documents it\nbecomes a paragraph or Corpus so let me\njust go ahead and let me just write down\nall the unique unique documents so the\nis one unique uh Sor not document\nvocabulary the is there food is there\nright is is there right\nand good is there so over here you can\nsee good is also there then from the\nsecond sentence you can see the food is\ngetting repeated so I'm just going to\nwrite bad okay and if I go probably to\nthe third sentence or the documents here\nyou'll be able to see Piza is there\nwhich is a unique word or unique\nvocabulary is is again getting repeated\nand finally we have something called as\namazing so all these words that are\nprobably present over here these are my\nunique vocabulary right vocabulary so\nthese are just nothing but the unique\nwords unique words that are available in\nthis entire paragraph or sentence or in\nthis particular data set now based on\nthese unique words one hot encoding\nmeans what okay now let's say if I\nprobably consider this particular\ndocument one D1 okay so let's say this\nis my\nD1 now what does D1 do is that like if\nwe are probably applying one hot\nencoding we will convert all the\nspecific words into this Vector\nrepresentation that basically means\nlet's say if I'm considering the word\nthen the vector representation for the\nword will be 1 0 0 0 0 0 0 why because\none is present over there that basically\nmeans the is present in this specific\nword so this is basically becoming one\nif I consider for the next word that is\nfood then how it how we'll be able to\nrepresent it it will be 0 1 0 0 0 0 0\nright so here clearly you'll be able to\nsee wherever those specific word is it\nwill be represented by a vector which\nwill be of a dimension V okay now what\nis this v v is nothing but the words\nthat are present over here so here\nyou'll be able to see one word two word\n3 word four word five word six word\nseven word so so the unique vocabulary I\nhave seven so it is going to be\nrepresented by the seven vectors where\nfor those specific word it will be\nrepresented as one and remaining all\nwill be represented as zero right so now\nif I probably consider how can I\nrepresent this D1 that is the food is\ngood with respect to this particular\nvectors okay so now understand that in\nthis particular food uh there are four\nwords right 1 2 3 4 so over here with\nrespect to D1 the first word will have a\nrepresent of 1 0 0 0 0 so how many zeros\nare there 1 2 3 4 5 6 right 1 2 3 4 5 6\nright so this will be my first word okay\nthen coming to the second word now the\nsecond word over here is food right so\nhere you'll be able to see 0 1 0 0 0 0 0\nright so this will be my second word now\nif I come to my third word it is\nbasically is right so is is basically\nrepresented by 0 0 1 0 0 0 0 so this is\nmy third word and coming to the four\nword which is good so here it will\nbecome 0 0 0\n1 0 0 0 right so this is my sentence one\none hot code rep one hot code\nrepresentation with respect to this\nparticular text so this text if I apply\none hot encoding I'm going to get this\nkind of representation so guys now if I\ntalk about the shape of this D1 sentence\nwhich has four words right so here you\ncan see the first word representation is\nthis one the second word representation\nis this one vector representation third\nword Vector representation is this one\nand fourth word Vector representation is\nthis one so if I probably see the shape\nright it is basically 4 cross 7 four\nwords 1 2 3 4 okay and seven seven uh\nvocabulary size right 1 2 3 uh 4 5 6 7\nokay now what we are going to do we are\nalso going to do the same thing for D2\nokay so let's go ahead and try with\nrespect to D2 I would suggest just pause\nthe video Try it by yourself and then\nprobably continue the video anyhow I'll\nbe showing you the entire uh\nrepresentation again okay so over here\nyou'll be able to see that which is my\nfirst word again the in D2 I have the\nfood is bad right so again these two are\nsame these three words are same so I'm\ngoing to basically write the same\nrepresentation it will be 1 0 uh 1 0 0 0\nI think there are 6 zos 1 2 3 4 5 6 zos\nokay so this will be my first word\ncoming to the second word this is this\none okay I'm just going to write this\nover here okay so this is my second word\nso here I'm going to write 0 1 0 0 0 0 I\nhope the size is seven 1 2 3 4 5 6 7\nperfect so uh coming to the third word\nnow here you'll be able to see 0 0 1 0 0\n0 0 okay now here you can see this food\nis the food is it is a similar word like\nD1 only so the first three is just a\nreplica coming to the last word right\nnow if I say last word over here you can\nsee good over here you can see bad now\nwhenever you have bad this particular\nword will become one right so you'll be\nhaving 0 0 0 0 0 0 0 0 0 and then 1 0 0\nso I'll just go ahead and write 0 0 0\nagain 0 1 0 0 perfect so this is how the\nD2 is basically repres presented and\nagain over here you'll be able to see\nthe shape will be somewhere around 4\ncross 7 okay now uh yes uh this was the\ntechnique simple technique of one\nencoding like how we converted a text\ninto a vectors uh over here you can see\neach and every word is basically\nrepresented by a one hot encoded based\non the vocabulary size whatever the\nvocabulary size is basically present so\nhere you can have you can see that I\nhave a vocabulary size of seven So based\non that one hot repres presentation is\ngiven for each and every word okay now\nuh going forward in the next video we\nare going to discuss about all the\nadvantages and disadvantages of this and\nagain one hot encoding is not getting\nused for this NLP use cases we have\ntechniques like bag of word and tfidf\nwe'll also understand what are the\nadvantage and disadvantage why should we\nnot use this and then we'll be trying to\nunderstand the bag of wordss and TF IDF\nokay hello guys so we are going to\ncontinue the discussion with respect to\none hot encoding and NLP and I hope you\nhave understood that how we can convert\nwords into vectors and probably the\nentire process I've actually explained\nyou over here in this video we are going\nto talk about the advantages and\ndisadvantages of using one hot encoding\nso let me go ahead and let me write it\ndown so here are the\nadvantages okay and here are the\ndisadvantages okay now first of all the\nbasic Advantage is that it is very easy\nto implement with Python programming\nlanguage\nright easy to implement with python now\nwhy I'm saying you that because in SK\nlearn we have a specific Library where\nwe can easily implement this and it is\nbasically called as one hot encoder okay\nand in pandas if you're familiar with\npandas Library we have something called\nas\npd.\ngetor dummies right so this function\nwill basically help you to create this\nentire one hot encoding based on the\nwords right and again the second\nadvantage and obviously uh we'll see\nthis kind of examples as we go ahead but\nagain I'm not going to implement\nspecifically separately as a video in\none hot encoding because we don't mostly\nuse this NLP in NLP technique and I'll\ntell you why because there's a lot of\ndisadvantages also now let's go ahead\nand talk about the disadvantage the\nfirst disadvantage over here you see\nthat over here at the end of the day if\nI probably consider consider\nright the food is good right at the end\nof the day I'm getting 1 0 0's and for\nthe next word I'm getting the 0 1 0 0\nand there's so many number of zeros and\nones in in arithmetic in in linear\nalgebra we basically say this as sparse\nmetrix okay so this basically creates a\nsparse metrix right now what is sparse\nMatrix exactly to talk about sparse\nMatrix is that in an array in metrix you\nhave lot of ones and zeros okay and what\nis the problem with respect to sparse\nmetrix I can also say this as we can\nalso convert this into an array also so\nwe can also say this as sparse arrays\nbut understand what is the disadvantage\nwith respect to sparse metrics whenever\nwe apply any machine learning\nalgorithm this specific machine learning\nalgorithm you know after we convert the\ntext into vectors right in most of the\nmachine learning algorithms this leads\nto something called as overfitting right\nnow what exactly is overfitting\noverfitting is a process wherein you get\na very good accuracy with the training\ndata but with respect to any new data it\nwill not be able to give you a very good\naccuracy so this parse metrics usually\nleads to something called as overfitting\nright so this is the one of the major\ndisadvantage now let me talk about the\nnext disadvantage over here okay now\nover here let's say that I have this\nwords like the food is good the Piza is\nthe sorry the food is good the food is\nbad Piza is amazing right now see one\nthing over here right in any machine\nlearning algorithm whenever you give the\ndata right and in this case also if I\nprobably say this vocabulary right these\nare all my features the food is good bad\nPizza amazing right now over here you\ncan see that most of the time my inputs\nfor every word that is getting converted\ninto a vector the size is seven right 1\n2 3 4 5 6 7 right and based on the\nnumber of words here you'll be seeing\nthat I'm I'm having four words so I'm\ngetting 4 cross 7 here also I'm having\nfour words which I'm actually getting 4\ncross 7 but if I consider with respect\nto the third statement or third sentence\nright so if I probably consider\nD3 right and if I start creating a one\nhot encoded format for this so what it\nwill be see Piza is there right so Piza\nwhere it is one at this particular\ninstance so I will probably create\nsomething like this 0 0 0 0 0 1 0 right\nso this is Piza right is again we have\nis over here is is nothing but this will\nbe one and remaining all will be zero so\nI'll just write 0 0 1 0 0 0 0 right and\nthe third statement is amazing which\nwill be the last one as far as I\nremember from the vocabulary so this is\nhow my D3 looks right in one hot encoded\nformat but one one thing you really need\nto understand over here the size is 3\ncross 7 right now in machine learning\nyou need to understand one thing\nwhenever we perform NLP or let's say any\nmachine learning use case the number of\nfeatures should be fixed with respect to\nthe length but here you can see this is\n4 CR 7 this is 4 CR 7 this is 3 cross 7\nso I cannot train I cannot train my this\nparticular data for a machine learning\nalgorithm because over here we still\ndon't have a fixed text size right so\nthis is one of the major major\ndisadvantage unless and until and with\nthe help of one notot encoding we are\nnot getting a fixed text size right over\nhere I got 4 cross 7 4 cross 7 if this\nwas also 4 cross 7 I could have trained\nit right so over here I'll say that for\nML\nalgorithm we need we need fixed size we\nneed fixed size input right and right\nnow it is not there so this is again a\nmajor disadvantage right we'll be seeing\nin the upcoming lectures how with the\nhelp of bag of words DF IDF will'll be\ngetting a fixed size of words okay now\nthe third one that you'll be seeing most\nof the times we are finding zeros and\nones right zeros and ones in zeros and\nones most of the times see if if a\nspecific word is there that will become\none and remaining all will be zero but\nif I talk about the semantic meaning\nbetween these two words like the and fo\nright we are not able to exactly\ncalculate like how far how equal it is\nhow similar that specific word is and\nthis process is basically called as\nsemantic so here I will say that no\nsemantic meaning is getting\ncaptured no semantic meaning is getting\ncaptured now let me talk about this in a\nvery good example okay let's say I have\nsomething like this\nfood\npizza burger okay now you know that\nlet's say that in my vocabulary I have\nthree words so for the foot\nrepresentation I will basically write it\nas 1 0 0 for Piza let's say I'm going to\nwrite it as 0 1 0 and for Burger I'm\njust going to write it as 0 01 Now\nunderstand these are my\nvectors and right now since there are\nthree features I'm basically having\nthree\nvectors now if you have probably heard\nof something called as cosine similarity\nor if you really want to find out the\ndistance between this Vector to this\nvector and then this Vector to this\nvector if I probably consider this let's\nsay I'm just going to draw a three\ndimension okay let's say this is being\ndetermined by something like burger and\nthis is being determined by something\nlike food and this is being determined\nby something like\npizza now if I talk about all these\nthings right like let's say in case of\nfood it is 1 0 0 so in this axis I will\nprobably getting one so this will be\nrepresented by 1 comma 0a 0 in three\ndimension and if I talk about p is a 0 1\n0 so this will be my another Point let's\nsay I'm just going to denote this by\nanother Point 0a 1 comma 0 and let's\nlet's talk about the third point over\nhere the third point will be somewhere\nhere burer it'll be in the same distance\nso this will be 0a 0a 1 now if I\nprobably calculate the distance between\nthis all these things it will almost be\nequal right so it is not being able to\ntell the exact difference between food p\nand burger it is obviously considering\nthat okay all this words are in equal\ndistance so we cannot understand that\nhow this particular word is different\nfrom\nthis right this is super important to\nunderstand so in short what I am\nactually trying to say over here is that\nno semantic meaning is basically getting\ncaptured that basically means in this\nparticular sentence I'm not able to\nunderstand which is the most important\nword how this word is related to this\nword or how this word is much more\nsimilar to this word so that information\nis not getting captured because at the\nend of the day I'm getting one zeros\nokay so this was the uh third major\ndisadvantage now talking about the\nfourth disadvantage and this is also\nvery super important and this particular\nconcept is something called as out of\nvocabulary out of\nvocabulary and what does this basically\nmean o okay now what what is this all\nabout let's say\nthat right now I have this many\nvocabularies of word let's say for the\nafter I train my model now I want to\ntest it for my new data set so for\ntesting on my new data set this will be\nmy test data let's say I will say\nBurg is bad and I need to predict this\nso this is my test data and I need to\npredict this now you know that over here\nwith respect to this particular\nsentences okay Burger is nowhere present\nin this particular uh vocabulary so what\nwill be the problem we will not find out\na way we will not have have any way to\nrepresent this burger in the form of\nvectors right we'll not be able to form\na vectors over here because anyhow we\ndon't have a vocabulary word so what\nwill happen in this particular case and\nobviously we will not be able to perform\nthis one hot encoding because in our\nvocabulary I just have this many number\nof words so this will not work when a\nnew word is actually coming wherein it\nis not present in the vocabulary with\nrespect to the test data and obviously\nthis is again a major disadvantage right\nso this technique is basically called as\nout of vocabulary so this was in short\nabout the advantages and disadvantage of\nthis understand sparse metric basically\nmeans many ones and zeros it leads to\noverfitting with respect to various\nmachine learning algorithm then for any\nmachine learning algorithm I definitely\nrequire a fixed size input right now we\nare not able to get it that basically\nmeans all my sentences should be of\nfixed size it will not work no semantic\nmeeting sorry meaning is getting\ncaptured I've already told you because\nif you try to calculate a distance all\nthis words words are equidistance it is\nnot saying that whether Pizza is very\nmuch similar to food or Burger how much\nsimilar it is to food right that that\nsimilarity is not there and based on\nthat semantic meaning is not getting\ncaptured because here either we are\ngetting ones or zeros so we are not able\nto provide right over here you can see\nthat much more maximum information which\nis the most most important word that\ninformation is also not getting captured\nso this was in short about the advantage\nand disadvantage I hope uh you have\nunderstood this and again the next uh\ntechnique with respect to converting\nword to vectors is something called as\nbag of wordss we'll try to see that how\nit is fixing some of the disadvantage\nfrom here and uh yes one more one more\nmajor disadvantage I'll say you that\nlet's say that if you're right now I\njust have three sentences let's say I\nhave just seven vocabulary let's say\ntomorrow I will be having 50k unique\nvocabulary size vocabulary size now what\nwill happen in this particular case\nright I'll be getting so many number of\nones and zeros so in short again this is\nactually leading it to sparse MC but in\na real world scenario I'll just not be\nhaving three sentences right I'll be\nhaving bigger sentences and I'll also be\nhaving many sentences as such so this is\nalso one of the thing in any interview\nif they ask you you probably need to\ntalk about or explain like this with\nrespect to Advantage and disadvantage\nhello guys we are going to continue the\ndiscussion with respect to NLP in our\nprevious video we have already seen that\nwe have discussed how one hot encoding\nworks right and uh if I probably talk\nwith respect to different types of ways\nhow we can convert a text into a vectors\nwe had completed already one hot\nencoding now we are into the second that\nis bag of words now let's go ahead and\nunderstand how bag of words actually\nwork and this is super important because\nthis is the technique which you can do\nsimple task like sentiment\nclassifications or whether uh basically\nin short a text classification kind of\nof task easily you'll be able to solve\nokay uh some of the applications famous\napplication like whether a mail is a\nspam or a ham everything you'll be able\nto solve it now let's say that I have a\ndata set okay and in this particular\ndata set uh I'm just saying that this is\na positive or negative statements now in\nthis particular data set I have three\nsentences let's say he is a good boy she\nis a good girl boy and girl are good\nokay and all these are like positive\nstatements so the output of all this are\nones right in supervised machine\nlearning we really need to know the\noutput also now let me go step by step\nand show you that how bag of words are\nimplemented so this is basically the\nstep one right I have the data set now\nlet's go to the step two now what\nhappens in the step two is that once I\ngo over here right there's multiple\nsteps that uh actually occur and\ninitially we should also go with respect\nto this the first thing is that if I\nprobably consider two things you're\ngoing to basically happen first of all\nwe will lower all the words and usually\nall the steps we will even be doing for\nall the other techniques also first we\nwill lower all the words and then we\nwill probably apply stop words right and\nthis also I have already shown you now\nwhen I lower all the words let's say my\nsentence one right my sentence one now\nbecomes I'm going to take only this text\nData I don't have to worry about the\noutput because later on once we convert\nthis text into vectors we can apply it\nto the machine learning algorithm so in\nthe sentence one here you can see as\nsoon as I lower the words what will\nhappen this all will become smaller now\nwhy I'm doing this because there may be\nsome repeated words now in this\nparticular case the capital with capital\nB you have boy here small B you have a\nboy so both these words are same but\nsince it is in uppercase this will be\ntreated as a separate word so we really\nneed to lower all lower case all the\nwords so I'm just going to write lower\ncase all the words now in sentence one\nafter lowering casing so what will\nhappen is that this all will become\nsmaller letters then once we apply stop\nwords now what happens in stop words\nthese all words like he she is a you\nknow it'll get deleted right because we\ndon't require this particular word for\nany kind of task like sentiment analysis\nimportant words like good boy good girl\nis basically required so this all things\nwill go like and are will also go now\nwhat will happen with respect to this\nthe sentence one words will become only\nlike this good boy okay I'm showing you\nstep by step uh with the the help of\npython when we do with the help of\nlibraries when we do it is very much\nsimple we don't have to worry that much\nabout it okay uh we just have to use one\nLibrary so sentence two what it will\nbecome once the stop words will get\nremoved it will become good girl\nright and similarly with sentence three\nI will be having the specific words that\nis\nboy girl\ngood right so all these things are there\nI have sentence one sentence two\nsentence sentence three perfect uh now\nwhat we do is that we go ahead and\ncalculate the vocabulary now how many\nwords are there in this vocabulary we\nhave good boy girl boy and girl and\nagain it is getting repeated so if I\nprobably consider how many unique words\nare there in the vocabulary so I will be\nable to write it over the first word is\nnothing but good so I'll go ahead and\nwrite good good is my first word and one\nmore thing that I will probably write\nthat what is the frequency of this\nspecific word like how how many times\nthese words are there in different\ndifferent sentences and obviously you\ncan see 1 2 3 three are there so I'm\nkeep going to keep the count as three\nthen we have something called as boy\nover here you'll be able to see how many\ntimes boy are there so in sentence three\nalso boy is there and sentence one also\nboy is there so I'm just going to make\nthe count to two and coming to the next\none which is called as girl so again\ngirl word is also there in this\nvocabulary and again girl also you will\nbe able to see two times it is there\nright now first of all all you need to\nsee that in this frequency with respect\nto different different vocabulary words\nis this in ascending order and obviously\nuh sorry is this in descending order so\nmaximum number of frequency will be put\nup in the first word right then boy is\npresent two times and girl is present\ntwo times so this can be moved up and\ndown but just understand what I've done\nis that based on this descending order I\nhave just ordered all this words so from\nit is basically from Maximum to minimum\nokay so this is is there\nperfect now before applying bag of words\nyou already have seen that how many\nvocabulary size what is the vocabulary\nsize the vocabulary size is three right\nand I can see in a in a bigger data set\nI can have this kind of words a lot so I\nwill be having all these words like this\nand frequency will also be there and one\nimportant step is that it is not\nnecessary that you use all the words\nthis is also there in this uh bag of\nwords itself let's say that over here in\nthe vocabulary there are 100 100 unique\nwords\nand let's say some of the words are just\npresent once right so if some of the\nwords are just present once we even not\ntake that so when we are when we'll be\ndoing the coding with respect to a bag\nof words you know we also have an option\nthat we just select the top 10 features\nor top 20 features which of the words\nare getting repeated more and more and\nthat is the importance of this\nparticular frequency okay now perfect uh\nwe have we are we are till here now the\nnext step will be that it is very very\nsimple now based on the topmost\nfrequency what I'm actually going to do\nI'm going to keep this words as my\nfeature so good will come over here boy\nwill come over here and girl will come\nover here okay now you already know what\nis a sentence one right so sentence one\nwhat will happen is that this sentence\none has good boy now see how this will\nget converted into a vectors wherever\ngood is present that will become one\nwherever boy is present that will become\none and remaining will become zero so\nthis entire sentence you'll be able to\nsee uh that that it is getting converted\nto 110 as a vector okay so this was the\ntext now this is getting converted to\n110 as a vector now similarly if I go\nwith respect to sentence two wherever\nthere is a word like good that will\nbecome one wherever there is a girl that\nwill become one and remaining all will\nbecome zero I'll talk about the\nadvantages and disadvantages why we are\ndoing this previously in one hot encoded\nyou saw that for every word we are doing\nthis for every word we are creating a\nvector but here for the entire sentence\nthis Vector is is coming okay and there\nare lot of advantages if when I talk\nabout this which I'll discuss uh first\nof all let's understand what more we'll\nbe having in this so S3 you'll be able\nto see the sentence three here I have\nboy girl and good so wherever boy is\nthere that will become one girl is there\nthat will become one and good will be\nthere that will become one okay\nnow this is my entire vectors and\nobviously I will also have my output\nvariable the output variable can be 1\nZer anything as such right if you are\nprobably solving the sentiment analysis\nor something like that okay now this are\nmy entire vectors okay and this is the\nvector for the entire\nsentence okay entire sentex and this is\nhow the entire bag of words converts a\ntext into of vectors okay now what we\ncan do is that we can take this\nparticular vectors we can train with an\nmachine learning model and we'll be able\nto get the output now one more important\nthing that I really want to put let's\nsay that I I have a word over here right\ngood girl girl right so let's say if I\nhave one more word like\ngood now in this case what will happen\nOkay in this case what will happen\nusually in bag of words since good is\nrepeated two times so I will increase\nthe count to two Okay in this particular\ncase so I what I'll do I will increase\nthe count to two Okay now there are two\nthings one is binary bag of\nwords and one is normal bag of words now\nin the case of binary bag of words even\nthough the count is two what it is going\nto do it is going to force it to become\none so most of the time the word is\npresent it may be present any number of\ntime the value is either one or it is\nzero and in normal bag of words I can\nincrease the count to 2 3 4 based on the\nnumber of words that is there so that is\nthe basic difference between binary bag\nof words and bag of words that basically\nmeans here you'll be having only ones\nand zeros and here based on the\nfrequency count will get up\nupdated count will get\nupdated based on frequency so this is\nthe basic difference with respect to\nthis\nokay so I hope you have understood like\nhow with the help of bag of words we are\nconverting a text into a\nvectors now in the next video I'm going\nto discuss about what are the advantages\nand disadvantages with respect to this\nlike how we have actually discussed in\none not encoding okay hello guys so we\nare going to continue discussion with\nrespect to bag of words uh already we\nhave understood the intution behind bag\nof words how it is converting a text\ninto vectors now as usual let's go ahead\nand discuss about the advantages and\ndisadvantages so first of all I will go\nahead and write the\nadvantages and then I will go ahead and\nwrite the\ndisadvantages okay so uh and obviously\nwe have also discussed about the\nadvantages and disadvantages with\nrespect to one hot encoding we'll try to\ncompare with this and we'll try to see\nthat what all problems is getting fixed\nokay first of all Yes again uh this is\neasy to implement and it is intuitive so\nI'll just write something like simple\nand\nintuitive simple\nand intuitive okay the second Point uh\nwith respect to advantages now here what\nwhat you'll do in one encoding you you\nsee that you have seen that some\nimportant important thing is that in\nmachine learning algorithm right okay\nwith respect to sparse metrics I'll be\ndiscussing with respect to semantic\nmeaning out of vocabulary everything\nI'll be discussing first let's consider\nthis particular second topic which is\nlike uh for ML algorithms it give fixed\nsize inputs now over here with respect\nto bag of words any statement now here\nyou can see that some of the sentence\nmay be three words five words 10 words\nat the end of the day based on the\nvocabulary size you are able to get all\nthe sentences converted into that many\nnumber of Di iions of words so here the\nvectors is getting fixed the inputs are\ngetting fixed because here our\nvocabulary is getting fixed so this\nparticular problem is getting solved\nokay in uh one hot encoding you do not\nhad a fix size inputs since we are\ncreating words for every vectors sorry\nwe are creating vectors for every words\nokay so what we are going to do over\nhere the second point that you'll be\nseeing yes you have a fixed size input\nright and this will superbly help you\nfor ML algorithms training\nokay ml algorithms now this is the two\nmajor advantages now if I talk about the\ndisadvantage see over here the first\ndisadvantage with respect to one\nencoding is spse metrix and I've already\ntold you what exactly is sparse Matrix\nit is nothing but ones and zeros let's\nsay if your vocabulary size is 50,000\nthen what will happen every sentence\nwill get converted into you know that\nsize of the vocabulary right so still\nsparse met problem is there so with\nrespect to disadvantage again I'm going\nto write it as\nsparse metrix and\narray or array is still there and this\nwill actually lead to\noverfitting okay now second major\ndisadvantage again see at the end of the\nday whatever statement that you have\nlike good boy good girl you know or it\ncan be boy girl good okay something like\nthis you'll be seeing that based on this\nsentence right and based on this\nvocabulary and based on the frequency of\nthe vocabulary The Ordering of the word\nis changing now see understand if in a\nsentence The Ordering of the word\nchanges and based on that this Vector is\ngetting created because see based on the\nfrequency we have written all the ve all\nthe all the vocabularies right over here\ngood was present maximum number of times\nso we wrote it as first boy was present\nin the second number so we wrote it over\nhere right and a girl was present uh\nlike two for two times and we have\nwritten it at the last right now over\nhere you can see that if I probably\nconsider the third statement boy girl\ngood right but here you can see that\nentire word is getting ordered uh like\nit is completely changed right The\nOrdering of the word is completely\nchanged so I'm having one one0 for\nsentence 3 I'm having 111 now when word\nordering is changed the meaning of the\nsentences also gets changed and because\nof that some of the semantic information\nis not getting captured I'll talk about\nmore semantic information but here\nyou'll be able to see that ordering of\nthe words is getting changed this is\nsuper important ordering of the word is\ngetting\nchanged because if this is getting\nchanged the meaning of the sentence is\nchanges right is getting changed so this\nwas the second disadvantage if I\nprobably talk about the third\ndisadvantage okay third disadvantage\nagain we'll go and see over here with\nrespect to out of vocabulary now what\nhappens if I probably add a new word\nlike boy girl good and let's say I'm\ngoing to add something called as school\nnow here you'll be seeing that the\nschool word is not present in the\nvocabulary so what it is going to do for\nthis specific word anyhow it is going to\nget rejected right it is it is not at\nall getting considered in this training\ndata let's say that for our new test\ndata in our new test data we have\nincluded a school word and we need to do\nthe prediction for this particular word\nwith respect to Output so the first step\nwill be that we'll do text preprocess\nand then we'll try to convert this into\na bag of words using the same technique\nwhat we did in the training data set but\nhere you see that in my training data\nset I do not have a vocabulary which is\ncalled as school so what it is going to\ndo it is just going to ignore the\nspecific word and it is just going to\nsee that where good and boy and girl are\nthere right so still out of vocabulary\nstill exist because this word may be an\nimportant word for the sentence but it\nis getting removed because we don't have\nthat in the vocabulary right major\nproblem so yes out of vocabulary is\nobviously a issue over\nhere right this still persist okay o\noov now this was the there now one more\nimportant thing semantic meaning in this\nis still not being getting captured why\nI'll tell\nyou semantic\nmeaning is still not getting captured\nand there are multiple things to explain\nin\nthis okay now first of all obviously you\nknow that I'm having either ones and\nzeros okay now in this particular case\ngood and boy they are getting the same\nimportance right for girl obviously if\nthe word is not present I'm getting zero\nsmall amount of semantic information is\ngetting captured when compared to the\none hot encoding format but here you say\nthat when we have many vocabularies\neither my values will be on or zeros one\nis just indicating whether the word is\npresent or not but which is the most\nimportant word what is the most\nimportant context in that particular\nsentence that is obviously not getting\ncaptured and if that is not getting\ncaptured semantic in turn will not get\ncaptured now the other thing over here\nis that there is there is also very\nimportant thing let's say that I'm\nhaving two sentence okay it is like the\nfood is\ngood let's say in my data set I have\nthis sentence the food is not good\nnot good now let's say I don't go ahead\nand remove all the Stop wordss and all\nfor this I will be having a vocabulary\nlike one uh one let's say all these\nwords are there okay and the is also a\nseparate vocabulary food is also a\nseparate vocabulary is is also separate\nhow many unique vocabulary are there\nfour because not is also there right so\nis will also become one not will be zero\nand good will be one right so this is\nhow we convert from this to this right\nsimilarly from here to here if I really\nneed to convert then what it will happen\n1 one one one 1 because not is also\npresent so I'm writing one now let's say\nthis is my Vector one and this is my\nVector 2 if I try to find out the\ndistance or how similar this Vector is\njust by plotting some points let's say\nthat I've converted this particular\nDimension into two Dimension using PC\nand probably have plotted it based on\nthis right only one value is getting\nchanged is and not right so I will get\nboth these vectors very much near to\neach other and this we can basically do\nit through something called as cosine\nsimilarity so let's say this is my\nVector 1 this is my Vector 1 this is my\nVector 2 so Vector 1 is basically\npresent over here Vector 2 is present\nover here if it is near to each other if\nthe angle between them is very near to\neach other or if the angle between them\nis very less I may say that this both\nthe sentences are same almost same or\nsimilar right this is almost similar\nbut do you think this both sentences are\nalmost similar because it is the\ncomplete opposite of them right but\nsince there is only one word that is\ngetting changed because of that only one\none of the values getting changed over\nhere right like zeros and ones are\nhappening and when we plot this it is\nbecoming kind of kind of a similar word\nbut this should not be a similar word\nthis is completely opposite word right\nso this kind of situation is also not\ngetting handled well with the bag of\nwords and later on the techniques that\nwill be learning like uh word to and all\nthis will be solving all these\nproblems right so I hope you are able to\nunderstand the advantages and\ndisadvantages of bag of super important\nwith respect to interview and if your\nbasics of this is getting strong trust\nme you'll be able to understand bag of\nwords average word to sorry you'll be\nable to understand word to average word\nto we in a very easy Manner and there\nare techniques in deep learning which is\nalso going to come which is called as\nembedding techniques word embedding at\nall all those will get solved in a very\neasy way right hello guys so we are\ngoing to continue the discussion with\nrespect to natural language processing\nin our previous video we have already\nseen what is NRS now what we are going\nto do is that we are going to see one\nmore efficient way of converting a words\ninto vectors and we specifically say it\nas TF IDF now what exactly TF IDF is it\nis nothing but term frequency and\ninverse document frequency so we'll try\nto understand how with the help help of\nTF IDF we will be converting all these\nparticular sentences into vectors okay\nand I've taken the same example what we\nspecifically did with bag of words like\ngood boy and this is after lowering all\nthe uh cases uh character cases along\nwith that after performing or after\nremoving the stop words so I have\nsentence one as good boy sentence two as\ngood girl sentence three as boy girl\ngood okay and this is the same thing\nlike uh from the same materials if\nyou'll be able to see I've done the same\nthing I've taken the same thing over\nhere okay now there are two components\nin tfidf one is term frequency and the\nother one is something called as inverse\ndocument frequency so whenever I talk\nabout term frequency term frequency\ndefinition or the formula how we\ncalculate it is given by number of\nrepetition of words in sentence divided\nby number of words in sentence okay I'll\ntry to show you completely taking this\nas an example how we can calculate term\nfrequency and over here inverse document\nfrequency formula is very simple we\nbasically calculate it by inverse\ndocument frequency is nothing but log to\nthe Bas e number of sentences divided by\nnumber of sentences containing the word\nnow this is super important and uh\nprobably don't get confused with the\nformula right now uh I will try to\nexplain you each and everything okay so\nlet's go step by step and let's see how\nwe can calculate the term frequency now\nfirst thing that I am going to make sure\nthat what I calculate is nothing but I\nwill be using this term frequency now\nwith respect to this term frequency you\nknow that how many vocabulary of words\nare there okay so first of all uh I will\njust try to show you in a different way\nby creating a table so I have S1\nsentence I have S2 sentence and I have\nS3 sentence okay and then with respect\nto my vocabulary of words I have\nsomething like good okay then I have boy\nand then I have girl and already we know\nthat only three words are basically\npresent in the vocabulary I'm trying to\nshowing show you with a simple example\nso that you'll be able to understand how\ntfidf will work okay now the first thing\nlet's go back to the definition term\nfrequency is nothing but number of\nrepetition of words in a sentence\ndivided by number of words in a sentence\nso suppose if I take S1 and with respect\nto S1 if I really want to find out the\nterm frequency of this particular word\nthat is good how do I calculate I just\nneed to see how many number of times\nthis particular word is repeated in the\nsentence so here you know that it is\nrepeated just one time and then I will\nbe dividing by number of words in that\nspecific sentence so it will become 1 by\ntwo because I have two words now let's\ngo to the next word that is boy so boy\nis also present how many number of times\none and this will be divided by two okay\nI'll tell you why we are doing this\nbecause when we understand the\nadvantages and disadvantages you'll get\na clear idea about why TF IDF will\nbetter play perform will perform better\nthan compared to bag of words okay then\nI have the word girl so girl I know that\nover here it is not present right in the\nsentence one so it will be 0 by 0 by2 so\nwhich is nothing but zero okay similarly\nwith respect to S2 here you'll be able\nto see how many time good is present\nonly one time and the total number of\nwords is two so 1 by two boy is present\nZ times so it will be 0 by2 which will\nbe nothing but zero and girl is\nbasically present over here one time so\nagain I'm going to write 1 by 2 now\nlet's go with respect to the sentence\nthree so sentence three how many times\ngood is present one time and the total\nnumber of words now is three right and\nthen if I go next uh to see the boy word\nhow many times boy is present only one\ntime so this will also be 1x3 and girl\nwill also be present 1 by3 because the\ntotal number of words are three okay so\nthis is how simple we able to calculate\nthe term frequency okay now let's go\nahead and let's try to find out the\ninverse document frequency so I'm just\ngoing to write over here as IDF now with\nrespect to IDF also we will be creating\ntwo Fields very simple fields and\nremember uh this will basically be my\nIDF and this is with respect to my words\nover here so my words are nothing but\ngood\nboy let me write it down in a better way\nso that in the it should look in the\nsame order so here will be my\ngood here will be my boy and here the\nnext word is something like girl now in\norder to calculate the inverse document\nfrequency it is very much simple now all\nI have to apply this log base e right\nhow many number of sentences are there\nwith respect to good right with respect\nto good uh suppose if I really want to\ncalculate the inverse document frequency\nof good okay so here what I'm writing\nI'll write log basee e multiplied by\nnumber of sentences how many sentences\nare there there are three sentences so\nthree divided by number of sentences\ncontaining the word so good is present\nin all these three sentence 1 2 3 right\nso I will basically be writing log to\nthe base 3 / 3 okay and if I basically\ncalculate this if you try to calculate\nit it is nothing but I will be getting\nas zero okay and you can basically do\nwith the calculator\nboy over here again log to the base e\nnumber of sentences are three and how\nmany time boy is present how many time\nboy is present in uh how many sentences\nboy is basically present it is present\none two right sentence one and sentence\nthree so I will be writing log based uh\nbase e multiplied by 3 by 2 similarly\ngirl will also be present same number of\ntime if you probably see how many\nsentences girl is basically present okay\nso I have independent dependently\ncalculated term frequency and I have\nindependently calculated inverse\ndocument frequency this is perfect right\nnow whenever we say TF IDF in short what\nI'm actually doing I'm multiplying this\ntwo okay term frequency and inverse\ndocument I'm taking the combination of\nthis two now let me go ahead and write\nit down in a better way still uh in the\nway that we specifically want and\nfinally how our vectors will look like\nso this is my vocabulary good boy girl\nand this is the final uh TF IDF okay so\nfinal TF IDF based on this calculation\nand this will differ Based on data set\nto data set okay so first of all with\nrespect to sentence one whenever I see\nthe combination of TF IDF with respect\nto good all I have to do is that\nmultiply 1 by two with this zero okay so\nI'm going to multiply this with this so\nin sentence one so see this is the\nsentence one right this is this entire\nthing is the sentence one one right so\nthis is my sentence one okay so I will\nbe taking this combination I'll multiply\nwith this right so sentence one good 1x2\n* by 0 it is nothing but 0 with respect\nto boy 1x2 * by log base e 1x2 * log\nbase e 3x2 will be the value that I'll\nbe getting in sentence one and with\nrespect to girl 0 multiplied by this it\nwill be zero now let's go to the\nsentence two in sentence two I will go\nahead and look for this now again 1X 2 *\nby 0 again good will be 0 and this boy\nis nothing but 0 by 0 so this 0 * by 0\nis nothing but 0 and here I have 1X 2 *\nby log base E\n3x2 I will tell you the exact thing what\nwe really need to know why we are doing\nthis specific thing everything will make\nsense uh and it will make sense and I'll\nmake you definitely understand all these\nthings okay so coming to the the next\none with respect to sentence three in\nsentence three I will do this\nmultiplication with this right so 1X 3\nmultiped by 0 again it will be 0 1X 3\nmultiplied by log base e 3x 2 and 1X 3 *\nlog base e 3x2 perfect so we have got\nall this calculation and this is how my\nvectors will look like so here you'll be\nseeing that for sentence one we\nconverted this entire sentence into\nvectors which looks like this right so\nthis is my sentence one vector this is\nmy sentence 2 vector and this is my\nsentence three vector and obviously I'll\nbe having an output with respect to any\nkind of classification that I want to do\nand then I will train my model by\npassing my sentence one so in short if\nyou see good boy is basically converted\ninto a vector which will look like this\nokay this entire sentence one is getting\na converted into a vector 0o this and\nthis 0 right so again you can calculate\nthis with the help of calculator but\nthis is what is the way that we have\nactually done we have converted all our\nsentences into vectors and in tfidf this\nis the phenomenon that is used in\nconverting the words into vectors now\nyou may be thinking Kish what is so\nspecial about this we have got some\nvalues okay that is fine and that is\nwhat I'm going to talk about in my next\nvideo about the advantages and\ndisadvantages of tfidf hello guys so we\nare going to continue to disc discussion\nwith respect to tfidf and already I've\nshown you how we can what is the formula\nof tfidf that is term frequency and IDF\nthat is iners document frequency and I\nalso shown you an example over here\nright so till here that is everything is\nfine now let's talk about the most\nimportant thing about advantages and\ndisadvantages and why this is probably\nbetter than bag of words okay so first\nof all uh the basic advantage that we\nhave again this is quite intuitive uh\nthe implementation is also quite\nintuitive\nuh coming to the second Advantage okay\nlike bag of words uh here also our\ninputs are basically fixed size and this\nis based on the wab size right and this\nAdvantage is also present with respect\nto bag of words that is also there but\nthe third advantage that I'm actually\ngoing to talk about see in bag of words\nalso we had fixed size right but this\nthird Advantage is a major advantage now\nlet's talk about the third Advantage\nokay so the third advantage is that the\nword importance is getting\ncaptured I'll explain you what exactly\nthis is word importance is getting\ncaptured super important point and\nprobably they may also ask you this\nspecific thing in interviews now if I\nprobably go and see my entire paragraph\nlet's say this is my paragraph good boy\ngood girl boy girl good right I've I'm\ngetting a tfidf of this number right and\nover here I I have also written this\nwith the help of bag of words in bag of\nwords I used to get either ones or zeros\nwherever that word is present that is\ncoming as one otherwise it is zero if it\nis not present in the sentence but how\nthe word importance is getting captured\nover here equal importance is given to\nboth the word like good and boy right\nbecause it is present in the sentences\nbut here it does not work like that here\nconsidering the entire paragraph what it\nis happening is that we are focusing on\ntwo things term frequency and inverse\ndocument frequency\nif if a word is present in all the\nsentences it should be given less\nimportance understand this okay if a\nword is present in all the sentences in\nthat paragraph it should be given less\nimportance why because all the all the\nsentences having that specific word so\nit is not playing that amazing or\nimportant role word importance needs to\nbe captured from every sentence that is\nwhat we specifically want now over here\nyou can see that boy is there right over\nhere here girl is there now boy and girl\nare getting repeated in one or two\nsentences not in every sentences so if\nit is not repeated in every sentences we\nneed to Value this particular word in\nevery sentences as such so if I probably\ntake an example of good good is present\nin all the three sentences so after we\ncalculate tfidf here you'll be seeing\nthat all zeros we are getting over here\nmajor major issue right so not an issue\nbut it is a good thing we are ignoring\nthe good word because it is present in\nall the sentence now if I consider with\nrespect to boy so sentence one boy will\nplay a very important role now right so\nwith respect to boy here you'll be\nseeing that I'm getting some values\nright I'm getting some values now in the\nsecond sentence obviously boy was not\nthere so it became zero but if I\nconsider girl in the second sentence so\nhere you'll be seeing that I'm getting\nsome value that basically means in this\nparticular sentence the girl word is\nsuper important and the context is based\non that specific word that we are having\na value of TF IDF right so in in short\nwhat is happening is that word\nimportance is getting captured and in\nthe third senten is it is talking about\nboth boy and girl so you'll be seeing\nthat both this boy and girl has some\nvalues so in short we are capturing some\nword importance over here based on the\ncontext right super important point and\nby this our machine learning model will\nbe able to understand that okay\nsomething specific we are basically\ntalking about and that way the\nmathematical models will be able to find\nout what kind of predictions it actually\nand through this the accuracy increases\nnow let's talk about the disadvantages\nobviously in this particular case also\nyou have Lot number of zeros so sparsity\nstill exist okay sparity still exist\nover here uh and again we will try to\nsee how we can solve sparsity using word\nto the second thing is that what we\nspecifically discuss about is something\ncalled as oov out of vocabulary now here\nalso if I probably add any more words\nover here with respect to the test data\nthat is going to get ignored because uh\nover here also all my features is\nbasically made based on a training\nvocabulary size right so this is\nbasically the advantages and\ndisadvantages with respect to uh tfidf\nbut definitely just by seeing the\nadvantages and disadvantage we can\ndefinitely know that tfidf performs\nbetter than bag of words right now uh in\nthe next video we'll try to see some\npractical uh implementation with the\nhelp of nltk python and again guys you\nreally need to practice this considering\ndifferent different data sets we will\ntry to provide you more assignments as\npossible so that you can practice these\nthings also hello guys so we are going\nto continue a discussion with respect to\nnatural language processing for machine\nlearning in this video we are going to\ndiscuss about word embeddings and this\nwas probably the topic that I should\nhave covered long back but I'm\ndeliberately keeping this particular\nTopic at this point of time why because\nwe have discussed so many topics wherein\nwe focused on converting word into\nvectors so I now you'll be getting a\nvery clear idea about what what exactly\nword embeddings is and here I've given\nyou a Wikipedia definition so this is a\nvery simple Wikipedia definition I've\nhave taken it from Wikipedia so the\nentire credit goes to Wikipedia over\nhere so over here you can see the\ndefinition that in natural language\nprocessing word embeddings a term used\nfor representation of the words right\nfor text analysis typically in the form\nof real valued vectors that encodes the\nmeaning of the word such that the word\nare closer in the vector space are\nexpected to be similar in the meaning so\nlet's say that I have two words king and\nqueen okay or forget about king and\nqueen let's say that I have two words\nand the two word is like\nhappy and\nexcited right let's say I have this two\nspecific word now when I have this two\nspecific word with the help of word\nembedding techniques what we do is that\nwe convert this particular word into\nvectors and let's say if I try to plot\nthis vectors in a two-dimensional\ngraph okay and mean if I really want to\nconvert this into two dimensional CFT we\nhave techniques like PCA or other\ntechniques which is an unsupervised\ntechnique to do dimensionality deduction\nso once I probably plot this let's say\nhappy and excited are coming near to\neach other based on this particular\nvectors it basically indicates both are\nsimilar word okay let's say that I have\none more word like angry now in this\ncase with the help of word embeddings if\nI'm trying to convert this into vectors\nthe tentative thing is that obviously\nhappy is the opposite to angry so angry\nwill be somewhere here if I probably try\nto plot this particular uh vectors over\nhere why why it is coming so far because\nit is an opposite word so the distance\nbetween this word will be quite High\nwhereas the distance between this\nparticular word will be quite less so\nthis indicates that this both the words\nare similar whereas this both words are\nopposite to each other right and this is\nall possible because of efficient\nconversion of the word into vectors\nright and how we are doing this again\nwith the help of word embeddings but the\ntechniques we have learned till now is\nit is something like one hot encoded we\nhave learned about bag of words we have\nlearned tfidf we have learned and all\nthese techniques are a part of word\nembeddings so if I properly clearly show\nyou the division you know in the first\nstep if I have the word embedding\ntechniques so let me just again go ahead\nand write it word embedding techniques\nare specifically of two types so this is\nmy first type and this is my second type\nthe first type is based on count or\nfrequency count or\nfrequency and the second type is based\non deep learning trained models please\nhear to this very properly because this\ndeep learning trained model models will\ngive you very good accuracies okay now\nin count of frequency we have learned\nabout three different types one is one\nhot\nencoded second one is something called\nas bag of words and the third one is\nsomething called as\ntfidf right so all these techniques we\nhave learned right and we know the\nadvantages and disadvantages but here we\nfocusing more on count of frequency\nright but the major one which is having\na better accuracy which is actually and\nall these techniques at the end of the\nday is also converting words into\nvectors or it is\nconverting sentence into vectors right\nsentence into vectors but we have seen a\nlot of advantages and disadvantages\nmaximum number of disadvantages are\nthere with respect to all these\ntechniques and this all disadvantages\nare getting solved by this deep learning\ntrained model and the train model is\nnothing but which is basically called as\nword to W it's not like we can cannot\ncreate it from scratch we can definitely\ncreate it from scratch but again you\nrequire a huge amount of data and what\nwe are going to do in that first of all\nwe are going to understand in the\nupcoming videos uh what is exactly word\nto W and how it is basically converting\na word into vectors and how it is\nsolving all the disadvantage things that\nwere there in this particular technique\neverything we are going to discuss but\njust understand that what is word to it\nis a word embedding technique which will\nefficiently convert a word into a\nvectors which will be making sure that\nboth this property it is expected to\nhave similar in meaning whenever it is\nconverting into a vector space along\nwith that it will also give you a very\ngood representation of the words right\nsparity will not be there and all there\nare many points which I'm going to\ndiscuss in the upcoming uh videos with\nrespect to word to now word to are of\ntwo types one is because the entire deep\nlearning architecture is built on two\ndifferent types one\nis we basically say\ncball C Ball\ncow is nothing but continuous bag of\nwords super important continuous bag of\nwords and we also going to see that how\nthe models get strained but for this you\nreally need to have a prerequisite\nknowledge of how Ann Works what is loss\nfunction what is optimizers and all\nright the second technique is something\ncalled as skip gram skip gram again this\nis a different technique uh and again it\nis a part of word to itself it is a\ndifferent type of word to at the end of\nthe day we can either use CBO or skip\ngram to get an efficient conversion of\nword to vectors right and this is what\nwe going to see we also going to see\nsome pre-trained models of word to VC\nyou know probably created by Google and\nit is somewhere around 1.5 GB big model\nsize we'll try to download it we'll try\nto see we we'll try to execute it but in\nthe upcoming videos what we are going to\nsee is that how word to W word embedding\nworks right and how it is making sure\nthat all these disadvantages that are\npresent in this techniques is getting\nremoved right so this is what we are\ngoing to discuss but I hope you got an\nidea at the end of the day whatever\ntechniques we have discussed till now in\nconverting word into vectors it falls\nunder word embeddings right hello guys\nso we going to continue a discussion\nwith respect to natural language\nprocessing in this video we are going to\ncover word to w we have already seen\nwhat exactly what to Beck is it is a\ndeep learning train model and again it\nis a kind of a word embedding techniques\nwherein the focus is to convert word\ninto vectors making sure that the\nmeaning of different different words are\nactually maintained like if there are a\nsimilar words we'll be getting vectors\nthat are very near to each other if we\nprobably find out the difference and\nwe'll also be able to see that which all\nwords are completely opposite based on\nthese vectors okay so let's discuss\nabout word to W and I will give you an\nidea about like what exactly is word to\nW and how the words is getting converted\ninto a vectors in the upcoming videos I\nwill try to show you that how word to W\nmodels are basically prepared with\nrespect to\narchitecture uh in in the case of deep\nlearning models and for that you really\nneed to have knowledge about a&n models\nright if you really need to understand\nthat how we can train word to W from\nscratch okay and uh yeah let's go ahead\nand with the definition and let's see\nthat what problems it actually fixes so\nover here word to W is a technique for\nnatural language process\npublished in\n2013 and it was published by Google an\namazing company already you know that\nand they had done some tons of work with\nrespect to NLP you know they're doing\nlot of research the word to algorithm\nuses a neural network model we'll be\ndiscussing about this how we'll be using\nit uh how it uses a neural network to\nlearn word association please M make\nsure that you understand this words to\nlearn word association from a large\nCorpus of of text once trained such a\nmodel can detect synonym words or\nsuggest additional words for partial\nsentence so it will be able to detect\nsynonyms it'll be able to detect uh\nopposite words and many more things as\nthe name implies word to W represents\neach distinct word with a particular\nlist of number called as vectors so at\nthe end of the day we are converting a\nword into vectors but this Vector will\nhave many things it will be able to\ndetect synonym words or it'll also be\nable to suggest additional words for\npartial sentence okay now let's\nunderstand what exactly this is now see\nguys in bag of words TF IDF we have\nalready seen right based on the\nvocabulary size we'll either get on\nzeros ones or zeros and in in short we\ngetting a sparse Matrix in TF IDF also\nwe may get decimals like0 2 5.6 then\nagain Zer zeros are there in word to\nit'll be little bit different now let me\ntalk about how it'll be different let's\nconsider that I have a vocabulary okay\nand I have a\nvocabulary let's say I have my\nvocabulary so this are basically my this\nmany number of unique words I have in my\nCorpus okay unique words I have in my\nCorpus Corpus basically means paragraph\nokay now let's say uh the vocabulary\nwords that I specifically have like\nsomething like boy\ngirl okay and then we have something\nlike\nking\nqueen\nand if I probably talk about some more\nwords like apple and mango let's say in\nmy vocabulary I have this many words\nokay now one very important word I'm\ngoing to put up over here which is\ncalled as feature\nrepresentation\nfeature representation please listen to\nme very very carefully a very important\ntopic now each and every word that are\npresent in the vocabulary will be conver\nconverted into a feature representation\nnow what this exactly means this\nbasically means that we are going to\nconvert this all words into a\nvectors based on some features now what\nall features can it be right let me give\nyou a very good example about it but\nunderstand that when we are training a\nvery big word to W model at that time\nyou'll not be getting a clear idea about\nall the features but here just to make\nthis intuition to make you understand\nthe intuition\nlet's discuss about this feature\nrepresentation so in the left hand side\nwhat will happen is that let's say that\nI will be having lot of features like\nthis so I may be having a feature called\nas gender I may be having a feature\ncalled as Royal I may be having a\nfeature called as age I may be having a\nfeature called as food like this I will\nbe having lot of features and let's say\nthat the total number of features you\nknow are basically 300 Dimension that\nbasically means I will be having one\nmore nth feature over here and the size\nof this entire features like if I\nprobably count all of this this will be\n300 Dimensions let's consider 300\nDimension basically means I will be\nhaving this 300 features now with\nrespect to all this\nvocabulary we are going to represent\nthis word in the form of vector\nconsidering this feature representation\nso this is my entire feature okay what\nI'm actually going to do I'm going to\ntake up all these words that are present\nin the vocabulary based on this\nparticular features we are going to ass\nassign one numerical value okay and\nunderstand one thing this numerical\nvalue will be assigned based on the\nrelation of this two words that is the\nvocabulary and this feature\nrepresentation that we have here I have\ngiven you as an example but when we are\ntraining a very large word to model we\nwill not be able to see this features\nentirely right if I take an example of\nGoogle you know they have come up with\nthis amazing word to W model which is\nbasically trained in 3 billion words I\nguess 3 billion ion Words which is\ncoming from the news feed right and that\npractical example also I'll try to show\nyou and at the end of the day you'll be\nable to see that each and every word is\nbasically represented by 300 dimension\nof feature representation that basically\nevery word will be having a 300\nDimension uh vectors okay now this is\nsuper important what kind of values that\nI can have over here right what kind of\nvalues I can have over here now let's\nsay with respect to Bo which is present\nin the vocabulary okay and it's and it's\nrelationship with respect to gender\nlet's say that I'm having one minus one\nover here let's let let's just consider\nokay minus one over here now with\nrespect to girl and gender the value can\nbe+ one because it is the opposite of\nboy right opposite with respect to\ngender if I say boy is there opposite of\nboy is nothing but girl so minus one and\none this kind of vectors can come now\nhere you can see with respect to the\nnext word over here we have boy we have\nRoyal we obviously know we cannot say\nright we we don't have a sentence that\noh he he is a royal boy he can be a\nroyal Prince or he can be a royal king\nright so there is no no proper\nrelationship so in this particular case\nyou know there will be a value like 01\nI'm just giving you as an example right\nsimilarly we'll also be having with\nrespect to boy and age let's say that\nthere is not much relation so I'm just\ngoing to put it as\n03 okay very near to zero now similarly\nI can have all these values over here\nand these all values comes through\nproper trained models like word to word\nto which is trained by Deep learning\ntechniques like Ann okay and I'll show\nyou in the next video how those models\nare basically trained but just\nunderstand over here each and every\nvocabulary that we are seeing is\nrepresented based on this feature\nrepresentation so that basically means\nfor boy I will be having this Vector so\nfor\nboy for boy you can see over here I will\nbe having this specific\nVector okay so this all vectors will be\nhere okay okay now similarly with\nrespect to girl now you can see with\nrespect to gender if boy is having minus\none then this will be + one right\nbecause it is completely opposite with\nrespect to Royal again no relationship\nso it will be 02 let's say here I'm\ngoing to put 02 because age with respect\nto this also no specific relationship\nright so similarly I will be having\nother values like this now see with\nrespect to King you know I may have\ngender like uh there is a relationship\nwith respect to gender and King so\nminus. 92 will be there with respect to\nQuin it can be plus. N3 you know so here\nyou can see opposite right opposite\nopposite over here right and similarly I\ncan have other vectors like over here\nRoyal and King are related right so I\ncan have 95 okay and over here you can\nsee also Royal and queen can also be a\nroyal right so over here probably the\nvalue can be\n9697 very much near by each other right\nnow understand because of these vectors\nyou know similar words will be will be\nvery very close to each other because if\nI try to subtract I'll just give you an\nidea about it so right now with respect\nto age also obviously there will be some\nrelationship between age and King\nbecause we say that Old King right with\nrespect to age so suppose let's say here\nI'm having 75 here I'm having 68 and\nlike this I will be having multiple\nvectors like this right and we can\nclearly see right over here it is now\nsee with respect to Apple obviously\nit'll have no relationship with respect\nto gend uh gender so I can probably\nwrite .5 or 01 something right with\nrespect to Mango also it cannot be so\nI'll write just 23 I'm just saying that\nokay we basically uh can have some\nvalues over here if it is not having a\nrelation it'll be very much near to zero\nlike\n05 okay I'm just putting some values but\nonce we train all these models right we\nwill be getting this value here I'm\ngiving you a Crux idea about like how\neach and every vectors may look like\nokay now over here with respect to Apple\nand Roy also I'll not be having much\nrelationship so let's say I'm putting\n-202 this will be plus2 with respect to\na an apple this may have a very good\nrelationship right because uh let's say\nif the apple is kept for 10 days outside\nyou know it may it may it may rotten up\nright it may not have that nutritional\nvalue so this uh with respect to age it\nmay have a direct relationship mango\nalso it may have a direct relationship\nso we are going to have this vectors\npretty much similar with respect to food\nyes Apple belongs to a food item right\nso this will probably have a good value\nit may be 91 this is also 0.92 right and\nsimilarly I'll be having different\ndifferent vectors right so here what we\nhave done is that each and every\nvocabulary word is represented based on\nthis feature\nrepresentation right and here the\nfeature representation may not only be\n300 Dimensions it can be 100 Dimension\nit can be different different dimensions\nright but here I'm just say showing you\nan example with respect to Google and\nwhat will be this features that will\nalso not be exactly known but just\nconsider that I'm just giving you an\nintuitive example that yes based on some\nrelationship with respect to the word\nyou're able to get this specific\nvectors now there are lot of advantages\nwith respect to this you know why I'm\nsaying you because suppose let's say if\nI do a calculation which is like King\nminus man plus Queen if I do this\ncalculation and this is a famous\ncalculation which is also written in\nGoogle research paper if I probably do\nthis calculation the output that I'm\ngoing to get is something called as\nwoman\nokay I will definitely get is human why\noh human is not there it's okay so let\nme just remove it suppose if I say\nKing minus boy plus\nQueen then the output that I'm going to\nget see King is this Vector right I'm\nsubtracting with boy and then I'm adding\nit up Queen at the end of the day you'll\nbe seeing that the girl will be much\nmore related to Boy so I'm going to get\nthe output as girl here I'm just doing\nthe vector calculation and this is what\nkind of relations we'll be able to get\nit you know this is what this is an\namazing thing we are getting this kind\nof relation just by seeing these vectors\nwhich has been provided by word to again\nI'm not going to do the calculation here\nI've just randomly stuffed some values\nbut in the real word to use case you'll\nbe seeing that if you do this kind of\ncalculation you're going to get girl and\nthis I will show you practically also as\nwe go ahead once we use this Google uh\nGoogle word to which is basically train\non 3 billion Words which is quite\namazing right so let me give you another\nexample let's say here what I'm doing\ninstead of using 300 Dimension I'll\nrepresent Every Word by two Dimension\nlet's say I have\n9596 man is represented by something\nlike\n9598 let's let's represent like this\nokay I'm I'm just giving some values\nokay some meaningful values so that\nyou'll get an idea about it let's say\nqueen is given as96 let's say because\nthis is is the opposite of this right\nand again this can be the similar\nkeyword uh similar vocabulary and with\nrespect to human let's say that I'm\nhaving something like 94 or 96 let's say\nthen what I do is that if we do the\ncalculation of King minus man plus queen\nI'm going to get the human right as a\noutput now what this vectors represent\nthat also you really need to understand\nit is super super important and that is\nwhere I'm going to to discuss about\nsomething called as cosine similarity\nokay cosine similarity super important\ntopic with respect to understanding\nthese things because if you understand\nthese things and all now see over here\nKing is given by two two two vectors\nright 9596 so obviously I can I can\nbasically construct this into a two\nDimension let's say that I'm getting\nKing over here okay and in order to make\nyou understand let's say queen is over\nhere okay or queen can come over\nanywhere let's say man is over here\nright man is over here if I probably say\nking with respect to gender right this\ntwo is going to be the most nearest word\nwhen compared to queen right so if how\ndo I calculate the distance between this\nvector and this Vector which is provided\nin this form all I do is that I try to\nprobably calculate the distance like\nthis okay I try to find out the angle\nand for this to find out the distance\nbetween two vectors we use a distance\nformula which says it is nothing but 1\nminus cosine similarity now what is\ncosine similarity this is super\nimportant to understand cosine\nsimilarity I can basically say that\ncosine similarity is nothing but cosine\nsimilarity is nothing but the angle\nbetween these two vectors let's say the\nangle between these two vectors let's\nsay I'm just taking as an example is 45\nlet's let's consider then this is\nnothing but cos 45 cos 45 is nothing but\n1 by < tk2 and probably I think this is\napproximately equal to 771 I've done the\ncalculation if it is wrong just let me\nknow okay but over here then the\ndistance between these two vectors will\nbe nothing but 1 minus\n7071 now if I'm calculating the distance\nit is. 29 let's say so if I'm getting\nthis 29 distance right I will say that\nokay almost this particular word are\nsimilar let's say if I have two more\nvectors if I have two different vectors\none of the vector is basically over here\none of the vector is over here then the\nangle between these two is nothing but\n90 in the case of 190 my distance will\nbe nothing but 1us cos Theta cos Theta\nis nothing but cos 90 cos 90 is nothing\nbut 0 1 - 0 is 1 so I can definitely say\nthis vector and this Vector are\ncompletely different because the\ndistance between them is one if the\ndistance is nearer to zero I will say\nthat they almost similar vectors now in\nthis case it is 0. 29 I can say that\nokay somewhat similar right let's say if\nI have one more Vector which is in this\npoint only then I can say that these two\nvectors are almost same because the\nangle between them is uh cos 0 cos 0 is\nnothing but 1 1 - 1 which is nothing but\nzero right because over here the the\nangle between this two point is nothing\nbut zero right there's no angle at all\nright so if there is no angle then I\nwill be able to find out the distance in\nthis case my distance will be nothing\nbut 1 - cos 0 1 - cos 0 is nothing but 1\n- 1 which is nothing but zero now if the\ndistance is coming as zero that\nbasically means these two are same word\nright now this super important because\nrecommendation also happens in this way\nitself now in recommendation let's say I\nhave a movie which is like a Avengers\nlet's say Avengers is over here where do\nyou think Iron Man will come Iron Man\nwill come again near this or near this\nat this particular Point only right Iron\nMan will be coming and it will be based\non different different feature\nrepresentation right whether it is a\ncomic movie whether it is an action\nmovie so action comic right or comedy\nthese all are feature representation try\nto understand this and movie name is\nbasically my Vector my vocabulary it can\nbe Avengers it can be this one right so\nI hope you're getting an idea about how\nword to W is basically working at the\nend of the day we are basically creating\na feuture representation of every word\nokay and we are able to find it out so\nyes this was about word to W now what we\nare going to do is that you need to\nunderstand that how this feature\nrepresentation is created and how this\nvectors is basically created how this\nvectors because here I have randomly\nwritten boy to gender is minus one boy\nto girl g girl to gender is plus one\nbecause I said that okay this may be the\nopposite one now you'll get an idea that\nhow in an deep new deep learning neural\nnetwork basically a simple neural\nnetwork how this entire word to is\ntrained and that is what I'm going to\ndiscuss in my next video then I'm going\nto also show you the Practical\nimplementation so I hope you're able to\nunderstand this with respect to this uh\na very good amazing model developed by\nGoogle a very good architecture for with\nrespect to this and we'll try to solve\nthat in the upcoming video hello guys so\nwe are going to continue the discussion\nwith respect to word Tock uh already we\nknow word Tock are basically of two\ntypes one is cow which we have already\nseen previously continuous bag of words\nand script gram in this video what we\nare going to do is that we're going to\nunderstand that how what to model is\nbasically created what is the Deep\nlearning model that we are specifically\nsaying you know how the inputs and\noutputs are there and how the model is\nbasically trained one important thing is\nthat you really need to have a\nprerequisite knowledge about Ann loss\nfunction and optimizers so if you do not\nhave this I would suggest first of all\nplease make sure that you have some\nknowledge about this right before you\nunderstand understand this now with\nrespect to word to one more important\nthing is that in word to we also have a\npre-trained models right now if I talk\nabout a pre-trained model like Google\nright Google has a pre-trained model\nwith respect to word to which is trained\non three 3 billion words and we can also\ntrain a model from scratch train a model\nfrom scratch okay and again uh the\nreason why I'm taking this because you\nreally need to understand that how that\nfeature representation is basically\ngetting created okay now let me go ahead\nand let me say is that uh uh let me take\na simple Corpus and let's say that first\nof all we'll start with cow okay so we\ngoing to discuss about cow which is\nnothing but continuous bag of words and\nhow this model is basically created it\nis a type of word to right continuous\nbag of words now to solve any problem I\nwill definitely have a data set so let's\nsay that this is my Corpus and remember\nCorpus because all these models you know\nlike word to is trained on a huge data\nset huge data set like this particular\npre-train model from Google is basically\ntrained on 3 billion words so let's say\nI have a corpus or a statement or a data\nset or or a paragraph it can be anything\nand for just making you understand I'm\njust going to take a simple paragraph\nI'm going to say that okay I\nneuron I\nneuron is is I neuron company or I\nneuron\ncompany is related\nto data science let's say I have this\nparticular\ncorpus now remember like this in a use\ncase you'll be having a bigger Corpus it\ncan have millions of words right but\nlet's say that I'm digging just a simple\nCorpus over here just a single liner now\nyou're going to understand that how a\ncow what to is basically created and how\na model is basically trained with the\nhelp of deep learning now first thing is\nthat whenever we have a corpus we really\nneed to know what is our input data and\nwhat is our output data because word to\naltogether is a supervised machine\nlearning right so first of all what we\ndo is that we select a window\nsize and I'll talk about this window\nsize and why it is super important let's\nsay that I'm going to select a window\nsize of five now this window size is\nsuper important to basically create your\ninput data and output data okay super\nsuper important like what should be your\ninput data and what should be your\noutput data so that you can train your\nmodel right now this window size 5\nindicates that how many words I need to\nselect initially so let's say I'm\nselecting five words so here is my five\nwords now from this particular five\nwords I will take the center\nword Center word Now understand how I\nwill take up this window size 5 words\nand convert into input and output data\nso let's say this is my input data and\nthis is my output data okay now the\ncentral element that I've actually taken\nover here is is right now in the input I\nwill be having I\nneuron\ncompany and then on the right hand side\nI have related and I have two okay now\nwhy you why you may be thinking that I'm\ntaking the forward and the backward\nbecause\nunderstand if I'm taking this as a\ncentral word and this will basically be\nmy output word okay is okay this is is\nbasically my output word now it should\nbe knowing that what all words are in\nthe forward context and what all words\nare in the backward context so that is\nthe reason we are creating in this\nparticular way so that this is output\nshould be knowing about its forward word\nand its backward word just to get some\nidea about the context of that specific\nsentence now this is the first step now\nwhen I took the window size as five the\ninitial five words I divide my data set\ninto input and output perfect\nnow the next step is that I will go\nahead and I'll move this window by one\nstep and take the next five words so the\nnext five words over here is nothing but\nso here this is my sentence one so sorry\ninput one and now this will become my\ninput two and here I will be having\ncompany okay uh and again from all these\nfive words which is my Center word so\nthis is basically my Center word related\nright so related so here I'll write\ncompany is uh two and data so this is my\nsecond input and over here the output\nwill basically be related right then\nsimilarly I will go to the next step and\nI will push this Windows to One Step\nmore okay so here now I'll be having my\nthird sentence with respect to my input\nand output and the third sentence again\nwhich will be Central word so this will\nbasically be my central word so here you\nhave two okay and here I'm going to have\nis related\nokay and then the right hand side data\nand science now you may be thinking\nchrish should we only take the window\nsize as five no you can take any value\nyou can take any value and why this\nwindow size is playing an important role\nI'll just say in some time you can take\nup any value but don't take an even\nnumber take an odd number so that I will\nbe getting the central element which I'm\ntaking as an output will have the\ncorrect number of words in the forward\ncontext and in the backward context okay\nso is related data Sciences there now\nright which is the central word over\nhere two right so I'm just going to\nwrite it as two now this became my input\nand output now what I'm actually going\nto do is that I'm going to train my\nmodel with this very simple right I'm\ngoing to train my model with this now\nhow the training will basically happen\nnow one very important thing that you\nneed to understand over here you'll be\nseeing in neuron company related to all\nthese inputs and outputs so I cannot\nprobably send this text directly I need\nto convert this into some vectors\ninitially to send it in as an input to\nthe neural Network also so for this what\nI'm actually going to do first of all\nyou know that how many number of words I\nhave how many number of words I have in\nthe vocabulary I have I neuron I have\ncompany I have is related to data\nscience right so they are around 1 2 3 4\n5 6 7 right seven words are there now if\nI probably use one hot encoding\ntechnique now see this okay this is\nsuper important in one hot encoding\ntechnique if I probably consider I\nneuron let's consider the first sentence\nover here I have I neuron and then I\nhave\ncompany then I have\nrelated and then I have two right so for\nall these words how I'll be giving the\none hot code rep one hot encoding\nrepresentation wherever there will be I\nneuron I'm just going to make it as one\nremaining all will be zeros so there\nwill be around 1 2 3 4 5 6 zeros\nsimilarly when when company is there\nI'll make this as one and remaining all\nwill be zero right similarly related\nrelated is in the fourth word so I'm\ngoing to make it at 0 0 1 0 0 0 and then\ntwo is present after this so I'm\nbasically going to write it as uh 0 1 0\n0 right so this is this is pretty much\nClear till here right so here you can\nsee that I have I've basically done this\na simple one hod encoded format that\nbasically means if I really want to pass\nI neuron I need to give this as my\nVector this is what is the understanding\nand this Vector is basically Ally given\nby seven dimensions so seven vectors I'm\ngiving it over here right 1 0 0 0 0 if\nI'm sending company as my next word then\nthis should be the vector it should go\nright then if I'm sending related this\nshould be the vector I should go so\nsimilarly all these particular words\nwill be converted into this particular\nVector using one in coding now let's go\nto the next step which is super super\nimportant okay super super important\nwhat does c basically mean continuous\nbag of words okay this is nothing but\nthis is a fully connected neural Network\nnow you will be able to understand how\nthese models are created fully connected\nneural network okay now in this fully\nconnected neural network you'll be able\nto see one very very important thing one\nis first of all just understand how many\nnumber of input should I be giving right\nlike how many words I will be giving as\nmy input right since my window size is\nsame window size is five all my inputs\nare fixed I hope that is very much clear\nright now in this particular problem\nstatement you'll be seeing I'm giving\nfour words in every sentence so my input\nis basically fixed now I neuron when I\ngive my word in neuron let's say in the\nfirst case I give my sentence one and\nthis is my sentence one when I give in\nneuron in neuron is represented by seven\nvectors over here it is represented by\nthis Vector then company is basically\nrepresented by this Vector so if I\nprobably see in fully connected layer my\nfirst input layer will basically be\nnothing but so here you'll be able to\nsee this will be my input okay and this\nis super important guys see this so this\nwill be my input my first input word\nokay and understand I'm giving I'm\ncreating this circle as my\ninputs okay I'm I'm just creating this\ncircle as my input so if you see 1 2 3 4\n5 6 7 seven inputs I'm giving it right\nnow when I give this seven inputs then\nsimilarly how many words will be going\nfour words will be going right so one\nword two word so this is basically my\ninput layer so this layer is nothing but\nmy input layer in a fully connected\nlayer a simple a Ann if I probably\nconsider an example of a Ann so here\nalso how many uh inputs I'll be having 1\n2 3 4 5 6 7 right and this is my first\nword second word third word fourth word\nso I will be having four different words\nover here that will be going and each\nword will be having a dimension of seven\nvectors 1 2 3 4 5 6 7 right I'm not\ngiving this value don't consider that\nthese all are zeros okay I'm just saying\nthat these are my input layer input\nlayer input circles okay that is how we\ncreated Inn right and then probably I\nhave my last one so this is my input\nright I'm basically designing the neural\nnetwork how it will look like when we\nare training a word to we so these are\nmy four four words understand in the\nfirst case I'm going to pass I neuron\nover here so let's say I'm going to pass\nover here I neuron this will be my input\nover here this is represented I neuron\nwill be represented by 1 0 0 0 0 0 right\nand similarly if I go to the second word\nthat is like company then it will be\nrepresented by another word like this\nit'll be represented by a different\nVector like 0 1 0 0 0 0 0 right seven it\nwill be 1 2 3 4 5 6 7 right now\nsimilarly other words will be\nrepresented like this okay so this\nbecomes my input layer okay and every\ninput is is basically given by a vector\nof seven seven dimensions so because I'm\nrepresenting every word based on the\nvocabulary size using one hot encoded\nnow this becomes my input layer now\nlet's go to the middle layer that is\ncalled as the hidden layer now in this\nhidden layer this is super important\njust pause the video and guess what will\nbe the size you know that our window\nsize is how much our window size is\nbasically five right so I'm just going\nto make this as my window size okay so\nthis is my window size now in my window\nsize if you remember how many how many\nwe are having in our window size our\nwindow size is nothing but five so I'll\nbe having 1 2 3 4 5 right window size is\nbasically five so in my hidden layer\nI'll be having this five vectors okay so\njust understand that with respect to\nthis five our window size will be set\nover here okay now with respect to the\noutput in output how many values I have\nI just have one value and each word I\njust have one word in the output right\nand each word is represented by a vector\nof seven because if I'm also considering\nthis is using one hot incant I'm going\nto get this vectors of this Dimension\nthat is seven right so what I will be\ndoing in output I will basically be\nhaving another output layer like this\nwhich will again be having seven\ndifferent\noutputs 1 2 3 4 5 6 7 okay now this is\nhow my fully connected neural will look\nlike neural network will look like now\nyou need to understand one thing over\nhere each and every node each and every\nnode will be connected to the other node\nright like\nthis like this it will be connected like\nlike how an Ann will work it'll be\nconnected like this only right similarly\nright now similarly these all will be\nconnected to this also so in short I can\nbasically make a very simple connection\nlike this which will look like this\nitself and this will be entirely\nconnected to this right understand all\nthese lines will have some initialized\nweight initialized weights and we need\nto train these weights and this is what\nit happens in a&n right similarly this\nwill be connected to this this will be\nconnected to\nthis this will be connected to this\nright and finally this will also be\nconnected to\nthis right so everything is basically\ngetting connected and from The Hidden\nlayer this is my hidden layer one hl1\nand this is my output layer right now\nfrom this hidden layer it will basically\nget connected over here and this will\nget connected over here okay now\nunderstand one very very important thing\nokay this is super super important fine\nwe are connecting it with the help of\nloss function we'll uh we'll also do\nforward and backward propagation now\nlet's see we'll consider let's let's\npass this particular word I neuron\ncompany related to so I have passed I\nneuron company and here also you'll be\nable to see I'm passing related to very\nsimple okay let me just zoom out a bit\nokay now once I pass all these things\nwhat happens over here with respect to\nthis s output I already know what is my\nreal output is right I'll be getting\nsome values over here okay I'll be\ngetting some values okay but the real\noutput is what if I consider is is is my\nthird word so is is my real output so\nthis will basically be represented in\nthis vector format\nthat is 0 0 1 0 0 0 0 right but after\ntraining while we are training the model\nwith different different weights this is\nmy true output this is my y I may also\nget different y hat right I may get some\nvalues\nlike25 I may get some values like 33\nthen like this 0 1 0 0 0 something like\nthis then what we do we basically\ncalculate the loss function and based on\nthis loss we need to reduce this we do\nthe backward\npropagation right backward propagation\nand we do it unless and until the\ndifference between Y and Y hat are\nminimal okay and this process is\ncontinuous very simple but now you near\nneed to understand one very important\nthing very very important thing now\nsince this is giving me a specific\noutput okay this is basically giving me\na specific output when I say my my\nmiddle layer is basically basically\nwindow size of five window size of five\nthat basically means over here in the\nword to when I said I will be getting a\n300 Dimensions over here when I'm using\nGoogle word\nToc this is all because of this window\nsize okay that basically means if my\nwindow size is five I'm going to get the\noutput as five for every word that\nbasically means when a word is getting\nconverted into a vector\nI am going to get a size of five vectors\nand this will basically be my final\noutput now I hope you able to understand\nagain let me repeat it the reason I have\nactually selected window size is equal\nto 5 because I want to probably provide\na feature\nrepresentation with a vector size of\nfive okay that basically means every\nword will be converted into a five\nVector now when I took an example of\nGoogle which was getting converted into\n300 Dimension that basically means my\nwindow size is 300 and more the bigger\nwindow size the better the model can\nbasically perform okay so in this case\nyou'll be able to see that over here my\nwindow size is five that basically means\nif I see from starting right my metrix\nfor every word will be 7 cross 5 that\nmany number of Weights will be there\nthen here also I'll be having 7 cross 5\nweights here also I'll be having 7 cross\n5 weights because I'm giving seven\ndifferent vectors here also I'm having 7\ncross 5 but in this case I will\nbasically be having 5 cross 7 now what\ndoes 5 cross 7 basically mean when this\nloss gets reduced then my final Vector\nwill look something like this this all\nwill get connected to this this all will\nget connected to this let's say it is\ngetting connected to this one it is\ngetting connected to this one it is\ngetting connected to this one so once we\nhave this particular connection let\nlet's say our first word over here is\nwhat is our first word if you probably\nsee what is our first word with respect\nto this particular vectors with\nvocabulary you'll be seeing I neuron\nright this is my vocabulary the first\nword that is basically getting\nrepresented over here is I neuron right\nso I neuron will have a output dimension\nof five\nvectors because I'm getting this five\nvectors over here joined to this so this\nfive vectors will be like 092 it can be\n94 based on the training it can be 0.25\nit can be 36 and it can be0\n45 and this is based on some feature\nrepresentation so I hope you're able to\nunderstand and this is how in neuron\nwill be represented the second word that\nwe have in the vocabulary that is\ncompany right this will again get\nconnected to this company this will also\nget connected this will also get\nconnected this will also get connected\nthis will also get and this entire word\nwill be the vector for the company\nitself and for the company we may have a\ndifferent Vector but again the size will\nbe five dimension because our window\nsize is\nfive and this training of forward and\nthe backward propagation when the loss\nis minimal then only we'll be able to\nget the vectors and that Vector is\nbasically taken and it will be\nrepresented in the uh format of the\nfeature representation for each and\nevery word so guys so we are going to\ncontinue the discussion with respect to\nnatural language processing in this\nvideo video we are going to discuss\nabout the second architecture that is\nSkip Gram now already I've actually\nshown you how does seow actually works\ncontinuous bag of words and I also\nshowed you that how the neural network\ngets strained right now what is the\ndifference between cow and Skip gram\nwhat is the difference between the\narchitecture it is very simple guys\nright now just focus I'm going to take\nthe same data set over here let's say\nthat ion compan is related to data\nscience I've written over here right and\nwith respect to this you can see that\nI've created my input and output now if\nI am using skip gr then the thing that\nis going to change is that everything\nwill be same let's say that I have taken\nthe window size as this so if I probably\ngo ahead and show you now what will\nhappen is\nthat before the input was this specific\ntest and the output was this specific\ntest now with the help of skip gram over\nhere the the the initially let's say\nthat if this was input now this is going\nto become the input and this will\nbasically be the output right and this\nis with respect to window size is equal\nto\n5 right window size is equal to 5 all\nthe steps will be same only what we are\ndoing is that we changing the input and\nwe are changing the output before the\ninput was this all text and the output\nwas this now what will happen is that\nentirely when we are creating this\nneural network now in the input let's\nsay that I have this isword related word\nto word right so in the input I'm going\nto basically\nhave uh input layer with seven vectors\nthat will be going Why seven because if\nwe probably see this our vectors how\nmany number of uh vocabulary what is the\nvocabulary size 1 2 3 4 5 6 7 right so\ninitially in the input layer I will be\ngiving a input which will be basically\nhaving uh seven vectors seven dimension\nvectors and then in the middle I will\nbasically be having my window size\nvectors so window size is nothing but\nfive 1 2 3 4 5 just understand this\nthese are just nodes okay and in the\noutput you can see that I'm having four\nwords right so everything that is\npresent with respect to this right\nsimilarly this will get constructed over\nhere at this point of time so over here\nin the output layer you'll be able to\nsee that I will be having one word two\nword\nthree\nword and the fourth word so this is what\nis the kind of output we will be getting\nand again uh here it is very much simple\nbecause every one will be like seven\nDimension over here again right so I\nhope you able to understand we have just\nchanged the Direction with respect to\nthis right so this will be my input\nlayer so here you can see that we will\nbasically be having a 7 cross 5 metrix\nwith respect to the weights because\nweights initially be randomly and\ninitialized right and then we need to\ntrain this right and then we will be\nhaving with respect to this all I'll be\nconnecting here to here here to here and\nthis will basically be a 5 cross 7 uh\nMatrix with respect to weights and then\nthis will be 5 cross 7 similarly over\nhere you'll be seeing that this will be\n5 cross 7 and Below one will also be a 5\ncross 7 but in with respect to that you\ncan see that initially when I give the\ninput is is will be uh the is vectors\nwill be going over here and as you know\nthat is is the third we third word so\nthis will be going like this right so 0\n0 1 0 0 0 0 so this is the vectors that\nwill be going over here and again since\nmy window size is five over here you'll\nbe able to see other vectors that will\nget initialized uh randomly this all\nwill be connected with weights so 7\ncross 5 weight metrix will be created\nand with respect to this our forward\npropagation will happen and obviously\nyou know that if you probably know a Ann\nright what all things happens in between\nthe hidden layer the input weights is\ngetting multiplied by the weights itself\nand then a bias is added and an\nactivation function is applied on top of\nit in the output layer we basically\napply a soft Max function soft Max\nfunction so that we compute it with Y\nand Y hat y hat is the predicted one y\nis the real data in this particular case\nthis is my y sorry this is my y right in\nthe first case Y is I neuron so I neuron\nwhatever um whatever things will be\nthere whatever vectors will be there\nhere you'll be initializing it right so\nhere I basically apply a softmax\nfunction let's say with respect to I\nneuron my Y is nothing but 1 0 0 0 0 0 0\n0 7 zeros and then y hat will be\ncomputed since we apply a soft Max over\nhere right so y hat will be something\nright some values over here and then\nwhat we do we calculate our loss\nfunction and we make sure sure that we\nkeep on doing the forward and the\nbackward propagation unless and until\nall the loss function decreases right\nthe loss value decreases and finally\nyou'll be seeing that whatever is\nconnected right this particular word\nwill be shown in the form of five\nvectors this word will be shown in the\nform of five vectors once the loss is\ncompletely minimized so same process you\nshould definitely know how the Ann\nactually works how the optimizer\nactually work and this was just a brief\nidea about script gram right now how can\nwe improve this or the the basic\nquestion is that when should we apply C\nboy C so the question is when\nshould we\napply\ncball\nor skip gram right the simple thing is\nthat according to the research right\nwhenever you have a small data set small\nCorpus we can Bally go with something\nlike C that is continuous bag of word if\nyou have a huge data set you should\ndefinitely go with skip gram and that is\nproven uh in many research paper so I'm\njust giving you the direct um\nobservation out of it so that you'll be\nable to do this now let's say if you\nwant to increase cow or skip gram how\ncan you basically do it one thing is\nthat you should increase your training\ndata set increase the training data that\nbasically means the more the training\ndata the better the accuracy right\nincrease the training\ndata the second thing is that you can\nalso increase\nincrease the window\nsize window size which in\nturn which in turn leads\nto leads to increase of\nDimensions increase of\nVector Dimension this is super\nimportant okay so here I'm\nsaying how to improve cow or skip gram\nso this is what you can basically do\nincrease the window size okay this is\nsuper important increase the window size\nI'm again going to take this separately\nor forget it out so over here again uh\nlet me write this we have to increase we\ncan also increase the window size\ninstead of having five I can make this\nas uh 100 you know so obviously with the\nincrease in the window size increase the\nwindow\nsize if we are increasing the window\nsize that basically means the vector\nDimension is also increasing right the\nvector\nDimension is also increasing so when we\nkeep on increasing and trying uh try it\nyou'll be able to see that we'll be\ngetting better performance so this is\nalso you can basically use you can you\ncan increase it now when we see in the\nnext example right we'll be we'll be\nusing a pre-trained model with respect\nto Google so Google word to right now\nthis is basically trained in 3 billion\nwords I guess it is 3 billion words 3\nbillion words and it is going to give me\na feature representation feature\nrepresentation of 300 vectors sorry 300\nDimensions that basically means suppose\nif I have a word\nCricket okay since Cricket is always\nthere in the news this 3 billion word is\nfrom the Google news right and Google is\na very big company guys with them this\namount of data it is very much easy so\nwhat they're going to do is that if we\ngive a word called as cricet then it is\ngoing to basically convert this into a\n300 Dimension 300 Dimension\nvectors okay vectors they is super\nimportant 300 Dimension vectors okay so\nI'm just going to write it as 300\nDimension vectors and this all example I\nwill try try to show you uh when we use\nthis what we'll do is that in the\nupcoming session we'll try to we'll try\nto use a pre-trained model also and\nwe'll try to also make sure that we\ntrain a new data set from scratch with\nthe help of word Tock and that we are\ngoing to basically do with the Gen Sim\nLibrary okay so yes uh this we'll be\ndoing in the next video so I hope you\nhave understood both the architecture\none is cball and one is the skip gram\nhello guys so we are going to continue a\ndiscussion with respect to NLP and in\nthis this video we're going to discuss\nabout average word to w a super\nimportant topic again because uh just by\nwith the help of word to you'll not be\nable to solve uh the classification\nproblem you really need to perform\nsomething called as average word to\nlet's take a simple example I'll try to\nshow you uh right now we'll just discuss\nwith respect to theoretical intution but\nas we go ahead uh we'll be discussing\nabout practical part also and how we can\nImplement average word to work also so\nuh over here I have my text Data the\nfoot is good and the output is 1 Z and\none and you can see all this are my\ndocuments now as you know that with the\nhelp of word to what we do is that we\ntake every word and we convert that into\nvectors let's say that I'm using a\nGoogle pre-train model okay I'm using a\nGoogle pre-trained word to model\npre-trained word to W\nmodel now in this Google pre-trained\nword to model what I'm actually doing is\nthat\nwhat I'm actually doing is that over\nhere you can definitely see that okay so\nthis if this is my sentence the food is\ngood right now the will get converted\ninto some vectors okay and uh if when\nI've already shown you that with respect\nto feature representation we will be\ngetting a vector of 300 Dimensions so\nlet's say this is the vector of 300\nDimensions okay so this the will get\nconverted into 300 Dimensions with\nrespect to this okay the will be there\nnow coming to the next word that is food\nright food will also get converted into\n300 dimensions and that is what happens\nwith respect to this word t right every\nword that is present over here like is\nwill also get uh will also\nget formed in 300 dimensions and here\nalso all this words will get converted\ninto vectors with respect to this many\nnumber of Dimension okay perfect so till\nhere I think uh you're superbly clear\nbecause we have already discussed about\nall these things uh in our previous\nsession but now you need to understand\none thing is that when we have this kind\nof sentences and if we are basically\nwriting this many number of Dimensions\nbasically this many number of vectors\nare there let's say this is my 300\nDimension Vector which is basically\ngetting converted for the foot and this\nin the real world scenario I should be\nconverting this entire sentence into\nof vectors right but here you can see\nI'm getting vectors for the also 300 300\nvectors for foot I'm getting separately\n300 vectors for is I'm getting 300\nvectors and for good I'm getting 300\nvectors right 300 Dimension vectors now\nthis is a\nproblem at the end of the day for this\nentire sentence let's say if I'm getting\na 300 dimension for this entire sentence\nlet's say this is my sentence one or\ndocument\none my main aim should be that I should\nbe getting only 1 300 diamension with\nrespect to this so that I can take this\nas an input and this will basically be\nmy output and I can basically train my\nmodel right but here you can see that\nsince we applying word to we every every\nword is basically getting converted into\na separate 300 Dimension vectors so in\norder to solve this what we do is that\nwe take all these\nvectors we take all these vectors and we\nfind the average of it once we find the\naverage of it then we try to write that\nparticular V Vector over here for this\nentire sentence and then this Vector\nwill be considered for this entire\ndocument or sentence similarly the\nsecond Vector also is there we will go\nahead and calculate the average of it\nand we'll write it over here similarly\nlike this every Vector will basically\nhappen right for every every Vector so\nevery all the all the word vectors you\nknow averaging is basically happening\nand that is the reason we say average\nword to work okay and why this will work\nbecause see at the end of the day you'll\nbe seeing that we just require the\nvectors for this entire sentence and\nright now we have for every word a\nseparate 300 Dimension vectors so if we\nare trying to take out the average of\nall this particular vector and if we try\nto write down as one\nvector and this will also be the length\nwill obviously be 300 itself right this\nwill be 300 Dimension itself but now\nwith respect to this particular Vector I\nwill be having my output I can pass it\nthrough my model and get trained with\nrespect to this this so at the end of\nthe day what happens is that if I'm\nhaving a sentence like this after\napplying average word to\nW I'm just going to get let's say if I'm\nusing the pre-train model I'm going to\nget a 300 Dimension vectors which will\nbe all these particular average of all\nthese particular vectors itself and all\nthis will be over here and I will be\ngetting my output variable which is\nalready over here and similarly this\nwill happen for the second word set\ndocument third document also so average\nword to work basically says that okay\nwe're not doing anything whatever\nvectors is getting converted I'm just\ntrying to average out each and every\nVector with respect to that right so\nover here let's say the is having 300\nDimension Vector foood is having 300\nDimension Vector is is having 300 di\ngood is having 300 for every line by\nline all the vectors I'm just averaging\nthat out and I'm actually finding out a\nvalue and why we are doing this because\nwe really want only one specific set of\nvectors for the entire\nsentence right so this is what average\nword to is basically and for for any\ntext classification we basically needed\nto solve through this specific way only\nnow in the upcoming tutorials what I'm\nactually going to show is that I'm going\nto basically use a library which is\ncalled as genim okay and there is also a\nlibrary separately which is called as\nGlobe but if you probably understand Jim\nyou'll be able to do everything so first\nof all we will go ahead and we will see\nwith respect to a pre-rain Google word\nToc and then in the second instance what\nwe are going going to do is that we\ngoing to\ntrain a word to W from\nscratch from scratch that basically me\nwe'll take a data set and we'll try to\ntrain a word to we model and again with\nthe help of genim Library we'll be doing\nit so I hope you got an idea about\naverage word to at the end of why we are\ndoing this because for the entire\nsentence we want some vectors and uh\nsince we combining all this particular\nword that semantic information will also\nbe maintained right so yes I will see\nyou all in the next video with the\npractical IC implementation hello guys\nso in this video we are going to\nbasically see the word to W practical\nimplementation I really want to show you\nsome Google pre-trained models and\nbasically give you an idea like how what\ncreates a vector right so for this\ntutorial I'm going to use a library\nwhich is called as genim so let's go\nahead and let's install this particular\nLibrary so you just need to write pip\ninstall genim so here you can see that\nrequirement is already satisfied and uh\nwhat I'm actually going to do is that\nI'm going to import this import genim\nand from jim. model I'm going to import\nword to and key vectors okay and I'll\ntalk about this why these two libraries\nare specifically required now one very\nimportant thing for this as I said that\nto show you the Practical implementation\nof word to here I'm going to take a\nGoogle pre-trained model in the upcoming\nvideo I will try to show you a different\nmodel which can be trained from scratch\nbut here I'm going to show show you a\nGoogle word to pre-train model and what\nthis particular model is all about so\nI'm basically taking a word to Google\nNews 300 okay and uh this is basically a\npre-trained vector trained on Google\nNews data set about 100 billion words\nthe model contains 300 dimensional\nvectors for 3 million words and phrases\nright this phrases were obtained using a\nsample data driven approached and\ndistributed represent of this and all\nall the research paper and everything is\nbasically given over here okay so this\nsame model we are going to use and we\ngoing to see that how it can easily\ncreate a vectors whenever we give any\nkind of words so in gen Sim you know you\nhave something called as API right so\nthere is a library called as API so you\njust need to write import jim.\ndownloader as API and you just need to\nwrite api. load and basically the model\nname right so the model name is nothing\nbut what to Google new use uh- 300 okay\nnow once you do this all you have to do\nis that when you provide any word inside\nthis WV variable which is nothing but\nthis is a instance of the specific model\nit will try to give you the vector I'm\nnot going to execute this line of code\nbecause I've already done it because the\nmodel size is 166 2.8 MB right so it'll\nprobably take some time for you all to\ndownload so I have already downloaded\nthis so that I can record the video\ndirectly you can just go ahead and and\ndownloaded now let me go ahead see that\nhow the king Vector look like okay so\nthis is basically the king uh vectors\nhow the word is basically converted into\nvectors and the dimensions that you'll\nbe seeing right how many vectors are\nthere as said from here right we have\n300 Dimensions so this is what we\ngetting this entire vectors you'll be\nable to see will be having 300\nDimensions so if I probably use this\nVector uncore King do shape right so if\nif I execute this here you'll be able to\nsee 300 Dimensions right so with respect\nto this you use any kind of word you\nwill be able to get some kind of vectors\nnow let me give you some of the example\nokay suppose I make over here and uh for\nthis particular Vector what all things I\nuse I use this WV variable which is a\nword to object right all I have to do I\nhave to give this and give any word of\nyour choice let's say I want to give\nCricut right so if I give probably\ncricet you'll be able to see that the\nvector is Auto atically generated so\nhere is again the vector which you are\nbasically getting with respect to this\nparticular shape that is 300 Dimension\nnow this WV this WV variable right which\nis a word Vector it also has some of the\nfunctions you can actually use something\ncalled as most underscore similar now\nlet's say that I've given this most\nunderscore similar I'm saying that from\nuh if I'm giving this Cricket word right\nwhich is the most similar word present\nin this entire Corpus right and I'm\nactually going to use the same word to W\nobject and I'm going to find out all the\nwords so if I execute this here you'll\nbe able to see the most similar with\nrespect to Cricket you know it will\nfirst of all try to convert into a\nvector and then it'll probably check\nwhether that Vector is being able to see\nany similar kind of words or not so\nthese are all the words that is similar\nto Cricket in that specific Corpus so\nyou have cricketing it is having like 83\nsimilarity Point 83% similarity then\ncricketers 81% similarity then you have\ntest cricket here you can see 80%\nsimilarity 20 Cricket right you have 80%\nsimilarity then you have Cricket then\nyou have cricketer all this things you\nknow they are showing some kind of\nsimilarities with respect to this Google\nNews field right now similarly if I\nreally want to find out which is the\nmost similar for word with respect to\nhappy you'll also be able to see I'll be\nhaving words like glad pleased ecstatic\nOverjoyed thrilled satisfied proud\ndelighted and understand that these all\nare similar words when I compare to\nHappy right so it is also being able to\nsay that okay these are the similar\nwords when compared to the Happy word\nand they also being able to show with\nrespect to how much distance and I've\nalready shown you how this distance is\nbasically calculated right so we have a\nconcept of cosign similarity and all\nsimilarly I can also provide two two\nwords and I can basically say that okay\nhow much this two word are basically\nsimilar so if I probably execute this\nhere you'll be able to see wv.\nsimilarity I'm getting hockey and sports\nare somewhere on a 53 person similar\nright now this is very interesting okay\nI'm going to take the vector of King and\nI'm going to subtract with the vector of\nman and I'm going to add the vector of\nwoman now let's see what will be the\nkind of output vectors that I'll be\ngetting okay so once I do this in short\nwhat I'm doing I'm subtracting King\nminus man plus woman obviously the\nanswer should be Queen but I really want\nto prove you through the vectors itself\nwhether we are able to get the queen or\nnot so here I'm going to execute this\nI'll got I got my vectors so this is my\nentire Vector so this is my entire\nvector and again this Vector is of 300\nDimensions okay so this is my entire\nVector now what I'm going to do I'm\ngoing to use wv. most uncore similar and\nI'm just going to pass this entire\nVector over here so once I execute this\nhere you'll be able to see that see King\nis the first obviously uh King should be\nthe most similar Vector but here I'm\ngetting Queen Monarch princess crown\nprince prince prince sorry Kings Sultan\nQueen Resort but here you can see that\nthe most similar word after King is\nQueen so the kind of vector we are\ngetting after doing all this particular\nsubtraction matches matches more towards\nthe vector with respect to Queen right\nand this is what we are able to get with\nthe help of word to W try to use this\nguys this is an amazing model altogether\nyou'll be able to see uh that you can\nactually use this model itself you know\nyou can actually use word to Google News\n300 which will be able to solve many\nproblem of yours and this is just just a\nbrief idea about a pre-trained model you\nknow how a what to pre-train model looks\nlike you can also take your own text and\ntrain it from scratch but again there's\na different process for allog together\nand this entire thing I've executed in\nGoogle collab because the model size is\nquite huge okay uh so in the upcoming\nvideos what I'm actually going to do I'm\nalso going to show you a what to\ntechnique by using genim where you can\ntrain the model from scratch okay so yes\nuh this was it for for this particular\nvideo I will see you all in the next\nvideo thank you\n",
  "words": [
    "hello",
    "guys",
    "quite",
    "excited",
    "start",
    "nlp",
    "series",
    "natural",
    "language",
    "processing",
    "machine",
    "learning",
    "video",
    "going",
    "see",
    "entire",
    "road",
    "map",
    "like",
    "go",
    "ahead",
    "prepare",
    "natural",
    "language",
    "processing",
    "nlp",
    "amazing",
    "domain",
    "amazing",
    "tech",
    "part",
    "respect",
    "machine",
    "learning",
    "deep",
    "learning",
    "lot",
    "research",
    "basically",
    "happening",
    "natural",
    "language",
    "processing",
    "let",
    "go",
    "ahead",
    "let",
    "share",
    "screen",
    "uh",
    "let",
    "go",
    "ahead",
    "let",
    "explain",
    "road",
    "map",
    "like",
    "basically",
    "go",
    "ahead",
    "preparation",
    "uh",
    "begin",
    "uh",
    "till",
    "probably",
    "know",
    "machine",
    "learning",
    "okay",
    "machine",
    "learning",
    "actually",
    "seen",
    "right",
    "solve",
    "two",
    "different",
    "kind",
    "problems",
    "one",
    "supervised",
    "supervised",
    "one",
    "basically",
    "solve",
    "something",
    "called",
    "unsupervised",
    "machine",
    "learning",
    "supervis",
    "machine",
    "learning",
    "use",
    "cases",
    "specifically",
    "solve",
    "two",
    "different",
    "kind",
    "problem",
    "statements",
    "like",
    "classification",
    "regression",
    "right",
    "problem",
    "statement",
    "right",
    "seen",
    "let",
    "say",
    "specific",
    "set",
    "features",
    "like",
    "f1",
    "fs2",
    "f3",
    "f4",
    "like",
    "number",
    "features",
    "features",
    "usually",
    "called",
    "independent",
    "features",
    "right",
    "probably",
    "talk",
    "input",
    "features",
    "also",
    "say",
    "independent",
    "features",
    "independent",
    "features",
    "right",
    "similarly",
    "also",
    "output",
    "feature",
    "dependent",
    "feature",
    "right",
    "dependent",
    "feature",
    "aim",
    "supervis",
    "machine",
    "learning",
    "model",
    "respect",
    "features",
    "obviously",
    "lot",
    "data",
    "points",
    "output",
    "classification",
    "problem",
    "regression",
    "problem",
    "may",
    "continuous",
    "values",
    "may",
    "uh",
    "classified",
    "points",
    "like",
    "ones",
    "zeros",
    "binary",
    "multiclass",
    "okay",
    "let",
    "say",
    "kind",
    "data",
    "points",
    "main",
    "aim",
    "usually",
    "create",
    "model",
    "okay",
    "usually",
    "create",
    "model",
    "train",
    "model",
    "data",
    "right",
    "basically",
    "create",
    "model",
    "train",
    "particular",
    "model",
    "specific",
    "data",
    "set",
    "right",
    "specific",
    "data",
    "set",
    "model",
    "able",
    "make",
    "capable",
    "make",
    "predictions",
    "whenever",
    "give",
    "kind",
    "input",
    "data",
    "machine",
    "learning",
    "specifically",
    "talk",
    "features",
    "right",
    "f1",
    "continuous",
    "features",
    "fs2",
    "basically",
    "categorical",
    "features",
    "also",
    "different",
    "types",
    "features",
    "particular",
    "scenario",
    "let",
    "say",
    "features",
    "completely",
    "made",
    "text",
    "let",
    "say",
    "one",
    "basic",
    "example",
    "really",
    "want",
    "give",
    "spam",
    "class",
    "classification",
    "let",
    "say",
    "example",
    "spam",
    "classification",
    "spam",
    "classification",
    "features",
    "may",
    "may",
    "let",
    "say",
    "uh",
    "main",
    "aim",
    "basically",
    "detect",
    "whether",
    "email",
    "comes",
    "spam",
    "spam",
    "okay",
    "let",
    "say",
    "classification",
    "problem",
    "really",
    "want",
    "solve",
    "scenario",
    "features",
    "may",
    "one",
    "feature",
    "may",
    "something",
    "called",
    "email",
    "subject",
    "okay",
    "may",
    "next",
    "feature",
    "email",
    "body",
    "output",
    "feature",
    "basically",
    "whether",
    "mail",
    "spam",
    "ham",
    "ham",
    "basically",
    "means",
    "spam",
    "let",
    "say",
    "give",
    "one",
    "example",
    "let",
    "say",
    "email",
    "subject",
    "like",
    "billion",
    "see",
    "completely",
    "text",
    "right",
    "email",
    "body",
    "like",
    "wanton",
    "lottery",
    "okay",
    "giving",
    "example",
    "lottery",
    "billion",
    "dollar",
    "think",
    "get",
    "kind",
    "emails",
    "right",
    "obviously",
    "get",
    "email",
    "u",
    "real",
    "world",
    "scenario",
    "classifying",
    "particular",
    "points",
    "basically",
    "spam",
    "put",
    "category",
    "one",
    "also",
    "put",
    "spam",
    "one",
    "thing",
    "notice",
    "whenever",
    "input",
    "features",
    "continuous",
    "variable",
    "categorical",
    "variables",
    "obviously",
    "different",
    "different",
    "techniques",
    "convert",
    "categorical",
    "variables",
    "continuous",
    "values",
    "right",
    "uh",
    "techniques",
    "like",
    "one",
    "hot",
    "encoding",
    "techniques",
    "like",
    "target",
    "encoding",
    "ordinal",
    "encoding",
    "techniques",
    "basically",
    "feature",
    "engineering",
    "let",
    "say",
    "entire",
    "data",
    "text",
    "sentence",
    "like",
    "right",
    "particular",
    "scenario",
    "definitely",
    "easy",
    "model",
    "understand",
    "right",
    "obviously",
    "model",
    "understand",
    "human",
    "language",
    "right",
    "right",
    "written",
    "english",
    "tomorrow",
    "chinese",
    "tomorrow",
    "languages",
    "model",
    "directly",
    "capable",
    "understanding",
    "particular",
    "text",
    "particular",
    "scenario",
    "techniques",
    "techniques",
    "convert",
    "text",
    "meaningful",
    "vectors",
    "meaningful",
    "vectors",
    "vectors",
    "vectors",
    "like",
    "numbers",
    "understand",
    "vectors",
    "represent",
    "meaningful",
    "information",
    "respect",
    "particular",
    "text",
    "okay",
    "whenever",
    "input",
    "data",
    "form",
    "text",
    "sentences",
    "basically",
    "use",
    "something",
    "called",
    "natural",
    "language",
    "processing",
    "able",
    "process",
    "particular",
    "data",
    "able",
    "make",
    "model",
    "understand",
    "solve",
    "use",
    "cases",
    "like",
    "spam",
    "classification",
    "right",
    "entire",
    "context",
    "behind",
    "nlp",
    "much",
    "popular",
    "nowadays",
    "see",
    "lot",
    "examples",
    "alexa",
    "right",
    "think",
    "many",
    "people",
    "use",
    "alexa",
    "many",
    "people",
    "use",
    "google",
    "home",
    "right",
    "many",
    "people",
    "use",
    "automated",
    "device",
    "like",
    "let",
    "say",
    "ac",
    "running",
    "right",
    "say",
    "hey",
    "switch",
    "ac",
    "switch",
    "ac",
    "particular",
    "machine",
    "able",
    "understand",
    "natural",
    "language",
    "processing",
    "google",
    "extensively",
    "amazing",
    "research",
    "respect",
    "nlp",
    "coming",
    "amazing",
    "things",
    "yes",
    "learning",
    "uh",
    "respect",
    "machine",
    "learning",
    "go",
    "also",
    "try",
    "learn",
    "respect",
    "deep",
    "learning",
    "try",
    "understand",
    "video",
    "going",
    "understand",
    "road",
    "map",
    "like",
    "go",
    "ahead",
    "prepare",
    "respect",
    "nlp",
    "going",
    "write",
    "road",
    "map",
    "nlp",
    "okay",
    "also",
    "going",
    "use",
    "different",
    "different",
    "libraries",
    "going",
    "draw",
    "pyramid",
    "kind",
    "structure",
    "go",
    "bottom",
    "top",
    "approach",
    "okay",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "understand",
    "road",
    "map",
    "nlp",
    "initially",
    "begin",
    "initially",
    "begin",
    "going",
    "create",
    "small",
    "block",
    "okay",
    "begin",
    "first",
    "block",
    "basically",
    "initially",
    "need",
    "know",
    "one",
    "programming",
    "language",
    "let",
    "say",
    "going",
    "probably",
    "go",
    "ahead",
    "python",
    "programming",
    "language",
    "super",
    "important",
    "right",
    "help",
    "python",
    "programming",
    "language",
    "obviously",
    "able",
    "solve",
    "lot",
    "use",
    "cases",
    "nlp",
    "go",
    "step",
    "one",
    "basically",
    "step",
    "one",
    "step",
    "one",
    "nothing",
    "basically",
    "called",
    "text",
    "preprocessing",
    "text",
    "initially",
    "start",
    "basic",
    "things",
    "like",
    "whenever",
    "kind",
    "text",
    "data",
    "text",
    "things",
    "need",
    "bic",
    "basically",
    "clean",
    "particular",
    "text",
    "data",
    "things",
    "basically",
    "come",
    "text",
    "okay",
    "techniques",
    "probably",
    "going",
    "apply",
    "like",
    "tokenization",
    "tokenization",
    "concept",
    "wherein",
    "convert",
    "paragraph",
    "sentence",
    "sentence",
    "words",
    "different",
    "different",
    "things",
    "also",
    "going",
    "learn",
    "techniques",
    "like",
    "climatization",
    "going",
    "learn",
    "techniques",
    "like",
    "stemming",
    "also",
    "going",
    "introduce",
    "words",
    "like",
    "something",
    "called",
    "stop",
    "words",
    "things",
    "basically",
    "come",
    "covering",
    "text",
    "part",
    "one",
    "basically",
    "step",
    "one",
    "okay",
    "going",
    "write",
    "step",
    "one",
    "super",
    "important",
    "initially",
    "starting",
    "okay",
    "uh",
    "entire",
    "detailed",
    "syllabus",
    "obviously",
    "going",
    "make",
    "video",
    "video",
    "make",
    "understand",
    "everything",
    "thousand",
    "feet",
    "overview",
    "giving",
    "lot",
    "topics",
    "inside",
    "really",
    "need",
    "familiar",
    "coming",
    "second",
    "one",
    "okay",
    "second",
    "one",
    "basically",
    "say",
    "text",
    "step",
    "two",
    "okay",
    "also",
    "uh",
    "text",
    "technique",
    "little",
    "bit",
    "try",
    "increase",
    "complexity",
    "tries",
    "solve",
    "problems",
    "text",
    "focus",
    "converting",
    "text",
    "data",
    "vectors",
    "going",
    "write",
    "step",
    "two",
    "write",
    "basically",
    "text",
    "part",
    "two",
    "going",
    "learn",
    "topics",
    "like",
    "bag",
    "words",
    "okay",
    "tf",
    "idf",
    "also",
    "going",
    "learn",
    "things",
    "like",
    "unigrams",
    "byrams",
    "lot",
    "concepts",
    "like",
    "going",
    "cover",
    "text",
    "technique",
    "understand",
    "main",
    "aim",
    "main",
    "aim",
    "basically",
    "convert",
    "let",
    "write",
    "like",
    "step",
    "one",
    "step",
    "two",
    "instead",
    "writing",
    "like",
    "like",
    "write",
    "simpler",
    "way",
    "okay",
    "basically",
    "focusing",
    "cleaning",
    "text",
    "right",
    "cleaning",
    "input",
    "cleaning",
    "input",
    "particular",
    "step",
    "trying",
    "focus",
    "converting",
    "input",
    "text",
    "vectors",
    "super",
    "important",
    "step",
    "vector",
    "able",
    "make",
    "sure",
    "context",
    "statement",
    "able",
    "get",
    "captured",
    "right",
    "end",
    "day",
    "nlp",
    "whatever",
    "techniques",
    "right",
    "like",
    "transformers",
    "birds",
    "quite",
    "advanced",
    "techniques",
    "able",
    "convert",
    "input",
    "text",
    "meaningful",
    "vectors",
    "able",
    "solve",
    "particular",
    "use",
    "cases",
    "better",
    "manner",
    "right",
    "basically",
    "second",
    "step",
    "focus",
    "uh",
    "converting",
    "input",
    "text",
    "vectors",
    "still",
    "advanced",
    "techniques",
    "text",
    "go",
    "third",
    "step",
    "focus",
    "going",
    "write",
    "text",
    "preprocessing",
    "respect",
    "part",
    "three",
    "okay",
    "things",
    "basically",
    "focus",
    "use",
    "advanced",
    "techniques",
    "like",
    "word",
    "word",
    "w",
    "average",
    "word",
    "w",
    "also",
    "technique",
    "convert",
    "input",
    "text",
    "vectors",
    "may",
    "thinking",
    "krish",
    "second",
    "step",
    "also",
    "written",
    "thing",
    "third",
    "step",
    "also",
    "written",
    "thing",
    "yes",
    "guys",
    "understand",
    "go",
    "second",
    "step",
    "third",
    "step",
    "conversion",
    "input",
    "text",
    "vectors",
    "better",
    "approach",
    "basically",
    "use",
    "b",
    "bag",
    "words",
    "tfidf",
    "unigram",
    "byrams",
    "right",
    "learner",
    "learner",
    "really",
    "need",
    "know",
    "particular",
    "steps",
    "right",
    "focus",
    "techniques",
    "like",
    "word",
    "average",
    "word",
    "uh",
    "word",
    "average",
    "word",
    "kind",
    "deep",
    "learning",
    "technique",
    "uh",
    "try",
    "learn",
    "try",
    "understand",
    "basically",
    "happens",
    "okay",
    "coming",
    "next",
    "step",
    "uh",
    "focus",
    "right",
    "continue",
    "understanding",
    "entire",
    "road",
    "map",
    "also",
    "focus",
    "understanding",
    "rnn",
    "lstm",
    "rnn",
    "gru",
    "okay",
    "gru",
    "rnn",
    "guys",
    "deep",
    "learning",
    "technique",
    "basically",
    "used",
    "handling",
    "solving",
    "text",
    "related",
    "use",
    "cases",
    "like",
    "spam",
    "classification",
    "text",
    "summarization",
    "many",
    "things",
    "neural",
    "networks",
    "familiar",
    "road",
    "map",
    "uh",
    "basically",
    "enter",
    "deep",
    "learning",
    "part",
    "super",
    "important",
    "understand",
    "uh",
    "said",
    "part",
    "deep",
    "learning",
    "technique",
    "okay",
    "since",
    "writing",
    "road",
    "map",
    "really",
    "need",
    "mention",
    "things",
    "coming",
    "next",
    "one",
    "uh",
    "also",
    "technique",
    "called",
    "word",
    "embedding",
    "okay",
    "also",
    "amazing",
    "way",
    "convert",
    "input",
    "text",
    "vectors",
    "internally",
    "talk",
    "particular",
    "text",
    "preprocessing",
    "also",
    "uses",
    "techniques",
    "like",
    "word",
    "embedding",
    "okay",
    "also",
    "technique",
    "technique",
    "basically",
    "called",
    "word",
    "embeddings",
    "word",
    "embeddings",
    "internally",
    "uses",
    "amount",
    "word",
    "w",
    "train",
    "word",
    "embedding",
    "techniques",
    "okay",
    "uh",
    "technique",
    "converting",
    "input",
    "text",
    "vectors",
    "similarly",
    "techniques",
    "like",
    "transformers",
    "birds",
    "also",
    "coming",
    "next",
    "one",
    "basically",
    "transformer",
    "advanced",
    "technique",
    "really",
    "wanted",
    "mention",
    "things",
    "able",
    "understand",
    "really",
    "want",
    "become",
    "pro",
    "really",
    "need",
    "go",
    "particular",
    "pattern",
    "try",
    "complete",
    "till",
    "b",
    "try",
    "see",
    "application",
    "go",
    "bottom",
    "top",
    "accuracy",
    "model",
    "keeps",
    "increasing",
    "remember",
    "one",
    "thing",
    "size",
    "model",
    "also",
    "increases",
    "right",
    "whichever",
    "machine",
    "learning",
    "models",
    "deep",
    "learning",
    "models",
    "probably",
    "trying",
    "create",
    "solve",
    "nlp",
    "use",
    "cases",
    "keep",
    "increasing",
    "go",
    "words",
    "transformer",
    "bd",
    "initially",
    "said",
    "going",
    "learn",
    "respect",
    "nlp",
    "respect",
    "machine",
    "learning",
    "nlp",
    "machine",
    "learning",
    "things",
    "going",
    "focus",
    "going",
    "focus",
    "three",
    "things",
    "first",
    "start",
    "deep",
    "learning",
    "probably",
    "focus",
    "three",
    "okay",
    "going",
    "write",
    "important",
    "thing",
    "three",
    "steps",
    "part",
    "machine",
    "learning",
    "say",
    "machine",
    "learning",
    "using",
    "libraries",
    "like",
    "nltk",
    "also",
    "libraries",
    "like",
    "spacey",
    "actually",
    "help",
    "us",
    "perform",
    "task",
    "available",
    "bottom",
    "three",
    "right",
    "focus",
    "one",
    "library",
    "called",
    "called",
    "nltk",
    "suppose",
    "know",
    "nltk",
    "think",
    "learning",
    "space",
    "also",
    "much",
    "easy",
    "case",
    "deep",
    "learning",
    "usually",
    "go",
    "libraries",
    "like",
    "tensor",
    "flow",
    "pytorch",
    "libraries",
    "quite",
    "amazing",
    "respect",
    "know",
    "tensorflow",
    "open",
    "source",
    "py",
    "toch",
    "open",
    "source",
    "tensorflow",
    "created",
    "google",
    "py",
    "toch",
    "created",
    "facebook",
    "right",
    "end",
    "day",
    "main",
    "aim",
    "input",
    "data",
    "form",
    "text",
    "data",
    "basically",
    "perform",
    "amazing",
    "kind",
    "text",
    "pre",
    "processing",
    "convert",
    "input",
    "text",
    "data",
    "vectors",
    "able",
    "solve",
    "amazing",
    "use",
    "cases",
    "nlp",
    "using",
    "machine",
    "learning",
    "deep",
    "learning",
    "brief",
    "idea",
    "entire",
    "road",
    "map",
    "app",
    "uh",
    "like",
    "going",
    "prepare",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "nlp",
    "video",
    "going",
    "discuss",
    "amazing",
    "use",
    "cases",
    "use",
    "cases",
    "use",
    "every",
    "day",
    "activities",
    "right",
    "let",
    "go",
    "ahead",
    "let",
    "share",
    "screen",
    "let",
    "talk",
    "every",
    "use",
    "cases",
    "one",
    "one",
    "discussing",
    "give",
    "clear",
    "idea",
    "natural",
    "language",
    "processing",
    "opened",
    "gmail",
    "tab",
    "specific",
    "gmail",
    "able",
    "see",
    "uh",
    "let",
    "say",
    "really",
    "want",
    "write",
    "email",
    "right",
    "say",
    "hello",
    "crush",
    "okay",
    "let",
    "say",
    "made",
    "spelling",
    "mistake",
    "okay",
    "wanted",
    "clarify",
    "actually",
    "written",
    "wrong",
    "spelling",
    "see",
    "automatically",
    "spelling",
    "corrected",
    "right",
    "nlp",
    "also",
    "like",
    "suppose",
    "receive",
    "email",
    "someone",
    "right",
    "also",
    "write",
    "automated",
    "email",
    "basically",
    "means",
    "basically",
    "tell",
    "us",
    "kind",
    "text",
    "autogenerate",
    "give",
    "us",
    "actually",
    "send",
    "would",
    "like",
    "wanted",
    "clarify",
    "data",
    "science",
    "let",
    "say",
    "write",
    "course",
    "okay",
    "let",
    "see",
    "basically",
    "gives",
    "also",
    "give",
    "suggestion",
    "like",
    "basically",
    "select",
    "course",
    "right",
    "also",
    "one",
    "amazing",
    "use",
    "cases",
    "use",
    "every",
    "day",
    "let",
    "say",
    "go",
    "linkedin",
    "uh",
    "rudra",
    "pratap",
    "singh",
    "one",
    "student",
    "automatically",
    "see",
    "two",
    "tags",
    "automated",
    "replies",
    "linkedin",
    "also",
    "amazing",
    "application",
    "nlp",
    "really",
    "want",
    "provide",
    "particular",
    "two",
    "tag",
    "click",
    "reply",
    "back",
    "short",
    "saving",
    "lot",
    "time",
    "similarly",
    "go",
    "respect",
    "one",
    "application",
    "google",
    "translate",
    "hope",
    "everybody",
    "uses",
    "let",
    "say",
    "go",
    "ahead",
    "write",
    "one",
    "language",
    "say",
    "right",
    "probably",
    "want",
    "convert",
    "arabic",
    "okay",
    "looks",
    "something",
    "like",
    "specific",
    "language",
    "suppose",
    "really",
    "want",
    "convert",
    "hindi",
    "uh",
    "basically",
    "say",
    "automatically",
    "text",
    "coming",
    "like",
    "right",
    "hindi",
    "basically",
    "say",
    "also",
    "happening",
    "nlp",
    "amazing",
    "application",
    "use",
    "websites",
    "also",
    "right",
    "let",
    "say",
    "linkedin",
    "somebody",
    "posting",
    "different",
    "language",
    "get",
    "option",
    "something",
    "called",
    "c",
    "translation",
    "click",
    "particular",
    "thing",
    "automatically",
    "gets",
    "converted",
    "english",
    "guys",
    "let",
    "say",
    "going",
    "go",
    "ahead",
    "search",
    "krishn",
    "krishn",
    "name",
    "youtuber",
    "also",
    "ion",
    "probably",
    "go",
    "selct",
    "select",
    "images",
    "see",
    "already",
    "able",
    "detect",
    "image",
    "turn",
    "converting",
    "text",
    "images",
    "understanding",
    "text",
    "basically",
    "thinking",
    "okay",
    "knows",
    "talking",
    "based",
    "web",
    "right",
    "able",
    "capture",
    "images",
    "available",
    "web",
    "also",
    "amazing",
    "thing",
    "probably",
    "probably",
    "go",
    "ahead",
    "select",
    "videos",
    "also",
    "able",
    "see",
    "videos",
    "youtube",
    "channel",
    "probably",
    "public",
    "platform",
    "accessible",
    "web",
    "also",
    "amazing",
    "example",
    "text",
    "image",
    "text",
    "video",
    "lot",
    "research",
    "also",
    "currently",
    "going",
    "respect",
    "different",
    "different",
    "companies",
    "amazing",
    "company",
    "called",
    "hugging",
    "face",
    "hugging",
    "face",
    "actually",
    "created",
    "amazing",
    "models",
    "solutions",
    "solving",
    "question",
    "answering",
    "session",
    "uh",
    "question",
    "answering",
    "basically",
    "question",
    "answering",
    "models",
    "sum",
    "summarization",
    "text",
    "classification",
    "translation",
    "right",
    "particular",
    "use",
    "cases",
    "see",
    "many",
    "different",
    "kind",
    "models",
    "like",
    "question",
    "answering",
    "right",
    "23",
    "380",
    "models",
    "summarization",
    "588",
    "models",
    "uh",
    "test",
    "classification",
    "much",
    "models",
    "right",
    "see",
    "many",
    "companies",
    "basically",
    "using",
    "google",
    "ai",
    "intel",
    "speen",
    "bridge",
    "microsoft",
    "grammarly",
    "right",
    "things",
    "basically",
    "using",
    "uh",
    "amazing",
    "application",
    "nlp",
    "applications",
    "short",
    "taking",
    "text",
    "data",
    "performing",
    "different",
    "kind",
    "task",
    "top",
    "one",
    "amazing",
    "thing",
    "really",
    "want",
    "show",
    "application",
    "respect",
    "alexa",
    "google",
    "assistant",
    "uh",
    "alexa",
    "home",
    "configured",
    "control",
    "ac",
    "lights",
    "respect",
    "google",
    "assistant",
    "simple",
    "application",
    "go",
    "open",
    "google",
    "assistant",
    "see",
    "hey",
    "google",
    "doctor",
    "appointments",
    "tomorrow",
    "sorry",
    "ca",
    "find",
    "anything",
    "calendar",
    "matches",
    "see",
    "uh",
    "already",
    "able",
    "retrieve",
    "calendar",
    "able",
    "see",
    "whether",
    "doctor",
    "appointment",
    "tomorrow",
    "way",
    "really",
    "really",
    "easying",
    "task",
    "respect",
    "critical",
    "things",
    "right",
    "activities",
    "use",
    "extensively",
    "nlp",
    "lot",
    "activities",
    "uh",
    "going",
    "learn",
    "specific",
    "course",
    "build",
    "kind",
    "models",
    "kind",
    "text",
    "actually",
    "happens",
    "get",
    "clex",
    "idea",
    "behind",
    "applications",
    "also",
    "works",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "natural",
    "language",
    "processing",
    "video",
    "going",
    "cover",
    "basic",
    "terminologies",
    "required",
    "nlp",
    "really",
    "need",
    "understand",
    "terminologies",
    "going",
    "repeat",
    "terminologies",
    "discussing",
    "topics",
    "topics",
    "going",
    "get",
    "covered",
    "video",
    "corpus",
    "documents",
    "vocabulary",
    "words",
    "really",
    "need",
    "know",
    "topics",
    "exactly",
    "basic",
    "examples",
    "usually",
    "whenever",
    "get",
    "paragraph",
    "paragraph",
    "usually",
    "called",
    "corpus",
    "okay",
    "respect",
    "documents",
    "whenever",
    "kind",
    "sentences",
    "really",
    "need",
    "understand",
    "sentences",
    "also",
    "usually",
    "called",
    "documents",
    "vocabulary",
    "vocabulary",
    "nothing",
    "unique",
    "words",
    "present",
    "paragraph",
    "basically",
    "called",
    "vocabulary",
    "usually",
    "dictionary",
    "right",
    "usually",
    "say",
    "vocabulary",
    "particular",
    "dictionary",
    "unique",
    "words",
    "count",
    "unique",
    "words",
    "unique",
    "words",
    "present",
    "dictionary",
    "called",
    "vocabulary",
    "respect",
    "words",
    "words",
    "present",
    "corpus",
    "basically",
    "define",
    "separately",
    "specific",
    "words",
    "basic",
    "terminologies",
    "really",
    "need",
    "understand",
    "said",
    "video",
    "going",
    "discuss",
    "something",
    "called",
    "tokenization",
    "tokenization",
    "important",
    "step",
    "whenever",
    "try",
    "solve",
    "kind",
    "use",
    "cases",
    "respect",
    "nlp",
    "exactly",
    "tokenization",
    "right",
    "let",
    "say",
    "paragraph",
    "write",
    "name",
    "crush",
    "name",
    "crush",
    "okay",
    "interest",
    "teaching",
    "interest",
    "teaching",
    "machine",
    "learning",
    "nlp",
    "deep",
    "learning",
    "dl",
    "let",
    "say",
    "specific",
    "text",
    "test",
    "consider",
    "basically",
    "paragraphs",
    "entirely",
    "corpus",
    "okay",
    "entire",
    "corpus",
    "available",
    "nothing",
    "paragraph",
    "uh",
    "words",
    "right",
    "probably",
    "combine",
    "words",
    "becomes",
    "paragraph",
    "tokenization",
    "process",
    "wherein",
    "take",
    "either",
    "paragraph",
    "sentences",
    "convert",
    "tokens",
    "right",
    "suppose",
    "let",
    "say",
    "want",
    "perform",
    "tokenization",
    "particular",
    "paragraph",
    "paragraph",
    "tokens",
    "usually",
    "generated",
    "basically",
    "called",
    "sentences",
    "documents",
    "let",
    "say",
    "applying",
    "tokenization",
    "respect",
    "let",
    "say",
    "try",
    "convert",
    "entire",
    "paragraph",
    "sentence",
    "may",
    "also",
    "add",
    "one",
    "line",
    "let",
    "say",
    "full",
    "stop",
    "writing",
    "one",
    "full",
    "stop",
    "okay",
    "also",
    "write",
    "also",
    "youtuber",
    "okay",
    "two",
    "sentences",
    "present",
    "paragraph",
    "respect",
    "particular",
    "tokenization",
    "perform",
    "tokenization",
    "paragraph",
    "basically",
    "create",
    "sentences",
    "first",
    "sentence",
    "particular",
    "case",
    "name",
    "crish",
    "okay",
    "interest",
    "interest",
    "teaching",
    "ml",
    "nlp",
    "nlp",
    "dl",
    "okay",
    "basically",
    "document",
    "one",
    "sentence",
    "one",
    "next",
    "sentence",
    "going",
    "probably",
    "write",
    "full",
    "stop",
    "right",
    "convert",
    "paragraph",
    "talk",
    "tokenization",
    "paragraph",
    "sentence",
    "looking",
    "kind",
    "characters",
    "like",
    "full",
    "stop",
    "exclamation",
    "show",
    "practically",
    "actually",
    "done",
    "help",
    "python",
    "programming",
    "language",
    "second",
    "sentence",
    "probably",
    "like",
    "also",
    "youtuber",
    "right",
    "also",
    "youtuber",
    "really",
    "want",
    "understand",
    "exactly",
    "tokenization",
    "tokenization",
    "simple",
    "process",
    "wherein",
    "converting",
    "sentences",
    "sorry",
    "converting",
    "paragraph",
    "sentences",
    "may",
    "also",
    "scenar",
    "ario",
    "let",
    "say",
    "sentences",
    "okay",
    "top",
    "also",
    "perform",
    "tokenization",
    "let",
    "say",
    "top",
    "performing",
    "tokenization",
    "tokenization",
    "technique",
    "probably",
    "applying",
    "convert",
    "sentences",
    "words",
    "right",
    "let",
    "say",
    "say",
    "basically",
    "getting",
    "converted",
    "words",
    "every",
    "word",
    "separate",
    "word",
    "separate",
    "word",
    "name",
    "separate",
    "word",
    "separate",
    "word",
    "kish",
    "separate",
    "word",
    "separate",
    "word",
    "separate",
    "word",
    "interest",
    "teaching",
    "separate",
    "words",
    "right",
    "process",
    "also",
    "called",
    "tokenization",
    "short",
    "words",
    "also",
    "token",
    "sentences",
    "also",
    "token",
    "right",
    "important",
    "understand",
    "required",
    "part",
    "text",
    "every",
    "word",
    "nlp",
    "needs",
    "converted",
    "vectors",
    "really",
    "need",
    "take",
    "word",
    "try",
    "kind",
    "lot",
    "steps",
    "like",
    "cleaning",
    "also",
    "showing",
    "video",
    "going",
    "trying",
    "understand",
    "tokenization",
    "hope",
    "got",
    "idea",
    "corpus",
    "got",
    "idea",
    "sentences",
    "let",
    "go",
    "ahead",
    "understand",
    "vocabulary",
    "also",
    "called",
    "unique",
    "words",
    "okay",
    "let",
    "say",
    "two",
    "sentences",
    "like",
    "eat",
    "apple",
    "juice",
    "sorry",
    "eat",
    "apple",
    "juice",
    "like",
    "drink",
    "apple",
    "juice",
    "okay",
    "continue",
    "write",
    "friend",
    "likes",
    "mango",
    "juice",
    "okay",
    "let",
    "say",
    "entire",
    "paragraph",
    "okay",
    "paragraph",
    "know",
    "many",
    "sentences",
    "two",
    "sentences",
    "full",
    "stop",
    "right",
    "divide",
    "tokens",
    "let",
    "say",
    "going",
    "perform",
    "something",
    "called",
    "tokenization",
    "okay",
    "get",
    "converted",
    "tokens",
    "right",
    "tokens",
    "present",
    "sentences",
    "right",
    "first",
    "sentence",
    "like",
    "drink",
    "apple",
    "juice",
    "first",
    "sentence",
    "second",
    "sentence",
    "nothing",
    "full",
    "stop",
    "friend",
    "likes",
    "mango",
    "juice",
    "mango",
    "juice",
    "see",
    "sentences",
    "obviously",
    "go",
    "count",
    "every",
    "words",
    "right",
    "let",
    "say",
    "many",
    "total",
    "number",
    "words",
    "probably",
    "go",
    "count",
    "1",
    "2",
    "3",
    "4",
    "5",
    "5",
    "6",
    "7",
    "8",
    "9",
    "10",
    "right",
    "count",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "10",
    "11",
    "total",
    "11",
    "words",
    "try",
    "count",
    "unique",
    "words",
    "many",
    "unique",
    "words",
    "make",
    "count",
    "one",
    "unique",
    "word",
    "like",
    "another",
    "unique",
    "word",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "see",
    "like",
    "likes",
    "two",
    "different",
    "word",
    "say",
    "9",
    "10",
    "juice",
    "getting",
    "repeated",
    "total",
    "number",
    "unique",
    "words",
    "basically",
    "10",
    "words",
    "right",
    "let",
    "say",
    "instead",
    "likes",
    "something",
    "called",
    "like",
    "point",
    "time",
    "number",
    "unique",
    "word",
    "number",
    "unique",
    "word",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "right",
    "count",
    "like",
    "juice",
    "right",
    "already",
    "likes",
    "counted",
    "separate",
    "word",
    "whenever",
    "get",
    "unique",
    "word",
    "10",
    "words",
    "basically",
    "means",
    "dictionary",
    "complete",
    "paragraph",
    "vocabulary",
    "possible",
    "words",
    "right",
    "10",
    "words",
    "right",
    "since",
    "converted",
    "like",
    "going",
    "make",
    "n",
    "words",
    "hope",
    "able",
    "understand",
    "basic",
    "differences",
    "corpus",
    "documents",
    "vocabulary",
    "words",
    "right",
    "entire",
    "thing",
    "super",
    "important",
    "learning",
    "tokenization",
    "somebody",
    "ask",
    "definition",
    "tokenization",
    "say",
    "tokenization",
    "process",
    "convert",
    "either",
    "paragraph",
    "sentences",
    "tokens",
    "convert",
    "paragraph",
    "tokens",
    "basically",
    "means",
    "converting",
    "parag",
    "sentences",
    "also",
    "convert",
    "paragraph",
    "words",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "nlp",
    "previous",
    "video",
    "already",
    "seen",
    "tokenization",
    "understood",
    "basic",
    "terminologies",
    "like",
    "corpus",
    "paragraph",
    "understood",
    "vocabulary",
    "understood",
    "words",
    "let",
    "go",
    "ahead",
    "practical",
    "things",
    "know",
    "much",
    "learning",
    "respect",
    "theory",
    "first",
    "go",
    "ahead",
    "open",
    "python",
    "3",
    "notebook",
    "file",
    "uh",
    "first",
    "uh",
    "right",
    "going",
    "going",
    "use",
    "libraries",
    "like",
    "nltk",
    "nltk",
    "one",
    "amazing",
    "library",
    "let",
    "go",
    "let",
    "show",
    "nltk",
    "leading",
    "platform",
    "building",
    "python",
    "programs",
    "work",
    "human",
    "language",
    "data",
    "really",
    "want",
    "work",
    "nlp",
    "things",
    "like",
    "tokenization",
    "creating",
    "converting",
    "sentence",
    "vectors",
    "easily",
    "done",
    "help",
    "nltk",
    "libraries",
    "also",
    "one",
    "library",
    "basically",
    "say",
    "spacey",
    "probably",
    "search",
    "spacey",
    "nlp",
    "able",
    "see",
    "completely",
    "source",
    "libraries",
    "also",
    "use",
    "spacy",
    "also",
    "showing",
    "help",
    "spy",
    "perform",
    "different",
    "different",
    "things",
    "right",
    "spacey",
    "also",
    "nltk",
    "also",
    "one",
    "important",
    "assignment",
    "really",
    "want",
    "give",
    "try",
    "find",
    "differences",
    "nltk",
    "spacey",
    "uh",
    "let",
    "know",
    "probably",
    "comment",
    "section",
    "particular",
    "video",
    "upcoming",
    "videos",
    "okay",
    "one",
    "task",
    "want",
    "really",
    "give",
    "try",
    "try",
    "understand",
    "difference",
    "two",
    "open",
    "source",
    "libraries",
    "begin",
    "uh",
    "since",
    "going",
    "initially",
    "start",
    "nltk",
    "actually",
    "going",
    "first",
    "going",
    "install",
    "nltk",
    "directly",
    "install",
    "also",
    "open",
    "command",
    "prompt",
    "directly",
    "install",
    "nltk",
    "right",
    "write",
    "pip",
    "install",
    "nltk",
    "automatically",
    "installation",
    "done",
    "actually",
    "going",
    "going",
    "basically",
    "install",
    "going",
    "show",
    "tokenization",
    "example",
    "let",
    "write",
    "tokenization",
    "uh",
    "example",
    "okay",
    "perfect",
    "tokenization",
    "example",
    "see",
    "uh",
    "collecting",
    "nltk",
    "requirement",
    "already",
    "done",
    "installation",
    "done",
    "new",
    "rel",
    "release",
    "pip",
    "available",
    "want",
    "update",
    "right",
    "pip",
    "almost",
    "cover",
    "everything",
    "right",
    "right",
    "uh",
    "perfect",
    "actually",
    "going",
    "going",
    "show",
    "perform",
    "tokenization",
    "like",
    "let",
    "say",
    "paragraph",
    "convert",
    "sentences",
    "convert",
    "words",
    "things",
    "discussing",
    "multiple",
    "ways",
    "tokenization",
    "also",
    "showing",
    "okay",
    "let",
    "go",
    "ahead",
    "let",
    "make",
    "cells",
    "directly",
    "go",
    "ahead",
    "execute",
    "one",
    "thing",
    "yeah",
    "let",
    "go",
    "ahead",
    "let",
    "start",
    "first",
    "try",
    "define",
    "corpus",
    "okay",
    "corpus",
    "basically",
    "means",
    "paragraph",
    "let",
    "say",
    "uh",
    "really",
    "want",
    "create",
    "comments",
    "basically",
    "use",
    "triple",
    "cotes",
    "write",
    "hello",
    "welcome",
    "comma",
    "okay",
    "giving",
    "sentences",
    "able",
    "understand",
    "okay",
    "hello",
    "welcome",
    "fresh",
    "nx",
    "tutorials",
    "let",
    "say",
    "write",
    "neurons",
    "right",
    "write",
    "something",
    "like",
    "kushak",
    "nlp",
    "kush",
    "n",
    "nlps",
    "tutorials",
    "okay",
    "sentence",
    "okay",
    "continue",
    "second",
    "line",
    "write",
    "please",
    "watch",
    "entire",
    "course",
    "writing",
    "exclamation",
    "using",
    "different",
    "different",
    "characters",
    "become",
    "expert",
    "nlp",
    "okay",
    "actually",
    "done",
    "basically",
    "defined",
    "simple",
    "corpus",
    "uh",
    "like",
    "paragraph",
    "around",
    "two",
    "sentences",
    "going",
    "use",
    "particular",
    "corpus",
    "let",
    "go",
    "ahead",
    "see",
    "corpus",
    "see",
    "corpus",
    "see",
    "even",
    "though",
    "printing",
    "right",
    "also",
    "print",
    "see",
    "sln",
    "go",
    "slash",
    "indicates",
    "new",
    "line",
    "okay",
    "probably",
    "printing",
    "corpus",
    "able",
    "see",
    "text",
    "basically",
    "visible",
    "respect",
    "tokenization",
    "first",
    "step",
    "actually",
    "going",
    "going",
    "convert",
    "sentence",
    "paragraphs",
    "okay",
    "going",
    "basically",
    "convert",
    "sentence",
    "paragraphs",
    "convert",
    "help",
    "nltk",
    "definitely",
    "possible",
    "write",
    "nltk",
    "dot",
    "tokenize",
    "okay",
    "going",
    "import",
    "okay",
    "nltk",
    "uh",
    "library",
    "called",
    "toonize",
    "import",
    "sentore",
    "tokenize",
    "sentore",
    "tokenize",
    "tries",
    "convert",
    "paragraph",
    "sentences",
    "function",
    "basically",
    "going",
    "use",
    "functionality",
    "going",
    "use",
    "present",
    "inside",
    "nltk",
    "tokenize",
    "okay",
    "say",
    "kind",
    "package",
    "inside",
    "initialize",
    "package",
    "order",
    "convert",
    "sentence",
    "uh",
    "sorry",
    "paragraph",
    "sentences",
    "going",
    "write",
    "sent",
    "tokenize",
    "let",
    "go",
    "ahead",
    "give",
    "corpus",
    "give",
    "corpus",
    "see",
    "giving",
    "us",
    "list",
    "list",
    "sentence",
    "see",
    "sentence",
    "hello",
    "welcome",
    "krish",
    "nik",
    "nlp",
    "tutorial",
    "see",
    "first",
    "sentence",
    "second",
    "sentence",
    "exclamation",
    "also",
    "divided",
    "getting",
    "three",
    "right",
    "see",
    "hello",
    "welcome",
    "krishna",
    "nlp",
    "tutorials",
    "full",
    "stop",
    "soon",
    "finds",
    "full",
    "stop",
    "going",
    "make",
    "next",
    "sentence",
    "wherever",
    "slash",
    "present",
    "using",
    "wherever",
    "exclamation",
    "also",
    "present",
    "basically",
    "making",
    "sure",
    "new",
    "sentence",
    "getting",
    "created",
    "respect",
    "send",
    "tokenize",
    "actually",
    "really",
    "want",
    "find",
    "definition",
    "see",
    "uh",
    "also",
    "provide",
    "different",
    "different",
    "languages",
    "languages",
    "supports",
    "go",
    "ahead",
    "look",
    "onto",
    "uh",
    "find",
    "much",
    "documentation",
    "go",
    "search",
    "nltk",
    "sent",
    "tokenize",
    "right",
    "sent",
    "tokenize",
    "able",
    "find",
    "doc",
    "documentation",
    "page",
    "directly",
    "refer",
    "right",
    "different",
    "different",
    "tokenization",
    "see",
    "send",
    "tokenize",
    "word",
    "tokenize",
    "basically",
    "going",
    "discuss",
    "right",
    "going",
    "focus",
    "okay",
    "short",
    "returns",
    "sent",
    "tokenized",
    "copy",
    "text",
    "using",
    "nalt",
    "case",
    "recommended",
    "sentence",
    "tokenizer",
    "current",
    "class",
    "uses",
    "called",
    "punk",
    "send",
    "tokenizer",
    "along",
    "full",
    "stop",
    "making",
    "sure",
    "whever",
    "exclamation",
    "basically",
    "coming",
    "going",
    "make",
    "uh",
    "another",
    "sentence",
    "perfect",
    "uh",
    "able",
    "get",
    "right",
    "go",
    "next",
    "tokenization",
    "also",
    "want",
    "probably",
    "save",
    "list",
    "sentences",
    "let",
    "say",
    "sentence",
    "also",
    "called",
    "documents",
    "already",
    "discussed",
    "previous",
    "class",
    "uh",
    "previous",
    "session",
    "right",
    "probably",
    "go",
    "see",
    "documents",
    "basically",
    "list",
    "also",
    "check",
    "help",
    "help",
    "type",
    "type",
    "documents",
    "right",
    "probably",
    "want",
    "iterate",
    "let",
    "say",
    "sent",
    "documents",
    "sentences",
    "sentence",
    "documents",
    "also",
    "print",
    "sentence",
    "parall",
    "right",
    "define",
    "sentence",
    "first",
    "sentence",
    "second",
    "sentence",
    "third",
    "sentence",
    "perfect",
    "able",
    "respect",
    "sentences",
    "let",
    "go",
    "ahead",
    "respect",
    "word",
    "tokenize",
    "next",
    "tokenization",
    "technique",
    "actually",
    "going",
    "next",
    "tokenization",
    "convert",
    "paragraph",
    "convert",
    "paragraph",
    "words",
    "also",
    "convert",
    "sentence",
    "words",
    "sentence",
    "words",
    "okay",
    "perfect",
    "actually",
    "going",
    "first",
    "let",
    "go",
    "ahead",
    "see",
    "respect",
    "paragraph",
    "uh",
    "already",
    "uh",
    "know",
    "uh",
    "uh",
    "respect",
    "converting",
    "paragraph",
    "words",
    "using",
    "another",
    "library",
    "write",
    "nltk",
    "dot",
    "okay",
    "tokenize",
    "nltk",
    "tokenize",
    "okay",
    "spelling",
    "mistake",
    "tokenize",
    "going",
    "import",
    "word",
    "let",
    "write",
    "word",
    "tokenize",
    "okay",
    "basically",
    "use",
    "word",
    "tokenize",
    "let",
    "execute",
    "let",
    "go",
    "ahead",
    "write",
    "word",
    "tokenize",
    "directly",
    "give",
    "corpus",
    "see",
    "every",
    "word",
    "divided",
    "hello",
    "world",
    "hello",
    "welcome",
    "see",
    "characters",
    "like",
    "comma",
    "full",
    "stop",
    "treated",
    "separate",
    "character",
    "together",
    "separate",
    "words",
    "together",
    "right",
    "definitely",
    "clearly",
    "see",
    "every",
    "word",
    "splitted",
    "respect",
    "one",
    "word",
    "right",
    "probably",
    "go",
    "see",
    "hello",
    "welcome",
    "krishn",
    "kind",
    "word",
    "splitted",
    "considered",
    "single",
    "word",
    "consider",
    "respect",
    "full",
    "stop",
    "respect",
    "exclamation",
    "respect",
    "comma",
    "considered",
    "separate",
    "word",
    "also",
    "find",
    "respect",
    "right",
    "simple",
    "respect",
    "uh",
    "converting",
    "paragraph",
    "words",
    "every",
    "word",
    "different",
    "importance",
    "really",
    "need",
    "perform",
    "top",
    "right",
    "initially",
    "get",
    "know",
    "important",
    "words",
    "take",
    "clean",
    "know",
    "uh",
    "reason",
    "specifically",
    "focus",
    "every",
    "word",
    "okay",
    "word",
    "tokenize",
    "uh",
    "respect",
    "sentences",
    "also",
    "know",
    "basically",
    "go",
    "write",
    "paste",
    "sentences",
    "printing",
    "sentences",
    "right",
    "basically",
    "probably",
    "get",
    "sentence",
    "get",
    "sentence",
    "together",
    "directly",
    "apply",
    "word",
    "tokenize",
    "right",
    "basically",
    "write",
    "word",
    "tokenize",
    "sentences",
    "able",
    "print",
    "everything",
    "right",
    "hello",
    "welcome",
    "uh",
    "please",
    "watch",
    "entire",
    "course",
    "right",
    "perfect",
    "seen",
    "basically",
    "convert",
    "sentence",
    "words",
    "right",
    "also",
    "one",
    "thing",
    "uh",
    "use",
    "another",
    "one",
    "library",
    "let",
    "talk",
    "one",
    "library",
    "probably",
    "seen",
    "going",
    "write",
    "word",
    "punct",
    "toonize",
    "word",
    "punct",
    "tokenize",
    "basically",
    "going",
    "try",
    "apply",
    "try",
    "initialize",
    "word",
    "punk",
    "tokenize",
    "try",
    "provide",
    "corpus",
    "let",
    "say",
    "try",
    "find",
    "difference",
    "one",
    "difference",
    "clearly",
    "seen",
    "apostrophe",
    "also",
    "got",
    "splitted",
    "getting",
    "splitted",
    "right",
    "see",
    "apostrophe",
    "single",
    "word",
    "see",
    "splitted",
    "reason",
    "using",
    "punctuations",
    "um",
    "punctuation",
    "basically",
    "uh",
    "making",
    "sure",
    "punctuation",
    "also",
    "treated",
    "separate",
    "word",
    "perfect",
    "good",
    "also",
    "one",
    "technique",
    "basically",
    "called",
    "tree",
    "bank",
    "word",
    "tokenizer",
    "also",
    "try",
    "tell",
    "difference",
    "exactly",
    "respect",
    "tree",
    "bank",
    "word",
    "tokenizer",
    "uh",
    "try",
    "execute",
    "try",
    "find",
    "differences",
    "respect",
    "let",
    "see",
    "much",
    "able",
    "write",
    "nltk",
    "dot",
    "toize",
    "import",
    "tre",
    "bank",
    "tokenizer",
    "initialize",
    "tree",
    "bank",
    "tokenizer",
    "okay",
    "let",
    "say",
    "initializing",
    "tokenizer",
    "something",
    "like",
    "something",
    "one",
    "variable",
    "basically",
    "use",
    "function",
    "called",
    "tokenizer",
    "tokenize",
    "give",
    "corpus",
    "able",
    "see",
    "actually",
    "able",
    "get",
    "okay",
    "see",
    "difference",
    "respect",
    "definitely",
    "able",
    "find",
    "difference",
    "comparing",
    "specific",
    "thing",
    "okay",
    "let",
    "know",
    "difference",
    "able",
    "see",
    "know",
    "minute",
    "difference",
    "let",
    "tell",
    "answer",
    "pause",
    "time",
    "check",
    "let",
    "tell",
    "answer",
    "help",
    "tree",
    "bank",
    "word",
    "tokenizer",
    "see",
    "full",
    "stop",
    "treated",
    "separate",
    "word",
    "included",
    "previous",
    "word",
    "see",
    "full",
    "stop",
    "separate",
    "word",
    "also",
    "see",
    "full",
    "stop",
    "separate",
    "word",
    "right",
    "respect",
    "last",
    "word",
    "right",
    "full",
    "stop",
    "separate",
    "know",
    "see",
    "right",
    "probably",
    "see",
    "sentence",
    "right",
    "new",
    "line",
    "right",
    "sentence",
    "getting",
    "closed",
    "last",
    "full",
    "stop",
    "considering",
    "separate",
    "word",
    "respect",
    "particular",
    "full",
    "stop",
    "considering",
    "part",
    "considered",
    "separate",
    "word",
    "difference",
    "respect",
    "handed",
    "use",
    "cases",
    "generic",
    "way",
    "basically",
    "time",
    "use",
    "word",
    "tokenize",
    "send",
    "tokenize",
    "right",
    "yes",
    "uh",
    "respect",
    "tokenization",
    "example",
    "hope",
    "like",
    "particular",
    "video",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "natural",
    "language",
    "processing",
    "going",
    "move",
    "towards",
    "techniques",
    "respect",
    "text",
    "already",
    "previous",
    "video",
    "seen",
    "tokenization",
    "seen",
    "convert",
    "paragraph",
    "sentences",
    "probably",
    "paragraph",
    "words",
    "converting",
    "sentences",
    "words",
    "right",
    "short",
    "seen",
    "actually",
    "tokenization",
    "help",
    "nltk",
    "video",
    "going",
    "focus",
    "something",
    "called",
    "stemming",
    "important",
    "process",
    "altogether",
    "uh",
    "exactly",
    "stemming",
    "also",
    "provide",
    "definition",
    "also",
    "see",
    "lot",
    "examples",
    "respect",
    "also",
    "try",
    "see",
    "different",
    "types",
    "steming",
    "opened",
    "file",
    "see",
    "regarding",
    "stemming",
    "also",
    "given",
    "definition",
    "let",
    "understand",
    "exactly",
    "good",
    "examples",
    "first",
    "see",
    "definition",
    "shows",
    "stemming",
    "definition",
    "taken",
    "wikipedia",
    "stemming",
    "process",
    "reducing",
    "word",
    "word",
    "stem",
    "okay",
    "super",
    "important",
    "guys",
    "word",
    "stem",
    "affixes",
    "suffixes",
    "prefixes",
    "roots",
    "word",
    "known",
    "lma",
    "okay",
    "stemming",
    "important",
    "natural",
    "language",
    "understanding",
    "natural",
    "language",
    "processing",
    "exactly",
    "tell",
    "examples",
    "let",
    "say",
    "want",
    "create",
    "cell",
    "okay",
    "lot",
    "text",
    "right",
    "let",
    "say",
    "trying",
    "solve",
    "sentiment",
    "trying",
    "solve",
    "classification",
    "problem",
    "classification",
    "problem",
    "classification",
    "problem",
    "simple",
    "basically",
    "need",
    "find",
    "whether",
    "comments",
    "product",
    "positive",
    "review",
    "negative",
    "review",
    "right",
    "solving",
    "kind",
    "problem",
    "statement",
    "data",
    "set",
    "comments",
    "reviews",
    "say",
    "reviews",
    "based",
    "particular",
    "reviews",
    "reviews",
    "obviously",
    "kind",
    "text",
    "data",
    "need",
    "basically",
    "create",
    "model",
    "basically",
    "classify",
    "whether",
    "positive",
    "review",
    "negative",
    "review",
    "simple",
    "usually",
    "reviews",
    "let",
    "say",
    "words",
    "like",
    "eating",
    "okay",
    "eat",
    "right",
    "like",
    "eaten",
    "right",
    "different",
    "kind",
    "words",
    "end",
    "day",
    "actually",
    "represents",
    "thing",
    "regarding",
    "eating",
    "right",
    "basically",
    "eat",
    "eat",
    "root",
    "word",
    "also",
    "say",
    "stem",
    "word",
    "word",
    "stem",
    "words",
    "right",
    "itat",
    "much",
    "common",
    "variety",
    "words",
    "problem",
    "statement",
    "impact",
    "much",
    "respect",
    "finding",
    "output",
    "like",
    "positive",
    "negative",
    "review",
    "try",
    "understand",
    "actually",
    "trying",
    "say",
    "may",
    "different",
    "kind",
    "words",
    "like",
    "eating",
    "eat",
    "eaten",
    "may",
    "also",
    "words",
    "like",
    "also",
    "make",
    "combination",
    "like",
    "going",
    "gone",
    "right",
    "gone",
    "goes",
    "right",
    "end",
    "day",
    "basically",
    "talking",
    "go",
    "right",
    "go",
    "word",
    "stem",
    "words",
    "present",
    "right",
    "necessary",
    "need",
    "similar",
    "kind",
    "words",
    "increases",
    "number",
    "input",
    "features",
    "short",
    "every",
    "word",
    "represents",
    "vector",
    "see",
    "go",
    "ahead",
    "know",
    "text",
    "try",
    "convert",
    "text",
    "vectors",
    "similar",
    "kind",
    "words",
    "instead",
    "similar",
    "kind",
    "words",
    "one",
    "word",
    "like",
    "go",
    "right",
    "try",
    "try",
    "solve",
    "problem",
    "respect",
    "stemming",
    "actually",
    "helping",
    "us",
    "thing",
    "finding",
    "word",
    "stem",
    "actually",
    "done",
    "help",
    "stemming",
    "also",
    "concept",
    "called",
    "limitation",
    "try",
    "understand",
    "difference",
    "go",
    "ahead",
    "first",
    "let",
    "go",
    "ahead",
    "see",
    "help",
    "nltk",
    "perform",
    "stemming",
    "okay",
    "actually",
    "going",
    "going",
    "take",
    "example",
    "examples",
    "let",
    "say",
    "words",
    "okay",
    "going",
    "remove",
    "let",
    "say",
    "words",
    "right",
    "words",
    "like",
    "eating",
    "eats",
    "eaten",
    "writing",
    "writes",
    "programming",
    "programs",
    "history",
    "finally",
    "finalized",
    "okay",
    "going",
    "execute",
    "let",
    "make",
    "cells",
    "okay",
    "going",
    "delete",
    "cell",
    "require",
    "okay",
    "create",
    "cell",
    "okay",
    "let",
    "press",
    "excape",
    "let",
    "us",
    "go",
    "ahead",
    "right",
    "see",
    "particular",
    "words",
    "let",
    "see",
    "find",
    "word",
    "stem",
    "particular",
    "words",
    "help",
    "stemming",
    "first",
    "stemming",
    "technique",
    "probably",
    "going",
    "use",
    "something",
    "called",
    "porter",
    "stemmer",
    "okay",
    "porter",
    "stemmer",
    "different",
    "different",
    "types",
    "stemming",
    "techniques",
    "probably",
    "showing",
    "able",
    "understand",
    "things",
    "able",
    "give",
    "us",
    "okay",
    "order",
    "apply",
    "pter",
    "stemmer",
    "much",
    "simple",
    "nltk",
    "already",
    "functionalities",
    "basically",
    "present",
    "write",
    "nltk",
    "dot",
    "stem",
    "import",
    "pter",
    "stemmer",
    "okay",
    "initialize",
    "see",
    "going",
    "use",
    "pter",
    "stemmer",
    "kind",
    "class",
    "initialize",
    "initialize",
    "object",
    "let",
    "create",
    "object",
    "basically",
    "stemming",
    "next",
    "step",
    "actually",
    "going",
    "every",
    "word",
    "going",
    "apply",
    "stemming",
    "process",
    "okay",
    "simple",
    "iterate",
    "say",
    "word",
    "word",
    "words",
    "right",
    "basically",
    "going",
    "write",
    "print",
    "uh",
    "let",
    "write",
    "word",
    "plus",
    "give",
    "type",
    "marking",
    "word",
    "stem",
    "part",
    "nothing",
    "using",
    "object",
    "stemming",
    "dot",
    "stem",
    "functionality",
    "function",
    "called",
    "stem",
    "actually",
    "whenever",
    "push",
    "words",
    "inside",
    "stemming",
    "thing",
    "basically",
    "means",
    "eating",
    "probably",
    "may",
    "give",
    "eat",
    "eats",
    "may",
    "give",
    "eat",
    "right",
    "something",
    "like",
    "going",
    "give",
    "word",
    "execute",
    "see",
    "perfect",
    "eating",
    "coming",
    "eat",
    "eats",
    "comes",
    "eat",
    "eaten",
    "coming",
    "eaten",
    "writing",
    "coming",
    "right",
    "good",
    "rights",
    "comes",
    "right",
    "programming",
    "comes",
    "program",
    "programs",
    "nothing",
    "program",
    "history",
    "see",
    "history",
    "becoming",
    "h",
    "r",
    "major",
    "issue",
    "right",
    "also",
    "talk",
    "disadvantages",
    "finally",
    "becomes",
    "final",
    "finalized",
    "becomes",
    "final",
    "right",
    "let",
    "say",
    "uh",
    "looks",
    "good",
    "right",
    "probably",
    "eaten",
    "see",
    "nothing",
    "happened",
    "given",
    "word",
    "see",
    "words",
    "like",
    "history",
    "actually",
    "getting",
    "history",
    "ri",
    "entire",
    "meaning",
    "particular",
    "word",
    "actually",
    "changed",
    "major",
    "disadvantage",
    "stemming",
    "stemming",
    "basically",
    "applied",
    "know",
    "words",
    "know",
    "may",
    "get",
    "correct",
    "exact",
    "meaning",
    "form",
    "specific",
    "word",
    "may",
    "change",
    "major",
    "major",
    "disadvantage",
    "respect",
    "stemming",
    "let",
    "show",
    "examples",
    "suppose",
    "say",
    "stemming",
    "stem",
    "going",
    "apply",
    "particular",
    "stem",
    "word",
    "word",
    "called",
    "congratulations",
    "okay",
    "execute",
    "see",
    "word",
    "meaning",
    "word",
    "basically",
    "changing",
    "told",
    "like",
    "something",
    "like",
    "congratulate",
    "right",
    "see",
    "congratul",
    "basically",
    "changing",
    "form",
    "word",
    "word",
    "kind",
    "meaning",
    "right",
    "one",
    "major",
    "major",
    "disadvantage",
    "stemming",
    "okay",
    "similarly",
    "try",
    "show",
    "something",
    "like",
    "stemming",
    "stem",
    "probably",
    "write",
    "something",
    "like",
    "sitting",
    "able",
    "see",
    "let",
    "write",
    "sit",
    "see",
    "stay",
    "sitting",
    "giving",
    "good",
    "word",
    "basically",
    "stemming",
    "works",
    "uh",
    "good",
    "number",
    "words",
    "words",
    "give",
    "us",
    "good",
    "answer",
    "right",
    "major",
    "disadvantage",
    "stemming",
    "get",
    "fixed",
    "help",
    "lemmatization",
    "whenever",
    "kind",
    "problem",
    "statement",
    "like",
    "uh",
    "classification",
    "problem",
    "review",
    "classification",
    "really",
    "want",
    "see",
    "whether",
    "whether",
    "email",
    "spam",
    "ham",
    "right",
    "whether",
    "spam",
    "spam",
    "definitely",
    "go",
    "ahead",
    "using",
    "stemming",
    "know",
    "uh",
    "words",
    "come",
    "right",
    "form",
    "yes",
    "instead",
    "using",
    "pter",
    "stemmer",
    "different",
    "kind",
    "stem",
    "stemmer",
    "techniques",
    "definitely",
    "use",
    "improve",
    "okay",
    "let",
    "go",
    "respect",
    "second",
    "one",
    "respect",
    "second",
    "one",
    "going",
    "cell",
    "require",
    "second",
    "cell",
    "called",
    "regus",
    "stemmer",
    "class",
    "regular",
    "expression",
    "stemmer",
    "class",
    "nothing",
    "class",
    "help",
    "easily",
    "implement",
    "regular",
    "expression",
    "stemmer",
    "algorithm",
    "provide",
    "regular",
    "expression",
    "able",
    "apply",
    "stemming",
    "purpose",
    "okay",
    "basically",
    "takes",
    "single",
    "regular",
    "expression",
    "remove",
    "prefix",
    "suffix",
    "matches",
    "expression",
    "perfect",
    "create",
    "cell",
    "make",
    "cells",
    "try",
    "show",
    "example",
    "first",
    "need",
    "need",
    "initialize",
    "write",
    "nltk",
    "stem",
    "import",
    "name",
    "regus",
    "right",
    "going",
    "write",
    "reg",
    "stem",
    "perfect",
    "basically",
    "need",
    "initialize",
    "say",
    "regor",
    "stemma",
    "equal",
    "reg",
    "right",
    "initialized",
    "perfect",
    "uh",
    "giving",
    "us",
    "uh",
    "error",
    "saying",
    "regular",
    "expression",
    "required",
    "super",
    "important",
    "let",
    "press",
    "go",
    "ahead",
    "press",
    "shift",
    "tab",
    "see",
    "first",
    "parameter",
    "goes",
    "something",
    "called",
    "regular",
    "expression",
    "okay",
    "minimum",
    "value",
    "try",
    "understand",
    "exactly",
    "see",
    "stemmer",
    "uses",
    "regular",
    "expression",
    "identify",
    "morphology",
    "affixes",
    "right",
    "understand",
    "morphological",
    "affixes",
    "try",
    "show",
    "example",
    "know",
    "meaning",
    "substring",
    "match",
    "regular",
    "expression",
    "match",
    "removed",
    "morphological",
    "affixes",
    "basically",
    "short",
    "regular",
    "expression",
    "match",
    "whatever",
    "words",
    "basically",
    "giving",
    "matches",
    "get",
    "removed",
    "okay",
    "let",
    "take",
    "example",
    "okay",
    "uh",
    "uh",
    "see",
    "param",
    "minimum",
    "length",
    "string",
    "stem",
    "okay",
    "minimum",
    "length",
    "somewhere",
    "around",
    "four",
    "able",
    "apply",
    "going",
    "apply",
    "thing",
    "going",
    "paste",
    "okay",
    "reg",
    "stemmer",
    "actually",
    "going",
    "going",
    "write",
    "reg",
    "stemmer",
    "understand",
    "actually",
    "given",
    "given",
    "ing",
    "dollar",
    "okay",
    "given",
    "dollar",
    "e",
    "dollar",
    "able",
    "dollar",
    "okay",
    "given",
    "regular",
    "expression",
    "make",
    "completely",
    "sense",
    "implementing",
    "see",
    "pause",
    "video",
    "let",
    "know",
    "output",
    "feel",
    "getting",
    "eating",
    "eating",
    "see",
    "ing",
    "regular",
    "expression",
    "dollar",
    "also",
    "okay",
    "super",
    "important",
    "dollar",
    "also",
    "try",
    "execute",
    "able",
    "get",
    "something",
    "called",
    "eight",
    "okay",
    "short",
    "happening",
    "basically",
    "saying",
    "wherever",
    "last",
    "word",
    "ing",
    "e",
    "able",
    "try",
    "remove",
    "next",
    "example",
    "really",
    "want",
    "give",
    "let",
    "say",
    "want",
    "particular",
    "word",
    "ining",
    "okay",
    "see",
    "ing",
    "eating",
    "right",
    "think",
    "output",
    "whether",
    "eat",
    "whether",
    "something",
    "else",
    "okay",
    "try",
    "execute",
    "seeing",
    "in8",
    "regular",
    "expression",
    "says",
    "last",
    "last",
    "dollar",
    "probably",
    "remove",
    "dollar",
    "execute",
    "see",
    "everything",
    "get",
    "removed",
    "okay",
    "probably",
    "want",
    "starting",
    "use",
    "see",
    "ing",
    "eating",
    "work",
    "okay",
    "basically",
    "something",
    "like",
    "uh",
    "basically",
    "check",
    "actually",
    "remove",
    "perfect",
    "see",
    "wherever",
    "ing",
    "getting",
    "removed",
    "perfect",
    "good",
    "done",
    "okay",
    "eat",
    "let",
    "say",
    "uh",
    "also",
    "want",
    "try",
    "something",
    "else",
    "uh",
    "definitely",
    "try",
    "different",
    "different",
    "word",
    "use",
    "able",
    "e",
    "whatever",
    "regular",
    "expression",
    "basically",
    "write",
    "go",
    "ahead",
    "write",
    "check",
    "okay",
    "respect",
    "regular",
    "expression",
    "stemmer",
    "class",
    "uh",
    "next",
    "one",
    "basically",
    "going",
    "discuss",
    "uh",
    "uh",
    "let",
    "see",
    "portter",
    "stemmer",
    "written",
    "thing",
    "want",
    "write",
    "thing",
    "go",
    "ahead",
    "something",
    "called",
    "snowball",
    "stemmer",
    "also",
    "amazing",
    "technique",
    "better",
    "technique",
    "compared",
    "okay",
    "snowball",
    "stemmer",
    "think",
    "given",
    "definition",
    "somewhere",
    "okay",
    "snowball",
    "stemmer",
    "stemming",
    "technique",
    "snowball",
    "stemmer",
    "performs",
    "better",
    "porter",
    "stemmer",
    "okay",
    "reason",
    "snowball",
    "stemmer",
    "actually",
    "come",
    "initially",
    "came",
    "pter",
    "stemmer",
    "saw",
    "lot",
    "things",
    "lot",
    "words",
    "getting",
    "messed",
    "know",
    "reason",
    "use",
    "snowball",
    "stemmer",
    "gives",
    "better",
    "accuracy",
    "compared",
    "uh",
    "p",
    "stl",
    "say",
    "accuracy",
    "better",
    "form",
    "word",
    "okay",
    "using",
    "snowball",
    "stl",
    "write",
    "escalon",
    "stem",
    "import",
    "snowball",
    "stemmer",
    "see",
    "write",
    "snowball",
    "scalon",
    "oh",
    "scalon",
    "sorry",
    "ltk",
    "basically",
    "present",
    "nltk",
    "r",
    "nltk",
    "stem",
    "import",
    "snowball",
    "stem",
    "going",
    "import",
    "going",
    "initialize",
    "respect",
    "snowball",
    "stemmer",
    "let",
    "see",
    "parameters",
    "first",
    "snowball",
    "stemmer",
    "also",
    "provided",
    "different",
    "different",
    "languages",
    "like",
    "uh",
    "arabic",
    "okay",
    "danish",
    "uh",
    "english",
    "finnish",
    "french",
    "german",
    "hungarian",
    "italian",
    "use",
    "basically",
    "words",
    "okay",
    "right",
    "going",
    "use",
    "english",
    "going",
    "quotes",
    "say",
    "english",
    "okay",
    "uh",
    "use",
    "finally",
    "basically",
    "use",
    "snowball",
    "stemmer",
    "create",
    "variable",
    "okay",
    "execute",
    "perfect",
    "next",
    "thing",
    "use",
    "another",
    "condition",
    "saying",
    "word",
    "words",
    "okay",
    "uh",
    "write",
    "words",
    "words",
    "uh",
    "write",
    "snowball",
    "stemmer",
    "stem",
    "word",
    "word",
    "okay",
    "print",
    "better",
    "way",
    "able",
    "understand",
    "print",
    "word",
    "plus",
    "like",
    "something",
    "like",
    "formatting",
    "purpose",
    "writing",
    "look",
    "good",
    "better",
    "going",
    "write",
    "word",
    "right",
    "thing",
    "things",
    "see",
    "getting",
    "eating",
    "eat",
    "eats",
    "eat",
    "eaten",
    "eat",
    "right",
    "right",
    "fine",
    "right",
    "history",
    "also",
    "able",
    "give",
    "correct",
    "form",
    "right",
    "may",
    "thinking",
    "chrish",
    "difference",
    "right",
    "let",
    "see",
    "respect",
    "porter",
    "stemmer",
    "pter",
    "stemmer",
    "give",
    "bad",
    "results",
    "bad",
    "form",
    "word",
    "snowball",
    "give",
    "us",
    "better",
    "form",
    "word",
    "going",
    "uh",
    "execute",
    "line",
    "code",
    "see",
    "applying",
    "stemming",
    "stemming",
    "bter",
    "stemmer",
    "right",
    "apply",
    "stemming",
    "stem",
    "fairly",
    "sportingly",
    "output",
    "getting",
    "getting",
    "fairly",
    "sportingly",
    "right",
    "li",
    "li",
    "try",
    "use",
    "word",
    "respect",
    "snowball",
    "right",
    "able",
    "see",
    "let",
    "write",
    "snowball",
    "stemmer",
    "able",
    "see",
    "getting",
    "amazing",
    "answers",
    "snow",
    "balls",
    "stemmer",
    "okay",
    "okay",
    "let",
    "remove",
    "think",
    "problem",
    "respect",
    "work",
    "okay",
    "snowball",
    "stemmer",
    "comma",
    "copy",
    "apply",
    "word",
    "something",
    "called",
    "sportingly",
    "okay",
    "probably",
    "go",
    "see",
    "getting",
    "good",
    "output",
    "like",
    "fair",
    "sport",
    "altogether",
    "seeing",
    "snowball",
    "stemmer",
    "applied",
    "various",
    "words",
    "performs",
    "better",
    "stemming",
    "right",
    "see",
    "pter",
    "stemmer",
    "obviously",
    "technique",
    "able",
    "find",
    "word",
    "stem",
    "many",
    "words",
    "able",
    "give",
    "good",
    "answers",
    "examples",
    "like",
    "fairly",
    "sportingly",
    "order",
    "overcome",
    "disadvantage",
    "snowball",
    "stammer",
    "basically",
    "used",
    "guys",
    "understand",
    "techniques",
    "actually",
    "help",
    "find",
    "word",
    "step",
    "specifically",
    "getting",
    "used",
    "text",
    "really",
    "need",
    "clean",
    "data",
    "need",
    "make",
    "sure",
    "data",
    "ready",
    "able",
    "convert",
    "vectors",
    "efficient",
    "way",
    "right",
    "part",
    "seen",
    "stemming",
    "one",
    "major",
    "disadvantage",
    "stemming",
    "obviously",
    "see",
    "even",
    "though",
    "snowball",
    "stemmer",
    "performing",
    "exceptionally",
    "well",
    "words",
    "like",
    "history",
    "also",
    "something",
    "like",
    "going",
    "right",
    "suppose",
    "probably",
    "write",
    "right",
    "snowball",
    "stem",
    "right",
    "use",
    "going",
    "think",
    "going",
    "whether",
    "give",
    "us",
    "good",
    "going",
    "performing",
    "well",
    "write",
    "goes",
    "see",
    "goes",
    "also",
    "gives",
    "us",
    "bad",
    "word",
    "right",
    "even",
    "stemming",
    "stem",
    "probably",
    "try",
    "see",
    "right",
    "short",
    "much",
    "try",
    "right",
    "words",
    "obviously",
    "form",
    "word",
    "changing",
    "one",
    "disadvantage",
    "respect",
    "stemming",
    "understand",
    "use",
    "cases",
    "like",
    "chat",
    "bots",
    "techniques",
    "used",
    "go",
    "ahead",
    "something",
    "called",
    "lemmatization",
    "lemmatization",
    "solves",
    "particular",
    "problem",
    "dictionary",
    "words",
    "root",
    "words",
    "basically",
    "whatever",
    "word",
    "basically",
    "give",
    "giving",
    "good",
    "grammatical",
    "form",
    "word",
    "like",
    "giving",
    "goes",
    "go",
    "giving",
    "fairly",
    "fair",
    "know",
    "giving",
    "eating",
    "eat",
    "kind",
    "disadvantage",
    "getting",
    "removed",
    "completely",
    "help",
    "lemmatization",
    "particular",
    "part",
    "seeing",
    "next",
    "video",
    "hope",
    "understood",
    "till",
    "please",
    "make",
    "sure",
    "try",
    "practice",
    "different",
    "different",
    "examples",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "natural",
    "language",
    "processing",
    "video",
    "going",
    "discuss",
    "limitation",
    "previous",
    "video",
    "already",
    "seen",
    "something",
    "called",
    "stemming",
    "understood",
    "stemming",
    "process",
    "reducing",
    "word",
    "word",
    "stem",
    "right",
    "understood",
    "disadvantage",
    "stemming",
    "words",
    "right",
    "perform",
    "stemming",
    "get",
    "correct",
    "form",
    "word",
    "entire",
    "meaning",
    "word",
    "actually",
    "gets",
    "changed",
    "please",
    "remember",
    "word",
    "called",
    "word",
    "stem",
    "right",
    "short",
    "kind",
    "algorithm",
    "tries",
    "find",
    "word",
    "stem",
    "words",
    "working",
    "absolutely",
    "fine",
    "seen",
    "different",
    "types",
    "stemming",
    "techniques",
    "like",
    "pto",
    "stemmer",
    "regus",
    "stemmer",
    "snowball",
    "stemmer",
    "also",
    "found",
    "snowball",
    "stemmer",
    "better",
    "poto",
    "things",
    "covered",
    "today",
    "video",
    "right",
    "video",
    "going",
    "discuss",
    "lemmatization",
    "lemmatization",
    "technique",
    "going",
    "use",
    "something",
    "called",
    "word",
    "net",
    "lemmatizer",
    "already",
    "told",
    "disadvantage",
    "steming",
    "words",
    "getting",
    "correct",
    "form",
    "meaning",
    "word",
    "basically",
    "changing",
    "order",
    "prevent",
    "use",
    "something",
    "called",
    "lemmatizer",
    "something",
    "called",
    "word",
    "net",
    "lemmatizer",
    "lemmatizer",
    "okay",
    "lemmatization",
    "technique",
    "like",
    "stemming",
    "output",
    "get",
    "lemmatization",
    "called",
    "lma",
    "root",
    "word",
    "understand",
    "root",
    "word",
    "like",
    "suppose",
    "eating",
    "become",
    "eat",
    "root",
    "word",
    "right",
    "probably",
    "talk",
    "history",
    "history",
    "root",
    "word",
    "right",
    "talk",
    "go",
    "goes",
    "go",
    "root",
    "word",
    "right",
    "getting",
    "stem",
    "word",
    "right",
    "stemming",
    "right",
    "probably",
    "apply",
    "algorithm",
    "respect",
    "try",
    "find",
    "stem",
    "word",
    "right",
    "word",
    "stem",
    "basically",
    "say",
    "get",
    "root",
    "exact",
    "word",
    "main",
    "aim",
    "lemmatizer",
    "give",
    "exact",
    "form",
    "word",
    "meaningful",
    "change",
    "meaning",
    "happening",
    "stemming",
    "rather",
    "root",
    "stem",
    "output",
    "steming",
    "repeat",
    "definition",
    "lemmatization",
    "technique",
    "like",
    "stemming",
    "output",
    "get",
    "lemmatization",
    "called",
    "lma",
    "root",
    "word",
    "rather",
    "root",
    "stem",
    "output",
    "stemming",
    "lemmatization",
    "get",
    "getting",
    "valid",
    "word",
    "means",
    "thing",
    "suppose",
    "words",
    "like",
    "eating",
    "eats",
    "eaten",
    "become",
    "eat",
    "right",
    "giving",
    "meaningful",
    "word",
    "represent",
    "many",
    "words",
    "okay",
    "nltk",
    "provides",
    "word",
    "let",
    "nemati",
    "class",
    "try",
    "show",
    "respect",
    "understand",
    "guys",
    "um",
    "know",
    "lization",
    "occurs",
    "gets",
    "performed",
    "help",
    "word",
    "net",
    "corpus",
    "reader",
    "dictionary",
    "kind",
    "words",
    "comparing",
    "able",
    "worder",
    "much",
    "properly",
    "first",
    "let",
    "go",
    "ahead",
    "see",
    "basically",
    "implemented",
    "first",
    "try",
    "import",
    "ltk",
    "stem",
    "uh",
    "import",
    "word",
    "net",
    "lemmatizer",
    "okay",
    "going",
    "import",
    "word",
    "net",
    "lemmatizer",
    "uh",
    "helps",
    "us",
    "perform",
    "lemmatization",
    "uh",
    "first",
    "create",
    "object",
    "write",
    "litier",
    "going",
    "right",
    "word",
    "net",
    "nemati",
    "see",
    "nothing",
    "word",
    "net",
    "nemzer",
    "let",
    "go",
    "ahead",
    "execute",
    "got",
    "executed",
    "perfectly",
    "one",
    "thing",
    "really",
    "need",
    "see",
    "okay",
    "writing",
    "something",
    "called",
    "let",
    "say",
    "let",
    "try",
    "easy",
    "word",
    "okay",
    "lemmatizer",
    "equal",
    "oh",
    "going",
    "say",
    "dot",
    "limiti",
    "function",
    "called",
    "limiti",
    "give",
    "words",
    "let",
    "say",
    "giving",
    "going",
    "try",
    "see",
    "answer",
    "actually",
    "getting",
    "going",
    "basically",
    "means",
    "uh",
    "trying",
    "find",
    "root",
    "word",
    "respect",
    "let",
    "go",
    "ahead",
    "see",
    "functionality",
    "litti",
    "lemiti",
    "give",
    "two",
    "important",
    "parameters",
    "one",
    "word",
    "something",
    "called",
    "post",
    "tag",
    "okay",
    "talk",
    "post",
    "tag",
    "right",
    "written",
    "n",
    "n",
    "basically",
    "means",
    "word",
    "actually",
    "passing",
    "treated",
    "noun",
    "may",
    "thinking",
    "ch",
    "many",
    "uh",
    "post",
    "tags",
    "let",
    "write",
    "comment",
    "okay",
    "different",
    "different",
    "post",
    "tag",
    "noun",
    "give",
    "small",
    "n",
    "verb",
    "give",
    "v",
    "adjective",
    "give",
    "adverb",
    "give",
    "r",
    "actually",
    "going",
    "default",
    "right",
    "post",
    "tag",
    "n",
    "right",
    "respect",
    "n",
    "getting",
    "output",
    "okay",
    "let",
    "say",
    "change",
    "v",
    "want",
    "show",
    "whether",
    "changed",
    "give",
    "post",
    "tag",
    "n",
    "saying",
    "consider",
    "going",
    "noun",
    "keyword",
    "going",
    "noun",
    "keyword",
    "right",
    "consider",
    "respect",
    "verve",
    "see",
    "respect",
    "verb",
    "able",
    "get",
    "good",
    "lemmatization",
    "like",
    "go",
    "okay",
    "similarly",
    "probably",
    "try",
    "see",
    "respect",
    "adjective",
    "see",
    "getting",
    "going",
    "right",
    "obviously",
    "going",
    "kind",
    "verb",
    "right",
    "adjective",
    "let",
    "say",
    "try",
    "go",
    "respect",
    "ad",
    "also",
    "getting",
    "going",
    "right",
    "respect",
    "going",
    "word",
    "right",
    "feel",
    "verb",
    "correct",
    "one",
    "really",
    "need",
    "select",
    "let",
    "one",
    "thing",
    "words",
    "copied",
    "right",
    "let",
    "apply",
    "entire",
    "litiz",
    "entire",
    "words",
    "okay",
    "going",
    "execute",
    "execute",
    "going",
    "write",
    "loop",
    "okay",
    "going",
    "copy",
    "instead",
    "writing",
    "stemming",
    "stem",
    "going",
    "write",
    "litier",
    "dot",
    "limiti",
    "okay",
    "going",
    "apply",
    "words",
    "see",
    "eating",
    "becoming",
    "eating",
    "eats",
    "becomes",
    "eat",
    "eaten",
    "eaten",
    "writing",
    "writing",
    "wr",
    "wr",
    "programming",
    "programs",
    "programs",
    "become",
    "program",
    "default",
    "litti",
    "remember",
    "given",
    "post",
    "tag",
    "n",
    "right",
    "considering",
    "words",
    "words",
    "basically",
    "noun",
    "okay",
    "programming",
    "programs",
    "history",
    "finally",
    "seeing",
    "lemmatization",
    "occurred",
    "much",
    "done",
    "kind",
    "able",
    "find",
    "considered",
    "noun",
    "noun",
    "whenever",
    "give",
    "noun",
    "suppose",
    "let",
    "say",
    "words",
    "names",
    "names",
    "obviously",
    "lz",
    "occur",
    "right",
    "like",
    "krishna",
    "sudan",
    "shukumar",
    "kind",
    "words",
    "name",
    "right",
    "short",
    "nouns",
    "right",
    "famous",
    "place",
    "name",
    "like",
    "taj",
    "mahal",
    "india",
    "uh",
    "places",
    "name",
    "right",
    "considered",
    "nouns",
    "right",
    "basic",
    "difference",
    "really",
    "need",
    "know",
    "order",
    "understand",
    "let",
    "say",
    "want",
    "convert",
    "adjective",
    "see",
    "eating",
    "become",
    "eats",
    "eats",
    "writing",
    "everything",
    "right",
    "let",
    "try",
    "respect",
    "verb",
    "words",
    "basically",
    "verbs",
    "right",
    "see",
    "eating",
    "become",
    "eat",
    "eats",
    "become",
    "eat",
    "writing",
    "wr",
    "wr",
    "programming",
    "program",
    "history",
    "see",
    "right",
    "uh",
    "history",
    "becoming",
    "history",
    "stemming",
    "used",
    "get",
    "something",
    "like",
    "history",
    "right",
    "quite",
    "amazing",
    "right",
    "finally",
    "finally",
    "could",
    "also",
    "see",
    "stemming",
    "right",
    "also",
    "words",
    "actually",
    "able",
    "perform",
    "well",
    "right",
    "like",
    "fairly",
    "sportingly",
    "right",
    "even",
    "goes",
    "right",
    "suppose",
    "probably",
    "go",
    "ahead",
    "write",
    "something",
    "like",
    "litier",
    "lemiti",
    "let",
    "say",
    "going",
    "write",
    "something",
    "like",
    "goes",
    "right",
    "going",
    "get",
    "go",
    "good",
    "right",
    "also",
    "play",
    "post",
    "tag",
    "right",
    "let",
    "say",
    "using",
    "v",
    "post",
    "tag",
    "going",
    "get",
    "go",
    "let",
    "go",
    "try",
    "going",
    "write",
    "litier",
    "okay",
    "going",
    "say",
    "try",
    "fairly",
    "let",
    "see",
    "kind",
    "output",
    "getting",
    "lemmatizer",
    "super",
    "amazing",
    "right",
    "giving",
    "good",
    "word",
    "form",
    "meaning",
    "word",
    "also",
    "maintained",
    "right",
    "see",
    "fairly",
    "sportingly",
    "let",
    "write",
    "post",
    "tag",
    "obviously",
    "noun",
    "may",
    "give",
    "something",
    "also",
    "respect",
    "verb",
    "also",
    "getting",
    "answer",
    "good",
    "right",
    "tell",
    "good",
    "litiz",
    "one",
    "question",
    "really",
    "wanted",
    "ask",
    "take",
    "time",
    "wordland",
    "nezer",
    "stemming",
    "answer",
    "simple",
    "wordland",
    "ntier",
    "see",
    "nltk",
    "provides",
    "word",
    "ntier",
    "crass",
    "within",
    "thin",
    "wrapper",
    "around",
    "word",
    "net",
    "corpus",
    "going",
    "compare",
    "right",
    "uses",
    "morphe",
    "function",
    "word",
    "net",
    "corpus",
    "reader",
    "class",
    "find",
    "lma",
    "basically",
    "take",
    "time",
    "right",
    "number",
    "words",
    "happening",
    "fast",
    "paragraph",
    "bigger",
    "sentence",
    "lemmatization",
    "going",
    "take",
    "time",
    "right",
    "basic",
    "difference",
    "stemming",
    "lization",
    "use",
    "cases",
    "use",
    "uh",
    "one",
    "simple",
    "use",
    "cas",
    "uh",
    "really",
    "want",
    "write",
    "uh",
    "q",
    "chat",
    "bots",
    "right",
    "chat",
    "bots",
    "amazing",
    "examples",
    "right",
    "basically",
    "use",
    "right",
    "let",
    "write",
    "q",
    "chat",
    "bots",
    "right",
    "uh",
    "text",
    "summarization",
    "right",
    "uh",
    "q",
    "examples",
    "basically",
    "consider",
    "uh",
    "text",
    "summarization",
    "also",
    "one",
    "example",
    "many",
    "many",
    "companies",
    "used",
    "right",
    "use",
    "basically",
    "things",
    "basically",
    "uh",
    "implement",
    "word",
    "lemmatizer",
    "get",
    "exact",
    "good",
    "form",
    "word",
    "root",
    "form",
    "word",
    "meaningful",
    "right",
    "hello",
    "guys",
    "continuing",
    "discussion",
    "respect",
    "natural",
    "language",
    "processing",
    "still",
    "text",
    "techniques",
    "seen",
    "tokenization",
    "seen",
    "stemming",
    "also",
    "seen",
    "different",
    "types",
    "along",
    "seen",
    "lemmatization",
    "going",
    "consider",
    "topic",
    "called",
    "stop",
    "words",
    "video",
    "going",
    "discuss",
    "stop",
    "wordss",
    "importance",
    "stop",
    "wordss",
    "show",
    "help",
    "nltk",
    "text",
    "processing",
    "important",
    "step",
    "natural",
    "language",
    "processing",
    "really",
    "need",
    "clean",
    "data",
    "need",
    "make",
    "data",
    "right",
    "format",
    "later",
    "try",
    "convert",
    "text",
    "data",
    "vectors",
    "able",
    "train",
    "model",
    "model",
    "know",
    "whenever",
    "say",
    "machine",
    "learning",
    "model",
    "internally",
    "really",
    "need",
    "train",
    "mathematical",
    "equations",
    "whenever",
    "train",
    "something",
    "mathematical",
    "equations",
    "really",
    "need",
    "give",
    "input",
    "data",
    "form",
    "numerical",
    "floating",
    "values",
    "let",
    "us",
    "go",
    "ahead",
    "let",
    "understand",
    "exactly",
    "stop",
    "wordss",
    "opened",
    "new",
    "notebook",
    "file",
    "one",
    "amazing",
    "speech",
    "dr",
    "apj",
    "abdul",
    "kalam",
    "former",
    "president",
    "india",
    "amazing",
    "speech",
    "probably",
    "read",
    "completely",
    "obviously",
    "given",
    "materials",
    "actually",
    "going",
    "going",
    "probably",
    "talk",
    "stop",
    "words",
    "important",
    "try",
    "remove",
    "stop",
    "words",
    "okay",
    "uh",
    "definition",
    "exactly",
    "stop",
    "words",
    "particular",
    "speech",
    "see",
    "uh",
    "lot",
    "sentences",
    "like",
    "three",
    "visions",
    "india",
    "years",
    "history",
    "people",
    "world",
    "come",
    "invaded",
    "us",
    "entire",
    "speech",
    "amazing",
    "speech",
    "probably",
    "learning",
    "would",
    "like",
    "uh",
    "tell",
    "please",
    "read",
    "uh",
    "getting",
    "lot",
    "information",
    "motivational",
    "speech",
    "alog",
    "together",
    "particular",
    "speech",
    "see",
    "definitely",
    "say",
    "paragraph",
    "corpus",
    "right",
    "words",
    "like",
    "know",
    "uh",
    "let",
    "say",
    "uh",
    "know",
    "right",
    "kind",
    "words",
    "right",
    "play",
    "big",
    "role",
    "task",
    "like",
    "spam",
    "classification",
    "let",
    "say",
    "trying",
    "kind",
    "task",
    "respect",
    "uh",
    "know",
    "like",
    "spam",
    "ham",
    "classification",
    "already",
    "told",
    "along",
    "see",
    "whether",
    "positive",
    "review",
    "negative",
    "review",
    "words",
    "like",
    "actually",
    "play",
    "important",
    "role",
    "help",
    "stop",
    "words",
    "know",
    "try",
    "remove",
    "particular",
    "words",
    "uh",
    "kind",
    "use",
    "cases",
    "specifically",
    "focusing",
    "important",
    "words",
    "determine",
    "outc",
    "words",
    "like",
    "required",
    "basically",
    "pass",
    "entire",
    "paragraph",
    "particular",
    "stop",
    "words",
    "see",
    "words",
    "basically",
    "removed",
    "okay",
    "importance",
    "stop",
    "wordss",
    "short",
    "let",
    "us",
    "go",
    "ahead",
    "right",
    "go",
    "ahead",
    "execute",
    "let",
    "make",
    "cells",
    "much",
    "easy",
    "understand",
    "apply",
    "stop",
    "words",
    "along",
    "stop",
    "wordss",
    "also",
    "apply",
    "stemming",
    "show",
    "combination",
    "uh",
    "super",
    "important",
    "everyone",
    "okay",
    "let",
    "go",
    "ahead",
    "let",
    "uh",
    "try",
    "okay",
    "first",
    "really",
    "need",
    "import",
    "uh",
    "stemming",
    "obviously",
    "know",
    "need",
    "import",
    "stemming",
    "write",
    "nltk",
    "stem",
    "import",
    "pter",
    "stemmer",
    "basically",
    "write",
    "pter",
    "stem",
    "execute",
    "along",
    "uh",
    "uh",
    "obviously",
    "need",
    "also",
    "import",
    "stop",
    "words",
    "stop",
    "words",
    "english",
    "different",
    "entire",
    "list",
    "words",
    "like",
    "right",
    "going",
    "going",
    "say",
    "nltk",
    "corpus",
    "import",
    "stop",
    "wordss",
    "able",
    "use",
    "stop",
    "wordss",
    "imported",
    "uh",
    "nltk",
    "doc",
    "corpus",
    "import",
    "stop",
    "wordss",
    "stop",
    "wordss",
    "know",
    "also",
    "download",
    "uh",
    "let",
    "one",
    "thing",
    "also",
    "import",
    "nltk",
    "execute",
    "along",
    "write",
    "nltk",
    "doownload",
    "parameter",
    "going",
    "give",
    "stop",
    "wordss",
    "different",
    "different",
    "language",
    "stop",
    "wordss",
    "also",
    "also",
    "try",
    "see",
    "probably",
    "write",
    "able",
    "see",
    "downloading",
    "package",
    "stop",
    "wordss",
    "particular",
    "location",
    "package",
    "stop",
    "wordss",
    "already",
    "date",
    "getting",
    "true",
    "short",
    "downloaded",
    "different",
    "different",
    "languages",
    "stop",
    "words",
    "already",
    "present",
    "nltk",
    "library",
    "perfect",
    "done",
    "let",
    "see",
    "stop",
    "words",
    "available",
    "english",
    "order",
    "already",
    "imported",
    "nltk",
    "corpus",
    "import",
    "stop",
    "wordss",
    "take",
    "stop",
    "wordss",
    "copy",
    "paste",
    "say",
    "dot",
    "download",
    "okay",
    "instead",
    "download",
    "write",
    "wordss",
    "need",
    "give",
    "language",
    "like",
    "language",
    "really",
    "want",
    "uh",
    "give",
    "like",
    "english",
    "something",
    "else",
    "like",
    "german",
    "going",
    "write",
    "let",
    "write",
    "english",
    "execute",
    "see",
    "list",
    "stop",
    "wordss",
    "obviously",
    "stop",
    "words",
    "actually",
    "removed",
    "right",
    "may",
    "thinking",
    "k",
    "may",
    "depend",
    "data",
    "data",
    "right",
    "see",
    "guys",
    "list",
    "also",
    "create",
    "stop",
    "wordss",
    "english",
    "like",
    "let",
    "say",
    "important",
    "words",
    "like",
    "could",
    "right",
    "words",
    "actually",
    "play",
    "important",
    "role",
    "uh",
    "find",
    "whether",
    "statement",
    "positive",
    "negative",
    "like",
    "also",
    "probably",
    "search",
    "also",
    "able",
    "find",
    "okay",
    "always",
    "good",
    "way",
    "create",
    "stop",
    "words",
    "try",
    "remove",
    "kind",
    "words",
    "paragraph",
    "hope",
    "everybody",
    "able",
    "understand",
    "respect",
    "english",
    "let",
    "see",
    "whether",
    "respect",
    "different",
    "different",
    "language",
    "obviously",
    "go",
    "ahead",
    "check",
    "documentation",
    "try",
    "show",
    "respect",
    "german",
    "german",
    "also",
    "specific",
    "stop",
    "words",
    "along",
    "also",
    "use",
    "french",
    "particular",
    "stop",
    "words",
    "respect",
    "different",
    "different",
    "text",
    "different",
    "different",
    "language",
    "text",
    "definitely",
    "apply",
    "different",
    "different",
    "stop",
    "words",
    "respect",
    "may",
    "thinking",
    "hindi",
    "arabic",
    "think",
    "arabic",
    "also",
    "think",
    "let",
    "see",
    "whether",
    "yes",
    "arabic",
    "also",
    "find",
    "hindi",
    "guess",
    "documentation",
    "check",
    "till",
    "arabic",
    "able",
    "see",
    "information",
    "given",
    "nlt",
    "documentation",
    "actually",
    "going",
    "sentence",
    "already",
    "english",
    "right",
    "going",
    "perform",
    "two",
    "important",
    "task",
    "one",
    "apply",
    "stemming",
    "applying",
    "stemming",
    "know",
    "actually",
    "going",
    "wherever",
    "find",
    "stop",
    "words",
    "going",
    "remove",
    "stop",
    "words",
    "particular",
    "paragraph",
    "entire",
    "paragraph",
    "shortened",
    "right",
    "actually",
    "going",
    "see",
    "whatever",
    "things",
    "learned",
    "starting",
    "everything",
    "actually",
    "going",
    "cover",
    "okay",
    "first",
    "thing",
    "first",
    "going",
    "say",
    "nltk",
    "stem",
    "going",
    "import",
    "pter",
    "stemmer",
    "port",
    "stemma",
    "go",
    "execute",
    "going",
    "execute",
    "okay",
    "actually",
    "going",
    "going",
    "write",
    "stemmer",
    "equal",
    "pter",
    "stemmer",
    "really",
    "need",
    "initialize",
    "task",
    "right",
    "next",
    "step",
    "actually",
    "going",
    "going",
    "perform",
    "tokenization",
    "entire",
    "paragraph",
    "use",
    "nltk",
    "sent",
    "tokenize",
    "going",
    "give",
    "paragraph",
    "see",
    "guys",
    "going",
    "get",
    "entire",
    "par",
    "entire",
    "sentences",
    "like",
    "see",
    "three",
    "visions",
    "india",
    "years",
    "things",
    "able",
    "get",
    "second",
    "sentence",
    "third",
    "sentence",
    "four",
    "sentence",
    "like",
    "sentences",
    "form",
    "list",
    "able",
    "get",
    "using",
    "sentore",
    "tokenize",
    "right",
    "tokenization",
    "process",
    "wherein",
    "take",
    "paragraph",
    "divide",
    "sentences",
    "okay",
    "let",
    "one",
    "thing",
    "going",
    "save",
    "variable",
    "called",
    "sentences",
    "let",
    "later",
    "become",
    "list",
    "right",
    "sentences",
    "probably",
    "see",
    "type",
    "sentences",
    "going",
    "basically",
    "see",
    "list",
    "perfect",
    "till",
    "done",
    "amazingly",
    "well",
    "right",
    "done",
    "port",
    "stemmer",
    "uh",
    "sorry",
    "initialized",
    "stemmer",
    "tokenized",
    "understand",
    "going",
    "going",
    "traverse",
    "sentences",
    "first",
    "apply",
    "stop",
    "words",
    "words",
    "present",
    "stop",
    "words",
    "going",
    "take",
    "apply",
    "stemming",
    "really",
    "want",
    "saying",
    "first",
    "apply",
    "stop",
    "wordss",
    "filter",
    "apply",
    "tokenization",
    "right",
    "sorry",
    "apply",
    "stemming",
    "step",
    "actually",
    "going",
    "see",
    "simple",
    "important",
    "write",
    "loop",
    "saying",
    "range",
    "range",
    "going",
    "basically",
    "give",
    "length",
    "sentences",
    "also",
    "go",
    "respect",
    "sentences",
    "getting",
    "indexes",
    "getting",
    "indexes",
    "okay",
    "range",
    "basically",
    "says",
    "whatever",
    "length",
    "actually",
    "giving",
    "becomes",
    "index",
    "right",
    "0",
    "specific",
    "length",
    "actually",
    "going",
    "take",
    "going",
    "take",
    "specific",
    "n",
    "um",
    "going",
    "write",
    "n",
    "nltk",
    "dot",
    "word",
    "tokenize",
    "getting",
    "form",
    "sentences",
    "need",
    "get",
    "every",
    "word",
    "right",
    "getting",
    "word",
    "inside",
    "word",
    "unders",
    "token",
    "give",
    "sorry",
    "sentence",
    "index",
    "sentences",
    "index",
    "able",
    "get",
    "word",
    "make",
    "list",
    "words",
    "short",
    "getting",
    "list",
    "words",
    "inside",
    "sentences",
    "perfect",
    "till",
    "done",
    "going",
    "apply",
    "one",
    "important",
    "thing",
    "first",
    "need",
    "apply",
    "stop",
    "words",
    "every",
    "word",
    "see",
    "whether",
    "falls",
    "stop",
    "word",
    "fall",
    "stop",
    "word",
    "stemming",
    "understand",
    "task",
    "step",
    "step",
    "super",
    "important",
    "respect",
    "steps",
    "actually",
    "taking",
    "okay",
    "write",
    "list",
    "comprehension",
    "say",
    "stemmer",
    "stem",
    "okay",
    "going",
    "write",
    "word",
    "okay",
    "word",
    "uh",
    "particular",
    "words",
    "words",
    "list",
    "words",
    "take",
    "every",
    "word",
    "write",
    "loop",
    "okay",
    "called",
    "list",
    "comprehension",
    "write",
    "word",
    "words",
    "word",
    "see",
    "word",
    "present",
    "stop",
    "words",
    "apply",
    "stemming",
    "actually",
    "trying",
    "okay",
    "basically",
    "see",
    "word",
    "use",
    "set",
    "along",
    "download",
    "stop",
    "words",
    "respect",
    "english",
    "right",
    "using",
    "step",
    "set",
    "words",
    "may",
    "get",
    "repeated",
    "want",
    "going",
    "basically",
    "write",
    "right",
    "actually",
    "going",
    "get",
    "specific",
    "word",
    "present",
    "stop",
    "words",
    "stemming",
    "getting",
    "applied",
    "specific",
    "word",
    "perfect",
    "actually",
    "going",
    "going",
    "save",
    "variable",
    "called",
    "words",
    "perfect",
    "hope",
    "much",
    "clear",
    "getting",
    "back",
    "everything",
    "stemming",
    "back",
    "words",
    "finally",
    "actually",
    "going",
    "going",
    "take",
    "sentences",
    "going",
    "replace",
    "index",
    "respect",
    "words",
    "get",
    "words",
    "right",
    "need",
    "join",
    "words",
    "together",
    "join",
    "obviously",
    "space",
    "dot",
    "use",
    "join",
    "join",
    "together",
    "convert",
    "sentences",
    "exactly",
    "converting",
    "words",
    "sentences",
    "right",
    "much",
    "simple",
    "actually",
    "done",
    "perfect",
    "things",
    "done",
    "let",
    "repeat",
    "iterating",
    "every",
    "sentence",
    "word",
    "tokenize",
    "basically",
    "every",
    "senten",
    "getting",
    "list",
    "words",
    "list",
    "words",
    "iterating",
    "seeing",
    "whether",
    "present",
    "stop",
    "words",
    "present",
    "stemming",
    "stemming",
    "storing",
    "back",
    "list",
    "converting",
    "words",
    "sentences",
    "say",
    "converting",
    "list",
    "words",
    "sentences",
    "perfect",
    "execute",
    "go",
    "see",
    "sentences",
    "able",
    "see",
    "i3",
    "vision",
    "india",
    "right",
    "year",
    "history",
    "right",
    "h",
    "h",
    "r",
    "became",
    "r",
    "okay",
    "people",
    "people",
    "became",
    "people",
    "world",
    "came",
    "invaded",
    "us",
    "invade",
    "became",
    "capture",
    "land",
    "conquer",
    "mind",
    "see",
    "special",
    "words",
    "like",
    "like",
    "everything",
    "gone",
    "see",
    "see",
    "gone",
    "right",
    "able",
    "find",
    "anywhere",
    "right",
    "whatever",
    "stop",
    "words",
    "present",
    "got",
    "removed",
    "performed",
    "stemming",
    "right",
    "may",
    "saying",
    "kish",
    "uh",
    "stemming",
    "look",
    "good",
    "right",
    "need",
    "already",
    "taught",
    "respect",
    "snowball",
    "stemmer",
    "import",
    "okay",
    "simple",
    "think",
    "task",
    "snowball",
    "stemmer",
    "try",
    "import",
    "respect",
    "english",
    "obviously",
    "able",
    "get",
    "good",
    "sentence",
    "right",
    "let",
    "remove",
    "one",
    "snowball",
    "stemmer",
    "already",
    "done",
    "going",
    "copy",
    "thing",
    "okay",
    "going",
    "say",
    "apply",
    "snowball",
    "stemmer",
    "stemming",
    "right",
    "instead",
    "stemmer",
    "write",
    "word",
    "snowball",
    "stemmer",
    "yeah",
    "execute",
    "uh",
    "let",
    "go",
    "back",
    "back",
    "sentences",
    "sentences",
    "got",
    "changed",
    "sentences",
    "let",
    "see",
    "okay",
    "uh",
    "sentences",
    "got",
    "executed",
    "going",
    "execute",
    "probably",
    "go",
    "see",
    "sentences",
    "see",
    "good",
    "right",
    "snow",
    "uh",
    "one",
    "important",
    "thing",
    "snowball",
    "done",
    "see",
    "still",
    "capital",
    "letters",
    "right",
    "like",
    "may",
    "sentences",
    "may",
    "small",
    "letter",
    "also",
    "becomes",
    "repeated",
    "word",
    "since",
    "capital",
    "letter",
    "considered",
    "separate",
    "word",
    "right",
    "model",
    "understand",
    "right",
    "snowball",
    "one",
    "advantage",
    "making",
    "sure",
    "letter",
    "becoming",
    "small",
    "right",
    "letter",
    "becoming",
    "small",
    "words",
    "even",
    "giving",
    "good",
    "result",
    "like",
    "poverty",
    "become",
    "poverty",
    "try",
    "help",
    "lemmatization",
    "also",
    "get",
    "good",
    "word",
    "let",
    "try",
    "help",
    "lemmatization",
    "going",
    "uh",
    "thing",
    "okay",
    "hope",
    "everybody",
    "understood",
    "respect",
    "snowball",
    "stemmer",
    "going",
    "going",
    "go",
    "back",
    "limitation",
    "code",
    "going",
    "import",
    "nltk",
    "stem",
    "simple",
    "guys",
    "think",
    "uh",
    "repeating",
    "things",
    "also",
    "practice",
    "better",
    "way",
    "okay",
    "got",
    "word",
    "net",
    "climatizer",
    "going",
    "initialize",
    "perfect",
    "done",
    "going",
    "go",
    "go",
    "ahead",
    "copy",
    "code",
    "right",
    "write",
    "instead",
    "writing",
    "snowball",
    "going",
    "copy",
    "going",
    "paste",
    "perfect",
    "done",
    "let",
    "go",
    "ahead",
    "execute",
    "sentence",
    "part",
    "need",
    "get",
    "updated",
    "sentence",
    "think",
    "somewhere",
    "paragraph",
    "okay",
    "uh",
    "sentence",
    "okay",
    "perfect",
    "let",
    "go",
    "ahead",
    "execute",
    "thing",
    "okay",
    "getting",
    "stem",
    "okay",
    "sorry",
    "lemiti",
    "lemiti",
    "okay",
    "liter",
    "lemiti",
    "see",
    "time",
    "took",
    "right",
    "basically",
    "checking",
    "entire",
    "corpus",
    "probably",
    "go",
    "see",
    "sentences",
    "amazing",
    "things",
    "respect",
    "words",
    "coming",
    "correctly",
    "things",
    "right",
    "respect",
    "able",
    "get",
    "good",
    "thing",
    "one",
    "thing",
    "word",
    "also",
    "put",
    "post",
    "tag",
    "put",
    "post",
    "tag",
    "v",
    "right",
    "see",
    "output",
    "get",
    "better",
    "output",
    "guess",
    "words",
    "basically",
    "considering",
    "um",
    "know",
    "adverb",
    "oh",
    "sorry",
    "verb",
    "anyhow",
    "try",
    "understand",
    "post",
    "tag",
    "things",
    "right",
    "i3",
    "visions",
    "india",
    "years",
    "history",
    "people",
    "come",
    "would",
    "world",
    "come",
    "invade",
    "us",
    "capture",
    "land",
    "stop",
    "words",
    "got",
    "deleted",
    "getting",
    "good",
    "one",
    "know",
    "uh",
    "least",
    "better",
    "stemming",
    "okay",
    "one",
    "snowball",
    "stemming",
    "right",
    "entire",
    "process",
    "respect",
    "text",
    "discussed",
    "stop",
    "words",
    "also",
    "go",
    "ahead",
    "text",
    "hope",
    "everybody",
    "got",
    "idea",
    "lemmatization",
    "also",
    "see",
    "lowering",
    "actually",
    "basically",
    "lower",
    "sentences",
    "right",
    "let",
    "say",
    "write",
    "sentences",
    "equal",
    "sentences",
    "dot",
    "dot",
    "lower",
    "right",
    "basically",
    "give",
    "two",
    "lower",
    "let",
    "see",
    "whether",
    "work",
    "uh",
    "uh",
    "let",
    "see",
    "whether",
    "completely",
    "work",
    "sure",
    "whether",
    "dot",
    "lower",
    "work",
    "respect",
    "list",
    "think",
    "work",
    "going",
    "execute",
    "go",
    "okay",
    "apply",
    "litier",
    "okay",
    "uh",
    "str",
    "str",
    "object",
    "uh",
    "attribute",
    "lower",
    "okay",
    "okay",
    "problem",
    "problem",
    "actually",
    "going",
    "going",
    "comment",
    "writing",
    "say",
    "word",
    "lower",
    "okay",
    "definitely",
    "try",
    "different",
    "different",
    "things",
    "okay",
    "google",
    "know",
    "google",
    "able",
    "get",
    "going",
    "execute",
    "think",
    "execute",
    "know",
    "lot",
    "downs",
    "try",
    "follow",
    "lecture",
    "uh",
    "string",
    "object",
    "attribute",
    "lower",
    "two",
    "underscore",
    "lower",
    "two",
    "lower",
    "um",
    "okay",
    "let",
    "see",
    "uh",
    "str",
    "str",
    "lower",
    "case",
    "right",
    "python",
    "let",
    "see",
    "okay",
    "way",
    "see",
    "think",
    "lower",
    "definitely",
    "work",
    "let",
    "see",
    "dot",
    "lower",
    "okay",
    "perfect",
    "worked",
    "go",
    "see",
    "sentences",
    "done",
    "okay",
    "regrets",
    "search",
    "google",
    "also",
    "search",
    "google",
    "see",
    "small",
    "letters",
    "along",
    "limitation",
    "going",
    "continue",
    "discussion",
    "respect",
    "nlp",
    "video",
    "going",
    "discuss",
    "something",
    "called",
    "parts",
    "speech",
    "tagging",
    "hope",
    "understood",
    "limitation",
    "parts",
    "speech",
    "tagging",
    "plays",
    "important",
    "role",
    "giving",
    "verb",
    "noun",
    "right",
    "based",
    "know",
    "able",
    "get",
    "root",
    "form",
    "specific",
    "word",
    "right",
    "seen",
    "lot",
    "examples",
    "respect",
    "video",
    "try",
    "understand",
    "many",
    "different",
    "types",
    "parts",
    "speech",
    "tagging",
    "respect",
    "also",
    "see",
    "practical",
    "example",
    "let",
    "say",
    "actually",
    "getting",
    "sentence",
    "like",
    "taj",
    "mahal",
    "beautiful",
    "monument",
    "help",
    "nltk",
    "able",
    "understand",
    "seeing",
    "output",
    "let",
    "say",
    "taking",
    "particular",
    "example",
    "output",
    "tajil",
    "considered",
    "noun",
    "beautiful",
    "may",
    "considered",
    "adjective",
    "monument",
    "considered",
    "verb",
    "right",
    "giving",
    "example",
    "definitely",
    "see",
    "particular",
    "example",
    "also",
    "see",
    "extensive",
    "example",
    "altogether",
    "respect",
    "parts",
    "speech",
    "many",
    "different",
    "types",
    "things",
    "right",
    "something",
    "called",
    "cc",
    "coordinating",
    "conjunction",
    "cd",
    "cardinal",
    "digit",
    "dt",
    "determiner",
    "ex",
    "uh",
    "existential",
    "fw",
    "foreign",
    "word",
    "preposition",
    "jj",
    "adjective",
    "jjr",
    "adjective",
    "basically",
    "mean",
    "suppose",
    "giving",
    "sentence",
    "automatically",
    "able",
    "categorize",
    "specific",
    "different",
    "different",
    "parts",
    "speech",
    "automatically",
    "help",
    "nltk",
    "similarly",
    "lot",
    "things",
    "see",
    "personal",
    "personal",
    "pronoun",
    "like",
    "uh",
    "letters",
    "like",
    "hi",
    "put",
    "prp",
    "right",
    "getting",
    "tag",
    "called",
    "pr",
    "p",
    "see",
    "example",
    "like",
    "rb",
    "adverb",
    "right",
    "words",
    "like",
    "silently",
    "put",
    "rbr",
    "also",
    "adverb",
    "like",
    "comparative",
    "one",
    "right",
    "example",
    "like",
    "better",
    "rbs",
    "adverb",
    "like",
    "lot",
    "right",
    "actually",
    "going",
    "going",
    "take",
    "good",
    "example",
    "one",
    "assignment",
    "take",
    "particular",
    "simple",
    "example",
    "try",
    "find",
    "try",
    "write",
    "comment",
    "comment",
    "section",
    "okay",
    "able",
    "see",
    "particular",
    "speech",
    "dr",
    "uh",
    "apj",
    "abdul",
    "kalam",
    "obviously",
    "example",
    "shown",
    "stop",
    "wordss",
    "example",
    "right",
    "going",
    "take",
    "simple",
    "okay",
    "let",
    "consider",
    "uh",
    "parts",
    "speech",
    "tagging",
    "really",
    "wanted",
    "perform",
    "uh",
    "stemming",
    "let",
    "say",
    "uh",
    "want",
    "perform",
    "stemming",
    "want",
    "see",
    "every",
    "word",
    "right",
    "kind",
    "post",
    "speech",
    "tagging",
    "need",
    "import",
    "things",
    "okay",
    "going",
    "uh",
    "remove",
    "things",
    "instead",
    "focusing",
    "importing",
    "nltk",
    "write",
    "import",
    "nltk",
    "see",
    "writing",
    "nltk",
    "sentore",
    "tokenize",
    "basically",
    "means",
    "actually",
    "converting",
    "paragraph",
    "sentences",
    "let",
    "execute",
    "get",
    "able",
    "see",
    "sentences",
    "example",
    "trying",
    "show",
    "every",
    "word",
    "able",
    "find",
    "post",
    "tag",
    "okay",
    "next",
    "thing",
    "actually",
    "going",
    "apply",
    "stop",
    "wordss",
    "okay",
    "need",
    "apply",
    "stop",
    "words",
    "also",
    "want",
    "remove",
    "anything",
    "going",
    "uncomment",
    "things",
    "going",
    "say",
    "find",
    "pa",
    "tag",
    "okay",
    "find",
    "pa",
    "tag",
    "right",
    "perfect",
    "uh",
    "till",
    "going",
    "good",
    "right",
    "actually",
    "uh",
    "much",
    "clear",
    "much",
    "easy",
    "understand",
    "uh",
    "able",
    "uh",
    "like",
    "simply",
    "putting",
    "loop",
    "respect",
    "sentences",
    "sentences",
    "trying",
    "convert",
    "words",
    "getting",
    "list",
    "words",
    "need",
    "stemming",
    "remove",
    "stemming",
    "okay",
    "uh",
    "add",
    "w",
    "word",
    "getting",
    "word",
    "saying",
    "word",
    "set",
    "stop",
    "words",
    "wordss",
    "english",
    "uh",
    "let",
    "see",
    "okay",
    "told",
    "apply",
    "stop",
    "wordss",
    "let",
    "let",
    "us",
    "remove",
    "stop",
    "wordss",
    "stop",
    "wordss",
    "playing",
    "important",
    "role",
    "order",
    "uh",
    "uh",
    "import",
    "libraries",
    "respect",
    "stop",
    "word",
    "obviously",
    "already",
    "written",
    "let",
    "contrl",
    "zed",
    "stop",
    "words",
    "respect",
    "also",
    "remove",
    "okay",
    "two",
    "things",
    "import",
    "stop",
    "wordss",
    "writing",
    "nltk",
    "corpus",
    "import",
    "stop",
    "wordss",
    "let",
    "apply",
    "thing",
    "whatever",
    "done",
    "previous",
    "time",
    "uh",
    "part",
    "actually",
    "repeating",
    "see",
    "word",
    "word",
    "words",
    "word",
    "particular",
    "stop",
    "word",
    "respect",
    "english",
    "going",
    "take",
    "words",
    "going",
    "comment",
    "code",
    "going",
    "join",
    "back",
    "sentences",
    "really",
    "need",
    "understand",
    "pause",
    "tag",
    "word",
    "getting",
    "basically",
    "write",
    "print",
    "okay",
    "going",
    "write",
    "simple",
    "print",
    "statement",
    "particular",
    "print",
    "statement",
    "uh",
    "printing",
    "also",
    "write",
    "basically",
    "next",
    "line",
    "using",
    "something",
    "called",
    "nltk",
    "post",
    "tag",
    "okay",
    "able",
    "see",
    "post",
    "tag",
    "also",
    "apply",
    "words",
    "also",
    "apply",
    "sentences",
    "right",
    "going",
    "apply",
    "words",
    "entire",
    "thing",
    "uh",
    "create",
    "uh",
    "p",
    "tag",
    "variable",
    "basically",
    "indicate",
    "everything",
    "getting",
    "stored",
    "respect",
    "going",
    "print",
    "let",
    "revise",
    "things",
    "iterating",
    "every",
    "sentence",
    "converting",
    "particular",
    "sentence",
    "words",
    "words",
    "applying",
    "stop",
    "words",
    "initially",
    "thought",
    "apply",
    "let",
    "apply",
    "stop",
    "words",
    "see",
    "end",
    "day",
    "small",
    "small",
    "words",
    "like",
    "want",
    "particular",
    "words",
    "right",
    "remove",
    "stop",
    "words",
    "take",
    "list",
    "words",
    "apply",
    "stemming",
    "really",
    "need",
    "find",
    "whatever",
    "words",
    "present",
    "needs",
    "uh",
    "like",
    "nk",
    "able",
    "find",
    "different",
    "different",
    "types",
    "parts",
    "speech",
    "tagging",
    "respect",
    "apply",
    "particular",
    "simple",
    "code",
    "nltk",
    "postore",
    "tag",
    "ofws",
    "finally",
    "printing",
    "see",
    "execute",
    "getting",
    "error",
    "going",
    "uh",
    "make",
    "sure",
    "nothing",
    "going",
    "like",
    "error",
    "going",
    "edit",
    "video",
    "says",
    "nltk",
    "download",
    "average",
    "percepton",
    "trigger",
    "tagger",
    "definitely",
    "require",
    "particular",
    "tagger",
    "apply",
    "post",
    "tag",
    "go",
    "uh",
    "copy",
    "paste",
    "also",
    "also",
    "face",
    "particular",
    "er",
    "see",
    "downloaded",
    "probably",
    "go",
    "ahead",
    "run",
    "seeing",
    "first",
    "sentence",
    "basically",
    "prp",
    "prp",
    "basically",
    "means",
    "probably",
    "go",
    "see",
    "personal",
    "pronoun",
    "given",
    "examples",
    "like",
    "right",
    "similarly",
    "able",
    "see",
    "words",
    "like",
    "three",
    "three",
    "cd",
    "vision",
    "nns",
    "uh",
    "india",
    "nnp",
    "let",
    "go",
    "ahead",
    "see",
    "nnp",
    "india",
    "categorized",
    "nnp",
    "nnp",
    "nothing",
    "proper",
    "noun",
    "singular",
    "like",
    "name",
    "place",
    "monuments",
    "different",
    "different",
    "things",
    "like",
    "nnps",
    "pro",
    "proper",
    "noun",
    "plural",
    "americans",
    "indians",
    "like",
    "right",
    "things",
    "easily",
    "able",
    "simply",
    "know",
    "show",
    "post",
    "tags",
    "right",
    "help",
    "nltk",
    "amazing",
    "thing",
    "respect",
    "sentences",
    "able",
    "see",
    "okay",
    "powerful",
    "nltk",
    "uh",
    "let",
    "show",
    "whatever",
    "assignment",
    "actually",
    "given",
    "right",
    "sentence",
    "going",
    "copy",
    "going",
    "take",
    "entirely",
    "okay",
    "let",
    "see",
    "whether",
    "able",
    "going",
    "use",
    "nltk",
    "pa",
    "tag",
    "sense",
    "let",
    "see",
    "respect",
    "sense",
    "also",
    "able",
    "going",
    "put",
    "taj",
    "mahel",
    "beautiful",
    "monument",
    "going",
    "execute",
    "see",
    "okay",
    "particular",
    "case",
    "happened",
    "every",
    "word",
    "considered",
    "right",
    "basically",
    "happen",
    "actually",
    "going",
    "going",
    "basically",
    "use",
    "post",
    "tag",
    "let",
    "see",
    "output",
    "respect",
    "also",
    "thing",
    "coming",
    "simple",
    "way",
    "particular",
    "scenario",
    "put",
    "loop",
    "okay",
    "say",
    "sentence",
    "entire",
    "quotes",
    "put",
    "go",
    "every",
    "word",
    "okay",
    "write",
    "dot",
    "split",
    "respect",
    "using",
    "dot",
    "split",
    "string",
    "okay",
    "probably",
    "use",
    "split",
    "able",
    "see",
    "getting",
    "taj",
    "mahel",
    "beautiful",
    "monument",
    "right",
    "uh",
    "see",
    "iterating",
    "every",
    "things",
    "right",
    "every",
    "word",
    "going",
    "print",
    "nltk",
    "dop",
    "tag",
    "actually",
    "going",
    "use",
    "uh",
    "go",
    "ahead",
    "write",
    "specific",
    "instead",
    "writing",
    "write",
    "words",
    "okay",
    "word",
    "nl",
    "post",
    "tag",
    "going",
    "use",
    "specific",
    "word",
    "uh",
    "still",
    "uh",
    "issue",
    "iterating",
    "everything",
    "probably",
    "get",
    "entire",
    "thing",
    "uh",
    "respect",
    "list",
    "able",
    "see",
    "okay",
    "uh",
    "let",
    "see",
    "let",
    "see",
    "let",
    "see",
    "something",
    "going",
    "delete",
    "going",
    "see",
    "word",
    "getting",
    "okay",
    "take",
    "ltk",
    "post",
    "tag",
    "respect",
    "specific",
    "word",
    "uh",
    "okay",
    "problem",
    "let",
    "see",
    "uh",
    "need",
    "provide",
    "list",
    "words",
    "okay",
    "applying",
    "okay",
    "problem",
    "okay",
    "uh",
    "give",
    "word",
    "word",
    "giving",
    "word",
    "word",
    "able",
    "see",
    "every",
    "single",
    "character",
    "giving",
    "right",
    "really",
    "need",
    "provide",
    "list",
    "words",
    "probably",
    "copy",
    "part",
    "put",
    "remove",
    "things",
    "actually",
    "done",
    "error",
    "see",
    "editing",
    "particular",
    "video",
    "respect",
    "error",
    "really",
    "need",
    "see",
    "errors",
    "short",
    "give",
    "like",
    "getting",
    "list",
    "words",
    "execute",
    "see",
    "taj",
    "showing",
    "nnp",
    "mahal",
    "nnp",
    "vbz",
    "dt",
    "beautiful",
    "jj",
    "monument",
    "nn",
    "see",
    "post",
    "tag",
    "basically",
    "uh",
    "whatever",
    "parameter",
    "give",
    "giving",
    "form",
    "list",
    "words",
    "able",
    "getting",
    "answer",
    "right",
    "guys",
    "hope",
    "able",
    "understand",
    "parts",
    "speech",
    "tagging",
    "definitely",
    "got",
    "errors",
    "really",
    "wanted",
    "show",
    "errors",
    "getting",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "natural",
    "language",
    "processing",
    "video",
    "going",
    "discuss",
    "name",
    "entity",
    "recognition",
    "amazing",
    "topic",
    "let",
    "open",
    "file",
    "see",
    "let",
    "say",
    "lot",
    "sentences",
    "right",
    "like",
    "one",
    "example",
    "particular",
    "sentence",
    "fl",
    "tower",
    "built",
    "1887",
    "1889",
    "french",
    "engineer",
    "gustav",
    "eel",
    "uh",
    "whose",
    "company",
    "specialized",
    "building",
    "metal",
    "frameworks",
    "structures",
    "right",
    "sentence",
    "particular",
    "sentence",
    "know",
    "parts",
    "speech",
    "tagging",
    "right",
    "noun",
    "along",
    "help",
    "nltk",
    "also",
    "able",
    "get",
    "something",
    "called",
    "name",
    "entity",
    "tags",
    "right",
    "examples",
    "name",
    "entity",
    "tags",
    "see",
    "one",
    "tag",
    "something",
    "called",
    "person",
    "second",
    "tag",
    "location",
    "place",
    "third",
    "tag",
    "date",
    "time",
    "right",
    "also",
    "given",
    "examples",
    "probably",
    "say",
    "fl",
    "tower",
    "may",
    "coming",
    "place",
    "come",
    "location",
    "right",
    "see",
    "okay",
    "gf",
    "f",
    "basically",
    "getting",
    "tagged",
    "name",
    "right",
    "suppose",
    "numbers",
    "actually",
    "something",
    "else",
    "right",
    "suppose",
    "giving",
    "money",
    "value",
    "like",
    "1",
    "million",
    "let",
    "say",
    "1",
    "million",
    "present",
    "somewhere",
    "right",
    "uh",
    "know",
    "basically",
    "tagged",
    "something",
    "called",
    "money",
    "right",
    "given",
    "kind",
    "name",
    "entity",
    "tags",
    "right",
    "let",
    "us",
    "see",
    "examples",
    "let",
    "us",
    "see",
    "name",
    "entity",
    "recognition",
    "given",
    "help",
    "nltk",
    "library",
    "begin",
    "actually",
    "going",
    "going",
    "take",
    "particular",
    "sentence",
    "going",
    "execute",
    "okay",
    "going",
    "like",
    "uh",
    "past",
    "videos",
    "let",
    "one",
    "thing",
    "let",
    "create",
    "cells",
    "okay",
    "first",
    "word",
    "first",
    "thing",
    "go",
    "ahead",
    "import",
    "nltk",
    "know",
    "uh",
    "respect",
    "nltk",
    "also",
    "use",
    "something",
    "called",
    "uh",
    "word",
    "tokenize",
    "word",
    "word",
    "tokenize",
    "word",
    "unor",
    "tokenize",
    "going",
    "give",
    "complete",
    "sentence",
    "uh",
    "let",
    "go",
    "ahead",
    "give",
    "sentence",
    "execute",
    "able",
    "see",
    "fl",
    "tower",
    "everything",
    "coming",
    "entire",
    "words",
    "list",
    "words",
    "right",
    "entire",
    "list",
    "words",
    "perfect",
    "see",
    "usually",
    "uh",
    "parts",
    "speech",
    "tagging",
    "basically",
    "write",
    "nltk",
    "post",
    "underscore",
    "tag",
    "basically",
    "give",
    "entire",
    "words",
    "based",
    "every",
    "every",
    "word",
    "assigned",
    "tags",
    "okay",
    "let",
    "write",
    "uh",
    "tagged",
    "uh",
    "elements",
    "tagged",
    "something",
    "giving",
    "variable",
    "name",
    "like",
    "tagged",
    "elements",
    "get",
    "stored",
    "okay",
    "see",
    "really",
    "want",
    "provide",
    "uh",
    "named",
    "entity",
    "recognition",
    "use",
    "nltk",
    "dot",
    "named",
    "entity",
    "something",
    "called",
    "n",
    "ne",
    "e",
    "okay",
    "let",
    "show",
    "ne",
    "uh",
    "ne",
    "chunk",
    "okay",
    "ne",
    "chunk",
    "function",
    "probably",
    "see",
    "definition",
    "particular",
    "thing",
    "uh",
    "function",
    "seeing",
    "use",
    "nltk",
    "currently",
    "recommended",
    "name",
    "entity",
    "chunker",
    "chunk",
    "given",
    "list",
    "given",
    "list",
    "tag",
    "tokens",
    "okay",
    "inside",
    "past",
    "tagg",
    "elements",
    "okay",
    "tag",
    "elements",
    "going",
    "give",
    "probably",
    "may",
    "get",
    "error",
    "reason",
    "need",
    "download",
    "one",
    "right",
    "see",
    "nltk",
    "doownload",
    "maxnet",
    "chunker",
    "specifically",
    "using",
    "chunker",
    "techniques",
    "basically",
    "get",
    "named",
    "entity",
    "really",
    "need",
    "download",
    "first",
    "requirement",
    "see",
    "downloading",
    "downloaded",
    "gets",
    "uh",
    "entire",
    "uh",
    "max",
    "max",
    "chunker",
    "gets",
    "downloaded",
    "good",
    "run",
    "specific",
    "code",
    "may",
    "take",
    "amount",
    "time",
    "may",
    "also",
    "huge",
    "uh",
    "huge",
    "library",
    "inside",
    "needs",
    "downloaded",
    "go",
    "ahead",
    "execute",
    "still",
    "getting",
    "error",
    "also",
    "says",
    "okay",
    "need",
    "download",
    "nltk",
    "download",
    "wordss",
    "perfect",
    "actually",
    "going",
    "go",
    "make",
    "one",
    "cell",
    "please",
    "worry",
    "whenever",
    "get",
    "errors",
    "seen",
    "people",
    "gets",
    "worried",
    "first",
    "go",
    "ahead",
    "see",
    "error",
    "exactly",
    "much",
    "simple",
    "need",
    "execute",
    "guys",
    "nlt",
    "download",
    "words",
    "gets",
    "downloaded",
    "seeing",
    "nltk",
    "neore",
    "chunk",
    "give",
    "tag",
    "elements",
    "need",
    "write",
    "dot",
    "draw",
    "execute",
    "able",
    "see",
    "amazing",
    "graph",
    "able",
    "get",
    "see",
    "everybody",
    "observe",
    "know",
    "whether",
    "able",
    "see",
    "properly",
    "clearly",
    "able",
    "see",
    "given",
    "information",
    "much",
    "clearly",
    "much",
    "clear",
    "uh",
    "entire",
    "sentence",
    "right",
    "organization",
    "recognized",
    "fl",
    "tower",
    "right",
    "noun",
    "right",
    "see",
    "word",
    "vbd",
    "built",
    "1887",
    "something",
    "like",
    "cd",
    "see",
    "gp",
    "right",
    "french",
    "jj",
    "right",
    "able",
    "determine",
    "jj",
    "see",
    "person",
    "information",
    "able",
    "get",
    "captured",
    "like",
    "gustav",
    "np",
    "clearly",
    "see",
    "information",
    "nicely",
    "right",
    "ever",
    "entity",
    "able",
    "get",
    "recognized",
    "able",
    "find",
    "person",
    "gp",
    "organization",
    "able",
    "find",
    "entire",
    "sentence",
    "graph",
    "able",
    "understand",
    "able",
    "get",
    "entire",
    "information",
    "like",
    "probably",
    "go",
    "see",
    "uh",
    "entities",
    "find",
    "person",
    "place",
    "location",
    "date",
    "time",
    "money",
    "hope",
    "able",
    "understand",
    "named",
    "entity",
    "recognition",
    "able",
    "see",
    "able",
    "see",
    "diagrams",
    "yes",
    "respect",
    "particular",
    "video",
    "understand",
    "nltk",
    "provides",
    "amazing",
    "feature",
    "definitely",
    "use",
    "required",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "nlp",
    "already",
    "already",
    "seen",
    "next",
    "step",
    "know",
    "text",
    "specifically",
    "performed",
    "stemming",
    "lemmatization",
    "stop",
    "wordss",
    "cleaned",
    "data",
    "right",
    "main",
    "thing",
    "really",
    "need",
    "convert",
    "text",
    "vectors",
    "multiple",
    "ways",
    "first",
    "way",
    "going",
    "discuss",
    "something",
    "called",
    "one",
    "hot",
    "encoding",
    "okay",
    "try",
    "understand",
    "help",
    "one",
    "hot",
    "encoding",
    "converting",
    "words",
    "vectors",
    "let",
    "take",
    "specific",
    "example",
    "text",
    "specific",
    "output",
    "right",
    "main",
    "aim",
    "first",
    "need",
    "take",
    "particular",
    "text",
    "need",
    "convert",
    "vectors",
    "okay",
    "respect",
    "obviously",
    "uh",
    "also",
    "going",
    "lower",
    "sentences",
    "words",
    "let",
    "focus",
    "like",
    "basically",
    "implement",
    "one",
    "hot",
    "encoding",
    "theoretical",
    "intuition",
    "behind",
    "vectors",
    "actually",
    "created",
    "uh",
    "begin",
    "let",
    "say",
    "text",
    "document",
    "one",
    "food",
    "good",
    "food",
    "bad",
    "p",
    "piza",
    "amazing",
    "okay",
    "basically",
    "piza",
    "amazing",
    "okay",
    "find",
    "many",
    "unique",
    "vocabulary",
    "see",
    "unique",
    "vocabulary",
    "play",
    "important",
    "role",
    "okay",
    "creating",
    "vectors",
    "okay",
    "begin",
    "actually",
    "going",
    "need",
    "find",
    "many",
    "unique",
    "words",
    "let",
    "go",
    "ahead",
    "write",
    "obviously",
    "know",
    "combine",
    "three",
    "documents",
    "becomes",
    "paragraph",
    "corpus",
    "let",
    "go",
    "ahead",
    "let",
    "write",
    "unique",
    "unique",
    "documents",
    "one",
    "unique",
    "uh",
    "sor",
    "document",
    "vocabulary",
    "food",
    "right",
    "right",
    "good",
    "see",
    "good",
    "also",
    "second",
    "sentence",
    "see",
    "food",
    "getting",
    "repeated",
    "going",
    "write",
    "bad",
    "okay",
    "go",
    "probably",
    "third",
    "sentence",
    "documents",
    "able",
    "see",
    "piza",
    "unique",
    "word",
    "unique",
    "vocabulary",
    "getting",
    "repeated",
    "finally",
    "something",
    "called",
    "amazing",
    "words",
    "probably",
    "present",
    "unique",
    "vocabulary",
    "right",
    "vocabulary",
    "nothing",
    "unique",
    "words",
    "unique",
    "words",
    "available",
    "entire",
    "paragraph",
    "sentence",
    "particular",
    "data",
    "set",
    "based",
    "unique",
    "words",
    "one",
    "hot",
    "encoding",
    "means",
    "okay",
    "let",
    "say",
    "probably",
    "consider",
    "particular",
    "document",
    "one",
    "d1",
    "okay",
    "let",
    "say",
    "d1",
    "d1",
    "like",
    "probably",
    "applying",
    "one",
    "hot",
    "encoding",
    "convert",
    "specific",
    "words",
    "vector",
    "representation",
    "basically",
    "means",
    "let",
    "say",
    "considering",
    "word",
    "vector",
    "representation",
    "word",
    "1",
    "0",
    "0",
    "0",
    "0",
    "0",
    "0",
    "one",
    "present",
    "basically",
    "means",
    "present",
    "specific",
    "word",
    "basically",
    "becoming",
    "one",
    "consider",
    "next",
    "word",
    "food",
    "able",
    "represent",
    "0",
    "1",
    "0",
    "0",
    "0",
    "0",
    "0",
    "right",
    "clearly",
    "able",
    "see",
    "wherever",
    "specific",
    "word",
    "represented",
    "vector",
    "dimension",
    "v",
    "okay",
    "v",
    "v",
    "nothing",
    "words",
    "present",
    "able",
    "see",
    "one",
    "word",
    "two",
    "word",
    "3",
    "word",
    "four",
    "word",
    "five",
    "word",
    "six",
    "word",
    "seven",
    "word",
    "unique",
    "vocabulary",
    "seven",
    "going",
    "represented",
    "seven",
    "vectors",
    "specific",
    "word",
    "represented",
    "one",
    "remaining",
    "represented",
    "zero",
    "right",
    "probably",
    "consider",
    "represent",
    "d1",
    "food",
    "good",
    "respect",
    "particular",
    "vectors",
    "okay",
    "understand",
    "particular",
    "food",
    "uh",
    "four",
    "words",
    "right",
    "1",
    "2",
    "3",
    "4",
    "respect",
    "d1",
    "first",
    "word",
    "represent",
    "1",
    "0",
    "0",
    "0",
    "0",
    "many",
    "zeros",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "right",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "right",
    "first",
    "word",
    "okay",
    "coming",
    "second",
    "word",
    "second",
    "word",
    "food",
    "right",
    "able",
    "see",
    "0",
    "1",
    "0",
    "0",
    "0",
    "0",
    "0",
    "right",
    "second",
    "word",
    "come",
    "third",
    "word",
    "basically",
    "right",
    "basically",
    "represented",
    "0",
    "0",
    "1",
    "0",
    "0",
    "0",
    "0",
    "third",
    "word",
    "coming",
    "four",
    "word",
    "good",
    "become",
    "0",
    "0",
    "0",
    "1",
    "0",
    "0",
    "0",
    "right",
    "sentence",
    "one",
    "one",
    "hot",
    "code",
    "rep",
    "one",
    "hot",
    "code",
    "representation",
    "respect",
    "particular",
    "text",
    "text",
    "apply",
    "one",
    "hot",
    "encoding",
    "going",
    "get",
    "kind",
    "representation",
    "guys",
    "talk",
    "shape",
    "d1",
    "sentence",
    "four",
    "words",
    "right",
    "see",
    "first",
    "word",
    "representation",
    "one",
    "second",
    "word",
    "representation",
    "one",
    "vector",
    "representation",
    "third",
    "word",
    "vector",
    "representation",
    "one",
    "fourth",
    "word",
    "vector",
    "representation",
    "one",
    "probably",
    "see",
    "shape",
    "right",
    "basically",
    "4",
    "cross",
    "7",
    "four",
    "words",
    "1",
    "2",
    "3",
    "4",
    "okay",
    "seven",
    "seven",
    "uh",
    "vocabulary",
    "size",
    "right",
    "1",
    "2",
    "3",
    "uh",
    "4",
    "5",
    "6",
    "7",
    "okay",
    "going",
    "also",
    "going",
    "thing",
    "d2",
    "okay",
    "let",
    "go",
    "ahead",
    "try",
    "respect",
    "d2",
    "would",
    "suggest",
    "pause",
    "video",
    "try",
    "probably",
    "continue",
    "video",
    "anyhow",
    "showing",
    "entire",
    "uh",
    "representation",
    "okay",
    "able",
    "see",
    "first",
    "word",
    "d2",
    "food",
    "bad",
    "right",
    "two",
    "three",
    "words",
    "going",
    "basically",
    "write",
    "representation",
    "1",
    "0",
    "uh",
    "1",
    "0",
    "0",
    "0",
    "think",
    "6",
    "zos",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "zos",
    "okay",
    "first",
    "word",
    "coming",
    "second",
    "word",
    "one",
    "okay",
    "going",
    "write",
    "okay",
    "second",
    "word",
    "going",
    "write",
    "0",
    "1",
    "0",
    "0",
    "0",
    "0",
    "hope",
    "size",
    "seven",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "perfect",
    "uh",
    "coming",
    "third",
    "word",
    "able",
    "see",
    "0",
    "0",
    "1",
    "0",
    "0",
    "0",
    "0",
    "okay",
    "see",
    "food",
    "food",
    "similar",
    "word",
    "like",
    "d1",
    "first",
    "three",
    "replica",
    "coming",
    "last",
    "word",
    "right",
    "say",
    "last",
    "word",
    "see",
    "good",
    "see",
    "bad",
    "whenever",
    "bad",
    "particular",
    "word",
    "become",
    "one",
    "right",
    "0",
    "0",
    "0",
    "0",
    "0",
    "0",
    "0",
    "0",
    "0",
    "1",
    "0",
    "0",
    "go",
    "ahead",
    "write",
    "0",
    "0",
    "0",
    "0",
    "1",
    "0",
    "0",
    "perfect",
    "d2",
    "basically",
    "repres",
    "presented",
    "able",
    "see",
    "shape",
    "somewhere",
    "around",
    "4",
    "cross",
    "7",
    "okay",
    "uh",
    "yes",
    "uh",
    "technique",
    "simple",
    "technique",
    "one",
    "encoding",
    "like",
    "converted",
    "text",
    "vectors",
    "uh",
    "see",
    "every",
    "word",
    "basically",
    "represented",
    "one",
    "hot",
    "encoded",
    "based",
    "vocabulary",
    "size",
    "whatever",
    "vocabulary",
    "size",
    "basically",
    "present",
    "see",
    "vocabulary",
    "size",
    "seven",
    "based",
    "one",
    "hot",
    "repres",
    "presentation",
    "given",
    "every",
    "word",
    "okay",
    "uh",
    "going",
    "forward",
    "next",
    "video",
    "going",
    "discuss",
    "advantages",
    "disadvantages",
    "one",
    "hot",
    "encoding",
    "getting",
    "used",
    "nlp",
    "use",
    "cases",
    "techniques",
    "like",
    "bag",
    "word",
    "tfidf",
    "also",
    "understand",
    "advantage",
    "disadvantage",
    "use",
    "trying",
    "understand",
    "bag",
    "wordss",
    "tf",
    "idf",
    "okay",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "one",
    "hot",
    "encoding",
    "nlp",
    "hope",
    "understood",
    "convert",
    "words",
    "vectors",
    "probably",
    "entire",
    "process",
    "actually",
    "explained",
    "video",
    "going",
    "talk",
    "advantages",
    "disadvantages",
    "using",
    "one",
    "hot",
    "encoding",
    "let",
    "go",
    "ahead",
    "let",
    "write",
    "advantages",
    "okay",
    "disadvantages",
    "okay",
    "first",
    "basic",
    "advantage",
    "easy",
    "implement",
    "python",
    "programming",
    "language",
    "right",
    "easy",
    "implement",
    "python",
    "saying",
    "sk",
    "learn",
    "specific",
    "library",
    "easily",
    "implement",
    "basically",
    "called",
    "one",
    "hot",
    "encoder",
    "okay",
    "pandas",
    "familiar",
    "pandas",
    "library",
    "something",
    "called",
    "pd",
    "getor",
    "dummies",
    "right",
    "function",
    "basically",
    "help",
    "create",
    "entire",
    "one",
    "hot",
    "encoding",
    "based",
    "words",
    "right",
    "second",
    "advantage",
    "obviously",
    "uh",
    "see",
    "kind",
    "examples",
    "go",
    "ahead",
    "going",
    "implement",
    "specifically",
    "separately",
    "video",
    "one",
    "hot",
    "encoding",
    "mostly",
    "use",
    "nlp",
    "nlp",
    "technique",
    "tell",
    "lot",
    "disadvantages",
    "also",
    "let",
    "go",
    "ahead",
    "talk",
    "disadvantage",
    "first",
    "disadvantage",
    "see",
    "end",
    "day",
    "probably",
    "consider",
    "consider",
    "right",
    "food",
    "good",
    "right",
    "end",
    "day",
    "getting",
    "1",
    "0",
    "0",
    "next",
    "word",
    "getting",
    "0",
    "1",
    "0",
    "0",
    "many",
    "number",
    "zeros",
    "ones",
    "arithmetic",
    "linear",
    "algebra",
    "basically",
    "say",
    "sparse",
    "metrix",
    "okay",
    "basically",
    "creates",
    "sparse",
    "metrix",
    "right",
    "sparse",
    "matrix",
    "exactly",
    "talk",
    "sparse",
    "matrix",
    "array",
    "metrix",
    "lot",
    "ones",
    "zeros",
    "okay",
    "problem",
    "respect",
    "sparse",
    "metrix",
    "also",
    "say",
    "also",
    "convert",
    "array",
    "also",
    "also",
    "say",
    "sparse",
    "arrays",
    "understand",
    "disadvantage",
    "respect",
    "sparse",
    "metrics",
    "whenever",
    "apply",
    "machine",
    "learning",
    "algorithm",
    "specific",
    "machine",
    "learning",
    "algorithm",
    "know",
    "convert",
    "text",
    "vectors",
    "right",
    "machine",
    "learning",
    "algorithms",
    "leads",
    "something",
    "called",
    "overfitting",
    "right",
    "exactly",
    "overfitting",
    "overfitting",
    "process",
    "wherein",
    "get",
    "good",
    "accuracy",
    "training",
    "data",
    "respect",
    "new",
    "data",
    "able",
    "give",
    "good",
    "accuracy",
    "parse",
    "metrics",
    "usually",
    "leads",
    "something",
    "called",
    "overfitting",
    "right",
    "one",
    "major",
    "disadvantage",
    "let",
    "talk",
    "next",
    "disadvantage",
    "okay",
    "let",
    "say",
    "words",
    "like",
    "food",
    "good",
    "piza",
    "sorry",
    "food",
    "good",
    "food",
    "bad",
    "piza",
    "amazing",
    "right",
    "see",
    "one",
    "thing",
    "right",
    "machine",
    "learning",
    "algorithm",
    "whenever",
    "give",
    "data",
    "right",
    "case",
    "also",
    "probably",
    "say",
    "vocabulary",
    "right",
    "features",
    "food",
    "good",
    "bad",
    "pizza",
    "amazing",
    "right",
    "see",
    "time",
    "inputs",
    "every",
    "word",
    "getting",
    "converted",
    "vector",
    "size",
    "seven",
    "right",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "right",
    "based",
    "number",
    "words",
    "seeing",
    "four",
    "words",
    "getting",
    "4",
    "cross",
    "7",
    "also",
    "four",
    "words",
    "actually",
    "getting",
    "4",
    "cross",
    "7",
    "consider",
    "respect",
    "third",
    "statement",
    "third",
    "sentence",
    "right",
    "probably",
    "consider",
    "d3",
    "right",
    "start",
    "creating",
    "one",
    "hot",
    "encoded",
    "format",
    "see",
    "piza",
    "right",
    "piza",
    "one",
    "particular",
    "instance",
    "probably",
    "create",
    "something",
    "like",
    "0",
    "0",
    "0",
    "0",
    "0",
    "1",
    "0",
    "right",
    "piza",
    "right",
    "nothing",
    "one",
    "remaining",
    "zero",
    "write",
    "0",
    "0",
    "1",
    "0",
    "0",
    "0",
    "0",
    "right",
    "third",
    "statement",
    "amazing",
    "last",
    "one",
    "far",
    "remember",
    "vocabulary",
    "d3",
    "looks",
    "right",
    "one",
    "hot",
    "encoded",
    "format",
    "one",
    "one",
    "thing",
    "really",
    "need",
    "understand",
    "size",
    "3",
    "cross",
    "7",
    "right",
    "machine",
    "learning",
    "need",
    "understand",
    "one",
    "thing",
    "whenever",
    "perform",
    "nlp",
    "let",
    "say",
    "machine",
    "learning",
    "use",
    "case",
    "number",
    "features",
    "fixed",
    "respect",
    "length",
    "see",
    "4",
    "cr",
    "7",
    "4",
    "cr",
    "7",
    "3",
    "cross",
    "7",
    "train",
    "train",
    "particular",
    "data",
    "machine",
    "learning",
    "algorithm",
    "still",
    "fixed",
    "text",
    "size",
    "right",
    "one",
    "major",
    "major",
    "disadvantage",
    "unless",
    "help",
    "one",
    "notot",
    "encoding",
    "getting",
    "fixed",
    "text",
    "size",
    "right",
    "got",
    "4",
    "cross",
    "7",
    "4",
    "cross",
    "7",
    "also",
    "4",
    "cross",
    "7",
    "could",
    "trained",
    "right",
    "say",
    "ml",
    "algorithm",
    "need",
    "need",
    "fixed",
    "size",
    "need",
    "fixed",
    "size",
    "input",
    "right",
    "right",
    "major",
    "disadvantage",
    "right",
    "seeing",
    "upcoming",
    "lectures",
    "help",
    "bag",
    "words",
    "df",
    "idf",
    "getting",
    "fixed",
    "size",
    "words",
    "okay",
    "third",
    "one",
    "seeing",
    "times",
    "finding",
    "zeros",
    "ones",
    "right",
    "zeros",
    "ones",
    "zeros",
    "ones",
    "times",
    "see",
    "specific",
    "word",
    "become",
    "one",
    "remaining",
    "zero",
    "talk",
    "semantic",
    "meaning",
    "two",
    "words",
    "like",
    "fo",
    "right",
    "able",
    "exactly",
    "calculate",
    "like",
    "far",
    "equal",
    "similar",
    "specific",
    "word",
    "process",
    "basically",
    "called",
    "semantic",
    "say",
    "semantic",
    "meaning",
    "getting",
    "captured",
    "semantic",
    "meaning",
    "getting",
    "captured",
    "let",
    "talk",
    "good",
    "example",
    "okay",
    "let",
    "say",
    "something",
    "like",
    "food",
    "pizza",
    "burger",
    "okay",
    "know",
    "let",
    "say",
    "vocabulary",
    "three",
    "words",
    "foot",
    "representation",
    "basically",
    "write",
    "1",
    "0",
    "0",
    "piza",
    "let",
    "say",
    "going",
    "write",
    "0",
    "1",
    "0",
    "burger",
    "going",
    "write",
    "0",
    "01",
    "understand",
    "vectors",
    "right",
    "since",
    "three",
    "features",
    "basically",
    "three",
    "vectors",
    "probably",
    "heard",
    "something",
    "called",
    "cosine",
    "similarity",
    "really",
    "want",
    "find",
    "distance",
    "vector",
    "vector",
    "vector",
    "vector",
    "probably",
    "consider",
    "let",
    "say",
    "going",
    "draw",
    "three",
    "dimension",
    "okay",
    "let",
    "say",
    "determined",
    "something",
    "like",
    "burger",
    "determined",
    "something",
    "like",
    "food",
    "determined",
    "something",
    "like",
    "pizza",
    "talk",
    "things",
    "right",
    "like",
    "let",
    "say",
    "case",
    "food",
    "1",
    "0",
    "0",
    "axis",
    "probably",
    "getting",
    "one",
    "represented",
    "1",
    "comma",
    "0a",
    "0",
    "three",
    "dimension",
    "talk",
    "p",
    "0",
    "1",
    "0",
    "another",
    "point",
    "let",
    "say",
    "going",
    "denote",
    "another",
    "point",
    "0a",
    "1",
    "comma",
    "0",
    "let",
    "let",
    "talk",
    "third",
    "point",
    "third",
    "point",
    "somewhere",
    "burer",
    "distance",
    "0a",
    "0a",
    "1",
    "probably",
    "calculate",
    "distance",
    "things",
    "almost",
    "equal",
    "right",
    "able",
    "tell",
    "exact",
    "difference",
    "food",
    "p",
    "burger",
    "obviously",
    "considering",
    "okay",
    "words",
    "equal",
    "distance",
    "understand",
    "particular",
    "word",
    "different",
    "right",
    "super",
    "important",
    "understand",
    "short",
    "actually",
    "trying",
    "say",
    "semantic",
    "meaning",
    "basically",
    "getting",
    "captured",
    "basically",
    "means",
    "particular",
    "sentence",
    "able",
    "understand",
    "important",
    "word",
    "word",
    "related",
    "word",
    "word",
    "much",
    "similar",
    "word",
    "information",
    "getting",
    "captured",
    "end",
    "day",
    "getting",
    "one",
    "zeros",
    "okay",
    "uh",
    "third",
    "major",
    "disadvantage",
    "talking",
    "fourth",
    "disadvantage",
    "also",
    "super",
    "important",
    "particular",
    "concept",
    "something",
    "called",
    "vocabulary",
    "vocabulary",
    "basically",
    "mean",
    "okay",
    "let",
    "say",
    "right",
    "many",
    "vocabularies",
    "word",
    "let",
    "say",
    "train",
    "model",
    "want",
    "test",
    "new",
    "data",
    "set",
    "testing",
    "new",
    "data",
    "set",
    "test",
    "data",
    "let",
    "say",
    "say",
    "burg",
    "bad",
    "need",
    "predict",
    "test",
    "data",
    "need",
    "predict",
    "know",
    "respect",
    "particular",
    "sentences",
    "okay",
    "burger",
    "nowhere",
    "present",
    "particular",
    "uh",
    "vocabulary",
    "problem",
    "find",
    "way",
    "way",
    "represent",
    "burger",
    "form",
    "vectors",
    "right",
    "able",
    "form",
    "vectors",
    "anyhow",
    "vocabulary",
    "word",
    "happen",
    "particular",
    "case",
    "obviously",
    "able",
    "perform",
    "one",
    "hot",
    "encoding",
    "vocabulary",
    "many",
    "number",
    "words",
    "work",
    "new",
    "word",
    "actually",
    "coming",
    "wherein",
    "present",
    "vocabulary",
    "respect",
    "test",
    "data",
    "obviously",
    "major",
    "disadvantage",
    "right",
    "technique",
    "basically",
    "called",
    "vocabulary",
    "short",
    "advantages",
    "disadvantage",
    "understand",
    "sparse",
    "metric",
    "basically",
    "means",
    "many",
    "ones",
    "zeros",
    "leads",
    "overfitting",
    "respect",
    "various",
    "machine",
    "learning",
    "algorithm",
    "machine",
    "learning",
    "algorithm",
    "definitely",
    "require",
    "fixed",
    "size",
    "input",
    "right",
    "able",
    "get",
    "basically",
    "means",
    "sentences",
    "fixed",
    "size",
    "work",
    "semantic",
    "meeting",
    "sorry",
    "meaning",
    "getting",
    "captured",
    "already",
    "told",
    "try",
    "calculate",
    "distance",
    "words",
    "words",
    "equidistance",
    "saying",
    "whether",
    "pizza",
    "much",
    "similar",
    "food",
    "burger",
    "much",
    "similar",
    "food",
    "right",
    "similarity",
    "based",
    "semantic",
    "meaning",
    "getting",
    "captured",
    "either",
    "getting",
    "ones",
    "zeros",
    "able",
    "provide",
    "right",
    "see",
    "much",
    "maximum",
    "information",
    "important",
    "word",
    "information",
    "also",
    "getting",
    "captured",
    "short",
    "advantage",
    "disadvantage",
    "hope",
    "uh",
    "understood",
    "next",
    "uh",
    "technique",
    "respect",
    "converting",
    "word",
    "vectors",
    "something",
    "called",
    "bag",
    "wordss",
    "try",
    "see",
    "fixing",
    "disadvantage",
    "uh",
    "yes",
    "one",
    "one",
    "major",
    "disadvantage",
    "say",
    "let",
    "say",
    "right",
    "three",
    "sentences",
    "let",
    "say",
    "seven",
    "vocabulary",
    "let",
    "say",
    "tomorrow",
    "50k",
    "unique",
    "vocabulary",
    "size",
    "vocabulary",
    "size",
    "happen",
    "particular",
    "case",
    "right",
    "getting",
    "many",
    "number",
    "ones",
    "zeros",
    "short",
    "actually",
    "leading",
    "sparse",
    "mc",
    "real",
    "world",
    "scenario",
    "three",
    "sentences",
    "right",
    "bigger",
    "sentences",
    "also",
    "many",
    "sentences",
    "also",
    "one",
    "thing",
    "interview",
    "ask",
    "probably",
    "need",
    "talk",
    "explain",
    "like",
    "respect",
    "advantage",
    "disadvantage",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "nlp",
    "previous",
    "video",
    "already",
    "seen",
    "discussed",
    "one",
    "hot",
    "encoding",
    "works",
    "right",
    "uh",
    "probably",
    "talk",
    "respect",
    "different",
    "types",
    "ways",
    "convert",
    "text",
    "vectors",
    "completed",
    "already",
    "one",
    "hot",
    "encoding",
    "second",
    "bag",
    "words",
    "let",
    "go",
    "ahead",
    "understand",
    "bag",
    "words",
    "actually",
    "work",
    "super",
    "important",
    "technique",
    "simple",
    "task",
    "like",
    "sentiment",
    "classifications",
    "whether",
    "uh",
    "basically",
    "short",
    "text",
    "classification",
    "kind",
    "task",
    "easily",
    "able",
    "solve",
    "okay",
    "uh",
    "applications",
    "famous",
    "application",
    "like",
    "whether",
    "mail",
    "spam",
    "ham",
    "everything",
    "able",
    "solve",
    "let",
    "say",
    "data",
    "set",
    "okay",
    "particular",
    "data",
    "set",
    "uh",
    "saying",
    "positive",
    "negative",
    "statements",
    "particular",
    "data",
    "set",
    "three",
    "sentences",
    "let",
    "say",
    "good",
    "boy",
    "good",
    "girl",
    "boy",
    "girl",
    "good",
    "okay",
    "like",
    "positive",
    "statements",
    "output",
    "ones",
    "right",
    "supervised",
    "machine",
    "learning",
    "really",
    "need",
    "know",
    "output",
    "also",
    "let",
    "go",
    "step",
    "step",
    "show",
    "bag",
    "words",
    "implemented",
    "basically",
    "step",
    "one",
    "right",
    "data",
    "set",
    "let",
    "go",
    "step",
    "two",
    "happens",
    "step",
    "two",
    "go",
    "right",
    "multiple",
    "steps",
    "uh",
    "actually",
    "occur",
    "initially",
    "also",
    "go",
    "respect",
    "first",
    "thing",
    "probably",
    "consider",
    "two",
    "things",
    "going",
    "basically",
    "happen",
    "first",
    "lower",
    "words",
    "usually",
    "steps",
    "even",
    "techniques",
    "also",
    "first",
    "lower",
    "words",
    "probably",
    "apply",
    "stop",
    "words",
    "right",
    "also",
    "already",
    "shown",
    "lower",
    "words",
    "let",
    "say",
    "sentence",
    "one",
    "right",
    "sentence",
    "one",
    "becomes",
    "going",
    "take",
    "text",
    "data",
    "worry",
    "output",
    "later",
    "convert",
    "text",
    "vectors",
    "apply",
    "machine",
    "learning",
    "algorithm",
    "sentence",
    "one",
    "see",
    "soon",
    "lower",
    "words",
    "happen",
    "become",
    "smaller",
    "may",
    "repeated",
    "words",
    "particular",
    "case",
    "capital",
    "capital",
    "b",
    "boy",
    "small",
    "b",
    "boy",
    "words",
    "since",
    "uppercase",
    "treated",
    "separate",
    "word",
    "really",
    "need",
    "lower",
    "lower",
    "case",
    "words",
    "going",
    "write",
    "lower",
    "case",
    "words",
    "sentence",
    "one",
    "lowering",
    "casing",
    "happen",
    "become",
    "smaller",
    "letters",
    "apply",
    "stop",
    "words",
    "happens",
    "stop",
    "words",
    "words",
    "like",
    "know",
    "get",
    "deleted",
    "right",
    "require",
    "particular",
    "word",
    "kind",
    "task",
    "like",
    "sentiment",
    "analysis",
    "important",
    "words",
    "like",
    "good",
    "boy",
    "good",
    "girl",
    "basically",
    "required",
    "things",
    "go",
    "like",
    "also",
    "go",
    "happen",
    "respect",
    "sentence",
    "one",
    "words",
    "become",
    "like",
    "good",
    "boy",
    "okay",
    "showing",
    "step",
    "step",
    "uh",
    "help",
    "python",
    "help",
    "libraries",
    "much",
    "simple",
    "worry",
    "much",
    "okay",
    "uh",
    "use",
    "one",
    "library",
    "sentence",
    "two",
    "become",
    "stop",
    "words",
    "get",
    "removed",
    "become",
    "good",
    "girl",
    "right",
    "similarly",
    "sentence",
    "three",
    "specific",
    "words",
    "boy",
    "girl",
    "good",
    "right",
    "things",
    "sentence",
    "one",
    "sentence",
    "two",
    "sentence",
    "sentence",
    "three",
    "perfect",
    "uh",
    "go",
    "ahead",
    "calculate",
    "vocabulary",
    "many",
    "words",
    "vocabulary",
    "good",
    "boy",
    "girl",
    "boy",
    "girl",
    "getting",
    "repeated",
    "probably",
    "consider",
    "many",
    "unique",
    "words",
    "vocabulary",
    "able",
    "write",
    "first",
    "word",
    "nothing",
    "good",
    "go",
    "ahead",
    "write",
    "good",
    "good",
    "first",
    "word",
    "one",
    "thing",
    "probably",
    "write",
    "frequency",
    "specific",
    "word",
    "like",
    "many",
    "times",
    "words",
    "different",
    "different",
    "sentences",
    "obviously",
    "see",
    "1",
    "2",
    "3",
    "three",
    "keep",
    "going",
    "keep",
    "count",
    "three",
    "something",
    "called",
    "boy",
    "able",
    "see",
    "many",
    "times",
    "boy",
    "sentence",
    "three",
    "also",
    "boy",
    "sentence",
    "one",
    "also",
    "boy",
    "going",
    "make",
    "count",
    "two",
    "coming",
    "next",
    "one",
    "called",
    "girl",
    "girl",
    "word",
    "also",
    "vocabulary",
    "girl",
    "also",
    "able",
    "see",
    "two",
    "times",
    "right",
    "first",
    "need",
    "see",
    "frequency",
    "respect",
    "different",
    "different",
    "vocabulary",
    "words",
    "ascending",
    "order",
    "obviously",
    "uh",
    "sorry",
    "descending",
    "order",
    "maximum",
    "number",
    "frequency",
    "put",
    "first",
    "word",
    "right",
    "boy",
    "present",
    "two",
    "times",
    "girl",
    "present",
    "two",
    "times",
    "moved",
    "understand",
    "done",
    "based",
    "descending",
    "order",
    "ordered",
    "words",
    "basically",
    "maximum",
    "minimum",
    "okay",
    "perfect",
    "applying",
    "bag",
    "words",
    "already",
    "seen",
    "many",
    "vocabulary",
    "size",
    "vocabulary",
    "size",
    "vocabulary",
    "size",
    "three",
    "right",
    "see",
    "bigger",
    "data",
    "set",
    "kind",
    "words",
    "lot",
    "words",
    "like",
    "frequency",
    "also",
    "one",
    "important",
    "step",
    "necessary",
    "use",
    "words",
    "also",
    "uh",
    "bag",
    "words",
    "let",
    "say",
    "vocabulary",
    "100",
    "100",
    "unique",
    "words",
    "let",
    "say",
    "words",
    "present",
    "right",
    "words",
    "present",
    "even",
    "take",
    "coding",
    "respect",
    "bag",
    "words",
    "know",
    "also",
    "option",
    "select",
    "top",
    "10",
    "features",
    "top",
    "20",
    "features",
    "words",
    "getting",
    "repeated",
    "importance",
    "particular",
    "frequency",
    "okay",
    "perfect",
    "uh",
    "till",
    "next",
    "step",
    "simple",
    "based",
    "topmost",
    "frequency",
    "actually",
    "going",
    "going",
    "keep",
    "words",
    "feature",
    "good",
    "come",
    "boy",
    "come",
    "girl",
    "come",
    "okay",
    "already",
    "know",
    "sentence",
    "one",
    "right",
    "sentence",
    "one",
    "happen",
    "sentence",
    "one",
    "good",
    "boy",
    "see",
    "get",
    "converted",
    "vectors",
    "wherever",
    "good",
    "present",
    "become",
    "one",
    "wherever",
    "boy",
    "present",
    "become",
    "one",
    "remaining",
    "become",
    "zero",
    "entire",
    "sentence",
    "able",
    "see",
    "uh",
    "getting",
    "converted",
    "110",
    "vector",
    "okay",
    "text",
    "getting",
    "converted",
    "110",
    "vector",
    "similarly",
    "go",
    "respect",
    "sentence",
    "two",
    "wherever",
    "word",
    "like",
    "good",
    "become",
    "one",
    "wherever",
    "girl",
    "become",
    "one",
    "remaining",
    "become",
    "zero",
    "talk",
    "advantages",
    "disadvantages",
    "previously",
    "one",
    "hot",
    "encoded",
    "saw",
    "every",
    "word",
    "every",
    "word",
    "creating",
    "vector",
    "entire",
    "sentence",
    "vector",
    "coming",
    "okay",
    "lot",
    "advantages",
    "talk",
    "discuss",
    "uh",
    "first",
    "let",
    "understand",
    "s3",
    "able",
    "see",
    "sentence",
    "three",
    "boy",
    "girl",
    "good",
    "wherever",
    "boy",
    "become",
    "one",
    "girl",
    "become",
    "one",
    "good",
    "become",
    "one",
    "okay",
    "entire",
    "vectors",
    "obviously",
    "also",
    "output",
    "variable",
    "output",
    "variable",
    "1",
    "zer",
    "anything",
    "right",
    "probably",
    "solving",
    "sentiment",
    "analysis",
    "something",
    "like",
    "okay",
    "entire",
    "vectors",
    "okay",
    "vector",
    "entire",
    "sentence",
    "okay",
    "entire",
    "sentex",
    "entire",
    "bag",
    "words",
    "converts",
    "text",
    "vectors",
    "okay",
    "take",
    "particular",
    "vectors",
    "train",
    "machine",
    "learning",
    "model",
    "able",
    "get",
    "output",
    "one",
    "important",
    "thing",
    "really",
    "want",
    "put",
    "let",
    "say",
    "word",
    "right",
    "good",
    "girl",
    "girl",
    "right",
    "let",
    "say",
    "one",
    "word",
    "like",
    "good",
    "case",
    "happen",
    "okay",
    "case",
    "happen",
    "usually",
    "bag",
    "words",
    "since",
    "good",
    "repeated",
    "two",
    "times",
    "increase",
    "count",
    "two",
    "okay",
    "particular",
    "case",
    "increase",
    "count",
    "two",
    "okay",
    "two",
    "things",
    "one",
    "binary",
    "bag",
    "words",
    "one",
    "normal",
    "bag",
    "words",
    "case",
    "binary",
    "bag",
    "words",
    "even",
    "though",
    "count",
    "two",
    "going",
    "going",
    "force",
    "become",
    "one",
    "time",
    "word",
    "present",
    "may",
    "present",
    "number",
    "time",
    "value",
    "either",
    "one",
    "zero",
    "normal",
    "bag",
    "words",
    "increase",
    "count",
    "2",
    "3",
    "4",
    "based",
    "number",
    "words",
    "basic",
    "difference",
    "binary",
    "bag",
    "words",
    "bag",
    "words",
    "basically",
    "means",
    "ones",
    "zeros",
    "based",
    "frequency",
    "count",
    "get",
    "updated",
    "count",
    "get",
    "updated",
    "based",
    "frequency",
    "basic",
    "difference",
    "respect",
    "okay",
    "hope",
    "understood",
    "like",
    "help",
    "bag",
    "words",
    "converting",
    "text",
    "vectors",
    "next",
    "video",
    "going",
    "discuss",
    "advantages",
    "disadvantages",
    "respect",
    "like",
    "actually",
    "discussed",
    "one",
    "encoding",
    "okay",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "bag",
    "words",
    "uh",
    "already",
    "understood",
    "intution",
    "behind",
    "bag",
    "words",
    "converting",
    "text",
    "vectors",
    "usual",
    "let",
    "go",
    "ahead",
    "discuss",
    "advantages",
    "disadvantages",
    "first",
    "go",
    "ahead",
    "write",
    "advantages",
    "go",
    "ahead",
    "write",
    "disadvantages",
    "okay",
    "uh",
    "obviously",
    "also",
    "discussed",
    "advantages",
    "disadvantages",
    "respect",
    "one",
    "hot",
    "encoding",
    "try",
    "compare",
    "try",
    "see",
    "problems",
    "getting",
    "fixed",
    "okay",
    "first",
    "yes",
    "uh",
    "easy",
    "implement",
    "intuitive",
    "write",
    "something",
    "like",
    "simple",
    "intuitive",
    "simple",
    "intuitive",
    "okay",
    "second",
    "point",
    "uh",
    "respect",
    "advantages",
    "one",
    "encoding",
    "see",
    "seen",
    "important",
    "important",
    "thing",
    "machine",
    "learning",
    "algorithm",
    "right",
    "okay",
    "respect",
    "sparse",
    "metrics",
    "discussing",
    "respect",
    "semantic",
    "meaning",
    "vocabulary",
    "everything",
    "discussing",
    "first",
    "let",
    "consider",
    "particular",
    "second",
    "topic",
    "like",
    "uh",
    "ml",
    "algorithms",
    "give",
    "fixed",
    "size",
    "inputs",
    "respect",
    "bag",
    "words",
    "statement",
    "see",
    "sentence",
    "may",
    "three",
    "words",
    "five",
    "words",
    "10",
    "words",
    "end",
    "day",
    "based",
    "vocabulary",
    "size",
    "able",
    "get",
    "sentences",
    "converted",
    "many",
    "number",
    "di",
    "iions",
    "words",
    "vectors",
    "getting",
    "fixed",
    "inputs",
    "getting",
    "fixed",
    "vocabulary",
    "getting",
    "fixed",
    "particular",
    "problem",
    "getting",
    "solved",
    "okay",
    "uh",
    "one",
    "hot",
    "encoding",
    "fix",
    "size",
    "inputs",
    "since",
    "creating",
    "words",
    "every",
    "vectors",
    "sorry",
    "creating",
    "vectors",
    "every",
    "words",
    "okay",
    "going",
    "second",
    "point",
    "seeing",
    "yes",
    "fixed",
    "size",
    "input",
    "right",
    "superbly",
    "help",
    "ml",
    "algorithms",
    "training",
    "okay",
    "ml",
    "algorithms",
    "two",
    "major",
    "advantages",
    "talk",
    "disadvantage",
    "see",
    "first",
    "disadvantage",
    "respect",
    "one",
    "encoding",
    "spse",
    "metrix",
    "already",
    "told",
    "exactly",
    "sparse",
    "matrix",
    "nothing",
    "ones",
    "zeros",
    "let",
    "say",
    "vocabulary",
    "size",
    "happen",
    "every",
    "sentence",
    "get",
    "converted",
    "know",
    "size",
    "vocabulary",
    "right",
    "still",
    "sparse",
    "met",
    "problem",
    "respect",
    "disadvantage",
    "going",
    "write",
    "sparse",
    "metrix",
    "array",
    "array",
    "still",
    "actually",
    "lead",
    "overfitting",
    "okay",
    "second",
    "major",
    "disadvantage",
    "see",
    "end",
    "day",
    "whatever",
    "statement",
    "like",
    "good",
    "boy",
    "good",
    "girl",
    "know",
    "boy",
    "girl",
    "good",
    "okay",
    "something",
    "like",
    "seeing",
    "based",
    "sentence",
    "right",
    "based",
    "vocabulary",
    "based",
    "frequency",
    "vocabulary",
    "ordering",
    "word",
    "changing",
    "see",
    "understand",
    "sentence",
    "ordering",
    "word",
    "changes",
    "based",
    "vector",
    "getting",
    "created",
    "see",
    "based",
    "frequency",
    "written",
    "vocabularies",
    "right",
    "good",
    "present",
    "maximum",
    "number",
    "times",
    "wrote",
    "first",
    "boy",
    "present",
    "second",
    "number",
    "wrote",
    "right",
    "girl",
    "present",
    "uh",
    "like",
    "two",
    "two",
    "times",
    "written",
    "last",
    "right",
    "see",
    "probably",
    "consider",
    "third",
    "statement",
    "boy",
    "girl",
    "good",
    "right",
    "see",
    "entire",
    "word",
    "getting",
    "ordered",
    "uh",
    "like",
    "completely",
    "changed",
    "right",
    "ordering",
    "word",
    "completely",
    "changed",
    "one",
    "one0",
    "sentence",
    "3",
    "111",
    "word",
    "ordering",
    "changed",
    "meaning",
    "sentences",
    "also",
    "gets",
    "changed",
    "semantic",
    "information",
    "getting",
    "captured",
    "talk",
    "semantic",
    "information",
    "able",
    "see",
    "ordering",
    "words",
    "getting",
    "changed",
    "super",
    "important",
    "ordering",
    "word",
    "getting",
    "changed",
    "getting",
    "changed",
    "meaning",
    "sentence",
    "changes",
    "right",
    "getting",
    "changed",
    "second",
    "disadvantage",
    "probably",
    "talk",
    "third",
    "disadvantage",
    "okay",
    "third",
    "disadvantage",
    "go",
    "see",
    "respect",
    "vocabulary",
    "happens",
    "probably",
    "add",
    "new",
    "word",
    "like",
    "boy",
    "girl",
    "good",
    "let",
    "say",
    "going",
    "add",
    "something",
    "called",
    "school",
    "seeing",
    "school",
    "word",
    "present",
    "vocabulary",
    "going",
    "specific",
    "word",
    "anyhow",
    "going",
    "get",
    "rejected",
    "right",
    "getting",
    "considered",
    "training",
    "data",
    "let",
    "say",
    "new",
    "test",
    "data",
    "new",
    "test",
    "data",
    "included",
    "school",
    "word",
    "need",
    "prediction",
    "particular",
    "word",
    "respect",
    "output",
    "first",
    "step",
    "text",
    "preprocess",
    "try",
    "convert",
    "bag",
    "words",
    "using",
    "technique",
    "training",
    "data",
    "set",
    "see",
    "training",
    "data",
    "set",
    "vocabulary",
    "called",
    "school",
    "going",
    "going",
    "ignore",
    "specific",
    "word",
    "going",
    "see",
    "good",
    "boy",
    "girl",
    "right",
    "still",
    "vocabulary",
    "still",
    "exist",
    "word",
    "may",
    "important",
    "word",
    "sentence",
    "getting",
    "removed",
    "vocabulary",
    "right",
    "major",
    "problem",
    "yes",
    "vocabulary",
    "obviously",
    "issue",
    "right",
    "still",
    "persist",
    "okay",
    "oov",
    "one",
    "important",
    "thing",
    "semantic",
    "meaning",
    "still",
    "getting",
    "captured",
    "tell",
    "semantic",
    "meaning",
    "still",
    "getting",
    "captured",
    "multiple",
    "things",
    "explain",
    "okay",
    "first",
    "obviously",
    "know",
    "either",
    "ones",
    "zeros",
    "okay",
    "particular",
    "case",
    "good",
    "boy",
    "getting",
    "importance",
    "right",
    "girl",
    "obviously",
    "word",
    "present",
    "getting",
    "zero",
    "small",
    "amount",
    "semantic",
    "information",
    "getting",
    "captured",
    "compared",
    "one",
    "hot",
    "encoding",
    "format",
    "say",
    "many",
    "vocabularies",
    "either",
    "values",
    "zeros",
    "one",
    "indicating",
    "whether",
    "word",
    "present",
    "important",
    "word",
    "important",
    "context",
    "particular",
    "sentence",
    "obviously",
    "getting",
    "captured",
    "getting",
    "captured",
    "semantic",
    "turn",
    "get",
    "captured",
    "thing",
    "also",
    "important",
    "thing",
    "let",
    "say",
    "two",
    "sentence",
    "okay",
    "like",
    "food",
    "good",
    "let",
    "say",
    "data",
    "set",
    "sentence",
    "food",
    "good",
    "good",
    "let",
    "say",
    "go",
    "ahead",
    "remove",
    "stop",
    "wordss",
    "vocabulary",
    "like",
    "one",
    "uh",
    "one",
    "let",
    "say",
    "words",
    "okay",
    "also",
    "separate",
    "vocabulary",
    "food",
    "also",
    "separate",
    "vocabulary",
    "also",
    "separate",
    "many",
    "unique",
    "vocabulary",
    "four",
    "also",
    "right",
    "also",
    "become",
    "one",
    "zero",
    "good",
    "one",
    "right",
    "convert",
    "right",
    "similarly",
    "really",
    "need",
    "convert",
    "happen",
    "1",
    "one",
    "one",
    "one",
    "1",
    "also",
    "present",
    "writing",
    "one",
    "let",
    "say",
    "vector",
    "one",
    "vector",
    "2",
    "try",
    "find",
    "distance",
    "similar",
    "vector",
    "plotting",
    "points",
    "let",
    "say",
    "converted",
    "particular",
    "dimension",
    "two",
    "dimension",
    "using",
    "pc",
    "probably",
    "plotted",
    "based",
    "right",
    "one",
    "value",
    "getting",
    "changed",
    "right",
    "get",
    "vectors",
    "much",
    "near",
    "basically",
    "something",
    "called",
    "cosine",
    "similarity",
    "let",
    "say",
    "vector",
    "1",
    "vector",
    "1",
    "vector",
    "2",
    "vector",
    "1",
    "basically",
    "present",
    "vector",
    "2",
    "present",
    "near",
    "angle",
    "near",
    "angle",
    "less",
    "may",
    "say",
    "sentences",
    "almost",
    "similar",
    "right",
    "almost",
    "similar",
    "think",
    "sentences",
    "almost",
    "similar",
    "complete",
    "opposite",
    "right",
    "since",
    "one",
    "word",
    "getting",
    "changed",
    "one",
    "one",
    "values",
    "getting",
    "changed",
    "right",
    "like",
    "zeros",
    "ones",
    "happening",
    "plot",
    "becoming",
    "kind",
    "kind",
    "similar",
    "word",
    "similar",
    "word",
    "completely",
    "opposite",
    "word",
    "right",
    "kind",
    "situation",
    "also",
    "getting",
    "handled",
    "well",
    "bag",
    "words",
    "later",
    "techniques",
    "learning",
    "like",
    "uh",
    "word",
    "solving",
    "problems",
    "right",
    "hope",
    "able",
    "understand",
    "advantages",
    "disadvantages",
    "bag",
    "super",
    "important",
    "respect",
    "interview",
    "basics",
    "getting",
    "strong",
    "trust",
    "able",
    "understand",
    "bag",
    "words",
    "average",
    "word",
    "sorry",
    "able",
    "understand",
    "word",
    "average",
    "word",
    "easy",
    "manner",
    "techniques",
    "deep",
    "learning",
    "also",
    "going",
    "come",
    "called",
    "embedding",
    "techniques",
    "word",
    "embedding",
    "get",
    "solved",
    "easy",
    "way",
    "right",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "natural",
    "language",
    "processing",
    "previous",
    "video",
    "already",
    "seen",
    "nrs",
    "going",
    "going",
    "see",
    "one",
    "efficient",
    "way",
    "converting",
    "words",
    "vectors",
    "specifically",
    "say",
    "tf",
    "idf",
    "exactly",
    "tf",
    "idf",
    "nothing",
    "term",
    "frequency",
    "inverse",
    "document",
    "frequency",
    "try",
    "understand",
    "help",
    "help",
    "tf",
    "idf",
    "converting",
    "particular",
    "sentences",
    "vectors",
    "okay",
    "taken",
    "example",
    "specifically",
    "bag",
    "words",
    "like",
    "good",
    "boy",
    "lowering",
    "uh",
    "cases",
    "uh",
    "character",
    "cases",
    "along",
    "performing",
    "removing",
    "stop",
    "words",
    "sentence",
    "one",
    "good",
    "boy",
    "sentence",
    "two",
    "good",
    "girl",
    "sentence",
    "three",
    "boy",
    "girl",
    "good",
    "okay",
    "thing",
    "like",
    "uh",
    "materials",
    "able",
    "see",
    "done",
    "thing",
    "taken",
    "thing",
    "okay",
    "two",
    "components",
    "tfidf",
    "one",
    "term",
    "frequency",
    "one",
    "something",
    "called",
    "inverse",
    "document",
    "frequency",
    "whenever",
    "talk",
    "term",
    "frequency",
    "term",
    "frequency",
    "definition",
    "formula",
    "calculate",
    "given",
    "number",
    "repetition",
    "words",
    "sentence",
    "divided",
    "number",
    "words",
    "sentence",
    "okay",
    "try",
    "show",
    "completely",
    "taking",
    "example",
    "calculate",
    "term",
    "frequency",
    "inverse",
    "document",
    "frequency",
    "formula",
    "simple",
    "basically",
    "calculate",
    "inverse",
    "document",
    "frequency",
    "nothing",
    "log",
    "bas",
    "e",
    "number",
    "sentences",
    "divided",
    "number",
    "sentences",
    "containing",
    "word",
    "super",
    "important",
    "uh",
    "probably",
    "get",
    "confused",
    "formula",
    "right",
    "uh",
    "try",
    "explain",
    "everything",
    "okay",
    "let",
    "go",
    "step",
    "step",
    "let",
    "see",
    "calculate",
    "term",
    "frequency",
    "first",
    "thing",
    "going",
    "make",
    "sure",
    "calculate",
    "nothing",
    "using",
    "term",
    "frequency",
    "respect",
    "term",
    "frequency",
    "know",
    "many",
    "vocabulary",
    "words",
    "okay",
    "first",
    "uh",
    "try",
    "show",
    "different",
    "way",
    "creating",
    "table",
    "s1",
    "sentence",
    "s2",
    "sentence",
    "s3",
    "sentence",
    "okay",
    "respect",
    "vocabulary",
    "words",
    "something",
    "like",
    "good",
    "okay",
    "boy",
    "girl",
    "already",
    "know",
    "three",
    "words",
    "basically",
    "present",
    "vocabulary",
    "trying",
    "showing",
    "show",
    "simple",
    "example",
    "able",
    "understand",
    "tfidf",
    "work",
    "okay",
    "first",
    "thing",
    "let",
    "go",
    "back",
    "definition",
    "term",
    "frequency",
    "nothing",
    "number",
    "repetition",
    "words",
    "sentence",
    "divided",
    "number",
    "words",
    "sentence",
    "suppose",
    "take",
    "s1",
    "respect",
    "s1",
    "really",
    "want",
    "find",
    "term",
    "frequency",
    "particular",
    "word",
    "good",
    "calculate",
    "need",
    "see",
    "many",
    "number",
    "times",
    "particular",
    "word",
    "repeated",
    "sentence",
    "know",
    "repeated",
    "one",
    "time",
    "dividing",
    "number",
    "words",
    "specific",
    "sentence",
    "become",
    "1",
    "two",
    "two",
    "words",
    "let",
    "go",
    "next",
    "word",
    "boy",
    "boy",
    "also",
    "present",
    "many",
    "number",
    "times",
    "one",
    "divided",
    "two",
    "okay",
    "tell",
    "understand",
    "advantages",
    "disadvantages",
    "get",
    "clear",
    "idea",
    "tf",
    "idf",
    "better",
    "play",
    "perform",
    "perform",
    "better",
    "compared",
    "bag",
    "words",
    "okay",
    "word",
    "girl",
    "girl",
    "know",
    "present",
    "right",
    "sentence",
    "one",
    "0",
    "0",
    "by2",
    "nothing",
    "zero",
    "okay",
    "similarly",
    "respect",
    "s2",
    "able",
    "see",
    "many",
    "time",
    "good",
    "present",
    "one",
    "time",
    "total",
    "number",
    "words",
    "two",
    "1",
    "two",
    "boy",
    "present",
    "z",
    "times",
    "0",
    "by2",
    "nothing",
    "zero",
    "girl",
    "basically",
    "present",
    "one",
    "time",
    "going",
    "write",
    "1",
    "2",
    "let",
    "go",
    "respect",
    "sentence",
    "three",
    "sentence",
    "three",
    "many",
    "times",
    "good",
    "present",
    "one",
    "time",
    "total",
    "number",
    "words",
    "three",
    "right",
    "go",
    "next",
    "uh",
    "see",
    "boy",
    "word",
    "many",
    "times",
    "boy",
    "present",
    "one",
    "time",
    "also",
    "1x3",
    "girl",
    "also",
    "present",
    "1",
    "by3",
    "total",
    "number",
    "words",
    "three",
    "okay",
    "simple",
    "able",
    "calculate",
    "term",
    "frequency",
    "okay",
    "let",
    "go",
    "ahead",
    "let",
    "try",
    "find",
    "inverse",
    "document",
    "frequency",
    "going",
    "write",
    "idf",
    "respect",
    "idf",
    "also",
    "creating",
    "two",
    "fields",
    "simple",
    "fields",
    "remember",
    "uh",
    "basically",
    "idf",
    "respect",
    "words",
    "words",
    "nothing",
    "good",
    "boy",
    "let",
    "write",
    "better",
    "way",
    "look",
    "order",
    "good",
    "boy",
    "next",
    "word",
    "something",
    "like",
    "girl",
    "order",
    "calculate",
    "inverse",
    "document",
    "frequency",
    "much",
    "simple",
    "apply",
    "log",
    "base",
    "e",
    "right",
    "many",
    "number",
    "sentences",
    "respect",
    "good",
    "right",
    "respect",
    "good",
    "uh",
    "suppose",
    "really",
    "want",
    "calculate",
    "inverse",
    "document",
    "frequency",
    "good",
    "okay",
    "writing",
    "write",
    "log",
    "basee",
    "e",
    "multiplied",
    "number",
    "sentences",
    "many",
    "sentences",
    "three",
    "sentences",
    "three",
    "divided",
    "number",
    "sentences",
    "containing",
    "word",
    "good",
    "present",
    "three",
    "sentence",
    "1",
    "2",
    "3",
    "right",
    "basically",
    "writing",
    "log",
    "base",
    "3",
    "3",
    "okay",
    "basically",
    "calculate",
    "try",
    "calculate",
    "nothing",
    "getting",
    "zero",
    "okay",
    "basically",
    "calculator",
    "boy",
    "log",
    "base",
    "e",
    "number",
    "sentences",
    "three",
    "many",
    "time",
    "boy",
    "present",
    "many",
    "time",
    "boy",
    "present",
    "uh",
    "many",
    "sentences",
    "boy",
    "basically",
    "present",
    "present",
    "one",
    "two",
    "right",
    "sentence",
    "one",
    "sentence",
    "three",
    "writing",
    "log",
    "based",
    "uh",
    "base",
    "e",
    "multiplied",
    "3",
    "2",
    "similarly",
    "girl",
    "also",
    "present",
    "number",
    "time",
    "probably",
    "see",
    "many",
    "sentences",
    "girl",
    "basically",
    "present",
    "okay",
    "independent",
    "dependently",
    "calculated",
    "term",
    "frequency",
    "independently",
    "calculated",
    "inverse",
    "document",
    "frequency",
    "perfect",
    "right",
    "whenever",
    "say",
    "tf",
    "idf",
    "short",
    "actually",
    "multiplying",
    "two",
    "okay",
    "term",
    "frequency",
    "inverse",
    "document",
    "taking",
    "combination",
    "two",
    "let",
    "go",
    "ahead",
    "write",
    "better",
    "way",
    "still",
    "uh",
    "way",
    "specifically",
    "want",
    "finally",
    "vectors",
    "look",
    "like",
    "vocabulary",
    "good",
    "boy",
    "girl",
    "final",
    "uh",
    "tf",
    "idf",
    "okay",
    "final",
    "tf",
    "idf",
    "based",
    "calculation",
    "differ",
    "based",
    "data",
    "set",
    "data",
    "set",
    "okay",
    "first",
    "respect",
    "sentence",
    "one",
    "whenever",
    "see",
    "combination",
    "tf",
    "idf",
    "respect",
    "good",
    "multiply",
    "1",
    "two",
    "zero",
    "okay",
    "going",
    "multiply",
    "sentence",
    "one",
    "see",
    "sentence",
    "one",
    "right",
    "entire",
    "thing",
    "sentence",
    "one",
    "one",
    "right",
    "sentence",
    "one",
    "okay",
    "taking",
    "combination",
    "multiply",
    "right",
    "sentence",
    "one",
    "good",
    "1x2",
    "0",
    "nothing",
    "0",
    "respect",
    "boy",
    "1x2",
    "log",
    "base",
    "e",
    "1x2",
    "log",
    "base",
    "e",
    "3x2",
    "value",
    "getting",
    "sentence",
    "one",
    "respect",
    "girl",
    "0",
    "multiplied",
    "zero",
    "let",
    "go",
    "sentence",
    "two",
    "sentence",
    "two",
    "go",
    "ahead",
    "look",
    "1x",
    "2",
    "0",
    "good",
    "0",
    "boy",
    "nothing",
    "0",
    "0",
    "0",
    "0",
    "nothing",
    "0",
    "1x",
    "2",
    "log",
    "base",
    "e",
    "3x2",
    "tell",
    "exact",
    "thing",
    "really",
    "need",
    "know",
    "specific",
    "thing",
    "everything",
    "make",
    "sense",
    "uh",
    "make",
    "sense",
    "make",
    "definitely",
    "understand",
    "things",
    "okay",
    "coming",
    "next",
    "one",
    "respect",
    "sentence",
    "three",
    "sentence",
    "three",
    "multiplication",
    "right",
    "1x",
    "3",
    "multiped",
    "0",
    "0",
    "1x",
    "3",
    "multiplied",
    "log",
    "base",
    "e",
    "3x",
    "2",
    "1x",
    "3",
    "log",
    "base",
    "e",
    "3x2",
    "perfect",
    "got",
    "calculation",
    "vectors",
    "look",
    "like",
    "seeing",
    "sentence",
    "one",
    "converted",
    "entire",
    "sentence",
    "vectors",
    "looks",
    "like",
    "right",
    "sentence",
    "one",
    "vector",
    "sentence",
    "2",
    "vector",
    "sentence",
    "three",
    "vector",
    "obviously",
    "output",
    "respect",
    "kind",
    "classification",
    "want",
    "train",
    "model",
    "passing",
    "sentence",
    "one",
    "short",
    "see",
    "good",
    "boy",
    "basically",
    "converted",
    "vector",
    "look",
    "like",
    "okay",
    "entire",
    "sentence",
    "one",
    "getting",
    "converted",
    "vector",
    "0o",
    "0",
    "right",
    "calculate",
    "help",
    "calculator",
    "way",
    "actually",
    "done",
    "converted",
    "sentences",
    "vectors",
    "tfidf",
    "phenomenon",
    "used",
    "converting",
    "words",
    "vectors",
    "may",
    "thinking",
    "kish",
    "special",
    "got",
    "values",
    "okay",
    "fine",
    "going",
    "talk",
    "next",
    "video",
    "advantages",
    "disadvantages",
    "tfidf",
    "hello",
    "guys",
    "going",
    "continue",
    "disc",
    "discussion",
    "respect",
    "tfidf",
    "already",
    "shown",
    "formula",
    "tfidf",
    "term",
    "frequency",
    "idf",
    "iners",
    "document",
    "frequency",
    "also",
    "shown",
    "example",
    "right",
    "till",
    "everything",
    "fine",
    "let",
    "talk",
    "important",
    "thing",
    "advantages",
    "disadvantages",
    "probably",
    "better",
    "bag",
    "words",
    "okay",
    "first",
    "uh",
    "basic",
    "advantage",
    "quite",
    "intuitive",
    "uh",
    "implementation",
    "also",
    "quite",
    "intuitive",
    "uh",
    "coming",
    "second",
    "advantage",
    "okay",
    "like",
    "bag",
    "words",
    "uh",
    "also",
    "inputs",
    "basically",
    "fixed",
    "size",
    "based",
    "wab",
    "size",
    "right",
    "advantage",
    "also",
    "present",
    "respect",
    "bag",
    "words",
    "also",
    "third",
    "advantage",
    "actually",
    "going",
    "talk",
    "see",
    "bag",
    "words",
    "also",
    "fixed",
    "size",
    "right",
    "third",
    "advantage",
    "major",
    "advantage",
    "let",
    "talk",
    "third",
    "advantage",
    "okay",
    "third",
    "advantage",
    "word",
    "importance",
    "getting",
    "captured",
    "explain",
    "exactly",
    "word",
    "importance",
    "getting",
    "captured",
    "super",
    "important",
    "point",
    "probably",
    "may",
    "also",
    "ask",
    "specific",
    "thing",
    "interviews",
    "probably",
    "go",
    "see",
    "entire",
    "paragraph",
    "let",
    "say",
    "paragraph",
    "good",
    "boy",
    "good",
    "girl",
    "boy",
    "girl",
    "good",
    "right",
    "getting",
    "tfidf",
    "number",
    "right",
    "also",
    "written",
    "help",
    "bag",
    "words",
    "bag",
    "words",
    "used",
    "get",
    "either",
    "ones",
    "zeros",
    "wherever",
    "word",
    "present",
    "coming",
    "one",
    "otherwise",
    "zero",
    "present",
    "sentence",
    "word",
    "importance",
    "getting",
    "captured",
    "equal",
    "importance",
    "given",
    "word",
    "like",
    "good",
    "boy",
    "right",
    "present",
    "sentences",
    "work",
    "like",
    "considering",
    "entire",
    "paragraph",
    "happening",
    "focusing",
    "two",
    "things",
    "term",
    "frequency",
    "inverse",
    "document",
    "frequency",
    "word",
    "present",
    "sentences",
    "given",
    "less",
    "importance",
    "understand",
    "okay",
    "word",
    "present",
    "sentences",
    "paragraph",
    "given",
    "less",
    "importance",
    "sentences",
    "specific",
    "word",
    "playing",
    "amazing",
    "important",
    "role",
    "word",
    "importance",
    "needs",
    "captured",
    "every",
    "sentence",
    "specifically",
    "want",
    "see",
    "boy",
    "right",
    "girl",
    "boy",
    "girl",
    "getting",
    "repeated",
    "one",
    "two",
    "sentences",
    "every",
    "sentences",
    "repeated",
    "every",
    "sentences",
    "need",
    "value",
    "particular",
    "word",
    "every",
    "sentences",
    "probably",
    "take",
    "example",
    "good",
    "good",
    "present",
    "three",
    "sentences",
    "calculate",
    "tfidf",
    "seeing",
    "zeros",
    "getting",
    "major",
    "major",
    "issue",
    "right",
    "issue",
    "good",
    "thing",
    "ignoring",
    "good",
    "word",
    "present",
    "sentence",
    "consider",
    "respect",
    "boy",
    "sentence",
    "one",
    "boy",
    "play",
    "important",
    "role",
    "right",
    "respect",
    "boy",
    "seeing",
    "getting",
    "values",
    "right",
    "getting",
    "values",
    "second",
    "sentence",
    "obviously",
    "boy",
    "became",
    "zero",
    "consider",
    "girl",
    "second",
    "sentence",
    "seeing",
    "getting",
    "value",
    "basically",
    "means",
    "particular",
    "sentence",
    "girl",
    "word",
    "super",
    "important",
    "context",
    "based",
    "specific",
    "word",
    "value",
    "tf",
    "idf",
    "right",
    "short",
    "happening",
    "word",
    "importance",
    "getting",
    "captured",
    "third",
    "senten",
    "talking",
    "boy",
    "girl",
    "seeing",
    "boy",
    "girl",
    "values",
    "short",
    "capturing",
    "word",
    "importance",
    "based",
    "context",
    "right",
    "super",
    "important",
    "point",
    "machine",
    "learning",
    "model",
    "able",
    "understand",
    "okay",
    "something",
    "specific",
    "basically",
    "talking",
    "way",
    "mathematical",
    "models",
    "able",
    "find",
    "kind",
    "predictions",
    "actually",
    "accuracy",
    "increases",
    "let",
    "talk",
    "disadvantages",
    "obviously",
    "particular",
    "case",
    "also",
    "lot",
    "number",
    "zeros",
    "sparsity",
    "still",
    "exist",
    "okay",
    "sparity",
    "still",
    "exist",
    "uh",
    "try",
    "see",
    "solve",
    "sparsity",
    "using",
    "word",
    "second",
    "thing",
    "specifically",
    "discuss",
    "something",
    "called",
    "oov",
    "vocabulary",
    "also",
    "probably",
    "add",
    "words",
    "respect",
    "test",
    "data",
    "going",
    "get",
    "ignored",
    "uh",
    "also",
    "features",
    "basically",
    "made",
    "based",
    "training",
    "vocabulary",
    "size",
    "right",
    "basically",
    "advantages",
    "disadvantages",
    "respect",
    "uh",
    "tfidf",
    "definitely",
    "seeing",
    "advantages",
    "disadvantage",
    "definitely",
    "know",
    "tfidf",
    "performs",
    "better",
    "bag",
    "words",
    "right",
    "uh",
    "next",
    "video",
    "try",
    "see",
    "practical",
    "uh",
    "implementation",
    "help",
    "nltk",
    "python",
    "guys",
    "really",
    "need",
    "practice",
    "considering",
    "different",
    "different",
    "data",
    "sets",
    "try",
    "provide",
    "assignments",
    "possible",
    "practice",
    "things",
    "also",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "natural",
    "language",
    "processing",
    "machine",
    "learning",
    "video",
    "going",
    "discuss",
    "word",
    "embeddings",
    "probably",
    "topic",
    "covered",
    "long",
    "back",
    "deliberately",
    "keeping",
    "particular",
    "topic",
    "point",
    "time",
    "discussed",
    "many",
    "topics",
    "wherein",
    "focused",
    "converting",
    "word",
    "vectors",
    "getting",
    "clear",
    "idea",
    "exactly",
    "word",
    "embeddings",
    "given",
    "wikipedia",
    "definition",
    "simple",
    "wikipedia",
    "definition",
    "taken",
    "wikipedia",
    "entire",
    "credit",
    "goes",
    "wikipedia",
    "see",
    "definition",
    "natural",
    "language",
    "processing",
    "word",
    "embeddings",
    "term",
    "used",
    "representation",
    "words",
    "right",
    "text",
    "analysis",
    "typically",
    "form",
    "real",
    "valued",
    "vectors",
    "encodes",
    "meaning",
    "word",
    "word",
    "closer",
    "vector",
    "space",
    "expected",
    "similar",
    "meaning",
    "let",
    "say",
    "two",
    "words",
    "king",
    "queen",
    "okay",
    "forget",
    "king",
    "queen",
    "let",
    "say",
    "two",
    "words",
    "two",
    "word",
    "like",
    "happy",
    "excited",
    "right",
    "let",
    "say",
    "two",
    "specific",
    "word",
    "two",
    "specific",
    "word",
    "help",
    "word",
    "embedding",
    "techniques",
    "convert",
    "particular",
    "word",
    "vectors",
    "let",
    "say",
    "try",
    "plot",
    "vectors",
    "graph",
    "okay",
    "mean",
    "really",
    "want",
    "convert",
    "two",
    "dimensional",
    "cft",
    "techniques",
    "like",
    "pca",
    "techniques",
    "unsupervised",
    "technique",
    "dimensionality",
    "deduction",
    "probably",
    "plot",
    "let",
    "say",
    "happy",
    "excited",
    "coming",
    "near",
    "based",
    "particular",
    "vectors",
    "basically",
    "indicates",
    "similar",
    "word",
    "okay",
    "let",
    "say",
    "one",
    "word",
    "like",
    "angry",
    "case",
    "help",
    "word",
    "embeddings",
    "trying",
    "convert",
    "vectors",
    "tentative",
    "thing",
    "obviously",
    "happy",
    "opposite",
    "angry",
    "angry",
    "somewhere",
    "probably",
    "try",
    "plot",
    "particular",
    "uh",
    "vectors",
    "coming",
    "far",
    "opposite",
    "word",
    "distance",
    "word",
    "quite",
    "high",
    "whereas",
    "distance",
    "particular",
    "word",
    "quite",
    "less",
    "indicates",
    "words",
    "similar",
    "whereas",
    "words",
    "opposite",
    "right",
    "possible",
    "efficient",
    "conversion",
    "word",
    "vectors",
    "right",
    "help",
    "word",
    "embeddings",
    "techniques",
    "learned",
    "till",
    "something",
    "like",
    "one",
    "hot",
    "encoded",
    "learned",
    "bag",
    "words",
    "learned",
    "tfidf",
    "learned",
    "techniques",
    "part",
    "word",
    "embeddings",
    "properly",
    "clearly",
    "show",
    "division",
    "know",
    "first",
    "step",
    "word",
    "embedding",
    "techniques",
    "let",
    "go",
    "ahead",
    "write",
    "word",
    "embedding",
    "techniques",
    "specifically",
    "two",
    "types",
    "first",
    "type",
    "second",
    "type",
    "first",
    "type",
    "based",
    "count",
    "frequency",
    "count",
    "frequency",
    "second",
    "type",
    "based",
    "deep",
    "learning",
    "trained",
    "models",
    "please",
    "hear",
    "properly",
    "deep",
    "learning",
    "trained",
    "model",
    "models",
    "give",
    "good",
    "accuracies",
    "okay",
    "count",
    "frequency",
    "learned",
    "three",
    "different",
    "types",
    "one",
    "one",
    "hot",
    "encoded",
    "second",
    "one",
    "something",
    "called",
    "bag",
    "words",
    "third",
    "one",
    "something",
    "called",
    "tfidf",
    "right",
    "techniques",
    "learned",
    "right",
    "know",
    "advantages",
    "disadvantages",
    "focusing",
    "count",
    "frequency",
    "right",
    "major",
    "one",
    "better",
    "accuracy",
    "actually",
    "techniques",
    "end",
    "day",
    "also",
    "converting",
    "words",
    "vectors",
    "converting",
    "sentence",
    "vectors",
    "right",
    "sentence",
    "vectors",
    "seen",
    "lot",
    "advantages",
    "disadvantages",
    "maximum",
    "number",
    "disadvantages",
    "respect",
    "techniques",
    "disadvantages",
    "getting",
    "solved",
    "deep",
    "learning",
    "trained",
    "model",
    "train",
    "model",
    "nothing",
    "basically",
    "called",
    "word",
    "w",
    "like",
    "create",
    "scratch",
    "definitely",
    "create",
    "scratch",
    "require",
    "huge",
    "amount",
    "data",
    "going",
    "first",
    "going",
    "understand",
    "upcoming",
    "videos",
    "uh",
    "exactly",
    "word",
    "w",
    "basically",
    "converting",
    "word",
    "vectors",
    "solving",
    "disadvantage",
    "things",
    "particular",
    "technique",
    "everything",
    "going",
    "discuss",
    "understand",
    "word",
    "word",
    "embedding",
    "technique",
    "efficiently",
    "convert",
    "word",
    "vectors",
    "making",
    "sure",
    "property",
    "expected",
    "similar",
    "meaning",
    "whenever",
    "converting",
    "vector",
    "space",
    "along",
    "also",
    "give",
    "good",
    "representation",
    "words",
    "right",
    "sparity",
    "many",
    "points",
    "going",
    "discuss",
    "upcoming",
    "uh",
    "videos",
    "respect",
    "word",
    "word",
    "two",
    "types",
    "one",
    "entire",
    "deep",
    "learning",
    "architecture",
    "built",
    "two",
    "different",
    "types",
    "one",
    "basically",
    "say",
    "cball",
    "c",
    "ball",
    "cow",
    "nothing",
    "continuous",
    "bag",
    "words",
    "super",
    "important",
    "continuous",
    "bag",
    "words",
    "also",
    "going",
    "see",
    "models",
    "get",
    "strained",
    "really",
    "need",
    "prerequisite",
    "knowledge",
    "ann",
    "works",
    "loss",
    "function",
    "optimizers",
    "right",
    "second",
    "technique",
    "something",
    "called",
    "skip",
    "gram",
    "skip",
    "gram",
    "different",
    "technique",
    "uh",
    "part",
    "word",
    "different",
    "type",
    "word",
    "end",
    "day",
    "either",
    "use",
    "cbo",
    "skip",
    "gram",
    "get",
    "efficient",
    "conversion",
    "word",
    "vectors",
    "right",
    "going",
    "see",
    "also",
    "going",
    "see",
    "models",
    "word",
    "vc",
    "know",
    "probably",
    "created",
    "google",
    "somewhere",
    "around",
    "gb",
    "big",
    "model",
    "size",
    "try",
    "download",
    "try",
    "see",
    "try",
    "execute",
    "upcoming",
    "videos",
    "going",
    "see",
    "word",
    "w",
    "word",
    "embedding",
    "works",
    "right",
    "making",
    "sure",
    "disadvantages",
    "present",
    "techniques",
    "getting",
    "removed",
    "right",
    "going",
    "discuss",
    "hope",
    "got",
    "idea",
    "end",
    "day",
    "whatever",
    "techniques",
    "discussed",
    "till",
    "converting",
    "word",
    "vectors",
    "falls",
    "word",
    "embeddings",
    "right",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "natural",
    "language",
    "processing",
    "video",
    "going",
    "cover",
    "word",
    "w",
    "already",
    "seen",
    "exactly",
    "beck",
    "deep",
    "learning",
    "train",
    "model",
    "kind",
    "word",
    "embedding",
    "techniques",
    "wherein",
    "focus",
    "convert",
    "word",
    "vectors",
    "making",
    "sure",
    "meaning",
    "different",
    "different",
    "words",
    "actually",
    "maintained",
    "like",
    "similar",
    "words",
    "getting",
    "vectors",
    "near",
    "probably",
    "find",
    "difference",
    "also",
    "able",
    "see",
    "words",
    "completely",
    "opposite",
    "based",
    "vectors",
    "okay",
    "let",
    "discuss",
    "word",
    "w",
    "give",
    "idea",
    "like",
    "exactly",
    "word",
    "w",
    "words",
    "getting",
    "converted",
    "vectors",
    "upcoming",
    "videos",
    "try",
    "show",
    "word",
    "w",
    "models",
    "basically",
    "prepared",
    "respect",
    "architecture",
    "uh",
    "case",
    "deep",
    "learning",
    "models",
    "really",
    "need",
    "knowledge",
    "n",
    "models",
    "right",
    "really",
    "need",
    "understand",
    "train",
    "word",
    "w",
    "scratch",
    "okay",
    "uh",
    "yeah",
    "let",
    "go",
    "ahead",
    "definition",
    "let",
    "see",
    "problems",
    "actually",
    "fixes",
    "word",
    "w",
    "technique",
    "natural",
    "language",
    "process",
    "published",
    "2013",
    "published",
    "google",
    "amazing",
    "company",
    "already",
    "know",
    "done",
    "tons",
    "work",
    "respect",
    "nlp",
    "know",
    "lot",
    "research",
    "word",
    "algorithm",
    "uses",
    "neural",
    "network",
    "model",
    "discussing",
    "using",
    "uh",
    "uses",
    "neural",
    "network",
    "learn",
    "word",
    "association",
    "please",
    "make",
    "sure",
    "understand",
    "words",
    "learn",
    "word",
    "association",
    "large",
    "corpus",
    "text",
    "trained",
    "model",
    "detect",
    "synonym",
    "words",
    "suggest",
    "additional",
    "words",
    "partial",
    "sentence",
    "able",
    "detect",
    "synonyms",
    "able",
    "detect",
    "uh",
    "opposite",
    "words",
    "many",
    "things",
    "name",
    "implies",
    "word",
    "w",
    "represents",
    "distinct",
    "word",
    "particular",
    "list",
    "number",
    "called",
    "vectors",
    "end",
    "day",
    "converting",
    "word",
    "vectors",
    "vector",
    "many",
    "things",
    "able",
    "detect",
    "synonym",
    "words",
    "also",
    "able",
    "suggest",
    "additional",
    "words",
    "partial",
    "sentence",
    "okay",
    "let",
    "understand",
    "exactly",
    "see",
    "guys",
    "bag",
    "words",
    "tf",
    "idf",
    "already",
    "seen",
    "right",
    "based",
    "vocabulary",
    "size",
    "either",
    "get",
    "zeros",
    "ones",
    "zeros",
    "short",
    "getting",
    "sparse",
    "matrix",
    "tf",
    "idf",
    "also",
    "may",
    "get",
    "decimals",
    "like0",
    "2",
    "zer",
    "zeros",
    "word",
    "little",
    "bit",
    "different",
    "let",
    "talk",
    "different",
    "let",
    "consider",
    "vocabulary",
    "okay",
    "vocabulary",
    "let",
    "say",
    "vocabulary",
    "basically",
    "many",
    "number",
    "unique",
    "words",
    "corpus",
    "okay",
    "unique",
    "words",
    "corpus",
    "corpus",
    "basically",
    "means",
    "paragraph",
    "okay",
    "let",
    "say",
    "uh",
    "vocabulary",
    "words",
    "specifically",
    "like",
    "something",
    "like",
    "boy",
    "girl",
    "okay",
    "something",
    "like",
    "king",
    "queen",
    "probably",
    "talk",
    "words",
    "like",
    "apple",
    "mango",
    "let",
    "say",
    "vocabulary",
    "many",
    "words",
    "okay",
    "one",
    "important",
    "word",
    "going",
    "put",
    "called",
    "feature",
    "representation",
    "feature",
    "representation",
    "please",
    "listen",
    "carefully",
    "important",
    "topic",
    "every",
    "word",
    "present",
    "vocabulary",
    "conver",
    "converted",
    "feature",
    "representation",
    "exactly",
    "means",
    "basically",
    "means",
    "going",
    "convert",
    "words",
    "vectors",
    "based",
    "features",
    "features",
    "right",
    "let",
    "give",
    "good",
    "example",
    "understand",
    "training",
    "big",
    "word",
    "w",
    "model",
    "time",
    "getting",
    "clear",
    "idea",
    "features",
    "make",
    "intuition",
    "make",
    "understand",
    "intuition",
    "let",
    "discuss",
    "feature",
    "representation",
    "left",
    "hand",
    "side",
    "happen",
    "let",
    "say",
    "lot",
    "features",
    "like",
    "may",
    "feature",
    "called",
    "gender",
    "may",
    "feature",
    "called",
    "royal",
    "may",
    "feature",
    "called",
    "age",
    "may",
    "feature",
    "called",
    "food",
    "like",
    "lot",
    "features",
    "let",
    "say",
    "total",
    "number",
    "features",
    "know",
    "basically",
    "300",
    "dimension",
    "basically",
    "means",
    "one",
    "nth",
    "feature",
    "size",
    "entire",
    "features",
    "like",
    "probably",
    "count",
    "300",
    "dimensions",
    "let",
    "consider",
    "300",
    "dimension",
    "basically",
    "means",
    "300",
    "features",
    "respect",
    "vocabulary",
    "going",
    "represent",
    "word",
    "form",
    "vector",
    "considering",
    "feature",
    "representation",
    "entire",
    "feature",
    "okay",
    "actually",
    "going",
    "going",
    "take",
    "words",
    "present",
    "vocabulary",
    "based",
    "particular",
    "features",
    "going",
    "ass",
    "assign",
    "one",
    "numerical",
    "value",
    "okay",
    "understand",
    "one",
    "thing",
    "numerical",
    "value",
    "assigned",
    "based",
    "relation",
    "two",
    "words",
    "vocabulary",
    "feature",
    "representation",
    "given",
    "example",
    "training",
    "large",
    "word",
    "model",
    "able",
    "see",
    "features",
    "entirely",
    "right",
    "take",
    "example",
    "google",
    "know",
    "come",
    "amazing",
    "word",
    "w",
    "model",
    "basically",
    "trained",
    "3",
    "billion",
    "words",
    "guess",
    "3",
    "billion",
    "ion",
    "words",
    "coming",
    "news",
    "feed",
    "right",
    "practical",
    "example",
    "also",
    "try",
    "show",
    "end",
    "day",
    "able",
    "see",
    "every",
    "word",
    "basically",
    "represented",
    "300",
    "dimension",
    "feature",
    "representation",
    "basically",
    "every",
    "word",
    "300",
    "dimension",
    "uh",
    "vectors",
    "okay",
    "super",
    "important",
    "kind",
    "values",
    "right",
    "kind",
    "values",
    "let",
    "say",
    "respect",
    "bo",
    "present",
    "vocabulary",
    "okay",
    "relationship",
    "respect",
    "gender",
    "let",
    "say",
    "one",
    "minus",
    "one",
    "let",
    "let",
    "let",
    "consider",
    "okay",
    "minus",
    "one",
    "respect",
    "girl",
    "gender",
    "value",
    "one",
    "opposite",
    "boy",
    "right",
    "opposite",
    "respect",
    "gender",
    "say",
    "boy",
    "opposite",
    "boy",
    "nothing",
    "girl",
    "minus",
    "one",
    "one",
    "kind",
    "vectors",
    "come",
    "see",
    "respect",
    "next",
    "word",
    "boy",
    "royal",
    "obviously",
    "know",
    "say",
    "right",
    "sentence",
    "oh",
    "royal",
    "boy",
    "royal",
    "prince",
    "royal",
    "king",
    "right",
    "proper",
    "relationship",
    "particular",
    "case",
    "know",
    "value",
    "like",
    "01",
    "giving",
    "example",
    "right",
    "similarly",
    "also",
    "respect",
    "boy",
    "age",
    "let",
    "say",
    "much",
    "relation",
    "going",
    "put",
    "03",
    "okay",
    "near",
    "zero",
    "similarly",
    "values",
    "values",
    "comes",
    "proper",
    "trained",
    "models",
    "like",
    "word",
    "word",
    "trained",
    "deep",
    "learning",
    "techniques",
    "like",
    "ann",
    "okay",
    "show",
    "next",
    "video",
    "models",
    "basically",
    "trained",
    "understand",
    "every",
    "vocabulary",
    "seeing",
    "represented",
    "based",
    "feature",
    "representation",
    "basically",
    "means",
    "boy",
    "vector",
    "boy",
    "boy",
    "see",
    "specific",
    "vector",
    "okay",
    "vectors",
    "okay",
    "okay",
    "similarly",
    "respect",
    "girl",
    "see",
    "respect",
    "gender",
    "boy",
    "minus",
    "one",
    "one",
    "right",
    "completely",
    "opposite",
    "respect",
    "royal",
    "relationship",
    "02",
    "let",
    "say",
    "going",
    "put",
    "02",
    "age",
    "respect",
    "also",
    "specific",
    "relationship",
    "right",
    "similarly",
    "values",
    "like",
    "see",
    "respect",
    "king",
    "know",
    "may",
    "gender",
    "like",
    "uh",
    "relationship",
    "respect",
    "gender",
    "king",
    "minus",
    "92",
    "respect",
    "quin",
    "plus",
    "n3",
    "know",
    "see",
    "opposite",
    "right",
    "opposite",
    "opposite",
    "right",
    "similarly",
    "vectors",
    "like",
    "royal",
    "king",
    "related",
    "right",
    "95",
    "okay",
    "see",
    "also",
    "royal",
    "queen",
    "also",
    "royal",
    "right",
    "probably",
    "value",
    "9697",
    "much",
    "near",
    "right",
    "understand",
    "vectors",
    "know",
    "similar",
    "words",
    "close",
    "try",
    "subtract",
    "give",
    "idea",
    "right",
    "respect",
    "age",
    "also",
    "obviously",
    "relationship",
    "age",
    "king",
    "say",
    "old",
    "king",
    "right",
    "respect",
    "age",
    "suppose",
    "let",
    "say",
    "75",
    "68",
    "like",
    "multiple",
    "vectors",
    "like",
    "right",
    "clearly",
    "see",
    "right",
    "see",
    "respect",
    "apple",
    "obviously",
    "relationship",
    "respect",
    "gend",
    "uh",
    "gender",
    "probably",
    "write",
    "01",
    "something",
    "right",
    "respect",
    "mango",
    "also",
    "write",
    "23",
    "saying",
    "okay",
    "basically",
    "uh",
    "values",
    "relation",
    "much",
    "near",
    "zero",
    "like",
    "05",
    "okay",
    "putting",
    "values",
    "train",
    "models",
    "right",
    "getting",
    "value",
    "giving",
    "crux",
    "idea",
    "like",
    "every",
    "vectors",
    "may",
    "look",
    "like",
    "okay",
    "respect",
    "apple",
    "roy",
    "also",
    "much",
    "relationship",
    "let",
    "say",
    "putting",
    "plus2",
    "respect",
    "apple",
    "may",
    "good",
    "relationship",
    "right",
    "uh",
    "let",
    "say",
    "apple",
    "kept",
    "10",
    "days",
    "outside",
    "know",
    "may",
    "may",
    "may",
    "rotten",
    "right",
    "may",
    "nutritional",
    "value",
    "uh",
    "respect",
    "age",
    "may",
    "direct",
    "relationship",
    "mango",
    "also",
    "may",
    "direct",
    "relationship",
    "going",
    "vectors",
    "pretty",
    "much",
    "similar",
    "respect",
    "food",
    "yes",
    "apple",
    "belongs",
    "food",
    "item",
    "right",
    "probably",
    "good",
    "value",
    "may",
    "91",
    "also",
    "right",
    "similarly",
    "different",
    "different",
    "vectors",
    "right",
    "done",
    "every",
    "vocabulary",
    "word",
    "represented",
    "based",
    "feature",
    "representation",
    "right",
    "feature",
    "representation",
    "may",
    "300",
    "dimensions",
    "100",
    "dimension",
    "different",
    "different",
    "dimensions",
    "right",
    "say",
    "showing",
    "example",
    "respect",
    "google",
    "features",
    "also",
    "exactly",
    "known",
    "consider",
    "giving",
    "intuitive",
    "example",
    "yes",
    "based",
    "relationship",
    "respect",
    "word",
    "able",
    "get",
    "specific",
    "vectors",
    "lot",
    "advantages",
    "respect",
    "know",
    "saying",
    "suppose",
    "let",
    "say",
    "calculation",
    "like",
    "king",
    "minus",
    "man",
    "plus",
    "queen",
    "calculation",
    "famous",
    "calculation",
    "also",
    "written",
    "google",
    "research",
    "paper",
    "probably",
    "calculation",
    "output",
    "going",
    "get",
    "something",
    "called",
    "woman",
    "okay",
    "definitely",
    "get",
    "human",
    "oh",
    "human",
    "okay",
    "let",
    "remove",
    "suppose",
    "say",
    "king",
    "minus",
    "boy",
    "plus",
    "queen",
    "output",
    "going",
    "get",
    "see",
    "king",
    "vector",
    "right",
    "subtracting",
    "boy",
    "adding",
    "queen",
    "end",
    "day",
    "seeing",
    "girl",
    "much",
    "related",
    "boy",
    "going",
    "get",
    "output",
    "girl",
    "vector",
    "calculation",
    "kind",
    "relations",
    "able",
    "get",
    "know",
    "amazing",
    "thing",
    "getting",
    "kind",
    "relation",
    "seeing",
    "vectors",
    "provided",
    "word",
    "going",
    "calculation",
    "randomly",
    "stuffed",
    "values",
    "real",
    "word",
    "use",
    "case",
    "seeing",
    "kind",
    "calculation",
    "going",
    "get",
    "girl",
    "show",
    "practically",
    "also",
    "go",
    "ahead",
    "use",
    "google",
    "uh",
    "google",
    "word",
    "basically",
    "train",
    "3",
    "billion",
    "words",
    "quite",
    "amazing",
    "right",
    "let",
    "give",
    "another",
    "example",
    "let",
    "say",
    "instead",
    "using",
    "300",
    "dimension",
    "represent",
    "every",
    "word",
    "two",
    "dimension",
    "let",
    "say",
    "9596",
    "man",
    "represented",
    "something",
    "like",
    "9598",
    "let",
    "let",
    "represent",
    "like",
    "okay",
    "giving",
    "values",
    "okay",
    "meaningful",
    "values",
    "get",
    "idea",
    "let",
    "say",
    "queen",
    "given",
    "as96",
    "let",
    "say",
    "opposite",
    "right",
    "similar",
    "keyword",
    "uh",
    "similar",
    "vocabulary",
    "respect",
    "human",
    "let",
    "say",
    "something",
    "like",
    "94",
    "96",
    "let",
    "say",
    "calculation",
    "king",
    "minus",
    "man",
    "plus",
    "queen",
    "going",
    "get",
    "human",
    "right",
    "output",
    "vectors",
    "represent",
    "also",
    "really",
    "need",
    "understand",
    "super",
    "super",
    "important",
    "going",
    "discuss",
    "something",
    "called",
    "cosine",
    "similarity",
    "okay",
    "cosine",
    "similarity",
    "super",
    "important",
    "topic",
    "respect",
    "understanding",
    "things",
    "understand",
    "things",
    "see",
    "king",
    "given",
    "two",
    "two",
    "two",
    "vectors",
    "right",
    "9596",
    "obviously",
    "basically",
    "construct",
    "two",
    "dimension",
    "let",
    "say",
    "getting",
    "king",
    "okay",
    "order",
    "make",
    "understand",
    "let",
    "say",
    "queen",
    "okay",
    "queen",
    "come",
    "anywhere",
    "let",
    "say",
    "man",
    "right",
    "man",
    "probably",
    "say",
    "king",
    "respect",
    "gender",
    "right",
    "two",
    "going",
    "nearest",
    "word",
    "compared",
    "queen",
    "right",
    "calculate",
    "distance",
    "vector",
    "vector",
    "provided",
    "form",
    "try",
    "probably",
    "calculate",
    "distance",
    "like",
    "okay",
    "try",
    "find",
    "angle",
    "find",
    "distance",
    "two",
    "vectors",
    "use",
    "distance",
    "formula",
    "says",
    "nothing",
    "1",
    "minus",
    "cosine",
    "similarity",
    "cosine",
    "similarity",
    "super",
    "important",
    "understand",
    "cosine",
    "similarity",
    "basically",
    "say",
    "cosine",
    "similarity",
    "nothing",
    "cosine",
    "similarity",
    "nothing",
    "angle",
    "two",
    "vectors",
    "let",
    "say",
    "angle",
    "two",
    "vectors",
    "let",
    "say",
    "taking",
    "example",
    "let",
    "let",
    "consider",
    "nothing",
    "cos",
    "45",
    "cos",
    "45",
    "nothing",
    "1",
    "tk2",
    "probably",
    "think",
    "approximately",
    "equal",
    "771",
    "done",
    "calculation",
    "wrong",
    "let",
    "know",
    "okay",
    "distance",
    "two",
    "vectors",
    "nothing",
    "1",
    "minus",
    "7071",
    "calculating",
    "distance",
    "29",
    "let",
    "say",
    "getting",
    "29",
    "distance",
    "right",
    "say",
    "okay",
    "almost",
    "particular",
    "word",
    "similar",
    "let",
    "say",
    "two",
    "vectors",
    "two",
    "different",
    "vectors",
    "one",
    "vector",
    "basically",
    "one",
    "vector",
    "angle",
    "two",
    "nothing",
    "case",
    "distance",
    "nothing",
    "1us",
    "cos",
    "theta",
    "cos",
    "theta",
    "nothing",
    "cos",
    "90",
    "cos",
    "90",
    "nothing",
    "0",
    "1",
    "0",
    "1",
    "definitely",
    "say",
    "vector",
    "vector",
    "completely",
    "different",
    "distance",
    "one",
    "distance",
    "nearer",
    "zero",
    "say",
    "almost",
    "similar",
    "vectors",
    "case",
    "0",
    "29",
    "say",
    "okay",
    "somewhat",
    "similar",
    "right",
    "let",
    "say",
    "one",
    "vector",
    "point",
    "say",
    "two",
    "vectors",
    "almost",
    "angle",
    "uh",
    "cos",
    "0",
    "cos",
    "0",
    "nothing",
    "1",
    "1",
    "1",
    "nothing",
    "zero",
    "right",
    "angle",
    "two",
    "point",
    "nothing",
    "zero",
    "right",
    "angle",
    "right",
    "angle",
    "able",
    "find",
    "distance",
    "case",
    "distance",
    "nothing",
    "1",
    "cos",
    "0",
    "1",
    "cos",
    "0",
    "nothing",
    "1",
    "1",
    "nothing",
    "zero",
    "distance",
    "coming",
    "zero",
    "basically",
    "means",
    "two",
    "word",
    "right",
    "super",
    "important",
    "recommendation",
    "also",
    "happens",
    "way",
    "recommendation",
    "let",
    "say",
    "movie",
    "like",
    "avengers",
    "let",
    "say",
    "avengers",
    "think",
    "iron",
    "man",
    "come",
    "iron",
    "man",
    "come",
    "near",
    "near",
    "particular",
    "point",
    "right",
    "iron",
    "man",
    "coming",
    "based",
    "different",
    "different",
    "feature",
    "representation",
    "right",
    "whether",
    "comic",
    "movie",
    "whether",
    "action",
    "movie",
    "action",
    "comic",
    "right",
    "comedy",
    "feature",
    "representation",
    "try",
    "understand",
    "movie",
    "name",
    "basically",
    "vector",
    "vocabulary",
    "avengers",
    "one",
    "right",
    "hope",
    "getting",
    "idea",
    "word",
    "w",
    "basically",
    "working",
    "end",
    "day",
    "basically",
    "creating",
    "feuture",
    "representation",
    "every",
    "word",
    "okay",
    "able",
    "find",
    "yes",
    "word",
    "w",
    "going",
    "need",
    "understand",
    "feature",
    "representation",
    "created",
    "vectors",
    "basically",
    "created",
    "vectors",
    "randomly",
    "written",
    "boy",
    "gender",
    "minus",
    "one",
    "boy",
    "girl",
    "g",
    "girl",
    "gender",
    "plus",
    "one",
    "said",
    "okay",
    "may",
    "opposite",
    "one",
    "get",
    "idea",
    "deep",
    "new",
    "deep",
    "learning",
    "neural",
    "network",
    "basically",
    "simple",
    "neural",
    "network",
    "entire",
    "word",
    "trained",
    "going",
    "discuss",
    "next",
    "video",
    "going",
    "also",
    "show",
    "practical",
    "implementation",
    "hope",
    "able",
    "understand",
    "respect",
    "uh",
    "good",
    "amazing",
    "model",
    "developed",
    "google",
    "good",
    "architecture",
    "respect",
    "try",
    "solve",
    "upcoming",
    "video",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "word",
    "tock",
    "uh",
    "already",
    "know",
    "word",
    "tock",
    "basically",
    "two",
    "types",
    "one",
    "cow",
    "already",
    "seen",
    "previously",
    "continuous",
    "bag",
    "words",
    "script",
    "gram",
    "video",
    "going",
    "going",
    "understand",
    "model",
    "basically",
    "created",
    "deep",
    "learning",
    "model",
    "specifically",
    "saying",
    "know",
    "inputs",
    "outputs",
    "model",
    "basically",
    "trained",
    "one",
    "important",
    "thing",
    "really",
    "need",
    "prerequisite",
    "knowledge",
    "ann",
    "loss",
    "function",
    "optimizers",
    "would",
    "suggest",
    "first",
    "please",
    "make",
    "sure",
    "knowledge",
    "right",
    "understand",
    "understand",
    "respect",
    "word",
    "one",
    "important",
    "thing",
    "word",
    "also",
    "models",
    "right",
    "talk",
    "model",
    "like",
    "google",
    "right",
    "google",
    "model",
    "respect",
    "word",
    "trained",
    "three",
    "3",
    "billion",
    "words",
    "also",
    "train",
    "model",
    "scratch",
    "train",
    "model",
    "scratch",
    "okay",
    "uh",
    "reason",
    "taking",
    "really",
    "need",
    "understand",
    "feature",
    "representation",
    "basically",
    "getting",
    "created",
    "okay",
    "let",
    "go",
    "ahead",
    "let",
    "say",
    "uh",
    "uh",
    "let",
    "take",
    "simple",
    "corpus",
    "let",
    "say",
    "first",
    "start",
    "cow",
    "okay",
    "going",
    "discuss",
    "cow",
    "nothing",
    "continuous",
    "bag",
    "words",
    "model",
    "basically",
    "created",
    "type",
    "word",
    "right",
    "continuous",
    "bag",
    "words",
    "solve",
    "problem",
    "definitely",
    "data",
    "set",
    "let",
    "say",
    "corpus",
    "remember",
    "corpus",
    "models",
    "know",
    "like",
    "word",
    "trained",
    "huge",
    "data",
    "set",
    "huge",
    "data",
    "set",
    "like",
    "particular",
    "model",
    "google",
    "basically",
    "trained",
    "3",
    "billion",
    "words",
    "let",
    "say",
    "corpus",
    "statement",
    "data",
    "set",
    "paragraph",
    "anything",
    "making",
    "understand",
    "going",
    "take",
    "simple",
    "paragraph",
    "going",
    "say",
    "okay",
    "neuron",
    "neuron",
    "neuron",
    "company",
    "neuron",
    "company",
    "related",
    "data",
    "science",
    "let",
    "say",
    "particular",
    "corpus",
    "remember",
    "like",
    "use",
    "case",
    "bigger",
    "corpus",
    "millions",
    "words",
    "right",
    "let",
    "say",
    "digging",
    "simple",
    "corpus",
    "single",
    "liner",
    "going",
    "understand",
    "cow",
    "basically",
    "created",
    "model",
    "basically",
    "trained",
    "help",
    "deep",
    "learning",
    "first",
    "thing",
    "whenever",
    "corpus",
    "really",
    "need",
    "know",
    "input",
    "data",
    "output",
    "data",
    "word",
    "altogether",
    "supervised",
    "machine",
    "learning",
    "right",
    "first",
    "select",
    "window",
    "size",
    "talk",
    "window",
    "size",
    "super",
    "important",
    "let",
    "say",
    "going",
    "select",
    "window",
    "size",
    "five",
    "window",
    "size",
    "super",
    "important",
    "basically",
    "create",
    "input",
    "data",
    "output",
    "data",
    "okay",
    "super",
    "super",
    "important",
    "like",
    "input",
    "data",
    "output",
    "data",
    "train",
    "model",
    "right",
    "window",
    "size",
    "5",
    "indicates",
    "many",
    "words",
    "need",
    "select",
    "initially",
    "let",
    "say",
    "selecting",
    "five",
    "words",
    "five",
    "words",
    "particular",
    "five",
    "words",
    "take",
    "center",
    "word",
    "center",
    "word",
    "understand",
    "take",
    "window",
    "size",
    "5",
    "words",
    "convert",
    "input",
    "output",
    "data",
    "let",
    "say",
    "input",
    "data",
    "output",
    "data",
    "okay",
    "central",
    "element",
    "actually",
    "taken",
    "right",
    "input",
    "neuron",
    "company",
    "right",
    "hand",
    "side",
    "related",
    "two",
    "okay",
    "may",
    "thinking",
    "taking",
    "forward",
    "backward",
    "understand",
    "taking",
    "central",
    "word",
    "basically",
    "output",
    "word",
    "okay",
    "okay",
    "basically",
    "output",
    "word",
    "knowing",
    "words",
    "forward",
    "context",
    "words",
    "backward",
    "context",
    "reason",
    "creating",
    "particular",
    "way",
    "output",
    "knowing",
    "forward",
    "word",
    "backward",
    "word",
    "get",
    "idea",
    "context",
    "specific",
    "sentence",
    "first",
    "step",
    "took",
    "window",
    "size",
    "five",
    "initial",
    "five",
    "words",
    "divide",
    "data",
    "set",
    "input",
    "output",
    "perfect",
    "next",
    "step",
    "go",
    "ahead",
    "move",
    "window",
    "one",
    "step",
    "take",
    "next",
    "five",
    "words",
    "next",
    "five",
    "words",
    "nothing",
    "sentence",
    "one",
    "sorry",
    "input",
    "one",
    "become",
    "input",
    "two",
    "company",
    "okay",
    "uh",
    "five",
    "words",
    "center",
    "word",
    "basically",
    "center",
    "word",
    "related",
    "right",
    "related",
    "write",
    "company",
    "uh",
    "two",
    "data",
    "second",
    "input",
    "output",
    "basically",
    "related",
    "right",
    "similarly",
    "go",
    "next",
    "step",
    "push",
    "windows",
    "one",
    "step",
    "okay",
    "third",
    "sentence",
    "respect",
    "input",
    "output",
    "third",
    "sentence",
    "central",
    "word",
    "basically",
    "central",
    "word",
    "two",
    "okay",
    "going",
    "related",
    "okay",
    "right",
    "hand",
    "side",
    "data",
    "science",
    "may",
    "thinking",
    "chrish",
    "take",
    "window",
    "size",
    "five",
    "take",
    "value",
    "take",
    "value",
    "window",
    "size",
    "playing",
    "important",
    "role",
    "say",
    "time",
    "take",
    "value",
    "take",
    "even",
    "number",
    "take",
    "odd",
    "number",
    "getting",
    "central",
    "element",
    "taking",
    "output",
    "correct",
    "number",
    "words",
    "forward",
    "context",
    "backward",
    "context",
    "okay",
    "related",
    "data",
    "sciences",
    "right",
    "central",
    "word",
    "two",
    "right",
    "going",
    "write",
    "two",
    "became",
    "input",
    "output",
    "actually",
    "going",
    "going",
    "train",
    "model",
    "simple",
    "right",
    "going",
    "train",
    "model",
    "training",
    "basically",
    "happen",
    "one",
    "important",
    "thing",
    "need",
    "understand",
    "seeing",
    "neuron",
    "company",
    "related",
    "inputs",
    "outputs",
    "probably",
    "send",
    "text",
    "directly",
    "need",
    "convert",
    "vectors",
    "initially",
    "send",
    "input",
    "neural",
    "network",
    "also",
    "actually",
    "going",
    "first",
    "know",
    "many",
    "number",
    "words",
    "many",
    "number",
    "words",
    "vocabulary",
    "neuron",
    "company",
    "related",
    "data",
    "science",
    "right",
    "around",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "right",
    "seven",
    "words",
    "probably",
    "use",
    "one",
    "hot",
    "encoding",
    "technique",
    "see",
    "okay",
    "super",
    "important",
    "one",
    "hot",
    "encoding",
    "technique",
    "probably",
    "consider",
    "neuron",
    "let",
    "consider",
    "first",
    "sentence",
    "neuron",
    "company",
    "related",
    "two",
    "right",
    "words",
    "giving",
    "one",
    "hot",
    "code",
    "rep",
    "one",
    "hot",
    "encoding",
    "representation",
    "wherever",
    "neuron",
    "going",
    "make",
    "one",
    "remaining",
    "zeros",
    "around",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "zeros",
    "similarly",
    "company",
    "make",
    "one",
    "remaining",
    "zero",
    "right",
    "similarly",
    "related",
    "related",
    "fourth",
    "word",
    "going",
    "make",
    "0",
    "0",
    "1",
    "0",
    "0",
    "0",
    "two",
    "present",
    "basically",
    "going",
    "write",
    "uh",
    "0",
    "1",
    "0",
    "0",
    "right",
    "pretty",
    "much",
    "clear",
    "till",
    "right",
    "see",
    "basically",
    "done",
    "simple",
    "one",
    "hod",
    "encoded",
    "format",
    "basically",
    "means",
    "really",
    "want",
    "pass",
    "neuron",
    "need",
    "give",
    "vector",
    "understanding",
    "vector",
    "basically",
    "ally",
    "given",
    "seven",
    "dimensions",
    "seven",
    "vectors",
    "giving",
    "right",
    "1",
    "0",
    "0",
    "0",
    "0",
    "sending",
    "company",
    "next",
    "word",
    "vector",
    "go",
    "right",
    "sending",
    "related",
    "vector",
    "go",
    "similarly",
    "particular",
    "words",
    "converted",
    "particular",
    "vector",
    "using",
    "one",
    "coding",
    "let",
    "go",
    "next",
    "step",
    "super",
    "super",
    "important",
    "okay",
    "super",
    "super",
    "important",
    "c",
    "basically",
    "mean",
    "continuous",
    "bag",
    "words",
    "okay",
    "nothing",
    "fully",
    "connected",
    "neural",
    "network",
    "able",
    "understand",
    "models",
    "created",
    "fully",
    "connected",
    "neural",
    "network",
    "okay",
    "fully",
    "connected",
    "neural",
    "network",
    "able",
    "see",
    "one",
    "important",
    "thing",
    "one",
    "first",
    "understand",
    "many",
    "number",
    "input",
    "giving",
    "right",
    "like",
    "many",
    "words",
    "giving",
    "input",
    "right",
    "since",
    "window",
    "size",
    "window",
    "size",
    "five",
    "inputs",
    "fixed",
    "hope",
    "much",
    "clear",
    "right",
    "particular",
    "problem",
    "statement",
    "seeing",
    "giving",
    "four",
    "words",
    "every",
    "sentence",
    "input",
    "basically",
    "fixed",
    "neuron",
    "give",
    "word",
    "neuron",
    "let",
    "say",
    "first",
    "case",
    "give",
    "sentence",
    "one",
    "sentence",
    "one",
    "give",
    "neuron",
    "neuron",
    "represented",
    "seven",
    "vectors",
    "represented",
    "vector",
    "company",
    "basically",
    "represented",
    "vector",
    "probably",
    "see",
    "fully",
    "connected",
    "layer",
    "first",
    "input",
    "layer",
    "basically",
    "nothing",
    "able",
    "see",
    "input",
    "okay",
    "super",
    "important",
    "guys",
    "see",
    "input",
    "first",
    "input",
    "word",
    "okay",
    "understand",
    "giving",
    "creating",
    "circle",
    "inputs",
    "okay",
    "creating",
    "circle",
    "input",
    "see",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "seven",
    "inputs",
    "giving",
    "right",
    "give",
    "seven",
    "inputs",
    "similarly",
    "many",
    "words",
    "going",
    "four",
    "words",
    "going",
    "right",
    "one",
    "word",
    "two",
    "word",
    "basically",
    "input",
    "layer",
    "layer",
    "nothing",
    "input",
    "layer",
    "fully",
    "connected",
    "layer",
    "simple",
    "ann",
    "probably",
    "consider",
    "example",
    "ann",
    "also",
    "many",
    "uh",
    "inputs",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "right",
    "first",
    "word",
    "second",
    "word",
    "third",
    "word",
    "fourth",
    "word",
    "four",
    "different",
    "words",
    "going",
    "word",
    "dimension",
    "seven",
    "vectors",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "right",
    "giving",
    "value",
    "consider",
    "zeros",
    "okay",
    "saying",
    "input",
    "layer",
    "input",
    "layer",
    "input",
    "circles",
    "okay",
    "created",
    "inn",
    "right",
    "probably",
    "last",
    "one",
    "input",
    "right",
    "basically",
    "designing",
    "neural",
    "network",
    "look",
    "like",
    "training",
    "word",
    "four",
    "four",
    "words",
    "understand",
    "first",
    "case",
    "going",
    "pass",
    "neuron",
    "let",
    "say",
    "going",
    "pass",
    "neuron",
    "input",
    "represented",
    "neuron",
    "represented",
    "1",
    "0",
    "0",
    "0",
    "0",
    "0",
    "right",
    "similarly",
    "go",
    "second",
    "word",
    "like",
    "company",
    "represented",
    "another",
    "word",
    "like",
    "represented",
    "different",
    "vector",
    "like",
    "0",
    "1",
    "0",
    "0",
    "0",
    "0",
    "0",
    "right",
    "seven",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "right",
    "similarly",
    "words",
    "represented",
    "like",
    "okay",
    "becomes",
    "input",
    "layer",
    "okay",
    "every",
    "input",
    "basically",
    "given",
    "vector",
    "seven",
    "seven",
    "dimensions",
    "representing",
    "every",
    "word",
    "based",
    "vocabulary",
    "size",
    "using",
    "one",
    "hot",
    "encoded",
    "becomes",
    "input",
    "layer",
    "let",
    "go",
    "middle",
    "layer",
    "called",
    "hidden",
    "layer",
    "hidden",
    "layer",
    "super",
    "important",
    "pause",
    "video",
    "guess",
    "size",
    "know",
    "window",
    "size",
    "much",
    "window",
    "size",
    "basically",
    "five",
    "right",
    "going",
    "make",
    "window",
    "size",
    "okay",
    "window",
    "size",
    "window",
    "size",
    "remember",
    "many",
    "many",
    "window",
    "size",
    "window",
    "size",
    "nothing",
    "five",
    "1",
    "2",
    "3",
    "4",
    "5",
    "right",
    "window",
    "size",
    "basically",
    "five",
    "hidden",
    "layer",
    "five",
    "vectors",
    "okay",
    "understand",
    "respect",
    "five",
    "window",
    "size",
    "set",
    "okay",
    "respect",
    "output",
    "output",
    "many",
    "values",
    "one",
    "value",
    "word",
    "one",
    "word",
    "output",
    "right",
    "word",
    "represented",
    "vector",
    "seven",
    "also",
    "considering",
    "using",
    "one",
    "hot",
    "incant",
    "going",
    "get",
    "vectors",
    "dimension",
    "seven",
    "right",
    "output",
    "basically",
    "another",
    "output",
    "layer",
    "like",
    "seven",
    "different",
    "outputs",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "okay",
    "fully",
    "connected",
    "neural",
    "look",
    "like",
    "neural",
    "network",
    "look",
    "like",
    "need",
    "understand",
    "one",
    "thing",
    "every",
    "node",
    "every",
    "node",
    "connected",
    "node",
    "right",
    "like",
    "like",
    "connected",
    "like",
    "like",
    "ann",
    "work",
    "connected",
    "like",
    "right",
    "similarly",
    "right",
    "similarly",
    "connected",
    "also",
    "short",
    "basically",
    "make",
    "simple",
    "connection",
    "like",
    "look",
    "like",
    "entirely",
    "connected",
    "right",
    "understand",
    "lines",
    "initialized",
    "weight",
    "initialized",
    "weights",
    "need",
    "train",
    "weights",
    "happens",
    "n",
    "right",
    "similarly",
    "connected",
    "connected",
    "connected",
    "right",
    "finally",
    "also",
    "connected",
    "right",
    "everything",
    "basically",
    "getting",
    "connected",
    "hidden",
    "layer",
    "hidden",
    "layer",
    "one",
    "hl1",
    "output",
    "layer",
    "right",
    "hidden",
    "layer",
    "basically",
    "get",
    "connected",
    "get",
    "connected",
    "okay",
    "understand",
    "one",
    "important",
    "thing",
    "okay",
    "super",
    "super",
    "important",
    "fine",
    "connecting",
    "help",
    "loss",
    "function",
    "uh",
    "also",
    "forward",
    "backward",
    "propagation",
    "let",
    "see",
    "consider",
    "let",
    "let",
    "pass",
    "particular",
    "word",
    "neuron",
    "company",
    "related",
    "passed",
    "neuron",
    "company",
    "also",
    "able",
    "see",
    "passing",
    "related",
    "simple",
    "okay",
    "let",
    "zoom",
    "bit",
    "okay",
    "pass",
    "things",
    "happens",
    "respect",
    "output",
    "already",
    "know",
    "real",
    "output",
    "right",
    "getting",
    "values",
    "okay",
    "getting",
    "values",
    "okay",
    "real",
    "output",
    "consider",
    "third",
    "word",
    "real",
    "output",
    "basically",
    "represented",
    "vector",
    "format",
    "0",
    "0",
    "1",
    "0",
    "0",
    "0",
    "0",
    "right",
    "training",
    "training",
    "model",
    "different",
    "different",
    "weights",
    "true",
    "output",
    "may",
    "also",
    "get",
    "different",
    "hat",
    "right",
    "may",
    "get",
    "values",
    "like25",
    "may",
    "get",
    "values",
    "like",
    "33",
    "like",
    "0",
    "1",
    "0",
    "0",
    "0",
    "something",
    "like",
    "basically",
    "calculate",
    "loss",
    "function",
    "based",
    "loss",
    "need",
    "reduce",
    "backward",
    "propagation",
    "right",
    "backward",
    "propagation",
    "unless",
    "difference",
    "hat",
    "minimal",
    "okay",
    "process",
    "continuous",
    "simple",
    "near",
    "need",
    "understand",
    "one",
    "important",
    "thing",
    "important",
    "thing",
    "since",
    "giving",
    "specific",
    "output",
    "okay",
    "basically",
    "giving",
    "specific",
    "output",
    "say",
    "middle",
    "layer",
    "basically",
    "basically",
    "window",
    "size",
    "five",
    "window",
    "size",
    "five",
    "basically",
    "means",
    "word",
    "said",
    "getting",
    "300",
    "dimensions",
    "using",
    "google",
    "word",
    "toc",
    "window",
    "size",
    "okay",
    "basically",
    "means",
    "window",
    "size",
    "five",
    "going",
    "get",
    "output",
    "five",
    "every",
    "word",
    "basically",
    "means",
    "word",
    "getting",
    "converted",
    "vector",
    "going",
    "get",
    "size",
    "five",
    "vectors",
    "basically",
    "final",
    "output",
    "hope",
    "able",
    "understand",
    "let",
    "repeat",
    "reason",
    "actually",
    "selected",
    "window",
    "size",
    "equal",
    "5",
    "want",
    "probably",
    "provide",
    "feature",
    "representation",
    "vector",
    "size",
    "five",
    "okay",
    "basically",
    "means",
    "every",
    "word",
    "converted",
    "five",
    "vector",
    "took",
    "example",
    "google",
    "getting",
    "converted",
    "300",
    "dimension",
    "basically",
    "means",
    "window",
    "size",
    "300",
    "bigger",
    "window",
    "size",
    "better",
    "model",
    "basically",
    "perform",
    "okay",
    "case",
    "able",
    "see",
    "window",
    "size",
    "five",
    "basically",
    "means",
    "see",
    "starting",
    "right",
    "metrix",
    "every",
    "word",
    "7",
    "cross",
    "5",
    "many",
    "number",
    "weights",
    "also",
    "7",
    "cross",
    "5",
    "weights",
    "also",
    "7",
    "cross",
    "5",
    "weights",
    "giving",
    "seven",
    "different",
    "vectors",
    "also",
    "7",
    "cross",
    "5",
    "case",
    "basically",
    "5",
    "cross",
    "7",
    "5",
    "cross",
    "7",
    "basically",
    "mean",
    "loss",
    "gets",
    "reduced",
    "final",
    "vector",
    "look",
    "something",
    "like",
    "get",
    "connected",
    "get",
    "connected",
    "let",
    "say",
    "getting",
    "connected",
    "one",
    "getting",
    "connected",
    "one",
    "getting",
    "connected",
    "one",
    "particular",
    "connection",
    "let",
    "let",
    "say",
    "first",
    "word",
    "first",
    "word",
    "probably",
    "see",
    "first",
    "word",
    "respect",
    "particular",
    "vectors",
    "vocabulary",
    "seeing",
    "neuron",
    "right",
    "vocabulary",
    "first",
    "word",
    "basically",
    "getting",
    "represented",
    "neuron",
    "right",
    "neuron",
    "output",
    "dimension",
    "five",
    "vectors",
    "getting",
    "five",
    "vectors",
    "joined",
    "five",
    "vectors",
    "like",
    "092",
    "94",
    "based",
    "training",
    "36",
    "be0",
    "45",
    "based",
    "feature",
    "representation",
    "hope",
    "able",
    "understand",
    "neuron",
    "represented",
    "second",
    "word",
    "vocabulary",
    "company",
    "right",
    "get",
    "connected",
    "company",
    "also",
    "get",
    "connected",
    "also",
    "get",
    "connected",
    "also",
    "get",
    "connected",
    "also",
    "get",
    "entire",
    "word",
    "vector",
    "company",
    "company",
    "may",
    "different",
    "vector",
    "size",
    "five",
    "dimension",
    "window",
    "size",
    "five",
    "training",
    "forward",
    "backward",
    "propagation",
    "loss",
    "minimal",
    "able",
    "get",
    "vectors",
    "vector",
    "basically",
    "taken",
    "represented",
    "uh",
    "format",
    "feature",
    "representation",
    "every",
    "word",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "natural",
    "language",
    "processing",
    "video",
    "video",
    "going",
    "discuss",
    "second",
    "architecture",
    "skip",
    "gram",
    "already",
    "actually",
    "shown",
    "seow",
    "actually",
    "works",
    "continuous",
    "bag",
    "words",
    "also",
    "showed",
    "neural",
    "network",
    "gets",
    "strained",
    "right",
    "difference",
    "cow",
    "skip",
    "gram",
    "difference",
    "architecture",
    "simple",
    "guys",
    "right",
    "focus",
    "going",
    "take",
    "data",
    "set",
    "let",
    "say",
    "ion",
    "compan",
    "related",
    "data",
    "science",
    "written",
    "right",
    "respect",
    "see",
    "created",
    "input",
    "output",
    "using",
    "skip",
    "gr",
    "thing",
    "going",
    "change",
    "everything",
    "let",
    "say",
    "taken",
    "window",
    "size",
    "probably",
    "go",
    "ahead",
    "show",
    "happen",
    "input",
    "specific",
    "test",
    "output",
    "specific",
    "test",
    "help",
    "skip",
    "gram",
    "initially",
    "let",
    "say",
    "input",
    "going",
    "become",
    "input",
    "basically",
    "output",
    "right",
    "respect",
    "window",
    "size",
    "equal",
    "5",
    "right",
    "window",
    "size",
    "equal",
    "5",
    "steps",
    "changing",
    "input",
    "changing",
    "output",
    "input",
    "text",
    "output",
    "happen",
    "entirely",
    "creating",
    "neural",
    "network",
    "input",
    "let",
    "say",
    "isword",
    "related",
    "word",
    "word",
    "right",
    "input",
    "going",
    "basically",
    "uh",
    "input",
    "layer",
    "seven",
    "vectors",
    "going",
    "seven",
    "probably",
    "see",
    "vectors",
    "many",
    "number",
    "uh",
    "vocabulary",
    "vocabulary",
    "size",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "right",
    "initially",
    "input",
    "layer",
    "giving",
    "input",
    "basically",
    "uh",
    "seven",
    "vectors",
    "seven",
    "dimension",
    "vectors",
    "middle",
    "basically",
    "window",
    "size",
    "vectors",
    "window",
    "size",
    "nothing",
    "five",
    "1",
    "2",
    "3",
    "4",
    "5",
    "understand",
    "nodes",
    "okay",
    "output",
    "see",
    "four",
    "words",
    "right",
    "everything",
    "present",
    "respect",
    "right",
    "similarly",
    "get",
    "constructed",
    "point",
    "time",
    "output",
    "layer",
    "able",
    "see",
    "one",
    "word",
    "two",
    "word",
    "three",
    "word",
    "fourth",
    "word",
    "kind",
    "output",
    "getting",
    "uh",
    "much",
    "simple",
    "every",
    "one",
    "like",
    "seven",
    "dimension",
    "right",
    "hope",
    "able",
    "understand",
    "changed",
    "direction",
    "respect",
    "right",
    "input",
    "layer",
    "see",
    "basically",
    "7",
    "cross",
    "5",
    "metrix",
    "respect",
    "weights",
    "weights",
    "initially",
    "randomly",
    "initialized",
    "right",
    "need",
    "train",
    "right",
    "respect",
    "connecting",
    "basically",
    "5",
    "cross",
    "7",
    "uh",
    "matrix",
    "respect",
    "weights",
    "5",
    "cross",
    "7",
    "similarly",
    "seeing",
    "5",
    "cross",
    "7",
    "one",
    "also",
    "5",
    "cross",
    "7",
    "respect",
    "see",
    "initially",
    "give",
    "input",
    "uh",
    "vectors",
    "going",
    "know",
    "third",
    "third",
    "word",
    "going",
    "like",
    "right",
    "0",
    "0",
    "1",
    "0",
    "0",
    "0",
    "0",
    "vectors",
    "going",
    "since",
    "window",
    "size",
    "five",
    "able",
    "see",
    "vectors",
    "get",
    "initialized",
    "uh",
    "randomly",
    "connected",
    "weights",
    "7",
    "cross",
    "5",
    "weight",
    "metrix",
    "created",
    "respect",
    "forward",
    "propagation",
    "happen",
    "obviously",
    "know",
    "probably",
    "know",
    "ann",
    "right",
    "things",
    "happens",
    "hidden",
    "layer",
    "input",
    "weights",
    "getting",
    "multiplied",
    "weights",
    "bias",
    "added",
    "activation",
    "function",
    "applied",
    "top",
    "output",
    "layer",
    "basically",
    "apply",
    "soft",
    "max",
    "function",
    "soft",
    "max",
    "function",
    "compute",
    "hat",
    "hat",
    "predicted",
    "one",
    "real",
    "data",
    "particular",
    "case",
    "sorry",
    "right",
    "first",
    "case",
    "neuron",
    "neuron",
    "whatever",
    "um",
    "whatever",
    "things",
    "whatever",
    "vectors",
    "initializing",
    "right",
    "basically",
    "apply",
    "softmax",
    "function",
    "let",
    "say",
    "respect",
    "neuron",
    "nothing",
    "1",
    "0",
    "0",
    "0",
    "0",
    "0",
    "0",
    "0",
    "7",
    "zeros",
    "hat",
    "computed",
    "since",
    "apply",
    "soft",
    "max",
    "right",
    "hat",
    "something",
    "right",
    "values",
    "calculate",
    "loss",
    "function",
    "make",
    "sure",
    "sure",
    "keep",
    "forward",
    "backward",
    "propagation",
    "unless",
    "loss",
    "function",
    "decreases",
    "right",
    "loss",
    "value",
    "decreases",
    "finally",
    "seeing",
    "whatever",
    "connected",
    "right",
    "particular",
    "word",
    "shown",
    "form",
    "five",
    "vectors",
    "word",
    "shown",
    "form",
    "five",
    "vectors",
    "loss",
    "completely",
    "minimized",
    "process",
    "definitely",
    "know",
    "ann",
    "actually",
    "works",
    "optimizer",
    "actually",
    "work",
    "brief",
    "idea",
    "script",
    "gram",
    "right",
    "improve",
    "basic",
    "question",
    "apply",
    "c",
    "boy",
    "c",
    "question",
    "apply",
    "cball",
    "skip",
    "gram",
    "right",
    "simple",
    "thing",
    "according",
    "research",
    "right",
    "whenever",
    "small",
    "data",
    "set",
    "small",
    "corpus",
    "bally",
    "go",
    "something",
    "like",
    "c",
    "continuous",
    "bag",
    "word",
    "huge",
    "data",
    "set",
    "definitely",
    "go",
    "skip",
    "gram",
    "proven",
    "uh",
    "many",
    "research",
    "paper",
    "giving",
    "direct",
    "um",
    "observation",
    "able",
    "let",
    "say",
    "want",
    "increase",
    "cow",
    "skip",
    "gram",
    "basically",
    "one",
    "thing",
    "increase",
    "training",
    "data",
    "set",
    "increase",
    "training",
    "data",
    "basically",
    "means",
    "training",
    "data",
    "better",
    "accuracy",
    "right",
    "increase",
    "training",
    "data",
    "second",
    "thing",
    "also",
    "increase",
    "increase",
    "window",
    "size",
    "window",
    "size",
    "turn",
    "turn",
    "leads",
    "leads",
    "increase",
    "dimensions",
    "increase",
    "vector",
    "dimension",
    "super",
    "important",
    "okay",
    "saying",
    "improve",
    "cow",
    "skip",
    "gram",
    "basically",
    "increase",
    "window",
    "size",
    "okay",
    "super",
    "important",
    "increase",
    "window",
    "size",
    "going",
    "take",
    "separately",
    "forget",
    "uh",
    "let",
    "write",
    "increase",
    "also",
    "increase",
    "window",
    "size",
    "instead",
    "five",
    "make",
    "uh",
    "100",
    "know",
    "obviously",
    "increase",
    "window",
    "size",
    "increase",
    "window",
    "size",
    "increasing",
    "window",
    "size",
    "basically",
    "means",
    "vector",
    "dimension",
    "also",
    "increasing",
    "right",
    "vector",
    "dimension",
    "also",
    "increasing",
    "keep",
    "increasing",
    "trying",
    "uh",
    "try",
    "able",
    "see",
    "getting",
    "better",
    "performance",
    "also",
    "basically",
    "use",
    "increase",
    "see",
    "next",
    "example",
    "right",
    "using",
    "model",
    "respect",
    "google",
    "google",
    "word",
    "right",
    "basically",
    "trained",
    "3",
    "billion",
    "words",
    "guess",
    "3",
    "billion",
    "words",
    "3",
    "billion",
    "words",
    "going",
    "give",
    "feature",
    "representation",
    "feature",
    "representation",
    "300",
    "vectors",
    "sorry",
    "300",
    "dimensions",
    "basically",
    "means",
    "suppose",
    "word",
    "cricket",
    "okay",
    "since",
    "cricket",
    "always",
    "news",
    "3",
    "billion",
    "word",
    "google",
    "news",
    "right",
    "google",
    "big",
    "company",
    "guys",
    "amount",
    "data",
    "much",
    "easy",
    "going",
    "give",
    "word",
    "called",
    "cricet",
    "going",
    "basically",
    "convert",
    "300",
    "dimension",
    "300",
    "dimension",
    "vectors",
    "okay",
    "vectors",
    "super",
    "important",
    "300",
    "dimension",
    "vectors",
    "okay",
    "going",
    "write",
    "300",
    "dimension",
    "vectors",
    "example",
    "try",
    "try",
    "show",
    "uh",
    "use",
    "upcoming",
    "session",
    "try",
    "try",
    "use",
    "model",
    "also",
    "try",
    "also",
    "make",
    "sure",
    "train",
    "new",
    "data",
    "set",
    "scratch",
    "help",
    "word",
    "tock",
    "going",
    "basically",
    "gen",
    "sim",
    "library",
    "okay",
    "yes",
    "uh",
    "next",
    "video",
    "hope",
    "understood",
    "architecture",
    "one",
    "cball",
    "one",
    "skip",
    "gram",
    "hello",
    "guys",
    "going",
    "continue",
    "discussion",
    "respect",
    "nlp",
    "video",
    "going",
    "discuss",
    "average",
    "word",
    "w",
    "super",
    "important",
    "topic",
    "uh",
    "help",
    "word",
    "able",
    "solve",
    "uh",
    "classification",
    "problem",
    "really",
    "need",
    "perform",
    "something",
    "called",
    "average",
    "word",
    "let",
    "take",
    "simple",
    "example",
    "try",
    "show",
    "uh",
    "right",
    "discuss",
    "respect",
    "theoretical",
    "intution",
    "go",
    "ahead",
    "uh",
    "discussing",
    "practical",
    "part",
    "also",
    "implement",
    "average",
    "word",
    "work",
    "also",
    "uh",
    "text",
    "data",
    "foot",
    "good",
    "output",
    "1",
    "z",
    "one",
    "see",
    "documents",
    "know",
    "help",
    "word",
    "take",
    "every",
    "word",
    "convert",
    "vectors",
    "let",
    "say",
    "using",
    "google",
    "model",
    "okay",
    "using",
    "google",
    "word",
    "model",
    "word",
    "w",
    "model",
    "google",
    "word",
    "model",
    "actually",
    "actually",
    "definitely",
    "see",
    "okay",
    "sentence",
    "food",
    "good",
    "right",
    "get",
    "converted",
    "vectors",
    "okay",
    "uh",
    "already",
    "shown",
    "respect",
    "feature",
    "representation",
    "getting",
    "vector",
    "300",
    "dimensions",
    "let",
    "say",
    "vector",
    "300",
    "dimensions",
    "okay",
    "get",
    "converted",
    "300",
    "dimensions",
    "respect",
    "okay",
    "coming",
    "next",
    "word",
    "food",
    "right",
    "food",
    "also",
    "get",
    "converted",
    "300",
    "dimensions",
    "happens",
    "respect",
    "word",
    "right",
    "every",
    "word",
    "present",
    "like",
    "also",
    "get",
    "uh",
    "also",
    "get",
    "formed",
    "300",
    "dimensions",
    "also",
    "words",
    "get",
    "converted",
    "vectors",
    "respect",
    "many",
    "number",
    "dimension",
    "okay",
    "perfect",
    "till",
    "think",
    "uh",
    "superbly",
    "clear",
    "already",
    "discussed",
    "things",
    "uh",
    "previous",
    "session",
    "need",
    "understand",
    "one",
    "thing",
    "kind",
    "sentences",
    "basically",
    "writing",
    "many",
    "number",
    "dimensions",
    "basically",
    "many",
    "number",
    "vectors",
    "let",
    "say",
    "300",
    "dimension",
    "vector",
    "basically",
    "getting",
    "converted",
    "foot",
    "real",
    "world",
    "scenario",
    "converting",
    "entire",
    "sentence",
    "vectors",
    "right",
    "see",
    "getting",
    "vectors",
    "also",
    "300",
    "300",
    "vectors",
    "foot",
    "getting",
    "separately",
    "300",
    "vectors",
    "getting",
    "300",
    "vectors",
    "good",
    "getting",
    "300",
    "vectors",
    "right",
    "300",
    "dimension",
    "vectors",
    "problem",
    "end",
    "day",
    "entire",
    "sentence",
    "let",
    "say",
    "getting",
    "300",
    "dimension",
    "entire",
    "sentence",
    "let",
    "say",
    "sentence",
    "one",
    "document",
    "one",
    "main",
    "aim",
    "getting",
    "1",
    "300",
    "diamension",
    "respect",
    "take",
    "input",
    "basically",
    "output",
    "basically",
    "train",
    "model",
    "right",
    "see",
    "since",
    "applying",
    "word",
    "every",
    "every",
    "word",
    "basically",
    "getting",
    "converted",
    "separate",
    "300",
    "dimension",
    "vectors",
    "order",
    "solve",
    "take",
    "vectors",
    "take",
    "vectors",
    "find",
    "average",
    "find",
    "average",
    "try",
    "write",
    "particular",
    "v",
    "vector",
    "entire",
    "sentence",
    "vector",
    "considered",
    "entire",
    "document",
    "sentence",
    "similarly",
    "second",
    "vector",
    "also",
    "go",
    "ahead",
    "calculate",
    "average",
    "write",
    "similarly",
    "like",
    "every",
    "vector",
    "basically",
    "happen",
    "right",
    "every",
    "every",
    "vector",
    "every",
    "word",
    "vectors",
    "know",
    "averaging",
    "basically",
    "happening",
    "reason",
    "say",
    "average",
    "word",
    "work",
    "okay",
    "work",
    "see",
    "end",
    "day",
    "seeing",
    "require",
    "vectors",
    "entire",
    "sentence",
    "right",
    "every",
    "word",
    "separate",
    "300",
    "dimension",
    "vectors",
    "trying",
    "take",
    "average",
    "particular",
    "vector",
    "try",
    "write",
    "one",
    "vector",
    "also",
    "length",
    "obviously",
    "300",
    "right",
    "300",
    "dimension",
    "respect",
    "particular",
    "vector",
    "output",
    "pass",
    "model",
    "get",
    "trained",
    "respect",
    "end",
    "day",
    "happens",
    "sentence",
    "like",
    "applying",
    "average",
    "word",
    "w",
    "going",
    "get",
    "let",
    "say",
    "using",
    "model",
    "going",
    "get",
    "300",
    "dimension",
    "vectors",
    "particular",
    "average",
    "particular",
    "vectors",
    "getting",
    "output",
    "variable",
    "already",
    "similarly",
    "happen",
    "second",
    "word",
    "set",
    "document",
    "third",
    "document",
    "also",
    "average",
    "word",
    "work",
    "basically",
    "says",
    "okay",
    "anything",
    "whatever",
    "vectors",
    "getting",
    "converted",
    "trying",
    "average",
    "every",
    "vector",
    "respect",
    "right",
    "let",
    "say",
    "300",
    "dimension",
    "vector",
    "foood",
    "300",
    "dimension",
    "vector",
    "300",
    "di",
    "good",
    "300",
    "every",
    "line",
    "line",
    "vectors",
    "averaging",
    "actually",
    "finding",
    "value",
    "really",
    "want",
    "one",
    "specific",
    "set",
    "vectors",
    "entire",
    "sentence",
    "right",
    "average",
    "word",
    "basically",
    "text",
    "classification",
    "basically",
    "needed",
    "solve",
    "specific",
    "way",
    "upcoming",
    "tutorials",
    "actually",
    "going",
    "show",
    "going",
    "basically",
    "use",
    "library",
    "called",
    "genim",
    "okay",
    "also",
    "library",
    "separately",
    "called",
    "globe",
    "probably",
    "understand",
    "jim",
    "able",
    "everything",
    "first",
    "go",
    "ahead",
    "see",
    "respect",
    "google",
    "word",
    "toc",
    "second",
    "instance",
    "going",
    "going",
    "going",
    "train",
    "word",
    "w",
    "scratch",
    "scratch",
    "basically",
    "take",
    "data",
    "set",
    "try",
    "train",
    "word",
    "model",
    "help",
    "genim",
    "library",
    "hope",
    "got",
    "idea",
    "average",
    "word",
    "end",
    "entire",
    "sentence",
    "want",
    "vectors",
    "uh",
    "since",
    "combining",
    "particular",
    "word",
    "semantic",
    "information",
    "also",
    "maintained",
    "right",
    "yes",
    "see",
    "next",
    "video",
    "practical",
    "ic",
    "implementation",
    "hello",
    "guys",
    "video",
    "going",
    "basically",
    "see",
    "word",
    "w",
    "practical",
    "implementation",
    "really",
    "want",
    "show",
    "google",
    "models",
    "basically",
    "give",
    "idea",
    "like",
    "creates",
    "vector",
    "right",
    "tutorial",
    "going",
    "use",
    "library",
    "called",
    "genim",
    "let",
    "go",
    "ahead",
    "let",
    "install",
    "particular",
    "library",
    "need",
    "write",
    "pip",
    "install",
    "genim",
    "see",
    "requirement",
    "already",
    "satisfied",
    "uh",
    "actually",
    "going",
    "going",
    "import",
    "import",
    "genim",
    "jim",
    "model",
    "going",
    "import",
    "word",
    "key",
    "vectors",
    "okay",
    "talk",
    "two",
    "libraries",
    "specifically",
    "required",
    "one",
    "important",
    "thing",
    "said",
    "show",
    "practical",
    "implementation",
    "word",
    "going",
    "take",
    "google",
    "model",
    "upcoming",
    "video",
    "try",
    "show",
    "different",
    "model",
    "trained",
    "scratch",
    "going",
    "show",
    "show",
    "google",
    "word",
    "model",
    "particular",
    "model",
    "basically",
    "taking",
    "word",
    "google",
    "news",
    "300",
    "okay",
    "uh",
    "basically",
    "vector",
    "trained",
    "google",
    "news",
    "data",
    "set",
    "100",
    "billion",
    "words",
    "model",
    "contains",
    "300",
    "dimensional",
    "vectors",
    "3",
    "million",
    "words",
    "phrases",
    "right",
    "phrases",
    "obtained",
    "using",
    "sample",
    "data",
    "driven",
    "approached",
    "distributed",
    "represent",
    "research",
    "paper",
    "everything",
    "basically",
    "given",
    "okay",
    "model",
    "going",
    "use",
    "going",
    "see",
    "easily",
    "create",
    "vectors",
    "whenever",
    "give",
    "kind",
    "words",
    "gen",
    "sim",
    "know",
    "something",
    "called",
    "api",
    "right",
    "library",
    "called",
    "api",
    "need",
    "write",
    "import",
    "jim",
    "downloader",
    "api",
    "need",
    "write",
    "api",
    "load",
    "basically",
    "model",
    "name",
    "right",
    "model",
    "name",
    "nothing",
    "google",
    "new",
    "use",
    "300",
    "okay",
    "provide",
    "word",
    "inside",
    "wv",
    "variable",
    "nothing",
    "instance",
    "specific",
    "model",
    "try",
    "give",
    "vector",
    "going",
    "execute",
    "line",
    "code",
    "already",
    "done",
    "model",
    "size",
    "166",
    "mb",
    "right",
    "probably",
    "take",
    "time",
    "download",
    "already",
    "downloaded",
    "record",
    "video",
    "directly",
    "go",
    "ahead",
    "downloaded",
    "let",
    "go",
    "ahead",
    "see",
    "king",
    "vector",
    "look",
    "like",
    "okay",
    "basically",
    "king",
    "uh",
    "vectors",
    "word",
    "basically",
    "converted",
    "vectors",
    "dimensions",
    "seeing",
    "right",
    "many",
    "vectors",
    "said",
    "right",
    "300",
    "dimensions",
    "getting",
    "entire",
    "vectors",
    "able",
    "see",
    "300",
    "dimensions",
    "probably",
    "use",
    "vector",
    "uncore",
    "king",
    "shape",
    "right",
    "execute",
    "able",
    "see",
    "300",
    "dimensions",
    "right",
    "respect",
    "use",
    "kind",
    "word",
    "able",
    "get",
    "kind",
    "vectors",
    "let",
    "give",
    "example",
    "okay",
    "suppose",
    "make",
    "uh",
    "particular",
    "vector",
    "things",
    "use",
    "use",
    "wv",
    "variable",
    "word",
    "object",
    "right",
    "give",
    "give",
    "word",
    "choice",
    "let",
    "say",
    "want",
    "give",
    "cricut",
    "right",
    "give",
    "probably",
    "cricet",
    "able",
    "see",
    "vector",
    "auto",
    "atically",
    "generated",
    "vector",
    "basically",
    "getting",
    "respect",
    "particular",
    "shape",
    "300",
    "dimension",
    "wv",
    "wv",
    "variable",
    "right",
    "word",
    "vector",
    "also",
    "functions",
    "actually",
    "use",
    "something",
    "called",
    "underscore",
    "similar",
    "let",
    "say",
    "given",
    "underscore",
    "similar",
    "saying",
    "uh",
    "giving",
    "cricket",
    "word",
    "right",
    "similar",
    "word",
    "present",
    "entire",
    "corpus",
    "right",
    "actually",
    "going",
    "use",
    "word",
    "w",
    "object",
    "going",
    "find",
    "words",
    "execute",
    "able",
    "see",
    "similar",
    "respect",
    "cricket",
    "know",
    "first",
    "try",
    "convert",
    "vector",
    "probably",
    "check",
    "whether",
    "vector",
    "able",
    "see",
    "similar",
    "kind",
    "words",
    "words",
    "similar",
    "cricket",
    "specific",
    "corpus",
    "cricketing",
    "like",
    "83",
    "similarity",
    "point",
    "83",
    "similarity",
    "cricketers",
    "81",
    "similarity",
    "test",
    "cricket",
    "see",
    "80",
    "similarity",
    "20",
    "cricket",
    "right",
    "80",
    "similarity",
    "cricket",
    "cricketer",
    "things",
    "know",
    "showing",
    "kind",
    "similarities",
    "respect",
    "google",
    "news",
    "field",
    "right",
    "similarly",
    "really",
    "want",
    "find",
    "similar",
    "word",
    "respect",
    "happy",
    "also",
    "able",
    "see",
    "words",
    "like",
    "glad",
    "pleased",
    "ecstatic",
    "overjoyed",
    "thrilled",
    "satisfied",
    "proud",
    "delighted",
    "understand",
    "similar",
    "words",
    "compare",
    "happy",
    "right",
    "also",
    "able",
    "say",
    "okay",
    "similar",
    "words",
    "compared",
    "happy",
    "word",
    "also",
    "able",
    "show",
    "respect",
    "much",
    "distance",
    "already",
    "shown",
    "distance",
    "basically",
    "calculated",
    "right",
    "concept",
    "cosign",
    "similarity",
    "similarly",
    "also",
    "provide",
    "two",
    "two",
    "words",
    "basically",
    "say",
    "okay",
    "much",
    "two",
    "word",
    "basically",
    "similar",
    "probably",
    "execute",
    "able",
    "see",
    "wv",
    "similarity",
    "getting",
    "hockey",
    "sports",
    "somewhere",
    "53",
    "person",
    "similar",
    "right",
    "interesting",
    "okay",
    "going",
    "take",
    "vector",
    "king",
    "going",
    "subtract",
    "vector",
    "man",
    "going",
    "add",
    "vector",
    "woman",
    "let",
    "see",
    "kind",
    "output",
    "vectors",
    "getting",
    "okay",
    "short",
    "subtracting",
    "king",
    "minus",
    "man",
    "plus",
    "woman",
    "obviously",
    "answer",
    "queen",
    "really",
    "want",
    "prove",
    "vectors",
    "whether",
    "able",
    "get",
    "queen",
    "going",
    "execute",
    "got",
    "got",
    "vectors",
    "entire",
    "vector",
    "entire",
    "vector",
    "vector",
    "300",
    "dimensions",
    "okay",
    "entire",
    "vector",
    "going",
    "going",
    "use",
    "wv",
    "uncore",
    "similar",
    "going",
    "pass",
    "entire",
    "vector",
    "execute",
    "able",
    "see",
    "see",
    "king",
    "first",
    "obviously",
    "uh",
    "king",
    "similar",
    "vector",
    "getting",
    "queen",
    "monarch",
    "princess",
    "crown",
    "prince",
    "prince",
    "prince",
    "sorry",
    "kings",
    "sultan",
    "queen",
    "resort",
    "see",
    "similar",
    "word",
    "king",
    "queen",
    "kind",
    "vector",
    "getting",
    "particular",
    "subtraction",
    "matches",
    "matches",
    "towards",
    "vector",
    "respect",
    "queen",
    "right",
    "able",
    "get",
    "help",
    "word",
    "w",
    "try",
    "use",
    "guys",
    "amazing",
    "model",
    "altogether",
    "able",
    "see",
    "uh",
    "actually",
    "use",
    "model",
    "know",
    "actually",
    "use",
    "word",
    "google",
    "news",
    "300",
    "able",
    "solve",
    "many",
    "problem",
    "brief",
    "idea",
    "model",
    "know",
    "model",
    "looks",
    "like",
    "also",
    "take",
    "text",
    "train",
    "scratch",
    "different",
    "process",
    "allog",
    "together",
    "entire",
    "thing",
    "executed",
    "google",
    "collab",
    "model",
    "size",
    "quite",
    "huge",
    "okay",
    "uh",
    "upcoming",
    "videos",
    "actually",
    "going",
    "also",
    "going",
    "show",
    "technique",
    "using",
    "genim",
    "train",
    "model",
    "scratch",
    "okay",
    "yes",
    "uh",
    "particular",
    "video",
    "see",
    "next",
    "video",
    "thank"
  ],
  "keywords": [
    "hello",
    "guys",
    "nlp",
    "natural",
    "language",
    "processing",
    "machine",
    "learning",
    "video",
    "going",
    "see",
    "entire",
    "like",
    "go",
    "ahead",
    "amazing",
    "part",
    "respect",
    "deep",
    "lot",
    "basically",
    "let",
    "uh",
    "till",
    "probably",
    "know",
    "okay",
    "actually",
    "seen",
    "right",
    "solve",
    "two",
    "different",
    "kind",
    "one",
    "something",
    "called",
    "use",
    "cases",
    "specifically",
    "problem",
    "classification",
    "statement",
    "say",
    "specific",
    "set",
    "features",
    "number",
    "usually",
    "talk",
    "input",
    "also",
    "similarly",
    "output",
    "feature",
    "model",
    "obviously",
    "data",
    "may",
    "continuous",
    "values",
    "ones",
    "zeros",
    "create",
    "train",
    "particular",
    "able",
    "make",
    "whenever",
    "give",
    "types",
    "completely",
    "text",
    "basic",
    "example",
    "really",
    "want",
    "spam",
    "whether",
    "next",
    "means",
    "billion",
    "giving",
    "think",
    "get",
    "put",
    "thing",
    "variable",
    "techniques",
    "convert",
    "hot",
    "encoding",
    "sentence",
    "definitely",
    "understand",
    "written",
    "english",
    "vectors",
    "information",
    "form",
    "sentences",
    "process",
    "much",
    "examples",
    "many",
    "google",
    "coming",
    "things",
    "yes",
    "try",
    "write",
    "libraries",
    "initially",
    "small",
    "first",
    "need",
    "super",
    "important",
    "help",
    "step",
    "nothing",
    "come",
    "apply",
    "tokenization",
    "paragraph",
    "words",
    "stemming",
    "stop",
    "everything",
    "second",
    "technique",
    "increase",
    "focus",
    "converting",
    "bag",
    "tf",
    "idf",
    "instead",
    "writing",
    "way",
    "trying",
    "vector",
    "sure",
    "captured",
    "end",
    "day",
    "whatever",
    "better",
    "still",
    "third",
    "three",
    "word",
    "w",
    "average",
    "tfidf",
    "continue",
    "related",
    "neural",
    "since",
    "become",
    "size",
    "models",
    "using",
    "nltk",
    "us",
    "perform",
    "task",
    "library",
    "suppose",
    "case",
    "created",
    "idea",
    "discussion",
    "discuss",
    "every",
    "tell",
    "provide",
    "tag",
    "short",
    "time",
    "hope",
    "converted",
    "name",
    "already",
    "based",
    "company",
    "test",
    "taking",
    "show",
    "simple",
    "sorry",
    "find",
    "corpus",
    "documents",
    "vocabulary",
    "exactly",
    "unique",
    "present",
    "count",
    "consider",
    "take",
    "full",
    "document",
    "done",
    "getting",
    "separate",
    "got",
    "eat",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "repeated",
    "point",
    "n",
    "definition",
    "understood",
    "work",
    "creating",
    "difference",
    "perfect",
    "new",
    "execute",
    "dot",
    "tokenize",
    "import",
    "function",
    "order",
    "list",
    "wherever",
    "look",
    "along",
    "considered",
    "importance",
    "good",
    "given",
    "stem",
    "eating",
    "root",
    "similar",
    "remove",
    "history",
    "finally",
    "stemmer",
    "major",
    "disadvantages",
    "meaning",
    "changed",
    "disadvantage",
    "fixed",
    "lemmatization",
    "regular",
    "expression",
    "algorithm",
    "equal",
    "saying",
    "value",
    "four",
    "e",
    "seeing",
    "snowball",
    "post",
    "noun",
    "wordss",
    "speech",
    "download",
    "0",
    "advantage",
    "lower",
    "happen",
    "food",
    "representation",
    "represented",
    "dimension",
    "five",
    "seven",
    "zero",
    "cross",
    "advantages",
    "sparse",
    "training",
    "inputs",
    "trained",
    "times",
    "semantic",
    "calculate",
    "similarity",
    "distance",
    "boy",
    "girl",
    "frequency",
    "opposite",
    "term",
    "king",
    "queen",
    "skip",
    "gram",
    "network",
    "300",
    "dimensions",
    "relationship",
    "minus",
    "neuron",
    "window",
    "connected",
    "layer",
    "weights"
  ]
}