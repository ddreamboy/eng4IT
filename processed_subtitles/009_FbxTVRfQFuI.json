{
  "text": "hello and welcome to the session on deep\nlearning my name is mohan and in this\nvideo we are going to talk about what\ndeep learning is all about some of you\nmay be already familiar with the image\nrecognition how does image recognition\nwork you can train this application or\nyour machine to recognize whether a\ngiven image is a cat or a dog and this\nis how it works at a very high level it\nuses artificial neural network it is\ntrained with some known images and\nduring the training it is told if it is\nrecognizing correctly or not and then\nwhen new images are submitted it\nrecognizes correctly based on the\naccuracy of course so a little quick\nunderstanding about artificial neural\nnetworks so this is the way it does is\nyou provide a lot of training data also\nknown as labeled data for example in\nthis case these are the images of dogs\nand the network extracts some features\nthat makes a dog a dog right so that is\nknown as feature extraction and based on\nthat when you submit a new image of dog\nthe basic features remain pretty much\nthe same it may be a completely\ndifferent image but the features of a\ndog still remain pretty much the same in\nvarious different images let's say\ncompared to a cat and that's the way\nartificial neural network works we'll go\ninto details of this uh very shortly and\nonce the training is done with training\ndata we then test it with some test data\ntoo which is basically completely new\ndata which the system has not seen\nbefore unlike the training data and then\nwe find out whether it is predicting\ncorrectly or not thereby we know whether\nthe training is complete or it needs\nmore training so that's not a very high\nlevel artificial neural network works so\nthis is what we are going to talk about\ntoday our agenda looks something like\nthis what is deep learning why do we\nneed deep learning and then what are the\napplications of deep learning one of the\nmain components the secret sauce in deep\nlearning is neural networks so we're\ngoing to talk about what is neural\nnetwork and\nhow it works and some of its components\nlike for example the activation function\nthe gradient descent and so on and so\nforth so that as a part of working of a\nneural network we will go into little\nbit more details how this whole thing\nworks so without much further ado let's\nget started so deep learning is\nconsidered to be a part of machine\nlearning so this diagram very nicely\ndepicts what deep learning is at a very\nhigh level you have the all-encompassing\nartificial intelligence which is more a\nconcept rather than a technology or a\ntechnical concept right so it is it's\nmore of a concept at a very high level\nartificial intelligence under the herd\nis actually machine learning and deep\nlearning and machine learning is a\nbroader concept you can say or a broader\ntechnology and deep learning is a subset\nof machine learning the primary\ndifference between machine learning and\ndeep learning is that deep learning uses\nneural networks and it is suitable for\nhandling large amounts of unstructured\ndata and the last but not least one of\nthe major differences between machine\nlearning and deep learning is that in\nmachine learning the feature extraction\nor the feature engineering is done by\nthe data scientists manually but in deep\nlearning since we use neural networks\nthe feature engineering happens\nautomatically so that's a little bit of\na quick difference between machine\nlearning and deep learning and this\ndiagram very nicely depicts the relation\nbetween artificial intelligence machine\nlearning and deep learning now why do we\nneed deep learning machine learning was\nthere for quite some time and it can do\na lot of stuff that probably what deep\nlearning can do but it's not very good\nat handling large amounts of\nunstructured data like images voice or\neven text for that matter so traditional\nmachine learning is not that very good\nat doing this traditional machine\nlearning can handle large amounts of\nstructured data but when it comes to\nunstructured data it's a big challenge\nso that is one of the key\ndifferentiators for deep learning so\nthat is number one and increasingly for\nartificial intelligence we need image\nrecognition and we need to process\nanalyze images and voice that's the\nreason deep learning is required\ncompared to let's say traditional\nmachine learning it can also perform\ncomplex algorithms more complex than\nlet's say what machine learning can do\nand it can achieve best performance with\nthe large amounts of data so the more\nyou have the data let's say reference\ndata or label data the better the system\nwill do because the training process\nwill be that much better and last but\nnot least with deep learning you can\nreally avoid the manual process of\nfeature extraction those are some of the\nreasons why we need deep learning some\nof the applications of deep learning\ndeep learning has made major inroads and\nit is a major area in which deep\nlearning is applied is healthcare and\nwithin healthcare particularly oncology\nwhich is basically cancer related stuff\none of the issues with cancer is that a\nlot of cancers today are curable they\ncan be cured they are detected early on\nand the challenge with that is when a\ndiagnostics is performed let's say an\nimage has been taken of a patient to\ndetect whether there is cancer or not\nyou need a specialist to look at the\nimage and determine whether it is the\npatient is fine or there is any onset of\ncancer and the number of specialists are\nlimited so if we use deep learning if we\nuse automation here or if we use\nartificial intelligence here then the\nsystem can with a certain amount of the\ngood amount of accuracy determine\nwhether a particular patient is having\ncancer or not so the prediction or the\ndetection process of a disease like\ncancer can be expedited the detection\nprocess can be expedited can be faster\nwithout really waiting for a specialist\nwe can obviously then once the\napplication once the artificial\nintelligence detects or predicts that\nthere is an onset of a cancer this can\nbe cross-checked by a doctor but at\nleast the initial screening process can\nbe automated and that is where the\ncurrent focus is with respect to deep\nlearning in healthcare what else\nrobotics is another area deep learning\nis majorly used in robotics and you must\nhave seen nowadays robots are everywhere\nhumanoids the industrial robots which\nare used for manufacturing process you\nmust have heard about sofia who got\ncitizenship with saudi arabia and so on\nthere are multiple such robots which are\nknowledge oriented but there are also\nindustrial robots are used in industries\nin the manufacturing process and\nincreasingly in security and also in\ndefense for example image processing\nvideo is fed to them and they need to be\nable to detect objects obstacles and so\non and so forth so that's where deep\nlearning is used they need to be able to\nhear and make sense of the sounds that\nthey are hearing that needs deep\nlearning as well so robotics is a major\narea where deep learning is applied then\nwe have self-driving cars or autonomous\ncars you must have heard of google's\nautonomous car which has been tested for\nmillions of miles and pretty much\nincident free there were of course a\ncouple of incidents here and there but\nit is uh considered to be fairly safe\nand there are today a lot of automotive\ncompanies in fact pretty much every\nautomotive company worth its name is\ninvesting in self-driving cars or\nautonomous cars and it is predicted that\nin the next probably 10 to 15 years\nthese will be in production and they\nwill be used extensively in real life\nright now they are all in rnd and in\ntest phases but pretty soon these will\nbe on the road so this is another area\nwhere deep learning is used and how is\nit used where is it used within\nautonomous driving the car actually is\nfed with video of surroundings and it is\nsupposed to process that information\nprocess that video and determine if\nthere are any obstacles it has to\ndetermine if there are any cars in the\nsite will detect whether it is driving\nin the lane also it has to determine\nwhether the signal is green or red so\nthat accordingly it can move forward or\nwait so for all these video analysis\ndeep learning is used in addition to\nthat the training overall training to\ndrive the car happens in a deep learning\nenvironment so again a lot of scope here\nto use deep learning a couple of other\napplications are mission translations\ntoday we have a lot of information and\nvery often this information is in one\nparticular language and more\nspecifically in english and people need\ninformation in various parts of the\nworld it is pretty difficult for human\nbeings to translate each and every piece\nof information or every document into\nall possible languages there are\nprobably at least hundreds of languages\nor if not more to translate each and\nevery document into every language is\npretty difficult therefore we can use\ndeep learning to do pretty much like a\nreal-time translation mechanism so we\ndon't have to translate everything and\nkeep it ready but we train applications\nor artificial intelligence systems that\nwill do the translation on the fly for\nexample you go to somewhere like china\nand you want to know what is written on\na signboard now it is impossible for\nsomebody to translate that and put it on\nthe web or something like that so you\nhave an application which is trained to\ntranslate stuff on the fly so you\nprobably this can be running on your\nmobile phone on your smartphone you scan\nthis the application will instantly\ntranslate that from chinese to english\nthat is one then there could be web\napplications where there may be a\nresearch document which is all in maybe\nchinese or japanese and you want to\ntranslate that to study that document or\nin that case you need to translate so\ntherefore deep learning is used in such\nsituations as well and that is again on\ndemand so it is not like you have to\ntranslate all these documents from other\nlanguages into english and one shot and\nkeep it somewhere that is again pretty\nmuch an impossible task but on a neat\nbasis so you have systems that are\ntrained to translate on the fly so\nmission translation is another major\narea where deep learning is used then\nthere are a few other upcoming areas\nwhere synthesizing is done by neural\nnets for example music composition and\ngeneration of music so you can train a\nneural net to produce music even to\ncompose music so this is a fun thing\nthis is still upcoming it needs a lot of\neffort to train such neural net it has\nbeen proved that it is possible so this\nis a relatively new area and on the same\nlines colorization of images so these\ntwo images on the left hand side is a\ngrayscale image or a black and white\nimage this was colored by a neural net\nor a deep learning application as you\ncan see it's done a very good job of\napplying the colors and obviously this\nwas trained to do this colorization but\nyes this is one more application of deep\nlearning now one of the major secret\nsauce of deep learning is neural network\ndeep learning works on neural network or\nconsists of neural network so let us see\nwhat is neural network neural network or\nartificial neural network is designed or\nbased on the human brain now human brain\nconsists of billions of small cells that\nare known as neurons artificial neural\nnetworks is in a way trying to simulate\nthe human brain so this is a quick\ndiagram of biological neuron a\nbiological neuron consists of the major\npart which is the cell nucleus and then\nit has some tentacles kind of stuff on\nthe top called dendrite and then there\nis like a long tail which is known as\nthe axon further again at the end of\nthis action are what are known as\nsynapses these in turn are connected to\nthe dendrites of the next neuron and all\nthese neurons are interconnected with\neach other therefore they are like\nbillions of them sitting in our brain\nand they're all active they're working\nthey based on the signals they receive\nsignals as inputs from other neurons or\nmaybe from other parts of the body and\nbased on certain criteria they send\nsignals to the neurons at the other end\nso they they get either activated or\nthey don't get activated based on so it\nis like a binary gates so they get\nactivated or not activated based on the\ninputs that they receive and so on so we\nwill see a little bit of those details\nas we move forward in our artificial\nneuron but this is a biological neuron\nthis is the structure of a biological\nneuron and artificial neural network is\nbased on the human brain the smallest\ncomponent of artificial neural network\nis an artificial neuron as shown here\nsometimes is also referred to as\nperceptron now this is a very high level\ndiagram the artificial neuron has a\nsmall central unit which will receive\nthe input if it is doing let's say image\nprocessing the inputs could be pixel\nvalues of the image which is represented\nhere as x1 x2 and so on each of the\ninputs are multiplied by what is known\nas weights which are represented as w1\nw2 and so on there is in the central\nunit basically there is a summation of\nthese weighted inputs which is like x1\ninto w1 plus x2 into w2 and so on the\nproducts are then added and then there\nis a bias that is added to that in the\nnext slide we will see that passes\nthrough an activation function and the\noutput comes as a y which is the output\nand based on certain criteria the cell\ngets either activated or not activated\nso this output would be like a zero or a\none binary format okay so we will see\nthat in a little bit more detail but\nlet's do a quick comparison between\nbiological and artificial neurons just\nlike a biological neuron there are\ndendrites and then there is a cell\nnucleus and synapse and an axon\nwe have in the artificial neuron as well\nthese inputs come like the dead right if\nyou will act like the dendrites there is\na like a central unit which performs the\nsummation of these uh weighted inputs\nwhich is basically w1 x1 w2 x2 and so on\nand then our bias is added here and then\nthat passes through what is known as an\nactivation function okay so these are\nknown as the weights w1 w2 and then\nthere is a bias which will come out here\nand that is added the bias is by the way\ncommon for a particular neuron so there\nwon't be like b1 b2 b3 and so on only\nweights will be one per input the bias\nis common for the entire neuron it is\nalso common for or the value of the bias\nremains the same for all the neurons in\na particular layer we will also see this\nas we move forward and we see deep\nneural network where there are multiple\nneurons so that's the output now the\nwhole exercise of training the neuron is\nabout changing these weights and biases\nas i mentioned artificial neural network\nwill consist of several such neurons and\nas a part of the training process these\nweights keep changing initially they are\nassigned some random values through the\ntraining process the weights the whole\nprocess of training is to come up with\nthe optimum values of w1 w2 and wn and\nthen the b4 or the bias for this\nparticular neuron such that it gives an\naccurate output as required so let's see\nwhat exactly that means so the training\nprocess this is how it happens it takes\nthe inputs each input is multiplied by a\nweight and these weights during training\nkeep changing so initially they are\nassigned some random values and based on\nthe output whether it is correct or\nwrong there is a feedback coming back\nand that will basically change these\nweights until it starts giving the right\noutput that is represented in here as\nsigma i going from 1 to n if there are n\ninputs wi into x i so this is the\nproduct of w1 x1 w2 x2 and so on right\nand there is a bias that gets added here\nand that entire thing goes to what is\nknown as an activation function so\nessentially this is sigma of w i x i\nplus a value of bias which is a b so\nthat entire thing goes as an input to an\nactivation function now this activation\nfunction takes this as an input gives\nthe output as a binary output it could\nbe a zero or a one there are of course\nto start with let's assume it's a binary\noutput later we will see that there are\ndifferent types of activation functions\nso it need not always be binary output\nbut to start with let's keep simple so\nit decides whether the neuron should be\nfired or not so that is the output like\na binary output 0 or 1. all right so\nagain let me summarize this so it takes\nthe inputs so if you're processing an\nimage for example the inputs are the\npixel values of the image x1 x2 up to xn\nthere could be hundreds of these so all\nof those are fed as so these are some\nvalues and these pixel values again can\nbe from 0 to 56 each of those pixel\nvalues are then multiplied with what is\nknown as a weight this is a numeric\nvalue can be any value so this is a\nnumber w1 similarly w2 is a number so\ninitially some random values will be\nassigned and each of these weights are\nmultiplied with the input value and\ntheir sum this is known as the weighted\nsum so that is performed in this kind of\nthe central unit and then a bias is\nadded remember the bias is common for\neach neuron so this is not the bias\nvalue is not one\nbias value for per input so just keep\nthat in mind the bias value there is one\nbias per neuron so it is like this\nsummation plus bias is the output from\nthe section this is not the complete\noutput of the neuron but this is the\nbias for output for step one that goes\nas an input to what is known as\nactivation function and that activation\nfunction results in an output usually a\nbinary output like a zero or a one which\nis known as the firing of the neuron\nokay good so we talked about activation\nfunction so what is an activation\nfunction an activation function\nbasically takes the weighted sum which\nis we saw w1 x1 w2 x2 the sum of all\nthat plus the bias so it takes that as\nan input and it generates a certain\noutput now there are different types of\nactivation functions and the output is\ndifferent for different types of\nactivation functions moreover why is an\nactivation function required it is\nbasically required to bring in\nnon-linearity that's the main reason why\nan activation function is required so\nwhat are the different types of\nactivation functions there are several\ntypes of activation functions but these\nare the most common ones these are the\nones that are currently in use sigmoid\nfunction was one of the early activation\nfunctions but today relu has kind of\ntaken over so relu is by far the most\npopular activation function that is used\ntoday but still sigmoid function is\nstill used in many situations these\ndifferent types of activation functions\nare used in different situations based\non the kind of problem we are trying to\nsolve so what exactly is the difference\nbetween these two sigmoid gives the\nvalues of the output will be between 0\nand 1. threshold function is the value\nwill be\n0 up to a certain value and beyond that\nthis is also known as a step function\nand beyond that it will be 1. in case of\nsigmoid there is a gradual increase but\nin case of threshold it's like also\nknown as a step function there's a rapid\nor instantaneous change from zero to one\nwhereas in sigmoid we will see in the\nnext slide there is a gradual increase\nbut the value in this case is between\nzero and one as well now relu function\non the other hand it is equal to\nbasically if the input is 0 or less than\n0 then the output is 0 whereas if the\ninput is greater than 0 then the output\nis equal to the input i know it's a\nlittle confusing but in the next slides\nwhere we show the relu function it will\nbecome clear similarly hyperbolic\ntangent this is similar to sigmoid in\nterms of the shape of the function\nhowever while sigmoid goes from 0 to 1\nhyperbolic tangent goes from -1 to 1 and\nhere again the increase or the change\nfrom -1 to 1 is gradual and not like\nthreshold or step function where it\nhappens instantaneously so let's take a\nlittle detailed look at some of these\nfunctions so let's start with the\nsigmoid function so this is the equation\nof a sigmoid function which is 1 by 1\nplus e to the power of minus x so x is\nthe value that is the input it goes from\n0 to -1 so this is sigmoid function the\nequation is phi x is equal to 1 by 1\nplus e to the power of minus x and as\nyou can see here this is the input on\nthe x-axis as x is where the value is\ncoming from in fact it can also go\nnegative this is negative actually so\nthis is the zero so this is the negative\nvalue of x so as x is coming from\nnegative value towards zero the value of\nthe output slowly as it is approaching\nzero it it slowly and very gently\nincreases and actually at the point let\nme just use a pen at the point here it\nis it is 0.5 it is actually 0.5 okay and\nslowly gradually it increases to 1 as\nthe value of x increases but then as the\nvalue of x increases it tapers off it\ndoesn't go beyond one so that is the\nspeciality of sigmoid function so the\noutput value will remain between zero\nand one it will never go below zero or\nabove one okay then so that is sigmoid\nfunction now this is threshold function\nor this is also referred to as a step\nfunction and here we can also set the\nthreshold in this case it is that's why\nit's called the threshold function\nnormally it is 0 but you can also set a\ndifferent value for the threshold now\nthe difference between this and the\nsigmoid is that here the change is rapid\nor instantaneous as the x value comes\nfrom negative up to zero it remains zero\nand at zero it pretty much immediately\nincreases to 1 okay so this is a\nmathematical representation of threshold\nfunction phi x is equal to 1 if x is\ngreater than equal to 0 and 0 if x is\nless than 0. so for all negative values\nit is 0 which since we have set the\nthreshold to be 0 so as soon as it\nreaches 0 it becomes 1. you see the\ndifference between this and the previous\none which is basically the sigmoid where\nthe increase from 0 to 1 is gradual and\nhere it is instantaneous and that's why\nthis is also known as a step function\nthreshold function or step function this\nis a relu a relu is one of the most\npopular activation functions today this\nis the definition of relu phi x is equal\nto max of x comma zero what it says is\nif the value of x is less than zero then\nphi x is\nzero the moment it increases goes beyond\nzero the value of phi x is equal to x so\nit doesn't stop at one actually it goes\nall the way so as the value of x\nincreases the value of y will also\nincrease infinitely so there is no limit\nhere unlike your sigmoid or threshold or\nthe next one which is basically\nhyperbolic tangent okay so in case of\nrelu remember there is no upper limit\nthe output is equal to either 0 in case\nthe value of x is negative or it is\nequal to the value of x so for example\nhere if the value of x is 10 then the\nvalue of y is also 10 right okay so that\nis relu and there are several advantages\nof relu and it is much more efficient\nand provides much more accuracy compared\nto other activation functions like\nsigmoid and so on so that's the reason\nit is very popular all right so this is\nhyperbolic tangent activation function\nthe function looks similar to sigmoid\nfunction the curve if you see the shape\nit looks similar to sigmoid function but\nthe difference between hyperbolic\ntangent and sigmoid function is that in\ncase of sigmoid the output goes from\nzero to one whereas in case of\nhyperbolic tangent it goes from -1 to 1\nso that is the difference between\nhyperbolic tangent and sigmoid function\notherwise the shape looks very similar\nthere is a gradual increase unlike the\nstep function where there was an instant\nincrease or instant change here again\nvery similar to sigmoid function the\nvalue changes gradually from -1 to 1. so\nthis is the equation of hyperbolic\ntangent activation function yeah so then\nlet's move on this is a diagrammatic\nrepresentation of the activation\nfunction and how the overall data how\nthe overall progression happens from\ninput to the output so we get the input\nfrom the input layer by the way the\nneural network has three layers\ntypically there will be three layers\nthere is an input layer there is an\noutput layer and then you have the\nhidden layer so the inputs come from the\ninput layer and they get processed in\nthe hidden layer and then you get the\noutput in the output layer so let's take\na little bit of a detailed look into the\nworking of a neural network so let's say\nwe want to classify some images between\ndogs and cats how do we do this this is\nknown as a classification process and we\nare trying to use neural networks and\ndeep learning to implement this\nclassification so how do we do that so\nthis is how it works so you have four\nlayer neural network there is an input\nlayer there is an output layer and then\nthere are two hidden layers and what we\ndo is we provide labeled training data\nwhich means these images are fed to the\nnetwork with the label saying that okay\nthis is a cat the neural network is\nallowed to process it and come up with a\nprediction saying whether it is a cat or\na dog and obviously in the beginning\nthere may be mistakes a cat may be\nclassified as a dog so we then say that\nokay this is wrong this output is wrong\nbut every time it predicts correctly we\nsay yes this output is correct so that\nlearning process so it will go back make\nsome changes to its weights and biases\nwe again feed these inputs and it will\ngive us the output we will check whether\nit is correct or not and so on so this\nis a iterative process which is known as\nthe training process so we are training\nthe neural network and what happens in\nthe training process these weights and\nbiases you remember there were weights\nlike w1 w2 and so on so these weights\nand biases keep changing every time you\nfeed these which is known as an epoch so\nthere are multiple iterations every\niteration is known as an epoch and each\ntime the weights are dated to make sure\nthat the maximum number of images are\nclassified correctly so once again what\nis the input this input could be like\n1000 images of cats and dogs and they\nare labeled because we know which is a\ncat and which is a dog and we feed those\nthousand images the neural network will\ninitially assign some weights and biases\nfor each neuron and it will try to\nprocess extract the features from the\nimages and it will try to come up with a\nprediction for each image and that\nprediction that is calculated by the\nnetwork is compared with the actual\nvalue whether it is a cat or a dog and\nthat's how the error is calculated so\nlet's say there are a thousand images\nand in the first run only 500 of them\nhave been correctly classified that\nmeans we are getting only 50 accuracy so\nwe feed that information back to the\nnetwork further update these weights and\nbiases for each of the neurons and we\nrun this these inputs once again it will\ntry to calculate extract the features\nand it will try to predict which of\nthese is cats and dogs and this time\nlet's say out of thousand 700 of them\nhave been predicted correctly so that\nmeans in the second iteration the\naccuracy has increased from 50 to 70\npercent all right then we go back again\nwe feed this maybe for a third iteration\nfourth iteration and so on and slowly\nand steadily the accuracy of this\nnetwork will keep increasing and it may\nreach probably you never know 90 95\npercent and there are several parameters\nthat are known as hyper parameters that\nneed to be changed and tweaked and that\nis the overall training process and\nultimately at some point we say okay you\nwill probably never reach hundred\npercent accuracy but then we set a limit\nsaying that okay if we receive 95\npercent accuracy that is good enough for\nour application and then we say okay our\ntraining process is done so that is the\nway training happens and once the\ntraining is done now with the training\ndata set the system has let's say seen\nall these thousand images therefore what\nwe do is the next step like in any\nnormal machine learning process we do\nthe testing where we take a fresh set of\nimages and we feed it to the network the\nfresh set which it has not seen before\nas a part of the training process and\nthis is again nothing new in deep\nlearning this was there in machine\nlearning as well so you feed the test\nimages and then find out whether we are\ngetting a similar accuracy or not so\nmaybe that accuracy may reduce a little\nbit while training you may get 98\npercent and then for test you may get 95\npercent but there shouldn't be a drastic\ndrop like for example you get 98 in\ntraining and then you get 50 or 40\npercent with the test that means your\nnetwork has not learned you may have to\nretrain your network so that is the way\nneural network training works and\nremember the whole process is about\nchanging these weights and biases and\ncoming up with the optimal values of\nthese weights and biases so that the\naccuracy is the maximum possible all\nright so a little bit more detail about\nhow this whole thing works so this is\nknown as forward propagation which is\nthe data or the information is going in\nthe forward direction the inputs are\ntaken weighted summation is done bias is\nadded here and then that is fed to the\nactivation function and then that is\nthat comes out as an output so that is\nforward propagation and the output is\ncompared with the actual value and that\nwill give us the error the difference\nbetween them is the error and in\ntechnical terms that is also known as\nour cost function and this is what we\nwould like to minimize there are\ndifferent ways of defining the cost\nfunction but one of the simplest ways is\nmean square error so it is nothing but\nthe square of the difference of the\nerrors or the sum of the squares of the\ndifference of the errors and this is\nalso nothing new we have probably if\nyou're familiar with machine learning\nyou must have come across this mean\nsquare now there are different ways of\ndefining cost function it need not\nalways be the mean square error but the\nmost common one is this so you define\nthis cost function and you ask the\nsystem to minimize this error so we use\nwhat is known as an optimization\nfunction to minimize this error and the\nerror itself sent back to the system as\nfeedback and that is known as back\npropagation and so this is the cost\nfunction and how do we optimize the cost\nfunction we use what is known as\ngradient descent so the gradient descent\nmechanism identifies how to change the\nweights and biases so that the cost\nfunction is minimized and there is also\nwhat is known as the rate or the\nlearning rate that is what is shown here\nas slower and faster so you need to\nspecify what should be the learning rate\nnow if the learning rate is very small\nthen it will probably take very long to\ntrain whereas if the learning rate is\nvery high then it will appear to be\nfaster but then it will probably never\nwhat is known as converge now what is\nconvergence now we are talking about a\nfew terms here convergence is like this\nthis is a representation of convergence\nso the whole idea of gradient descent is\nto optimize the cost function or\nminimize the cost function in order to\ndo that we need to represent the cost\nfunction as this curve we need to come\nto this minimum value that is what is\nknown as the minimization of the cost\nfunction now what happens if we have the\nlearning rate very small is that it will\ntake very long to come to this point on\nthe other hand if you have large higher\nlearning rate what will happen is\ninstead of stopping here it will cross\nover because the learning rate is high\nand then it has to come back so it will\nresult in what is known as like an\noscillation so it will never come to\nthis point which is known as convergence\ninstead it will go back and forth so\nthese are known as hyper parameters the\nlearning rate and so on and these have\nto be those numbers or those values we\ncan determine typically using trial and\nerror out of experience we we try to\nfind out these values so that is the\ngradient descent mechanism to optimize\nthe cost function and that is what is\nused to train our neural network this is\nanother representation of how the\ntraining process works and here in this\nexample we are trying to classify these\nimages whether they are cats or dogs and\nas you can see actually each image is\nfed in each time one image is fed rather\nand these values of x1 x2 up to xn are\nthe pixel values within this image okay\nso those values are then taken and for\neach of those values a weight is\nmultiplied and then it goes to the next\nlayer and then to the next layer and so\non ultimately it comes as the output\nlayer and it gives an output as whether\nit is a dog or a cat remember the output\nwill never be a named output so these\nwould be like a zero or a one and we say\nokay zero corresponds to dogs and one\ncorresponds to catch so that is the way\nit typically happens this is a binary\nclassification we have similar\nsituations where there can be multiple\nclasses which means that there will be\nmultiple more neurons in the output\nlayer okay so this is once again a quick\nrepresentation of how the forward\npropagation and the backward propagation\nworks so the information is going\nin this direction which is basically the\nforward propagation and at the output\nlevel\nwe find out what is the cost function\nthe difference is basically sent back as\npart of the backward propagation and\ngradient descent then adjust the weights\nand biases for the next iteration this\nhappens iteratively till the cost\nfunction is minimized and that is when\nwe say the whole the network has\nconverged or the training process has\nconverged and there can be situations\nwhere convergence may not happen in rare\ncases but by and large the network will\nconverge and after maybe a few\niterations it could be tens of\niterations or hundreds of iterations\ndepending on what exactly the number of\niterations can vary and then we say okay\nwe are getting a certain accuracy and we\nsay that is our threshold maybe 90\naccuracy we stop at that and we say that\nthe system is trained the trained model\nis then deployed for production and so\non so that is the way the neural network\ntraining happens okay so that is the way\nclassification works in deep learning\nusing neural network and this slide is\nan animation of this whole process as\nyou can see the forward propagation the\ndata is going forward from the input\nlayer to the output layer and there is\nan output\nand the error is calculated the cost\nfunction is calculated and that is fed\nback as a part of backward propagation\nand that whole process repeats once\nagain okay so remember in neural\nnetworks the training process is nothing\nbut the finding the best values of the\nweights and biases for each and every\nneuron in the network that's all\ntraining of neural network consists of\nfinding the optimal values of the\nweights and biases so that the accuracy\nis maximum all right so with that we\ncome to the end of the session we all\nhave a great day thank you very much\nhi there if you like this video\nsubscribe to the simply learn youtube\nchannel and click here to watch similar\nvideos turn it up and get certified\nclick here\n",
  "words": [
    "hello",
    "welcome",
    "session",
    "deep",
    "learning",
    "name",
    "mohan",
    "video",
    "going",
    "talk",
    "deep",
    "learning",
    "may",
    "already",
    "familiar",
    "image",
    "recognition",
    "image",
    "recognition",
    "work",
    "train",
    "application",
    "machine",
    "recognize",
    "whether",
    "given",
    "image",
    "cat",
    "dog",
    "works",
    "high",
    "level",
    "uses",
    "artificial",
    "neural",
    "network",
    "trained",
    "known",
    "images",
    "training",
    "told",
    "recognizing",
    "correctly",
    "new",
    "images",
    "submitted",
    "recognizes",
    "correctly",
    "based",
    "accuracy",
    "course",
    "little",
    "quick",
    "understanding",
    "artificial",
    "neural",
    "networks",
    "way",
    "provide",
    "lot",
    "training",
    "data",
    "also",
    "known",
    "labeled",
    "data",
    "example",
    "case",
    "images",
    "dogs",
    "network",
    "extracts",
    "features",
    "makes",
    "dog",
    "dog",
    "right",
    "known",
    "feature",
    "extraction",
    "based",
    "submit",
    "new",
    "image",
    "dog",
    "basic",
    "features",
    "remain",
    "pretty",
    "much",
    "may",
    "completely",
    "different",
    "image",
    "features",
    "dog",
    "still",
    "remain",
    "pretty",
    "much",
    "various",
    "different",
    "images",
    "let",
    "say",
    "compared",
    "cat",
    "way",
    "artificial",
    "neural",
    "network",
    "works",
    "go",
    "details",
    "uh",
    "shortly",
    "training",
    "done",
    "training",
    "data",
    "test",
    "test",
    "data",
    "basically",
    "completely",
    "new",
    "data",
    "system",
    "seen",
    "unlike",
    "training",
    "data",
    "find",
    "whether",
    "predicting",
    "correctly",
    "thereby",
    "know",
    "whether",
    "training",
    "complete",
    "needs",
    "training",
    "high",
    "level",
    "artificial",
    "neural",
    "network",
    "works",
    "going",
    "talk",
    "today",
    "agenda",
    "looks",
    "something",
    "like",
    "deep",
    "learning",
    "need",
    "deep",
    "learning",
    "applications",
    "deep",
    "learning",
    "one",
    "main",
    "components",
    "secret",
    "sauce",
    "deep",
    "learning",
    "neural",
    "networks",
    "going",
    "talk",
    "neural",
    "network",
    "works",
    "components",
    "like",
    "example",
    "activation",
    "function",
    "gradient",
    "descent",
    "forth",
    "part",
    "working",
    "neural",
    "network",
    "go",
    "little",
    "bit",
    "details",
    "whole",
    "thing",
    "works",
    "without",
    "much",
    "ado",
    "let",
    "get",
    "started",
    "deep",
    "learning",
    "considered",
    "part",
    "machine",
    "learning",
    "diagram",
    "nicely",
    "depicts",
    "deep",
    "learning",
    "high",
    "level",
    "artificial",
    "intelligence",
    "concept",
    "rather",
    "technology",
    "technical",
    "concept",
    "right",
    "concept",
    "high",
    "level",
    "artificial",
    "intelligence",
    "herd",
    "actually",
    "machine",
    "learning",
    "deep",
    "learning",
    "machine",
    "learning",
    "broader",
    "concept",
    "say",
    "broader",
    "technology",
    "deep",
    "learning",
    "subset",
    "machine",
    "learning",
    "primary",
    "difference",
    "machine",
    "learning",
    "deep",
    "learning",
    "deep",
    "learning",
    "uses",
    "neural",
    "networks",
    "suitable",
    "handling",
    "large",
    "amounts",
    "unstructured",
    "data",
    "last",
    "least",
    "one",
    "major",
    "differences",
    "machine",
    "learning",
    "deep",
    "learning",
    "machine",
    "learning",
    "feature",
    "extraction",
    "feature",
    "engineering",
    "done",
    "data",
    "scientists",
    "manually",
    "deep",
    "learning",
    "since",
    "use",
    "neural",
    "networks",
    "feature",
    "engineering",
    "happens",
    "automatically",
    "little",
    "bit",
    "quick",
    "difference",
    "machine",
    "learning",
    "deep",
    "learning",
    "diagram",
    "nicely",
    "depicts",
    "relation",
    "artificial",
    "intelligence",
    "machine",
    "learning",
    "deep",
    "learning",
    "need",
    "deep",
    "learning",
    "machine",
    "learning",
    "quite",
    "time",
    "lot",
    "stuff",
    "probably",
    "deep",
    "learning",
    "good",
    "handling",
    "large",
    "amounts",
    "unstructured",
    "data",
    "like",
    "images",
    "voice",
    "even",
    "text",
    "matter",
    "traditional",
    "machine",
    "learning",
    "good",
    "traditional",
    "machine",
    "learning",
    "handle",
    "large",
    "amounts",
    "structured",
    "data",
    "comes",
    "unstructured",
    "data",
    "big",
    "challenge",
    "one",
    "key",
    "differentiators",
    "deep",
    "learning",
    "number",
    "one",
    "increasingly",
    "artificial",
    "intelligence",
    "need",
    "image",
    "recognition",
    "need",
    "process",
    "analyze",
    "images",
    "voice",
    "reason",
    "deep",
    "learning",
    "required",
    "compared",
    "let",
    "say",
    "traditional",
    "machine",
    "learning",
    "also",
    "perform",
    "complex",
    "algorithms",
    "complex",
    "let",
    "say",
    "machine",
    "learning",
    "achieve",
    "best",
    "performance",
    "large",
    "amounts",
    "data",
    "data",
    "let",
    "say",
    "reference",
    "data",
    "label",
    "data",
    "better",
    "system",
    "training",
    "process",
    "much",
    "better",
    "last",
    "least",
    "deep",
    "learning",
    "really",
    "avoid",
    "manual",
    "process",
    "feature",
    "extraction",
    "reasons",
    "need",
    "deep",
    "learning",
    "applications",
    "deep",
    "learning",
    "deep",
    "learning",
    "made",
    "major",
    "inroads",
    "major",
    "area",
    "deep",
    "learning",
    "applied",
    "healthcare",
    "within",
    "healthcare",
    "particularly",
    "oncology",
    "basically",
    "cancer",
    "related",
    "stuff",
    "one",
    "issues",
    "cancer",
    "lot",
    "cancers",
    "today",
    "curable",
    "cured",
    "detected",
    "early",
    "challenge",
    "diagnostics",
    "performed",
    "let",
    "say",
    "image",
    "taken",
    "patient",
    "detect",
    "whether",
    "cancer",
    "need",
    "specialist",
    "look",
    "image",
    "determine",
    "whether",
    "patient",
    "fine",
    "onset",
    "cancer",
    "number",
    "specialists",
    "limited",
    "use",
    "deep",
    "learning",
    "use",
    "automation",
    "use",
    "artificial",
    "intelligence",
    "system",
    "certain",
    "amount",
    "good",
    "amount",
    "accuracy",
    "determine",
    "whether",
    "particular",
    "patient",
    "cancer",
    "prediction",
    "detection",
    "process",
    "disease",
    "like",
    "cancer",
    "expedited",
    "detection",
    "process",
    "expedited",
    "faster",
    "without",
    "really",
    "waiting",
    "specialist",
    "obviously",
    "application",
    "artificial",
    "intelligence",
    "detects",
    "predicts",
    "onset",
    "cancer",
    "doctor",
    "least",
    "initial",
    "screening",
    "process",
    "automated",
    "current",
    "focus",
    "respect",
    "deep",
    "learning",
    "healthcare",
    "else",
    "robotics",
    "another",
    "area",
    "deep",
    "learning",
    "majorly",
    "used",
    "robotics",
    "must",
    "seen",
    "nowadays",
    "robots",
    "everywhere",
    "humanoids",
    "industrial",
    "robots",
    "used",
    "manufacturing",
    "process",
    "must",
    "heard",
    "sofia",
    "got",
    "citizenship",
    "saudi",
    "arabia",
    "multiple",
    "robots",
    "knowledge",
    "oriented",
    "also",
    "industrial",
    "robots",
    "used",
    "industries",
    "manufacturing",
    "process",
    "increasingly",
    "security",
    "also",
    "defense",
    "example",
    "image",
    "processing",
    "video",
    "fed",
    "need",
    "able",
    "detect",
    "objects",
    "obstacles",
    "forth",
    "deep",
    "learning",
    "used",
    "need",
    "able",
    "hear",
    "make",
    "sense",
    "sounds",
    "hearing",
    "needs",
    "deep",
    "learning",
    "well",
    "robotics",
    "major",
    "area",
    "deep",
    "learning",
    "applied",
    "cars",
    "autonomous",
    "cars",
    "must",
    "heard",
    "google",
    "autonomous",
    "car",
    "tested",
    "millions",
    "miles",
    "pretty",
    "much",
    "incident",
    "free",
    "course",
    "couple",
    "incidents",
    "uh",
    "considered",
    "fairly",
    "safe",
    "today",
    "lot",
    "automotive",
    "companies",
    "fact",
    "pretty",
    "much",
    "every",
    "automotive",
    "company",
    "worth",
    "name",
    "investing",
    "cars",
    "autonomous",
    "cars",
    "predicted",
    "next",
    "probably",
    "10",
    "15",
    "years",
    "production",
    "used",
    "extensively",
    "real",
    "life",
    "right",
    "rnd",
    "test",
    "phases",
    "pretty",
    "soon",
    "road",
    "another",
    "area",
    "deep",
    "learning",
    "used",
    "used",
    "used",
    "within",
    "autonomous",
    "driving",
    "car",
    "actually",
    "fed",
    "video",
    "surroundings",
    "supposed",
    "process",
    "information",
    "process",
    "video",
    "determine",
    "obstacles",
    "determine",
    "cars",
    "site",
    "detect",
    "whether",
    "driving",
    "lane",
    "also",
    "determine",
    "whether",
    "signal",
    "green",
    "red",
    "accordingly",
    "move",
    "forward",
    "wait",
    "video",
    "analysis",
    "deep",
    "learning",
    "used",
    "addition",
    "training",
    "overall",
    "training",
    "drive",
    "car",
    "happens",
    "deep",
    "learning",
    "environment",
    "lot",
    "scope",
    "use",
    "deep",
    "learning",
    "couple",
    "applications",
    "mission",
    "translations",
    "today",
    "lot",
    "information",
    "often",
    "information",
    "one",
    "particular",
    "language",
    "specifically",
    "english",
    "people",
    "need",
    "information",
    "various",
    "parts",
    "world",
    "pretty",
    "difficult",
    "human",
    "beings",
    "translate",
    "every",
    "piece",
    "information",
    "every",
    "document",
    "possible",
    "languages",
    "probably",
    "least",
    "hundreds",
    "languages",
    "translate",
    "every",
    "document",
    "every",
    "language",
    "pretty",
    "difficult",
    "therefore",
    "use",
    "deep",
    "learning",
    "pretty",
    "much",
    "like",
    "translation",
    "mechanism",
    "translate",
    "everything",
    "keep",
    "ready",
    "train",
    "applications",
    "artificial",
    "intelligence",
    "systems",
    "translation",
    "fly",
    "example",
    "go",
    "somewhere",
    "like",
    "china",
    "want",
    "know",
    "written",
    "signboard",
    "impossible",
    "somebody",
    "translate",
    "put",
    "web",
    "something",
    "like",
    "application",
    "trained",
    "translate",
    "stuff",
    "fly",
    "probably",
    "running",
    "mobile",
    "phone",
    "smartphone",
    "scan",
    "application",
    "instantly",
    "translate",
    "chinese",
    "english",
    "one",
    "could",
    "web",
    "applications",
    "may",
    "research",
    "document",
    "maybe",
    "chinese",
    "japanese",
    "want",
    "translate",
    "study",
    "document",
    "case",
    "need",
    "translate",
    "therefore",
    "deep",
    "learning",
    "used",
    "situations",
    "well",
    "demand",
    "like",
    "translate",
    "documents",
    "languages",
    "english",
    "one",
    "shot",
    "keep",
    "somewhere",
    "pretty",
    "much",
    "impossible",
    "task",
    "neat",
    "basis",
    "systems",
    "trained",
    "translate",
    "fly",
    "mission",
    "translation",
    "another",
    "major",
    "area",
    "deep",
    "learning",
    "used",
    "upcoming",
    "areas",
    "synthesizing",
    "done",
    "neural",
    "nets",
    "example",
    "music",
    "composition",
    "generation",
    "music",
    "train",
    "neural",
    "net",
    "produce",
    "music",
    "even",
    "compose",
    "music",
    "fun",
    "thing",
    "still",
    "upcoming",
    "needs",
    "lot",
    "effort",
    "train",
    "neural",
    "net",
    "proved",
    "possible",
    "relatively",
    "new",
    "area",
    "lines",
    "colorization",
    "images",
    "two",
    "images",
    "left",
    "hand",
    "side",
    "grayscale",
    "image",
    "black",
    "white",
    "image",
    "colored",
    "neural",
    "net",
    "deep",
    "learning",
    "application",
    "see",
    "done",
    "good",
    "job",
    "applying",
    "colors",
    "obviously",
    "trained",
    "colorization",
    "yes",
    "one",
    "application",
    "deep",
    "learning",
    "one",
    "major",
    "secret",
    "sauce",
    "deep",
    "learning",
    "neural",
    "network",
    "deep",
    "learning",
    "works",
    "neural",
    "network",
    "consists",
    "neural",
    "network",
    "let",
    "us",
    "see",
    "neural",
    "network",
    "neural",
    "network",
    "artificial",
    "neural",
    "network",
    "designed",
    "based",
    "human",
    "brain",
    "human",
    "brain",
    "consists",
    "billions",
    "small",
    "cells",
    "known",
    "neurons",
    "artificial",
    "neural",
    "networks",
    "way",
    "trying",
    "simulate",
    "human",
    "brain",
    "quick",
    "diagram",
    "biological",
    "neuron",
    "biological",
    "neuron",
    "consists",
    "major",
    "part",
    "cell",
    "nucleus",
    "tentacles",
    "kind",
    "stuff",
    "top",
    "called",
    "dendrite",
    "like",
    "long",
    "tail",
    "known",
    "axon",
    "end",
    "action",
    "known",
    "synapses",
    "turn",
    "connected",
    "dendrites",
    "next",
    "neuron",
    "neurons",
    "interconnected",
    "therefore",
    "like",
    "billions",
    "sitting",
    "brain",
    "active",
    "working",
    "based",
    "signals",
    "receive",
    "signals",
    "inputs",
    "neurons",
    "maybe",
    "parts",
    "body",
    "based",
    "certain",
    "criteria",
    "send",
    "signals",
    "neurons",
    "end",
    "get",
    "either",
    "activated",
    "get",
    "activated",
    "based",
    "like",
    "binary",
    "gates",
    "get",
    "activated",
    "activated",
    "based",
    "inputs",
    "receive",
    "see",
    "little",
    "bit",
    "details",
    "move",
    "forward",
    "artificial",
    "neuron",
    "biological",
    "neuron",
    "structure",
    "biological",
    "neuron",
    "artificial",
    "neural",
    "network",
    "based",
    "human",
    "brain",
    "smallest",
    "component",
    "artificial",
    "neural",
    "network",
    "artificial",
    "neuron",
    "shown",
    "sometimes",
    "also",
    "referred",
    "perceptron",
    "high",
    "level",
    "diagram",
    "artificial",
    "neuron",
    "small",
    "central",
    "unit",
    "receive",
    "input",
    "let",
    "say",
    "image",
    "processing",
    "inputs",
    "could",
    "pixel",
    "values",
    "image",
    "represented",
    "x1",
    "x2",
    "inputs",
    "multiplied",
    "known",
    "weights",
    "represented",
    "w1",
    "w2",
    "central",
    "unit",
    "basically",
    "summation",
    "weighted",
    "inputs",
    "like",
    "x1",
    "w1",
    "plus",
    "x2",
    "w2",
    "products",
    "added",
    "bias",
    "added",
    "next",
    "slide",
    "see",
    "passes",
    "activation",
    "function",
    "output",
    "comes",
    "output",
    "based",
    "certain",
    "criteria",
    "cell",
    "gets",
    "either",
    "activated",
    "activated",
    "output",
    "would",
    "like",
    "zero",
    "one",
    "binary",
    "format",
    "okay",
    "see",
    "little",
    "bit",
    "detail",
    "let",
    "quick",
    "comparison",
    "biological",
    "artificial",
    "neurons",
    "like",
    "biological",
    "neuron",
    "dendrites",
    "cell",
    "nucleus",
    "synapse",
    "axon",
    "artificial",
    "neuron",
    "well",
    "inputs",
    "come",
    "like",
    "dead",
    "right",
    "act",
    "like",
    "dendrites",
    "like",
    "central",
    "unit",
    "performs",
    "summation",
    "uh",
    "weighted",
    "inputs",
    "basically",
    "w1",
    "x1",
    "w2",
    "x2",
    "bias",
    "added",
    "passes",
    "known",
    "activation",
    "function",
    "okay",
    "known",
    "weights",
    "w1",
    "w2",
    "bias",
    "come",
    "added",
    "bias",
    "way",
    "common",
    "particular",
    "neuron",
    "wo",
    "like",
    "b1",
    "b2",
    "b3",
    "weights",
    "one",
    "per",
    "input",
    "bias",
    "common",
    "entire",
    "neuron",
    "also",
    "common",
    "value",
    "bias",
    "remains",
    "neurons",
    "particular",
    "layer",
    "also",
    "see",
    "move",
    "forward",
    "see",
    "deep",
    "neural",
    "network",
    "multiple",
    "neurons",
    "output",
    "whole",
    "exercise",
    "training",
    "neuron",
    "changing",
    "weights",
    "biases",
    "mentioned",
    "artificial",
    "neural",
    "network",
    "consist",
    "several",
    "neurons",
    "part",
    "training",
    "process",
    "weights",
    "keep",
    "changing",
    "initially",
    "assigned",
    "random",
    "values",
    "training",
    "process",
    "weights",
    "whole",
    "process",
    "training",
    "come",
    "optimum",
    "values",
    "w1",
    "w2",
    "wn",
    "b4",
    "bias",
    "particular",
    "neuron",
    "gives",
    "accurate",
    "output",
    "required",
    "let",
    "see",
    "exactly",
    "means",
    "training",
    "process",
    "happens",
    "takes",
    "inputs",
    "input",
    "multiplied",
    "weight",
    "weights",
    "training",
    "keep",
    "changing",
    "initially",
    "assigned",
    "random",
    "values",
    "based",
    "output",
    "whether",
    "correct",
    "wrong",
    "feedback",
    "coming",
    "back",
    "basically",
    "change",
    "weights",
    "starts",
    "giving",
    "right",
    "output",
    "represented",
    "sigma",
    "going",
    "1",
    "n",
    "n",
    "inputs",
    "wi",
    "x",
    "product",
    "w1",
    "x1",
    "w2",
    "x2",
    "right",
    "bias",
    "gets",
    "added",
    "entire",
    "thing",
    "goes",
    "known",
    "activation",
    "function",
    "essentially",
    "sigma",
    "w",
    "x",
    "plus",
    "value",
    "bias",
    "b",
    "entire",
    "thing",
    "goes",
    "input",
    "activation",
    "function",
    "activation",
    "function",
    "takes",
    "input",
    "gives",
    "output",
    "binary",
    "output",
    "could",
    "zero",
    "one",
    "course",
    "start",
    "let",
    "assume",
    "binary",
    "output",
    "later",
    "see",
    "different",
    "types",
    "activation",
    "functions",
    "need",
    "always",
    "binary",
    "output",
    "start",
    "let",
    "keep",
    "simple",
    "decides",
    "whether",
    "neuron",
    "fired",
    "output",
    "like",
    "binary",
    "output",
    "0",
    "right",
    "let",
    "summarize",
    "takes",
    "inputs",
    "processing",
    "image",
    "example",
    "inputs",
    "pixel",
    "values",
    "image",
    "x1",
    "x2",
    "xn",
    "could",
    "hundreds",
    "fed",
    "values",
    "pixel",
    "values",
    "0",
    "56",
    "pixel",
    "values",
    "multiplied",
    "known",
    "weight",
    "numeric",
    "value",
    "value",
    "number",
    "w1",
    "similarly",
    "w2",
    "number",
    "initially",
    "random",
    "values",
    "assigned",
    "weights",
    "multiplied",
    "input",
    "value",
    "sum",
    "known",
    "weighted",
    "sum",
    "performed",
    "kind",
    "central",
    "unit",
    "bias",
    "added",
    "remember",
    "bias",
    "common",
    "neuron",
    "bias",
    "value",
    "one",
    "bias",
    "value",
    "per",
    "input",
    "keep",
    "mind",
    "bias",
    "value",
    "one",
    "bias",
    "per",
    "neuron",
    "like",
    "summation",
    "plus",
    "bias",
    "output",
    "section",
    "complete",
    "output",
    "neuron",
    "bias",
    "output",
    "step",
    "one",
    "goes",
    "input",
    "known",
    "activation",
    "function",
    "activation",
    "function",
    "results",
    "output",
    "usually",
    "binary",
    "output",
    "like",
    "zero",
    "one",
    "known",
    "firing",
    "neuron",
    "okay",
    "good",
    "talked",
    "activation",
    "function",
    "activation",
    "function",
    "activation",
    "function",
    "basically",
    "takes",
    "weighted",
    "sum",
    "saw",
    "w1",
    "x1",
    "w2",
    "x2",
    "sum",
    "plus",
    "bias",
    "takes",
    "input",
    "generates",
    "certain",
    "output",
    "different",
    "types",
    "activation",
    "functions",
    "output",
    "different",
    "different",
    "types",
    "activation",
    "functions",
    "moreover",
    "activation",
    "function",
    "required",
    "basically",
    "required",
    "bring",
    "main",
    "reason",
    "activation",
    "function",
    "required",
    "different",
    "types",
    "activation",
    "functions",
    "several",
    "types",
    "activation",
    "functions",
    "common",
    "ones",
    "ones",
    "currently",
    "use",
    "sigmoid",
    "function",
    "one",
    "early",
    "activation",
    "functions",
    "today",
    "relu",
    "kind",
    "taken",
    "relu",
    "far",
    "popular",
    "activation",
    "function",
    "used",
    "today",
    "still",
    "sigmoid",
    "function",
    "still",
    "used",
    "many",
    "situations",
    "different",
    "types",
    "activation",
    "functions",
    "used",
    "different",
    "situations",
    "based",
    "kind",
    "problem",
    "trying",
    "solve",
    "exactly",
    "difference",
    "two",
    "sigmoid",
    "gives",
    "values",
    "output",
    "0",
    "threshold",
    "function",
    "value",
    "0",
    "certain",
    "value",
    "beyond",
    "also",
    "known",
    "step",
    "function",
    "beyond",
    "case",
    "sigmoid",
    "gradual",
    "increase",
    "case",
    "threshold",
    "like",
    "also",
    "known",
    "step",
    "function",
    "rapid",
    "instantaneous",
    "change",
    "zero",
    "one",
    "whereas",
    "sigmoid",
    "see",
    "next",
    "slide",
    "gradual",
    "increase",
    "value",
    "case",
    "zero",
    "one",
    "well",
    "relu",
    "function",
    "hand",
    "equal",
    "basically",
    "input",
    "0",
    "less",
    "0",
    "output",
    "0",
    "whereas",
    "input",
    "greater",
    "0",
    "output",
    "equal",
    "input",
    "know",
    "little",
    "confusing",
    "next",
    "slides",
    "show",
    "relu",
    "function",
    "become",
    "clear",
    "similarly",
    "hyperbolic",
    "tangent",
    "similar",
    "sigmoid",
    "terms",
    "shape",
    "function",
    "however",
    "sigmoid",
    "goes",
    "0",
    "1",
    "hyperbolic",
    "tangent",
    "goes",
    "1",
    "increase",
    "change",
    "1",
    "gradual",
    "like",
    "threshold",
    "step",
    "function",
    "happens",
    "instantaneously",
    "let",
    "take",
    "little",
    "detailed",
    "look",
    "functions",
    "let",
    "start",
    "sigmoid",
    "function",
    "equation",
    "sigmoid",
    "function",
    "1",
    "1",
    "plus",
    "e",
    "power",
    "minus",
    "x",
    "x",
    "value",
    "input",
    "goes",
    "0",
    "sigmoid",
    "function",
    "equation",
    "phi",
    "x",
    "equal",
    "1",
    "1",
    "plus",
    "e",
    "power",
    "minus",
    "x",
    "see",
    "input",
    "x",
    "value",
    "coming",
    "fact",
    "also",
    "go",
    "negative",
    "negative",
    "actually",
    "zero",
    "negative",
    "value",
    "x",
    "x",
    "coming",
    "negative",
    "value",
    "towards",
    "zero",
    "value",
    "output",
    "slowly",
    "approaching",
    "zero",
    "slowly",
    "gently",
    "increases",
    "actually",
    "point",
    "let",
    "use",
    "pen",
    "point",
    "actually",
    "okay",
    "slowly",
    "gradually",
    "increases",
    "1",
    "value",
    "x",
    "increases",
    "value",
    "x",
    "increases",
    "tapers",
    "go",
    "beyond",
    "one",
    "speciality",
    "sigmoid",
    "function",
    "output",
    "value",
    "remain",
    "zero",
    "one",
    "never",
    "go",
    "zero",
    "one",
    "okay",
    "sigmoid",
    "function",
    "threshold",
    "function",
    "also",
    "referred",
    "step",
    "function",
    "also",
    "set",
    "threshold",
    "case",
    "called",
    "threshold",
    "function",
    "normally",
    "0",
    "also",
    "set",
    "different",
    "value",
    "threshold",
    "difference",
    "sigmoid",
    "change",
    "rapid",
    "instantaneous",
    "x",
    "value",
    "comes",
    "negative",
    "zero",
    "remains",
    "zero",
    "zero",
    "pretty",
    "much",
    "immediately",
    "increases",
    "1",
    "okay",
    "mathematical",
    "representation",
    "threshold",
    "function",
    "phi",
    "x",
    "equal",
    "1",
    "x",
    "greater",
    "equal",
    "0",
    "0",
    "x",
    "less",
    "negative",
    "values",
    "0",
    "since",
    "set",
    "threshold",
    "0",
    "soon",
    "reaches",
    "0",
    "becomes",
    "see",
    "difference",
    "previous",
    "one",
    "basically",
    "sigmoid",
    "increase",
    "0",
    "1",
    "gradual",
    "instantaneous",
    "also",
    "known",
    "step",
    "function",
    "threshold",
    "function",
    "step",
    "function",
    "relu",
    "relu",
    "one",
    "popular",
    "activation",
    "functions",
    "today",
    "definition",
    "relu",
    "phi",
    "x",
    "equal",
    "max",
    "x",
    "comma",
    "zero",
    "says",
    "value",
    "x",
    "less",
    "zero",
    "phi",
    "x",
    "zero",
    "moment",
    "increases",
    "goes",
    "beyond",
    "zero",
    "value",
    "phi",
    "x",
    "equal",
    "x",
    "stop",
    "one",
    "actually",
    "goes",
    "way",
    "value",
    "x",
    "increases",
    "value",
    "also",
    "increase",
    "infinitely",
    "limit",
    "unlike",
    "sigmoid",
    "threshold",
    "next",
    "one",
    "basically",
    "hyperbolic",
    "tangent",
    "okay",
    "case",
    "relu",
    "remember",
    "upper",
    "limit",
    "output",
    "equal",
    "either",
    "0",
    "case",
    "value",
    "x",
    "negative",
    "equal",
    "value",
    "x",
    "example",
    "value",
    "x",
    "10",
    "value",
    "also",
    "10",
    "right",
    "okay",
    "relu",
    "several",
    "advantages",
    "relu",
    "much",
    "efficient",
    "provides",
    "much",
    "accuracy",
    "compared",
    "activation",
    "functions",
    "like",
    "sigmoid",
    "reason",
    "popular",
    "right",
    "hyperbolic",
    "tangent",
    "activation",
    "function",
    "function",
    "looks",
    "similar",
    "sigmoid",
    "function",
    "curve",
    "see",
    "shape",
    "looks",
    "similar",
    "sigmoid",
    "function",
    "difference",
    "hyperbolic",
    "tangent",
    "sigmoid",
    "function",
    "case",
    "sigmoid",
    "output",
    "goes",
    "zero",
    "one",
    "whereas",
    "case",
    "hyperbolic",
    "tangent",
    "goes",
    "1",
    "difference",
    "hyperbolic",
    "tangent",
    "sigmoid",
    "function",
    "otherwise",
    "shape",
    "looks",
    "similar",
    "gradual",
    "increase",
    "unlike",
    "step",
    "function",
    "instant",
    "increase",
    "instant",
    "change",
    "similar",
    "sigmoid",
    "function",
    "value",
    "changes",
    "gradually",
    "equation",
    "hyperbolic",
    "tangent",
    "activation",
    "function",
    "yeah",
    "let",
    "move",
    "diagrammatic",
    "representation",
    "activation",
    "function",
    "overall",
    "data",
    "overall",
    "progression",
    "happens",
    "input",
    "output",
    "get",
    "input",
    "input",
    "layer",
    "way",
    "neural",
    "network",
    "three",
    "layers",
    "typically",
    "three",
    "layers",
    "input",
    "layer",
    "output",
    "layer",
    "hidden",
    "layer",
    "inputs",
    "come",
    "input",
    "layer",
    "get",
    "processed",
    "hidden",
    "layer",
    "get",
    "output",
    "output",
    "layer",
    "let",
    "take",
    "little",
    "bit",
    "detailed",
    "look",
    "working",
    "neural",
    "network",
    "let",
    "say",
    "want",
    "classify",
    "images",
    "dogs",
    "cats",
    "known",
    "classification",
    "process",
    "trying",
    "use",
    "neural",
    "networks",
    "deep",
    "learning",
    "implement",
    "classification",
    "works",
    "four",
    "layer",
    "neural",
    "network",
    "input",
    "layer",
    "output",
    "layer",
    "two",
    "hidden",
    "layers",
    "provide",
    "labeled",
    "training",
    "data",
    "means",
    "images",
    "fed",
    "network",
    "label",
    "saying",
    "okay",
    "cat",
    "neural",
    "network",
    "allowed",
    "process",
    "come",
    "prediction",
    "saying",
    "whether",
    "cat",
    "dog",
    "obviously",
    "beginning",
    "may",
    "mistakes",
    "cat",
    "may",
    "classified",
    "dog",
    "say",
    "okay",
    "wrong",
    "output",
    "wrong",
    "every",
    "time",
    "predicts",
    "correctly",
    "say",
    "yes",
    "output",
    "correct",
    "learning",
    "process",
    "go",
    "back",
    "make",
    "changes",
    "weights",
    "biases",
    "feed",
    "inputs",
    "give",
    "us",
    "output",
    "check",
    "whether",
    "correct",
    "iterative",
    "process",
    "known",
    "training",
    "process",
    "training",
    "neural",
    "network",
    "happens",
    "training",
    "process",
    "weights",
    "biases",
    "remember",
    "weights",
    "like",
    "w1",
    "w2",
    "weights",
    "biases",
    "keep",
    "changing",
    "every",
    "time",
    "feed",
    "known",
    "epoch",
    "multiple",
    "iterations",
    "every",
    "iteration",
    "known",
    "epoch",
    "time",
    "weights",
    "dated",
    "make",
    "sure",
    "maximum",
    "number",
    "images",
    "classified",
    "correctly",
    "input",
    "input",
    "could",
    "like",
    "1000",
    "images",
    "cats",
    "dogs",
    "labeled",
    "know",
    "cat",
    "dog",
    "feed",
    "thousand",
    "images",
    "neural",
    "network",
    "initially",
    "assign",
    "weights",
    "biases",
    "neuron",
    "try",
    "process",
    "extract",
    "features",
    "images",
    "try",
    "come",
    "prediction",
    "image",
    "prediction",
    "calculated",
    "network",
    "compared",
    "actual",
    "value",
    "whether",
    "cat",
    "dog",
    "error",
    "calculated",
    "let",
    "say",
    "thousand",
    "images",
    "first",
    "run",
    "500",
    "correctly",
    "classified",
    "means",
    "getting",
    "50",
    "accuracy",
    "feed",
    "information",
    "back",
    "network",
    "update",
    "weights",
    "biases",
    "neurons",
    "run",
    "inputs",
    "try",
    "calculate",
    "extract",
    "features",
    "try",
    "predict",
    "cats",
    "dogs",
    "time",
    "let",
    "say",
    "thousand",
    "700",
    "predicted",
    "correctly",
    "means",
    "second",
    "iteration",
    "accuracy",
    "increased",
    "50",
    "70",
    "percent",
    "right",
    "go",
    "back",
    "feed",
    "maybe",
    "third",
    "iteration",
    "fourth",
    "iteration",
    "slowly",
    "steadily",
    "accuracy",
    "network",
    "keep",
    "increasing",
    "may",
    "reach",
    "probably",
    "never",
    "know",
    "90",
    "95",
    "percent",
    "several",
    "parameters",
    "known",
    "hyper",
    "parameters",
    "need",
    "changed",
    "tweaked",
    "overall",
    "training",
    "process",
    "ultimately",
    "point",
    "say",
    "okay",
    "probably",
    "never",
    "reach",
    "hundred",
    "percent",
    "accuracy",
    "set",
    "limit",
    "saying",
    "okay",
    "receive",
    "95",
    "percent",
    "accuracy",
    "good",
    "enough",
    "application",
    "say",
    "okay",
    "training",
    "process",
    "done",
    "way",
    "training",
    "happens",
    "training",
    "done",
    "training",
    "data",
    "set",
    "system",
    "let",
    "say",
    "seen",
    "thousand",
    "images",
    "therefore",
    "next",
    "step",
    "like",
    "normal",
    "machine",
    "learning",
    "process",
    "testing",
    "take",
    "fresh",
    "set",
    "images",
    "feed",
    "network",
    "fresh",
    "set",
    "seen",
    "part",
    "training",
    "process",
    "nothing",
    "new",
    "deep",
    "learning",
    "machine",
    "learning",
    "well",
    "feed",
    "test",
    "images",
    "find",
    "whether",
    "getting",
    "similar",
    "accuracy",
    "maybe",
    "accuracy",
    "may",
    "reduce",
    "little",
    "bit",
    "training",
    "may",
    "get",
    "98",
    "percent",
    "test",
    "may",
    "get",
    "95",
    "percent",
    "drastic",
    "drop",
    "like",
    "example",
    "get",
    "98",
    "training",
    "get",
    "50",
    "40",
    "percent",
    "test",
    "means",
    "network",
    "learned",
    "may",
    "retrain",
    "network",
    "way",
    "neural",
    "network",
    "training",
    "works",
    "remember",
    "whole",
    "process",
    "changing",
    "weights",
    "biases",
    "coming",
    "optimal",
    "values",
    "weights",
    "biases",
    "accuracy",
    "maximum",
    "possible",
    "right",
    "little",
    "bit",
    "detail",
    "whole",
    "thing",
    "works",
    "known",
    "forward",
    "propagation",
    "data",
    "information",
    "going",
    "forward",
    "direction",
    "inputs",
    "taken",
    "weighted",
    "summation",
    "done",
    "bias",
    "added",
    "fed",
    "activation",
    "function",
    "comes",
    "output",
    "forward",
    "propagation",
    "output",
    "compared",
    "actual",
    "value",
    "give",
    "us",
    "error",
    "difference",
    "error",
    "technical",
    "terms",
    "also",
    "known",
    "cost",
    "function",
    "would",
    "like",
    "minimize",
    "different",
    "ways",
    "defining",
    "cost",
    "function",
    "one",
    "simplest",
    "ways",
    "mean",
    "square",
    "error",
    "nothing",
    "square",
    "difference",
    "errors",
    "sum",
    "squares",
    "difference",
    "errors",
    "also",
    "nothing",
    "new",
    "probably",
    "familiar",
    "machine",
    "learning",
    "must",
    "come",
    "across",
    "mean",
    "square",
    "different",
    "ways",
    "defining",
    "cost",
    "function",
    "need",
    "always",
    "mean",
    "square",
    "error",
    "common",
    "one",
    "define",
    "cost",
    "function",
    "ask",
    "system",
    "minimize",
    "error",
    "use",
    "known",
    "optimization",
    "function",
    "minimize",
    "error",
    "error",
    "sent",
    "back",
    "system",
    "feedback",
    "known",
    "back",
    "propagation",
    "cost",
    "function",
    "optimize",
    "cost",
    "function",
    "use",
    "known",
    "gradient",
    "descent",
    "gradient",
    "descent",
    "mechanism",
    "identifies",
    "change",
    "weights",
    "biases",
    "cost",
    "function",
    "minimized",
    "also",
    "known",
    "rate",
    "learning",
    "rate",
    "shown",
    "slower",
    "faster",
    "need",
    "specify",
    "learning",
    "rate",
    "learning",
    "rate",
    "small",
    "probably",
    "take",
    "long",
    "train",
    "whereas",
    "learning",
    "rate",
    "high",
    "appear",
    "faster",
    "probably",
    "never",
    "known",
    "converge",
    "convergence",
    "talking",
    "terms",
    "convergence",
    "like",
    "representation",
    "convergence",
    "whole",
    "idea",
    "gradient",
    "descent",
    "optimize",
    "cost",
    "function",
    "minimize",
    "cost",
    "function",
    "order",
    "need",
    "represent",
    "cost",
    "function",
    "curve",
    "need",
    "come",
    "minimum",
    "value",
    "known",
    "minimization",
    "cost",
    "function",
    "happens",
    "learning",
    "rate",
    "small",
    "take",
    "long",
    "come",
    "point",
    "hand",
    "large",
    "higher",
    "learning",
    "rate",
    "happen",
    "instead",
    "stopping",
    "cross",
    "learning",
    "rate",
    "high",
    "come",
    "back",
    "result",
    "known",
    "like",
    "oscillation",
    "never",
    "come",
    "point",
    "known",
    "convergence",
    "instead",
    "go",
    "back",
    "forth",
    "known",
    "hyper",
    "parameters",
    "learning",
    "rate",
    "numbers",
    "values",
    "determine",
    "typically",
    "using",
    "trial",
    "error",
    "experience",
    "try",
    "find",
    "values",
    "gradient",
    "descent",
    "mechanism",
    "optimize",
    "cost",
    "function",
    "used",
    "train",
    "neural",
    "network",
    "another",
    "representation",
    "training",
    "process",
    "works",
    "example",
    "trying",
    "classify",
    "images",
    "whether",
    "cats",
    "dogs",
    "see",
    "actually",
    "image",
    "fed",
    "time",
    "one",
    "image",
    "fed",
    "rather",
    "values",
    "x1",
    "x2",
    "xn",
    "pixel",
    "values",
    "within",
    "image",
    "okay",
    "values",
    "taken",
    "values",
    "weight",
    "multiplied",
    "goes",
    "next",
    "layer",
    "next",
    "layer",
    "ultimately",
    "comes",
    "output",
    "layer",
    "gives",
    "output",
    "whether",
    "dog",
    "cat",
    "remember",
    "output",
    "never",
    "named",
    "output",
    "would",
    "like",
    "zero",
    "one",
    "say",
    "okay",
    "zero",
    "corresponds",
    "dogs",
    "one",
    "corresponds",
    "catch",
    "way",
    "typically",
    "happens",
    "binary",
    "classification",
    "similar",
    "situations",
    "multiple",
    "classes",
    "means",
    "multiple",
    "neurons",
    "output",
    "layer",
    "okay",
    "quick",
    "representation",
    "forward",
    "propagation",
    "backward",
    "propagation",
    "works",
    "information",
    "going",
    "direction",
    "basically",
    "forward",
    "propagation",
    "output",
    "level",
    "find",
    "cost",
    "function",
    "difference",
    "basically",
    "sent",
    "back",
    "part",
    "backward",
    "propagation",
    "gradient",
    "descent",
    "adjust",
    "weights",
    "biases",
    "next",
    "iteration",
    "happens",
    "iteratively",
    "till",
    "cost",
    "function",
    "minimized",
    "say",
    "whole",
    "network",
    "converged",
    "training",
    "process",
    "converged",
    "situations",
    "convergence",
    "may",
    "happen",
    "rare",
    "cases",
    "large",
    "network",
    "converge",
    "maybe",
    "iterations",
    "could",
    "tens",
    "iterations",
    "hundreds",
    "iterations",
    "depending",
    "exactly",
    "number",
    "iterations",
    "vary",
    "say",
    "okay",
    "getting",
    "certain",
    "accuracy",
    "say",
    "threshold",
    "maybe",
    "90",
    "accuracy",
    "stop",
    "say",
    "system",
    "trained",
    "trained",
    "model",
    "deployed",
    "production",
    "way",
    "neural",
    "network",
    "training",
    "happens",
    "okay",
    "way",
    "classification",
    "works",
    "deep",
    "learning",
    "using",
    "neural",
    "network",
    "slide",
    "animation",
    "whole",
    "process",
    "see",
    "forward",
    "propagation",
    "data",
    "going",
    "forward",
    "input",
    "layer",
    "output",
    "layer",
    "output",
    "error",
    "calculated",
    "cost",
    "function",
    "calculated",
    "fed",
    "back",
    "part",
    "backward",
    "propagation",
    "whole",
    "process",
    "repeats",
    "okay",
    "remember",
    "neural",
    "networks",
    "training",
    "process",
    "nothing",
    "finding",
    "best",
    "values",
    "weights",
    "biases",
    "every",
    "neuron",
    "network",
    "training",
    "neural",
    "network",
    "consists",
    "finding",
    "optimal",
    "values",
    "weights",
    "biases",
    "accuracy",
    "maximum",
    "right",
    "come",
    "end",
    "session",
    "great",
    "day",
    "thank",
    "much",
    "hi",
    "like",
    "video",
    "subscribe",
    "simply",
    "learn",
    "youtube",
    "channel",
    "click",
    "watch",
    "similar",
    "videos",
    "turn",
    "get",
    "certified",
    "click"
  ],
  "keywords": [
    "deep",
    "learning",
    "video",
    "going",
    "may",
    "image",
    "train",
    "application",
    "machine",
    "whether",
    "cat",
    "dog",
    "works",
    "high",
    "level",
    "artificial",
    "neural",
    "network",
    "trained",
    "known",
    "images",
    "training",
    "correctly",
    "new",
    "based",
    "accuracy",
    "little",
    "quick",
    "networks",
    "way",
    "lot",
    "data",
    "also",
    "example",
    "case",
    "dogs",
    "features",
    "right",
    "feature",
    "pretty",
    "much",
    "different",
    "still",
    "let",
    "say",
    "compared",
    "go",
    "done",
    "test",
    "basically",
    "system",
    "seen",
    "find",
    "know",
    "today",
    "looks",
    "like",
    "need",
    "applications",
    "one",
    "activation",
    "function",
    "gradient",
    "descent",
    "part",
    "bit",
    "whole",
    "thing",
    "get",
    "diagram",
    "intelligence",
    "concept",
    "actually",
    "difference",
    "large",
    "amounts",
    "least",
    "major",
    "use",
    "happens",
    "time",
    "stuff",
    "probably",
    "good",
    "comes",
    "number",
    "process",
    "required",
    "area",
    "cancer",
    "taken",
    "determine",
    "certain",
    "particular",
    "prediction",
    "another",
    "used",
    "must",
    "robots",
    "multiple",
    "fed",
    "well",
    "cars",
    "autonomous",
    "every",
    "next",
    "information",
    "move",
    "forward",
    "overall",
    "human",
    "translate",
    "document",
    "therefore",
    "keep",
    "could",
    "maybe",
    "situations",
    "music",
    "see",
    "consists",
    "brain",
    "small",
    "neurons",
    "trying",
    "biological",
    "neuron",
    "kind",
    "receive",
    "inputs",
    "activated",
    "binary",
    "central",
    "unit",
    "input",
    "pixel",
    "values",
    "x1",
    "x2",
    "multiplied",
    "weights",
    "w1",
    "w2",
    "summation",
    "weighted",
    "plus",
    "added",
    "bias",
    "output",
    "zero",
    "okay",
    "come",
    "common",
    "value",
    "layer",
    "changing",
    "biases",
    "several",
    "initially",
    "gives",
    "means",
    "takes",
    "coming",
    "back",
    "change",
    "1",
    "x",
    "goes",
    "types",
    "functions",
    "0",
    "sum",
    "remember",
    "step",
    "sigmoid",
    "relu",
    "threshold",
    "beyond",
    "gradual",
    "increase",
    "whereas",
    "equal",
    "hyperbolic",
    "tangent",
    "similar",
    "take",
    "phi",
    "negative",
    "slowly",
    "increases",
    "point",
    "never",
    "set",
    "representation",
    "cats",
    "classification",
    "feed",
    "iterations",
    "iteration",
    "thousand",
    "try",
    "calculated",
    "error",
    "percent",
    "nothing",
    "propagation",
    "cost",
    "minimize",
    "square",
    "rate",
    "convergence"
  ]
}