{
  "text": "welcome to the rnn tutorial that's the\nrecurrent neural network my name is\nrichard kirschner i'm with the\nsimplylearn team that's\nwww.simplylearn.com get certified get\nahead what's in it for you we will start\nwith the course of fundamentals what is\na neural network in popular neural\nnetworks it's important to know the\nframework we're in and what we're going\nto be looking at specifically then we'll\ntouch on why a recurrent neural network\nwhat is a recurrent neural network and\nhow does an rn in work\none of the big things about rnns is what\nthey call the vanishing and exploding\ngradient problem so we'll look at that\nand then we're going to be using a use\ncase\nstudy that's going to be in keras on\ntensorflow cross is a python module for\ndoing neural networks in deep learning\nand in there there's the what they call\nlong short term memory lstm and then\nwe'll use the use case to implement our\nlstm on the keras so when you see that\nlstm that is basically the rnn network\nand we'll get into that the use case is\nalways my favorite part before we dive\ninto any of this we're going to take a\nlook at what is an rnn or an\nintroduction to the rnn do you know how\ngoogle's auto complete feature predicts\nthe rest of the words a user is typing i\nlove that auto complete feature as i'm\ntyping away saves me a lot of time i can\njust kind of hit the enter key and it\nauto fills everything and i don't have\nto type as much well first there's a\ncollection of large volumes of most\nfrequently occurring consecutive words\nthis is fed into a recurrent neural\nnetwork analysis the data by finding the\nsequence of words occurring frequently\nand builds a model to predict the next\nword in the sentence and then google\nwhat is the best food to eat in loss i'm\nguessing you're going to say loss mexico\nno it's going to be las vegas so the\ngoogle search will take a look at that\nand say hey the most common autocomplete\nis going to be vegas in there it usually\ngives you three or four different\nchoices so it's a very powerful tool it\nsaves us a lot of time especially when\nwe're doing a google search or even in\nmicrosoft words has a some people get\nvery mad at it auto fills with the wrong\nstuff but you know you're typing away\nand it helps you autofill i have that in\na lot of my different packages it's just\na standard feature that we're all used\nto now so before we dive into the rnn\nand getting into the depths let's go\nahead and talk about what is a neural\nnetwork neural networks used in deep\nlearning consist of different layers\nconnected to each other and work on the\nstructure and functions of a human brain\nyou're going to see that thread human in\nhuman brain and human thinking\nthroughout deep learning the only way we\ncan evaluate an artificial intelligence\nor anything like that is to compare it\nto human function very important note on\nthere and it learns from a huge volumes\nof data and it uses complex algorithm to\ntrain a neural net so in here we have\nimage pixels of two different breeds of\ndog uh one looks like a nice floppy\neared lab and one a german shepherd you\nknow both wonderful breeds of animals\nthat image then goes into an input layer\nthat input layer might be formatted at\nsome point because you have to let it\nknow like you know different pictures\nare going to be different sizes and\ndifferent color content then it'll feed\ninto hidden layers so each of those\npixels or each point of data goes in and\nthen splits into the hidden layer which\nthen goes into another hidden layer\nwhich then goes to an output layer r and\nn there's some changes in there which\nwe're going to get into so it's not just\na straightforward propagation of data\nlike we've covered in many other\ntutorials and finally you have an output\nlayer and the output layer has two\noutputs it has one that lights up if\nit's a german shepherd and another that\nlights up is if it's a labrador so\nidentify as a dog's breed set networks\ndo not require memorizing the past\noutput so our forward propagation is\njust that it goes forward and doesn't\nhave to be memorized stuff and you can\nsee there that's not actually me in the\npicture uh dressed up in my suit\ni haven't worn a suit in years so as\nwe're looking at this we're going to\nchange it up a little bit before we\ncover that let's talk about popular\nneural networks first there's the feed\nforward neural network used in general\nregression and classification problems\nand we have the convolution neural\nnetwork used for image recognition deep\nneural network used for acoustic\nmodeling deep belief network used for\ncancer detection and recurrent neural\nnetwork used for speech recognition now\ntaken a lot of these and mixed them\naround a little bit so just because it's\nused for one thing doesn't mean it can't\nbe used for other modeling but generally\nthis is where the field is and this is\nhow those models are generally being\nused right now so we talk about a feed\nforward neural network in a feed forward\nneural network information flows only in\nthe forward direction from the input\nnodes through the hidden layers if any\ninto the output nodes there are no\ncycles or loops in the network and so\nyou can see here we have our input layer\ni was talking about how it just goes\nstraight forward into the hidden layers\nso each one of those connects and then\nconnects to the next hidden layer\nconnects to the output layer and of\ncourse we have a nice simplified version\nwhere it has a predicted output the\nrefer to the input is x a lot of times\nand the output as y decisions are based\non current input no memory about the\npast no future scope why recurrent\nneural network issues in feed forward\nneural network so one of the biggest\nissues is because it doesn't have a\nscope of memory or time a feed forward\nneural network doesn't know how to\nhandle sequential data it only considers\nonly the current input so if you have a\nseries of things and because three\npoints back affects what's happening now\nand what your output affects what's\nhappening that's very important so\nwhatever i put as an output is going to\naffect the next one a feed forward\ndoesn't look at any of that it just\nlooks at this is what's coming in and it\ncannot memorize previous inputs so it\ndoesn't have that list of inputs coming\nin solution to feed forward neural\nnetwork you'll see here where it says\nrecurrent neural network and we have our\nx on the bottom going to h going to y\nthat's your feed forward uh but right in\nthe middle it has a value c so there's a\nwhole another process so it's memorizing\nwhat's going on in the hidden layers and\nthe hidden layers as they produce data\nfeed into the next one so your hidden\nlayer might have an output that goes off\nto y\nbut that output goes back into the next\nprediction coming in what this does is\nthis allows it to handle sequential data\nit considers the current input and also\nthe previously received inputs and if\nwe're going to look at general drawings\nand solutions we should also look at\napplications of the rnn image captioning\nrnn is used to caption an image by\nanalyzing the activities present in it a\ndog catching a ball in midair that's\nvery tough i mean you know we have a lot\nof stuff that analyzes images of a dog\nand the image of a ball but it's able to\nadd one more feature in there that's\nactually catching the ball in midair\ntime series prediction any time series\nproblem like predicting the prices of\nstocks in a particular month can be\nsolved using rnn and we'll dive into\nthat in our use case and actually take a\nlook at some stock one of the things you\nshould know about analyzing stock today\nis that it is very difficult and if\nyou're analyzing the whole stock the\nstock market at the new york stock\nexchange in the u.s produces somewhere\nin the neighborhood if you count all the\nindividual trades in fluctuations by the\nsecond um it's like three terabytes a\nday of data so we're going to look at\none stock just analyzing one stock is\nreally tricky in here we'll give you a\nlittle jump on that so that's exciting\nbut don't expect to get rich off of it\nimmediately another application of the\nrnn is natural language processing text\nmining and sentiment analysis can be\ncarried out using rnn for natural\nlanguage processing and you can see\nright here the term natural language\nprocessing when you stream those three\nwords together is very different than\nice if i said processing language\nnatural leap so the time series is very\nimportant when we're analyzing\nsentiments it can change the whole value\nof a sentence just by switching the\nwords around or if you're just counting\nthe words you may get one sentiment\nwhere if you actually look at the order\nthey're in you get a completely\ndifferent sentiment when it rains look\nfor rainbows when it's dark look for\nstars both of these are positive\nsentiments and they're based upon the\norder of which the sentence is going in\nmachine translation given an input in\none language rnn can be used to\ntranslate the input into a different\nlanguages as output i myself very\nlinguistically challenged but if you\nstudy languages and you're good with\nlanguages you know right away that if\nyou're speaking english you would say\nbig cat and if you're speaking spanish\nyou would say cat big so that\ntranslation is really important to get\nthe right order to get there's all kinds\nof parts of speech that are important to\nknow by the order of the words here this\nperson is speaking in english and\ngetting translated and you can see here\na person is speaking in english in this\nlittle diagram i guess that's denoted by\nthe flags i have a flag i own it no um\nbut they're speaking in english and it's\ngetting translated into\nchinese italian french german and\nspanish languages some of the tools\ncoming out are just so cool so somebody\nlike myself who's very linguistically\nchallenged i can now travel into worlds\ni would never think of because i can\nhave something translate my english back\nand forth readily and i'm not stuck with\na communication gap so let's dive into\nwhat is a recurrent neural network\nrecurrent neural network works on the\nprinciple of saving the output of a\nlayer and feeding this back to the input\nin order to predict the output of the\nlayer sounds a little confusing when we\nstart breaking it down it'll make more\nsense and usually we have a propagation\nforward neural network with the input\nlayers the hidden layers the output\nlayer with the recurrent neural network\nwe turn that on its side so here it is\nand now our x comes up from the bottom\ninto the hidden layers into y and they\nusually draw very simplified x to h with\nc as a loop a to y where a b and c are\nthe perimeters a lot of times you'll see\nthis kind of drawing in here digging\ncloser and closer into the h and how it\nworks going from left to right you'll\nsee that the c goes in and then the x\ngoes in so the x is going upward bound\nand c is going to the right a is going\nout and c is also going out that's where\nit gets a little confusing so here we\nhave xn\ncn and then we have y out and c out and\nc is based on ht minus 1. so our value\nis based on the y and the h value are\nconnected to each other they're not\nnecessarily the same value because h can\nbe its own thing and usually we draw\nthis or we represent it as a function h\nof t equals a function of c where h of t\nminus 1 that's the last h output and x\nof t going in so it's the last output of\nh combined with the new input of x where\nh t is the new state fc is a function\nwith the parameters c that's a common\nway of denoting it h t minus 1 is the\nold state coming out and then x of t is\nan input vector at time of step t\nwell we need to cover types of recurrent\nneural networks and so the first one is\nthe most common one which is a\none-to-one single output\none-to-one neural network is usually\nknown as a vanilla neural network used\nfor regular machine learning problems\nwhy because vanilla is usually\nconsidered kind of a just a real basic\nflavor but because it's very basic a lot\nof times they'll call it the vanilla\nneural network which is not the common\nterm but it is you know kind of a slang\nterm people will know what you're\ntalking about usually if you say that\nthen we run one to mini so you have a\nsingle input and you might have a\nmultiple outputs in this case\nimage captioning as we looked at earlier\nwhere we have not just looking at it as\na dog but a dog catching a ball in the\nair and then you have mini to one\nnetwork takes in a sequence of inputs\nexamples sentiment analysis where a\ngiven sentence can be classified as\nexpressing positive or negative\nsentiments and we looked at that as we\nwere discussing if it rains look for a\nrainbow so positive sentiment where rain\nmight be a negative sentiment if you're\njust adding up the words in there and\nthen the course if you're going to do a\none-to-one mini to one one to many\nthere's many to many networks takes in a\nsequence of inputs and generates a\nsequence of outputs example machine\ntranslation so we have a lengthy\nsentence coming in english and then\ngoing out in all the different languages\nuh you know just a wonderful tool very\ncomplicated set of computations you know\nif you're a translator you realize just\nhow difficult it is to translate into\ndifferent languages one of the biggest\nthings you need to understand when we're\nworking with this neural network is\nwhat's called the vanishing gradient\nproblem while training an rnn your slope\ncan be either too small or very large\nand this makes training difficult when\nthe slope is too small the problem is\nknown as vanishing gradient and you'll\nsee here they have a nice uh image loss\nof information through time so if you're\npushing not enough information forward\nthat information is lost and then when\nyou go to train it you start losing the\nthird word in the sentence or something\nlike that or it doesn't quite follow the\nfull logic of what you're working on\nexploding gradient problem oh this is\none that runs into everybody when you're\nworking with this particular neural\nnetwork when the slope tends to grow\nexponentially instead of decaying this\nproblem is called exploding gradient\nissues in gradient problem long tracking\ntime poor performance bad accuracy and\ni'll add one more in there your computer\nif you're on a lower end computer\ntesting out a model will lock up and\ngive you the memory error explaining\ngradient problem consider the following\ntwo examples to understand what should\nbe the next word in the sequence\nthe person who took my bike and blank a\nthief the students who got into\nengineering with blank from asia and you\ncan see in here we have our x value\ngoing in we have the previous value\ngoing forward and then you back\npropagate the error like you do with any\nneural network and as we're looking for\nthat missing word maybe we'll have the\nperson took my bike and blank was a\nthief and the student who got into\nengineering with a blank were from asia\nconsider the following example the\nperson who took the bike so we'll go\nback to the person who took the bike was\nblank a thief in order to understand\nwhat would be the next word in the\nsequence the rnn must memorize the\nprevious context whether the subject was\nsingular noun or a plural noun so was a\nthief as singular the student who got\ninto engineering well in order to\nunderstand what would be the next word\nin the sequence the rnn must memorize\nthe previous context whether the subject\nwas singular noun or a plural noun and\nso you can see here the students who got\ninto engineering with blank were from\nasia it might be sometimes difficult for\nthe air to back propagate to the\nbeginning of the sequence to predict\nwhat should be the output so when you\nrun into the gradient problem we need a\nsolution the solution to the gradient\nproblem first we're going to look at\nexploding gradient where we have three\ndifferent solutions depending on what's\ngoing on one is identity initialization\nso the first thing we want to do is see\nif we can find a way to minimize the\nidentities coming in instead of having\nit identify everything just the\nimportant information we're looking at\nnext is to truncate the back propagation\nso instead of having whatever\ninformation it's sending to the next\nseries we can truncate what it's sending\nwe can lower that particular set of\nlayers make those smaller and finally is\na gradient clipping so when we're\ntraining it we can clip what that\ngradient looks like and narrow the\ntraining model that we're using when you\nhave a vanishing gradient the option\nproblem we can take a look at weight\ninitialization very similar to the\nidentity but we're going to add more\nweights in there so it can identify\ndifferent aspects of what's coming in\nbetter choosing the right activation\nfunction that's huge so we might be\nactivating based on one thing and we\nneed to limit that we haven't talked too\nmuch about activation function so we'll\nlook at that just minimally there's a\nlot of choices out there and then\nfinally there's long short term memory\nnetworks the lstms and we can make\nadjustments to that so just like we can\nclip the gradient as it comes out we can\nalso\nexpand on that we can increase the\nmemory network the size of it so it\nhandles more information and one of the\nmost common problems in today's setup is\nwhat they call long-term dependencies\nsuppose we try to predict the last word\nin the text the clouds are in the and\nyou probably said sky here we do not\nneed any further context it's pretty\nclear that the last word is going to be\nsky suppose we try to predict the last\nword in the text i have been staying in\nspain for the last 10 years i can speak\nfluent maybe you said portuguese or\nfrench no you probably said spanish the\nword we predict will depend on the\nprevious few words in context here we\nneed the context of spain to predict the\nlast word in the text it's possible that\nthe gap between the relevant information\nand the point where it is needed to\nbecome very large\nlstms help us solve this problem so the\nlstms are a special kind of recurrent\nneural network capable of learning\nlong-term dependencies remembering\ninformation for long periods of time is\ntheir default behavior all recurrent\nneural networks have the form of a chain\nof repeating modules of neural network\nconnections in standard rnns this\nrepeating module will have a very simple\nstructure such as a single tangent h\nlayer lstms\nalso have a chain-like structure but the\nrepeating module has a different\nstructure instead of having a single\nneural network layer there are four\ninteracting layers communicating in a\nvery special way lstms are a special\nkind of recurrent neural network capable\nof learning long-term dependencies\nremembering information for long periods\nof time is their default behavior ls\ntmss also have a chain like structure\nbut the repeating module has a different\nstructure instead of having a single\nneural network layer there are four\ninteracting layers communicating in a\nvery special way as you can see the\ndeeper we dig into this the more\ncomplicated the graphs get in here i\nwant you to note that you have x of t\nminus one coming in you have x of t\ncoming in and you have x at t plus one\nand you have h of t minus one and h of t\ncoming in and h of t plus one going out\nand of course uh on the other side is\nthe output a um in the middle we have\nour tangent h but it occurs in two\ndifferent places so not only when we're\ncomputing the exit t plus one or we\ngetting the tangent h from x to t but\nwe're also getting that value coming in\nfrom the x of t minus one so the short\nof it is as you look at these layers not\nonly does it does the propagate through\nthe first layer goes into the second\nlayer back into itself but it's also\ngoing into the third layer so now we're\nkind of stacking those up and this can\nget very complicated as you grow that in\nsize it also grows in memory too and in\nthe amount of resources it takes but\nit's a very powerful tool to help us\naddress the problem of complicated long\nsequential information coming in like we\nwere just looking at in the sentence and\nwhen we're looking at our long short\nterm memory network uh there's three\nsteps of processing sensing in the lstms\nthat we look at the first one is we want\nto forget irrelevant parts of the\nprevious state you know a lot of times\nlike you know is as in a unless we're\ntrying to look at whether it's a plural\nnoun or not they don't really play a\nhuge part in the language so we want to\nget rid of them then selectively update\ncell state values so we only want to\nupdate the cell state values that\nreflect what we're working on and\nfinally we want to put only output\ncertain parts of the cell state so\nwhatever is coming out we want to limit\nwhat's going out too and let's dig a\nlittle deeper into this let's just see\nwhat this really looks like uh so step\none decides how much of the past it\nshould remember first step in the lstm\nis to decide which information to be\nomitted in from the cell in that\nparticular time step it is decided by\nthe sigmoid function it looks at the\nprevious state h to t minus 1 and the\ncurrent input x t and computes the\nfunction so you can see over here we\nhave a function of t\nequals the sigmoid function of the\nweight of f the h at t minus 1 and then\nx a t plus of course you have a bias in\nthere with any of our neural networks so\nwe have a bias function so f of t equals\nforget gate decides which information to\ndelete that is not important from the\nprevious time step considering an stm is\nfed with the following inputs from the\nprevious and present time step alice is\ngood in physics john on the other hand\nis good in chemistry so previous output\njohn plays football well he told me\nyesterday over the phone that he had\nserved as a captain of his college\nfootball team that's our current input\nso as we look at this the first step is\nthe forget gate realizes there might be\na change in context after encountering\nthe first full stop compares with the\ncurrent input sentence of x a t so we're\nlooking at that full stop and then\ncompares it with the input of the new\nsentence the next sentence talks about\njohn so the information on alice is\ndeleted okay that's important to know so\nwe have this input coming in and if\nwe're going to continue on with john\nthen that's going to be the primary\ninformation we're looking at the\nposition of the subject is vacated and\nis assigned to john and so in this one\nwe've seen that we've weeded out a whole\nbunch of information and we're only\npassing information on john since that's\nnow the new topic so step two is then to\ndecide how much should this unit add to\nthe current state in the second layer\nthere are two parts one is the sigmoid\nfunction and the other is the tangent h\nin the sigmoid function it decides which\nvalues to let through 0 or 1. tangent h\nfunction gives a weightage to the values\nwhich are passed deciding their level of\nimportance minus one to one and you can\nsee the two formulas that come up uh the\ni of t equals the sigmoid of the weight\nof i h to t minus one x of t plus the\nbias of i and the c of t equals the\ntangent of h of the weight of c of h to\nt minus 1 x of t plus the bias of c so\nour i of t equals the input gate\ndetermines which information to let\nthrough based on its significance in the\ncurrent time step if this seems a little\ncomplicated don't worry because a lot of\nthe programming is already done when we\nget to the case study understanding\nthough that this is part of the program\nis important when you're trying to\nfigure out these what to set your\nsettings at you should also note when\nyou're looking at this it should have\nsome semblance to your forward\npropagation neural networks where we\nhave a value assigned to a weight plus a\nbias very important steps than any of\nthe neural network layers whether we're\npropagating into them the information\nfrom one to the next or we're just doing\na straightforward neural network\npropagation let's take a quick look at\nthis what it looks like from the human\nstandpoint as i step out of my suit\nagain consider the current input at x of\nt john plays football well he told me\nyesterday over the phone that he had\nserved as a captain of his college\nfootball team that's our input input\ngate analysis the important information\njohn plays football and he was a captain\nof his college team is important he told\nme over the phone yesterday is less\nimportant hence it is forgotten this\nprocess of adding some new information\ncan be done via the input gate now this\nexample is as a human form and we'll\nlook at training this stuff in just a\nminute but as a human being if i wanted\nto get this information from a\nconversation maybe it's a google voice\nlistening in on you or something like\nthat um how do we weed out the\ninformation that he was talking to me on\nthe phone yesterday well i don't want to\nmemorize that he talked to me on the\nphone yesterday or maybe that is\nimportant but in this case it's not i\nwant to know that he was the captain of\nthe football team i want to know that he\nserved i want to know that john plays\nfootball and he was a captain of the\ncollege football team those are the two\nthings that i want to take away as a\nhuman being again we measure a lot of\nthis from the human viewpoint and that's\nalso how we try to train them so we can\nunderstand these neural networks finally\nwe get to step three decides what part\nof the current cell state makes it to\nthe output the third step is to decide\nwhat will be our output first we run a\nsigmoid layer which decides what parts\nof the cell state make it to the output\nthen we put the cell state through the\ntangent h to push the values to be\nbetween -1 and 1 and multiply it by the\noutput of the sigmoid gate so when we\ntalk about the output of t we set that\nequal to the sigmoid of the weight of 0\nof the h of t minus 1 and back one step\nin time by the x of t plus of course the\nbias the h of t equals the outer t times\nthe tangent of the tangent h of c of t\nso our o t equals the output gate allows\nthe passed in information to impact the\noutput in the current time step let's\nconsider the example to predicting the\nnext word in the sentence john played\ntremendously well against the opponent\nand one for his team for his\ncontributions brave blank was awarded\nplayer of the match there could be a lot\nof choices for the empty space current\ninput brave is an adjective adjectives\ndescribe a noun john could be the best\noutput after brave thumbs up for john\nawarded player of the match and if you\nwere to pull just the nouns out of the\nsentence team doesn't look right because\nthat's not really the subject we're\ntalking about contributions you know\nbrave contributions or brave teen brave\nplayer brave match\nso you look at this and you can start to\ntrain this these this neural network so\nstarts looking at and goes oh no john is\nwhat we're talking about so brave is an\nadjective\njohn's going to be the best output and\nwe give john a big thumbs up and then of\ncourse we jump into my favorite part the\ncase study use case implementation of\nlstm let's predict the prices of stocks\nusing the lstm network based on the\nstock price data between 2012 2016.\nwe're going to try to predict the stock\nprices of 2017\nand this will be a narrow set of data\nwe're not going to do the whole stock\nmarket it turns out that the new york\nstock exchange generates roughly three\nterabytes of data per day that's all the\ndifferent trades up and down of all the\ndifferent stocks going on and each\nindividual one\nsecond to second or nanosecond to\nnanosecond but we're going to limit that\nto just some very basic fundamental\ninformation so don't think you're going\nto get rich off this today but at least\nyou can give an eye you can give a step\nforward in how to start processing\nsomething like stock prices a very valid\nuse for machine learning in today's\nmarkets\nuse case implementation of lstm let's\ndive in we're going to import our\nlibraries we're going to import the\ntraining set and get the scaling going\nnow if you watch any of our other\ntutorials a lot of these pieces just\nstart to look very familiar because it's\nvery similar setup let's take a look at\nthat and just reminder we're going to be\nusing anaconda the jupiter notebook so\nhere i have my anaconda navigator when\nwe go under environments i've actually\nset up a cross python 36 i'm in python36\nand nice thing about anaconda especially\nthe newer version i remember a year ago\nmessing with anaconda in different\nversions of python in different\nenvironments anaconda now has a nice\ninterface\nand i have this installed both on a\nubuntu linux machine and on windows so\nit works fine on there you can go in\nhere and open a terminal window and then\nin here once you're in the terminal\nwindow this is where you're going to\nstart\ninstalling using pip to install your\ndifferent modules and everything now\nwe've already pre-installed them so we\ndon't need to do that in here but if you\ndon't have them installed in your\nparticular environment you'll need to do\nthat and of course you don't need to use\nthe anaconda or the jupiter you can use\nwhatever favorite python id you like i'm\njust a big fan of this because it keeps\nall my stuff separate you can see on\nthis machine i have specifically\ninstalled one for cross since we're\ngoing to be working with cross under\ntensorflow we go back to home i've gone\nup here to application and that's the\nenvironment i've loaded on here and then\nwe'll click on the launch jupiter\nnotebook now i've already in my jupiter\nnotebook\nhave set up a lot of stuff so that we're\nready to go kind of like martha\nstewart's in the old cooking show so we\nwant to make sure we have all our tools\nfor you so you're not waiting for them\nto load and if we go up here to where it\nsays new you can see where you can\ncreate a new python 3. that's what we\ndid here underneath the setup so it\nalready has all the modules installed on\nit and i'm actually renamed this if you\ngo under file you can rename it we've\ni'm calling it rnn stock and let's just\ntake a look and start diving into the\ncode let's get into the exciting part\nnow we've looked at the tool and of\ncourse you might be using a different\ntool which is fine let's start putting\nthat code in there and seeing what those\nimports and uploading everything looks\nlike now first half is kind of boring\nwhen we hit the run button because we're\ngoing to be importing numpy as np that's\nuh the number python which is your numpy\narray and the matplot library because\nwe're going to do some plotting at the\nend\nand our pandas for our data set our\npandas as pd and when i hit run it\nreally doesn't do anything except for\nload those modules just a quick note let\nme just do a quick draw here oops shift\nalt there we go you'll notice when we're\ndoing this setup if i was to divide this\nup oops i'm going to actually let's\noverlap these here we go\nthis first part that we're going to do\nis\nour data\nprep\na lot of prepping involved\num in fact depending on what your system\nis since we're using karass i put an\noverlap here\nbut you'll find that almost\nmaybe even half of the code we do is all\nabout the data prep and the reason i\noverlapped this with uh cross let me\njust put that down because that's what\nwe're working in uh is because cross has\nlike their own preset stuff so it's\nalready pre-built in which is really\nnice so there's a couple steps a lot of\ntimes that are in the kara setup we'll\ntake a look at that to see what comes up\nin our code as we go through and look at\nstock and the last part is to evaluate\nand if you're working with\nshareholders or\nclassroom whatever it is you're working\nwith uh the evaluate is the next biggest\npiece um so the actual code here crossed\nis a little bit more but when you're\nworking with some of the other packages\nyou might have like three lines that\nmight be it all your stuff is in your\npre-processing and your data since cross\nhas is is cutting edge and you load the\nindividual layers you'll see that\nthere's a few more lines here and\ncrosses a little bit more robust and\nthen you spin a lot of times like i said\nwith the evaluate you want to have\nsomething you present to everybody else\nand say hey this is what i did this is\nwhat it looks like so let's go through\nthose steps this is like a kind of just\ngeneral overview and let's just take a\nlook and see what the next set of code\nlooks like and in here we have a data\nset train and it's going to be read\nusing the pd or\npandas.readcsv and it's a\ngooglestockpricetrain.csv\nand so under this we have trainingset\nequals datasettrain.ilocation\nand we've kind of sorted out part of\nthat so what's going on here let's just\ntake a look at let's look at the actual\nfile and see what's going on there now\nif we look at this uh ignore all the\nextra files on this i already have a\ntrain and a test set where it's sorted\nout this is important to notice because\na lot of times we do that as part of the\npre-processing of the data we take\n20 of the data out so we can test it and\nthen we train the rest of it that's what\nwe use to create our neural network that\nway we can find out how good it is uh\nbut let's go ahead and just take a look\nand see what that looks like as far as\nthe file itself and i went ahead and\njust opened this up in a basic word pad\nand text editor just so we can take a\nlook at it certainly you can open up an\nexcel or any other kind of spreadsheet\nand we note that this is a comma\nseparated variables we have a date uh\nopen high low close volume this is the\nstandard stuff that we import into our\nstock or the most basic set of\ninformation you can look at in stock\nit's all free to download in this case\nwe downloaded it from google that's why\nwe call it the google stock price\nand this specifically is google this is\nthe google stock values from as you can\nsee here we started off at 1 3 2012.\nso when we look at this first setup up\nhere\nwe have a data set train equals pd\nunderscore csv and if you noticed on the\noriginal frame\nlet me just go back there\nthey had it set to home ubuntu downloads\ngoogle stock price train i went ahead\nand changed that because we're in the\nsame file where i'm running the code so\ni've saved this particular python code\nand i don't need to go through any\nspecial paths or have the full path on\nthere and then of course we want to take\nout\ncertain values in here and you're going\nto notice that we're using\nour data set\nand we're now in pandas\nso pandas basically it looks like a\nspreadsheet\nand in this case we're going to do i\nlocation which is going to get specific\nlocations the first value is going to\nshow us that we're pulling all the rows\nin the data and the second one is we're\nonly going to look at columns one and\ntwo and if you remember here from our\ndata as we switch back on over columns\nwe saw we start with zero which is the\ndate\nand we're going to be looking at open\nand high which would be one and two\nwe'll just label that right there so you\ncan see now\nwhen you go back and do this you\ncertainly can extrapolate and do this on\nall the columns\nbut for the example let's just limit a\nlittle bit here so that we can focus on\njust some\nkey aspects of stock\nand then we'll go up here and run the\ncode and again i said the first half is\nvery boring whenever we hit the run\nbutton it doesn't do anything because\nwe're still just loading the data and\nsetting it up now that we've loaded our\ndata we want to go ahead and scale it we\nwant to do what they call feature\nscaling and in here we're going to pull\nit up from the sk learn or the sk kit\npre-processing import min max scalar and\nwhen you look at this you got to\nremember that biases in our data we want\nto get rid of that so if you have\nsomething that's like a really high\nvalue let's just draw a quick graph\nand i have something here like the maybe\nthe stock has a value one stock has a\nvalue of a hundred and another stock has\na value of five\num\nyou start to get a bias between\ndifferent stocks and so when we do this\nwe go ahead and say okay 100 is going to\nbe the max\nand 5 is going to be the min\nand then everything else goes and then\nwe change this so we just squish it down\ni like the word squish so it's between 1\nand 0. so 100 equals one or one equals a\nhundred and zero equals five and you can\njust multiply it's usually just a simple\nmultiplication we're using uh\nmultiplication so it's going to be uh\nminus five and then 100 divided or 95\ndivided by one so or whatever value is\nis divided by ninety-five\nand uh once we've actually created our\nscale we've tolling is going to be from\nzero to one we wanna take our training\nset and we're gonna create a training\nset scaled and we're gonna use our\nscalar sc we're going to fit we're going\nto fit and transform the training set uh\nso we can now use the sc this this\nparticular object we'll use it later on\nour testing set because remember we have\nto also scale that when we go to test\nour model and see how it works\nand we'll go ahead and click on the run\nagain it's not going to have any output\nyet because we're just setting up all\nthe variables\nokay so we pasted the data in here and\nwe're going to create the data structure\nwith the 60 time steps and output\nfirst note we're running 60 time steps\nand that is where this value here also\ncomes in so the first thing we do is we\ncreate our x train and y train variables\nwe set them to an empty python array\nvery important to remember what kind of\narray we're in what we're working with\nand then we're going to come in here\nwe're going to go for i in range 60 to\n1258 there's our 60 60 time steps and\nthe reason we want to do this is as\nwe're adding the data in there there's\nnothing below the 60 so if we're going\nto use 60 time steps we have to start at\npoint 60 because it includes everything\nunderneath of it otherwise you'll get a\npointer error and then we're going to\ntake our x train and we're going to\nappend training set scaled this is a\nscaled value between 0 and 1. and then\nas i is equal to 60 this value is going\nto be\n60 minus 60 is 0. so this actually is\n0 to i so it's going to be 0 60 1 to 61.\nlet me just circle this part right here\n1 to 61\n2 to 62 and so on and so on and if you\nremember i said 0 to 60 that's incorrect\nbecause it does not count remember it\nstarts at 0 so this is a count of 60. so\nit's actually 59. important to remember\nthat as we're looking at this and then\nthe second part of this that we're\nlooking at so if you remember correctly\nhere we go we go from 0 to 59 of i and\nthen we have a comma a 0 right here and\nso finally we're just going to look at\nthe open value now i know we did put it\nin there for one to two\nif you move quickly it doesn't count the\nsecond one so it's just the open value\nwe're looking at just open\nand then finally we have y train dot\nappend training set i to zero and if you\nremember correctly i two or i comma zero\nif you remember correctly this is 0 to\n59 so there's 60 values in it so we do i\ndown here this is number 60. so we're\ngoing to do this is we're creating an\narray and we have 0\nto 59\nand over here we have number 60 which is\ngoing into the y train it's being\nappended on there and then this just\ngoes all the way up so this is down here\nis a\n0 to 59 and we'll call it 60 since\nthat's the value over here and it goes\nall the way up to 12\n58. that's where this value here comes\nin that's the length of the data we're\nloading\nso we've loaded two arrays we've loaded\none array that has\nwhich is filled with arrays from 0 to 59\nand we loaded one array which is just\nthe value and what we're looking at you\nwant to think about this as a time\nsequence uh here's my open open open\nopenopenopen what's the next one in the\nseries so we're looking at the google\nstock and each time it opens we want to\nknow what the next one 0 through 59\nwhat's 60 1 through 60 what's 61. 2\nthrough 62 what's 62 and so on and so on\ngoing up and then once we've loaded\nthose in our for loop we go ahead and\ntake x-train and y-train equals np.array\nx-train dot np array y-train we're just\nconverting this back into a numpy array\nthat way we can use all the cool tools\nthat we get with numpy array including\nreshaping so if we take a look and see\nwhat's going on here we're going to take\nour x train\nwe're going to reshape it\nwow what the heck does reshape mean\nthat means we have an array if you\nremember correctly\nso many numbers by 60.\nthat's how wide it is\nand so we're when you when you do x\ntrain dot shape that gets one of the\nshapes and you get um x train dot shape\nof one gets the other shape and we're\njust making sure the data is formatted\ncorrectly and so you use this to pull\nthe fact that it's 60 by\nin this case where's that value\n60 by 1199\n1258 minus 60 1199 and we're making sure\nthat that is shaped correctly so the\ndata is grouped into\n11 99 by 60 different arrays and then\nthe one on the end just means at the end\nbecause this when you're dealing with\nshapes and numpy they look at this as\nlayers and so the in layer needs to be\none value that's like the leaf of a tree\nwhere this is the branch and then it\nbranches out some more um and then you\nget the leaf np reshape comes from and\nusing the existing shapes to form it\nwe'll go ahead and run this piece of\ncode again there's no real output and\nthen we'll import our different cross\nmodules that we need so from cross\nmodels we're going to import the\nsequential model dealing with sequential\ndata we have our dense layers we have\nactually three layers we're going to\nbring in our dents our lstm which is\nwhat we're focusing on and our dropout\nand we'll discuss these three layers\nmore in just a moment but you do need\nthe with the lstm you do need the\ndropout and then the final layer will be\nthe dents but let's go ahead and run\nthis and they'll bring port our modules\nand you'll see we get an error on here\nand if you read it closer it's not\nactually an error it's a warning what\ndoes this warning mean these things come\nup all the time when you're working with\nsuch cutting edge modules that are\ncompletely being updated all the time\nwe're not going to worry too much about\nthe warning all it's saying is that the\nh5py\nmodule which is part of cross is going\nto be updated at some point and if\nyou're running new stuff on cross and\nyou start updating your cross system you\nbetter make sure that your h5 pi is\nupdated too otherwise you're going to\nhave an error later on and you can\nactually just run an update on the h5 pi\nnow if you wanted to not a big deal\nwe're not going to worry about that\ntoday and i said we were going to jump\nin and start looking at what those\nlayers mean i meant that and we're going\nto start off with initializing the rnn\nand then we'll start adding those layers\nin and you'll see that we have the lstm\nand then the dropout lstm then dropout\nlstm then dropout what the heck is that\ndoing so let's explore that we'll start\nby initializing the rnn regressor equals\nsequential because we're using the\nsequential model and we'll run that and\nload that up and then we're going to\nstart adding our lstm layer and some\ndropout regularization and right there\nshould be the q dropout regularization\nand if we go back here and remember our\nexploding gradient well that's what\nwe're talking about the dropout drops\nout unnecessary data so we're not just\nshifting huge amounts of data through\nthe network so and so we go in here\nlet's just go ahead and add this in i'll\ngo ahead and run this and we had three\nof them so let me go ahead and put all\nthree of them in and then we can go back\nover them there's the second one and\nlet's put one more in let's put that in\nand we'll go ahead and put two more in i\nmean i said one more in but it's\nactually two more in and then let's add\none more after that\nand as you can see each time i run these\nthey don't actually have an output so\nlet's take a closer look and see what's\ngoing on here so we're going to add our\nfirst lstm layer in here we're going to\nhave units 50. the units is the positive\ninteger and it's the dimensionality of\nthe output space this is what's going\nout into the next layer so we might have\n60 coming in but we have 50 going out we\nhave a return sequence because it is a\nsequence data so we want to keep that\ntrue and then you have to tell it what\nshape it's in well we already know the\nshape by just going in here and looking\nat x train shape so input shape equals\nthe x train shape of one comma one it\nmakes it really easy you don't have to\nremember all the numbers that put in 60\nor whatever else is in there you just\nlet it tell the regressor what model to\nuse and so we follow our stm with a\ndropout layer now understanding the\ndropout layer is kind of exciting\nbecause one of the things that happens\nis we can over train our network that\nmeans that our neural network will\nmemorize such specific data that it has\ntrouble predicting anything that's not\nin that specific realm to fix for that\neach time we run through the training\nmode we're going to take 0.2 or 20\npercent of our neurons they just turn\nthem off so we're only going to train on\nthe other ones and it's going to be\nrandom that way each time we pass\nthrough this we don't over train these\nnodes come back in in the next training\ncycle we randomly pick a different 20.\nand finally i see a big difference as we\ngo from the first to the second and\nthird and fourth the first thing is we\ndon't have to input the shape because\nthe shapes already the output units is\n50 here this item the next step\nautomatically knows this layer is\nputting out 50 and because it's the next\nlayer it automatically sets that and\nsays so 50 is coming out from our last\nlayer that's coming up you know goes\ninto the regressor and of course we have\nour dropout and that's what's coming\ninto this one and so on and so on and so\nthe next three layers we don't have to\nlet it know what the shape is it\nautomatically understands that and we're\ngonna keep the units the same we're\nstill gonna do 50 units it's still a\nsequence coming through 50 units and a\nsequence now the next piece of code is\nwhat brings it all together let's go\nahead and take a look at that and we\ncome in here we put the output layer the\ndense layer and if you remember up here\nwe had the three layers we had lstm\ndropout and dents dense just says we're\ngoing to bring this all down into one\noutput instead of putting out a sequence\nwe just know i want to know the answer\nat this point and let's go ahead and run\nthat and so in here you notice all we're\ndoing is setting things up one step at a\ntime so far we've brought in our way up\nhere we brought in our data we brought\nin our different modules we formatted\nthe data for training it we set it up\nyou know we have our y x train and our y\ntrain we have our source of data and the\nanswers where we know so far that we're\ngoing to put in there we've reshaped\nthat we've come in and built our cross\nwe've imported our different layers and\nwe have in here if you look we have what\nuh five total layers now cross is a\nlittle different than a lot of other\nsystems a lot of other systems put this\nall in one line and do it automatic but\nthey don't give you the options of how\nthose layers interface and they don't\ngive you the options of how the data\ncomes in cross is cutting edge for this\nreason so even though there's a lot of\nextra steps in building the model this\nhas a huge impact on the output and what\nwe can do with this these new models\nfrom cross so we brought in our dense we\nhave our full model put together a\nregressor so we need to go ahead and\ncompile it and then we're going to go\nahead and fit the data we're going to\ncompile the pieces so they all come\ntogether and then we're going to run our\ntraining data on there and actually\nrecreate our regressor so it's ready to\nbe used so let's go ahead and compile\nthat and i can go ahead and run that and\nif you've been looking at any of our\nother tutorials on neural networks\nyou'll see we're going to use the\noptimizer atom atom is optimized for big\ndata there's a couple other optimizers\nout there beyond the scope of this\ntutorial but certainly atom will work\npretty good for this and loss equals\nmean squared value so when we're\ntraining it this is what we want to base\nthe loss on how bad is our error we're\ngoing to use the mean squared value for\nour error and the atom optimizer for its\ndifferential equations you don't have to\nknow the math behind them but certainly\nit helps to know what they're doing and\nwhere they fit into the bigger models\nand then finally we're going to do our\nfit fitting the rn into the training set\nwe have the regressor.fit x train y\ntrain epochs and batch size so we know\nwhere this is this is our data coming in\nfor the x train our y train is the\nanswer we're looking for of our data our\nsequential input epics is how many times\nwe're going to go over the whole data\nset we created a whole data set of x\ntrain so this is each each of those rows\nwhich includes a time sequence of 60.\nand bad size another one of those things\nwhere cross really shines is if you were\npulling the save from a large file\ninstead of trying to load it all into\nram it can now pick smaller batches up\nand load those indirectly we're not\nworried about pulling them off a file\ntoday because this isn't big enough to\ncause a computer too much of a problem\nto run not too straining on the\nresources but as we run this you can\nimagine what happened if i was doing a\nlot more than just one column in one set\nof stock in this case google stock\nimagine if i was doing this across all\nthe stocks and i had instead of just the\nopen i had open close high low and you\ncan actually find yourself with about 13\ndifferent variables times 60 because\nit's a time sequence suddenly you find\nyourself with a gig of memory you're\nloading into your ram which will just\ncompletely you know if it's just if\nyou're not on multiple computers or\ncluster you can start running into\nresource problems but for this we don't\nhave to worry about that so let's go\nahead and run this and this will\nactually take a little bit on my\ncomputer because it's an older laptop\nand give it a second to kick in there\nthere we go all right so we have epic so\nthis is going to tell me it's running\nthe first run through all the data and\nas it's going through it's batching them\nin 32 pieces so 32 lines each time and\nthere's 1198 i think i said 11.99\nearlier but it's 11.98 i was off by one\nand each one of these is 13 seconds so\nyou can imagine this is roughly 20 to 30\nminutes run time on this computer like i\nsaid it's an older laptop running at uh\n0.9 gigahertz on a dual processor and\nthat's fine what we'll do is i'll go\nahead and stop go get a drink of coffee\nand come back and let's see what happens\nat the end and where this takes us and\nlike any good cooking show\ni've kind of gotten my latte i also have\nsome other stuff running in the\nbackground so you'll see these numbers\njumped up to like 19 seconds 15 seconds\nbut you can scroll through and you can\nsee we've run it through 100 steps or\n100 epics so the question is what does\nall this mean one of the first things\nyou'll notice is that our loss can is\nover here it kind of stopped at 0.0014\nbut you can see it kind of goes down\nuntil we hit about 0.014 three times in\na row so we guessed our epic pretty\nclose since our losses remain the same\non there so to find out we're looking at\nwe're going to go ahead and load up our\ntest data the test data that we didn't\nprocess yet and a real stock price data\nset test eye location this is the same\nthing we did when we prepped the data in\nthe first place so let's go ahead and go\nthrough this code and we can see we've\nlabeled it part three making the\npredictions and visualizing the results\nso the first thing we need to do is go\nahead and read the data in from our test\ncsv you see i've changed the path on it\nfor my computer and\nthen we'll call it the real stock price\nand again we're doing just the one\ncolumn here and the values from i\nlocation so it's all the rows and just\nthe values from these that one location\nthat's the open stock open let's go\nahead and run that so that's loaded in\nthere and then let's go ahead and create\nwe have our inputs we're going to create\ninputs here and this should all look\nfamiliar because this is the same thing\nwe did before we're going to take our\ndata set total we're going to do a\nlittle pandas concat from the datastate\ntrain now remember the end of the\ndataset train is part of the data going\nin let's just visualize that just a\nlittle bit here's our train data let me\njust put tr for train and it went up to\nthis value here but each one of these\nvalues generated a bunch of columns it\nwas 60 across and this value here equals\nthis one and this value here equals this\none and this value here equals this one\nand so we need these top 60 to go into\nour new data so to find out what we're\nlooking at we're going to go ahead and\nload up our test data the test data that\nwe didn't process yet and a real stock\nprice data set test eye location this is\nthe same thing we did when we prepped\nthe data in the first place so let's go\nahead and go through this code and we\ncan see we've labeled it part three\nmaking the predictions and visualizing\nthe results so the first thing that we\nneed to do is go ahead and read the data\nin from our test csv you see i've\nchanged the path on it for my computer\nand then we'll call it the real stock\nprice and again we're doing just the one\ncolumn here and the values from i\nlocation so it's all the rows and just\nthe values from these that one location\nthat's the open stock open let's go\nahead and run that so that's loaded in\nthere and then let's go ahead and create\nwe have our inputs we're going to create\ninputs here and this should all look\nfamiliar this is the same thing we did\nbefore we're going to take our data set\ntotal we're going to do a little panda\nconcat from the datastate train now\nremember the end of the data set train\nis part of the data going in let's just\nvisualize that just a little bit here's\nour train data let me just put tr for\ntrain and it went up to this value here\nbut each one of these values generated a\nbunch of columns it was 60 across and\nthis value here equals this one and this\nvalue here equals this one and this\nvalue here equals this one and so we\nneed these top 60 to go into our new\ndata because that's part of the next\ndata or it's actually the top 59. so\nthat's what this first setup is over\nhere is we're going in we're doing the\nreal stock price and we're going to just\ntake the data set test and we're going\nto load that in and then the real stock\nprice is our data test.test location so\nwe're just looking at that first column\nthe open price and then our data set\ntotal we're going to take pandas and\nwe're going to concat and we're going to\ntake our data set train for the open and\nour dataset test open and this is one\nway you can reference these columns\nwe've referenced them a couple different\nways we've referenced them up here with\nthe one two but we know it's labeled as\na panda set as open so pandas is great\nthat way lots of versatility there and\nwe'll go ahead and go back up here and\nrun this there we go and you'll notice\nthis is the same as what we did before\nwe have our open data set where pended\nour two different or concatenated our\ntwo data sets together we have our\ninputs equals data set total length data\nset total minus length data set minus\ntest minus 60 values so we're going to\nrun this over all of them and you'll see\nwhy this works because normally when\nyou're running your test set versus your\ntraining set you run them completely\nseparate but when we graph this you'll\nsee that we're just going to be we'll be\nlooking at the part that we didn't train\nit with to see how well it graphs and we\nhave our inputs equals inputs dot\nreshapes or reshaping like we did before\nwe're transforming our inputs so if you\nremember from the transform between zero\nand one and uh finally we want to go\nahead and take our x test and we're\ngoing to create that x test and for i in\nrange 60 to 80. so here's our x test and\nwe're appending our inputs i to 60 which\nremember is 0 to 59 and i comma 0 on the\nother side so it's just the first column\nwhich is our open column and once again\nwe take our x test we convert it to a\nnumpy array we do the same reshape we\ndid before and then we get down to the\nfinal two lines and here we have\nsomething new right here on these last\ntwo lines let me just highlight those or\nor mark them predicted stock price\nequals regressor dot predicts x test so\nwe're predicting all the stock including\nboth the training and the testing model\nhere and then we want to take this\nprediction and we want to inverse the\ntransform so remember we put them\nbetween zero and one well that's not\ngoing to mean very much to me to look at\na float number between zero and one i\nwant the dollar amount so i want to know\nwhat the cash value is and we'll go\nahead and run this and you'll see it\nruns much quicker than the training\nthat's what's so wonderful about these\nneural networks once you put them\ntogether it takes just a second to run\nthe same neural network that took us\nwhat a half hour to train add and plot\nthe data we're going to plot what we\nthink it's going to be and we're going\nto plot it against the real data what\nthe google stock actually did so let's\ngo ahead and take a look at that in code\nand let's uh pull this code up so we\nhave our plt that's our oh if you\nremember from the very beginning let me\njust go back up to the top we have our\nmatplot library.pipeline as plt that's\nwhere that comes in and we come down\nhere we're going to plot let me get my\ndrawing thing out again we're going to\ngo ahead and plt is basically kind of\nlike an object it's one of the things\nthat always threw me when i'm doing\ngraphs in python because i always think\nyou have to create an object and then it\nloads that class in there well in this\ncase plt is like a canvas you're putting\nstuff on so if you've done html5 you'll\nhave the canvas object this is the\ncanvas so we're going to plot the real\nstock price that's what it actually is\nand we're going to give that color red\nso it's going to be in bright red we're\ngoing to label it real google stock\nprice and then we're going to do our\npredicted stock and we're going to do it\nin blue and it's going to be labeled\npredicted and we'll give it a title it's\nalways nice to give a title to your\ngraph especially if you're going to\npresent this to somebody you know to\nyour shareholders in the office and the\nx label is going to be time because it's\na time series and we didn't actually put\nthe actual date and times on here but\nthat's fine we just know they're\nincremented by time and then of course\nthe y label is the actual stock price\nplt.legend tells us to build the legend\non here so that the color red and real\ngoogle stock price show up on there and\nthen the plot shows us that actual graph\nso let's go ahead and run this and see\nwhat that looks like and you can see\nhere we have a nice graph and let's talk\njust a little bit about this graph\nbefore we wrap it up here's our legend i\nwas telling you about that's why we have\nthe legend to show the prices we have\nour title and everything and you'll\nnotice on the bottom we have a time\nsequence we didn't put the actual time\nin here now we could have we could have\ngone ahead and\nplotted the x since we know what the\ndates are and plotted this to dates but\nwe also know that it's only the last\npiece of data that we're looking at so\nlast piece of data which in somewhere\nprobably around here on the graph i\nthink it's like about 20 of the data\nprobably less than that we have the\ngoogle price and the google price has\nthis little up jump and then down and\nyou'll see that the actual google\ninstead of a turn down here just didn't\ngo up as high and didn't load go\ndown so our prediction has the same\npattern but the overall value is pretty\nfar off as far as um stock but then\nagain we're only looking at one column\nwe're only looking at the open price\nwe're not looking at how many volumes\nwere traded like i was pointing out\nearlier we talk about stock just right\noff the bat there's six columns there's\nopen high low close volume then there's\nweather i mean volume shares then\nthere's the adjusted open adjusted high\nadjusted low adjusted close they have a\nspecial formula to predict exactly what\nit would really be worth based on the\nvalue of the stock and then from there\nthere's all kinds of other stuff you can\nput in here so we're only looking at one\nsmall aspect the opening price of the\nstock and as you can see here we did a\npretty good job this curve follows the\ncurve pretty well it has like a little\njumps on it bins they don't quite match\nup so this bin here does not quite match\nup with that bin there but it's pretty\ndarn close we have the basic shape of it\nand the prediction isn't too far off and\nyou can imagine that as we add more data\nin and look at different aspects in the\nspecific domain of stock we should be\nable to get a better representation each\ntime we drill in deeper of course this\ntook a half hour for my program my\ncomputer to train so you can imagine\nthat if i was running it across all\nthose different variables it might take\na little bit longer to train the data\nnot so good for doing a quick tutorial\nlike this so we covered a lot of really\ncool things in this tutorial today\nhopefully you'll be able to get rich\npredicting stock or maybe get a job if\nyou are familiar with the stock domain\nand business domain but you can see how\nthis will be applied to all kinds of\ndifferent uh tools and trades across\ndifferent domains so the biggest\ntakeaways are as we had an introduction\nto the rnn and it's set up on there we\nwent over the popular neural networks\nthat are out there we discussed what is\na recurrent neural network we went into\nuh one of the big problems with the rnn\nand that's the exploding gradient\nproblem we also discussed the long short\nshort-term memory networks which is part\nof what we're working on that's the the\nlst which is part of the rnn setup and\nfinally we went through and we predicted\nthe google stock to see how it did so i\nwant to thank you for joining us today\nagain my name is richard kirschner one\nof the simply learn team that's\nwww.simplylearn.com for more information\nplease visit our website feel free to\nask any questions and check out our\ndifferent courses we have you can also\nplace comments below in the youtube\nvideo and we will keep monitoring those\nand try to address those again thank you\nfor joining us today\nhi there if you like this video\nsubscribe to the simply learn youtube\nchannel and click here to watch similar\nvideos turn it up and get certified\nclick here\n",
  "words": [
    "welcome",
    "rnn",
    "tutorial",
    "recurrent",
    "neural",
    "network",
    "name",
    "richard",
    "kirschner",
    "simplylearn",
    "team",
    "get",
    "certified",
    "get",
    "ahead",
    "start",
    "course",
    "fundamentals",
    "neural",
    "network",
    "popular",
    "neural",
    "networks",
    "important",
    "know",
    "framework",
    "going",
    "looking",
    "specifically",
    "touch",
    "recurrent",
    "neural",
    "network",
    "recurrent",
    "neural",
    "network",
    "rn",
    "work",
    "one",
    "big",
    "things",
    "rnns",
    "call",
    "vanishing",
    "exploding",
    "gradient",
    "problem",
    "look",
    "going",
    "using",
    "use",
    "case",
    "study",
    "going",
    "keras",
    "tensorflow",
    "cross",
    "python",
    "module",
    "neural",
    "networks",
    "deep",
    "learning",
    "call",
    "long",
    "short",
    "term",
    "memory",
    "lstm",
    "use",
    "use",
    "case",
    "implement",
    "lstm",
    "keras",
    "see",
    "lstm",
    "basically",
    "rnn",
    "network",
    "get",
    "use",
    "case",
    "always",
    "favorite",
    "part",
    "dive",
    "going",
    "take",
    "look",
    "rnn",
    "introduction",
    "rnn",
    "know",
    "google",
    "auto",
    "complete",
    "feature",
    "predicts",
    "rest",
    "words",
    "user",
    "typing",
    "love",
    "auto",
    "complete",
    "feature",
    "typing",
    "away",
    "saves",
    "lot",
    "time",
    "kind",
    "hit",
    "enter",
    "key",
    "auto",
    "fills",
    "everything",
    "type",
    "much",
    "well",
    "first",
    "collection",
    "large",
    "volumes",
    "frequently",
    "occurring",
    "consecutive",
    "words",
    "fed",
    "recurrent",
    "neural",
    "network",
    "analysis",
    "data",
    "finding",
    "sequence",
    "words",
    "occurring",
    "frequently",
    "builds",
    "model",
    "predict",
    "next",
    "word",
    "sentence",
    "google",
    "best",
    "food",
    "eat",
    "loss",
    "guessing",
    "going",
    "say",
    "loss",
    "mexico",
    "going",
    "las",
    "vegas",
    "google",
    "search",
    "take",
    "look",
    "say",
    "hey",
    "common",
    "autocomplete",
    "going",
    "vegas",
    "usually",
    "gives",
    "three",
    "four",
    "different",
    "choices",
    "powerful",
    "tool",
    "saves",
    "us",
    "lot",
    "time",
    "especially",
    "google",
    "search",
    "even",
    "microsoft",
    "words",
    "people",
    "get",
    "mad",
    "auto",
    "fills",
    "wrong",
    "stuff",
    "know",
    "typing",
    "away",
    "helps",
    "autofill",
    "lot",
    "different",
    "packages",
    "standard",
    "feature",
    "used",
    "dive",
    "rnn",
    "getting",
    "depths",
    "let",
    "go",
    "ahead",
    "talk",
    "neural",
    "network",
    "neural",
    "networks",
    "used",
    "deep",
    "learning",
    "consist",
    "different",
    "layers",
    "connected",
    "work",
    "structure",
    "functions",
    "human",
    "brain",
    "going",
    "see",
    "thread",
    "human",
    "human",
    "brain",
    "human",
    "thinking",
    "throughout",
    "deep",
    "learning",
    "way",
    "evaluate",
    "artificial",
    "intelligence",
    "anything",
    "like",
    "compare",
    "human",
    "function",
    "important",
    "note",
    "learns",
    "huge",
    "volumes",
    "data",
    "uses",
    "complex",
    "algorithm",
    "train",
    "neural",
    "net",
    "image",
    "pixels",
    "two",
    "different",
    "breeds",
    "dog",
    "uh",
    "one",
    "looks",
    "like",
    "nice",
    "floppy",
    "eared",
    "lab",
    "one",
    "german",
    "shepherd",
    "know",
    "wonderful",
    "breeds",
    "animals",
    "image",
    "goes",
    "input",
    "layer",
    "input",
    "layer",
    "might",
    "formatted",
    "point",
    "let",
    "know",
    "like",
    "know",
    "different",
    "pictures",
    "going",
    "different",
    "sizes",
    "different",
    "color",
    "content",
    "feed",
    "hidden",
    "layers",
    "pixels",
    "point",
    "data",
    "goes",
    "splits",
    "hidden",
    "layer",
    "goes",
    "another",
    "hidden",
    "layer",
    "goes",
    "output",
    "layer",
    "r",
    "n",
    "changes",
    "going",
    "get",
    "straightforward",
    "propagation",
    "data",
    "like",
    "covered",
    "many",
    "tutorials",
    "finally",
    "output",
    "layer",
    "output",
    "layer",
    "two",
    "outputs",
    "one",
    "lights",
    "german",
    "shepherd",
    "another",
    "lights",
    "labrador",
    "identify",
    "dog",
    "breed",
    "set",
    "networks",
    "require",
    "memorizing",
    "past",
    "output",
    "forward",
    "propagation",
    "goes",
    "forward",
    "memorized",
    "stuff",
    "see",
    "actually",
    "picture",
    "uh",
    "dressed",
    "suit",
    "worn",
    "suit",
    "years",
    "looking",
    "going",
    "change",
    "little",
    "bit",
    "cover",
    "let",
    "talk",
    "popular",
    "neural",
    "networks",
    "first",
    "feed",
    "forward",
    "neural",
    "network",
    "used",
    "general",
    "regression",
    "classification",
    "problems",
    "convolution",
    "neural",
    "network",
    "used",
    "image",
    "recognition",
    "deep",
    "neural",
    "network",
    "used",
    "acoustic",
    "modeling",
    "deep",
    "belief",
    "network",
    "used",
    "cancer",
    "detection",
    "recurrent",
    "neural",
    "network",
    "used",
    "speech",
    "recognition",
    "taken",
    "lot",
    "mixed",
    "around",
    "little",
    "bit",
    "used",
    "one",
    "thing",
    "mean",
    "ca",
    "used",
    "modeling",
    "generally",
    "field",
    "models",
    "generally",
    "used",
    "right",
    "talk",
    "feed",
    "forward",
    "neural",
    "network",
    "feed",
    "forward",
    "neural",
    "network",
    "information",
    "flows",
    "forward",
    "direction",
    "input",
    "nodes",
    "hidden",
    "layers",
    "output",
    "nodes",
    "cycles",
    "loops",
    "network",
    "see",
    "input",
    "layer",
    "talking",
    "goes",
    "straight",
    "forward",
    "hidden",
    "layers",
    "one",
    "connects",
    "connects",
    "next",
    "hidden",
    "layer",
    "connects",
    "output",
    "layer",
    "course",
    "nice",
    "simplified",
    "version",
    "predicted",
    "output",
    "refer",
    "input",
    "x",
    "lot",
    "times",
    "output",
    "decisions",
    "based",
    "current",
    "input",
    "memory",
    "past",
    "future",
    "scope",
    "recurrent",
    "neural",
    "network",
    "issues",
    "feed",
    "forward",
    "neural",
    "network",
    "one",
    "biggest",
    "issues",
    "scope",
    "memory",
    "time",
    "feed",
    "forward",
    "neural",
    "network",
    "know",
    "handle",
    "sequential",
    "data",
    "considers",
    "current",
    "input",
    "series",
    "things",
    "three",
    "points",
    "back",
    "affects",
    "happening",
    "output",
    "affects",
    "happening",
    "important",
    "whatever",
    "put",
    "output",
    "going",
    "affect",
    "next",
    "one",
    "feed",
    "forward",
    "look",
    "looks",
    "coming",
    "memorize",
    "previous",
    "inputs",
    "list",
    "inputs",
    "coming",
    "solution",
    "feed",
    "forward",
    "neural",
    "network",
    "see",
    "says",
    "recurrent",
    "neural",
    "network",
    "x",
    "bottom",
    "going",
    "h",
    "going",
    "feed",
    "forward",
    "uh",
    "right",
    "middle",
    "value",
    "c",
    "whole",
    "another",
    "process",
    "memorizing",
    "going",
    "hidden",
    "layers",
    "hidden",
    "layers",
    "produce",
    "data",
    "feed",
    "next",
    "one",
    "hidden",
    "layer",
    "might",
    "output",
    "goes",
    "output",
    "goes",
    "back",
    "next",
    "prediction",
    "coming",
    "allows",
    "handle",
    "sequential",
    "data",
    "considers",
    "current",
    "input",
    "also",
    "previously",
    "received",
    "inputs",
    "going",
    "look",
    "general",
    "drawings",
    "solutions",
    "also",
    "look",
    "applications",
    "rnn",
    "image",
    "captioning",
    "rnn",
    "used",
    "caption",
    "image",
    "analyzing",
    "activities",
    "present",
    "dog",
    "catching",
    "ball",
    "midair",
    "tough",
    "mean",
    "know",
    "lot",
    "stuff",
    "analyzes",
    "images",
    "dog",
    "image",
    "ball",
    "able",
    "add",
    "one",
    "feature",
    "actually",
    "catching",
    "ball",
    "midair",
    "time",
    "series",
    "prediction",
    "time",
    "series",
    "problem",
    "like",
    "predicting",
    "prices",
    "stocks",
    "particular",
    "month",
    "solved",
    "using",
    "rnn",
    "dive",
    "use",
    "case",
    "actually",
    "take",
    "look",
    "stock",
    "one",
    "things",
    "know",
    "analyzing",
    "stock",
    "today",
    "difficult",
    "analyzing",
    "whole",
    "stock",
    "stock",
    "market",
    "new",
    "york",
    "stock",
    "exchange",
    "produces",
    "somewhere",
    "neighborhood",
    "count",
    "individual",
    "trades",
    "fluctuations",
    "second",
    "um",
    "like",
    "three",
    "terabytes",
    "day",
    "data",
    "going",
    "look",
    "one",
    "stock",
    "analyzing",
    "one",
    "stock",
    "really",
    "tricky",
    "give",
    "little",
    "jump",
    "exciting",
    "expect",
    "get",
    "rich",
    "immediately",
    "another",
    "application",
    "rnn",
    "natural",
    "language",
    "processing",
    "text",
    "mining",
    "sentiment",
    "analysis",
    "carried",
    "using",
    "rnn",
    "natural",
    "language",
    "processing",
    "see",
    "right",
    "term",
    "natural",
    "language",
    "processing",
    "stream",
    "three",
    "words",
    "together",
    "different",
    "ice",
    "said",
    "processing",
    "language",
    "natural",
    "leap",
    "time",
    "series",
    "important",
    "analyzing",
    "sentiments",
    "change",
    "whole",
    "value",
    "sentence",
    "switching",
    "words",
    "around",
    "counting",
    "words",
    "may",
    "get",
    "one",
    "sentiment",
    "actually",
    "look",
    "order",
    "get",
    "completely",
    "different",
    "sentiment",
    "rains",
    "look",
    "rainbows",
    "dark",
    "look",
    "stars",
    "positive",
    "sentiments",
    "based",
    "upon",
    "order",
    "sentence",
    "going",
    "machine",
    "translation",
    "given",
    "input",
    "one",
    "language",
    "rnn",
    "used",
    "translate",
    "input",
    "different",
    "languages",
    "output",
    "linguistically",
    "challenged",
    "study",
    "languages",
    "good",
    "languages",
    "know",
    "right",
    "away",
    "speaking",
    "english",
    "would",
    "say",
    "big",
    "cat",
    "speaking",
    "spanish",
    "would",
    "say",
    "cat",
    "big",
    "translation",
    "really",
    "important",
    "get",
    "right",
    "order",
    "get",
    "kinds",
    "parts",
    "speech",
    "important",
    "know",
    "order",
    "words",
    "person",
    "speaking",
    "english",
    "getting",
    "translated",
    "see",
    "person",
    "speaking",
    "english",
    "little",
    "diagram",
    "guess",
    "denoted",
    "flags",
    "flag",
    "um",
    "speaking",
    "english",
    "getting",
    "translated",
    "chinese",
    "italian",
    "french",
    "german",
    "spanish",
    "languages",
    "tools",
    "coming",
    "cool",
    "somebody",
    "like",
    "linguistically",
    "challenged",
    "travel",
    "worlds",
    "would",
    "never",
    "think",
    "something",
    "translate",
    "english",
    "back",
    "forth",
    "readily",
    "stuck",
    "communication",
    "gap",
    "let",
    "dive",
    "recurrent",
    "neural",
    "network",
    "recurrent",
    "neural",
    "network",
    "works",
    "principle",
    "saving",
    "output",
    "layer",
    "feeding",
    "back",
    "input",
    "order",
    "predict",
    "output",
    "layer",
    "sounds",
    "little",
    "confusing",
    "start",
    "breaking",
    "make",
    "sense",
    "usually",
    "propagation",
    "forward",
    "neural",
    "network",
    "input",
    "layers",
    "hidden",
    "layers",
    "output",
    "layer",
    "recurrent",
    "neural",
    "network",
    "turn",
    "side",
    "x",
    "comes",
    "bottom",
    "hidden",
    "layers",
    "usually",
    "draw",
    "simplified",
    "x",
    "h",
    "c",
    "loop",
    "b",
    "c",
    "perimeters",
    "lot",
    "times",
    "see",
    "kind",
    "drawing",
    "digging",
    "closer",
    "closer",
    "h",
    "works",
    "going",
    "left",
    "right",
    "see",
    "c",
    "goes",
    "x",
    "goes",
    "x",
    "going",
    "upward",
    "bound",
    "c",
    "going",
    "right",
    "going",
    "c",
    "also",
    "going",
    "gets",
    "little",
    "confusing",
    "xn",
    "cn",
    "c",
    "c",
    "based",
    "ht",
    "minus",
    "value",
    "based",
    "h",
    "value",
    "connected",
    "necessarily",
    "value",
    "h",
    "thing",
    "usually",
    "draw",
    "represent",
    "function",
    "h",
    "equals",
    "function",
    "c",
    "h",
    "minus",
    "1",
    "last",
    "h",
    "output",
    "x",
    "going",
    "last",
    "output",
    "h",
    "combined",
    "new",
    "input",
    "x",
    "h",
    "new",
    "state",
    "fc",
    "function",
    "parameters",
    "c",
    "common",
    "way",
    "denoting",
    "h",
    "minus",
    "1",
    "old",
    "state",
    "coming",
    "x",
    "input",
    "vector",
    "time",
    "step",
    "well",
    "need",
    "cover",
    "types",
    "recurrent",
    "neural",
    "networks",
    "first",
    "one",
    "common",
    "one",
    "single",
    "output",
    "neural",
    "network",
    "usually",
    "known",
    "vanilla",
    "neural",
    "network",
    "used",
    "regular",
    "machine",
    "learning",
    "problems",
    "vanilla",
    "usually",
    "considered",
    "kind",
    "real",
    "basic",
    "flavor",
    "basic",
    "lot",
    "times",
    "call",
    "vanilla",
    "neural",
    "network",
    "common",
    "term",
    "know",
    "kind",
    "slang",
    "term",
    "people",
    "know",
    "talking",
    "usually",
    "say",
    "run",
    "one",
    "mini",
    "single",
    "input",
    "might",
    "multiple",
    "outputs",
    "case",
    "image",
    "captioning",
    "looked",
    "earlier",
    "looking",
    "dog",
    "dog",
    "catching",
    "ball",
    "air",
    "mini",
    "one",
    "network",
    "takes",
    "sequence",
    "inputs",
    "examples",
    "sentiment",
    "analysis",
    "given",
    "sentence",
    "classified",
    "expressing",
    "positive",
    "negative",
    "sentiments",
    "looked",
    "discussing",
    "rains",
    "look",
    "rainbow",
    "positive",
    "sentiment",
    "rain",
    "might",
    "negative",
    "sentiment",
    "adding",
    "words",
    "course",
    "going",
    "mini",
    "one",
    "one",
    "many",
    "many",
    "many",
    "networks",
    "takes",
    "sequence",
    "inputs",
    "generates",
    "sequence",
    "outputs",
    "example",
    "machine",
    "translation",
    "lengthy",
    "sentence",
    "coming",
    "english",
    "going",
    "different",
    "languages",
    "uh",
    "know",
    "wonderful",
    "tool",
    "complicated",
    "set",
    "computations",
    "know",
    "translator",
    "realize",
    "difficult",
    "translate",
    "different",
    "languages",
    "one",
    "biggest",
    "things",
    "need",
    "understand",
    "working",
    "neural",
    "network",
    "called",
    "vanishing",
    "gradient",
    "problem",
    "training",
    "rnn",
    "slope",
    "either",
    "small",
    "large",
    "makes",
    "training",
    "difficult",
    "slope",
    "small",
    "problem",
    "known",
    "vanishing",
    "gradient",
    "see",
    "nice",
    "uh",
    "image",
    "loss",
    "information",
    "time",
    "pushing",
    "enough",
    "information",
    "forward",
    "information",
    "lost",
    "go",
    "train",
    "start",
    "losing",
    "third",
    "word",
    "sentence",
    "something",
    "like",
    "quite",
    "follow",
    "full",
    "logic",
    "working",
    "exploding",
    "gradient",
    "problem",
    "oh",
    "one",
    "runs",
    "everybody",
    "working",
    "particular",
    "neural",
    "network",
    "slope",
    "tends",
    "grow",
    "exponentially",
    "instead",
    "decaying",
    "problem",
    "called",
    "exploding",
    "gradient",
    "issues",
    "gradient",
    "problem",
    "long",
    "tracking",
    "time",
    "poor",
    "performance",
    "bad",
    "accuracy",
    "add",
    "one",
    "computer",
    "lower",
    "end",
    "computer",
    "testing",
    "model",
    "lock",
    "give",
    "memory",
    "error",
    "explaining",
    "gradient",
    "problem",
    "consider",
    "following",
    "two",
    "examples",
    "understand",
    "next",
    "word",
    "sequence",
    "person",
    "took",
    "bike",
    "blank",
    "thief",
    "students",
    "got",
    "engineering",
    "blank",
    "asia",
    "see",
    "x",
    "value",
    "going",
    "previous",
    "value",
    "going",
    "forward",
    "back",
    "propagate",
    "error",
    "like",
    "neural",
    "network",
    "looking",
    "missing",
    "word",
    "maybe",
    "person",
    "took",
    "bike",
    "blank",
    "thief",
    "student",
    "got",
    "engineering",
    "blank",
    "asia",
    "consider",
    "following",
    "example",
    "person",
    "took",
    "bike",
    "go",
    "back",
    "person",
    "took",
    "bike",
    "blank",
    "thief",
    "order",
    "understand",
    "would",
    "next",
    "word",
    "sequence",
    "rnn",
    "must",
    "memorize",
    "previous",
    "context",
    "whether",
    "subject",
    "singular",
    "noun",
    "plural",
    "noun",
    "thief",
    "singular",
    "student",
    "got",
    "engineering",
    "well",
    "order",
    "understand",
    "would",
    "next",
    "word",
    "sequence",
    "rnn",
    "must",
    "memorize",
    "previous",
    "context",
    "whether",
    "subject",
    "singular",
    "noun",
    "plural",
    "noun",
    "see",
    "students",
    "got",
    "engineering",
    "blank",
    "asia",
    "might",
    "sometimes",
    "difficult",
    "air",
    "back",
    "propagate",
    "beginning",
    "sequence",
    "predict",
    "output",
    "run",
    "gradient",
    "problem",
    "need",
    "solution",
    "solution",
    "gradient",
    "problem",
    "first",
    "going",
    "look",
    "exploding",
    "gradient",
    "three",
    "different",
    "solutions",
    "depending",
    "going",
    "one",
    "identity",
    "initialization",
    "first",
    "thing",
    "want",
    "see",
    "find",
    "way",
    "minimize",
    "identities",
    "coming",
    "instead",
    "identify",
    "everything",
    "important",
    "information",
    "looking",
    "next",
    "truncate",
    "back",
    "propagation",
    "instead",
    "whatever",
    "information",
    "sending",
    "next",
    "series",
    "truncate",
    "sending",
    "lower",
    "particular",
    "set",
    "layers",
    "make",
    "smaller",
    "finally",
    "gradient",
    "clipping",
    "training",
    "clip",
    "gradient",
    "looks",
    "like",
    "narrow",
    "training",
    "model",
    "using",
    "vanishing",
    "gradient",
    "option",
    "problem",
    "take",
    "look",
    "weight",
    "initialization",
    "similar",
    "identity",
    "going",
    "add",
    "weights",
    "identify",
    "different",
    "aspects",
    "coming",
    "better",
    "choosing",
    "right",
    "activation",
    "function",
    "huge",
    "might",
    "activating",
    "based",
    "one",
    "thing",
    "need",
    "limit",
    "talked",
    "much",
    "activation",
    "function",
    "look",
    "minimally",
    "lot",
    "choices",
    "finally",
    "long",
    "short",
    "term",
    "memory",
    "networks",
    "lstms",
    "make",
    "adjustments",
    "like",
    "clip",
    "gradient",
    "comes",
    "also",
    "expand",
    "increase",
    "memory",
    "network",
    "size",
    "handles",
    "information",
    "one",
    "common",
    "problems",
    "today",
    "setup",
    "call",
    "dependencies",
    "suppose",
    "try",
    "predict",
    "last",
    "word",
    "text",
    "clouds",
    "probably",
    "said",
    "sky",
    "need",
    "context",
    "pretty",
    "clear",
    "last",
    "word",
    "going",
    "sky",
    "suppose",
    "try",
    "predict",
    "last",
    "word",
    "text",
    "staying",
    "spain",
    "last",
    "10",
    "years",
    "speak",
    "fluent",
    "maybe",
    "said",
    "portuguese",
    "french",
    "probably",
    "said",
    "spanish",
    "word",
    "predict",
    "depend",
    "previous",
    "words",
    "context",
    "need",
    "context",
    "spain",
    "predict",
    "last",
    "word",
    "text",
    "possible",
    "gap",
    "relevant",
    "information",
    "point",
    "needed",
    "become",
    "large",
    "lstms",
    "help",
    "us",
    "solve",
    "problem",
    "lstms",
    "special",
    "kind",
    "recurrent",
    "neural",
    "network",
    "capable",
    "learning",
    "dependencies",
    "remembering",
    "information",
    "long",
    "periods",
    "time",
    "default",
    "behavior",
    "recurrent",
    "neural",
    "networks",
    "form",
    "chain",
    "repeating",
    "modules",
    "neural",
    "network",
    "connections",
    "standard",
    "rnns",
    "repeating",
    "module",
    "simple",
    "structure",
    "single",
    "tangent",
    "h",
    "layer",
    "lstms",
    "also",
    "structure",
    "repeating",
    "module",
    "different",
    "structure",
    "instead",
    "single",
    "neural",
    "network",
    "layer",
    "four",
    "interacting",
    "layers",
    "communicating",
    "special",
    "way",
    "lstms",
    "special",
    "kind",
    "recurrent",
    "neural",
    "network",
    "capable",
    "learning",
    "dependencies",
    "remembering",
    "information",
    "long",
    "periods",
    "time",
    "default",
    "behavior",
    "ls",
    "tmss",
    "also",
    "chain",
    "like",
    "structure",
    "repeating",
    "module",
    "different",
    "structure",
    "instead",
    "single",
    "neural",
    "network",
    "layer",
    "four",
    "interacting",
    "layers",
    "communicating",
    "special",
    "way",
    "see",
    "deeper",
    "dig",
    "complicated",
    "graphs",
    "get",
    "want",
    "note",
    "x",
    "minus",
    "one",
    "coming",
    "x",
    "coming",
    "x",
    "plus",
    "one",
    "h",
    "minus",
    "one",
    "h",
    "coming",
    "h",
    "plus",
    "one",
    "going",
    "course",
    "uh",
    "side",
    "output",
    "um",
    "middle",
    "tangent",
    "h",
    "occurs",
    "two",
    "different",
    "places",
    "computing",
    "exit",
    "plus",
    "one",
    "getting",
    "tangent",
    "h",
    "x",
    "also",
    "getting",
    "value",
    "coming",
    "x",
    "minus",
    "one",
    "short",
    "look",
    "layers",
    "propagate",
    "first",
    "layer",
    "goes",
    "second",
    "layer",
    "back",
    "also",
    "going",
    "third",
    "layer",
    "kind",
    "stacking",
    "get",
    "complicated",
    "grow",
    "size",
    "also",
    "grows",
    "memory",
    "amount",
    "resources",
    "takes",
    "powerful",
    "tool",
    "help",
    "us",
    "address",
    "problem",
    "complicated",
    "long",
    "sequential",
    "information",
    "coming",
    "like",
    "looking",
    "sentence",
    "looking",
    "long",
    "short",
    "term",
    "memory",
    "network",
    "uh",
    "three",
    "steps",
    "processing",
    "sensing",
    "lstms",
    "look",
    "first",
    "one",
    "want",
    "forget",
    "irrelevant",
    "parts",
    "previous",
    "state",
    "know",
    "lot",
    "times",
    "like",
    "know",
    "unless",
    "trying",
    "look",
    "whether",
    "plural",
    "noun",
    "really",
    "play",
    "huge",
    "part",
    "language",
    "want",
    "get",
    "rid",
    "selectively",
    "update",
    "cell",
    "state",
    "values",
    "want",
    "update",
    "cell",
    "state",
    "values",
    "reflect",
    "working",
    "finally",
    "want",
    "put",
    "output",
    "certain",
    "parts",
    "cell",
    "state",
    "whatever",
    "coming",
    "want",
    "limit",
    "going",
    "let",
    "dig",
    "little",
    "deeper",
    "let",
    "see",
    "really",
    "looks",
    "like",
    "uh",
    "step",
    "one",
    "decides",
    "much",
    "past",
    "remember",
    "first",
    "step",
    "lstm",
    "decide",
    "information",
    "omitted",
    "cell",
    "particular",
    "time",
    "step",
    "decided",
    "sigmoid",
    "function",
    "looks",
    "previous",
    "state",
    "h",
    "minus",
    "1",
    "current",
    "input",
    "x",
    "computes",
    "function",
    "see",
    "function",
    "equals",
    "sigmoid",
    "function",
    "weight",
    "f",
    "h",
    "minus",
    "1",
    "x",
    "plus",
    "course",
    "bias",
    "neural",
    "networks",
    "bias",
    "function",
    "f",
    "equals",
    "forget",
    "gate",
    "decides",
    "information",
    "delete",
    "important",
    "previous",
    "time",
    "step",
    "considering",
    "stm",
    "fed",
    "following",
    "inputs",
    "previous",
    "present",
    "time",
    "step",
    "alice",
    "good",
    "physics",
    "john",
    "hand",
    "good",
    "chemistry",
    "previous",
    "output",
    "john",
    "plays",
    "football",
    "well",
    "told",
    "yesterday",
    "phone",
    "served",
    "captain",
    "college",
    "football",
    "team",
    "current",
    "input",
    "look",
    "first",
    "step",
    "forget",
    "gate",
    "realizes",
    "might",
    "change",
    "context",
    "encountering",
    "first",
    "full",
    "stop",
    "compares",
    "current",
    "input",
    "sentence",
    "x",
    "looking",
    "full",
    "stop",
    "compares",
    "input",
    "new",
    "sentence",
    "next",
    "sentence",
    "talks",
    "john",
    "information",
    "alice",
    "deleted",
    "okay",
    "important",
    "know",
    "input",
    "coming",
    "going",
    "continue",
    "john",
    "going",
    "primary",
    "information",
    "looking",
    "position",
    "subject",
    "vacated",
    "assigned",
    "john",
    "one",
    "seen",
    "weeded",
    "whole",
    "bunch",
    "information",
    "passing",
    "information",
    "john",
    "since",
    "new",
    "topic",
    "step",
    "two",
    "decide",
    "much",
    "unit",
    "add",
    "current",
    "state",
    "second",
    "layer",
    "two",
    "parts",
    "one",
    "sigmoid",
    "function",
    "tangent",
    "h",
    "sigmoid",
    "function",
    "decides",
    "values",
    "let",
    "0",
    "tangent",
    "h",
    "function",
    "gives",
    "weightage",
    "values",
    "passed",
    "deciding",
    "level",
    "importance",
    "minus",
    "one",
    "one",
    "see",
    "two",
    "formulas",
    "come",
    "uh",
    "equals",
    "sigmoid",
    "weight",
    "h",
    "minus",
    "one",
    "x",
    "plus",
    "bias",
    "c",
    "equals",
    "tangent",
    "h",
    "weight",
    "c",
    "h",
    "minus",
    "1",
    "x",
    "plus",
    "bias",
    "c",
    "equals",
    "input",
    "gate",
    "determines",
    "information",
    "let",
    "based",
    "significance",
    "current",
    "time",
    "step",
    "seems",
    "little",
    "complicated",
    "worry",
    "lot",
    "programming",
    "already",
    "done",
    "get",
    "case",
    "study",
    "understanding",
    "though",
    "part",
    "program",
    "important",
    "trying",
    "figure",
    "set",
    "settings",
    "also",
    "note",
    "looking",
    "semblance",
    "forward",
    "propagation",
    "neural",
    "networks",
    "value",
    "assigned",
    "weight",
    "plus",
    "bias",
    "important",
    "steps",
    "neural",
    "network",
    "layers",
    "whether",
    "propagating",
    "information",
    "one",
    "next",
    "straightforward",
    "neural",
    "network",
    "propagation",
    "let",
    "take",
    "quick",
    "look",
    "looks",
    "like",
    "human",
    "standpoint",
    "step",
    "suit",
    "consider",
    "current",
    "input",
    "x",
    "john",
    "plays",
    "football",
    "well",
    "told",
    "yesterday",
    "phone",
    "served",
    "captain",
    "college",
    "football",
    "team",
    "input",
    "input",
    "gate",
    "analysis",
    "important",
    "information",
    "john",
    "plays",
    "football",
    "captain",
    "college",
    "team",
    "important",
    "told",
    "phone",
    "yesterday",
    "less",
    "important",
    "hence",
    "forgotten",
    "process",
    "adding",
    "new",
    "information",
    "done",
    "via",
    "input",
    "gate",
    "example",
    "human",
    "form",
    "look",
    "training",
    "stuff",
    "minute",
    "human",
    "wanted",
    "get",
    "information",
    "conversation",
    "maybe",
    "google",
    "voice",
    "listening",
    "something",
    "like",
    "um",
    "weed",
    "information",
    "talking",
    "phone",
    "yesterday",
    "well",
    "want",
    "memorize",
    "talked",
    "phone",
    "yesterday",
    "maybe",
    "important",
    "case",
    "want",
    "know",
    "captain",
    "football",
    "team",
    "want",
    "know",
    "served",
    "want",
    "know",
    "john",
    "plays",
    "football",
    "captain",
    "college",
    "football",
    "team",
    "two",
    "things",
    "want",
    "take",
    "away",
    "human",
    "measure",
    "lot",
    "human",
    "viewpoint",
    "also",
    "try",
    "train",
    "understand",
    "neural",
    "networks",
    "finally",
    "get",
    "step",
    "three",
    "decides",
    "part",
    "current",
    "cell",
    "state",
    "makes",
    "output",
    "third",
    "step",
    "decide",
    "output",
    "first",
    "run",
    "sigmoid",
    "layer",
    "decides",
    "parts",
    "cell",
    "state",
    "make",
    "output",
    "put",
    "cell",
    "state",
    "tangent",
    "h",
    "push",
    "values",
    "1",
    "multiply",
    "output",
    "sigmoid",
    "gate",
    "talk",
    "output",
    "set",
    "equal",
    "sigmoid",
    "weight",
    "0",
    "h",
    "minus",
    "1",
    "back",
    "one",
    "step",
    "time",
    "x",
    "plus",
    "course",
    "bias",
    "h",
    "equals",
    "outer",
    "times",
    "tangent",
    "tangent",
    "h",
    "c",
    "equals",
    "output",
    "gate",
    "allows",
    "passed",
    "information",
    "impact",
    "output",
    "current",
    "time",
    "step",
    "let",
    "consider",
    "example",
    "predicting",
    "next",
    "word",
    "sentence",
    "john",
    "played",
    "tremendously",
    "well",
    "opponent",
    "one",
    "team",
    "contributions",
    "brave",
    "blank",
    "awarded",
    "player",
    "match",
    "could",
    "lot",
    "choices",
    "empty",
    "space",
    "current",
    "input",
    "brave",
    "adjective",
    "adjectives",
    "describe",
    "noun",
    "john",
    "could",
    "best",
    "output",
    "brave",
    "thumbs",
    "john",
    "awarded",
    "player",
    "match",
    "pull",
    "nouns",
    "sentence",
    "team",
    "look",
    "right",
    "really",
    "subject",
    "talking",
    "contributions",
    "know",
    "brave",
    "contributions",
    "brave",
    "teen",
    "brave",
    "player",
    "brave",
    "match",
    "look",
    "start",
    "train",
    "neural",
    "network",
    "starts",
    "looking",
    "goes",
    "oh",
    "john",
    "talking",
    "brave",
    "adjective",
    "john",
    "going",
    "best",
    "output",
    "give",
    "john",
    "big",
    "thumbs",
    "course",
    "jump",
    "favorite",
    "part",
    "case",
    "study",
    "use",
    "case",
    "implementation",
    "lstm",
    "let",
    "predict",
    "prices",
    "stocks",
    "using",
    "lstm",
    "network",
    "based",
    "stock",
    "price",
    "data",
    "2012",
    "going",
    "try",
    "predict",
    "stock",
    "prices",
    "2017",
    "narrow",
    "set",
    "data",
    "going",
    "whole",
    "stock",
    "market",
    "turns",
    "new",
    "york",
    "stock",
    "exchange",
    "generates",
    "roughly",
    "three",
    "terabytes",
    "data",
    "per",
    "day",
    "different",
    "trades",
    "different",
    "stocks",
    "going",
    "individual",
    "one",
    "second",
    "second",
    "nanosecond",
    "nanosecond",
    "going",
    "limit",
    "basic",
    "fundamental",
    "information",
    "think",
    "going",
    "get",
    "rich",
    "today",
    "least",
    "give",
    "eye",
    "give",
    "step",
    "forward",
    "start",
    "processing",
    "something",
    "like",
    "stock",
    "prices",
    "valid",
    "use",
    "machine",
    "learning",
    "today",
    "markets",
    "use",
    "case",
    "implementation",
    "lstm",
    "let",
    "dive",
    "going",
    "import",
    "libraries",
    "going",
    "import",
    "training",
    "set",
    "get",
    "scaling",
    "going",
    "watch",
    "tutorials",
    "lot",
    "pieces",
    "start",
    "look",
    "familiar",
    "similar",
    "setup",
    "let",
    "take",
    "look",
    "reminder",
    "going",
    "using",
    "anaconda",
    "jupiter",
    "notebook",
    "anaconda",
    "navigator",
    "go",
    "environments",
    "actually",
    "set",
    "cross",
    "python",
    "36",
    "python36",
    "nice",
    "thing",
    "anaconda",
    "especially",
    "newer",
    "version",
    "remember",
    "year",
    "ago",
    "messing",
    "anaconda",
    "different",
    "versions",
    "python",
    "different",
    "environments",
    "anaconda",
    "nice",
    "interface",
    "installed",
    "ubuntu",
    "linux",
    "machine",
    "windows",
    "works",
    "fine",
    "go",
    "open",
    "terminal",
    "window",
    "terminal",
    "window",
    "going",
    "start",
    "installing",
    "using",
    "pip",
    "install",
    "different",
    "modules",
    "everything",
    "already",
    "need",
    "installed",
    "particular",
    "environment",
    "need",
    "course",
    "need",
    "use",
    "anaconda",
    "jupiter",
    "use",
    "whatever",
    "favorite",
    "python",
    "id",
    "like",
    "big",
    "fan",
    "keeps",
    "stuff",
    "separate",
    "see",
    "machine",
    "specifically",
    "installed",
    "one",
    "cross",
    "since",
    "going",
    "working",
    "cross",
    "tensorflow",
    "go",
    "back",
    "home",
    "gone",
    "application",
    "environment",
    "loaded",
    "click",
    "launch",
    "jupiter",
    "notebook",
    "already",
    "jupiter",
    "notebook",
    "set",
    "lot",
    "stuff",
    "ready",
    "go",
    "kind",
    "like",
    "martha",
    "stewart",
    "old",
    "cooking",
    "show",
    "want",
    "make",
    "sure",
    "tools",
    "waiting",
    "load",
    "go",
    "says",
    "new",
    "see",
    "create",
    "new",
    "python",
    "underneath",
    "setup",
    "already",
    "modules",
    "installed",
    "actually",
    "renamed",
    "go",
    "file",
    "rename",
    "calling",
    "rnn",
    "stock",
    "let",
    "take",
    "look",
    "start",
    "diving",
    "code",
    "let",
    "get",
    "exciting",
    "part",
    "looked",
    "tool",
    "course",
    "might",
    "using",
    "different",
    "tool",
    "fine",
    "let",
    "start",
    "putting",
    "code",
    "seeing",
    "imports",
    "uploading",
    "everything",
    "looks",
    "like",
    "first",
    "half",
    "kind",
    "boring",
    "hit",
    "run",
    "button",
    "going",
    "importing",
    "numpy",
    "np",
    "uh",
    "number",
    "python",
    "numpy",
    "array",
    "matplot",
    "library",
    "going",
    "plotting",
    "end",
    "pandas",
    "data",
    "set",
    "pandas",
    "pd",
    "hit",
    "run",
    "really",
    "anything",
    "except",
    "load",
    "modules",
    "quick",
    "note",
    "let",
    "quick",
    "draw",
    "oops",
    "shift",
    "alt",
    "go",
    "notice",
    "setup",
    "divide",
    "oops",
    "going",
    "actually",
    "let",
    "overlap",
    "go",
    "first",
    "part",
    "going",
    "data",
    "prep",
    "lot",
    "prepping",
    "involved",
    "um",
    "fact",
    "depending",
    "system",
    "since",
    "using",
    "karass",
    "put",
    "overlap",
    "find",
    "almost",
    "maybe",
    "even",
    "half",
    "code",
    "data",
    "prep",
    "reason",
    "overlapped",
    "uh",
    "cross",
    "let",
    "put",
    "working",
    "uh",
    "cross",
    "like",
    "preset",
    "stuff",
    "already",
    "really",
    "nice",
    "couple",
    "steps",
    "lot",
    "times",
    "kara",
    "setup",
    "take",
    "look",
    "see",
    "comes",
    "code",
    "go",
    "look",
    "stock",
    "last",
    "part",
    "evaluate",
    "working",
    "shareholders",
    "classroom",
    "whatever",
    "working",
    "uh",
    "evaluate",
    "next",
    "biggest",
    "piece",
    "um",
    "actual",
    "code",
    "crossed",
    "little",
    "bit",
    "working",
    "packages",
    "might",
    "like",
    "three",
    "lines",
    "might",
    "stuff",
    "data",
    "since",
    "cross",
    "cutting",
    "edge",
    "load",
    "individual",
    "layers",
    "see",
    "lines",
    "crosses",
    "little",
    "bit",
    "robust",
    "spin",
    "lot",
    "times",
    "like",
    "said",
    "evaluate",
    "want",
    "something",
    "present",
    "everybody",
    "else",
    "say",
    "hey",
    "looks",
    "like",
    "let",
    "go",
    "steps",
    "like",
    "kind",
    "general",
    "overview",
    "let",
    "take",
    "look",
    "see",
    "next",
    "set",
    "code",
    "looks",
    "like",
    "data",
    "set",
    "train",
    "going",
    "read",
    "using",
    "pd",
    "trainingset",
    "equals",
    "kind",
    "sorted",
    "part",
    "going",
    "let",
    "take",
    "look",
    "let",
    "look",
    "actual",
    "file",
    "see",
    "going",
    "look",
    "uh",
    "ignore",
    "extra",
    "files",
    "already",
    "train",
    "test",
    "set",
    "sorted",
    "important",
    "notice",
    "lot",
    "times",
    "part",
    "data",
    "take",
    "20",
    "data",
    "test",
    "train",
    "rest",
    "use",
    "create",
    "neural",
    "network",
    "way",
    "find",
    "good",
    "uh",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "see",
    "looks",
    "like",
    "far",
    "file",
    "went",
    "ahead",
    "opened",
    "basic",
    "word",
    "pad",
    "text",
    "editor",
    "take",
    "look",
    "certainly",
    "open",
    "excel",
    "kind",
    "spreadsheet",
    "note",
    "comma",
    "separated",
    "variables",
    "date",
    "uh",
    "open",
    "high",
    "low",
    "close",
    "volume",
    "standard",
    "stuff",
    "import",
    "stock",
    "basic",
    "set",
    "information",
    "look",
    "stock",
    "free",
    "download",
    "case",
    "downloaded",
    "google",
    "call",
    "google",
    "stock",
    "price",
    "specifically",
    "google",
    "google",
    "stock",
    "values",
    "see",
    "started",
    "1",
    "3",
    "look",
    "first",
    "setup",
    "data",
    "set",
    "train",
    "equals",
    "pd",
    "underscore",
    "csv",
    "noticed",
    "original",
    "frame",
    "let",
    "go",
    "back",
    "set",
    "home",
    "ubuntu",
    "downloads",
    "google",
    "stock",
    "price",
    "train",
    "went",
    "ahead",
    "changed",
    "file",
    "running",
    "code",
    "saved",
    "particular",
    "python",
    "code",
    "need",
    "go",
    "special",
    "paths",
    "full",
    "path",
    "course",
    "want",
    "take",
    "certain",
    "values",
    "going",
    "notice",
    "using",
    "data",
    "set",
    "pandas",
    "pandas",
    "basically",
    "looks",
    "like",
    "spreadsheet",
    "case",
    "going",
    "location",
    "going",
    "get",
    "specific",
    "locations",
    "first",
    "value",
    "going",
    "show",
    "us",
    "pulling",
    "rows",
    "data",
    "second",
    "one",
    "going",
    "look",
    "columns",
    "one",
    "two",
    "remember",
    "data",
    "switch",
    "back",
    "columns",
    "saw",
    "start",
    "zero",
    "date",
    "going",
    "looking",
    "open",
    "high",
    "would",
    "one",
    "two",
    "label",
    "right",
    "see",
    "go",
    "back",
    "certainly",
    "extrapolate",
    "columns",
    "example",
    "let",
    "limit",
    "little",
    "bit",
    "focus",
    "key",
    "aspects",
    "stock",
    "go",
    "run",
    "code",
    "said",
    "first",
    "half",
    "boring",
    "whenever",
    "hit",
    "run",
    "button",
    "anything",
    "still",
    "loading",
    "data",
    "setting",
    "loaded",
    "data",
    "want",
    "go",
    "ahead",
    "scale",
    "want",
    "call",
    "feature",
    "scaling",
    "going",
    "pull",
    "sk",
    "learn",
    "sk",
    "kit",
    "import",
    "min",
    "max",
    "scalar",
    "look",
    "got",
    "remember",
    "biases",
    "data",
    "want",
    "get",
    "rid",
    "something",
    "like",
    "really",
    "high",
    "value",
    "let",
    "draw",
    "quick",
    "graph",
    "something",
    "like",
    "maybe",
    "stock",
    "value",
    "one",
    "stock",
    "value",
    "hundred",
    "another",
    "stock",
    "value",
    "five",
    "um",
    "start",
    "get",
    "bias",
    "different",
    "stocks",
    "go",
    "ahead",
    "say",
    "okay",
    "100",
    "going",
    "max",
    "5",
    "going",
    "min",
    "everything",
    "else",
    "goes",
    "change",
    "squish",
    "like",
    "word",
    "squish",
    "1",
    "100",
    "equals",
    "one",
    "one",
    "equals",
    "hundred",
    "zero",
    "equals",
    "five",
    "multiply",
    "usually",
    "simple",
    "multiplication",
    "using",
    "uh",
    "multiplication",
    "going",
    "uh",
    "minus",
    "five",
    "100",
    "divided",
    "95",
    "divided",
    "one",
    "whatever",
    "value",
    "divided",
    "uh",
    "actually",
    "created",
    "scale",
    "tolling",
    "going",
    "zero",
    "one",
    "wan",
    "na",
    "take",
    "training",
    "set",
    "gon",
    "na",
    "create",
    "training",
    "set",
    "scaled",
    "gon",
    "na",
    "use",
    "scalar",
    "sc",
    "going",
    "fit",
    "going",
    "fit",
    "transform",
    "training",
    "set",
    "uh",
    "use",
    "sc",
    "particular",
    "object",
    "use",
    "later",
    "testing",
    "set",
    "remember",
    "also",
    "scale",
    "go",
    "test",
    "model",
    "see",
    "works",
    "go",
    "ahead",
    "click",
    "run",
    "going",
    "output",
    "yet",
    "setting",
    "variables",
    "okay",
    "pasted",
    "data",
    "going",
    "create",
    "data",
    "structure",
    "60",
    "time",
    "steps",
    "output",
    "first",
    "note",
    "running",
    "60",
    "time",
    "steps",
    "value",
    "also",
    "comes",
    "first",
    "thing",
    "create",
    "x",
    "train",
    "train",
    "variables",
    "set",
    "empty",
    "python",
    "array",
    "important",
    "remember",
    "kind",
    "array",
    "working",
    "going",
    "come",
    "going",
    "go",
    "range",
    "60",
    "1258",
    "60",
    "60",
    "time",
    "steps",
    "reason",
    "want",
    "adding",
    "data",
    "nothing",
    "60",
    "going",
    "use",
    "60",
    "time",
    "steps",
    "start",
    "point",
    "60",
    "includes",
    "everything",
    "underneath",
    "otherwise",
    "get",
    "pointer",
    "error",
    "going",
    "take",
    "x",
    "train",
    "going",
    "append",
    "training",
    "set",
    "scaled",
    "scaled",
    "value",
    "0",
    "equal",
    "60",
    "value",
    "going",
    "60",
    "minus",
    "60",
    "actually",
    "0",
    "going",
    "0",
    "60",
    "1",
    "let",
    "circle",
    "part",
    "right",
    "1",
    "61",
    "2",
    "62",
    "remember",
    "said",
    "0",
    "60",
    "incorrect",
    "count",
    "remember",
    "starts",
    "0",
    "count",
    "actually",
    "important",
    "remember",
    "looking",
    "second",
    "part",
    "looking",
    "remember",
    "correctly",
    "go",
    "go",
    "0",
    "59",
    "comma",
    "0",
    "right",
    "finally",
    "going",
    "look",
    "open",
    "value",
    "know",
    "put",
    "one",
    "two",
    "move",
    "quickly",
    "count",
    "second",
    "one",
    "open",
    "value",
    "looking",
    "open",
    "finally",
    "train",
    "dot",
    "append",
    "training",
    "set",
    "zero",
    "remember",
    "correctly",
    "two",
    "comma",
    "zero",
    "remember",
    "correctly",
    "0",
    "59",
    "60",
    "values",
    "number",
    "going",
    "creating",
    "array",
    "0",
    "59",
    "number",
    "60",
    "going",
    "train",
    "appended",
    "goes",
    "way",
    "0",
    "59",
    "call",
    "60",
    "since",
    "value",
    "goes",
    "way",
    "12",
    "value",
    "comes",
    "length",
    "data",
    "loading",
    "loaded",
    "two",
    "arrays",
    "loaded",
    "one",
    "array",
    "filled",
    "arrays",
    "0",
    "59",
    "loaded",
    "one",
    "array",
    "value",
    "looking",
    "want",
    "think",
    "time",
    "sequence",
    "uh",
    "open",
    "open",
    "open",
    "openopenopen",
    "next",
    "one",
    "series",
    "looking",
    "google",
    "stock",
    "time",
    "opens",
    "want",
    "know",
    "next",
    "one",
    "0",
    "59",
    "60",
    "1",
    "60",
    "61",
    "2",
    "62",
    "62",
    "going",
    "loaded",
    "loop",
    "go",
    "ahead",
    "take",
    "equals",
    "dot",
    "np",
    "array",
    "converting",
    "back",
    "numpy",
    "array",
    "way",
    "use",
    "cool",
    "tools",
    "get",
    "numpy",
    "array",
    "including",
    "reshaping",
    "take",
    "look",
    "see",
    "going",
    "going",
    "take",
    "x",
    "train",
    "going",
    "reshape",
    "wow",
    "heck",
    "reshape",
    "mean",
    "means",
    "array",
    "remember",
    "correctly",
    "many",
    "numbers",
    "wide",
    "x",
    "train",
    "dot",
    "shape",
    "gets",
    "one",
    "shapes",
    "get",
    "um",
    "x",
    "train",
    "dot",
    "shape",
    "one",
    "gets",
    "shape",
    "making",
    "sure",
    "data",
    "formatted",
    "correctly",
    "use",
    "pull",
    "fact",
    "60",
    "case",
    "value",
    "60",
    "1199",
    "1258",
    "minus",
    "60",
    "1199",
    "making",
    "sure",
    "shaped",
    "correctly",
    "data",
    "grouped",
    "11",
    "99",
    "60",
    "different",
    "arrays",
    "one",
    "end",
    "means",
    "end",
    "dealing",
    "shapes",
    "numpy",
    "look",
    "layers",
    "layer",
    "needs",
    "one",
    "value",
    "like",
    "leaf",
    "tree",
    "branch",
    "branches",
    "um",
    "get",
    "leaf",
    "np",
    "reshape",
    "comes",
    "using",
    "existing",
    "shapes",
    "form",
    "go",
    "ahead",
    "run",
    "piece",
    "code",
    "real",
    "output",
    "import",
    "different",
    "cross",
    "modules",
    "need",
    "cross",
    "models",
    "going",
    "import",
    "sequential",
    "model",
    "dealing",
    "sequential",
    "data",
    "dense",
    "layers",
    "actually",
    "three",
    "layers",
    "going",
    "bring",
    "dents",
    "lstm",
    "focusing",
    "dropout",
    "discuss",
    "three",
    "layers",
    "moment",
    "need",
    "lstm",
    "need",
    "dropout",
    "final",
    "layer",
    "dents",
    "let",
    "go",
    "ahead",
    "run",
    "bring",
    "port",
    "modules",
    "see",
    "get",
    "error",
    "read",
    "closer",
    "actually",
    "error",
    "warning",
    "warning",
    "mean",
    "things",
    "come",
    "time",
    "working",
    "cutting",
    "edge",
    "modules",
    "completely",
    "updated",
    "time",
    "going",
    "worry",
    "much",
    "warning",
    "saying",
    "h5py",
    "module",
    "part",
    "cross",
    "going",
    "updated",
    "point",
    "running",
    "new",
    "stuff",
    "cross",
    "start",
    "updating",
    "cross",
    "system",
    "better",
    "make",
    "sure",
    "h5",
    "pi",
    "updated",
    "otherwise",
    "going",
    "error",
    "later",
    "actually",
    "run",
    "update",
    "h5",
    "pi",
    "wanted",
    "big",
    "deal",
    "going",
    "worry",
    "today",
    "said",
    "going",
    "jump",
    "start",
    "looking",
    "layers",
    "mean",
    "meant",
    "going",
    "start",
    "initializing",
    "rnn",
    "start",
    "adding",
    "layers",
    "see",
    "lstm",
    "dropout",
    "lstm",
    "dropout",
    "lstm",
    "dropout",
    "heck",
    "let",
    "explore",
    "start",
    "initializing",
    "rnn",
    "regressor",
    "equals",
    "sequential",
    "using",
    "sequential",
    "model",
    "run",
    "load",
    "going",
    "start",
    "adding",
    "lstm",
    "layer",
    "dropout",
    "regularization",
    "right",
    "q",
    "dropout",
    "regularization",
    "go",
    "back",
    "remember",
    "exploding",
    "gradient",
    "well",
    "talking",
    "dropout",
    "drops",
    "unnecessary",
    "data",
    "shifting",
    "huge",
    "amounts",
    "data",
    "network",
    "go",
    "let",
    "go",
    "ahead",
    "add",
    "go",
    "ahead",
    "run",
    "three",
    "let",
    "go",
    "ahead",
    "put",
    "three",
    "go",
    "back",
    "second",
    "one",
    "let",
    "put",
    "one",
    "let",
    "put",
    "go",
    "ahead",
    "put",
    "two",
    "mean",
    "said",
    "one",
    "actually",
    "two",
    "let",
    "add",
    "one",
    "see",
    "time",
    "run",
    "actually",
    "output",
    "let",
    "take",
    "closer",
    "look",
    "see",
    "going",
    "going",
    "add",
    "first",
    "lstm",
    "layer",
    "going",
    "units",
    "units",
    "positive",
    "integer",
    "dimensionality",
    "output",
    "space",
    "going",
    "next",
    "layer",
    "might",
    "60",
    "coming",
    "50",
    "going",
    "return",
    "sequence",
    "sequence",
    "data",
    "want",
    "keep",
    "true",
    "tell",
    "shape",
    "well",
    "already",
    "know",
    "shape",
    "going",
    "looking",
    "x",
    "train",
    "shape",
    "input",
    "shape",
    "equals",
    "x",
    "train",
    "shape",
    "one",
    "comma",
    "one",
    "makes",
    "really",
    "easy",
    "remember",
    "numbers",
    "put",
    "60",
    "whatever",
    "else",
    "let",
    "tell",
    "regressor",
    "model",
    "use",
    "follow",
    "stm",
    "dropout",
    "layer",
    "understanding",
    "dropout",
    "layer",
    "kind",
    "exciting",
    "one",
    "things",
    "happens",
    "train",
    "network",
    "means",
    "neural",
    "network",
    "memorize",
    "specific",
    "data",
    "trouble",
    "predicting",
    "anything",
    "specific",
    "realm",
    "fix",
    "time",
    "run",
    "training",
    "mode",
    "going",
    "take",
    "20",
    "percent",
    "neurons",
    "turn",
    "going",
    "train",
    "ones",
    "going",
    "random",
    "way",
    "time",
    "pass",
    "train",
    "nodes",
    "come",
    "back",
    "next",
    "training",
    "cycle",
    "randomly",
    "pick",
    "different",
    "finally",
    "see",
    "big",
    "difference",
    "go",
    "first",
    "second",
    "third",
    "fourth",
    "first",
    "thing",
    "input",
    "shape",
    "shapes",
    "already",
    "output",
    "units",
    "50",
    "item",
    "next",
    "step",
    "automatically",
    "knows",
    "layer",
    "putting",
    "50",
    "next",
    "layer",
    "automatically",
    "sets",
    "says",
    "50",
    "coming",
    "last",
    "layer",
    "coming",
    "know",
    "goes",
    "regressor",
    "course",
    "dropout",
    "coming",
    "one",
    "next",
    "three",
    "layers",
    "let",
    "know",
    "shape",
    "automatically",
    "understands",
    "gon",
    "na",
    "keep",
    "units",
    "still",
    "gon",
    "na",
    "50",
    "units",
    "still",
    "sequence",
    "coming",
    "50",
    "units",
    "sequence",
    "next",
    "piece",
    "code",
    "brings",
    "together",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "come",
    "put",
    "output",
    "layer",
    "dense",
    "layer",
    "remember",
    "three",
    "layers",
    "lstm",
    "dropout",
    "dents",
    "dense",
    "says",
    "going",
    "bring",
    "one",
    "output",
    "instead",
    "putting",
    "sequence",
    "know",
    "want",
    "know",
    "answer",
    "point",
    "let",
    "go",
    "ahead",
    "run",
    "notice",
    "setting",
    "things",
    "one",
    "step",
    "time",
    "far",
    "brought",
    "way",
    "brought",
    "data",
    "brought",
    "different",
    "modules",
    "formatted",
    "data",
    "training",
    "set",
    "know",
    "x",
    "train",
    "train",
    "source",
    "data",
    "answers",
    "know",
    "far",
    "going",
    "put",
    "reshaped",
    "come",
    "built",
    "cross",
    "imported",
    "different",
    "layers",
    "look",
    "uh",
    "five",
    "total",
    "layers",
    "cross",
    "little",
    "different",
    "lot",
    "systems",
    "lot",
    "systems",
    "put",
    "one",
    "line",
    "automatic",
    "give",
    "options",
    "layers",
    "interface",
    "give",
    "options",
    "data",
    "comes",
    "cross",
    "cutting",
    "edge",
    "reason",
    "even",
    "though",
    "lot",
    "extra",
    "steps",
    "building",
    "model",
    "huge",
    "impact",
    "output",
    "new",
    "models",
    "cross",
    "brought",
    "dense",
    "full",
    "model",
    "put",
    "together",
    "regressor",
    "need",
    "go",
    "ahead",
    "compile",
    "going",
    "go",
    "ahead",
    "fit",
    "data",
    "going",
    "compile",
    "pieces",
    "come",
    "together",
    "going",
    "run",
    "training",
    "data",
    "actually",
    "recreate",
    "regressor",
    "ready",
    "used",
    "let",
    "go",
    "ahead",
    "compile",
    "go",
    "ahead",
    "run",
    "looking",
    "tutorials",
    "neural",
    "networks",
    "see",
    "going",
    "use",
    "optimizer",
    "atom",
    "atom",
    "optimized",
    "big",
    "data",
    "couple",
    "optimizers",
    "beyond",
    "scope",
    "tutorial",
    "certainly",
    "atom",
    "work",
    "pretty",
    "good",
    "loss",
    "equals",
    "mean",
    "squared",
    "value",
    "training",
    "want",
    "base",
    "loss",
    "bad",
    "error",
    "going",
    "use",
    "mean",
    "squared",
    "value",
    "error",
    "atom",
    "optimizer",
    "differential",
    "equations",
    "know",
    "math",
    "behind",
    "certainly",
    "helps",
    "know",
    "fit",
    "bigger",
    "models",
    "finally",
    "going",
    "fit",
    "fitting",
    "rn",
    "training",
    "set",
    "x",
    "train",
    "train",
    "epochs",
    "batch",
    "size",
    "know",
    "data",
    "coming",
    "x",
    "train",
    "train",
    "answer",
    "looking",
    "data",
    "sequential",
    "input",
    "epics",
    "many",
    "times",
    "going",
    "go",
    "whole",
    "data",
    "set",
    "created",
    "whole",
    "data",
    "set",
    "x",
    "train",
    "rows",
    "includes",
    "time",
    "sequence",
    "bad",
    "size",
    "another",
    "one",
    "things",
    "cross",
    "really",
    "shines",
    "pulling",
    "save",
    "large",
    "file",
    "instead",
    "trying",
    "load",
    "ram",
    "pick",
    "smaller",
    "batches",
    "load",
    "indirectly",
    "worried",
    "pulling",
    "file",
    "today",
    "big",
    "enough",
    "cause",
    "computer",
    "much",
    "problem",
    "run",
    "straining",
    "resources",
    "run",
    "imagine",
    "happened",
    "lot",
    "one",
    "column",
    "one",
    "set",
    "stock",
    "case",
    "google",
    "stock",
    "imagine",
    "across",
    "stocks",
    "instead",
    "open",
    "open",
    "close",
    "high",
    "low",
    "actually",
    "find",
    "13",
    "different",
    "variables",
    "times",
    "60",
    "time",
    "sequence",
    "suddenly",
    "find",
    "gig",
    "memory",
    "loading",
    "ram",
    "completely",
    "know",
    "multiple",
    "computers",
    "cluster",
    "start",
    "running",
    "resource",
    "problems",
    "worry",
    "let",
    "go",
    "ahead",
    "run",
    "actually",
    "take",
    "little",
    "bit",
    "computer",
    "older",
    "laptop",
    "give",
    "second",
    "kick",
    "go",
    "right",
    "epic",
    "going",
    "tell",
    "running",
    "first",
    "run",
    "data",
    "going",
    "batching",
    "32",
    "pieces",
    "32",
    "lines",
    "time",
    "1198",
    "think",
    "said",
    "earlier",
    "one",
    "one",
    "13",
    "seconds",
    "imagine",
    "roughly",
    "20",
    "30",
    "minutes",
    "run",
    "time",
    "computer",
    "like",
    "said",
    "older",
    "laptop",
    "running",
    "uh",
    "gigahertz",
    "dual",
    "processor",
    "fine",
    "go",
    "ahead",
    "stop",
    "go",
    "get",
    "drink",
    "coffee",
    "come",
    "back",
    "let",
    "see",
    "happens",
    "end",
    "takes",
    "us",
    "like",
    "good",
    "cooking",
    "show",
    "kind",
    "gotten",
    "latte",
    "also",
    "stuff",
    "running",
    "background",
    "see",
    "numbers",
    "jumped",
    "like",
    "19",
    "seconds",
    "15",
    "seconds",
    "scroll",
    "see",
    "run",
    "100",
    "steps",
    "100",
    "epics",
    "question",
    "mean",
    "one",
    "first",
    "things",
    "notice",
    "loss",
    "kind",
    "stopped",
    "see",
    "kind",
    "goes",
    "hit",
    "three",
    "times",
    "row",
    "guessed",
    "epic",
    "pretty",
    "close",
    "since",
    "losses",
    "remain",
    "find",
    "looking",
    "going",
    "go",
    "ahead",
    "load",
    "test",
    "data",
    "test",
    "data",
    "process",
    "yet",
    "real",
    "stock",
    "price",
    "data",
    "set",
    "test",
    "eye",
    "location",
    "thing",
    "prepped",
    "data",
    "first",
    "place",
    "let",
    "go",
    "ahead",
    "go",
    "code",
    "see",
    "labeled",
    "part",
    "three",
    "making",
    "predictions",
    "visualizing",
    "results",
    "first",
    "thing",
    "need",
    "go",
    "ahead",
    "read",
    "data",
    "test",
    "csv",
    "see",
    "changed",
    "path",
    "computer",
    "call",
    "real",
    "stock",
    "price",
    "one",
    "column",
    "values",
    "location",
    "rows",
    "values",
    "one",
    "location",
    "open",
    "stock",
    "open",
    "let",
    "go",
    "ahead",
    "run",
    "loaded",
    "let",
    "go",
    "ahead",
    "create",
    "inputs",
    "going",
    "create",
    "inputs",
    "look",
    "familiar",
    "thing",
    "going",
    "take",
    "data",
    "set",
    "total",
    "going",
    "little",
    "pandas",
    "concat",
    "datastate",
    "train",
    "remember",
    "end",
    "dataset",
    "train",
    "part",
    "data",
    "going",
    "let",
    "visualize",
    "little",
    "bit",
    "train",
    "data",
    "let",
    "put",
    "tr",
    "train",
    "went",
    "value",
    "one",
    "values",
    "generated",
    "bunch",
    "columns",
    "60",
    "across",
    "value",
    "equals",
    "one",
    "value",
    "equals",
    "one",
    "value",
    "equals",
    "one",
    "need",
    "top",
    "60",
    "go",
    "new",
    "data",
    "find",
    "looking",
    "going",
    "go",
    "ahead",
    "load",
    "test",
    "data",
    "test",
    "data",
    "process",
    "yet",
    "real",
    "stock",
    "price",
    "data",
    "set",
    "test",
    "eye",
    "location",
    "thing",
    "prepped",
    "data",
    "first",
    "place",
    "let",
    "go",
    "ahead",
    "go",
    "code",
    "see",
    "labeled",
    "part",
    "three",
    "making",
    "predictions",
    "visualizing",
    "results",
    "first",
    "thing",
    "need",
    "go",
    "ahead",
    "read",
    "data",
    "test",
    "csv",
    "see",
    "changed",
    "path",
    "computer",
    "call",
    "real",
    "stock",
    "price",
    "one",
    "column",
    "values",
    "location",
    "rows",
    "values",
    "one",
    "location",
    "open",
    "stock",
    "open",
    "let",
    "go",
    "ahead",
    "run",
    "loaded",
    "let",
    "go",
    "ahead",
    "create",
    "inputs",
    "going",
    "create",
    "inputs",
    "look",
    "familiar",
    "thing",
    "going",
    "take",
    "data",
    "set",
    "total",
    "going",
    "little",
    "panda",
    "concat",
    "datastate",
    "train",
    "remember",
    "end",
    "data",
    "set",
    "train",
    "part",
    "data",
    "going",
    "let",
    "visualize",
    "little",
    "bit",
    "train",
    "data",
    "let",
    "put",
    "tr",
    "train",
    "went",
    "value",
    "one",
    "values",
    "generated",
    "bunch",
    "columns",
    "60",
    "across",
    "value",
    "equals",
    "one",
    "value",
    "equals",
    "one",
    "value",
    "equals",
    "one",
    "need",
    "top",
    "60",
    "go",
    "new",
    "data",
    "part",
    "next",
    "data",
    "actually",
    "top",
    "first",
    "setup",
    "going",
    "real",
    "stock",
    "price",
    "going",
    "take",
    "data",
    "set",
    "test",
    "going",
    "load",
    "real",
    "stock",
    "price",
    "data",
    "location",
    "looking",
    "first",
    "column",
    "open",
    "price",
    "data",
    "set",
    "total",
    "going",
    "take",
    "pandas",
    "going",
    "concat",
    "going",
    "take",
    "data",
    "set",
    "train",
    "open",
    "dataset",
    "test",
    "open",
    "one",
    "way",
    "reference",
    "columns",
    "referenced",
    "couple",
    "different",
    "ways",
    "referenced",
    "one",
    "two",
    "know",
    "labeled",
    "panda",
    "set",
    "open",
    "pandas",
    "great",
    "way",
    "lots",
    "versatility",
    "go",
    "ahead",
    "go",
    "back",
    "run",
    "go",
    "notice",
    "open",
    "data",
    "set",
    "pended",
    "two",
    "different",
    "concatenated",
    "two",
    "data",
    "sets",
    "together",
    "inputs",
    "equals",
    "data",
    "set",
    "total",
    "length",
    "data",
    "set",
    "total",
    "minus",
    "length",
    "data",
    "set",
    "minus",
    "test",
    "minus",
    "60",
    "values",
    "going",
    "run",
    "see",
    "works",
    "normally",
    "running",
    "test",
    "set",
    "versus",
    "training",
    "set",
    "run",
    "completely",
    "separate",
    "graph",
    "see",
    "going",
    "looking",
    "part",
    "train",
    "see",
    "well",
    "graphs",
    "inputs",
    "equals",
    "inputs",
    "dot",
    "reshapes",
    "reshaping",
    "like",
    "transforming",
    "inputs",
    "remember",
    "transform",
    "zero",
    "one",
    "uh",
    "finally",
    "want",
    "go",
    "ahead",
    "take",
    "x",
    "test",
    "going",
    "create",
    "x",
    "test",
    "range",
    "60",
    "x",
    "test",
    "appending",
    "inputs",
    "60",
    "remember",
    "0",
    "59",
    "comma",
    "0",
    "side",
    "first",
    "column",
    "open",
    "column",
    "take",
    "x",
    "test",
    "convert",
    "numpy",
    "array",
    "reshape",
    "get",
    "final",
    "two",
    "lines",
    "something",
    "new",
    "right",
    "last",
    "two",
    "lines",
    "let",
    "highlight",
    "mark",
    "predicted",
    "stock",
    "price",
    "equals",
    "regressor",
    "dot",
    "predicts",
    "x",
    "test",
    "predicting",
    "stock",
    "including",
    "training",
    "testing",
    "model",
    "want",
    "take",
    "prediction",
    "want",
    "inverse",
    "transform",
    "remember",
    "put",
    "zero",
    "one",
    "well",
    "going",
    "mean",
    "much",
    "look",
    "float",
    "number",
    "zero",
    "one",
    "want",
    "dollar",
    "amount",
    "want",
    "know",
    "cash",
    "value",
    "go",
    "ahead",
    "run",
    "see",
    "runs",
    "much",
    "quicker",
    "training",
    "wonderful",
    "neural",
    "networks",
    "put",
    "together",
    "takes",
    "second",
    "run",
    "neural",
    "network",
    "took",
    "us",
    "half",
    "hour",
    "train",
    "add",
    "plot",
    "data",
    "going",
    "plot",
    "think",
    "going",
    "going",
    "plot",
    "real",
    "data",
    "google",
    "stock",
    "actually",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "code",
    "let",
    "uh",
    "pull",
    "code",
    "plt",
    "oh",
    "remember",
    "beginning",
    "let",
    "go",
    "back",
    "top",
    "matplot",
    "plt",
    "comes",
    "come",
    "going",
    "plot",
    "let",
    "get",
    "drawing",
    "thing",
    "going",
    "go",
    "ahead",
    "plt",
    "basically",
    "kind",
    "like",
    "object",
    "one",
    "things",
    "always",
    "threw",
    "graphs",
    "python",
    "always",
    "think",
    "create",
    "object",
    "loads",
    "class",
    "well",
    "case",
    "plt",
    "like",
    "canvas",
    "putting",
    "stuff",
    "done",
    "html5",
    "canvas",
    "object",
    "canvas",
    "going",
    "plot",
    "real",
    "stock",
    "price",
    "actually",
    "going",
    "give",
    "color",
    "red",
    "going",
    "bright",
    "red",
    "going",
    "label",
    "real",
    "google",
    "stock",
    "price",
    "going",
    "predicted",
    "stock",
    "going",
    "blue",
    "going",
    "labeled",
    "predicted",
    "give",
    "title",
    "always",
    "nice",
    "give",
    "title",
    "graph",
    "especially",
    "going",
    "present",
    "somebody",
    "know",
    "shareholders",
    "office",
    "x",
    "label",
    "going",
    "time",
    "time",
    "series",
    "actually",
    "put",
    "actual",
    "date",
    "times",
    "fine",
    "know",
    "incremented",
    "time",
    "course",
    "label",
    "actual",
    "stock",
    "price",
    "tells",
    "us",
    "build",
    "legend",
    "color",
    "red",
    "real",
    "google",
    "stock",
    "price",
    "show",
    "plot",
    "shows",
    "us",
    "actual",
    "graph",
    "let",
    "go",
    "ahead",
    "run",
    "see",
    "looks",
    "like",
    "see",
    "nice",
    "graph",
    "let",
    "talk",
    "little",
    "bit",
    "graph",
    "wrap",
    "legend",
    "telling",
    "legend",
    "show",
    "prices",
    "title",
    "everything",
    "notice",
    "bottom",
    "time",
    "sequence",
    "put",
    "actual",
    "time",
    "could",
    "could",
    "gone",
    "ahead",
    "plotted",
    "x",
    "since",
    "know",
    "dates",
    "plotted",
    "dates",
    "also",
    "know",
    "last",
    "piece",
    "data",
    "looking",
    "last",
    "piece",
    "data",
    "somewhere",
    "probably",
    "around",
    "graph",
    "think",
    "like",
    "20",
    "data",
    "probably",
    "less",
    "google",
    "price",
    "google",
    "price",
    "little",
    "jump",
    "see",
    "actual",
    "google",
    "instead",
    "turn",
    "go",
    "high",
    "load",
    "go",
    "prediction",
    "pattern",
    "overall",
    "value",
    "pretty",
    "far",
    "far",
    "um",
    "stock",
    "looking",
    "one",
    "column",
    "looking",
    "open",
    "price",
    "looking",
    "many",
    "volumes",
    "traded",
    "like",
    "pointing",
    "earlier",
    "talk",
    "stock",
    "right",
    "bat",
    "six",
    "columns",
    "open",
    "high",
    "low",
    "close",
    "volume",
    "weather",
    "mean",
    "volume",
    "shares",
    "adjusted",
    "open",
    "adjusted",
    "high",
    "adjusted",
    "low",
    "adjusted",
    "close",
    "special",
    "formula",
    "predict",
    "exactly",
    "would",
    "really",
    "worth",
    "based",
    "value",
    "stock",
    "kinds",
    "stuff",
    "put",
    "looking",
    "one",
    "small",
    "aspect",
    "opening",
    "price",
    "stock",
    "see",
    "pretty",
    "good",
    "job",
    "curve",
    "follows",
    "curve",
    "pretty",
    "well",
    "like",
    "little",
    "jumps",
    "bins",
    "quite",
    "match",
    "bin",
    "quite",
    "match",
    "bin",
    "pretty",
    "darn",
    "close",
    "basic",
    "shape",
    "prediction",
    "far",
    "imagine",
    "add",
    "data",
    "look",
    "different",
    "aspects",
    "specific",
    "domain",
    "stock",
    "able",
    "get",
    "better",
    "representation",
    "time",
    "drill",
    "deeper",
    "course",
    "took",
    "half",
    "hour",
    "program",
    "computer",
    "train",
    "imagine",
    "running",
    "across",
    "different",
    "variables",
    "might",
    "take",
    "little",
    "bit",
    "longer",
    "train",
    "data",
    "good",
    "quick",
    "tutorial",
    "like",
    "covered",
    "lot",
    "really",
    "cool",
    "things",
    "tutorial",
    "today",
    "hopefully",
    "able",
    "get",
    "rich",
    "predicting",
    "stock",
    "maybe",
    "get",
    "job",
    "familiar",
    "stock",
    "domain",
    "business",
    "domain",
    "see",
    "applied",
    "kinds",
    "different",
    "uh",
    "tools",
    "trades",
    "across",
    "different",
    "domains",
    "biggest",
    "takeaways",
    "introduction",
    "rnn",
    "set",
    "went",
    "popular",
    "neural",
    "networks",
    "discussed",
    "recurrent",
    "neural",
    "network",
    "went",
    "uh",
    "one",
    "big",
    "problems",
    "rnn",
    "exploding",
    "gradient",
    "problem",
    "also",
    "discussed",
    "long",
    "short",
    "memory",
    "networks",
    "part",
    "working",
    "lst",
    "part",
    "rnn",
    "setup",
    "finally",
    "went",
    "predicted",
    "google",
    "stock",
    "see",
    "want",
    "thank",
    "joining",
    "us",
    "today",
    "name",
    "richard",
    "kirschner",
    "one",
    "simply",
    "learn",
    "team",
    "information",
    "please",
    "visit",
    "website",
    "feel",
    "free",
    "ask",
    "questions",
    "check",
    "different",
    "courses",
    "also",
    "place",
    "comments",
    "youtube",
    "video",
    "keep",
    "monitoring",
    "try",
    "address",
    "thank",
    "joining",
    "us",
    "today",
    "hi",
    "like",
    "video",
    "subscribe",
    "simply",
    "learn",
    "youtube",
    "channel",
    "click",
    "watch",
    "similar",
    "videos",
    "turn",
    "get",
    "certified",
    "click"
  ],
  "keywords": [
    "rnn",
    "recurrent",
    "neural",
    "network",
    "team",
    "get",
    "ahead",
    "start",
    "course",
    "networks",
    "important",
    "know",
    "going",
    "looking",
    "one",
    "big",
    "things",
    "call",
    "exploding",
    "gradient",
    "problem",
    "look",
    "using",
    "use",
    "case",
    "cross",
    "python",
    "module",
    "deep",
    "learning",
    "long",
    "short",
    "term",
    "memory",
    "lstm",
    "see",
    "part",
    "dive",
    "take",
    "google",
    "feature",
    "words",
    "lot",
    "time",
    "kind",
    "hit",
    "everything",
    "much",
    "well",
    "first",
    "data",
    "sequence",
    "model",
    "predict",
    "next",
    "word",
    "sentence",
    "loss",
    "say",
    "common",
    "usually",
    "three",
    "different",
    "tool",
    "us",
    "stuff",
    "used",
    "getting",
    "let",
    "go",
    "talk",
    "layers",
    "structure",
    "human",
    "way",
    "like",
    "function",
    "note",
    "huge",
    "train",
    "image",
    "two",
    "dog",
    "uh",
    "looks",
    "nice",
    "goes",
    "input",
    "layer",
    "might",
    "point",
    "feed",
    "hidden",
    "another",
    "output",
    "propagation",
    "many",
    "finally",
    "set",
    "forward",
    "actually",
    "little",
    "bit",
    "problems",
    "thing",
    "mean",
    "right",
    "information",
    "talking",
    "predicted",
    "x",
    "times",
    "based",
    "current",
    "sequential",
    "series",
    "back",
    "whatever",
    "put",
    "coming",
    "memorize",
    "previous",
    "inputs",
    "h",
    "value",
    "c",
    "whole",
    "prediction",
    "also",
    "analyzing",
    "add",
    "predicting",
    "prices",
    "stocks",
    "particular",
    "stock",
    "today",
    "new",
    "second",
    "um",
    "really",
    "give",
    "language",
    "processing",
    "text",
    "sentiment",
    "together",
    "said",
    "order",
    "machine",
    "languages",
    "good",
    "speaking",
    "english",
    "would",
    "parts",
    "person",
    "think",
    "something",
    "works",
    "make",
    "comes",
    "minus",
    "equals",
    "1",
    "last",
    "state",
    "step",
    "need",
    "single",
    "real",
    "basic",
    "run",
    "takes",
    "adding",
    "example",
    "complicated",
    "understand",
    "working",
    "training",
    "full",
    "instead",
    "computer",
    "end",
    "error",
    "took",
    "blank",
    "got",
    "maybe",
    "context",
    "noun",
    "want",
    "find",
    "weight",
    "lstms",
    "setup",
    "try",
    "pretty",
    "special",
    "modules",
    "tangent",
    "plus",
    "steps",
    "cell",
    "values",
    "decides",
    "remember",
    "sigmoid",
    "bias",
    "gate",
    "john",
    "football",
    "yesterday",
    "phone",
    "captain",
    "since",
    "0",
    "come",
    "already",
    "quick",
    "brave",
    "match",
    "price",
    "import",
    "anaconda",
    "open",
    "loaded",
    "show",
    "load",
    "create",
    "file",
    "code",
    "half",
    "numpy",
    "array",
    "pandas",
    "notice",
    "piece",
    "actual",
    "lines",
    "test",
    "far",
    "went",
    "comma",
    "variables",
    "high",
    "close",
    "running",
    "location",
    "columns",
    "zero",
    "graph",
    "100",
    "na",
    "fit",
    "60",
    "correctly",
    "59",
    "dot",
    "shape",
    "dropout",
    "regressor",
    "units",
    "50",
    "total",
    "imagine",
    "column",
    "across",
    "plot"
  ]
}