{
  "text": "hello data scientist welcome to skill\ngate\nin this course we will build a sentiment\nclassification model using neural\nnetworks\nwe will use the popular imdb movie\nreviews data set and train three\nseparate neural networks namely a simple\nneural net a convolutional neural net or\ncnn and lastly a long short term memory\nor lstm neural net\nlstm networks are actually considered to\nbe quite suitable for handling nlp\nproblems and by the end of this video\nyou will understand why\njust to give you a quick glimpse the\nsentiment classification lstm model we\nshall be training as part of this\ntutorial is so good that it not only\npredicts the user sentiment as positive\nnegative it also does a fabulous job in\npredicting the imdb rating itself\ncorresponding to these reviews\nwith quite good accuracy as you can see\nhere\nduring the course of the next 20 minutes\nor so we shall discuss the mighty word\nembeddings which is a very powerful\nfeature extraction technique\nintuition on various neural network\napproaches\nthe concept of overfitting that happens\nwhen a model memorizes training data\nrather than understanding general\ndistribution\nand of course our high level solution\narchitecture for training our sentiment\nclassification model\nso without further ado let's get started\nintroductions we are skillgate and we're\non a mission to bring you application\nbased machine learning education\nwe launch new machine learning projects\nevery week so make sure to subscribe to\nour channel and also hit that bell icon\nso you get notified of all our new ml\nprojects\nreally seek your support here guys as\nthis keeps us motivated and helps us in\nreaching out to more learners like you\nand yes let's talk machine learning\npeople in case you need any ml project\nrelated help or want to discuss your\ncrazy ideas you can go ahead and book a\nfree one-on-one online session with us\nby visiting our website skillkate.com\nand filling out this one-on-one\nmentoring session registration form\nor you may write to us over email\nwhatsapp as well\nall right coming to the topic of\ndiscussion for today which is sentiment\nclassification using neural networks\nlet me start off with our solution\narchitecture and a brief on the basics\nthis is our high level solution\narchitecture\nas a first step here we are using the\nimdb movie reviews data set that has 50\n000 reviews along with positive negative\nsentiment labels\nthis is how our data set looks like we\nhave these\nmovie reviews in the first column mapped\nwith their sentiment labels positive\nnegative\nthis bar chart depicts the distribution\nof our data across sentiment labels and\nit seemed to be fairly uniform\nokay back to the architecture as a\nsecond step we do certain pre-processing\noperations on our review text\nthis is one of the movie reviews i\nrandomly picked from our data set\nover here you may see we have these\npunctuation marks\nspecial characters uh html tags and\nnumbers\nall of these add little to no value in\ntelling whether a review is positive or\nnegative\nso we shall drop them all as part of\nthis pre-processing step\ni'll give you more insights on this when\nwe get into the coding part\nmoving on our third step is word\nembedding where we provision for\nconverting our textual data to a numeric\nform using a very powerful feature\nextraction technique called word\nembedding\nto use\ntext data as input to a neural network\nmodel we need to convert text to numbers\nfirst\nthere are popular techniques like one\nhotend coding which do a fair job in\ngetting this done\nhowever unlike machine learning models\npassing sparse vectors of huge sizes can\ngreatly affect neural network models\ntherefore we need to convert our text to\nsmall dense vectors and this is where\nword embedding comes in\nin the case of word embeddings every\nword is represented as a n-dimensional\ndense vector\nand the beauty of word embeddings is in\nthe way it manages to retain the\nsemantic relationship among words as\nevery token has n-dimensional parameters\nexplaining its characteristics for\nexample here we have these four tokens\nman women king and queen along with\ntheir seven dimensional vector\nrepresentation depicting semantic\nrelevance on a scale of negative one and\none\nand these dimensions are nothing but a\nmathematical way to capture semantic\nrelations among these tokens\nlike this first dimension\nwe could have is living being\nas all of these four tokens correspond\nto human beings so we have these weights\ntouching uh almost one for all of these\ntokens and likewise for these remaining\nparameters as well\njust for visualization word weddings are\nlike this huge nexus of words that have\nsemantically similar words forming\nclusters\nbuilt with the help of a complex\nalgorithm that establishes semantic\nrelationships among words based on their\nusage in millions of sentences\nso here we may see clusters of words\nthat depict cities\nuh travel\nfood etc and of course the words in each\nof these clusters shall have similar\nn-dimensional vectors in our case we\nshall use the pre-trained globe e word\nembedding with 100 dimensional vector\nfor text to numeric transformation of\nour training data\nall right coming back to our solution\narchitecture\nthis next modal training step is what\nsits at the heart of this entire\nexercise where we shall train three\nseparate neural network models namely a\nsimple neural net a convolutional neural\nnet and a lstm neural net which are\nessentially three very popular but\ndifferent approaches\nnow let me give you a quick refresher on\nneural networks as well before we jump\ninto the coding part of our project to\nexecute this solution architecture we\nhave been discussing\nas one advances into the field of\nmachine learning and neural networks\nyou would start appreciating more and\nmore of how the human brain works\nit's so intriguing how humans may learn\nnew skills within a matter of days or\nmonths and be prohibited one day\nneural networks attempt to mimic the\ncomputation capabilities of our human\nbrain\nand that's the reason why these are\nquite suited to recognize numerical\npatterns contained in feature\nrepresentations of real world\nunstructured data like images\nsound text time series etc\nhaving said that feature representations\nhold the key to train a powerful neural\nnetwork for image data input pixels form\nthe\nbasis of derivation of these features\nfor sound it's the\ntime amplitude and frequency of the\naudio file and in our specific use case\nwhich is text data it is the word tokens\nthat form the user review sentences\nforming the basis for derivation of\nfeatures\nafter future representation the next\nimportant thing is to choose the neural\nnetwork approach we may want to apply on\nour specific problem statement\nhere we have prepared this crisp\ncomparison chart for you to have a\nholistic idea on these different\napproaches\nyou may post this video here if you wish\nto go through this detail sheet at this\npoint\nif you may have any queries post them in\nthe comment section below and i'll be\nmore than happy answering all of them\ni'll use this moment to also introduce\nyou to the problem of overfitting\nremember as a child many of us would\ncram up or memorize answers to complex\nmathematical problems and speed it out\nin the exam sheet\nneural networks also have a tendency to\ndo so\nit happens when the networks do not\nlearn the general distribution of\ntraining data points\nand rather memorize the expected\noutcomes like this\noverfitting happens when the model\nreturns high testing error as against a\nlow training error\nand if\nboth error values are\nappropriately low and in close proximity\nwe have a well fitting model\nwe shall observe this phenomena of\noverfitting during modal training in our\nproject\nall right now it's time for us to jump\ninto the coding part of our project and\ntrain our sentiment classification\nneural network models\nso\nlet's get coding\nthis is our skill kit do it yourself\ntool kit for you\nit has all these model related files\nincluding this training data set\npre-trained glovy word embeddings this\ncsv with fresh imdb reviews\nalong with their movie name and\nreal imdb rating\nas you may see\nthen this jupiter notebook code file and\nlastly this pre-trained lstm model that\nwe created previously\nlet's open this jupyter code file now to\ndo a quick code walkthrough\nin fact i already have it open\nlet's switch the tabs\nnow we are here in collab we have\nsegmented our entire code into these\nsections we see on the left side here\nfirst one here we have is plan of action\nwhich is pretty much self explanatory by\nnow\nlet's move on to the second section\nwhich is setting the environment\nby the way i ran this entire code file\nprior to recording this video to save on\ntime\nwill take you through all the outputs\nanyway\nhere first up we are mounting google\ndrive to collab\nthen we are setting our working\ndirectory to our project toolkit folder\nwithin google drive\npost that we are importing all essential\nlibraries and functions with this next\ncode cell\nand that's it for setting up the\nenvironment\nnext up we have loading the data set\nover here first up we are importing our\nimdb movies dataset\nthen for exploring our data set we are\nfirst checking for the shape\nthen we are using this head function to\nactually see how\nour data set looks\nas you may see we have two columns here\nreview and sentiment\nas a good data science practice we also\ncheck for null values\nand we see there are none over here\nthen we also check for data skewness\nacross our data labels and again we see\nthere's no issues there as well\nnext section is data preprocessing\nhere in this first code cell we are\nprinting a review from our data set\nas you may see we have punctuation marks\nnumbers\nhtml tags etc over here\nas part of preprocessing we shall filter\nall of these out\nthen we are defining this function\ncalled remove tags to actually replace\nhtml tags from the given reviews with\nwhite spaces\nnext up we are downloading the\nstopwords from nltk\nand then we are defining this mighty\npre-process text function that performs\na series of operations on the input\nreview\nfirst one is this conversion of review\ntext to lower case\nthen replacement of html tags\npunctuation marks and numbers with white\nspaces\nthese\nsingle characters and multi spaces\nare byproducts of operations conducted\nso far\nso these are also dropped and replaced\nwith white spaces\nand finally we drop stopwords as well\nand then the pre-processed review is\nreturned as output of our function\npre-process text\nin this next cell we are calling this\npre-process text function on our reviews\none at a time\nthen we may have a look at the process\nreview in this next cell\nin the next cell we are converting our\nlabels positive negative to ones and\nzeros and in the subsequent cell we are\nsplitting our data set into 80 20\ntraining test\nwith this train test split function\nin the next section we are preparing our\nembedding layer\nas a first step to this we are using\ntokenizer class from the keras\npre-processing text module\nto create a word to index dictionary\nin the word to index dictionary each\nword in the corpus is used as a key\nwhile a corresponding unique index is\nused as the value of the key\nthen we compute the vocabulary size\nwhich is 92 394.\nwhat it essentially means is that our\ncorpus has 92 394 unique words\nthen next up we are performing padding\nto set the length of all reviews to\nexactly 100 words\nyou can also try a different size\nthe list with size greater than 100 will\nbe truncated 200 for the list that have\nless than 100 we add zeros at the end of\nthe list until it reaches the maximum\nlength which is 100 again\npost this\nwe are using glow v embeddings to create\nour feature matrix for this we first\nload the glovy word embeddings and\ncreate a dictionary that will contain\nwords as keys and their corresponding\nembeddings list as values\nand finally we create an embedding\nmatrix where each row number will\ncorrespond to the index of the word in\nthe corpus the matrix will have hundred\ncolumns where each column will contain\nthe glowy word embedding for the words\nin our corpus\ncalling this shape function on the\nembedding matrix we may also check for\nits shape\nas expected we have exactly ninety two\nthousand three ninety four rows\nwhich is our vocabulary size and hundred\ncolumns which are the hundred glowy\ndimensions\nall right now we are at the model\ntraining step first up we are training a\nsimple neural network\nover here we create a sequential model\nthen we create our embedding layer the\nembedding layer will have an input of\n100\nwhich is the max length\nthe output vector dimension will also be\n100\nthe vocabulary size is 92394\nas we know by now\nand since we are not training our own\nembeddings\nand using the pre-trained glovy\nembedding we set trainable\nparameter to false\nand in the weights attribute we pass our\nown embedding matrix that we generated\npreviously\nthe embedding layer is then added to our\nmodel\nnext since we are directly connecting\nour embedding layer to a densely\nconnected layer we flatten the embedding\nlayer\nand then we add a dense layer with\nsigmoid activation function\npost that we\ncompile our model and then we start\ntraining\nonce the training gets complete then we\ncompute predictions on the test set\npost that we print model performance\nparameters\nand finally we plot modal performance\ncharts using this code snippet\nas you may see we get a test accuracy of\n74.98\nour training accuracy\nwas\n84.35 percent\nthis means that our model is over\nfeeding on the training set overfitting\noccurs when the model performs better on\nthe training set than the test set\nideally the performance difference\nbetween training and test sets should be\nminimum\nnext up we move to training a cnn model\ncnn is a type of network that is\nprimarily used for 2d\ndata classification\nsuch as images\na convolutional network tries to find\nspecific features in an image in the\nfirst layer\nin the next layers the initially\ndetected features are joined together to\nform bigger features in this way the\nwhole image is detected\nconvolutional neural networks have been\nfound to do a fair job with text data as\nwell\nthough text data is one dimensional we\ncan use\n1d cnns to extract features from our\ndata\ntalking about modal architecture\nhere we are creating a simple cnn with\none convolutional layer\nand one pooling layer\ncode up to the creation of the embedding\nlayer\nis the same\nas we have been using\nthen we compile the model and start\ntraining\nonce the training is over\nwe compute predictions on the test set\nprint model performance and plot the\ncharts\navi methi our accuracy here is a good\n85.79\nwhich is much better than the simple\nneural network results\nhowever our cnn model is still\noverfitting as there is a vast\ndifference between the training\nand test accuracy\nand finally we come to the lstm model\ntraining step\nrecurrent neural network is a type of\nneural network that is proven to work\nwell with sequence data\nand since text is actually a sequence of\nwords a recurrent neural network is an\nautomatic choice to solve text related\nproblems\nin this section we are using lstm which\nis a variant of rnn\ntalking about modal architecture here\nafter the same embedding layer that we\nhave been using\nwe are creating an lstm layer with 128\nneurons you can play around with the\nnumber of neurons by the way\nand rest of the code remains the same as\nis\nthen we compile\nthe model\nwe then we train it and once the\ntraining is over we print model\nperformance and plot charts the same way\nhere we get the highest test accuracy of\n86.43\nwhich is in very close proximity to the\ntraining accuracy of 87.12\neven the charts show that the difference\nbetween the accuracy values for training\nand test sets is much smaller compared\nto the other two models\nso with this we may conclude that for\nour problem rnn based lstm is the most\nsuited\napproach for training a neural network\nguys congratulations to you for making\nit to this point and training your own\nsentiment classification model using\nneural networks\nyou are the best\nnow to finish things up here we have\nthis last section left where we make\npredictions on the fresh imdb reviews\nfor which we kept a test file in our\ntoolkit\nover here we are first loading this uh\ntest reviews csv\nas you may see it looks like this here\nwe have the review text and the real\nimdb rating\ncaptured in the csv file\nthen as usual we pre-process our reviews\ntext followed by tokenization and\npadding\nand then we call our lstm model on these\nreviews for predictions\noutput of lstm model is a number between\n0 and 1 where 1 is positive sentiment\nwith this next cell we are representing\nmodal results vis-a-vis our\ntest\nfile data\nwe are multiplying our model results\nwith 10 so as to bring them on a scale\nof 0 to 10\nand as you may see here not only does\nour model predicts positive reviews as\npositive and negative reviews likewise\nit also gives an insanely accurate\nprediction on the imdb rating itself\nwhich is mind blowing really\ni would really encourage you to prepare\nyour own such test file and run\npredictions using your trained lstm\nmodel\njust to\nfurther appreciate how cool this model\nis that we just trained\npeople in case you have any questions on\nthis project we discussed today or you\nare preparing for a job interview or\njust need some career guidance in\ngeneral do set up a free one-on-one\nmentoring session with us by visiting\nour website skillca.com\nto better your chances of getting hired\nwith our expert advice you may also\nwrite to us over email and whatsapp\ni hope you liked our work\nplease subscribe to our channel now if\nyou haven't so far\nby the way we have done a couple more\nsentiment analysis\nprojects previously this first one is a\nbasic sentiment analysis project where\nwe use the traditional\nbag of words representation for feature\nextraction and naive bayes estimator for\nbinary classification if you are just\nbeginning your ml journey i'll strongly\nrecommend you to go through this project\nas well\nthe second project is built with\nscikit-learn pipeline using tf idf\ntokenizer for feature extraction and\nsupport vector machine for\nclassification\nsql and pipeline by the way is a low\ncode magical way of building able models\nso do check this out as well if you are\ncurious already\nand this third project is on how to\ndeploy a sentiment analysis project in a\nlive environment and performing uh live\nqueries\nthis for sure is the coolest among the\nthree projects we have here\nlink to all of these is in the\ndescription part below\ni would highly recommend you to go\nthrough these projects as well to\nfurther strengthen your understanding of\nsentiment classification problem and\nhaving a complete end-to-end visibility\nof solving a machine learning project we\nlaunch new machine learning projects\nevery week so to get\nregular updates on our ml projects do\nsubscribe to our channel and recommend\nus to your friends as well so they may\nalso benefit from our work\nthanks for all your support\nkeep learning\nbye\n",
  "words": [
    "hello",
    "data",
    "scientist",
    "welcome",
    "skill",
    "gate",
    "course",
    "build",
    "sentiment",
    "classification",
    "model",
    "using",
    "neural",
    "networks",
    "use",
    "popular",
    "imdb",
    "movie",
    "reviews",
    "data",
    "set",
    "train",
    "three",
    "separate",
    "neural",
    "networks",
    "namely",
    "simple",
    "neural",
    "net",
    "convolutional",
    "neural",
    "net",
    "cnn",
    "lastly",
    "long",
    "short",
    "term",
    "memory",
    "lstm",
    "neural",
    "net",
    "lstm",
    "networks",
    "actually",
    "considered",
    "quite",
    "suitable",
    "handling",
    "nlp",
    "problems",
    "end",
    "video",
    "understand",
    "give",
    "quick",
    "glimpse",
    "sentiment",
    "classification",
    "lstm",
    "model",
    "shall",
    "training",
    "part",
    "tutorial",
    "good",
    "predicts",
    "user",
    "sentiment",
    "positive",
    "negative",
    "also",
    "fabulous",
    "job",
    "predicting",
    "imdb",
    "rating",
    "corresponding",
    "reviews",
    "quite",
    "good",
    "accuracy",
    "see",
    "course",
    "next",
    "20",
    "minutes",
    "shall",
    "discuss",
    "mighty",
    "word",
    "embeddings",
    "powerful",
    "feature",
    "extraction",
    "technique",
    "intuition",
    "various",
    "neural",
    "network",
    "approaches",
    "concept",
    "overfitting",
    "happens",
    "model",
    "memorizes",
    "training",
    "data",
    "rather",
    "understanding",
    "general",
    "distribution",
    "course",
    "high",
    "level",
    "solution",
    "architecture",
    "training",
    "sentiment",
    "classification",
    "model",
    "without",
    "ado",
    "let",
    "get",
    "started",
    "introductions",
    "skillgate",
    "mission",
    "bring",
    "application",
    "based",
    "machine",
    "learning",
    "education",
    "launch",
    "new",
    "machine",
    "learning",
    "projects",
    "every",
    "week",
    "make",
    "sure",
    "subscribe",
    "channel",
    "also",
    "hit",
    "bell",
    "icon",
    "get",
    "notified",
    "new",
    "ml",
    "projects",
    "really",
    "seek",
    "support",
    "guys",
    "keeps",
    "us",
    "motivated",
    "helps",
    "us",
    "reaching",
    "learners",
    "like",
    "yes",
    "let",
    "talk",
    "machine",
    "learning",
    "people",
    "case",
    "need",
    "ml",
    "project",
    "related",
    "help",
    "want",
    "discuss",
    "crazy",
    "ideas",
    "go",
    "ahead",
    "book",
    "free",
    "online",
    "session",
    "us",
    "visiting",
    "website",
    "filling",
    "mentoring",
    "session",
    "registration",
    "form",
    "may",
    "write",
    "us",
    "email",
    "whatsapp",
    "well",
    "right",
    "coming",
    "topic",
    "discussion",
    "today",
    "sentiment",
    "classification",
    "using",
    "neural",
    "networks",
    "let",
    "start",
    "solution",
    "architecture",
    "brief",
    "basics",
    "high",
    "level",
    "solution",
    "architecture",
    "first",
    "step",
    "using",
    "imdb",
    "movie",
    "reviews",
    "data",
    "set",
    "50",
    "000",
    "reviews",
    "along",
    "positive",
    "negative",
    "sentiment",
    "labels",
    "data",
    "set",
    "looks",
    "like",
    "movie",
    "reviews",
    "first",
    "column",
    "mapped",
    "sentiment",
    "labels",
    "positive",
    "negative",
    "bar",
    "chart",
    "depicts",
    "distribution",
    "data",
    "across",
    "sentiment",
    "labels",
    "seemed",
    "fairly",
    "uniform",
    "okay",
    "back",
    "architecture",
    "second",
    "step",
    "certain",
    "operations",
    "review",
    "text",
    "one",
    "movie",
    "reviews",
    "randomly",
    "picked",
    "data",
    "set",
    "may",
    "see",
    "punctuation",
    "marks",
    "special",
    "characters",
    "uh",
    "html",
    "tags",
    "numbers",
    "add",
    "little",
    "value",
    "telling",
    "whether",
    "review",
    "positive",
    "negative",
    "shall",
    "drop",
    "part",
    "step",
    "give",
    "insights",
    "get",
    "coding",
    "part",
    "moving",
    "third",
    "step",
    "word",
    "embedding",
    "provision",
    "converting",
    "textual",
    "data",
    "numeric",
    "form",
    "using",
    "powerful",
    "feature",
    "extraction",
    "technique",
    "called",
    "word",
    "embedding",
    "use",
    "text",
    "data",
    "input",
    "neural",
    "network",
    "model",
    "need",
    "convert",
    "text",
    "numbers",
    "first",
    "popular",
    "techniques",
    "like",
    "one",
    "hotend",
    "coding",
    "fair",
    "job",
    "getting",
    "done",
    "however",
    "unlike",
    "machine",
    "learning",
    "models",
    "passing",
    "sparse",
    "vectors",
    "huge",
    "sizes",
    "greatly",
    "affect",
    "neural",
    "network",
    "models",
    "therefore",
    "need",
    "convert",
    "text",
    "small",
    "dense",
    "vectors",
    "word",
    "embedding",
    "comes",
    "case",
    "word",
    "embeddings",
    "every",
    "word",
    "represented",
    "dense",
    "vector",
    "beauty",
    "word",
    "embeddings",
    "way",
    "manages",
    "retain",
    "semantic",
    "relationship",
    "among",
    "words",
    "every",
    "token",
    "parameters",
    "explaining",
    "characteristics",
    "example",
    "four",
    "tokens",
    "man",
    "women",
    "king",
    "queen",
    "along",
    "seven",
    "dimensional",
    "vector",
    "representation",
    "depicting",
    "semantic",
    "relevance",
    "scale",
    "negative",
    "one",
    "one",
    "dimensions",
    "nothing",
    "mathematical",
    "way",
    "capture",
    "semantic",
    "relations",
    "among",
    "tokens",
    "like",
    "first",
    "dimension",
    "could",
    "living",
    "four",
    "tokens",
    "correspond",
    "human",
    "beings",
    "weights",
    "touching",
    "uh",
    "almost",
    "one",
    "tokens",
    "likewise",
    "remaining",
    "parameters",
    "well",
    "visualization",
    "word",
    "weddings",
    "like",
    "huge",
    "nexus",
    "words",
    "semantically",
    "similar",
    "words",
    "forming",
    "clusters",
    "built",
    "help",
    "complex",
    "algorithm",
    "establishes",
    "semantic",
    "relationships",
    "among",
    "words",
    "based",
    "usage",
    "millions",
    "sentences",
    "may",
    "see",
    "clusters",
    "words",
    "depict",
    "cities",
    "uh",
    "travel",
    "food",
    "etc",
    "course",
    "words",
    "clusters",
    "shall",
    "similar",
    "vectors",
    "case",
    "shall",
    "use",
    "globe",
    "e",
    "word",
    "embedding",
    "100",
    "dimensional",
    "vector",
    "text",
    "numeric",
    "transformation",
    "training",
    "data",
    "right",
    "coming",
    "back",
    "solution",
    "architecture",
    "next",
    "modal",
    "training",
    "step",
    "sits",
    "heart",
    "entire",
    "exercise",
    "shall",
    "train",
    "three",
    "separate",
    "neural",
    "network",
    "models",
    "namely",
    "simple",
    "neural",
    "net",
    "convolutional",
    "neural",
    "net",
    "lstm",
    "neural",
    "net",
    "essentially",
    "three",
    "popular",
    "different",
    "approaches",
    "let",
    "give",
    "quick",
    "refresher",
    "neural",
    "networks",
    "well",
    "jump",
    "coding",
    "part",
    "project",
    "execute",
    "solution",
    "architecture",
    "discussing",
    "one",
    "advances",
    "field",
    "machine",
    "learning",
    "neural",
    "networks",
    "would",
    "start",
    "appreciating",
    "human",
    "brain",
    "works",
    "intriguing",
    "humans",
    "may",
    "learn",
    "new",
    "skills",
    "within",
    "matter",
    "days",
    "months",
    "prohibited",
    "one",
    "day",
    "neural",
    "networks",
    "attempt",
    "mimic",
    "computation",
    "capabilities",
    "human",
    "brain",
    "reason",
    "quite",
    "suited",
    "recognize",
    "numerical",
    "patterns",
    "contained",
    "feature",
    "representations",
    "real",
    "world",
    "unstructured",
    "data",
    "like",
    "images",
    "sound",
    "text",
    "time",
    "series",
    "etc",
    "said",
    "feature",
    "representations",
    "hold",
    "key",
    "train",
    "powerful",
    "neural",
    "network",
    "image",
    "data",
    "input",
    "pixels",
    "form",
    "basis",
    "derivation",
    "features",
    "sound",
    "time",
    "amplitude",
    "frequency",
    "audio",
    "file",
    "specific",
    "use",
    "case",
    "text",
    "data",
    "word",
    "tokens",
    "form",
    "user",
    "review",
    "sentences",
    "forming",
    "basis",
    "derivation",
    "features",
    "future",
    "representation",
    "next",
    "important",
    "thing",
    "choose",
    "neural",
    "network",
    "approach",
    "may",
    "want",
    "apply",
    "specific",
    "problem",
    "statement",
    "prepared",
    "crisp",
    "comparison",
    "chart",
    "holistic",
    "idea",
    "different",
    "approaches",
    "may",
    "post",
    "video",
    "wish",
    "go",
    "detail",
    "sheet",
    "point",
    "may",
    "queries",
    "post",
    "comment",
    "section",
    "happy",
    "answering",
    "use",
    "moment",
    "also",
    "introduce",
    "problem",
    "overfitting",
    "remember",
    "child",
    "many",
    "us",
    "would",
    "cram",
    "memorize",
    "answers",
    "complex",
    "mathematical",
    "problems",
    "speed",
    "exam",
    "sheet",
    "neural",
    "networks",
    "also",
    "tendency",
    "happens",
    "networks",
    "learn",
    "general",
    "distribution",
    "training",
    "data",
    "points",
    "rather",
    "memorize",
    "expected",
    "outcomes",
    "like",
    "overfitting",
    "happens",
    "model",
    "returns",
    "high",
    "testing",
    "error",
    "low",
    "training",
    "error",
    "error",
    "values",
    "appropriately",
    "low",
    "close",
    "proximity",
    "well",
    "fitting",
    "model",
    "shall",
    "observe",
    "phenomena",
    "overfitting",
    "modal",
    "training",
    "project",
    "right",
    "time",
    "us",
    "jump",
    "coding",
    "part",
    "project",
    "train",
    "sentiment",
    "classification",
    "neural",
    "network",
    "models",
    "let",
    "get",
    "coding",
    "skill",
    "kit",
    "tool",
    "kit",
    "model",
    "related",
    "files",
    "including",
    "training",
    "data",
    "set",
    "glovy",
    "word",
    "embeddings",
    "csv",
    "fresh",
    "imdb",
    "reviews",
    "along",
    "movie",
    "name",
    "real",
    "imdb",
    "rating",
    "may",
    "see",
    "jupiter",
    "notebook",
    "code",
    "file",
    "lastly",
    "lstm",
    "model",
    "created",
    "previously",
    "let",
    "open",
    "jupyter",
    "code",
    "file",
    "quick",
    "code",
    "walkthrough",
    "fact",
    "already",
    "open",
    "let",
    "switch",
    "tabs",
    "collab",
    "segmented",
    "entire",
    "code",
    "sections",
    "see",
    "left",
    "side",
    "first",
    "one",
    "plan",
    "action",
    "pretty",
    "much",
    "self",
    "explanatory",
    "let",
    "move",
    "second",
    "section",
    "setting",
    "environment",
    "way",
    "ran",
    "entire",
    "code",
    "file",
    "prior",
    "recording",
    "video",
    "save",
    "time",
    "take",
    "outputs",
    "anyway",
    "first",
    "mounting",
    "google",
    "drive",
    "collab",
    "setting",
    "working",
    "directory",
    "project",
    "toolkit",
    "folder",
    "within",
    "google",
    "drive",
    "post",
    "importing",
    "essential",
    "libraries",
    "functions",
    "next",
    "code",
    "cell",
    "setting",
    "environment",
    "next",
    "loading",
    "data",
    "set",
    "first",
    "importing",
    "imdb",
    "movies",
    "dataset",
    "exploring",
    "data",
    "set",
    "first",
    "checking",
    "shape",
    "using",
    "head",
    "function",
    "actually",
    "see",
    "data",
    "set",
    "looks",
    "may",
    "see",
    "two",
    "columns",
    "review",
    "sentiment",
    "good",
    "data",
    "science",
    "practice",
    "also",
    "check",
    "null",
    "values",
    "see",
    "none",
    "also",
    "check",
    "data",
    "skewness",
    "across",
    "data",
    "labels",
    "see",
    "issues",
    "well",
    "next",
    "section",
    "data",
    "preprocessing",
    "first",
    "code",
    "cell",
    "printing",
    "review",
    "data",
    "set",
    "may",
    "see",
    "punctuation",
    "marks",
    "numbers",
    "html",
    "tags",
    "etc",
    "part",
    "preprocessing",
    "shall",
    "filter",
    "defining",
    "function",
    "called",
    "remove",
    "tags",
    "actually",
    "replace",
    "html",
    "tags",
    "given",
    "reviews",
    "white",
    "spaces",
    "next",
    "downloading",
    "stopwords",
    "nltk",
    "defining",
    "mighty",
    "text",
    "function",
    "performs",
    "series",
    "operations",
    "input",
    "review",
    "first",
    "one",
    "conversion",
    "review",
    "text",
    "lower",
    "case",
    "replacement",
    "html",
    "tags",
    "punctuation",
    "marks",
    "numbers",
    "white",
    "spaces",
    "single",
    "characters",
    "multi",
    "spaces",
    "byproducts",
    "operations",
    "conducted",
    "far",
    "also",
    "dropped",
    "replaced",
    "white",
    "spaces",
    "finally",
    "drop",
    "stopwords",
    "well",
    "review",
    "returned",
    "output",
    "function",
    "text",
    "next",
    "cell",
    "calling",
    "text",
    "function",
    "reviews",
    "one",
    "time",
    "may",
    "look",
    "process",
    "review",
    "next",
    "cell",
    "next",
    "cell",
    "converting",
    "labels",
    "positive",
    "negative",
    "ones",
    "zeros",
    "subsequent",
    "cell",
    "splitting",
    "data",
    "set",
    "80",
    "20",
    "training",
    "test",
    "train",
    "test",
    "split",
    "function",
    "next",
    "section",
    "preparing",
    "embedding",
    "layer",
    "first",
    "step",
    "using",
    "tokenizer",
    "class",
    "keras",
    "text",
    "module",
    "create",
    "word",
    "index",
    "dictionary",
    "word",
    "index",
    "dictionary",
    "word",
    "corpus",
    "used",
    "key",
    "corresponding",
    "unique",
    "index",
    "used",
    "value",
    "key",
    "compute",
    "vocabulary",
    "size",
    "92",
    "essentially",
    "means",
    "corpus",
    "92",
    "394",
    "unique",
    "words",
    "next",
    "performing",
    "padding",
    "set",
    "length",
    "reviews",
    "exactly",
    "100",
    "words",
    "also",
    "try",
    "different",
    "size",
    "list",
    "size",
    "greater",
    "100",
    "truncated",
    "200",
    "list",
    "less",
    "100",
    "add",
    "zeros",
    "end",
    "list",
    "reaches",
    "maximum",
    "length",
    "100",
    "post",
    "using",
    "glow",
    "v",
    "embeddings",
    "create",
    "feature",
    "matrix",
    "first",
    "load",
    "glovy",
    "word",
    "embeddings",
    "create",
    "dictionary",
    "contain",
    "words",
    "keys",
    "corresponding",
    "embeddings",
    "list",
    "values",
    "finally",
    "create",
    "embedding",
    "matrix",
    "row",
    "number",
    "correspond",
    "index",
    "word",
    "corpus",
    "matrix",
    "hundred",
    "columns",
    "column",
    "contain",
    "glowy",
    "word",
    "embedding",
    "words",
    "corpus",
    "calling",
    "shape",
    "function",
    "embedding",
    "matrix",
    "may",
    "also",
    "check",
    "shape",
    "expected",
    "exactly",
    "ninety",
    "two",
    "thousand",
    "three",
    "ninety",
    "four",
    "rows",
    "vocabulary",
    "size",
    "hundred",
    "columns",
    "hundred",
    "glowy",
    "dimensions",
    "right",
    "model",
    "training",
    "step",
    "first",
    "training",
    "simple",
    "neural",
    "network",
    "create",
    "sequential",
    "model",
    "create",
    "embedding",
    "layer",
    "embedding",
    "layer",
    "input",
    "100",
    "max",
    "length",
    "output",
    "vector",
    "dimension",
    "also",
    "100",
    "vocabulary",
    "size",
    "92394",
    "know",
    "since",
    "training",
    "embeddings",
    "using",
    "glovy",
    "embedding",
    "set",
    "trainable",
    "parameter",
    "false",
    "weights",
    "attribute",
    "pass",
    "embedding",
    "matrix",
    "generated",
    "previously",
    "embedding",
    "layer",
    "added",
    "model",
    "next",
    "since",
    "directly",
    "connecting",
    "embedding",
    "layer",
    "densely",
    "connected",
    "layer",
    "flatten",
    "embedding",
    "layer",
    "add",
    "dense",
    "layer",
    "sigmoid",
    "activation",
    "function",
    "post",
    "compile",
    "model",
    "start",
    "training",
    "training",
    "gets",
    "complete",
    "compute",
    "predictions",
    "test",
    "set",
    "post",
    "print",
    "model",
    "performance",
    "parameters",
    "finally",
    "plot",
    "modal",
    "performance",
    "charts",
    "using",
    "code",
    "snippet",
    "may",
    "see",
    "get",
    "test",
    "accuracy",
    "training",
    "accuracy",
    "percent",
    "means",
    "model",
    "feeding",
    "training",
    "set",
    "overfitting",
    "occurs",
    "model",
    "performs",
    "better",
    "training",
    "set",
    "test",
    "set",
    "ideally",
    "performance",
    "difference",
    "training",
    "test",
    "sets",
    "minimum",
    "next",
    "move",
    "training",
    "cnn",
    "model",
    "cnn",
    "type",
    "network",
    "primarily",
    "used",
    "2d",
    "data",
    "classification",
    "images",
    "convolutional",
    "network",
    "tries",
    "find",
    "specific",
    "features",
    "image",
    "first",
    "layer",
    "next",
    "layers",
    "initially",
    "detected",
    "features",
    "joined",
    "together",
    "form",
    "bigger",
    "features",
    "way",
    "whole",
    "image",
    "detected",
    "convolutional",
    "neural",
    "networks",
    "found",
    "fair",
    "job",
    "text",
    "data",
    "well",
    "though",
    "text",
    "data",
    "one",
    "dimensional",
    "use",
    "1d",
    "cnns",
    "extract",
    "features",
    "data",
    "talking",
    "modal",
    "architecture",
    "creating",
    "simple",
    "cnn",
    "one",
    "convolutional",
    "layer",
    "one",
    "pooling",
    "layer",
    "code",
    "creation",
    "embedding",
    "layer",
    "using",
    "compile",
    "model",
    "start",
    "training",
    "training",
    "compute",
    "predictions",
    "test",
    "set",
    "print",
    "model",
    "performance",
    "plot",
    "charts",
    "avi",
    "methi",
    "accuracy",
    "good",
    "much",
    "better",
    "simple",
    "neural",
    "network",
    "results",
    "however",
    "cnn",
    "model",
    "still",
    "overfitting",
    "vast",
    "difference",
    "training",
    "test",
    "accuracy",
    "finally",
    "come",
    "lstm",
    "model",
    "training",
    "step",
    "recurrent",
    "neural",
    "network",
    "type",
    "neural",
    "network",
    "proven",
    "work",
    "well",
    "sequence",
    "data",
    "since",
    "text",
    "actually",
    "sequence",
    "words",
    "recurrent",
    "neural",
    "network",
    "automatic",
    "choice",
    "solve",
    "text",
    "related",
    "problems",
    "section",
    "using",
    "lstm",
    "variant",
    "rnn",
    "talking",
    "modal",
    "architecture",
    "embedding",
    "layer",
    "using",
    "creating",
    "lstm",
    "layer",
    "128",
    "neurons",
    "play",
    "around",
    "number",
    "neurons",
    "way",
    "rest",
    "code",
    "remains",
    "compile",
    "model",
    "train",
    "training",
    "print",
    "model",
    "performance",
    "plot",
    "charts",
    "way",
    "get",
    "highest",
    "test",
    "accuracy",
    "close",
    "proximity",
    "training",
    "accuracy",
    "even",
    "charts",
    "show",
    "difference",
    "accuracy",
    "values",
    "training",
    "test",
    "sets",
    "much",
    "smaller",
    "compared",
    "two",
    "models",
    "may",
    "conclude",
    "problem",
    "rnn",
    "based",
    "lstm",
    "suited",
    "approach",
    "training",
    "neural",
    "network",
    "guys",
    "congratulations",
    "making",
    "point",
    "training",
    "sentiment",
    "classification",
    "model",
    "using",
    "neural",
    "networks",
    "best",
    "finish",
    "things",
    "last",
    "section",
    "left",
    "make",
    "predictions",
    "fresh",
    "imdb",
    "reviews",
    "kept",
    "test",
    "file",
    "toolkit",
    "first",
    "loading",
    "uh",
    "test",
    "reviews",
    "csv",
    "may",
    "see",
    "looks",
    "like",
    "review",
    "text",
    "real",
    "imdb",
    "rating",
    "captured",
    "csv",
    "file",
    "usual",
    "reviews",
    "text",
    "followed",
    "tokenization",
    "padding",
    "call",
    "lstm",
    "model",
    "reviews",
    "predictions",
    "output",
    "lstm",
    "model",
    "number",
    "0",
    "1",
    "1",
    "positive",
    "sentiment",
    "next",
    "cell",
    "representing",
    "modal",
    "results",
    "test",
    "file",
    "data",
    "multiplying",
    "model",
    "results",
    "10",
    "bring",
    "scale",
    "0",
    "10",
    "may",
    "see",
    "model",
    "predicts",
    "positive",
    "reviews",
    "positive",
    "negative",
    "reviews",
    "likewise",
    "also",
    "gives",
    "insanely",
    "accurate",
    "prediction",
    "imdb",
    "rating",
    "mind",
    "blowing",
    "really",
    "would",
    "really",
    "encourage",
    "prepare",
    "test",
    "file",
    "run",
    "predictions",
    "using",
    "trained",
    "lstm",
    "model",
    "appreciate",
    "cool",
    "model",
    "trained",
    "people",
    "case",
    "questions",
    "project",
    "discussed",
    "today",
    "preparing",
    "job",
    "interview",
    "need",
    "career",
    "guidance",
    "general",
    "set",
    "free",
    "mentoring",
    "session",
    "us",
    "visiting",
    "website",
    "better",
    "chances",
    "getting",
    "hired",
    "expert",
    "advice",
    "may",
    "also",
    "write",
    "us",
    "email",
    "whatsapp",
    "hope",
    "liked",
    "work",
    "please",
    "subscribe",
    "channel",
    "far",
    "way",
    "done",
    "couple",
    "sentiment",
    "analysis",
    "projects",
    "previously",
    "first",
    "one",
    "basic",
    "sentiment",
    "analysis",
    "project",
    "use",
    "traditional",
    "bag",
    "words",
    "representation",
    "feature",
    "extraction",
    "naive",
    "bayes",
    "estimator",
    "binary",
    "classification",
    "beginning",
    "ml",
    "journey",
    "strongly",
    "recommend",
    "go",
    "project",
    "well",
    "second",
    "project",
    "built",
    "pipeline",
    "using",
    "tf",
    "idf",
    "tokenizer",
    "feature",
    "extraction",
    "support",
    "vector",
    "machine",
    "classification",
    "sql",
    "pipeline",
    "way",
    "low",
    "code",
    "magical",
    "way",
    "building",
    "able",
    "models",
    "check",
    "well",
    "curious",
    "already",
    "third",
    "project",
    "deploy",
    "sentiment",
    "analysis",
    "project",
    "live",
    "environment",
    "performing",
    "uh",
    "live",
    "queries",
    "sure",
    "coolest",
    "among",
    "three",
    "projects",
    "link",
    "description",
    "part",
    "would",
    "highly",
    "recommend",
    "go",
    "projects",
    "well",
    "strengthen",
    "understanding",
    "sentiment",
    "classification",
    "problem",
    "complete",
    "visibility",
    "solving",
    "machine",
    "learning",
    "project",
    "launch",
    "new",
    "machine",
    "learning",
    "projects",
    "every",
    "week",
    "get",
    "regular",
    "updates",
    "ml",
    "projects",
    "subscribe",
    "channel",
    "recommend",
    "us",
    "friends",
    "well",
    "may",
    "also",
    "benefit",
    "work",
    "thanks",
    "support",
    "keep",
    "learning",
    "bye"
  ],
  "keywords": [
    "data",
    "course",
    "sentiment",
    "classification",
    "model",
    "using",
    "neural",
    "networks",
    "use",
    "popular",
    "imdb",
    "movie",
    "reviews",
    "set",
    "train",
    "three",
    "simple",
    "net",
    "convolutional",
    "cnn",
    "lstm",
    "actually",
    "quite",
    "problems",
    "video",
    "give",
    "quick",
    "shall",
    "training",
    "part",
    "good",
    "positive",
    "negative",
    "also",
    "job",
    "rating",
    "corresponding",
    "accuracy",
    "see",
    "next",
    "word",
    "embeddings",
    "powerful",
    "feature",
    "extraction",
    "network",
    "approaches",
    "overfitting",
    "happens",
    "general",
    "distribution",
    "high",
    "solution",
    "architecture",
    "let",
    "get",
    "based",
    "machine",
    "learning",
    "new",
    "projects",
    "every",
    "subscribe",
    "channel",
    "ml",
    "really",
    "support",
    "us",
    "like",
    "case",
    "need",
    "project",
    "related",
    "go",
    "session",
    "form",
    "may",
    "well",
    "right",
    "start",
    "first",
    "step",
    "along",
    "labels",
    "looks",
    "second",
    "operations",
    "review",
    "text",
    "one",
    "punctuation",
    "marks",
    "uh",
    "html",
    "tags",
    "numbers",
    "add",
    "coding",
    "embedding",
    "input",
    "models",
    "vectors",
    "dense",
    "vector",
    "way",
    "semantic",
    "among",
    "words",
    "parameters",
    "four",
    "tokens",
    "dimensional",
    "representation",
    "human",
    "clusters",
    "etc",
    "100",
    "modal",
    "entire",
    "different",
    "would",
    "real",
    "time",
    "key",
    "image",
    "features",
    "file",
    "specific",
    "problem",
    "post",
    "section",
    "error",
    "low",
    "values",
    "glovy",
    "csv",
    "code",
    "previously",
    "much",
    "setting",
    "environment",
    "cell",
    "shape",
    "function",
    "two",
    "columns",
    "check",
    "white",
    "spaces",
    "finally",
    "output",
    "test",
    "layer",
    "create",
    "index",
    "dictionary",
    "corpus",
    "used",
    "compute",
    "vocabulary",
    "size",
    "length",
    "list",
    "matrix",
    "number",
    "hundred",
    "since",
    "compile",
    "predictions",
    "print",
    "performance",
    "plot",
    "charts",
    "better",
    "difference",
    "results",
    "work",
    "analysis",
    "recommend"
  ]
}