{
  "text": "The initials GPT stand for Generative Pretrained Transformer.\nSo that first word is straightforward enough, these are bots that generate new text.\nPretrained refers to how the model went through a process of learning \nfrom a massive amount of data, and the prefix insinuates that there's \nmore room to fine-tune it on specific tasks with additional training.\nBut the last word, that's the real key piece.\nA transformer is a specific kind of neural network, a machine learning model, \nand it's the core invention underlying the current boom in AI.\nWhat I want to do with this video and the following chapters is go through \na visually-driven explanation for what actually happens inside a transformer.\nWe're going to follow the data that flows through it and go step by step.\nThere are many different kinds of models that you can build using transformers.\nSome models take in audio and produce a transcript.\nThis sentence comes from a model going the other way around, \nproducing synthetic speech just from text.\nAll those tools that took the world by storm in 2022 like Dolly and Midjourney \nthat take in a text description and produce an image are based on transformers.\nEven if I can't quite get it to understand what a pie creature is supposed to be, \nI'm still blown away that this kind of thing is even remotely possible.\nAnd the original transformer introduced in 2017 by Google was invented for \nthe specific use case of translating text from one language into another.\nBut the variant that you and I will focus on, which is the type that \nunderlies tools like ChatGPT, will be a model that's trained to take in a piece of text, \nmaybe even with some surrounding images or sound accompanying it, \nand produce a prediction for what comes next in the passage.\nThat prediction takes the form of a probability distribution \nover many different chunks of text that might follow.\nAt first glance, you might think that predicting the next \nword feels like a very different goal from generating new text.\nBut once you have a prediction model like this, \na simple thing you generate a longer piece of text is to give it an initial \nsnippet to work with, have it take a random sample from the distribution \nit just generated, append that sample to the text, \nand then run the whole process again to make a new prediction based on all the new text, \nincluding what it just added.\nI don't know about you, but it really doesn't feel like this should actually work.\nIn this animation, for example, I'm running GPT-2 on my laptop and having it repeatedly \npredict and sample the next chunk of text to generate a story based on the seed text.\nThe story just doesn't really make that much sense.\nBut if I swap it out for API calls to GPT-3 instead, which is the same basic model, \njust much bigger, suddenly almost magically we do get a sensible story, \none that even seems to infer that a pi creature would live in a land of math and \ncomputation.\nThis process here of repeated prediction and sampling is essentially \nwhat's happening when you interact with ChatGPT or any of these other \nlarge language models and you see them producing one word at a time.\nIn fact, one feature that I would very much enjoy is the ability to \nsee the underlying distribution for each new word that it chooses.\nLet's kick things off with a very high level preview \nof how data flows through a transformer.\nWe will spend much more time motivating and interpreting and expanding \non the details of each step, but in broad strokes, \nwhen one of these chatbots generates a given word, here's what's going on under the hood.\nFirst, the input is broken up into a bunch of little pieces.\nThese pieces are called tokens, and in the case of text these tend to be \nwords or little pieces of words or other common character combinations.\nIf images or sound are involved, then tokens could be \nlittle patches of that image or little chunks of that sound.\nEach one of these tokens is then associated with a vector, \nmeaning some list of numbers, which is meant to somehow encode the meaning of that piece.\nIf you think of these vectors as giving coordinates in some very high dimensional space, \nwords with similar meanings tend to land on vectors that are \nclose to each other in that space.\nThis sequence of vectors then passes through an operation that's \nknown as an attention block, and this allows the vectors to talk to \neach other and pass information back and forth to update their values.\nFor example, the meaning of the word model in the phrase a machine \nlearning model is different from its meaning in the phrase a fashion model.\nThe attention block is what's responsible for figuring out which \nwords in context are relevant to updating the meanings of which other words, \nand how exactly those meanings should be updated.\nAnd again, whenever I use the word meaning, this is \nsomehow entirely encoded in the entries of those vectors.\nAfter that, these vectors pass through a different kind of operation, \nand depending on the source that you're reading this will be referred \nto as a multi-layer perceptron or maybe a feed-forward layer.\nAnd here the vectors don't talk to each other, \nthey all go through the same operation in parallel.\nAnd while this block is a little bit harder to interpret, \nlater on we'll talk about how the step is a little bit like asking a long list \nof questions about each vector, and then updating them based on the answers \nto those questions.\nAll of the operations in both of these blocks look like a \ngiant pile of matrix multiplications, and our primary job is \ngoing to be to understand how to read the underlying matrices.\nI'm glossing over some details about some normalization steps that happen in between, \nbut this is after all a high-level preview.\nAfter that, the process essentially repeats, you go back and forth \nbetween attention blocks and multi-layer perceptron blocks, \nuntil at the very end the hope is that all of the essential meaning \nof the passage has somehow been baked into the very last vector in the sequence.\nWe then perform a certain operation on that last vector that produces a probability \ndistribution over all possible tokens, all possible little chunks of text that might come \nnext.\nAnd like I said, once you have a tool that predicts what comes next \ngiven a snippet of text, you can feed it a little bit of seed text and \nhave it repeatedly play this game of predicting what comes next, \nsampling from the distribution, appending it, and then repeating over and over.\nSome of you in the know may remember how long before ChatGPT came into the scene, \nthis is what early demos of GPT-3 looked like, \nyou would have it autocomplete stories and essays based on an initial snippet.\nTo make a tool like this into a chatbot, the easiest starting point is to have \na little bit of text that establishes the setting of a user interacting with a \nhelpful AI assistant, what you would call the system prompt, \nand then you would use the user's initial question or prompt as the first bit of \ndialogue, and then you have it start predicting what such a helpful AI assistant \nwould say in response.\nThere is more to say about an step of training that's required to make this work well, \nbut at a high level this is the idea.\nIn this chapter, you and I are going to expand on the details of what happens at the very \nbeginning of the network, at the very end of the network, \nand I also want to spend a lot of time reviewing some important bits of background \nknowledge, things that would have been second nature to any machine learning engineer by \nthe time transformers came around.\nIf you're comfortable with that background knowledge and a little impatient, \nyou could feel free to skip to the next chapter, \nwhich is going to focus on the attention blocks, \ngenerally considered the heart of the transformer.\nAfter that I want to talk more about these multi-layer perceptron blocks, \nhow training works, and a number of other details that will have been skipped up to \nthat point.\nFor broader context, these videos are additions to a mini-series about deep learning, \nand it's okay if you haven't watched the previous ones, \nI think you can do it out of order, but before diving into transformers specifically, \nI do think it's worth making sure that we're on the same page about the basic premise \nand structure of deep learning.\nAt the risk of stating the obvious, this is one approach to machine learning, \nwhich describes any model where you're using data to somehow determine how a model \nbehaves.\nWhat I mean by that is, let's say you want a function that takes in \nan image and it produces a label describing it, \nor our example of predicting the next word given a passage of text, \nor any other task that seems to require some element of intuition and pattern recognition.\nWe almost take this for granted these days, but the idea with machine learning is \nthat rather than trying to explicitly define a procedure for how to do that task in code, \nwhich is what people would have done in the earliest days of AI, \ninstead you set up a very flexible structure with tunable parameters, \nlike a bunch of knobs and dials, and then somehow you use many examples of what the \noutput should look like for a given input to tweak and tune the values of those \nparameters to mimic this behavior.\nFor example, maybe the simplest form of machine learning is linear regression, \nwhere your inputs and outputs are each single numbers, \nsomething like the square footage of a house and its price, \nand what you want is to find a line of best fit through this data, you know, \nto predict future house prices.\nThat line is described by two continuous parameters, \nsay the slope and the y-intercept, and the goal of linear \nregression is to determine those parameters to closely match the data.\nNeedless to say, deep learning models get much more complicated.\nGPT-3, for example, has not two, but 175 billion parameters.\nBut here's the thing, it's not a given that you can create some giant \nmodel with a huge number of parameters without it either grossly \noverfitting the training data or being completely intractable to train.\nDeep learning describes a class of models that in the \nlast couple decades have proven to scale remarkably well.\nWhat unifies them is the same training algorithm, called backpropagation, \nand the context I want you to have as we go in is that in order for this training \nalgorithm to work well at scale, these models have to follow a certain specific format.\nIf you know this format going in, it helps to explain many of the choices for how \na transformer processes language, which otherwise run the risk of feeling arbitrary.\nFirst, whatever model you're making, the input \nhas to be formatted as an array of real numbers.\nThis could mean a list of numbers, it could be a two-dimensional array, \nor very often you deal with higher dimensional arrays, \nwhere the general term used is tensor.\nYou often think of that input data as being progressively transformed into many \ndistinct layers, where again, each layer is always structured as some kind of \narray of real numbers, until you get to a final layer which you consider the output.\nFor example, the final layer in our text processing model is a list of \nnumbers representing the probability distribution for all possible next tokens.\nIn deep learning, these model parameters are almost always referred to as weights, \nand this is because a key feature of these models is that the only way these \nparameters interact with the data being processed is through weighted sums.\nYou also sprinkle some non-linear functions throughout, \nbut they won't depend on parameters.\nTypically though, instead of seeing the weighted sums all naked \nand written out explicitly like this, you'll instead find them \npackaged together as various components in a matrix vector product.\nIt amounts to saying the same thing, if you think back to how matrix vector \nmultiplication works, each component in the output looks like a weighted sum.\nIt's just often conceptually cleaner for you and me to think \nabout matrices that are filled with tunable parameters that \ntransform vectors that are drawn from the data being processed.\nFor example, those 175 billion weights in GPT-3 are \norganized into just under 28,000 distinct matrices.\nThose matrices in turn fall into eight different categories, \nand what you and I are going to do is step through each one of those categories to \nunderstand what that type does.\nAs we go through, I think it's kind of fun to reference the specific \nnumbers from GPT-3 to count up exactly where those 175 billion come from.\nEven if nowadays there are bigger and better models, \nthis one has a certain charm as the large-language model to really capture the world's \nattention outside of ML communities.\nAlso, practically speaking, companies tend to keep much tighter \nlips around the specific numbers for more modern networks.\nI just want to set the scene going in, that as you peek under the \nhood to see what happens inside a tool like ChatGPT, \nalmost all of the actual computation looks like matrix vector multiplication.\nThere's a little bit of a risk getting lost in the sea of billions of numbers, \nbut you should draw a very sharp distinction in your mind between \nthe weights of the model, which I'll always color in blue or red, \nand the data being processed, which I'll always color in gray.\nThe weights are the actual brains, they are the things learned during training, \nand they determine how it behaves.\nThe data being processed simply encodes whatever specific input is \nfed into the model for a given run, like an example snippet of text.\nWith all of that as foundation, let's dig into the first step of this text processing \nexample, which is to break up the input into little chunks and turn those chunks into \nvectors.\nI mentioned how those chunks are called tokens, \nwhich might be pieces of words or punctuation, \nbut every now and then in this chapter and especially in the next one, \nI'd like to just pretend that it's broken more cleanly into words.\nBecause we humans think in words, this will just make it much \neasier to reference little examples and clarify each step.\nThe model has a predefined vocabulary, some list of all possible words, \nsay 50,000 of them, and the first matrix that we'll encounter, \nknown as the embedding matrix, has a single column for each one of these words.\nThese columns are what determines what vector each word turns into in that first step.\nWe label it We, and like all the matrices we see, \nits values begin random, but they're going to be learned based on data.\nTurning words into vectors was common practice in machine learning long before \ntransformers, but it's a little weird if you've never seen it before, \nand it sets the foundation for everything that follows, \nso let's take a moment to get familiar with it.\nWe often call this embedding a word, which invites you to think of \nthese vectors very geometrically as points in some high dimensional space.\nVisualizing a list of three numbers as coordinates for points in 3D space \nwould be no problem, but word embeddings tend to be much much higher dimensional.\nIn GPT-3 they have 12,288 dimensions, and as you'll see, \nit matters to work in a space that has a lot of distinct directions.\nIn the same way that you could take a two-dimensional slice through a 3D space \nand project all the points onto that slice, for the sake of animating word \nembeddings that a simple model is giving me, I'm going to do an analogous \nthing by choosing a three-dimensional slice through this very high dimensional space, \nand projecting the word vectors down onto that and displaying the results.\nThe big idea here is that as a model tweaks and tunes its weights to determine \nhow exactly words get embedded as vectors during training, \nit tends to settle on a set of embeddings where directions in the space have a \nkind of semantic meaning.\nFor the simple word-to-vector model I'm running here, \nif I run a search for all the words whose embeddings are closest to that of tower, \nyou'll notice how they all seem to give very similar tower-ish vibes.\nAnd if you want to pull up some Python and play along at home, \nthis is the specific model that I'm using to make the animations.\nIt's not a transformer, but it's enough to illustrate the \nidea that directions in the space can carry semantic meaning.\nA very classic example of this is how if you take the difference between the vectors \nfor woman and man, something you would visualize as a little vector connecting the tip \nof one to the tip of the other, it's very similar to the difference between king and \nqueen.\nSo let's say you didn't know the word for a female monarch, \nyou could find it by taking king, adding this woman-man direction, \nand searching for the embeddings closest to that point.\nAt least, kind of.\nDespite this being a classic example for the model I'm playing with, \nthe true embedding of queen is actually a little farther off than this would suggest, \npresumably because the way queen is used in training data is not merely a feminine \nversion of king.\nWhen I played around, family relations seemed to illustrate the idea much better.\nThe point is, it looks like during training the model found it advantageous to \nchoose embeddings such that one direction in this space encodes gender information.\nAnother example is that if you take the embedding of Italy, \nand you subtract the embedding of Germany, and add that to the embedding of Hitler, \nyou get something very close to the embedding of Mussolini.\nIt's as if the model learned to associate some directions with Italian-ness, \nand others with WWII axis leaders.\nMaybe my favorite example in this vein is how in some models, \nif you take the difference between Germany and Japan, and add it to sushi, \nyou end up very close to bratwurst.\nAlso in playing this game of finding nearest neighbors, \nI was pleased to see how close Kat was to both beast and monster.\nOne bit of mathematical intuition that's helpful to have in mind, \nespecially for the next chapter, is how the dot product of two \nvectors can be thought of as a way to measure how well they align.\nComputationally, dot products involve multiplying all the \ncorresponding components and then adding the results, which is good, \nsince so much of our computation has to look like weighted sums.\nGeometrically, the dot product is positive when vectors point in similar directions, \nit's zero if they're perpendicular, and it's negative whenever \nthey point in opposite directions.\nFor example, let's say you were playing with this model, \nand you hypothesize that the embedding of cats minus cat might represent a sort of \nplurality direction in this space.\nTo test this, I'm going to take this vector and compute its dot \nproduct against the embeddings of certain singular nouns, \nand compare it to the dot products with the corresponding plural nouns.\nIf you play around with this, you'll notice that the plural ones \ndo indeed seem to consistently give higher values than the singular ones, \nindicating that they align more with this direction.\nIt's also fun how if you take this dot product with the embeddings of the words 1, \n2, 3, and so on, they give increasing values, so it's as if we can \nquantitatively measure how plural the model finds a given word.\nAgain, the specifics for how words get embedded is learned using data.\nThis embedding matrix, whose columns tell us what happens to each word, \nis the first pile of weights in our model.\nUsing the GPT-3 numbers, the vocabulary size specifically is 50,257, \nand again, technically this consists not of words per se, but of tokens.\nThe embedding dimension is 12,288, and multiplying \nthose tells us this consists of about 617 million weights.\nLet's go ahead and add this to a running tally, \nremembering that by the end we should count up to 175 billion.\nIn the case of transformers, you really want to think of the vectors \nin this embedding space as not merely representing individual words.\nFor one thing, they also encode information about the position of that word, \nwhich we'll talk about later, but more importantly, \nyou should think of them as having the capacity to soak in context.\nA vector that started its life as the embedding of the word king, for example, \nmight progressively get tugged and pulled by various blocks in this network, \nso that by the end it points in a much more specific and nuanced direction that \nsomehow encodes that it was a king who lived in Scotland, \nand who had achieved his post after murdering the previous king, \nand who's being described in Shakespearean language.\nThink about your own understanding of a given word.\nThe meaning of that word is clearly informed by the surroundings, \nand sometimes this includes context from a long distance away, \nso in putting together a model that has the ability to predict what word comes next, \nthe goal is to somehow empower it to incorporate context efficiently.\nTo be clear, in that very first step, when you create the array of \nvectors based on the input text, each one of those is simply plucked \nout of the embedding matrix, so initially each one can only encode \nthe meaning of a single word without any input from its surroundings.\nBut you should think of the primary goal of this network that it flows through \nas being to enable each one of those vectors to soak up a meaning that's much \nmore rich and specific than what mere individual words could represent.\nThe network can only process a fixed number of vectors at a time, \nknown as its context size.\nFor GPT-3 it was trained with a context size of 2048, \nso the data flowing through the network always looks like this array of 2048 columns, \neach of which has 12,000 dimensions.\nThis context size limits how much text the transformer can \nincorporate when it's making a prediction of the next word.\nThis is why long conversations with certain chatbots, \nlike the early versions of ChatGPT, often gave the feeling of \nthe bot kind of losing the thread of conversation as you continued too long.\nWe'll go into the details of attention in due time, \nbut skipping ahead I want to talk for a minute about what happens at the very end.\nRemember, the desired output is a probability \ndistribution over all tokens that might come next.\nFor example, if the very last word is Professor, \nand the context includes words like Harry Potter, \nand immediately preceding we see least favorite teacher, \nand also if you give me some leeway by letting me pretend that tokens simply \nlook like full words, then a well-trained network that had built up knowledge \nof Harry Potter would presumably assign a high number to the word Snape.\nThis involves two different steps.\nThe first one is to use another matrix that maps the very last vector in \nthat context to a list of 50,000 values, one for each token in the vocabulary.\nThen there's a function that normalizes this into a probability distribution, \nit's called Softmax and we'll talk more about it in just a second, \nbut before that it might seem a little bit weird to only use this last embedding \nto make a prediction, when after all in that last step there are thousands of \nother vectors in the layer just sitting there with their own context-rich meanings.\nThis has to do with the fact that in the training process it turns out to be \nmuch more efficient if you use each one of those vectors in the final layer \nto simultaneously make a prediction for what would come immediately after it.\nThere's a lot more to be said about training later on, \nbut I just want to call that out right now.\nThis matrix is called the Unembedding matrix and we give it the label WU.\nAgain, like all the weight matrices we see, its entries begin at random, \nbut they are learned during the training process.\nKeeping score on our total parameter count, this Unembedding \nmatrix has one row for each word in the vocabulary, \nand each row has the same number of elements as the embedding dimension.\nIt's very similar to the embedding matrix, just with the order swapped, \nso it adds another 617 million parameters to the network, \nmeaning our count so far is a little over a billion, \na small but not wholly insignificant fraction of the 175 billion \nwe'll end up with in total.\nAs the last mini-lesson for this chapter, I want to talk more about this softmax \nfunction, since it makes another appearance for us once we dive into the attention blocks.\nThe idea is that if you want a sequence of numbers to act as a probability distribution, \nsay a distribution over all possible next words, \nthen each value has to be between 0 and 1, and you also need all of them to add up to 1.\nHowever, if you're playing the learning game where everything you do looks like \nmatrix-vector multiplication, the outputs you get by default don't abide by this at all.\nThe values are often negative, or much bigger than 1, \nand they almost certainly don't add up to 1.\nSoftmax is the standard way to turn an arbitrary list of numbers \ninto a valid distribution in such a way that the largest values end up closest to 1, \nand the smaller values end up very close to 0.\nThat's all you really need to know.\nBut if you're curious, the way it works is to first raise e to the power \nof each of the numbers, which means you now have a list of positive values, \nand then you can take the sum of all those positive values and divide each \nterm by that sum, which normalizes it into a list that adds up to 1.\nYou'll notice that if one of the numbers in the input is meaningfully bigger than \nthe rest, then in the output the corresponding term dominates the distribution, \nso if you were sampling from it you'd almost certainly just be picking the maximizing \ninput.\nBut it's softer than just picking the max in the sense that when other \nvalues are similarly large, they also get meaningful weight in the distribution, \nand everything changes continuously as you continuously vary the inputs.\nIn some situations, like when ChatGPT is using this distribution to create a next word, \nthere's room for a little bit of extra fun by adding a little extra spice into this \nfunction, with a constant t thrown into the denominator of those exponents.\nWe call it the temperature, since it vaguely resembles the role of temperature in \ncertain thermodynamics equations, and the effect is that when t is larger, \nyou give more weight to the lower values, meaning the distribution is a little bit \nmore uniform, and if t is smaller, then the bigger values will dominate more \naggressively, where in the extreme, setting t equal to zero means all of the weight \ngoes to maximum value.\nFor example, I'll have GPT-3 generate a story with the seed text, \nonce upon a time there was A, but I'll use different temperatures in each case.\nTemperature zero means that it always goes with the most predictable word, \nand what you get ends up being a trite derivative of Goldilocks.\nA higher temperature gives it a chance to choose less likely words, \nbut it comes with a risk.\nIn this case, the story starts out more originally, \nabout a young web artist from South Korea, but it quickly degenerates into nonsense.\nTechnically speaking, the API doesn't actually let you pick a temperature bigger than 2.\nThere's no mathematical reason for this, it's just an arbitrary constraint imposed \nto keep their tool from being seen generating things that are too nonsensical.\nSo if you're curious, the way this animation is actually working is I'm taking the \n20 most probable next tokens that GPT-3 generates, \nwhich seems to be the maximum they'll give me, \nand then I tweak the probabilities based on an exponent of 1 5th.\nAs another bit of jargon, in the same way that you might call the components of \nthe output of this function probabilities, people often refer to the inputs as logits, \nor some people say logits, some people say logits, I'm gonna say logits.\nSo for instance, when you feed in some text, you have all these word embeddings \nflow through the network, and you do this final multiplication with the \nunembedding matrix, machine learning people would refer to the components in that raw, \nunnormalized output as the logits for the next word prediction.\nA lot of the goal with this chapter was to lay the foundations for \nunderstanding the attention mechanism, Karate Kid wax-on-wax-off style.\nYou see, if you have a strong intuition for word embeddings, for softmax, \nfor how dot products measure similarity, and also the underlying premise that \nmost of the calculations have to look like matrix multiplication with matrices \nfull of tunable parameters, then understanding the attention mechanism, \nthis cornerstone piece in the whole modern boom in AI, should be relatively smooth.\nFor that, come join me in the next chapter.\nAs I'm publishing this, a draft of that next chapter \nis available for review by Patreon supporters.\nA final version should be up in public in a week or two, \nit usually depends on how much I end up changing based on that review.\nIn the meantime, if you want to dive into attention, \nand if you want to help the channel out a little bit, it's there waiting.\n",
  "words": [
    "initials",
    "gpt",
    "stand",
    "generative",
    "pretrained",
    "transformer",
    "first",
    "word",
    "straightforward",
    "enough",
    "bots",
    "generate",
    "new",
    "text",
    "pretrained",
    "refers",
    "model",
    "went",
    "process",
    "learning",
    "massive",
    "amount",
    "data",
    "prefix",
    "insinuates",
    "room",
    "specific",
    "tasks",
    "additional",
    "training",
    "last",
    "word",
    "real",
    "key",
    "piece",
    "transformer",
    "specific",
    "kind",
    "neural",
    "network",
    "machine",
    "learning",
    "model",
    "core",
    "invention",
    "underlying",
    "current",
    "boom",
    "ai",
    "want",
    "video",
    "following",
    "chapters",
    "go",
    "explanation",
    "actually",
    "happens",
    "inside",
    "transformer",
    "going",
    "follow",
    "data",
    "flows",
    "go",
    "step",
    "step",
    "many",
    "different",
    "kinds",
    "models",
    "build",
    "using",
    "transformers",
    "models",
    "take",
    "audio",
    "produce",
    "transcript",
    "sentence",
    "comes",
    "model",
    "going",
    "way",
    "around",
    "producing",
    "synthetic",
    "speech",
    "text",
    "tools",
    "took",
    "world",
    "storm",
    "2022",
    "like",
    "dolly",
    "midjourney",
    "take",
    "text",
    "description",
    "produce",
    "image",
    "based",
    "transformers",
    "even",
    "ca",
    "quite",
    "get",
    "understand",
    "pie",
    "creature",
    "supposed",
    "still",
    "blown",
    "away",
    "kind",
    "thing",
    "even",
    "remotely",
    "possible",
    "original",
    "transformer",
    "introduced",
    "2017",
    "google",
    "invented",
    "specific",
    "use",
    "case",
    "translating",
    "text",
    "one",
    "language",
    "another",
    "variant",
    "focus",
    "type",
    "underlies",
    "tools",
    "like",
    "chatgpt",
    "model",
    "trained",
    "take",
    "piece",
    "text",
    "maybe",
    "even",
    "surrounding",
    "images",
    "sound",
    "accompanying",
    "produce",
    "prediction",
    "comes",
    "next",
    "passage",
    "prediction",
    "takes",
    "form",
    "probability",
    "distribution",
    "many",
    "different",
    "chunks",
    "text",
    "might",
    "follow",
    "first",
    "glance",
    "might",
    "think",
    "predicting",
    "next",
    "word",
    "feels",
    "like",
    "different",
    "goal",
    "generating",
    "new",
    "text",
    "prediction",
    "model",
    "like",
    "simple",
    "thing",
    "generate",
    "longer",
    "piece",
    "text",
    "give",
    "initial",
    "snippet",
    "work",
    "take",
    "random",
    "sample",
    "distribution",
    "generated",
    "append",
    "sample",
    "text",
    "run",
    "whole",
    "process",
    "make",
    "new",
    "prediction",
    "based",
    "new",
    "text",
    "including",
    "added",
    "know",
    "really",
    "feel",
    "like",
    "actually",
    "work",
    "animation",
    "example",
    "running",
    "laptop",
    "repeatedly",
    "predict",
    "sample",
    "next",
    "chunk",
    "text",
    "generate",
    "story",
    "based",
    "seed",
    "text",
    "story",
    "really",
    "make",
    "much",
    "sense",
    "swap",
    "api",
    "calls",
    "instead",
    "basic",
    "model",
    "much",
    "bigger",
    "suddenly",
    "almost",
    "magically",
    "get",
    "sensible",
    "story",
    "one",
    "even",
    "seems",
    "infer",
    "pi",
    "creature",
    "would",
    "live",
    "land",
    "math",
    "computation",
    "process",
    "repeated",
    "prediction",
    "sampling",
    "essentially",
    "happening",
    "interact",
    "chatgpt",
    "large",
    "language",
    "models",
    "see",
    "producing",
    "one",
    "word",
    "time",
    "fact",
    "one",
    "feature",
    "would",
    "much",
    "enjoy",
    "ability",
    "see",
    "underlying",
    "distribution",
    "new",
    "word",
    "chooses",
    "let",
    "kick",
    "things",
    "high",
    "level",
    "preview",
    "data",
    "flows",
    "transformer",
    "spend",
    "much",
    "time",
    "motivating",
    "interpreting",
    "expanding",
    "details",
    "step",
    "broad",
    "strokes",
    "one",
    "chatbots",
    "generates",
    "given",
    "word",
    "going",
    "hood",
    "first",
    "input",
    "broken",
    "bunch",
    "little",
    "pieces",
    "pieces",
    "called",
    "tokens",
    "case",
    "text",
    "tend",
    "words",
    "little",
    "pieces",
    "words",
    "common",
    "character",
    "combinations",
    "images",
    "sound",
    "involved",
    "tokens",
    "could",
    "little",
    "patches",
    "image",
    "little",
    "chunks",
    "sound",
    "one",
    "tokens",
    "associated",
    "vector",
    "meaning",
    "list",
    "numbers",
    "meant",
    "somehow",
    "encode",
    "meaning",
    "piece",
    "think",
    "vectors",
    "giving",
    "coordinates",
    "high",
    "dimensional",
    "space",
    "words",
    "similar",
    "meanings",
    "tend",
    "land",
    "vectors",
    "close",
    "space",
    "sequence",
    "vectors",
    "passes",
    "operation",
    "known",
    "attention",
    "block",
    "allows",
    "vectors",
    "talk",
    "pass",
    "information",
    "back",
    "forth",
    "update",
    "values",
    "example",
    "meaning",
    "word",
    "model",
    "phrase",
    "machine",
    "learning",
    "model",
    "different",
    "meaning",
    "phrase",
    "fashion",
    "model",
    "attention",
    "block",
    "responsible",
    "figuring",
    "words",
    "context",
    "relevant",
    "updating",
    "meanings",
    "words",
    "exactly",
    "meanings",
    "updated",
    "whenever",
    "use",
    "word",
    "meaning",
    "somehow",
    "entirely",
    "encoded",
    "entries",
    "vectors",
    "vectors",
    "pass",
    "different",
    "kind",
    "operation",
    "depending",
    "source",
    "reading",
    "referred",
    "perceptron",
    "maybe",
    "layer",
    "vectors",
    "talk",
    "go",
    "operation",
    "parallel",
    "block",
    "little",
    "bit",
    "harder",
    "interpret",
    "later",
    "talk",
    "step",
    "little",
    "bit",
    "like",
    "asking",
    "long",
    "list",
    "questions",
    "vector",
    "updating",
    "based",
    "answers",
    "questions",
    "operations",
    "blocks",
    "look",
    "like",
    "giant",
    "pile",
    "matrix",
    "multiplications",
    "primary",
    "job",
    "going",
    "understand",
    "read",
    "underlying",
    "matrices",
    "glossing",
    "details",
    "normalization",
    "steps",
    "happen",
    "preview",
    "process",
    "essentially",
    "repeats",
    "go",
    "back",
    "forth",
    "attention",
    "blocks",
    "perceptron",
    "blocks",
    "end",
    "hope",
    "essential",
    "meaning",
    "passage",
    "somehow",
    "baked",
    "last",
    "vector",
    "sequence",
    "perform",
    "certain",
    "operation",
    "last",
    "vector",
    "produces",
    "probability",
    "distribution",
    "possible",
    "tokens",
    "possible",
    "little",
    "chunks",
    "text",
    "might",
    "come",
    "next",
    "like",
    "said",
    "tool",
    "predicts",
    "comes",
    "next",
    "given",
    "snippet",
    "text",
    "feed",
    "little",
    "bit",
    "seed",
    "text",
    "repeatedly",
    "play",
    "game",
    "predicting",
    "comes",
    "next",
    "sampling",
    "distribution",
    "appending",
    "repeating",
    "know",
    "may",
    "remember",
    "long",
    "chatgpt",
    "came",
    "scene",
    "early",
    "demos",
    "looked",
    "like",
    "would",
    "autocomplete",
    "stories",
    "essays",
    "based",
    "initial",
    "snippet",
    "make",
    "tool",
    "like",
    "chatbot",
    "easiest",
    "starting",
    "point",
    "little",
    "bit",
    "text",
    "establishes",
    "setting",
    "user",
    "interacting",
    "helpful",
    "ai",
    "assistant",
    "would",
    "call",
    "system",
    "prompt",
    "would",
    "use",
    "user",
    "initial",
    "question",
    "prompt",
    "first",
    "bit",
    "dialogue",
    "start",
    "predicting",
    "helpful",
    "ai",
    "assistant",
    "would",
    "say",
    "response",
    "say",
    "step",
    "training",
    "required",
    "make",
    "work",
    "well",
    "high",
    "level",
    "idea",
    "chapter",
    "going",
    "expand",
    "details",
    "happens",
    "beginning",
    "network",
    "end",
    "network",
    "also",
    "want",
    "spend",
    "lot",
    "time",
    "reviewing",
    "important",
    "bits",
    "background",
    "knowledge",
    "things",
    "would",
    "second",
    "nature",
    "machine",
    "learning",
    "engineer",
    "time",
    "transformers",
    "came",
    "around",
    "comfortable",
    "background",
    "knowledge",
    "little",
    "impatient",
    "could",
    "feel",
    "free",
    "skip",
    "next",
    "chapter",
    "going",
    "focus",
    "attention",
    "blocks",
    "generally",
    "considered",
    "heart",
    "transformer",
    "want",
    "talk",
    "perceptron",
    "blocks",
    "training",
    "works",
    "number",
    "details",
    "skipped",
    "point",
    "broader",
    "context",
    "videos",
    "additions",
    "deep",
    "learning",
    "okay",
    "watched",
    "previous",
    "ones",
    "think",
    "order",
    "diving",
    "transformers",
    "specifically",
    "think",
    "worth",
    "making",
    "sure",
    "page",
    "basic",
    "premise",
    "structure",
    "deep",
    "learning",
    "risk",
    "stating",
    "obvious",
    "one",
    "approach",
    "machine",
    "learning",
    "describes",
    "model",
    "using",
    "data",
    "somehow",
    "determine",
    "model",
    "behaves",
    "mean",
    "let",
    "say",
    "want",
    "function",
    "takes",
    "image",
    "produces",
    "label",
    "describing",
    "example",
    "predicting",
    "next",
    "word",
    "given",
    "passage",
    "text",
    "task",
    "seems",
    "require",
    "element",
    "intuition",
    "pattern",
    "recognition",
    "almost",
    "take",
    "granted",
    "days",
    "idea",
    "machine",
    "learning",
    "rather",
    "trying",
    "explicitly",
    "define",
    "procedure",
    "task",
    "code",
    "people",
    "would",
    "done",
    "earliest",
    "days",
    "ai",
    "instead",
    "set",
    "flexible",
    "structure",
    "tunable",
    "parameters",
    "like",
    "bunch",
    "knobs",
    "dials",
    "somehow",
    "use",
    "many",
    "examples",
    "output",
    "look",
    "like",
    "given",
    "input",
    "tweak",
    "tune",
    "values",
    "parameters",
    "mimic",
    "behavior",
    "example",
    "maybe",
    "simplest",
    "form",
    "machine",
    "learning",
    "linear",
    "regression",
    "inputs",
    "outputs",
    "single",
    "numbers",
    "something",
    "like",
    "square",
    "footage",
    "house",
    "price",
    "want",
    "find",
    "line",
    "best",
    "fit",
    "data",
    "know",
    "predict",
    "future",
    "house",
    "prices",
    "line",
    "described",
    "two",
    "continuous",
    "parameters",
    "say",
    "slope",
    "goal",
    "linear",
    "regression",
    "determine",
    "parameters",
    "closely",
    "match",
    "data",
    "needless",
    "say",
    "deep",
    "learning",
    "models",
    "get",
    "much",
    "complicated",
    "example",
    "two",
    "175",
    "billion",
    "parameters",
    "thing",
    "given",
    "create",
    "giant",
    "model",
    "huge",
    "number",
    "parameters",
    "without",
    "either",
    "grossly",
    "overfitting",
    "training",
    "data",
    "completely",
    "intractable",
    "train",
    "deep",
    "learning",
    "describes",
    "class",
    "models",
    "last",
    "couple",
    "decades",
    "proven",
    "scale",
    "remarkably",
    "well",
    "unifies",
    "training",
    "algorithm",
    "called",
    "backpropagation",
    "context",
    "want",
    "go",
    "order",
    "training",
    "algorithm",
    "work",
    "well",
    "scale",
    "models",
    "follow",
    "certain",
    "specific",
    "format",
    "know",
    "format",
    "going",
    "helps",
    "explain",
    "many",
    "choices",
    "transformer",
    "processes",
    "language",
    "otherwise",
    "run",
    "risk",
    "feeling",
    "arbitrary",
    "first",
    "whatever",
    "model",
    "making",
    "input",
    "formatted",
    "array",
    "real",
    "numbers",
    "could",
    "mean",
    "list",
    "numbers",
    "could",
    "array",
    "often",
    "deal",
    "higher",
    "dimensional",
    "arrays",
    "general",
    "term",
    "used",
    "tensor",
    "often",
    "think",
    "input",
    "data",
    "progressively",
    "transformed",
    "many",
    "distinct",
    "layers",
    "layer",
    "always",
    "structured",
    "kind",
    "array",
    "real",
    "numbers",
    "get",
    "final",
    "layer",
    "consider",
    "output",
    "example",
    "final",
    "layer",
    "text",
    "processing",
    "model",
    "list",
    "numbers",
    "representing",
    "probability",
    "distribution",
    "possible",
    "next",
    "tokens",
    "deep",
    "learning",
    "model",
    "parameters",
    "almost",
    "always",
    "referred",
    "weights",
    "key",
    "feature",
    "models",
    "way",
    "parameters",
    "interact",
    "data",
    "processed",
    "weighted",
    "sums",
    "also",
    "sprinkle",
    "functions",
    "throughout",
    "wo",
    "depend",
    "parameters",
    "typically",
    "though",
    "instead",
    "seeing",
    "weighted",
    "sums",
    "naked",
    "written",
    "explicitly",
    "like",
    "instead",
    "find",
    "packaged",
    "together",
    "various",
    "components",
    "matrix",
    "vector",
    "product",
    "amounts",
    "saying",
    "thing",
    "think",
    "back",
    "matrix",
    "vector",
    "multiplication",
    "works",
    "component",
    "output",
    "looks",
    "like",
    "weighted",
    "sum",
    "often",
    "conceptually",
    "cleaner",
    "think",
    "matrices",
    "filled",
    "tunable",
    "parameters",
    "transform",
    "vectors",
    "drawn",
    "data",
    "processed",
    "example",
    "175",
    "billion",
    "weights",
    "organized",
    "distinct",
    "matrices",
    "matrices",
    "turn",
    "fall",
    "eight",
    "different",
    "categories",
    "going",
    "step",
    "one",
    "categories",
    "understand",
    "type",
    "go",
    "think",
    "kind",
    "fun",
    "reference",
    "specific",
    "numbers",
    "count",
    "exactly",
    "175",
    "billion",
    "come",
    "even",
    "nowadays",
    "bigger",
    "better",
    "models",
    "one",
    "certain",
    "charm",
    "model",
    "really",
    "capture",
    "world",
    "attention",
    "outside",
    "ml",
    "communities",
    "also",
    "practically",
    "speaking",
    "companies",
    "tend",
    "keep",
    "much",
    "tighter",
    "lips",
    "around",
    "specific",
    "numbers",
    "modern",
    "networks",
    "want",
    "set",
    "scene",
    "going",
    "peek",
    "hood",
    "see",
    "happens",
    "inside",
    "tool",
    "like",
    "chatgpt",
    "almost",
    "actual",
    "computation",
    "looks",
    "like",
    "matrix",
    "vector",
    "multiplication",
    "little",
    "bit",
    "risk",
    "getting",
    "lost",
    "sea",
    "billions",
    "numbers",
    "draw",
    "sharp",
    "distinction",
    "mind",
    "weights",
    "model",
    "always",
    "color",
    "blue",
    "red",
    "data",
    "processed",
    "always",
    "color",
    "gray",
    "weights",
    "actual",
    "brains",
    "things",
    "learned",
    "training",
    "determine",
    "behaves",
    "data",
    "processed",
    "simply",
    "encodes",
    "whatever",
    "specific",
    "input",
    "fed",
    "model",
    "given",
    "run",
    "like",
    "example",
    "snippet",
    "text",
    "foundation",
    "let",
    "dig",
    "first",
    "step",
    "text",
    "processing",
    "example",
    "break",
    "input",
    "little",
    "chunks",
    "turn",
    "chunks",
    "vectors",
    "mentioned",
    "chunks",
    "called",
    "tokens",
    "might",
    "pieces",
    "words",
    "punctuation",
    "every",
    "chapter",
    "especially",
    "next",
    "one",
    "like",
    "pretend",
    "broken",
    "cleanly",
    "words",
    "humans",
    "think",
    "words",
    "make",
    "much",
    "easier",
    "reference",
    "little",
    "examples",
    "clarify",
    "step",
    "model",
    "predefined",
    "vocabulary",
    "list",
    "possible",
    "words",
    "say",
    "first",
    "matrix",
    "encounter",
    "known",
    "embedding",
    "matrix",
    "single",
    "column",
    "one",
    "words",
    "columns",
    "determines",
    "vector",
    "word",
    "turns",
    "first",
    "step",
    "label",
    "like",
    "matrices",
    "see",
    "values",
    "begin",
    "random",
    "going",
    "learned",
    "based",
    "data",
    "turning",
    "words",
    "vectors",
    "common",
    "practice",
    "machine",
    "learning",
    "long",
    "transformers",
    "little",
    "weird",
    "never",
    "seen",
    "sets",
    "foundation",
    "everything",
    "follows",
    "let",
    "take",
    "moment",
    "get",
    "familiar",
    "often",
    "call",
    "embedding",
    "word",
    "invites",
    "think",
    "vectors",
    "geometrically",
    "points",
    "high",
    "dimensional",
    "space",
    "visualizing",
    "list",
    "three",
    "numbers",
    "coordinates",
    "points",
    "3d",
    "space",
    "would",
    "problem",
    "word",
    "embeddings",
    "tend",
    "much",
    "much",
    "higher",
    "dimensional",
    "dimensions",
    "see",
    "matters",
    "work",
    "space",
    "lot",
    "distinct",
    "directions",
    "way",
    "could",
    "take",
    "slice",
    "3d",
    "space",
    "project",
    "points",
    "onto",
    "slice",
    "sake",
    "animating",
    "word",
    "embeddings",
    "simple",
    "model",
    "giving",
    "going",
    "analogous",
    "thing",
    "choosing",
    "slice",
    "high",
    "dimensional",
    "space",
    "projecting",
    "word",
    "vectors",
    "onto",
    "displaying",
    "results",
    "big",
    "idea",
    "model",
    "tweaks",
    "tunes",
    "weights",
    "determine",
    "exactly",
    "words",
    "get",
    "embedded",
    "vectors",
    "training",
    "tends",
    "settle",
    "set",
    "embeddings",
    "directions",
    "space",
    "kind",
    "semantic",
    "meaning",
    "simple",
    "model",
    "running",
    "run",
    "search",
    "words",
    "whose",
    "embeddings",
    "closest",
    "tower",
    "notice",
    "seem",
    "give",
    "similar",
    "vibes",
    "want",
    "pull",
    "python",
    "play",
    "along",
    "home",
    "specific",
    "model",
    "using",
    "make",
    "animations",
    "transformer",
    "enough",
    "illustrate",
    "idea",
    "directions",
    "space",
    "carry",
    "semantic",
    "meaning",
    "classic",
    "example",
    "take",
    "difference",
    "vectors",
    "woman",
    "man",
    "something",
    "would",
    "visualize",
    "little",
    "vector",
    "connecting",
    "tip",
    "one",
    "tip",
    "similar",
    "difference",
    "king",
    "queen",
    "let",
    "say",
    "know",
    "word",
    "female",
    "monarch",
    "could",
    "find",
    "taking",
    "king",
    "adding",
    "direction",
    "searching",
    "embeddings",
    "closest",
    "point",
    "least",
    "kind",
    "despite",
    "classic",
    "example",
    "model",
    "playing",
    "true",
    "embedding",
    "queen",
    "actually",
    "little",
    "farther",
    "would",
    "suggest",
    "presumably",
    "way",
    "queen",
    "used",
    "training",
    "data",
    "merely",
    "feminine",
    "version",
    "king",
    "played",
    "around",
    "family",
    "relations",
    "seemed",
    "illustrate",
    "idea",
    "much",
    "better",
    "point",
    "looks",
    "like",
    "training",
    "model",
    "found",
    "advantageous",
    "choose",
    "embeddings",
    "one",
    "direction",
    "space",
    "encodes",
    "gender",
    "information",
    "another",
    "example",
    "take",
    "embedding",
    "italy",
    "subtract",
    "embedding",
    "germany",
    "add",
    "embedding",
    "hitler",
    "get",
    "something",
    "close",
    "embedding",
    "mussolini",
    "model",
    "learned",
    "associate",
    "directions",
    "others",
    "wwii",
    "axis",
    "leaders",
    "maybe",
    "favorite",
    "example",
    "vein",
    "models",
    "take",
    "difference",
    "germany",
    "japan",
    "add",
    "sushi",
    "end",
    "close",
    "bratwurst",
    "also",
    "playing",
    "game",
    "finding",
    "nearest",
    "neighbors",
    "pleased",
    "see",
    "close",
    "kat",
    "beast",
    "monster",
    "one",
    "bit",
    "mathematical",
    "intuition",
    "helpful",
    "mind",
    "especially",
    "next",
    "chapter",
    "dot",
    "product",
    "two",
    "vectors",
    "thought",
    "way",
    "measure",
    "well",
    "align",
    "computationally",
    "dot",
    "products",
    "involve",
    "multiplying",
    "corresponding",
    "components",
    "adding",
    "results",
    "good",
    "since",
    "much",
    "computation",
    "look",
    "like",
    "weighted",
    "sums",
    "geometrically",
    "dot",
    "product",
    "positive",
    "vectors",
    "point",
    "similar",
    "directions",
    "zero",
    "perpendicular",
    "negative",
    "whenever",
    "point",
    "opposite",
    "directions",
    "example",
    "let",
    "say",
    "playing",
    "model",
    "hypothesize",
    "embedding",
    "cats",
    "minus",
    "cat",
    "might",
    "represent",
    "sort",
    "plurality",
    "direction",
    "space",
    "test",
    "going",
    "take",
    "vector",
    "compute",
    "dot",
    "product",
    "embeddings",
    "certain",
    "singular",
    "nouns",
    "compare",
    "dot",
    "products",
    "corresponding",
    "plural",
    "nouns",
    "play",
    "around",
    "notice",
    "plural",
    "ones",
    "indeed",
    "seem",
    "consistently",
    "give",
    "higher",
    "values",
    "singular",
    "ones",
    "indicating",
    "align",
    "direction",
    "also",
    "fun",
    "take",
    "dot",
    "product",
    "embeddings",
    "words",
    "1",
    "2",
    "3",
    "give",
    "increasing",
    "values",
    "quantitatively",
    "measure",
    "plural",
    "model",
    "finds",
    "given",
    "word",
    "specifics",
    "words",
    "get",
    "embedded",
    "learned",
    "using",
    "data",
    "embedding",
    "matrix",
    "whose",
    "columns",
    "tell",
    "us",
    "happens",
    "word",
    "first",
    "pile",
    "weights",
    "model",
    "using",
    "numbers",
    "vocabulary",
    "size",
    "specifically",
    "technically",
    "consists",
    "words",
    "per",
    "se",
    "tokens",
    "embedding",
    "dimension",
    "multiplying",
    "tells",
    "us",
    "consists",
    "617",
    "million",
    "weights",
    "let",
    "go",
    "ahead",
    "add",
    "running",
    "tally",
    "remembering",
    "end",
    "count",
    "175",
    "billion",
    "case",
    "transformers",
    "really",
    "want",
    "think",
    "vectors",
    "embedding",
    "space",
    "merely",
    "representing",
    "individual",
    "words",
    "one",
    "thing",
    "also",
    "encode",
    "information",
    "position",
    "word",
    "talk",
    "later",
    "importantly",
    "think",
    "capacity",
    "soak",
    "context",
    "vector",
    "started",
    "life",
    "embedding",
    "word",
    "king",
    "example",
    "might",
    "progressively",
    "get",
    "tugged",
    "pulled",
    "various",
    "blocks",
    "network",
    "end",
    "points",
    "much",
    "specific",
    "nuanced",
    "direction",
    "somehow",
    "encodes",
    "king",
    "lived",
    "scotland",
    "achieved",
    "post",
    "murdering",
    "previous",
    "king",
    "described",
    "shakespearean",
    "language",
    "think",
    "understanding",
    "given",
    "word",
    "meaning",
    "word",
    "clearly",
    "informed",
    "surroundings",
    "sometimes",
    "includes",
    "context",
    "long",
    "distance",
    "away",
    "putting",
    "together",
    "model",
    "ability",
    "predict",
    "word",
    "comes",
    "next",
    "goal",
    "somehow",
    "empower",
    "incorporate",
    "context",
    "efficiently",
    "clear",
    "first",
    "step",
    "create",
    "array",
    "vectors",
    "based",
    "input",
    "text",
    "one",
    "simply",
    "plucked",
    "embedding",
    "matrix",
    "initially",
    "one",
    "encode",
    "meaning",
    "single",
    "word",
    "without",
    "input",
    "surroundings",
    "think",
    "primary",
    "goal",
    "network",
    "flows",
    "enable",
    "one",
    "vectors",
    "soak",
    "meaning",
    "much",
    "rich",
    "specific",
    "mere",
    "individual",
    "words",
    "could",
    "represent",
    "network",
    "process",
    "fixed",
    "number",
    "vectors",
    "time",
    "known",
    "context",
    "size",
    "trained",
    "context",
    "size",
    "2048",
    "data",
    "flowing",
    "network",
    "always",
    "looks",
    "like",
    "array",
    "2048",
    "columns",
    "dimensions",
    "context",
    "size",
    "limits",
    "much",
    "text",
    "transformer",
    "incorporate",
    "making",
    "prediction",
    "next",
    "word",
    "long",
    "conversations",
    "certain",
    "chatbots",
    "like",
    "early",
    "versions",
    "chatgpt",
    "often",
    "gave",
    "feeling",
    "bot",
    "kind",
    "losing",
    "thread",
    "conversation",
    "continued",
    "long",
    "go",
    "details",
    "attention",
    "due",
    "time",
    "skipping",
    "ahead",
    "want",
    "talk",
    "minute",
    "happens",
    "end",
    "remember",
    "desired",
    "output",
    "probability",
    "distribution",
    "tokens",
    "might",
    "come",
    "next",
    "example",
    "last",
    "word",
    "professor",
    "context",
    "includes",
    "words",
    "like",
    "harry",
    "potter",
    "immediately",
    "preceding",
    "see",
    "least",
    "favorite",
    "teacher",
    "also",
    "give",
    "leeway",
    "letting",
    "pretend",
    "tokens",
    "simply",
    "look",
    "like",
    "full",
    "words",
    "network",
    "built",
    "knowledge",
    "harry",
    "potter",
    "would",
    "presumably",
    "assign",
    "high",
    "number",
    "word",
    "snape",
    "involves",
    "two",
    "different",
    "steps",
    "first",
    "one",
    "use",
    "another",
    "matrix",
    "maps",
    "last",
    "vector",
    "context",
    "list",
    "values",
    "one",
    "token",
    "vocabulary",
    "function",
    "normalizes",
    "probability",
    "distribution",
    "called",
    "softmax",
    "talk",
    "second",
    "might",
    "seem",
    "little",
    "bit",
    "weird",
    "use",
    "last",
    "embedding",
    "make",
    "prediction",
    "last",
    "step",
    "thousands",
    "vectors",
    "layer",
    "sitting",
    "meanings",
    "fact",
    "training",
    "process",
    "turns",
    "much",
    "efficient",
    "use",
    "one",
    "vectors",
    "final",
    "layer",
    "simultaneously",
    "make",
    "prediction",
    "would",
    "come",
    "immediately",
    "lot",
    "said",
    "training",
    "later",
    "want",
    "call",
    "right",
    "matrix",
    "called",
    "unembedding",
    "matrix",
    "give",
    "label",
    "wu",
    "like",
    "weight",
    "matrices",
    "see",
    "entries",
    "begin",
    "random",
    "learned",
    "training",
    "process",
    "keeping",
    "score",
    "total",
    "parameter",
    "count",
    "unembedding",
    "matrix",
    "one",
    "row",
    "word",
    "vocabulary",
    "row",
    "number",
    "elements",
    "embedding",
    "dimension",
    "similar",
    "embedding",
    "matrix",
    "order",
    "swapped",
    "adds",
    "another",
    "617",
    "million",
    "parameters",
    "network",
    "meaning",
    "count",
    "far",
    "little",
    "billion",
    "small",
    "wholly",
    "insignificant",
    "fraction",
    "175",
    "billion",
    "end",
    "total",
    "last",
    "chapter",
    "want",
    "talk",
    "softmax",
    "function",
    "since",
    "makes",
    "another",
    "appearance",
    "us",
    "dive",
    "attention",
    "blocks",
    "idea",
    "want",
    "sequence",
    "numbers",
    "act",
    "probability",
    "distribution",
    "say",
    "distribution",
    "possible",
    "next",
    "words",
    "value",
    "0",
    "1",
    "also",
    "need",
    "add",
    "however",
    "playing",
    "learning",
    "game",
    "everything",
    "looks",
    "like",
    "multiplication",
    "outputs",
    "get",
    "default",
    "abide",
    "values",
    "often",
    "negative",
    "much",
    "bigger",
    "1",
    "almost",
    "certainly",
    "add",
    "softmax",
    "standard",
    "way",
    "turn",
    "arbitrary",
    "list",
    "numbers",
    "valid",
    "distribution",
    "way",
    "largest",
    "values",
    "end",
    "closest",
    "1",
    "smaller",
    "values",
    "end",
    "close",
    "really",
    "need",
    "know",
    "curious",
    "way",
    "works",
    "first",
    "raise",
    "e",
    "power",
    "numbers",
    "means",
    "list",
    "positive",
    "values",
    "take",
    "sum",
    "positive",
    "values",
    "divide",
    "term",
    "sum",
    "normalizes",
    "list",
    "adds",
    "notice",
    "one",
    "numbers",
    "input",
    "meaningfully",
    "bigger",
    "rest",
    "output",
    "corresponding",
    "term",
    "dominates",
    "distribution",
    "sampling",
    "almost",
    "certainly",
    "picking",
    "maximizing",
    "input",
    "softer",
    "picking",
    "max",
    "sense",
    "values",
    "similarly",
    "large",
    "also",
    "get",
    "meaningful",
    "weight",
    "distribution",
    "everything",
    "changes",
    "continuously",
    "continuously",
    "vary",
    "inputs",
    "situations",
    "like",
    "chatgpt",
    "using",
    "distribution",
    "create",
    "next",
    "word",
    "room",
    "little",
    "bit",
    "extra",
    "fun",
    "adding",
    "little",
    "extra",
    "spice",
    "function",
    "constant",
    "thrown",
    "denominator",
    "exponents",
    "call",
    "temperature",
    "since",
    "vaguely",
    "resembles",
    "role",
    "temperature",
    "certain",
    "thermodynamics",
    "equations",
    "effect",
    "larger",
    "give",
    "weight",
    "lower",
    "values",
    "meaning",
    "distribution",
    "little",
    "bit",
    "uniform",
    "smaller",
    "bigger",
    "values",
    "dominate",
    "aggressively",
    "extreme",
    "setting",
    "equal",
    "zero",
    "means",
    "weight",
    "goes",
    "maximum",
    "value",
    "example",
    "generate",
    "story",
    "seed",
    "text",
    "upon",
    "time",
    "use",
    "different",
    "temperatures",
    "case",
    "temperature",
    "zero",
    "means",
    "always",
    "goes",
    "predictable",
    "word",
    "get",
    "ends",
    "trite",
    "derivative",
    "goldilocks",
    "higher",
    "temperature",
    "gives",
    "chance",
    "choose",
    "less",
    "likely",
    "words",
    "comes",
    "risk",
    "case",
    "story",
    "starts",
    "originally",
    "young",
    "web",
    "artist",
    "south",
    "korea",
    "quickly",
    "degenerates",
    "nonsense",
    "technically",
    "speaking",
    "api",
    "actually",
    "let",
    "pick",
    "temperature",
    "bigger",
    "mathematical",
    "reason",
    "arbitrary",
    "constraint",
    "imposed",
    "keep",
    "tool",
    "seen",
    "generating",
    "things",
    "nonsensical",
    "curious",
    "way",
    "animation",
    "actually",
    "working",
    "taking",
    "20",
    "probable",
    "next",
    "tokens",
    "generates",
    "seems",
    "maximum",
    "give",
    "tweak",
    "probabilities",
    "based",
    "exponent",
    "1",
    "5th",
    "another",
    "bit",
    "jargon",
    "way",
    "might",
    "call",
    "components",
    "output",
    "function",
    "probabilities",
    "people",
    "often",
    "refer",
    "inputs",
    "logits",
    "people",
    "say",
    "logits",
    "people",
    "say",
    "logits",
    "gon",
    "na",
    "say",
    "logits",
    "instance",
    "feed",
    "text",
    "word",
    "embeddings",
    "flow",
    "network",
    "final",
    "multiplication",
    "unembedding",
    "matrix",
    "machine",
    "learning",
    "people",
    "would",
    "refer",
    "components",
    "raw",
    "unnormalized",
    "output",
    "logits",
    "next",
    "word",
    "prediction",
    "lot",
    "goal",
    "chapter",
    "lay",
    "foundations",
    "understanding",
    "attention",
    "mechanism",
    "karate",
    "kid",
    "style",
    "see",
    "strong",
    "intuition",
    "word",
    "embeddings",
    "softmax",
    "dot",
    "products",
    "measure",
    "similarity",
    "also",
    "underlying",
    "premise",
    "calculations",
    "look",
    "like",
    "matrix",
    "multiplication",
    "matrices",
    "full",
    "tunable",
    "parameters",
    "understanding",
    "attention",
    "mechanism",
    "cornerstone",
    "piece",
    "whole",
    "modern",
    "boom",
    "ai",
    "relatively",
    "smooth",
    "come",
    "join",
    "next",
    "chapter",
    "publishing",
    "draft",
    "next",
    "chapter",
    "available",
    "review",
    "patreon",
    "supporters",
    "final",
    "version",
    "public",
    "week",
    "two",
    "usually",
    "depends",
    "much",
    "end",
    "changing",
    "based",
    "review",
    "meantime",
    "want",
    "dive",
    "attention",
    "want",
    "help",
    "channel",
    "little",
    "bit",
    "waiting"
  ],
  "keywords": [
    "transformer",
    "first",
    "word",
    "generate",
    "new",
    "text",
    "model",
    "process",
    "learning",
    "data",
    "specific",
    "training",
    "last",
    "real",
    "piece",
    "kind",
    "network",
    "machine",
    "underlying",
    "ai",
    "want",
    "go",
    "actually",
    "happens",
    "going",
    "follow",
    "flows",
    "step",
    "many",
    "different",
    "models",
    "using",
    "transformers",
    "take",
    "produce",
    "comes",
    "way",
    "around",
    "like",
    "image",
    "based",
    "even",
    "get",
    "understand",
    "thing",
    "possible",
    "use",
    "case",
    "one",
    "language",
    "another",
    "chatgpt",
    "maybe",
    "sound",
    "prediction",
    "next",
    "passage",
    "probability",
    "distribution",
    "chunks",
    "might",
    "think",
    "predicting",
    "goal",
    "simple",
    "give",
    "initial",
    "snippet",
    "work",
    "random",
    "sample",
    "run",
    "make",
    "know",
    "really",
    "example",
    "running",
    "predict",
    "story",
    "seed",
    "much",
    "instead",
    "bigger",
    "almost",
    "seems",
    "would",
    "computation",
    "sampling",
    "see",
    "time",
    "let",
    "things",
    "high",
    "details",
    "given",
    "input",
    "little",
    "pieces",
    "called",
    "tokens",
    "tend",
    "words",
    "could",
    "vector",
    "meaning",
    "list",
    "numbers",
    "somehow",
    "encode",
    "vectors",
    "dimensional",
    "space",
    "similar",
    "meanings",
    "close",
    "sequence",
    "operation",
    "known",
    "attention",
    "block",
    "talk",
    "information",
    "back",
    "values",
    "context",
    "exactly",
    "perceptron",
    "layer",
    "bit",
    "later",
    "long",
    "blocks",
    "look",
    "matrix",
    "matrices",
    "end",
    "certain",
    "come",
    "tool",
    "play",
    "game",
    "point",
    "helpful",
    "call",
    "say",
    "well",
    "idea",
    "chapter",
    "also",
    "lot",
    "knowledge",
    "works",
    "number",
    "deep",
    "ones",
    "order",
    "making",
    "risk",
    "determine",
    "function",
    "label",
    "intuition",
    "people",
    "set",
    "tunable",
    "parameters",
    "output",
    "inputs",
    "single",
    "something",
    "find",
    "two",
    "175",
    "billion",
    "create",
    "arbitrary",
    "array",
    "often",
    "higher",
    "term",
    "distinct",
    "always",
    "final",
    "weights",
    "processed",
    "weighted",
    "sums",
    "components",
    "product",
    "multiplication",
    "looks",
    "sum",
    "turn",
    "fun",
    "count",
    "learned",
    "simply",
    "encodes",
    "vocabulary",
    "embedding",
    "columns",
    "everything",
    "points",
    "embeddings",
    "directions",
    "slice",
    "closest",
    "notice",
    "seem",
    "difference",
    "king",
    "queen",
    "adding",
    "direction",
    "playing",
    "add",
    "dot",
    "measure",
    "products",
    "corresponding",
    "since",
    "positive",
    "zero",
    "plural",
    "1",
    "us",
    "size",
    "understanding",
    "softmax",
    "unembedding",
    "weight",
    "means",
    "temperature",
    "logits"
  ]
}